{"cell_type":{"45b2317a":"code","3d7b3dfe":"code","e51ce7ca":"code","7c6d67d5":"code","a024322f":"code","2ac81bd9":"code","3e4a2f51":"code","ffa468db":"code","a6b3713c":"code","36b915ee":"code","eb120596":"code","7d4a5d03":"code","d542b089":"code","1a7d3979":"code","f1212407":"code","15695b56":"code","f9a6821d":"code","e38c8c1a":"code","7718959e":"code","9d25101f":"code","5db3f238":"code","b3bfe944":"code","3a534b0a":"code","361694cb":"code","81bb7102":"code","21950b0f":"code","09a17edb":"code","a56cf1ee":"code","931fcff7":"code","37d43f25":"code","7a7bc3b3":"code","bd4a2eb5":"code","d8404e48":"code","6f2885a2":"code","2b11eccc":"code","96f06da3":"code","7a03a480":"code","adc93ab9":"code","963e7629":"code","95b838f0":"code","5134d6b4":"code","5fefc88c":"code","80f7b8c6":"code","7102c336":"code","349924d6":"code","5484bf93":"code","6d489524":"code","08902ed1":"code","2914fa63":"code","f87143ad":"code","bafa269c":"code","e3f8151d":"code","08acc1fa":"code","b1521faf":"code","0627e120":"code","0816d526":"code","246c6c55":"code","54141a08":"code","4b5a2083":"code","af160af0":"code","41556c19":"code","55f38cd1":"code","8edf48ce":"code","ccc56478":"code","bfeb5c70":"code","52132838":"code","6cc08e45":"code","8380746b":"code","f6a9c3e0":"code","fc8b5fc5":"code","c027df74":"code","dd21762d":"code","7e8e1a64":"code","460eefbb":"code","a902f611":"code","e5900fac":"code","fed07bcf":"code","ef27a5d6":"code","479fdb6a":"code","be24bf47":"code","4be71c76":"code","b1e27ddc":"code","495932b1":"code","fc7bfcf0":"code","416e516f":"code","ab5aed49":"code","bb3de100":"code","d4d5af46":"code","cef24ada":"code","71d98a74":"code","fd27ed6d":"code","b57942a8":"code","86f26e08":"code","d2cb21d6":"code","4379315a":"code","47a98b9f":"code","05d82fd4":"code","58a1c874":"code","51566e32":"code","997e0e75":"code","395b6aac":"code","fa581483":"code","0b9f91a4":"code","cb523516":"code","6f1b32f7":"code","70f2c5d8":"code","e0b0d52f":"code","a86ddffb":"code","37127e99":"code","07ee7730":"code","3243f9ae":"code","eac91779":"code","1ba86a0c":"code","3a0bf77b":"code","89a1b4c6":"code","09bfa9ce":"code","9b9e363f":"code","c62b5128":"code","2a25117a":"code","c466317e":"code","bcc442d9":"code","01a1d40e":"code","119aed71":"code","488dee6f":"code","a5e933d8":"code","57c97b5e":"code","e970aafb":"code","7bf8c2b2":"code","f923f8b6":"code","57e1db09":"code","906eebed":"code","6994d6a0":"code","533aa583":"code","88d2174f":"code","29277427":"code","7cc0425a":"code","13dc1c2e":"code","b0fd7781":"code","f83437f3":"code","c318aef5":"code","954e329c":"code","31a3f92d":"code","38ac7172":"code","0c983083":"code","146998bf":"code","fdec1fa0":"code","cbb09576":"code","fc12fb81":"code","a22a9b9a":"code","25252e66":"code","779721c8":"code","405385c0":"code","a06e3fb3":"code","26481fd4":"code","7dc1a93e":"code","3ea25980":"code","c905a0f0":"code","c7617101":"code","ec8de374":"code","2c0ba257":"code","52c1a1ae":"code","3aa220ed":"code","67073239":"code","e5d65c07":"code","630e2976":"code","d5f864b3":"code","8d95e3b0":"code","72a76838":"code","2bd1fbc4":"code","31a778c4":"code","9fd9a80e":"code","ce677fc0":"code","29c40e69":"code","3b85c831":"code","3432d2ad":"code","ca4bcef5":"code","7624ef26":"code","163bae68":"code","13163af9":"code","bc1ab6bd":"code","05eb8fc9":"code","87ca2a0b":"code","ade3e879":"code","d8884838":"code","9426d07c":"code","0f81c080":"code","7cf43e13":"code","cd7da4e6":"code","6460be3c":"code","dfd658ba":"markdown","f94ae889":"markdown","0acdf4c9":"markdown","85c0c23e":"markdown","8749e493":"markdown","f863f4f5":"markdown","f28e570e":"markdown","8be33b7a":"markdown","01ccc5ba":"markdown","dc11c17e":"markdown","5356e4e4":"markdown","918321db":"markdown","99f01f36":"markdown","e44a3555":"markdown","8e1c6b62":"markdown","43a6fd93":"markdown","98f3ec65":"markdown","5848cafc":"markdown","6ab99ef2":"markdown","7e71ce20":"markdown","40387139":"markdown","edc3cd19":"markdown","5bfc0c03":"markdown","1ed08539":"markdown","49d5f12a":"markdown","5f1e1b06":"markdown","00cb880f":"markdown","97d255e9":"markdown","256bdcd5":"markdown","fde64c74":"markdown","526ff3fe":"markdown","ac65f859":"markdown","0209f025":"markdown","d3588eb5":"markdown","a0aad1e4":"markdown","7ebbc6ca":"markdown","f943ba73":"markdown","9dbf85f4":"markdown","222d4575":"markdown","61358873":"markdown","fbccdf55":"markdown","faa73832":"markdown","515c8b42":"markdown","7055fc2c":"markdown","c6652f1a":"markdown","c56be814":"markdown","1c848097":"markdown","78015f0b":"markdown","6ebfafa8":"markdown","9729722d":"markdown","96b39a1f":"markdown","f6fbf2c0":"markdown","899bd431":"markdown","90b15842":"markdown","60489901":"markdown","f9dcbc91":"markdown","07d3ea3e":"markdown","61f2a9a2":"markdown","ea301d7d":"markdown","77a1757d":"markdown","b6d7b70b":"markdown","3d817959":"markdown","6f7502c4":"markdown","10350905":"markdown","f7cfc140":"markdown","e5ab5935":"markdown","3f4a84fa":"markdown","d1045362":"markdown","42296b47":"markdown","43cf3d33":"markdown","dde8aac0":"markdown","bd07f70a":"markdown","371c066b":"markdown","f7049512":"markdown"},"source":{"45b2317a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport json, glob, cv2\nfrom math import copysign, log10\nfrom PIL import Image\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostClassifier, FeaturesData, Pool\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm, tqdm_notebook\n\npalette = sns.color_palette(\"Paired\")\nsns.set()\nsns.set_palette(palette)\n\nsplit_char = '\/'\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\".\/\"))\n\n# Any results you write to the current directory are saved as output.","3d7b3dfe":"# local\n# trainPath = '.\/train.csv'\n# testPath = '.\/test.csv'\n# trainSentimentPath = '.\/train_sentiment\/'\n# testSentimentPath = '.\/test_sentiment\/'\n# trainMetadataPath = '.\/train_metadata\/'\n# testMetadataPath = '.\/test_metadata\/'\n# trainImagePath = '.\/train_images\/'\n# testImagePath = '.\/test_images\/'\n# breedPath = '.\/breed_labels.csv'\n# colorPath = '.\/color_labels.csv'\n# statePath = 'state_labels.csv'\n# kaggle kernel\ntrainPath = '..\/input\/petfinder-adoption-prediction\/train\/train.csv'\ntestPath = '..\/input\/petfinder-adoption-prediction\/test\/test.csv'\ntrainSentimentPath = '..\/input\/petfinder-adoption-prediction\/train_sentiment\/'\ntestSentimentPath = '..\/input\/petfinder-adoption-prediction\/test_sentiment\/'\ntrainMetadataPath = '..\/input\/petfinder-adoption-prediction\/train_metadata\/'\ntestMetadataPath = '..\/input\/petfinder-adoption-prediction\/test_metadata\/'\ntrainImagePath = '..\/input\/petfinder-adoption-prediction\/train_images\/'\ntestImagePath = '..\/input\/petfinder-adoption-prediction\/test_images\/'\nbreedPath = '..\/input\/petfinder-adoption-prediction\/breed_labels.csv'\ncolorPath = '..\/input\/petfinder-adoption-prediction\/color_labels.csv'\nstatePath = '..\/input\/petfinder-adoption-prediction\/state_labels.csv'\ntrainPrecomputedPath = '..\/input\/precomputedfeaturespetfinder\/train_precomputed.csv'\ntestPrecomputedPath = '..\/input\/precomputedfeaturespetfinder\/test_precomputed.csv'","e51ce7ca":"categoricalFeatures = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n                       'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n                       'Health', 'State', 'RescuerID', 'AdoptionSpeed']\nnumericalFeatures = ['Age', 'Quantity', 'Fee', 'PhotoAmt', 'VideoAmt']","7c6d67d5":"#  local\n# train = pd.read_csv(trainPath)\n# test = pd.read_csv(testPath)\n# kaggle kernel\ntrain = pd.read_csv(trainPath)\ntest = pd.read_csv(testPath)\ntrain.info()","a024322f":"import cv2\nimport os\nfrom keras.applications.densenet import preprocess_input, DenseNet121","2ac81bd9":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","3e4a2f51":"img_size = 256\nbatch_size = 256","ffa468db":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\n\n# denseNetPath = '..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5'\ndenseNetPath = '..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5'\n\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=denseNetPath,\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","a6b3713c":"pet_ids = train['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(trainImagePath, pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","36b915ee":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]","eb120596":"pet_ids = test['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\n\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(testImagePath, pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","7d4a5d03":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]","d542b089":"train_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)","1a7d3979":"all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\nall_ids.shape","f1212407":"n_components = 32\nsvd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n\nfeatures_df = pd.concat([train_feats, test_feats], axis=0)\nfeatures = features_df[[f'pic_{i}' for i in range(256)]].values\n\nsvd_col = svd_.fit_transform(features)\nsvd_col = pd.DataFrame(svd_col)\nsvd_col = svd_col.add_prefix('IMG_SVD_')\n\nimg_features = pd.concat([all_ids, svd_col], axis=1)","15695b56":"img_features.info()","f9a6821d":"img_features.head()","e38c8c1a":"labels_breed = pd.read_csv(breedPath)\nlabels_state = pd.read_csv(colorPath)\nlabels_color = pd.read_csv(statePath)","7718959e":"train_image_files = sorted(glob.glob(trainImagePath + '*.jpg'))\ntrain_metadata_files = sorted(glob.glob(trainMetadataPath + '*.json'))\ntrain_sentiment_files = sorted(glob.glob(trainSentimentPath + '*.json'))\n\nprint(f'num of train images files: {len(train_image_files)}')\nprint(f'num of train metadata files: {len(train_metadata_files)}')\nprint(f'num of train sentiment files: {len(train_sentiment_files)}')\n\n\ntest_image_files = sorted(glob.glob(testImagePath + '*.jpg'))\ntest_metadata_files = sorted(glob.glob(testMetadataPath + '*.json'))\ntest_sentiment_files = sorted(glob.glob(testSentimentPath + '*.json'))\n\nprint(f'num of test images files: {len(test_image_files)}')\nprint(f'num of test metadata files: {len(test_metadata_files)}')\nprint(f'num of test sentiment files: {len(test_sentiment_files)}')","9d25101f":"# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ train_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ train_df_ids.shape[0]:.3f}')","5db3f238":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\n# Metadata:\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with metadata: {pets_with_metadatas \/ test_df_ids.shape[0]:.3f}')\n\n# Sentiment:\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split(split_char)[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint(f'fraction of pets with sentiment: {pets_with_sentiments \/ test_df_ids.shape[0]:.3f}')","b3bfe944":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        self.extract_sentiment_text = False\n    \n    def open_json_file(self, filename):\n        with open(filename, 'r', encoding='utf-8') as f:\n            json_file = json.load(f)\n        return json_file\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n        \n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns')\n        file_sentences_sentiment_df = pd.DataFrame(\n            {\n                'magnitude_sum': file_sentences_sentiment['magnitude'].sum(axis=0),\n                'score_sum': file_sentences_sentiment['score'].sum(axis=0),\n                'magnitude_mean': file_sentences_sentiment['magnitude'].mean(axis=0),\n                'score_mean': file_sentences_sentiment['score'].mean(axis=0),\n                'magnitude_var': file_sentences_sentiment['magnitude'].var(axis=0),\n                'score_var': file_sentences_sentiment['score'].var(axis=0),\n            }, index=[0]\n        )\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        df_sentiment = pd.concat([df_sentiment, file_sentences_sentiment_df], axis=1)\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = f'..\/input\/petfinder-adoption-prediction\/{mode}_sentiment\/{pet_id}.json'\n    try:\n        sentiment_file = pet_parser.open_json_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob(f'..\/input\/petfinder-adoption-prediction\/{mode}_metadata\/{pet_id}*.json'))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_json_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","3a534b0a":"debug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\ndfs_train = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\ndfs_test = Parallel(n_jobs=-1, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","361694cb":"aggregates = ['sum', 'mean', 'var']\nsent_agg = ['sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntrain_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index([f'{c[0]}_{c[1].upper()}' for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(sent_agg)\ntest_sentiment_gr.columns = pd.Index([f'{c[0]}' for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","81bb7102":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","21950b0f":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","09a17edb":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)","a56cf1ee":"X_temp = X.copy()\n\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\nto_drop_columns = ['PetID', 'Name', 'RescuerID']","931fcff7":"rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","37d43f25":"for i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","7a7bc3b3":"X_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('none')","bd4a2eb5":"X_temp['Length_Description'] = X_text['Description'].map(len)\nX_temp['Length_metadata_annots_top_desc'] = X_text['metadata_annots_top_desc'].map(len)\nX_temp['Lengths_sentiment_entities'] = X_text['sentiment_entities'].map(len)","d8404e48":"n_components = 16\ntext_features = []\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print(f'generating features from: {i}')\n    tfv = TfidfVectorizer(min_df=2,  max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('TFIDF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    \ntext_features = pd.concat(text_features, axis=1)\n\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","6f2885a2":"X_temp = X_temp.merge(img_features, how='left', on='PetID')","2b11eccc":"from PIL import Image\ntrain_df_ids = train[['PetID']]\ntest_df_ids = test[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var'],\n    'width': ['sum', 'mean', 'var'],\n    'height': ['sum', 'mean', 'var'],\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\nagg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)","96f06da3":"X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')","7a03a480":"# X_temp = X_temp.drop(to_drop_columns, axis=1)","adc93ab9":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\ntrain_cols = X_train.columns.tolist()\n# train_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","963e7629":"# save final train and test (with NA)\nX_train.to_csv('train_precomputed.csv', index=False)\nX_test.to_csv('test_precomputed.csv', index=False)","95b838f0":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)","5134d6b4":"X_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","5fefc88c":"X_train_non_null.shape, X_test_non_null.shape","80f7b8c6":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","7102c336":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","349924d6":"breedDf = pd.read_csv(breedPath)\nbreedDf.head()","5484bf93":"colorDf = pd.read_csv(colorPath)\ncolorDf.head()","6d489524":"stateDf = pd.read_csv(statePath)\nstateDf.head()","08902ed1":"def cleanTransformDataset(dataset, categoricalFeatures):\n    breedDf2 = breedDf.set_index('BreedID')\n    idx = breedDf2.to_dict()\n    dataset.Breed1 = dataset.Breed1.map(idx['BreedName'])\n    dataset.Breed2 = dataset.Breed2.map(idx['BreedName'])\n    \n    colorDf2 = colorDf.set_index('ColorID')\n    idx = colorDf2.to_dict()\n    dataset.Color1 = dataset.Color1.map(idx['ColorName'])\n    dataset.Color2 = dataset.Color2.map(idx['ColorName'])\n    dataset.Color3 = dataset.Color3.map(idx['ColorName'])\n    \n    stateDf2 = stateDf.set_index('StateID')\n    idx = stateDf2.to_dict()\n    dataset.State = dataset.State.map(idx['StateName'])\n    \n    # 1 = Dog, 2 = Cat\n    dataset.Type = dataset.Type.map({1: 'Dog', 2: 'Cat'})\n    # 1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets\n    dataset.Gender = dataset.Gender.map({1: 'Male', 2: 'Female', 3: 'Mixed'})\n    # 1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified\n    dataset.MaturitySize = dataset.MaturitySize.map({1: 'Small', 2: 'Medium', 3: 'Large', 4: 'Extra Large', 0: 'Not Specified'})\n    # 1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified\n    dataset.FurLength = dataset.FurLength.map({1: 'Short', 2: 'Medium', 3: 'Long', 0: 'Not Specified'})\n    # 1 = Yes, 2 = No, 3 = Not Sure\n    dataset.Vaccinated = dataset.Vaccinated.map({1: 'Yes', 2: 'No', 3: 'Not Sure'})\n    # 1 = Yes, 2 = No, 3 = Not Sure\n    dataset.Dewormed = dataset.Dewormed.map({1: 'Yes', 2: 'No', 3: 'Not Sure'})\n    # 1 = Yes, 2 = No, 3 = Not Sure\n    dataset.Sterilized = dataset.Sterilized.map({1: 'Yes', 2: 'No', 3: 'Not Sure'})\n    # 1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified\n    dataset.Health = dataset.Health.map({1: 'Healthy', 2: 'Minor Injury', 3: 'Serious Injury', 0: 'Not Specified'})\n    # transform to categorical\n    dataset[categoricalFeatures] = dataset[categoricalFeatures].astype('category')\n    return dataset","2914fa63":"train = cleanTransformDataset(train, categoricalFeatures)\ntest = cleanTransformDataset(test, list(set(categoricalFeatures) - set(['AdoptionSpeed'])))","f87143ad":"def extraFeatures(train):\n    # Color (Create a Flag pet has 1 color, 2 colors, 3 colors)\n    train['L_Color1'] = (pd.isnull(train['Color3']) & pd.isnull(train['Color2']) & pd.notnull(train['Color1'])).astype(int)\n    train['L_Color2'] = (pd.isnull(train['Color3']) & pd.notnull(train['Color2']) & pd.notnull(train['Color1'])).astype(int)\n    train['L_Color3'] = (pd.notnull(train['Color3']) & pd.notnull(train['Color2']) & pd.notnull(train['Color1'])).astype(int)\n\n    # Breed (create a flag if the pet has 1 breed or 2)\n    train['L_Breed1'] = (pd.isnull(train['Breed2']) & pd.notnull(train['Breed1'])).astype(int)\n    train['L_Breed2'] = (pd.notnull(train['Breed2']) & pd.notnull(train['Breed1'])).astype(int)\n\n    #Name (create a flag if the name is missing, with less than two letters)\n    train['Name_Length']= train['Name'].str.len()\n    train['L_Name_missing'] = (pd.isnull(train['Name'])).astype(int)\n\n    # Breed create columns\n    train['L_Breed1_Siamese'] =(train['Breed1']=='Siamese').astype(int)\n    train['L_Breed1_Persian']=(train['Breed1']=='Persian').astype(int)\n    train['L_Breed1_Labrador_Retriever']=(train['Breed1']=='Labrador Retriever').astype(int)\n    train['L_Breed1_Terrier']=(train['Breed1']=='Terrier').astype(int)\n    train['L_Breed1_Golden_Retriever ']=(train['Breed1']=='Golden Retriever').astype(int)\n\n    #Description \n    train['Description_Length']=train['Description'].str.len() \n\n    # Fee Amount\n    train['L_Fee_Free'] =  (train['Fee']==0).astype(int)\n\n    #Add the Number of Pets per Rescuer \n    pets_total = train.groupby(['RescuerID']).size().reset_index(name='N_pets_total')\n    train= pd.merge(train, pets_total, left_on='RescuerID', right_on='RescuerID', how='inner')\n    train.count()\n\n    # No photo\n    train['L_NoPhoto'] =  (train['PhotoAmt']==0).astype(int)\n\n    #No Video\n    train['L_NoVideo'] =  (train['VideoAmt']==0).astype(int)\n\n    #Log Age \n    train['Log_Age']= np.log(train.Age + 1) \n\n    #Quantity Amount >5\n    train.loc[train['Quantity'] > 5, 'Quantity'] = 5\n    return train","bafa269c":"train = extraFeatures(train)\ntest = extraFeatures(test)","e3f8151d":"train.describe()","08acc1fa":"train.info()","b1521faf":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericalFeatures = list(train.select_dtypes(include=numerics).columns)","0627e120":"train.isna().sum()","0816d526":"msno.matrix(train)","246c6c55":"numColumns = train.select_dtypes(include='number').columns.tolist()","54141a08":"len(numColumns)","4b5a2083":"i = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6, 4, figsize=(28,38))\nfor column in numColumns:\n    i += 1\n    plt.subplot(6, 4, i)\n    sns.distplot(train[column].dropna())\n    sns.distplot(test[column].dropna())\n    plt.legend(title=column, loc='upper left', labels=['train', 'test'])\nplt.show()","af160af0":"i = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(6, 4, figsize=(28,38))\nfor column in numColumns:\n    i += 1\n    plt.subplot(6, 4, i)\n    sns.kdeplot(train[column], bw=0.5)\n    sns.kdeplot(test[column], bw=0.5)\n    plt.legend(title=column, loc='upper left', labels=['train', 'test'])\nplt.show()","41556c19":"speeds = train.AdoptionSpeed.unique()\ndef plotDistributionPerTarget(data, num_rows, num_columns, size=(28,38)):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(num_rows, num_columns, figsize=size)\n    for column in data.columns:\n        if column == 'AdoptionSpeed':\n            continue\n        i += 1\n        plt.subplot(num_rows, num_columns, i)\n        for speed in speeds:\n            sns.kdeplot(data[data['AdoptionSpeed'] == speed][column], bw=0.5)\n        plt.legend(title=column, loc='upper left', labels=speeds)\n    plt.show()\n\nplotDistributionPerTarget(train[['AdoptionSpeed'] + numColumns], 6, 4)","55f38cd1":"sns.catplot(x=\"AdoptionSpeed\", kind=\"count\", data=train)","8edf48ce":"fig, ax = plt.subplots(figsize=(4,8))\nsns.boxplot(y=train.Age)","ccc56478":"fig, ax = plt.subplots(figsize=(10,6))\nsns.distplot(train.Age)","bfeb5c70":"sns.barplot(x=\"AdoptionSpeed\", y=\"Age\", data=train)","52132838":"sns.catplot(y=\"AdoptionSpeed\", x=\"Age\", data=train, orient=\"h\", kind=\"box\")","6cc08e45":"train['AgeInterval'] = pd.Series(['0-3', '3-6', '6-12', '12-24', '24-48', '48-120', '>120'], dtype='category')\ntrain.loc[(train['Age'] >= 0) & (train['Age'] <= 3),'AgeInterval'] = '0-3'\ntrain.loc[(train['Age'] > 3) & (train['Age'] <= 6),'AgeInterval'] = '3-6'\ntrain.loc[(train['Age'] > 6) & (train['Age'] <= 12),'AgeInterval'] = '6-12'\ntrain.loc[(train['Age'] > 12) & (train['Age'] <= 24),'AgeInterval'] = '12-24'\ntrain.loc[(train['Age'] > 24) & (train['Age'] <= 48),'AgeInterval'] = '24-48'\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 120),'AgeInterval'] = '48-120'\ntrain.loc[train['Age'] > 120,'AgeInterval'] = '>120'","8380746b":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x=\"AdoptionSpeed\", hue=\"AgeInterval\", data=train)","f6a9c3e0":"total = train[train.AgeInterval == '0-3'].size\ninterval1 = [\n train[(train.AdoptionSpeed == 0) & (train.AgeInterval == '0-3')].size,\n train[(train.AdoptionSpeed == 1) & (train.AgeInterval == '0-3')].size,\n train[(train.AdoptionSpeed == 2) & (train.AgeInterval == '0-3')].size,\n train[(train.AdoptionSpeed == 3) & (train.AgeInterval == '0-3')].size,\n train[(train.AdoptionSpeed == 4) & (train.AgeInterval == '0-3')].size\n] \/ total\n\npercents = np.append(interval1, [])\ntypes = (['0-3'] * 5)\n\nfeePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 1, hue='type', style='type', markers=True, data=feePercentDf)","fc8b5fc5":"fig, ax = plt.subplots(figsize=(10,6))\nsns.distplot(train.Quantity)","c027df74":"sns.jointplot(x=\"Quantity\", y=\"AdoptionSpeed\", data=train, kind=\"reg\", height=8)","dd21762d":"sns.barplot(x=\"AdoptionSpeed\", y=\"Quantity\", data=train)","7e8e1a64":"fig, ax = plt.subplots(figsize=(10,6))\nsns.distplot(train.VideoAmt)","460eefbb":"sns.jointplot(x=\"VideoAmt\", y=\"AdoptionSpeed\", data=train, kind=\"reg\", height=8)","a902f611":"sns.barplot(x=\"AdoptionSpeed\", y=\"VideoAmt\", data=train)","e5900fac":"fig, ax = plt.subplots(figsize=(10,6))\nsns.distplot(train.PhotoAmt)","fed07bcf":"sns.jointplot(x=\"PhotoAmt\", y=\"AdoptionSpeed\", data=train, kind=\"reg\", height=8)","ef27a5d6":"sns.barplot(x=\"AdoptionSpeed\", y=\"PhotoAmt\", data=train)","479fdb6a":"train['PhotoAmtInterval'] = pd.Series(['0', '1', '2', '3', '4', '5', '>5'], dtype='category')\ntrain.loc[train['PhotoAmt'] == 0 ,'PhotoAmtInterval'] = '0'\ntrain.loc[train['PhotoAmt'] == 1 ,'PhotoAmtInterval'] = '1'\ntrain.loc[train['PhotoAmt'] == 2 ,'PhotoAmtInterval'] = '2'\ntrain.loc[train['PhotoAmt'] == 3 ,'PhotoAmtInterval'] = '3'\ntrain.loc[train['PhotoAmt'] == 4 ,'PhotoAmtInterval'] = '4'\ntrain.loc[train['PhotoAmt'] == 5 ,'PhotoAmtInterval'] = '5'\ntrain.loc[train['PhotoAmt'] > 5 ,'PhotoAmtInterval'] = '>5'\n\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x='AdoptionSpeed', hue='PhotoAmtInterval', data=train)","be24bf47":"fig, ax = plt.subplots(figsize=(10,6))\nsns.distplot(train.Fee)","4be71c76":"train = train[train.Fee <= 1500]","b1e27ddc":"sns.jointplot(x='Fee', y='AdoptionSpeed', data=train, kind='reg', height=8)","495932b1":"sns.barplot(x='AdoptionSpeed', y='Fee', data=train)","fc7bfcf0":"train['FeeInterval'] = pd.Series(['Free', 'Paid'], dtype='category')\ntrain.loc[train['Fee'] == 0 ,'FeeInterval'] = 'Free'\ntrain.loc[train['Fee'] > 0 ,'FeeInterval'] = 'Paid'\n\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x='AdoptionSpeed', hue='FeeInterval', data=train)","416e516f":"total = train[train.Fee == 0].size\nfeeFreePercent = [\n train[(train.AdoptionSpeed == 0) & (train.Fee == 0)].size,\n train[(train.AdoptionSpeed == 1) & (train.Fee == 0)].size,\n train[(train.AdoptionSpeed == 2) & (train.Fee == 0)].size,\n train[(train.AdoptionSpeed == 3) & (train.Fee == 0)].size,\n train[(train.AdoptionSpeed == 4) & (train.Fee == 0)].size\n] \/ total\n\ntotal = train[train.Fee > 0].size\nfeePaidPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Fee > 0)].size,\n train[(train.AdoptionSpeed == 1) & (train.Fee > 0)].size,\n train[(train.AdoptionSpeed == 2) & (train.Fee > 0)].size,\n train[(train.AdoptionSpeed == 3) & (train.Fee > 0)].size,\n train[(train.AdoptionSpeed == 4) & (train.Fee > 0)].size\n] \/ total\n\npercents = np.append(feeFreePercent, feePaidPercent)\ntypes = (['free'] * 5) + (['paid'] * 5)\n\nfeePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 2, hue='type', style='type', markers=True, data=feePercentDf)","ab5aed49":"# 1 = Dog, 2 = Cat\nsns.catplot(x=\"Type\", kind=\"count\", data=train)","bb3de100":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x='AdoptionSpeed', hue='Type', data=train)","d4d5af46":"total = train[train.Type == 'Dog'].size\ndogPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Dog')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Dog')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Dog')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Dog')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Dog')].size\n] \/ total\n\ntotal = train[train.Type == 'Cat'].size\ncatPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Cat')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Cat')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Cat')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Cat')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Cat')].size\n] \/ total\n\npercents = np.append(dogPercent, catPercent)\ntypes = (['dog'] * 5) + (['cat'] * 5)\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 2, hue='type', style='type', markers=True, data=typePercentDf)","cef24ada":"dogCounts = train[train.Type == 'Dog'].Breed1.value_counts()\nfig, ax = plt.subplots(figsize=(8,8))\ndogCounts.nlargest(15).plot(kind='bar')","71d98a74":"catCounts = train[train.Type == 'Cat'].Breed1.value_counts()\nfig, ax = plt.subplots(figsize=(8,8))\ncatCounts.nlargest(15).plot(kind='bar')","fd27ed6d":"strayCat = ['Domestic Short Hair', 'Domestic Medium Hair', 'Domestic Long Hair']\nstrayDog = ['Mixed Breed']\n\ntrain.loc[(train.Breed1.isin(strayDog)) & (train.Type == 'Dog'), 'Breed1Type'] = 'Stray'\ntrain.loc[(train.Breed1.isin(strayCat)) & (train.Type == 'Cat'), 'Breed1Type'] = 'Stray'\ntrain.loc[(~train.Breed1.isin(strayDog)) & (train.Type == 'Dog'), 'Breed1Type'] = 'Breed'\ntrain.loc[(~train.Breed1.isin(strayCat)) & (train.Type == 'Cat'), 'Breed1Type'] = 'Breed'","b57942a8":"sns.countplot(x='AdoptionSpeed', hue='Breed1Type', data=train[train.Type == 'Dog'])","86f26e08":"sns.countplot(x='AdoptionSpeed', hue='Breed1Type', data=train[train.Type == 'Cat'])","d2cb21d6":"total = train[(train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size\ndogPercentStray = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Dog') & (train.Breed1Type == 'Stray')].size\n] \/ total\n\ntotal = train[(train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size\ndogPercentBreed = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Dog') & (train.Breed1Type == 'Breed')].size\n] \/ total\n\ntotal = train[(train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size\ncatPercentStray = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Cat') & (train.Breed1Type == 'Stray')].size\n] \/ total\n\ntotal = train[(train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size\ncatPercentBreed = [\n train[(train.AdoptionSpeed == 0) & (train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 1) & (train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 2) & (train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 3) & (train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size,\n train[(train.AdoptionSpeed == 4) & (train.Type == 'Cat') & (train.Breed1Type == 'Breed')].size\n] \/ total\n\npercents = np.append(dogPercentStray, dogPercentBreed)\npercents = np.append(percents, catPercentStray)\npercents = np.append(percents, catPercentBreed)\ntypes = (['dog-stray'] * 5 + ['dog-breed'] * 5 + ['cat-stray'] * 5 + ['cat-breed'] * 5)\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 4, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","4379315a":"sns.catplot(x='Color1', kind='count', data=train)","47a98b9f":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='Color1', data=train)","05d82fd4":"sns.catplot(x='MaturitySize', kind='count', data=train)","58a1c874":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='MaturitySize', data=train)","51566e32":"total = train[(train.MaturitySize == 'Large')].size\nlargePercent = [\n train[(train.AdoptionSpeed == 0) & (train.MaturitySize == 'Large')].size,\n train[(train.AdoptionSpeed == 1) & (train.MaturitySize == 'Large')].size,\n train[(train.AdoptionSpeed == 2) & (train.MaturitySize == 'Large')].size,\n train[(train.AdoptionSpeed == 3) & (train.MaturitySize == 'Large')].size,\n train[(train.AdoptionSpeed == 4) & (train.MaturitySize == 'Large')].size\n] \/ total\n\ntotal = train[(train.MaturitySize == 'Medium')].size\nmediumPercent = [\n train[(train.AdoptionSpeed == 0) & (train.MaturitySize == 'Medium')].size,\n train[(train.AdoptionSpeed == 1) & (train.MaturitySize == 'Medium')].size,\n train[(train.AdoptionSpeed == 2) & (train.MaturitySize == 'Medium')].size,\n train[(train.AdoptionSpeed == 3) & (train.MaturitySize == 'Medium')].size,\n train[(train.AdoptionSpeed == 4) & (train.MaturitySize == 'Medium')].size\n] \/ total\n\ntotal = train[(train.MaturitySize == 'Small')].size\nsmallPercent = [\n train[(train.AdoptionSpeed == 0) & (train.MaturitySize == 'Small')].size,\n train[(train.AdoptionSpeed == 1) & (train.MaturitySize == 'Small')].size,\n train[(train.AdoptionSpeed == 2) & (train.MaturitySize == 'Small')].size,\n train[(train.AdoptionSpeed == 3) & (train.MaturitySize == 'Small')].size,\n train[(train.AdoptionSpeed == 4) & (train.MaturitySize == 'Small')].size\n] \/ total\n\npercents = np.append(largePercent, mediumPercent)\npercents = np.append(percents, smallPercent)\ntypes = (['Large'] * 5 + ['Medium'] * 5 + ['Small'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","997e0e75":"sns.catplot(x='FurLength', kind='count', data=train)","395b6aac":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='FurLength', data=train)","fa581483":"total = train[(train.FurLength == 'Long')].size\nlongPercent = [\n train[(train.AdoptionSpeed == 0) & (train.FurLength == 'Long')].size,\n train[(train.AdoptionSpeed == 1) & (train.FurLength == 'Long')].size,\n train[(train.AdoptionSpeed == 2) & (train.FurLength == 'Long')].size,\n train[(train.AdoptionSpeed == 3) & (train.FurLength == 'Long')].size,\n train[(train.AdoptionSpeed == 4) & (train.FurLength == 'Long')].size\n] \/ total\n\ntotal = train[(train.FurLength == 'Medium')].size\nmediumPercent = [\n train[(train.AdoptionSpeed == 0) & (train.FurLength == 'Medium')].size,\n train[(train.AdoptionSpeed == 1) & (train.FurLength == 'Medium')].size,\n train[(train.AdoptionSpeed == 2) & (train.FurLength == 'Medium')].size,\n train[(train.AdoptionSpeed == 3) & (train.FurLength == 'Medium')].size,\n train[(train.AdoptionSpeed == 4) & (train.FurLength == 'Medium')].size\n] \/ total\n\ntotal = train[(train.FurLength == 'Short')].size\nshortPercent = [\n train[(train.AdoptionSpeed == 0) & (train.FurLength == 'Short')].size,\n train[(train.AdoptionSpeed == 1) & (train.FurLength == 'Short')].size,\n train[(train.AdoptionSpeed == 2) & (train.FurLength == 'Short')].size,\n train[(train.AdoptionSpeed == 3) & (train.FurLength == 'Short')].size,\n train[(train.AdoptionSpeed == 4) & (train.FurLength == 'Short')].size\n] \/ total\n\npercents = np.append(longPercent, mediumPercent)\npercents = np.append(percents, shortPercent)\ntypes = (['Long'] * 5 + ['Medium'] * 5 + ['Short'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","0b9f91a4":"sns.catplot(x='Vaccinated', kind='count', data=train)","cb523516":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='Vaccinated', data=train)","6f1b32f7":"total = train[(train.Vaccinated == 'Yes')].size\nyesPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Vaccinated == 'Yes')].size,\n train[(train.AdoptionSpeed == 1) & (train.Vaccinated == 'Yes')].size,\n train[(train.AdoptionSpeed == 2) & (train.Vaccinated == 'Yes')].size,\n train[(train.AdoptionSpeed == 3) & (train.Vaccinated == 'Yes')].size,\n train[(train.AdoptionSpeed == 4) & (train.Vaccinated == 'Yes')].size\n] \/ total\n\ntotal = train[(train.Vaccinated == 'No')].size\nnoPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Vaccinated == 'No')].size,\n train[(train.AdoptionSpeed == 1) & (train.Vaccinated == 'No')].size,\n train[(train.AdoptionSpeed == 2) & (train.Vaccinated == 'No')].size,\n train[(train.AdoptionSpeed == 3) & (train.Vaccinated == 'No')].size,\n train[(train.AdoptionSpeed == 4) & (train.Vaccinated == 'No')].size\n] \/ total\n\ntotal = train[(train.Vaccinated == 'Not Sure')].size\nnotSurePercent = [\n train[(train.AdoptionSpeed == 0) & (train.Vaccinated == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 1) & (train.Vaccinated == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 2) & (train.Vaccinated == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 3) & (train.Vaccinated == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 4) & (train.Vaccinated == 'Not Sure')].size\n] \/ total\n\npercents = np.append(yesPercent, noPercent)\npercents = np.append(percents, notSurePercent)\ntypes = (['Yes'] * 5 + ['No'] * 5 + ['Not Sure'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","70f2c5d8":"sns.catplot(x='Dewormed', kind='count', data=train)","e0b0d52f":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='Dewormed', data=train)","a86ddffb":"total = train[(train.Dewormed == 'Yes')].size\nyesPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Dewormed == 'Yes')].size,\n train[(train.AdoptionSpeed == 1) & (train.Dewormed == 'Yes')].size,\n train[(train.AdoptionSpeed == 2) & (train.Dewormed == 'Yes')].size,\n train[(train.AdoptionSpeed == 3) & (train.Dewormed == 'Yes')].size,\n train[(train.AdoptionSpeed == 4) & (train.Dewormed == 'Yes')].size\n] \/ total\n\ntotal = train[(train.Dewormed == 'No')].size\nnoPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Dewormed == 'No')].size,\n train[(train.AdoptionSpeed == 1) & (train.Dewormed == 'No')].size,\n train[(train.AdoptionSpeed == 2) & (train.Dewormed == 'No')].size,\n train[(train.AdoptionSpeed == 3) & (train.Dewormed == 'No')].size,\n train[(train.AdoptionSpeed == 4) & (train.Dewormed == 'No')].size\n] \/ total\n\ntotal = train[(train.Dewormed == 'Not Sure')].size\nnotSurePercent = [\n train[(train.AdoptionSpeed == 0) & (train.Dewormed == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 1) & (train.Dewormed == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 2) & (train.Dewormed == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 3) & (train.Dewormed == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 4) & (train.Dewormed == 'Not Sure')].size\n] \/ total\n\npercents = np.append(yesPercent, noPercent)\npercents = np.append(percents, notSurePercent)\ntypes = (['Yes'] * 5 + ['No'] * 5 + ['Not Sure'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","37127e99":"sns.catplot(x='Sterilized', kind='count', data=train)","07ee7730":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='Sterilized', data=train)","3243f9ae":"total = train[(train.Sterilized == 'Yes')].size\nyesPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Sterilized == 'Yes')].size,\n train[(train.AdoptionSpeed == 1) & (train.Sterilized == 'Yes')].size,\n train[(train.AdoptionSpeed == 2) & (train.Sterilized == 'Yes')].size,\n train[(train.AdoptionSpeed == 3) & (train.Sterilized == 'Yes')].size,\n train[(train.AdoptionSpeed == 4) & (train.Sterilized == 'Yes')].size\n] \/ total\n\ntotal = train[(train.Sterilized == 'No')].size\nnoPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Sterilized == 'No')].size,\n train[(train.AdoptionSpeed == 1) & (train.Sterilized == 'No')].size,\n train[(train.AdoptionSpeed == 2) & (train.Sterilized == 'No')].size,\n train[(train.AdoptionSpeed == 3) & (train.Sterilized == 'No')].size,\n train[(train.AdoptionSpeed == 4) & (train.Sterilized == 'No')].size\n] \/ total\n\ntotal = train[(train.Sterilized == 'Not Sure')].size\nnotSurePercent = [\n train[(train.AdoptionSpeed == 0) & (train.Sterilized == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 1) & (train.Sterilized == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 2) & (train.Sterilized == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 3) & (train.Sterilized == 'Not Sure')].size,\n train[(train.AdoptionSpeed == 4) & (train.Sterilized == 'Not Sure')].size\n] \/ total\n\npercents = np.append(yesPercent, noPercent)\npercents = np.append(percents, notSurePercent)\ntypes = (['Yes'] * 5 + ['No'] * 5 + ['Not Sure'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","eac91779":"sns.catplot(x='Health', kind='count', data=train)","1ba86a0c":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x='AdoptionSpeed', hue='Health', data=train)","3a0bf77b":"total = train[(train.Health == 'Healthy')].size\nhealthyPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Health == 'Healthy')].size,\n train[(train.AdoptionSpeed == 1) & (train.Health == 'Healthy')].size,\n train[(train.AdoptionSpeed == 2) & (train.Health == 'Healthy')].size,\n train[(train.AdoptionSpeed == 3) & (train.Health == 'Healthy')].size,\n train[(train.AdoptionSpeed == 4) & (train.Health == 'Healthy')].size\n] \/ total\n\ntotal = train[(train.Health == 'Minor Injury')].size\nminorPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Health == 'Minor Injury')].size,\n train[(train.AdoptionSpeed == 1) & (train.Health == 'Minor Injury')].size,\n train[(train.AdoptionSpeed == 2) & (train.Health == 'Minor Injury')].size,\n train[(train.AdoptionSpeed == 3) & (train.Health == 'Minor Injury')].size,\n train[(train.AdoptionSpeed == 4) & (train.Health == 'Minor Injury')].size\n] \/ total\n\ntotal = train[(train.Health == 'Serious Injury')].size\nseriousPercent = [\n train[(train.AdoptionSpeed == 0) & (train.Health == 'Serious Injury')].size,\n train[(train.AdoptionSpeed == 1) & (train.Health == 'Serious Injury')].size,\n train[(train.AdoptionSpeed == 2) & (train.Health == 'Serious Injury')].size,\n train[(train.AdoptionSpeed == 3) & (train.Health == 'Serious Injury')].size,\n train[(train.AdoptionSpeed == 4) & (train.Health == 'Serious Injury')].size\n] \/ total\n\npercents = np.append(healthyPercent, minorPercent)\npercents = np.append(percents, seriousPercent)\ntypes = (['Healthy'] * 5 + ['Minor Injury'] * 5 + ['Serious Injury'] * 5 )\n\ntypePercentDf = pd.DataFrame({ 'percent': percents, 'type': types })\n\nfig, ax = plt.subplots(figsize=(8,8))\nsns.lineplot(y='percent', x=[0, 1, 2, 3, 4] * 3, hue='type', style='type', markers=True, data=typePercentDf, palette='coolwarm')","89a1b4c6":"target = 'AdoptionSpeed'\n# categoricalFeatures = list(set(categoricalFeatures) - set([target, 'State', 'RescuerID', 'PetID', 'Color2', 'Color3', 'Breed2']))\n#numericalFeatures = ['Age', 'Quantity', 'Fee', 'PhotoAmt', 'VideoAmt']\n# catFeaturesIndex = list(range(0, len(categoricalFeatures)))\n\n# cateogrical features to train\ncategorical = ['Type', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'State']\n\nfeatures = categorical + numericalFeatures\ndata = train[features + [target]].dropna()\n\n# X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.25, random_state=42)","09bfa9ce":"# train_data = FeaturesData(\n#     num_feature_data=X_train[numericalFeatures].astype('float32').values,\n#     cat_feature_data=X_train[categoricalFeatures].__array__(dtype=object)\n# )\n\n# train_labels = y_train.astype('int').values\n\n# clf = CatBoostClassifier(loss_function='MultiClass', verbose=True, depth=10, iterations= 100, l2_leaf_reg= 9, learning_rate= 0.15)\n# clf.fit(train_data, train_labels)","9b9e363f":"# test_data = FeaturesData(\n#     num_feature_data=X_test[numericalFeatures].astype('float32').values,\n#     cat_feature_data=X_test[categoricalFeatures].__array__(dtype=object)\n# )\n\n# test_labels = y_test.astype('int').values\n# y_predicted = clf.predict(test_data)\n\ndef generateConfusionMatrix(y_real, y_predicted):\n    cm = pd.DataFrame()\n    cm['Satisfaction'] = y_real\n    cm['Predict'] = y_predicted\n    mappingSatisfaction = {0:'Same Day', 1: 'First Week', 2: 'First Month', 3: '2-3 Month', 4: 'Non-Adopted >100'}\n    mappingPredict = {0.0:'Same Day', 1.0: 'First Week', 2.0: 'First Month', 3.0: '2-3 Month', 4.0: 'Non-Adopted >100'}\n    cm = cm.replace({'Satisfaction': mappingSatisfaction, 'Predict': mappingPredict})\n    return pd.crosstab(cm['Satisfaction'], cm['Predict'], margins=True)\n\n# generateConfusionMatrix(y_test, y_predicted)","c62b5128":"# clf.score(test_data, test_labels)","2a25117a":"def plotMostRelevantFeatures(indexes, model, train_data, train_labels, title='Feature Importance Ranking'):\n    feature_score = pd.DataFrame(list(zip(indexes, model.get_feature_importance(Pool(train_data, label=train_labels)))),\n                columns=['Feature','Score'])\n    feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n    plt.rcParams[\"figure.figsize\"] = (12,7)\n    ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n    ax.set_title(title, fontsize = 14)\n    ax.set_xlabel('')\n\n    rects = ax.patches\n\n    # get feature score as labels round to 2 decimal\n    labels = feature_score['Score'].round(2)\n\n    for rect, label in zip(rects, labels):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='center', va='bottom')\n\n    plt.show()\n\n# plotMostRelevantFeatures(X_train.dtypes.index, clf, train_data, train_labels)","c466317e":"# bestCatFeatures = ['Sterilized', 'Breed1', 'Type', 'MaturitySize', 'FurLength', 'Gender', 'Health', 'Color1', 'Fee', 'Vaccinated']\n# bestNumFeatures = ['Age', 'Quantity', 'PhotoAmt']\n\n# train_data = FeaturesData(\n#     num_feature_data=X_train[bestNumFeatures].astype('float32').values,\n#     cat_feature_data=X_train[bestCatFeatures].__array__(dtype=object)\n# )\n\n# train_labels = y_train.astype('int').values\n\n# clf = CatBoostClassifier(loss_function='MultiClass', verbose=False, depth=10, iterations= 100, l2_leaf_reg= 9, learning_rate= 0.15)\n# clf.fit(data[mostImportantFeatures], data[target], cat_features= mostImportantCatIndex, plot=False)\n# predictions = clf.predict(test[mostImportantFeatures])\n# predictions","bcc442d9":"# test['AdoptionSpeed'] = predictions\n# test.AdoptionSpeed = test['AdoptionSpeed'].map({0.0: '0', 1.0: '1', 2.0: '2', 3.0: '3', 4.0: '4'})\n# test[['PetID', 'AdoptionSpeed']].to_csv('submission.csv', index=False)","01a1d40e":"def calculateClassificationScores(y_true, y_predicted, model, X_test, average='macro'):\n    accuracy = accuracy_score(y_true, y_predicted)\n    f1 = f1_score(y_true, y_predicted, average=average)\n    precision = precision_score(y_true, y_predicted, average=average)\n    recall = recall_score(y_true, y_predicted, average=average)\n    if model and isinstance(model, CatBoostClassifier):\n        score = model.get_best_score()\n        if score.get('validation_0'):\n            multiclass = score['validation_0']['MultiClass']\n        else:\n            multiclass = model.score(X_test, y_true)\n        return (accuracy, f1, precision, recall, multiclass)\n    \n    return (accuracy, f1, precision, recall)\n\ndef crossValidation(params, X, y):\n    f1_scores = []\n    accuracy_scores = []\n    precision_scores = []\n    recall_scores = []\n    multiclass_scores = []\n\n    for train_index, val_index in skf.split(X.values, y.values):\n        \n        X_train = X[X.index.isin(train_index)]\n        X_train = FeaturesData(\n            num_feature_data=X_train[numericalFeatures].astype('float32').values,\n            cat_feature_data=X_train[categoricalFeatures].__array__(dtype=object)\n        )\n        y_train = y[y.index.isin(train_index)].astype('int').values\n        \n        X_valid = X[X.index.isin(val_index)]\n        X_valid = FeaturesData(\n            num_feature_data=X_valid[numericalFeatures].astype('float32').values,\n            cat_feature_data=X_valid[categoricalFeatures].__array__(dtype=object)\n        )\n        y_valid = y[y.index.isin(val_index)].astype('int').values\n        \n        pool_test = Pool(X_valid, label=y_valid)\n        \n        clf = CatBoostClassifier(\n            loss_function='MultiClass',\n            verbose=False,\n            depth=params['depth'],\n            iterations=params['iterations'],\n            l2_leaf_reg=params['l2_leaf_reg'],\n            learning_rate=params['learning_rate'],\n            task_type='CPU'\n        )\n        \n        clf.fit(X_train, y_train, eval_set=pool_test, use_best_model=True)\n        \n        y_pred = clf.predict(X_valid)\n        \n        # calculateClassificationScores(y_test, y_predicted, clfSentiment, X_test_pool)\n        \n        (accuracy, f1, precision, recall, multiclass) = calculateClassificationScores(y_valid, y_pred, clf, X_valid)\n        \n        multiclass_scores.append(multiclass)\n        accuracy_scores.append(accuracy)\n        f1_scores.append(f1)\n        precision_scores.append(precision)\n        recall_scores.append(recall)\n        \n    return (multiclass_scores, accuracy_scores, f1_scores, precision_scores, recall_scores)\n    \n\ndef searchBestParams(grid, X, y):\n    catboostDf = pd.DataFrame({\n        'model':[],\n        'multiclass_score_mean':[],\n        'multiclass_score_std':[],\n        'f1_score_mean':[],\n        'f1_score_std':[],\n        'accuracy_score_mean':[],\n        'accuracy_score_std':[],\n        'precision_score_mean':[],\n        'precision_score_std':[],\n        'recall_score_mean':[],\n        'recall_score_std':[],\n        'params': []}\n    )\n    for params in grid:\n        print(params)\n        (multiclass_scores, accuracy_scores, f1_scores, precision_scores, recall_scores) = crossValidation(params, X, y)\n        catboostDf = catboostDf.append({\n            'multiclass_score_mean': np.mean(multiclass_scores),\n            'multiclass_score_std': np.std(multiclass_scores),\n            'f1_score_mean': np.mean(f1_scores),\n            'f1_score_std': np.std(f1_scores),\n            'accuracy_score_mean': np.mean(accuracy_scores),\n            'accuracy_score_std': np.std(accuracy_scores),\n            'precision_score_mean': np.mean(precision_scores),\n            'precision_score_std': np.std(precision_scores),\n            'recall_score_mean': np.mean(recall_scores),\n            'recall_score_std': np.std(recall_scores),\n            'params': params\n        }, ignore_index=True)\n\n    return catboostDf","119aed71":"# params = {\n#     'depth':[6, 8, 10, 14, 20],\n#     'iterations':[100, 150, 200, 300, 500, 1000],\n#     'learning_rate':[0.15], \n#     'l2_leaf_reg':[12]\n# }\n\n# print(numericalFeatures + categoricalFeatures)\n\n# grid = ParameterGrid(params)\n\n# skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\n# bestScores = []\n# X = data[numericalFeatures + categoricalFeatures]\n# y = data[target]\n\n# catboostDf = searchBestParams(grid, X, y)\n\n# run only if you want to check a new combination of params\n# catboostDf = searchBestParams(grid, X, y)\n# bestCatModel = catboostDf[catboostDf.f1_score_mean == catboostDf.f1_score_mean.max()]\n# bestCatModel.params.values[0]","488dee6f":"# catboostDf.sort_values(by=['f1_score_mean'])","a5e933d8":"# categoricalFeatures = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n#                        'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n#                        'Health', 'State', 'RescuerID', 'AdoptionSpeed']\n\n# target = 'AdoptionSpeed'\n# categoricalFeatures = ['Type', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n#                        'Health']\n# numericalFeatures = ['Age', 'Quantity', 'Fee', 'PhotoAmt', 'VideoAmt']","57c97b5e":"def trainCatboost(numericalFeatures, categoricalFeatures, target, data, params=None):\n    print(numericalFeatures)\n    print(categoricalFeatures)\n    print(target)\n    print(data.size)\n    X_train, X_test, y_train, y_test = train_test_split(\n        data[numericalFeatures + categoricalFeatures],\n        data[target],\n        test_size=0.25,\n        random_state=42\n    )\n\n    X_train_pool = FeaturesData(\n        num_feature_data=X_train[numericalFeatures].astype('float32').values,\n        cat_feature_data=X_train[categoricalFeatures].__array__(dtype=object)\n    )\n    y_train_pool = y_train.astype('int').values\n\n    X_test_pool = FeaturesData(\n        num_feature_data=X_test[numericalFeatures].astype('float32').values,\n        cat_feature_data=X_test[categoricalFeatures].__array__(dtype=object)\n    )\n    y_test_pool = y_test.astype('int').values\n    \n    model = None\n    \n    if params:\n        model = CatBoostClassifier(\n            loss_function='MultiClass',\n            verbose=False,\n            depth=params['depth'],\n            iterations=params['iterations'],\n            l2_leaf_reg=params['l2_leaf_reg'],\n            learning_rate=params['learning_rate'],\n            task_type='GPU',\n            class_weights=[4, 1, 1, 1, 1]\n        )\n    else:\n        model = CatBoostClassifier(\n            loss_function='MultiClass',\n            verbose=False,\n            depth=8,\n            iterations=140,\n            l2_leaf_reg=12,\n            learning_rate=0.15,\n            task_type='GPU',\n            class_weights=[4, 1, 1, 1, 1]\n        )\n\n    model.fit(X_train_pool, y_train_pool,logging_level='Silent')\n    \n    y_predicted = model.predict(X_test_pool)\n    return (model, y_predicted, X_train, y_train, X_train_pool, X_test_pool, y_test)","e970aafb":"# params = {\n#     'depth':8,\n#     'iterations': 140,\n#     'learning_rate': 0.15, \n#     'l2_leaf_reg': 12\n# }\n\n# (bestClf, \n#  y_predicted, \n#  X_train, \n#  y_train, \n#  X_train_pool,\n#  X_test_pool,\n#  y_test) = trainCatboost(numericalFeatures, categorical, target, data, params)","7bf8c2b2":"# scores = calculateClassificationScores(y_test, y_predicted, bestClf, X_test_pool)\n# print('accuracy: %f f1: %f precision: %f recall: %f multiclass: %f' % scores )","f923f8b6":"# generateConfusionMatrix(y_test, y_predicted)","57e1db09":"# np.sum(y_predicted == 0.0)","906eebed":"# plotMostRelevantFeatures(X_train.dtypes.index, bestClf, X_train_pool, y_train.astype('int').values)","6994d6a0":"# targetSentiment = 'AdoptionSpeed'\n# categoricalFeaturesSentiment = ['Type', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n#                        'Health']\n# numericalFeaturesSentiment = ['Age', 'Quantity', 'Fee', 'PhotoAmt', 'VideoAmt', 'sentiment_document_score', 'sentiment_document_magnitude']\n\n# trying new set of features from different models\n# categoricalFeaturesSentiment = ['Breed1', 'Sterilized', 'Vaccinated', 'MaturitySize', 'Gender']\n# numericalFeaturesSentiment = ['sentiment_document_magnitude', 'Age', 'PhotoAmt', 'Quantity']","533aa583":"# msno.matrix(train[categoricalFeaturesSentiment + numericalFeaturesSentiment])","88d2174f":"# params = {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 12, 'learning_rate': 0.15}\n\n# (clfSentiment, \n#  y_predicted, \n#  X_train, \n#  y_train, \n#  X_train_pool,\n#  X_test_pool,\n#  y_test) = trainCatboost(numericalFeaturesSentiment, categoricalFeaturesSentiment, targetSentiment, train[categoricalFeaturesSentiment + numericalFeaturesSentiment + [targetSentiment]].dropna(), params)","29277427":"# scores = calculateClassificationScores(y_test, y_predicted, clfSentiment, X_test_pool)\n# print('accuracy: %f f1: %f precision: %f recall: %f multiclass: %f' % scores )","7cc0425a":"# generateConfusionMatrix(y_test, y_predicted)","13dc1c2e":"# plotMostRelevantFeatures(X_train.dtypes.index, bestClf, X_train_pool, y_train.astype('int').values)","b0fd7781":"# search best params for new features\n# params = {\n#     'depth':[6, 8, 10, 12],\n#     'iterations':[100, 150, 300],\n#     'learning_rate':[0.15], \n#     'l2_leaf_reg':[12]\n# }\n\n# grid = ParameterGrid(params)\n\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n#dataS = train[numericalFeaturesSentiment + categoricalFeaturesSentiment + [targetSentiment]].dropna()\n\n#bestScores = []\n#X = dataS[numericalFeaturesSentiment + categoricalFeaturesSentiment]\n#y = dataS[targetSentiment]\n\n#catboostDf = searchBestParams(grid, X, y)\n#bestCatModel = catboostDf[catboostDf.f1_score_mean == catboostDf.f1_score_mean.max()]\n#bestCatModel.params.values[0]","f83437f3":"#bestCatModel.f1_score_mean","c318aef5":"# data['AgeInterval'] = pd.Series(['0-3', '3-6', '6-12', '12-24', '24-48', '48-120', '>120'], dtype='category')\n# data.loc[(data['Age'] >= 0) & (data['Age'] <= 3),'AgeInterval'] = '0-3'\n# data.loc[(data['Age'] > 3) & (data['Age'] <= 6),'AgeInterval'] = '3-6'\n# data.loc[(data['Age'] > 6) & (data['Age'] <= 12),'AgeInterval'] = '6-12'\n# data.loc[(data['Age'] > 12) & (data['Age'] <= 24),'AgeInterval'] = '12-24'\n# data.loc[(data['Age'] > 24) & (data['Age'] <= 48),'AgeInterval'] = '24-48'\n# data.loc[(data['Age'] > 48) & (data['Age'] <= 120),'AgeInterval'] = '48-120'\n# data.loc[data['Age'] > 120,'AgeInterval'] = '>120'","954e329c":"# target = 'AdoptionSpeed'\n# categoricalFeatures = ['Type', 'Breed1', 'Gender', 'Color1', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n#                        'Health', 'AgeInterval']\n# numericalFeatures = ['Age', 'Quantity', 'Fee', 'PhotoAmt', 'VideoAmt']\n\n# (model, \n#  y_predicted, \n#  X_train, \n#  y_train, \n#  X_train_pool, \n#  X_test_pool) = trainCatboost(numericalFeatures, categoricalFeatures, target, data)","31a3f92d":"# scores = calculateClassificationScores(y_test, y_predicted, model, X_test_pool)\n# print('accuracy: %f f1: %f precision: %f recall: %f multiclass: %f' % scores )","38ac7172":"# plotMostRelevantFeatures(X_train.dtypes.index, bestClf, X_train_pool, y_train.astype('int').values)","0c983083":"# dataS = train[numericalFeatures + categorical + [target]].dropna()\n\n# X = dataS[numericalFeatures + categorical]\n# y = dataS[target]\n\n# prepare data for catboost\n# X_train = FeaturesData(\n#     num_feature_data=X[numericalFeatures].astype('float32').values,\n#     cat_feature_data=X[categorical].__array__(dtype=object)\n# )\n# y_train = y.astype('int').values\n\n# best model is with sentiment data {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 12, 'learning_rate': 0.15}\n# bestClf = CatBoostClassifier(\n#             loss_function='MultiClass',\n#             verbose=False,\n#             depth=8,\n#             iterations=140,\n#             l2_leaf_reg=12,\n#             learning_rate=0.15,\n#             task_type='CPU',\n#             class_weights=[4, 1, 1, 1, 1]\n#         )\n        \n# bestClf.fit(X_train, y_train,logging_level='Silent')","146998bf":"# transforming test as we transformed train\n# testClean = cleanTransformDataset(test)\n# testClean.head()","fdec1fa0":"def generateSubmissionCatboost(model, numericalFeatures, categoricalFeatures, test, fileName):\n    X = test[numericalFeatures + categoricalFeatures]\n    X = FeaturesData(\n            num_feature_data=X[numericalFeatures].astype('float32').values,\n            cat_feature_data=X[categoricalFeatures].__array__(dtype=object)\n        )\n    predictions = model.predict(X)\n    test['AdoptionSpeed'] = predictions\n    test.AdoptionSpeed = test['AdoptionSpeed'].map({0.0: '0', 1.0: '1', 2.0: '2', 3.0: '3', 4.0: '4'})\n    test[['PetID', 'AdoptionSpeed']].to_csv(fileName + '.csv', index=False)","cbb09576":"# generateSubmissionCatboost(bestClf, numericalFeatures, categorical, 'submission')","fc12fb81":"# X_train, X_test, y_train, y_test = train_test_split(\n#     pd.get_dummies(X),\n#     y.astype('int64'),\n#     test_size=0.25,\n#     random_state=42\n# )\n\n# rf = RandomForestClassifier()\n\n# rf.fit(X_train, y_train)\n# y_predicted = rf.predict(X_test)","a22a9b9a":"# scores = calculateClassificationScores(y_test, y_predicted, rf, X_test)\n# print('accuracy: %f f1: %f precision: %f recall: %f' % scores )","25252e66":"# Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 400, num = 10)]\n# Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(6, 110, num = 11)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}","779721c8":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# rf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n# rf_random.fit(X_train, y_train)","405385c0":"# y_predicted = rf_random.predict(X_test)\n# scores = calculateClassificationScores(y_test, y_predicted, rf_random, X_test)\n# print('accuracy: %f f1: %f precision: %f recall: %f' % scores )","a06e3fb3":"# rf_random.best_params_","26481fd4":"# generateConfusionMatrix(y_test, y_predicted)","7dc1a93e":"# ab = AdaBoostClassifier()\n# ab.fit(X_train, y_train)","3ea25980":"# y_predicted = ab.predict(X_test)\n# scores = calculateClassificationScores(y_test, y_predicted, ab, X_test)\n# print('accuracy: %f f1: %f precision: %f recall: %f' % scores )","c905a0f0":"# gb = GradientBoostingClassifier()\n# gb.fit(X_train, y_train)","c7617101":"# y_predicted = gb.predict(X_test)\n# scores = calculateClassificationScores(y_test, y_predicted, gb, X_test)\n# print('accuracy: %f f1: %f precision: %f recall: %f' % scores )","ec8de374":"# param = {'max_depth':8, 'eta':0.15, 'silent':1, 'objective':'multi:softmax' }\n# num_round = 140\n# xgbModel = xgb.XGBClassifier(max_depth=8, n_estimators=140, learning_rate=0.15)\n# xgbModel.fit(X_train, y_train)","2c0ba257":"# vc = VotingClassifier(estimators=[('cb', bestClf), ('rf', rf_random), ('ab', ab), ('gb', gb) ], voting='hard')\n# vc = vc.fit(X_train, y_train)","52c1a1ae":"# y_predicted = vc.predict(X_test)\n# scores = calculateClassificationScores(y_test, y_predicted, vc, X_test)\n# print('accuracy: %f f1: %f precision: %f recall: %f' % scores )  ","3aa220ed":"# X_train_non_null = trainPrecomputed.fillna(-1)\n# X_test_non_null = testPrecomputed.fillna(-1)","67073239":"X_train_non_null.head()","e5d65c07":"train.columns","630e2976":"# newFeatures = [\n#     'metadata_topicality_max',\n#     'metadata_topicality_mean',\n#     'metadata_topicality_min',\n#     'metadata_topicality_0_mean',\n#     'metadata_topicality_0_max',\n#     'metadata_topicality_0_min',\n#     'L_metadata_0_cat_sum',\n#     'L_metadata_0_dog_sum',\n#     'L_metadata_any_cat_sum',\n#     'L_metadata_any_dog_sum',\n#     'blur_max',\n#     'blur_sum',\n#     'huMoments0',\n#     'huMoments1',\n#     'huMoments2',\n#     'huMoments3',\n#     'huMoments4',\n#     'huMoments5',\n#     'huMoments6',\n#     'state_gdp',\n#     'state_population',\n#     'state_area',\n#     'state_unemployment',\n#     'state_birth_rate',\n#     'L_Fee_Free',\n#     'N_pets_total',\n#     'L_NoPhoto',\n#     'L_NoVideo',\n#     'Log_Age',\n#     'L_scoreneg',\n#     'PetID'\n# ]\n# X_train_non_null = X_train_non_null.join(train[newFeatures].set_index('PetID'), 'PetID')\n# X_test_non_null = X_test_non_null.join(test[newFeatures].set_index('PetID'), 'PetID')","d5f864b3":"to_drop_columns = ['PetID', 'Name', 'RescuerID']\nX_train_non_null = X_train_non_null.drop(to_drop_columns, axis=1)","8d95e3b0":"testIds = X_test_non_null['PetID']\nX_test_non_null = X_test_non_null.drop(to_drop_columns, axis=1)","72a76838":"X_test_non_null = X_test_non_null.drop(['AdoptionSpeed'], axis=1)","2bd1fbc4":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","31a778c4":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","9fd9a80e":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 0.8,\n    'colsample_bytree': 0.85,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","ce677fc0":"def run_xgb(params, X_train, X_test):\n    n_splits = 10\n    verbose_eval = 1000\n    num_rounds = 60000\n    early_stop = 500\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, X_train['AdoptionSpeed'].values):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = X_tr['AdoptionSpeed'].values\n        X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n\n        y_val = X_val['AdoptionSpeed'].values\n        X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","29c40e69":"model, oof_train, oof_test = run_xgb(xgb_params, X_train_non_null, X_test_non_null)","3b85c831":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})","3432d2ad":"plot_pred(oof_train)","ca4bcef5":"plot_pred(oof_test.mean(axis=1))","7624ef26":"optR = OptimizedRounder()\noptR.fit(oof_train, X_train_non_null['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\nvalid_pred = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(X_train_non_null['AdoptionSpeed'].values, valid_pred)\nprint(\"QWK = \", qwk)","163bae68":"coefficients_ = coefficients.copy()\ncoefficients_[0] = 1.66\ncoefficients_[1] = 2.13\ncoefficients_[3] = 2.85\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(np.int8)\nprint(f'train pred distribution: {Counter(train_predictions)}')\ntest_predictions = optR.predict(oof_test.mean(axis=1), coefficients_).astype(np.int8)\nprint(f'test pred distribution: {Counter(test_predictions)}')","13163af9":"Counter(train_predictions)","bc1ab6bd":"Counter(test_predictions)","05eb8fc9":"X_test_non_null.shape","87ca2a0b":"len(test_predictions)","ade3e879":"submission = pd.DataFrame({'PetID': testIds.values, 'AdoptionSpeed': test_predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","d8884838":"# X_train_cb = X_train_non_null.copy()\n\n\n# target = 'AdoptionSpeed'\n# categoricalFeatures = ['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n#                        'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized',\n#                        'Health', 'State', 'AdoptionSpeed']\n\n# X_train_cb[categoricalFeatures] = X_train_cb[categoricalFeatures].astype('category')\n\n# numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# numericalFeatures = list(X_train_cb.select_dtypes(include=numerics).columns)\n\n# X_train_cb[numericalFeatures] = X_train_cb[numericalFeatures].fillna(-1)\n\n# categoricalFeaturesTest = list(set(categoricalFeatures) - set([target]))\n# X_test_cb = X_test_non_null.copy()\n# X_test_cb[categoricalFeaturesTest] = X_test_cb[categoricalFeaturesTest].astype('category')\n# X_test_cb[numericalFeatures] = X_test_cb[numericalFeatures].fillna(-1)\n\n# # cleanTransformDataset(X_train_cb, categoricalFeatures)\n\n# (model, \n#  y_predicted, \n#  X_train, \n#  y_train, \n#  X_train_pool, \n#  X_test_pool,\n#  y_test) = trainCatboost(numericalFeatures, categoricalFeatures, target, X_train_cb)","9426d07c":"# scores = calculateClassificationScores(y_test, y_predicted, model, X_test_pool)\n# print('accuracy: %f f1: %f precision: %f recall: %f multiclass: %f' % scores )","0f81c080":"# generateConfusionMatrix(y_test, y_predicted)","7cf43e13":"# plotMostRelevantFeatures(X_train.dtypes.index, model, X_train_pool, y_train.astype('int').values)","cd7da4e6":"# X = X_train_cb[numericalFeatures + categoricalFeatures]\n# y = X_train_cb[target]\n\n# # prepare data for catboost\n# X_train = FeaturesData(\n#     num_feature_data=X[numericalFeatures].astype('float32').values,\n#     cat_feature_data=X[categoricalFeatures].__array__(dtype=object)\n# )\n# y_train = y.astype('int').values\n\n# # best model is with sentiment data {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 12, 'learning_rate': 0.15}\n# bestClf = CatBoostClassifier(\n#             loss_function='MultiClass',\n#             verbose=False,\n#             depth=8,\n#             iterations=140,\n#             l2_leaf_reg=12,\n#             learning_rate=0.15,\n#             task_type='CPU',\n#             class_weights=[4, 1, 1, 1, 1]\n#         )\n        \n# bestClf.fit(X_train, y_train,logging_level='Silent')","6460be3c":"# generateSubmissionCatboost(bestClf, numericalFeatures, categoricalFeatures, X_test_cb, 'submission')","dfd658ba":"Findings:\n* `age` has a max value too high, need to be investigated. 255 months is 21 years is hard to believe that this is accurate.\n*  need to join `breed` and `color` values from other dataset\n* categorical features: `Type`, `Breed1`, `Breed2`, `Gender`, `Color1`, `Color2`, `Color3`, `MaturitySize`, `FurLength`, `Vaccinated`, `Dewormed`, `Sterilized`, `Health`, `Fee`, `State`, `RescuerID`, `AdoptionSpeed`\n* unstructured text features: `Description`\n* numerical features: `Age`, `Quantity`, `VideoAmt`, `PhotoAmt`","f94ae889":"### Catboost with new features","0acdf4c9":"#### **Vaccinated**","85c0c23e":"### TFIDF","8749e493":"## Extract features from json","f863f4f5":"Voting ensembler","f28e570e":"Train model for submission","8be33b7a":"Findings:\n* Long fur have a high adoption rate in the first week and the lowest probability of not being adopted after 100 days\n* Short fur have the highest probability of not being adopted after 100 days\n* My conclusion is that pets with long furs tend to appear more cute and that has a huge impact in a decision to adopt or not.","01ccc5ba":"### **Catboost**","dc11c17e":"**distributions**","5356e4e4":"## Image features","918321db":"**We clearly have some outliers here on Fee, let's remove them**","99f01f36":"### Test","e44a3555":"Findings: The amount of videos doesn't seem to have much impact in AdoptionSpeed","8e1c6b62":"### XGBoost and OptimizeRounder with Kappa Loss","43a6fd93":"### merge processed DFs with base train\/test DF:","98f3ec65":"#### **PhotoAmt**","5848cafc":"**Confusion Matrix to compare predictions per class**","6ab99ef2":"Evaluate model with best params","7e71ce20":"Findings:\n* Normally we have 1 pet per adoption\n* We can see that the mean of quantity for adoption increases for later adoptions and non adopted","40387139":"XGBoost","edc3cd19":"### Extra features","5bfc0c03":"### Add image_size features","1ed08539":"Findings: Dewormed seems to have the same impact as Vaccinated. I assume that other features define the decision of the adopter and vaccinate and deworm it's just something it's not really a deal breaker.","49d5f12a":"### Feature engineering","5f1e1b06":"Findings:\n* The majority of pets that have no pictures are on AdoptionSpeed 4\n* It seems that putting 1-3 pictures it's relevant for getting adopted, more than that not so much.\n* I assume from the >5 trend that people tend to add more picture when non-adopted time increases but after a certain threshold that trend stops.","00cb880f":"Let's get a better visual look between age intervals and the adoption speed.\nBased on our distribution we can divide our age in 4 categories: 0-1 year, 1-2 years, 2-4 years, 4-10 years and >10 years","97d255e9":"## **Numerical attributes and outliers**","256bdcd5":"Findings:\n* Being healthy has a lower probability of not being adopted after 100 days\n* The more serious the injuries higher are the chances of not being adopted after 100 days","fde64c74":"Random Forest","526ff3fe":"#### **AdoptionSpeed**","ac65f859":"## **Missing value**","0209f025":"AdaBoost","d3588eb5":"#### **Type**","a0aad1e4":"Findings: \n* Proportionally we have more `paid` pets on non adopt class.\n* We can conclude that charging for a fee can increases your changes of not being adopted.","7ebbc6ca":"#### **Health**","f943ba73":"## **General properties**","9dbf85f4":"**Model score on test data split**","222d4575":"### Drop ID, name and rescuerID","61358873":"Findings:\n* We have more stray dogs than cats proporcionally\n* Stray dogs have a greater likelihood to not be adopted\n* Stray cats are adopted at the same rate as with breed. People don't care much about breeds of cats appearently.","fbccdf55":"Best params: {'depth': 8,\n 'iterations': 140,\n 'l2_leaf_reg': 12,\n 'learning_rate': 0.15,\n 'thread_count': 4}","faa73832":"Findings: Being Vaccinated doesn't seem to be relevant to be adopted or not. Appearently if the adopter likes the pet for other features that end up being irrelevant.","515c8b42":"{'depth': 6, 'iterations': 300, 'l2_leaf_reg': 12, 'learning_rate': 0.15}","7055fc2c":"#### **MaturitySize**","c6652f1a":"#### **Dewormed**","c56be814":"Findings: clearly color doesn't have much effect on AdoptionSpeed. It follows always the same proportional in all classes","1c848097":"#### Optimze coefficients based on kappa loss. Final competition metric","78015f0b":"#### **Fee**","6ebfafa8":"#### **Quantity**","9729722d":"Findings:\n* We have a lot of puppies in our dataset\n* The mean age tends to grow on pets that are more time for adoption","96b39a1f":"#### **Sterilized**","f6fbf2c0":"### group extracted features by PetID:","899bd431":"## **Categorical attributes and outliers**","90b15842":"Sentiment Analysis Data","60489901":"Let's replace the ids from the categorical features for human readable values to make our analysis easier","f9dcbc91":"### OptimizeRounder from [OptimizedRounder() - Improved](https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved)","07d3ea3e":"### Merge image features","61f2a9a2":"Findings: Being Sterilizzed doesn't seem to have impact in being adopted.","ea301d7d":"## Table of contents\n\n* Purpose\n* Dataset\n    * General properties\n    * Missing Values\n    * Numerical attributes and outliers\n    * Categorical attributes and outliers","77a1757d":"## About metadata and sentiment","b6d7b70b":"#### **VideoAmt**","3d817959":"### Train","6f7502c4":"#### **FurLength**","10350905":"**Submission**","f7cfc140":"**Check most relevant features**","e5ab5935":"## Purpose\nIn this competition you will predict the speed at which a pet is adopted, based on the pet\u2019s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \nThis is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n\n**File descriptions**\n* train.csv - Tabular\/text data for the training set\n* test.csv - Tabular\/text data for the test set\n* sample_submission.csv - A sample submission file in the correct format\n* breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n* color_labels.csv - Contains ColorName for each ColorID\n* state_labels.csv - Contains StateName for each StateID\n\n**Data Fields**\n* PetID - Unique hash ID of pet profile\n* AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n* Type - Type of animal (1 = Dog, 2 = Cat)\n* Name - Name of pet (Empty if not named)\n* Age - Age of pet when listed, in months\n* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n* Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n* Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n* Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n* Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n* MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n* FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n* Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n* Quantity - Number of pets represented in profile\n* Fee - Adoption fee (0 = Free)\n* State - State location in Malaysia (Refer to StateLabels dictionary)\n* RescuerID - Unique hash ID of rescuer\n* VideoAmt - Total uploaded videos for this pet\n* PhotoAmt - Total uploaded photos for this pet\n* Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n\n**AdoptionSpeed**\n\nContestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n\n0 - Pet was adopted on the same day as it was listed. \n\n1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n\n2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n\n3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n\n4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).","3f4a84fa":"Findings: \n* We have an unbalanced class, that could be a problem for some models.\n* The probability of being adopted increases in the first month and then declines","d1045362":"#### **Age**","42296b47":"## **Classification**","43cf3d33":"#### **Breed**","dde8aac0":"GradientBoosting","bd07f70a":"#### **Color**","371c066b":"Findings:\n* We have a lot of Medium pets\n* Large and Small are more likely to be adopted in the first week than Medium\n* All follow the same trends\n* Small has a greater change to be adopted","f7049512":"Findings:\n* Cats seems to have a higher adoption speed than dogs.\n* Both follow the same trend. Up trend on first month and then the probability of being adopted drops.\n* The probability of non adoption seems to be higher for dogs"}}