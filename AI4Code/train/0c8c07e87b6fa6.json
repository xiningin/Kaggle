{"cell_type":{"0d977524":"code","48cbdc69":"code","643b4287":"code","d06a972f":"code","e54ee864":"code","b44793d9":"code","56dc98fe":"code","36c13a74":"code","a65cd800":"code","58e3581e":"code","586304b3":"code","3d1d57ec":"code","3ec7f705":"code","4cce21e8":"code","efe2dd19":"code","dde695f3":"code","f9dc659b":"code","7c8dd69d":"code","df4722cb":"code","9daa82db":"code","50face7f":"code","451b86f6":"code","fafd7e95":"code","e72fded1":"code","02e3aa90":"code","a4c8a473":"code","0d429756":"code","3ec50bb4":"code","e3183f30":"code","575ff364":"code","ce6b5be1":"code","4e3cf014":"code","0a7d1689":"code","d46e51c5":"code","f14e7b17":"code","37e2c5e8":"code","54ec2f85":"code","611b5aa4":"code","81871b53":"code","7d96f62a":"code","e0d408f6":"code","7697631f":"code","7c715da8":"code","70339222":"code","b8d7b690":"code","6b4852e4":"code","5074105e":"code","b8a3e3db":"code","a03c4653":"code","49f9b5d5":"code","7ecc1941":"code","ffda5fe3":"code","a3decb4b":"code","74c7b3f7":"code","49e6aefa":"code","3a1b997a":"code","7a69f530":"markdown","cff8e465":"markdown","8d6de1df":"markdown","11019e15":"markdown","6f414ddf":"markdown","6043ffc6":"markdown","cdd6af6f":"markdown","2316322b":"markdown","fe6cf0c8":"markdown","018254d2":"markdown","a1ba9271":"markdown","68c186bc":"markdown"},"source":{"0d977524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48cbdc69":"# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n# Pandas display options\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n# Set random seed \nRSEED = 2020\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 12\nimport seaborn as sns\npalette = sns.color_palette('Paired', 10)\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer, PowerTransformer\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n# from sklearn.ensemble import StackingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar","643b4287":"data = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows = 5_000_00, \n                   parse_dates = ['pickup_datetime']).drop(columns = 'key')\ndata = data.dropna()","d06a972f":"data.head()","e54ee864":"data.describe()","b44793d9":"data.isna().sum()\/data.shape[0]*100","56dc98fe":"#distribution of target (fare amount)\nsns.distplot(data['fare_amount'])","36c13a74":"def ecdf(x):\n    \"\"\"Empirical cumulative distribution function of a variable\"\"\"\n    # Sort in ascending order\n    x = np.sort(x)\n    n = len(x)\n    # Go from 1\/n to 1\n    y = np.arange(1, n + 1, 1) \/ n\n    return x, y","a65cd800":"xs, ys = ecdf(data['fare_amount'])\nplt.figure(figsize = (8, 6))\nplt.plot(xs, ys, '.')\nplt.ylabel('Percentile'); plt.title('ECDF of Fare Amount'); plt.xlabel('Fare Amount ($)');","58e3581e":"data['passenger_count'].value_counts().plot.bar(color = 'b', edgecolor = 'k');\nplt.title('Passenger Counts'); plt.xlabel('Number of Passengers'); plt.ylabel('Count');","586304b3":"print('ada '+str(data[data['passenger_count']==0].shape[0])+'transaksi dengan 0 passangger')\nprint('ada '+str(data[data['passenger_count']==6].shape[0])+'transaksi dengan 6 passangger')","3d1d57ec":"data = data.loc[data['pickup_latitude'].between(40, 42)]\ndata = data.loc[data['pickup_longitude'].between(-75, -72)]\ndata = data.loc[data['dropoff_latitude'].between(40, 42)]\ndata = data.loc[data['dropoff_longitude'].between(-75, -72)]","3ec7f705":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5)\ncluster_pickup = kmeans.fit_predict(data[['pickup_longitude','pickup_latitude']])\ncluster_dropoff = kmeans.fit_predict(data[['dropoff_longitude','dropoff_latitude']])\ndata['cluster_pickup']=cluster_pickup\ndata['cluster_dropoff']=cluster_dropoff","4cce21e8":"# this function will also be used with the test set below\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n\n# load extra image to zoom in on NYC\nBB_zoom = (-74.1, -73.7, 40.6, 40.85)\nnyc_map_zoom = plt.imread('https:\/\/github.com\/WillKoehrsen\/Machine-Learning-Projects\/blob\/master\/images\/nyc_-74.1_-73.7_40.6_40.85.PNG?raw=true')","efe2dd19":"# this function will be used more often to plot data on the NYC map\ndef plot_on_map(df, BB, nyc_map, s=10, alpha=0.2, color = False):\n    fig, axs = plt.subplots(1, 2, figsize=(18, 22))\n    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='r', s=s)\n    axs[0].set_xlim((BB[0], BB[1]))\n    axs[0].set_ylim((BB[2], BB[3]))\n    axs[0].set_title('Pickup locations')\n    axs[0].axis('off')\n    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n\n    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='b', s=s)\n    axs[1].set_xlim((BB[0], BB[1]))\n    axs[1].set_ylim((BB[2], BB[3]))\n    axs[1].set_title('Dropoff locations')\n    axs[1].axis('off')\n    axs[1].imshow(nyc_map, zorder=0, extent=BB)\n    \n# plot training data on map zoomed in\nplot_on_map(data.sample(4_000_00, random_state = RSEED), \n            BB_zoom, nyc_map_zoom, s=0.05, alpha=0.05)","dde695f3":"# Create a color mapping based on fare bins\ncolor_mapping = {cluster_pickup: palette[i] for i, cluster_pickup in enumerate(data['cluster_pickup'].unique())}\ndata['color'] = data['cluster_pickup'].map(color_mapping)\nplot_data = data.sample(4_000_00, random_state = RSEED)","f9dc659b":"BB = BB_zoom\n\nfig, axs = plt.subplots(1, 1, figsize=(20, 18))\n\n# Plot the pickups\nfor b, df in plot_data.groupby('cluster_pickup'):\n    # Set the zorder to 1 to plot on top of map\n    axs.scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=0.2, c=df.color, s=30, label = f'{b}')\n    axs.set_xlim((BB[0], BB[1]))\n    axs.set_ylim((BB[2], BB[3]))\n    axs.set_title('Pickup locations', size = 32)\n    axs.axis('off')\n    \n# Legend\nleg = axs.legend(fontsize = 28, markerscale = 3)\n\n# Adjust alpha of legend markers\nfor lh in leg.legendHandles: \n    lh.set_alpha(1)\n\nleg.set_title('Cluster Pickup', prop = {'size': 28})\n\n# Show map in background (zorder = 0)\naxs.imshow(nyc_map_zoom, zorder=0, extent=BB_zoom);\n","7c8dd69d":"# Create a color mapping based on fare bins\ncolor_mapping = {cluster_pickup: palette[i] for i, cluster_pickup in enumerate(data['cluster_dropoff'].unique())}\ndata['color'] = data['cluster_dropoff'].map(color_mapping)\nplot_data = data.sample(4_000_00, random_state = RSEED)","df4722cb":"fig, axs = plt.subplots(1, 1, figsize=(20, 18))\n\n# Plot the pickups\nfor b, df in plot_data.groupby('cluster_dropoff'):\n    axs.scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, \n                alpha=0.2, c=df.color, s=30, label = f'{b}')\n    axs.set_xlim((BB[0], BB[1]))\n    axs.set_ylim((BB[2], BB[3]))\n    axs.set_title('cluster_dropoff', size = 32)\n    axs.axis('off')\n    \n# Legend\nleg = axs.legend(fontsize = 28, markerscale = 3)\n\n# Adjust alpha of legend markers\nfor lh in leg.legendHandles: \n    lh.set_alpha(1)\n\nleg.set_title('Cluster Dropoff', prop = {'size': 28})\n\n# Show map in background (zorder = 0)\naxs.imshow(nyc_map_zoom, zorder=0, extent=BB_zoom);","9daa82db":"def minkowski_distance(x1, x2, y1, y2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 \/ p)\n                                                           \nR = 6378\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n    \n    source: https:\/\/stackoverflow.com\/a\/29546836\n\n    \"\"\"\n    # Convert latitude and longitude to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # Find the differences\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    # Apply the formula \n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n    # Calculate the angle (in radians)\n    c = 2 * np.arcsin(np.sqrt(a))\n    # Convert to kilometers\n    km = R * c\n    \n    return km\n                                                           \n                                                           \nplace = pd.DataFrame({'loc' : ['jfk','nyc','ewr','lgr'], 'long' : [-73.7822222222,-74.0063889,-74.175,-73.87], 'lat' : [40.6441666667,40.7141667,40.69,40.77]})\n\ndef distance_to_place(df,location,source_long,source_lat):\n    selected_place = place[place['loc']==location]\n    selected_place = selected_place.reset_index()\n    xx = haversine_np(df[source_long], df[source_lat], selected_place['long'][0], selected_place['lat'][0])\n    \n    return xx\n                                                           \n\ndef calculate_direction(df):\n    d_lon = df['pickup_longitude'] - df['dropoff_longitude']\n    d_lat = df['pickup_latitude'] - df['dropoff_latitude']\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180\/np.pi)*np.arcsin(d_lat[d_lon>0]\/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    return result","50face7f":"data['abs_lat_diff'] = (data['dropoff_latitude'] - data['pickup_latitude']).abs()\ndata['abs_lon_diff'] = (data['dropoff_longitude'] - data['pickup_longitude']).abs()\n\ndata['manhattan'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n                                       data['pickup_latitude'], data['dropoff_latitude'], 1)\n\ndata['euclidean'] = minkowski_distance(data['pickup_longitude'], data['dropoff_longitude'],\n                                       data['pickup_latitude'], data['dropoff_latitude'], 2)\n\n\ndata['haversine'] =  haversine_np(data['pickup_longitude'], data['pickup_latitude'],\n                         data['dropoff_longitude'], data['dropoff_latitude']) \n\nfor i in place['loc'].tolist():\n    for j in ['pickup','dropoff']:\n        data[str(j)+'_distance_to'+str(i)] = distance_to_place(data,i,str(j)+'_longitude',str(j)+'_latitude')\n\ndata['direction'] = calculate_direction(data)","451b86f6":"data['haversine'].describe()","fafd7e95":"cek = data[data['haversine']<200]\nsns.distplot(cek['haversine'])","e72fded1":"(data['haversine']>25).sum()","02e3aa90":"# scatter plot distance - fare\nfig, axs = plt.subplots(1, 2, figsize=(16,6))\naxs[0].scatter(data.haversine, data.fare_amount, alpha=0.2)\naxs[0].set_xlabel('distance km')\naxs[0].set_ylabel('fare $USD')\naxs[0].set_title('All data')\n\n# zoom in on part of data\nidx = (data.haversine <= 25) & (data.fare_amount < 100)\naxs[1].scatter(data[idx].haversine, data[idx].fare_amount, alpha=0.2)\naxs[1].set_xlabel('distance km')\naxs[1].set_ylabel('fare $USD')\naxs[1].set_title('Zoom in on distance < 15 km, fare < $100');","a4c8a473":"data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)].shape[0], data.shape[0], data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)].shape[0]\/data.shape[0]","0d429756":"sns.distplot(data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)]['passenger_count'])\nplt.show()\nsns.distplot(data[(data['abs_lat_diff']==0)&(data['abs_lon_diff']==0)]['fare_amount'])\nplt.show()","3ec50bb4":"corrs = data.corr()\ncorrs = corrs.drop('fare_amount',axis=0)\ncorrs['fare_amount'].plot.bar(color = 'b');\nplt.title('Correlation with Fare Amount');","e3183f30":"cek_cols = corrs.columns.tolist()","575ff364":"cek = data.copy()\nfor i in cek_cols:\n    cek[i] = np.log(cek[i])\ncorrs = cek.corr()\ncorrs = corrs.drop('fare_amount',axis=0)\ncorrs['fare_amount'].plot.bar(color = 'b');\nplt.title('Correlation with Fare Amount');","ce6b5be1":"#time\nimport re\ndef extract_dateinfo(df, date_col, drop=True, time=False, \n                     start_ref = pd.datetime(1900, 1, 1),\n                     extra_attr = False):\n    \"\"\"\n    Extract Date (and time) Information from a DataFrame\n    Adapted from: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/structured.py\n    \"\"\"\n    df = df.copy()\n    \n    # Extract the field\n    fld = df[date_col]\n    \n    # Check the time\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    # Convert to datetime if not already\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    \n\n    # Prefix for new columns\n    pre = re.sub('[Dd]ate', '', date_col)\n    pre = re.sub('[Tt]ime', '', pre)\n    \n    # Basic attributes\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n    \n    # Additional attributes\n    if extra_attr:\n        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    # If time is specified, extract time information\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    # Iterate through each attribute\n    for n in attr: \n        df[pre + n] = getattr(fld.dt, n.lower())\n        \n    # Calculate days in year\n    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n        \n    if time:\n        # Add fractional time of day (0 - 1) units of day\n        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] \/ 60) + (df[pre + 'Second'] \/ 60 \/ 60)) \/ 24\n        \n        # Add fractional time of week (0 - 1) units of week\n        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) \/ 7\n    \n        # Add fractional time of month (0 - 1) units of month\n        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) \/ (df[pre + 'Days_in_month'] +  1)\n        \n        # Add fractional time of year (0 - 1) units of year\n        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) \/ (df[pre + 'Days_in_year'] + 1)\n        \n    # Add seconds since start of reference\n    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n    \n    if drop: \n        df = df.drop(date_col, axis=1)\n        \n    return df\n\ndata = extract_dateinfo(data, 'pickup_datetime', drop = False,time = True, start_ref = df['pickup_datetime'].min())","4e3cf014":"def time_slicer(df, timeframes, value, color=\"purple\"):\n    \"\"\"\n    Function to count observation occurrence through different lenses of time.\n    \"\"\"\n    f, ax = plt.subplots(len(timeframes), figsize = [12,12])\n    for i,x in enumerate(timeframes):\n        df.loc[:,[x,value]].groupby([x]).mean().plot(ax=ax[i],color=color)\n        ax[i].set_ylabel(value.replace(\"_\", \" \").title())\n        ax[i].set_title(\"{} by {}\".format(value.replace(\"_\", \" \").title(), x.replace(\"_\", \" \").title()))\n        ax[i].set_xlabel(\"\")\n    ax[len(timeframes)-1].set_xlabel(\"Time Frame\")\n    plt.tight_layout(pad=0)","0a7d1689":"time_slicer(df=data, timeframes=['pickup_Year', 'pickup_Month', 'pickup_Day','pickup_Hour'], value = \"fare_amount\", color=\"blue\")","d46e51c5":"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nholidays = calendar().holidays()\ndata[\"usFedHoliday\"] =  data.pickup_datetime.dt.date.astype('datetime64').isin(holidays)","f14e7b17":"cek= data[(data.haversine <= 25) & (data.fare_amount >=0) & (data.fare_amount <= 50)]\nsns.scatterplot(x='haversine',y='fare_amount',data=cek,hue='usFedHoliday')","37e2c5e8":"data = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows = 3_000_0, \n                   parse_dates = ['pickup_datetime']).drop(columns = 'key')\n\n# Remove na\ndata = data.dropna()","54ec2f85":"def minkowski_distance(x1, x2, y1, y2, p):\n    return ((abs(x2 - x1) ** p) + (abs(y2 - y1)) ** p) ** (1 \/ p)\n                                                           \nR = 6378\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n    \n    source: https:\/\/stackoverflow.com\/a\/29546836\n\n    \"\"\"\n    # Convert latitude and longitude to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # Find the differences\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    # Apply the formula \n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n    # Calculate the angle (in radians)\n    c = 2 * np.arcsin(np.sqrt(a))\n    # Convert to kilometers\n    km = R * c\n    \n    return km\n                                                           \n                                                           \nplace = pd.DataFrame({'loc' : ['jfk','nyc','ewr','lgr'], 'long' : [-73.7822222222,-74.0063889,-74.175,-73.87], 'lat' : [40.6441666667,40.7141667,40.69,40.77]})\n\ndef distance_to_place(df,location,source_long,source_lat):\n    selected_place = place[place['loc']==location]\n    selected_place = selected_place.reset_index()\n    xx = haversine_np(df[source_long], df[source_lat], selected_place['long'][0], selected_place['lat'][0])\n    \n    return xx\n                                                           \n\ndef calculate_direction(df):\n    d_lon = df['pickup_longitude'] - df['dropoff_longitude']\n    d_lat = df['pickup_latitude'] - df['dropoff_latitude']\n    result = np.zeros(len(d_lon))\n    l = np.sqrt(d_lon**2 + d_lat**2)\n    result[d_lon>0] = (180\/np.pi)*np.arcsin(d_lat[d_lon>0]\/l[d_lon>0])\n    idx = (d_lon<0) & (d_lat>0)\n    result[idx] = 180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    idx = (d_lon<0) & (d_lat<0)\n    result[idx] = -180 - (180\/np.pi)*np.arcsin(d_lat[idx]\/l[idx])\n    return result","611b5aa4":"#time\nimport re\ndef extract_dateinfo(df, date_col, drop=True, time=False, \n                     start_ref = pd.datetime(1900, 1, 1),\n                     extra_attr = False):\n    \"\"\"\n    Extract Date (and time) Information from a DataFrame\n    Adapted from: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/structured.py\n    \"\"\"\n    df = df.copy()\n    \n    # Extract the field\n    fld = df[date_col]\n    \n    # Check the time\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    # Convert to datetime if not already\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[date_col] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    \n\n    # Prefix for new columns\n    pre = re.sub('[Dd]ate', '', date_col)\n    pre = re.sub('[Tt]ime', '', pre)\n    \n    # Basic attributes\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Days_in_month', 'is_leap_year']\n    \n    # Additional attributes\n    if extra_attr:\n        attr = attr + ['Is_month_end', 'Is_month_start', 'Is_quarter_end', \n                       'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    \n    # If time is specified, extract time information\n    if time: \n        attr = attr + ['Hour', 'Minute', 'Second']\n        \n    # Iterate through each attribute\n    for n in attr: \n        df[pre + n] = getattr(fld.dt, n.lower())\n        \n    # Calculate days in year\n    df[pre + 'Days_in_year'] = df[pre + 'is_leap_year'] + 365\n        \n    if time:\n        # Add fractional time of day (0 - 1) units of day\n        df[pre + 'frac_day'] = ((df[pre + 'Hour']) + (df[pre + 'Minute'] \/ 60) + (df[pre + 'Second'] \/ 60 \/ 60)) \/ 24\n        \n        # Add fractional time of week (0 - 1) units of week\n        df[pre + 'frac_week'] = (df[pre + 'Dayofweek'] + df[pre + 'frac_day']) \/ 7\n    \n        # Add fractional time of month (0 - 1) units of month\n        df[pre + 'frac_month'] = (df[pre + 'Day'] + (df[pre + 'frac_day'])) \/ (df[pre + 'Days_in_month'] +  1)\n        \n        # Add fractional time of year (0 - 1) units of year\n        df[pre + 'frac_year'] = (df[pre + 'Dayofyear'] + df[pre + 'frac_day']) \/ (df[pre + 'Days_in_year'] + 1)\n        \n    # Add seconds since start of reference\n    df[pre + 'Elapsed'] = (fld - start_ref).dt.total_seconds()\n    \n    if drop: \n        df = df.drop(date_col, axis=1)\n        \n    return df","81871b53":"class data_transform():\n    \n    \n    def __init__(self, num, cat, is_cat):\n        self.num = num\n        self.cat = cat\n        self.is_cat = is_cat\n        \n    def fit(self, X):\n        # do not do anything\n        return self\n    \n    def transform(self, X, y = None):\n        num_cols = self.num\n        cat_cols = self.cat\n        df = X.copy()\n        df['passenger_count'] = np.where(df['passenger_count']<1,1,df['passenger_count'])\n        df['passenger_count'] = np.where(df['passenger_count']>6,5,df['passenger_count'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']<40, 40,df['pickup_latitude'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']>42,42,df['pickup_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']<40, 40,df['dropoff_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']>42, 42,df['dropoff_latitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']<-75, -75,df['pickup_longitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']>-72, -72,df['pickup_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']<-75, -75,df['dropoff_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']>-72, -72,df['dropoff_longitude'])\n\n        kmeans = KMeans(n_clusters=5)\n        cluster_pickup = kmeans.fit_predict(data[['pickup_longitude','pickup_latitude']])\n        cluster_dropoff = kmeans.fit_predict(data[['dropoff_longitude','dropoff_latitude']])\n        data['cluster_pickup']=cluster_pickup\n        data['cluster_dropoff']=cluster_dropoff\n        \n        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n        # Absolute difference in latitude and longitude\n        df['abs_lat_diff'] = (df['dropoff_latitude'] - df['pickup_latitude']).abs()\n        df['abs_lon_diff'] = (df['dropoff_longitude'] - df['pickup_longitude']).abs()\n\n        df['manhattan'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 1)\n\n        df['euclidean'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 2)\n\n        df['haversine'] =  haversine_np(df['pickup_longitude'], df['pickup_latitude'],\n                                 df['dropoff_longitude'], df['dropoff_latitude']) \n\n        for i in place['loc'].tolist():\n            for j in ['pickup','dropoff']:\n                df[str(j)+'_distance_to'+str(i)] = distance_to_place(df,i,str(j)+'_longitude',str(j)+'_latitude')\n\n        df['direction'] = calculate_direction(df)\n\n        df = extract_dateinfo(df, 'pickup_datetime', drop = False, \n                                 time = True, start_ref = df['pickup_datetime'].min())\n        \n        holidays = calendar().holidays()\n        data[\"usFedHoliday\"] =  data.pickup_datetime.dt.date.astype('datetime64').isin(holidays)\n\n        df[cat_cols] = df[cat_cols].astype(str)\n        \n        \n        if self.is_cat==1:\n            df = df[cat_cols]\n        elif self.is_cat==0:\n            df = df[num_cols] \n        else:\n            df = df\n            \n        return df\n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)","7d96f62a":"class data_transform():\n    \n    \n    def __init__(self, num, cat, is_cat):\n        self.num = num\n        self.cat = cat\n        self.is_cat = is_cat\n        \n    def fit(self, X):\n        # do not do anything\n        return self\n    \n    def transform(self, X, y = None):\n        num_cols = self.num\n        cat_cols = self.cat\n        df = X.copy()\n        df['passenger_count'] = np.where(df['passenger_count']<1,1,df['passenger_count'])\n        df['passenger_count'] = np.where(df['passenger_count']>6,5,df['passenger_count'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']<40, 40,df['pickup_latitude'])\n        df['pickup_latitude'] = np.where(df['pickup_latitude']>42,42,df['pickup_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']<40, 40,df['dropoff_latitude'])\n        df['dropoff_latitude'] = np.where(df['dropoff_latitude']>42, 42,df['dropoff_latitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']<-75, -75,df['pickup_longitude'])\n        df['pickup_longitude'] = np.where(df['pickup_longitude']>-72, -72,df['pickup_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']<-75, -75,df['dropoff_longitude'])\n        df['dropoff_longitude'] = np.where(df['dropoff_longitude']>-72, -72,df['dropoff_longitude'])\n        \n        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n        # Absolute difference in latitude and longitude\n        df['abs_lat_diff'] = (df['dropoff_latitude'] - df['pickup_latitude']).abs()\n        df['abs_lon_diff'] = (df['dropoff_longitude'] - df['pickup_longitude']).abs()\n\n        df['manhattan'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 1)\n\n        df['euclidean'] = minkowski_distance(df['pickup_longitude'], df['dropoff_longitude'],\n                                               df['pickup_latitude'], df['dropoff_latitude'], 2)\n\n        df['haversine'] =  haversine_np(df['pickup_longitude'], df['pickup_latitude'],\n                                 df['dropoff_longitude'], df['dropoff_latitude']) \n\n        for i in place['loc'].tolist():\n            for j in ['pickup','dropoff']:\n                df[str(j)+'_distance_to'+str(i)] = distance_to_place(df,i,str(j)+'_longitude',str(j)+'_latitude')\n\n        df['direction'] = calculate_direction(df)\n\n        df = extract_dateinfo(df, 'pickup_datetime', drop = False, \n                                 time = True, start_ref = df['pickup_datetime'].min())\n\n        holidays = calendar().holidays()\n        df[\"usFedHoliday\"] =  df.pickup_datetime.dt.date.astype('datetime64').isin(holidays)\n        df[cat_cols] = df[cat_cols].astype(str)\n        \n        \n        if self.is_cat==1:\n            df = df[cat_cols]\n        elif self.is_cat==0:\n            df = df[num_cols] \n        else:\n            df = df\n            \n        return df\n    \n    def fit_transform(self, X, y = None):\n        self.fit(X)\n        return self.transform(X)","e0d408f6":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    result = np.sqrt(mean_squared_error(y_true, y_pred))\n    return result\n\n# def mape(y_true, y_pred):\n#     result = np.abs(y_true-y_pred) \/ y_true\n#     result = np.mean(result)\n#     return result\n\n# def my_scorer1():\n#     return make_scorer(rmse, greater_is_better=False)","7697631f":"ori_cols = ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count']","7c715da8":"num_cols = ['pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n       'abs_lat_diff', 'abs_lon_diff', 'manhattan', 'euclidean', 'haversine',\n       'pickup_distance_tojfk', 'dropoff_distance_tojfk',\n       'pickup_distance_tonyc', 'dropoff_distance_tonyc',\n       'pickup_distance_toewr', 'dropoff_distance_toewr',\n       'pickup_distance_tolgr', 'dropoff_distance_tolgr', 'direction',\n       'pickup_Year', 'pickup_Month', 'pickup_Week', 'pickup_Day',\n       'pickup_Dayofweek', 'pickup_Dayofyear', 'pickup_Days_in_month',\n       'pickup_Hour', 'pickup_Minute', 'pickup_Second',\n       'pickup_Days_in_year', 'pickup_frac_day', 'pickup_frac_week',\n       'pickup_frac_month', 'pickup_frac_year', 'pickup_Elapsed']\ncat_cols = ['pickup_is_leap_year','usFedHoliday']\ntarget = 'fare_amount'","70339222":"# # num_cols = ['passenger_count', 'abs_lat_diff', 'abs_lon_diff', 'manhattan', 'euclidean', 'haversine','pickup_Year', 'pickup_Month','pickup_Hour','direction','pickup_Elapsed','pickup_distance_tojfk', 'dropoff_distance_tojfk'] #, 'abs_lat_diff', 'abs_lon_diff', 'manhattan', 'euclidean', 'haversine', 'pickup_distance_tojfk', 'dropoff_distance_tojfk', 'pickup_Year','pickup_Elapsed'\n# num_cols = ['passenger_count', 'abs_lat_diff', 'abs_lon_diff', 'manhattan', 'euclidean', 'haversine','pickup_Year']\n# cat_cols = ['pickup_Month'] #'pickup_Month'\n# target = 'fare_amount'","b8d7b690":"# data1 = data","6b4852e4":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(data,test_size=0.1, random_state=RSEED)\n#del data","5074105e":"num_transformer = Pipeline(steps=[\n                                ('dataprep', data_transform(num_cols,cat_cols,is_cat=0)),\n                                ('imputer', SimpleImputer(strategy = \"mean\")),\n                                ('scaler', PowerTransformer())  #PowerTransformer\n                                ])\n\ncat_transformer = Pipeline(steps=[\n                                ('dataprep', data_transform(num_cols,cat_cols,is_cat=1)),\n                                ('imputer', SimpleImputer(strategy='most_frequent')),\n                                ('onehot', OneHotEncoder(handle_unknown='error')) #, drop = \"if_binary\"\n                                ])\n\ntransformer = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, ori_cols),\n        ('cat', cat_transformer, ori_cols)\n    ])\n\nknn = KNeighborsRegressor(n_neighbors=3)\ndt = DecisionTreeRegressor(random_state=123)\nrf = DecisionTreeRegressor(random_state=123)\neln = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=2020)\nrg = Ridge(alpha=1.0, random_state=2020)\nls = Lasso(alpha=1.0, random_state=2020)\nxgb = XGBRegressor(random_state=2020, booster='gbtree',n_estimators=20, tree_method='hist')\nlgb = LGBMRegressor(objective='regression',random_states=2020, metric = 'rmse', num_leaves = 31, boosting_type='gbdt', max_depth=5, learning_rate=0.034)\ncatb = CatBoostRegressor(iterations=2,learning_rate=0.5,depth=3, silent=True)\n# lgb_goss = lgb.LGBMRegressor(objective='regression',random_states=2020,  boosting_type = 'goss', metric = 'rmse', learning_rate=0.034, num_leaves = 31)\nlr = LinearRegression()\n\n\nstack = StackingCVRegressor(regressors=(knn, lgb, xgb, catb, dt, rf, eln, rg, ls), meta_regressor=lr, cv=3)\n\nstack_pipeline = Pipeline(steps=[('transformer', transformer),\n                      ('stack', stack)\n                      ])\n\nmain_pipeline = TransformedTargetRegressor(stack_pipeline,\n                                    transformer = PowerTransformer())\n\nparams = {  \n#           'regressor__stack__lgbmregressor__boosting_type': ['gbdt','dart'],\n#           'regressor__stack__lgbmregressor__max_depth': [3,5,7],\n#           'regressor__stack__lgbmregressor__learning_rate': [0.05,0.5,1],\n    \n#           'regressor__stack__xgbregressor__n_estimators': [10,20,30],\n#           'regressor__stack__xgbregressor__tree_method: ['exact','approx','hist'],\n           \n#           'regressor__stack__catboostregressor__n_estimators': [10,20,30],\n#           'regressor__stack__catboostregressor__learning_rate': [0.05,0.5,1]\n    \n          }\n\n\nmodel = GridSearchCV(estimator=main_pipeline, param_grid=params,  cv=3, n_jobs=-1, scoring='neg_mean_squared_error' ,refit=True)\n\n\ntrain_X = train.drop(target,axis=1)\ntrain_y = train[target]\n\nmodel.fit(train_X, train_y)\n\n\n","b8a3e3db":"print('Best parameters cv: %s' % model.best_params_)\nprint('Best nmse cv: %.2f' % model.best_score_)\nrmse_cv = np.sqrt(model.best_score_*-1) \nprint('Best rmse cv: %.2f' % rmse_cv)\nprint('rmse all train data: %.2f' %rmse(model.predict(train_X),train['fare_amount']))\npd.DataFrame(model.cv_results_)","a03c4653":"print('rmse all train data: %.2f' %rmse(model.predict(val.drop(target,axis=1)),val['fare_amount']))","49f9b5d5":"test = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv', \n                   parse_dates = ['pickup_datetime'])","7ecc1941":"test_X = test[ori_cols]\n# Use the model to make predictions\npredicted_fare = model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_fare)","ffda5fe3":"my_submission = pd.DataFrame({'key': test.key, 'fare_amount': predicted_fare})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_v1a.csv', index=False)","a3decb4b":"import joblib\njoblib.dump(model, f'nyk_taxi_stack.pkl')","74c7b3f7":"model","49e6aefa":"dt    = '2019-06-20 12:26:21'\nplong =  -73.844\nplat  =  41.721\ndlong =  -74.842\ndlat  =  42.712\npc    =  5","3a1b997a":"cek = pd.DataFrame({'pickup_datetime' : [dt], 'pickup_longitude' : [plong], 'pickup_latitude' : [plat], 'dropoff_longitude' : [dlong], 'dropoff_latitude' : [dlat], 'passenger_count' : [pc]})\nmodel.predict(cek)","7a69f530":"Ada harga 0 ke bawah","cff8e465":"###############################################################################################################################","8d6de1df":"# EDA","11019e15":"# Single Prediction","6f414ddf":"Findings :\n\n1. Fare amount ada yang minus dan max harganya 500\n2. nilai longitute dan latitude aneh karena nilainya ribuan padahal harusnya:\n   Latitudes range from -90 ke 90, and longitudes range from -180 ke 180, bahkan NewYork Latitude 40 ke 42 dan Longitudes -75 ke -72\n3. passanger count ada yg 0 dan max 6 padahal kapasitas taxi max <6","6043ffc6":"Fare Amount diata 100 itu outlier","cdd6af6f":"ada 14250 transaksi yang tidak ada perubahan longituted dan latitude tapi kalau dilihat lagi ini tidak ada hubungan ngsung dengan fare amount <=0 (assumsi cancle booking) ataupun jumlah passanger. jadi kemungkinan gps nya mati selama perjalanan","2316322b":"## Bulk Prediction & Kaggle Submission`","fe6cf0c8":"Asumsi :\n\n1. 0 passanger terjadi karena cancel booking\n2. 6 passanger terjadi karena melebihi kapasita, kalo taxi harusnya 4 max, taxicab 5\n\n   https:\/\/www1.nyc.gov\/site\/tlc\/passengers\/passenger-frequently-asked-questions.page#:~:text=The%20maximum%20amount%20of%20passengers,of%20an%20adult%20passenger%20seated","018254d2":"findings :\n1. Secara tahun semakin tinggi tahun ongkos semakin naik\n2. Secara bulan naik dengan puncak di bulan 5 kemduian  turun sampe paling parah di bulan 7 & 8 terus naik lagi peak dibulan 9 ters turun sampe akhir tahun\n3. secara tangal pola fluktuatif\n4. paling peak jam 5 pagi dan jam 3 sore agak meningkat","a1ba9271":"kalau outlier haversine di exclude kita dapat pola linear antara pertambahan jarak dan harga","68c186bc":"# Modelling"}}