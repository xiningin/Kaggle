{"cell_type":{"54552055":"code","626194bf":"code","c7008592":"code","d6ea36ce":"code","0f44c0ea":"code","d37d2980":"code","74aa41d7":"code","5327a9a1":"code","2a7e8769":"code","08116702":"code","ada13b6f":"code","53bccf8c":"code","88b62155":"code","a452346b":"code","3bc27c3b":"code","262598ba":"code","49299612":"code","1b0c10f9":"code","50419673":"code","443832b2":"code","6cb475aa":"code","f8d40477":"code","61580533":"code","74c1c5db":"code","1ca4c2b6":"code","655c55a3":"code","01f14fd0":"code","d5e2b53f":"code","c8ad5804":"markdown","7182f279":"markdown","fe80cfac":"markdown","24927fc9":"markdown","ba457afc":"markdown","2d078ed3":"markdown","b77a8e5c":"markdown","55de1d9c":"markdown","33806878":"markdown","9e8f7ca1":"markdown","f0f4428e":"markdown","989e9739":"markdown","657b8585":"markdown","378fe76d":"markdown"},"source":{"54552055":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","626194bf":"#Code By Paul Mooney  https:\/\/www.kaggle.com\/paultimothymooney\/starter-notebook-for-end-als-kaggle-challenge\n\nctrl_vs_case = '\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/ctrl_vs_case.csv'","c7008592":"#Code by Paul Mooney  https:\/\/www.kaggle.com\/paultimothymooney\/starter-notebook-for-end-als-kaggle-challenge\n\ndf = pd.read_csv(ctrl_vs_case)\ndf.to_csv('\/kaggle\/working\/ctrl_vs_case.csv')","d6ea36ce":"df.info()","0f44c0ea":"df.head()","d37d2980":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","74aa41d7":"df = pd.get_dummies(df)\n\n#ValueError: could not convert string to float: 'NEUDJ536EVH'","5327a9a1":"df.head()","2a7e8769":"x = df.drop(['Participant_ID', 'CtrlVsCase_Classifier'], axis=1)\nx.fillna(999999, inplace=True)\ny = df['CtrlVsCase_Classifier']","08116702":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state = 0)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","ada13b6f":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nx_train_std = ss.fit_transform(x_train)\nx_test_std = ss.transform(x_test)","53bccf8c":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors':[5,10,15,25,30,50]}\n\ngrid_knn = GridSearchCV(knn,param_grid,scoring='accuracy',cv = 10,refit = True)","88b62155":"grid_knn.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_knn.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_knn.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_knn.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_knn.score(x_test_std,y_test))","a452346b":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n\nparam_grid = {'criterion':['gini','entropy'],'max_depth':np.arange(2,10),'min_samples_leaf':[0.2,0.4,0.6,0.8,0.9,1]}\n\ngrid_dtc = GridSearchCV(dtc,param_grid,scoring='accuracy',cv = 10,refit = True)","3bc27c3b":"grid_dtc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_dtc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_dtc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_dtc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_dtc.score(x_test_std,y_test))","262598ba":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(probability=True)\n\nparam_grid = {'kernel':['rbf','linear'],'C':[0.01,0.1,1,0.001],'gamma':[0.1,0.01,0.2,0.4]}\n\ngrid_svc = GridSearchCV(svc,param_grid,scoring='accuracy',cv = 10,refit = True)","49299612":"grid_svc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_svc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_svc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_svc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_svc.score(x_test_std,y_test))","1b0c10f9":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.ensemble import VotingClassifier\n\nclassifiers = [('knn',grid_knn),('tree',grid_dtc),('svc',grid_svc)]\n\nvtc = VotingClassifier(classifiers)","50419673":"vtc.fit(x_train_std,y_train)\nprint(\"Accuracy on Test set ==> \", vtc.score(x_test_std,y_test))","443832b2":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\n#for i in range(2,7):\n #   rfe = RFE(estimator=RandomForestClassifier(),n_features_to_select=i, verbose=0)\n  #  rfe.fit(x_train_std,y_train)\n   # print(f\"Accuracy with Feature {i} ==>\",metrics.accuracy_score(y_test, rfe.predict(x_test_std)))","6cb475aa":"#rfe = RFE(estimator=RandomForestClassifier(),n_features_to_select=5, verbose=0)\n#rfe.fit(x_train_std,y_train)","f8d40477":"#print(\"Important Features are ==> \",list(df.columns[:7][rfe.support_]))","61580533":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\n\nparam_grid = {'n_estimators':[200,500,1000],\n              'max_depth':[2,3,4,5],\n              'min_samples_leaf':[0.2,0.4,0.6,0.8,1],\n              'max_features':['auto','sqrt'],\n              'criterion':['gini','entropy']}\n\ngrid_rfc = RandomizedSearchCV(rfc,param_grid,n_iter=20,scoring='accuracy',cv = 10,refit = True)","74c1c5db":"grid_rfc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_rfc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_rfc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_rfc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_rfc.score(x_test_std,y_test))","1ca4c2b6":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\n#import xgboost as xgb\n\n#xgbcl = xgb.XGBClassifier()\n\n#param_grid = {'booster':['gbtree','gblinear'],\n #            'colsample_bytree':[0.4,0.6,0.8,1],\n  #           'learning_rate':[0.01,0.1,0.2,0.4],\n   #          'max_depth':[2,3,4,6],\n    #         'n_estimators':[200,300,400,500],\n     #         'subsample':[0.4,0.6,0.8,1]}\n\n#grid_xgb = RandomizedSearchCV(xgbcl,param_grid,n_iter=30,scoring='accuracy',cv = 10,refit = True)","655c55a3":"#grid_xgb.fit(x_train_std,y_train)\n#print(\"Best Score ==> \", grid_xgb.best_score_)\n#print(\"Tuned Paramerers ==> \",grid_xgb.best_params_)\n#print(\"Accuracy on Train set ==> \", grid_xgb.score(x_train_std,y_train))\n#print(\"Accuracy on Test set ==> \", grid_xgb.score(x_test_std,y_test))","01f14fd0":"#Code by Ravi Chaubey https:\/\/www.kaggle.com\/ravichaubey1506\/predictive-modelling-knn-ann-xgboost\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential()\n\nclassifier.add(Dense(units= 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 53859))\nclassifier.add(Dense(units= 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nclassifier.fit(x_train_std, y_train, batch_size = 10, epochs = 100)","d5e2b53f":"import sklearn.metrics as metrics\n\n\ny_pred_test = classifier.predict(x_test_std)\ny_pred_test=y_pred_test>0.5\n\ny_pred_train = classifier.predict(x_train_std)\ny_pred_train=y_pred_train>0.5\n\nprint(\"Accuracy on Train Set ==> \",metrics.accuracy_score(y_train,y_pred_train))\nprint(\"Accuracy on Test Set ==> \",metrics.accuracy_score(y_test,y_pred_test))","c8ad5804":"#Decision Tree","7182f279":"#ANN","fe80cfac":"#XGBoost Classifier","24927fc9":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">The Limitations of Machine Learning Approaches<\/h1>\n\nDespite the pragmatic advantages, the application of ML models requires a clear understanding of what determines model performance and the potential pitfalls of specific models.\n\nConcerns regarding data analysesshould be analyzed, which include data sparsity, data bias, and causality assumptions. Good practice recommendations for model design will then be presented, including the management of missing data, model overfitting, model validation, and performance reporting.\n\nhttps:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_t\/fnins-13-00135-g005.gif","ba457afc":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Machine Learning in Amyotrophic Lateral Sclerosis: Achievements, Pitfalls, and Future Directions<\/h1>\n\nAuthors: Vincent Grollemund, Pierre-Fran\u00e7ois Pradat, Giorgia Querin, Fran\u00e7ois Delbot, Ga\u00e9tan Le Chat, Jean-Fran\u00e7ois Pradat-Peyre and Peter Bede\n\nFront. Neurosci., 28 February 2019 | https:\/\/doi.org\/10.3389\/fnins.2019.00135\n\nSurvival from symptom onset ranges from 3 to 5 years depending on genetic, demographic, and phenotypic factors. Despite tireless research efforts, the core etiology of the disease remains elusive and drug development efforts are confounded by the lack of accurate monitoring markers. \n\nFrom a mathematical perspective, the main barrier to the development of validated diagnostic, prognostic, and monitoring indicators stem from limited sample sizes. The combination of multiple clinical, biofluid, and imaging biomarkers is likely to increase the accuracy of mathematical modeling and contribute to optimized clinical trial designs.\n\nML methods utilized in ALS research. These include Random Forests (RF), Support Vector Machines (SVM), Neural Networks (NN), Gaussian Mixture Models (GMM), Boosting methods, k-Nearest Neighbors (k-NN), Generalized linear regression models, Latent Factor models and Hidden Markov Models (HMM).\n\nhttps:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2019.00135\/full","2d078ed3":"#Feature Selection","b77a8e5c":"#Voting Classifier","55de1d9c":"#SVC","33806878":"#Random forest for diagnosis. \n \nThe available data consist of basic biomarkers features which are MUNIX, CSF Neurofilament (NF) levels, Vital Capacity (VC), and BMI. The objective is to classify subjects between healthy and ALS patients. The RF contains 3 decisions trees which use different feature subsets to learn a diagnosis model. Tree A learns on all available features, Tree B learns on MUNIX and VC, Tree C learns on NF levels and BMI. Each tree proposes a diagnosis. RF diagnosis is computed based on the majority vote of each of the trees contained in the forest. Given that two out of three trees concluded that patient 0 had ALS, the final diagnosis suggested by the model is ALS.\n\n![](https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_m\/fnins-13-00135-g004.jpg)https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2019.00135\/full","9e8f7ca1":"#SVM model for prognosis.\n\nThe available data consist of basic clinical and demographic features; age and site of onset. The objective is to classify patients according to 3-year survival. In the input space (where features are interpretable), no linear hyperplane can divide the two patient populations. The SVM model projects the data into a higher dimensional space\u2014in our example a three dimensional space. The set of two features is mapped to a set of three features. In the feature space, a linear hyperplane can be computed which discriminates the two populations accurately. The three features used for discrimination are unavailable for analysis and interpretability is lost in the process.\n\n\n![](https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_m\/fnins-13-00135-g005.jpg)https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_t\/fnins-13-00135-g005.gif","f0f4428e":"#Neural Network model for prognosis.\n\nThe available data consist of basic demographic and clinical features: age, BMI and diagnostic delay. For patient 0, these features are 50, 15kg\/m2, and 15 months, respectively. The objective is to predict ALSFRS-r in 1 year. The multi-layer perceptron consists of two layers. Nodes are fed by input with un-shaded arrows. At layer 1, the three features are combined linearly to compute three node values, C1, C2, and C3. C1 is a linear combination of age and delay, C2 is a linear combination of age, delay and BMI, and C3 is a linear combination of BMI and delay. For patient 0, computing the three values returns 10, 2, and 2 for C1, C2, and C3, respectively. At layer 2, outputs from layer 1 (i.e., C1, C2, and C3) are combined linearly to compute two values, CA and CB. CA is a linear combination of C1 and C2 while CB is a linear combination of C1 and C3. For patient 0, computing the two values gives 24 and 14 for CA and CB, respectively. Model output is computed after computing linear combination of CA and CB and applying a non-linear function (in this case a maximum function which can be seen as a thresholding function which accepts only positive values). The output is the predicted motor functions decline rate. For patient 0, the returned score is 26.\n\n![](https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_m\/fnins-13-00135-g006.jpg)https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_t\/fnins-13-00135-g005.gif","989e9739":"#Decision tree model for diagnosis.\n\nThe available data consist of three basic neuroimaging features: average Corticospinal Tract (CST) Fractional Anisotropy (FA), Motor Cortex (MC) thickness, and average Corpus Callosum (CC) FA. For patient 0, these features are reduced CST FA, reduced MC thickness, reduced CC FA. The target is to classify subjects between healthy and ALS subjects. Establishing a diagnosis requires to run through the decision tree till there are no more questions to answer. At step 1, the closed question directs to the right node due to patient 0's CST pathology. At step 2, the closed question directs to the right node due to patient 0's MC pathology. At step 3, the closed question directs to the left node due to patient 0 CC involvement. Step 3 is the last step as there is no more steps below. The diagnosis for patient 0 is the arrival cell value.\n\n![](https:\/\/www.frontiersin.org\/files\/Articles\/438192\/fnins-13-00135-HTML\/image_m\/fnins-13-00135-g003.jpg)https:\/\/www.frontiersin.org\/articles\/10.3389\/fnins.2019.00135\/full","657b8585":"#Random Forest Classifier","378fe76d":"#KNN"}}