{"cell_type":{"7ccbc39e":"code","32189c87":"code","b1e01847":"code","72a5449d":"code","61648fe7":"code","f3e98f7e":"code","2bec6108":"code","22cbb499":"code","4a76ebf4":"code","71c64ea0":"code","4ac0f503":"code","eada7c42":"code","2720486d":"code","49590f1e":"code","2851150f":"code","51bc6164":"code","0b71b182":"code","317fe004":"code","18cd93a6":"code","734fc9eb":"code","fac766ab":"code","0e5c17f5":"code","a6503cd2":"code","5c0bfe71":"code","17e12b84":"code","2210c7e8":"code","43b6eaff":"code","c6fdc9ad":"code","6926e103":"code","c167ecf6":"code","1368257c":"code","cf0a22b5":"code","fb7b6d9d":"code","83920e73":"code","9b12a11b":"code","6675d1eb":"code","d342af55":"code","65a3a4b4":"code","ac01f93f":"code","64df3f47":"code","3f1e60c5":"code","0571ee6e":"code","f6dce582":"code","4a7c31e4":"code","3e4f3805":"code","5ff85039":"code","c0dc9b22":"code","7a11064d":"code","70f57c61":"code","08e93bad":"code","853b2d66":"code","eb8eb50d":"code","60bec360":"code","ac4a0aba":"code","06ccff52":"code","fcd87d9c":"code","84f6795c":"code","ca76f90e":"code","b7df3ac0":"code","2b1d4481":"code","bf23ccae":"code","c66db292":"code","872fe46d":"code","e1ff1262":"code","5e6ffa05":"code","ce1aefff":"code","d5d024f4":"code","f7b817d8":"code","84ec45cd":"code","7d9b3989":"code","9ead2393":"code","97e5fafd":"code","39f65257":"code","a32e6017":"code","dc1de5d6":"code","f5b126dc":"code","21e43c15":"code","9d1f0767":"code","11552323":"code","351d358d":"code","62a1ea0d":"code","af7915d6":"code","bf6b8f9d":"code","eabfe899":"code","8b82243d":"code","4721cb5c":"code","486194e6":"code","af8cb62a":"code","4f673056":"code","3fb9bca6":"code","5301249a":"code","c3a04ede":"code","1f0c3815":"code","cf038a3a":"code","74949fe1":"code","7ca08f9a":"code","2b0e320b":"code","4067f504":"code","44f2000d":"code","915e04ce":"code","af87d15c":"code","5b93c3da":"code","cf761b4c":"code","d280565c":"code","dd18de09":"code","06d3b667":"code","f65b1e90":"code","b3e136a9":"code","7c603152":"code","14053cf2":"code","8dc09a6c":"code","efef88ee":"code","01b88586":"code","96a43825":"code","8e4d698d":"code","d20126e1":"markdown","f8b37af0":"markdown","7868391a":"markdown","a2529044":"markdown","b3cd4437":"markdown","6a6f8add":"markdown","ae903cf5":"markdown","968661ef":"markdown","2d91dcad":"markdown","e2e92d77":"markdown","367ea35e":"markdown","d946371a":"markdown","f67c9d01":"markdown","39010a03":"markdown","a2c15378":"markdown","196b7f5f":"markdown","82289645":"markdown","b9d535f7":"markdown","0edde4c3":"markdown","ab90bd27":"markdown","c3978f12":"markdown","b420a310":"markdown","825b4a60":"markdown","5b151383":"markdown","55eccea9":"markdown","8d163982":"markdown","7c1d5304":"markdown","9518cde3":"markdown","ac1b4978":"markdown","2664e221":"markdown","6d83c2e5":"markdown","ea1f3c06":"markdown","cf7ba09a":"markdown"},"source":{"7ccbc39e":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n# special matplotlib argument for improved plots\nfrom matplotlib import rcParams\n\n\nnp.set_printoptions(precision=4)\n\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nsns.set(font_scale=1.5)","32189c87":"# Load the Boston housing dataset\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b1e01847":"train_data.head()","72a5449d":"train_data.tail()","61648fe7":"# quike look to get various summary statistics in numeric data.\ntrain_data.describe()","f3e98f7e":"# quike look to get various summary statistics in categorical data.\ntrain_data.describe(include=['O'])","2bec6108":"# grab numeric columns.\nnumeric_features = train_data.select_dtypes(include=[np.number])\n\nnumeric_features.columns","22cbb499":"# grab object columns.\ncategorical_features = train_data.select_dtypes(include=[np.object])\n\nnumeric_features.columns","4a76ebf4":"# shows missing data per column\nplt.figure(figsize=(19,9))\nsns.heatmap(train_data.isnull(),cbar=False)\n","71c64ea0":"#the skewness of train data is rghite skewnsess\nsns.distplot(train_data.skew(),color='blue',axlabel ='Skewness')","4ac0f503":"sns.barplot(y='SalePrice',x='Utilities',data=train_data)\nplt.title('Sale price by utilites')","eada7c42":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='Neighborhood',data=train_data)\nxt = plt.xticks(rotation=45)\nplt.title('Sale Price by Neighborhood')","2720486d":"plt.figure(figsize = (19, 9))\nsns.countplot(x = 'Neighborhood', data = train_data)\nxt = plt.xticks(rotation=45)\nplt.title('Number of Sales per Neighborhood')","49590f1e":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='SaleType',data=train_data)\nplt.title('Sale Price by Sale Type')","2851150f":"plt.figure(figsize=(19,9))\nsns.barplot(y='SalePrice',x='SaleCondition',data=train_data)\nplt.title('Sale Price by Sale Condition')","51bc6164":"#to see which kind of relationship we have \nsns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train_data[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","0b71b182":"fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nOverallQual_scatter_plot = pd.concat([train_data['SalePrice'],train_data['OverallQual']],axis = 1)\nsns.regplot(x='OverallQual',y = 'SalePrice',data = OverallQual_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\nTotalBsmtSF_scatter_plot = pd.concat([train_data['SalePrice'],train_data['TotalBsmtSF']],axis = 1)\nsns.regplot(x='TotalBsmtSF',y = 'SalePrice',data = TotalBsmtSF_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\nGrLivArea_scatter_plot = pd.concat([train_data['SalePrice'],train_data['GrLivArea']],axis = 1)\nsns.regplot(x='GrLivArea',y = 'SalePrice',data = GrLivArea_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\nGarageArea_scatter_plot = pd.concat([train_data['SalePrice'],train_data['GarageArea']],axis = 1)\nsns.regplot(x='GarageArea',y = 'SalePrice',data = GarageArea_scatter_plot,scatter= True, fit_reg=True, ax=ax4)\nFullBath_scatter_plot = pd.concat([train_data['SalePrice'],train_data['FullBath']],axis = 1)\nsns.regplot(x='FullBath',y = 'SalePrice',data = FullBath_scatter_plot,scatter= True, fit_reg=True, ax=ax5)\nYearBuilt_scatter_plot = pd.concat([train_data['SalePrice'],train_data['YearBuilt']],axis = 1)\nsns.regplot(x='YearBuilt',y = 'SalePrice',data = YearBuilt_scatter_plot,scatter= True, fit_reg=True, ax=ax6)\nYearRemodAdd_scatter_plot = pd.concat([train_data['SalePrice'],train_data['YearRemodAdd']],axis = 1)\nYearRemodAdd_scatter_plot.plot.scatter('YearRemodAdd','SalePrice')","317fe004":"saleprice_overall_quality= train_data.pivot_table(index ='OverallQual',values = 'SalePrice', aggfunc = np.median)\nsaleprice_overall_quality.plot(kind = 'bar',color = 'blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.show()","18cd93a6":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","734fc9eb":"var = 'Neighborhood'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","fac766ab":"for c in categorical_features:\n    train_data[c] = train_data[c].astype('category')\n    if train_data[c].isnull().any():\n        train_data[c] = train_data[c].cat.add_categories(['MISSING'])\n        train_data[c] = train_data[c].fillna('MISSING')\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train_data, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","0e5c17f5":"k= 11\ncorrelation=train_data.corr()\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train_data[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","a6503cd2":"train_data.head()","5c0bfe71":"train_data.shape","17e12b84":"train_data.info()","2210c7e8":"#we did this to 3 columns, the observation were originally numbers( int) but each number refer to a certain category\n#so we replaced each one to its meaning ( string) we took these definitions from the data description file ","43b6eaff":"td_MSSubClass_col=     {20: '1-STORY 1946 & NEWER ALL STYLES',\n                        30: '1-STORY 1945 & OLDER',\n                        40: '1-STORY W\/FINISHED ATTIC ALL AGES',\n                        45: '1-1\/2 STORY - UNFINISHED ALL AGES',\n                        50: '1-1\/2 STORY FINISHED ALL AGES',\n                        60: '2-STORY 1946 & NEWER',\n                        70: '2-STORY 1945 & OLDER',\n                        75: '2-1\/2 STORY ALL AGES',\n                        80: 'SPLIT OR MULTI-LEVEL',\n                        85: 'SPLIT FOYER',\n                        90: 'DUPLEX - ALL STYLES AND AGES',\n                        120:'1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n                        150:'1-1\/2 STORY PUD - ALL AGES',\n                        160:'2-STORY PUD - 1946 & NEWER',\n                        180: 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n                        190: '2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\ntrain_data['MSSubClass']=train_data['MSSubClass'].replace(td_MSSubClass_col)\n\n#train_data.MSSubClass.value_counts()","c6fdc9ad":"OverallQual_col=     {10:\"Very Excellent\",\n                      9:\"Excellent\",\n                       8:\"Very Good\",\n                       7:\"Good\",\n                       6:\"Above Average\",\n                       5:\"Average\",\n                       4:\"Below Average\",\n                       3:\"Fair\",\n                       2:\"Poor\",\n                       1:\"Very Poor\"}\n\n\ntrain_data['OverallQual']=train_data['OverallQual'].replace(OverallQual_col)\n\n#train_data.OverallQual.value_counts()","6926e103":"OverallCond_col = {10:\"Very Excellent\",\n                   9:\"Excellent\",\n                   8:\"Very Good\",\n                   7:\"Good\",\n                   6:\"Above Average\",\n                   5:\"Average\",\n                   4:\"Below Average\",\n                   3:\"Fair\",\n                   2:\"Poor\",\n                   1:\"Very Poor\"}\n\ntrain_data['OverallCond']=train_data['OverallCond'].replace(OverallCond_col)\n\n\n#train_data.OverallCond.value_counts()","c167ecf6":"# to check how many null cells we have in every column \ntrain_data.isnull().sum().sort_values(ascending = False).head()","1368257c":"# we replaced each null cells with eather the median if the column's type were organnly int or flout.\n#otherwise we replaced it with the mode  if it was catagorical.","cf0a22b5":"#print(\"LotFrontage median = \",train_data.LotFrontage.median())\n#print (\"MasVnrArea median = \",train_data.MasVnrArea.median())","fb7b6d9d":"train_data[\"PoolQC\"].fillna(\"No Pool\",inplace=True)\ntrain_data[\"Electrical\"].fillna(\"SBrkr\",inplace=True)\ntrain_data[\"MasVnrArea\"].fillna(0,inplace=True)\ntrain_data[\"LotFrontage\"].fillna(69,inplace=True)\ntrain_data[\"MasVnrType\"].fillna(\"None\",inplace=True)\ntrain_data[\"FireplaceQu\"].fillna(\"No Fireplace\",inplace=True)\ntrain_data[\"MiscFeature\"].fillna(\"standerd\",inplace=True)\ntrain_data[\"Alley\"].fillna(\"No alley access\",inplace=True)\ntrain_data[\"Fence\"].fillna(\"No Fence\",inplace=True)\ntrain_data[\"GarageCond\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"GarageType\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"GarageYrBlt\"].fillna(0,inplace=True)\ntrain_data[\"GarageFinish\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"BsmtExposure\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"GarageQual\"].fillna(\"No Garage\",inplace=True)\ntrain_data[\"BsmtFinType2\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtCond\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtFinType1\"].fillna(\"No Basement\",inplace=True)\ntrain_data[\"BsmtQual\"].fillna(\"No Basement\",inplace=True)","83920e73":"test_data.head()","9b12a11b":"test_data.shape","6675d1eb":"test_data.info()","d342af55":"# to check how many null cells we have in every column \ntest_data.isnull().sum().sort_values(ascending = False).head() ","65a3a4b4":"#test_data.MSZoning.value_counts()\ntest_data[\"MSZoning\"].fillna(\"RL\",inplace=True)\n\n#test_data.LotFrontage.median()\ntest_data[\"LotFrontage\"].fillna(67,inplace=True)\n\ntest_data[\"Alley\"].fillna(\"No alley access\",inplace=True)\n\n#test_data.Utilities.value_counts()\ntest_data[\"Utilities\"].fillna(\"AllPub\",inplace=True)\n\n#test_data.Exterior1st.value_counts()\ntest_data[\"Exterior1st\"].fillna(\"VinylSd\",inplace=True)\n\n#test_data.Exterior2nd.value_counts()\ntest_data[\"Exterior2nd\"].fillna(\"VinylSd\",inplace=True)\n\n#test_data.BsmtQual.value_counts()\ntest_data[\"BsmtQual\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.MasVnrType.value_counts()\ntest_data[\"MasVnrType\"].fillna(\"None\",inplace=True)\n\n#test_data.BsmtCond.value_counts()\ntest_data[\"BsmtCond\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.BsmtExposure.value_counts()\ntest_data[\"BsmtExposure\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.PoolQC.value_counts()\ntest_data[\"PoolQC\"].fillna(\"No Pool\",inplace=True)\n\n#test_data.MiscFeature.value_counts()\ntest_data[\"MiscFeature\"].fillna(\"None\",inplace=True)\n\n#test_data.MiscFeature.value_counts()\ntest_data[\"MiscFeature\"].fillna(\"None\",inplace=True)\n\n#test_data.Fence.value_counts()\ntest_data[\"Fence\"].fillna(\"No Fence\",inplace=True)\n\n#test_data.FireplaceQu.value_counts()\ntest_data[\"FireplaceQu\"].fillna(\"No Fireplace\",inplace=True)\n\n#test_data.GarageYrBlt.value_counts()\ntest_data[\"GarageYrBlt\"].fillna(0,inplace=True)\n\n#test_data.GarageCond.value_counts()\ntest_data[\"GarageCond\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageFinish.value_counts()\ntest_data[\"GarageFinish\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageQual.value_counts()\ntest_data[\"GarageQual\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.GarageType.value_counts()\ntest_data[\"GarageType\"].fillna(\"No Garage\",inplace=True)\n\n#test_data.BsmtFinType2.value_counts()\ntest_data[\"BsmtFinType2\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.BsmtFinType1.value_counts()\ntest_data[\"BsmtFinType1\"].fillna(\"No Basement\",inplace=True)\n\n#test_data.MasVnrArea.value_counts()\ntest_data[\"MasVnrArea\"].fillna(0,inplace=True)\n\n#test_data.Functional.value_counts()\ntest_data[\"Functional\"].fillna(\"Typ\",inplace=True)\n\n#test_data.BsmtFullBath.value_counts()\ntest_data[\"BsmtFullBath\"].fillna(0,inplace=True)\n\n#test_data.BsmtUnfSF.value_counts()\ntest_data[\"BsmtUnfSF\"].fillna(0,inplace=True)\n\n#test_data.TotalBsmtSF.value_counts()\ntest_data[\"TotalBsmtSF\"].fillna(0,inplace=True)\n\n#test_data.BsmtHalfBath.value_counts()\ntest_data[\"BsmtHalfBath\"].fillna(0,inplace=True)\n\n#test_data.BsmtFinSF2.value_counts()\ntest_data[\"BsmtFinSF2\"].fillna(0,inplace=True)\n\n#test_data.SaleType.value_counts()\ntest_data[\"SaleType\"].fillna(\"WD\",inplace=True)\n\n#test_data.BsmtFinSF1.value_counts()\ntest_data[\"BsmtFinSF1\"].fillna(0,inplace=True)\n\n#test_data.GarageCars.value_counts()\ntest_data[\"GarageCars\"].fillna(2,inplace=True)\n\n# t=test_data[test_data.GarageArea.isnull()]\n# t[[\"GarageArea\",\"GarageCars\"]]\n#test_data.groupby(by=\"GarageCars\").mean()\n# the mean of GarageArea with 2 cars (just like the null cell here) is 519.042857\n#test_data.GarageArea.value_counts()\ntest_data[\"GarageArea\"].fillna(519.042857,inplace=True)\n\n# k=test_data[test_data.KitchenQual.isnull()]\n# test_data.loc[ :,[\"KitchenQual\",\"KitchenAbvGr\"]].groupby(by=\"KitchenAbvGr\").describe()\n# we saw that TA is the most frequent (KitchenQual) whenver the (KitchenAbvGr) is equal to 1, which is the case here\n# in the null cell\n#test_data.KitchenQual.value_counts()\ntest_data[\"KitchenQual\"].fillna(\"TA\",inplace=True)\n","ac01f93f":"td_MSSubClass_col=     {20: '1-STORY 1946 & NEWER ALL STYLES',\n                        30: '1-STORY 1945 & OLDER',\n                        40: '1-STORY W\/FINISHED ATTIC ALL AGES',\n                        45: '1-1\/2 STORY - UNFINISHED ALL AGES',\n                        50: '1-1\/2 STORY FINISHED ALL AGES',\n                        60: '2-STORY 1946 & NEWER',\n                        70: '2-STORY 1945 & OLDER',\n                        75: '2-1\/2 STORY ALL AGES',\n                        80: 'SPLIT OR MULTI-LEVEL',\n                        85: 'SPLIT FOYER',\n                        90: 'DUPLEX - ALL STYLES AND AGES',\n                        120:'1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n                        150:'1-1\/2 STORY PUD - ALL AGES',\n                        160:'2-STORY PUD - 1946 & NEWER',\n                        180: 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n                        190: '2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\ntest_data['MSSubClass']=test_data['MSSubClass'].replace(td_MSSubClass_col)","64df3f47":"OverallQual_col=     {10:\"Very Excellent\",\n                      9:\"Excellent\",\n                       8:\"Very Good\",\n                       7:\"Good\",\n                       6:\"Above Average\",\n                       5:\"Average\",\n                       4:\"Below Average\",\n                       3:\"Fair\",\n                       2:\"Poor\",\n                       1:\"Very Poor\"}\n\n\ntest_data['OverallQual']=test_data['OverallQual'].replace(OverallQual_col)","3f1e60c5":"OverallCond_col = {10:\"Very Excellent\",\n                   9:\"Excellent\",\n                   8:\"Very Good\",\n                   7:\"Good\",\n                   6:\"Above Average\",\n                   5:\"Average\",\n                   4:\"Below Average\",\n                   3:\"Fair\",\n                   2:\"Poor\",\n                   1:\"Very Poor\"}\n\ntest_data['OverallCond']=test_data['OverallCond'].replace(OverallCond_col)","0571ee6e":"%matplotlib inline \n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport sklearn\nimport statsmodels.api as sm\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n# special matplotlib argument for improved plots\nfrom matplotlib import rcParams","f6dce582":"# Let\u2019s first plot the distribution of the target variable. \n# We will use the histogram plot function from the matplotlib library.\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nplt.hist(train_data['SalePrice'], bins=30)\nplt.xlabel(\"House prices in $1000\")\nplt.show()","4a7c31e4":"plt.figure(figsize=(20, 5))\n\nfeatures = ['SaleType', 'SaleCondition']\ntarget = train_data['SalePrice']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = train_data[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(\"Variation in House prices\")\n    plt.xlabel(col)\n    plt.ylabel('\"House prices in $1000\"')","3e4f3805":"# Jointplots for high correlations - no. of rooms\n\n\nplt.figure (figsize=(10,10))\nsns.jointplot(x = 'TotRmsAbvGrd', y = 'SalePrice', data = train_data, kind = 'hex', color = 'green', height = 10)","5ff85039":"# Jointplots for high correlations \n\nplt.figure (figsize=(10,10))\nsns.jointplot(x = 'YearBuilt', y = 'SalePrice', data = train_data, kind = 'reg', height = 10, color = 'orange');","c0dc9b22":"plt.figure(figsize=(20, 5))\n\nfeatures = ['SaleType', 'SaleCondition']\ntarget = train_data['SalePrice']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = train_data[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(\"Variation in House prices\")\n    plt.xlabel(col)\n    plt.ylabel('\"House prices in $1000\"')","7a11064d":"# extracting only the numeric columns from the training data, why?\n# because after getting dummies, and scaling, the dummy columns will have only 2 numbers\n#and we want to remove outliers but we can't rely on those, because they weren't originally numbers.\n#and the numeric values they represent aren't real.\n# later on we do a heatmap to find the most numaric featuers that effect the saleprice (target)\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nr = train_data.select_dtypes(include=numerics)\n\nscaler = StandardScaler()\nr_std = pd.DataFrame(scaler.fit_transform(r),columns = r.columns)\n\nfig = plt.figure(figsize= (15,15))\ncorr=r_std.corr()\nsns.heatmap(corr[['SalePrice']].sort_values(by=['SalePrice'],ascending=False),\n            vmin=-1,\n            cmap='coolwarm',\n            annot=True);\nfont = {\n        'color':  'black',\n        'weight': 'normal',\n        'size': 20,\n        }\nplt.title (\"Correlation\",fontdict=font)\n\n\n\n","70f57c61":"train_data.shape","08e93bad":"X= train_data.iloc[:,0:80]\n\nX_dum=pd.get_dummies(X)\ny=train_data[\"SalePrice\"]\n\n#adding columns that will exist in the testing data if you get dummies in it.\n#make them equal to zero in the trainng data\nu=['MSSubClass_1-1\/2 STORY PUD - ALL AGES', 'MiscFeature_None']\nfor i in u:\n    X_dum[i]=0\n\n\nscaler = StandardScaler()\ntrain_no_std = pd.DataFrame(scaler.fit_transform(X_dum),columns = X_dum.columns)\ntrain_no_std.shape\n","853b2d66":"train_no_std[\"GrLivArea\"]=train_no_std[\"GrLivArea\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"TotalBsmtSF\"]=train_no_std[\"TotalBsmtSF\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"GarageArea\"]=train_no_std[\"GarageArea\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\ntrain_no_std[\"GarageCars\"]=train_no_std[\"GarageCars\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"1stFlrSF\"]=train_no_std[\"1stFlrSF\"].apply(lambda x:np.nan if x<-2.69 or x>2.69 else x)\n#train_no_std[\"FullBath\"]=train_no_std[\"FullBath\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"TotRmsAbvGrd\"]=train_no_std[\"TotRmsAbvGrd\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"YearBuilt\"]=train_no_std[\"YearBuilt\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)\n#train_no_std[\"YearRemodAdd\"]=train_no_std[\"YearRemodAdd\"].apply(lambda x:np.nan if x<-1.96 or x>1.96 else x)","eb8eb50d":"train_no_std[\"SalePrice\"]=y","60bec360":"train_no_std.shape","ac4a0aba":"train_no_std.dropna(inplace= True)","06ccff52":"train_no_std.shape","fcd87d9c":"train_no_std.head()","84f6795c":"train_data.head()","ca76f90e":"train_data.shape","b7df3ac0":"t= train_data.iloc[:,0:80]\nt_dummy=pd.get_dummies(t)\nz=train_data[\"SalePrice\"]","2b1d4481":"#adding columns that will exist in the testing data if you get dummies in it.\n#make them equal to zero in the trainng data\n\nu=['MSSubClass_1-1\/2 STORY PUD - ALL AGES', 'MiscFeature_None']\nfor i in u:\n    t_dummy[\"MSSubClass_1-1\/2 STORY PUD - ALL AGES\"]=0\n    t_dummy[\"MiscFeature_None\"]=0","bf23ccae":"scaler = StandardScaler()\ntrain_standerdized = pd.DataFrame(scaler.fit_transform(t_dummy),columns = t_dummy.columns)\ntrain_standerdized.shape","c66db292":"train_standerdized[\"SalePrice\"]=z","872fe46d":"train_standerdized.shape","e1ff1262":"# we are testing the no outlier module train_no_std\n\nX=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","5e6ffa05":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","ce1aefff":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n# create a LassoCV model instance\nmodel = LassoCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","d5d024f4":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nlasso_cross=Lasso(alpha=532.9994080844093)\n\nscores = cross_val_score(lasso_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","f7b817d8":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","84ec45cd":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","7d9b3989":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n# create a LassoCV model instance\nmodel = RidgeCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","9ead2393":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nRidg_cross=Ridge(alpha=136.18652367560827)\n\nscores = cross_val_score(Ridg_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n\n\n","97e5fafd":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","39f65257":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","a32e6017":"from sklearn.linear_model import ElasticNetCV\n\nmodel__E = ElasticNetCV(alphas=np.logspace(-4, 4, 100),l1_ratio=np.arange(0.1, 1, 0.1) ,\n                       n_jobs=-1,verbose=1,cv=5) \n\n# fit the model\nmodel__E.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model__E.alpha_)\n\nprint('Best l1_ratio:', model__E.l1_ratio_)\n\n# evaluate on the training set\nprint('Training score:', model__E.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model__E.score(X_test, y_test))","dc1de5d6":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\nelastic_cross=ElasticNet(alpha=2.782559402207126,l1_ratio=0.9)\n\nscores = cross_val_score(elastic_cross,X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n","f5b126dc":"from sklearn.ensemble import RandomForestRegressor","21e43c15":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","9d1f0767":"rand = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11,\n                      max_features=150, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=400,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrand.fit(X,y)","11552323":"from sklearn.model_selection import cross_val_score\n\nkf = KFold(n_splits=5, shuffle=True, random_state=10) # notice shuffle \nprint('Cross Validation Score:',cross_val_score(rand,X,y,cv=kf).mean())","351d358d":"from sklearn.ensemble import RandomForestRegressor\nX=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","62a1ea0d":" rand_fo= RandomForestRegressor()\n\nrand_pa = {'n_estimators': [200, 400],\n     'max_features':np.arange(100,250,10),\n    'max_depth': [5, 6,7,8,9,10,11]}\n","af7915d6":"rand_grid = GridSearchCV(rand_fo, rand_pa, n_jobs = -1, verbose = 1)\nrand_grid.fit(X,y)","bf6b8f9d":"rand_grid.best_estimator_","eabfe899":"X= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","8b82243d":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","4721cb5c":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nmodel_lass = LassoCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel_lass.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_lass.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model_lass.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model_lass.score(X_test, y_test))","486194e6":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nlasso_cross=Lasso(alpha=1581.9734815786014)\n\nscores = cross_val_score(lasso_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","af8cb62a":"X= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","4f673056":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","3fb9bca6":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV , ElasticNetCV\n\nmodel = RidgeCV(alphas=np.logspace(-4, 4, 1000), cv=5) \n\n# fit the model\nmodel.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model.score(X_test, y_test))","5301249a":"from sklearn.model_selection import cross_val_score, cross_val_predict\n\nredg_cross=Ridge(alpha=845.1366330684722)\n\nscores = cross_val_score(redg_cross, X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())","c3a04ede":"from sklearn.linear_model import ElasticNetCV\nX= train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice\n","1f0c3815":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=10)","cf038a3a":"model_E = ElasticNetCV(alphas=np.logspace(-4, 4, 100),l1_ratio=np.arange(0.1, 1, 0.1) ,\n                       n_jobs=-1,verbose=1,cv=5) \n\n\n# fit the model\nmodel_E.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_E.alpha_)\n\nprint('Best l1_ratio:', model_E.l1_ratio_)\n\n# evaluate on the training set\nprint('Training score:', model_E.score(X_train, y_train))\n\n# evaluate on the test set\nprint(\"Test Score:\", model_E.score(X_test, y_test))","74949fe1":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\nelastic_cross=ElasticNet(alpha=8.497534359086455,l1_ratio=0.9)\n\nscores = cross_val_score(elastic_cross,X_train,y_train, cv=5)\n\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean of Ccoss-validated scores:\", scores.mean())\n","7ca08f9a":"X=train_standerdized.iloc[:,0:336]\ny=train_standerdized.SalePrice","2b0e320b":"rand_fo= RandomForestRegressor()\n\nrand_pa = {'n_estimators': [200, 400],\n     'max_features':np.arange(100,250,10),\n     'max_depth': [5, 6,7,8,9,10,11]}\n","4067f504":"rand_grid = GridSearchCV(rand_fo, rand_pa, n_jobs = -1, verbose = 1)\nrand_grid.fit(X,y)","44f2000d":"rand_grid.best_estimator_","915e04ce":"X=train_no_std.iloc[:,0:336]\ny=train_no_std.SalePrice","af87d15c":"rand_ = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11,\n                      max_features=150, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=400,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrand_.fit(X,y)","5b93c3da":"kf = KFold(n_splits=5, shuffle=True, random_state=10) # notice shuffle \nprint('Cross Validation Score:',cross_val_score(rand_,X,y,cv=kf).mean())","cf761b4c":"test_data.head()","d280565c":"test_dum=pd.get_dummies(test_data)","dd18de09":"test_dum.shape","06d3b667":"# finding columns names that were in the trainng data but not on the testing data,\n# adding them and make them equal to zero  ( not including the target column saleprice)\ndiff = [x for x in train_no_std.columns if x not in test_dum.columns]\ndiff","f65b1e90":"r=['Utilities_NoSeWa',\n 'Condition2_RRAe',\n 'Condition2_RRAn',\n 'Condition2_RRNn',\n 'HouseStyle_2.5Fin',\n 'RoofMatl_ClyTile',\n 'RoofMatl_Membran',\n 'RoofMatl_Metal',\n 'RoofMatl_Roll',\n 'Exterior1st_ImStucc',\n 'Exterior1st_Stone',\n 'Exterior2nd_Other',\n 'Heating_Floor',\n 'Heating_OthW',\n 'Electrical_Mix',\n 'GarageQual_Ex',\n 'PoolQC_Fa',\n 'MiscFeature_TenC',\n 'MiscFeature_standerd']\n\n\nfor i in r:\n    test_dum[i]=0\n\n    ","b3e136a9":"train_no_std.shape","7c603152":"test_dum.shape","14053cf2":"test_dum = pd.DataFrame(scaler.transform(test_dum),columns = test_dum.columns)\n#scaler.transform(test_dum)","8dc09a6c":"test_dum.shape","efef88ee":"rand.predict(test_dum)","01b88586":"submit=pd.DataFrame(columns=['Id',\"SalePrice\"])","96a43825":"submit.Id=test_data.Id\nsubmit.SalePrice=rand.predict(test_dum)\nsubmit.head()","8e4d698d":"submit.to_csv('randomeforest - 2 featuers - 2.69 outliers jjj.csv', index=False)","d20126e1":"# Visualization","f8b37af0":"### fist of all we have worked with 2 diffrent trainng data, one that we have orignllay recived form kaggle, and the other one where we have selecet 2 numaric fetuers that have a hihgly corlation with the target (Saleprice).\n\n### both of these data are scaled, filled the missing values, and completllt clean, the only diffrence is that in one we have removed the outliers using 2 featuers.\n\n\n### now in both of these trainng dataset, we have tried 4 diffrent approches:\n1-Ridge\n2-Lasso\n3- ElasticNet \n4- randome forest \nin all of these tests, we used the cv virsion before to get the best parameter ( RidgeCV, LassoCV , ElasticNetCV, GridSearchCV)\n\n### the scores of the mean cross valdation were like the following:\n#### after removing the outliers (train_no_std)\n1- ridge: 0.9053094586067507\n2- lasso: 0.9053094586067507\n3- ElasticNet: 0.8987689482673635\n4- randomeforest :  0.0.8833751936703015\n\n\n#### the orignal data  (train_standerdized)\n1- lasso : 0.7605890442091523\n2- ridge : 0.7930685106393112\n3- ElasticNet : 0.79267206152532\n4- randomforest : 0.0.8835427127158999\n\n\n### in all the test (exept the randomforest) the data with the removed outliers preformed better in the cross-valedation and even in the randomforest there wasn't great diffrence.\n\n\n# At the end the randome forest with the removed outliers preforemd better in kaggle with the 0.20624\n\nwe had a better results also using the randomeforest (0.20024)  but we forget which parameter we used with ): \n\n\n","7868391a":"# lasso ","a2529044":"# Conclusion","b3cd4437":"# lasso ","6a6f8add":"# EDA","ae903cf5":"# splittng the train data into 2 types","968661ef":"Screen Shot 2019-10-11 at 16.41.56![image.png](attachment:image.png)","2d91dcad":"# 4. EDA on test_data ","e2e92d77":"____\n____\n____","367ea35e":"# randome forest  GridSearchCV","d946371a":"# 3. Getting the right data types ","f67c9d01":"\n\n\n\n<img src=\"http:\/\/imgur.com\/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n\n# Project-2 | Part-1 : Housing Values in Suburbs of Boston\n\n\n**Wejdan, Yazeed and Amal.**\n\n---","39010a03":"# Ridge","a2c15378":"# 6. Gitting the right data types: Test-data","196b7f5f":"# fixing the testing data types","82289645":"# 1-without outliers ( train_no_std)","b9d535f7":"# 2- without deleiting the outliers (train_standerdized)","0edde4c3":"# testing on the reguler trainning data  (train_standerdized)\n\n1- lasso \n\n2- ridge\n\n3- Elastic Net Cv \n\n4- randome forest GridSearchCV\n\n5- randome forest","ab90bd27":"**Initial Visualization**","c3978f12":"# 2. Preparing Train Data:","b420a310":"# 4. Treating missing data in train datasets:","825b4a60":"# randome forest ","5b151383":"# randomeforest gridsearch","55eccea9":"# Elastic Net Cv ","8d163982":"# 5. Treating missing data in test datasets:","7c1d5304":"# Elastic Net Cv ","9518cde3":"# 1. Load Libraries and Data","ac1b4978":"# testing on the data without outliers (train_no_std)\n1- lasso \n\n2- ridge\n\n3- Elastic Net Cv \n\n4- randome forest \n\n5- randome forest  GridSearchCV","2664e221":"____\n____\n____","6d83c2e5":"# Ridge","ea1f3c06":"# we selected the fetuers that have 0.5 or more in the heatmap to remove the outliers.\n# the approch we followed was the same as the IQR approch but with the standerdevation, see the picture below.","cf7ba09a":"# randomeforest"}}