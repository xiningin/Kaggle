{"cell_type":{"72c36787":"code","15f1f551":"code","f9de67c1":"code","f7fe873a":"code","78a6cf6e":"code","b0d427af":"code","dcf01e3c":"code","6d434111":"code","9e49ed37":"code","8c8105f8":"code","4e1955cc":"code","51f8aa58":"code","e214f72e":"code","279df761":"code","3a94f60f":"code","fcd1b563":"code","fec156bb":"code","56d656ad":"code","8e0fd83c":"code","3efae918":"code","3cf1946d":"code","e7463530":"code","f56dd734":"code","59375214":"code","6119a067":"code","33baf3aa":"code","189b0da6":"code","75414670":"code","8dbc4283":"code","e35ad6dd":"code","077b2312":"code","860d2a54":"code","336f6220":"code","c04127d1":"code","e436fdb0":"code","3711ed62":"code","c87a1ebf":"code","504923db":"code","d31b72d6":"code","59ad1489":"code","5e116edc":"code","985653e1":"code","92c79514":"code","a270e030":"code","a6ff882c":"code","be176310":"code","4bc42e81":"code","d76c1013":"code","5384eeab":"code","4fa74d96":"code","f25f5932":"code","e6d8c483":"code","47b9dbe0":"code","c5055b68":"code","837863e1":"code","40369c3e":"code","7bab7c42":"code","40b30666":"code","758bf1e3":"code","5cbea2a4":"code","557c0a55":"code","f55a51d6":"code","99e8b6e3":"code","b68d37e8":"code","90d8020f":"markdown","d5410523":"markdown","a2721131":"markdown","c53bb94c":"markdown","f89b9be2":"markdown","9eebf0a3":"markdown","40e14056":"markdown","587824ff":"markdown","bc94f78b":"markdown","37291271":"markdown","35f47737":"markdown","7d21be13":"markdown","ddb9dfaf":"markdown","9c56f907":"markdown","aa52f6e7":"markdown","d4a6db8e":"markdown","76a3a13c":"markdown","9339fdcf":"markdown","5d4e0854":"markdown","a6c6ef86":"markdown","9d247d6f":"markdown","3ca4b057":"markdown","b23e43c1":"markdown","11538503":"markdown","0b853aac":"markdown","fc7d2077":"markdown","af421b9a":"markdown","1e53c460":"markdown","de32bce7":"markdown","f55a15c9":"markdown","3f23c1e5":"markdown","f4250b45":"markdown","7f71bbb3":"markdown","bcd8cf4f":"markdown","2f9fcedd":"markdown","86e0d274":"markdown","e14a4ba3":"markdown","a4e544eb":"markdown","e37d81d3":"markdown","5532e7cf":"markdown","7d067442":"markdown","2ea04b5b":"markdown","abd02283":"markdown","6b776f1e":"markdown","579b2352":"markdown","9730b7b8":"markdown","fd644142":"markdown","9490e07d":"markdown","4d2de21b":"markdown","179def90":"markdown","e14453c1":"markdown","5468dcc4":"markdown","dd113c37":"markdown","a357adf5":"markdown"},"source":{"72c36787":"import numpy as np \nimport pandas as pd\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer, LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.linear_model import Lasso, Ridge\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error","15f1f551":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","f9de67c1":"train.shape","f7fe873a":"train.dtypes","78a6cf6e":"types = []\n[types.append(x) for x in train.dtypes]\n\nprint(set(types))","b0d427af":"train.head()","dcf01e3c":"test.shape","6d434111":"test.dtypes","9e49ed37":"types = []\n[types.append(x) for x in test.dtypes]\n\nprint(set(types))","8c8105f8":"test.head()","4e1955cc":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\n\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(23)","51f8aa58":"missing_data_train['Total'].sum()","e214f72e":"train[missing_data_train.index[0:19].tolist()].dtypes","279df761":"def missing_data(data):\n    \n    missing_datatypes = [j for i in data.columns for j in ['-','?','--','@','NA','NaN','na','Na',' '] \n                         if j in data[i].unique()]\n\n    if len(missing_datatypes) > 0 and data.isnull().values.any() == False:\n        print(set(missing_datatypes))\n\n    elif len(missing_datatypes) > 0 and data.isnull().values.any() == True:\n        missing_datatypes.append('NaN')\n        print(set(missing_datatypes))\n\n    elif len(missing_datatypes) == 0 and data.isnull().values.any() == True:\n        print('NaN')\n\n    else:\n        print('No missing data founded')","3a94f60f":"missing_data(train)","fcd1b563":"missing_data(test)","fec156bb":"train_id = train['Id']\ntest_id = test['Id']\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","56d656ad":"data_distribution = train.hist(figsize=(15,15))","8e0fd83c":"sns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","3efae918":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","3cf1946d":"#correlation matrix\nf, ax = plt.subplots(figsize = (12, 9))\nsns.heatmap(train.corr(),annot = False, vmax=.8)","e7463530":"var = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', alpha=0.3, ylim=(0,800000));","f56dd734":"var = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', alpha=0.3, ylim=(0,800000));","59375214":"var = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice',alpha=0.3, ylim=(0,800000));","6119a067":"var = '1stFlrSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice',alpha=0.3, ylim=(0,800000));","33baf3aa":"var = 'GarageArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', alpha=0.3, ylim=(0,800000));","189b0da6":"var = 'GarageCars'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', alpha=0.3, ylim=(0,800000));","75414670":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","8dbc4283":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\n\nf, ax = plt.subplots(figsize=(20, 6))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","e35ad6dd":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea','TotalBsmtSF','1stFlrSF']\nsns.pairplot(train[cols], height = 1.5)\nplt.show();","077b2312":"train.corr().nlargest(15, 'SalePrice')['SalePrice'].index","860d2a54":"y = train['SalePrice'].values\nX = train.drop(['SalePrice'], axis=1)\n\n\nattrs = X.corr()\n# only important correlations and not auto-correlations\nthreshold = 0.6\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), columns=['attribute pair', 'correlation'])\n# sorted by absolute value\n\nunique_important_corrs = unique_important_corrs.loc[abs(unique_important_corrs['correlation']).argsort()[::-1]]\n\nunique_important_corrs\n","336f6220":"# PoolQC: Pool quality. NA = \"No Pool\"\ntrain['PoolQC']  = train[\"PoolQC\"].fillna(\"None\")\n#MiscFeature: Miscellaneous feature not covered in other categories. NA = None.\ntrain['MiscFeature']  = train[\"MiscFeature\"].fillna(\"None\")\n#Alley: Type of alley access to property. NA = No alley access\ntrain['Alley']  = train[\"Alley\"].fillna(\"None\")\n#Fence: Fence quality. NA = No Fence\ntrain['Fence']  = train[\"Fence\"].fillna(\"None\")\n#FireplaceQu: Fireplace quality. NA = No Fireplace.\ntrain['FireplaceQu']  = train[\"FireplaceQu\"].fillna(\"None\")\n\n# Replace the missing values with the mode.SBrkr: Standard Circuit Breakers & Romex\ntrain['Electrical'] = train['Electrical'].fillna(\"SBrkr\")\n\n#MasVnr features: Masonry veneer type. NA = None.\nfor col in ('MasVnrType', 'MasVnrArea'):\n    train[col] = train[col].fillna('None') \n\n#Garage features. NA = No garage.\nfor col in ('GarageCond', 'GarageType','GarageYrBlt', 'GarageFinish', 'GarageQual'):\n    train[col] = train[col].fillna('None') \n    \n#Bsmt: Basement features. NA = No Basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train[col] = train[col].fillna('None') \n    \n#LotFrontage: Linear feet of street connected to property. We can group the by neighborhoods, and fill in missing values by the median.\ntrain['LotFrontage']  = train[\"LotFrontage\"].fillna(train['LotFrontage'].median())\n\n\ntest.fillna(0, inplace = True)","c04127d1":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\n\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(20)","e436fdb0":"train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>400000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<400000)].index, inplace=True)\ntrain.drop(train[(train['GarageArea']>950) & (train['SalePrice']<500000)].index, inplace=True)\ntrain.drop(train[(train['GarageCars']>3) & (train['SalePrice']<650000)].index, inplace=True)\ntrain.drop(train[(train['TotalBsmtSF']>2500) & (train['SalePrice']<500000)].index, inplace=True)\ntrain.drop(train[(train['1stFlrSF']>2500) & (train['SalePrice']<500000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","3711ed62":"le = LabelEncoder()\n\ncat_cols_train = train.select_dtypes(include=['string', 'object']).columns.tolist()\ncat_cols_test = test.select_dtypes(include=['string', 'object']).columns.tolist()\n\n\n# I use two loops although you can use zip() because at the beginning I dropped some columns in the train dataset \nfor col in cat_cols_train:\n    train[col] = le.fit_transform(train[col].astype('string'))\n    \nfor col in cat_cols_test:\n    #Have to fit again the LabelEncoder object because there is new values to encode like 'RH'\n    test[col] = le.fit_transform(test[col].astype('string'))","c87a1ebf":"train.head(3)","504923db":"test.head(3)","d31b72d6":"train['SalePrice'] = np.log(train['SalePrice'])\n\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","59ad1489":"skew_features = train._get_numeric_data().apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","5e116edc":"f, ax = plt.subplots(figsize=(8, 7))\n\nax.set_xscale(\"log\")\nax = sns.boxplot(data=train[skew_index] , orient=\"h\", palette=\"Set2\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\n\nsns.despine(trim=True, left=True)","985653e1":"#Check that there is negative data and\/our zero values.\n\nprint(np.sum((train._get_numeric_data() < 0).values.ravel()))\nprint(np.sum((train._get_numeric_data() == 0).values.ravel()))","92c79514":"skewed_features = skew_index.tolist()\n\n#Store the SalePrice label because if not we could not apply the train dataset fit to transform the test dataset, \n#because different number of columns\ntrain_SalePrice = train['SalePrice']\ntrain.drop(['SalePrice'], axis=1, inplace=True)\n\nscaler = MinMaxScaler(feature_range=(1, 2))\npower = PowerTransformer(method = 'box-cox')\npipeline = Pipeline(steps=[('s', scaler),('p', power)])\npipeline.fit(train[skewed_features])\ntrain[skewed_features] = pipeline.transform(train[skewed_features])","a270e030":"f, ax = plt.subplots(figsize=(8, 7))\n\nax.set_xscale(\"log\")\nax = sns.boxplot(data=train[skew_index] , orient=\"h\", palette=\"Set2\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\n\nsns.despine(trim=True, left=True)","a6ff882c":"skew_features = test._get_numeric_data().apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nskewed_features = skew_index.tolist()\n\ntest[skewed_features] = pipeline.transform(test[skewed_features])","be176310":"f, ax = plt.subplots(figsize=(8, 7))\n\nax.set_xscale(\"log\")\nax = sns.boxplot(data=test[skew_index] , orient=\"h\", palette=\"Set2\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\n\nsns.despine(trim=True, left=True)","4bc42e81":"# Returning the SalePrice target label to the train dataset\n\ntrain['SalePrice'] = train_SalePrice","d76c1013":"train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntrain['YrBltAndRemod'] = train['YearBuilt'] + train['YearRemodAdd']\ntrain['Total_sqr_footage'] = (train['BsmtFinSF1'] + train['BsmtFinSF2'] + train['1stFlrSF'] + train['2ndFlrSF'])\ntrain['Total_Bathrooms'] = (train['FullBath'] + (0.5 * train['HalfBath']) + train['BsmtFullBath'] + (0.5 * train['BsmtHalfBath']))\ntrain['Total_porch_sf'] = (train['OpenPorchSF'] + train['3SsnPorch'] + train['EnclosedPorch'] + train['ScreenPorch'] + train['WoodDeckSF'])\n\ntrain['haspool'] = train['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['has2ndfloor'] = train['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasgarage'] = train['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasbsmt'] = train['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain['hasfireplace'] = train['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']\ntest['YrBltAndRemod'] = test['YearBuilt'] + test['YearRemodAdd']\ntest['Total_sqr_footage'] = (test['BsmtFinSF1'] + test['BsmtFinSF2'] + test['1stFlrSF'] + test['2ndFlrSF'])\ntest['Total_Bathrooms'] = (test['FullBath'] + (0.5 * test['HalfBath']) + test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath']))\ntest['Total_porch_sf'] = (test['OpenPorchSF'] + test['3SsnPorch'] + test['EnclosedPorch'] + test['ScreenPorch'] + test['WoodDeckSF'])\n\ntest['haspool'] = test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['has2ndfloor'] = test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasgarage'] = test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasbsmt'] = test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest['hasfireplace'] = test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","5384eeab":"X = train.drop(['SalePrice'], axis=1)\ny = train['SalePrice'].values\n\n# To reproducible results we can set a random state seed:\nseed = 1\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = seed)","4fa74d96":"model = Lasso(alpha=0.02)\nmodel.fit(X_train,y_train)\n\nplt.figure(figsize=(10,10))\n\nmodel_ft_imp = pd.DataFrame(data=model.coef_,columns=['FeatureImp'], index = X_train.columns).sort_values(by='FeatureImp', ascending=False)\n\nmodel_ft_imp_nonzero = model_ft_imp[model_ft_imp['FeatureImp'] != 0]\n\nsns.barplot(x=model_ft_imp_nonzero['FeatureImp'], y=model_ft_imp_nonzero.index, palette=\"Reds\")\n\nplt.title('Lasso Feature importance', fontsize=20)\nplt.show()","f25f5932":"X_train = train.copy()\n\nX_train = X_train.drop(['SalePrice'], axis=1)\n\ny_train = train['SalePrice'].values","e6d8c483":"def estimator_params(X,y):\n\n    estimator_params = []\n    score = []    \n    Time = []\n\n    estimators = [GradientBoostingRegressor(),\n                   LGBMRegressor(),\n                   Ridge(),\n                   Lasso()]\n\n\n    params = [ {'max_depth':[5,10,15], \n                'min_samples_split':[10, 50, 100],\n                'learning_rate':[0.01,0.1,0.5], \n                'max_features':['sqrt'],\n                'random_state': [seed]},\n                            \n               {'num_leaves': [5,10,20], \n                'max_depth': [None, 5, 10, 20], \n                'learning_rate': [0.01,0.1,0.5], \n                'n_estimators': [10, 50, 100],\n                'random_state': [seed]},\n    \n                {'alpha': [5, 10, 20, 50,100],\n                'tol': [0.5,0.9],\n                'random_state': [seed]},\n             \n                {'alpha' : [0.1, 1],\n                 'max_iter': [1000, 2000],\n                 'random_state': [seed]}]\n    \n    # KFold\n    \n    kf = KFold(n_splits = 5, shuffle=True, random_state = seed)\n    \n    cv_params = {'cv': kf, 'scoring': 'neg_root_mean_squared_error', 'verbose': 0}\n\n\n    # GridSearchCV\n    \n    for estimator,param in zip(estimators, params):\n        start = time.time()\n        \n        grid_solver = GridSearchCV(estimator, param_grid = param, **cv_params).fit(X_train, y_train)\n\n        estimator_params.append(grid_solver.best_estimator_)\n        score.append(-(grid_solver.best_score_))\n        stop = time.time()\n        print('{} optimization finished'.format(str(estimator)))\n        print()\n        Time.append(stop-start)\n    \n    estimator_params_df = pd.DataFrame(columns = ['Estimator_params','Score_RMSE','Time'])\n    estimator_params_df['Estimator_params']= estimator_params\n    estimator_params_df['Score_RMSE'] = score\n    estimator_params_df['Time']= Time\n    \n    return estimator_params_df","47b9dbe0":"estimator_params(X_train.values, y_train)","c5055b68":"X_train_lasso = X_train[model_ft_imp_nonzero.index.tolist()[0:10]].values\n\nestimator_params(X_train_lasso, y_train)","837863e1":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","40369c3e":"# Final X_train will have only 10 features, as well as X_test.\nX_train = X_train_lasso\n\nX_test = test[model_ft_imp_nonzero.index.tolist()[0:10]].values","7bab7c42":"gbr = GradientBoostingRegressor(max_depth=15, max_features='sqrt',\n                          min_samples_split=50, random_state=seed)\n\nlgbmr = LGBMRegressor(max_depth=None, n_estimators=50, num_leaves=20, random_state=seed)\n\nridge = Ridge(alpha=5, random_state=seed, tol=0.5)\n\nlasso = Lasso(alpha=0.1, random_state=seed)\n\nstacking_regressor = StackingCVRegressor(regressors = (gbr, lgbmr, ridge, lasso),\n                                         meta_regressor = gbr,\n                                         random_state = seed,\n                                         use_features_in_secondary = True)\n\ndef stacked_predictions(X, y): \n    \n    gbr_model = gbr.fit(X, y)\n    lgbmr_model = lgbmr.fit(X, y)\n    ridge_model = ridge.fit(X, y)\n    lasso_model = lasso.fit(X, y)\n    stack_gen_model = stacking_regressor.fit(np.array(X), np.array(y))\n    \n    return ((0.2 * gbr_model.predict(X)) + \\\n            (0.2 * lgbmr_model.predict(X)) + \\\n            (0.2 * ridge_model.predict(X)) + \\\n            (0.05 * lasso_model.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","40b30666":"stacked_score = rmsle(y_train, stacked_predictions(X_train, y_train))\nprint('RMSLE score train data using a stacked model: {}'.format(str(stacked_score)))\n    ","758bf1e3":"model_names = ['GBR', 'LGBMR', 'Ridge', 'Lasso']\nRMSLE = []\n\nfor model, model_name in zip((gbr, lgbmr, ridge, lasso), model_names):\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_train)\n    rmsle_score = rmsle(y_train, y_pred)\n    RMSLE.append(rmsle_score)\n    \nrmsle_df = pd.DataFrame(columns = ['Model','Score_RMSLE'])\nmodel_names.append('Stacked model')\nRMSLE.append(stacked_score)\n\nrmsle_df['Model'] = model_names\nrmsle_df['Score_RMSLE'] = RMSLE\nrmsle_df.sort_values(by = 'Score_RMSLE', ascending = True, inplace = True)","5cbea2a4":"rmsle_df","557c0a55":"gbr.fit(X_train,y_train)\n\npredictions = gbr.predict(X_test)\npredictions = np.exp(predictions)\n\nsubmission_gbr = pd.DataFrame({\"Id\": test_id, \"SalePrice\": predictions})","f55a51d6":"submission_gbr.head(3)","99e8b6e3":"def stacked_predictions(X, X_test, y): \n    \n    gbr_model = gbr.fit(X, y)\n    lgbmr_model = lgbmr.fit(X, y)\n    ridge_model = ridge.fit(X, y)\n    lasso_model = lasso.fit(X, y)\n    stack_gen_model = stacking_regressor.fit(np.array(X), np.array(y))\n    \n    return ((0.2 * gbr_model.predict(X_test)) + \\\n            (0.2 * lgbmr_model.predict(X_test)) + \\\n            (0.2 * ridge_model.predict(X_test)) + \\\n            (0.1 * lasso_model.predict(X_test)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X_test))))\n\nstacked_predictions(X_train,X_test, y_train)\n\nsubmission_stacked = pd.DataFrame({\"Id\": test_id, \"SalePrice\": predictions})","b68d37e8":"submission_gbr.to_csv('submission_gbr.csv', index = False)\nsubmission_stacked.to_csv('submission_stacked.csv', index = False)","90d8020f":"Lets search for other highly skewed features:","d5410523":"### Train dataset - Preliminar information","a2721131":"I want to know if there is some usual missing data notations apart from NaN:","c53bb94c":"I will apply the box-cox transformation to normalize the previous highly skewed features: it is a power transform which doesn't allow 0 and\/or negative values. \n\nOne way to solve this problem is to use a MixMaxScaler transform first to scale the data to positive values, then apply the transform. It will also avoid division by zero if the feature range does not include it.\n\nThe optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.","f89b9be2":"Now I can deal with the data analysis of the train dataset to get some insight of the different features.","9eebf0a3":"## Feature importance","40e14056":"To see strongth correlations more clearly, we can get a list of the top most correlated features:","587824ff":"There is some normal distributed features but a higher predominance of skewed distributed ones. Also the categorical variables are evidenced.","bc94f78b":"Just using logic: grouping related columns in a new one. Also \"one-hot-encoding\" categorical features which values can be simplified as \"Yes-No\"--> 1-0.","37291271":"The sales price (our target label) shows some deviation from the normal distribution, with positive skewness.","35f47737":"Viewing the results, they perform identically with little time execution differences. \n\n- This is maybe because only the top features made a significant difference in the regression, being the rest of the features irrelevant. For simplication purposes, I will only take the top 10 features selected by the lasso method.\n\n- I didn't use XGBoostRegressor or SVR because they gave me a lot of trouble: I saw XGB have many common errors through out the community when used with GridSearchCV and SVR failed me always in convergence, stopping my custom function. \n\nThe validation for the train model was based on the RMSE metric\/ loss function (Root Mean Squared Error), which is a metric requirement for this Kaggle submission.","7d21be13":"At this time we have to deal with the categorical values, because they need to be transformed to numerical data. \n\nThere are different methods like get_dummies or OneHotEncoding but due the high number of features we have I don't want to create more, so LabelEncoder is a good choice: each different value will have a unique numerical label in each feature.","ddb9dfaf":"The objetive of this kernel is to train a model which make predictions such that our loss function (RMSE) is minimum.\n\nThe list of regression algorithms I will test are:\n\n- **Gradient Boosting Regressor:** Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. GB Regressir builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n\n- **LightGBM Regressor:** Light GBM is a gradient boosting framework that uses tree based learning algorithm. It differs from other tree based algorithms in growing vertically while other algorithms grow trees horizontally, meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\n- **Ridge Regressor:** Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. \n\n- **Lasso Regressor:** as described earlier it uses L1 regularization technique.","9c56f907":"Having the best hyperparameters for each estimator, I will create a stacked model to combine the different predictions of each estimator for more robustness against overfitting and to see if this model could perform better.","aa52f6e7":"A exponential transformation is used in the prediction data to reverse the previous logaritmic transformation made for better regression and evaluation purposes  and finally we have true sale prices to submit. Also the Id feature was added to finish our data submission.","d4a6db8e":"\n- First I will use all the features. \n\n- Then, I will use only the top 10 features obtained by the Lasso feature importance.\n\n- Finally, I will choose between the two previous options based on the models performance.","76a3a13c":"### Loading the train and test dataset","9339fdcf":"There are some coincidences in the top features with the prior correlation analysis that was made.","5d4e0854":"With skewness above 1, the data are highly skewed: the tail region may act as an outlier for the statistical model.\n\nKurtosis is the characteristic of being flat or peaked. A large kurtosis value often means that the tails of the distribution are getting more extreme values than the tail of the normal distribution. In this case we have a length of 6.5 aprox. standard deviations from the mean.\n\n","a6c6ef86":"- Finally I want to compare this stacked model with the other ones:","9d247d6f":"## Feature engineering","3ca4b057":"The lighter colored squares indicate the most correlated paired features.\n\nLet's visualize how our target label, SalesPrice, is related to the features we see more correlated in the previous correlation matrix:\n","b23e43c1":"### Label encoding categorical features","11538503":"## Data cleaning","0b853aac":"Now is a good time to search for the most important features which will help our model to perform a better regression.\n\nI will use the Lasso regression: using Linear Regression with L1 regularization is called Lasso Regularization: \n\n- The Lasso method puts a constraint on the sum of the absolute values of the model parameters, the sum has to be less than a fixed value (upper bound). In order to do so the method apply a shrinking (regularization) process where it penalizes the coefficients of the regression variables shrinking some of them to zero i.e. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. ","fc7d2077":"There is a little tendency in the increasing sale price through time.","af421b9a":"### Handling Outliers\n\nAn outlier may be defined as a piece of data or observation that deviates drastically from the given norm or average of the data set.\n\nBased in the previous linear representations of the variables, I will cast out manually the outliers in some features. There are better methods and techniques than manually-handling the outliers but for simplification purposes I will just filter the features:","1e53c460":"## Model","de32bce7":"### Imputation\n\n Imputation is the process of replacing missing data with substituted values.\n \n Because we have the features description in a separate file, drop missing values could be avoided: its is better to fill the missing values with significant ones.\n\n- For the test dataset we can not drop rows because we need each row (each ID) and its predicted sale price (target label). So I will replace the missing data with 0 to simplify things.\n","f55a15c9":"The submission must be in the form of a .csv file.","3f23c1e5":"### Correlations","f4250b45":"## Data analysis","7f71bbb3":"We know now that there is categorical data (dtype('O')) and numerical data (int and float). Also we can see some NaN\/missing values we will have to deal with.","bcd8cf4f":"I have also to transform the same features in the test dataset with the train dataset pipeline fit.\n\nWe only use transform() on the test dataset because we use the scaling paramaters learned on the train dataset to scale the test dataset. If not, new values would be created in the test dataset with no relation to the training dataset.","2f9fcedd":"We saw before we have I high skeweness and kurtosis in the classification label.\n\nThe data can be transfromed to change its distribution: log transformations is a great option when dealing positive skewness. \n\nAlso it is required by the Kaggle competition to evaluate our model on the RMSE between the logarithm of the predicted value and the logarithm of the observed sales price, so we have acomplish also this requirement.","86e0d274":"### Creating new features","e14a4ba3":"# Comprehensive guide through Regression Predictive Modeling\n\n*Last edition: 28-09-2020: fixed test dataset transformations (LabelEncoder,MinMaxScaler, PowerTransform): only fit the training dataset, no the test dataset*\n              \n*21-09-2020: text errors fixed*\n\nThis kernel is about how to straight apply a Machine Learning methodology step by step to resolve a predictive regression problem.The dataset used is the Ames Housing, an expanded version of the commonly known Boston Housing dataset, compiled by Dean De Cock.\n\n\n## Goal\n\n- Predict SalePrice, a continous variable.\n- Evaluated our models on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. This is the used loss function. \n\n## Data\n\n- A train and a test dataset are provided with categorical and numerical features. Also a data_description.txt with the columns\/features descriptions and a sample_submission.csv.\n- Each row in the train and test dataset represents a House characteristics.\n\n## The Model:\n\n- **Regression predictive modeling:** Because the label\/goal feature is a continous one, a regression model is needed.\n\n- **Statistical Resampling**: k-fold Cross-Validation is a resampling procedure used to evaluate machine learning models on a limited data sample.The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.\n\n- **Stacked model for improved predictions**: Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor.Stacking uses a similar idea to k-folds cross validation to create out-of-sample predictions.\n\n\n","a4e544eb":"### Drop unnecessary columns\n\nI will remove the \"Id\" column because they are not useful in the classification but it is need for the kaggle submission so I will keep them.","e37d81d3":"## Libraries used","5532e7cf":"Deal with the missing data is neccesary to avoid problems and to improve it the regression.","7d067442":"We can conclude that the Gradient Boosting model performs best. However, sometimes it doesn't mean it will get the best score in the kaggle competition evaluation because Kaggle computes its score internally with your submission and it could vary, because it maybe  could be tested with another test dataset different than the original one: it is better to keep the best models and submit their predictions (if possible) for scoring and then see which one performed best.\n\n- The difference between the previous KFold cross validation score and this one is because KFold cross validation make an average of all the metrics obtained in each iteration (k=5 in this case --> 5 different splits): in each iteration the train dataset is splittled in k-1 groups which are trained and tested against the remaining one, which acts as a test dataset).","2ea04b5b":"There is some clear linear relationships which will facilitate the final regression.","abd02283":"Only 4 pairs of features have a correlation above 0.7 and there are other ones little less correlated. This will be taken into account for possible reduction of the selected features when making the regression model.\n","6b776f1e":"A quicker look at this comparisons can be made using a pairplot:","579b2352":"The OverallQual (Rates the overall material and finish quality of the house) tend to be above average with some outliers in case of the top materials, which make sense.","9730b7b8":"### Missing data","fd644142":"### Highly skewed features","9490e07d":"### Test dataset - Preliminar information","4d2de21b":"First I want to tune some algorithms's hyperparameters one by one to obtain the best regression possible, followed by an evaluation of the model with K-Fold cross validation method. \n\nFor this I will use a custom function which use GridSearchCV: a function which helps to loop through predefined hyperparameters and fit the resulting estimator (model) to the cross validation, allowing to compare the models with its best hyperparameters. I will apply it in a loop for each estimator.","179def90":"#### Correlation matrix\n\n- We can compare each feature against each other one to search for the best correlated features. The corr() Pandas function uses as default the Pearson standard correlation coefficient.","e14453c1":"### Data distribution","5468dcc4":"The first I want to know is the shape of the datsets, their different data types to check if overall they seem correct and a brief look to the data to get some general insight.","dd113c37":"## Submission","a357adf5":"There are not more missing data notations. The imputation of the missing data will be deal in the feature engineering part. "}}