{"cell_type":{"b108ed24":"code","36354fcf":"code","113bd441":"code","1b22c278":"code","20bb8a11":"code","b49568c8":"code","c409ef22":"code","d13e0887":"code","553e0971":"code","e569d65d":"code","104e389c":"code","a41aa0f0":"code","38c4e828":"code","ee00d0e8":"code","317e99a1":"code","77951616":"code","e887b11d":"markdown","0891aa24":"markdown","18457aaa":"markdown","d55256fa":"markdown","bfcc7c51":"markdown","e2075065":"markdown","7c328439":"markdown","526819b9":"markdown","21a81003":"markdown","dfa9d4c5":"markdown","e956345d":"markdown","27c59dc8":"markdown","541c108d":"markdown","032a915c":"markdown","38aeb025":"markdown","e2a7a8f3":"markdown","e164c522":"markdown","b842f5f9":"markdown"},"source":{"b108ed24":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1","36354fcf":"import os\nimport gc\nimport cv2\nimport sys\nimport time\nimport copy\n\nimport numpy as np\nimport pandas as pd\nfrom colorama import Fore\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.hub import load\nfrom torch.optim import Adam\nfrom torch import DoubleTensor, FloatTensor, LongTensor\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom torchvision.transforms import Normalize\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom albumentations import VerticalFlip, HorizontalFlip, Compose","113bd441":"H = 512\nW = 512\nVF = 0.5\nHF = 0.5\nDELAY = 30\nFRAC = 0.1\nDROP = 0.225\n\nFOLDS = 8\nEPOCHS = 3\nLR = 1e-3, 1e-3\nBATCH_SIZE = 32\nVAL_BATCH_SIZE = 32\nMODEL_NAME = 'efficientnet_b3'\nMODEL = 'rwightman\/gen-efficientnet-pytorch'","1b22c278":"PATH = '..\/input\/'\nMODEL_PATH = 'efficientnet_model'\nDATA_PATH = PATH + 'alaska2-image-steganalysis\/'\nSAMPLE_SUB_PATH = DATA_PATH + 'sample_submission.csv'\n\nTEST_PATH = DATA_PATH + 'Test\/'\nUERD_PATH = DATA_PATH + 'UERD\/'\nCOVER_PATH = DATA_PATH + 'Cover\/'\nJMiPOD_PATH = DATA_PATH + 'JMiPOD\/'\nJUNIWARD_PATH = DATA_PATH + 'JUNIWARD\/'\nTRAIN_PATHS = [COVER_PATH, JMiPOD_PATH, JUNIWARD_PATH, UERD_PATH]","20bb8a11":"sample_submission = pd.read_csv(SAMPLE_SUB_PATH)","b49568c8":"sample_submission.head()","c409ef22":"def display_images(num):\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(TEST_PATH)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(20, 20))\n\n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            img = cv2.imread(TEST_PATH + image_ids[idx])\n            ax[i, j].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n            ax[i, j].set_title('Image {}'.format(idx), fontsize=12)\n\n    plt.show()","d13e0887":"display_images(36)","553e0971":"def get_img(path, aug):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\/255\n    return aug(image=cv2.resize(img, (H, W)))['image']\n\nclass ALASKADataset(Dataset):\n    def __init__(self, image_id, is_test, is_val):\n        self.is_test = is_test\n        self.image_id = image_id\n        self.no_aug = is_test or is_val\n\n        self.vertical = VerticalFlip(p=VF)\n        self.horizontal = HorizontalFlip(p=HF)\n        if self.no_aug: self.transform = lambda image: {'image': image}\n        else: self.transform = Compose([self.vertical, self.horizontal], p=1)\n\n    def __len__(self):\n        multiplier = 1 if self.is_test else 4\n        return multiplier*len(self.image_id)\n    \n    def __getitem__(self, idx):\n        index = idx%len(self.image_id)\n\n        if self.is_test:\n            category = None\n            path = TEST_PATH + self.image_id[index]\n            return FloatTensor(get_img(path, self.transform))\n        else:\n            target = idx\/len(self.image_id)\n            category = [int(np.floor(target) > 0)]\n            path = TRAIN_PATHS[int(target)] + self.image_id[index]\n            return FloatTensor(get_img(path, self.transform)), FloatTensor(category)","e569d65d":"class ENSModel(nn.Module):\n    def __init__(self):\n        super(ENSModel, self).__init__()\n        self.dropout = nn.Dropout(p=DROP)\n        self.dense_output = nn.Linear(1536, 1)\n        self.efn = load(MODEL, MODEL_NAME, pretrained=True)\n        self.efn = nn.Sequential(*list(self.efn.children())[:-1])\n        \n    def forward(self, x):\n        x = x.reshape(-1, 3, H, W)\n        return self.dense_output(self.dropout(self.efn(x).reshape(-1, 1536)))","104e389c":"kfolds = KFold(n_splits=FOLDS)\nimage_id = os.listdir(COVER_PATH)\nsplit_indices = kfolds.split(image_id)\n\nval_ids, train_ids = [], []\nfor index in split_indices:\n    val_ids.append(np.array(image_id)[index[1]])\n    train_ids.append(np.array(image_id)[index[0]])","a41aa0f0":"def bce(inp, targ):\n    return nn.BCELoss()(nn.Sigmoid()(inp), targ)\n\ndef acc(inp, targ):\n    targ_idx = targ.squeeze()\n    inp_idx = torch.round(nn.Sigmoid()(inp)).squeeze()\n    return (inp_idx == targ_idx).float().sum(axis=0)\/len(inp_idx)","38c4e828":"def print_metric(data, fold, start, end, metric, typ):\n    n, value = \"Steganalysis\", np.round(data.item(), 3)\n    g, c, y, r = Fore.GREEN, Fore.CYAN, Fore.YELLOW, Fore.RESET\n    \n    tick = g + '\\u2714' + r\n    t = typ, n, metric, c, value, r\n    time = np.round(end - start, 1)\n    time = \"Time: {}{}{} s\".format(y, time, r)\n    string = \"FOLD {} \".format(fold + 1) + tick + \"  \"\n    print(string + \"{} {} {}: {}{}{}\".format(*t) + \"  \" + time)","ee00d0e8":"class ImbSamp(sampler.Sampler):\n\n    def __len__(self): return self.num_samples\n    def __iter__(self): return (self.indices[i] for i in self._get_probs())\n    def _get_label(self, dataset, idx): return int(idx\/(len(self.dataset)\/4) >= 1)\n    \n    def _get_weight(self, idx, count_dict):\n        return 1.0\/count_dict[self._get_label(self.dataset, idx)]\n    \n    def _get_probs(self):\n        return torch.multinomial(self.weights, self.num_samples, replacement=True)\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n        self.indices = list(range(len(dataset))) if indices is None else indices\n        self.num_samples = len(self.indices) if num_samples is None else num_samples\n\n        count = {}\n        self.dataset = dataset\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in count: count[label] += 1\n            if label not in count: count[label] = 1\n\n        self.weights = DoubleTensor([self._get_weight(idx, count) for idx in self.indices])","317e99a1":"model = ENSModel()\n\ndef run(fold):\n    val = val_ids[fold]\n    train = train_ids[fold]\n    device = xm.xla_device(fold + 1)\n\n    val_set = ALASKADataset(val, False, True)\n    train_set = ALASKADataset(train, False, False)\n    val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE)\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=ImbSamp(train_set))\n\n    network = copy.deepcopy(model).to(device)\n    optimizer = Adam([{'params': network.efn.parameters(), 'lr': LR[0]},\n                      {'params': network.dense_output.parameters(), 'lr': LR[1]}])\n\n    start = time.time()\n    for epoch in range(EPOCHS):\n\n        batch = 1\n        for train_batch in train_loader:\n            train_img, train_targs = train_batch\n            \n            network = network.to(device)\n            train_img = train_img.to(device)\n            train_targs = train_targs.to(device)\n            \n            network.train()\n            train_preds = network.forward(train_img)\n            train_loss = bce(train_preds, train_targs)\n\n            optimizer.zero_grad()\n            train_loss.backward()\n            xm.optimizer_step(optimizer, barrier=True)\n\n            batch = batch + 1\n            if batch >= FRAC*len(train_loader): break\n\n    network.eval()\n    val_loss, val_acc = 0, 0\n    for val_batch in tqdm(val_loader):\n\n        img, targ = val_batch\n        with torch.no_grad():\n            img = img.to(device)\n            targ = targ.to(device)\n            network = network.to(device)\n            pred = network.forward(val_img)\n                \n            pred = network.forward(img)\n            val_acc += acc(pred, targ.squeeze(dim=1)).item()*len(pred)\n            val_loss += bce(pred, targ.squeeze(dim=1)).item()*len(pred)\n\n    end = time.time()\n    time.sleep(DELAY*fold)\n    network = network.cpu()\n    model_path = MODEL_PATH + \"_{}.pt\"\n    \n    val_acc \/= len(val_set)\n    val_loss \/= len(val_set)\n    print_metric(val_acc, fold, start, end, metric=\"Accuracy\", typ=\"Val\")\n    torch.save(network.state_dict(), model_path.format(fold + 1)); del network; gc.collect()","77951616":"Parallel(n_jobs=FOLDS, backend=\"threading\")(delayed(run)(i) for i in range(FOLDS))","e887b11d":"## Define helper function for training logs <a id=\"2.5\"><\/a> <font color=\"#1373f0\" size=4>(to check training status)<\/font>","0891aa24":"## Set hyperparamerters and paths <a id=\"1.3\"><\/a> <font color=\"#1373f0\" size=4>(adjust these to improve CV and LB :D)<\/font>","18457aaa":"# Introduction\n\nHello everyone! Welcome to the <font color=\"#1373f0\">\"ALASKA2 Image Steganalysis\"<\/font> competition on Kaggle! In this competition, contestants are challenged to build machine learning models to predict whether a message has been <font color=\"#1373f0\">hidden in an image using steganography<\/font>. An accurate solution to this problem can open up new possibilities in the area of steganalysis and deep learning-based encryption.\n\nIn this kernel, I will demonstrate how one can <font color=\"#1373f0\">finetune EfficientNet-B3<\/font> to solve this task using PyTorch. I will use PyTorch v1.5 and Kaggle's <font color=\"#1373f0\">TPU v3-8<\/font> to train the model on 8 folds and 1 epoch in less than three hours.","d55256fa":"## Load .csv data <a id=\"1.4\"><\/a> <font color=\"#1373f0\" size=4>(to access image IDs for training and validation)<\/font>","bfcc7c51":"# Modeling <a id=\"2\"><\/a>","e2075065":"## Define binary cross entropy and accuracy <a id=\"2.4\"><\/a> <font color=\"#1373f0\" size=4>(for backpropagation)<\/font>","7c328439":"<center><img src=\"https:\/\/i.imgur.com\/F4T8Ys2.jpg\" width=\"600px\"><\/center>","526819b9":"## Train model on all 8 TPU cores in parallel <a id=\"2.6\"><\/a> <font color=\"#1373f0\" size=4>(one fold per core)<\/font>","21a81003":"## Split 300,000 images into 8 folds <a id=\"2.3\"><\/a> <font color=\"#1373f0\" size=4>(for cross-validation)<\/font>","dfa9d4c5":"# Acknowledgements\n\n1. [<font color=\"#1373f0\">PyTorch XLA<\/font><font color=\"#5e5d5d\"> ~ <u>by PyTorch<\/u><\/font>](https:\/\/pytorch.org\/xla\/release\/1.5\/index.html)\n2. [<font color=\"#1373f0\">Torchvision Models<\/font><font color=\"#5e5d5d\"> ~ <u>by PyTorch<\/u><\/font>](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html)\n3. [<font color=\"#1373f0\">PANDA \/ submit test<\/font><font color=\"#5e5d5d\"> ~ <u>Yasufumi Nakama<\/u><\/font>](https:\/\/www.kaggle.com\/yasufuminakama\/panda-submit-test)\n4. [<font color=\"#1373f0\">Super-duper fast pytorch tpu kernel...<\/font><font color=\"#5e5d5d\"> ~ <u>by Abhishek<\/u>](https:\/\/www.kaggle.com\/abhishek\/super-duper-fast-pytorch-tpu-kernel)","e956345d":"# Takeaways <a id=\"3\"><\/a>\n\n1. Using all 8 TPU cores in parallel can dramatically speed up KFold training.\n2. Using complex models (like ResNet-152, DenseNet-201, Efficient-B3, etc) can improve the model's performance.\n3. Training and inference should be done in separate notebooks to avoid confusion and make it easier to iterate fast.","27c59dc8":"## Display few images <a id=\"1.5\"><\/a> <font color=\"#1373f0\" size=4>(from <i>Test<\/i> directory)<\/font>","541c108d":"## Set up PyTorch-XLA <a id=\"1.1\"><\/a> <font color=\"#1373f0\" size=4>(inspired by Abhishek's kernel :D)<\/font>","032a915c":"## Import libraries <a id=\"1.2\"><\/a> <font color=\"#1373f0\" size=4>(for data loading, processing, and modeling on TPU)<\/font>","38aeb025":"## Build EfficientNet-B3 model <a id=\"2.2\"><\/a> <font color=\"#1373f0\" size=4>(with a custom Dense head)<\/font>","e2a7a8f3":"# Preparing the ground <a id=\"1\"><\/a>","e164c522":"## Build PyTorch dataset <a id=\"2.1\"><\/a> <font color=\"#1373f0\" size=4>(with image transforms and targets)<\/font>","b842f5f9":"# Contents\n\n* [<font size=4 color=\"#1373f0\">Preparing the ground<\/font>](#1)\n    * [<font color=\"#5e5d5d\"><u>Set up PyTorch-XLA<\/u><\/font>](#1.1)\n    * [<font color=\"#5e5d5d\"><u>Import libraries<\/u><\/font>](#1.2)\n    * [<font color=\"#5e5d5d\"><u>Set hyperparameters and paths<\/u><\/font>](#1.3)\n    * [<font color=\"#5e5d5d\"><u>Load .csv data<\/u><\/font>](#1.4)\n    * [<font color=\"#5e5d5d\"><u>Display few images<\/u><\/font>](#1.5)\n\n    \n* [<font size=4 color=\"#1373f0\">Modeling<\/font>](#2)\n    * [<font color=\"#5e5d5d\"><u>Build PyTorch dataset<\/u><\/font>](#2.1)\n    * [<font color=\"#5e5d5d\"><u>Build EfficientNet-B3 model<\/u><\/font>](#2.2)\n    * [<font color=\"#5e5d5d\"><u>Split 300, 000 images into 8 folds<\/u><\/font>](#2.3)\n    * [<font color=\"#5e5d5d\"><u>Define cross entropy and accuracy<\/u><\/font>](#2.4)\n    * [<font color=\"#5e5d5d\"><u>Define helper function for training logs<\/u><\/font>](#2.5)\n    * [<font color=\"#5e5d5d\"><u>Train model on all 8 TPU cores in parallel<\/u><\/font>](#2.6)\n\n\n* [<font size=4 color=\"#1373f0\">Takeaways<\/font>](#3)"}}