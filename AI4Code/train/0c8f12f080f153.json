{"cell_type":{"b1df4ffb":"code","eae27bb7":"code","7200649f":"code","62938ce9":"code","e6a83824":"code","5c6c1827":"code","9ada30ef":"code","20267cf1":"code","40f83a8c":"code","9729cf29":"code","143afc44":"code","5f0b99df":"code","346c3989":"code","d8aa2f71":"code","610a3240":"code","41ce07ff":"code","209a784d":"code","ca59b984":"code","d6e20304":"code","044986c0":"code","46dfe1f4":"code","e20168b3":"code","9219c48a":"code","a0a3ba80":"code","260db198":"code","26799e01":"code","bb7a3943":"code","88839cc8":"code","5e5e51bf":"code","13593de3":"code","bd8ccca8":"code","3a925c9e":"code","d66ee1ff":"code","48891e01":"code","76f36523":"code","e1069bbb":"code","dd05f705":"code","a562c0de":"code","175f8b3d":"code","306b4be2":"code","ee31b8d4":"code","cb768aa4":"code","cd8efebe":"code","453fe167":"code","959a2b99":"code","421b630d":"code","dbc181bc":"code","cefe3ac7":"code","8d49f970":"code","ceec1614":"code","bb089535":"code","69e76c79":"code","d66db986":"code","91c5f82c":"code","b7f8800c":"code","914e2545":"code","39ff2405":"code","8b1d9c71":"code","828c5c84":"code","154ecfb4":"code","8b854e3b":"code","5bba3656":"code","2f6807a5":"code","ee6d5198":"code","39d7336b":"code","1ae08807":"markdown","d8d8364b":"markdown","373c8895":"markdown","d018e575":"markdown","a89a429b":"markdown","6bdea364":"markdown","ff160ab0":"markdown","19ff9547":"markdown","5d70dd2c":"markdown","bf264081":"markdown","b4290ab6":"markdown"},"source":{"b1df4ffb":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport sklearn as skl\nimport random\nimport time\nfrom IPython import display \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eae27bb7":"# our data\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# create copies\n\ndf1 = df_train.copy(deep = True)\ndf2 = df_test.copy(deep =True)\n\n# create reference to clean when needed\n\ndf_clean = [df1,df2]\n\n# preview data\n\ndf1.head()","7200649f":"print('Missing values in the training dataset:')\nprint (df1.isnull().sum())\nprint()\nprint ('Missing values in the Test dataset:')\nprint(df2.isnull().sum())","62938ce9":"# remove unnecessary columns\ndropped_cols = ['PassengerId','Ticket','Cabin']\ndf1.drop(dropped_cols, axis=1,inplace=True)\ndf2.drop(dropped_cols, axis=1,inplace=True)","e6a83824":"# fill missing values in both datasets\n\nfor df in df_clean:\n    # numbers are filled with the median\n    df['Age'].fillna(df['Age'].median(),inplace=True)\n    df['Fare'].fillna(df['Fare'].median(),inplace=True)\n    # filled by mode since it has categorical choices (S class)\n    df['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)","5c6c1827":"# recheck values again\nprint('Missing values in the training dataset:')\nprint (df1.isnull().sum() == 0)\nprint()\nprint ('Missing values in the Test dataset:')\nprint(df2.isnull().sum()== 0)","9ada30ef":"df1.head( )","20267cf1":"#sibsp and parch can be used to compute family size:\n\nfor df in df_clean:\n    df['Family'] = df['SibSp'] + df['Parch'] + 1\n    df['Alone'] = 1\n    df['Alone'].loc[df['Family'] > 1 ] = 0 # if family is greater than 1 then they are not alone\n","40f83a8c":"# extract titles from names:\n# certain titles are replaced because they are similar\nreplace_titles = {\"Mlle\": \"Miss\",\"Ms\": \"Miss\",'Mme':'Miss' }\nfor df in df_clean:\n    df['Title'] = ''\n    df['Title'] = df['Name'].str.split(', ',expand=True)[1].str.split('.',expand=True)\n    df.replace({'Title':replace_titles},inplace=True)\n    rare_titles = df['Title'].value_counts() <10  # titles that are not statistically significant will be grouped together\n    df['Title'] =df['Title'].apply(lambda x: 'Other' if rare_titles.loc[x] == True else x)\n\n    df['FareBin'] = pd.qcut (df['Fare'],4) # cut according to equal frequencies \n    df['AgeBin'] = pd.cut(df['Age'].astype('int'),5) # cut into equal intervals\n","9729cf29":"print(df1['Title'].value_counts())\n\nprint('_'*15)\nprint(df1.info())\nprint('_'*15)\nprint(df2.info())\nprint('_'*15)\ndf1.head()","143afc44":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nlabel = LabelEncoder()\n\nlabel_cols = ['Sex','Embarked','Title','AgeBin','FareBin']\n\nfor df in df_clean:\n    for col in label_cols:\n        df[col +'_Coded'] = label.fit_transform(df[col])","5f0b99df":"y = ['Survived']\nx_pretty = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'Family', 'Alone'] # main features used when printing \nx_coded = ['Sex_Coded','Pclass', 'Embarked_Coded', 'Title_Coded','SibSp', 'Parch', 'Age', 'Fare'] # used for calculations\nx_bin = ['Sex_Coded','Pclass', 'Embarked_Coded', 'Title_Coded', 'Family', 'AgeBin_Coded', 'FareBin_Coded'] # continuous variables and bins\nxy = y + x_pretty \nxy_bin = y+x_bin\n\ndf_x_dummy = pd.get_dummies(df1[x_pretty])\nx_dummy = df_x_dummy.columns.tolist()\nxy_dummy = y+ x_dummy \n\n\nprint('Original Columns:',xy)\n\nprint('Bins: ',xy_bin)\nprint('Dummy: ',xy_dummy)\n\ndf_x_dummy.head()","346c3989":"from sklearn import model_selection\n\n# we are using columns in 3 different formats\n\ntrain_x, test_x, train_y, test_y = model_selection.train_test_split(df1[x_coded],df1[y])\ntrain_x_bin, test_x_bin, train_y_bin, test_y_bin = model_selection.train_test_split(df1[x_bin],df1[y])","d8aa2f71":"# correlations\n\nfor x in x_pretty:\n    if df1[x].dtype != 'float64': \n        print('Survival Correlation by:', x)\n        print(df1[[x, y[0]]].groupby(x,as_index=False).mean())\n        print('_'*15)","610a3240":"plt.figure(figsize=(15,6))\n\nplt.subplot(131)\n\nplt.boxplot(x=df1['Fare'],meanline=True,showmeans=True)\nplt.title('Fare Boxplot')\n\nplt.subplot(132)\nplt.boxplot(x=df1['Age'],meanline=True,showmeans=True)\nplt.title('Age Boxplot')\n\nplt.subplot(133)\nplt.boxplot(x=df1['Family'],meanline=True,showmeans=True)\nplt.title('Family Boxplot');","41ce07ff":"# survival rate \nfrom operator import attrgetter\nplt.figure(figsize=(15,6))\n\nplt.subplot(131)\nsurvival_rate = df1.groupby('FareBin',as_index=False).mean()['Survived']\nxtick_labels = ['very low','low','medium','high']\nplt.bar(range(4),survival_rate)\nplt.xticks(range(4),xtick_labels)\nplt.xlabel('Fare')\nplt.title('Fare Survival Rare (survived\/not survived)')\n\nplt.subplot(132)\ngroup1 = df1.groupby('AgeBin',as_index=False).mean()\nsurvival_rate = group1['Survived']\nxtick_labels = np.insert(group1['AgeBin'].map(attrgetter('right')).tolist(),0,0)\nplt.bar(x=range(6),height=np.insert(survival_rate.values,0,0),align='edge',width=-1,tick_label=xtick_labels)\nplt.title('Survival Rate By Age Group')\nplt.xlim(0,5)\n\nplt.subplot(133)\nsurvival_rate = df1.groupby('Family',as_index=False).mean()[['Family','Survived']]\nplt.bar(survival_rate['Family'],survival_rate['Survived'])\nplt.xticks(range(1,12))\nplt.title('Survival Rate By Family Size (1==alone)');","209a784d":"fig, saxis = plt.subplots(1,3,figsize=(15, 6))\n\nsns.barplot(data=df1,x='Embarked',y='Survived',ax=saxis[0],hue='Sex')\nsns.barplot(data=df1,x='Pclass',y='Survived',ax=saxis[1],hue='Sex')\nsns.barplot(data=df1,x='Alone',y='Survived',ax=saxis[2],hue='Sex');","ca59b984":"# Pclass vs other variables\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,6))\n\nsns.boxplot(x=df1['Pclass'],y=df1['Fare'],hue=df1['Survived'],ax=ax1)\nax1.set_title('Pclass vs Fare Survival')\n\nsns.boxplot(data=df1,x='Pclass',y='Age',hue='Survived',ax=ax2)\nax2.set_title('Pclass vs Age Survival')\n\nsns.violinplot(data=df1,x='Pclass',y='Family',hue='Survived',ax=ax3,split=True)\nax3.set_title('Pclass vs Family Size Survival');","d6e20304":"plt.figure(figsize=(15,6))\nplt.subplot(121)\n\nsns.pointplot(data=df1,x='Family',y='Survived',hue='Sex')\nplt.title('Family Size vs Gender Survival')\n\nplt.subplot(122)\nsns.pointplot(data=df1,x='Pclass',y='Survived',hue='Sex')\nplt.title('Pclass vs Gender Survival');","044986c0":"g = sns.FacetGrid(data=df1,col='Embarked',height=4)\ng.map(sns.pointplot,'Pclass','Survived','Sex',palette='deep')\ng.add_legend();","46dfe1f4":"#heatmap\nplt.figure(figsize=(20,8))\nsns.heatmap(df1.corr(),annot=True,cmap=sns.diverging_palette(10, 220, sep=80, n=50))\nplt.title('Pearson Correlation Heatmap of Features');","e20168b3":"# models with default parameters\nfrom sklearn import ensemble, svm ,tree, naive_bayes,neighbors,linear_model, discriminant_analysis, gaussian_process\nfrom sklearn import feature_selection, model_selection, metrics\nfrom xgboost import XGBClassifier\n\nMLA =[\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    \n    gaussian_process.GaussianProcessClassifier(),\n    \n    \n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    \n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    XGBClassifier()\n    \n]\n\n\n# split training data \n\ncv_split= model_selection.ShuffleSplit(n_splits=10,test_size=0.3,train_size=0.6,random_state=42)\n\nMLA_cols = ['Name','Params', 'Train Accuracy (mean)', 'Test Accuracy (mean)', 'Test_STD_3', 'Time']\nMLA_df = pd.DataFrame(columns = MLA_cols)\nMLA_predict = df1[y]\n\nfor row_nb, alg in enumerate(MLA):\n    \n    MLA_df.loc[row_nb, 'Name'] = alg.__class__.__name__ # algorithm name\n    cv_result = model_selection.cross_validate(alg,df1[x_bin],df1[y],cv=cv_split,return_train_score=True)\n    MLA_df.loc[row_nb,'Params'] = str(alg.get_params())\n    MLA_df.loc[row_nb,'Time'] = cv_result['fit_time'].mean()\n    MLA_df.loc[row_nb,'Train Accuracy (mean)'] = cv_result['train_score'].mean()\n    MLA_df.loc[row_nb,'Test Accuracy (mean)'] = cv_result['test_score'].mean()\n    MLA_df.loc[row_nb,'Test_STD_3'] = cv_result['test_score'].std() * 3\n  \n    alg.fit(df1[x_bin],df1[y].values.ravel())\n    MLA_predict[alg.__class__.__name__] =alg.predict(df1[x_bin])\n\nMLA_df.sort_values(by='Test Accuracy (mean)',inplace=True,ascending=False)\nMLA_df\n    ","9219c48a":"# Confusion Matrix Example\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = cross_val_predict(svm.SVC(probability=True),df1[x_bin],df1[y],cv=10)\nsns.heatmap(confusion_matrix(df1[y],y_pred,normalize='true'),annot=True)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix For SVC classifier');\n","a0a3ba80":"# Tuning parameters for SVC classifier\n\n\nmodel = svm.SVC(probability=True)\nbase_results = model_selection.cross_validate(model, df1[x_bin],df1[y],cv=cv_split)\nmodel.fit(df1[x_bin],df1[y])\n\nprint ('Default params:',model.get_params())\nprint('Test Score With Default Params:',base_results['test_score'].mean()*100 )\nprint('STD *3 with Default Params:', base_results['test_score'].std() *300)\n\nprint('-'*25)\n\n\nparam_grid = {\n    'kernel': ['rbf'], # kernel parameters selects the type of hyperplane used to separate the data.\n    'gamma': [0.01], # gamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set\n    'C': [1], # C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n    'degree': [2], #degree is a parameter used when kernel is set to \u2018poly\u2019. It\u2019s basically the degree of the polynomial used to find the hyperplane to split the data.\n}\n\ntune_model = model_selection.GridSearchCV(model,param_grid=param_grid,cv=cv_split,scoring='roc_auc')\ntune_model.fit(df1[x_bin],df1[y])\n\nprint ('Best params:',tune_model.best_params_)\nprint('Test Score With Default Params:',tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100 )\nprint('STD *3 with Default Params:', tune_model.cv_results_['std_test_score'][tune_model.best_index_]*300)\n","260db198":"# Tuning parameters for RandomForest classifier\n\n\nmodel = ensemble.RandomForestClassifier()\nbase_results = model_selection.cross_validate(model, df1[x_bin],df1[y],cv=cv_split)\nmodel.fit(df1[x_bin],df1[y])\n\nprint ('Default params:',model.get_params())\nprint('Test Score With Default Params:',base_results['test_score'].mean()*100 )\nprint('STD *3 with Default Params:', base_results['test_score'].std() *300)\n\nprint('-'*25)\n\n\nparam_grid = {\n    'n_estimators' : [100],\n    'max_features' : ['auto'],\n    'max_depth': [5],\n    'min_samples_split': [2],\n    'min_samples_leaf' : [2],\n    'bootstrap': [False],\n}\n\ntune_model = model_selection.GridSearchCV(model,param_grid=param_grid,cv=cv_split,scoring='roc_auc')\ntune_model.fit(df1[x_bin],df1[y])\n\nprint ('Best params:',tune_model.best_params_)\nprint('Test Score With Default Params:',tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100 )\nprint('STD *3 with Default Params:', tune_model.cv_results_['std_test_score'][tune_model.best_index_]*300)\n","26799e01":"# base model\n\nprint('Data Default Shape: ',df1[x_bin].shape)\nprint('Default Features: ',df1[x_bin].columns.values)\n\nprint('For model: ',model.__class__.__name__)\n\nprint('Test Score With Default Params:',base_results['test_score'].mean()*100 )\nprint('STD *3 with Default Params:', base_results['test_score'].std() *300)\n\n# feature selection\n\nfeature_model = feature_selection.RFECV(model, step=1,scoring='accuracy',cv=cv_split,n_jobs=-1)\nfeature_model.fit(df1[x_bin],df1[y])\n\n# transform to reduced features and fit model\n\nx_rfe = df1[x_bin].columns.values[feature_model.get_support()]\nx_rfe_results = model_selection.cross_validate(model,df1[x_rfe],df1[y],cv=cv_split)\n\n\nprint('_'*25)\nprint('Data Current Shape: ',df1[x_rfe].shape)\nprint('Current Features: ',df1[x_rfe].columns.values)\nprint('Test Score With Current Params:',x_rfe_results['test_score'].mean() )\nprint('STD *3 with Default Params:', x_rfe_results['test_score'].std() *300)\n\n# tune rfe model\nprint('-'*25)","bb7a3943":"param_grid = {\n    'criterion': ['entropy'],\n    'n_estimators' : [1000],\n    'max_features' : ['auto'],\n    'max_depth': [4],\n    'min_samples_split': [10],\n    'min_samples_leaf' : [1],\n    'bootstrap': [False],\n}\n\ntune_model = model_selection.GridSearchCV(model,param_grid=param_grid,cv=cv_split,scoring='roc_auc')\ntune_model.fit(df1[x_rfe],df1[y])\n\nprint ('Best params:',tune_model.best_params_)\nprint('Test Score With Default Params:',tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100 )\nprint('STD *3 with Default Params:', tune_model.cv_results_['std_test_score'][tune_model.best_index_]*300)","88839cc8":"# show wrong predictions\n\npredictions = tune_model.predict(df1[x_rfe])\ndf_error = pd.DataFrame(columns = df1[x_rfe].columns)\ndf_error['idx'] =1\ndf_error['pred'] = 1\ndf_error['real'] =1\ncnt =0 \nfor i in range(891):\n    if predictions[i] != df1[y].loc[i][0]:\n        df_error.loc[cnt] = df1[x_rfe].loc[i]\n        df_error['idx'].loc[cnt] = i\n        df_error['pred'].loc[cnt] = predictions[i]\n        df_error['real'].loc[cnt] = df1[y].loc[i][0]\n        cnt +=1\ndf_error.Sex_Coded.value_counts().plot(kind='pie')\nplt.show()\ndf_error.Family.value_counts().plot(kind='bar')\nplt.show()\n\ndf_error","5e5e51bf":"plt.figure(figsize=(20,8))\nsns.heatmap(MLA_predict.corr(),annot=True)","13593de3":"# voting classifier \n# remove all similar estimators first (those with correlation close to 1)\n\nmodels = [\n    \n    ('ada',ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n#     ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    \n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    ('svc', svm.SVC(probability=True)),\n    \n    ('xgb', XGBClassifier())\n\n]\n\n# Hard Vote (majority rules)\n\nvote_hard = ensemble.VotingClassifier(estimators=models,voting='hard')\n\nvote_hard_cv = model_selection.cross_validate(vote_hard,df1[x_rfe],df1[y],cv=cv_split)\nvote_hard.fit(df1[x_rfe],df1[y])\n\nprint('Hard Voting:')\n\nprint('Test accuracy: ', vote_hard_cv['test_score'].mean() *100 )\nprint('Test STD *3 +\/-: ', vote_hard_cv['test_score'].std() *300)\n\nprint('-'*25)\n\n# soft voting (weighted average)\n\n\nvote_soft = ensemble.VotingClassifier(estimators=models,voting='soft')\n\nvote_soft_cv = model_selection.cross_validate(vote_hard,df1[x_rfe],df1[y],cv=cv_split)\nvote_soft.fit(df1[x_rfe],df1[y])\n\nprint('Soft Voting:')\n\nprint('Test accuracy: ', vote_soft_cv['test_score'].mean() *100 )\nprint('Test STD *3 +\/-: ', vote_soft_cv['test_score'].std() *300)","bd8ccca8":"# Hyperparameter Tune\nn_estimators = [10,50,100,300]\nratio = [.1, .25, .5, .75, 1.0]\nlearn = [.01, .03, .05, .1, .25]\nmax_depth = [2, 4, 6, 8, 10, None]\nmin_samples = [5, 10, .03, .05, .10]\ncriterion = ['gini', 'entropy']\nbools = [True, False]\n\n\ngrid_params = [\n    [{\n        # adaboost params\n        'n_estimators': [300],\n        'learning_rate': [0.1],\n        \n        \n    }],\n    \n    [{\n        # bagging classifier\n        'n_estimators': [300],\n        'max_samples': [0.1]\n    }],\n    \n#     [{\n#         # Extra Tree Classifier\n#         'n_estimators': n_estimators, #default=10\n#             'criterion': criterion, #default=\u201dgini\u201d\n#             'max_depth': max_depth, #default=None\n#     }],\n    \n    [{\n        #gradient boost classifier\n        \n        'learning_rate': [.05],\n        'n_estimators': [300],\n        'max_depth': [2],\n    }],\n    \n    [{\n        # RandomForestClassifier \n        'n_estimators': [100],\n         'criterion': ['entropy'], #default=\u201dgini\u201d\n        'max_depth': [6], #default=None\n    }],\n    \n    [{\n    # gaussian classifier\n        \n        'max_iter_predict': [10]\n    }],\n    \n    [{\n        #linearregressioncv\n        'fit_intercept': [True],\n        'solver': [ 'sag']\n    }],\n    \n    [{\n        #bernoulli\n        \n    }],\n    \n    [{\n        #gaussiannb\n    }],\n    \n    [{\n        #knn\n        'n_neighbors': [7], #default: 5\n        'weights': ['uniform'], #default = \u2018uniform\u2019\n        'algorithm': ['brute']\n    }],\n    \n    [{\n    'kernel': ['rbf'], # kernel parameters selects the type of hyperplane used to separate the data.\n    'gamma': [0.01], # gamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set\n    'C': [1], # C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n    'degree': [2], #degree is a parameter used when kernel is set to \u2018poly\u2019. It\u2019s basically the degree of the polynomial used to find the hyperplane to split the data.\n        \n    }],\n    \n    [{\n        'learning_rate': [0.1],\n        'max_depth': [6],\n        'n_estimators': [10]\n    }]\n    \n]\n\nfor est, param in zip(models, grid_params):\n    \n    best_modelcv = model_selection.RandomizedSearchCV(estimator= est[1],param_distributions=param,cv=cv_split,scoring='roc_auc')\n    best_modelcv.fit(df1[x_rfe],df1[y])\n    print('Best params for {} is {}, test_accuracy: {}'.format(est[1].__class__.__name__, best_modelcv.best_params_,best_modelcv.cv_results_['mean_test_score'][best_modelcv.best_index_] *100))\n    \n    est[1].set_params(**best_modelcv.best_params_)\n    \n    \n# Best params for AdaBoostClassifier is {'n_estimators': 300, 'learning_rate': 0.1}, test_accuracy: 87.86301576600705\n# Best params for BaggingClassifier is {'n_estimators': 300, 'max_samples': 0.1}, test_accuracy: 87.70164012872436\n# Best params for GradientBoostingClassifier is {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.05}, test_accuracy: 88.89252539733737\n# Best params for RandomForestClassifier is {'n_estimators': 100, 'max_depth': 6, 'criterion': 'entropy'}, test_accuracy: 88.17643055916224\n# Best params for GaussianProcessClassifier is {'max_iter_predict': 10}, test_accuracy: 87.10541901964929\n# Best params for LogisticRegressionCV is {'solver': 'newton-cg', 'fit_intercept': True}, test_accuracy: 86.34680076314885\n# Best params for BernoulliNB is {}, test_accuracy: 81.09156034962865\n# Best params for GaussianNB is {}, test_accuracy: 86.7309713272034\n# Best params for KNeighborsClassifier is {'weights': 'uniform', 'n_neighbors': 7, 'algorithm': 'brute'}, test_accuracy: 85.4860481775351\n# Best params for SVC is {'kernel': 'rbf', 'gamma': 0.01, 'degree': 2, 'C': 1}, test_accuracy: 86.78288856249846\n# Best params for XGBClassifier is {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.05}, test_accuracy: 88.91386910048607","3a925c9e":"## Same steps but now with tuned params\n\n# Hard Vote (majority rules)\n\nvote_hard = ensemble.VotingClassifier(estimators=models,voting='hard')\n\nvote_hard_cv = model_selection.cross_validate(vote_hard,df1[x_rfe],df1[y],cv=cv_split)\nvote_hard.fit(df1[x_rfe],df1[y])\n\nprint('Hard Voting:')\n\nprint('Test accuracy: ', vote_hard_cv['test_score'].mean() *100 )\nprint('Test STD *3 +\/-: ', vote_hard_cv['test_score'].std() *300)\n\nprint('-'*25)\n\n# soft voting (weighted average)\n\n\nvote_soft = ensemble.VotingClassifier(estimators=models,voting='soft')\n\nvote_soft_cv = model_selection.cross_validate(vote_soft,df1[x_rfe],df1[y],cv=cv_split)\nvote_soft.fit(df1[x_rfe],df1[y])\n\nprint('Soft Voting:')\n\nprint('Test accuracy: ', vote_soft_cv['test_score'].mean() *100 )\nprint('Test STD *3 +\/-: ', vote_soft_cv['test_score'].std() *300)","d66ee1ff":"# model = models[-1][1]\n# model.fit(df1[x_rfe],df1[y])\n# df_test['Survived'] = models[-1][1].predict(df2[x_rfe])\n# submit = df_test[['PassengerId','Survived']]\n\n\n# submit.to_csv('..\/working\/submit.csv',index=False)\n# if you submit this cell the public score is around 0.77","48891e01":"dropped_cols = ['Sex_male','Embarked_S','Title_Other']\ndf_x_dummy.drop(dropped_cols,axis=1,inplace=True)","76f36523":"df_x_dummy[['Age','Fare']] =(df_x_dummy[['Age','Fare']]-df_x_dummy[['Age','Fare']].min())\/(df_x_dummy[['Age','Fare']].max()-df_x_dummy[['Age','Fare']].min())\ndf_x_dummy\n\n# df_x_dummy['Fare'] = min_max_scaler.fit_transform(df_x_dummy['Fare'] ) ","e1069bbb":"train_x_dummy, test_x_dummy, train_y_dummy, test_y_dummy = model_selection.train_test_split(df_x_dummy[[x for x in x_dummy if x not in dropped_cols]],df1[y])","dd05f705":"# # important modules needed for GP\n# import pickle\n# import operator\n# import math\n# import string\n# import random\n\n# from deap import algorithms, base, creator, tools, gp\n# nb_iterations = 1 # Warning Can be very computationally expensive. Set to high values  only if you can wait.\n\n\n# def protectedDiv(left, right):\n#     '''\n#     This is a function that replaces normal division to prevent zero division errors \n#     '''\n#     try:\n#         return left \/ right\n#     except ZeroDivisionError:\n#             return left\n\n\n#     # next step is selecting primitives (operators such as addition, multiplication etc)\n\n# pset = gp.PrimitiveSet('Main', 14)\n# pset.addPrimitive(operator.add,2) #  addprimitive takes 2 arguments: the operator and number of operations (e.g you add 2 numbers)\n# pset.addPrimitive(operator.sub, 2)\n# pset.addPrimitive(operator.mul, 2)\n# pset.addPrimitive(protectedDiv,2) # note we use protected div instead of normal div\n# pset.addPrimitive(math.cos, 1)\n# pset.addPrimitive(math.sin, 1)\n# pset.addPrimitive(max, 2)\n# pset.addPrimitive(min, 2)\n# pset.addPrimitive(math.tanh,1)\n# pset.addPrimitive(abs,1)\n\n\n\n# pset.renameArguments(ARG0='x1')\n# pset.renameArguments(ARG1='x2')\n# pset.renameArguments(ARG2='x3')\n# pset.renameArguments(ARG3='x4')\n# pset.renameArguments(ARG4='x5')\n# pset.renameArguments(ARG5='x6')\n# pset.renameArguments(ARG6='x7')\n# pset.renameArguments(ARG7='x8')\n# pset.renameArguments(ARG8='x9')\n# pset.renameArguments(ARG9='x10')\n# pset.renameArguments(ARG10='x11')\n# pset.renameArguments(ARG11='x12')\n# pset.renameArguments(ARG12='x13')\n# pset.renameArguments(ARG13='x14')\n# pset.renameArguments(ARG14='x15')\n# pset.renameArguments(ARG15='x16')\n# pset.renameArguments(ARG16='x17')\n# pset.renameArguments(ARG17='x18')\n# pset.renameArguments(ARG18='x19')\n# pset.renameArguments(ARG19='x20')\n# pset.renameArguments(ARG20='x21')\n# pset.renameArguments(ARG21='x22')\n# pset.renameArguments(ARG22='x23')\n# pset.renameArguments(ARG23='x24')\n# pset.renameArguments(ARG24='x25')\n# pset.renameArguments(ARG25='x26')\n# pset.renameArguments(ARG26='x27')\n# pset.renameArguments(ARG27='x28')\n# pset.renameArguments(ARG28='x29')\n# pset.renameArguments(ARG29='x30')\n# pset.renameArguments(ARG30='x31')\n# pset.renameArguments(ARG31='x32')\n# pset.renameArguments(ARG32='x33')\n# pset.renameArguments(ARG33='x34')\n# pset.renameArguments(ARG34='x35')\n# pset.renameArguments(ARG35='x36')\n# pset.renameArguments(ARG36='x37')\n# pset.renameArguments(ARG37='x38')\n# pset.renameArguments(ARG38='x39')\n# pset.renameArguments(ARG39='x40')\n# pset.renameArguments(ARG40='x41')\n# pset.renameArguments(ARG41='x42')\n# pset.renameArguments(ARG42='x43')\n# pset.renameArguments(ARG43='x44')\n# pset.renameArguments(ARG44='x45')\n# pset.renameArguments(ARG45='x46')\n# pset.renameArguments(ARG46='x47')\n# pset.renameArguments(ARG47='x48')\n# pset.renameArguments(ARG48='x49')\n# pset.renameArguments(ARG49='x50')\n\n# def randomString(stringLength=10):\n#     \"\"\"Generate a random string of fixed length \"\"\"\n#     letters = string.ascii_lowercase\n#     return ''.join(random.choice(letters) for i in range(stringLength))\n\n# pset.addEphemeralConstant(randomString(), lambda: random.uniform(-10,10)) # ephermal constant used to terminate the terminal\n\n\n# # create fitness and genotype \n\n# creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,))\n# creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n\n# # we want to register some parameters specific to the evolution process,using the toolbox\n# # some code taken from https:\/\/www.kaggle.com\/guesejustin\/91-genetic-algorithms-explained-using-geap\n\n# toolbox = base.Toolbox()\n# toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3)\n# toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n# toolbox.register(\"compile\", gp.compile, pset=pset)\n\n# inputs =  train_x_dummy.values.tolist() #df1[x_rfe].values.tolist()\n# outputs = train_y_dummy.values.tolist()\n# length_input = len(inputs) +1 \n\n# def evalSymbReg(individual):\n# # Transform the tree expression in a callable function\n#     func = toolbox.compile(expr=individual)\n#     # Evaluate the accuracy of individuals \/\/ 1|0 == survived\n#     return math.fsum(np.round(1.-(1.\/(1.+np.exp(-func(*in_))))) == out for in_, out in zip(inputs, outputs)) \/ length_input,\n# toolbox.register(\"evaluate\", evalSymbReg)\n# toolbox.register(\"select\", tools.selTournament, tournsize=3)\n# toolbox.register(\"mate\", gp.cxOnePoint)\n# toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n# toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n\n# toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n# toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n\n# # stats functions \n\n\n# stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n# stats_size = tools.Statistics(len)\n# mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n# mstats.register(\"avg\", np.mean)\n# mstats.register(\"std\", np.std)\n# mstats.register(\"min\", np.min)\n# mstats.register(\"max\", np.max)\n\n\n# pop = toolbox.population(n=300)\n# hof = tools.HallOfFame(1)\n\n# pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.1, ngen=nb_iterations, stats=mstats,\n#                                halloffame=hof, verbose=True)\n    \n# result = toolbox.compile(expr=hof[0]) ","a562c0de":"# from sklearn.metrics import accuracy_score\n# def Outputs(data):\n#     return np.round(1.-(1.\/(1.+np.exp(-data))))\n# predicted = Outputs(np.array([result(*x) for x in test_x_dummy.values.tolist()]))\n# print(accuracy_score(predicted.astype('int'),test_y_dummy.astype('int')))","175f8b3d":"# # submission with genetic algorithm\n\n# df_test['surv1'] = vote_soft.predict(df2[x_rfe])\n\n# df_test['Survived'] = Outputs(np.array([result(*x) for x in df2[x_rfe].values.tolist()])).astype(int)\n \n# print(accuracy_score(df_test['Survived'].astype('int'),df_test['surv1'].astype('int')))\n\n# submit = df_test[['PassengerId','Survived']]\n\n\n# # with about  200  iterations the 2 algorithms become very similar (accuracy 99.5)\n# # if we want better accuracy, we need more than 200 iterations\n# # submit.to_csv('..\/working\/submit3.csv',index=False)\n","306b4be2":"# import tensorflow as tf","ee31b8d4":"# model = tf.keras.Sequential([\n#     tf.keras.layers.Flatten(input_shape=(14,)),\n#     tf.keras.layers.Dense(42,activation=tf.nn.relu),\n#     tf.keras.layers.Dense(28,activation=tf.nn.relu),\n\n#     tf.keras.layers.Dense(14,activation=tf.nn.relu),\n#     tf.keras.layers.Dense(7,activation=tf.nn.relu),\n\n#     tf.keras.layers.Dense(1,activation=tf.nn.sigmoid),\n# ])","cb768aa4":"# model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","cd8efebe":"# history = model.fit(train_x_dummy,train_y_dummy,epochs=5,verbose=False)\n# test_loss, test_acc = model.evaluate(test_x_dummy, test_y_dummy)\n# test_acc # accuracy is about 82% similar to other models (epochs=50)","453fe167":"# df_test_dummy =  pd.get_dummies(df2[x_pretty])\n# df_test_dummy.drop(dropped_cols,axis=1,inplace=True)\n","959a2b99":"# x_final = [x for x in x_dummy if x not in dropped_cols]\n# df_test_dummy['Survived'] = model.predict_classes(df_test_dummy)","421b630d":"# df_test_dummy['PassengerId'] = df_test['PassengerId']\n# submit = df_test_dummy[['PassengerId','Survived']]\n\n# submit.to_csv('..\/working\/submit.csv',index=False)","dbc181bc":"# check our data again\ndf_feature = pd.get_dummies(df1[x_rfe].astype(str),drop_first=1)\ndf_feature","cefe3ac7":"train_x_dummy, test_x_dummy, train_y_dummy, test_y_dummy = model_selection.train_test_split(df_feature,df1[y])","8d49f970":"# using polynomials \n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2).fit(train_x_dummy)\n\ntrain_x_transformed = poly.transform(train_x_dummy)\ntest_x_transformed = poly.transform(test_x_dummy)","ceec1614":"print(poly.get_feature_names())","bb089535":"df_feature","69e76c79":"# feature selection\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import cross_val_score\n\nmodel = XGBClassifier(n_estimators= 10, max_depth= 6, learning_rate = 0.1)\nbase_results = model_selection.cross_validate(model, df_feature,df1[y],cv=cv_split)\nmodel.fit(df_feature,df1[y])\nprint('Data Default Shape: ',train_x_transformed.shape)\nprint('Default Features: ',df_feature.columns.values)\n\nprint('For model: ',model.__class__.__name__)\n\nprint('Test Score With Default Params:',base_results['test_score'].mean()*100 )\nprint('STD *3 with Default Params:', base_results['test_score'].std() *300)\nhighest_score = 0\nfor i in range(1,train_x_transformed.shape[1] + 1,1):\n    select = SelectKBest(score_func=chi2,k=i)\n    select.fit(train_x_transformed,train_y_dummy)\n    train_x_selected = select.transform(train_x_transformed)\n    \n    model.fit(train_x_selected,train_y_dummy)\n    scores = cross_val_score(model,train_x_selected,train_y_dummy,cv=cv_split)\n    print('features: %i, score %.3f +\/- %.3f' % (i,np.mean(scores),np.std(scores)))\n    \n    \n    # save feature with highest score\n    \n    if np.mean(scores) > highest_score:\n        highest_score = np.mean(scores)\n        std = np.std(scores)\n        number_features = i\n    elif np.mean(scores) == highest_score:\n        if np.std(scores) < std:\n            highest_score = np.mean(scores)\n            std = np.std(scores)\n            number_features = i\nprint('best number of features  %i,  with score: %.3f +\/- %.3f' %(number_features,highest_score,std))","d66db986":"select = SelectKBest(score_func=chi2, k=number_features)\nselect.fit(train_x_transformed,train_y_dummy)\ntrain_x_selected = select.transform(train_x_transformed)\ntest_x_selected = select.transform(test_x_transformed)","91c5f82c":"import tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(train_x_selected.shape[1],)),\n    tf.keras.layers.Dense(145,activation=tf.nn.relu),\n    tf.keras.layers.Dense(100,activation=tf.nn.relu),\n    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n    tf.keras.layers.Dense(10,activation=tf.nn.relu),\n\n    tf.keras.layers.Dense(1,activation=tf.nn.sigmoid),\n])","b7f8800c":"model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","914e2545":"history = model.fit(train_x_selected,train_y_dummy,epochs=300,verbose=True)\ntest_loss, test_acc = model.evaluate(test_x_selected, test_y_dummy)\ntest_acc # here we reach a test score of 0.78 which is slightly better than others.","39ff2405":"df_feature","8b1d9c71":"df_feature['Survival_Rate'] =  0 # set default value\n\nfor idx,row in df_feature.iterrows():\n    if (df_feature.loc[idx,'Sex_Coded_1'] == 0): # if female \n        df_feature.loc[idx,'Survival_Rate'] = 1\n    \n    if(df_feature.loc[idx,'Sex_Coded_1'] == 0) & (df_feature.loc[idx,'Pclass_3'] == 1) & (df_feature.loc[idx,'FareBin_Coded_3']):\n        df_feature.loc[idx,'Survival_Rate'] = 0\n        \n    if(df_feature.loc[idx,'Sex_Coded_1'] == 0) & (df_feature.loc[idx,'Title_Coded_3'] == 0) & (df_feature.loc[idx,'Title_Coded_2'] == 0) & (df_feature.loc[idx,'Title_Coded_1'] == 0) & (df_feature.loc[idx,'Title_Coded_4'] == 0):\n        df_feature.loc[idx,'Survival_Rate'] = 1\n","828c5c84":"poly = PolynomialFeatures(degree=2).fit(df_feature)\ndf_transformed = poly.transform(df_feature)","154ecfb4":"len(poly.get_feature_names())","8b854e3b":"highest_score= 0\nmodel = XGBClassifier(n_estimators= 10, max_depth= 6, learning_rate = 0.1)\nfor i in range(1,df_transformed.shape[1] + 1,1):\n    select = SelectKBest(score_func=chi2,k=i)\n    select.fit(df_transformed,df1[y])\n    df_selected = select.transform(df_transformed)\n    \n    model.fit(df_selected,df1[y])\n    scores = cross_val_score(model,df_selected,df1[y],cv=cv_split) # using xgboost model\n    print('features: %i, score %.3f +\/- %.3f' % (i,np.mean(scores),np.std(scores)))\n    \n    \n    # save feature with highest score\n    \n    if np.mean(scores) > highest_score:\n        highest_score = np.mean(scores)\n        std = np.std(scores)\n        number_features = i\n    elif np.mean(scores) == highest_score:\n        if np.std(scores) < std:\n            highest_score = np.mean(scores)\n            std = np.std(scores)\n            number_features = i\nprint('best number of features  %i,  with score: %.3f +\/- %.3f' %(number_features,highest_score,std))","5bba3656":"select = SelectKBest(score_func=chi2, k=number_features)\nselect.fit(df_transformed, df1[y])\ndf_selected = select.transform(df_transformed)","2f6807a5":"# # important modules needed for GP\n# import pickle\n# import operator\n# import math\n# import string\n# import random\n\n# from deap import algorithms, base, creator, tools, gp\n# nb_iterations = 1000 # Warning Can be very computationally expensive. Set to high values  only if you can wait.\n\n\n# def protectedDiv(left, right):\n#     '''\n#     This is a function that replaces normal division to prevent zero division errors \n#     '''\n#     try:\n#         return left \/ right\n#     except ZeroDivisionError:\n#             return left\n\n\n#     # next step is selecting primitives (operators such as addition, multiplication etc)\n\n# pset = gp.PrimitiveSet('Main', df_selected.shape[1])\n# pset.addPrimitive(operator.add,2) #  addprimitive takes 2 arguments: the operator and number of operations (e.g you add 2 numbers)\n# pset.addPrimitive(operator.sub, 2)\n# pset.addPrimitive(operator.mul, 2)\n# pset.addPrimitive(protectedDiv,2) # note we use protected div instead of normal div\n# pset.addPrimitive(math.cos, 1)\n# pset.addPrimitive(math.sin, 1)\n# pset.addPrimitive(max, 2)\n# pset.addPrimitive(min, 2)\n# pset.addPrimitive(math.tanh,1)\n# pset.addPrimitive(abs,1)\n\n\n\n# pset.renameArguments(ARG0='x1')\n# pset.renameArguments(ARG1='x2')\n# pset.renameArguments(ARG2='x3')\n# pset.renameArguments(ARG3='x4')\n# pset.renameArguments(ARG4='x5')\n# pset.renameArguments(ARG5='x6')\n# pset.renameArguments(ARG6='x7')\n# pset.renameArguments(ARG7='x8')\n# pset.renameArguments(ARG8='x9')\n# pset.renameArguments(ARG9='x10')\n# pset.renameArguments(ARG10='x11')\n# pset.renameArguments(ARG11='x12')\n# pset.renameArguments(ARG12='x13')\n# pset.renameArguments(ARG13='x14')\n# pset.renameArguments(ARG14='x15')\n# pset.renameArguments(ARG15='x16')\n# pset.renameArguments(ARG16='x17')\n# pset.renameArguments(ARG17='x18')\n# pset.renameArguments(ARG18='x19')\n# pset.renameArguments(ARG19='x20')\n# pset.renameArguments(ARG20='x21')\n# pset.renameArguments(ARG21='x22')\n# pset.renameArguments(ARG22='x23')\n# pset.renameArguments(ARG23='x24')\n# pset.renameArguments(ARG24='x25')\n# pset.renameArguments(ARG25='x26')\n# pset.renameArguments(ARG26='x27')\n# pset.renameArguments(ARG27='x28')\n# pset.renameArguments(ARG28='x29')\n# pset.renameArguments(ARG29='x30')\n# pset.renameArguments(ARG30='x31')\n# pset.renameArguments(ARG31='x32')\n# pset.renameArguments(ARG32='x33')\n# pset.renameArguments(ARG33='x34')\n# pset.renameArguments(ARG34='x35')\n# pset.renameArguments(ARG35='x36')\n# pset.renameArguments(ARG36='x37')\n# pset.renameArguments(ARG37='x38')\n# pset.renameArguments(ARG38='x39')\n# pset.renameArguments(ARG39='x40')\n# pset.renameArguments(ARG40='x41')\n# pset.renameArguments(ARG41='x42')\n# pset.renameArguments(ARG42='x43')\n# pset.renameArguments(ARG43='x44')\n# pset.renameArguments(ARG44='x45')\n# pset.renameArguments(ARG45='x46')\n# pset.renameArguments(ARG46='x47')\n# pset.renameArguments(ARG47='x48')\n# pset.renameArguments(ARG48='x49')\n# pset.renameArguments(ARG49='x50')\n\n# def randomString(stringLength=10):\n#     \"\"\"Generate a random string of fixed length \"\"\"\n#     letters = string.ascii_lowercase\n#     return ''.join(random.choice(letters) for i in range(stringLength))\n\n# pset.addEphemeralConstant(randomString(), lambda: random.uniform(-10,10)) # ephermal constant used to terminate the terminal\n\n\n# # create fitness and genotype \n\n# creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,))\n# creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin)\n\n# # we want to register some parameters specific to the evolution process,using the toolbox\n# # some code taken from https:\/\/www.kaggle.com\/guesejustin\/91-genetic-algorithms-explained-using-geap\n\n# toolbox = base.Toolbox()\n# toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3)\n# toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\n# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n# toolbox.register(\"compile\", gp.compile, pset=pset)\n\n# inputs =  df_selected.tolist() #df1[x_rfe].values.tolist()\n# outputs = df1[y].values.tolist()\n# length_input = len(inputs) +1 \n\n# def evalSymbReg(individual):\n# # Transform the tree expression in a callable function\n#     func = toolbox.compile(expr=individual)\n#     # Evaluate the accuracy of individuals \/\/ 1|0 == survived\n#     return math.fsum(np.round(1.-(1.\/(1.+np.exp(-func(*in_))))) == out for in_, out in zip(inputs, outputs)) \/ length_input,\n# toolbox.register(\"evaluate\", evalSymbReg)\n# toolbox.register(\"select\", tools.selTournament, tournsize=3)\n# toolbox.register(\"mate\", gp.cxOnePoint)\n# toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n# toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr_mut, pset=pset)\n\n# toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n# toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=17))\n\n# # stats functions \n\n\n# stats_fit = tools.Statistics(lambda ind: ind.fitness.values)\n# stats_size = tools.Statistics(len)\n# mstats = tools.MultiStatistics(fitness=stats_fit, size=stats_size)\n# mstats.register(\"avg\", np.mean)\n# mstats.register(\"std\", np.std)\n# mstats.register(\"min\", np.min)\n# mstats.register(\"max\", np.max)\n\n\n# pop = toolbox.population(n=300)\n# hof = tools.HallOfFame(1)\n\n# pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.1, ngen=nb_iterations, stats=mstats,\n#                                halloffame=hof, verbose=True)\n    \n# result = toolbox.compile(expr=hof[0]) \n# result","ee6d5198":"import tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(df_selected.shape[1],)),\n    tf.keras.layers.Dense(200,activation=tf.nn.relu),\n    tf.keras.layers.Dense(100,activation=tf.nn.relu),\n    tf.keras.layers.Dense(50,activation=tf.nn.relu),\n    tf.keras.layers.Dense(10,activation=tf.nn.relu),\n\n    tf.keras.layers.Dense(1,activation=tf.nn.sigmoid),\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(df_selected,df1[y],epochs=30,verbose=True)\ntest_loss, test_acc = model.evaluate(df_selected,df1[y])\ntest_acc","39d7336b":"df2_feature = pd.get_dummies(df2[x_rfe].astype(str),drop_first=1)\ndf2_feature['Survival_Rate'] =  0 # set default value\n\nfor idx,row in df2_feature.iterrows():\n    if (df2_feature.loc[idx,'Sex_Coded_1'] == 0): # if female \n        df2_feature.loc[idx,'Survival_Rate'] = 1\n    \n    if(df2_feature.loc[idx,'Sex_Coded_1'] == 0) & (df2_feature.loc[idx,'Pclass_3'] == 1) & (df2_feature.loc[idx,'FareBin_Coded_3']):\n        df2_feature.loc[idx,'Survival_Rate'] = 0\n        \n    if(df2_feature.loc[idx,'Sex_Coded_1'] == 0) & (df2_feature.loc[idx,'Title_Coded_3'] == 0) & (df2_feature.loc[idx,'Title_Coded_2'] == 0) & (df2_feature.loc[idx,'Title_Coded_1'] == 0) & (df2_feature.loc[idx,'Title_Coded_4'] == 0):\n        df2_feature.loc[idx,'Survival_Rate'] = 1\n# poly = PolynomialFeatures(degree=2).fit(df2_feature)\ndf2_transformed = poly.transform(df2_feature)\ndf2_selected = select.transform(df2_transformed)\ndf_test['Survived'] = model.predict_classes(df2_selected)\nsubmit = df_test[['PassengerId','Survived']]\n\nsubmit.to_csv('..\/working\/submit.csv',index=False)\n","1ae08807":"### Converting Columns\n\nAll categorical columns that we want to use we will need to convert them to dummy variables in order to be used.","d8d8364b":"## Using Genetic Algorithm With DEAP\n\n\nUsing Feature Engeneering and ensemble models, Our best score was 0.77. Now Lets try a genetic algorithm to see how good it becomes.","373c8895":"By looking at the data we can quickly see some irrelevant columns:  ticket, cabin,passengerId.  (these are unlikely to provide any new information)\n\nHere is a quick overview of the columns:\n\n- survived: 1=survived\/0=didn't\n- Pclass: \tTicket class\n- sibsp: # of siblings \/ spouses aboard the Titanic\n- parch: # of parents \/ children aboard the Titanic\n- embarked:\tPort of Embarkation\t(C = Cherbourg, Q = Queenstown, S = Southampton)","d018e575":"## Solution Using Neural Networks\n\nWe can try using neural networks to find the solution. ","a89a429b":"## More Feature Engineering\n\nUp till now the public score has reached a maximum of 0.78 which is relatively ok but still can be better.\n\nThe only method that can increase such score in my opinion is using leakage in data (which is generally a frowned upon practice, but still used in most notebooks in this competetion).\n\nWhat this means is that we make use of our survival data and assume it is going to be similar to the test data. Such data might not be available if this was a real dataset and thats why we are discouraged from using it. However, for information purposes only we can use such methods.","6bdea364":"## Feature Selection\n\nTuning Parameters Increased the accuracy up to ~88%. Another approach is to select the appropriate features by feature selection.","ff160ab0":"## Creating Models\n\nAfter Exploring the Data and getting insights. It is now possible to start creating our models to predict survival chance of individuals.","19ff9547":"## Problem Statment\n\n**Project Summary**: The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this project, we will be looking at data of people who were on the Titanic. We Will try to estimate the survival probability of each person depending on different factors:  age, gender, status...\n\nThe data is split into training and testing data. The training data will first be used to train the model and the test data will be used to calculate the accuracy of each model.","5d70dd2c":"## Training Data\n\nBefore we train our models, we need to split data into training and testing. The split will be done as 75\/25 split","bf264081":"## Cleaning Data\n\nThe first step is to clean data. This means: filling in missing values, correcting wrong ones, converting data into appropriate categories...\n\n### Missing Values","b4290ab6":"## Adding More Features\n\nAfter Trying different models. The highest test score reached was about ~77%. If we wanted to go higher we need to think differently about the problem and perhaps find new features."}}