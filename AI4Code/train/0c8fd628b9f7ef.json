{"cell_type":{"9b1d5b25":"code","35b08833":"code","e0ca4c3f":"code","4225a104":"code","4bcdef20":"code","8d148295":"code","e5068a30":"code","97af6d21":"code","316d4f08":"code","25893017":"code","f2170d16":"code","02ad0543":"code","cc54e4b6":"code","e82268d2":"code","10707743":"code","0cea2284":"code","508b1f4e":"markdown","b5b30518":"markdown","83ee8ed3":"markdown","2ff72dfc":"markdown","cfc011ee":"markdown","ae5cb6a5":"markdown","dc85fa4b":"markdown","3ee7e0f8":"markdown"},"source":{"9b1d5b25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35b08833":"# Import the important modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSeed = 42\nnp.random.seed(Seed)\npd.set_option('display.max_rows', None, 'display.max_columns', None)\n\n","e0ca4c3f":"def detect_outliers(df, row, col = 'TARGET'):\n    plt.scatter(df[row], df[col])\n    plt.xlim((df[row].min(), df[row].max()))\n\n\n# remove outliers function\ndef remove_outliers_with_mean(df, col, value):\n    outliers = df[bank[col] > value][col].values\n    df[col].replace(outliers, df[col].mean(), inplace = True)\n    \n","4225a104":"# read the dataset\npath = '\/kaggle\/input\/santander-customer-satisfaction\/'\nbank = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')","4bcdef20":"# Data Exploratory\n\n# bank.isnull().sum()  # it seems the dataset has no null values\n\n# remove features that conatin zeros more than 75K\nremoved_features = []\nfor col in list(bank.columns):\n    print(f'Checking Feature {col}...')\n    feature = col if len(bank[bank[col] == 0]) > 75_000 else None\n    if feature:\n        print(f'{feature} will be removed!')\n        removed_features.append(feature)\n\n\nprint('Droped Features: ')\nprint(len(removed_features), removed_features)\n# so as we see there are 229 features that will be removed from dataset\n\n# Drop these featres from bank dataset and test dataset\nprint('Drop removed Features from train and test set')\nbank.drop(removed_features, axis=1, inplace=True)\ntest.drop(removed_features, axis=1, inplace=True)\n\n","8d148295":"# Checking for outliers\n# print(bank.describe())\n\n# var3\ndetect_outliers(bank, 'var3') # we notice that var3 has min -999999 while 25% is 2 then remove any value that less than 0\n# set negative values to zero in var3 feature\nbank['var3'].replace(-999999, 0, inplace=True) \n\n# remove outliers features based on its outlier values\ndics = [\n    { 'col': 'imp_ent_var16_ult1',      'value': 7000 },\n    { 'col': 'imp_op_var39_comer_ult1', 'value': 7000 },\n    { 'col': 'imp_op_var39_comer_ult3', 'value': 13000 },\n    { 'col': 'imp_op_var41_comer_ult1', 'value': 8000 },\n    { 'col': 'imp_op_var41_comer_ult3', 'value': 12500 },   \n    { 'col': 'imp_op_var41_efect_ult1', 'value': 20_000 },\n    { 'col': 'imp_op_var41_efect_ult3', 'value': 60_000 },\n    { 'col': 'imp_op_var41_ult1', 'value': 20_000 },    \n    { 'col': 'imp_op_var39_efect_ult1', 'value': 20_000 },    \n    { 'col': 'imp_op_var39_efect_ult3', 'value': 60000 },    \n    { 'col': 'imp_op_var39_ult1', 'value': 20000 },    \n    { 'col': 'num_var5', 'value': 8 },    \n    { 'col': 'num_var8_0', 'value': 3 },\n    { 'col': 'num_var12_0', 'value': 50 },    \n    { 'col': 'num_var12', 'value': 6 },    \n    { 'col': 'num_var14_0', 'value': 10 },    \n    { 'col': 'num_var26', 'value': 20 },    \n    { 'col': 'num_var25_0', 'value': 20 },    \n    { 'col': 'num_var25', 'value': 20 },    \n    { 'col': 'num_op_var41_hace2', 'value': 200 },    \n    { 'col': 'num_op_var41_ult1', 'value': 300 },    \n    { 'col': 'num_op_var41_ult3', 'value': 400 },    \n    { 'col': 'num_op_var39_hace2', 'value': 200 },    \n    { 'col': 'num_op_var39_ult1', 'value': 300 },    \n    { 'col': 'num_op_var39_ult3', 'value': 350 },    \n    { 'col': 'num_var30_0', 'value': 25 },    \n    { 'col': 'num_var30', 'value': 15 },    \n    { 'col': 'num_var37_med_ult2', 'value': 60 },    \n    { 'col': 'num_var37_0', 'value': 50 },    \n    { 'col': 'num_var37', 'value': 50 },    \n    { 'col': 'num_var39_0', 'value': 20 },    \n    { 'col': 'num_var41_0', 'value': 20 },    \n    { 'col': 'num_var42_0', 'value': 20 },    \n    { 'col': 'saldo_var5', 'value': 500_000 },    \n    { 'col': 'saldo_var8', 'value': 200_000 },    \n    { 'col': 'saldo_var12', 'value': 1.6e6 },    \n    { 'col': 'saldo_var13', 'value': 700_000 },    \n    { 'col': 'saldo_var24', 'value': 1.6e6 },    \n    { 'col': 'saldo_var26', 'value': 30_000 },    \n    { 'col': 'saldo_var25', 'value': 30_000 },    \n    { 'col': 'saldo_var30', 'value': 2.5e6 },    \n    { 'col': 'saldo_var37', 'value': 30_000 },    \n    { 'col': 'delta_imp_aport_var13_1y3', 'value': 0.5e10 },    \n    { 'col': 'imp_aport_var13_hace3', 'value': 600_000 },    \n    { 'col': 'imp_var43_emit_ult1', 'value': 0.8e6 },    \n    { 'col': 'imp_trans_var37_ult1', 'value': 1.5e6 },    \n    { 'col': 'num_aport_var13_hace3', 'value': 15 },    \n    { 'col': 'num_ent_var16_ult1', 'value': 30 },    \n    { 'col': 'num_var22_hace2', 'value': 80 },    \n    { 'col': 'num_var22_hace3', 'value': 60 },    \n    { 'col': 'num_var22_ult1', 'value': 60 },\n    { 'col': 'num_var22_ult3', 'value': 100 },    \n    { 'col': 'num_med_var22_ult3', 'value': 40 },    \n    { 'col': 'num_op_var39_comer_ult1', 'value': 200 },    \n    { 'col': 'num_op_var39_comer_ult3', 'value': 400 },    \n    { 'col': 'num_op_var41_comer_ult1', 'value': 200 },    \n    { 'col': 'num_op_var41_efect_ult1', 'value': 70 },    \n    { 'col': 'num_op_var41_efect_ult3', 'value': 120 },    \n    { 'col': 'num_op_var39_efect_ult1', 'value': 80 },    \n    { 'col': 'num_op_var39_efect_ult3', 'value': 120 },    \n    { 'col': 'num_var43_emit_ult1', 'value': 80 },    \n    { 'col': 'num_var43_recib_ult1', 'value': 120 },    \n    { 'col': 'num_trasp_var11_ult1', 'value': 60 },    \n    { 'col': 'num_var45_hace2', 'value': 280 },    \n    { 'col': 'num_var45_hace3', 'value': 200 },    \n    { 'col': 'num_var45_ult1', 'value': 350 },    \n    { 'col': 'saldo_medio_var5_hace2', 'value': 500_000 },    \n    { 'col': 'saldo_medio_var5_hace3', 'value': 0.3e6 },    \n    { 'col': 'saldo_medio_var5_ult1', 'value': 400_000 },    \n    { 'col': 'saldo_medio_var5_ult3', 'value': 400_000 },    \n    { 'col': 'saldo_medio_var8_hace2', 'value': 130_000 },    \n    { 'col': 'saldo_medio_var8_ult1', 'value': 150_000 },    \n    { 'col': 'saldo_medio_var8_ult3', 'value': 100_000 },    \n    { 'col': 'saldo_medio_var12_hace2', 'value': 2e6 },    \n    { 'col': 'saldo_medio_var12_hace3', 'value': 500000 },    \n    { 'col': 'saldo_medio_var12_ult1', 'value': 2e6 },    \n    { 'col': 'saldo_medio_var12_ult3', 'value': 1.5e6 },    \n    { 'col': 'var38', 'value': 1.5e7 },\n\n\n\n\n\n\n\n\n\n\n\n\n\n]\n\nfor dic in dics:\n    x = remove_outliers_with_mean(bank, dic['col'], dic['value'])\n","e5068a30":"# var15\nplt.figure(figsize=(13, 8))\n\ncol = bank.columns[141]\n\nsns.boxplot(x = bank[col], fliersize = 2)\n\ncol, len(bank[bank[col] > 1.5e7]) #, bank[col].value_counts()","97af6d21":"target = bank.iloc[:, -1]\nbank.drop('TARGET', axis = 1, inplace = True)","316d4f08":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(bank, target, test_size = 5000)\n\nx_train.shape, x_valid.shape, y_train.shape, y_valid.shape","25893017":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\nx_test = sc.transform(test)","f2170d16":"import xgboost as xg\n\nxg_clf = xg.XGBClassifier()\n\nxg_clf.fit(x_train, y_train)\n","02ad0543":"xg_clf.score(x_valid, y_valid)","cc54e4b6":"y_pred_xg = xg_clf.predict(x_test)\ny_pred_xg","e82268d2":"import copy\nsubmit = pd.read_csv(path + 'sample_submission.csv')\ny_true = copy.deepcopy(submit['TARGET'])\nsubmit['TARGET'] = y_pred_xg\nsubmit.to_csv('solution_submission.csv', index = False)","10707743":"from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred_xg)","0cea2284":"from sklearn.ensemble import AdaBoostClassifier\n","508b1f4e":"### Build the Model","b5b30518":"### Split Train and Valid also Target","83ee8ed3":"###  Build Functions","2ff72dfc":"#### XgBoost","cfc011ee":"### Read the dataset","ae5cb6a5":"### Scale the features","dc85fa4b":"### Data Exploratory","3ee7e0f8":"### Split Target from train df"}}