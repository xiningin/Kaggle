{"cell_type":{"36470552":"code","a91df68d":"code","abc2df53":"code","52e06ff9":"code","b6917c45":"code","08a16f2a":"code","f6fd10db":"code","fb584557":"code","4a9ff3b3":"code","b0404258":"code","e20b4628":"code","fb3044db":"code","05f42c6a":"code","dde67b64":"code","5f63f45c":"code","d14fc21a":"code","28d71560":"code","4e44a22a":"code","54513e99":"code","43031d0d":"code","5b336316":"code","01e64e78":"code","788b57cd":"code","27ab3238":"code","a00005eb":"code","b83585c9":"code","342eaa65":"code","ae4f1c7c":"code","d2e1f24e":"code","701254b7":"code","28794829":"code","aa85a6ba":"code","e37a6d23":"code","7e1eeb7d":"code","78cbe405":"code","3fe7a181":"code","80e7d9c2":"markdown","27e94108":"markdown","f51bcd16":"markdown","26c7b30c":"markdown","9a84b337":"markdown","b9256392":"markdown","04bb2a56":"markdown","4f2105c5":"markdown","de264a63":"markdown"},"source":{"36470552":"from sklearn import datasets\nimport pandas as pd\niris=datasets.load_iris()","a91df68d":"df=pd.DataFrame(data=iris['data'],columns=iris['feature_names'])\ndf['target']=iris['target']","abc2df53":"df","52e06ff9":"from sklearn.model_selection import train_test_split","b6917c45":"X_train,X_test,y_train,y_test=train_test_split(df.iloc[:,:-1],df['target'],test_size=0.3)","08a16f2a":"m=df.corr().iloc[:-1,:-1]","f6fd10db":"import seaborn as sns\nsns.heatmap(df.corr(),cmap='Blues',annot=True,square=True)","fb584557":"def remove_feature(m):\n    drop=[]\n    for column in m.columns:\n        if(column in drop):\n            continue\n        else:\n            for index in m.index:\n                if(index==column or index in drop):\n                    continue\n                elif(m.loc[:,column][index]>0.85):\n                    drop.append(index)\n    return drop","4a9ff3b3":"remove_feature(m)","b0404258":"def scale(df):\n    mean=[mean for mean in df.describe().iloc[1]]\n    std =[std for std in df.describe().iloc[2]]\n    df_new=(df-mean)\/std\n    return df_new","e20b4628":"df_new=scale(df.iloc[:,:-1])","fb3044db":"df_new","05f42c6a":"from sklearn.model_selection import train_test_split","dde67b64":"X_train,X_test,y_train,y_test=train_test_split(df_new,df['target'],test_size=0.3)","5f63f45c":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)","d14fc21a":"y_=knn.predict(X_test)","28d71560":"import numpy as np\ny_real=np.array(y_test).reshape(-1,1)","4e44a22a":"knn.score(X_test,y_test)*100","54513e99":"from sklearn.metrics import classification_report,confusion_matrix\ncm=confusion_matrix(y_test,y_)","43031d0d":"print(classification_report(y_test,y_))","5b336316":"import matplotlib.pyplot as plt\nplt.title('Confusion Matrix')\naxs=sns.heatmap(cm,cmap='YlGnBu',annot=True,square=True)\n#axs.invert_yaxis()\nplt.show()","01e64e78":"from sklearn.model_selection import cross_val_score\ndef find_optimal_k():\n    cv_accuracies = []\n    k_values = list(range(1,50))\n    for k in k_values:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        accuracies = cross_val_score(knn, X_train, y_train, cv=10)\n        cv_accuracies.append(accuracies.mean())\n\n    plt.plot(k_values, cv_accuracies)\n    plt.xlabel(\"Number of Neighbors K\")\n    plt.ylabel(\"Accuracies\")\n    plt.show()\n\n    optimal_k = k_values[cv_accuracies.index(max(cv_accuracies))]\n    print('Optimal K: {} with CV Score of {}'.format(optimal_k, max(cv_accuracies)))\n    \nfind_optimal_k()","788b57cd":"knn=KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train,y_train)\nprint(knn.score(X_test,y_test)*100)\ncm_best_k=confusion_matrix(y_test,knn.predict(X_test))","27ab3238":"plt.title('Confusion Matrix')\naxs=sns.heatmap(cm_best_k,cmap='YlGnBu',annot=True,square=True)\n#axs.invert_yaxis()\nplt.show()","a00005eb":"def find_optimal_k_with_distance():\n    distances = ['euclidean', 'manhattan', 'chebyshev']\n    k_range = list(range(1,50))\n\n    cv_results = {}\n    max_accuracy = 0\n\n    for distance in distances:\n        for k in k_range:\n            knn = KNeighborsClassifier(n_neighbors=k, metric=distance)\n            accuracy = cross_val_score(knn, X_train, y_train, cv=10).mean()\n            cv_results[(distance, k)] = accuracy\n            if accuracy > max_accuracy + 0.000001:\n                max_accuracy = accuracy\n                max_params = (distance, k) # -------->Tuple\n\n    print('Optimal hyperparameters: {}'.format(max_params))\n    print('With CV Score of {}'.format(max_accuracy))\n    return max_params\n\nmax_params = find_optimal_k_with_distance()","b83585c9":"from sklearn.neighbors import KNeighborsRegressor\ndef plt_knn_regression():\n    np.random.seed(0)\n    X = np.sort(5 * np.random.rand(40, 1), axis=0)\n    T = np.linspace(0, 5, 500)[:, np.newaxis]\n    y = np.sin(X).ravel()\n    y[::5] += 1 * (0.5 - np.random.rand(8)) # Add normally distributed noise to output\n\n    # Fit regression model\n    n_neighbors = 5\n    f = plt.figure(figsize=(13, 13))\n    \n    for i, weights in enumerate(['uniform', 'distance']):\n        knn = KNeighborsRegressor(n_neighbors, weights=weights)\n        y_ = knn.fit(X, y).predict(T)\n        plt.subplot(2, 1, i + 1)\n        plt.scatter(X, y, color='darkorange', label='data')\n        plt.plot(T, y_, color='navy', label='prediction')\n        plt.axis('tight')\n        plt.legend()\n        plt.title(\"KNeighborsRegressor (k = {}, weights = '{}')\".format(n_neighbors,\n                                                                    weights))\n    plt.tight_layout()\n    plt.show()\n\nplt_knn_regression()\n","342eaa65":"import numpy as np","ae4f1c7c":"class KNearestNeighbor():\n    def __init__(self,X,k=5):\n        self.k=k\n        self.num_examples=X.shape[0]\n        self.max_iterations=100\n        self.plot_figure=True\n        self.num_features=X.shape[1]\n    def initialize_centeroids(self,X):\n        centeroids=np.zeros((self.k,self.num_features))\n        for i in range(self.k):\n            centeroid=X[np.random.choice(self.num_examples)]\n            centeroids[i]=centeroid\n        return centeroids\n    def create_clusters(self,X,centeroids):\n        clusters=[[] for _ in range(self.k)]\n        for point_idx,point in enumerate(X):\n            closest_centeroid=np.argmin(np.sqrt(np.sum((point-centeroids)**2,axis=1)))\n            clusters[closest_centeroid].append(point_idx)\n        return clusters\n    def calculate_new_centeroids(self,clusters,X):\n        centeroids=np.zeros((self.k,self.num_features))\n        for idx,cluster in enumerate(clusters):\n            new_centeroid=np.mean(X[cluster],axis=0)\n            centeroids[idx]=new_centeroid\n        return centeroids\n    def predict_cluster(self,clusters,X):\n        y_pred=np.zeros(self.num_examples)\n        for cluster_idx,cluster in enumerate(clusters):\n            for sample_idx in cluster:\n                y_pred[sample_idx]=cluster_idx\n            return y_pred\n    def plot_fig(self, X, y):\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n        plt.show()\n    def fit(self,X):\n        centeroids=self.initialize_centeroids(X)\n        for it in range(self.max_iterations):\n            clusters = self.create_clusters(X, centeroids)\n\n            previous_centroids = centeroids\n            centroids = self.calculate_new_centeroids(clusters, X)\n\n            diff = centroids - previous_centroids\n\n            if not diff.any():\n                print(\"Termination criterion satisfied\")\n                break\n\n        # Get label predictions\n        y_pred = self.predict_cluster(clusters, X)\n\n        if self.plot_figure:\n            self.plot_fig(X, y_pred)\n\n        return y_pred","d2e1f24e":"from sklearn.datasets import make_blobs\nnp.random.seed(10)\nnum_clusters = 3\nX, _ = make_blobs(n_samples=1000, n_features=2, centers=num_clusters)","701254b7":"Kmeans = KNearestNeighbor(X, num_clusters)\ny_pred = Kmeans.fit(X)","28794829":"k_=KNearestNeighbor(X)\nk_.initialize_centeroids(X)","aa85a6ba":"clusters = [[] for _ in range(5)]\n\n        # Loop through each point and check which is the closest cluster\nfor point_idx, point in enumerate(X):\n    closest_centroid = np.argmin(\n        np.sqrt(np.sum((point - k_.initialize_centeroids(X)) ** 2, axis=1))\n    )\n    clusters[closest_centroid].append(point_idx)","e37a6d23":"closest_centroid","7e1eeb7d":"centroids = np.zeros((5, 2))\nfor idx, cluster in enumerate(clusters):\n    new_centroid = np.mean(X[cluster], axis=0)\n    centroids[idx] = new_centroid","78cbe405":"np.mean(X[clusters[1]],axis=0)","3fe7a181":"import matplotlib.pyplot as plt\nplt.hist(X[:,1])","80e7d9c2":"### Implementing KNN from scratch","27e94108":"### Important Points\n- K is generally odd\n- Instance Based Learning Technique","f51bcd16":"### Refrences \n- [Sklearn Docummentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsRegressor.html)","26c7b30c":"### K-Neaarest neighbour classifier","9a84b337":"### Types of Distances Used in K-means clustering Algo","b9256392":"- Euclidean Distance\n- Cosine Distance\n- Manhatten Distance\n- Minkowski Distance\n- SEuclidean Distance\n\nRefer this [link](https:\/\/towardsdatascience.com\/log-book-guide-to-distance-measuring-approaches-for-k-means-clustering-f137807e8e21) for clear understanding.","04bb2a56":"#### Finding Optimal Distance and k","4f2105c5":"### Removing based on Correlation","de264a63":"### KNN For Regression\n![image_2.png](attachment:image.png)"}}