{"cell_type":{"2f79a2a2":"code","148f5629":"code","10389954":"code","655188ec":"code","b1692b27":"code","f8469ca5":"code","0c6a874c":"code","2aa6974e":"code","83d9d2a2":"code","79eefe13":"code","938b1a60":"code","7f39a4ed":"code","48e09874":"code","7db28f69":"code","81209b52":"code","83877043":"code","53e7e753":"code","c7e5df7a":"code","26a7bcc7":"code","1c9516a5":"code","42381d22":"code","874cbde9":"code","86b7f1c9":"code","69543e1f":"code","d7307032":"code","7be4f4b4":"code","ec8a9108":"code","96e388e1":"code","d6403548":"code","6ac03cc8":"code","fe695dc8":"markdown","d50d3001":"markdown","f5485ba8":"markdown","d2274867":"markdown","2366bf5d":"markdown","b6b9bd24":"markdown","c2c7bc8b":"markdown","055db6b1":"markdown","91da36a5":"markdown","6e2df0cf":"markdown","25ddd166":"markdown","076ad925":"markdown","d3c01350":"markdown","a95ef302":"markdown","a08a235e":"markdown","7be8871c":"markdown","316df69e":"markdown"},"source":{"2f79a2a2":"import pandas as pd\npd.options.display.float_format = \"{:,.4f}\".format\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For data scaling\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler\n\n#For classes re labeling\nfrom sklearn.preprocessing import LabelBinarizer\n\n# For Dimension Reduction\nfrom sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\nfrom sklearn.manifold import TSNE, Isomap\nfrom umap import UMAP\n\n## For building auto encoder\nfrom tensorflow.keras.layers import Input,Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import regularizers\n\n# For Imbalanced classes\nfrom imblearn.over_sampling import SMOTE\n\n# For Machine Learning modeling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# For Cross vaidation\nfrom sklearn.model_selection import cross_validate\nplt.style.use('fivethirtyeight')\nimport random\n\n# silence NumbaPerformanceWarning\nimport warnings\nfrom numba.errors import NumbaPerformanceWarning\nwarnings.filterwarnings(\"ignore\", category=NumbaPerformanceWarning)\n","148f5629":"train = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\ntest = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')","10389954":"lb = LabelBinarizer()\ntrain['LABEL'] = lb.fit_transform(train['LABEL'])\ntest['LABEL'] = lb.transform(test['LABEL'])","655188ec":"print(train.shape)\nprint(test.shape)","b1692b27":"train.dtypes","f8469ca5":"train.describe().T","0c6a874c":"train.isna().sum()","2aa6974e":"train['LABEL'].value_counts().plot(kind = 'bar', title = 'Class Distributions \\n (0: Not Exoplanet || 1: Exoplanet)', rot=0)\n# PROBLEM OF IMBALANCED CLASS\n# RUNNING ANY ALGORITHM WILL RESULT IN A ACCURACY MEASURE THAT IS HIGHLY INFLUENCED BY DOMINATING CLASS","83d9d2a2":"#Visualizing the the 5 five rercords\nplt.figure(figsize=(25,10))\nplt.title('Distribution of flux values', fontsize=15)\nplt.xlabel('Flux values')\nplt.ylabel('Flux intensity')\nplt.plot(train.iloc[0,])\nplt.plot(train.iloc[1,])\nplt.plot(train.iloc[2,])\nplt.plot(train.iloc[3,])\nplt.plot(train.iloc[4,])\nplt.legend(('Data1', 'Data2', 'Data3', 'Data4', 'Data5'))\nplt.show()","79eefe13":"Exoplanet = train[train['LABEL']==1]\nNot_Exoplanet = train[train['LABEL']==0]","938b1a60":"#Visualizing the Exoplanets data, Since columns are 3198, bining is one way to see the gaussian hist\nfor i in range(5):\n    flux = random.choice(Exoplanet.index)\n    plt.figure(figsize=(20,10))\n    plt.hist(Exoplanet.iloc[flux,:], bins=100)\n    plt.title(\"Gaussian Histogram of Exoplanets\")\n    plt.xlabel(\"Flux values\")\n    plt.show()","7f39a4ed":"#Visualizing the Non Exoplanets data, Since columns are 3198, bining is one way to see the gaussian hist\nfor i in range(5):\n    flux = random.choice(Not_Exoplanet.index)\n    plt.figure(figsize=(20,10))\n    plt.hist(Not_Exoplanet.iloc[flux,:], bins=100)\n    plt.title(\"Gaussian Histogram of Non Exoplanets\")\n    plt.xlabel(\"Flux values\")\n    plt.show()","48e09874":"#pd.to_numeric(train.iloc[:,0], downcast='integer')\n#train.iloc[:,1:].apply(pd.to_numeric(train.iloc[:,x:], downcast='float'))\ndef memory_manager(df):\n    for col, types in df.dtypes.iteritems():\n        if types == 'int64':\n            df[col] = pd.to_numeric(df[col], downcast='integer')    \n        elif types == 'float64':\n            df[col] = pd.to_numeric(df[col], downcast='float') \n    return df","7db28f69":"# Normalizing the Dataset\n#For this just for exploring best technique we will use all the normalization techniques\ndef data_normalizing(df_train,df_test, option):\n    if option ==  1:\n        scale = StandardScaler()  \n        df_train = scale.fit_transform(df_train)\n        df_test = scale.transform(df_test)\n    elif option == 2:\n        scale = MinMaxScaler()\n        df_train = scale.fit_transform(df_train)\n        df_test = scale.transform(df_test)\n    elif option == 3:\n        scale = MaxAbsScaler()\n        df_train = scale.fit_transform(df_train)\n        df_test = scale.transform(df_test)\n    elif option ==  4:\n        scale = Normalizer()\n        df_train = scale.fit_transform(df_train)\n        df_test = scale.transform(df_test)\n    elif option ==  5:\n        scale = RobustScaler()\n        df_train = scale.fit_transform(df_train)\n        df_test = scale.transform(df_test)\n    elif option == 6:\n        scale = 'None'\n        df_train\n        df_test \n    else:\n        print('Enter Valid option')\n        \n    \n    return df_train, df_test , str(scale).replace(\"()\",\"\")\n        ","81209b52":"# High Dimensional issues\n# No of columns is >3000\n#Reduce dimension using technique which maximizes the result\ndef auto_encoder(df,df_test, component):\n    # input placeholder\n    input_data = Input(shape=df.shape[1:]) # 6 is the number of features\/columns\n\n    encoded = Dense(2048, activation = 'relu')(input_data)\n    encoded = Dense(1024, activation = 'relu')(encoded)\n    encoded = Dense(512, activation = 'relu')(encoded)\n    encoded = Dense(256, activation = 'relu')(encoded)\n    encoded = Dense(128, activation = 'relu')(encoded)\n    encoded = Dense(64, activation = 'relu')(encoded)\n    encoded = Dense(component, activation = 'relu')(encoded)\n\n    decoded = Dense(64, activation = 'relu')(encoded)\n    decoded = Dense(128, activation = 'relu')(decoded)\n    decoded = Dense(256, activation = 'relu')(decoded)\n    decoded = Dense(512, activation = 'relu')(decoded)\n    decoded = Dense(1024, activation = 'relu')(decoded)\n    decoded = Dense(2048, activation = 'relu')(decoded)\n    decoded = Dense(df.shape[1:][0], activation ='sigmoid')(decoded) # 6 again number of features and should match input_data\n\n\n\n    # this model maps an input to its reconstruction\n    autoencoder = Model(input_data, decoded)\n\n    # this model maps an input to its encoded representation\n    encoder = Model(input_data, encoded)\n\n    # model optimizer and loss\n    autoencoder = Model(input_data, decoded)\n\n    # loss function and optimizer\n    autoencoder.compile(optimizer='adam', loss='mse')\n\n    # train test split\n    from sklearn.model_selection import train_test_split\n    x_train, x_test, = train_test_split(df, test_size=0.1, random_state=42)\n\n\n    # train the model\n    autoencoder.fit(df,\n                  df,\n                  epochs=20,\n                  batch_size=8,\n                  shuffle=True, verbose=0)\n\n    #autoencoder.summary()\n\n    # predict after training\n    # note that we take them from the *test* set\n    encoded_data = encoder.predict(df)\n    encoded_data_test = encoder.predict(df_test)\n    df_train_new=pd.DataFrame(encoded_data)\n    df_test_new=pd.DataFrame(encoded_data_test)\n    return(df_train_new,df_test_new)\n\n\ndef dimension_reduction(df_train,df_test,components, algo):\n    if algo == 'PCA':\n        pca = PCA(n_components=components)\n        df_train = pca.fit_transform(df_train)\n        df_test = pca.transform(df_test)\n    elif algo == 'KPCA':\n        kpca = KernelPCA(kernel = 'rbf', n_components=components)\n        df_train = kpca.fit_transform(df_train)\n        df_test = kpca.transform(df_test)\n    elif algo == 'TSNE':\n        tsne = TSNE(method='exact',  n_components=components)\n        df_train = tsne.fit_transform(df_train)\n        df_test = tsne.fit_transform(df_test)\n    elif algo == 'UMAP':\n        umaps = UMAP(n_components=components)\n        df_train = umaps.fit_transform(df_train)\n        df_test = umaps.transform(df_test)\n    elif algo == 'TSVD':\n        svd = TruncatedSVD(n_components=components)\n        df_train = svd.fit_transform(df_train)\n        df_test = svd.transform(df_test)\n    elif algo == 'ISO':\n        isomap = Isomap(n_components=components)\n        df_train = isomap.fit_transform(df_train)\n        df_test = isomap.transform(df_test)\n    elif algo == 'AE' :\n        df_train, df_test = auto_encoder(df_train,df_test, components)\n        #df_test = auto_encoder(df_test, components)\n    else:\n        print('Data looks good enough, NO High Dimensionality')\n        \n    return df_train,df_test","83877043":"#Dimensionality reduction\ndef optimal_components(df_train, df_test):\n    pca = PCA() \n    X_train = pca.fit_transform(df_train)\n    X_test = pca.transform(df_test)\n    total=sum(pca.explained_variance_)\n    k=0\n    current_variance=0\n    while current_variance\/total < 0.95: # Change this to see increased accuracy in majority models, I tried with 98 as well but nof components will be 20+ in that case\n        current_variance += pca.explained_variance_[k]\n        k=k+1\n    return k","53e7e753":"# Testing the logic\nk =  optimal_components(train, test)\n#No of components for dimension Reduction\n\n#Apply PCA with n_componenets\npca = PCA(n_components=k)\nx_train = pca.fit_transform(train)\nx_test = pca.transform(test)\nplt.figure(figsize=(20,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Exoplanet Dataset Explained Variance')\nplt.show()","c7e5df7a":"# SYNTHETIC SAMPLING\n# GENERATE SYNTHETIC BUT DIFF SAMPLE(OVERSAMPLE THE MINORITY CLASS)\ndef label_balance(df_train, df_test):\n    balancing = SMOTE(random_state=42, sampling_strategy='minority')\n    X_train_res, Y_train_res = balancing.fit_resample(df_train, df_test)\n    return X_train_res, Y_train_res","26a7bcc7":"# Trying the model for one time just to check\nX_train, X_test, Y_train, Y_test = train.iloc[:,1:], test.iloc[:,1:],train.iloc[:,0], test.iloc[:,0]\n\n# Step 2: Normalizing the data\nX_train,X_test, scaling = data_normalizing(X_train,X_test,4)\n\n#Step 3.0 finding optimal components\nk = optimal_components(X_train, X_test)\n\n#Step 3: Dimension reduction in this case, very high dimesnions\n#print(algo)\nX_train,X_test = dimension_reduction(X_train,X_test, k, 'PCA')\n\n#Step 4: Imbalanced class, dominant class\n#print(Y_train.value_counts())\nX_train, Y_train = label_balance(X_train,Y_train)\nrf = RandomForestClassifier(random_state=1,n_jobs =-1)\nrf.fit(X_train, Y_train)\nY_predict = rf.predict(X_test)\nscores = cross_validate(rf, X_train, Y_train, cv=5 ,scoring=['accuracy','f1', 'precision','recall']  )\n\nprint(scores['test_accuracy'].mean(),\n      scores['test_accuracy'].std(),\n        scores['test_f1'].mean(),\n        scores['test_precision'].mean(),\n        scores['test_recall'].mean())","1c9516a5":"# Building Model\ndef ml_model(classifier, X_train, X_test, Y_train, Y_test):\n    classifier.fit(X_train, Y_train)\n    Y_predict = classifier.predict(X_test)\n    scores = cross_validate(classifier,X_train, Y_train, cv=5, scoring=['accuracy','f1', 'precision','recall'] )\n    cv_accuracy = scores['test_accuracy']\n    acc_mean  = scores['test_accuracy'].mean()\n    acc_variance  = scores['test_accuracy'].std()\n    f1_mean = scores['test_f1'].mean()\n    precision_mean = scores['test_precision'].mean()\n    recall_mean = scores['test_recall'].mean()\n    #print(\"Accuracy mean: \"+ str(cv_mean))\n    #print(\"Accuracy variance: \"+ str(cv_variance))\n    acc = accuracy_score(Y_test, Y_predict)\n    conf_matrix = confusion_matrix(Y_test, Y_predict)\n    return acc, conf_matrix, cv_accuracy, acc_mean, acc_variance, f1_mean, precision_mean, recall_mean","42381d22":"def data_scaling(df_train,df_test,option):\n    #Step1: Memory management of dataset, changing type by downcasting it so that it occupies less space\n    df_train = memory_manager(df_train)\n    df_test = memory_manager(df_test)\n    X_train, X_test, Y_train, Y_test = df_train.iloc[:,1:], df_test.iloc[:,1:],df_train.iloc[:,0], df_test.iloc[:,0]\n    \n    # Step 2: Normalizing the data\n    X_train,X_test, scaling = data_normalizing(X_train,X_test,option)\n    \n    # Step check : we check for optimal no of component using PCA for every scaling type\n    k = optimal_components(X_train, X_test)\n    return X_train,X_test,Y_train,Y_test, scaling ,k\n\n\ndef data_prep(df_train,df_test,label_train, label_test, scale_type,dim_red_ago,components):\n    \n    #Step 3: Dimension reduction in this case, very high dimesnions\n    #print(algo)\n    df_train_reduced,df_test_reduced = dimension_reduction(df_train,df_test, components, dim_red_ago)\n    \n    #Step 4: Imbalanced class, dominant class\n    #print(Y_train.value_counts())\n    print\n    X_train, Y_train = label_balance( df_train_reduced,label_train)\n    X_test, Y_test = df_test_reduced, label_test\n    #print(Y_train.value_counts())\n    \n    pipeline_optim =  {'Scaling Algorithm': scale_type, 'Dimensional Reduction': dim_red_ago}\n    \n    return X_train, X_test, Y_train, Y_test, pipeline_optim\n\n\ndef result(df_train, df_test, scaling, dimension_algo):\n    # Identifying the train ad\n    #X_train, X_test, Y_train, Y_test = df_train.iloc[:,1:], df_test.iloc[:,1:],df_train.iloc[:,0], df_test.iloc[:,0]\n    X_train,X_test,Y_train,Y_test, scaling ,k = data_scaling(df_train,df_test,scaling)\n    X_train, X_test, Y_train, Y_test, pipes = data_prep(X_train,X_test,Y_train, Y_test, scaling, dimension_algo ,k)\n    RFC = RandomForestClassifier(random_state = 7, n_jobs =-1)\n    acc, conf_matrix, cv_accuracy, acc_mean, acc_variance, f1_mean, precision_mean, recall_mean = ml_model(RFC,X_train, X_test, Y_train, Y_test)\n    #pipes['result'] = acc*100\n    pipes['Dimensional Components'] = k\n    pipes['Accuracy CV'] = cv_accuracy*100\n    #pipes['cv accuracy'] = [round(val,3) for val in pipes['cv accuracy']]\n    pipes['Mean Accuracy CV'] = acc_mean*100\n    pipes['Variance Accuracy CV'] = acc_variance*100\n    pipes['Mean F1 CV'] = f1_mean*100\n    pipes['Mean Precision CV'] = precision_mean*100\n    pipes['Mean Recall CV'] = recall_mean*100\n    return pipes   ","874cbde9":"# Testing the model for single instance for Standard Scaler and PCA for optmial component\nmy_result = result(train, test, 1 , 'TSVD')\nmy_result","86b7f1c9":"def final(X_train, X_test, Y_train, Y_test, scaling, dimension_algo,k):\n    X_train, X_test, Y_train, Y_test, pipes = data_prep(X_train,X_test,Y_train, Y_test, scaling, dimension_algo ,k)\n    RFC = RandomForestClassifier(random_state = 7)\n    acc, conf_matrix, cv_accuracy, acc_mean, acc_variance, f1_mean, precision_mean, recall_mean = ml_model(RFC,X_train, X_test, Y_train, Y_test)\n    #pipes['result'] = acc*100\n    pipes['Dimensional Components'] = k\n    pipes['Accuracy CV'] = cv_accuracy*100\n    #pipes['cv accuracy'] = [round(val,3) for val in pipes['cv accuracy']]\n    pipes['Mean Accuracy CV'] = acc_mean*100\n    pipes['Variance Accuracy CV'] = acc_variance*100\n    pipes['Mean F1 CV'] = f1_mean*100\n    pipes['Mean Precision CV'] = precision_mean*100\n    pipes['Mean Recall CV'] = recall_mean*100\n    return pipes   \n\n#==============================================================================================================================\n#    FINAL MODEL WHICH WILL RUN OUR MODEL RANDOM FOREST FOR ALL COMBINATIONS OF SCALING AND DIMENSION REDUCTION TECHNIQUES\n#==============================================================================================================================\n\nscaling_options = {1: 'Standard Scalar',2:'MinMax Scalar',3:'MaxAbs Scalar',4:'Normalizer',5:'Robust Scaler'}\nDimension_red_options = ['PCA','KPCA','UMAP','TSVD','ISO']\n\nlist_result = []\n\nfor scale in scaling_options:\n    X_train,X_test,Y_train, Y_test, scaling ,k = data_scaling(train,test,scale)\n    for dim in Dimension_red_options:\n        #start_time = time.time()\n        res = final(X_train, X_test, Y_train, Y_test, scaling, dim,k)\n        #end_time = time.time()\n        print(res)\n        list_result.append(res) ","69543e1f":"Results = pd.DataFrame.from_dict(list_result, orient='columns')","d7307032":"Results","7be4f4b4":"def highlight_greaterthan(s, threshold, column):\n    is_max = pd.Series(data=False, index=s.index)\n    is_max[column] = s.loc[column] >= threshold\n    return ['background-color: darkgreen' if is_max.any() else '' for v in is_max]\n\ndef highlight_lesserthan(s, threshold, column):\n    is_min = pd.Series(data=False, index=s.index)\n    is_min[column] = s.loc[column] <= threshold\n    return ['background-color: darkorange' if is_min.any() else '' for v in is_min]\n\nprint(\"\\x1b[32m\\\" Cross Validation Accuracy Mean in Dark Green\\\"\\x1b[0m\")\nprint(\"\\x1b[33m\\\" Cross Validation Accuracy Variance in color Dark Orange\\\"\\x1b[0m\")\nResults.style.\\\n         apply(highlight_greaterthan, threshold=max(Results['Mean Accuracy CV']), column=['Mean Accuracy CV'], axis=1).\\\n         apply(highlight_lesserthan, threshold=min(Results['Variance Accuracy CV']), column=['Variance Accuracy CV'], axis=1)","ec8a9108":"box = pd.DataFrame({'preprocess': Results['Scaling Algorithm'] + '-->' + Results['Dimensional Reduction'] , 'res': Results['Accuracy CV'] })\nboxt = box.T\nboxt = boxt.reset_index(drop = True)\nboxt.columns = boxt.iloc[0]\nboxt = boxt.iloc[1:]\nboxt = boxt.apply(pd.Series.explode).reset_index(drop = True)\nboxt.astype('float64').dtypes","96e388e1":"import numpy as np; np.random.seed(42)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(20,15) )\nsns.boxplot(y=0, x=\"value\", data=pd.melt(boxt), orient=\"h\" , showfliers = False )\nax.set_title('Box Plot of accuracies for different models')\nax.set_xlabel('Accuracy')\nax.set_ylabel('Model')\nax.grid(True)\nplt.show()","d6403548":"#replicate the best result\n#scaling_options = {1: 'Standard Scalar',2:'MinMax Scalar',3:'MaxAbs Scalar',4:'Normalizer',5:'Robust Scaler'}\n#Dimension_red_options = ['PCA','KPCA','UMAP','TSVD','ISO']\nBest = result(train, test, 4 , 'PCA')\nBest","6ac03cc8":"# Notes:\n# We need to understand the tradeoff between accuracy and Computation cost. \n# Also when we are doing PCA variance check to find the best no of components, Lot depends on cutoff value as well\n# comments are welcomed","fe695dc8":"# What Is an Exoplanet?\n## The Short Answer:\n\n==============================================================================================================================\n> # \"All of the planets in our solar system orbit around the Sun. Planets that orbit around other stars are called exoplanets.\" \n\n\nAll of the planets in our solar system orbit around the Sun. Planets that orbit around other stars are called exoplanets. Exoplanets are very hard to see directly with telescopes. They are hidden by the bright glare of the stars they orbit.\n\nSo, astronomers use other ways to detect and study these distant planets. They search for exoplanets by looking at the effects these planets have on the stars they orbit.\n\n<!-- -->|\n------------ | \n![Exoplanet](https:\/\/spaceplace.nasa.gov\/all-about-exoplanets\/en\/all-about-exoplanets1.en.png) |\n` An artist's representation of Kepler-11, a small, cool star around which six planets orbit. Credit: NASA\/Tim Pyle `|\n\n## How do we look for exoplanets?\n\nOne way to search for exoplanets is to look for \"wobbly\" stars. A star that has planets doesn\u2019t orbit perfectly around its center. From far away, this off-center orbit makes the star look like it\u2019s wobbling.\n\n<!-- -->|\n------------ | \n![an animation of a wobbling star](https:\/\/spaceplace.nasa.gov\/all-about-exoplanets\/en\/barycenter-side.en.gif) |\n` an animation of a wobbling star and its transiting planet, from the side. ` | \n\nAn orbiting planet (small blue ball) causes a star (large yellow ball) to orbit slightly off-center. From a distance, this makes it look like the star is wobbling.\n\n\nHundreds of planets have been discovered using this method. However, only big planets\u2014like Jupiter, or even larger\u2014can be seen this way. Smaller Earth-like planets are much harder to find because they create only small wobbles that are hard to detect.\n\n\n## How can we find Earth-like planets in other solar systems?\nIn 2009, NASA launched a spacecraft called Kepler to look for exoplanets. Kepler looked for planets in a wide range of sizes and orbits. And these planets orbited around stars that varied in size and temperature.\n\nSome of the planets discovered by Kepler are rocky planets that are at a very special distance from their star. This sweet spot is called the habitable zone, where life might be possible.\n\n<!-- -->|\n------------ | \n![Artist's rendition of the Kepler spacecraft](https:\/\/spaceplace.nasa.gov\/all-about-exoplanets\/en\/all-about-exoplanets2.en.png) |\n` Artist's rendition of the Kepler spacecraft. Credit: NASA\/Kepler mission\/Wendy Stenzel` |\n\n\nKepler detected exoplanets using something called the transit method. When a planet passes in front of its star, it\u2019s called a transit. As the planet transits in front of the star, it blocks out a little bit of the star's light. That means a star will look a little less bright when the planet passes in front of it.\n\nAstronomers can observe how the brightness of the star changes during a transit. This can help them figure out the size of the planet.\n\n<!-- -->|\n------------ | \n![A 2012 image of Venus transiting the Sun.](https:\/\/spaceplace.nasa.gov\/all-about-exoplanets\/en\/venus-transit.en.jpg) |\n`See that little black circle? That's Venus transiting our Sun back in 2012. Credit: NASA Solar Dynamics Observatory` |\n\n\n\nBy studying the time between transits, astronomers can also find out how far away the planet is from its star. This tells us something about the planet\u2019s temperature. If a planet is just the right temperature, it could contain liquid water\u2014an important ingredient for life.\n\nSo far, thousands of planets have been discovered by the Kepler mission. And more will be found by NASA's Transiting Exoplanet Survey Satellite (TESS) mission, which is observing the entire sky to locate planets orbiting the nearest and brightest stars.\n\nWe now know that exoplanets are very common in the universe. And future NASA missions have been planned to discover many more!\nFor more information\n\n[Exoplanets 101](https:\/\/exoplanets.nasa.gov\/what-is-an-exoplanet\/about-exoplanets\/)","d50d3001":"### CHECK: A test for optimal components","f5485ba8":"##  STEP 3.3: Handling Minority Class","d2274867":"## STEP 3.2: Dimensionality Reduction\n* PCA\n* KPCA\n* TSNE\n* UMAP\n* SVD\n* ISO\n* If low dimension then actual columns","2366bf5d":"# STEP 6: Understanding the Results","b6b9bd24":"# STEP 2: Memory Management","c2c7bc8b":"##### Testing the model logic","055db6b1":"# STEP 1: Reading Data and EDA ","91da36a5":"# STEP 3: Machine Learning begins here","6e2df0cf":"# STEP 5: Experiment, Executing all the variations\n### The  idea is to have score for all the variations and pick one, for this particular case a simple model is already giving the best accuracy so, not proceeding with Hypertuning","25ddd166":"# Visualizing Exoplanets and non Exoplanets","076ad925":"# Visualizing Classes","d3c01350":"# STEP 4: Building the Model\n* data_scaling() --> Scaling data and memory management\n* modeling() --> Dimension reduction based on optimal component provided by PCA explained variance --> over sampling minority class using SMOTE\n* Random forest --> base model","a95ef302":"## STEP 3.1 Normalizing the dataset\n* Standard Scaler\n* Normalize\n* MinMax Scaler\n* MaxAbs Scaler\n* Normal data","a08a235e":"===================================================================================================================================================\n# Problem Statement: Correctly identify Exoplanets based on flux values\n===================================================================================================================================================\n# Solution: A Step - by - Step Modeling approach\n\n## STEP 1: Study the data\n    * Re-encode the classes as 0(Not Exoplanet) & 1(Exoplanet)\n    * Visualize the Exoplanets and Non Exoplanets\n\n## STEP 2: Memory Management\n    * Since entire data is numeric(int and float) we will downcast the dataype\n\n## STEP 3: Building Machine Learning Model\n\n    STEP 3.1 : Data Preprocessing\n        * scaling data to make computation faster\n            * Standard Scaler\n            * Normalize\n            * MinMax Scaler\n            * MaxAbs Scaler\n            * Normal data\n    \n    Step 3.2 : Dimension Reduction\n        * Data has very large dimensions 3198 to be exact so dimension reduction, i used PCA dimension reduction check to identify optimal no of components based on the scaling used\n            * Standard Scaler\n            * Normalize\n            * MinMax Scaler\n            * MaxAbs Scaler\n            * Normal data (only when dimension are low, not used in this case)\n            \n    Step 3.3 : Unbalanced Classes\n        *  A look at classes reveal a very unique picture. The no of cases of finding an Exoplanet is very very small compared to planet being a non exoplanet\n            * for this purpose i used  SMOTE (Synthetic Minority Over-sampling Technique) oversampling the minority class(Exoplanets)\n    \n## STEP 4: Model Building\n\n    1. For this we used Random Forest algorithm (ensemble learning) from scikit-learn and we use the base model.\n    2. We used Cross validation = 5 \n    3. Evaluation metrics = Accuracy. F1, Precision, Recall (Cross validated score)\n\n## STEP 5: Evaluation and Understanding the Results\n    Box plot of the results obtained to select the best model\n\n===================================================================================================================================================","7be8871c":"### Building the Random Forest Model with default arguments, for evaluation metrics we tried to reun cross validation of cv=5 and measure accuracy, f1 score, precision and recall (we are dealing with minority class)","316df69e":"## Visualizing Result"}}