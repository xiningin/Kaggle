{"cell_type":{"e757dee3":"code","32ca8a5b":"code","575de9a7":"code","2b91308b":"code","cbd6889b":"code","8a380337":"code","5e480cef":"code","14cb7b3d":"code","fb530936":"code","875b827b":"code","747a7842":"code","9319326b":"code","0c495b99":"code","9ba3fafa":"code","1efbfd7b":"code","ae987f3b":"code","39d1e281":"code","2d99dc25":"code","93298f44":"code","0b0c2550":"code","95eb35ee":"code","56e5a00c":"code","6b586b61":"code","26dd7a32":"code","91e5332e":"code","52dad4b7":"code","3cb0081f":"code","a496c3d7":"code","85161e9f":"code","c0ddfc32":"code","eed5951c":"code","185c315b":"code","2e6e9a03":"code","d9d5379c":"code","c770c729":"code","4f10aa63":"code","3a75482f":"code","f41d385f":"code","2b796f26":"code","f2ecf9d3":"code","daab72d0":"code","d2bb86e8":"code","89f1fb22":"code","63f36358":"code","a35cfb59":"code","c81d95e6":"markdown","2496ddd6":"markdown","15121e42":"markdown","dc43f871":"markdown","be8b44b3":"markdown","d6b66ae9":"markdown","ae063bc6":"markdown","5c8268f7":"markdown","17c328b2":"markdown","596bc9d5":"markdown","6f2bbfc2":"markdown","64bc9cfa":"markdown","725ede49":"markdown","8bac3f28":"markdown","7ce5af94":"markdown","b26d9d08":"markdown","c183ba94":"markdown","d5d9fcec":"markdown","cf0e37a8":"markdown"},"source":{"e757dee3":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import v_measure_score\nfrom sklearn.model_selection import cross_val_score","32ca8a5b":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes_data_copy = df.copy(deep = True)\ndf.head()","575de9a7":"df.info()\n\n# looks like no null values...","2b91308b":"df.describe()\n\n# the values of 'Glucose','BloodPressure','SkinThickness','Insulin','BMI' are not supposed to be zero\n# thier probably NaN values","cbd6889b":"# lets replace the zero's in these columns with NaN\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\ndf.head()","8a380337":"df.isna().sum()","5e480cef":"sns.pairplot(df);","14cb7b3d":"def view_mmm(df,col,type='bar'):\n    df1 = df[col].fillna(df[col].mean())\n    df2 = df[col].fillna(df[col].median())\n    df3 = df[col].fillna(df[col].mode()[0])\n    if type=='bar':\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n        sns.distplot(df1, ax=ax1)\n        sns.distplot(df[col],ax=ax1)\n        ax1.set_xlabel(col+\"_mean\")\n\n        sns.distplot(df2, ax=ax2) \n        sns.distplot(df[col],ax=ax2)\n        ax2.set_xlabel(col+\"_median\")\n\n        sns.distplot(df3, ax=ax3)\n        sns.distplot(df[col],ax=ax3)\n        ax3.set_xlabel(col+\"_mode\")\n        plt.tight_layout()\n    \n    else:\n        from scipy.stats import probplot\n        probplot(df1, dist=\"norm\", plot=plt)\n        plt.show()\n        probplot(df2, dist=\"norm\", plot=plt)   \n        plt.show()\n        probplot(df3, dist=\"norm\", plot=plt) \n        plt.show()\n\ndef to_mmm(df,col):\n    \"\"\" pass dataframe and column name whose missing values you want replaced by mean, median, mode\"\"\"\n    print(df.shape)\n    df1 = df[col].fillna(df[col].mean())\n    df2 = df[col].fillna(df[col].median())\n    df3 = df[col].fillna(df[col].mode()[0])\n    print(df1.size)\n    df_mmm = pd.DataFrame({col+'_mean':df1,col+'_median':df2,col+'_mode':df3})\n    print(df_mmm.shape)\n    return df_mmm\n\nview_mmm(df,'Glucose', type='proba');","fb530936":"view_mmm(df,'BloodPressure', type='proba');","875b827b":"view_mmm(df,'SkinThickness', type='proba');","747a7842":"view_mmm(df,'Insulin', type='proba');","9319326b":"view_mmm(df,'BMI', type='proba');","0c495b99":"def rand_samp(df,col):\n#     print(df)\n    if df.shape[0]>2000:\n        print(df.shape)\n        return\n    print(df.shape)\n    rand_samp = df[col].dropna().sample(df[col].isna().sum(),random_state=0)\n    rand_samp.index = df[df[col].isna()].index\n    df_rand = df[col].copy()\n    print(df_rand.shape)\n    df_rand.loc[df[col].isna(),] = rand_samp\n    df_rand = pd.DataFrame({col+\"_rand_samp\":df_rand})\n    print(df_rand.shape)\n    return df_rand\n\ndef view_rand_samp(df,col):\n    rand_samp = df[col].dropna().sample(df[col].isna().sum(),random_state=0)\n    rand_samp.index = df[df[col].isna()].index\n    df_rand = df[col].copy()\n    df_rand.loc[df[col].isna(),] = rand_samp\n    \n    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,5))\n    sns.distplot(df_rand, ax=ax1)\n    sns.distplot(df[col],ax=ax1)\n    from scipy.stats import probplot \n#     ax2.get_lines()[0].set_markerfacecolor('C0')\n    probplot(df_rand, dist=\"norm\", plot=plt)\n    plt.show()","9ba3fafa":"view_rand_samp(df,'Glucose')","1efbfd7b":"view_rand_samp(df,'Insulin')","ae987f3b":"view_rand_samp(df,'BloodPressure')","39d1e281":"view_rand_samp(df,'SkinThickness')","2d99dc25":"view_rand_samp(df,'BMI')","93298f44":"# Glucose has the least number of missing values so we'll just replace it with mode\n\ndf['Glucose'].fillna(df['Glucose'].mode()[0], inplace=True)","0b0c2550":"df = pd.concat([df, to_mmm(df,'BloodPressure')], axis=1)\ndf = pd.concat([df, to_mmm(df,'SkinThickness')], axis=1)\ndf = pd.concat([df, to_mmm(df,'Insulin')], axis=1)\ndf = pd.concat([df, to_mmm(df,'BMI')], axis=1)\n","95eb35ee":"# df = pd.concat([df, rand_samp(df,'Glucose')], axis=1) # view_rand_samp(df,'Age')\ndf = pd.concat([df, rand_samp(df,'BloodPressure')], axis=1) # view_rand_samp(df,'Age')\ndf = pd.concat([df, rand_samp(df,'SkinThickness')], axis=1) # view_rand_samp(df,'Age')\ndf = pd.concat([df, rand_samp(df,'Insulin')], axis=1) # view_rand_samp(df,'Age')\ndf = pd.concat([df, rand_samp(df,'BMI')], axis=1) # view_rand_samp(df,'Age')\n","56e5a00c":"df.drop(['BloodPressure','SkinThickness','Insulin','BMI'], axis=1, inplace=True)","6b586b61":"plt.figure(figsize=(15,15))\ncor = round(df.corr(),2)\nsns.heatmap(cor, annot=True);","26dd7a32":"cor['Outcome'].sort_values(ascending=False)","91e5332e":"# we'll just select these functions\n\nX = df[['Glucose','BMI_rand_samp','Age','SkinThickness_mean','Pregnancies','Insulin_mean','BloodPressure_median','DiabetesPedigreeFunction']].copy()\ny = df['Outcome']","52dad4b7":"# lets make a model that predicts our missing values\n\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.neighbors import KNeighborsRegressor\n# from sklearn.linear_model import LinearRegression\n\n# df = pd.read_csv(\"diabetes.csv\")\n# def filling_model(df1):\n    \n#     df = df1.copy()\n#     values = {}\n#     all_cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n    \n#     for col in ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']:\n#         model_col = [x for x in all_cols if x != col]\n#         test = df[df[col] == 0].copy()\n#         train = df[df[col] !=0 ].copy()\n#         for i in [x for x in [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"] if x != col]:\n#             train[i].replace(0,np.nan,inplace=True)\n#         train.dropna(inplace=True)\n#         x_train,x_test,y_train = train[model_col],test[model_col],train[col]\n#         model = LinearRegression()\n#         model.fit(x_train,y_train)\n#         values[col] = model.predict(x_test)\n#     return values\n\n# def update_predictions(row):\n#     for col in ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']:\n#         if row[col] == 0:\n#             if values[col] != []:\n#                 val = values[col][0]\n#                 values[col] = values[col][1:]\n#                 row[col] = val\n#     return row\n\n\n# values = filling_model(df)\n\n# df = pd.DataFrame(df.apply(update_predictions, axis=1))\n\n# for i in ['Glucose','BloodPressure','SkinThickness','Insulin']:\n#     df[i] = df[i].fillna(df[i].mean())","3cb0081f":"# df = pd.read_csv(\"diabetes.csv\")\n# for i in ['Glucose','BloodPressure','SkinThickness','Insulin']:\n#     df[i] = df[i].fillna(df[i].mean())","a496c3d7":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(X)\n\n\nx_train,x_test,y_train,y_test = train_test_split(x_scaled, y, test_size=0.3, random_state=30)","85161e9f":"model = DecisionTreeClassifier(random_state=44)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","c0ddfc32":"model = DecisionTreeClassifier(random_state=44)\npath = model.cost_complexity_pruning_path(x_train,y_train)\nccp_alphas = path.ccp_alphas\n\nscores = []\n\nfor ccp_alpha in ccp_alphas:\n    model = DecisionTreeClassifier(ccp_alpha=ccp_alpha,random_state=44)\n    model.fit(x_train,y_train)\n    scores.append((str(ccp_alpha) + \"  \" + str(model.score(x_train,y_train)) + \"  \" + str(model.score(x_test,y_test))))\n    \n\nscores","eed5951c":"# we have to select value which looks like it generalizes our model...\n# that is where train and test scores are balanced\n# highest value is 9th element from last","185c315b":"model = DecisionTreeClassifier(ccp_alpha=0.00684675923328408,random_state=30)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","2e6e9a03":"model = RandomForestClassifier(random_state=44)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","d9d5379c":"model = LogisticRegression(random_state=44)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","c770c729":"model = xgb.XGBClassifier(random_state=44)\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","4f10aa63":"model = KNeighborsClassifier(n_neighbors=25)\nmodel.fit(x_train,y_train)\nprint(model.score(x_test,y_test))\n\n\ntest_scores = []\ntrain_scores = []\n\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(x_train,y_train)\n    \n    train_scores.append(knn.score(x_train,y_train))\n    test_scores.append(knn.score(x_test,y_test))\n    \nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))\n\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","3a75482f":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","f41d385f":"#lets try with 5 neighbors\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nprint(classification_report(y_test,y_pred))","2b796f26":"y_pred_proba = model.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","f2ecf9d3":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=5) ROC curve')\nplt.show()","daab72d0":"roc_auc_score(y_test,y_pred_proba)","d2bb86e8":"model = SVC()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","89f1fb22":"model = GaussianNB()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)","63f36358":"models = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),xgb.XGBClassifier(),KNeighborsClassifier(),GaussianNB(),SVC()]\nresults = []\nfor model in models:\n    results.append((model, cross_val_score(model, x_scaled, y, cv=5)))","a35cfb59":"results","c81d95e6":"### Scaling and spliting into train and test data","2496ddd6":"### Using decision tree model","15121e42":"### We can also use a decision tree to predict most of missing values (NOT USED)","dc43f871":"### We can apply a few different techniques to get rid of missing values","be8b44b3":"### Imports","d6b66ae9":"### logistic regression","ae063bc6":"### svm","5c8268f7":"### Naive bayes","17c328b2":"### Using mean of the values (some values were still NaN)","596bc9d5":"### xgboost","6f2bbfc2":"### we will try to use both mean, median, mode and random sampling method","64bc9cfa":"### Lets try cross val score","725ede49":"### knn","8bac3f28":"### Random forest","7ce5af94":"### If we apply logistic regression then the correlation and normality matters... but for algorithms which use decision trees it doesn't matter","b26d9d08":"### pregnancies, skinThickness, insulin, BMI, DiabetesPedigreeFunction, age are all right skewed","c183ba94":"## Trying other classification algorithms","d5d9fcec":"### In this notebook i'll be trying to use some techniques to handle missing values and try out a few algorithms","cf0e37a8":"### decision tree using hyperparameter tuning to reduce overfitting"}}