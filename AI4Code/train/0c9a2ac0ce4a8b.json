{"cell_type":{"cc650e1b":"code","20b573d8":"code","c27a4e07":"code","5dd68d23":"code","96f8e112":"code","4ff7ddf0":"code","352fff7c":"code","98d170b8":"code","8900b1df":"code","9c75c183":"code","7037c88c":"code","669b7e5a":"code","248c5607":"code","c5b8c375":"code","a5f95eec":"code","d331cc22":"code","805caf22":"code","a6c77a74":"code","044a8fd9":"code","e462d2aa":"code","15a07a75":"code","1d35d1cc":"code","bbd44043":"code","99cb7167":"code","ba491d3b":"code","b2f2e3e1":"code","f5e6127d":"code","b5d5e63e":"code","4758e239":"code","183d8fdd":"code","66b8afec":"code","20c320c0":"code","a1f0e29a":"code","01c731b1":"code","cbb98ac0":"code","15af3529":"code","9912480e":"code","5c09383a":"code","656c47e1":"markdown","114be97c":"markdown","ba757519":"markdown","3580211b":"markdown","c3e6d3c4":"markdown","2893f62d":"markdown","f09241f0":"markdown","62604fde":"markdown","34b641f5":"markdown","57686902":"markdown","d57c434a":"markdown","96ce74ec":"markdown","0335b6b7":"markdown","0d4bb043":"markdown","f50a13cf":"markdown","09928f02":"markdown","fa58a90a":"markdown","a1b0e376":"markdown"},"source":{"cc650e1b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics as st","20b573d8":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nid=test['Id']\ntrain.head()","c27a4e07":"train.info()","5dd68d23":"train.shape","96f8e112":"test.shape","4ff7ddf0":"test.info()","352fff7c":"train.isnull().sum()","98d170b8":"test.isnull().sum()","8900b1df":"train.columns","9c75c183":"test.columns","7037c88c":"total = train.isnull().sum().sort_values(ascending=False)\npercent=(train.isnull().sum()\/len(train)*100).sort_values(ascending=False)\nmiss_train_data=pd.concat([total,percent],axis=1,keys=['Total_Null','Percent'])\nmiss_train_data.head(40).style.background_gradient(cmap='viridis')","669b7e5a":"total = test.isnull().sum().sort_values(ascending=False)\npercent=(test.isnull().sum()\/len(test)*100).sort_values(ascending=False)\nmiss_test_data=pd.concat([total,percent],axis=1,keys=['Total_Null','Percent'])\nmiss_test_data.head(40).style.background_gradient(cmap='viridis')","248c5607":"train=train.drop(columns=['Id', 'PoolQC', 'Fence','MSZoning','MiscFeature','Alley'], axis=1)\ntest=test.drop(columns=['Id','PoolQC', 'Fence','MSZoning','MiscFeature','Alley'], axis=1)\ntrain.head()","c5b8c375":"train.shape","a5f95eec":"test.shape","d331cc22":"train.isna().sum()","805caf22":"cat_train=[cat for cat in train.columns if train[cat].dtype=='object']\nnum_train=[cat for cat in train.columns if train[cat].dtype=='int64' or train[cat].dtype=='float64']\n\ncat_train","a6c77a74":"cat_test=[cat for cat in test.columns if test[cat].dtype=='object']\nnum_test=[cat for cat in test.columns if test[cat].dtype=='int64' or test[cat].dtype=='float64']\ncat_test","044a8fd9":"pd.unique(test['Functional'])","e462d2aa":"pd.unique(train['Functional'])","15a07a75":"for col in cat_train:\n    train[col]=train[col].fillna(st.mode(train[col].dropna()))\n    \nfor col in cat_test:\n    test[col]=test[col].fillna(st.mode(test[col].dropna()))\n\nfor col in num_train:\n    train[col]=train[col].fillna(train[col].mean())\n    \nfor col in num_test:\n    test[col]=test[col].fillna(test[col].mean())","1d35d1cc":"train.shape","bbd44043":"test.shape","99cb7167":"from sklearn.preprocessing import LabelEncoder\nenc=LabelEncoder()\n\nfor col in cat_train:\n    train[col]=enc.fit_transform(train[col])\n    \nfor col in cat_test:\n    test[col]=enc.fit_transform(test[col])","ba491d3b":"test","b2f2e3e1":"train = train.loc[:,~train.columns.duplicated()]\ntest = test.loc[:,~test.columns.duplicated()]","f5e6127d":"X=train.drop(['SalePrice'],axis=1).values\ny=train['SalePrice'].values\ntest=test[:].values","b5d5e63e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=10)\ny_test","4758e239":"from sklearn.ensemble import GradientBoostingRegressor\nreg=GradientBoostingRegressor()\nreg.fit(X_train,y_train)","183d8fdd":"pred=reg.predict(X_test).reshape(-1,1)\npred.shape","66b8afec":"y_test=y_test.reshape(-1,1)\ny_test.shape","20c320c0":"from sklearn.metrics import accuracy_score,r2_score\nscore=r2_score(y_test,pred)\nscore","a1f0e29a":"test.shape","01c731b1":"predictions=reg.predict(test)","cbb98ac0":"predictions.shape","15af3529":"sample","9912480e":"sample['SalePrice']=predictions.astype(float)","5c09383a":"sample.to_csv('sample_submission.csv')","656c47e1":"##  Train_Test_Split with 25% testing size","114be97c":"## Finding the Data-type of each column","ba757519":"## Handling those NULL values.","3580211b":"# Data Cleaning\n###  Dropping some useless columns. Also dropping some columns with too many NULL values","c3e6d3c4":"## Basic Imputation techniques used for Data cleaning and preprocessing.\n### Using XGboost for the first time    :P   [Noob Alert]","2893f62d":"## Using the Trained Model to Predict","f09241f0":"## Finding the number of null values in each columns","62604fde":"## Finding all columns with Categorical & Numerical values\n\nGotta do some List Comprehension, where a list is created satisfying some condition.","34b641f5":"# Predicting Sales Prices ","57686902":"## Splitting X & y for training and testing our model.","d57c434a":"## Removing all duplicate columns","96ce74ec":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)","0335b6b7":"![](https:\/\/cdn.shortpixel.ai\/spai\/w_977+q_glossy+ret_img+to_webp\/https:\/\/uncorneredmarket.com\/wp-content\/uploads\/2014\/01\/4759535950_7bca6684c8.jpg)","0d4bb043":"## Finding the columns in each dataset","f50a13cf":"### **If you like this notebook and loved my work, do give an upvote.**","09928f02":"## Fetching the data (.csv files)","fa58a90a":"## Transforming the data with LabelEncoding\n","a1b0e376":"## Using XGBoost for the first time to fit the Data for modeling purpose."}}