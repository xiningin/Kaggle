{"cell_type":{"81532520":"code","a253d3b3":"code","d66b2697":"code","1a247cbf":"code","fb7f2a45":"code","caeb3ee9":"code","75e4c0e4":"code","e1d751e6":"code","55ceb672":"code","38082256":"code","53242820":"code","5a3beea4":"code","e985b94a":"code","7006fdf3":"code","fd1d256c":"code","7554f48b":"code","cf86f2eb":"code","7f307f6b":"code","abfdaff7":"code","681ffc9e":"code","86d076fb":"code","c46975ad":"code","1f63fe97":"code","94a3a4d9":"code","61882ee1":"code","f5e40dd0":"code","2c15abc7":"code","840fd449":"code","0f3ee400":"code","c8589e2a":"code","5dba4007":"code","62ab0387":"code","ead79eae":"code","e4788a8a":"code","7714e7fe":"code","1b89b1ba":"code","c200f360":"code","e4d1c020":"code","e818d67b":"code","d630026a":"code","14edfcee":"code","62c9f57d":"code","81a01248":"code","29ff48c6":"code","6538d7e5":"code","16b4f967":"code","eb8ff34e":"code","4729b95b":"code","107bad6a":"code","3c650e9d":"code","8ac57cd1":"code","14808a88":"code","4f578200":"markdown","ab718177":"markdown","9ffc792a":"markdown","7edf863f":"markdown","5506352c":"markdown","8ef48de2":"markdown","a7e736d9":"markdown","38b61044":"markdown","d7688d02":"markdown","26b27de6":"markdown","273f5ffe":"markdown","632bb587":"markdown","9b1bf504":"markdown","d2196b0e":"markdown","f205087e":"markdown","588e72fc":"markdown","29e463db":"markdown","95e30128":"markdown","7456cb34":"markdown","8a948330":"markdown","df4ff868":"markdown","a18cac6e":"markdown","76b4226f":"markdown","f7682cc1":"markdown","985d7ddd":"markdown","df8eedc9":"markdown","a738dcd1":"markdown","32f9d350":"markdown","59349b0e":"markdown","967be13c":"markdown","dd25c67e":"markdown","d551fb08":"markdown","aa53651d":"markdown","a86ce2ab":"markdown","f3b0db0e":"markdown","642a312e":"markdown","75ffa7d4":"markdown","71197587":"markdown","164821c3":"markdown","a86568b5":"markdown","93effcde":"markdown","6c18993b":"markdown","3469d64b":"markdown"},"source":{"81532520":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a253d3b3":"credits=pd.read_csv(r\"\/kaggle\/input\/tmdb-movie-metadata\/tmdb_5000_credits.csv\")\nmovies=pd.read_csv(r\"\/kaggle\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv\")\n","d66b2697":"movies.shape","1a247cbf":"credits.head()","fb7f2a45":"credits.columns","caeb3ee9":"movies.head()","75e4c0e4":"credits.rename(columns={'movie_id':'id'},inplace=True) \n#I have chanced the 'movie_id' name to 'id' so that it matches the movies dataframes 'id' column\n\n\n#Now i am going to merge credits and movies dataframe based on id\nmovies=movies.merge(credits,on='id')","e1d751e6":"movies.head()","55ceb672":"C= movies['vote_average'].mean()  #mean vote across the whole report\nC","38082256":"m= movies['vote_count'].quantile(0.9)\nm","53242820":"q_movies = movies.loc[movies['vote_count'] >= m]\nq_movies.shape","5a3beea4":"def weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # Calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","e985b94a":"q_movies['score'] = q_movies.apply(weighted_rating, axis=1)","7006fdf3":"#Sort movies based on score calculated above\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#Print the top 15 movies\nq_movies[['original_title', 'vote_count', 'vote_average', 'score']].reset_index(drop=True).head(10)","fd1d256c":"pop= movies.sort_values('popularity', ascending=False)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.barh(pop['original_title'].head(6),pop['popularity'].head(6), align='center',\n        color='skyblue')\nplt.gca().invert_yaxis()\nplt.xlabel(\"Popularity\")\nplt.title(\"Popular Movies\")","7554f48b":"movies['overview'].head(5)","cf86f2eb":"#Import TfIdfVectorizer from scikit-learn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Replace NaN with an empty string\nmovies['overview'] = movies['overview'].fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(movies['overview'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","7f307f6b":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","abfdaff7":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(movies.index, index=movies['original_title']).drop_duplicates()","681ffc9e":"pd.DataFrame(indices).head()","86d076fb":"def get_recomendation(title,cosine_sim =cosine_sim ):\n    #Get the index of the movie given its title\n    idx=indices[title]\n    #Get the list of cosine similarity scores\n    sim_scores=list(enumerate(cosine_sim[idx]))\n    #sort based on sim_score\n    sim_scores=sorted(sim_scores,key=lambda x:x[1],reverse=True)\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n    #Get the  movies indices \n    sim_scores=[i[0] for i in sim_scores]\n     \n    return movies['original_title'].iloc[sim_scores]\n        ","c46975ad":"get_recomendation('Avatar')","1f63fe97":"get_recomendation('The Dark Knight Rises')","94a3a4d9":"\n# Parse the stringified features into their corresponding python objects\nfrom ast import literal_eval\n\nfeatures = ['cast', 'crew', 'keywords', 'genres']\nfor feature in features:\n    movies[feature] = movies[feature].apply(literal_eval)","61882ee1":"movies[feature].head()","f5e40dd0":"movies.head()","2c15abc7":"movies['crew'][1]","840fd449":"# Get the director's name from the crew feature. If director is not listed, return NaN\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","0f3ee400":"movies['director'] = movies['crew'].apply(get_director)","c8589e2a":"movies['director'].head()","5dba4007":"def get_list(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    #Return empty list in case of missing\/malformed data\n    return []\n","62ab0387":"features = ['cast', 'keywords', 'genres']\nfor feature in features:\n    movies[feature] = movies[feature].apply(get_list)","ead79eae":"movies.head()","e4788a8a":"movies[['original_title','director','cast', 'keywords','genres']].head()","7714e7fe":"def clean_data(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        #Check if director exists. If not, return empty string\n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''","1b89b1ba":"# Apply clean_data function to your features.\nfeatures = ['cast', 'keywords', 'director', 'genres']\n\nfor feature in features:\n    movies[feature] = movies[feature].apply(clean_data)","c200f360":"def create_soup(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\nmovies['soup'] = movies.apply(create_soup, axis=1)","e4d1c020":"pd.DataFrame(movies['soup']).head()","e818d67b":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(movies['soup'])","d630026a":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","14edfcee":"# Reset index of our main DataFrame and construct reverse mapping as before\nmovies = movies.reset_index()","62c9f57d":"indices = pd.Series(movies.index, index=movies['original_title'])","81a01248":"def get_recomendation(title,cosine_sim =cosine_sim ):\n    #Get the index of the movie given its title\n    idx=indices[title]\n    #Get the list of cosine similarity scores\n    sim_scores=list(enumerate(cosine_sim[idx]))\n    #sort based on sim_score\n    sim_scores=sorted(sim_scores,key=lambda x:x[1],reverse=True)\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n    #Get the  movies indices \n    sim_scores=[i[0] for i in sim_scores]\n     \n    return movies['original_title'].iloc[sim_scores]","29ff48c6":"get_recomendation('Avatar',cosine_sim2)","6538d7e5":"get_recomendation('The Godfather',cosine_sim2)","16b4f967":"from surprise import Reader, Dataset, SVD\nreader = Reader()\nratings = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/ratings_small.csv')\nratings.head()","eb8ff34e":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n","4729b95b":"data","107bad6a":"\n\nfrom surprise.model_selection import cross_validate\n\n\n\n# Use the famous SVD algorithm\nsvd = SVD()\n\n# Run 5-fold cross-validation and then print results\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","3c650e9d":"\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)","8ac57cd1":"ratings[ratings['userId'] == 1]","14808a88":"svd.predict(1, 302, 3)","4f578200":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*LqFnWb-cm92HoMYBL840Ew.png)","ab718177":"To determine an appropriate value for m, the minimum votes required to be listed in the chart we will use 90th percentile as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 90% of the movies in the list.","9ffc792a":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*9TC6BrfxYttJwiATFAIFBg.png) ","7edf863f":"We are now in a good position to define our recommendation function. These are the following steps we'll follow :-\n\n* Get the index of the movie given its title.\n* Get the list of cosine similarity scores for that particular movie with all movies. Convert it into   a list of tuples where the first element is its position and the second is the similarity score.\n* Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n* Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most   similar to a particular movie is the movie itself).\n* Return the titles corresponding to the indices of the top elements.","5506352c":"Now something to keep in mind is that these popularity based recommender provide a general chart of recommended movies to all the users. They are not sensitive to the interests and tastes of a particular user. This is when we move on to a more refined system- Content Basesd Filtering","8ef48de2":"We see that our recommender has been successful in capturing more information due to more metadata and has given us (arguably) better recommendations. It is more likely that Marvels or DC comics fans will like the movies of the same production house. Therefore, to our features above we can add production_company . We can also increase the weight of the director , by adding the feature multiple times in the soup.","a7e736d9":"While our system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great. \"The Dark Knight Rises\" returns all Batman movies while it is more likely that the people who liked that movie are more inclined to enjoy other Christopher Nolan movies. This is something that cannot be captured by the present system.","38b61044":"We are now in a position to create our \"metadata soup\", which is a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords).","d7688d02":"Now enough said , let's see how to implement this. Since the dataset we used before did not have userId(which is necessary for collaborative filtering) let's load another dataset. We'll be using the Surprise library to implement SVD.","26b27de6":"*Lets  find the values for the variables v,m,r,c*","273f5ffe":"![](http:\/\/image.ibb.co\/f6mDXU\/conten.png)","632bb587":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*GUw90kG2ltTd2k_iv3Vo0Q.png)","9b1bf504":"This is of basic and very easy to implement recommender system.We are actually going to filter the movies based on most no of votes and the rating average.But we should take some care because there might be a chance to get some non popular movie for instance there is a movie called **A** which was rated by only three members and the avg rating came to be 8\/10 and there is one very popular movie called **B**which was rated by 1 thousand members and the avg rating came to be  7.8\/10 .Here if we recommend movie A keeping movie B aside we cant say that it is not a  good recommender system. So,IMDB weighted avg formula handle this kind of problems efficiently.Lets look at it...!","d2196b0e":"For movie with ID 302, we get an estimated prediction of 2.618. One startling feature of this recommender system is that it doesn't care what the movie is (or what it contains). It works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have predicted the movie.","f205087e":"It successfully avoids the problem posed by dynamic user preference as item-based CF is more static. However, several problems remain for this method. First, the main issue is scalability. The computation grows with both the customer and the product. The worst case complexity is O(mn) with m users and n items. In addition, sparsity is another concern. Take a look at the above table again. Although there is only one user that rated both Matrix and Titanic rated, the similarity between them is 1. In extreme cases, we can have millions of users and the similarity between two fairly different movies could be very high simply because they have similar rank for the only user who ranked them both.","588e72fc":"We see that over 20,000 different words were used to describe the 4800 movies in our dataset.\n\nWith this matrix in hand, we can now compute a similarity score. There are several candidates for this; such as the euclidean, the Pearson and the cosine similarity scores. There is no right answer to which score is the best. Different scores work well in different scenarios and it is often a good idea to experiment with different metrics.\n\nWe will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically, it is defined as follows:\n","29e463db":"* **Item Based Collaborative Filtering - **Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated. Likewise, the similarity can be computed with Pearson Correlation or Cosine Similarity. The major difference is that, with item-based collaborative filtering, we fill in the blank vertically, as oppose to the horizontal manner that user-based CF does. The following table shows how to do so for the movie Me Before You.","95e30128":"Since we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster.","7456cb34":"**Note:We can see that column movie_id in credits dataframe and id in movies dataframe is same.We will see what to do with it as we go further.**","8a948330":"In this recommender system the content of the movie (overview, cast, crew, keyword, tagline etc) is used to find its similarity with other movies. Then the movies that are most likely to be similar are recommended.","df4ff868":"# Content Based Filtering","a18cac6e":"Since user A and F do not share any movie ratings in common with user E, their similarities with user E are not defined in Pearson Correlation. Therefore, we only need to consider user B, C, and D. Based on Pearson Correlation, we can compute the following similarity.","76b4226f":"# Collaborative Filtering","f7682cc1":"## Plot description based Recommender\nWe will compute pairwise similarity scores for all movies based on their plot descriptions and recommend movies based on that similarity score. The plot description is given in the overview feature of our dataset. Let's take a look at the data. ..","985d7ddd":"Although computing user-based CF is very simple, it suffers from several problems. One main issue is that users\u2019 preference can change over time. It indicates that precomputing the matrix based on their neighboring users may lead to bad performance. To tackle this problem, we can apply item-based CF.","df8eedc9":"# Popularity Based Filtering","a738dcd1":"Where,\n* v is the number of votes for the movie;\n* m is the minimum votes required to be listed in the chart;\n* R is the average rating of the movie; And\n* C is the mean vote across the whole report","32f9d350":"The next steps are the same as what we did with our plot description based recommender. One important difference is that we use the CountVectorizer() instead of TF-IDF. This is because we do not want to down-weight the presence of an actor\/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense.","59349b0e":"## Credits, Genres and Keywords Based Recommender","967be13c":"* User based filtering- These systems recommend products to a user that similar users have liked. For measuring the similarity between two users we can either use pearson correlation or cosine similarity. This filtering technique can be illustrated with an example. In the following matrixes, each row represents a user, while the columns correspond to different movies except the last one which records the similarity between that user and the target user. Each cell represents the rating that the user gives to that movie. Assume user E is the target.","dd25c67e":"Note that in this dataset movies are rated on a scale of 5 unlike the earlier one.","d551fb08":"It goes without saying that the quality of our recommender would be increased with the usage of better metadata. That is exactly what we are going to do in this section. We are going to build a recommender based on the following metadata: the 3 top actors, the director, related genres and the movie plot keywords.\n\nFrom the cast, crew and keywords features, we need to extract the three most important actors, the director and the keywords associated with that movie. Right now, our data is present in the form of \"stringified\" lists , we need to convert it into a safe and usable structure","aa53651d":"![]()","a86ce2ab":"The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that our vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same.","f3b0db0e":"![](http:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/1d94e5903f7936d3c131e040ef2c51b473dd071d)","642a312e":"Our content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n\nAlso, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who she\/he is.\n\nTherefore, in this section, we will use a technique called Collaborative Filtering to make recommendations to Movie Watchers. It is basically of two types:-","75ffa7d4":"From the above table we can see that user D is very different from user E as the Pearson Correlation between them is negative. He rated Me Before You higher than his rating average, while user E did the opposite. Now, we can start to fill in the blank for the movies that user E has not rated based on other users.","71197587":"![](http:\/\/cdn-images-1.medium.com\/max\/1000\/1*jZIMJzKM1hKTFftHfcSxRw.png)","164821c3":"![](http:\/\/cdn-images-1.medium.com\/max\/1000\/1*9NBFo4AUQABKfoUOpE3F8Q.png)","a86568b5":"## **Single Value Decomposition**\nOne way to handle the scalability and sparsity issue created by CF is to leverage a latent factor model to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as how good we are in predicting the rating for items given a user. One common metric is Root Mean Square Error (RMSE). The lower the RMSE, the better the performance.\n\nNow talking about latent factor you might be wondering what is it ?It is a broad idea which describes a property or concept that a user or an item have. For instance, for music, latent factor can refer to the genre that the music belongs to. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable. The below figure illustrates this idea.","93effcde":"We see that there are 481 movies which qualify to be in this list. Now, we need to calculate our metric for each qualified movie. To do this, we will define a function, weighted_rating() and define a new feature score, of which we'll calculate the value by applying this function to our DataFrame of qualified movies:","6c18993b":"We are going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, we need a reverse mapping of movie titles and DataFrame indices. In other words, we need a mechanism to identify the index of a movie in our metadata DataFrame, given its title.","3469d64b":"**IMDB weighted avg formula:**\n![](http:\/\/image.ibb.co\/jYWZp9\/wr.png)"}}