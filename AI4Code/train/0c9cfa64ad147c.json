{"cell_type":{"0a49131d":"code","5fa6b621":"code","a111b553":"code","281c2b75":"code","e924f5df":"code","164c4014":"code","775a1292":"code","6125a4d9":"code","951fae20":"code","7ccdda51":"code","0b1cd8bb":"code","99701284":"code","84908e35":"code","8bcbcde5":"code","f1e3a146":"code","b63dc3a7":"code","daf0d8fc":"code","243cf9e1":"code","249e4942":"code","2d18dfcd":"code","6d079434":"code","7dc3aa7a":"code","d323776b":"code","bcb84280":"code","a2bee988":"code","db1beaf4":"code","920f9335":"code","b77fe97b":"code","42e420ef":"code","5f3da424":"code","db6f84a5":"code","645655c6":"code","7f6af05a":"markdown","29a5248f":"markdown","5495f8a4":"markdown"},"source":{"0a49131d":"import pandas as pd\nimport numpy as np","5fa6b621":"train = pd.read_csv('\/kaggle\/input\/machinehack-financial-risk-prediction\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/machinehack-financial-risk-prediction\/Test.csv')","a111b553":"train.shape, test.shape","281c2b75":"train.head()","e924f5df":"combine = train.append(test)\ncombine.shape","164c4014":"combine['City'].value_counts()","775a1292":"combine['Location_Score'].describe()","6125a4d9":"combine['External_Audit_Score'].value_counts()","951fae20":"combine['Internal_Audit_Score'].value_counts()","7ccdda51":"combine['Fin_Score'].value_counts()","0b1cd8bb":"combine['Loss_score'].value_counts()","99701284":"combine['Past_Results'].value_counts()","84908e35":"combine.columns","8bcbcde5":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncols = ['External_Audit_Score', 'Fin_Score', 'Internal_Audit_Score',\n        'Location_Score', 'Loss_score', 'Past_Results']\n\ncombine[cols] = scaler.fit_transform(combine[cols])","f1e3a146":"combine.head()","b63dc3a7":"X = combine[combine['IsUnderRisk'].isnull()!=True].drop(['IsUnderRisk'], axis=1)\ny = combine[combine['IsUnderRisk'].isnull()!=True]['IsUnderRisk'].reset_index(drop=True)\n\nX_test = combine[combine['IsUnderRisk'].isnull()==True].drop(['IsUnderRisk'], axis=1)\n\nX.shape, y.shape, X_test.shape","daf0d8fc":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2)","243cf9e1":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type='gbdt',\n                       max_depth=5,\n                       learning_rate=0.05,\n                       n_estimators=5000,\n                       min_child_weight=0.01,\n                       colsample_bytree=0.5,\n                       num_leaves=30,\n                       random_state=1994)\n\nmodel.fit(x_train,y_train,\n          eval_set=[(x_train,y_train),(x_val, y_val.values)],\n          eval_metric='log_loss',\n          early_stopping_rounds=100,\n          verbose=200)\n\npred_y = model.predict_proba(x_val)","249e4942":"from sklearn.metrics import log_loss\nlog_loss(y_val, pred_y)","2d18dfcd":"pred_test = model.predict_proba(X_test)","6d079434":"import lightgbm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nlightgbm.plot_importance(model)","7dc3aa7a":"X.shape","d323776b":"err = []\ny_pred_tot_lgm = []\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfold = StratifiedKFold(n_splits=15, shuffle=True, random_state=2020)\ni = 1\nfor train_index, test_index in fold.split(X, y):\n    x_train, x_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y[train_index], y[test_index]\n    m = LGBMClassifier(boosting_type='gbdt',\n                       max_depth=5,\n                       learning_rate=0.05,\n                       n_estimators=5000,\n                       min_child_weight=0.01,\n                       colsample_bytree=0.5,\n                       num_leaves=30,\n                       random_state=1994)\n    m.fit(x_train, y_train,\n          eval_set=[(x_train,y_train),(x_val, y_val)],\n          early_stopping_rounds=200,\n          eval_metric='log_loss',\n          verbose=200)\n    pred_y = m.predict_proba(x_val)\n    print(\"err_lgm: \",log_loss(y_val,pred_y))\n    err.append(log_loss(y_val, pred_y))\n    pred_test = m.predict_proba(X_test)\n    i = i + 1\n    y_pred_tot_lgm.append(pred_test)","bcb84280":"np.mean(err,0)","a2bee988":"from xgboost import XGBClassifier\n\nerrxgb = []\ny_pred_tot_xgb = []\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfold = StratifiedKFold(n_splits=15, shuffle=True, random_state=2020)\ni = 1\nfor train_index, test_index in fold.split(X,y):\n    x_train, x_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y[train_index], y[test_index]\n    m = XGBClassifier(boosting_type='gbdt',\n                      max_depth=5,\n                      learning_rate=0.07,\n                      n_estimators=5000,\n                      random_state=1994)\n    m.fit(x_train, y_train,\n          eval_set=[(x_train,y_train),(x_val, y_val)],\n          early_stopping_rounds=200,\n          eval_metric='logloss',\n          verbose=200)\n    pred_y = m.predict_proba(x_val)\n    print(\"err_xgb: \",log_loss(y_val,pred_y))\n    errxgb.append(log_loss(y_val, pred_y))\n    pred_test = m.predict_proba(X_test)\n    i = i + 1\n    y_pred_tot_xgb.append(pred_test)","db1beaf4":"np.mean(errxgb,0)","920f9335":"from catboost import CatBoostClassifier,Pool, cv\nerrCB = []\ny_pred_tot_cb = []\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfold = StratifiedKFold(n_splits=15, shuffle=True, random_state=2020)\ni = 1\nfor train_index, test_index in fold.split(X,y):\n    x_train, x_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y[train_index], y[test_index]\n    m = CatBoostClassifier(n_estimators=1000,\n                           random_state=1994,\n                           eval_metric='Logloss',\n                           learning_rate=0.5, \n                           max_depth=10)\n    m.fit(x_train, y_train,\n          eval_set=[(x_train,y_train),(x_val, y_val)],\n          early_stopping_rounds=200,\n          verbose=200)\n    pred_y = m.predict_proba(x_val)\n    print(\"err_cb: \",log_loss(y_val,pred_y))\n    errCB.append(log_loss(y_val,pred_y))\n    pred_test = m.predict_proba(X_test)\n    i = i + 1\n    y_pred_tot_cb.append(pred_test)","b77fe97b":"np.mean(errCB, 0)","42e420ef":"(np.mean(errxgb, 0) + np.mean(err, 0) + np.mean(errCB, 0))\/3","5f3da424":"submission = pd.DataFrame((np.mean(y_pred_tot_lgm, 0)+np.mean(y_pred_tot_xgb, 0)+np.mean(y_pred_tot_cb, 0))\/3)\nsubmission.to_excel('Submission.xlsx', index=False)","db6f84a5":"submission.shape","645655c6":"submission.head()","7f6af05a":"Github Link: https:\/\/github.com\/bilalProgTech\/online-data-science-ml-challenges\/tree\/master\/Machine-Hack-Financial-Risk-Prediction ","29a5248f":"For any organization, even the slightest chance of financial risk can not be ignored. Organizations conduct regular inspections on their expenditures and revenue to make sure that they do not fall below the critical limit. In this hackathon, you as a data scientist must use the given data to predict whether an organization is under a possible financial risk or not.\n\nGiven are 7 distinguishing factors that can provide insight into whether an organization may face a financial risk or not. Your objective as a data scientist is to build a machine learning model that can predict if an organization will fall under the risk using the given features.","5495f8a4":"# Problem Statement"}}