{"cell_type":{"b43b07b2":"code","40cfe6e1":"code","54fe6939":"code","bd38690f":"code","3ec4bcdb":"code","84f8d575":"code","14470053":"code","a000a0f3":"code","4fa16fa6":"code","295fcca3":"code","586f8084":"code","6dca672a":"code","ee741a8d":"code","bfa27f1b":"code","1b165c1b":"code","70b330ae":"code","920bc225":"code","6aa9b65f":"code","5b680b60":"code","8b6b2f3e":"code","d3a23f15":"code","5045a7f7":"code","c5186688":"code","90e1c43d":"code","0a0ab33a":"code","22be8a9e":"code","8f05ba27":"code","6d05cc7b":"code","3fccc313":"code","499fc675":"code","98decca1":"code","4cf2d5e1":"code","ee94a296":"code","5bcf1b40":"code","1d3f03e0":"code","a8ee9692":"code","671e62f5":"code","8294a972":"code","af109ec8":"code","188bb973":"code","88d6d4a8":"code","bc53f28d":"code","1e9283ac":"code","7c2ba46a":"code","8809a6f7":"code","4668d33a":"code","08326756":"code","b8cae984":"code","8438f852":"code","b282e811":"code","bbd239e6":"code","7e22374f":"code","2b02b316":"code","528747b0":"code","71100103":"code","026e8937":"code","885f4824":"code","a54e6016":"code","365a8e45":"code","e685951d":"code","eca6f202":"code","72c9ca20":"code","1b31a214":"code","f3aa37fb":"code","5e4fec23":"code","7a18944a":"code","bbebe89f":"code","c5aa05a9":"code","aab5bd8f":"code","ccf5783d":"code","fc512078":"code","df89245d":"code","8d05285c":"code","39241d21":"code","dfea2435":"code","10b945c9":"code","c5c3934b":"code","3ded4a25":"code","63251c79":"code","2f43729f":"markdown","62a659f1":"markdown","9c131e6f":"markdown","b584e439":"markdown","98bf8e2c":"markdown","04b9aea3":"markdown","59879393":"markdown","cb10ab6d":"markdown","ade5f3c0":"markdown","84606a52":"markdown","2433aec9":"markdown","40835d60":"markdown","599ad1ec":"markdown","4ce5f1e3":"markdown","55b77bb2":"markdown","61817817":"markdown","59adc3d3":"markdown","8c2ac7a7":"markdown","f3441a63":"markdown","4972ae8d":"markdown","1f4904c3":"markdown","465b43e8":"markdown","41456680":"markdown","837982b5":"markdown","88c402a3":"markdown","b49d83f3":"markdown","defa4165":"markdown","b2e8117b":"markdown","fafaacc9":"markdown","af4e22e1":"markdown","c0ead9e9":"markdown","b8f0b1d2":"markdown","48644473":"markdown","46e3ba3f":"markdown","00ca9097":"markdown","7cc9544b":"markdown","bd2cdfe2":"markdown","1312223b":"markdown","cd0c1967":"markdown","91a3fc78":"markdown","2ce83180":"markdown","7156b9fd":"markdown","7355e999":"markdown","5e687cba":"markdown","d8c549b0":"markdown","7bc3c7a8":"markdown","84a745a8":"markdown","03d4d0ec":"markdown","010ca2fc":"markdown","255e820f":"markdown","cbcf8b6c":"markdown","aef62c3e":"markdown","2d81bf9f":"markdown","c3aeb5f3":"markdown","80a20ef6":"markdown","5242b237":"markdown"},"source":{"b43b07b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40cfe6e1":"import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (10,6)\n\nimport seaborn as sns\nimport random","54fe6939":"df = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/train.csv\")","bd38690f":"df.info()","3ec4bcdb":"df_test = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/test.csv\")","84f8d575":"df_test.info()","14470053":"df_submit = pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv\")","a000a0f3":"df_submit.info()","4fa16fa6":"df.head()","295fcca3":"df_test.head()","586f8084":"df_submit.head()","6dca672a":"assert df.duplicated().sum() == 0\nassert df_test.duplicated().sum() == 0\nassert df_submit.duplicated().sum() == 0","ee741a8d":"assert df.id.duplicated().sum() == 0\nassert df_test.id.duplicated().sum() == 0\nassert df_submit.id.duplicated().sum() == 0","bfa27f1b":"df_missing = pd.DataFrame(df.isna().sum() \/ len(df), columns=[\"train\"])\ndf_missing.drop(\"song_popularity\", inplace=True)\ndf_missing[\"test\"] = df_test.isna().sum() \/ len(df_test)\ndf_missing.plot.bar()\nplt.title(\"Missing values per column\", size=14)\nplt.xlabel(\"Features\", size=12)\nplt.ylabel(\"Share of missing values\", size=12);","1b165c1b":"df[\"nulls\"] = df.isna().sum(axis=1)\ndf_test[\"nulls\"] = df_test.isna().sum(axis=1)","70b330ae":"df_tmp = pd.DataFrame(df[\"nulls\"].value_counts().sort_index())\ndf_tmp[\"nulls_test\"] = df_test[\"nulls\"].value_counts().sort_index().values\ndf_tmp.plot.bar()\nplt.title(\"Number of missing values per row\", size=14)\nplt.xticks(rotation=0)\nplt.legend(labels=[\"train\", \"test\"])\nplt.xlabel(\"Number of missing values\", size=12)\nplt.ylabel(\"Number of rows\", size=12);","920bc225":"df.drop(columns=\"nulls\", inplace=True)\ndf_test.drop(columns=\"nulls\", inplace=True)","6aa9b65f":"assert df.song_popularity.isna().sum() == 0","5b680b60":"df.song_popularity.value_counts()","8b6b2f3e":"(df.song_popularity.value_counts() \/ len(df)).plot.bar(color = ['tab:blue', \"tab:orange\"])\nplt.title(\"Target distribution\", size=14)\nplt.xlabel(\"Popularity\", size=12)\nplt.ylabel(\"Share\", size=12)\nplt.xticks(rotation=0);","d3a23f15":"feature_cols = df.columns[1:-1]\nprint(f\"We have {len(feature_cols)} features in total\")","5045a7f7":"for c in feature_cols:\n    print(c, df[c].nunique())","c5186688":"cat_features = [\"key\", \"audio_mode\", \"time_signature\"]\nnum_features = [c for c in feature_cols if not c in cat_features]","90e1c43d":"print(f\"Number of categorical features: {len(cat_features)}\")\nprint(f\"Number of numerical features: {len(num_features)}\")","0a0ab33a":"def plot_feature_dist(df, df_test, feature_col):\n    df_tmp = pd.DataFrame(df[feature_col].value_counts(normalize=True).sort_index())\n    df_tmp[feature_col+\"_test\"] = df_test[feature_col].value_counts(normalize=True).sort_index().values\n    df_tmp.plot.bar()\n    plt.title((\" \".join(feature_col.split(\"_\")+[\"feature distribution\"])).title(), size=14)\n    plt.xlabel(\"Categories\", size=12)\n    plt.ylabel(\"Shares\", size=12)\n    plt.xticks(rotation=0)\n    plt.legend(labels=[\"train\", \"test\"])\n    plt.show()","22be8a9e":"plot_feature_dist(df, df_test, \"key\")","8f05ba27":"len(df[df.key == 11.0]), len(df_test[df_test.key == 11.0])","6d05cc7b":"df[\"key\"] = df[\"key\"].apply(lambda x: 3.0 if x == 11.0 else x)\ndf_test[\"key\"] = df_test[\"key\"].apply(lambda x: 3.0 if x == 11.0 else x)","3fccc313":"plot_feature_dist(df, df_test, \"audio_mode\")","499fc675":"plot_feature_dist(df, df_test, \"time_signature\")","98decca1":"df[\"time_signature\"].value_counts().sort_index()","4cf2d5e1":"df[\"time_signature\"] = df[\"time_signature\"].apply(lambda x: 2 if x == 5 else x)\ndf_test[\"time_signature\"] = df_test[\"time_signature\"].apply(lambda x: 2 if x == 5 else x)","ee94a296":"df[\"time_signature\"].value_counts().sort_index()","5bcf1b40":"fig, axes = plt.subplots(1, len(cat_features), figsize=(24, 5))\nfor c in range(len(cat_features)):\n    col = cat_features[c]\n    sns.countplot(x=col, data=df, hue=\"song_popularity\", ax=axes[c])\n    axes[c].set_title((\" \".join(col.split(\"_\"))).title(), size=14)\n    if c==1:\n        axes[c].set_xlabel(\"Categories\", size=12)\n    else:\n        axes[c].set_xlabel(\"\")\n    if c==0:\n        axes[c].set_ylabel(\"Count\", size=12)\n    else:\n        axes[c].set_ylabel(\"\")\n    axes[c].legend(labels=[\"popular\", \"unpopular\"])\nplt.show()","1d3f03e0":"df[num_features].describe().T.sort_values(by=\"std\", ascending=False).style.background_gradient()","a8ee9692":"df_test[num_features].describe().T.sort_values(by=\"std\", ascending=False).style.background_gradient()","671e62f5":"def create_dist_plots(df, features, hue):\n    plt.figure(figsize = (16,14))\n    for i in enumerate(df[features].columns):\n\n        plt.subplot(4,3,i[0]+1)\n        sns.kdeplot(data=df, x=i[1], hue=hue, fill=\"#003389\", color=\"#003389\")\n        plt.title (i[1], color='maroon')\n        plt.xlabel(\" \")\n        plt.ylabel(\" \")\n        plt.xticks(rotation = 45)\n        plt.tight_layout()","8294a972":"df[\"split\"] = \"train\"\ndf_test[\"split\"] = \"test\"\ndf_full = pd.concat([df, df_test]).reset_index(drop=True)","af109ec8":"create_dist_plots(df_full, num_features, \"split\")","188bb973":"from sklearn.preprocessing import MinMaxScaler","88d6d4a8":"sc = MinMaxScaler()","bc53f28d":"df[num_features] = sc.fit_transform(df[num_features])\ndf_test[num_features] = np.clip(sc.transform(df_test[num_features]), 0, 1)\ndf[\"split\"] = \"train\"\ndf_test[\"split\"] = \"test\"\ndf_full = pd.concat([df, df_test]).reset_index(drop=True)","1e9283ac":"create_dist_plots(df_full, num_features, \"split\")","7c2ba46a":"num_log_features = []\nfor c in num_features:\n    num_log_features.append(c+\"_log\")\n    df_full[num_log_features[-1]] = np.log(np.clip(df_full[c], df_full[df_full[c] != 0][c].min(), 1))","8809a6f7":"create_dist_plots(df_full, num_log_features, \"split\")","4668d33a":"create_dist_plots(df, num_features, \"song_popularity\")","08326756":"plt.figure(figsize = (14,12))\ncorrelations=df.iloc[:,1:].corr()\nsns.heatmap(correlations.T, mask=np.tril(correlations), cmap=\"viridis\", annot=True)\nplt.xticks(size = 12)\nplt.yticks(size = 12)\nplt.show()","b8cae984":"def interaction_plot(data, x_feature, y_feature, target=\"song_popularity\"):\n    sns.scatterplot(x=x_feature, y=y_feature, data=data, hue=target)\n    plt.xlabel(x_feature.title(), size=12)\n    plt.ylabel(y_feature.title(), size=12)\n    plt.show()","8438f852":"interaction_plot(df, \"energy\", \"loudness\")","b282e811":"interaction_plot(df, \"energy\", \"acousticness\")","bbd239e6":"from sklearn.model_selection import StratifiedKFold","7e22374f":"df[\"fold\"] = -1","2b02b316":"n_folds = 5\nkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)","528747b0":"for i, (train_idx, valid_idx) in enumerate(kf.split(df, df[\"song_popularity\"].values)):\n    print(f\"Fold {i}: train {len(train_idx)}, valid {len(valid_idx)}\")\n    df.loc[valid_idx, \"fold\"] = i","71100103":"print(\"Share of positive targets\")\nfor i in range(n_folds):\n    train_pos_frac = round(df[df.fold != i].song_popularity.value_counts(normalize=True)[1],4)\n    valid_pos_frac = round(df[df.fold == i].song_popularity.value_counts(normalize=True)[1],4)\n    print(f\"Fold {i}: train {train_pos_frac}, valid {valid_pos_frac}\")","026e8937":"df_test[\"fold\"] = -1\ndf_full = pd.concat([df, df_test]).reset_index(drop=True)","885f4824":"df_full.to_csv(\"song_popularity_train_test_folds.csv\", index=False)","a54e6016":"from sklearn.impute import SimpleImputer","365a8e45":"imptr = SimpleImputer(strategy=\"mean\", add_indicator=True)","e685951d":"na_cols = [c for c in num_features if df_full[c].isna().sum() > 0] # select numerical columns with missing values\nna_ind_cols = [c+\"_na\" for c in na_cols] # create na indicator columns\nfeature_cols = list(feature_cols) + na_ind_cols # extend feature columns with newly created na indicator columns","eca6f202":"imputed = pd.DataFrame(imptr.fit_transform(df_full[na_cols]), columns = na_cols + na_ind_cols)","72c9ca20":"df_full = pd.concat([df_full.drop(columns=na_cols), imputed], axis=1)","1b31a214":"from sklearn.preprocessing import LabelEncoder","f3aa37fb":"target = \"song_popularity\"","5e4fec23":"for c in cat_features + na_ind_cols + [target]:\n    if df_full[c].isna().sum() > 0:\n        df_full.loc[:,c] = df_full[c].astype(str).fillna(\"NONE\")\n    enc = LabelEncoder()\n    df_full.loc[:,c] = enc.fit_transform(df_full[c])","7a18944a":"df_full.info()","bbebe89f":"df_full.head()","c5aa05a9":"df = df_full[df_full.split == \"train\"].reset_index(drop=True)\ndf_test = df_full[df_full.split == \"test\"].reset_index(drop=True)","aab5bd8f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score","ccf5783d":"for i in range(n_folds):\n    \n    df_train = df[df.fold != i].reset_index(drop=True)\n    df_valid = df[df.fold == i].reset_index(drop=True)\n    \n    X_train = df_train[feature_cols].values\n    y_train = df_train[target].values\n    X_valid = df_valid[feature_cols].values\n    y_valid = df_valid[target].values\n    X_test = df_test[feature_cols].values\n    \n    model = RandomForestClassifier(n_estimators=40,\n                                   max_features=0.5,\n                                   min_samples_leaf=100,\n                                   n_jobs=-1)\n    \n    model.fit(X_train, y_train)\n    \n    train_preds = model.predict_proba(X_train)[:,1]\n    train_auc = roc_auc_score(y_train, train_preds)\n    valid_preds = model.predict_proba(X_valid)[:,1]\n    valid_auc = roc_auc_score(y_valid, valid_preds)\n    print(f\"Fold {i} AUC: train {train_auc}, valid {valid_auc}\") \n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    \n    if i == 0:\n        all_valid_preds = valid_preds\n        all_y_valid = y_valid\n        all_valid_ids = df_valid[\"id\"].values\n        all_test_preds = np.expand_dims(test_preds, axis=1)\n        \n    else:\n        all_valid_preds = np.concatenate([all_valid_preds, valid_preds])\n        all_y_valid = np.concatenate([all_y_valid, y_valid])\n        all_valid_ids = np.concatenate([all_valid_ids, df_valid[\"id\"].values])\n        all_test_preds = np.concatenate([all_test_preds, np.expand_dims(test_preds, axis=1)], axis=1)","fc512078":"valid_exp = pd.DataFrame(data={\"id\": all_valid_ids,\n                              \"target\": all_y_valid,\n                              \"preds\": all_valid_preds})","df89245d":"roc_auc_score(valid_exp.target.values, valid_exp.preds.values)","8d05285c":"valid_exp.to_csv(\"cv_scores_rf_baseline.csv\", index=False)","39241d21":"df_test[\"preds\"] = np.mean(all_test_preds, axis=1)","dfea2435":"sns.kdeplot(data = df_test, x = \"preds\", fill=\"#003389\", color=\"#003389\")\nplt.title(\"Distribution of predicted probabilities\", size=14)\nplt.xlabel(\"Probability for popular songs\", size=12);","10b945c9":"assert (df_test.id.values == df_submit.id.values).mean() == 1","c5c3934b":"df_submit[\"song_popularity\"] = df_test.preds.values","3ded4a25":"df_submit.head()","63251c79":"df_submit.to_csv(\"submission_rf_baseline.csv\", index=False)","2f43729f":"Since many features seem to be roughly distributed between 0 and 1 (and values below and above might be noise), let's apply `MinMaxScaler`","62a659f1":"## Credits","9c131e6f":"The relationship between categories and target seems similar across all categorical features","b584e439":"Valid AUC across all cross validation folds","98bf8e2c":"**Missing values by row**","04b9aea3":"Define helper function to plot categorical features in train and test sets","59879393":"Again, missing values appear to be similarly distributed in train and test sets with max 6 missing values per row (in very few cases)","cb10ab6d":"Check that there are no duplicates","ade5f3c0":"## Preprocessing","84606a52":"## EDA","2433aec9":"## Features","40835d60":"### Duplicates","599ad1ec":"This part might be better done within the cross validation loop but for simplicity reasons we will apply it to the entire dataset","4ce5f1e3":"There is some imbalance in the target variable but nothing too extreme","55b77bb2":"**Comparison of train and test distributions**","61817817":"Creating folds","59adc3d3":"## Baseline","8c2ac7a7":"Verify that target distribution is the same across train\/valid splits and folds","f3441a63":"We have roghly the same share of missing values (~10% in 8 feature columns) in train and test set","4972ae8d":"**Missing values by column**","1f4904c3":"Check number of unique features","465b43e8":"Replace missing values in key column and apply label encoder to all categorical columns (including newly created na indicator columns)","41456680":"Let's start with descriptive statistics","837982b5":"## Cross validation","88c402a3":"Make sure there are no missing values in the target","b49d83f3":"Save test predictions for submission. We will take the average of each model's test set predictions","defa4165":"Now let's look at interactions","b2e8117b":"### Categorical features","fafaacc9":"## Load data","af4e22e1":"### Numerical features","c0ead9e9":"This leaves us with a 80\/20 train\/valid split in each fold","b8f0b1d2":"Check if there are duplicates in the id column","48644473":"**Log transformations**","46e3ba3f":"Save all validation predictions","00ca9097":"Let's now create a cross validation loop with random forest as a simple baseline","7cc9544b":"**Target impact of categorical variables**\n\nInspired by https:\/\/www.kaggle.com\/odins0n\/song-popularity-prediction-eda-modelling","bd2cdfe2":"# Song Popularity Prediction - EDA and Baseline","1312223b":"There are not many strong correlations among the features, especially not between any features and the target variable","cd0c1967":"## Interactions","91a3fc78":"Replacing missing values in numerical features with their mean","2ce83180":"### Missing values","7156b9fd":"From analyzing individual numerical feautres, there are no clear relationships visible between features and target variable","7355e999":"Save the dataset to use the same folds in other notebooks","5e687cba":"Looking at the strongest correlations in more detail","d8c549b0":"There is a similar problem in time signature as in key. Again, since I don't know if there is a meaningful order in this category, I think the best option here is to at least group the two smallest categories into one.","7bc3c7a8":"Taken from https:\/\/www.kaggle.com\/snikhil17\/song-popularity-eda-coding-fun-python-version","84a745a8":"This notebook draws inspiration for these great notebooks:\n- https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun\n- https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream\n- https:\/\/www.kaggle.com\/snikhil17\/song-popularity-eda-coding-fun-python-version\n- https:\/\/www.kaggle.com\/odins0n\/song-popularity-prediction-eda-modelling\n\n...as well as Abhishek's awesome book: https:\/\/github.com\/abhishekkrthakur\/approachingalmost\n\nThanks for reading my notebook! If you found it helpful, please give it an upvote :)","03d4d0ec":"**Target impact of numerical features**","010ca2fc":"Create 5 splits using stratified k-fold cross validation since the target variable is imbalanced","255e820f":"### Target","cbcf8b6c":"Key, audio mode and time signature are definitely categorical features","aef62c3e":"Save predictions for later","2d81bf9f":"Surprisingly, there are very few songs in key eleven (117) while all other keys have more than thousand songs. We cannot be sure that there is a meaningful order in the numbered keys. All we know is that key 11 is very rare, so my intuition tells me that it would be a good idea to group it together with the second rarest key (3).","c3aeb5f3":"### Looking at the data","80a20ef6":"## Inference","5242b237":"**Feature scaling**"}}