{"cell_type":{"7d0a1157":"code","e6500b14":"code","88c65616":"code","2c4b43fe":"code","a8c514ea":"code","0d42d060":"code","d7ba60d1":"code","2568b4c3":"code","a9d701f0":"code","8dd9f699":"code","d24a5e72":"code","7b14159f":"code","ae0e3ed1":"code","e7115d57":"code","b44c1e04":"code","8c91ed31":"code","d3cf10a2":"code","8d281125":"code","79dd8821":"code","ee28f302":"code","97e8899d":"code","378d3f9d":"code","a9e7a217":"code","e2ff4728":"code","20202dc6":"code","c5d1ef12":"code","cdeac636":"code","acde4244":"code","5cae12ec":"code","7d4ab93b":"code","7efec952":"code","a964b374":"code","e38b9551":"code","2c1b86cd":"code","3dcc592a":"code","2b4d57c7":"code","e396a66c":"code","580ceecf":"code","2dbbd989":"code","35fb7580":"code","af57ae17":"code","95dfee65":"code","c2e8dbd2":"code","d9cef66f":"code","fe459f4b":"code","4a3c4cd3":"code","b6a73897":"code","96d2fc17":"code","e658d9a7":"code","b8f5a247":"code","e70b02dc":"code","5b729d97":"code","cc722b4a":"code","9e97f191":"code","b6bd7b9b":"code","9a313fc4":"code","7b2f3398":"code","85396670":"code","3bba10a5":"code","ff73dc39":"code","275e30bd":"code","ed191156":"code","387ef473":"markdown","9a510c9f":"markdown","9b35700c":"markdown","bc8ac99c":"markdown","5a48b021":"markdown","85dd8738":"markdown"},"source":{"7d0a1157":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6500b14":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","88c65616":"train = pd.read_csv('\/kaggle\/input\/ecommerce-data\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ecommerce-data\/Test.csv')\n","2c4b43fe":"train.head()","a8c514ea":"sns.distplot(train['Selling_Price'])","0d42d060":"train['Selling_Price'] = np.log1p(train['Selling_Price'])","d7ba60d1":"sns.distplot(train['Selling_Price'])","2568b4c3":"sns.boxplot(train['Selling_Price'])","a9d701f0":"train.isnull().sum()","8dd9f699":"train.describe()","d24a5e72":"train.describe(include=\"O\")","7b14159f":"train.nunique()","ae0e3ed1":"train.shape","e7115d57":"def create_date_features(df):\n    df['Date']=pd.to_datetime(df['Date'], format= '%Y%m%d', errors = 'ignore')\n   \n    df['Year'] = pd.to_datetime(df['Date']).dt.year\n    df['Month'] = pd.to_datetime(df['Date']).dt.month\n    df['Day'] = pd.to_datetime(df['Date']).dt.day\n    df['Dayofweek'] = pd.to_datetime(df['Date']).dt.dayofweek\n    df['DayOfyear'] = pd.to_datetime(df['Date']).dt.dayofyear\n    df['Week'] = pd.to_datetime(df['Date']).dt.week \n    df['Quarter'] = pd.to_datetime(df['Date']).dt.quarter  \n    df['Is_month_start'] = pd.to_datetime(df['Date']).dt.is_month_start \n    df['Is_month_end'] = pd.to_datetime(df['Date']).dt.is_month_end \n    df['Is_quarter_start'] = pd.to_datetime(df['Date']).dt.is_quarter_start\n    df['Is_quarter_end'] = pd.to_datetime(df['Date']).dt.is_quarter_end \n    df['Is_year_start'] = pd.to_datetime(df['Date']).dt.is_year_start \n    df['Is_year_end'] = pd.to_datetime(df['Date']).dt.is_year_end\n#     df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)   \n#     df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\n#     df['Days_in_month'] = pd.to_datetime(df['Date']).dt.days_in_month \n    return df","b44c1e04":"train = create_date_features(train)\ntest = create_date_features(test)","8c91ed31":"train","d3cf10a2":"train['Unique_Item_category_per_product_brand'] = train.groupby(['Product_Brand'])['Item_Category'].transform('nunique')","8d281125":"train['Unique_Subcategory_1_per_product_brand']=train.groupby(['Product_Brand'])['Subcategory_1'].transform('nunique')\ntrain['Unique_Subcategory_2_per_product_brand']=train.groupby(['Product_Brand'])['Subcategory_2'].transform('nunique')\n\ntest['Unique_Item_category_per_product_brand']=test.groupby(['Product_Brand'])['Item_Category'].transform('nunique')\ntest['Unique_Subcategory_1_per_product_brand']=test.groupby(['Product_Brand'])['Subcategory_1'].transform('nunique')\ntest['Unique_Subcategory_2_per_product_brand']=test.groupby(['Product_Brand'])['Subcategory_2'].transform('nunique')","79dd8821":"train.head()","ee28f302":"# calc = df.groupby(['Product_Brand'], axis=0).agg({'Product_Brand':['count']}).reset_index() \n# calc\n# calc.columns = ['Product_Brand','Product_Brand Count']\n# calc\n# df = df.merge(calc, on=['Product_Brand'], how='left')\n# df","97e8899d":"train","378d3f9d":"total = pd.concat([train,test], axis=0)","a9e7a217":"total['Product_Brand'] = total['Product_Brand'].str.lstrip('B-').astype(int)","e2ff4728":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()","20202dc6":"\ncat=['Item_Category','Subcategory_1','Subcategory_2']\nfor items in cat:\n    total[items]=le.fit_transform(total[items])","c5d1ef12":"total.head()","cdeac636":"train.shape","acde4244":"test.shape","5cae12ec":"train_final = total[:train.shape[0]]\ntest_final = total[train.shape[0]:]","7d4ab93b":"test_final.drop(['Product','Selling_Price','Date'], axis=1 , inplace=True)\n","7efec952":"x = train_final.drop(['Product','Selling_Price','Date'], axis=1)","a964b374":"y = train_final['Selling_Price']\ntest_final = test_final.drop(['Product','Selling_Price'], 1)","e38b9551":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.1, random_state=7)","2c1b86cd":"from sklearn.preprocessing import StandardScaler\n\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold","3dcc592a":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","2b4d57c7":"pipe = Pipeline([('scaler','passthrough'),\n                    ( 'regressor', XGBRegressor())])","e396a66c":"# param_grid = [{'regressor' : [XGBRegressor()],\n#                'learning_rate' : [0.01,0.05,0.1],\n#               'scaler' : ['passthrough']\n#               },\n              \n#              { 'regressor' : [RandomForestRegressor()],\n#               'scaler' : ['passthrough'],\n#               'regressor_max_depth' : [2,3,4],\n#               'regressor_n_estimators': [200,300],\n#               'regressor_max_features' : ['auto', 'sqrt', 'log2']\n#              }\n             \n#              ]","580ceecf":"param_grid = {'learning_rate' : [0.01,0.05,0.1,0.2,0.25]}","2dbbd989":"fold = KFold(n_splits=15, shuffle=True, random_state=42)\n# grid = GridSearchCV(XGBRegressor(), param_grid= param_grid, cv = 5)","35fb7580":"# grid.fit(X_train, y_train)","af57ae17":"model_2 = XGBRegressor(\n learning_rate =0.1,\n eval_metric='rmse',\n    n_estimators=5000,\n  \n )\n#model.fit(X_train, y_train)\nmodel_2.fit(X_train, y_train, eval_metric='rmse', \n          eval_set=[(X_test, y_test)], early_stopping_rounds=500, verbose=100)","95dfee65":"model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, eval_metric='rmse',\n             gamma=0, gpu_id=-1, importance_type='gain',\n             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n             max_depth=6, min_child_weight=1, \n             monotone_constraints='()', n_estimators=5000, n_jobs=0,\n             num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1, verbosity=None)","c2e8dbd2":"model.fit(X_train, y_train)","d9cef66f":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(X=X_train,y=y_train,estimator=model,scoring='neg_root_mean_squared_error',cv=fold)","fe459f4b":"np.mean(score)","4a3c4cd3":"test_final = sc.fit_transform(test_final)","b6a73897":"y_pred1=model.predict(test_final)","96d2fc17":"\ny_pred1=np.expm1(y_pred1)\n","e658d9a7":"y_pred1","b8f5a247":"from lightgbm import LGBMRegressor\nlgb_fit_params={\"early_stopping_rounds\":100, \n            \"eval_metric\" : 'rmse', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose':100\n           }\n\nlgb_params = {'boosting_type': 'gbdt',\n 'objective': 'regression',\n 'metric': 'rmse',\n 'verbose': 0,\n 'bagging_fraction': 0.8,\n 'bagging_freq': 1,\n 'lambda_l1': 0.01,\n 'lambda_l2': 0.01,\n 'learning_rate': 0.05,\n 'max_bin': 255,\n 'max_depth': 6,\n 'min_data_in_bin': 1,\n 'min_data_in_leaf': 1,\n 'num_leaves': 31}\n","e70b02dc":"clf_lgb = LGBMRegressor(n_estimators=10000, **lgb_params, random_state=123456789, n_jobs=-1)\nclf_lgb.fit(X_train, y_train, **lgb_fit_params)\n\n\n","5b729d97":"int(clf_lgb.best_iteration_)","cc722b4a":"model_lgbm = LGBMRegressor(bagging_fraction=0.8, bagging_freq=1, lambda_l1=0.01,\n              lambda_l2=0.01, learning_rate=0.05, max_bin=255, max_depth=6,\n              metric='rmse', min_data_in_bin=1, min_data_in_leaf=1,\n              n_estimators=170)","9e97f191":"model_lgbm.fit(X_train, y_train)","b6bd7b9b":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(X=X_train,y=y_train,estimator=model_lgbm,scoring='neg_root_mean_squared_error',cv=5)","9a313fc4":"np.mean(score)","7b2f3398":"y_pred2=model_lgbm.predict(test_final)","85396670":"\ny_pred2=np.expm1(y_pred2)","3bba10a5":"y_pred=(0.3*y_pred1)+(y_pred2*0.7)\ny_pred","ff73dc39":"from sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom math import sqrt \nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nerrrf = []\ny_pred_totrf = []\n\nfold = KFold(n_splits=15, shuffle=True, random_state=42)\n\nfor train_index, test_index in fold.split(x):\n    X_train, X_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = x.loc[train_index], x.loc[test_index]\n    \n    rf = RandomForestRegressor(random_state=42, n_estimators=200)\n    rf.fit(X_train, y_train)\n\n    y_pred_rf = rf.predict(X_test)\n#     print(\"RMSLE: \", sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_rf))))\n\n#     errrf.append(sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_rf))))\n    p = rf.predict(test_final)\n    y_pred_totrf.append(p)","275e30bd":"np.mean(errrf)  ","ed191156":"# final = np.exp(np.mean(y_pred_totrf,0))","387ef473":"HIHGLY SKEWED DATA SO WE NEED IT TO DO NORMALIZATION.","9a510c9f":"****another method to do above operation****","9b35700c":"****Trying another method****","bc8ac99c":"**FINAL SCORE**","5a48b021":"****XGBoost****","85dd8738":"*LGBM*"}}