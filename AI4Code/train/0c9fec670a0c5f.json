{"cell_type":{"3f47fe65":"code","bf806e5c":"code","5f215cf0":"code","e56118b7":"code","dec59d29":"code","5f7303d1":"code","93b63a9d":"code","ff53e5fc":"code","e55b85cb":"code","af418964":"code","ee508e5c":"code","f7ce9335":"code","1413b8ae":"code","1198e0e0":"code","ff36c4a4":"code","efb3f4bb":"code","a5fa2b06":"code","f0357a55":"code","5d582466":"code","240a6438":"code","a15a7b1c":"code","e64bebd8":"code","d9f39ea1":"code","e2bae7be":"code","40532caf":"markdown","8ce411aa":"markdown","bd51d679":"markdown","d6261e66":"markdown","1bd0f567":"markdown","7d64c430":"markdown","6e1c1c85":"markdown","5448a12a":"markdown","a8455c0c":"markdown","b9ffa904":"markdown","327702ad":"markdown","5d131d17":"markdown","e91794d9":"markdown","6c642cc6":"markdown","a00a4db1":"markdown","46147669":"markdown"},"source":{"3f47fe65":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output\n# when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf806e5c":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)","5f215cf0":"# We're not building a model and generating a prediction in this notebook. The StevenFerrer prediction\n# was highest scoring XGboost hyperparameters displayed in a public notebook, so that's what's loaded here.\n# But you can change the file path to another submissions file to see if this notebook works for that one too.\n\npredictions_base = pd.read_csv(\"\/kaggle\/input\/submissionstevenferrercsv\/submissionStevenFerrer.csv\", low_memory=False)","e56118b7":"predictions_base.head()\n\n# Steven Ferrer's first 5 rows should be:\n# \tid\ttarget\n# 0\t0\t8.084795\n# 1\t5\t8.395957\n# 2\t15\t8.407892\n# 3\t16\t8.495907\n# 4\t17\t8.140703","dec59d29":"from scipy import stats\nstats.mstats.skew(train['target']).data\n# target is a roughly symmetric distribution according to this numerical test\n# visually (see below) it looks a little skewed","5f7303d1":"from scipy import stats\nstats.mstats.kurtosis(train['target'])\n# target has fewer outliers (thinner tails) than is typical for a normal distribution\n","93b63a9d":"# Statistical description of the train dataset\n# train[\"target\"].describe(percentiles=[.01,0.1, 0.25, 0.5, 0.75, 0.9, .99])\ntrain.describe()","ff53e5fc":"fig, ax = plt.subplots(figsize=(12, 6))\n\nbars = ax.hist(train[\"target\"],\n               bins=40,\n               range=(0,11),\n               color=\"orange\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","e55b85cb":"cutoff = 5\nprint(f\"{(train['target'] < cutoff).sum() \/ len(train) * 100:.3f}% of the target values are less than {cutoff}\")","af418964":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(train[\"target\"],\n               bins=3500,\n               range=(6.9,10.4),\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","ee508e5c":"cuts = [i\/1000 for i in range(6940,10400)]\ntrain[\"target\"].value_counts(bins = cuts)","f7ce9335":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               range=(8.05,8.15),\n               color=\"orange\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","1413b8ae":"cuts = [i\/1000 for i in range(8050,8150)]\ntrain[\"target\"].value_counts(bins = cuts).sort_index()[60:75]","1198e0e0":"# inverse_log=np.power(10, train[\"target\"])\ninverse_log=np.exp(train[\"target\"]) # comment this out, uncomment line above to see 10^train[\"target\"] instead of e^train[\"target\"]\n\nfig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(inverse_log,\n               range=(0,40000),\n               bins=4000,\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","ff36c4a4":"cuts = [i for i in range(0,40000,10)]\ninverse_log.value_counts(bins = cuts)","efb3f4bb":"predictions_base.head()","a5fa2b06":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(predictions_base[\"target\"],\n               bins=3500,\n               range=(6.9,10.4),\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Prediction distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Prediction value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","f0357a55":"cuts = [i\/1000 for i in range(8050,8150)]\ntrain[\"target\"].value_counts(bins = cuts).sort_index()[35:95]","5d582466":"predictions_base[\"target\"].value_counts(bins = cuts).sort_index().tail(50)","240a6438":"p = predictions_base.copy()\n\n# peak represents a value of the target which is a local maximixum frequency of occurence; above this number is a valley of far fewer occurences\n# full_valley_width represents a range of target values just above the peak\n# the full_valley_width is divided in half so experiments can be conducted on both the upper and lower bands of the valley\n\nfull_valley_width = .04\nhalf_valley_width = full_valley_width\/2\npeaks = [8.1184, 8.5241, 8.8411, 9.1089, 9.3723, 9.6729, 10.0079] # Repeating the above analysis found precise values for all 7 peaks\n\nfor peak in peaks:\n#     mask_upper = p['target'].between(peak+half_valley_width, peak+full_valley_width)\n    mask_lower = p['target'].between(peak, peak+half_valley_width)\n#     p.loc[mask_upper, \"target\"] = peak + half_valley_width\n    p.loc[mask_lower, \"target\"] = peak - .0001   \n#     p.loc[mask_upper, \"target\"] = 2 * p.loc[mask_upper, \"target\"] - (peak + full_valley_width)  # tried hard to make this spreading out effect work, but it produced worse scores\n#     p.loc[mask_lower, \"target\"] = peak - .004*np.random.randn(len(p.loc[mask_lower, \"target\"])) # 1.5 * p.loc[mask_lower, \"target\"] - .5*(peak + full_valley_width)\n#     or\n#     p.loc[mask_lower, \"target\"] = 1.5 * p.loc[mask_lower, \"target\"] - .5*(peak + full_valley_width)\n\n# I experimented with a few different formulas, but in the end found that simplest was best.\n# If you replace the last (not commented) line of the for loop with the next 2, you can get the predictions histogram\n# to look a lot more like the original target data,\n# but it produces worse results on the test data according to the leaderboard.\n","a15a7b1c":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(p[\"target\"],\n               bins=3500,\n               range=(6.9,10.4),\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Prediction distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Prediction value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","e64bebd8":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(p[\"target\"],\n               bins=100,\n#                range=(8.05,8.15),\n               range=(8.48,8.58),\n               color=\"orange\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","d9f39ea1":"new_predictions = p.copy()\nnew_predictions.to_csv('submission.csv', index=False, header=predictions_base.columns)","e2bae7be":"new_predictions.head()","40532caf":"In addition to the counts, we have the location of the bins with high counts. Let's take a close look at one first of these around 8.117:","8ce411aa":"# **Exploring the Target**\n\nWhat can be gained by looking really, really carefully at the target data?\n\nThis notebook attempts to answer this question by:\n\n* Looking at detailed historgrams of the target (with many more bins than is typical)\n* Zeroing in on anomolies\n* Hypothesizing what might explain the anomolies\n* Attempting to improve results by taking into account the anomlies\n\nWhether or not much success is obtained, there's something to be learned.","bd51d679":"Does this help anything? We can generate a sumbissions file and find out:","d6261e66":"Notice how the predictions are roughly equal from cut to cut in Steven Ferrer's prediction data. The numbers between 8.1184 and 8.1584 are more likely than not to be subject to a policy limit (or some other limiting factor), and much more likely to be adjusted downwards to 8.1184 in reality than stay the same. So let's see what happens if numbers in this range are pushed a little lower:\n\n(Note: Many different methods were tried - some made things slightly worse, some had no effect, and some led to a very slighty improvement. The method below is one of several that resulted in a slight improvement - and it's the simplest)\n","1bd0f567":"Interesting! Any number of bins 100 or higher is interesting, and the larger the number of bins, the more interesting it gets.\n\n3500 is convenient because it divides the data into bins of .001 width, which is a useful size in later analysis as us humans work with round numbers more easily than non-round numbers.\n\nUsing these round-numbered .001 increments, let's look at the exact numbers in each .001 width bin with value_counts:","7d64c430":"Target distribution is not that far off from a normal curve. With data clumped around 7 to 9, I'd guess this is the log value of the original data. \"Amount of insurance claims\" is the only thing we've been told about where this data came from, so the original amounts were likely spread out, and in the hundreds of thousands, at least if the unit currency is dollars.\n\nA look at target distribution with a small number of bins shows:","6e1c1c85":"As the Kurtosis measure already suggested, the tail is very thin (visually, we can see an especially thin tail on the left, and no tail at all on the right). Otherwise, this looks like it roughly follows a normal distribution, with not much odd going on. But . . .\n\nTry restricting the range to where most of the data is at (virtually all values fall between 6.9 to 10.4), and increasing the number of bins:","5448a12a":"The predictions file I loaded here are from Steven Ferrer's notebook which found hyperparameters for XGBoost that got a good score near the beginning of the competition. Let's look at a histogram of the predictions in Steven Ferrer's submission file:","a8455c0c":"# **Exploratory Data Analysis: Focusing on Target**","b9ffa904":"# **Data import**","327702ad":"We can visually see the steep dropoff closeup with the graph. With numbers, we see that the target has a value between 8.117 to 8.118 a total of 906 times. There is a cliff that occurs at some point between 8.118 to 8.119. Beyond that cliff, there is a valley with only 75 for each .001 thereafter, about 1\/12 as often as the peak that occured before the dropoff, and 1\/8 as often as the few bins just before the peak.\n\nSomething is going on there. Since this is about insurance claim amounts, one possibility is that popular maximum purcahsed amounts for policy limits such as 300,000, 500,000, 1 million, etc. limit the claim amount. Claim amounts may be limited for other reasons such as a maximum for a particular type of claim. Given that the target data looks like it could be a log of some kind, we can see if there are nice even numbers if we look at the inverse of the log, an exponential, as applied to the data:","5d131d17":"I've tested several submissions files generated from XGboost models, and in most instances it did make for a very small .00001 improvement on the leaderboard, when using just the lower_band adjustments. I never managed an upper_band adjustment that helped (though some types of upper-band adjustments didn't hurt).\n\nAltering predictions after they've been generated is called post-processing, and it's possible the way it's done here is not good practice. There are probably many other ways to use the observations of the 7 peaks for benefit.\n\nThe way I experimented most was with preprocessing by eliminating certain rows:\n\nI tried removing rows (or sometimes half of the rows) just before the peak as part of preprocessing before fitting a model. I found that with some widths, this led to results which were about the same or a hair better, but when the widths were wider than .003, results were always worse. While this finding is mildly interesting (dropping thousands of rows just before peak(s) had little impact on results, which suggests that that something about these rows are not helpful to model building), I was never able to see enough benefit to siginficaly effect standings on the leaderboard.\n\nIt may be that the perecentage of data before cliffs is just not big enough to really matter in the overall results, as 4.6% of the data is within a .005 width band before one of the 7 peaks (and most of that is the first two peaks). One could argue that the width is quite a bit larger than .005, but only very close to the peak is the amount of artificially clipped target values much higher than what would naturally occur in that value range. So any adjustments made to values that are .005 or more under a peak value are going to be impacting nearly as much naturally occuring values as artificially clipped values.","e91794d9":"Not seeing nice even numbers here like 300,000 or 500,000 at the peaks, so maybe not. At least not natural log. Trying with power (base of 10) also didn't produce nice even numbers. This doesn't lend support to my theory that these frequency clusters are around popular maximum payout amounts. But it doesn't prove I'm wrong either - whoever prepared this data set may have deliberitly obscured the original numbers with multiple adjustments, making it hard to reverse engineer back to the real numbers. Or maybe it's something simple like a currency conversion, but who knows.","6c642cc6":"The adjustments were made, as you can see visually below (first overview, then looking up close at the first cliff\/valley):","a00a4db1":"Looks to me like the exact cutoff point is pretty close to 8.1184, so let's try doing an adjustment based on this number (and then let's get numbers for the other 6 peaks the same way).\n\nThe idea is that anything that is slightly above 8.1184 should be rare, and should therefore be adjusted somehow:","46147669":"The predictions follow a beautiful bell curve, and that has been the case with every set of XGBoost predictions so far tested with this notebook (only XGBoost predictions were examined).\n\nThough the real life target values did have some aspects of a bell curve, there were cliff-like peaks followed by valleys when you looked at it closely with many bins.\n\nPerhaps this has to do with popular limits for max insurance claim value, or perhaps it's something else, but the peaks and valleys exist, so . . .\n\nLet's see if we can take advantage of the biggest spikes and dropoffs. Let's start with one of the biggest, at around 8.11:"}}