{"cell_type":{"6e2a9225":"code","c8869756":"code","b550e3b7":"code","cd4998da":"code","63bb53b2":"code","e6b777b1":"code","64782971":"code","37a0472d":"code","6df4360c":"code","c6e546e2":"code","7c47a0d7":"code","864d7a86":"code","b00cd40c":"code","9c870d1c":"code","918a1066":"code","fb90537c":"code","56b8ede5":"code","50b6aeca":"code","a92a56ca":"code","894ed580":"code","aa266a48":"code","561f2db3":"code","d3d366e8":"code","ff862b17":"code","b81e0a90":"code","26e844ce":"code","436d6ad3":"code","9f72b4ab":"code","f39541d2":"code","14ef46cb":"code","c82bafa0":"code","e380d2dc":"code","5fe9298d":"code","cd1ca221":"code","28f6de1b":"code","2cbf5e71":"code","c2015493":"code","f1554bcb":"code","6bc25cd4":"code","784a123b":"code","7b69fa8f":"code","f176a63a":"code","015e43b5":"code","8091eb73":"code","57310425":"code","7af5a389":"code","ad59848f":"code","ab206203":"code","4e028de6":"code","49c56c08":"code","7d1975d4":"code","1f66c28e":"code","a72f627c":"code","a2bc3a9e":"code","afee6af8":"code","74e8aa45":"code","caa9498b":"code","324e92d9":"code","41245d61":"code","b247f916":"code","050171ec":"code","cb3f67d1":"code","96484a84":"code","c6199fda":"code","f9988e0a":"code","d7594773":"code","9c65a46a":"code","b48e9e14":"code","dd81b69f":"code","4cbc2d2f":"code","1b883bb6":"code","b0d2a282":"code","328bc00e":"code","2ccd6c92":"code","68fbb738":"code","cdf7e648":"code","89262197":"markdown","572dbb09":"markdown","995c4618":"markdown","5aa871b4":"markdown","5a8cf504":"markdown","611dacc3":"markdown","209a1a7e":"markdown","b122ca0e":"markdown","ba8a6b26":"markdown","a777ccad":"markdown","e7516c57":"markdown","a72168cb":"markdown","376f128e":"markdown","2fd7c928":"markdown","b4bba622":"markdown","007913cf":"markdown","816ac7ac":"markdown","a8ddbe06":"markdown","bcee36f5":"markdown","8d78ad8d":"markdown","153e907a":"markdown","8b5ca6a0":"markdown","abdf0704":"markdown","99bfcdae":"markdown","246314ed":"markdown","1e6cecc6":"markdown","692e20c6":"markdown","0c8fc84b":"markdown","673911b3":"markdown","45d80d0f":"markdown","889498ee":"markdown","0bad86eb":"markdown"},"source":{"6e2a9225":"import pandas as pd\nimport numpy as np\nimport itertools\nimport glob\nfrom tqdm.notebook import tqdm\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nimport torch.autograd as autograd\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","c8869756":"# ---------\n# training\n# ---------\nepoch = 0 # epoch to start training from\nn_epochs = 5 # number of epochs of training (suggested default : 200)\nbatch_size = 16 # size of the batches. suggested.\nlr = 0.0002 # adam : learning rate\nb1 = 0.5 # adam : decay of first order momentum of gradient\nb2 = 0.999 # adam : decay of first order momentum of gradient\ndecay_epoch = 4 # epoch fom which to start lr decay (suggested default : 100)\n\n# ---------\n# image data\n# ---------\nroot = '..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/'\nimg_height = 128 # size of image height\nimg_width = 128 # size of image width\nchannels = 3 # number of image channels\n\n# ---------\n# modeling\n# ---------\nresidual_blocks = 6 # number of residual blocks in generator\nn_critic = 5 # number of training iterations for WGAN discriminator\nselected_attrs = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'] # selected attributes for the CelebA dataset","b550e3b7":"# number of cpu (in kaggle server - Accelerator : GPU)\n!cat \/proc\/cpuinfo | grep processor","cd4998da":"n_cpu = 2 # number of cpu threads to use during batch generation","63bb53b2":"c_dim = len(selected_attrs) # number of input-attributes\nc_dim","e6b777b1":"img_shape = (channels, img_height, img_width) # set image shape for pytorch\nimg_shape","64782971":"cuda = torch.cuda.is_available()\ncuda","37a0472d":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        \n        conv_block = [\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True)\n        ]\n        \n        self.conv_block = nn.Sequential(*conv_block) # list-unpacking\n    \n    def forward(self, x):\n        return x + self.conv_block(x)","6df4360c":"class GeneratorResNet(nn.Module):\n    def __init__(self, img_shape=(3,128,128), res_blocks=9, c_dim=5):\n        super(GeneratorResNet, self).__init__()\n        channels, img_size, _ = img_shape\n        \n        # Initial convolution block\n        model = [\n            nn.Conv2d(channels+c_dim, 64, 7, stride=1, padding=3, bias=False), # in_channels = channels+c_dim (domain added in channel)\n            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True)\n        ]\n        \n        # Downsampling\n        curr_dim = 64\n        for _ in range(2):\n            model += [\n                nn.Conv2d(curr_dim, curr_dim*2, 4, stride=2, padding=1, bias=False), \n                nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True)\n            ]\n            curr_dim *= 2 # 64->128\n        \n        # Residual blocks\n        for _ in range(res_blocks): # 9-loop\n            model += [ResidualBlock(curr_dim)] # 128->128\n        \n        # Upsampling\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(curr_dim, curr_dim\/\/2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim\/\/2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim = curr_dim\/\/2 # 128->64\n            \n        # Output layer\n        model += [\n            nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), # 64 -> 3 (return RGB Image)\n            nn.Tanh() # -1 < tanh(x) < 1\n        ]\n        \n        self.model = nn.Sequential(*model) # Unpack the list of layers \n    \n    def forward(self, x, c):\n        c = c.view(c.size(0), c.size(1), 1, 1)\n        c = c.repeat(1, 1, x.size(2), x.size(3))\n        x = torch.cat((x,c), 1) # get image(x) and domain(c) \n        return self.model(x)","c6e546e2":"class Discriminator(nn.Module):\n    def __init__(self, img_shape=(3,128,128), c_dim=5, n_strided=6):\n        super(Discriminator, self).__init__()\n        channels, img_size, _ = img_shape\n        \n        def discriminator_block(in_filters, out_filters):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [\n                nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), \n                nn.LeakyReLU(0.01)\n            ]\n            return layers\n        \n        layers = discriminator_block(channels, 64)\n        curr_dim = 64\n        for _ in range(n_strided-1):\n            layers.extend(discriminator_block(curr_dim, curr_dim*2))\n            curr_dim *= 2\n            \n        self.model = nn.Sequential(*layers)\n        \n        # Output 1 : PatchGAN\n        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n        # Output 2 : Class prediction\n        kernel_size = img_size\/\/(2**n_strided)\n        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n        \n    def forward(self, img):\n        feature_repr = self.model(img)\n        out_adv = self.out1(feature_repr) # real or fake\n        out_cls = self.out2(feature_repr) # matching-domain\n        return out_adv, out_cls.view(out_cls.size(0), -1)\n        ","7c47a0d7":"# Loss function - Cycle loss\ncriterion_cycle = torch.nn.L1Loss()","864d7a86":"# Loss function - Domain-Class loss\ndef criterion_cls(logit, target):\n    return F.binary_cross_entropy_with_logits(logit, target, size_average=False) \/ logit.size(0)","b00cd40c":"# Loss weights (suggested default in paper)\nlambda_cls = 1\nlambda_rec = 10\nlambda_gp = 10","9c870d1c":"generator = GeneratorResNet(img_shape=img_shape, res_blocks=residual_blocks, c_dim=c_dim)\ndiscriminator = Discriminator(img_shape=img_shape, c_dim=c_dim)","918a1066":"if cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    criterion_cycle.cuda()","fb90537c":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution","56b8ede5":"generator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal);","50b6aeca":"optimizer_G = torch.optim.Adam(\n    generator.parameters(),\n    lr=lr,\n    betas=(b1,b2)\n)\noptimizer_D = torch.optim.Adam(\n    discriminator.parameters(),\n    lr=lr,\n    betas=(b1,b2)\n)","a92a56ca":"class CelebADataset(Dataset):\n    def __init__(self, root, transforms_=None, mode='train', attributes=None):\n        self.transform = transforms.Compose(transforms_)\n        \n        self.selected_attrs = attributes\n        self.files = sorted(glob.glob('%s\/*.jpg' %root))\n        if mode == 'train':\n            self.files = self.files[:1000] # 1,000\n        elif mode == 'test':\n            self.files = self.files[1000:1200] # 200\n        self.annotations = self.get_annotations()\n        \n    def get_annotations(self):\n        \"\"\"Extracts annotations for CelebA\"\"\"\n        label_df = pd.read_csv('..\/input\/celeba-dataset\/list_attr_celeba.csv')\n        label_df = label_df[:12000]\n        label_names = list(label_df.columns)[1:]\n        annotations = {}\n        for _, row in label_df.iterrows():\n            filename = row.values[0]\n            values = row[1:]\n            labels = []\n            for attr in selected_attrs: # selected_attr = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"]\n                idx = label_names.index(attr) # get index of selected 'attr'\n                labels.append(1 * (values[idx]==1))\n            annotations[filename] = labels\n        return annotations\n    \n    def __getitem__(self, index):\n        filepath = self.files[index % len(self.files)]\n        filename = filepath.split('\/')[-1]\n        img = self.transform(Image.open(filepath))\n        label = self.annotations[filename]\n        label = torch.FloatTensor(np.array(label))\n        return img, label\n    \n    def __len__(self):\n        return len(self.files)","894ed580":"label_df = pd.read_csv('..\/input\/celeba-dataset\/list_attr_celeba.csv')","aa266a48":"label_df.head()","561f2db3":"label_df = label_df[:12000]\nlabel_df","d3d366e8":"label_names = list(label_df.columns)[1:]\nlabel_names","ff862b17":"print(*list(label_df.iloc[0]))","b81e0a90":"label_df.iloc[0][0]","26e844ce":"print(*list(label_df.iloc[0][1:]))","436d6ad3":"for _, row in label_df.iterrows():\n    print('filename :\\n\\t',row.values[0])\n    print('one-hot-label :\\n\\t',row.values[1:])\n    break","9f72b4ab":"annotations = {}\nfor _, row in tqdm(label_df.iterrows(), total=len(label_df)):\n    filename = row.values[0]\n    values = row[1:]\n    labels = []\n    for attr in selected_attrs: # selected_attr = [\"Black_Hair\", \"Blond_Hair\", \"Brown_Hair\", \"Male\", \"Young\"]\n        idx = label_names.index(attr) # get index of selected 'attr'\n        labels.append(1 * (values[idx]==1))\n    annotations[filename] = labels","f39541d2":"for i in range(1,10):\n    sample_filename = '00000{}.jpg'.format(i)\n    print('filename: ',sample_filename)\n    print('encoded-label:',annotations[sample_filename])","14ef46cb":"sample_train_files = sorted(glob.glob('%s\/*.jpg' %root))\nsample_train_files = sample_train_files[:10000] # 10,000","c82bafa0":"sample_index = 23\nsample_train_file_path = sample_train_files[sample_index % len(sample_train_files)]\nsample_train_file_path","e380d2dc":"sample_filename = sample_train_file_path.split('\/')[-1]\nsample_filename","5fe9298d":"sample_transforms = [\n    transforms.Resize(int(1.12*img_height), Image.BICUBIC),\n    transforms.ToTensor()\n]\nsample_transform = transforms.Compose(sample_transforms)\nsample_img = sample_transform(Image.open(sample_train_file_path))\nsample_img.shape","cd1ca221":"plt.imshow(sample_img.permute(1,2,0))\nplt.axis('off');","28f6de1b":"sample_label = annotations[sample_filename]\nsample_label","2cbf5e71":"torch.FloatTensor(np.array(sample_label))","c2015493":"# transforms\ntrain_transforms = [\n    transforms.Resize(int(1.12*img_height), Image.BICUBIC),\n    transforms.RandomCrop(img_height),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n]","f1554bcb":"# dataloader\ndataloader = DataLoader(\n    CelebADataset(\n        root, transforms_=train_transforms, mode='train', attributes=selected_attrs\n    ),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu\n)","6bc25cd4":"len(glob.glob('%s\/*.jpg'%root))","784a123b":"# transforms\nval_transforms = [\n    transforms.Resize((img_height, img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n]","7b69fa8f":"# dataloader\nval_dataloader = DataLoader(\n    CelebADataset(\n        root, transforms_=val_transforms, mode='test', attributes=selected_attrs\n    ),\n    batch_size=10,\n    shuffle=True,\n    num_workers=1\n)","f176a63a":"# Tensor type\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","015e43b5":"def compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty loss for WGAN-GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0),1,1,1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha*real_samples + ((1-alpha)*fake_samples)).requires_grad_(True) # requires_grad inplace\n    d_interpolates, _ = D(interpolates) # adv_info, cls_info = discriminator(interpolated image)\n    fake = Tensor(np.ones(d_interpolates.shape))\n    # Get gradient w.r.t interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    gradients = gradients.view(gradients.size(0),-1)\n    gradient_penalty = ((gradients.norm(2, dim=1)-1)**2).mean()\n    return gradient_penalty","8091eb73":"for i, (imgs, labels) in enumerate(dataloader):\n    print(imgs.size())\n    break","57310425":"temp_real_samples = torch.from_numpy(np.random.randn(16,3,128,128))\ntemp_fake_samples = torch.from_numpy(np.random.randn(16,3,128,128))\ntemp_real_samples = temp_real_samples.type(torch.FloatTensor).cuda()\ntemp_fake_samples = temp_fake_samples.type(torch.FloatTensor).cuda()\nprint(temp_real_samples.size(), temp_fake_samples.size())","7af5a389":"temp_alpha = torch.FloatTensor(np.random.random((temp_real_samples.size(0),1,1,1))).cuda()\ntemp_alpha.size()","ad59848f":"temp_interpolates = (temp_alpha*temp_real_samples + (1-temp_alpha)*temp_fake_samples).requires_grad_(True)\ntemp_interpolates.size()","ab206203":"temp_d_interpolates, temp_cls = discriminator(temp_interpolates)\nprint(temp_d_interpolates.size()) # 16 batches and 1 output channel\nprint(temp_d_interpolates[0]) \nprint(temp_cls.size()) # 16 batches and 5 domains\nprint(temp_cls) # predict `real or fake` probabilities of each domain","4e028de6":"temp_fake = Tensor(np.ones(temp_d_interpolates.shape))\ntemp_fake.size()","49c56c08":"temp_gradients = autograd.grad(\n        outputs=temp_d_interpolates, # size : [16, 1, 2, 2]\n        inputs=temp_interpolates, # size : [16, 3, 128, 128]\n        grad_outputs=temp_fake, # size : [16, 1, 2, 2]\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\ntemp_gradients.size()","7d1975d4":"temp_gradients = temp_gradients.view(temp_gradients.size(0),-1) # flatten\ntemp_gradients.size()","1f66c28e":"print(temp_gradients.size())\nprint(temp_gradients.norm(2, dim=1).size()) # Euclidean distance of the Metrix","a72f627c":"gradients_penalty = ((temp_gradients.norm(2, dim=1)-1)**2).mean()\ngradients_penalty","a2bc3a9e":"# selected_attrs = [\n#    'Black_Hair', \n#    'Blond_Hair', \n#    'Brown_Hair', \n#    'Male', \n#    'Young'\n#    ]\n\nlabel_changes = [\n    ((0, 1), (1, 0), (2, 0)),  # Set to black hair -->## Black_Hair : 1, Blond_Hair : 0, Brown_Hair : 0\n    ((0, 0), (1, 1), (2, 0)),  # Set to blonde hair ->## Black_Hair : 0, Blond_Hair : 1, Brown_Hair : 0\n    ((0, 0), (1, 0), (2, 1)),  # Set to brown hair -->## Black_Hair : 0, Blond_Hair : 0, Brown_Hair : 1\n    ((3, -1),),  # Flip gender -->## if 0 -> -1, elif 1 -> 0\n    ((4, -1),),  # Age flip    -->## if 0 -> -1, elif 1 -> 0\n]","afee6af8":"def sample_images():\n    \"\"\"Show a generated sample of domain translations\"\"\"\n    val_imgs, val_labels = next(iter(val_dataloader))\n    val_imgs = val_imgs.type(Tensor)\n    val_labels = val_labels.type(Tensor)\n    img_samples = None\n    for i in range(10):\n        img, label = val_imgs[i], val_labels[i]\n        # Repeat for number of label changes\n        imgs = img.repeat(c_dim, 1, 1, 1) # c_dim is number of domains (5)\n        labels = label.repeat(c_dim, 1)\n        # Make changes to lab els\n        for sample_i, changes in enumerate(label_changes):\n            for col, val in changes:\n                labels[sample_i, col] = 1 - labels[sample_i, col] if val == -1 else val\n        # Generate translations\n        gen_imgs = generator(imgs, labels)\n        # Concatenate images by width\n        gen_imgs = torch.cat([x for x in gen_imgs.data], -1)\n        img_sample = torch.cat((img.data, gen_imgs), -1)\n        # Add as row to generated samples\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample),-2)\n    plt.figure(figsize=(4,8))\n    plt.imshow(img_samples.permute(1,2,0).detach().cpu())\n    plt.axis('off')\n    plt.show()","74e8aa45":"temp_val_imgs, temp_val_labels = next(iter(val_dataloader))\ntemp_val_imgs = temp_val_imgs.type(Tensor)\ntemp_val_labels = temp_val_labels.type(Tensor)\n\nprint(temp_val_imgs.size()) # 10 batches(val_dataloader), 3 channels, image size of 128x128\nprint(temp_val_labels.size()) # 10 batches and 5 domains","caa9498b":"for i in range(10):\n    temp_img, temp_label = temp_val_imgs[i], temp_val_labels[i]\n    print('1 image --> ',temp_img.size())\n    print('1 label --> ',temp_label.size())\n    temp_imgs = temp_img.repeat(c_dim,1,1,1) # repeat(copy) image x 5(number of domain)\n    temp_labels = temp_label.repeat(c_dim,1) # repeat(copy) label x 5(number of domain)\n    print('5 images -> ',temp_imgs.size()) \n    print('5 labels -> ',temp_labels.size())\n    break","324e92d9":"for sample_i, changes in enumerate(label_changes):\n    print(sample_i, changes)","41245d61":"for sample_i, changes in enumerate(label_changes):\n    for col, val in changes:\n        print('iter :', col)\n        print('value :', val)","b247f916":"temp_labels # original labels","050171ec":"for sample_i, changes in enumerate(label_changes):\n    for col, val in changes:\n        temp_labels[sample_i, col] = 1 - temp_labels[sample_i, col] if val == -1 else val","cb3f67d1":"temp_labels # modified labels","96484a84":"copied_temp_imgs = temp_imgs\ncopied_temp_imgs.size()","c6199fda":"copied_temp_imgs_group = torch.cat([x for x in copied_temp_imgs.data],-1)\ncopied_temp_imgs_group.size()","f9988e0a":"plt.imshow(copied_temp_imgs_group.permute(1,2,0).detach().cpu())\nplt.axis('off');","d7594773":"temp_gen_imgs = generator(temp_imgs, temp_labels)","9c65a46a":"temp_gen_imgs.size()","b48e9e14":"len([x for x in temp_gen_imgs.data]) # loop of images (5)","dd81b69f":"temp_gen_imgs_group = torch.cat([x for x in temp_gen_imgs.data],-1)\ntemp_gen_imgs_group.size()","4cbc2d2f":"plt.imshow(temp_gen_imgs_group.permute(1,2,0).detach().cpu())\nplt.axis('off');","1b883bb6":"temp_img_sample = torch.cat((temp_img.data, temp_gen_imgs_group),-1)\ntemp_img_sample.size()","b0d2a282":"plt.imshow(temp_img_sample.permute(1,2,0).detach().cpu())\nplt.axis('off');","328bc00e":"temp_img_samples = torch.cat((temp_img_sample, temp_img_sample), -2)\ntemp_img_samples.size()","2ccd6c92":"plt.imshow(temp_img_samples.permute(1,2,0).detach().cpu())\nplt.axis('off');","68fbb738":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nfor epoch in range(epoch, n_epochs):\n    for i, (imgs, labels) in enumerate(tqdm(dataloader)):\n        # Model inputs\n        imgs = imgs.type(Tensor)\n        labels = labels.type(Tensor)\n        \n        # Sample labels as generator inputs\n        sampled_c = Tensor(np.random.randint(0, 2, (imgs.size(0), c_dim)))\n        # Generate fake batch of images\n        fake_imgs = generator(imgs, sampled_c)\n        \n# -------------------\n# Train Discriminator\n# -------------------\n        optimizer_D.zero_grad()\n    \n        # Real images\n        real_validity, pred_cls = discriminator(imgs)\n        # Fake images\n        fake_validity, _ = discriminator(fake_imgs.detach())\n        # Gradient penalty\n        gradient_penalty = compute_gradient_penalty(discriminator, imgs.data, fake_imgs.data)\n        # Adversarial loss\n        loss_D_adv = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp*gradient_penalty\n        # Classification loss\n        loss_D_cls = criterion_cls(pred_cls, labels)\n        # Total loss\n        loss_D = loss_D_adv + lambda_cls*loss_D_cls\n        \n        loss_D.backward()\n        optimizer_D.step()\n        \n        optimizer_G.zero_grad()\n        \n        # Every n_critic times update generator\n        if i % n_critic == 0: # n_critic : 5\n\n        # -------------------\n        # Train Generator\n        # -------------------\n            # Translate and reconstruct image\n            gen_imgs = generator(imgs, sampled_c)\n            recov_imgs = generator(gen_imgs, labels)\n            # Discriminator evaluates translated image\n            fake_validity, pred_cls = discriminator(gen_imgs)\n            # Adversarial loss\n            loss_G_adv = -torch.mean(fake_validity)\n            # Classification loss\n            loss_G_cls = criterion_cls(pred_cls, sampled_c)\n            # Reconstruction loss\n            loss_G_rec = criterion_cycle(recov_imgs, imgs)\n            # Total loss\n            loss_G = loss_G_adv + lambda_cls*loss_G_cls + lambda_rec*loss_G_rec\n            \n            loss_G.backward()\n            optimizer_G.step()\n            \n        # -------------------\n        # Show Progress\n        # -------------------\n        if (i+1) % 21 == 0: \n            sample_images()\n            print(\"[Epoch %d\/%d] [Batch %d\/%d] [D adv: %f, aux: %f] [G loss: %f, adv: %f, aux: %f, cycle: %f]\"\n                % (\n                    epoch+1, n_epochs,                     # Epoch\n                    i+1,len(dataloader),                   # Batch\n                    loss_D_adv.item(),loss_D_cls.item(),   # D loss\n                    loss_G.item(),loss_G_adv.item(),       # G loss (total, adv)\n                    loss_G_cls.item(),loss_G_rec.item(),   # G loss (cls, cycle)\n                ))\n\n","cdf7e648":"# 16x5 array that contains 0 or 1\nprint(np.random.randint(0,2,(16,5)))\nprint('--------------')\nprint(np.random.randint(0,2,(16,5)).shape)","89262197":"### Step 15. Training","572dbb09":"### Step 10. Define CelebADataset Class for handling labeled dataset","995c4618":"### Step 11. Set transforms and Configure Dataloader for training","5aa871b4":"> TEST CODE : np.random.randint","5a8cf504":"If it's your first time gan, it's good to see [\ud83c\udf08GAN Basic Tutorial : Generate MNIST Image](https:\/\/www.kaggle.com\/songseungwon\/gan-basic-tutorial-generate-mnist-image). This kernel requires an understanding of the basic gan model.\n\nAnd you need to understand `CycleGAN` Architecture. See also [\ud83c\udf08CycleGAN Tutorial : Monet-to-Photo\u26f5\ufe0f](https:\/\/www.kaggle.com\/songseungwon\/cyclegan-tutorial-monet-to-photo)If you are unfamiliar with `CycleGAN`","611dacc3":"![image](https:\/\/miro.medium.com\/max\/1494\/1*KzE7AMq8xbq6vOe1teaDDg.png)","209a1a7e":"### Step 12. Set transforms and Configure Dataloader for testing","b122ca0e":"![image.png](attachment:73f5e806-7fdf-4eb4-9a5b-0676c979b0fb.png)![image](https:\/\/i.imgur.com\/O1VrGT1.png)","ba8a6b26":"\nSo far, we have looked at stargan, which can express various domains at once. \n\nSince it was meaningful to implement the code, we went on to test the model with a very small dataset (1k) and a small epoch (5). If you have enough time, consider implementing a better-performing model with more data.\n\nIf it was helpful, please share it with upvote for more people to see, and continue to study with me through more kernels.\n\nThank You!","a777ccad":"> Read More\n- [torch.nn.Conv2d](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv2d.html)\n- [UnPooling \/ UpSampling \/ Deconvolution \/ Transposed convolution](https:\/\/www.programmersought.com\/article\/42421095350\/)\n- [Calculating the Output Size of Convolutions and Transpose Convolutions](http:\/\/makeyourownneuralnetwork.blogspot.com\/2020\/02\/calculating-output-size-of-convolutions.html)\n- Example | Upsampling - (stride = 2, padding = 1)\n![image](https:\/\/1.bp.blogspot.com\/-bVWdYb2CLX4\/XkrI1i27G4I\/AAAAAAAAArQ\/mlykCrbvqQoCaXPK-Oh4tqGi04RpukqbACEwYBhgL\/s640\/appendix_C_eg_7.png)\n- [torch.nn.Tanh](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Tanh.html)\n- Example | Tanh activate function\n![image](https:\/\/pytorch.org\/docs\/stable\/_images\/Tanh.png)\n","e7516c57":"# StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation\n[--> paper](https:\/\/openaccess.thecvf.com\/content_cvpr_2018\/papers\/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)\n\n![paper](https:\/\/openaccess.thecvf.com\/content_cvpr_2018\/papers\/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)","a72168cb":"### Step 1. Import Libraries","376f128e":"## Index\n```\nStep 1. Import Libraries\nStep 2. Initial Setting\nStep 3. Define Generator\nStep 4. Define Discriminator\nStep 5. Define Loss function and Initialize Loss weights\nStep 6. Initialize Generator and Discriminator\nStep 7. GPU Setting\nStep 8. Weight Setting\nStep 9. Configure Optimizers\nStep 10. Define CelebADataset Class for handling labeled dataset\nStep 11. Set transforms and Configure Dataloader for training\nStep 12. Set transforms and Configure Dataloader for testing\nStep 13. Define Gradient Penalty Function\nStep 14. Define function to get sample images with input label list\nStep 15. Training\n```\n---","2fd7c928":"### Step 6. Initialize Generator and Discriminator","b4bba622":"## Step 2. Initial Setting","007913cf":"### Step 4. Define Discriminator","816ac7ac":"### Step 14. Define function to get sample images with input label list","a8ddbe06":"### Step 8. Weight Setting","bcee36f5":"> TEST CODE : read csv file (check labels)","8d78ad8d":"> TEST CODE : gradient penalty","153e907a":"#  StarGAN Tutorial (Basic) - Make Fake Images\n\n**For beginners who are unfamiliar with Python code**, I solved the pytorch official code with Jupyter Notebook.\n\nThrough this kernel, I will implement StarGAN model from scratch with PyTorch and test them with Kaggle data. \n\nThen.. let's get started!\n\n---","8b5ca6a0":"> TEST CODE : getitem","abdf0704":"## Main Reference\n1. [PyTorch-GAN | Github\/eriklindernoren | Collection of PyTorch implementations of GAN](https:\/\/github.com\/sw-song\/PyTorch-GAN)\n2. [stargan | Github\/yunjey | Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https:\/\/github.com\/yunjey\/stargan)","99bfcdae":"> Read More\n- [CycleGAN Tutorial : Monet-to-Photo - Step 8. Weight Setting](https:\/\/www.kaggle.com\/songseungwon\/cyclegan-tutorial-monet-to-photo)","246314ed":"### Step 13. Define Gradient Penalty Function","1e6cecc6":"> TEST CODE : root","692e20c6":"### Step 5. Define Loss function and Initialize Loss weights","0c8fc84b":"### Step 7. GPU Setting","673911b3":"### Step 9. Configure Optimizers","45d80d0f":"> TEST CODE : show test images","889498ee":"### Step 3. Define Generator","0bad86eb":"> Read More \n- [GAN - Wasserstein GAN & WGAN-GP](https:\/\/jonathan-hui.medium.com\/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490)"}}