{"cell_type":{"8751b9ec":"code","3e657ffe":"code","1bebecc0":"code","37cea6ad":"code","dc9fe7ee":"code","65fdd0fe":"code","324b59e7":"code","5352263b":"code","8e824476":"code","c112acba":"code","2af7007f":"code","770d2e34":"code","88e81f0d":"code","b8dd1383":"code","2d4d1cb3":"code","225fe36c":"code","4d83a26d":"code","e6ac1e50":"code","050f3e83":"code","1bb78101":"code","e0781d3e":"code","5572ef04":"code","94e4045e":"code","7b04b750":"code","eac3b4cc":"code","d71689d6":"code","fe4300ef":"code","09813e80":"code","fe46a35c":"code","a0053512":"markdown","79e04c05":"markdown","f3fc1e45":"markdown","7bfb721e":"markdown","fdcbbf1b":"markdown","2962611e":"markdown","838c6b82":"markdown","263ab6f3":"markdown","53de5ad7":"markdown","181b4120":"markdown","6dabf1ab":"markdown","e502e52e":"markdown","f022a207":"markdown","6ec4872f":"markdown","8190192b":"markdown"},"source":{"8751b9ec":"# !pip install cutmix-keras -q","3e657ffe":"!pip install gdown -q","1bebecc0":"#general imports \nimport os \nimport shutil\nimport numpy as np \nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt \nimport random\nfrom IPython import display\nimport pickle\n\nimport multiprocessing as mp\n# mapping \nimport folium\nfrom folium.plugins import MarkerCluster,HeatMap\n\n#deep learning\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,Dropout,Dense,MaxPooling2D,GaussianNoise,Flatten,Input,BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n\n#backend\nfrom tensorflow.keras import backend as K\n\n#cutmix aug\n# from cutmix_keras import CutMixImageDataGenerator\n\n\nimport warnings \nwarnings.filterwarnings('ignore')","37cea6ad":"\nseed=7\n\ndef set_seed(seed):\n    \n    '''set seed for reproducablity'''\n    os.environ['PYTHONHASHSEED']= str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    \nset_seed(seed)    ","dc9fe7ee":"if not os.path.exists('.\/input_data'):\n    os.mkdir('.\/input_data')\n\n#download unzip\n!gdown https:\/\/drive.google.com\/uc?id=1EH3p84xKMs_2m4ISSR7aHOQz61yxIv8L\n!unzip .\/Post-hurricane.zip -d .\/input_data\n\n# clear output\ndisplay.clear_output()","65fdd0fe":"BATCH= 128 \nimg_size = (128,128)","324b59e7":"def get_img_coordinates(row):\n    path = row['paths']\n    # get coordinates of images \n    #split the name\n    \n    img_id=path.split('\/')[-1]\n    \n    lat,tmp = img_id.split('_')\n    \n    long = '.'.join(tmp.split('.')[:-1])\n    \n    row['lat']=float(lat)\n    row['long']=float(long)\n    row['geometry']=(float(long),float(lat))\n    \n    \n    return row\n    \ndef get_paths(data_dir):\n    \n    sub_dir = ['damage','no_damage']\n    paths,damage,no_damage=[],0,0\n\n    for i,path in enumerate(os.listdir(os.path.join(data_dir,'damage'))):\n        paths.append(path) \n        damage += 1\n       \n    \n    for i,path in enumerate(os.listdir(os.path.join(data_dir,'no_damage'))):\n        paths.append(path) \n        no_damage += 1\n        \n    df = pd.DataFrame({'paths':paths})\n    \n    df.loc[:damage,'damage'] = 1\n    df.loc[damage:,'no_damage'] = 1\n    \n    return df \n\n","5352263b":"\ntrain_dir = '.\/input_data\/train_another'\ntest_dir = '.\/input_data\/test'\ntest_another = '.\/input_data\/train_another'\nval_another = '.\/input_data\/validation_another'","8e824476":"#getting paths and coordinates \ntrain_csv = get_paths(data_dir = train_dir)\ntest_csv = get_paths(data_dir = test_dir)\ntest_an_csv = get_paths(data_dir = test_another)\nval_csv = get_paths(data_dir = val_another)\n\ntrain_csv=train_csv.apply(lambda x: get_img_coordinates(x),axis=1)\ntest_csv=test_csv.apply(lambda x: get_img_coordinates(x),axis=1)\ntest_an_csv=test_an_csv.apply(lambda x: get_img_coordinates(x),axis=1)\nval_csv=val_csv.apply(lambda x: get_img_coordinates(x),axis=1)","c112acba":"\n\n# lets see the points which are sensed in the study\n\ndef plot_heatmap(df_list,title,damage=1):\n    \n    loc = (df_list[0]['long'].mean(),df_list[0]['lat'].mean())\n    \n    m1=folium.Map(location=loc,\n                  tiles='Open Street Map',\n                  zoom_start=8,min_zoom=7)\n\n    for df1 in df_list:\n        if damage:\n            data = df1[df1['damage']==1]['geometry']\n        else:\n            data =df1[df1['damage']!=1]['geometry']\n        HeatMap(data=data,\n                       radius=5,\n                       opacity=0.5,\n                       name='Damage').add_to(m1)\n\n    \n    print(f'{title} regions in picture')\n    return m1\n\n\nplot_heatmap(df_list=[train_csv,test_csv,test_an_csv,val_csv],title='Damaged')","2af7007f":"plot_heatmap(df_list=[train_csv,test_csv,test_an_csv,val_csv],title='Not-damaged',damage=0)","770d2e34":"# loading data in image data generators\n\n# train gen\ntrain_gen = ImageDataGenerator( \n#     featurewise_center=False, #Set input mean to 0 over the dataset, feature-wise.\n#     samplewise_center=False, # Set each sample mean to 0\n#     featurewise_std_normalization=False, #Divide inputs by std of the dataset, feature-wise\n    width_shift_range=(0.05,0.05), # (-width_shift_range, +width_shift_range)\n    height_shift_range=(0.05,0.051),\n    brightness_range=(0.6,1.2),\n#     shear_range=1.2,\n#     samplewise_std_normalization=False, # Divide each input by its std\n    zca_whitening=False,  # A whitening transform of an image is a linear algebra operation that reduces the redundancy in the matrix of pixel images.\n#     zca_epsilon=1e-06,   \n    rotation_range=180,    # degree range for randm rot\n    zoom_range=(0.8,1.2),\n    channel_shift_range=0.1,\n    fill_mode='nearest',\n    cval=0.0,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rescale=1\/255)\n\n#val gen\nval_gen = ImageDataGenerator(rescale=1\/255)","88e81f0d":"# generating two data iterators for cutmix aug\ntrain_data1= train_gen.flow_from_directory(train_dir,\n                                           target_size = img_size,\n                                           batch_size=BATCH,\n                                           class_mode='binary',\n                                           shuffle=True,\n                                           seed=seed)\n\n# train_data2= train_gen.flow_from_directory(train_dir,\n#                                            target_size = img_size,\n#                                            batch_size=BATCH,\n#                                            class_mode='binary',\n#                                            shuffle=True,\n#                                            seed=seed)\n\n#val data \nval_data = val_gen.flow_from_directory(val_another,\n                                       target_size = img_size,\n                                       batch_size=BATCH,\n                                       class_mode='binary',\n                                       shuffle=True,\n                                       seed=seed)\n\n#test data \ntest_data = val_gen.flow_from_directory(test_dir,\n                                       target_size = img_size,\n                                       batch_size=BATCH,\n                                       class_mode='binary',\n                                       shuffle=True,\n                                       seed=seed)\n#unbalanced test data \ntest_data_unbalanced = val_gen.flow_from_directory(test_another,\n                                       target_size = img_size,\n                                       batch_size=BATCH,\n                                       class_mode='binary',\n                                       shuffle=True,\n                                       seed=seed)\n","b8dd1383":"def plot_image_grid(image_list,\n                    label_list,\n                    sample_images=False,\n                    num_images=12,\n                    pre_title='class',\n                    num_img_per_row=3,\n                    cmap=None,\n                    img_h_w=3):\n    '''viz images from a list of images and labels\n    INPUTS:\n    image_list: a list of images to be plotted,\n    label_list: a list of correspomding image labels'''\n    \n\n\n    #number of img rows\n    n_row= num_images\/\/num_img_per_row\n\n    plt.subplots(n_row,num_img_per_row,figsize=(img_h_w*num_img_per_row,img_h_w*n_row))\n\n    if sample_images:\n    #select_random images \n        sampled_ids = random.choices(np.arange(0,len(image_list)),k=num_images)\n\n        for i,idx in enumerate(sampled_ids):\n\n            img = image_list[idx]\n            label = label_list[i]\n            plt.subplot(n_row,num_img_per_row,i+1)\n            plt.title(f'{pre_title} - {label}')\n            plt.axis('off')\n            plt.imshow(img,cmap=cmap)\n    else:\n        for i,img in enumerate(image_list):\n\n            label = label_list[i]\n            plt.subplot(n_row,num_img_per_row,i+1)\n            plt.title(f'{pre_title} - {label}')\n            plt.axis('off')\n            plt.imshow(img,cmap=cmap)\n\n            # break the loop \n            if i==num_images-1 :\n                  break \n\n    #show\n    plt.tight_layout()\n    plt.show()\n","2d4d1cb3":"#select a batch for viewing test images \nimage_list,label_list=train_data1.next()\n\nplot_image_grid(image_list,\n                label_list,\n                sample_images=True,\n                num_images=20,\n               num_img_per_row=5)","225fe36c":"#select a batch for viewing test images \nimage_list,label_list=train_data1.next()\n\nplot_image_grid(image_list,\n                label_list,\n                sample_images=True,\n                num_images=20,\n               num_img_per_row=5)","4d83a26d":"#select a batch for viewing test images \nimage_list,label_list=train_data1.next()\n\nplot_image_grid(image_list,\n                label_list,\n                sample_images=True,\n                num_images=20,\n               num_img_per_row=5)","e6ac1e50":"#from :  https:\/\/pypi.org\/project\/cutmix-keras\/\n\n# CutMixImageDataGenerator\n# train_generator = CutMixImageDataGenerator(\n#     generator1=train_data1,\n#     generator2=train_data2,\n#     img_size=img_size,\n#     batch_size=BATCH,\n# )\n","050f3e83":"# source : https:\/\/keras.io\/examples\/vision\/cutmix\/\n\n# The CutMix function takes two image and label pairs to perform the augmentation. \n# It samples \u03bb(l) from the Beta distribution and returns a bounding box from get_box function. \n# We then crop the second image (image2) and pad this image in the final padded image at the same location.\n\n\ndef sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n    return gamma_1_sample \/ (gamma_1_sample + gamma_2_sample)\n\n\n@tf.function\ndef get_box(lambda_value):\n    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n\n    cut_w = IMG_SIZE * cut_rat  # rw\n    cut_w = tf.cast(cut_w, tf.int32)\n\n    cut_h = IMG_SIZE * cut_rat  # rh\n    cut_h = tf.cast(cut_h, tf.int32)\n\n    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n\n    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w \/\/ 2, 0, IMG_SIZE)\n    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h \/\/ 2, 0, IMG_SIZE)\n    bbx2 = tf.clip_by_value(cut_x[0] + cut_w \/\/ 2, 0, IMG_SIZE)\n    bby2 = tf.clip_by_value(cut_y[0] + cut_h \/\/ 2, 0, IMG_SIZE)\n\n    target_h = bby2 - boundaryy1\n    if target_h == 0:\n        target_h += 1\n\n    target_w = bbx2 - boundaryx1\n    if target_w == 0:\n        target_w += 1\n\n    return boundaryx1, boundaryy1, target_h, target_w\n\n\n@tf.function\ndef cutmix(train_ds_one, train_ds_two):\n    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n\n    alpha = [0.25]\n    beta = [0.25]\n\n    # Get a sample from the Beta distribution\n    lambda_value = sample_beta_distribution(1, alpha, beta)\n\n    # Define Lambda\n    lambda_value = lambda_value[0][0]\n\n    # Get the bounding box offsets, heights and widths\n    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n\n    # Get a patch from the second image (`image2`)\n    crop2 = tf.image.crop_to_bounding_box(\n        image2, boundaryy1, boundaryx1, target_h, target_w\n    )\n    # Pad the `image2` patch (`crop2`) with the same offset\n    image2 = tf.image.pad_to_bounding_box(\n        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n    )\n    # Get a patch from the first image (`image1`)\n    crop1 = tf.image.crop_to_bounding_box(\n        image1, boundaryy1, boundaryx1, target_h, target_w\n    )\n    # Pad the `image1` patch (`crop1`) with the same offset\n    img1 = tf.image.pad_to_bounding_box(\n        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n    )\n\n    # Modify the first image by subtracting the patch from `image1`\n    # (before applying the `image2` patch)\n    image1 = image1 - img1\n    # Add the modified `image1` and `image2`  together to get the CutMix image\n    image = image1 + image2\n\n    # Adjust Lambda in accordance to the pixel ration\n    lambda_value = 1 - (target_w * target_h) \/ (IMG_SIZE * IMG_SIZE)\n    lambda_value = tf.cast(lambda_value, tf.float32)\n\n    # Combine the labels of both images\n    label = lambda_value * label1 + (1 - lambda_value) * label2\n    return image, label\n","1bb78101":"# import pdb\nclass simple_cnn:\n    def __init__(self):\n\n        # layer regularization\n        self.layer_reg ={'kernel_reg':{'l1':1e-5,\n                                       'l2':1e-4},\n                       'bias_reg': {'l1':1e-5,\n                                   'l2':1e-4}\n                        }\n\n\n        # params for building model\n        self.params = {\n                  'n_dense' : [16],             # number of dense layer after conv\n                  'dense_act': ['relu','relu'],     # dnese layer activation\n                  'layer_reg': self.layer_reg,           # layer regularization params\n                  'drop'    :[0.6,0.5],             # dropout rates \n                  'kernel_size':[(3,3),(3,3),(3,3)],      # kernel sizes\n                  'pool_size' : [(2,2),(2,2),(2,2)],      # pooling size\n                  'conv'    : [8,16,32],             # conv layer number of filters * 2\n                  'strides' : [1,1,1],\n                  'activations': ['relu','relu','relu'],   # conv layer activations \n                  'batch_norm': True,\n                  'gaussian_noise':True,\n                  'Gaussian_noise_std':0.1\n                 }\n        \n    \n    \n    def build(self,\n              input_shape,\n              update_params,\n              output_layer\n             ):\n        self.input_shape=input_shape\n        \n        self.output_params = output_layer\n        #update params shape\n        if update_params:\n            self.params.update(update_params)\n        '''\n        build a CNN with given parameters\n        -input_shape   :  input image shape (example: (256,256,3),\n        - update_params: update build params\n        - output_layer : output_layer_params , ex {'n_out':num_classes,'activation':'softmax'}\n        \n        \n        # Build Params:\n        \n        n_dense : list of number of units in each dense layer\n        activations: list of activation in each CONV layer  \n        kernel_size:list of kernel size\n        conv: list of number of conv filters\n        pool_size: list of pooling size \n        layer_reg: dense layer regularization\n        drop     : list of dropouts after dense layers\n        compile_params: model compilation parameters ,\n        gaussian_noise: add gaussian noise layer to the model with std of given value'''\n           \n        # input layer\n        inp = Input(shape = self.input_shape)\n        if self.params['gaussian_noise']:\n          #add gaussian noise\n            x = GaussianNoise(stddev=self.params['Gaussian_noise_std'])(inp)\n            \n            x = Conv2D(self.params['conv'][0],\n                   activation=self.params['activations'][0],\n                   kernel_size=self.params['kernel_size'][0],\n                   strides =self.params['strides'][0])(x)\n        \n        else:\n        \n            x = Conv2D(self.params['conv'][0],\n                   activation=self.params['activations'][0],\n                   kernel_size=self.params['kernel_size'][0],\n                   strides =self.params['strides'][0])(inp)\n        \n        x = Conv2D(self.params['conv'][0],\n                   activation=self.params['activations'][0],\n                   kernel_size=self.params['kernel_size'][0],\n                   strides =self.params['strides'][0])(x)\n        x = MaxPooling2D(pool_size=self.params['pool_size'][0],\n                         strides =self.params['strides'][0])(x)\n        \n        if self.params['batch_norm']:\n            x = BatchNormalization()(x)\n\n        for i in range(1,len(self.params['conv'])):\n            x = Conv2D(self.params['conv'][i],\n                       activation =self.params['activations'][i],\n                       kernel_size=self.params['kernel_size'][i],\n                       strides =self.params['strides'][i])(x)\n            x = Conv2D(self.params['conv'][i],\n                       activation =self.params['activations'][i],\n                       kernel_size=self.params['kernel_size'][i],\n                      strides =self.params['strides'][i])(x)\n            x = MaxPooling2D(pool_size=self.params['pool_size'][i],\n                             strides =self.params['strides'][i])(x)\n            \n            if self.params['batch_norm']:\n                x = BatchNormalization()(x)\n\n        # \n        x = Flatten()(x)\n        \n        \n#         pdb.set_trace()\n        # regularization\n        self.kernel_reg=regularizers.l1_l2(l1=self.params['layer_reg']['kernel_reg']['l1'],\n                                           l2=self.params['layer_reg']['kernel_reg']['l2'])\n\n        x = Dense(self.params['n_dense'][0],\n                  activation=self.params['dense_act'][0],\n                  kernel_regularizer=self.kernel_reg,\n                  bias_regularizer = regularizers.l1_l2(\n                      l1=self.params['layer_reg']['bias_reg']['l1'],\n                      l2=self.params['layer_reg']['bias_reg']['l2']))(x)\n\n        x = BatchNormalization()(x)\n        x = Dropout(self.params['drop'][0])(x)\n\n        if len(self.params['n_dense'])>1:\n            for i in range(1,len(self.params['n_dense'])):\n                x = Dense(self.params['n_dense'][i],\n                          activation=self.params['dense_act'][i],\n                          kernel_regularizer=self.kernel_reg,\n                          bias_regularizer = regularizers.l1_l2(l1=self.params['layer_reg']['bias_reg']['l1'],\n                                                                l2=self.params['layer_reg']['bias_reg']['l2']))(x)\n                #drop\n                x = BatchNormalization()(x)\n                x = Dropout(self.params['drop'][i])(x)\n\n        out = Dense(self.output_params['n_out'],\n                    activation=self.output_params['activation'])(x)\n        \n        \n        model = Model(inputs=[inp],outputs = out)\n        return model \n    \n# plot train and val acc as  a function of epochs\ndef plot_history(history,addn_metric=None,add_metric=None):\n    '''\n    Inputs\n    history:history object from tensorflow\n    add_metric: metric name in the history (like f1_score)'''\n    his=pd.DataFrame(history.history)\n    \n    if addn_metric:\n        plt.subplots(1,3,figsize=(20,6))\n        \n        #loss:\n        ax1=plt.subplot(1,3,1)\n        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n        ax1.set_xlabel('EPOCHS')\n        ax1.set_ylabel('LOSS')\n        ax1.legend()\n        ax1.set_title('Loss Per Epoch')\n\n        #accuracy\n        ax2=plt.subplot(1,3,2)\n        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n        ax2.set_xlabel('EPOCHS')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.set_title('Accuracy Per Epoch')\n\n    \n        \n        ax3= plt.subplot(1,3,3)\n        ax3.plot(range(len(his)),his[f'{add_metric}'],color='g',label='training')\n        ax3.plot(range(len(his)),his[f'val_{add_metric}'],color='r',label='validation')\n        ax3.set_xlabel('EPOCHS')\n        ax3.set_ylabel(f'{add_metric}')\n        ax3.legend()\n        ax3.set_title(f'{add_metric} Per Epoch')\n\n        \n    else:\n        plt.subplots(1,2,figsize=(20,8))\n        \n    \n    \n        #loss:\n        ax1=plt.subplot(1,2,1)\n        ax1.plot(range(len(his)),his['loss'],color='g',label='training')\n        ax1.plot(range(len(his)),his['val_loss'],color='r',label='validation')\n        ax1.set_xlabel('EPOCHS')\n        ax1.set_ylabel('LOSS')\n        ax1.legend()\n        ax1.set_title('Loss Per Epoch')\n\n        #accuracy\n        ax2=plt.subplot(1,2,2)\n        ax2.plot(range(len(his)),his['accuracy'],color='g',label='training_acc')\n        ax2.plot(range(len(his)),his['val_accuracy'],color='r',label='validation_acc')\n        ax2.set_xlabel('EPOCHS')\n        ax2.set_ylabel('Accuracy')\n        ax2.legend()\n        ax2.set_title('Accuracy Per Epoch')\n\n        \n    \n    \n    plt.show()  \n","e0781d3e":"layer_reg ={'kernel_reg':{'l1':1e-4,'l2':1e-3},\n            'bias_reg': {'l1':1e-4,\n                         'l2':1e-3}}\n\nparams = {\n                  'n_dense' : [16],             # number of dense layer after conv\n                  'dense_act': ['relu','relu'],     # dnese layer activation\n                  'layer_reg': layer_reg,           # layer regularization params\n                  'drop'    :[0.5,0.5],             # dropout rates after dense\n                  'kernel_size':[(3,3),(3,3),(3,3)],      # kernel sizes\n                  'pool_size' : [(2,2),(2,2),(2,2)],      # pooling size\n                  'conv'    : [32,64],             # conv layer number of filters * 2\n                  'strides' : [2,1,1],\n                  'activations': ['relu','relu','relu'],   # conv layer activations \n                  'batch_norm': True,\n                  'gaussian_noise':True,\n                  'Gaussian_noise_std':0.11\n                 }\n\ncnn_model = simple_cnn().build(input_shape=(img_size+ (3,)),\n                               update_params=params,\n                               output_layer={'n_out':1,\n                                            'activation':'sigmoid'})\n\ncnn_model.summary()","5572ef04":"# from tensorflow_addons.metrics import F1Score\n#metrics for model evaluation:\n# f1_score=F1Score(num_classes=1,average='macro',name='f1_score')\n\n#from https:\/\/datascience.stackexchange.com\/questions\/45165\/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_score(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","94e4045e":"# compilation params\ncompile_params ={'loss':'binary_crossentropy',\n                'metrics':['accuracy',f1_score,recall_m,precision_m],\n                'optimizer':'adam'}\n\ncnn_model.compile(**compile_params)\n\n\n# callbacks \n#reduce_lr\nreduce_lr=ReduceLROnPlateau(patience=10,\n                            factor=0.9,\n                            min_delta=1e-2,\n                            monitor='val_f1_score',\n                            verbose=1,\n                            mode='max')\n\n#early stopping\nearly_stopping=EarlyStopping(patience=30,\n                             min_delta=1e-2,\n                              monitor='val_f1_score',\n                              restore_best_weights=True,\n                              mode='max')\n\nEPOCHS = 75\n\nhistory = cnn_model.fit(train_data1,\n                        validation_data=val_data,\n                        steps_per_epoch=train_data1.n\/\/BATCH,\n                       epochs=EPOCHS,\n                       callbacks = [reduce_lr,early_stopping],\n                       verbose=1)\n\ndisplay.clear_output()\n\nplot_history(history,addn_metric=True,add_metric='f1_score')","7b04b750":"#saving custom metrics for loading later \ncustom_metrics = {'recall_m':recall_m,\n                 'precision_m':precision_m,\n                 'f1_score': f1_score}\n\nwith open('f1_metrics.pkl','wb') as f:\n    pickle.dump(custom_metrics,f)","eac3b4cc":"models = 'saved_models'\nos.mkdir(models)\n\ncnn_model.save(filepath=os.path.join(models,'cnn_1.h5'))","d71689d6":"print('Evaluate on train set')\ncnn_model.evaluate(train_data1)","fe4300ef":"print('Evaluate on val set')\ncnn_model.evaluate(val_data)","09813e80":"print('Evaluate on Balanced test set')\ncnn_model.evaluate(test_data)","fe46a35c":"print('Evaluate on Un Balanced test set')\ncnn_model.evaluate(test_data_unbalanced)","a0053512":"# Image data generator","79e04c05":"# Training","f3fc1e45":"**Function for cutmix if using tfds**","7bfb721e":"# Generate a cutmix iterator","fdcbbf1b":"# Build Model","2962611e":"**Saving model for future use**","838c6b82":"# Helper Functions","263ab6f3":"\n# **Download and unzip**","53de5ad7":"# Imports","181b4120":"# Evaluate on test data ","6dabf1ab":"# Load data \n**DATA DESCRIPTION**\n* The data is provided to you has the following subfolders:\n\n    * train_another: the training data; 5000 images of each class(damage\/no damage)\n    * validation_another: the validation data; 1000 images of each class(damage\/no damage)\n    * test_another: the unbalanced test data; 8000\/1000 images of damaged\/undamaged classes\n    * test: the balanced test data; 1000 images of each class(damage\/no damage)","e502e52e":"# Problem Description: \n**Build a CNN to classify properties damaged by hurricanes**","f022a207":"# References and Resources \n* Deepsense AI blog : [ https:\/\/deepsense.ai\/satellite-images-semantic-segmentation-with-deep-learning\/ ]\n* Ship detection in Sattelite Images: [ https:\/\/medium.com\/intel-software-innovators\/ship-detection-in-satellite-images-from-scratch-849ccfcc3072 ]\n* Hurricane roof damage : [ https:\/\/www.restoremastersllc.com\/wind-damage-roof\/ ]\n* Blog : [ https:\/\/www.nature.com\/articles\/s41524-021-00620-7 ]\n* extra data : [ https:\/\/www.maxar.com\/open-data\/hurricane-delta ]\n* cutmix :  https:\/\/medium.com\/depurr\/cutmix-augmentation-in-python-bf099a97afac\n* cutmix :https:\/\/arxiv.org\/abs\/1905.04899\n* Augmentation : https:\/\/wandb.ai\/authors\/tfaugmentation\/reports\/Modern-Data-Augmentation-Techniques-for-Computer-Vision--VmlldzoxODA3NTQ\n* blog : https:\/\/omdena.com\/blog\/rooftops-classification\/\n","6ec4872f":"# Visualizing Images","8190192b":"# Visualizing Geographical Areas of Images"}}