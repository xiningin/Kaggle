{"cell_type":{"dd113d6e":"code","a261da45":"code","ee75c849":"code","c3f40857":"code","cff895b4":"code","5e7676b7":"code","0f8466a8":"code","c87e025a":"code","1b3ce520":"code","c23815b9":"code","c781b004":"code","bee7020a":"code","54b946ab":"code","7d8e5f75":"code","8850f60d":"code","745bab94":"code","145b14c6":"code","22debd6b":"code","720db72a":"code","43944a6a":"code","5e160769":"code","926132c1":"code","6603b0b8":"code","10e92ae4":"code","9b7c5c10":"code","87385ea5":"code","f06ebe6b":"code","20d45ca7":"code","ec231f98":"code","f950c5ff":"code","2b60d58d":"code","18b1e4b8":"code","65ca54e0":"code","1d1b7c89":"code","eaa1cf22":"code","3fa0de1d":"code","7a60852a":"markdown","de0d14af":"markdown","6ecaee88":"markdown","4cedefc8":"markdown","329305d6":"markdown"},"source":{"dd113d6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a261da45":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","ee75c849":"train_df = pd.read_csv('\/kaggle\/input\/jan2022playground\/Playground_folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nsub_df =  pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","c3f40857":"train_df.head()","cff895b4":"train_df.shape","5e7676b7":"train_df.info()","0f8466a8":"train_df.describe()","c87e025a":"train_df.head(3)","1b3ce520":"prods = train_df.groupby(['product'])['num_sold'].sum().reset_index()\nprods","c23815b9":"\nplt.figure()\nsns.barplot(x = prods['product'], y = prods['num_sold'])\n\nplt.title(\"Products Sold by category\")\nplt.xlabel('Kaggle Merchandies')\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.ylabel('No. of sold')\n\nplt.show()","c781b004":"store_selling = train_df.groupby(['store'])['num_sold'].sum().reset_index()\nstore_selling","bee7020a":"plt.figure()\nsns.barplot(x = store_selling['store'], y = store_selling['num_sold'])\n\nplt.title(\"Merchandise Sold by Store\")\nplt.xlabel('Store')\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.ylabel('No. of sold by Store')\n\nplt.show()","54b946ab":"ctryWise = train_df.groupby(['country'])['num_sold'].sum().reset_index()\nctryWise","7d8e5f75":"plt.figure()\nsns.barplot(x = ctryWise['country'], y = ctryWise['num_sold'])\n\nplt.title(\"Merchandise Sold by Store\")\nplt.xlabel('Country')\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.ylabel('No. of sold Item')\n\nplt.show()","8850f60d":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor","745bab94":"#preparing the dataset through folds\n\nfeatures = [col for col in train_df.columns if col not in ('num_sold', 'kfold')]\nobj_col = [col for col in train_df.columns if 'cat' in col]\n\ntest_df = test_df[features]","145b14c6":"numerical_transformer = SimpleImputer(strategy = 'most_frequent') # Your code here\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer' , SimpleImputer(strategy = 'most_frequent')),\n     ('oneHot' , OneHotEncoder(handle_unknown='ignore'))\n]) # Your code here\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, obj_col)\n    ])\n\n# Define model\n# model = RandomForestRegressor()\nmodel1 = GradientBoostingRegressor(n_estimators=50,\n            learning_rate=0.005, max_depth=6)","22debd6b":"def smape(A, F):\n    return 100\/len(A) * np.sum(2 * np.abs(F - A) \/ (np.abs(A) + np.abs(F)))","720db72a":"# preds = []\n\n# for i in range(5):\n#     X_train = train_df[train_df['kfold'] != i].reset_index(drop=False)\n#     X_valid = train_df[train_df['kfold'] == i].reset_index(drop=False)\n#     X_test = test_df.copy()\n    \n#     y_train  = X_train['num_sold']\n#     y_valid  = X_valid['num_sold']\n    \n#     X_train = X_train[features]\n#     X_valid = X_valid[features]\n    \n#     print(X_train)\n#     # Bundle preprocessing and modeling code in a pipeline\n#     my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                                   ('model', model1)\n#                                  ])\n    \n#     # Preprocessing of training data, fit model \n# #     my_pipeline.fit(X_train, y_train)\n\n#     # # Preprocessing of validation data, get predictions\n#     predictions = my_pipeline.predict(X_test)\n#     preds.append(predictions)\n    \n\n# smape = smape(y_valid, preds[:5260])\n    \n    ","43944a6a":"X_test","5e160769":"y_train.column = [\"num_sold\"]","926132c1":"y_train","6603b0b8":"# predictions\nsub_df['num_sold'] = preds\nsub_df.to_csv('submission.csv', index=False)","10e92ae4":"X = train_df.Error\ny = train_df.num_sold\nX.drop(['num_sold'], axis=1, inplace=True)\ny.columns = ['num_sold']","9b7c5c10":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n","87385ea5":"categorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]","f06ebe6b":"numerical_transformer = SimpleImputer(strategy = 'most_frequent') # Your code here\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer' , SimpleImputer(strategy = 'most_frequent')),\n     ('oneHot' , OneHotEncoder(handle_unknown='ignore'))\n]) # Your code here\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\n# model = RandomForestRegressor()\nmodel1 = GradientBoostingRegressor(n_estimators=50,\n            learning_rate=0.005, max_depth=6)","20d45ca7":"def smape(A, F):\n    return 100\/len(A) * np.sum(2 * np.abs(F - A) \/ (np.abs(A) + np.abs(F)))\n","ec231f98":"# # Bundle preprocessing and modeling code in a pipeline\n# my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                               ('model', model1)\n#                              ])\n\n# # Preprocessing of training data, fit model \n# my_pipeline.fit(X_train, y_train)\n\n# # # Preprocessing of validation data, get predictions\n# preds = my_pipeline.predict(test_df)\n\n# smape = smape(y_valid, preds[:5260])","f950c5ff":"smape","2b60d58d":"predictions = []\npredictions.append(preds)","18b1e4b8":"# predictions\nsub_df['num_sold'] = preds\nsub_df.to_csv('submission.csv', index=False)","65ca54e0":"# import optuna","1d1b7c89":"\n# preds = []\n\n# def run(trial):\n# #     i = 0\n#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n# #     reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n#     reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n#     subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n#     colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n#     max_depth = trial.suggest_int(\"max_depth\", 1, 12)\n    \n    \n#     model = GradientBoostingRegressor(\n#             n_estimators=1000,\n#             learning_rate=learning_rate,\n#             subsample=subsample,\n            \n#             max_depth=max_depth)\n    \n#     my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                               ('model', model)\n#                              ])\n\n#     # Preprocessing of training data, fit model \n#     my_pipeline.fit(X_train, y_train)\n\n#     # # Preprocessing of validation data, get predictions\n#     preds = my_pipeline.predict(test_df)\n\n#     smape(y_valid, preds[:5260])\n# #     print(smape)","eaa1cf22":"# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(run, n_trials=5)","3fa0de1d":"# study.best_params","7a60852a":"> * <b>Outliers exist in the train data set<\/b>","de0d14af":"---\n\n# Preprocessing and Model Training","6ecaee88":"---\n# 2. Exploratory Data Analysis","4cedefc8":"---\n\n# 1. Overview of Data","329305d6":"----\n\nfold work end here"}}