{"cell_type":{"68a4d70d":"code","55f70fe8":"code","5bf7ce24":"code","17999b9d":"code","c01c6e26":"code","e7a293a7":"code","3c9e2a77":"code","b1f18124":"code","1d744a6c":"code","4081ea84":"code","316fef44":"code","399adda1":"code","edefdf1a":"code","983e4729":"code","b14107cc":"code","a0f5fa78":"code","12d3bf6e":"code","c5375f64":"code","0d4d5435":"code","a1f70dec":"code","a810de6f":"code","2d06ea26":"code","bf07029b":"code","11f8bb25":"code","7d6286da":"code","91d2ff5e":"markdown","5587be1b":"markdown","8e70fe72":"markdown","2b9b53f2":"markdown","10634342":"markdown","36077738":"markdown","7d26802f":"markdown","8a164b85":"markdown","cf30a0b5":"markdown","d925e0cd":"markdown","540d0f0b":"markdown","5006e762":"markdown","1948e033":"markdown","42b95bb7":"markdown","26ed704e":"markdown","d0a0c5a4":"markdown","1ad87b03":"markdown","a5a97301":"markdown","51f3f20e":"markdown","e764d753":"markdown","7a8fb730":"markdown","427cf589":"markdown","f3d9ec64":"markdown","54ddb31d":"markdown"},"source":{"68a4d70d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","55f70fe8":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","5bf7ce24":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","17999b9d":"heart = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","c01c6e26":"heart.head()","e7a293a7":"heart.info()","3c9e2a77":"heart.describe()","b1f18124":"import seaborn as sns\ndef plot_correlation_map( df ):\n    corr = heart.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\nplot_correlation_map( heart )   ","1d744a6c":"heart.hist()","4081ea84":" heart['target'].value_counts()","316fef44":"rcParams['figure.figsize'] = 8,6\nplt.bar(heart['target'].unique(), heart['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","399adda1":"dataset = pd.get_dummies(heart, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])","edefdf1a":"dataset","983e4729":"standardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","b14107cc":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","a0f5fa78":"knn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","12d3bf6e":"plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')\nplt.show()","c5375f64":"print(\"The score for K Neighbors Classifier is {}% with {} nieghbors.\".format(knn_scores[7]*100, 8))","0d4d5435":"svc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","a1f70dec":"colors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')\nplt.show()","a810de6f":"print(\"The score for Support Vector Classifier is {}% with {} kernel.\".format(svc_scores[0]*100, 'linear'))","2d06ea26":"dt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","bf07029b":"plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'blue')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')\nplt.show()","11f8bb25":"rf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","7d6286da":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for variable no of Estimators')\nplt.show()","91d2ff5e":"\n**Support Vector Classifier**\n\nThere are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score.\n","5587be1b":"\n**Import libraries**\n\nLet's first import all the necessary libraries. I'll use numpy and pandas to start with. For visualization, I will use pyplot subpackage of matplotlib, use rcParams to add styling to the plots and rainbow for colors. For implementing Machine Learning models and processing of data, I will use the sklearn library.\n","8e70fe72":"\n**Random Forest Classifier**\n\nNow, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect.\n","2b9b53f2":"I have the scores for different neighbor values in the array knn_scores. I'll now plot it and see for which value of K did I get the best scores.","10634342":"\n# Conclusion\n\nIn this Notebook, we have used Machine Learning for  prediction of patient who has  suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. I then applied four Machine Learning algorithms, K Neighbors Classifier, Support Vector Classifier, Decision Tree Classifier and Random Forest Classifier. I varied parameters across each model to improve their scores. In the end, K Neighbors Classifier achieved the highest score of 87% with 8 nearest neighbors.\n","36077738":"The dataset is now loaded into the variable dataset. I'll just take a glimpse of the data using the desribe() and info() methods before I actually start processing and visualizing it.","7d26802f":"Now, we will use the StandardScaler from sklearn to scale my dataset.","8a164b85":"The scale of each feature column is different and quite varied as well. While the maximum for age reaches 77, the maximum of chol (serum cholestoral) is 564.","cf30a0b5":"The model achieved the best accuracy at three values of maximum features, 2, 4 and 18.","d925e0cd":"\n\nNext, I'll import all the Machine Learning algorithms I will be using.\n\n   - K Neighbors Classifier\n   - Support Vector Classifier\n   - Decision Tree Classifier\n   - Random Forest Classifier\n\n","540d0f0b":"*The score for Support Vector Classifier is 83.0% with linear kernel.*\n\n\n\n**Decision Tree Classifier**\n\nHere, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of max_features and see which returns the best accuracy.\n","5006e762":"\n**Import dataset**\n\nNow that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file dataset.csv. I'll use the pandas read_csv method to read the dataset.\n","1948e033":"The maximum score is achieved when the total estimators are 100 or 500.","42b95bb7":"**Data Preprocessing **\n- Need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. First, I'll use the get_dummies method to create dummy columns for categorical variables.","26ed704e":"\n**Understanding the data**\n\nNow, we can use visualizations to better understand our data and then look at any processing we might want to do.\n","d0a0c5a4":"\n**K Neighbors Classifier**\n\nThe classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score.\n","1ad87b03":"\n**Machine Learning**\n\nI'll now import train_test_split to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data.\n","a5a97301":"We have selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases.","51f3f20e":"Total no 1s are 165 and 0s are 138, which is not 50% of 1s .","e764d753":"Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive. Next, I'll take a look at the histograms for each variable.","7a8fb730":"# Heart Disease Prediction","427cf589":"For processing the data, I'll import a few libraries. To split the available dataset for testing and training, I'll use the train_test_split method. To scale the features, I am using StandardScaler.","f3d9ec64":"*The linear kernel performed the best, being slightly better than rbf kernel.*","54ddb31d":"The score for K Neighbors Classifier is 87.0% with 8 nieghbors."}}