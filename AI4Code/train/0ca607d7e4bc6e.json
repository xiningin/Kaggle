{"cell_type":{"17b87863":"code","1c3cc04f":"code","0245a1f2":"code","947524d5":"code","ae8978ca":"code","2bd0a9cd":"code","59fbd88f":"code","d7a51412":"code","526e2349":"code","66eb62d0":"code","b4b88335":"code","cf23824b":"code","b964fa96":"code","bc831631":"code","82d30077":"code","2b668d39":"code","a857af42":"code","9c7fc280":"code","2b4cec0c":"code","a7d3cfcc":"code","046b8c0c":"code","9850950b":"code","54f551da":"code","6b7f0f23":"code","4c05145d":"code","02eba45c":"code","db3c5f32":"code","3f713ce3":"code","cdd6463e":"code","2a34582e":"code","c3d99677":"code","eb5b714a":"code","7e032936":"code","b09f77b9":"code","fc9581f0":"code","e5a623a1":"code","f9f54ed3":"code","cb84289c":"code","d63b655e":"code","33688d79":"code","4411020c":"code","c8b29743":"code","40e08447":"code","ead294e1":"code","1b385590":"code","c69019dd":"code","1f64f0d5":"code","b21563e5":"code","18893832":"code","8de22183":"code","127a9128":"code","a69ec2e9":"code","b5e6ccb1":"code","dc18d6c0":"code","106614af":"code","7d639746":"code","d0c4633e":"code","398ece44":"code","28e1de3b":"code","bae2dfda":"code","04a257bc":"code","113b0bf4":"code","5b249fe9":"code","909ee434":"code","73954d24":"code","d48e6aa3":"code","73f6d58b":"code","d50d754b":"markdown","efbfa384":"markdown","a95d6ecd":"markdown","8ea82c17":"markdown","5bed86b1":"markdown","950327c0":"markdown","4bab91ae":"markdown","c69dc0f8":"markdown","6b8293ad":"markdown","03c53221":"markdown","a855df7f":"markdown","6ad15e93":"markdown","8d77ceef":"markdown"},"source":{"17b87863":"import re\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n%matplotlib inline","1c3cc04f":"nltk.download('stopwords')","0245a1f2":"train_df = pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')\ntrain_df.head()","947524d5":"train_df.info()","ae8978ca":"train_df.describe()","2bd0a9cd":"print(\"[title] \",train_df['title'][0])\nprint(\"[title] \",train_df['text'][0])","59fbd88f":"train_df.head()","d7a51412":"f,ax = plt.subplots(1,2,figsize=(14,6))\ntrain_df['label'].value_counts().plot.pie(explode=[0, 0.1],autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Pie plot - Label')\nax[0].set_ylabel('')\nsns.countplot('label', data = train_df, ax=ax[1])\nax[1].set_title('Count plot - Label')\nplt.show()","526e2349":"def get_length_alphabets(text):\n    text = str(text)\n    return len(text)","66eb62d0":"def get_length_words(text):\n    text = str(text)\n    return len(text.split(' '))","b4b88335":"train_df['length_alphabets'] = train_df['text'].apply(get_length_alphabets)\ntrain_df['length_words'] = train_df['text'].apply(get_length_words)","cf23824b":"train_df.head()","b964fa96":"f, ax = plt.subplots(2,1,figsize=(15,10))\n\nax[0].set_title('Text per alphabets length')\nax[0].hist(train_df[train_df['label'] == 0]['length_alphabets'], alpha = 0.4, bins=150, label = 'Real')\nax[0].hist(train_df[train_df['label'] == 1]['length_alphabets'], alpha = 0.4, bins=150, label = 'Not')\nax[0].set_xlabel('length')\nax[0].set_ylabel('numbers')\nax[0].legend(loc='upper right')\nax[0].set_xlim(0,60000)\nax[0].grid()\n\nax[1].set_title('Text per words length')\nax[1].hist(train_df[train_df['label'] == 0]['length_words'], alpha = 0.4, bins=150, label = 'Real')\nax[1].hist(train_df[train_df['label'] == 1]['length_words'], alpha = 0.4, bins=150, label = 'Not')\nax[1].set_xlabel('length')\nax[1].set_ylabel('numbers')\nax[1].legend(loc='upper right')\nax[1].set_xlim(0,10000)\nax[1].grid()\n\nf.tight_layout()\nplt.show()","bc831631":"train_df[train_df['text'].isnull()].head()","82d30077":"train_df['text'].fillna(\"\", inplace=True)\ntest_df['text'].fillna(\"\", inplace=True)","2b668d39":"train_df[train_df['author'].isnull()].head()","a857af42":"train_df['author'] = train_df['author'].fillna('unknown')\ntest_df['author'] = test_df['author'].fillna('unknown')","9c7fc280":"train_df[train_df['title'].isnull()].head()","2b4cec0c":"# Get average title size\ntrain_df['title_size'] = train_df['title'].apply(lambda x: len(str(x)))\n\n# If title size == 3(\"NAN\") then change title size = 0\ntrain_nan_index = train_df[train_df['title_size']==3].index\ntrain_df['title_size'][train_nan_index] = 0 \n\ntrain_avg_title_size = int(train_df.value_counts(['title_size']).mean())\nprint(\"train avg_title_size: \",train_avg_title_size)","a7d3cfcc":"# Get average title size\ntest_df['title_size'] = test_df['title'].apply(lambda x: len(str(x)))\n\n# If title size == 3(\"NAN\") then change title size = 0\ntest_nan_index = test_df[test_df['title_size']==3].index\ntest_df['title_size'][test_nan_index] = 0 \n\ntest_avg_title_size = int(test_df.value_counts(['title_size']).mean())\nprint(\"test avg_title_size: \",test_avg_title_size)","046b8c0c":"#change title \"NAN\" to text[:98]\ntrain_df['title'][train_nan_index] = train_df['text'][train_nan_index].apply(lambda x: str(x)[:train_avg_title_size])\n\ntest_df['title'][test_nan_index] = test_df['text'][test_nan_index].apply(lambda x: str(x)[:test_avg_title_size])","9850950b":"train_df[train_df['id']==120]","54f551da":"train_df.head()","6b7f0f23":"train_df.isnull().sum()","4c05145d":"train_df.drop(['length_alphabets','length_words','title_size'], axis=1, inplace = True)\ntest_df.drop(['title_size'], axis=1, inplace = True)","02eba45c":"train_df.head()","db3c5f32":"test_df.head()","3f713ce3":"train_df['text']= train_df['author'] + \" \" + train_df['text']\ntest_df['text']= test_df['author'] + \" \" + test_df['text']","cdd6463e":"train_df.drop(['id','title','author'], axis=1, inplace=True)\ntest_df.drop(['id','title','author'], axis=1, inplace=True)","2a34582e":"train_df['text'][0]","c3d99677":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer()","eb5b714a":"def preprocess(text,stem=False):\n    text = text.lower()  # lowercase\n\n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    text = re.sub(r\"'\", \"\", text)\n    text = re.sub('\\s+', ' ', text).strip()  # Remove and double spaces\n    text = re.sub(r'&amp;?', r'and', text)  # replace & -> and\n    text = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", text)  # Remove URLs\n    # remove some puncts (except . ! # ?)\n    text = re.sub(r'[:\"$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+', '', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'EMOJI', text)\n    \n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            tokens.append(lemmatizer.lemmatize(token))\n    return \" \".join(tokens)","7e032936":"train_df['text'] = train_df['text'].apply(lambda x: preprocess(x))\ntest_df['text'] = test_df['text'].apply(lambda x: preprocess(x))","b09f77b9":"plt.figure(figsize = (10,10))\nwc = WordCloud(max_words = 2000, width = 1600, height = 800).generate(\" \".join(train_df[train_df.label == 0].text))\nplt.axis('off')\nplt.title('Real News')\nplt.imshow(wc,interpolation = 'bilinear')","fc9581f0":"plt.figure(figsize = (10,10))\nwc = WordCloud(max_words = 2000, width = 1600, height = 800).generate(\" \".join(train_df[train_df.label == 1].text))\nplt.axis('off')\nplt.title('Fake News')\nplt.imshow(wc,interpolation = 'bilinear')","e5a623a1":"x_train = train_df['text']\ny_train = train_df['label']","f9f54ed3":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)","cb84289c":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)","d63b655e":"word_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\nprint(\"Vocab size : \",vocab_size)","33688d79":"x_train = pad_sequences(\n    tokenizer.texts_to_sequences(x_train),\n    maxlen = 256)\nx_val = pad_sequences(\n    tokenizer.texts_to_sequences(x_val),\n    maxlen = 256)\nx_test = pad_sequences(\n    tokenizer.texts_to_sequences(test_df['text']),\n    maxlen = 256)","4411020c":"print(\"x_val shape:\", x_val.shape)\nprint(\"y_val shape:\", y_val.shape)","c8b29743":"print(\"x_train shape:\", x_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"First Train data squence info : \", x_train[0].shape)\nprint(x_train[0])","40e08447":"inputs = Input(shape=(256,), dtype='int32')\n\nembedding = tf.keras.layers.Embedding(vocab_size, 300)(inputs)\nnet = SpatialDropout1D(0.2)(embedding)\nnet = Bidirectional(LSTM(128,dropout=0.2, recurrent_dropout=0.2))(net)\nnet = Dense(64,activation = 'relu')(net)\nnet = Dropout(0.3)(net)\nnet = Dense(1,activation = 'sigmoid')(net)\n\noutputs = net\nmodel = tf.keras.Model(inputs,outputs)","ead294e1":"model.summary()","1b385590":"model.compile(optimizer = tf.keras.optimizers.Adam(),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])","c69019dd":"history = model.fit(\n    x_train,\n    y_train,\n    batch_size=1024, \n    epochs=10,\n    validation_data=(x_val, y_val)\n)","1f64f0d5":"f,ax = plt.subplots(2,1,figsize=(20,15))\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Model Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Model Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epochs')\nax[1].legend(['train', 'val'], loc = 'upper left')\n\nf.tight_layout()\nplt.show()","b21563e5":"!pip install --quiet transformers\nfrom transformers import TFBertModel, BertTokenizer","18893832":"#get BERT layer\nbert_layers = TFBertModel.from_pretrained('bert-base-uncased')\n\n#get BERT tokenizer\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")","8de22183":"encode = bert_tokenizer.encode(\"Hi nice meet you !\")\ndecode = bert_tokenizer.decode(encode)\n\nprint(\"Encode: \", encode)\nprint(\"Decode: \", decode)","127a9128":"def bert_encode(data, max_len) :\n    input_ids = [] \n    attention_masks = []\n    \n    for i in range(len(data)):\n        encoded = bert_tokenizer.encode_plus(data[i],\n                                        add_special_tokens=True,\n                                        max_length=max_len,\n                                        pad_to_max_length=True,\n                                        return_attention_mask=True)\n        \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","a69ec2e9":"x_train_bert = train_df['text']\ny_train_bert = train_df['label']\n\ntrain_input_ids, train_attention_masks = bert_encode(x_train_bert,60)","b5e6ccb1":"input_ids = tf.keras.Input(shape=(60,),dtype='int32',name='input_ids')\nattention_masks = tf.keras.Input(shape=(60,),dtype='int32',name='attention_masks')\n\noutput = bert_layers([input_ids,attention_masks])\noutput = output[1]\nnet = tf.keras.layers.Dense(32,activation='relu')(output)\nnet = tf.keras.layers.Dropout(0.2)(net)\nnet = tf.keras.layers.Dense(1,activation='sigmoid')(net)\noutputs = net\n\nmodel = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = outputs)","dc18d6c0":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","106614af":"model.summary()","7d639746":"history = model.fit(\n    [train_input_ids, train_attention_masks],\n    y_train_bert,\n    validation_split=0.2, \n    epochs=10,\n    batch_size=32)","d0c4633e":"f,ax = plt.subplots(2,1,figsize=(20,15))\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Model Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_xticks([0,1,2,3,4,5,6,7,8,9])\nax[0].legend(['BERT_train', 'BERT_val'], loc='upper left')\n\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Model Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epochs')\nax[1].set_xticks([0,1,2,3,4,5,6,7,8,9])\nax[1].legend(['train', 'val'], loc = 'upper left')\n\nf.tight_layout()\nplt.show()","398ece44":"submission = pd.read_csv('\/kaggle\/input\/fake-news\/submit.csv')\nsubmission.shape","28e1de3b":"submission.head()","bae2dfda":"test_df.head()","04a257bc":"bert_x_test = test_df['text']","113b0bf4":"test_input_ids, test_attention_masks = bert_encode(bert_x_test,60)","5b249fe9":"pred = model.predict([test_input_ids,test_attention_masks])","909ee434":"pred[:5]","73954d24":"submission['label'] = (pred>0.5).astype(int)","d48e6aa3":"submission.head()","73f6d58b":"submission.to_csv('submission.csv', index=False)","d50d754b":"# 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray","efbfa384":"![collection-newspapers1.jpg](attachment:50848b57-11c4-4d1e-8f63-70fa63965f75.jpg)","a95d6ecd":"***\n## My Workflow\n\n#### 1. Import & Install libray\n* Import Basic libray\n* Import Enginnering libray\n\n#### 2. Check out my data\n* Check Shape \/ Info \/ Describe\n\n#### 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Label Percent\n* Text length \/ Word length per Label Percent\n\n#### 4. Preprocessing Data\n* Null value processing\n* Drop columns\n\n#### 5. Feature Engineering\n* Concat \"author\" & \"text\" Columns\n* Nomalization\n* Plot WordClouds\n* Words Tokenization\n* Split Train \/ Test \/ Validation Data\n\n#### 6. Modeling\n* LSTM Modeling\n* Bert Modeling\n\n#### 7. Submission\n* Submit the predictions\n<br\/><br\/>\n***","8ea82c17":"### - BERT Modeling","5bed86b1":"# 2. Check out my data\n* Check Shape \/ Info \/ Describe","950327c0":"##### reference\n* https:\/\/www.kaggle.com\/mnavaidd\/tweet-classification-using-lstm-bert\/notebook?select=glove.6B.zip\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\n###  If this notebook is useful for your kaggling, \"UPVOTE\" for it \ud83d\udc40\n#### THX to Reading My Notebook\ud83c\udf08","4bab91ae":"# Fake News\n\n### Intro\n* lable == 1: unreliable\n* label == 0: reliable","c69dc0f8":"# 7. Submission\n* Submit the predictions","6b8293ad":"# 5. Feature Engineering\n* Concat \"author\" & \"text\" Columns\n* Nomalization\n* Plot WordClouds\n* Words Tokenization\n* Split Train \/ Test \/ Validation Data","03c53221":"# 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Label Percent\n* Text length \/ Word length per Label Percent","a855df7f":"# 4. Preprocessing Data\n* Null value processing\n* Drop columns","6ad15e93":"#### WordCloud\n* Label == 0 Real News\n* Label == 1 Fake News","8d77ceef":"# 6. Modeling\n* LSTM Modeling\n* Bert Modeling"}}