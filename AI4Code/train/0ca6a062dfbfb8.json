{"cell_type":{"46a2b987":"code","ba57bd9a":"code","e3138ec8":"code","1eb05529":"code","5c47f0df":"code","4abb6842":"code","6a88342a":"code","1976efb1":"code","e9705666":"code","a11c9c21":"code","47375c01":"code","b2762d5c":"code","e12b603a":"code","c678d68c":"code","310a1cdb":"code","13425063":"code","88753d86":"code","e434f726":"code","5e2bc664":"code","bd1073d0":"code","ff19727e":"code","1faee4a1":"code","3abdfe5c":"code","005d2d85":"code","f5fa9f44":"code","9cf73564":"code","d54fd0fc":"code","a5dc5b8f":"code","8fc3fb6f":"code","78a5dbd3":"code","b237e2c6":"code","3d89dc58":"code","67cc3e7f":"code","7d566c22":"code","b179ae80":"code","79122259":"code","6b6086a4":"code","0d24ace1":"code","f8c88a9d":"code","126a98cf":"code","87c9baac":"code","d356ce9f":"code","2a3a786d":"code","d6c01e1f":"code","6621f18b":"code","ba376e25":"code","240abd31":"code","37e69401":"code","86d15dcb":"code","c1f16e00":"code","71603056":"code","63e94042":"code","f96fb1eb":"code","8cd62b88":"code","1467e933":"markdown","dd291740":"markdown","3b80f41c":"markdown","5609061e":"markdown","b40c1920":"markdown","355c857b":"markdown","461ff46e":"markdown","49ebb335":"markdown","9fe91bb2":"markdown","e9a1f80b":"markdown","7966ed9b":"markdown","7d01a6d2":"markdown","566374a9":"markdown","ca83a274":"markdown"},"source":{"46a2b987":"import numpy  as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","ba57bd9a":"# load the data into dataframe\n\ncountry = pd.read_csv(\"..\/input\/country\/Country-data.csv\")\ncountry.head()","e3138ec8":"# checking no. of rows and columns in the dataset\ncountry.shape","1eb05529":"# exports, import and health are given as % of of gdp. lets convert them into exact nos.\ncountry['exports'] = country['exports']*country['gdpp']\/100\ncountry['imports'] = country['imports']*country['gdpp']\/100\ncountry['health'] = country['health']*country['gdpp']\/100","5c47f0df":"# checking numeric variables\ncountry.describe()","4abb6842":"# checking the information\ncountry.info()\n\n# no null values in the dataset","6a88342a":"# lets reconfirm the null values in the dataset\ncountry.isnull().sum()","1976efb1":"# lets check for outliers in the data set\ncols = country.drop(\"country\",1)\n\nplt.figure(figsize=(20,15))\nfor idx,col in enumerate(cols):\n    plt.subplot(3, 3, idx+1)\n    sns.boxplot(y=col, data =country)\n    plt.title(\"Box Plot for \"+ col)\n    \nplt.show()","e9705666":"# lets check the correlation between different features\nplt.figure(figsize = (20,10))        \nsns.heatmap(country.corr(),annot = True)","a11c9c21":"# Lets apply scaling to the variables\nfrom sklearn.preprocessing import StandardScaler\n\ndata = country.drop([\"country\"],axis = 1)\n\nscaler = StandardScaler()\nscaled_data =  scaler.fit_transform(data)\n\n# lets check the dataframe with the scaled values\nscaled_data","47375c01":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","b2762d5c":"# fit the model to the scaled data\npca.fit(scaled_data)","e12b603a":"pca.components_","c678d68c":"#Let's check the variance ratios\nprint(\"variance explained by PCs:\",pca.explained_variance_ratio_)","310a1cdb":"# cumulative variance explained by PCs\nprint(\"\\ncumulative variance explained by PCs:\",np.cumsum(pca.explained_variance_ratio_))\n\n# top 3 PCs are explaining 87% variation present in the dataset","13425063":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","88753d86":"#Let's try and check the first three components now\ncolnames = list(data.columns)\npcs_df = pd.DataFrame({ 'Feature':colnames,'PC1':pca.components_[0],'PC2':pca.components_[1],'PC3':pca.components_[2]})\npcs_df","e434f726":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","5e2bc664":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=3)","bd1073d0":"# fit and transform the scaled dataset \ndf_train_pca = pca_final.fit_transform(scaled_data)\ndf_train_pca.shape","ff19727e":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","1faee4a1":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (20,10))\nsns.heatmap(corrmat,annot = True)","3abdfe5c":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","005d2d85":"# lets take the transpose of the trained dataset\npc = np.transpose(df_train_pca)\npc","f5fa9f44":"#Let's create the newer matrix according to the given principal components\nrownames = list(country['country'])\npcs_df2 = pd.DataFrame({'country':rownames,'PC1':pc[0],'PC2':pc[1],'PC3':pc[2]})\npcs_df2.head()","9cf73564":"#Let's do the outlier analysis before proceeding to clustering\nplt.boxplot(pcs_df2.PC1)\nQ1 = pcs_df2.PC1.quantile(0.05)\nQ3 = pcs_df2.PC1.quantile(0.95)\nIQR = Q3 - Q1\npcs_df2 = pcs_df2[(pcs_df2.PC1 >= Q1) & (pcs_df2.PC1 <= Q3)]","d54fd0fc":"# outlier treatment for PC2\nplt.boxplot(pcs_df2.PC2)\nQ1 = pcs_df2.PC2.quantile(0.05)\nQ3 = pcs_df2.PC2.quantile(0.95)\nIQR = Q3 - Q1\npcs_df2 = pcs_df2[(pcs_df2.PC2 >= Q1) & (pcs_df2.PC2 <= Q3)]","a5dc5b8f":"# outlier treatment for PC3\nplt.boxplot(pcs_df2.PC3)\nQ1 = pcs_df2.PC3.quantile(0.05)\nQ3 = pcs_df2.PC3.quantile(0.95)\nIQR = Q3 - Q1\ndat3 = pcs_df2[(pcs_df2.PC3 >= Q1 ) & (pcs_df2.PC3 <= Q3)]","8fc3fb6f":"#Outlier analysis is now done.Let's check the data again.\npcs_df2.shape","78a5dbd3":"#let's check the spread of the dataset\nsns.scatterplot(x='PC1',y='PC2',data=pcs_df2)","b237e2c6":"def hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","3d89dc58":"from numpy.random import uniform\nfrom random import sample\nfrom sklearn.neighbors import NearestNeighbors\nfrom math import isnan\nhopkins(pcs_df2.drop([\"country\"],1))","67cc3e7f":"# lets scale the dataset\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\n\npcs_df3 = pcs_df2\npcs_df3 = standard_scaler.fit_transform(pcs_df3.drop(['country'],axis=1))","7d566c22":"#Let's check the silhouette score first to identify the ideal number of clusters\nfrom sklearn.metrics import silhouette_score\nsse_ = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k).fit(pcs_df3)\n    sse_.append([k, silhouette_score(pcs_df3, kmeans.labels_)])","b179ae80":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);","79122259":"#Let's use the elbow curve method to identify the ideal number of clusters.\nssd = []\nfor num_clusters in list(range(1,10)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(pcs_df3)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)","6b6086a4":"# Lets make a model with k = 5 first\nmodel_clus5 = KMeans(n_clusters = 5, max_iter=50)\nmodel_clus5.fit(pcs_df3)","0d24ace1":"# lets concatenate the cluster ids to the PCA transformed dataset\npcs_df4=pcs_df2\npcs_df4.index = pd.RangeIndex(len(pcs_df4.index))\ndat_km = pd.concat([pcs_df4, pd.Series(model_clus5.labels_)], axis=1)\ndat_km.columns = ['country', 'PC1', 'PC2','PC3','ClusterID']\ndat_km.head()","f8c88a9d":"# add cluster ids to the original dataset\npcs_df5=pd.merge(country,dat_km,on='country')\npcs_df6=pcs_df5[['country','child_mort','exports','imports','health','income','inflation','life_expec','total_fer','gdpp','ClusterID']]\npcs_df6.head()","126a98cf":"pcs_df6.shape","87c9baac":"# lets check if each cluster has enough no. of data points\npcs_df6['ClusterID'].value_counts()","d356ce9f":"# lets take mean of the available features, these would be helpful for cluster analysis\nclu_chi  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).child_mort.mean())\nclu_exp  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).exports.mean())\nclu_imp  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).imports.mean())\nclu_hea  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).health.mean())\nclu_inc  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).income.mean())\nclu_inf  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).inflation.mean())         \nclu_lif  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).life_expec.mean())\nclu_tot  = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).total_fer.mean())\nclu_gdpp = pd.DataFrame(pcs_df6.groupby([\"ClusterID\"]).gdpp.mean())\n\nfinal = pd.concat([pd.Series([0,1,2,3,4]),clu_chi,clu_exp,clu_imp,clu_hea,clu_inc,clu_inf,clu_lif,clu_tot,clu_gdpp], axis=1)\nfinal.columns = [\"ClusterID\", \"Child_Mortality\", \"Exports\", \"Imports\",\"Health_Spending\",\"Income\",\"Inflation\",\"Life_Expectancy\",\"Total_Fertility\",\"GDPpcapita\"]\nfinal","2a3a786d":"# lets visualise the cluster formed\nfig= plt.figure(figsize = (25,20))\n\nfor idx,col in enumerate(final.drop(\"ClusterID\",1)):\n    plt.subplot(3,3,idx+1)\n    sns.barplot(x=final.ClusterID, y=final[col])\n    plt.title(col,fontsize = 15,fontweight='bold')\n    \nplt.show()","d6c01e1f":"# from the above data, we know that average gdpp for cluster 0 is minimum i.e. 1500\n# lets create a dataframe with gdpp less than 1500\npoor1 = country[country.gdpp<=1500]\n\n# income is minimum average income is around 3100, lets filter the dataset\npoor2 = poor1[poor1.income<=3100]\n\n# max average child mort is around 76, lets apply another filter\npoor3 = poor2[poor2.child_mort>=76]\nprint(\"Final list of poor countries\\n:\",poor3)","6621f18b":"# single linkage procedure.\nmergings = linkage(pcs_df3, method = \"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","ba376e25":"#Let's try complete linkage method\nmergings = linkage(pcs_df3, method = \"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","240abd31":"# we see good clustering here, lets cut the clusters\nclusterCut = pd.Series(cut_tree(mergings, n_clusters = 5).reshape(-1,))\npcs_df2_hc = pd.concat([pcs_df2, clusterCut], axis=1)\npcs_df2_hc.columns = ['country', 'PC1', 'PC2','PC3','ClusterID']","37e69401":"pcs_df2_hc.head()","86d15dcb":"pcs_df7=pd.merge(country,pcs_df2_hc,on='country')\npcs_df8=pcs_df7[['country','child_mort','exports','imports','health','income','inflation','life_expec','total_fer','gdpp','ClusterID']]\npcs_df8.head()","c1f16e00":"# lets check if the clusters formed have good no. of countries \npcs_df8['ClusterID'].value_counts()","71603056":"#Cluster 4 doesn't have enough amount of clusters. Let's check other clusters\npcs_df8[pcs_df8['ClusterID']==2]","63e94042":"# cluster 3\npcs_df8[pcs_df8['ClusterID']==3]","f96fb1eb":"# cluster 0\npcs_df8[pcs_df8['ClusterID']==0]","8cd62b88":"#looks like clusters are not formed correctly, lets visualize the clusters, we would stick with the clusters we formed earlier","1467e933":"from above graphs we can conclude that gdpp, income & child mort seems to have good variations for different clusters, lets use these variables to filter the dataset and come up with the final list of poor countries","dd291740":"# PCA","3b80f41c":"hopkin score of .80 is a good number, data is good enough to make clusters","5609061e":"## Lets check whether the data is good enough for clustering using hopkins method\n\n### The Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n\nIf the value is between {0.01, ...,0.3}, the data is regularly spaced.\n\nIf the value is around 0.5, it is random.\n\nIf the value is between {0.7, ..., 0.99}, it has a high tendency to cluster.","b40c1920":"There are outliers present in the dataset, we will treat them later","355c857b":"## Problem Statement\n\nHELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities. It runs a lot of operational projects from time to time along with advocacy drives to raise awareness as well as for funding purposes.\n \nAfter the recent funding programmes, they have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. The significant issues that come while making this decision are mostly related to choosing the countries that are in the direst need of aid. \n \nAnd this is where you come in as a data analyst. Your job is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.  The datasets containing those socio-economic factors and the corresponding data dictionary are provided below.\n","461ff46e":"features present in the transformed dataset are not correlated to each other ","49ebb335":"As expected there are outliers present in the dataset","9fe91bb2":"Lot of variables are correlated hence its a good data set to apply PCA.","e9a1f80b":"# Hierarchical Clustering","7966ed9b":"Graph shows the same thing that we discussed above, top 3 PCs explaining 87% of the variation present in the dataset","7d01a6d2":"## Now lets do Silhouette Analysis\n### silhouette score=p\u2212qmax(p,q)\n\np is the mean distance to the points in the nearest cluster that the data point is not a part of\n\nq is the mean intra-cluster distance to all the points in its own cluster.\n\nThe value of the silhouette score range lies between -1 to 1.\n\nA score closer to 1 indicates that the data point is very similar to other data points in the cluster,\n\nA score closer to -1 indicates that the data point is not similar to the data points in its cluster.","566374a9":"silhoutee score is maximum for 5 clusters","ca83a274":"# Cluster Analysis"}}