{"cell_type":{"b6c9402c":"code","d3d9cbf6":"code","6f0aa7e5":"code","6ed4c650":"code","73924af3":"code","20f6ffa6":"code","7ac363fb":"code","b5d36316":"code","5f5949df":"code","4d810772":"code","eb3a51e9":"code","a25c2681":"code","e4a33333":"code","1eacdaeb":"code","0cb8c2cd":"code","4ddf1fcd":"code","9fd04582":"code","bed55b46":"code","fab747f7":"code","8beea21b":"code","80eab5ac":"code","98bc60ee":"code","682852ee":"code","892dd4c6":"code","f5d93757":"code","9f149bda":"code","0071874a":"code","ec7188b4":"code","528422db":"code","f74c8899":"code","358949d4":"code","6f866032":"code","e3ecf4e8":"code","13f7425b":"code","697fcf26":"code","dcc40da9":"code","96b80475":"code","6b124ff0":"code","3ebea2b1":"code","96f80de2":"code","cf5b88b1":"code","8aa76de6":"markdown","2012731e":"markdown","5cc4b5e4":"markdown","208d41fc":"markdown","4e958b6a":"markdown","63bdf1a4":"markdown","6337e6dc":"markdown","f597d99d":"markdown","d0780dfc":"markdown","8acc06e6":"markdown","3204aacd":"markdown","d9d476c9":"markdown","a022a96d":"markdown","38e9743d":"markdown","cb943f09":"markdown","425b76bd":"markdown","28f4208d":"markdown","57aaf878":"markdown","26fea773":"markdown","ddc63d88":"markdown","ab899bac":"markdown","770e5a84":"markdown"},"source":{"b6c9402c":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport warnings \n# Setup the libraries\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=Warning)\npd.set_option(\"display.width\", 100)\npd.set_option(\"display.max_columns\", 25)\npd.set_option(\"display.max_rows\", 25)\n%matplotlib inline\n# Check if it works\nprint(f\"Tensorflow Version: {tf.__version__}\")\nprint(\"Setup complete!\")","d3d9cbf6":"audi_data = pd.read_csv(\"..\/input\/audi-used-car-listings\/audi.csv\") # read in dataset\naudi_data # review data","6f0aa7e5":"audi_data.dtypes.to_frame() # check the type","6ed4c650":"audi_data.isna().mean().to_frame() # check the missing value","73924af3":"audi_data.info() # getting the information of our data","20f6ffa6":"# getting the uniqueness categorial variable\ncategorical = audi_data.select_dtypes([\"category\", \"object\"]).columns \nfor cat_col in categorical: # print every unique values in categorical columns\n    print(f\"{cat_col} : {audi_data[cat_col].nunique()} unique value(s)\")","7ac363fb":"# getting the discrete and contiuous variables\nnumeric = audi_data.select_dtypes([\"int\", \"float\"]).columns\nfor num_col in numeric: # print every unique values in numeric columns\n    print(f\"{num_col} : {audi_data[num_col].nunique()} unique values(s)\")","b5d36316":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the encoder\nlabel_encoder = LabelEncoder()\naudi_data[\"model\"] = label_encoder.fit_transform(audi_data[\"model\"])\n\n# Create a columns transformer\ncolumn_transformer = make_column_transformer(\n    # turn all values in this columns between 0 and 1\n    (MinMaxScaler(), [\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\", \"model\"]), \n    # encode all values in this columns between 0 and 1\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"transmission\", \"fuelType\"]),\n)\n\n# Create X & y (features and label)\nX = audi_data.drop(columns=[\"price\"])\ny = audi_data[\"price\"]\n\n# Build our train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Fit the columns transformer to our training data\ncolumn_transformer.fit(X_train)\n\n# Transform training and test data with normalization (MinMaxScaler) and encoder (OneHotEncoder, LabelBinarizer)\nX_train_normal = column_transformer.transform(X_train)\nX_test_normal = column_transformer.transform(X_test)","5f5949df":"def change_type(X_train, X_test, y_train, y_test):\n    # X_train and X_test (features)\n    X_train, X_test = tf.constant(X_train, dtype=tf.float32), tf.constant(X_test, dtype=tf.float32)\n    \n    # y_train and y_test (label)\n    y_train, y_test = tf.constant(y_train, dtype=tf.float32), tf.constant(y_test, dtype=tf.float32)\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = change_type(X_train_normal, X_test_normal, y_train, y_test)","4d810772":"# What does our data look like now\nX_train[0] # check the first rows","eb3a51e9":"# Checking the shape and dimension of rows and columns (features)\nX_train.shape, X_test.shape, X_train.ndim, X_test.ndim","a25c2681":"# Checking the shape and dimension of rows and columns (label)\ny_train.shape, y_test.shape, y_train.ndim, y_test.ndim","e4a33333":"# Checking the type (features and label)\nX_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype","1eacdaeb":"# Checking the len (features and label)\nlen(X_train), len(X_test), len(y_train), len(y_test)","0cb8c2cd":"# Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\", name=\"input_1\"),\n    tf.keras.layers.Dense(10, activation=\"relu\", name=\"input_2\"),\n    tf.keras.layers.Dense(1, name=\"output_layer\")\n], name=\"model_1\")\n\n# Compile the model\nmodel_1.compile(\n    loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['mae']\n)\n\n# Fit the model\nhistory_1 = model_1.fit(\n    X_train, y_train, epochs=50\n)","4ddf1fcd":"model_1.summary() # see the layer","9fd04582":"pd.DataFrame(history_1.history).plot(figsize=(17, 6), title=\"History_1 Curves\");","bed55b46":"model_1.evaluate(X_test, y_test)","fab747f7":"y_pred_1 = model_1.predict(X_test)\ny_pred_1.shape, y_pred_1.ndim","8beea21b":"# Make some function to reuse MAE and MSE\ndef mae(y_true, y_pred):\n    return tf.metrics.mean_absolute_error(y_true=y_true,\n                                          y_pred=tf.squeeze(y_pred))\ndef mse(y_true, y_pred):\n    return tf.metrics.mean_squared_error(y_true=y_true,\n                                         y_pred=tf.squeeze(y_pred))","80eab5ac":"mae_1 = mae(y_test, y_pred_1).numpy()\nmse_1 = mse(y_test, y_pred_1).numpy()\nf\"mae: {mae_1}, mse: {mse_1}\"","98bc60ee":"# Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(100, activation=\"relu\", name=\"input_1\"),\n    tf.keras.layers.Dense(100, activation=\"relu\", name=\"input_2\"),\n    tf.keras.layers.Dense(10, activation=\"relu\", name=\"input_3\"),\n    tf.keras.layers.Dense(10, activation=\"relu\", name=\"input_4\"),\n    tf.keras.layers.Dense(1, activation=\"relu\", name=\"output_1\")\n], name=\"model_2\")\n\n# Compile the model\nmodel_2.compile(\n    loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nhistory_2 = model_2.fit(\n    X_train, y_train, epochs=100\n)","682852ee":"model_2.summary() # see the layer","892dd4c6":"model_2.evaluate(X_test, y_test) # Evaluate","f5d93757":"y_pred_2 = model_2.predict(X_test)\ny_pred_2.shape, y_pred_2.ndim","9f149bda":"mae_2 = mae(y_test, y_pred_2).numpy()\nmse_2 = mse(y_test, y_pred_2).numpy()\nf\"mae: {mae_2}, mse: {mse_2}\"","0071874a":"pd.DataFrame(history_2.history).plot(figsize=(17, 6), title=\"History_2 Curves\"); # see the history","ec7188b4":"# Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(200, activation=\"relu\", name=\"input_1\"),\n    tf.keras.layers.Dense(100, activation=\"relu\", name=\"input_2\"),\n    tf.keras.layers.Dense(10, activation=\"relu\", name=\"input_3\"),\n    tf.keras.layers.Dense(1, activation=\"relu\", name=\"output_layer\")\n], name=\"model_3\")\n\n# Compile the model\nmodel_3.compile(\n    loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nhistory_3 = model_3.fit(\n    X_train, y_train, epochs=100\n)","528422db":"model_3.summary() # see the layer","f74c8899":"model_3.evaluate(X_test, y_test)","358949d4":"y_pred_3 = model_3.predict(X_test)\ny_pred_3.shape, y_pred_3.ndim","6f866032":"mae_3 = mae(y_test, y_pred_3).numpy()\nmse_3 = mse(y_test, y_pred_3).numpy()\nf\"mae: {mae_3}, mse: {mse_3}\"","e3ecf4e8":"pd.DataFrame(history_3.history).plot(figsize=(17, 6), title=\"History_3 Curves\"); # see the history","13f7425b":"# Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(200, activation=\"relu\", name=\"input_1\"),\n    tf.keras.layers.Dense(100, activation=\"relu\", name=\"input_2\"),\n    tf.keras.layers.Dense(10, activation=\"relu\", name=\"input_3\"),\n    tf.keras.layers.Dense(1, activation=\"relu\", name=\"output_layer\")\n], name=\"model_4\")\n\n# Compile the model\nmodel_4.compile(\n    loss=tf.keras.losses.mae,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-1),\n    metrics=[\"mae\"]\n)\n\n# Fit the model\nhistory_4 = model_4.fit(\n    X_train, y_train, epochs=50\n)","697fcf26":"model_4.summary()","dcc40da9":"model_4.evaluate(X_test, y_test) # evaluate it...","96b80475":"y_pred_4 = model_4.predict(X_test)\ny_pred_4.shape, y_pred_4.ndim","6b124ff0":"mae_4 = mae(y_test, y_pred_4).numpy()\nmse_4 = mse(y_test, y_pred_4).numpy()\nf\"mae: {mae_4}, mse: {mse_4}\"","3ebea2b1":"pd.DataFrame(history_4.history).plot(figsize=(17, 6), title=\"History_4 Curves\");","96f80de2":"# Let's compare our models result using a Pandas DataFrame\nmodel_results = [[\"model_1\", mae_1, mse_1],\n                 [\"model_2\", mae_2, mse_2],\n                 [\"model_3\", mae_3, mse_3],\n                 [\"model_4\", mae_4, mse_4]]\n\nall_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\nall_results","cf5b88b1":"model_3.summary() # see its architecture","8aa76de6":"Beautiful! Our data has been normalized and one hot encoded. Now let's build a neural network model on it and see how it goes.\n<a id=\"3\"><\/a>\n# **-| Modeling with TensorFlow |-**\n\n1. **Creating a model** - define the input and output layers, as well as hidden layers of a deep learning model.\n2. **Compile our model** - define the loss function (in others words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evalution metrics (what we can use to interpret the performance of our model).\n3. **Fitting a model** - letting the model try to find patterns betwwen $X$ & $y$ (features and labels).\n\nThe first, don't build a large model, take it easy... step by step....\n\nLet's build a simple model...    ","2012731e":"* **Total params** - total number of parameters in the model.\n* **Trainable parameters** - these are the parameters (patterns) the model can update as it trains.\n* **Non-trainable params** - these paramters aren't updated during training (this typical when you're bringin in already learn pattenrs or parameters from other models during **transfer learning**).","5cc4b5e4":"Okay, let's build another model...","208d41fc":"<a id=\"4\"><\/a>\n# **-| Comparing the results of our experiments |-**\n\nWe've run a few experiments, let's compare the results.","4e958b6a":"That's beautiful... after we Increase the number of hidden units, and change the learning rate, we can see that the model learning better than before...\n\nOkay, let's improve it again!","63bdf1a4":"<a id=\"1\"><\/a>\n# **-| Import Necessary Libraries |-** \n\nImport the library that we need for this research...","6337e6dc":"Let's build another model...","f597d99d":"### **Discrete and Continuous Variables**\n\nLet's have a look at Discrete and Continuous variables, before we move to preprocessing step.","d0780dfc":"### **Table of Contents**\n\n1. [Import Libraries](#1)\n2. [Preprocessing data](#2)\n3. [Modelling](#3)\n4. [Comparing models](#4)","8acc06e6":"### **Uniqueness Categorical Variables**\nLet's have a look at categorical variables. How many unique values of these variables.","3204aacd":"# **Regression with a simple Neural Networks in TensorFlow**\n\nThere are many definitions for a regression problem but in our case we're going to simplify it: predicting a numerical variable based on some other combination of variables, even shorter...predicting a number. \n\n**Regression Analysis** is a set of statistical processes for estimating the relationships between a **dependent variable (often called 'outcome variable') or y \/ Label** and one or more **independent variable (often called 'predictors', 'covariates', or 'features') or X \/ our input Features cols.**\n\n### **Architecture of a Regreesion Model:**\n\n* **Input layer shape:** Same shape as a number of features (e.g. $3$ for #size, #diameter, #topping in pizza price prediction)\n* **Hidden layer(s):** Problem specific, minimum = $1$, maximum = $unlimited$\n* **Neurons per hidden layer:** Problem specific, generally $10$ to $100$\n* **Output layer shape:** Same shape as desired prediction shape (e.g. $1$ for huse price)\n* **Hidden activation:** Usually `RelU` (rectified linear unit)\n* **Output activation:** None, `RelU`, `logistic\/tanh`\n* **Loss function:** `MSE` (mean square error) or `MAE` (mean absolute error) \/ `Huber` (combination of `MAE`\/`MSE`) if outliers\n* **Optimizer:** `SGD` (stochastic gradient descent), `Adam` etc....","d9d476c9":"# **On Going...**","a022a96d":"### **Evaluating our models predicitions with regression evaluation metrics**\n\nDepending on the problem you're working on, there will be different evaluation metrics to evaluate your model's performance.\n\nSince we're working on a regression, two of the main metrics:\n* **MAE** - mean absolute error, \"on average, how wrong is each of my models predictions\"\n* **MSE** - mean square error, \"square the average errors\"","38e9743d":"<a id=\"2\"><\/a>\n## **-| Preprocessing data (normalization and standardiziation) |-**\n\nIn terms of scaling values, neural networks tend to prefer normalization.\n\nIf you're not sure on which to use, you could try both and see which performs better.","cb943f09":"> **Note:** One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember the machine learning motto: \"experiment, experiment, experiment\". \n\n","425b76bd":"Looks like `model_3` performed best....","28f4208d":"### **Common ways to improve a deep model:**\n\n* Adding layers\n* Increase the number of hidden units\n* Change the activation functions\n* Change the optimization function\n* Change the learning rate: this is the important\n* Fitting on more data\n* Fitting for longer\n\n> **Note:** because you can alter each of these, they're hyperparameters)","57aaf878":"Great! look like our model learning something...\n\nLet's see its architecture...","26fea773":"Okay, we can see that the **mae** stop decreasing in ~$40$ epochs of training... Let's improve our model.\n\n### **Improving our model**\n\nWe can impove our model, by altering the steps we took to create a model.\n\n1. **Creating a model** - here we might add more layers, increase the number of hidden unit (all called neurons) within each of the hidden layers, change the activation function of each layer.\n2. **Compiling a model** - here we might change the optimization function or perhaps the **learning rate** of the optimization function.\n3. **Fitting a model** - here we might fit the model for more **epochs** (leave it training for longer) or on more data (give the model more example for learn from). ","ddc63d88":"Yeappyyy! you look that? wow, so beautiful... Let's see its architecture...","ab899bac":"Wow... you see that? looks like our model learning new patterns...\n\nLet's see its architecture...","770e5a84":"Let's see its architecture and evaluate it..."}}