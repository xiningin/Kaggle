{"cell_type":{"97cba226":"code","3f0bc165":"code","7e7c1ad5":"code","e4afa08b":"code","a8e936d1":"code","35a26b7a":"code","baa13cb4":"code","86b1fc70":"code","6c70a945":"code","c850be48":"code","4d8871b7":"code","ae1e7216":"code","44654e9e":"code","67d9faff":"code","5183a678":"code","e1219288":"code","3cfd7da6":"code","f3b86b80":"code","5282b7e2":"code","603590a6":"code","34367c5c":"code","fa8b8848":"code","1804060d":"code","9d4005aa":"code","59f4f0c7":"code","14cd0705":"code","f8bfec50":"code","79b3eebb":"code","89e200f5":"code","a3332945":"code","3d294ad9":"code","845f8051":"code","d9bc4c6b":"code","bade24a6":"code","f0d23a5e":"code","5602581c":"code","edd4b7f8":"code","47fb71d0":"code","1c874b97":"code","71be82a6":"code","c9d3ca33":"code","75e50442":"code","87ea5119":"code","b8d9c87e":"code","9df64807":"code","c3ba061c":"code","0a3c7ce3":"code","dfab0684":"code","6a4ef9ad":"code","b9c7f754":"code","4fd13bfd":"markdown","8bba01a8":"markdown","37c79698":"markdown","8aa1c9b2":"markdown","cc1f111d":"markdown","38134729":"markdown","6980eeda":"markdown","e166cf6f":"markdown","6e76527d":"markdown","a2b87b6a":"markdown","97d2139e":"markdown","2908085a":"markdown","2ee29840":"markdown","674f100f":"markdown","adeb4a6e":"markdown","05fdf622":"markdown","7895ab4f":"markdown","9601d1e7":"markdown","eb1a92ac":"markdown","cd4533a3":"markdown","c5b38330":"markdown","fc9a38b6":"markdown","673ca866":"markdown","ed4c9fa7":"markdown","21cce99e":"markdown","34599aec":"markdown","5b76b208":"markdown","69c4dd3f":"markdown","01bc6996":"markdown","5cb11b87":"markdown","08306207":"markdown","4a43e627":"markdown","8c5cb986":"markdown","a254bbd8":"markdown","ba7e85fb":"markdown","1712b255":"markdown","7ae6de90":"markdown","9d319207":"markdown","ebd8531e":"markdown","0e213a94":"markdown","8e001e41":"markdown","0d9256af":"markdown","9a6949b7":"markdown"},"source":{"97cba226":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))","3f0bc165":"shops = pd.read_csv(\"..\/input\/shops.csv\")  # shop_name, shop_id\nitem_categories = pd.read_csv(\"..\/input\/item_categories.csv\")  # item_category_name, item_category_id\nsales_train = pd.read_csv(\"..\/input\/sales_train.csv\")  # date, date_block_num, shop_id, item_id, item_price, item_cnt_day\nitems = pd.read_csv(\"..\/input\/items.csv\")  # item_name, item_id, item_category_id\n\ntest = pd.read_csv(\"..\/input\/test.csv\")  # ID, (shop_id, item_id)\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")  # ID, item_cnt_month","7e7c1ad5":"print('\\n# shops\\n')\nshops.info()\nprint('\\n# item_categories\\n')\nitem_categories.info()\nprint('\\n# sales_train\\n')\nsales_train.info()\nprint('\\n# items\\n')\nitems.info()\nprint('\\n# test\\n')\ntest.info()","e4afa08b":"sales_train = pd.merge(sales_train, items, on=['item_id'])\nsales_train = pd.merge(sales_train, item_categories, on=['item_category_id'])\nsales_train = pd.merge(sales_train, shops, on=['shop_id'])\nsales_train.head()","a8e936d1":"shops.drop_duplicates(keep = 'first', inplace = True)\nitem_categories.drop_duplicates(keep = 'first', inplace = True)\nsales_train.drop_duplicates(keep = 'first', inplace = True)\nitems.drop_duplicates(keep = 'first', inplace = True)","35a26b7a":"pd.isnull(sales_train).sum()","baa13cb4":"sales_train.info()","86b1fc70":"sales_train_copy = sales_train.copy()","6c70a945":"# how many items in each category in training data\n\nfig, axs = plt.subplots(2,2,figsize=(15,12))\n\n# Total sales variation\n\nrecords_category = pd.concat([items.groupby('item_category_id')['item_id'].count(),sales_train_copy.groupby('item_category_id')['item_cnt_day'].sum()],axis=1).rename(columns={'item_id':'item_numbers','item_cnt_day':'total_sales'})  \nrecords_category.sort_index()\nrecords_category['average_sales'] = sales_train_copy.groupby(['item_category_id','item_id'])['item_cnt_day'].sum().groupby('item_category_id').mean().sort_index()  \nrecords_category['sales_std'] = sales_train_copy.groupby(['item_category_id','item_id'])['item_cnt_day'].sum().groupby('item_category_id').std().sort_index() \n\n# Month variations\n\n# Plot\n\nitems.groupby('item_category_id')['item_id'].count().plot(kind='bar',title='item counts in the category (training data)', ax=axs[0,0])     \nsales_train_copy.groupby('item_category_id')['item_cnt_day'].sum().plot(kind='bar',title='sales of all items in the category', ax = axs[0,1]) \nrecords_category['average_sales'].plot(kind='bar',title='average sales of the item in the category', ax = axs[1,0]) \nrecords_category['sales_std'].plot(kind='bar',title='sales std of the items in the category', ax = axs[1,1]) \n\nprint('Corelation', items.groupby('item_category_id')['item_id'].count().corr(sales_train_copy.groupby('item_category_id')['item_cnt_day'].sum())) ","c850be48":"records_category.sort_values(by='average_sales',ascending=False).head()","4d8871b7":"# Category sales through time\n\nsales_train_copy_fplot = sales_train_copy.groupby(['date_block_num','item_category_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('item_category_id').loc[:]\nplot_df.plot(legend=False, kind = 'line')\n","ae1e7216":"items[items['item_category_id']==40].head()","44654e9e":"cate40_items_ex = [0,10,24,37,22149,22156,22160,22163]\n#items in cate40_items_ex\nitems[items['item_id'].isin(cate40_items_ex) == True]","67d9faff":"sales_train_copy.groupby('shop_id').sum()['item_cnt_day'].plot(kind='bar', title='total sales of each shop')","5183a678":"a = sales_train_copy.groupby(['shop_id','item_category_id']).sum()['item_cnt_day'].reset_index()\nfig, axs = plt.subplots(1,3,figsize=(15,5))\na[a.groupby(['shop_id'])['item_cnt_day'].transform(max) == a['item_cnt_day']].reset_index()['item_category_id'].plot(kind='bar',ax=axs[0],title='Best sellinf cates in each shop')\na[a.groupby(['shop_id'])['item_cnt_day'].transform(max) == a['item_cnt_day']].reset_index().groupby('item_category_id').count()['shop_id'].plot(kind='pie', title='Ratio of Best Selling cates in 60 shops', ax=axs[1]).set_ylabel('') \na[a.groupby(['item_category_id'])['item_cnt_day'].transform(max) == a['item_cnt_day']].reset_index().groupby('shop_id').count()['item_category_id'].plot(kind='pie', title='Ratio of Best Selling shops in 84 shops', ax=axs[2]).set_ylabel('') ","e1219288":"plt.figure(figsize=(10,4)) # sales of all samples\nsns.boxplot(x=sales_train_copy.item_cnt_day)\n\nplt.figure(figsize=(10,4)) # price of all samples\nsns.boxplot(x=sales_train_copy.item_price)","3cfd7da6":"sales_train_copy.loc[sales_train_copy['item_cnt_day'].idxmax()]","f3b86b80":"sales_train_copy.loc[sales_train_copy['item_cnt_day'].idxmin()]","5282b7e2":"sales_train_copy.loc[sales_train_copy['item_price'].idxmax()]","603590a6":"sales_train_copy.loc[sales_train_copy['item_price'].idxmin()]","34367c5c":"sales_train_copy.groupby(['item_id']).sum()['item_cnt_day'].plot()","fa8b8848":"bestsale = sales_train_copy.groupby(['item_id']).get_group(sales_train_copy.groupby(['item_id']).sum()['item_cnt_day'].idxmax())\nbestsale.sort_values(by='item_cnt_day', ascending=False).head()","1804060d":"train_keys_shop_item = sales_train_copy.groupby(['item_id','shop_id']).groups.keys()\ntest_keys_shop_item = test.groupby(['item_id','shop_id']).groups.keys()\nprint('# train_keys and test_keys size', len(list(train_keys_shop_item)), len(list(test_keys_shop_item)))\nprint('# intersection', len(set(list(train_keys_shop_item)) & set(list(test_keys_shop_item))))","9d4005aa":"train_keys = sales_train_copy.groupby(['item_id']).groups.keys()\ntest_keys = test.groupby(['item_id']).groups.keys()\nprint('# train_keys and test_keys size', len(list(train_keys)), len(list(test_keys)))\nprint('# intersection', len(set(list(train_keys)) & set(list(test_keys))))","59f4f0c7":"test.isin({'item_id': list(train_keys)}).groupby('item_id').size()","14cd0705":"test_length = len(test)\nolditems_alreadylaunched = len(set(list(train_keys_shop_item)) & set(list(test_keys_shop_item)))\nnewitems_newlaunched = len(test.isin({'item_id': list(train_keys)}).groupby('item_id').get_group(False))\nolditems_newlaunched = test_length - olditems_alreadylaunched - newitems_newlaunched\n\n# Data to plot\n\nlabels = 'old items already launched', 'old items new launched', 'new items new launched'\nsizes = [olditems_alreadylaunched, olditems_newlaunched, newitems_newlaunched]\n\nplt.pie(sizes, labels=labels,\nautopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()","f8bfec50":"olditemsshopsintest = pd.DataFrame(np.asarray(list(set(list(train_keys_shop_item)) & set(list(test_keys_shop_item)))), columns=['item_id','shop_id']) \nolditemsshopsIDintest = pd.merge(test, olditemsshopsintest)['ID']\n\ntest1 = pd.concat([test, test.isin({'item_id': list(train_keys)})['item_id'].rename('item_in_train') ], axis=1)\nnewitemsIDintest = test1.groupby('item_in_train').get_group(False)['ID']\n\nolditemsnewshopIDintest = test.drop(pd.concat([olditemsshopsIDintest,newitemsIDintest]))['ID']\n\n# Index data\n\nolditemsnewshopintest_data = test.loc[test['ID'].isin(olditemsnewshopIDintest)]\nolditemsshopsintest_data = test.loc[test['ID'].isin(olditemsshopsIDintest)]\nnewitemsintest_data = test.loc[test['ID'].isin(newitemsIDintest)]","79b3eebb":"# Group by items\n\ntoplot = olditemsnewshopintest_data[['shop_id','item_id']].groupby('item_id').count().rename(columns = {'shop_id':'shop_numbers_intest'}).reset_index()   \n\nitemsoldinshops_train = sales_train_copy.groupby(['item_id','shop_id']).count().reset_index()[['item_id','shop_id']].groupby('item_id').count().rename(columns = {'shop_id':'shop_numbers_intrain'}).reset_index()[['item_id','shop_numbers_intrain']]  \ntoplot = pd.merge(toplot, itemsoldinshops_train)\ntoplot['test_train_shopsratio'] = toplot['shop_numbers_intest'] \/ toplot['shop_numbers_intrain']\n\nfig,axs = plt.subplots(1,3,figsize=(15,5))\ntoplot.sort_values(by='shop_numbers_intrain').reset_index()['shop_numbers_intrain'].plot(ax=axs[0])\ntoplot.sort_values(by='shop_numbers_intest').reset_index()['shop_numbers_intest'].plot(ax=axs[1])\ntoplot.sort_values(by='test_train_shopsratio').reset_index()['test_train_shopsratio'].plot(ax=axs[2])\naxs[0].title.set_text('shop_numbers_in_train')\naxs[1].title.set_text('shop_numbers_in_test')\naxs[2].title.set_text('test_train_shopsratio')","89e200f5":"# The category ratio of this task\ntoplot = pd.merge(toplot, items)\ntoplot.groupby('item_category_id').count()['item_id'].rename(columns={'item_id':'item_count'}).plot()   ","a3332945":"toplot[toplot['item_id'].isin([11286,13818,4240])]","3d294ad9":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_id']==4240].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[0])      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_id']==4240].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_id']==4240].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of item 4240')\naxs[1].set_title('Monthly sales of each shop of item 4240')\naxs[2].set_title('Monthly average sales of the shops having sales')","845f8051":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_category_id']==23].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[0])      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_category_id']==23].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_category_id']==23].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of cate 23')\naxs[1].set_title('Monthly sales of each shop of cate 23')\naxs[2].set_title('Monthly average cate sales of the shops having sales')","d9bc4c6b":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_id']==13818].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[0],kind='bar')      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_id']==13818].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_id']==13818].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of item 13818')\naxs[1].set_title('Monthly sales of each shop of item 13818')\naxs[2].set_title('Monthly average sales of the shops having sales')","bade24a6":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_category_id']==37].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[0])      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_category_id']==37].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_category_id']==37].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of cate 37')\naxs[1].set_title('Monthly sales of each shop of cate 37')\naxs[2].set_title('Monthly average cate sales of the shops having sales')","f0d23a5e":"sales_train_copy[sales_train_copy['item_id']==11286].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(title='Monthly sales of all shops of item 11286 (only one shop)')      ","5602581c":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_category_id']==31].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(kind='bar', ax=axs[0])      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_category_id']==31].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_category_id']==31].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of cate 31')\naxs[1].set_title('Monthly sales of each shop of cate 31')\naxs[2].set_title('Monthly average cate sales of the shops having sales')","edd4b7f8":"newitemsintest_data = pd.merge(newitemsintest_data,items[['item_id','item_category_id']],on='item_id')\n\nfig, axs = plt.subplots(1,2,figsize=(15,8))\nnewitemsintest_data.groupby('item_category_id').count()['item_id'].plot(kind='bar', ax=axs[0])\nnewitemsintest_data.groupby('item_id').count()['shop_id'].plot(kind='bar',ax=axs[1]) # all the same\n\naxs[0].set_title('test input counted in item_category')\naxs[1].set_title('test input counted in items [based on shop]')","47fb71d0":"fig, axs = plt.subplots(1,3,figsize=(15,5))\n\nsales_train_copy[sales_train_copy['item_category_id']==72].groupby('date_block_num').sum().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[0])      \n\nsales_train_copy_fplot = sales_train_copy[sales_train_copy['item_category_id']==72].groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sales_train_copy_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1])\n\nsales_train_copy[sales_train_copy['item_category_id']==72].groupby(['date_block_num','shop_id']).sum().groupby('date_block_num').mean().rename(columns={'item_cnt_day':'monthly_sales'})['monthly_sales'].plot(ax=axs[2],kind='bar')      \n\naxs[0].set_title('Monthly sales of all shops of cate 72')\naxs[1].set_title('Monthly sales of each shop of cate 72')\naxs[2].set_title('Monthly average cate sales of the shops having sales')","1c874b97":"sales_train_copy.groupby('item_price').size()","71be82a6":"sales_train_copy.groupby('item_price').get_group(-1)","c9d3ca33":"sales_train_copy[(sales_train_copy['item_id']==2973) & (sales_train_copy['shop_id']==32)].head()","75e50442":"sale2973 = sales_train_copy[sales_train_copy['item_id']==2973]\nsale2973.head()","87ea5119":"fig, axs = plt.subplots(1,2,figsize=(15,5))\n\nsale2973.groupby('date_block_num')['item_cnt_day'].sum().plot(kind=' bar', ax=axs[0], title='monthly total sales of the item in all shops') # monthly sales of the item \n\nsale2973_fplot = sale2973.groupby(['date_block_num','shop_id'])['item_cnt_day'].sum()\nplot_df = sale2973_fplot.unstack('shop_id').loc[:]\nplot_df.plot(legend=False, kind = 'line', ax=axs[1], title='monthly total sales of the item in each shops')\n","b8d9c87e":"fig, axs = plt.subplots(1,2,figsize=(15,5))\n\nsale2973.groupby(['date_block_num'])['item_price'].mean().plot(kind = 'bar', ax=axs[0], title='monthly average price of all shops of the item')\npd.Series(sale2973.groupby(['date_block_num', 'shop_id'])['item_price'].mean().groupby('date_block_num').std()).plot(kind = 'bar', ax=axs[1], title='monthly price std between all the shops')     ","9df64807":"print(sales_train_copy[sales_train_copy['item_cnt_day']<0].groupby(['item_id']).size())\nprint(sales_train_copy[sales_train_copy['item_cnt_day']<0].groupby(['item_id','date_block_num', 'shop_id']).size())","c3ba061c":"# Category with sales return \n\nfig, axs = plt.subplots(1,3,figsize=(15,5))\n\n# item-based negative-sold counts in each category\nsales_train_copy[sales_train_copy['item_cnt_day']<0].groupby(['item_category_id'])['item_id'].agg(['count']).plot(kind='bar',ax = axs[0])                               \n# item negative-sold total sales in each category\nsales_train_copy[sales_train_copy['item_cnt_day']<0].groupby(['item_category_id'])['item_cnt_day'].agg(['sum']).abs().plot(kind='bar', ax = axs[1])                           \n# item-based\nnegative_totalsales = sales_train_copy[sales_train_copy['item_cnt_day']<0].groupby('item_id')['item_cnt_day'].agg('sum').abs()\nnegative_totalsales.plot(kind='line', ax=axs[2], title='Counts of returns of each item')\n\naxs[0].title.set_text('Counts of items with return record in the category')\naxs[1].title.set_text('Ruturn sales of items in the category')","0a3c7ce3":"sales_2331 = sales_train_copy[sales_train_copy['item_id']==2331]\nprint('positive sales', sales_2331[sales_2331['item_cnt_day']>0]['item_cnt_day'].sum())\nprint('negative sales', sales_2331[sales_2331['item_cnt_day']<0]['item_cnt_day'].sum())","dfab0684":"sales_cate20 = sales_train_copy[sales_train_copy['item_category_id']==20]\n\n# 157 items in cate 20 in training data \nfig, axs = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle('Total sales of the 157 items in category 20 (training data)')\nsales_cate20.groupby('item_id').agg('sum')['item_cnt_day'].sort_values().plot(kind='bar', ax=axs[0])\nsales_cate20.groupby('item_id').agg('sum')['item_cnt_day'].sort_values().plot(kind='pie', ax=axs[1])\n","6a4ef9ad":"# negative sales of the category - 100\/157 items with negative sales in this category\nfig, axs = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle('Total returns of the 100 items in category 20 (training data)')\nsales_cate20[sales_cate20['item_cnt_day']<0].groupby('item_id').agg('sum')['item_cnt_day'].abs().sort_values().plot(kind='bar', ax=axs[0])\nsales_cate20[sales_cate20['item_cnt_day']<0].groupby('item_id').agg('sum')['item_cnt_day'].abs().sort_values().plot(kind='pie', ax=axs[1])","b9c7f754":"## Among these negative-sale items in cate20 - 100 items in 157\/175 items in category 20\n\n# negative sales\nsales20_negtotal = sales_cate20[sales_cate20['item_cnt_day']<0].groupby('item_id').agg('sum')['item_cnt_day'].reset_index()\n# positive sales\nsales20_postotal = sales_cate20[sales_cate20['item_cnt_day']>0].groupby('item_id').agg('sum')['item_cnt_day'].reset_index()\n# total sales\nsales20_total = sales_cate20.groupby('item_id').agg('sum')['item_cnt_day'].reset_index()\n\nsales20_compare = pd.merge(sales20_postotal, sales20_negtotal.abs(), on='item_id')\nsales20_compare.rename(columns={\"item_cnt_day_x\": \"sales_sold\", \"item_cnt_day_y\": \"sales_return\"}, inplace=True)\nsales20_compare = pd.merge(sales20_compare, sales20_total, on='item_id')\nsales20_compare.rename(columns={\"item_cnt_day\": \"sales_total\"}, inplace=True)\nsales20_compare.fillna(0)\nsales20_compare['return_ratio'] = sales20_compare['sales_return'] \/ sales20_compare['sales_sold']\n\nfig, axs = plt.subplots(2,2,figsize=(15,10))\nfig.suptitle('Sale records of the 100 items with returns in category 20 (training data)')\nsales20_compare[['item_id','sales_sold']].plot(x='item_id',y='sales_sold', kind='bar', ax=axs[0,0])\nsales20_compare[['item_id','sales_return']].plot(x='item_id',y='sales_return', kind='bar', ax=axs[0,1])\nsales20_compare[['item_id','sales_total']].plot(x='item_id',y='sales_total', kind='bar', ax=axs[1,0])\nsales20_compare[['item_id','return_ratio']].plot(x='item_id',y='return_ratio', kind='bar', ax=axs[1,1])","4fd13bfd":"### items in shops","8bba01a8":"From the data above, we can assume that maybe it's wrong keyed in, we can replace the the price with the same month price = 1249. Next, we take a little closer look and do some analysis on the item.","37c79698":"For the kind of data, there are around 87K shop+item combinations, with around 4.7K items in the test data. For each item, the number of other shops having sales records in training data can range to around 60 shops, while the number of shops not having sales records in testing data can also range to around 45 shops. \n\nMost of the inputs in this kind of data (old items new launched) have the phenomenon with testing data less than training data, with some having very extreme ratio (test num \/ train num more than 40). Therefore, we have to predict the sales of the items with most of the conditions that the items have lesser data of other shops in traning data compared to testing data.\n\nWe'll then take a detail looks to some of the examples in the case. From the perspective of items, we'll take 3 items and do further analysis from the view of item and category.\n- item_id: 11286  shop_numbers_intest: 41  shop_numbers_intrain: 1  \n- item_id: 13818  shop_numbers_intest: 18  shop_numbers_intrain: 25\n- item_id: 4240  shop_numbers_intest: 2  shop_numbers_intrain: 57\n","8aa1c9b2":"old items already lauched in shops","cc1f111d":"The figure shows the item counts in group of category. In this kind of task, there are lots of items between the category 14-75.","38134729":"##### About the negative-price item - sales, price, shop","6980eeda":"## Sales information \/ item based ","e166cf6f":"# EDA on training data","6e76527d":"# Train data and Test data overlapped?\n","a2b87b6a":"Similar trend for fig1 and fig2 (no big difference on item_id or item_sales), since that no sales return isn't that big for each item (max 60)","97d2139e":"## Negative sales\n- 3511 \/ 22K items have returns\n- 7259 kinds of varations on items+month+shops\n- distribution in each category","2908085a":"For each shop, the best selling categories varies, but there are category 40 and category 30 are having around 75% being the best category sellers in the 60 shops. \n\nFor each category, the sales are different in each shops. While category 31, 55 and 25 are containing more of the best selling conditions of the category.","2ee29840":"## Category\n\nThere are 84 categories from the csv file.\n\nRelationship to total items in each category, total \/ average sales in each category, price range in each category","674f100f":"# Input data information","adeb4a6e":"There are three conditions of data for the test data.\n- old items already luanched are the test data that have history data in training.\n- old items new luanched are the test data that have item history data 'in other shops' in training.\n- new items new luanched are items in test data that don't have any history data in training.\nHere, we'll take a closer look into 'old items new launched' and 'new items new launched'.","05fdf622":"# Conclusion\n\nSeveral analysis on training data are implemented. \n\nThe types of test data (item, shop) have 3 kinds of condition\n- having history data of the item in the shop\n- not having data of the item in shop but in other shops\n- not having any history data. \n\nDepending on the way we view the kind of data, the way we design the model varies. Note that there are 41% of data that are not having data of the item in shop but in other shops, and how to deal with them can also play an essential roles on predicting. \n\nIn addition, special cases like negative price and sales are discussed. \n\nMore analysis techniques and topics can be implemented in the future.","7895ab4f":"We can apply the history data of monthly item average sales of the category for the kind of data.","9601d1e7":"test items not in train data (False) = new lauched items","eb1a92ac":"Note that from the figure, we can see the sales trend of well-sold category. Cate 40 (Cinema -DVD) and cate 30 (PC games - standard edition) have the most sales throughout all the months, with the decreasing trends.\n\nThere are also lots of time series trend with monthly total sales lesser than 5000.","cd4533a3":"## Old items new launched in shop items\n\nFor old items new launched in shop items, the real condition can be\n- new launched in the shop \n- launched with 0 selling records\n- launched with missing selling records\n\nFew questions to ask for this kind of condition:\n- How's the item saling in other shops? (hotness of the item)\n- How's the category saling in the shop? in other shops? \n\nWe'll use eda techniques to help decide how to deal with this pipe of condition.","c5b38330":"old items already launched","fc9a38b6":"plot 3 types of data","673ca866":"## Negative price\n\n- We first find about the items with negative price (one case)\n- Further analysis on their features (shop, price, sales)","ed4c9fa7":"monthly total sales of the item in each shop and all shops","21cce99e":"# Special Case - negative values of price and sales in train data","34599aec":"### items in a category\nTake Cate 40 as examples, take some examples and find if those items has launching information (time) through item_id.","5b76b208":"Film release date of the items\n(You can search the film if the site https:\/\/www.kinopoisk.ru\/)\n\n- item 0: 2000\n- item 10: 2001\n- item 24: 2000\n- item 37: 2011\n- item 22149: 2011\n- item 22156: 2001\n- item 22160: 2004\n- item 22163: 2014\n\nThe item_id in category 40 doesn't really carry the launching information of the items in the 8 examples.","69c4dd3f":"There's one case with negative price, check the prices of the item sold in the shop.","01bc6996":"In the 100 items that are sold with return\n- The sales sold ranges from a few to 6000 items\n- The maximum of sales return is around 60 items\n- Most of the returns are smaller than 10 items\n- The most return items isn't the best seller in this category","5cb11b87":"## Price","08306207":"item 20949 is having extremely high sales compared to other items.","4a43e627":"## Data cleaning \ncheck duplicated, uniqness, null","8c5cb986":"monthly average price  and std of all shops of the item","a254bbd8":"#####  About the item with the most return sales\n* item_id: 2331\n* sales return and sold: 60\/542\n* category: 20 (Game-PS4)\n\n    * 100 negative-sales items\n    * 157 items with record in training data\n    * 175 items in total","ba7e85fb":"This is my first project for eda using the product sales records. The report will focus a lot more on discussing the type of the test data (the relationship between test and train), and take on some detail looks into some examples. Here's the outline: \n\n[Input data information](#Input-data-information)\n* [Original features](#Original-features)\n* [Data cleaning](#Data-cleaning)\n\n[EDA on features](#EDA-on-training-data)\n* [Category](#Category)\n* [Shops](#Shops)\n* [Price](#Price)\n* [Sales information \/ item based](#Sales-information-\/-item-based)\n\n[Test data (depending on training data)](#Train-data-and-Test-data-overlapped?)\n* [Items in test data](# items-in-shops)\n* [Old items new launched in shop items](#Old-items-new-launched-in-shop-items)\n* [New items new launched in shop items](#New-items-new-launched-in-shop-items)\n* [Discussion](#Discussion)\n\n[Special case](#Input-data-information)\n* [Negative price](#Negative-price)\n* [Negative sales](#Negative-sales)\n\n[Conclusion](#Input-data-information)","1712b255":"We look at the kind of data by using the history data of category sales in the shop.\nThere are around 15246 kinds of inputs, 16K groups when considering shops and category, distributed in 39 category, 42 shops, 363 items. (For each item, we're predicting the sales in the 42 shops.)\nSince we don't have the item sales record, for each input, we can take a look at the category sales in the shop and over all sales in all shop. Take cate 72 for example.\n\n","7ae6de90":"- item_id: 4240  shop_numbers_intest: 2  shop_numbers_intrain: 57\n\nThe item 4240 (Kinect Dance Central 3 (only for MS Kinect) [Xbox 360]), in category 23 (Games - Xbox360) has the sales peak around the 23rd date_block_num. Cate 23 is also having the similar trend with the item 4240.\n\n- item_id: 13818  shop_numbers_intest: 18  shop_numbers_intrain: 25\n\nThe item 13818 (LEGENDS OF NIGHT GUARDS WB (BD)), in category 37 (Games - Xbox360) start having better sales on the 30th date_block_num. The trend of cate 37 isn't similar to the item's.\n\n- item_id: 11286  shop_numbers_intest: 41  shop_numbers_intrain: 1  \n\nThe item 11286 (Truckers 3: The Conquest of America + Great Race [PC, Digital Version]), in category 37 (PC Games - Digital) have sales ranging from 2 to 16 through months, starting from month 20. Cate 37 is also having more sales from month 20 to month 30.\n\n- similar trends\n\nFor each shop, they follow similar trend of total sales, while there are also some with 0 saling records or missing points.\nIn addition, the average sales of each shop having sales are also following some of the trends of total sales.\nTo find out the previous records of not-having-sales-record-in-shop items, we can use the average of the item sales among the shops having saling records. ","9d319207":"## New items new launched in shop items\n\nThere are around 7% of test data in the category.\n\nFor new items new launched in shop items, the real conditions can be \n- new launched \n- launched with 0 selling records\n- launched with missing selling records\n\nQuestion to ask for this kind of condition:\n- How's the category saling in the shop? in other shops? \n\nAfter some observation, we'll find out how to deal with the kind of condition.","ebd8531e":"Shop 31 has the most item sales in 2 years.","0e213a94":"For the maximum sales among each category, cate 40 (Cinema - DVD) has 634K total item sales, which also has the most item variations (5025) in all the categories. The item sales has correlation of 0.82 with the item variations in the category. With higher sales in the category, there are more different kinds of items in it. But there are also some exceptions in it.\n\nThere are also some categories that have very high average sales per item. Cate 71 (Gifts - Bags, Albums, Mouse pads) and Cate 79 (Service) have average item sales of 31K and 16K, with only 6 items and 1 item in each category. ","8e001e41":"## Shops","0d9256af":"## Discussion\n\nFrom the above analysis, we've found that there are mainly three kinds of test data. In addition, depends on the kinds, we can think of the training data set presented to us in different ways. Here are some questions we can ask:\n\n- Whether the training data is really 'the complete' history data ?\n- Whether non-record item is really having no sales? or is its history missing? or is it new launched?\n\nHere are some directions that we can think of based on the kind of test data:\n\n- Old items already launched (52%)\n    - Believe all the training data are the total selling records in history\n    - Training data are missing some selling records\n- Old items new launched in shops (42%)\n    - 0 selling records in the shop\n    - Missing selling records of the shop\n    - New launched in the shop at the month to be predicted\n- New items new launched (7%)\n    - 0 selling records in the shop\n    - Missing selling records of the shop\n    - New launched in the shop at the month to be predicted\n\nDepending on the way we look at the training data, we will have different methods on dealing with these kinds of data:\n\n- Training data not missing history (0 sales if not having records)\nWhen predicting the sales, directly assign 0 to the time that don't have records and then predict the monthly sales.\n- Training data missing history (items launched in the shop)\nOn the analysis above, we found out that giving the average monthly sales can be a kind of solution, such as using average monthly sales of the item in all shop (Old items new launched) or using average monthly item sales of the category in the shop (New items new launched).\n- New launched at the month to be predicted\nNeed to find out the new launched items in training data, both as new launched in a shop or all shops, and use the kind of data to create new set of training data and then predict. This is different then two types above, since that they're using all the history data in training data. The method here needs to distinguish the start-selling day, and the predicted results will depend on their selling records afterwards, with other features like category, shops, etc included.\n\nNote that since 'Old items new launched' has around 40%, the way we deal with the kinds of data can also play an essential role on having a better predicting results.","9a6949b7":"## Original features"}}