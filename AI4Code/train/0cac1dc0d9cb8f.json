{"cell_type":{"85104cf4":"code","53b5fe1c":"code","9cd1b219":"code","13ea1b65":"code","d5be9146":"code","8d703568":"code","6b031d3b":"code","49e11fb4":"code","8653e27d":"code","7672d98a":"code","c9f39c64":"code","73ef7782":"code","2276174f":"code","f185d549":"code","0e65776f":"code","bc7d61b6":"code","898b0a8f":"code","f6e8fcfc":"code","cc2df465":"code","3e3b77a8":"code","174fa6c8":"code","b4cd89dd":"code","d5aa725f":"code","0593da54":"markdown","6d6540b4":"markdown","9e413825":"markdown","313b3772":"markdown","95ff9a99":"markdown","2ff12161":"markdown"},"source":{"85104cf4":"import numpy as np\nimport pandas as pd \nimport random\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random\nfrom tensorflow import keras\nimport tensorflow as tf\nimport os\nimport glob\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom os.path import dirname, basename\nfrom PIL import Image\nfrom tensorflow.keras.utils import Sequence\nfrom imgaug import augmenters as iaa\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.models import  Model\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy, BinaryAccuracy\nfrom tensorflow.keras.optimizers import Nadam, Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import load_model\nprint(os.listdir(\"..\/input\"))","53b5fe1c":"list_train_filenames = glob.glob('..\/input\/leafdataset\/2TPK_256\/trn\/*\/*.jpg')\nlist_train_images1 = [np.array(Image.open(filename)) for filename in list_train_filenames]\n\nlist_valid_filenames = glob.glob('..\/input\/leafdataset\/2TPK_256\/val\/*\/*.jpg')\nlist_valid_images1 = [np.array(Image.open(filename)) for filename in list_valid_filenames]\n\nlist_labels_train1 = [int(basename(dirname(filename))) for filename in list_train_filenames]\nlist_labels_valid1 = [int(basename(dirname(filename))) for filename in list_valid_filenames]","9cd1b219":"train = list(zip(list_train_images1,list_labels_train1))\nvalid = list(zip(list_valid_images1,list_labels_valid1))\n\nrandom.shuffle(train)\nlist_train_images = []\nlist_labels_train = []\nfor val in train :\n    list_train_images.append(val[0])\n    list_labels_train.append(val[1])\n    \nrandom.shuffle(valid)\nlist_valid_images = []\nlist_labels_valid = []\nfor val in valid :\n    list_valid_images.append(val[0])\n    list_labels_valid.append(val[1])","13ea1b65":"[int(val) for val in list_labels_train]","d5be9146":"plt.imshow(list_train_images[3])\nplt.title(list_labels_train[3])","8d703568":"x_train = np.stack(list_train_images)\ny_train = np.stack(list_labels_train)\nx_valid = np.stack(list_valid_images)\ny_valid = np.stack(list_labels_valid)","6b031d3b":"class DataGenerator(Sequence):\n    \"\"\"\n    Data Generator for Image\n    \"\"\"\n    def __init__(self, batch_size, np_images, np_labels):\n        self.batch_size = batch_size\n        self.np_labels = np_labels\n        self.np_images = np_images\n\n        self.num_images = len(self.np_images)\n        \n        self.preprocesser = iaa.Sequential(\n             [\n              iaa.Rot90((1,3)),\n                 iaa.AdditiveGaussianNoise(scale=0.1*255),\n                 iaa.Sharpen(alpha=0.5)\n             ],\n#             random_order= True\n        )\n        self.preprocesser_1 = iaa.Sequential(\n              [\n                  \n                 iaa.Rot90((1,3)),\n                  iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6)),\n              ],\n#             random_order= True\n        )\n        self.preprocesser_2 = iaa.Sequential(\n              [\n                 iaa.Rot90(1),\n                  iaa.LogContrast(gain=(0.6, 1.4))\n              ],\n#             random_order= True\n        )\n        self.preprocesser_3 = iaa.Sequential(\n              [\n                 iaa.Rot90(1),\n                  iaa.GammaContrast((0.5, 2.0))\n              ],\n#             random_order= True\n        )\n        \n        self.preprocesser_4 = iaa.Sequential(\n              [\n                 iaa.Fliplr(0.5),\n              ],\n#             random_order= True\n        )\n        self.preprocesser_5 = iaa.Sequential(\n              [\n                iaa.Fliplr(0.5),\n                  iaa.LogContrast(gain=(0.6, 1.4))\n              ],\n#             random_order= True\n        )\n        \n        self.preprocesser_6 = iaa.Sequential(\n              [\n                  iaa.Fliplr(0.5),\n                  iaa.GammaContrast((0.5, 2.0))\n                  \n              ],\n#             random_order= True\n        )\n        self.preprocesser_7 = iaa.Sequential(\n              [\n                  iaa.Fliplr(0.5),\n                  iaa.AdditiveGaussianNoise(scale=0.1*255)\n              ],\n#             random_order= True\n        )\n        self.preprocesser_8 = iaa.Sequential(\n              [\n                 \n                   iaa.Fliplr(0.5),\n                  iaa.SigmoidContrast(gain=(3, 10), cutoff=(0.4, 0.6)),\n              ],\n#             random_order= True\n        )\n        self.preprocesser_9 = iaa.Sequential(\n              [\n                 iaa.Fliplr(0.5),\n                  iaa.Sharpen(alpha=0.5)\n              ],\n#             random_order= True\n        )\n        \n\n        self.indices = np.random.permutation(self.num_images)\n\n    def on_epoch_end(self):\n        self.indices = np.random.permutation(self.num_images)\n\n    def __len__(self):\n        # return len = 46 ~~ Steps per epoch (Iteration)\n        # 1 iteration = 1 batchsize \n        # 46 * 128 \n        return int(self.num_images \/ self.batch_size)\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size: (index+1) * self.batch_size] # index c\u1ee7a 32 t\u1ea5m\/batch\n        batch_images = self.np_images[batch_indices] # 32 t\u1ea5m theo index shuffe\n        batch_labels = self.np_labels[batch_indices] # 32 label\n\n        batch_preprocessed_images = self.preprocesser(images = batch_images) # num(batch_size) t\u1ea5m m\u1edbi\n        batch_preprocessed_images_1 = self.preprocesser_1(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_2 = self.preprocesser_2(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_3 = self.preprocesser_3(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_4 = self.preprocesser_4(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_5 = self.preprocesser_5(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_6 = self.preprocesser_6(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_7 = self.preprocesser_7(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_8 = self.preprocesser_8(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        batch_preprocessed_images_9 = self.preprocesser_9(images = batch_images) # 32 t\u1ea5m m\u1edbi\n        \n        # Data x2\n        data_images_1 = np.concatenate([batch_images, batch_preprocessed_images]) # concat = 32 + 32 images\n        data_labels_1 = np.concatenate([batch_labels, batch_labels]) # concat = 32 + 32 labels\n        # Data x3\n        data_images_2 = np.concatenate([data_images_1, batch_preprocessed_images_1]) # concat = 64 + 32 images\n        data_labels_2 = np.concatenate([data_labels_1, batch_labels]) # concat = 64 + 32 \n        # Data x4\n        data_images_3 = np.concatenate([data_images_2, batch_preprocessed_images_2]) # concat = 96 + 32 images\n        data_labels_3 = np.concatenate([data_labels_2, batch_labels]) # concat = 96 + 32 labels\n        # Data x5\n        data_images_4 = np.concatenate([data_images_3, batch_preprocessed_images_3])\n        data_labels_4 = np.concatenate([data_labels_3, batch_labels])\n        # Data x6\n        data_images_5 = np.concatenate([data_images_4, batch_preprocessed_images_4])\n        data_labels_5 = np.concatenate([data_labels_4, batch_labels])\n        # Data x7\n        data_images_6 = np.concatenate([data_images_5, batch_preprocessed_images_5])\n        data_labels_6 = np.concatenate([data_labels_5, batch_labels])\n        # Data x8\n        data_images_7 = np.concatenate([data_images_6, batch_preprocessed_images_6])\n        data_labels_7 = np.concatenate([data_labels_6, batch_labels])\n        # Data x9\n        data_images_8 = np.concatenate([data_images_7, batch_preprocessed_images_7]) \n        data_labels_8 = np.concatenate([data_labels_7, batch_labels]) \n        # Data x10\n        data_images_9 = np.concatenate([data_images_8, batch_preprocessed_images_8]) \n        data_labels_9 = np.concatenate([data_labels_8, batch_labels]) \n        # Data x11\n        data_images = np.concatenate([data_images_9, batch_preprocessed_images_9]) # concat = 128 + 32 images\n        data_labels = np.concatenate([data_labels_9, batch_labels]) # concat = 128 + 32 labels\n        return data_images, data_labels\n#         return data_labels","49e11fb4":"class DataGenerator2(Sequence):\n    \"\"\"\n    Data Generator for Image\n    \"\"\"\n    def __init__(self, batch_size, np_images, np_labels):\n        self.batch_size = batch_size\n        self.np_labels = np_labels\n        self.np_images = np_images\n\n        self.num_images = len(self.np_images)\n        self.indices = np.random.permutation(self.num_images)\n\n    def on_epoch_end(self):\n        self.indices = np.random.permutation(self.num_images)\n\n    def __len__(self):\n        \n        return int(self.num_images \/ self.batch_size)\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size: (index+1) * self.batch_size]\n        batch_images = self.np_images[batch_indices]\n        batch_labels = self.np_labels[batch_indices]\n        \n        data_images = batch_images \n        data_labels = batch_labels \n        return data_images, data_labels\n#         return data_labels","8653e27d":"train_generator = DataGenerator(16, x_train, y_train)\nvalid_generator = DataGenerator2(50, x_valid, y_valid)","7672d98a":"train_generator[1][0].shape","c9f39c64":"print(len(train_generator))\nprint(len(valid_generator))","73ef7782":"class LeafClassifier:\n    def __init__(self):\n        self.model = None\n\n    def build_model(self):\n        input_layer = Input(shape = (256, 256, 3))\n\n        conv_layer_1 = (Conv2D(64, kernel_size = 3, strides = 2, padding='same', activation=\"relu\"))(input_layer)\n        max_pooling_2d_a = (MaxPooling2D(pool_size=(2, 2)))(conv_layer_1)\n\n        conv_layer_2 = (Conv2D(128, kernel_size = 3, strides = 2, padding='same', activation=\"relu\"))(max_pooling_2d_a)\n        max_pooling_2d_b = (MaxPooling2D(pool_size=(2, 2)))(conv_layer_2)\n        \n        conv_layer_3 = (Conv2D(256, kernel_size = 3, strides = 1,padding='same', activation=\"relu\",kernel_regularizer =tf.keras.regularizers.l2( l=0.01)))(max_pooling_2d_b)\n        max_pooling_2d_c = (MaxPooling2D(pool_size=(2, 2)))(conv_layer_3)\n        \n        conv_layer_4 = (Conv2D(256, kernel_size = 3, strides = 1,padding='same', activation=\"relu\",kernel_regularizer =tf.keras.regularizers.l2( l=0.01)))(max_pooling_2d_c)\n        max_pooling_2d_e = (MaxPooling2D(pool_size=(2, 2)))(conv_layer_4)      \n          \n        flat = Flatten()(max_pooling_2d_e)\n    \n        dense_layer_1 = Dense(512, activation ='relu',kernel_regularizer =tf.keras.regularizers.l2( l=0.01))(flat) \n        \n        dense_layer_2 = Dense(256, activation ='relu')(dense_layer_1)\n\n        output_layer = Dense(1, activation = 'sigmoid')(dense_layer_2)\n        \n        # Model Summary \n        self.model = Model(input_layer, output_layer)\n        print(self.model.summary())\n             \n        # Compile\n        loss = BinaryCrossentropy()     \n        optimizer = Adam(learning_rate=0.001)\n        self.model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics= ['accuracy'])\n#         self.model.compile(loss = loss, optimizer=optimizer, metrics=[CategoricalAccuracy()])\n        \n    def save_model(self):   \n        pass \n\n    def load_model(self):\n         self.model = load_model('model\/leaf2.hdf5')\n        \n    def train(self, train_generator, valid_generator,**kwargs):\n        keras_callbacks   = [\n              EarlyStopping(monitor='val_loss', patience=10, mode='min'),\n              ModelCheckpoint('model\/leaf2.hdf5',monitor = 'val_loss',save_best_only = True, verbose = 1 )\n        ]\n        self.history = classifier.model.fit(\n            train_generator,\n            validation_data=valid_generator,\n            steps_per_epoch= len(train_generator),\n            shuffle = True,\n            workers=2,\n            epochs=100,\n            validation_steps = len(valid_generator),\n            callbacks = [keras_callbacks]\n        )\n\n    def predict(self, x_test):\n        y_predict = self.model.predict(np.array(x_test))\n        return y_predict","2276174f":"classifier = LeafClassifier()\nclassifier.build_model()","f185d549":"classifier.train(train_generator, valid_generator)","0e65776f":"from matplotlib.pyplot import figure\n\nfigure(figsize=(10, 8), dpi=80)\nplt.plot(classifier.history.history['loss'], label='loss')\nplt.plot(classifier.history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.ylim([0, 2])\nplt.legend(loc='lower right')","bc7d61b6":"figure(figsize=(10, 8), dpi=80)\nplt.plot(classifier.history.history['accuracy'], label='accuracy')\nplt.plot(classifier.history.history['val_accuracy'], label = 'val_accuracy')\n# plt.plot(history.history['loss'], label='loss')\n# plt.plot(history.history['val_loss'], label = 'val_loss') \nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0 ,1])\nplt.legend(loc='lower right')","898b0a8f":"# classifier = LeafClassifier()\nclassifier.load_model()\nlabels_word = { 0 : \"l\u00e1 kh\u1ecfe\",\n                1 : \"l\u00e1 b\u1ec7nh\",\n              }","f6e8fcfc":"test_images = np.array(Image.open('..\/input\/leafdataset\/2TPK_256\/tst\/1\/leaf_tst_100016.jpg'))","cc2df465":"classifier.model.predict(np.array([test_images]))\n","3e3b77a8":"import matplotlib.pyplot as plt\nplt.imshow(test_images)","174fa6c8":"list_test_filenames = glob.glob('..\/input\/leafdataset\/2TPK_256\/tst\/*\/*.jpg')\nlist_test_images = [np.array(Image.open(filename)) for filename in list_test_filenames]\nlist_labels_test = [int(basename(dirname(filename))) for filename in list_test_filenames]\n\n# print(len(list_test_images))\n\n#fig, axs = plt.subplots(1,5,figsize=(25,10))\npredict_lbl = []\nfor i in range(len(list_test_filenames)):\n    img = list_test_images[i]\n    img = np.squeeze(img)\n    prob = classifier.model.predict(np.array([list_test_images[i]]))\n    if prob > .5:\n        predict_lbl.append(1)\n    else:\n        predict_lbl.append(0)\n    ","b4cd89dd":"from sklearn.metrics import roc_auc_score, accuracy_score\nprint(\"ROC-AUC: \", roc_auc_score(list_labels_test,predict_lbl))\nprint(\"Accuracy: \", accuracy_score(list_labels_test,predict_lbl))","d5aa725f":"0.8760026310500447\n0.8782051282051282\n\n","0593da54":"# **Evaluation**","6d6540b4":"# Data Augumentation\n* Nh\u00e2n 8 l\u1ea7n l\u01b0\u1ee3ng samples c\u1ee7a train set d\u1ef1a tr\u00ean c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i \u1ea3nh","9e413825":"# **Visualizing**","313b3772":"# **Create Train - Valid - Test Data**","95ff9a99":"*Shuffe data*","2ff12161":"# **Modeling**"}}