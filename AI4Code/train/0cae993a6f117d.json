{"cell_type":{"274f571a":"code","b202abc1":"code","e1562792":"code","37bff3b9":"code","a450575c":"code","c1b8a93d":"code","053086dc":"code","c138b771":"code","d3905f22":"code","e9274702":"code","addd7ebc":"code","072dae2a":"code","f40dc463":"code","4de704f4":"code","5c6a4c2b":"code","69a06e7e":"code","00bf1889":"code","2a936bd1":"code","29f47cbf":"code","61500b19":"code","eaffdb3b":"code","e329f42a":"code","8e6446f2":"code","7387b8ef":"code","a018f1f5":"code","90cbe35a":"code","e66fa46f":"code","5392706e":"code","c6d6c853":"code","bff705a5":"code","7f98fb7b":"code","765d4655":"code","b2cbc7c1":"code","d59b1c57":"code","08006765":"code","064623f2":"code","1168c7c1":"code","dceb3bc3":"code","9385a045":"code","389d33b9":"code","3670b092":"code","5d763bca":"code","111681ac":"code","7e30477a":"code","db1b0ace":"code","8270f5bb":"code","75516a0a":"code","6c7972d1":"code","cbf7a402":"code","03a75b35":"code","ba0d0842":"code","9b028d87":"code","6d80c584":"code","e9dda475":"code","6ebc9d57":"code","c006bae0":"markdown","ba00a006":"markdown","d144487c":"markdown","c5a689db":"markdown","98858805":"markdown","e431bde5":"markdown","853ff685":"markdown","69dee148":"markdown","6e30b487":"markdown","d74222fd":"markdown","c93a33d4":"markdown","91baaad8":"markdown","bb9a84b3":"markdown","e42393da":"markdown","86b7395b":"markdown","836984c5":"markdown","a8c0500a":"markdown","8bc28b83":"markdown","5658ee90":"markdown","2c91553e":"markdown","246a45b1":"markdown","5537d741":"markdown","eaabd88f":"markdown","8cdb7f10":"markdown","64acfe4c":"markdown","29e049a4":"markdown","80e1227c":"markdown","c09377a3":"markdown"},"source":{"274f571a":"# Imports\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom wordcloud import WordCloud\nfrom IPython.display import Image\nimport collections\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report, accuracy_score, roc_curve, auc, precision_score, recall_score, roc_auc_score \nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import Doc2Vec\nfrom tqdm import tqdm\nfrom sklearn import utils\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","b202abc1":"# Carregando o Dataset\nSuicide_Detection = pd.read_csv('..\/input\/suicide-watch\/Suicide_Detection.csv')","e1562792":"Suicide_Detection.shape","37bff3b9":"Suicide_Detection","a450575c":"print(Suicide_Detection.groupby(['class'])['class'].count())","c1b8a93d":"topic = Suicide_Detection['class'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(topic.index, topic.values, alpha=0.8)\nplt.ylabel('N\u00fameroi de ocorrencias', fontsize=12)\nplt.xlabel('Classe', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","053086dc":"# Verificando a quantidade de palavras existentes no conjunto de dados\nSuicide_Detection['text'].apply(lambda x: len(x.split(' '))).sum()","c138b771":"# Criando vari\u00e1veis auxiliares para explora\u00e7\u00e3o dos dados\nSuicide_Detection['post_len'] = Suicide_Detection['text'].astype(str).apply(len)\nSuicide_Detection['word_count'] = Suicide_Detection['text'].apply(lambda x: len(str(x).split()))","d3905f22":"ax = Suicide_Detection.hist(column='post_len', bins=25, grid=False, figsize=(12,8), color='blue', zorder=2, rwidth=0.9)\n\nax = ax[0]\nfor x in ax:\n\n    # Despine\n    x.spines['right'].set_visible(False)\n    x.spines['top'].set_visible(False)\n    x.spines['left'].set_visible(False)\n\n    # Switch off ticks\n    x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n    # Draw horizontal axis lines\n    vals = x.get_yticks()\n    for tick in vals:\n        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n\n    # title\n    x.set_title(\"Distribui\u00e7\u00e3o do comprimento do texto nas Postagens\")\n\n    # Set x-axis label\n    x.set_xlabel(\"Comprimento das Postagens\", labelpad=20, weight='bold', size=12)\n\n    # Set y-axis label\n    x.set_ylabel(\"Qtd Postagens\", labelpad=20, weight='bold', size=12)\n\n    # Format y-axis label\n    x.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,g}'))","e9274702":"ax = Suicide_Detection.hist(column='word_count', bins=25, grid=False, figsize=(12,8), color='blue', zorder=2, rwidth=0.9)\n\nax = ax[0]\nfor x in ax:\n\n    # Despine\n    x.spines['right'].set_visible(False)\n    x.spines['top'].set_visible(False)\n    x.spines['left'].set_visible(False)\n\n    # Switch off ticks\n    x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n    # Draw horizontal axis lines\n    vals = x.get_yticks()\n    for tick in vals:\n        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n\n    # title\n    x.set_title(\"Distribui\u00e7\u00e3o das contagens de palavras nas Postagens\")\n\n    # Set x-axis label\n    x.set_xlabel(\"Contagem de palavras\", labelpad=20, weight='bold', size=12)\n\n    # Set y-axis label\n    x.set_ylabel(\"Qtd Postagens\", labelpad=20, weight='bold', size=12)\n\n    # Format y-axis label\n    x.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,g}'))","addd7ebc":"stop_words = set(stopwords.words('english'))\nposts = Suicide_Detection['text']\nposts = [w for w in posts if not w in stop_words]\nall_posts = \" \".join(p for p in posts)","072dae2a":"wordcloud = WordCloud(background_color='black', width=1600,                            \n                      height=800, max_words=400).generate(all_posts)","f40dc463":"fig, ax = plt.subplots(figsize=(16,8))            \nax.imshow(wordcloud, interpolation='bilinear')       \nax.set_axis_off()\nplt.imshow(wordcloud)  ","4de704f4":"# Eliminando informa\u00e7\u00f5es que n\u00e3o ser\u00e3o utilizadas no processo de treinamento do modelo\nSuicide_Detection.drop(columns = [\"Unnamed: 0\", \"post_len\", \"word_count\"], inplace=True)","5c6a4c2b":"labels = Suicide_Detection['class'].values","69a06e7e":"# Definindo as percventagens de dados de treino, teste e valida\u00e7\u00e3o\ntrain_ratio = 0.70\nvalidation_ratio = 0.15\ntest_ratio = 0.15","00bf1889":"# Diivis\u00e3o dos dados em treino e teste\ndados_treino, dados_teste = train_test_split(Suicide_Detection,                                  \n                                             test_size = 1 - train_ratio, \n                                             random_state = 43, stratify=labels)","2a936bd1":"# Os dados de valida\u00e7\u00e3o estar\u00e1 representado por 15% do conjunto de dados inicial\ndados_validacao, dados_teste = train_test_split(dados_teste, test_size=test_ratio\/(test_ratio + validation_ratio),\n                                                random_state = 43) ","29f47cbf":"dados_teste.head(10)","61500b19":"import re\n\ncontraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \n                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n                    \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n                    \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n                    \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n                    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n                    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n                    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n                    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n                    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n                    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \n                    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \n                    \"you've\": \"you have\"}\ndef _get_contractions(contraction_dict):\n    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n    return contraction_dict, contraction_re\ncontractions, contractions_re = _get_contractions(contraction_dict)\ndef replace_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, text)","eaffdb3b":"def read_data(filename):\n    \"\"\"\n    Extrai as postagnes dos dados de treino e efetua o pr\u00e9-processamento usando a biblioteca nltk python\n    \n    \"\"\"\n    data  = [[],[]] \n    train_data = {}\n    for i in  tqdm(range(filename.shape[0])):\n        text_string = filename[i, 0]\n        text_string = text_string.lower()\n        \"\"\" Limpeza do texto\"\"\"\n        text_string = ''.join(c for c in text_string if not c.isdigit())\n        text_string = replace_contractions(text_string)\n        \"\"\"tokenize\"\"\"\n        tokenizer = RegexpTokenizer(r'\\w+')\n        text_string = tokenizer.tokenize(text_string)\n        # Atribui a classe aos arquivos\n        \"\"\" Elimina Sporwords \"\"\"\n        text_string  = [w for w in text_string if not w in stop_words]\n        \n        data[0].append(text_string)\n        data[1].append(filename[i, 1])\n        \n        train_data[str(filename[i, 1])+'_'+str(i)] = text_string\n    print('\\tConclu\u00edda a leitura de dados treino') \n               \n    return data, train_data\n\ndef read_test_data(filename):\n    \"\"\"\n    Extrai as postagens dos dados de teste e efetua o pr\u00e9-processamento usando a biblioteca nltk python\n    \"\"\"\n    test_data = {}\n    for i in  tqdm(range(filename.shape[0])):\n        text_string = filename[i, 0]\n        text_string = text_string.lower()\n        \"\"\" Limpeza do texto\"\"\"\n        text_string = ''.join(c for c in text_string if not c.isdigit())\n        text_string = replace_contractions(text_string) \n        tokenizer = RegexpTokenizer(r'\\w+')\n        text_string = tokenizer.tokenize(text_string)\n        # Atribui a classe aos arquivos\n        \"\"\" Atribui a rela\u00e7\u00e3o ao documento \"\"\"\n        test_data[str(filename[i, 1])+'_'+str(i)] = text_string\n    print('\\tConclu\u00edda a leitura de dados teste') \n               \n    return test_data\n\nprint('Processando dados de treinamento...\\n')\nwords, train_words = read_data(np.array(dados_treino))\n\nprint('\\nProcessando dados de teste...\\n')\n\ntest_words = read_test_data(np.array(dados_teste))","e329f42a":"vocabulary_size = 5500\nWords  = []\ndef build_dataset(words):\n    for word in words[0]:\n        Words.extend(word)  \n    count = [['UNK', -1]]\n    count.extend(collections.Counter(Words).most_common(vocabulary_size - 1))\n    # Dicion\u00e1rio das senten\u00e7as\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    \n    data = list()\n    unk_count = 0\n    for word in Words:\n        if  word in dictionary:            \n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']            \n            unk_count = unk_count + 1\n            \n        data.append(word)\n\n    count[0][1] = unk_count\n    assert len(dictionary) == vocabulary_size\n\n    return data, count, dictionary\n\ndef build_dataset_with_existing_dictionary(words, dictionary):\n    '''\n    Aqui usamos essa fun\u00e7\u00e3o para converter strings de palavras em IDs com um determinado dicion\u00e1rio\n    '''\n    data = list()\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']\n        data.append(word)\n    return data\n\n# Processando dados de treino\ndata, count, dictionary = build_dataset(words)\n\ntrain_data = {}\n\nprint('Construindo o dataset de treino para o documento\\n')\nfor k,v in tqdm(train_words.items()):\n    train_data[k] = build_dataset_with_existing_dictionary(train_words[k],dictionary)\n\n# Processando dados de teste\n\ntest_data = {}\n\nprint('Construindo o dataset de teste para o documento\\n')\nfor k,v in tqdm(test_words.items()):\n    test_data[k] = build_dataset_with_existing_dictionary(test_words[k],dictionary)\n    \nprint('\\nPalavras mais comuns (+UNK)', count[:5])\nprint('\\nAmostra de dados', data[:10])\n#print('\\nChaves: ', test_data.keys())\n#print('\\nItems: ', test_data.items())\n\n# Removemos para liberar mem\u00f3ria no computador, pois n\u00e3o precisamos mais desses objetos. Manterei apenas o dicion\u00e1rio \ndel words  \ndel data\ndel count\n#del dictionary","8e6446f2":"# Converte de Dicion\u00e1rio para lista\ndata_train = [ [k,v] for k, v in train_data.items() ]\ndata_train = np.array(data_train)\ndata_test = [ [k,v] for k, v in test_data.items() ]\ndata_test = np.array(data_test)","7387b8ef":"# fun\u00e7\u00e3o para Identificar as classes dos documentos e preparando os dados para o algoritimo\ndef prepara_dados(data):\n    datax = [[],[]]\n    for x in range(data.shape[0]):\n        s = data[x][0]\n        s = s.split(\"_\")\n        datax[0].append(s[0])\n        datax[1].append(data[x][1])                \n    return datax\n\ndata_train = prepara_dados(data_train)\ndata_test = prepara_dados(data_test)","a018f1f5":"def label_sentences(corpus, topics):\n    \"\"\"\n    A implementa\u00e7\u00e3o do Doc2Vec da Gensim exige que cada documento \/ par\u00e1grafo tenha um r\u00f3tulo associado a ele.\n\u00a0\u00a0\u00a0\u00a0Fiz isso usando o m\u00e9todo TaggedDocument, etiquetando com a pr\u00f3pria classe do documento.\n    \"\"\"\n   \n    labeled = []\n    tags = np.unique(topics, return_counts=False)\n    for i, v in enumerate(corpus):\n        label = [s for s in tags if topics[i] in s and len(s) == len(topics[i])]\n        doc =  \" \".join(str(x) for x in v)\n        labeled.append(TaggedDocument(doc.split(), label))\n    return labeled\nX_train = label_sentences(data_train[1], data_train[0])\nX_test  = label_sentences(data_test[1], data_test[0])","90cbe35a":"X_test[1]","e66fa46f":"len(X_train)","5392706e":"len(X_test)","c6d6c853":"# Instanciando um modelo Doc2Vec com um vetor de 128 palavras\n\nmodel_dbow = Doc2Vec(dm=0, vector_size=128, window=10, negative=5, cbow_mean=1, min_count=1, alpha=0.1, min_alpha=0.005)\nmodel_dbow.build_vocab([x for x in tqdm(X_train)])\n\n\n# Alicando 50 itera\u00e7\u00f5es sobre o corpus de treinamento.\n\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(X_train)]), total_examples=len(X_train), epochs=10)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha","bff705a5":"%%time\n\n# Dados de treino\n\ntrain_targets, train_regressors = zip(\n    *[(doc.tags[0], model_dbow.infer_vector(doc.words, alpha=0.1, min_alpha=0.005, epochs=100)) for doc in tqdm(X_train)])","7f98fb7b":"%%time\n\n# Dados de teste\n\ntest_targets, test_regressors = zip(\n    *[(doc.tags[0], model_dbow.infer_vector(doc.words, alpha=0.1, min_alpha=0.005, epochs=100)) for doc in tqdm(X_test)])","765d4655":"X_test[45]","b2cbc7c1":"# Aplicando de Regress\u00e3o Logistica\ntopics = ['suicide','non-suicide']\nlogreg = LogisticRegression(n_jobs=1, C=0.1, solver = 'lbfgs', penalty = 'l2', max_iter=10000)\nlogreg.fit(train_regressors, train_targets)\ny_pred = logreg.predict(test_regressors)\ny_score = logreg.predict_proba(test_regressors)","d59b1c57":"print('accuracy %s' % accuracy_score(y_pred, test_targets))\nprint(classification_report(test_targets, y_pred,target_names=topics))","08006765":"def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(topics))\n    target_names = topics\n    plt.xticks(tick_marks, target_names, rotation=45)\n    plt.yticks(tick_marks, target_names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","064623f2":"def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n    print('accuracy %s' % accuracy_score(target, predictions))\n    cm = confusion_matrix(target, predictions, labels=topics)\n    print('confusion matrix\\n %s' % cm)\n    print('(row=expected, col=predicted)')\n    \n    cm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plot_confusion_matrix(cm_normalized, title + ' Normalized')\n    return cm","1168c7c1":"cm = evaluate_prediction(y_pred, np.array(test_targets), \"Regress\u00e3o logistica com word2vec\")","dceb3bc3":"import scikitplot as skplt\nskplt.metrics.plot_roc(test_targets, y_score, figsize=(10, 8))","9385a045":"skplt.metrics.plot_precision_recall(test_targets, y_score, figsize=(10, 8))","389d33b9":"# Divis\u00e3o dos dados em X e Y\n\nX_validacao = dados_validacao.loc[:, dados_validacao.columns != 'class']\nY_validacao = dados_validacao['class']","3670b092":"def read_valid_data(filename):\n    \"\"\"\n    Extrai as postagens dos dados de valida\u00e7\u00e3o e efetua o pr\u00e9-processamento usando a biblioteca nltk python\n    \"\"\"\n    valid_data = {}\n    for i in tqdm(range(filename.shape[0])):\n        text_string = filename[i, 0]\n        text_string = text_string.lower()\n        \"\"\" Limpeza do texto\"\"\"\n        text_string = ''.join(c for c in text_string if not c.isdigit())\n        text_string = replace_contractions(text_string) \n        tokenizer = RegexpTokenizer(r'\\w+')\n        text_string = tokenizer.tokenize(text_string)\n        # Atribui a classe aos arquivos\n        \"\"\" Atribui a rela\u00e7\u00e3o ao documento \"\"\"\n        valid_data[str(i)] = text_string\n    print('\\tConclu\u00edda a leitura de dados de valida\u00e7\u00e3o\\n') \n               \n    return valid_data","5d763bca":"valid_words = read_valid_data(np.array(dados_validacao))","111681ac":"# Processando dados de valida\u00e7\u00e3o\n\nvalid_data = {}\n    \nprint('Construindo o dataset de valida\u00e7\u00e3o para a postagem\\n')\nfor k,v in tqdm(valid_words.items()):\n    valid_data[k] = build_dataset_with_existing_dictionary(valid_words[k],dictionary)","7e30477a":"# Converte de Dicion\u00e1rio para lista\ndata_valid = [ [k,v] for k, v in valid_data.items() ]\ndata_valid = np.array(data_valid)","db1b0ace":"data_valid = prepara_dados(data_valid)","8270f5bb":"X_valid = label_sentences(data_valid[1], data_valid[0])","75516a0a":"%%time\n\n# Dados de valida\u00e7\u00e3o\n\nvalid_targets, valid_regressors = zip(\n    *[(doc.tags[0], model_dbow.infer_vector(doc.words, alpha=0.1, min_alpha=0.005, epochs=100)) for doc in tqdm(X_valid)])","6c7972d1":"# Exclu\u00edndo o dicion\u00e1rio\ndel dictionary","cbf7a402":"y_pred = logreg.predict(valid_regressors)\ny_score = logreg.predict_proba(valid_regressors)","03a75b35":"print('accuracy %s' % accuracy_score(y_pred, Y_validacao))\nprint(classification_report(Y_validacao, y_pred,target_names=topics))","ba0d0842":"cm = evaluate_prediction(y_pred, np.array(Y_validacao), \"Regress\u00e3o logistica com word2vec\")","9b028d87":"skplt.metrics.plot_roc(Y_validacao, y_score, figsize=(10, 8))","6d80c584":"skplt.metrics.plot_precision_recall(Y_validacao, y_score, figsize=(10, 8))","e9dda475":"# Aplicando o Estat\u00edstica Kappa\ncohen_kappa = cohen_kappa_score(Y_validacao, y_pred)","6ebc9d57":"cohen_kappa","c006bae0":"Os graficos de Curva ROC e Precis\u00e3o-Recall demostram uma grande capacidade do modelo rotular todas as inst\u00e2ncias corretamente.","ba00a006":"Temos mais de 30 milh\u00f5es de palavras, trata-se de um conjunto de dados relativamente grande.","d144487c":"\n\n## Preven\u00e7\u00e3o de Suic\u00eddio\n![fd1f0842-10ce-4cd8-9529-11a2f11e0c5c.png](attachment:fdeb9d71-188d-434a-a440-449101ba5d35.png)\n\n## Defini\u00e7\u00e3o do Projeto\n\n\n### Porque \u00e9 importante prever o suic\u00eddio?\n\n\nSegundo a OMS, o suic\u00eddio \u00e9 a 3\u00aa causa de morte de jovens brasileiros entre 15 e 29 anos. No Brasil, cerca de 12 mil pessoas tiram a pr\u00f3pria vida por ano, quase 6% da popula\u00e7\u00e3o. No mundo, s\u00e3o cerca de 800 mil de suic\u00eddios anuais. O Brasil s\u00f3 perde para os EUA.\n\nNo mundo, as notifica\u00e7\u00f5es apontam para um suic\u00eddio a cada 40 segundos. No Brasil, a cada 46 minutos, uma realidade devastadora quando se identifica o perfil das v\u00edtimas brasileiras: a maioria \u00e9 homem, negro, com idade entre 10 e 29 anos, segundo dados do Minist\u00e9rio da Sa\u00fade. Na Bahia, entre 2010 e 2019, foram contabilizados 8.833 casos de les\u00f5es autoprovocadas e, destes, 5.160 foram casos de suic\u00eddio. \n\nFonte:\nhttp:\/\/www.saude.ba.gov.br\/2020\/09\/10\/oms-alerta-suicidio-e-a-3a-causa-de-morte-de-jovens-brasileiros-entre-15-e-29-anos\/\n\nAlguns dados sobre suic\u00eddio fornecidos pela Organiza\u00e7\u00e3o Mundial de Sa\u00fade:\n* Quase 800.000 pessoas morrem por suic\u00eddio todos os anos. Al\u00e9m disso, para cada suic\u00eddio, ocorrem mais de 20 tentativas de suic\u00eddio.\n* Suic\u00eddios e tentativas de suic\u00eddio t\u00eam um efeito cascata que afeta fam\u00edlias, amigos, colegas, comunidades e sociedades.\n* Os suic\u00eddios s\u00e3o evit\u00e1veis. Muito pode ser feito para prevenir o suic\u00eddio em n\u00edvel individual, comunit\u00e1rio e nacional.\n\nAlgumas estat\u00edsticas sobre suic\u00eddio no Brasil: \n* A  maioria  dos  registros  de  suic\u00eddio  no  Brasil  entre  2010  e  2019  foram  de  pessoas solteiras (54,46%).\n* Os  meses  de  Dezembro,  Outubro  e  Mar\u00e7o,  respectivamente,  contam  com  o  maior n\u00famero de casos de suic\u00eddio.\n* A m\u00e9dia de suic\u00eddios ao longo de 2010 e 2019, por ano, \u00e9 de 11249. De modo geral, o n\u00famero bruto de suic\u00eddios vem aumentando ao longo dos anos.-O local mais comum para ocorr\u00eancia do suic\u00eddio \u00e9 no domic\u00edlio (60,89% dos casos).\n* A maioria dos casos de suic\u00eddio n\u00e3o recebeu assist\u00eancia m\u00e9dica preventiva.\n\nNeste mini-projeto trabalharei com um tema relevante para a sociedade e ao mesmo tempo demonstrarei minhas habilidades em Machine Learning estudadas no curso.\n\n\n\n\n\n## Dataset\n\nPara essa tarefa, usarei um conjunto de dados disponibilizado publicamente no Kaggle [dataset](https:\/\/www.kaggle.com\/nikhileswarkomati\/suicide-watch), extraido de postagens dos subreddits \"SuicideWatch\" e \"depression\" da plataforma Reddit. O Reddit \u00e9 uma plataforma preferida por pessoas com problemas emocionais, por isso fornece pistas significativas para o suic\u00eddio.\n\nAs postagens foram coletadas usando a API Pushshift. Todas as postagens feitas para \"SuicideWatch\" foram coletadas de 16 de dezembro de 2008 (cria\u00e7\u00e3o) at\u00e9 2 de janeiro de 2021 , enquanto as postagens de \"depression\" foram coletadas de 1 de janeiro de 2009 a 2 de janeiro de 2021.\n\nCr\u00e9ditos para Nikhileswar Komati por coletar e disponibilizar as informa\u00e7\u00f5es na plataforma Kaggle.\n\n#### Obs.: A fonte de dados j\u00e1 est\u00e1 previamente balanceada.\n  \nUm exemplo de post categorizado como suic\u00eddio:\n\n'I've become so accustomed to the fact that I'm going to kill myself that other people's struggles with suicide don't register to me muchBecause I think about it so much I just see them as another person like me. It also makes me feel like everyone is like me, so if I went to kill myself and people were around nobody would care.'\n\nUsaremos 116037 postagens de cada categoria (suicide, non-suicide). Nosso vocabul\u00e1rio ser\u00e1 de tamanho 5.500. Al\u00e9m disso, cada documento ser\u00e1 representado por uma tag - para fins de visualiza\u00e7\u00e3o. Por exemplo, o 50\u00ba documento classificado com suic\u00eddio ser\u00e1 representado como suic\u00eddio-50.\n","c5a689db":"### Explorando os dados","98858805":"## Prepara\u00e7\u00e3o dos dados de valida\u00e7\u00e3o para aplica\u00e7\u00e3o do algor\u00edtmo","e431bde5":"Nuvens de palavras s\u00e3o representa\u00e7\u00f5es gr\u00e1ficas da frequ\u00eancia das palavras que d\u00e3o maior destaque \u00e0s palavras que aparecem com mais frequ\u00eancia em um texto de origem. Quanto maior a palavra no visual, mais comum ela \u00e9 no (s) texto (s). Esse tipo de visualiza\u00e7\u00e3o pode ajudar os avaliadores com an\u00e1lise textual explorat\u00f3ria, identificando palavras que aparecem com frequ\u00eancia em um conjunto de entrevistas, documentos ou texto. Tamb\u00e9m pode ser usado para comunicar os pontos ou temas mais salientes no est\u00e1gio de relat\u00f3rio.","853ff685":"A precision-recall curve mostra a compensa\u00e7\u00e3o entre precis\u00e3o e recupera\u00e7\u00e3o para diferentes limites. Uma \u00e1rea alta sob a curva representa alta recupera\u00e7\u00e3o e alta precis\u00e3o, onde alta precis\u00e3o se relaciona a uma taxa baixa de falsos positivos e alta recupera\u00e7\u00e3o se relaciona a uma baixa taxa de falsos negativos. Altas pontua\u00e7\u00f5es para ambos mostram que o classificador est\u00e1 retornando resultados precisos (alta precis\u00e3o), bem como retornando a maioria de todos os resultados positivos (alta recupera\u00e7\u00e3o).\n\nOs graficos de Curva ROC e Precis\u00e3o-Recall demostram uma grande capacidade do modelo rotular todas as inst\u00e2ncias corretamente.","69dee148":"A maiorias das postagens possuem at\u00e9 mais ou menos 1600 caracteres, com at\u00e9 mais ou menos 400 palavras.","6e30b487":"## Prepara\u00e7\u00e3o dos dados para cria\u00e7\u00e3o do vocabul\u00e1rio no doc2vec","d74222fd":"## Construindo os Vetores de Recurso para o Classificador","c93a33d4":"## Modelos de Treinamento e Avalia\u00e7\u00e3o doc2Vec\n\n\nDoc2vec \u00e9 um algoritmo n\u00e3o supervisionado para gerar vetores para frases, par\u00e1grafos ou documentos (Representa\u00e7\u00f5es distribu\u00eddas de senten\u00e7as e documento). Trata-se de um conceito que foi apresentado em 2014 por Le & Mikilov, veja neste [artigo](https:\/\/arxiv.org\/abs\/1405.4053). Este algoritmo \u00e9 uma adapta\u00e7\u00e3o do word2vec, sendo que os vetores gerados pelo doc2vec podem ser usados para tarefas como encontrar semelhan\u00e7as entre senten\u00e7as , par\u00e1grafos ou documentos.\n\nUm vetor de documento \u00e9 uma representa\u00e7\u00e3o abstrata de comprimento vari\u00e1vel do significado contextual de um determinado tipo de documento. Assim como um vetor de palavras, \u00e9 o produto do processo de treinamento para uma rede neural, onde a entrada \u00e9 tipicamente um termo one-hot encoded codificado a partir do vocabul\u00e1rio do modelo e a sa\u00edda \u00e9 uma distribui\u00e7\u00e3o de probabilidade para palavras na pr\u00f3xima janela de contexto. \n\nPrimeiramente instanciamos um modelo doc2vec - Distributed Bag of Words (DBOW). Na arquitetura word2vec, n\u00f3s temos os algoritmos  \u201ccontinuous bag of words\u201d (CBOW) e \u201cskip-gram\u201d (SG), j\u00e1 na arquitetura doc2vec, os algoritmos correspondentes s\u00e3o \u201cdistributed memory\u201d (DM) e \u201cdistributed bag of words\u201d (DBOW).\n\nO DBOW \u00e9 o modelo doc2vec an\u00e1logo ao modelo Skip-gram do word2vec. Os vetores de par\u00e1grafos s\u00e3o obtidos pelo treinamento de uma rede neural na tarefa de prever uma distribui\u00e7\u00e3o de probabilidade de palavras em um par\u00e1grafo, dada uma palavra aleatoriamente amostrada do par\u00e1grafo.","91baaad8":"## Carregando os Dados","bb9a84b3":"![image.png](attachment:5f2fa6e8-a1cd-4828-9af9-864888d96b29.png)","e42393da":"## Cohen Kappa","86b7395b":"### Lendo os dados com pr\u00e9-processamento utilizando a biblioteca NLTK\n\nAgora vamos efetuar a leitura dos dados, converter o texto para letras min\u00fascula, remo\u00e7\u00e3o de contra\u00e7\u00f5es e depois converter em tokens usando a biblioteca nltk. Temos duas fun\u00e7\u00f5es read_data para montar os dados de treino e read_test_data para montar os dados de teste.","836984c5":"# Fim","a8c0500a":"## Engenharia de Atributos","8bc28b83":"### Visualizando a distribui\u00e7\u00e3o da contagem de palavras das Postagens","5658ee90":"## Referencias\n\n[Curso de Processamento de Linguagem Natural e Reconhecimento de Voz da Forma\u00e7\u00e3o Intelig\u00eancia Artificial - Datascienceacademy](https:\/\/www.datascienceacademy.com.br)\n\n[Gemsin](https:\/\/radimrehurek.com\/gensim\/models\/keyedvectors.html)\n\n[Distributed Representations of Words and Phrases and their Compositionality](https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n\n[Distributed Representations of Sentences and Documents](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf)\n\n[A gentle introduction to Doc2Vec](https:\/\/medium.com\/scaleabout\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)\n\n[Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5\/docs\/notebooks\/doc2vec-IMDB.ipynb)\n\n[Document classification with word embeddings tutorial](https:\/\/github.com\/RaRe-Technologies\/movie-plots-by-genre\/blob\/master\/ipynb_with_output\/Document%20classification%20with%20word%20embeddings%20tutorial%20-%20with%20output.ipynb)\n\n[Using Deep Learning for End to End Multiclass Text Classification](https:\/\/lionbridge.ai\/articles\/using-deep-learning-for-end-to-end-multiclass-text-classification\/)\n\n[Extracting Information from Text](https:\/\/www.nltk.org\/book\/ch07.html)\n","2c91553e":"### Construindo os Dicion\u00e1rios\nPara entender cada um desses elementos, vamos tamb\u00e9m assumir o texto \"Eu gosto de ir \u00e0 escola\"\n\n* `dictionary`: mapeia uma palavra para um ID (i.e. {Eu:0, gosto:1, de:2, ir:3, \u00e0:4, escola:5})\n* `reverse_dictionary`: mapeia um ID para uma palavra (i.e. {0:Eu, 1:gosto, 2:de, 3:ir, 4:\u00e0, 5:escola}\n* `count`: Lista de elementos (palavra, frequ\u00eancia) (i.e. [(Eu,1),(gosto,1),(de,2),(ir,1),(\u00e0,1),(escola,1)]\n* `data` : Cont\u00e9m a string de texto que lemos, onde palavras s\u00e3o substitu\u00eddas por IDs de palavras (i.e. [0, 1, 2, 3, 2, 4])\n\nTamb\u00e9m introduzimos um token especial adicional chamado `UNK` para indicar que palavras raras s\u00e3o muito raras para serem usadas.","246a45b1":"## Treinando o Classificador de Regress\u00e3o Log\u00edstica.","5537d741":"Usaremos a seguinte estrat\u00e9gia:\n\n1. Extrair os dados dos textos das postegens e aprender as word embeddings.\n2. Extrair conjuntos rand\u00f4micos de documentos dos word embeddings j\u00e1 treinados.\n3. Preparar os dados para os modelos de treinamento e avalia\u00e7\u00e3o doc2Vec\n4. Construir o vocabul\u00e1rio atrav\u00e9s de um modelo doc2vec - Distributed Bag of Words (DBOW)\n5. Aplicar um modelo de Regress\u00e3o Log\u00edstica para classifica\u00e7\u00e3o\n6. Apresentar ao modelo dados n\u00e3o envolvidos no processo de treinamento e teste para validar a capacidade do modelo classificar corretamente as postagens.","eaabd88f":"## Construindo os Vetores de Recurso para o Classificador","8cdb7f10":"### Visualizando a Nuvem de Palavras das Postagens","64acfe4c":"O Cohen kappa \u00e9 uma m\u00e9trica frequentemente usada para avaliar a concord\u00e2ncia entre dois avaliadores. Tamb\u00e9m pode ser usado para avaliar o desempenho de um modelo de classifica\u00e7\u00e3o, como muitas outras m\u00e9tricas de avalia\u00e7\u00e3o, o Cohen kappa \u00e9 calculado com base na matriz de confus\u00e3o.\n\nLandis e Koch (1977) classificam os diferentes n\u00edveis de concord\u00e2ncia\n(ou reprodutibilidade), o Kappa varia de 0 a 1, conforme mostra tabela abaixo:\n\n\n      Valores de Kappa    Interpreta\u00e7\u00e3o\n           < 0            Aus\u00eancia de concord\u00e2ncia\n         0 - 0.20         Concord\u00e2ncia m\u00ednima\n      0.21 - 0.40         Concord\u00e2ncia razo\u00e1vel\n      0.41 - 0.60         Concord\u00e2ncia moderada\n      0.61 - 0.80         Concord\u00e2ncia substancial\n      0.81 - 1.00         Concord\u00e2ncia quase perfeita\n\nA estat\u00edstica kappa \u00e9 freq\u00fcentemente usada como uma medida de confiabilidade entre dois avaliadores humanos. No aprendizado de m\u00e1quina supervisionado, um \"avaliador\" reflete a verdade b\u00e1sica, ou seja, os valores reais de cada inst\u00e2ncia a ser classificada, obtidos a partir de dados rotulados, e o outro \"avaliador\" \u00e9 o classificador de aprendizado de m\u00e1quina usado para realizar a classifica\u00e7\u00e3o.","29e049a4":"##### Conclus\u00e3o \nNosso modelo obteve um score Kappa de 0,85, de acordo a tabela, isso significa que h\u00e1 uma concord\u00e2ncia quase perfeita entre a classifica\u00e7\u00e3o realizada pelo modelo e a classifica\u00e7\u00e3o relacionada as postagens dos subreddits da plataforma Reddit, demonstrando que o modelo atingiu uma execelnete capacidade de classificar corretamente as postagens com coment\u00e1rios de pessoas com potencial tendencia ao suic\u00eddio.","80e1227c":"### Visualizando a distribui\u00e7\u00e3o do comprimento do texto das Postagens","c09377a3":"Em uma curva Receiver Operating Characteristic (ROC), a taxa de verdadeiro positivo (Sensibilidade) \u00e9 tra\u00e7ada em fun\u00e7\u00e3o da taxa de falsos positivos (100-especificidade) para diferentes pontos de corte. Cada ponto na curva ROC representa um par de sensibilidade \/ especificidade correspondente a um determinado limite de decis\u00e3o. Um teste com discrimina\u00e7\u00e3o perfeita (sem sobreposi\u00e7\u00e3o nas duas distribui\u00e7\u00f5es) possui uma curva ROC que passa pelo canto superior esquerdo (100% de sensibilidade, 100% de especificidade). Portanto, quanto mais pr\u00f3xima a curva ROC estiver do canto superior esquerdo, maior ser\u00e1 a precis\u00e3o geral do teste (Zweig & Campbell, 1993)."}}