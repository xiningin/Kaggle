{"cell_type":{"920785d4":"code","4bd2bb7b":"code","187e2d45":"code","152c2cc0":"code","e1d8db3e":"code","2ca41227":"code","7fa9e416":"code","392fca24":"code","0d14e027":"code","ce44446c":"code","e0890ca9":"markdown"},"source":{"920785d4":"import os\nimport lzma\n\nimport cloudpickle\nimport datatable as dt\nimport gresearch_crypto\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.ensemble import VotingRegressor, BaggingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler","4bd2bb7b":"def get_feature_columns(df):\n    features = df.columns[df.columns.str.startswith('feature')]\n    return sorted(list(features))\n\ndef save_model(model, path):\n    data = cloudpickle.dumps(model)\n    data = lzma.compress(data)\n    with open(path, 'wb') as f:\n        f.write(data)\n        \ndef process_data(df, df_asset):\n    df = df.copy()\n    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', utc=True)\n    df = df.rename(columns={\n        'Asset_ID': 'market',\n        'Open': 'op',\n        'High': 'hi',\n        'Low': 'lo',\n        'Close': 'cl',\n        'Volume': 'volume',\n        'VWAP': 'vwap',\n        'Count': 'trade_count',\n        'Target': 'target',\n    })\n    df = df.join(df_asset[['weight']], on='market', how='left')\n    df = df.set_index(['timestamp', 'market'])\n    return df\n\ndef sort_and_remove_duplicates(df):\n    df = df.sort_index(kind='mergesort')\n    # https:\/\/stackoverflow.com\/questions\/13035764\/remove-rows-with-duplicate-indices-pandas-dataframe-and-timeseries\n    df = df.loc[~df.index.duplicated(keep='last')]\n    return df\n\ndef my_purge_kfold(n, n_splits=5, purge=3750 * 14):\n    idx = np.arange(n)\n    cv = []\n    for i in range(n_splits):\n        val_start = i * n \/\/ n_splits\n        val_end = (i + 1) * n \/\/ n_splits\n        val_idx = idx[val_start:val_end]\n        train_idx = idx[(idx < val_start - purge) | (val_end + purge <= idx)]\n        cv.append((\n            train_idx,\n            val_idx,\n        ))\n    return cv","187e2d45":"# preprocess asset data\n\ndf = dt.fread('..\/input\/g-research-crypto-forecasting\/asset_details.csv').to_pandas()\ndf = df.rename(columns={\n    'Asset_ID': 'market',\n    'Weight': 'weight',\n    'Asset_Name': 'name',\n})\ndf = df.set_index('market')\ndf = df.sort_values('market')\ndf.to_pickle('\/tmp\/df_asset.pkl')\ndisplay(df)","152c2cc0":"# preprocess train data\n\n# supplemental_train\u306f\u63d0\u51fa\u5f8c\u306b\u5897\u3048\u308b\u30c7\u30fc\u30bf\u3089\u3057\u3044\n# train\u3068supplemental_train\u3092\u304f\u3063\u3064\u3051\u3066train\u306b\u3059\u308c\u3070\u826f\u3055\u305d\u3046\ndf = pd.concat([\n    dt.fread('..\/input\/g-research-crypto-forecasting\/train.csv').to_pandas(),\n    dt.fread('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').to_pandas(),\n])\ndf_asset = pd.read_pickle('\/tmp\/df_asset.pkl')\ndf = process_data(df, df_asset)\ndf = sort_and_remove_duplicates(df)\ndf.to_pickle('\/tmp\/df.pkl')\ndisplay(df)","e1d8db3e":"# check interval is 1 min\n\ndf = pd.read_pickle('\/tmp\/df.pkl')\ndf = df.reset_index()\ndf['timestamp'] = df['timestamp'].view(int) \/ 10 ** 9\ndf['interval'] = df['timestamp'] - df.groupby('market')['timestamp'].shift(1)\ndf = df.dropna()\nfor interval in range(60, 361, 60):\n    print('{} {}'.format(interval, np.mean(df['interval'] == interval)))","2ca41227":"# calc features\n\ndef calc_features(df):\n    df = df.copy()\n    \n    df['ln_cl'] = np.log(df['cl'])\n    \n#     df['feature_upper_shadow'] = df['hi'] - np.maximum(df['op'], df['cl'])\n#     df['feature_lower_shadow'] = np.minimum(df['cl'], df['op']) - df['lo']\n\n    # shift is faster than diff\n    df['feature_cl_diff1'] = df['ln_cl'] - df.groupby('market')['ln_cl'].shift(15)\n    df['raw_return_causal'] = df['ln_cl'] - df.groupby('market')['ln_cl'].shift(15)\n    \n    inv_weight_sum = 1.0 \/ df.groupby('timestamp')['weight'].transform('sum')\n    \n    df['market_return_causal'] = (df['raw_return_causal'] * df['weight']).groupby('timestamp').transform('sum') * inv_weight_sum\n    \n    df['beta_causal'] = (\n        (df['raw_return_causal'] * df['market_return_causal']).groupby('market').transform(lambda x: x.rolling(3750, 1).mean())\n        \/ (df['market_return_causal'] ** 2).groupby('market').transform(lambda x: x.rolling(3750, 1).mean())\n    )\n    \n    df['feature_cl_diff1_mean_simple'] = df['feature_cl_diff1'].groupby('timestamp').transform('mean')\n    df['feature_cl_diff1_mean_weight'] = (df['feature_cl_diff1'] * df['weight']).groupby('timestamp').transform('sum') * inv_weight_sum\n    df['feature_cl_diff1_resid'] = df['feature_cl_diff1'] - df['beta_causal'] * df['feature_cl_diff1_mean_weight']\n    \n    df['feature_cl_diff1_rank'] = df.groupby('timestamp')['feature_cl_diff1'].transform('rank')\n    \n    df = df.rename(columns={\n        'beta_causal': 'feature_beta_causal',\n    })\n    \n    return df\n\ndf = pd.read_pickle('\/tmp\/df.pkl')\ndf = calc_features(df)\nfeatures = get_feature_columns(df)\ndf = df[features + ['target', 'weight']]\ndf.to_pickle('\/tmp\/df_features.pkl')","7fa9e416":"df = pd.read_pickle('\/tmp\/df_features.pkl')\ndf = df.dropna()\nfeatures = get_feature_columns(df)\nfor feature in features:\n    print('{} {}'.format(feature, pearsonr(df[feature], df['target'])))\n    \nfor market, df_market in df.groupby('market'):\n    for feature in features:\n        print('{} {} {}'.format(market, feature, pearsonr(df_market[feature], df_market['target'])))","392fca24":"# cv\ndf = pd.read_pickle('\/tmp\/df_features.pkl')\nfeatures = get_feature_columns(df)\ndf = df.dropna()\n# df = df.loc[df.index.get_level_values(0) < pd.to_datetime('2019-01-01 00:00:00Z')]\ndf = df.loc[df.index.get_level_values(0) < pd.to_datetime('2021-01-01 00:00:00Z')]\n\nmodel = Ridge()\n# model = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\n\nmodel = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', model)\n])\n\n# model = BaggingRegressor(\n#     model,\n#     n_estimators=1,\n#     random_state=1,\n# )\n\ncv = my_purge_kfold(df.shape[0])\ndf['y_pred'] = cross_val_predict(\n    model,\n    X=df[features],\n    y=df['target'], \n    cv=cv, \n#     n_jobs=-1,\n)\n\nprint(r2_score(df['target'], df['y_pred']))\nprint(pearsonr(df['target'], df['y_pred']))\nprint(df['target'].std())\n\nprint('pearsonr by market')\ndisplay(df.groupby('market').apply(lambda x: pearsonr(x['target'], x['y_pred'])[0]))\n\ndf2 = df.reset_index().set_index('timestamp')\nmarket_count = df2['market'].unique().size\ndf2['target'].rolling(3 * 30 * 24 * 60 * market_count).corr(df2['y_pred']).iloc[::24 * 60 * market_count].plot()\nplt.title('3 month rolling pearsonr')\nplt.show()\n\nif False:\n    for market, df_market in df.groupby('market'):\n        df2 = df_market.reset_index().set_index('timestamp')\n        df2['target'].rolling(3 * 30 * 24 * 60).corr(df2['y_pred']).iloc[::24 * 60].plot()\n        plt.title('3 month rolling pearsonr {}'.format(market))\n        plt.show()\n\n# \u30dc\u30c3\u30c8\u3067\u4f7f\u3063\u3066\u3044\u308b\u7279\u5fb4\u91cf(\u91cd\u3044\u306e\u306f\u524a\u9664) + ridge\u306e\u30b9\u30b3\u30a2\n# r2 0.00012130795512599324\n# pearsonr (0.013457722434615444, 0.0)\n# target std 0.005677241985371386","0d14e027":"# refit\nmodel.fit(df[get_feature_columns(df)], df['target'])\nsave_model(model, 'model.xz')","ce44446c":"# submit\n\nrecent_sec = 4000 * (5 * 60)\ndf = pd.read_pickle('\/tmp\/df.pkl')\ndf_asset = pd.read_pickle('\/tmp\/df_asset.pkl')\nmodel = joblib.load('model.xz')\n\nenv = gresearch_crypto.make_env() \niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    # \u6700\u65b0\u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\n    test_df2 = process_data(test_df, df_asset)\n    if 'row_id' in df.columns:\n        df = df.drop(columns=['row_id']) # \u5ff5\u306e\u70barow_id\u91cd\u8907\u306b\u5bfe\u5fdc\n    df = df.append(test_df2)\n    \n    # \u6700\u8fd1\u306e\u30c7\u30fc\u30bf\u3060\u3051\u306b\u3059\u308b(for performance)\n    test_min_timestamp = test_df2.index.get_level_values(0).min()\n    df = df.loc[test_min_timestamp - pd.to_timedelta(recent_sec, unit='s') <= df.index.get_level_values(0)]\n    df = sort_and_remove_duplicates(df)\n    \n    # \u7279\u5fb4\u91cf\u8a08\u7b97\n    df_features = calc_features(df)\n    \n    # \u4e88\u6e2c\n    df_features = df_features.loc[~df_features['row_id'].isna()]\n    df_features['Target'] = model.predict(df_features[get_feature_columns(df_features)].values)\n    sample_prediction_df = sample_prediction_df.merge(df_features[['row_id', 'Target']], how='left', on='row_id')\n    \n    if False:\n        display(test_df)\n        display(sample_prediction_df)\n    \n    env.predict(sample_prediction_df)","e0890ca9":"## \u30e1\u30e2\n\n\u30e9\u30b0\u3084\u30de\u30fc\u30b1\u30c3\u30c8\u30ea\u30bf\u30fc\u30f3\u306a\u3069\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u9069\u5f53\u306b\u7279\u5fb4\u91cf\u4f5c\u308a\u3001Ridge\u56de\u5e30\u3067Target\u3092\u4e88\u6e2c\u3057\u305fnotebook\u3002\n\u63d0\u51fa\u304c\u30bf\u30a4\u30e0\u30a2\u30a6\u30c8\u306b\u306a\u308b\u306e\u3067\u3001\u63d0\u51fa\u90e8\u5206\u306e\u9ad8\u901f\u5316\u304c\u5fc5\u8981\u3002\n\n\u30b3\u30fc\u30c9\u306f\u516c\u958b\u3057\u3066\u3044\u306a\u3044\u304c\u3001\u4ffa\u304c\u904b\u7528\u3057\u3066\u3044\u308b\u4eee\u60f3\u901a\u8ca8\u30dc\u30c3\u30c8\u3067\u4f7f\u3063\u3066\u3044\u308b\u7279\u5fb4\u91cf + ridge\u56de\u5e30(alpha\u306f\u9069\u5f53)\u306e\u30b9\u30b3\u30a2(\u76f8\u95a2)\u306f0.013\u7a0b\u5ea6\u3060\u3063\u305f\u3002\n\u3053\u306enotebook\u3060\u3068\u76f8\u95a2\u304c0.04\u304f\u3089\u3044\u3002\n\n## Notes\n\nA notebook that predicts the target by ridge regression by creating some features by combining lag and market returns.\nSince the submission will time out, it is necessary to speed up the submission part.\n\nAlthough the code is not disclosed, the score (correlation) when using the features used in my crypto trading bot was about 0.013.\nWith this notebook, the correlation is about 0.04.\n\n## TODO\n\n- ffill missing bars\n- evaluation metrics (use weights. other metrics like sharpe, double sharpe)\n- model and feature improvement\n- robust cv (nested cv or bbc-cv) + hyper parameter tuning"}}