{"cell_type":{"37c371b2":"code","e9dc573e":"code","049b49c0":"code","e9608713":"code","1c2cb561":"code","2c116ea7":"code","d7c3ff1e":"code","90341768":"code","ac24b6e0":"code","048ecfc3":"code","ab94bb7a":"code","bd12f7d6":"code","d23d8d7f":"code","c730d6af":"code","5b2c0418":"code","e5390308":"code","6f2cf50a":"code","ed5027c5":"code","a14092aa":"code","9a992165":"code","01aacb57":"code","79518cba":"code","eee1bfab":"code","5e907198":"code","4a00266a":"code","f7e00bed":"code","fcc9cde9":"code","dd761d71":"code","6d96f37d":"code","77c54305":"code","9111ae8c":"code","9d30314b":"code","7d24c014":"code","e2231865":"code","5a2e67bb":"code","50586e9e":"code","41d66da1":"code","e5788e51":"code","c72cb2e4":"code","f784f112":"code","034f707f":"code","50f6168e":"code","0a9a7723":"markdown","2e6135b2":"markdown","a17577eb":"markdown","22a0049e":"markdown","3b61f9b3":"markdown","7ba0a427":"markdown","7baeaa14":"markdown","a2122809":"markdown"},"source":{"37c371b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9dc573e":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndata.head()","049b49c0":"data.info()","e9608713":"data.describe()","1c2cb561":"# Checking for Null values\ndata.isnull().sum()","2c116ea7":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX = data.iloc[:,0:13] \ny = data.iloc[:,-1]     \n#apply SelectKBest class to extract top best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']\n#print best features\nprint(featureScores.nlargest(12,'Score'))","d7c3ff1e":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) \n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(13).plot(kind='barh')\nplt.show()","90341768":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(),annot=True,cmap=\"magma\",fmt='.2f')","ac24b6e0":"sns.set_style('darkgrid')\nsns.set_palette('Set2')","048ecfc3":"data1 = data.copy()\ndef chng(sex):\n    if sex == 0:\n        return 'female'\n    else:\n        return 'male'\ndata1['sex'] = data1['sex'].apply(chng)\ndef chng2(prob):\n    if prob == 0:\n        return 'Heart Disease'\n    else:\n        return 'No Heart Disease'\ndata['target'] = data1['target'].apply(chng2)","ab94bb7a":"# Countplot\ndata1['target'] = data1['target'].apply(chng2)\nsns.countplot(data= data1, x='sex',hue='target')\nplt.title('Gender v\/s target\\n')","bd12f7d6":"sns.countplot(data= data1, x='cp',hue='target')\nplt.title('Chest Pain Type v\/s target\\n')","d23d8d7f":"sns.countplot(data= data1, x='sex',hue='thal')\nplt.title('Gender v\/s Thalassemia\\n')","c730d6af":"sns.countplot(data= data1, x='slope',hue='target')\nplt.title('Slope v\/s Target\\n')","5b2c0418":"sns.countplot(data= data1, x='exang',hue='thal')\nplt.title('exang v\/s Thalassemia\\n')","e5390308":"# Boxplot\nsns.boxplot(data=data1,x='target',y='age')","6f2cf50a":"plt.figure(figsize=(14,8))\nsns.violinplot(data=data1,x='ca',y='age',hue='target')","ed5027c5":"sns.boxplot(data=data1,x='cp',y='thalach',hue='target')","a14092aa":"plt.figure(figsize=(10,7))\nsns.boxplot(data=data1,x='fbs',y='trestbps',hue='target')","9a992165":"plt.figure(figsize=(10,7))\nsns.violinplot(data=data1,x='exang',y='oldpeak',hue='target')","01aacb57":"plt.figure(figsize=(10,7))\nsns.boxplot(data=data1,x='slope',y='thalach',hue='target')","79518cba":"sns.violinplot(data=data1,x='thal',y='oldpeak',hue='target')","eee1bfab":"sns.violinplot(data=data1,x='target',y='thalach')","5e907198":"# PairPlot\nsns.pairplot(data,hue='cp')","4a00266a":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nX = data.iloc[:,0:13] # Features\ny = data.iloc[:,13] # Target variable\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","f7e00bed":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","fcc9cde9":"#Change Name of the column\ndata.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg_type', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope_type', 'num_major_vessels', 'thalassemia_type', 'target']\ndata.columns","dd761d71":"# Generating categorical columns values\n#cp - chest_pain_type\ndata.loc[data['chest_pain_type'] == 0, 'chest_pain_type'] = 'asymptomatic'\ndata.loc[data['chest_pain_type'] == 1, 'chest_pain_type'] = 'atypical angina'\ndata.loc[data['chest_pain_type'] == 2, 'chest_pain_type'] = 'non-anginal pain'\ndata.loc[data['chest_pain_type'] == 3, 'chest_pain_type'] = 'typical angina'\n#restecg - rest_ecg_type\ndata.loc[data['rest_ecg_type'] == 0, 'rest_ecg_type'] = 'left ventricular hypertrophy'\ndata.loc[data['rest_ecg_type'] == 1, 'rest_ecg_type'] = 'normal'\ndata.loc[data['rest_ecg_type'] == 2, 'rest_ecg_type'] = 'ST-T wave abnormality'\n#slope - st_slope_type\ndata.loc[data['st_slope_type'] == 0, 'st_slope_type'] = 'downsloping'\ndata.loc[data['st_slope_type'] == 1, 'st_slope_type'] = 'flat'\ndata.loc[data['st_slope_type'] == 2, 'st_slope_type'] = 'upsloping'\n#thal - thalassemia_type\ndata.loc[data['thalassemia_type'] == 0, 'thalassemia_type'] = 'nothing'\ndata.loc[data['thalassemia_type'] == 1, 'thalassemia_type'] = 'fixed defect'\ndata.loc[data['thalassemia_type'] == 2, 'thalassemia_type'] = 'normal'\ndata.loc[data['thalassemia_type'] == 3, 'thalassemia_type'] = 'reversable defect'","6d96f37d":"#One Hot Encoding\ndummy = pd.get_dummies(data, drop_first=False)\ndummy.columns","77c54305":"data_temp = dummy['thalassemia_type_fixed defect']\ndummy = pd.get_dummies(data, drop_first=True)\ndummy.head()","9111ae8c":"frames = [dummy, data_temp]\nresult = pd.concat(frames,axis=1)\nresult.drop('thalassemia_type_nothing',axis=1,inplace=True)\nresultc = result.copy()# making a copy for further analysis","9d30314b":"#Gather columns\nX = result.drop('target_No Heart Disease', axis = 1)\ny = result['target_No Heart Disease']","7d24c014":"#Splitting Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","e2231865":"#Normalization\nX_train=(X_train-np.min(X_train))\/(np.max(X_train)-np.min(X_train)).values\nX_test=(X_test-np.min(X_test))\/(np.max(X_test)-np.min(X_test)).values","5a2e67bb":"#Fitting into Model\nfrom sklearn.linear_model import LogisticRegression\nlogre = LogisticRegression()\nlogre.fit(X_train,y_train)","50586e9e":"#Prediction\ny_pred = logre.predict(X_test)\nactual = []\npredcition = []\nfor i,j in zip(y_test,y_pred):\n  actual.append(i)\n  predcition.append(j)\ndic = {'Actual':actual,\n       'Prediction':predcition\n       }\nresult  = pd.DataFrame(dic)\nimport plotly.graph_objects as go\n \nfig = go.Figure()\n \n \nfig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_test,\n                    mode='markers+lines',\n                    name='Test'))\nfig.add_trace(go.Scatter(x=np.arange(0,len(y_test)), y=y_pred,\n                    mode='markers',name='Pred'))","41d66da1":"#Model Evaluation\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_pred))","e5788e51":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","c72cb2e4":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True)","f784f112":"#ROC Score\nimport sklearn\nsklearn.metrics.roc_auc_score(y_test,y_pred)","034f707f":"final_data = {'Actual Value':y_test, 'Predicted Value':y_pred}\nsubmission = pd.DataFrame(data=final_data)","50f6168e":"submission.to_csv('submission_lr.csv', index =False)","0a9a7723":"# **Logistic Regression**  \n\n1. Gather columns\n2. Splitting Data  \n3. Normalization  \n4. Fitting into Model  \n5. Prediction  \n6. Model Evaluation","2e6135b2":"# **Classification Tree**","a17577eb":"# **Data Visualization**\n\nSeaborn","22a0049e":"**2.** **Feature Importance** \u2014 You can gain the significance of each feature of your dataset by using the Model Characteristics property.  \nFeature value gives you a score for every function of your results, the higher the score the more significant or appropriate the performance variable is.  \nWe will use the Extra Tree Classifier to extract the top features for the dataset.","3b61f9b3":"# **Data Pre-processing**  \n\nWe have 4 Categorical columns as seen in Data Description using pandas profiling:  \n1. cp \u2014 chest_pain_type  \n2. restecg \u2014 rest_ecg_type  \n3. slope \u2014 st_slope_type  \n4. thal \u2014 thalassemia_type  ","7ba0a427":"**3. Correlation Matrix with Heatmap** \u2014 Correlation indicates how the features are related to each other or to the target variable.  \nThe correlation may be positive (increase in one value of the feature increases the value of the target variable) or negative (increase in one value of the feature decreases the value of the target variable)  \nHeatmap makes it easy to classify the features are most relevant to the target variable, and we will plot the associated features of the heatmap using the seaborn library.","7baeaa14":"# **Data Analysis**\n\n**Feature Selection**\n\n1. Univariate Selction \u2014 Statistical tests may be used to pick certain features that have the best relationship to the performance variable. The scikit-learn library provides the SelectKBest class that can be used to select a specific number of features in a suite of different statistical tests.","a2122809":"# **Data Description and information**"}}