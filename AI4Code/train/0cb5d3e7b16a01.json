{"cell_type":{"22c70599":"code","6157bbd2":"code","0e4b3917":"code","e5a6aab6":"code","2b83a68c":"code","1e53287a":"code","37c5846b":"code","6d1b0fdd":"code","30febe94":"code","d8fe85a6":"code","23fae1e5":"code","5916d9ad":"code","ddd2293a":"code","5aba310c":"code","4ef9386c":"code","fe0c53b8":"code","9b99a810":"code","26f067ff":"code","b868e23c":"code","18668689":"code","0d6da48e":"code","7f54b42c":"code","1597be77":"code","672dabbb":"code","94f77bb4":"code","1c35ac67":"code","e76f6362":"code","96b5f4ef":"code","5a13de86":"code","c28ccdbc":"code","f8390175":"code","ef8c3512":"code","3b5130bd":"markdown"},"source":{"22c70599":"from __future__ import print_function\n#%matplotlib inline\nimport argparse\nimport os\nimport PIL\nimport glob\nimport xml.etree.ElementTree as ET\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport seaborn as sns\nfrom IPython.display import HTML\nfrom torchvision.utils import save_image\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom tqdm import tqdm_notebook as tqdm\nfrom IPython.display import clear_output\nfrom scipy.stats import truncnorm\n%matplotlib inline\nplt.rcParams['image.interpolation'] = 'nearest'","6157bbd2":"print(os.listdir(\"..\/input\/\"))","0e4b3917":"print(os.listdir(\"..\/input\/annotation\/\"))\nprint(os.listdir(\"..\/input\/all-dogs\/\"))","e5a6aab6":"anno_dir = \"..\/input\/annotation\/Annotation\/\"\ndata_dir = \"..\/input\/all-dogs\/all-dogs\/\"","2b83a68c":"# create a folder for saving extracted dogs' images\nclassed_input = \"..\/classed_input\/\"\nos.makedirs(classed_input, exist_ok=True)\nprint(\"Folder: \" + classed_input + \" created\")","1e53287a":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()","37c5846b":"# visualization random breeds\nbreeds = os.listdir(anno_dir)\nfig, axes = plt.subplots(nrows=5, ncols=5, figsize=(15,15))\nfor indx, axis in enumerate(axes.flatten()):\n    breed = np.random.choice(breeds)\n    dog = np.random.choice(os.listdir(anno_dir + breed))\n    img = PIL.Image.open(data_dir + dog + '.jpg') \n    tree = ET.parse(anno_dir + breed + '\/' + dog)\n    root = tree.getroot()\n    objects = root.findall('object')\n    axis.set_axis_off() \n    imgplot = axis.imshow(img)\n    for o in objects:\n        bndbox = o.find('bndbox') # reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        axis.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin]) # show box\n        axis.text(xmin, ymin, o.find('name').text, bbox={'ec': None}) # show breed\n\nplt.tight_layout(pad=0, w_pad=0, h_pad=0)","6d1b0fdd":"# visualization a breed in random\nfig, axes = plt.subplots(nrows=5, ncols=5, figsize=(15,15))\nbreed = np.random.choice(breeds)  \nfor indx, axis in enumerate(axes.flatten()):\n    dog = np.random.choice(os.listdir(anno_dir + breed)) \n    img = PIL.Image.open(data_dir + dog + '.jpg') \n    tree = ET.parse(anno_dir + breed + '\/' + dog)\n    root = tree.getroot()\n    objects = root.findall('object')\n    axis.set_axis_off() \n    imgplot = axis.imshow(img)\n    for o in objects:\n        bndbox = o.find('bndbox')\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        axis.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin]) # show box\n        axis.text(xmin, ymin, o.find('name').text, bbox={'ec': None}) # show breed\n\nplt.tight_layout(pad=0, w_pad=0, h_pad=0)","30febe94":"def get_all_objects(file_path):\n    bbxs = []\n    root = ET.parse(file_path).getroot()\n    for obj in root.findall(\"object\"):\n        bndbox = obj.find(\"bndbox\")\n        bbxs.append([int(it.text) for it in bndbox])\n    return bbxs\n\ndef get_subdir(dir):\n    return sorted([name for name in os.listdir(dir) if os.path.isdir(os.path.join(dir, name))])\n\ndef get_files(parent_dir, sub_dir):\n    return os.listdir(os.path.join(parent_dir, sub_dir))\n\nanno_folder = get_subdir(anno_dir)\nprint(\"n_folder(n_class): {}\".format(len(anno_folder)))","d8fe85a6":"exceptions = {}\n\nfor subdir in tqdm(anno_folder):\n    # print(\"Processing Directory: \" + subdir)\n    new_folder = os.path.join(classed_input, subdir)\n    if not os.path.exists(new_folder):\n        os.makedirs(new_folder)\n    \n    files = get_files(anno_dir, subdir)\n    for f in files:\n        basename = os.path.splitext(f)[0] \n        try:\n            objects = get_all_objects(os.path.join(anno_dir, subdir, basename))\n            for i, obj in enumerate(objects):\n                xmin, ymin, xmax, ymax = obj\n                image = PIL.Image.open(data_dir + basename + \".jpg\")\n                save_path = os.path.join(classed_input, subdir, \"cropped_\" + basename + str(i) + \".jpg\")\n                cropped = image.crop((xmin, ymin, xmax, ymax)).save(save_path, \"JPEG\")\n                \n        except Exception as e:\n            exceptions[str(e)] = os.path.join(anno_dir, subdir, basename)","23fae1e5":"exceptions\n# This image: n02105855_2933.jpg in \"..\/input\/all-dogs\/all-dogs\/\" is missed. \n# But the corresponding annotation exist.","5916d9ad":"classed_dir = get_subdir(classed_input)\nprint(len(classed_dir)) # Good: equal to the number of annoattions folders","ddd2293a":"lengths = []\nfor subdir in classed_dir:\n    lengths.append(len(os.listdir(os.path.join(classed_input, subdir))))\n    \nbreeds = [classed_dir[i].split(\"-\")[1] for i in range(len(classed_dir))] \nfig, ax = plt.subplots(figsize=(15,20))\ny_pos = np.arange(len(breeds))\nax.barh(y_pos, lengths, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(breeds)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Number of Images')\nax.set_title(\"Number of Dogs' Images  by breed\")\nplt.tight_layout()\nplt.show()","5aba310c":"# visualization all dogs\nfig = plt.figure(figsize=(20,40))\nfor i, dir in enumerate(classed_dir):\n    ax = fig.add_subplot(20,6,i+1)\n    imgs = os.listdir(os.path.join(classed_input, dir))\n    # print(dir)\n    img = PIL.Image.open(classed_input + \"\/\" + dir + \"\/\" + imgs[0])\n    ax.axis('off')\n    ax.set_title(breeds[i])\n    ax.imshow(img, cmap=\"gray\")\nplt.tight_layout()\nplt.show()    ","4ef9386c":"# Setting parameters\ndataroot = \"..\/classed_input\/\"\nworkers = 4\nbatch_size = 32\nimage_size = 64\nnc = 3\nnz = 128\nngf = 64\nndf = 64\nnum_epochs = 300\nlr = 0.001\nbeta1 = 0.5\nngpu = 1\nnum_show = 6\nn_class = 120\noutf = '..\/output_result'\nif not os.path.exists(outf):\n    os.mkdir(outf)","fe0c53b8":"dataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.RandomHorizontalFlip(),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)","9b99a810":"# visualization batch image\nreal_batch = iter(dataloader).next()\nplt.figure(figsize=(15,15))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nimage = np.transpose(vutils.make_grid(real_batch[0].to(device), normalize=True).cpu(),axes=(1,2,0))\nplt.imshow(image)","26f067ff":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","b868e23c":"class Generator(nn.Module):\n\n    def __init__(self, ngpu, nz=nz, ngf=ngf, nc=nc, n_class=n_class):\n\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.ReLU = nn.ReLU(True)\n        self.Tanh = nn.Tanh()\n        self.conv1 = nn.ConvTranspose2d(nz+n_class, ngf * 8, 4, 1, 0, bias=False)\n        self.BatchNorm1 = nn.BatchNorm2d(ngf * 8)\n\n        self.conv2 = nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n        self.BatchNorm2 = nn.BatchNorm2d(ngf * 4)\n\n        self.conv3 = nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n        self.BatchNorm3 = nn.BatchNorm2d(ngf * 2)\n\n        self.conv4 = nn.ConvTranspose2d(ngf * 2, ngf * 1, 4, 2, 1, bias=False)\n        self.BatchNorm4 = nn.BatchNorm2d(ngf * 1)\n\n        self.conv5 = nn.ConvTranspose2d(ngf * 1, nc, 4, 2, 1, bias=False)\n\n        self.apply(weights_init)\n\n\n    def forward(self, input):\n\n        x = self.conv1(input)\n        x = self.BatchNorm1(x)\n        x = self.ReLU(x)\n\n        x = self.conv2(x)\n        x = self.BatchNorm2(x)\n        x = self.ReLU(x)\n\n        x = self.conv3(x)\n        x = self.BatchNorm3(x)\n        x = self.ReLU(x)\n\n        x = self.conv4(x)\n        x = self.BatchNorm4(x)\n        x = self.ReLU(x)\n\n        x = self.conv5(x)\n        output = self.Tanh(x)\n        return output\nnetG = Generator(ngpu).to(device)\nnetG.apply(weights_init)\nprint(netG)","18668689":"class Discriminator(nn.Module):\n\n    def __init__(self, ngpu, ndf=ndf, nc=nc, n_class=n_class):\n\n        super(Discriminator, self).__init__()\n        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\n        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n        self.BatchNorm2 = nn.BatchNorm2d(ndf * 2)\n        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n        self.BatchNorm3 = nn.BatchNorm2d(ndf * 4)\n        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n        self.BatchNorm4 = nn.BatchNorm2d(ndf * 8)\n        self.conv5 = nn.Conv2d(ndf * 8, ndf * 1, 4, 1, 0, bias=False)\n        self.disc_linear = nn.Linear(ndf * 1, 1)\n        self.aux_linear = nn.Linear(ndf * 1, n_class)\n        self.softmax = nn.Softmax()\n        self.sigmoid = nn.Sigmoid()\n        self.ndf = ndf\n        self.apply(weights_init)\n\n    def forward(self, input):\n\n        x = self.conv1(input)\n        x = self.LeakyReLU(x)\n\n        x = self.conv2(x)\n        x = self.BatchNorm2(x)\n        x = self.LeakyReLU(x)\n\n        x = self.conv3(x)\n        x = self.BatchNorm3(x)\n        x = self.LeakyReLU(x)\n\n        x = self.conv4(x)\n        x = self.BatchNorm4(x)\n        x = self.LeakyReLU(x)\n\n        x = self.conv5(x)\n        x = x.view(-1, self.ndf * 1)\n        c = self.aux_linear(x)\n        c = self.softmax(c)\n        s = self.disc_linear(x)\n        s = self.sigmoid(s)\n        return s,c\n\nnetD = Discriminator(ngpu).to(device)\nnetD.apply(weights_init)\nprint(netD)","0d6da48e":"# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","7f54b42c":"def truncated_normal(size, threshold=1):\n    values = truncnorm.rvs(-threshold, threshold, size=size)\n    return values\n\ndef show_generated_img_all():\n    dog_label = torch.randint(n_class, (64,), dtype=torch.long, device=device)\n    noise = torch.randn(64, nz, 1, 1, device=device)\n    gen_image = concat_noise_label(noise, dog_label, device)  \n    gen_image = netG(gen_image).to(\"cpu\").clone().detach().squeeze(0)\n    gen_image = gen_image.numpy().transpose(0, 2, 3, 1)\n    # gen_image = gen_image.numpy().transpose(1, 2, 0)\n    gen_image = (gen_image + 1.0) \/ 2.0\n    \n    fig = plt.figure(figsize=(25, 16))\n    for ii, img in enumerate(gen_image):\n        ax = fig.add_subplot(8, 8, ii + 1, xticks=[], yticks=[])\n        plt.imshow(img)\n        \n\ndef show_generated_img(num_show):\n    gen_images = []\n    for _ in range(num_show):\n        noise = torch.randn(1, nz, 1, 1, device=device)\n        dog_label = torch.randint(0, n_class, (1, ), device=device)\n        gen_image = concat_noise_label(noise, dog_label, device)\n        gen_image = netG(gen_image).to(\"cpu\").clone().detach().squeeze(0)\n        # gen_image = gen_image.numpy().transpose(0, 2, 3, 1)\n        gen_image = gen_image.numpy().transpose(1, 2, 0)\n        gen_images.append(gen_image)\n        \n    fig = plt.figure(figsize=(10, 5))\n    for i, gen_image in enumerate(gen_images):\n        ax = fig.add_subplot(1, num_show, i + 1, xticks=[], yticks=[])\n        plt.imshow(gen_image + 1 \/ 2)\n    plt.show()\n    \ndef show_loss(ylim): \n    sns.set_style(\"white\")\n    fig = plt.figure(figsize=(10,5))\n    ax = fig.add_subplot(1,1,1)\n    ax.set_title(\"Generator and Discriminator Loss During Training\")\n    ax.plot(G_losses,label=\"G\",c=\"b\")\n    ax.plot(D_losses,label=\"D\",c=\"r\")\n    ax.set_xlabel(\"iterations\")\n    ax.set_ylabel(\"Loss\")\n    ax.legend()\n    if ylim == True:\n        ax.set_ylim(0,4)","1597be77":"def mse(imageA, imageB):\n    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n    err \/= float(imageA.shape[0] * imageA.shape[1])\n    return err\n\n\ndef analyse_generated_by_class(n_images):\n    good_breeds = []\n    for l in range(n_class):\n        sample = []\n        for _ in range(n_images):\n            noise = torch.randn(1, nz, 1, 1, device=device)\n            dog_label = l\n            noise_label = concat_noise_label(noise, dog_label, device)\n            gen_image = netG(noise_label).to(\"cpu\").clone().detach().squeeze(0)\n            gen_image = gen_image.numpy().transpose(1, 2, 0)\n            sample.append(gen_image)\n\n        d = np.round(np.sum([mse(sample[k], sample[k + 1]) for k in range(len(sample) - 1)]) \/ n_images, 1,)\n        if d < 1.0:\n            continue  # had mode colapse(discard)\n            \n        print(f\"Generated breed({d}): \", breeds[l])    \n        good_breeds.append(l)\n    return good_breeds\n\n\ndef create_submit(good_breeds):\n    print(\"Creating submit\")\n    os.makedirs(\"..\/output_images\", exist_ok=True)\n    im_batch_size = 100\n    n_images = 10000\n\n    for i_batch in range(0, n_images, im_batch_size):\n        z = truncated_normal((im_batch_size, nz, 1, 1), threshold=1)\n        noise = torch.from_numpy(z).float().to(device)\n        \n        dog_label = np.random.choice(good_breeds, size=im_batch_size, replace=True) \n        dog_label = torch.from_numpy(dog_label).to(device).clone().detach().squeeze(0)\n        noise_label = concat_noise_label(noise, dog_label, device)\n    \n        gen_images = (netG(noise_label) + 1) \/ 2\n        \n        for i_image in range(gen_images.size(0)):\n            save_image(gen_images[i_image, :, :, :], os.path.join('..\/output_images', f'image_{i_batch+i_image:05d}.png'))\n\n    import shutil\n    shutil.make_archive(\"images\", \"zip\", \"..\/output_images\")","672dabbb":"def onehot_encode(label, device, n_class=n_class):  \n    eye = torch.eye(n_class, device=device) \n    return eye[label].view(-1, n_class, 1, 1)   \n \ndef concat_image_label(image, label, device, n_class=n_class):\n    B, C, H, W = image.shape   \n    oh_label = onehot_encode(label, device=device)\n    oh_label = oh_label.expand(B, n_class, H, W)\n    return torch.cat((image, oh_label), dim=1)\n \ndef concat_noise_label(noise, label, device):\n    oh_label = onehot_encode(label, device=device)\n    return torch.cat((noise, oh_label), dim=1)","94f77bb4":"# Loss functions\ns_criterion = nn.BCELoss()\nc_criterion = nn.NLLLoss()\n\nr_label = 0.7\nf_label = 0\n\ninput = torch.tensor([batch_size, nc, image_size, image_size], device=device)\nnoise = torch.tensor([batch_size, nz, 1, 1], device=device)\n\nfixed_noise = torch.randn(1, nz, 1, 1, device=device)\nfixed_label = torch.randint(0, n_class, (1, ), device=device)\nfixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device)","1c35ac67":"# Training Loop\n\n# Lists to keep track of progress\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\nfor epoch in range(num_epochs):\n    for i, data in enumerate(tqdm(dataloader)):\n        # prepare real image and label\n        real_label = data[1].to(device)\n        real_image = data[0].to(device)\n        b_size = real_label.size(0)      \n        \n        # prepare fake image and label\n        fake_label = torch.randint(n_class, (b_size,), dtype=torch.long, device=device)\n        noise = torch.randn(b_size, nz, 1, 1, device=device).squeeze(0)\n        noise = concat_noise_label(noise, real_label, device)  \n        fake_image = netG(noise)\n        \n        # target\n        real_target = torch.full((b_size,), r_label, device=device)\n        fake_target = torch.full((b_size,), f_label, device=device)\n        \n        #-----------------------\n        # Update Discriminator\n        #-----------------------\n        netD.zero_grad()\n        \n        # train with real\n        s_output, c_output = netD(real_image)\n        s_errD_real = s_criterion(s_output, real_target)  # realfake\n        c_errD_real = c_criterion(c_output, real_label)  # class\n        errD_real = s_errD_real + c_errD_real\n        errD_real.backward()\n        D_x = s_output.data.mean()\n\n        # train with fake\n        s_output,c_output = netD(fake_image.detach())\n        s_errD_fake = s_criterion(s_output, fake_target)  # realfake\n        c_errD_fake = c_criterion(c_output, real_label)  # class\n        errD_fake = s_errD_fake + c_errD_fake\n        errD_fake.backward()\n        D_G_z1 = s_output.data.mean()\n        \n        errD = s_errD_real + s_errD_fake\n        optimizerD.step()        \n\n        #-----------------------\n        # Update Generator\n        #-----------------------\n        netG.zero_grad()\n        \n        s_output,c_output = netD(fake_image)\n        s_errG = s_criterion(s_output, real_target)  # realfake\n        c_errG = c_criterion(c_output, real_label)  # class\n        errG = s_errG + c_errG\n        errG.backward()\n        D_G_z2 = s_output.data.mean()\n        \n        optimizerG.step()\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        \n        iters += 1\n\n    # scheduler.step(errD.item())\n    \n    print('[%d\/%d][%d\/%d]\\nLoss_D: %.4f\\tLoss_G: %.4f\\nD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n          % (epoch+1, num_epochs, i+1, len(dataloader),\n             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n    \n    show_generated_img(num_show)\n    \n#     # --------- save fake image  ----------\n#     fake_image = netG(fixed_noise_label)   \n#     vutils.save_image(fake_image.detach(), '{}\/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n#                     normalize=True, nrow=5)\n \n#     # ---------  save model  ----------\n#     if (epoch + 1) % 10 == 0:  \n#         torch.save(netG.state_dict(), '{}\/netG_epoch_{}.pth'.format(outf, epoch + 1))\n#         torch.save(netD.state_dict(), '{}\/netD_epoch_{}.pth'.format(outf, epoch + 1))","e76f6362":"show_loss(ylim=False)","96b5f4ef":"show_loss(ylim=True)","5a13de86":"good_breeds = analyse_generated_by_class(6)\ncreate_submit(good_breeds)","c28ccdbc":"len(good_breeds)","f8390175":"show_generated_img_all()","ef8c3512":"# visualization generate image of all breeds \nfig = plt.figure(figsize=(20,40))\nfor i in range(n_class):\n    ax = fig.add_subplot(20,6,i+1)\n    noise = torch.randn(1, nz, 1, 1, device=device)\n    dog_label = i\n    noise_label = concat_noise_label(noise, dog_label, device)\n    gen_image = netG(noise_label).to(\"cpu\").clone().detach().squeeze(0)\n    # gen_image = gen_image.numpy().transpose(0, 2, 3, 1)\n    gen_image = gen_image.numpy().transpose(1, 2, 0)\n    gen_image = (gen_image + 1.0) \/ 2.0\n    ax.axis('off')\n    ax.set_title(breeds[i])\n    ax.imshow(gen_image, cmap=\"gray\")\nplt.tight_layout()\nplt.show()    ","3b5130bd":"### Reference\n* https:\/\/arxiv.org\/abs\/1610.09585  \n* https:\/\/github.com\/eriklindernoren\/PyTorch-GAN\/blob\/master\/implementations\/acgan\/acgan.py  \n* https:\/\/github.com\/gitlimlab\/ACGAN-PyTorch\n* https:\/\/github.com\/kimhc6028\/acgan-pytorch  \n* https:\/\/www.kaggle.com\/paulorzp\/show-annotations-and-breeds\/notebook  \n* https:\/\/www.kaggle.com\/mpalermo\/pytorch-rals-c-sagan"}}