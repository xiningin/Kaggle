{"cell_type":{"57739bcf":"code","955e087b":"code","08208b55":"code","0dabc64e":"code","368a4637":"code","6f299497":"code","dbf33169":"code","c3dd1ce9":"code","5e204d66":"code","51290536":"code","9c0b5b12":"code","869d33ed":"code","8b419855":"code","f76081ec":"code","68131436":"code","55c6d766":"code","fd6254cc":"code","67d7d709":"code","517bccd3":"code","430b5e22":"code","9b2221bb":"code","1e635bca":"code","269a2231":"code","db989a0b":"code","a6a219b6":"code","90bb7592":"code","aaba8684":"code","eba8fbf9":"code","68118e5f":"code","ccf97d9c":"code","47db4bfb":"markdown","54d206b4":"markdown","dc0c99dc":"markdown","6911b5d3":"markdown","c0da3dad":"markdown","29f1f03e":"markdown","c1751d9d":"markdown","64a3d3a5":"markdown","3b5878b3":"markdown"},"source":{"57739bcf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\nfrom catboost import CatBoostClassifier\nimport os\nfrom catboost import cv\nimport shap\n# load JS visualization code to notebook\nshap.initjs()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","955e087b":"submission = pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")\ntrain = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")","08208b55":"##Data Prepare \n\n# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n\n# Maping the category values in our dict\ntrain['bin_3'] = train['bin_3'].map(bin_dict)\ntrain['bin_4'] = train['bin_4'].map(bin_dict)\ntest['bin_3'] = test['bin_3'].map(bin_dict)\ntest['bin_4'] = test['bin_4'].map(bin_dict)\n\n\n\ndummies = pd.concat([train, test], axis=0, sort=False)\nprint(f'Shape before dummy transformation: {dummies.shape}')\ndummies = pd.get_dummies(dummies, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {dummies.shape}')\ntrain, test = dummies.iloc[:train.shape[0], :], dummies.iloc[train.shape[0]:, :]\ndel dummies\n#print(train.shape)\n#print(test.shape)\n\n\n# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\ndef transformingOrdinalFeatures(df, ord_1, ord_2, ord_3, ord_4):    \n    df.ord_1 = df.ord_1.astype(ord_1)\n    df.ord_2 = df.ord_2.astype(ord_2)\n    df.ord_3 = df.ord_3.astype(ord_3)\n    df.ord_4 = df.ord_4.astype(ord_4)\n\n    # Geting the codes of ordinal categoy's \n    df.ord_1 = df.ord_1.cat.codes\n    df.ord_2 = df.ord_2.cat.codes\n    df.ord_3 = df.ord_3.cat.codes\n    df.ord_4 = df.ord_4.cat.codes\n    \n    return df\ntrain = transformingOrdinalFeatures(train, ord_1, ord_2, ord_3, ord_4)\ntest = transformingOrdinalFeatures(test, ord_1, ord_2, ord_3, ord_4)\n\n#print(train.shape)\n#print(test.shape)\n\ndef date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df\n\ntrain = date_cyc_enc(train, 'day', 7)\ntest = date_cyc_enc(test, 'day', 7) \n\ntrain = date_cyc_enc(train, 'month', 12)\ntest = date_cyc_enc(test, 'month', 12)\n\n#print(train.shape)\n#print(test.shape)\n\n\nimport string\n\n# Then encode 'ord_5' using ACSII values\n\n# Option 1: Add up the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# Option 2: Join the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\ntest['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\nfor col in ['ord_5_oe1', 'ord_5_oe2', 'ord_5_oe_add', 'ord_5_oe_join']:\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')\n\n#print(train.shape)\n#print(test.shape)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nhigh_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in high_card_feats:    \n    train[f'hash_{col}'] = train[col].apply( lambda x: hash(str(x)) % 5000 )\n    test[f'hash_{col}'] = test[col].apply( lambda x: hash(str(x)) % 5000 )\n                                               \n    if train[col].dtype == 'object' or test[col].dtype == 'object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        train[f'le_{col}'] = lbl.transform(list(train[col].values))\n        test[f'le_{col}'] = lbl.transform(list(test[col].values))\n\n#print(train.shape)\n#print(test.shape)\n\n\ncol = high_card_feats + ['ord_5','day', 'month']\n                    # + ['hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9']\n                    # + ['le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9']\ntrain.drop(col, axis=1, inplace=True)\ntest.drop(col, axis=1, inplace=True)\n\n#print(train.shape)\n#print(test.shape)\n","0dabc64e":"train['target'].value_counts()","368a4637":"train.head(2)","6f299497":"X_train = train.drop('target', axis=1).values\ny_train = train['target'].values\nX_test = test.values\n\nX_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, \n                                                                test_size=0.4, \n                                                                random_state=17,stratify=y_train)\n\nSEED = 17\n\nmodel = CatBoostClassifier (iterations=500,learning_rate=0.5, custom_loss=['AUC','Accuracy'],\n                            early_stopping_rounds=100,eval_metric='AUC')\n\nmodel.fit(X_train_part, y_train_part,\n        eval_set=(X_valid, y_valid),\n        use_best_model=False,\n        verbose=20,\n        plot=True,early_stopping_rounds=20);","dbf33169":"print(\"Tree Count wiht early stopping \", (model.tree_count_))\n","c3dd1ce9":"train_pool = Pool(data=X_train,label=y_train)\nvalidation_pool = Pool(data=X_valid, label=y_valid)\n\nparams = {'loss_function' : 'Logloss',\n         'iterations': 100,\n         'custom_loss': ['AUC','Accuracy'],\n         'learning_rate': 0.5,\n         }\n\n\ncv_data = cv(params= params,\n             pool= train_pool,\n             shuffle=True,\n             fold_count=5,\n             partition_random_seed=0,\n             verbose=False,\n             plot=True,)","5e204d66":"cv_data.head(10)","51290536":"best_value = np.min(cv_data['test-Logloss-mean'])\nbest_iter = np.argmin(cv_data['test-Logloss-mean'])\n\nprint('Best validation score without stratified: {:.4f} {:.4f} on step {} '\n      .format(best_value, cv_data['test-Logloss-std'][best_iter],best_iter))","9c0b5b12":"cv_data = cv(params= params,\n             pool= train_pool,\n             shuffle=True,\n             fold_count=5,\n             stratified=True,\n             partition_random_seed=0,\n             verbose=False,\n             plot=True)","869d33ed":"best_value = np.min(cv_data['test-Logloss-mean'])\nbest_iter = np.argmin(cv_data['test-Logloss-mean'])\n\nprint('Best validation score with stratified: {:.4f} {:.4f} on step {} '\n      .format(best_value, cv_data['test-Logloss-std'][best_iter],best_iter)) ","8b419855":"'''from sklearn.model_selection import GridSearchCV\n\n\n#param_grid = {\"learning_rate\": [0.001,0.01,0.5],\"depth\":[3,1,2,6,4,5,7,8,9,10]}\n\nparam_grid = {\"depth\":[3,1,2,6,4,5,7,8,9,10],\n          \"iterations\":[50],\n          \"learning_rate\":[0.03,0.001,0.01,0.1,0.2,0.3], \n          \"l2_leaf_reg\":[3,1,5,10,100],\n          \"border_count\":[32,5,10,20,50,100,200],\n          #\"ctr_border_count\":[5,10,20,100,200],\n          \"thread_count\":[4]}\n\n\n\nclf  = CatBoostClassifier(\n            verbose=False,eval_metric='AUC',early_stopping_rounds=100)\n\ngrid_search = GridSearchCV(clf , param_grid=param_grid, cv = 3)\nresults = grid_search.fit(X_train,y_train)\nresults.best_estimator_.get_params()'''","f76081ec":"model_with_early_stop = CatBoostClassifier(eval_metric='AUC',\n                                          iterations=100,learning_rate=0.5,\n                                          early_stopping_rounds=20)\n\nmodel_with_early_stop.fit(train_pool,eval_set = validation_pool,\n                         verbose = False, plot=True)","68131436":"print(model_with_early_stop.tree_count_)","55c6d766":"model = CatBoostClassifier(iterations=200,learning_rate=0.01)\nmodel.fit(train_pool,verbose=50)","fd6254cc":"print(model.predict(X_test))","67d7d709":"#Probality of being 0 or 1\nprint(model.predict_proba(X_test))","517bccd3":"raw_pred = model.predict(\n    X_test, prediction_type='RawFormulaVal'\n)","430b5e22":"print(raw_pred)","9b2221bb":"metrics = model.eval_metrics(data= validation_pool,\n                            metrics = ['Logloss','AUC'],\n                            ntree_start = 0,\n                            ntree_end = 0,\n                            eval_period =1,\n                            plot=True)","1e635bca":"tunned_model = CatBoostClassifier(eval_metric='AUC',\niterations=500,\nlearning_rate=0.03,\ndepth=6,\nl2_leaf_reg=3,\nrandom_strength=1,\nbagging_temperature=1)\n\ntunned_model.fit(\ntrain_pool,verbose=False, eval_set = validation_pool,plot=True)","269a2231":"model_valid_predict = tunned_model.predict_proba(X_valid)[:, 1]","db989a0b":"roc_auc_score(y_valid, model_valid_predict)","a6a219b6":"cb_model = CatBoostClassifier(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='AUC',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20,use_best_model=True)\n","90bb7592":"cb_model.fit(\ntrain_pool,verbose=False, eval_set = validation_pool,plot=True)","aaba8684":"model_valid_predict = cb_model.predict_proba(X_valid)[:, 1]","eba8fbf9":"roc_auc_score(y_valid, model_valid_predict)","68118e5f":"model_test_predict = cb_model.predict(test)","ccf97d9c":"sub = pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")\nsubmission = pd.DataFrame({'id': sub[\"id\"], 'target': model_test_predict})\nsubmission.to_csv('submission_1.csv', index=False)","47db4bfb":"# Objective Function\n### Logloss - Binary traget\n### CrossEntropy - Target probabilities ","54d206b4":"# Overfiiting Detector with eval metric","dc0c99dc":"# Sklearn Grid Search","6911b5d3":"# Metric evaluation on a new dataset","c0da3dad":"# Hyperparameter tunning","29f1f03e":"This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform.This competition mainly deals with encoding","c1751d9d":"# Cross Validation ","64a3d3a5":"# Sample Model Predictions","3b5878b3":"# Stratified = True"}}