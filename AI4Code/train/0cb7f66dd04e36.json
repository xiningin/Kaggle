{"cell_type":{"a165afb3":"code","484dbf71":"code","1bee0d4f":"code","74874239":"code","ebc3a2bc":"code","dd565f37":"code","f4d19bb4":"code","fd7c06b2":"code","084a46e4":"code","3f5effda":"code","cdd5bd48":"code","210cd458":"code","76d49f21":"code","f2ef4ca7":"code","28e990ca":"code","63dff09e":"code","c43e476c":"code","0ceb899e":"code","9b9fd6a6":"code","a5458f6d":"code","ce8d37cf":"code","e98ead74":"code","b5242986":"code","56da3d19":"code","65caa053":"code","3a31975e":"code","83ce9279":"code","bde4e144":"code","d01b951b":"code","fa19ec6f":"code","32488651":"code","97893672":"code","7235aacf":"code","4d0c4253":"markdown","26ea25b3":"markdown","3ed30b2e":"markdown","a2b2f4b8":"markdown","3f053eba":"markdown","9c3c8be7":"markdown","db2c8ed3":"markdown","1effbbca":"markdown","70a97201":"markdown","e1747ed4":"markdown","833a4f65":"markdown","3c079867":"markdown","48ba426f":"markdown","6e455ed3":"markdown","36fb0540":"markdown","298e56e6":"markdown","d682e443":"markdown","1a720ae8":"markdown"},"source":{"a165afb3":"#importing the libraries\nimport pandas as pd # data processing I\\O CSV\nimport numpy as np #linear algebra\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import machine learning models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix , accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","484dbf71":"#import dataset\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\nall_data=[df_train,df_test]","1bee0d4f":"#to show some data \ndf_train.head()","74874239":"#statistical describtion for numerical data\ndf_train.describe()","ebc3a2bc":"#statistical describtion for categorical data\ndf_train.describe(include='O')","dd565f37":"#to show information for data\ndf_train.info()\nprint('='*50)\ndf_test.info()","f4d19bb4":"#in this lesson we can sum number of nulls\ntotal=df_train.isnull().sum().sort_values(ascending=False)\npercent=df_train.isnull().sum()\/df_train.isnull().count().sort_values(ascending=False)\nmissing_data=pd.concat([total,percent],axis=1,keys=['total','percent'])\nmissing_data.head(10)","fd7c06b2":"#we can replace the name with the title i take this idea from 'Manav Sehgal'\nfor dataset in all_data:\n    dataset['Title']=dataset['Name'].str.extract(' ([A-Z a-z]+)\\.')\n#describe the title\npd.crosstab(dataset['Title'],dataset['Sex'])","084a46e4":"list1=['Rev','Dr','Major','Col','Mlle','Lady','Capt','Mme','Jonkheer','Sir','Ms','the Countess','Don','Dona']\nfor dataset in all_data :\n    dataset['Title']=dataset['Title'].replace(list1,'Rare')\n\ndf_train[['Title','Survived']].groupby(['Title'],as_index=False).mean().sort_values(by='Survived',ascending=False)","3f5effda":"g=sns.FacetGrid(df_train,col='Survived')\ng.map(plt.hist,'Title')","cdd5bd48":"for dataset in all_data:\n    dataset['Title']=dataset['Title'].map({'Mrs': 1 ,'Miss': 2 ,'Master': 3 ,'Mr': 4 ,'Rare': 5 })\ndf_train.head(10)","210cd458":"df_test['Fare'].fillna(df_test['Fare'].value_counts().index[0],inplace=True)\ndf_train['Embarked'].fillna(df_train['Embarked'].value_counts().index[0],inplace=True)\n","76d49f21":"df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending=False)","f2ef4ca7":"g=sns.FacetGrid(df_train,col='Survived',row='Sex')\ng.map(plt.hist,'Embarked')","28e990ca":"for dataset in all_data:\n    dataset['Embarked']=dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})","63dff09e":"df_train['split Fare']=pd.qcut(df_train['Fare'],4)\ndf_train[['split Fare','Survived']].groupby(['split Fare'],as_index=False).mean().sort_values(by='split Fare',ascending=True)","c43e476c":"for dataset in all_data:\n    dataset.loc[(dataset['Fare'] <= 7.896) ,'Fare'] =0\n    dataset.loc[(dataset['Fare'] > 7.896) & (dataset['Fare'] <= 14.454) , 'Fare'] =1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31.275) , 'Fare'] =2\n    dataset.loc[(dataset['Fare'] > 31.275) & (dataset['Fare'] <= 512.329), 'Fare'] =3\n    dataset.loc[(dataset['Fare'] > 512.329) , 'Fare'] =4","0ceb899e":"df_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)","9b9fd6a6":"df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","a5458f6d":"g=sns.FacetGrid(df_train,col='Survived',row='Sex')\ng.map(plt.hist,'Age',bins=20)","ce8d37cf":"for dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} )","e98ead74":"mean_ages=np.zeros((2,3))\n\nfor datasets in all_data:\n    for i in range(0,2):\n        for j in range(0,3):\n            Class=datasets[(datasets['Pclass']==j+1) & (datasets['Sex']==i) ]['Age']\n            mean_ages[i,j]=Class.mean()\n    for i in range(0,2):\n        for j in range(0,3):\n            datasets.loc[(datasets['Pclass']==j+1) & (datasets['Sex']==i) & (datasets['Age'].isnull()) , 'Age']=mean_ages[i,j]\n    datasets['Age'] = datasets['Age']\n    \ndf_train['Age'].isnull().sum()\n            \n        \n            ","b5242986":"g=sns.FacetGrid(df_train,col='Pclass',row='Survived')\ng.map(plt.hist,'Age',bins=20)","56da3d19":"df_train['split Age']=pd.cut(df_train['Age'],5)\ndf_train[['split Age','Survived']].groupby(['split Age'],as_index=False).mean().sort_values(by='split Age',ascending=True)","65caa053":"for dataset in all_data:    \n    dataset.loc[ (dataset['Age']) <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ndf_train.head()","3a31975e":"df_train = df_train.drop(['PassengerId','Name','Ticket', 'Cabin','split Fare','split Age'], axis=1)\ndf_test = df_test.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","83ce9279":"#we will show correlation matrix\ncorrmat=df_train.corr()\nf,ax=plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,cmap='RdYlGn',annot=True)","bde4e144":"corrmat['Survived'].sort_values(ascending=False)","d01b951b":"X=df_train.drop('Survived',axis=1)\ny=df_train['Survived']","fa19ec6f":"X_train,X_val,y_train,y_val=train_test_split(X,y,random_state=150,test_size=0.2)\n","32488651":"#applying Support Vector Machines\nsvc = SVC()\nkernel=[ 'linear','poly' ,'rbf', 'sigmoid']\nparam=dict(kernel=kernel)\nSVCModel=GridSearchCV(estimator=svc,param_grid=param,cv=10,n_jobs=-1)\nSVCModel.fit(X_train,y_train)\ny_pred=SVCModel.predict(X_val)\nprint('best parameters : ',SVCModel.best_params_)\ny_pred=SVCModel.predict(X_val)\nprint('SVCModel score train : ',SVCModel.score(X_train,y_train))\nprint('SVCModel score test : ',accuracy_score(y_val,y_pred)) \nprint('confusion matrix : \\n',confusion_matrix(y_val,y_pred))\n\nprint('='*50)\n\n#applying k Nerest Neighbors\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nn_neighbors=[3,5,7,9]\nalgorithm=['auto', 'ball_tree', 'kd_tree', 'brute']\nweights=['uniform','distance']\nparam=dict(n_neighbors=n_neighbors,algorithm=algorithm,weights=weights)\nKNNModel=GridSearchCV(estimator=knn,param_grid=param,cv=10,n_jobs=-1)\nKNNModel.fit(X_train,y_train)\nprint('best parameters : ',KNNModel.best_params_)\ny_pred=KNNModel.predict(X_val)\nprint('KNNModel score train : ',KNNModel.score(X_train,y_train))\nprint('KNNModel score test : ',accuracy_score(y_val,y_pred)) \nprint('confusion matrix : \\n',confusion_matrix(y_val,y_pred))\n\nprint('='*50)\n\n#applying Random Forest\nrandom_forest = RandomForestClassifier(max_depth=10,random_state=100)\nrandom_forest.fit(X_train,y_train)\ny_pred=random_forest.predict(X_val)\nprint('random_forest score train : ',random_forest.score(X_train,y_train))\nprint('random_forest score test : ',accuracy_score(y_val,y_pred)) \nprint('confusion matrix : \\n',confusion_matrix(y_val,y_pred))\n\nprint('='*50)\n\n\n#applying GBoost Classifier\nxgb=XGBClassifier(max_depth=8).fit(X_train,y_train)\ny_pred=xgb.predict(X_val)\nprint('xgb score train : ',xgb.score(X_train,y_train))\nprint('xgb score test : ',accuracy_score(y_val,y_pred)) \nprint('confusion matrix : \\n',confusion_matrix(y_val,y_pred))\n\n","97893672":"X=df_train.drop('Survived',axis=1)\ny=df_train['Survived']\nX_test=df_test","7235aacf":"#applying Support Vector Machines\nsvc = SVC()\nkernel=[ 'linear','poly' ,'rbf', 'sigmoid']\nparam=dict(kernel=kernel)\nSVCModel=GridSearchCV(estimator=svc,param_grid=param,cv=10,n_jobs=-1)\nSVCModel.fit(X,y)\nprint('best parameters : ',SVCModel.best_params_)\ny_pred=SVCModel.predict(X_test)\nprint('SVCModel score train : ',SVCModel.score(X,y))\n\n\nprint('='*50)\n\n#applying k Nerest Neighbors\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nn_neighbors=[3,5,7,9]\nalgorithm=['auto', 'ball_tree', 'kd_tree', 'brute']\nweights=['uniform','distance']\nparam=dict(n_neighbors=n_neighbors,algorithm=algorithm,weights=weights)\nKNNModel=GridSearchCV(estimator=knn,param_grid=param,cv=10,n_jobs=-1)\nKNNModel.fit(X,y)\nprint('best parameters : ',KNNModel.best_params_)\ny_pred=KNNModel.predict(X_test)\nprint('KNNModel score train : ',KNNModel.score(X,y))\n\nprint('='*50)\n\n#applying Random Forest\nrandom_forest = RandomForestClassifier(max_depth=10,random_state=100)\nrandom_forest.fit(X,y)\ny_pred=random_forest.predict(X_test)\nprint('random_forest score train : ',random_forest.score(X,y))\n\n\nprint('='*50)\n\n\n#applying GBoost Classifier\nxgb=XGBClassifier(max_depth=8).fit(X_train,y_train)\ny_pred=xgb.predict(X_test)\nprint('xgb score train : ',xgb.score(X,y))\n\n\n","4d0c4253":"split feature Fare","26ea25b3":"# improt dataset\nOur first step is to extract train and test data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them.","3ed30b2e":"# Analyze by visualizing data\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.","a2b2f4b8":"# feature Name\n","3f053eba":"we can replace categorical varible with number","9c3c8be7":"# split data","db2c8ed3":"converting categorical features","1effbbca":"# model and predict df_train\nNow we are ready to train a model and predict the required solution.","70a97201":"# Converting a categorical feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","e1747ed4":"# Feature Embarked and Fare\nnew we can fill nulls in features with more variable variety","833a4f65":"Let us replace Age with ordinals based on these bands.","3c079867":"# feature Sex","48ba426f":" # model and predict df_train and df_test","6e455ed3":"Let us replace Fare with ordinals based on these bands.","36fb0540":"we can replace many title with Rare","298e56e6":"# feature Age\n","d682e443":"We can convert the categorical titles to ordinal.","1a720ae8":"# data wrangling"}}