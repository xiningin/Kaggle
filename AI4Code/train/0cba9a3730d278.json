{"cell_type":{"7cf86596":"code","dea7e391":"code","cd680790":"code","78143bde":"code","f94728df":"code","5293fcfc":"code","180575a6":"code","4fe90bbb":"code","1832d3c0":"code","1f2e0baf":"code","5eea4a43":"code","2ff04b62":"code","6f0593ef":"code","e147c530":"code","ee16ce2c":"code","172dd150":"code","cba486da":"code","ec63f9a6":"code","6a4a207d":"code","de276592":"code","e7f366c0":"code","282bdb2a":"code","38a07e7d":"code","4bd4698d":"code","88bd9a56":"code","6c71fdff":"code","a9fecf6f":"code","f1711322":"code","8d5b093b":"code","203bc55e":"code","03a4c4ba":"code","d9efa6a7":"code","74eb1567":"code","be24ae22":"code","0ab64792":"code","6256a775":"code","5eefd27a":"code","81ce519d":"code","7e77a40c":"code","d39c312c":"code","b6a59e46":"code","41a1357b":"code","20bf6ef6":"code","f9650274":"code","b3b52057":"code","c81b968c":"code","673715d9":"code","1a1ee36a":"code","2bc8d30f":"code","53b9cfa7":"code","06ce9804":"code","2467df57":"code","f59b530b":"code","e5178b77":"markdown","d8834882":"markdown","af1af001":"markdown","4451e363":"markdown","7fdc8563":"markdown","b19a7652":"markdown","41390728":"markdown","f517398e":"markdown","711fbc4b":"markdown","9d0ea517":"markdown","a8150f61":"markdown","de74f124":"markdown","d34db5e0":"markdown","7a91c50d":"markdown","4aebfb57":"markdown","a1af414e":"markdown","f35f1183":"markdown","9870ee3e":"markdown","a9b47ab2":"markdown","39fc9670":"markdown","dd729474":"markdown","b03da924":"markdown","a5c4bf5d":"markdown","1d2a332c":"markdown","1dd49410":"markdown","9d721257":"markdown","797fd29f":"markdown","7086b46d":"markdown","c663268e":"markdown","214faffb":"markdown","d55260fd":"markdown"},"source":{"7cf86596":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dea7e391":"!pip install contractions","cd680790":"import nltk\nimport re\nimport contractions\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() ","78143bde":"data = pd.read_csv(\"\/kaggle\/input\/dynamically-generated-hate-speech-dataset\/2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv\")\ndata.head()","f94728df":"data.drop(columns = ['Unnamed: 0'], axis = 1 , inplace = True)\ndata.head()","5293fcfc":"data.head()","180575a6":"data.info()","4fe90bbb":"data['id'].dtype","1832d3c0":"for i in data.columns:\n    print(i,\":\",data[str(i)].isnull().sum()\/data.shape[0])","1f2e0baf":"data.dropna(axis = 0,inplace = True)\nfor i in data.columns:\n    print(i,\":\",data[str(i)].isnull().sum()\/data.shape[0])","5eea4a43":"data.groupby('type').count()['id']","2ff04b62":"data['type'].nunique()","6f0593ef":"fig,ax = plt.subplots(ncols = 3, figsize = (20,4) , dpi = 100)\n#plt.tight_layout()\n\ncolors = ['#66c2a5', '#fc8d62' , '#8da0cb' ,'#e78ac3' , '#a6d854' , '#ffd92f','#e5c494']\ndata['type'].value_counts().plot(kind = 'pie',ax = ax[0], labels = data['type'].value_counts().index , colors = colors)\nsns.countplot(x = 'type',data = data , ax = ax[1] , palette = 'Paired')\nsns.countplot(x = 'type' , data = data , hue = 'model_wrong', palette = 'Paired')\n\nfor i in range(3):\n    ax[i].set_ylabel('')\n    ax[i].tick_params(axis='x', labelsize=15 , rotation = 45)\n    ax[i].tick_params(axis='y', labelsize=15)\n\nax[0].set_title('Type Distribution in Data', fontsize=13)\nax[1].set_title('Type Count in Data', fontsize=13)\nax[2].set_title('Model Evaluation on Type', fontsize = 13)\n\nplt.show()\n","e147c530":"plt.figure(figsize=(10,10))\nsns.countplot(x = 'label' , data = data, hue = 'model_wrong' , palette = 'Paired')\nplt.ylabel(\"\")\nplt.tick_params(axis = 'x',labelsize = 15)\nplt.tick_params(axis = 'y',labelsize = 15)\nplt.title(\"Model Evaluation on Label\" , fontsize = 15)\nplt.show()","ee16ce2c":"plt.figure(figsize=(10,10))\nsns.countplot(x = 'label' , data = data, palette = 'Paired')\nplt.ylabel(\"\")\nplt.tick_params(axis = 'x',labelsize = 15)\nplt.tick_params(axis = 'y',labelsize = 15)\nplt.title(\"Label Distribution\" , fontsize = 15)\nplt.show()","172dd150":"def clean_txt(txt):\n        ##html code\n        TAG_RE = re.compile(r'<[^>]+>') \n        txt = TAG_RE.sub('', txt.lower())\n        ##emojis\n        txt=txt.encode(\"ascii\",\"ignore\")\n        txt=txt.decode()\n        ##numbers removing\n        txt=''.join(i for i in txt if not i.isdigit())\n        ##punctuation\n        txt = re.sub(r'[^\\w\\s]', ' ', txt) \n        ##stopwords\n        txt = ' '.join([i for i in txt.split() if not i in STOPWORDS])\n        ##removing certain sized words\n        txt=' '.join([i for i in txt.split() if len(i)>2])\n        ##contractions\n        txt=contractions.fix(txt)\n        ##stemmers\n        ##txt= stemmer.stem(txt)  should stemming be performed or lemmatization and why?\n        ##lemmatizer\n        txt=lemmatizer.lemmatize(txt)\n        return txt","cba486da":"data['Clean Text'] = data['text'].apply(clean_txt)\ndata.head()","ec63f9a6":"data['model_wrong'] = data['model_wrong'].astype(\"string\")\ndata['model_wrong'].dtype","6a4a207d":"vocab = [ ]\nmodel_wrong = []\nlabel = []\nfor _,row in data.iterrows():\n    a = row['Clean Text'].split()\n    if(row['label'] == 'hate'):\n        label+=[0 for i in range(len(a))]\n    else:\n        label+=[1 for i in range(len(a))]\n    if(row['model_wrong'] == 'True'):\n        model_wrong+=[0 for i in range(len(a))]\n    else:\n        model_wrong+=[1 for i in range(len(a))]\n    vocab+=a\n    ","de276592":"vocab_model_relation = pd.DataFrame({'Words': vocab , 'Model Wrong': model_wrong ,'Label': label })\n#vocab_model_relation.drop_duplicates(subset=['Words'],inplace=True)\nvocab_model_relation.head()","e7f366c0":"words = vocab_model_relation[vocab_model_relation['Model Wrong'] == 1]['Words'].value_counts().index\nwords","282bdb2a":"words2 = vocab_model_relation[vocab_model_relation['Model Wrong'] == 0]['Words'].value_counts().index\nwords2","38a07e7d":"common_words = list(set(words)&set(words2))\ncommon_words[:10]","4bd4698d":"words = list(set(words).difference(set(common_words)))\nwords[:10]","88bd9a56":"words2 = list(set(words2).difference(set(common_words)))\nwords2[:10]","6c71fdff":"from wordcloud import WordCloud\ndef wc(data,bgcolor,title):\n    plt.figure(figsize = (13,10))\n    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.title(title , fontsize = 20)\n    plt.imshow(wc)\n    plt.axis('off')\n\nwc(common_words,'black','Common Words')","a9fecf6f":"wc(words,'black','Unique Words For Which Predictions Were Wrong')","f1711322":"wc(words2,'black','Unique Words For Which Model Evaluted True')","8d5b093b":"fig , ax = plt.subplots(ncols = 2,figsize = (20,10) , dpi = 100)\n\nsns.barplot(y = vocab_model_relation[vocab_model_relation['Model Wrong'] == 0]['Words'].value_counts().index[0:20] , x = vocab_model_relation[vocab_model_relation['Model Wrong'] == 0]['Words'].value_counts().values[:20], ax = ax[0] , color = '#97d83e')\nsns.barplot(y = vocab_model_relation[vocab_model_relation['Model Wrong'] == 1]['Words'].value_counts().index[0:20] , x = vocab_model_relation[vocab_model_relation['Model Wrong'] == 1]['Words'].value_counts().values[:20], ax = ax[1] , color = '#e55063')\n\nfor i in range(2):\n    ax[i].tick_params(axis = 'x' , labelsize = 13)\n    ax[i].tick_params(axis = 'y' , labelsize = 13)\n\nax[0].set_title('Model Got Them Right')\nax[1].set_title('Model Got Them Wrong')\n","203bc55e":"fig , ax = plt.subplots(ncols = 2,figsize = (20,10) , dpi = 100)\n\nsns.barplot(y = vocab_model_relation[vocab_model_relation['Label'] == 0]['Words'].value_counts().index[0:20] , x = vocab_model_relation[vocab_model_relation['Label'] == 0]['Words'].value_counts().values[:20], ax = ax[0] , color = '#97d83e')\nsns.barplot(y = vocab_model_relation[vocab_model_relation['Label'] == 1]['Words'].value_counts().index[0:20] , x = vocab_model_relation[vocab_model_relation['Label'] == 1]['Words'].value_counts().values[:20], ax = ax[1] , color = '#e55063')\n\nfor i in range(2):\n    ax[i].tick_params(axis = 'x' , labelsize = 13)\n    ax[i].tick_params(axis = 'y' , labelsize = 13)\n\nax[0].set_title('Top 20 Words Used In Hate Comments')\nax[1].set_title('Top 20 Words Used In Non Hate Comments')\n","03a4c4ba":"data['annotator'].unique()","d9efa6a7":"colors_false = ['grey' for i in data[data['model_wrong'] == 'False']['annotator'].value_counts().index]\ncolors_false[2] = '#dd5a5b'\ncolors_true = ['grey' for i in data[data['model_wrong'] == 'True']['annotator'].value_counts().index]\ncolors_true[1] = '#dd5a5b'","74eb1567":"fig , ax = plt.subplots(ncols = 2,figsize = (20,10) , dpi = 100)\n\nsns.barplot(y = data[data['model_wrong'] == 'False']['annotator'].value_counts().index , x = data[data['model_wrong'] == 'False']['annotator'].value_counts().values, ax = ax[0] , palette = colors_false)\nsns.barplot(y = data[data['model_wrong'] == 'True']['annotator'].value_counts().index, x = data[data['model_wrong'] == 'True']['annotator'].value_counts().values, ax = ax[1] , palette = colors_true)\n\nfor i in range(2):\n    ax[i].tick_params(axis = 'x' , labelsize = 13)\n    ax[i].tick_params(axis = 'y' , labelsize = 13)\n    \nax[0].set_title('Model Got Them Right')\nax[1].set_title('Model Got Them Wrong')\n","be24ae22":"new_data = pd.read_csv(\"\/kaggle\/input\/dynamically-generated-hate-speech-dataset\/2020-12-31-DynamicallyGeneratedHateDataset-targets-v0.1.csv\")","0ab64792":"tags = []\nfor i in range(new_data.shape[0]):\n    try:\n        tags.append(list(new_data.iloc[i,:].index)[list(new_data.iloc[i,:].values).index(1)])\n    except:\n        tags.append('Nothing')\n    \nprint(tags[:2])","6256a775":"m_data = data.merge(pd.DataFrame({'id':new_data['id'],'targets':tags}) , on = 'id' , how='inner')\nm_data.head()","5eefd27a":"m_data['label'].nunique()","81ce519d":"colors = ['grey' for i in range(len(m_data['targets'].value_counts().index))] \ncolors[2] = '#dd5a5b'\ncolors[3] = '#97d83e'","7e77a40c":"plt.figure(figsize=(15,15))\n\nsns.countplot(y=m_data['targets'],order = m_data['targets'].value_counts().index, palette = colors)\nplt.tick_params(axis = 'y' , labelsize = 15)\nplt.tick_params(axis = 'x' , labelsize = 15)\nplt.ylabel('Targets')\nplt.xlabel('')\nplt.title(\"Target Distribution in Dataset\" , fontsize = 20)\nplt.xticks(rotation = 90)","d39c312c":"words_black = [ ]\nlabels = []\nfor _,row in m_data[m_data['targets'] == 'bla'].iterrows():\n    a = row['Clean Text'].split()\n    if(row['label'] == 0):\n        labels+=[0 for i in range(len(a))]\n    else:\n        labels+=[1 for i in range(len(a))]\n    words_black+=a\n\nwords_black = pd.DataFrame({'Word':words_black , 'Label':labels})\nwords_black.head()","b6a59e46":"plt.figure(figsize = (10,10))\nsns.countplot(words_black['Label'] , palette = 'Paired')\nplt.ylabel(\"\")\nplt.legend('Hate')\nplt.title('Total Sentences Labeled Hate and Not Hate' , fontsize = 15)","41a1357b":"wc(words_black['Word'].unique(),'black','Unique Words Found in Sentences Targetting Black People')","20bf6ef6":"words_black[words_black['Label'] == 1]['Word'].value_counts()","f9650274":"plt.figure(figsize = (10,10))\nsns.barplot(y = words_black['Word'].value_counts()[:20].index , x = words_black['Word'].value_counts()[:20].values , color = '#97d83e')\nplt.title('Top 20 Words Appearing In Sentences Targetting The Black Community' , fontsize = 15)\nplt.xticks(rotation = 90)","b3b52057":"words_women = [ ]\nlabels = []\nfor _,row in m_data[m_data['targets'] == 'wom'].iterrows():\n    a = row['Clean Text'].split()\n    if(row['label'] == 0):\n        labels+=[0 for i in range(len(a))]\n    else:\n        labels+=[1 for i in range(len(a))]\n    words_women+=a\n\nwords_women = pd.DataFrame({'Word':words_women , 'Label':labels})\nwords_women.head()","c81b968c":"plt.figure(figsize = (10,10))\nsns.countplot(words_women['Label'] , palette = 'Paired')\nplt.ylabel(\"\")\nplt.legend('Hate')\nplt.title('Total Sentences Labeled Hate and Not Hate' , fontsize = 15)","673715d9":"wc(words_women['Word'].unique() , 'black' , 'Unique Words Found in Sentences Targetting Women')","1a1ee36a":"plt.figure(figsize = (10,10))\nsns.barplot(y = words_women['Word'].value_counts()[:30].index , x = words_women['Word'].value_counts()[:30].values , color = '#97d83e')\nplt.tick_params(axis = 'y', labelsize = 12)\nplt.title('Common Words Found in Sentences Targetting Women' , fontsize = 15)\nplt.xticks(rotation = 90)","2bc8d30f":"label = {'hate':0 , 'nothate':1}\ndata['label'] = data['label'].map(label)\ndata.head()","53b9cfa7":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD,PCA\ndef cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = data[\"text\"].tolist()\nlist_labels = data[\"label\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","06ce9804":"import matplotlib\nimport matplotlib.patches as mpatches\ndef plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Hate')\n            blue_patch = mpatches.Patch(color='blue', label='Not Hate')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(12, 12))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","2467df57":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","f59b530b":"fig = plt.figure(figsize=(12, 12))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","e5178b77":"# **Loading the Dataset**","d8834882":"## **Target Distribution in Dataset**","af1af001":"Thank you for reading ! If you liked what I did , give me an upvote ! Saw something which could have been better or have a suggestion to make it better ? Leave a comment and I'll get back to you ASAP.\n\n# **References**\n\nBelow are some awesome notebooks where I discovered new ways to do EDA for NLP . Do check them out !!\n\n1. https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n2. https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n","4451e363":"## **Exploring The Top Two Values**\n\nI am kinda scared and concerned about the type of results which might show up. ","7fdc8563":"Well, that makes quite the difference.","b19a7652":"Again these words have both , what most of us call 'good' and 'bad' words. The context in which these words are used ends up determining whether the sentences is hateful or not.","41390728":"When we remove the common words used , we get unique words for each category. The ones for which the model was wrong are shown above. \n","f517398e":"# **Checking for NaN values**\n\nI am basically looking for NaN values to see how they might influence the analysis I'll perform.","711fbc4b":"### **Sentences Targetting Black People**","9d0ea517":"Here we cannot actually make out much of a difference. Let's use TFIDF vectorizer for the same and check.","a8150f61":"The values which are nan only make up 35% of the data , hence I am thinking of dropping them for now.","de74f124":"# **Text Properties and Relations with Other Variables**\n\nLet's check the text properties and relations with other variables","d34db5e0":"We can see that the black community and women are the most targeted in the dataset we have.","7a91c50d":"\n\n#  **All About Type**\n\nLet's check out the distribution of types in the dataset.","4aebfb57":"## **Visualizing Embeddings**\n\nNow this visualization might be inaccurate considering I am using Count Vectorizer and TFIDF to make embeddings and not what was probably used in the model. Still let's check it out!","a1af414e":"Okay , this is not something I was expecting. The countplot prior to this would ideally suggest more number of not hate sentences as compared to hate , but here the graph shows a completely different story. \n\nBookmarking this for now and will come back to it later.","f35f1183":"## **Lists of Common Words**\n\nHere we are going to see the twenty most used words , their total number of occurences and how did the model perform when it encountered them.\nWe will also be checking whether our assumption that the common words have an equal influence on both hateful and not hateful sentences is correct or not.","9870ee3e":"# **Annotators and Model Wrong**","a9b47ab2":"# **Dynamically Generated Hate Speech Data : Data Analysis of Performance**\n\n**About The Dataset**\n\nThe Dynamically Generated Hate Speech Dataset is provided in two tables.\n\nThe first table is the dataset of entries, with the entry ID, label, type, annotator ID, status, round, split, round model predictions and whether the model was fooled (model_wrong).\n\nThe second table is the targets of the hate, in a wide format. Because annotators could identify targets inductively, a large number were identified with only or two associated entries, often if they were intersectional characteristics. We combine all identities mentioned in fewer than 15 entries into an 'Other category'. This affects less than 1% of all the hateful entries, whilst reducing the number of target identities to 41. The two tables can be merged on the 'ID' variable.\n\n**Let's dive into notebook without further Ado!**","39fc9670":"We can clearly see from the above graph that the Model gets more sentences right , if the annotator lqlkttromx has assigned the label and it gets them wrong in the case the annotator is elgzzdd8tvb.\n\nHowever implying that this is a causal relationship between annotators and label assignment by model  would be wrong as there is a possiblity of other confounders being present. We would require a way to test the same before inferring that this is indeed a causal relationship. ","dd729474":"# **Model Evaluation on Lables**\n\nLet's see how well the model performed on labels.","b03da924":"## **Sentences Targetting Women**","a5c4bf5d":"# **Exploratory Data Analysis**\n\n\nGiven below are the headings to analysis of certain topics:\n\n**1.  All About Type  : Exploring Type Distribution**\n\n**2.  Model Evaluation on Labels**\n\n**3.  Text Properties and Relations With Other Variables**\n        \n       3.1 Word Clouds and More\n    \n       3.2 Lists of Common Words\n    \n**4.  Annotators and Model Wrong**\n\n**5.  Exploring Targets : Merging Tables**\n        \n       5.1 Target Distribution in Dataset\n       \n       5.2 Exploring Top Two Values\n      \n**6.  Visualizing Embeddings**","1d2a332c":"Well , this wordcloud , is just ugly. But I guess it is merely a reflection of the population of people chosen for this study.","1dd49410":"From the above charts we can see that the top 5 words are same for both cases. While most words are common , if you look carefully , the count of these words is not. The frequency of words in usage is more when the model correctly assigns a label as compared to the ones in which it assigns them wrong.\n\nWe can also clearly see that sentences containing the words everyone, wrong and really are the words which do not make it to the top 20 usage of words in the Model Got them Wrong List.","9d721257":"Here is a true mixture of words.","797fd29f":"## **Word Clouds and More**\n\nWe are looking for possible keywords in our sentences which the model might have associated with a certain label while learning the kind of characterstics of words have, on making embeddings. ","7086b46d":"These words are used in abundance in both hate and non hate sentences. Hence , for now we are assuming that they have an equal influence on both hate and non hate sentences for the model.","c663268e":"While the model assigns correct labels nearly equal number of times in both cases of hate and not hate labels, when it comes to assigning a wrong label , it is more likely that that sentence would be a hate comment. This could possibly be because of the distribution of hate and not hate comments in dataset. Let's check that out.","214faffb":"From the above charts we can conclude the following about type :\n\n1.  None and Not Given are the types which are in maximum quantity. \n2.  Derogation follows them in the third place.\n3.  The Model was able to succesfully identify the label in the cases of sentences with types None and Derogation.\n4.  The Model was succesfully fooled maximum number of times by None and Not Given types. Since Not Given types is a mixture of other types, we cannot really point out the specific characterstic which might have resulted in this. ","d55260fd":"# **Exploring Targets**\n\nWe will now merge the tables and explore the targets."}}