{"cell_type":{"9493ed79":"code","e95f8a95":"code","4411f71e":"code","70fe1ca3":"code","5433b6fc":"code","a1fb2f6b":"code","14a21655":"code","77d163d1":"code","49afebfb":"code","a8831bf4":"code","a9208100":"code","3757309e":"code","f085f667":"code","2f1cb448":"code","91b1f0b0":"code","201675da":"code","9e6cca64":"code","5cd89f39":"code","b0537e07":"code","cd1ea46b":"code","d7ee2575":"code","c9cefe10":"code","0f0dc1dd":"code","54baf0fc":"code","32e83104":"code","6be7ef56":"code","5fd1e401":"code","8593b608":"code","e9dae899":"code","0f1d2a68":"code","727cf3dc":"code","c03ec3e5":"code","7eff37e9":"code","9e8dd772":"markdown","df89df9a":"markdown","09603ad5":"markdown","ebed6286":"markdown","225fa297":"markdown","0ee8b2a7":"markdown","d88d0e03":"markdown","187e51af":"markdown","fac6337c":"markdown","962f0f3c":"markdown","fdca870b":"markdown","b215da8b":"markdown","8e1d1278":"markdown","c87beb62":"markdown","d4be55d7":"markdown","e4d3755b":"markdown","f226ca36":"markdown","54fffc9d":"markdown","32d49177":"markdown","03b3ab8d":"markdown","6e8e9d27":"markdown","e3b1d057":"markdown"},"source":{"9493ed79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob\nimport json\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\nfrom textblob import TextBlob\nfrom tqdm.notebook import tqdm\npd.set_option('display.max_colwidth', -1)\nimport os","e95f8a95":"dir_list = [\n    '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv',\n    '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset',\n    '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license',\n    '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset'\n]\nresults_list = list()\nfor target_dir in dir_list:\n    \n    print(target_dir)\n    \n    for json_fp in tqdm(glob(target_dir + '\/*.json')):\n\n        with open(json_fp) as json_file:\n            target_json = json.load(json_file)\n\n        data_dict = dict()\n        data_dict['doc_id'] = target_json['paper_id']\n        data_dict['title'] = target_json['metadata']['title']\n\n        abstract_section = str()\n        for element in target_json['abstract']:\n            abstract_section += element['text'] + ' '\n        data_dict['abstract'] = abstract_section\n\n        full_text_section = str()\n        for element in target_json['body_text']:\n            full_text_section += element['text'] + ' '\n        data_dict['full_text'] = full_text_section\n        \n        results_list.append(data_dict)\n        \n    \ndf_results = pd.DataFrame(results_list)\ndf_results.head()        ","4411f71e":"df_results.info()","70fe1ca3":"dfdrugs=pd.read_csv('\/kaggle\/input\/drug-data\/drugsComTest_raw.csv')\ndfdrugs.info()","5433b6fc":"freq = pd.DataFrame(' '.join(df_results['full_text']).split(), columns=['drugName']).drop_duplicates()\nfreq.head()","a1fb2f6b":"result = pd.merge(freq, dfdrugs, on=['drugName'])","14a21655":"result.head()","77d163d1":"result.sample(10)","49afebfb":"result.drugName.value_counts()","a8831bf4":"result=result.where(result['rating']>7.0).dropna()","a9208100":"result.where(result['condition'].str.contains('Cough')).dropna().sample(5)\n#result.where(result['condition'].str.contains('Headache')).dropna()\n#result.where(result['condition'].str.contains('Cluster Headaches')).dropna()","3757309e":"articles=df_results['full_text'].values\nfor text in articles:\n    for sentences in text.split('.'):\n        if 'Benzonatate' in sentences:\n            print(sentences)        ","f085f667":"for text in articles:\n    for sentences in text.split('.'):\n        if 'Codeine' in sentences:\n            print(sentences)","2f1cb448":"dfdrugs2=pd.read_csv('..\/input\/usp-drug-classification\/usp_drug_classification.csv')\ndfdrugs2['drugName']=dfdrugs2['drug_example']\ndfdrugs2.head()","91b1f0b0":"result2 = pd.merge(freq, dfdrugs2, on=['drugName'])","201675da":"result2.head()","9e6cca64":"result2['usp_category'].value_counts()[:10]","5cd89f39":"antivirals=list(result2.drugName.where(result2['usp_category']=='Antivirals').dropna().unique())\nantivirals[:5]","b0537e07":"for text in articles:\n    for sentences in text.split('.'):\n        if 'entecavir' in sentences:\n            print(sentences) ","cd1ea46b":"CA=list(result2.drugName.where(result2['usp_category']=='Cardiovascular Agents').dropna().unique())","d7ee2575":"Cardiovascular_Agents =[]\nfor text in articles:\n    for sentences in text.split('.'):\n        if any(word in sentences for word in CA):\n            Cardiovascular_Agents .append(sentences)\n            #print(sentences) ","c9cefe10":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords =stopwords, width=1000, height=500).generate(\"+\".join(Cardiovascular_Agents))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0f0dc1dd":"antivirals_all=[]\nfor text in articles:\n    for sentences in text.split('.'):\n        if any(word in sentences for word in antivirals):\n            antivirals_all.append(sentences)\n            #print(sentences) ","54baf0fc":"wordcloud = WordCloud(stopwords =stopwords, width=1000, height=500).generate(\"+\".join(antivirals_all))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","32e83104":"df = pd.DataFrame(antivirals_all, columns=['sentence']) \ndef matcher(x):\n    for i in antivirals:\n        if i.lower() in x.lower():\n            return i\n    else:\n        return np.nan\n    \ndf['Match'] = df['sentence'].apply(matcher)    \ndf.sample(5)","6be7ef56":"df['Match'].value_counts()[:10]","5fd1e401":"df['sentence'] = df['sentence'].astype(str)\ndf['sentence'] = df['sentence'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf['sentence'] = df['sentence'].str.replace('[^\\w\\s]','')\n\nstop = stopwords.words('english')\ndf['sentence'] = df['sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\nst = PorterStemmer()\ndf['sentence'] = df['sentence'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n\ndef senti(x):\n    return TextBlob(x).sentiment  \n \ndf['senti_score'] = df['sentence'].apply(senti)","8593b608":"df.sample(5)","e9dae899":"polarity=[]\nsubjectivity=[]\nfor i, j in df.senti_score:\n    polarity.append(i)\n    subjectivity.append(j)","0f1d2a68":"df['subjectivity']=subjectivity\ndf['polarity']=polarity\ndf.where(df['polarity']==1).dropna().head(5)","727cf3dc":"vote_data=df[['Match', 'polarity']]\nitems=vote_data['Match']#item's column\nvotes=vote_data['polarity']#vote's column\nnum_of_votes=len(items)\n    \nm=min(votes)\navg_votes_for_item=vote_data.groupby('Match')['polarity'].mean()#mean of each item's vote\nmean_vote=np.mean(votes)#mean of all votes\npol=pd.DataFrame(((num_of_votes\/(num_of_votes+m))*avg_votes_for_item)+((m\/(num_of_votes+m))*mean_vote))\npol.head()","c03ec3e5":"vote_data=df[['Match', 'subjectivity']]\nitems=vote_data['Match']#item's column\nvotes=vote_data['subjectivity']#vote's column\nnum_of_votes=len(items)\n    \nm=min(votes)\navg_votes_for_item=vote_data.groupby('Match')['subjectivity'].mean()#mean of each item's vote\nmean_vote=np.mean(votes)#mean of all votes\nsub=pd.DataFrame(((num_of_votes\/(num_of_votes+m))*avg_votes_for_item)+((m\/(num_of_votes+m))*mean_vote))\nsub.head()","7eff37e9":"on_weighted_score=pd.concat([pol, sub.reindex(pol.index)], axis=1).sort_values(by=['polarity', 'subjectivity'], ascending=False)\nvalue_count=pd.DataFrame(df['Match'].value_counts())\nbests=pd.concat([value_count, on_weighted_score.reindex(value_count.index)], axis=1).sort_values(by=['polarity', 'subjectivity'], ascending=False)\nbests.head(5)","9e8dd772":"Another drug called Codeine.","df89df9a":"We have 13891 column left.","09603ad5":"A wordcloud on all sentences for antivirals.","ebed6286":"**Mentioned drugs category.**","225fa297":"We will split all texts of all article.","0ee8b2a7":"# Antivirals wordcloud&sentiment analysis.","d88d0e03":"* We will use weighted rating to balance the polarity and subjectivity then we can see which one has the best score on those values including number mentions.\n[source](http:\/\/github.com\/pytmar\/Python-Code-Collection\/blob\/master\/weighted-ratings.py)","187e51af":"## resources:\n* https:\/\/www.cdc.gov\/coronavirus\/2019-ncov\/symptoms-testing\/symptoms.html\n* https:\/\/www.kaggle.com\/iancornish\/drug-data\n* https:\/\/www.youtube.com\/watch?v=S6GVXk6kbcs&lc=z23czv4rezzqspkcnacdp434abyko0xfj3zyelkza01w03c010c\n* https:\/\/www.kaggle.com\/bgoss541\/training-set-labeling-jump-start-umls-linking\n* https:\/\/www.kaggle.com\/danofer\/usp-drug-classification\n* https:\/\/data-science-blog.com\/blog\/2018\/11\/04\/sentiment-analysis-using-python\/","fac6337c":"I matched drugs and the sentences that include that drug. Note that at same cases one sentences may include many drugs.","962f0f3c":"* Now finally we can see which antivirals mentioned positive with our sentiment analysis method. ","fdca870b":"Reading Drug dataset.","b215da8b":"**Antivirals**","8e1d1278":"We know that one of the main condition for Covid-19 is Cough so lets try to see drugs that been used for this purpose. I also Included headache so you can try yourself.","c87beb62":"I will use another dataset here, the source can be found below.","d4be55d7":"An example of sentences that mention an antiviral named entecavir. Next work will be try to analyze if this drug mentioned in a positive or negative way.","e4d3755b":"An example: lets see where Benzonatate mentioned in the articles.","f226ca36":"**Final Note**:  I am not an expert on medicine just tried to do some text mining and crossing my borders on text mining. Please type down if we found any issue or have suggestion.\nThanks.","54fffc9d":"# Mentioned drugs in the COVID-19 articles.\n\nIn this work i will try to find drugs that mentioned in all article and try to understand those drugs.\nI will use a small dataset.\n### Update 1:\n1-I have found another useful dataset which is richer. The source can be found below.\n2-I checked drugs with their class and will do some wordcloud next.\n### Update 2:\n1- I  created a wordcloud on all sentence that mentioned about antivirals.\n2- I merge sentences and mentioned antivirals as a single dataframe.\n3- As a beta feature, i applied a sentiment analysis method but it will need some update due to week performance(some sentence gives no result, may be it is because sentences doesn't contain enough words to understand if the sentence postive or negative).\n### Update 3:\n1- I have done working on sentiment analysis of sentences of antiviral mentioned. \n2-I did weighted average on order by polarity-subjectivity scores including value counts of antiviral.\n3 The same work can also be done for other drug categories.","32d49177":"Results show us that drugs had been used for different cases and rated based on effects.","03b3ab8d":" A wordcloud on all sentences for Cardiovascular Agents.","6e8e9d27":"I will split every word in all articles. To join it let's call the column drugName","e3b1d057":"Let's do some filter:\n* removing drugs that has lower then 7 rating"}}