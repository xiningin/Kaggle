{"cell_type":{"10499eb9":"code","ce6830ef":"code","f2b95454":"code","d9b70ba2":"code","d4ed1ce2":"code","0d4059b7":"code","f532bf1f":"code","9b9c468b":"code","8c3339c0":"code","2c37e58c":"code","f7c23d3a":"code","1e5ae59e":"code","38d8d8ba":"code","42586097":"code","cf1ccce4":"code","7511b29a":"code","8638744a":"code","0b265c87":"code","65909dd1":"code","a6024e8e":"code","221e8519":"code","cba7243c":"code","85c94496":"code","d1e30c9d":"code","5b949fb1":"code","4cfa61c0":"code","cb769265":"markdown","3a40b18c":"markdown","95f35706":"markdown","d15fd5ba":"markdown","14aa1594":"markdown","2b3347de":"markdown","9638f3ea":"markdown","993a9098":"markdown","dc5e605f":"markdown","73e95076":"markdown","9eb3b2f8":"markdown","92d11caf":"markdown","f2a4d83b":"markdown","e26febcf":"markdown","15ed6679":"markdown","78a177e4":"markdown","cd84efc3":"markdown","947f5e54":"markdown","67d32b9b":"markdown","3cef088d":"markdown","833b51d6":"markdown"},"source":{"10499eb9":"#Libraries\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as ms\n\n#Data\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nss = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","ce6830ef":"#Missing Value\nprint('Number of Missing Values in Target feature: {}'.format(train.target.isnull().sum()))","f2b95454":"#Distribution\ncanv, axs = plt.subplots(1,2,figsize=(22,8))\ncolor = ['darkgreen','darkslategrey']\n\nplt.sca(axs[0])\nplt.pie(train.groupby('target').count()['id'],explode=(0.1,0),startangle=120,colors=color,\n    textprops={'fontsize':15},labels=['Not Disaster (57%)', 'Disaster (43%)'])\n\nplt.sca(axs[1])\nbars = plt.bar([0,0.5],train.groupby('target').count()['id'],width=0.3,color=color)\nplt.xticks([0,0.5],['Not Disaster','Disaster'])\nplt.tick_params(axis='both',labelsize=15,size=0,labelleft=False)\n\nfor sp in plt.gca().spines.values():\n    sp.set_visible(False)\n    \nfor bar,val in zip(bars,train.groupby('target').count()['id']):\n    plt.text(bar.get_x()+0.113,bar.get_height()-250,val,color='w',fontdict={'fontsize':18,'fontweight':'bold'})\n\ncanv.suptitle('Target Value Distribution in Training Data',fontsize=18);","d9b70ba2":"#Train Data\ntrain_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na==0].index).sort_values(ascending=False)\n\npd.DataFrame({'Train Missing Ratio' :train_na}).head(3)","d4ed1ce2":"#Test Data\ntest_na = (test.isnull().sum() \/ len(test)) * 100\ntest_na = test_na.drop(test_na[test_na==0].index).sort_values(ascending=False)\n\npd.DataFrame({'Test Missing Ratio' :test_na}).head(3)","0d4059b7":"#Visualizing Mssing Values\ntitle = 'Train'\ndata = [train_na,test_na]\ncanv, axs = plt.subplots(1,2)\ncanv.set_size_inches(18,5)\nfor ax, dat in zip(axs,data):\n    plt.sca(ax)\n    sns.barplot(x=dat.index, y=dat,dodge=False)  \n    plt.xlabel('Features', fontsize=15,labelpad=10)\n    plt.ylabel('Percent of missing values', fontsize=15,labelpad=13)\n    plt.title('Percent missing data by feature in {} Data'.format(title), fontsize=15,pad=20)\n    plt.tick_params(axis='both',labelsize=12)\n    \n    sp = plt.gca().spines\n    sp['top'].set_visible(False)\n    sp['right'].set_visible(False)\n    \n    title = 'Test'","f532bf1f":"#Filling Missing Data\nfor df in [train,test]:\n    for col in ['keyword','location']:\n        df[col].fillna('None',inplace=True)","9b9c468b":"#Function\ndef add_feature(X, feature_to_add):\n    \"\"\"\n    Returns sparse feature matrix with added feature.\n    \"\"\"\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","8c3339c0":"#Target Value\ny = train.target\nX = train.text\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.2)","2c37e58c":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=5,ngram_range=(1,5)).fit(X_train)\n\n#train\nX_train_vect = tfidf.transform(X_train)\n\n#test\nX_test_vect = tfidf.transform(X_test)","f7c23d3a":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(str(x).split())))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(str(x).split())))","1e5ae59e":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(set(str(x).split()))))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(set(str(x).split()))))","38d8d8ba":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(str(x))))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(str(x))))","42586097":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('#')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('#')))","cf1ccce4":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('@')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('@')))","7511b29a":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('http')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('http')))","8638744a":"X_train_vect = add_feature(X_train_vect,X_train.str.count(r'[\\\\\/!?,\\.:=<>^-]'))\nX_test_vect = add_feature(X_test_vect,X_test.str.count(r'[\\\\\/!?,\\.:=<>^-]'))","0b265c87":"X_train_vect = add_feature(X_train_vect,X_train.str.count(r'\\d'))\nX_test_vect = add_feature(X_test_vect,X_test.str.count(r'\\d'))","65909dd1":"# Number of Features\nprint('The Number of Features in the Processed Data: {}'.format(X_train_vect.shape[-1]))","a6024e8e":"# Lets look at some of the Vacabulary from the Tfidf\nprint(tfidf.get_feature_names()[350:420])","221e8519":"#Metric and Model\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score,make_scorer,accuracy_score,roc_auc_score\n\n#Creating Log Loss Scorer\nLogLoss = make_scorer(f1_score, greater_is_better=True, needs_proba=True)\n\n#Two Classifier\nmlp1 = MLPClassifier(max_iter=200,verbose=False,solver='sgd',activation='relu',learning_rate='adaptive')\nmlp2 = MLPClassifier(max_iter=200,verbose=False,solver='adam',activation='relu',learning_rate='adaptive')\n\n#Parameters for tunning\nparameter_space = {\n      'hidden_layer_sizes': [(50,50,50), (50,100,50),(100,100,100),(100,100),(500,500)],\n      'alpha': [0.0001, 0.05],\n  }","cba7243c":"#Tunning the First Classifier\nclf1 = GridSearchCV(mlp1, parameter_space,verbose=2,cv=3,scoring='f1')\nclf1.fit(X_train_vect,y_train)","85c94496":"#Tunning the Second Classifier\nclf2 = GridSearchCV(mlp2, parameter_space,verbose=2,cv=3,scoring='f1')\nclf2.fit(X_train_vect,y_train)","d1e30c9d":"#Results from the first GridSearch\npd.DataFrame(data=clf1.cv_results_,columns=['param_hidden_layer_sizes','param_alpha','mean_test_score','std_test_score']).sort_values('mean_test_score',\n                                                                                                     ascending=False).reset_index(drop=True).iloc[:10]","5b949fb1":"#Results from the Second GridSearch\npd.DataFrame(data=clf2.cv_results_,columns=['param_hidden_layer_sizes','param_alpha','mean_test_score','std_test_score']).sort_values('mean_test_score',\n                                                                                                     ascending=False).reset_index(drop=True).iloc[:10]","4cfa61c0":"# So we can see that the best esitmator has the following parameters\nprm = {'alpha':0.05,'hidden_layer_sizes':(100,100,100),'max_iter':200,'solver':'adam','activation':'relu','learning_rate':'adaptive'}\nprint('The Best Parameters are:\\n')\nfor p,c in zip(prm.items(),range(1,7)):\n    print('{}. {} = {}'.format(c,p[0],p[1]))\n\n#Final F1 Score of test data\nprint('\\n\\nAnd the Accuracy of the Final Model on Test Data: {:.3f}'.format(accuracy_score(y_test,clf2.predict(X_test_vect))))\nprint('\\n\\nAnd the F1 score of the Final Model on Test Data: {:.3f}'.format(f1_score(y_test,clf2.predict(X_test_vect))))","cb769265":"## Feature Engineering:","3a40b18c":"### 4. Character Count","95f35706":"## Model Building\n\n1. Multilayer Perceptron Classifier","d15fd5ba":"### 7. Number of Out Links","14aa1594":"### 9. Number of Digits","2b3347de":"## Target value :\n\n### 1. Target Value Missing data","9638f3ea":"### 1. MLPClassifier\n\nMLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.\n\n![MLP](https:\/\/s3.amazonaws.com\/stackabuse\/media\/intro-to-neural-networks-scikit-learn-3.png)","993a9098":"\n### List of Features:\n- TFIDF with 2-5 ngrams and Min DF of 5\n- Word Count\n- Unique Word Count\n- Character Count\n- Number of Hastags\n- Number of Tagged People('@')\n- Number of Outlinks\n- Number of Non-WordCharacters\n- Number of Digits(Stats)","dc5e605f":"## Handling Missing Data:\nBoth training and test set have same ratio of missing values in `keyword` and `location`.\n* **0.8%** of `keyword` is missing in both training and test set\n* **33%** of `location` is missing in both training and test set\n\nSince missing value ratios between training and test set are too close, **they are most probably taken from the same sample**. Missing values in those features are filled with `None` and `None` respectively.","73e95076":"## Importing Data and Dependencies","9eb3b2f8":"### 3. Unique Words","92d11caf":"### 8. Number of Non-WordCharacters","f2a4d83b":"## Missing Values","e26febcf":"### 2. Target Value Distribution","15ed6679":"### 6. Number of Tagged People('@')","78a177e4":"### HyperParameter Tunning","cd84efc3":"## Looking at the final Data","947f5e54":"### Helper Function to add Features","67d32b9b":"### 2. Word Count","3cef088d":"### 5. Number of Hastags","833b51d6":"### 1. TFIDF\n\nIn information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the words.\n\n![TFIDF](https:\/\/www.researchgate.net\/profile\/Haider_Al-Khateeb2\/publication\/291950178\/figure\/fig1\/AS:330186932408324@1455734107458\/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png)\n\n- We will be using a min docmuent frequency of 5 that means a ngram needs to be in atleast 5 document or instances in the data to be able to get in the vacabulary of the vectorizer \n- And we will using ngrams of 2 to 5 words "}}