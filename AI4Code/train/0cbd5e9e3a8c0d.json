{"cell_type":{"29972af4":"code","6a145580":"code","6bd6945c":"code","5160de58":"code","25bb21a8":"code","325499cc":"code","a3fee521":"code","bd4679cd":"code","2f93a6a6":"code","c4b78fbe":"code","c5ffee31":"code","98249926":"code","a902c8a7":"code","e512cb49":"code","d268331d":"code","b9738334":"code","bfb1cfd4":"code","d32efe2c":"code","f7d1d639":"code","c1be49f7":"code","c7edf6f3":"code","4e0ea648":"code","ec35590c":"code","146ce365":"code","4e282c90":"code","37133b15":"code","d011c211":"code","f4254b54":"markdown","1b43b95f":"markdown","a6007c3b":"markdown","4b14d1b8":"markdown","45457750":"markdown","b19c7be6":"markdown","71abd6c9":"markdown","43b2a4b9":"markdown","4ab6f04c":"markdown","5f53c809":"markdown","43063075":"markdown","ebf67a7d":"markdown","330e55be":"markdown","8c42357d":"markdown","08721f5b":"markdown","244f4c99":"markdown","277f2188":"markdown","781f79b0":"markdown","271f1534":"markdown","6a48dfca":"markdown","a9d102a3":"markdown","0bf6cd8f":"markdown","38968a0c":"markdown","04865be6":"markdown","80899dfa":"markdown","e837e328":"markdown","59f09aed":"markdown","3f882bb0":"markdown","bde367e4":"markdown","5d1e6f33":"markdown","a39e9431":"markdown","42903153":"markdown","67e0d1fb":"markdown","246014be":"markdown","4c113c84":"markdown"},"source":{"29972af4":"import os #accessing directory structure\nimport numpy as np #linear algebra\nimport pandas as pd #data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #plotting\nimport matplotlib.pyplot as plt # plotting\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn import metrics #metrics selection\nfrom sklearn.metrics import confusion_matrix #confusion matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import model_selection #model selection\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.tree import DecisionTreeClassifier #Decision Trees\nfrom sklearn.ensemble import RandomForestClassifier #random forests\nfrom sklearn.model_selection import cross_val_score #Cross validarion import\nfrom sklearn.metrics import accuracy_score #Accuracy of models\nfrom sklearn.model_selection import train_test_split #Import train_test_split from sklearn library\n","6a145580":"print(os.listdir('..\/input'))","6bd6945c":"#Importing data\nchurn_data = pd.read_csv('..\/input\/Churn_Modelling.csv', delimiter=',')\n\n#Giving the dataframe a name\nchurn_data.dataframeName = 'Churn_Modelling.csv'\n\n#Computing the number of rows and columns\nnRow, nCol = churn_data.shape\n\n#Displays the number of rows(10 000) and columns (14)\nprint(f'The dataset has {nRow} rows and {nCol} columns')\nchurn_data.head()","5160de58":"churn_data.info()","25bb21a8":"churn_data.describe()","325499cc":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","a3fee521":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","bd4679cd":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","2f93a6a6":"slices=  [churn_data.Exited[churn_data.Exited ==1].count(), \n          churn_data.Exited[churn_data.Exited == 0].count()]\nslice_labels = ['Churned', 'Stayed']\ncolours = ['b', 'g']\nfig, ax = plt.subplots(figsize=(10, 10))\nax.pie(slices, labels = slice_labels, colors = colours, startangle= 90, autopct='%.1f%%')\nplt.title(\"The number of churned cutomers and  the ones that stayed\", size = 20)\nplt.show()","c4b78fbe":"plotPerColumnDistribution(churn_data, 10, 5)","c5ffee31":"# using seaborn library for visualization\nplot, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.flatten()\n\n# extracting features with unique values that are between 2 and 49\nnunique = churn_data.nunique()\n\n# array of categorical features\ncat_features = [col for col in churn_data.columns if nunique[col] >= 1 and nunique[col] < 50]\n\n# array of non-categorical non_categorical\nnum_features = [col for col in churn_data.columns if nunique[col] > 50]\n\n# looping through the array of categorical features and plotting their counts with the target variable\nfor axis, catplot in zip(axes, churn_data.dtypes[cat_features].index):\n    sns.countplot(x=catplot, hue = 'Exited', data=churn_data, ax=axis)   \nplt.tight_layout()  \nplt.show()  ","98249926":"plotCorrelationMatrix(churn_data, 8)","a902c8a7":"# Dropping RowNumber CustomerId Surname   \nchurn_data1 = churn_data.drop([\"RowNumber\", \"CustomerId\",\"Surname\"], axis=1)\n\n# Creating dummy variables from categorical variables\nchurn_data2 = pd.get_dummies(churn_data1, columns=['Gender','Geography'])\n","e512cb49":"#New dataset after creating dummy variables\nchurn_data2.head()","d268331d":"#Renaming dummy variables created from Gender ang Geography columns\nchurn_data2.rename(columns={\"Gender_Female\": \"Female\", \"Gender_Female\": \"Male\",\"Geography_France\":\"France\",\n                                      \"Geography_Spain\": \"Spain\",\"Geography_Germany\":\"Germany\"}, inplace=True)\nchurn_data2.head()","b9738334":"churn_data2.Exited.value_counts()","bfb1cfd4":"# upsampling the minority class so that it has the same number of records as the mijority\nchurn_balanced = resample(churn_data2[churn_data2.Exited ==1],replace=True,n_samples= 7963,random_state=1) #set the seed for random resampling\nchurn_balanced = pd.concat([churn_data2[churn_data2.Exited ==0], churn_balanced])","d32efe2c":"churn_balanced.Exited.value_counts()","f7d1d639":"# Explanatory variable from unbalanced dataset\n# Explanatory variable\nX = churn_data2.loc[:,churn_data2.columns != 'Exited']\n\n# Target variable\nY = churn_data2.loc[:,churn_data2.columns == 'Exited']","c1be49f7":"# Explanatory variable from the balanced dataset\n# Explanatory variable\nX_balanced = churn_balanced.loc[:,churn_balanced.columns != 'Exited']\n\n# Target variable\nY_balanced = churn_balanced.loc[:,churn_balanced.columns == 'Exited']","c7edf6f3":"#splitting into training and testing datasets from both balanced and unbalanced datasets\n\n#imbalanced\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) \n\n#balanced\nX_trainb, X_testb, Y_trainb, Y_testb = train_test_split(X_balanced, Y_balanced, test_size=0.2) ","4e0ea648":"#fitting a the Logistic regression model on unbalanced dataset\nlogregit = LogisticRegression()\nlogregit.fit(X_train, Y_train.values.ravel())\n\n#predicting \nY_pred = logregit.predict(X_test)\naccuracy = accuracy_score(Y_pred, Y_test)*100\nprint('Accuracy of LogisticRegression: {:.2f} %'.format(accuracy))\nprint(classification_report(Y_test, Y_pred))\n","ec35590c":"#fitting a the Logistic regression model\nlogregitb = LogisticRegression()\nlogregitb.fit(X_trainb, Y_trainb.values.ravel())\n\n#predicting \nY_predb = logregitb.predict(X_testb)\naccuracy = accuracy_score(Y_predb, Y_testb)*100\nprint('Accuracy of LogisticRegression: {:.2f} %'.format(accuracy))\nprint(classification_report(Y_testb, Y_predb))","146ce365":"#Decision tree classifier\ndtreeb = DecisionTreeClassifier(max_depth=5)\n\n#Training the decision tree classifier on a balanced dataset using all features\ndtreeb.fit(X_trainb, Y_trainb)\n\n#predicting \nY_pred = dtreeb.predict(X_testb)\naccuracy = accuracy_score(Y_predb, Y_testb)*100\nprint('Accuracy of DecisionTreeClassifier: {:.2f}'.format(accuracy))\nprint(classification_report(Y_testb, Y_predb))","4e282c90":"#Decision tree classifier\ndtree = DecisionTreeClassifier(max_depth=5)\n\n#Training the decision tree classifier\ndtree.fit(X_train, Y_train)\n\n#predicting \nY_pred = dtree.predict(X_test)\naccuracy = accuracy_score(Y_pred, Y_test)*100\nprint('Accuracy of DecisionTreeClassifier: {:.2f}'.format(accuracy))\nprint(classification_report(Y_test, Y_pred))","37133b15":"#Random forest classifier\nrandForest = RandomForestClassifier()\n\n#Training the decision tree classifier\nrandForest.fit(X_train, Y_train.values.ravel())\n\n#predicting \nY_pred = randForest.predict(X_test)\naccuracy = accuracy_score(Y_pred, Y_test.values.ravel())*100\nprint('RandomForestClassifier: {:.2f}'.format(accuracy))\nprint(classification_report(Y_test, Y_pred))","d011c211":"#Random forest classifier\nrandForestb = RandomForestClassifier()\n\n#Training the decision tree classifier\nrandForestb.fit(X_trainb, Y_trainb)\n\n#predicting \nY_predb = randForest.predict(X_testb)\naccuracy = accuracy_score(Y_predb, Y_testb.values.ravel())*100\nprint('RandomForestClassifier: {:.2f}'.format(accuracy))\nprint(classification_report(Y_testb, Y_predb))","f4254b54":"#### Feature extraction","1b43b95f":"### Dropping features that will not be used and creating dummy variables\nFirst, I start by dropping columns taht will not contribute in prediction modelling and then create \ndummy variables from categorical variables.","a6007c3b":"Correlation matrix:","4b14d1b8":"## Conclusion\nAs concludion, from the analysis done on this data set, it was seen groups such as Germany and Female have a higher ratio \nof customers who churned. After analysis, different classifiers were used and it was proved that Random Forests presented \nan overall better performance on both balanced (**86 % accuracy**) and imbalanced (**91.56 accuracy%**)  datasets in predicting whether or not customers will churn. \nThus Random Forests is better classifier for this problem.","45457750":"#### The following function plot distribution graphs","b19c7be6":"## 5. Predictive modelling\n\nFor this stage, different classification models mentioned in the introduction will be used to predict whether a given customer will or not churn.\nTo do that, our dataset will be shuffled and split into train and test data, the former is 80% and latter 20 % of total records.\n","71abd6c9":"## 2. Loading the dataset\n","43b2a4b9":"\n**The following observations can be seen from the graphs above**\n* It can be seen that the majority of customers come from France, followed by Spain and Germany in the last place. Despite Germany having approximately half the number of customers of France, both countries have almost equal the number of curners. This implies that Germany has the highest ratio of churners\n\n* The number of female churners is greater than the male's,  despite the number of male customers being greater than that of females.\n\n* Non active customers churn at a higher rate than active onesDistribution graphs (histogram\/bar graph) of sampled columns:","4ab6f04c":"## Introduction and problem statement \nEvery month BankCo loses thousands of customers to its competitors. The customers who leave the bank are known as 'churned customers'. In this chalenge, I will predict the customers who are likely to leave BankCO so that BankCo can take masures to make those customers stay by prioritising marketing efforts on those who are likely to churn.","5f53c809":"** On balanced dataset**","43063075":"Now we have a balanced dataset ","ebf67a7d":" **Metrics used**\n\n\n**Precision**:  This a measure of a classifier's exactness, i.e., the ability of a classifier to give correct labels.\n\n**Recall**: A measure of a classifiers completeness, i.e., the ability of a classifier to find all samples that belong a certain class.\n\n  **F1 Score (or F-score)**: A weighted average of precision and recall.\n  \n  As the results above show, eventhough the unbalanced dataset gives a higher accuracy, the precion and recall values are only high  for the majority class( class of those who did not churn) and very low for the minority class. This questions the ability of the model to predict that a customer will churn.\n  \nDespite having a lower accuracy, the Logistic Regression model trained on balanced data gives balanced precision and recall values for both the majority and minority classes.","330e55be":"#### Plotting  features mostly categorical with unique values < 50  siding with the target value","8c42357d":"The correlation matrix shows correlation between Age, isActiveMember, Balance with our the target variable. There is no strong correlation among the explanatory variables thus there will not be a need to avoid strongly correlated explanatory variables in modelling. ","08721f5b":"#### On imbalanced data","244f4c99":"From the balanced dataset","277f2188":"#### Balanced data","781f79b0":"As it can be seen on the pie chart above, 79.6% of the records of the dataset represent customers who stayed and 20.4% are those who churned. This reprents  a  risk of predicting in favor of the majority subset which represents the customers who stayed. Therefore before modelling the minority subset (churned) will be upsampled in order to balance our dataset. ","271f1534":"## 4. Data processing and feature engineering","6a48dfca":"**End**","a9d102a3":"From imbalanced dataset","0bf6cd8f":"### Decision Trees  \nDecision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. They are very powerful algorithms, capable of fitting complex datasets.\n\n**On balanced dataset**","38968a0c":"Decision trees gives overall better performance than Logistic Regression on both balanced and original data sets with higher precision and recall scores. Therefore, DecisionTreeClassifier is a better classifier than Logistic Regression.\nLet's see how it holds against Random Forests!","04865be6":"#### Dataset information ","80899dfa":"### Logistic Regression\nLogistic Regression is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that a given customer will churn?). If the estimated probability is greater than 50%, then the model predicts that the customer will churn (positive class, labeled \u201c1\u201d), or else it predicts that the customer does not (i.e., it belongs to the negative class, labeled \u201c0\u201d). This makes it a binary classifier.","e837e328":"\nDistribution graphs (histogram\/bar graph) of sampled columns:","59f09aed":"**on original (imbalanced) dataset**","3f882bb0":"### RandomForests\n\nRandom Forests is an ensemble of Decision Trees which averages multiple Decision Trees that suffer from high variance. Random Forests build  more robust model that has better generalisation performance.","bde367e4":"##  3.Data visualization","5d1e6f33":"### Dealing with imbalanced dataset\nAs the pie chart in section 3 (Data visualisation) shows, our dataset is imbalanced 79.6 % of records belong to one class (Exited\/churned) to  avoid overfitting to the majority class, the dataset will be balanced by upsampling the minority class.","a39e9431":"#### Function for computing and displaying the correlation matrix","42903153":"## 1. Data preparation and analysis\n\n####  Importing relevant libraries that will be used","67e0d1fb":"The ratio of the number of exited cutomers to the ones that stayed","246014be":"**On original (imbalanced) dataset**","4c113c84":"* As it can be seen using the cells above, the data set has 10000 non-null rows which is the number of observations and 14 non-null columns   which is the number of features. Features with non_numericl values will be encoded into numbers.\n\n* The last column 'Exited' is the target column, this implies that this is a **Supervised Learning** problem\n\nThis is a binary classification analysis where customers will be split into two subsets; churners ie those who a re likely to leave the Bank and non-churners, that is those who will stay with bank. To achieve this, different classification algorithms such as **K-Nearest Neighbours, Logistic Regression, Support Vector Machines, Decision Trees, Random Forests and Gaussian Naive bayes** will be used and the best classifier will be selected."}}