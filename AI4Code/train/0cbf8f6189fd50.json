{"cell_type":{"a9fe94d2":"code","94a3666e":"code","d1298414":"code","fbf470f5":"code","cb12743e":"code","c3fa5a35":"code","e6f4af45":"code","023ce728":"code","2391315d":"markdown","3730123d":"markdown","1f49a29a":"markdown"},"source":{"a9fe94d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport math as mt\nimport glob\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94a3666e":"df_districts_info = pd.read_csv(r\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\").dropna()\n\n\n#print the head of the data set.\nprint(\"THIS IS THE RESULT OF head() FUNCTION \\n\", df_districts_info.head())\n\n#using describe() function to look for the descriptive stats\nprint(\"THIS IS THE RESUTL OF describe() FUNCTION \\n\", df_districts_info.describe())\n\n#using info() to see how many are type() of object()\nprint(\"THIS IS THE RESULT OF info() FUNCTION \\n\",df_districts_info.info())\nprint(\"This info function shows that only one column is type() of int() and rest of them are type() of object()\")\n","d1298414":"#checking for total number of missing values in the all the columns\nprint(df_districts_info.isnull().sum())\nprint(df_districts_info.count())","fbf470f5":"#both isna() and isnull() provides the same result in pre processing of the data\nprint(df_districts_info.isna().sum())\nprint(df_districts_info.count())","cb12743e":"#cleaning of data with usind dropna() function which drops all the NaN values.\ndf_districts_info['state'] = df_districts_info['state'].dropna()\nfor i in df_districts_info['state'][:30].unique():\n    print(i)\n    \nprint(df_districts_info['state'][:30].dropna())","c3fa5a35":"# droping NaN the values of rest of th e columns\nplt.plot(df_districts_info['locale'].dropna())\nplt.show()\nplt.plot(df_districts_info['pct_black\/hispanic'].dropna())\nplt.show()\nplt.plot(df_districts_info['pct_free\/reduced'].dropna())\nplt.show()\nplt.plot(df_districts_info['county_connections_ratio'].dropna())\nplt.show()\nplt.plot(df_districts_info['pp_total_raw'].dropna())\nplt.show()\nplt.plot(df_districts_info['county_connections_ratio'].dropna())\nplt.xlabel(\"\")\nplt.show()","e6f4af45":"# importing libraries for encoding of the dataset.\n# from sklearn import preprocessing\n# from sklearn.preprocessing import LabelEncoder \n# from sklearn.preprocessing import OneHotEncoder\n\n# # transforming the data set values to int()\n# df_districts_info = OneHotEncoder().fit_transform(df_districts_info)\n\n# df_districts_info.describe()","023ce728":"# get data file names\n\npath = r'..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\nfilenames = glob.glob(path + \"\/*.csv\")\ndfs = [pd.read_csv(filename) for filename in filenames]\nframe = pd.concat(dfs, ignore_index=True)\n\nprint(frame.count())\n\n# if needed concatenate all data into one Data\n# df = []\n# def app(df):\n#     path = r'..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\n#     filenames = glob.glob(path + \"\/*.csv\")\n#     dfs = [pd.read_csv(filename) for filename in filenames]\n#     frame = pd.concat(dfs, ignore_index=True)\n#     df.append(frame)\n#     return app\n","2391315d":"#                                          ***LET'S GET STARTED***\n This note book is distributed in the following sections. Pre-processing till completion.\n \n1.   Reading \"districts_info.csv\" data set cleaning and perfroming analytics.\n2.   Reading \"product_info.csv\" data set cleaning and perfroming analytics.\n3.   Merging all the \"engagement_data\" data set form the file cleaning, pre-processing, and performing analytics.\n4.   Final studies collectively conducting studies, District Wise, includes trends and co-relation.\n5.   Answers of questions from the given information\n     * (1) the state of digital learning in 2020 and \n     * (2) how the engagement of digital learning relates to factors such as district demographics\n     *      broadband access, and state\/national level policies and events.\n \n \n","3730123d":"# Section 1\n**READING \"districts_info.csv\" AND \"product_info.csv\" SEPRATELY, THEN PERFORMING EDA ON THESE TWO DATA SETS.**\n\n\nThese are two individual data sets and later on will be concatinated with rest of the .csv files present in the engagement_data file. The enengagement_data consists of more than 200 files.\n\nHere we can see the data set has total of 7 columns and 6 are of type() object().","1f49a29a":"# Section 3\n\n**IMPORTING ALL THE CSV FILES AT ONCE**\nSince this data is consist of multiple .csv we have to import all of them all at once to process an anaylize them. In this notbeook we used \"glob\" library in order to read all the data sets available.\n\n**HOW DOES GLOB LIBRARY WORKS?**\nThe glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order\n\nUsefull link: https:\/\/docs.python.org\/3\/library\/glob.html"}}