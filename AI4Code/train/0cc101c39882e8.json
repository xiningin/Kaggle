{"cell_type":{"f60c337c":"code","a014080d":"markdown","cbc5cb28":"markdown","b622c2fd":"markdown","493335ca":"markdown","5f1a5929":"markdown","f2b973b4":"markdown","98392683":"markdown","16666906":"markdown","6b5f5e5f":"markdown","d1d8b23d":"markdown","f89298cd":"markdown","06ce0ed3":"markdown","34042b37":"markdown","f0b6d599":"markdown","afe54307":"markdown","537169fc":"markdown","70f48fdd":"markdown","6a15717b":"markdown","57bb9436":"markdown","9572dcae":"markdown","747065fd":"markdown"},"source":{"f60c337c":"import os\nimport re\n\nimport json\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport random\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom termcolor import colored\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nimport wandb\nfrom wandb.lightgbm import wandb_callback, log_summary\n\nwandb.login()","a014080d":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Code will be added soon!<\/center><\/h3>","cbc5cb28":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","b622c2fd":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","493335ca":"<h1><center>Understanding LGBM in Complete Depth<\/center><\/h1>\n<h3><center>Ubiquant Market Prediction<\/center><\/h3>\n\n<center><img src = \"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/ubiquant\/6.jpg\" width = \"2750\" height = \"2500\"\/><\/center>                                                                          ","5f1a5929":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","f2b973b4":"---","98392683":"## **<span style=\"color:orange;\">Problem with Traditional GBDT Methods<\/span>**\n\n\nGradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm,\nand has quite a few effective implementations such as XGBoost and pGBRT and these algorithms work well when the dimension of data is low or the data size is small.\n  \nHowever, when the feature dimension is high and data size is large the efficiency and scalability are still unsatisfactory even though they have been optimized a lot.\n\n## **<span style=\"color:orange;\">The Reason<\/span>**\n\nA major reason is that for each feature,they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. \n\n## **<span style=\"color:orange;\">Proposed solution by Researchers<\/span>**\n\nTo tackle this problem, researchers propose two novel techniques: \n1. **Gradient-based One-Side Sampling (GOSS)**\n2. **Exclusive Feature Bundling (EFB)**\n\n## **<span style=\"color:orange;\">Basic GOSS and EFB Understanding<\/span>**\n\nWith **GOSS**, researchers excluded a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. It is proven that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.\n  \nWith **EFB**, researchers bundle mutually exclusive features (i.e., they rarely take nonzero\nvalues simultaneously), to reduce the number of features. It is proven that finding\nthe optimal bundling of exclusive features is NP-hard, but a greedy algorithm\ncan achieve quite good approximation ratio (and thus can effectively reduce the\nnumber of features without hurting the accuracy of split point determination by\nmuch). \n  \nAnd thus, this new GBDT implementation with GOSS and EFB is called as LightGBM.","16666906":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","6b5f5e5f":"Gradient boosting decision tree (GBDT) is a widely-used machine learning algorithm, due to its \n- Efficiency, \n- Accuracy, and \n- Interpretability. \n  \nGBDT achieves state-of-the-art performances in many machine learning tasks, such as \n- Multi-class classification, \n- Click prediction, and \n- Learning to rank. \n  \nIn recent years, with the emergence of big data (in terms of both the number of features\nand the number of instances), GBDT is facing new challenges, especially in the tradeoff between accuracy and efficiency.\n  \nConventional implementations of GBDT need to, for every feature, scan all the data instances to estimate the information gain of all the possible split points. \nTherefore, their computational complexities will be proportional to both the number of features and the number of instances. This makes these implementations very time consuming when handling big data.\n\n---\n\nTo tackle this challenge, a straightforward idea is to reduce the number of data instances and the number of features. However, this turns out to be highly non-trivial. \n  \n**For example**, \nit is unclear how to perform data sampling for GBDT. While there are some works that sample data according to their weights to speed up the training process of boosting they cannot be directly applied to GBDT since there is no sample weight in GBDT at all. \n\n---\n\nAs a result two novel techniques were developed towards this goal which are GOSS and EFB. Let's gid deeper into both these techniques.\n\n## **<span style=\"color:orange;\">Gradient-based One-Side Sampling (GOSS)<\/span>**\n\n- While there is no native weight for data instance in GBDT, it was noticed that data instances with different gradients play different roles in the computation of information gain. \n- In particular, according to the definition of information gain, those instances with larger gradients (i.e., under-trained instances) will contribute more to the information gain.\n- Therefore, when down sampling the data instances, in order to retain the accuracy of information gain estimation, we should better keep those instances with large gradients (e.g., larger than a pre-defined threshold, or among the top percentiles), and only randomly drop those instances with small gradients.\n- In the paper, it was proven that such a treatment can lead to a more accurate gain estimation than uniformly random sampling, with the same target sampling rate, specially when the value of information gain has a large range.\n  \n## **<span style=\"color:orange;\">Exclusive Feature Bundling (EFB)<\/span>**\n\n- Usually in real applications, although there are a large number of features, the feature space is quite sparse, which provides us a possibility of designing a nearly lossless approach to reduce the number of effective features. \n- Specifically, in a sparse feature space, many features are (almost) exclusive, i.e., they rarely take nonzero values simultaneously. \n- Examples include the one-hot features (e.g., one-hot word representation in text mining). \n- We can safely bundle such exclusive features. \n- To this end, researchers designed an efficient algorithm by reducing the optimal bundling problem to a graph coloring problem (by taking features as vertices and adding edges for every two features if they are not mutually exclusive), and solving it by a greedy algorithm with a constant approximation ratio.\n\n> This new GBDT algorithm with GOSS and EFB was termed as LightGBM.","d1d8b23d":"For explaining Gradient Boosting and LightGBM, I am referring to the original paper [LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree](https:\/\/proceedings.neurips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)","f89298cd":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>  ","06ce0ed3":"> | S.No       |                   Heading                |\n> | :------------- | :-------------------:                |         \n> |  01 |  [**Competition Overview**](#competition-overview)  |                   \n> |  02 |  [**Libraries**](#libraries)                        |  \n> |  03 |  [**Weights and Biases**](#weights-and-biases)      |\n> |  04 |  [**Gradient Boosting Basic**](#gradient-boosting-basic)                |\n> |  05 |  [**Gradient Boosting Advance**](#gradient-boosting-advance)                |","34042b37":"---","f0b6d599":"<a id=\"gradient-boosting-advance\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Gradient Boosting Advance<\/center><\/h2>","afe54307":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","537169fc":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!<\/span>**\n> ### Reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)\n\n---","70f48fdd":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","6a15717b":"---","57bb9436":"## **<span style=\"color:orange;\">Description<\/span>**\n\n\nIn this competition, you\u2019ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible.\n\nIf successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions.\n\n---\n\n## **<span style=\"color:orange;\">Evaluation Metric<\/span>**\n\nSubmissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.\n  \nYou must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. \n  \nYou will get an error if you submission includes nulls or infinities and submissions that only include one prediction value will receive a score of -1.\n\n---","9572dcae":"---","747065fd":"<a id=\"gradient-boosting-basic\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Gradient Boosting Basic<\/center><\/h2>"}}