{"cell_type":{"d44f5dc9":"code","27332455":"code","aac5906f":"code","8181e1f6":"code","5dc306a0":"code","f8ee1e4b":"code","37588c71":"code","11e38f51":"code","4e97f3c6":"code","984fb4f0":"code","8b1419e0":"code","1a0337b3":"code","1a3b227d":"code","10943767":"code","134ac36f":"code","cad97352":"code","c058764e":"code","1e205a1a":"code","4d9ffeea":"code","6f1ac734":"code","986cb4da":"code","069bc49d":"code","a51503ab":"code","dff60102":"code","b48c25c7":"code","355bcb02":"code","a3cedee9":"code","6c195e40":"code","f41fded6":"code","18f6a937":"markdown","567afda4":"markdown","4deb29f4":"markdown","9001e8d0":"markdown"},"source":{"d44f5dc9":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox\nfrom scipy.special import inv_boxcox\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LinearRegression, LassoCV, Lasso\nfrom sklearn.metrics.regression import mean_squared_error, r2_score","27332455":"#function for data transformation\nimport scipy as sp\n\ndef gaussian_transformer(series):\n    z =  (sp.stats.rankdata(series) \/ (len(series)+1)) *2 - 1\n    z = np.arctanh(z)    \n    return z","aac5906f":"# finction to visualization of distribution\n\ndef distribution_check(df, QQ=False,Params=False):\n    sns.distplot(df, fit=norm);\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(df)\n\n    print('skew: ', skew(df))\n\n    #Now plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\n    plt.ylabel('Frequency')\n    plt.title('distribution')\n    if QQ == True:\n        #Get also the QQ-plot\n        fig = plt.figure()\n        res = stats.probplot(df, plot=plt)\n    plt.show()\n    plt.close()","8181e1f6":"def feature_importance(model,df):\n    reg_coef = pd.DataFrame({'coef': model.coef_, 'coef_abs': np.abs(model.coef_)},\n                          index=df.columns)\n    return reg_coef.sort_values(by='coef_abs', ascending=False)","5dc306a0":"# Load the training data\ndf_00 = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ndf_00.shape","f8ee1e4b":"#One-Hote Encoding for all categorical columns\ndf_cat = pd.concat([pd.get_dummies(df_00[col]).iloc[:,:-1] for col in df_00.columns if 'cat' in col], axis=1, keys=df_00.columns)","37588c71":"# list of all columns\nobject_cols = [col for col in df_00.columns if 'cat' in col]","11e38f51":"df_01 = pd.concat([df_cat,df_00],axis=1).drop(object_cols ,axis=1)\ndf_01.shape","4e97f3c6":"num_feature = df_00.select_dtypes('number').drop(['target'],axis=1)","984fb4f0":"num_list = num_feature.columns.tolist()\npair_list = [(num_list[i],num_list[j]) for i in range(len(num_list)) for j in range(i+1, len(num_list))]","8b1419e0":"count = 0\nfor pair in pair_list:\n    count += 1\n    df_01[count] = df_01[pair[0]]*df_01[pair[1]]","1a0337b3":"df_01.shape","1a3b227d":"for feature in num_list :\n    df_01[feature] = gaussian_transformer(df_00[feature])","10943767":"num_feature = num_feature.apply(lambda x: x**2)\ndf_02 = pd.concat([num_feature,df_01],axis=1)","134ac36f":"y = df_02['target']\nX = df_02.drop(['target'], axis=1)","cad97352":"scaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y,\n                                                     test_size=0.3,\n                                                     random_state=17)","c058764e":"X.shape","1e205a1a":"linreg = LinearRegression().fit(X_train, y_train)\n\npredictions = linreg.predict(X_valid)\n\nprint(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, linreg.predict(X_train)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_valid, predictions))\nprint(\"R 2 (test): %.3f\" % r2_score(y_valid, linreg.predict(X_valid)))","4d9ffeea":"feature_importance(linreg, X).iloc[:7,:]","6f1ac734":"from scipy.stats import pearsonr\ncorr, _ = pearsonr(predictions, y_valid)\ncorr","986cb4da":"distribution_check(y_valid)","069bc49d":"distribution_check(predictions)","a51503ab":"test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\nnum_feature = test.select_dtypes('number')\n#One-Hote Encoding for all categorical columns\ndf_cat = pd.concat([pd.get_dummies(test[col]).iloc[:,:-1] for col in test.columns if 'cat' in col], axis=1, keys=test.columns)\nobject_cols = [col for col in test.columns if 'cat' in col]\nX_test = pd.concat([df_cat, test],axis=1).drop(object_cols ,axis=1)","dff60102":"count = 0\nfor pair in pair_list:\n    count += 1\n    X_test[count] = X_test[pair[0]]*X_test[pair[1]]","b48c25c7":"for feature in num_list :\n    X_test[feature] = gaussian_transformer(test[feature])","355bcb02":"num_feature = num_feature.apply(lambda x: x**2)\nX_test_1 = pd.concat([num_feature, X_test],axis=1)","a3cedee9":"X_test_scaled = scaler.transform(X_test_1)","6c195e40":"model = LinearRegression().fit(X_scaled, y)","f41fded6":"# Use the model to generate predictions\npredictions = model.predict(X_test_scaled)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","18f6a937":"**Submission**","567afda4":"To get such score I did number of transformations:\n1. On Hot encoding for all category features\n2. Gaussian Transformation of all numeric features\n3. Add squared of numerical features\n4. Add multiplication of all pairs of numeric features\n5. Robust Scaler","4deb29f4":"**Linear Regression**","9001e8d0":"# Linear Regression with 0.735 score"}}