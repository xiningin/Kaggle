{"cell_type":{"fe131ac1":"code","c7a31743":"code","a9f7cb30":"code","8ed0f881":"code","ac5b8229":"code","40e477b4":"code","72799478":"code","90d51910":"code","6a2ab518":"code","8d504c62":"code","dcc59dd7":"code","610f4955":"code","ea8e3cee":"code","196b6ea5":"code","1c38bed9":"code","1a44fbd5":"code","111143e6":"code","07fd3413":"code","ab9e8e44":"code","8793d275":"code","5a01807c":"code","7646cf5a":"code","55cb67b1":"code","b23a90ed":"code","a9bb9d89":"code","227a9e64":"code","89538f10":"code","1d14e470":"code","038d0f56":"markdown","f34e715e":"markdown","265e45d6":"markdown","3931d8a7":"markdown","23af852d":"markdown","15ef0ccc":"markdown","863bf5a7":"markdown","fcc11c32":"markdown","a164fa61":"markdown","3931be4c":"markdown","bd2d5d8d":"markdown","40c01e9c":"markdown","cb38fa74":"markdown","9be65b29":"markdown","44874d80":"markdown","68c8e0b8":"markdown","7609745c":"markdown","6d1c8b8c":"markdown","24d48b20":"markdown","0798bfa8":"markdown","d36c6a8e":"markdown","fe73b45e":"markdown","be05cf2e":"markdown","641c8492":"markdown","b01b21a4":"markdown","41015dba":"markdown","fb10a91e":"markdown","bbc92415":"markdown","c06cf07e":"markdown","521e7b31":"markdown","fd361c59":"markdown","6ab496da":"markdown","f3d03d04":"markdown","3187bbf5":"markdown","62db7195":"markdown","17b5e613":"markdown","86094197":"markdown","1a3f0879":"markdown","4c3f5de5":"markdown","a6ec2f02":"markdown","e0af0495":"markdown"},"source":{"fe131ac1":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import learning_curve, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.compose import ColumnTransformer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom xgboost import XGBClassifier\nfrom scipy.sparse import hstack\nfrom matplotlib import pyplot as plt\nimport re\nimport string","c7a31743":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","a9f7cb30":"train.target.value_counts(normalize=True)","8ed0f881":"print(f\"Shape of the training dataset {train.shape}\")","ac5b8229":"train.head()","40e477b4":"print(f\"Proportion of NaN keyword values: {train.keyword.isna().sum() \/ len(train)}\")\nprint(f\"Proportion of NaN location values: {train.location.isna().sum() \/ len(train)}\")","72799478":"# Disaster example\ndisaster = train[train.target == 1]['text'].iloc[3]\n\n# Not a disaster example\nnot_disaster = train[train.target == 0]['text'].iloc[3]\n\nprint(f\"Disaster tweet example: {disaster}\")\nprint(f\"NOT a disaster tweet example: {not_disaster}\")","90d51910":"SEED = 42\nFOLDS = 5 # number of folds that we will create in the next steps\nnp.random.seed(SEED)","6a2ab518":"# Looking for urls\ntrain[train.text.str.contains(\"https.\")].head()","8d504c62":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain['text'] = train['text'].apply(lambda x : remove_URL(x))\ntest['text'] = test['text'].apply(lambda x : remove_URL(x))","dcc59dd7":"# Looking for hashtag symbols\ntrain[train.text.str.contains(\"#\")].head()","610f4955":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text'] = train['text'].apply(lambda x : remove_punct(x))\ntest['text'] = test['text'].apply(lambda x : remove_punct(x))","ea8e3cee":"train['text']=train['text'].str.replace('   ', ' ')\ntrain['text']=train['text'].str.replace('     ', ' ')\ntrain['text']=train['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ntrain['text']=train['text'].str.replace('  ', ' ')\ntrain['text']=train['text'].str.replace('\u2014', ' ')\ntrain['text']=train['text'].str.replace('\u2013', ' ')\n\ntest['text']=test['text'].str.replace('   ', ' ')\ntest['text']=test['text'].str.replace('     ', ' ')\ntest['text']=test['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ntest['text']=test['text'].str.replace('  ', ' ')\ntest['text']=test['text'].str.replace('\u2014', ' ')\ntest['text']=test['text'].str.replace('\u2013', ' ')","196b6ea5":"train.fillna(\"99999\", inplace=True)\ntest.fillna(\"99999\", inplace=True)","1c38bed9":"CV = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)","1a44fbd5":"def evaluate_model(y_true, y_pred):    \n    # calculate f1 score\n    f1 = f1_score(y_true, y_pred)    \n    print(f\"Validation F1: {f1}\")\n    print(\"\")\n    return f1","111143e6":"def get_model_scores(X, y, pipeline, cv):\n    scores = cross_val_score(pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\n    return scores\n\ndef print_score_summary(scores):\n    for fold, score in enumerate(scores):\n        print(f\"Fold {fold}:\")\n        print(f\"F1 validation score: {score}\")\n        print(\"\")\n    \n    print(f\"Average F1 validation: {np.mean(scores)}\")","07fd3413":"def plot_learning_curve(estimator, X, y, cv, title):\n    train_sizes, train_scores, valid_scores = learning_curve(estimator,\n                                                             X,\n                                                             y,\n                                                             scoring=\"f1\",\n                                                             cv=cv)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    val_scores_mean = np.mean(valid_scores, axis=1)\n\n    train_scores_std = np.std(train_scores, axis=1)\n    val_scores_std = np.std(valid_scores, axis=1)\n\n    fig, ax = plt.subplots()\n    ax.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n    ax.plot(train_sizes, val_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\")\n    ax.fill_between(\n        train_sizes,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.1,\n        color=\"r\",\n    )\n    ax.fill_between(\n        train_sizes,\n        val_scores_mean - val_scores_std,\n        val_scores_mean + val_scores_std,\n        alpha=0.1,\n        color=\"g\",\n    )\n    ax.set_ylim(0.0, 1.01)\n    ax.legend(loc=\"best\")\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"F1-score\")\n    ax.set_title(f\"{title}\")\n    plt.show()","ab9e8e44":"# create a list to append the steps of the preprossesing pipeline\nsteps_baseline = list()\nsteps_baseline.append(('encoder', OneHotEncoder(handle_unknown='ignore')))\nsteps_baseline.append(('model', LogisticRegression(solver='liblinear')))\n\n# create the pipeline\npipeline_baseline = Pipeline(steps=steps_baseline)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train[['keyword']],\n                          y=train['target'],\n                          pipeline=pipeline_baseline,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_baseline,\n                    X=train[['keyword']],\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - Baseline Model\")\n\nprint_score_summary(scores)","8793d275":"# create a list to append the steps of the preprossesing pipeline\nsteps_tfidf = list()\nvectorizer = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\nsteps_tfidf.append(('tf-idf', vectorizer))\nsteps_tfidf.append(('model', LogisticRegression(solver='liblinear')))\n\n# create the pipeline\npipeline_tfidf = Pipeline(steps=steps_tfidf)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train.text,\n                          y=train['target'],\n                          pipeline=pipeline_tfidf,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_tfidf,\n                    X=train.text,\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - Logistic Regression with TF-IDF\")\n\nprint_score_summary(scores)","5a01807c":"# create a list to append the steps of the preprossesing pipeline\nsteps_countvec = list()\nvectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\nsteps_countvec.append(('count-vectorizer', vectorizer))\nsteps_countvec.append(('model', LogisticRegression(solver='liblinear')))\n\n# create the pipeline\npipeline_countvec = Pipeline(steps=steps_countvec)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train.text,\n                          y=train['target'],\n                          pipeline=pipeline_countvec,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_countvec,\n                    X=train.text,\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - Logistic Regression with CountVectorizer\")\n\nprint_score_summary(scores)","7646cf5a":"# create a list to append the steps of the preprossesing pipeline\nsteps_countvec = list()\nvectorizer = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\nsteps_countvec.append(('count-vectorizer', vectorizer))\nsteps_countvec.append(('model', LogisticRegression(solver='saga',\n                                                   C=0.3, # inverse of the regularization parameter\n                                                   fit_intercept=False,\n                                                   max_iter=800,\n                                                   penalty=\"l2\")))\n\n# create the pipeline\npipeline_countvec = Pipeline(steps=steps_countvec)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train.text,\n                          y=train['target'],\n                          pipeline=pipeline_countvec,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_countvec,\n                    X=train.text,\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - Logistic Regression with CountVectorizer\")\n\nprint_score_summary(scores)","55cb67b1":"# Interface lemma tokenizer from nltk with sklearn\nclass LemmaTokenizer:\n    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]","b23a90ed":"# create a list to append the steps of the preprossesing pipeline\nsteps_exp_4 = list()\n\ncol_transformations = ColumnTransformer(transformers=[\n    (\"encoding\", OneHotEncoder(handle_unknown='ignore'), ['keyword']),\n    ('vectorizarion', CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None), 'text')\n])\nsteps_exp_4.append(('features', col_transformations))\nsteps_exp_4.append(('model', LogisticRegression(solver='saga',\n                                          C=0.3,\n                                          fit_intercept=False,\n                                          max_iter=800,\n                                          penalty=\"l2\",\n                                          random_state=SEED)))\n\npipeline_exp_4 = Pipeline(steps=steps_exp_4)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train[['keyword','text']],\n                          y=train['target'],\n                          pipeline=pipeline_exp_4,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_exp_4,\n                    X=train[['keyword','text']],\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - Logistic Regression with CountVectorizer and Lemmatization\")\n\nprint_score_summary(scores)","a9bb9d89":"# create a list to append the steps of the preprossesing pipeline\nsteps_exp_5 = list()\n\ncol_transformations = ColumnTransformer(transformers=[\n    (\"encoding\", OneHotEncoder(handle_unknown='ignore'), ['keyword']),\n    ('vectorizarion', CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None), 'text')\n])\nsteps_exp_5.append(('features', col_transformations))\nsteps_exp_5.append(('model', svm.LinearSVC(penalty=\"l2\",\n                                     C=0.02,\n                                     random_state=SEED)))\n\npipeline_exp_5 = Pipeline(steps=steps_exp_5)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train[['keyword','text']],\n                          y=train['target'],\n                          pipeline=pipeline_exp_5,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_exp_5,\n                    X=train[['keyword','text']],\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - LinearSVC with CountVectorizer and Lemmatization\")\n\nprint_score_summary(scores)","227a9e64":"# create a list to append the steps of the preprossesing pipeline\nsteps_exp_6 = list()\n\ncol_transformations = ColumnTransformer(transformers=[\n    (\"encoding\", OneHotEncoder(handle_unknown='ignore'), ['keyword']),\n    ('vectorizarion', CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None), 'text')\n])\nsteps_exp_6.append(('features', col_transformations))\nsteps_exp_6.append(('model', XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False, random_state=SEED)))\n\npipeline_exp_6 = Pipeline(steps=steps_exp_6)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train[['keyword','text']],\n                          y=train['target'],\n                          pipeline=pipeline_exp_6,\n                          cv=CV)\n\n# plot the learning curve\nplot_learning_curve(pipeline_exp_6,\n                    X=train[['keyword','text']],\n                    y=train['target'],\n                    cv=CV,\n                    title=\"Learning Curve - XGBoost with CountVectorizer and Lemmatization\")\n\nprint_score_summary(scores)","89538f10":"estimators = [\n    ('lgr', LogisticRegression(solver='saga',\n                               C=0.3,\n                               fit_intercept=False,\n                               max_iter=800,\n                               penalty=\"l2\",\n                               random_state=SEED)),\n    \n    ('svc', svm.LinearSVC(penalty=\"l2\",\n                          C=0.02,\n                          random_state=SEED)),\n    \n    ('xgboost', XGBClassifier(eval_metric=\"logloss\",\n                              use_label_encoder=False,\n                              random_state=SEED))\n]\n\n\n# create a list to append the steps of the preprossesing pipeline\nsteps_ensamble = list()\n\ncol_transformations = ColumnTransformer(transformers=[\n    (\"encoding\", OneHotEncoder(handle_unknown='ignore'), ['keyword']),\n    ('vectorizarion', CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None), 'text')\n])\nsteps_ensamble.append(('features', col_transformations))\nsteps_ensamble.append(('stacking', StackingClassifier(estimators, cv=CV)))\n\npipeline_ensamble = Pipeline(steps=steps_ensamble)\n\n# get model cross-validated scores.\n# using the cross-validation strategy defined\n# at the beggining of the notebook\nscores = get_model_scores(X=train[['keyword','text']],\n                          y=train['target'],\n                          pipeline=pipeline_ensamble,\n                          cv=CV)\n\nprint_score_summary(scores)","1d14e470":"pipeline_ensamble.fit(train[['keyword','text']], train['target'])\npreds = pipeline_ensamble.predict(test[['keyword', 'text']])\n\nsubmission = sample_submission.copy()\nsubmission.target = preds\n\nsubmission.to_csv('stacking_ensameble_lgr_svc_xgboost.csv', index=False)\nsubmission.target.value_counts()","038d0f56":"## Cleaning URLs","f34e715e":"The keyword feature seems like it could be usefull. It contains some NaN values but not that much to be imputed and still add some value to the models.","265e45d6":"# \"Global\" variables","3931d8a7":"# Final Predictions\n\nLet's use this stacked model to make the final predictions on the test set. And submit them to Kaggle.","23af852d":"Nice! We got a decent result for every fold. An average **F1 score of 0.753**. And an improvement from our previuos dummy model. Looks like we are on the right track!\n\nFor the next experiment let's try CountVectorizer instead TF-IDF.","15ef0ccc":"## Experiment 7: Stacking classifiers\n\nIn this experiment we will try to see if stacking our 3 main classifiers:\n* Logistics Regression\n* LinearSVC\n* XGBoost\n\nOutperforms our current benchmark: **F1: 0.764** from experiment 4.\n","863bf5a7":"## Cleaning Hashtag symbols","fcc11c32":"# NLP Distasters Tweets Project\n\nNow days Twitter is an important channel of communication. Thousands of people use it every day to share things from random thoughts, memes, scientific threads to messages about disasters and emergencies happening in real-time somewhere in the world.\n\nBeing able to separate these emergency tweets from the noise is going to be our main goal. We will be dealing with a dataset containing 7613 labeled tweets.\n\nWe will explore different machine learning models and tokenization\/vectorization strategies to find the one that perform the best. Our current best method shows an **F1-score of 0.80723** on the hidden test set provided by Kaggle.\n\n-----\nAuthor: Santiago V\u00edquez Segura\n\nI'm constantly sharing stuff about data science and machine learning on [Twitter](https:\/\/twitter.com\/santiviquez). Follow me if you want to join me and keep learning in public :)","a164fa61":"# Importing all dependencies","3931be4c":"# Experiments\n\nThis is where the fun part beggings. We will explore 7 different experiments. Each one tries to get a better F1 validation score than the last one.\n\nAt the end we will choose our three top performing models and make and esamble.","bd2d5d8d":"# Quick view of the data","40c01e9c":"The `plot_learning_curve` function gets a:\n* X: training test\n* y: target\n* estimator: can be an estimator or pipeline with the preprossesing\n* cv: cross-validation strategy\n* title: string with the title of the plot\n\nAnd plots the train and validation curves.","cb38fa74":"# Helper functions","9be65b29":"## Experiment 3: LogisticRegression + CountVectorizer\nCountVectorizer will transform the text data into numerical features. Each word in the tweets will be transform to a number. This number will be the amount of times it appears in a single tweet.\n\nUsing this simple method we are assuming that words contained in each tweet is a good indicator weather the tweet is about a disaster or not.","44874d80":"## Tweet examples\n\nLet's take a look at some disaster and non disaster tweets.","68c8e0b8":"## Experiment 5: LinearSVC + CountVectorizer + Lemmatization + Keyword column","7609745c":"## Experiment 4: LogisticRegression CountVectorizer + Lemmatization + Keyword column","6d1c8b8c":"The `get_model_scores` function gets a:\n* X: training test\n* y: target\n* pipeline: pipeline with the preprossing and estimator\n* cv: cross-validation strategy\n\nAnd computes the cross validatidated F1 score for each fold K in the cv.\n\nThe `print_score_summary` function gets a list of **f1-scores** and return a formated print with the score for each fold","24d48b20":"# Data Cleaning\nLet's check if our data contains urls, hashtags or weird non important symbols that we would like to remove before our analysis","0798bfa8":"# Evaluation Function\nAgain, since we are dealing with a binary classification problem and the target is not balanced we choose F1-score as our validation metric. During each experiment we will try to get a better F1 validation score.","d36c6a8e":"Look like the logistic regression model is still our benchmark.","fe73b45e":"## Plot Learning Curves","be05cf2e":"So far in the last 3 experiments we have been using only the text data. Let's see if we see any benefit when adding the keyword column and some lemmatization to the text data.","641c8492":"## Experiment 2: Logistic Regression and TF-IDF\n\nTF is term frequencies and IDF is inverse document frequency.","b01b21a4":"Another improvement! Indeed adding lemmatization and the keyword feature helped! And we found a new benchmark. We went from an average F1: 0.755 to an **average F1: 0.764**\n\nLet's explore other model arquitectures with this same preprocesssing pipeline that we have built so far.","41015dba":"Besides the text data we have keyword and location columns. Although it contains some null values","fb10a91e":"I'm constantly sharing stuff about data science and machine learning on [Twitter](https:\/\/twitter.com\/santiviquez). Follow me if you want to join me and keep learning in public :)","bbc92415":"## Experiment 1: Dummy Baseline\nLet's train a logistic regression learner using ONLY the **keyword** column. The purpuse of this model is to build a reference or baseline model. Any other more complex model should be better than this.","c06cf07e":"# Reading files\n\nDuring all the analysis and modeling we will only use the **train.csv** dataset. We will use the **test.csv** only once to make predictions to submit to the [kaggle competition](http:\/\/https:\/\/www.kaggle.com\/c\/nlp-getting-started)","521e7b31":"# Cross validation - Stratified Kfold\nAn important part of building a robust machine learning model is having a robust validation strategy. Since our target is not balanced we'll use stratified kfold to split the data. This way we make sure that each fold has the same 1\/0 distribution.","fd361c59":"It worked! We reduced a bit the gap between the training and the validation curves. And now we have a new benchmark to beat! F1-score of **0.755**\n\nWe are moving on the right direction with this logistic regression model. Let's try adding lemmatization to the text data and incorporating the **keyword** column as a feature","6ab496da":"We got a nice intial result. Using just the column **keyword** we got a better than random! An **F1 score of: 0.672**\n\nLet's see if we can get a better result by using the **text** column and TF-IDF vectorization.","f3d03d04":"## Removing whitespaces","3187bbf5":"We got a good result, however the logistic regressor is still performing better.\n\nFor the next step lets try a model that is quite popular in the Machine Learning community. XGBoost!","62db7195":"One important thing to notice is that the target column is not 100% balanced. This is something that we will take into account when choosing a validation strategy and a validation error metric.","17b5e613":"## Model scores","86094197":"## Working with NaN values\n\nWe will probably use the **keyword** column but it has some NaN values. But we don't want to delete the examples only because the keyword column has a NaN. So we'll fill the NaN fields with a random string. As a way to \"encode\" it.","1a3f0879":"Niiice! We see a small improvement but still an improvement. We have a new benchmark of **F1-score: 0.765**","4c3f5de5":"## Experiment 6: XGBoost + CountVectorizer + Lemmatization + Keyword column","a6ec2f02":"We got great results! An F1 score of 0.80723 on the test set. Even better than the results that we have been having on the validation folds. This is where having a good validation strategy pays off.\n\n![Screen Shot 2022-01-07 at 20.07.08.png](attachment:cb4ea08a-5d53-492f-bf6c-5682e15fc230.png)","e0af0495":"We got almost the same result that with TF-IDF. Although if we look at the learning curve it seems that the model is overfitting.\n\nLet's try adding a higher regularization parameter and increasing the number of iterations."}}