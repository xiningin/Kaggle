{"cell_type":{"de92481e":"code","ae3b5aa2":"code","b6e61039":"code","c722ac46":"code","9fd87452":"code","90cf3f8a":"code","8210b9e2":"code","67161000":"code","051ef06f":"code","cfcaf2fa":"code","37bf074f":"code","e4f1b017":"code","a5bbe88e":"code","5c3b8e30":"code","9e0a82e5":"code","4a4ab1c5":"code","b5b188d8":"code","24d83b68":"code","6192cdcc":"markdown","4f78c05c":"markdown","c906ed27":"markdown","9e6859a4":"markdown","9b9b0e74":"markdown","37b0a201":"markdown","2984b68e":"markdown","3f4fca31":"markdown"},"source":{"de92481e":"from transformers import AutoTokenizer\n\nfrom datasets import load_dataset\n\n# longformer, roberta all use the same tokenizer\n# bert, electra use the same tokenizer\n\ntokenizers = {\n    \"longformer\": AutoTokenizer.from_pretrained(\"allenai\/longformer-base-4096\"),\n    \"bigbird\": AutoTokenizer.from_pretrained(\"google\/bigbird-roberta-base\"),\n    \"albert\": AutoTokenizer.from_pretrained(\"albert-base-v2\"),\n    \"xlnet\": AutoTokenizer.from_pretrained(\"xlnet-base-cased\"),\n    \"electra\": AutoTokenizer.from_pretrained(\"google\/electra-small-discriminator\"), \n    \"deberta\": AutoTokenizer.from_pretrained(\"microsoft\/deberta-base\"), \n}","ae3b5aa2":"from pathlib import Path\nfrom tqdm.notebook import tqdm\n\ntexts, ids = [], []\n\nfor file in tqdm(Path(\"..\/input\/feedback-prize-2021\/train\").glob(\"*.txt\"), total=15594, desc=\"Loading text files from train folder\"):\n    with open(file) as fp:\n        texts.append(fp.read())\n    ids.append(file.stem)\n\nfor file in tqdm(Path(\"..\/input\/feedback-prize-2021\/test\").glob(\"*.txt\"), total=5, desc=\"Loading text files from test folder\"):\n    with open(file) as fp:\n        texts.append(fp.read())\n    ids.append(file.stem)","b6e61039":"from functools import partial\nfrom datasets import Dataset\n\ndef tokenize(examples, tokenizer, name):\n    \n    \n    ids = [tokenizer(text, truncation=False)[\"input_ids\"] for text in examples[\"text\"]]\n    lengths = list(map(len, ids))\n    \n    return {\n        f\"input_ids_{name}\": ids,\n        f\"lengths_{name}\": lengths,\n        \"text\": examples[\"text\"]\n    }\n\nbase_dataset = Dataset.from_dict({\"text\": texts, \"ids\": ids})\n\ndatasets = {}\n\nfor name, tokenizer in tokenizers.items():\n    base_dataset = base_dataset.map(\n        partial(\n            tokenize,\n            tokenizer=tokenizer,\n            name=name\n        ),\n        batched=True,\n        num_proc=4\n    )","c722ac46":"length_df = base_dataset.to_pandas()\nlength_df.head()","9fd87452":"import pandas as pd\nimport plotly.express as px\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n\nlong_df = pd.wide_to_long(length_df, stubnames=\"lengths\", i=\"ids\", j=\"name\", sep=\"_\", suffix=\".*\")[[\"lengths\"]].reset_index()\n\npx.histogram(long_df, x=\"lengths\", color=\"name\")","90cf3f8a":"truncated = long_df[long_df[\"lengths\"]<2000]\npx.histogram(truncated, x=\"lengths\", facet_row=\"name\", height=1000)","8210b9e2":"import numpy as np\nimport pandas as pd\n\nlength_percentiles = pd.DataFrame(columns=[\"length\", \"percentile\", \"model\"])\n\nfor name in tokenizers.keys():\n    column = f\"lengths_{name}\"\n    lengths = length_df[column].values\n    percs = np.linspace(0,1,101)\n    quantile_lengths = np.quantile(lengths, percs)\n    length_percentiles = length_percentiles.append(pd.DataFrame({\"length\": quantile_lengths, \"percentile\": percs, \"model\": [name]*len(percs)}))","67161000":"fig = px.line(\n    length_percentiles, \n    x=\"length\", \n    y=\"percentile\", \n    color=\"model\", \n    title=\"raw texts\",\n    labels={\"percentile\": \"percent texts with tokenized length below length\"}, \n    height=600)\n\nfig.update_xaxes(range=[0, 2000])","051ef06f":"def add_unk_tokens(example, tokenizer, name):\n    unk_id = tokenizer.unk_token_id\n    unk_idxs = [i for i, id_ in enumerate(example[f\"input_ids_{name}\"]) if id_==unk_id]\n    if unk_idxs:\n        example[f\"{name}_unk_tokens\"] = [x for i, x in enumerate(tokenizer.tokenize(example[\"text\"], add_special_tokens=True)) if i in unk_idxs]\n    else:\n        example[f\"{name}_unk_tokens\"] = []\n    example[f\"{name}_num_unk_toks\"] = len(example[f\"{name}_unk_tokens\"])\n    return example\n\nfor name in tokenizers.keys():\n    base_dataset = base_dataset.map(\n        partial(\n            add_unk_tokens,\n            tokenizer=tokenizers[name],\n            name=name\n        ),\n        num_proc=4\n    )","cfcaf2fa":"for name in tokenizers.keys():\n    print(f'Total number of unk tokens ({name}): {sum(base_dataset[f\"{name}_num_unk_toks\"])}')\n    print(f'Average number of unk tokens per text ({name}): {np.mean(base_dataset[f\"{name}_num_unk_toks\"])}')\n    print(f'Median number of unk tokens per text ({name}): {np.median(base_dataset[f\"{name}_num_unk_toks\"])}', \"\\n\")","37bf074f":"from collections import Counter\n\nunk_tokens = {}\nunk_counters = {}\n\nfor name in tokenizers.keys():\n    tkns = []\n    for tokens in base_dataset[f\"{name}_unk_tokens\"]:\n        tkns.extend(tokens)\n    token_string = \"\".join(tkns)\n    unk_tokens[name] = set(token_string)\n    unk_counters[name] = Counter(token_string)","e4f1b017":"for name in unk_counters.keys():\n    print(f\"All unique unk tokens for {name}\", unk_tokens[name])\n    print(f\"Unk token counts for {name}\", unk_counters[name], \"\\n\")","a5bbe88e":"all_chars = list(set(\"\".join(texts)))\n\nnon_alpha = [x for x in all_chars if not x.isalnum()]\nunprintable = [x for x in all_chars if not x.isprintable() and not x.isalnum()]\nwhitespace = [x for x in all_chars if x.isspace()]","5c3b8e30":"for x in [non_alpha, unprintable, whitespace]:\n    print(x, \"\\n\\n\")","9e0a82e5":"all_chars = \"\".join(texts)\n\nnon_alpha = [x for x in all_chars if not x.isalnum()]\nunprintable = [x for x in all_chars if not x.isprintable() and not x.isalnum()]\nwhitespace = [x for x in all_chars if x.isspace()]","4a4ab1c5":"for x in [non_alpha, unprintable, whitespace]:\n    c = Counter(x)\n    print(c.most_common(20), \"\\n\\n\")","b5b188d8":"x82 = []\nfor t in texts:\n    if \"\\x82\" in t:\n        x82.append(t) ","24d83b68":"import random\n\nrandom.sample(x82, 1)[0]","6192cdcc":"# Looking at weird characters","4f78c05c":"# Because there are many different types of tokenizers, this is a simple eda looking at \n### 1. tokenized text length\n### 2. UNK tokens \n\nThis can help guide how long to make your sequence length and what tokens your tokenizer cannot handle. At the end there is also a little glimpse at the context around some of these unk tokens.","c906ed27":"# Tokenized lengths","9e6859a4":"# Looking at a weird character in context","9b9b0e74":"# UNK tokens","37b0a201":"### It looks like `\u00c3\\x82\u00c2\u00b4` should be cleaned to an apostrophe '\n\nMaking this change will likely help the model slightly.","2984b68e":"## Truncate to ignore long tail","3f4fca31":"# Counting weird characters"}}