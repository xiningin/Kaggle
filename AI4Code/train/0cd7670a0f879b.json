{"cell_type":{"311f0b5a":"code","67ce4d08":"code","14437301":"code","d4c748c4":"code","1d75471d":"code","749bdae4":"code","6a26cd4f":"code","40911a7c":"code","b634cfa7":"code","01625c40":"code","6e4d168f":"code","5df39a5b":"code","c89431b5":"code","3fb9ce3d":"code","583eaa1e":"code","2ef75e4c":"markdown","ddd30cf4":"markdown","6477ae82":"markdown","fa6b3294":"markdown","f0868468":"markdown"},"source":{"311f0b5a":"# IMPORTING PACKAGES\n\nimport pandas as pd # data processing\nimport numpy as np # working with arrays\nimport matplotlib.pyplot as plt # visualization\nfrom termcolor import colored as cl # text customization\nimport itertools # advanced tools\n\nfrom sklearn.preprocessing import StandardScaler # data normalization\nfrom sklearn.model_selection import train_test_split # data split\nfrom sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\nfrom sklearn.neighbors import KNeighborsClassifier # KNN algorithm\nfrom sklearn.linear_model import LogisticRegression # Logistic regression algorithm\nfrom sklearn.svm import SVC # SVM algorithm\nfrom sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\nfrom xgboost import XGBClassifier # XGBoost algorithm\n\nfrom sklearn.metrics import confusion_matrix # evaluation metric\nfrom sklearn.metrics import accuracy_score # evaluation metric\nfrom sklearn.metrics import f1_score # evaluation metric","67ce4d08":"# IMPORTING DATA\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.drop('Time', axis = 1, inplace = True)\n\nprint(df.head())","14437301":"cases = len(df)\nnonfraud_count = len(df[df.Class == 0])\nfraud_count = len(df[df.Class == 1])\nfraud_percentage = round(fraud_count\/nonfraud_count*100, 2)\n\nprint(cl('CASE COUNT', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('Total number of cases are {}'.format(cases), attrs = ['bold']))\nprint(cl('Number of Non-fraud cases are {}'.format(nonfraud_count), attrs = ['bold']))\nprint(cl('Number of Non-fraud cases are {}'.format(fraud_count), attrs = ['bold']))\nprint(cl('Percentage of fraud cases is {}'.format(fraud_percentage), attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))","d4c748c4":"nonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]\n\nprint(cl('CASE AMOUNT STATISTICS', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('NON-FRAUD CASE AMOUNT STATS', attrs = ['bold']))\nprint(nonfraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('FRAUD CASE AMOUNT STATS', attrs = ['bold']))\nprint(fraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))","1d75471d":"sc = StandardScaler()\namount = df['Amount'].values\n\ndf['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n\nprint(cl(df['Amount'].head(10), attrs = ['bold']))","749bdae4":"# DATA SPLIT\n\nX = df.drop('Class', axis = 1).values\ny = df['Class'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nprint(cl('X_train samples : ', attrs = ['bold']), X_train[:1])\nprint(cl('X_test samples : ', attrs = ['bold']), X_test[0:1])\nprint(cl('y_train samples : ', attrs = ['bold']), y_train[0:20])\nprint(cl('y_test samples : ', attrs = ['bold']), y_test[0:20])","6a26cd4f":"# Decision Tree\ntree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\ntree_model.fit(X_train, y_train)\ntree_yhat = tree_model.predict(X_test)","40911a7c":"# K-Nearest Neighbors\n\nn = 5\n\nknn = KNeighborsClassifier(n_neighbors = n)\nknn.fit(X_train, y_train)\nknn_yhat = knn.predict(X_test)","b634cfa7":"# Logistic Regression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_yhat = lr.predict(X_test)","01625c40":"# SVM \n\nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_yhat = svm.predict(X_test)","6e4d168f":"# Random Forest Tree\n\nrf = RandomForestClassifier(max_depth = 4)\nrf.fit(X_train, y_train)\nrf_yhat = rf.predict(X_test)","5df39a5b":"# XGBoost\n\nxgb = XGBClassifier(max_depth = 4)\nxgb.fit(X_train, y_train)\nxgb_yhat = xgb.predict(X_test)","c89431b5":"print(cl('ACCURACY SCORE', attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test, svm_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))","3fb9ce3d":"print(cl('F1 SCORE', attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, tree_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the KNN model is {}'.format(f1_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the SVM model is {}'.format(f1_score(y_test, svm_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test, rf_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the XGBoost model is {}'.format(f1_score(y_test, xgb_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))","583eaa1e":"# defining the plot function\n\ndef plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues):\n    title = 'Confusion Matrix of {}'.format(title)\n    if normalize:\n        cm = cm.astype(float) \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix for the models\n\ntree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree\nknn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors\nlr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression\nsvm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine\nrf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree\nxgb_matrix = confusion_matrix(y_test, xgb_yhat, labels = [0, 1]) # XGBoost\n\n# Plot the confusion matrix\n\nplt.rcParams['figure.figsize'] = (6, 6)\n\n# 1. Decision tree\n\ntree_cm_plot = plot_confusion_matrix(tree_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Decision Tree')\nplt.savefig('tree_cm_plot.png')\nplt.show()\n\n# 2. K-Nearest Neighbors\n\nknn_cm_plot = plot_confusion_matrix(knn_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'KNN')\nplt.savefig('knn_cm_plot.png')\nplt.show()\n\n# 3. Logistic regression\n\nlr_cm_plot = plot_confusion_matrix(lr_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Logistic Regression')\nplt.savefig('lr_cm_plot.png')\nplt.show()\n\n# 4. Support Vector Machine\n\nsvm_cm_plot = plot_confusion_matrix(svm_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'SVM')\nplt.savefig('svm_cm_plot.png')\nplt.show()\n\n# 5. Random forest tree\n\nrf_cm_plot = plot_confusion_matrix(rf_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Random Forest Tree')\nplt.savefig('rf_cm_plot.png')\nplt.show()\n\n# 6. XGBoost\n\nxgb_cm_plot = plot_confusion_matrix(xgb_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'XGBoost')\nplt.savefig('xgb_cm_plot.png')\nplt.show()","2ef75e4c":"### F1 Score","ddd30cf4":"#### F1 score = 2( (precision * recall) \/ (precision + recall) )","6477ae82":"### Accuracy score","fa6b3294":"####                      Accuracy score = No.of correct predictions \/ Total no.of predictions","f0868468":"### Confusion Matrix"}}