{"cell_type":{"39cccadd":"code","7a8bb6c3":"code","ba904605":"code","a11bff55":"code","96183da6":"code","e5c944aa":"code","7ccac7d4":"code","7b650286":"code","e9f7c838":"code","59653ae2":"markdown","db6f23b0":"markdown","eaafbdd6":"markdown","f61dca64":"markdown","9adcfc8f":"markdown","82aadd73":"markdown","6f309e75":"markdown","63bce915":"markdown","8d265e08":"markdown","1bdebf38":"markdown","1204df27":"markdown"},"source":{"39cccadd":"import numpy as np\nimport matplotlib.pyplot as plt","7a8bb6c3":"# number of input data, you can change this and see how it effects all the result!\nN = 50 \nnp.random.seed(20)\nx_train = np.sort(np.random.rand(N,1),axis=0)\nnoise = np.random.normal(0,0.3,size=(N,1))\nt_train = np.sin(2*np.pi*x_train) + noise\n\n# function to generate plot with different size\ndef vis_input_data(N,wit,hig):\n    np.random.seed(20)\n    plt.figure(figsize=(wit,hig))\n    x_train = np.sort(np.random.rand(N,1),axis=0)\n    noise = np.random.normal(0,0.3,size=(N,1))\n    t_train = np.sin(2*np.pi*x_train) + noise\n\n    plt.scatter(x_train,t_train,c='b',marker='o',label='noise added input data')\n    plt.plot(np.linspace(0,1,50),np.sin(2*np.linspace(0,1,50)*np.pi),c='g',linewidth=2,label='function generates input data')\n    plt.title('number of training data N =' + str(N)) \n    plt.xlabel('x');plt.ylabel('t')\n    #plt.show()\n    \nvis_input_data(N,6,5)\nplt.legend()\nplt.show()","ba904605":"#ridge regression added, adjust lamda to make it work, if lamda = 0 becomes traditional\n# LS with no regularization\nlamda = 0.0  # ridge regression penalty term\npoly_deg = 3\n\nQ_train = np.zeros(shape = (N,poly_deg+1))\nQ_train[:,0] = 1\nfor i in range(1,poly_deg+1):\n    Q_train[:,i] = np.power(x_train,i).reshape((N,))    \n\nW = np.linalg.pinv((Q_train.T.dot(Q_train) + lamda*np.eye(poly_deg+1))).dot(Q_train.T).dot(t_train)\n\nvis_input_data(N,6,5)\nplt.plot(x_train,Q_train.dot(W),'r',label='poly fit degree =' + str(poly_deg))\nplt.legend()\nplt.show()","a11bff55":"def poly_fit(x_train,t_train,lam,polyfit_deg):\n    \n    Q_train = np.zeros(shape = (len(x_train),polyfit_deg+1))\n    Q_train[:,0] = 1\n    for i in range(1,polyfit_deg+1):\n        Q_train[:,i] = np.power(x_train,i).reshape((len(x_train),))\n    W = np.linalg.pinv((Q_train.T.dot(Q_train) + lam*np.eye(polyfit_deg+1))).dot(Q_train.T).dot(t_train)\n    J_cos = 0.5*(Q_train.dot(W)-t_train).T.dot(Q_train.dot(W)-t_train)\n    E_rms = np.sqrt(J_cos\/len(x_train))\n    return W,E_rms,Q_train","96183da6":"# plot roor-mean-square error\nerr = np.zeros((10,1))\nfor pol_deg in range(10):\n    w,er,q_train = poly_fit(x_train,t_train,0,pol_deg)\n    err[pol_deg]=er\n    \nplt.plot(err)\nplt.xlabel('polynomial degree');plt.ylabel('RMSE')\nplt.show()","e5c944aa":"from sklearn.preprocessing import PolynomialFeatures\nsk_poly_deg=3\npoly_feature = PolynomialFeatures(degree=sk_poly_deg,include_bias=False)\nx_poly = poly_feature.fit_transform(x_train)\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg=LinearRegression()\nlin_reg.fit(x_poly,t_train)\n\nvis_input_data(N,6,5)\nplt.plot(x_train,lin_reg.predict(x_poly),'r',label='scikit poly fit degree =' + str(sk_poly_deg))\nplt.legend()\nplt.show()\nprint('scikit_learn input transformed matrix:',x_poly)\nprint('scikit_learn weight vectors:',lin_reg.intercept_,lin_reg.coef_)\n\nvis_input_data(N,6,5)\nplt.plot(x_train,Q_train.dot(W),'r',label='my implementaion poly fit degree =' + str(sk_poly_deg))\nplt.legend()\nplt.show()\nprint('my implementation input transformed matrix:',Q_train)\nprint('my implementation weight vectors:',W)","7ccac7d4":"vis_input_data(N,7,6)\ndeg_1=2;deg_2=3;deg_3=40\nw,er,q_train = poly_fit(x_train,t_train,0,deg_1)\nplt.plot(x_train,q_train.dot(w),'y',label='poly fit degree =' + str(deg_1))\nw,er,q_train = poly_fit(x_train,t_train,0,deg_2)\nplt.plot(x_train,q_train.dot(w),'r',label='poly fit degree =' + str(deg_2))\nw,er,q_train = poly_fit(x_train,t_train,0,deg_3)\nplt.plot(x_train,q_train.dot(w),'k',label='poly fit degree =' + str(deg_3))\nplt.legend()\nplt.show()\n","7b650286":"vis_input_data(N,8,6)\ndeg_4 = 60;\nw,er,q_train = poly_fit(x_train,t_train,0,deg_4)\nplt.plot(x_train,q_train.dot(w),'y',label='poly fit degree =' + str(deg_4) + ' without regularization')\nw,er,q_train = poly_fit(x_train,t_train,0.02,deg_4)\nplt.plot(x_train,q_train.dot(w),'r',label='poly fit degree =' + str(deg_4) + ' with regularization')\nplt.legend()\nplt.show()","e9f7c838":"from sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\n\nvis_input_data(N,8,6)\n\nmodel = make_pipeline(PolynomialFeatures(deg_4),Ridge(alpha=0.02,solver='cholesky'))\nmodel.fit(x_train,t_train)\nplt.plot(x_train,model.predict(x_train),'r',label='sklearn poly fit degree =' + str(deg_4) + ' with regularization')\n\nmodel = make_pipeline(PolynomialFeatures(deg_4),Ridge(alpha=0.00,solver='cholesky'))\nmodel.fit(x_train,t_train)\nplt.plot(x_train,model.predict(x_train),'y',label='sklearn  fit degree =' + str(deg_4) + ' without regularization')\nplt.legend()\nplt.show()","59653ae2":"Here I am showing root-mean-square error(RMSE) vs polynomial increase, as you can see if one increases polynomial degree( more complex model) the RMSE will decrease.\nBut is it always good? If not, we will overfit our data!","db6f23b0":"Lets bring in scikit-learn to compare with my implementaion.","eaafbdd6":"**           Implementing regularized (Ridge Regression) polynomial regression algorithms  from scracth!\n**\n\nQuick outline:\n\n* Generate synthetic data\n* Implement ridge polynomial regression from scrath\n* Compare with Scikit-learn solution \n* Example of overfitting \n* Effect of penalty term\n* Scikit-learn Ridge regression","f61dca64":"How do we handle overfit data? One way is using regularization.\nBelow is using lam 0 and 0.02 to 60th degree poly fit. By adding regularization value of 0.02 , model performs better.","9adcfc8f":"Let me know if you want to see other function implementations!","82aadd73":"Below, I compare my implementation with scikit-learn. By looking at weight vectors and input matrix, both gives identical results.\nNote: Q_train in my code is same as fit transformed data from scikit-learn preprocessing method.","6f309e75":"Make my own polynomial fit function using code above.","63bce915":"Let us compare models with different polynomial degree.\n* Yellow curve is second degree polynomial which is underfitting data\n* Red curve is third degree polynomial which is correctfitting data\n* Black curev is overfit 40th degree polynomial which is overfitting data","8d265e08":"First lets generate synthetic data fron sin() function and add gaussian noise to it.","1bdebf38":"Now I used sklern Ridge() function to do the same task, and results are the same. \nIn sklearn alpha is regularization term same as lam in my implementaion. ","1204df27":"This my own implementation using closed form solution. I implemented gradient descent in earlier kernel and it's similar. \nThe main task here is to generate input matrix (Q_train here) then the rest is same as linear regression. \nAnother feature I added here is penalty term (lam here) which takes role of regularization! "}}