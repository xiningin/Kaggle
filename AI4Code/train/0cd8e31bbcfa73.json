{"cell_type":{"10ca7dba":"code","bc888d29":"code","cead6a63":"code","cc4ac933":"code","3b5c6142":"code","f38f1db0":"code","18b06c1e":"code","05a98df6":"code","75082c51":"code","b6ff98b9":"code","2b8dba02":"code","08c3fe35":"code","d96f6ef8":"code","571d66c5":"code","b6a8c791":"markdown","b216b2f7":"markdown","eb018184":"markdown","1d9f87bb":"markdown","c3c06e69":"markdown","861d247d":"markdown"},"source":{"10ca7dba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc888d29":"import matplotlib.pyplot as plt\n\n%matplotlib inline","cead6a63":"file_path = '\/kaggle\/input\/stress-analysis-in-social-media\/'\n\nos.listdir(file_path)\n\ntrain, test = [pd.read_csv(file_path + 'dreaddit-train.csv'), pd.read_csv(file_path + 'dreaddit-test.csv')]\n\n","cc4ac933":"cols = []\nfor x in train: cols.append(x)\nprint(cols)","3b5c6142":"subreddit_mean = pd.DataFrame(columns=['subreddit'])\nfor x in train.subreddit.unique():    \n    values = train.loc[train.subreddit == x, 'sentiment']\n    mean = {'subreddit': x, 'mean': np.mean(values), 'sum':  np.sum(values)}\n    subreddit_mean = subreddit_mean.append(mean, ignore_index=True)\n\nsubreddit_mean.sort_values('mean', ascending=False, ignore_index=True)","f38f1db0":"train_x = train.text\ntrain_y = train.subreddit\n\ntest_x = test.text\ntest_y = test.subreddit\n\ntrain_x[0], train_y[0]","18b06c1e":"from sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re","05a98df6":"train_x_tk = [word_tokenize(x) for x in train_x]\n\nprint(train_x_tk[0])","75082c51":"def lem_def_tokens(tokens):\n    lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer()\n\n    processed_tokens = []\n    for item in train_x_tk:\n        item_lem = [lemmatizer.lemmatize(x) for x in item]\n        item_stem = ' '.join([stemmer.stem(x) for x in item_lem])\n        item_str = re.sub(r'\\d', '', item_stem)\n        processed_tokens.append(item_str)\n        \n    return processed_tokens","b6ff98b9":"train_x_lem = lem_def_tokens(train_x_tk)\n\ntrain_x_lem[0]","2b8dba02":"vectorizer = CountVectorizer()\n\ntrain_x_vectors = vectorizer.fit_transform(train_x_lem)\n\nprint(vectorizer.get_feature_names()[25:30], '...')\nprint(train_x_vectors.toarray()[0, 25:30], '...')","08c3fe35":"clf = SVC(kernel='linear')\n\nclf.fit(train_x_vectors, train_y)","d96f6ef8":"print(test_y[2], test_x[2])\n\ntest_x_tokens = [word_tokenize(x) for x in test_x]\ntest_x_lem = lem_def_tokens(test_x_tokens)\ntest_x_vectors = vectorizer.transform(test_x_lem)\nprint(len(test_x_tokens), len(test_x_lem))\nclf.score(test_x_vectors, test_y)","571d66c5":"train_x_vectors","b6a8c791":"## *Tokenize*","b216b2f7":"## *Train SVM*","eb018184":"## *Filter Data*","1d9f87bb":"## *Lemmatize & Stem Tokens*","c3c06e69":"# *Cleaning and Vectorization*","861d247d":"## *Vectorize*"}}