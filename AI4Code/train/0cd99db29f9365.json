{"cell_type":{"e1c4cc0f":"code","f86fa0c8":"code","5b8c886d":"code","9b37c8db":"code","d302a2fb":"code","dbda599f":"code","88ea7c7b":"code","c92c57f7":"code","fde53716":"code","c0b31602":"code","6c06b25b":"code","f631686d":"code","4cbd6174":"code","633d62c0":"code","c948bc79":"code","b87ab1e0":"code","a1a2ad5e":"code","aa461018":"code","38001c13":"code","8acf4c3a":"code","b8ede14c":"code","8c789473":"code","9eba9a62":"code","c3d92c9f":"code","d1f6c66b":"code","733cbc11":"code","e7eb220b":"code","1e44211d":"code","0972d024":"code","a85cf11b":"code","71dff00a":"code","9abab16c":"code","27e7333d":"code","4c958291":"code","c06a8171":"code","4bebf9ad":"code","be1b467c":"code","a0cd75df":"code","65f8c7e4":"code","0db8f92d":"code","bd4a9238":"markdown","85fe37f6":"markdown","0e2f074c":"markdown","723cdb1b":"markdown","6cfb699c":"markdown","ec70bbd8":"markdown","05474471":"markdown","99c3a6a4":"markdown","d1961017":"markdown","a243656e":"markdown","1bb8c3ad":"markdown","eaf3e382":"markdown","a41c67b4":"markdown","ea4a2fcc":"markdown","bac3d654":"markdown","887ec0df":"markdown","5e93d377":"markdown","6d8cdf63":"markdown","0c4a72a1":"markdown","8e766129":"markdown","f8dc6aca":"markdown","c763147f":"markdown","1f78612a":"markdown","14e1c5bc":"markdown","492e197e":"markdown","d09301d6":"markdown","f1a24793":"markdown"},"source":{"e1c4cc0f":"#Libraries\n\n# Data Manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nimport category_encoders as ce\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f86fa0c8":"# List all files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5b8c886d":"# Load dataset\nraw_train = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/train.csv')\nraw_test = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/test.csv')\nraw_combined = [raw_train, raw_test]\nprint('train', raw_train.shape)\nprint('test', raw_test.shape)\n\npd.set_option('display.max_columns', None)\n\n# Test data doesn't contain the \"churn\" feature but has the same number of features as it contains \"id\" feature which will be required for submission.","9b37c8db":"raw_train.describe()","d302a2fb":"raw_train.describe(include='O')","dbda599f":"raw_train.head(20)","88ea7c7b":"raw_train.columns","c92c57f7":"raw_train.isnull().sum()","fde53716":"raw_test.isnull().sum()","c0b31602":"# Are there any customers which had voice message but didn't have a voice mail plan?\nraw_train[raw_train['number_vmail_messages'] > 0]['voice_mail_plan'].unique()\n\n# Answer: No customer","6c06b25b":"# Are there any customers which made international calls without an internation plan?\n# Similar as for voice messages\n\nprint(raw_train[raw_train['total_intl_calls'] > 0]['international_plan'].unique())\n\n# Answer: Yes, there were customers which didn't have an international plan, but made international calls\n\nnum_cust_intl_calls_without_plan = len(raw_train[(raw_train['total_intl_calls'] > 0) & (raw_train['international_plan'] == 'no')])\nnum_cust_made_intl_calls = len(raw_train[raw_train['total_intl_calls'] > 0])\nprint('Percentage of customers which made international calls but didn\\'t have a plan: {:.2f}%'.format(num_cust_intl_calls_without_plan \/ num_cust_made_intl_calls * 100))","f631686d":"sns.histplot(raw_train['total_intl_calls'], bins=10)","4cbd6174":"# Converting Binary features\nfor dataset in raw_combined:\n    dataset.international_plan.replace(['no', 'yes'], [0, 1], inplace=True)\n    dataset.voice_mail_plan.replace(['no', 'yes'], [0, 1], inplace=True)\n\n# Churn feature in train\nraw_train.churn.replace(['no', 'yes'], [0, 1], inplace=True)\n\nraw_train.head()\n# raw_test.head()","633d62c0":"# Converting Nominal feature\nohe = OneHotEncoder()\nohe.fit(raw_train[['area_code']])\n\n# Train\nencoded_values = ohe.transform(raw_train[['area_code']])\nraw_train[ohe.categories_[0]] = encoded_values.toarray()\nraw_train = raw_train.drop('area_code', axis=1)\n\n# Test\nencoded_values = ohe.transform(raw_test[['area_code']])\nraw_test[ohe.categories_[0]] = encoded_values.toarray()\nraw_test = raw_test.drop('area_code', axis=1)\n\nraw_train.head()","c948bc79":"raw_test.head()","b87ab1e0":"# account_length\nprint(raw_train['account_length'].unique())\n\nsns.histplot(raw_train['account_length'], bins=10)\n# Most of the customers had the service for between 50 to 150 months","a1a2ad5e":"# It might be worth to bin the account_length feature and create a new feature\n# Check correlation of bins to churn value\n\n# raw_train['account_length_bin'] = pd.cut(raw_train['account_length'], 8)\n# raw_train[['account_length_bin', 'churn']].groupby('account_length_bin').sum().sort_values(by=\"churn\", ascending=False)\n\n# Not a very good correlation, but we can use this feature later to see if it helps. TODO","aa461018":"raw_train.head(10)","38001c13":"raw_train.shape","8acf4c3a":"# raw_train = raw_train.drop('state', axis=1)\nce_hashing = ce.HashingEncoder(cols='state')\nraw_train = ce_hashing.fit_transform(raw_train)\nprint(raw_train.shape)\nraw_train.head()","b8ede14c":"raw_test = ce_hashing.transform(raw_test)\nprint(raw_test.shape)\nraw_test.head()","8c789473":"raw_train.dtypes","9eba9a62":"# Divide dataset into independent and dependent features\n\n# Moving the dependent column to the end\nraw_train_cols = raw_train.columns.tolist()\nprint(raw_train_cols)\nprint(len(raw_train_cols))\nraw_train_cols = raw_train_cols[:-4] + raw_train_cols[-3:] + raw_train_cols[-4:-3]\nprint(len(raw_train_cols))\nprint(raw_train_cols)\n\nraw_train = raw_train[raw_train_cols]\nraw_train.head()\nraw_train.shape\n\n# Divide datset\nraw_train_X = raw_train.iloc[:, :-1]\nraw_train_y = raw_train.iloc[:, -1]\nraw_train_X.shape\nraw_train_y.shape","c3d92c9f":"raw_train_y.head()","d1f6c66b":"X_train, X_val, y_train, y_val = train_test_split(raw_train_X, raw_train_y, test_size=0.3)","733cbc11":"X_train.head()","e7eb220b":"# So let's first rearrange the columns (move all columns to be normalized at the end)\n\nX_train_cols = X_train.columns.tolist()\nprint(len(X_train_cols))\nX_train_cols = X_train_cols[:8] + X_train_cols[-3:] + X_train_cols[8:-3]\nprint(len(X_train_cols))\n\nX_train = X_train[X_train_cols]\nX_val = X_val[X_train_cols]\n\nX_val.head()","1e44211d":"# Normalizing features only from index 11 till the end\n\nnormalizer = MinMaxScaler()\nnormalizer.fit(X_train.iloc[:, 11:])\n\nselected_cols_names = X_train.columns.tolist()[11:]\nprint(selected_cols_names)\n\nprint(X_train.shape)\n\nX_train_norm_selected = pd.DataFrame(normalizer.transform(X_train.iloc[:, 11:]), columns=selected_cols_names)\nX_val_norm_selected = pd.DataFrame(normalizer.transform(X_val.iloc[:, 11:]), columns=selected_cols_names)\n\n# Joining normalized and left our columns\n# We need to reset indexes, else if pandas is unable to match indexes, will result in NaN values \nX_train.reset_index(drop=True, inplace=True) \nX_train_norm_selected.reset_index(drop=True, inplace=True)\n\n# Similar index reset for validation dataset\nX_val.reset_index(drop=True, inplace=True) \nX_val_norm_selected.reset_index(drop=True, inplace=True)\n\nX_train_norm = pd.concat([X_train.iloc[:, :11], X_train_norm_selected], axis=1)\nX_val_norm = pd.concat([X_val.iloc[:, :11], X_val_norm_selected], axis=1)","0972d024":"X_train_norm.head()","a85cf11b":"X_val_norm.head()","71dff00a":"# Normalize test data\n\n# Remove 'id' column, but before that save 'id' feature for submission\nraw_test_ids = raw_test['id']\nraw_test = raw_test.drop('id', axis=1)","9abab16c":"# Similar re-arranging of columns for test data.\n# We can re-use the order of training columns as it's same as test\nraw_test = raw_test[X_train_cols]\nprint(raw_test.shape)\nraw_test.head()","27e7333d":"raw_test_norm_selected = pd.DataFrame(normalizer.transform(raw_test.iloc[:, 11:]), columns=selected_cols_names)\n\n# Concat all test data columns\n# Reset indexes\nraw_test.reset_index(drop=True, inplace=True) \nraw_test_norm_selected.reset_index(drop=True, inplace=True)\n\nraw_test_norm = pd.concat([raw_test.iloc[:, :11], raw_test_norm_selected], axis=1)\nraw_test_norm.head()","4c958291":"model_lr = LogisticRegression(solver='liblinear')\nmodel_lr.fit(X_train_norm, y_train)\ny_pred = model_lr.predict(X_val_norm)\n\n# Accuracy\nlr_accuracy = accuracy_score(y_val, y_pred)\nlr_f1_score = f1_score(y_val, y_pred)\nprint('LogisticRegression - accuracy score: {} and f1_score: {}'.format(lr_accuracy, lr_f1_score))\n\npd.DataFrame(y_pred)[0].head(20)","c06a8171":"pd.DataFrame(y_val).head(20)","4bebf9ad":"model_svc = SVC()\nmodel_svc.fit(X_train_norm, y_train)\ny_pred = model_lr.predict(X_val_norm)\n\n# Accuracy\nsvc_accuracy = accuracy_score(y_val, y_pred)\nsvc_f1_score = f1_score(y_val, y_pred)\nprint('SupportVector - accuracy score: {} and f1_score: {}'.format(svc_accuracy, svc_f1_score))","be1b467c":"model_rfc = RandomForestClassifier()\nmodel_rfc.fit(X_train_norm, y_train)\ny_pred = model_rfc.predict(X_val_norm)\n\n# Accuracy\nrfc_accuracy = accuracy_score(y_val, y_pred)\nrfc_f1_score = f1_score(y_val, y_pred)\nprint('RandomForestClassifier - accuracy score: {} and f1_score: {}'.format(rfc_accuracy, rfc_f1_score))","a0cd75df":"# Predict on Test data using RandomForestClassifier\ntest_pred = model_rfc.predict(raw_test_norm)\n\n# Check frequency of output values\nelements, count = np.unique(test_pred, return_counts=True)\n(elements, count)","65f8c7e4":"# Submission features\nsubmission_df = pd.DataFrame({'id': raw_test_ids, 'churn': test_pred})\n\n# As required, replace predicted values by 'yes'\/'no'\nsubmission_df['churn'].replace((0, 1), ('no', 'yes'), inplace=True)\n\nprint(submission_df.shape)\nsubmission_df.head()","0db8f92d":"# Create submission file\nsubmission_df.to_csv('submission6.csv', index=False)","bd4a9238":"#### 2. Creating new features","85fe37f6":"#### 2. Check for missing values","0e2f074c":"#### TODO list\nThese are the tasks we would like to perform:\n1. Were there instances where customer didn't have voice mail plan, but had voice mail messages? - DONE - Answer is No (see below).\n2. Similar scenario as point 1 for international calls. - DONE - Answer is 90% people made international calls without a plan! (see below).\n3. Check correlation between features\n4. Check outliers","723cdb1b":"Here we've used classic algorithms without hyperparameter tuning and without ensemble or boosting algorithms.\nNext steps:\n1. Hyperparamter tuning\n2. Apply Ensmble techniques\n3. Boosting","6cfb699c":"#### 1. Types of Features","ec70bbd8":"### E. Submissions score\nCompetition has ended, so submitted the CSV file on Leaderboard for score.  \n\n### Final score on Test data: 0.96888","05474471":"#### 3. Encode 'state' feature\nWe can use OneHotEncoder, but that would not be a good solution as this is a high cardinality feature (i.e. high number of unique values - 51).  \nWe can use other methods such as Hashing for encoding this feature.","99c3a6a4":"Segregation by looking at the data above:\n\n- Categorical  \n'state' - Nominal  \n'area_code' - Nominal  \n'international_plan' - Binary  \n'voice_mail_plan' - Binary  \n\n- Continuous  \n'account_length'  \n'number_vmail_messages'  \n'total_day_minutes'  \n'total_day_calls'  \n'total_day_charge'  \n'total_eve_minutes'  \n'total_eve_calls'  \n'total_eve_charge'  \n'total_night_minutes'  \n'total_night_calls'  \n'total_night_charge'  \n'total_intl_minutes'  \n'total_intl_calls'  \n'total_intl_charge'  \n'number_customer_service_calls'  ","d1961017":"Ideas:\n1. Encode binary features - international_plan, voice_mail_plan - DONE\n2. Encode nominal features - area_code - DONE\n3. Is it a good idea to encode 'state' feature? - has 51 unique values!\n4. Bin feature values - account_length, number_vmail_messages, number_customer_service_calls (see if this would be beneficial as it already has less unique values)","a243656e":"### A. Data Exploration","1bb8c3ad":"### D. Submission","eaf3e382":"No missing values, great!","a41c67b4":"We are creating validation dataset so that we can evaluate model performance before submission.","ea4a2fcc":"#### 1. Encoding categorical features","bac3d654":"**Note:** We'll fit the normalizer on training data and then transform training, validation and test datasets.  \nThis is important to avoid data leakage.","887ec0df":"### C. Data Modeling","5e93d377":"For now, RandomForestClassifier seems to be performing the best.","6d8cdf63":"Most of the customer made 2-6 calls in total in their whole subscription period.  \nThis explains the high 91% of customers making international calls without a plan.","0c4a72a1":"We only want to normalize data from indices 8 to -4, as the other column values are already between 0 an 1.  \nBut, even if we normalize all the columns including the ones which are already between 0 an 1, MinMaxScaler won't have an effect on them.  \n(As MinMaxScaler scales values to be between 0 and 1)  \n\nTaking the long approach below to only normalize specific columns, which is not required here, but doing it for practice.","8e766129":"#### 6. Normalizing data","f8dc6aca":"Training, Validation, Test - All datasets have been normalized!","c763147f":"Seems like the telecom provider didn't advertize its international call plan effectively.  \n91% of the customers calling internationally didn't take the plan.  \nBut, it might be possible that most customers made international calls rarely, which would explain the no-plan scenario. Let's check the distribution of international call minutes.","1f78612a":"### B. Data Wrangling","14e1c5bc":"#### 5. Get Validation dataset","492e197e":"Initial observations about the training data:\n1. Account length varies from 1 to 243 months (~ 20 years).\n2. 91% customers didn't have an international plan.\n3. 74% customer didn't opt for voice mail plan.\n4. 86% customers in the training data didn't change their provider (i.e. no churning).\n5. Unique values for area_code - Single area code covers multiple states.\n6. Most of the features seems to be having no missing values, although we will check that later for all the features.\n7. Average number of call minutes for day, evening, and night are comparable.\n8. Even though number of minutes are similar, average charges are different and in decreasing order of day > evening > night (least for night calls).\n9. International call minutes is very less compared to local calls. ","d09301d6":"#### 4. Data leakage\nThe idea of Data leakage doesn't apply to this notebook as we didn't perform any imputation tasks above.  \nWe only performed encoding which doesn't lead to data leakage (my understanding).","f1a24793":"#### 3. Check individual features"}}