{"cell_type":{"b885302c":"code","c8e8ca00":"code","d1d41670":"code","a7e62a0c":"code","12214b91":"code","b7073a5f":"code","c4c78c4e":"code","df83b6dd":"code","4aff7e6b":"code","de81f1e8":"code","38c88562":"code","5c087972":"code","33589e35":"code","533bf741":"code","3531d81c":"code","d6979629":"code","8557f22f":"code","6e9a75c5":"code","40c770ad":"code","e951acf9":"code","4525f0db":"code","b4ca14c9":"code","01d8c0c4":"code","69309ea5":"code","502b3286":"code","949a1ee2":"code","bd0cb3a2":"code","0384824b":"code","def8e286":"code","728452b2":"code","dbca2c9e":"code","75968898":"code","97e00e60":"code","fa8169ab":"code","7c9be68d":"code","144c868d":"code","a465b3b1":"code","e6c859b2":"code","8e8a7244":"code","b93a8a79":"code","1c8959b8":"code","5e6b3001":"code","41c95f4f":"code","9bf35978":"code","1e83f91b":"code","c30b9336":"code","018e400f":"code","30a9f630":"code","4bf64866":"code","cf934b32":"code","2e50b9bc":"code","53559373":"code","9cea755d":"code","ea848837":"code","3e0df4c7":"code","290d7905":"markdown","6c41ad5e":"markdown","aa876dd4":"markdown","5f568cd7":"markdown","96c621e2":"markdown","eb134d76":"markdown","902865a6":"markdown","89f2a4be":"markdown","25ac955a":"markdown","85fe019f":"markdown","7962ae77":"markdown","5fb4d3bd":"markdown","3297bba6":"markdown","39f9594c":"markdown","8906747c":"markdown"},"source":{"b885302c":"from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n%matplotlib inline","c8e8ca00":"boston_dataset = load_boston() ","d1d41670":"type(boston_dataset)","a7e62a0c":"dir(boston_dataset)","12214b91":"print(boston_dataset.DESCR)","b7073a5f":"type(boston_dataset.data)","c4c78c4e":"boston_dataset.data.shape","df83b6dd":"boston_dataset.feature_names","4aff7e6b":"# Actual prices in thousands\nboston_dataset.target[:5]","de81f1e8":"data = pd.DataFrame(data=boston_dataset.data, columns=boston_dataset.feature_names)\ndata['PRICE'] = boston_dataset.target\n\ndata.head()","38c88562":"data.tail()","5c087972":"data.info()","33589e35":"pd.isnull(data).any()","533bf741":"plt.figure(figsize=(10,6))\nplt.hist(data['PRICE'], bins=50, ec='black', color='#2196f3')\nplt.xlabel('Price in 000s')\nplt.ylabel('Nr. of Houses')\nplt.show()","3531d81c":"plt.figure(figsize=(10,6))\nplt.hist(data['RM'], ec='black', color='green', bins=50)\nplt.xlabel('Average Number of Rooms')\nplt.ylabel('Nr. of Houses')\nplt.show()","d6979629":"data['RM'].mean()\ndata['RM'].median()","8557f22f":"plt.figure(figsize=(10,6))\nplt.hist(data['RAD'], bins=50, ec='white', color='purple')\nplt.xlabel('Accessibility to Highways')\nplt.ylabel('Nr. of Houses')\nplt.show()\n\nprint(data['RAD'].value_counts())\n","6e9a75c5":"frequency = data['RAD'].value_counts()\nplt.figure(figsize=(10,6))\nplt.xlabel('Accessibility to Highways')\nplt.ylabel('Nr. of Houses')\nplt.bar(frequency.index, height=frequency)\nplt.show()","40c770ad":"data['CHAS'].value_counts() # dummy variable","e951acf9":"print('Min price:', data['PRICE'].min())\nprint('Min price:', data['PRICE'].max())","4525f0db":"data.min()","b4ca14c9":"data.max()","01d8c0c4":"data.describe()","69309ea5":"data.corr()","502b3286":"\"\"\"\n- CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000's\n\n\"\"\"\nprint()","949a1ee2":"mask = np.zeros_like(data.corr())\ntriangle_indices = np.triu_indices_from(mask)\nmask[triangle_indices] = True\nplt.figure(figsize=(16,10))\nsns.heatmap(data.corr(), mask=mask, annot=True, annot_kws={\"size\": 10})\nsns.set_style('white')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","bd0cb3a2":"nox_dis_corr = round(data['NOX'].corr(data['DIS']), 3)\n\nplt.figure(figsize=(9,6))\nplt.scatter(data['DIS'], data['NOX'], alpha=0.6, color='purple')\nplt.title(f'DIS vs NOX (Correlation {nox_dis_corr})', fontsize=14)\nplt.xlabel('DIS - Distance from employment', fontsize=14)\nplt.ylabel('NOX - Nitric Oxide Pollution', fontsize=14)\nplt.show()","0384824b":"sns.set()\nsns.set_context('talk')\nsns.set_style('whitegrid')\nsns.jointplot(x=data['DIS'], y=data['NOX'], kind='hex', size=7)\nplt.show()","def8e286":"sns.set()\nsns.set_context('talk')\nsns.set_style('whitegrid')\nsns.jointplot(x=data['TAX'], y=data['RAD'], size=7, color='darkred', joint_kws={'alpha':0.5})\nplt.show()\nprint(data['TAX'].corr(data['RAD']))","728452b2":"sns.lmplot(x='TAX', y='RAD', data=data, size=7)\nplt.show()","dbca2c9e":"sns.lmplot(x='RM', y='PRICE', data=data, size=7)\nplt.plot()\nprint('Corr between the RM and PRICE:',data['RM'].corr(data['PRICE']))","75968898":"# let's take a loak at the corr amongs all features \nsns.pairplot(data)\nplt.show()","97e00e60":"%%time\n\nsns.pairplot(data, kind='reg', plot_kws={'line_kws':{'color': 'cyan'}})\nplt.show()","fa8169ab":"prices = data['PRICE']\nfeatures = data.drop('PRICE', axis=1)\n\n# split and shuffle our dataset \nX_train, X_test, y_train, y_test = train_test_split(features, prices,test_size= 0.2, random_state=10)\n        ","7c9be68d":"regr = LinearRegression().fit(X_train, y_train)\n\nprint('Training data r-squared:', regr.score(X_train, y_train))\nprint('Test data r-squared:', regr.score(X_test, y_test))\nprint('Intercept', regr.intercept_)\npd.DataFrame(data= regr.coef_, index=X_train.columns, columns=['coef'])","144c868d":"data['PRICE'].skew()","a465b3b1":"y_log = np.log(data['PRICE'])\ny_log.tail()","e6c859b2":"y_log.skew()","8e8a7244":"sns.distplot(y_log)\nplt.title(f'Log price with skew {y_log.skew()}')\nplt.show()\n\nskeww = data['PRICE'].skew()\n\nsns.distplot(data['PRICE'])\nplt.title(f'price with skew {skeww}')\nplt.show()","b93a8a79":"sns.lmplot(x='LSTAT', y='PRICE', data=data, size=7, \n           scatter_kws={'alpha': 0.6}, \n           line_kws={'color':'darkred'})\nplt.show()","1c8959b8":"transformed_data = features\ntransformed_data['LOG_PRICE'] = y_log\n\nsns.lmplot(x='LSTAT', y='LOG_PRICE', data=transformed_data, size=7, \n           scatter_kws={'alpha': 0.6}, line_kws={'color':'cyan'})\nplt.show()","5e6b3001":"prices = np.log(data['PRICE']) # Use log prices\nfeatures = data.drop('PRICE', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, \n                                                    test_size=0.2, random_state=10)\n\nregr = LinearRegression()\nregr.fit(X_train, y_train)\n\nprint('Training data r-squared:', regr.score(X_train, y_train))\nprint('Test data r-squared:', regr.score(X_test, y_test))\n\nprint('Intercept', regr.intercept_)\npd.DataFrame(data=regr.coef_, index=X_train.columns, columns=['coef'])","41c95f4f":"X_incl_const = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit() \n\npd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})","9bf35978":"variance_inflation_factor(exog=X_incl_const.values, exog_idx=1)\n#type(X_incl_const)","1e83f91b":"vifs = []\nfor i in range(X_incl_const.shape[1]):\n    vifs.append(round(variance_inflation_factor(exog=X_incl_const.values, exog_idx=i),2))\npd.DataFrame({'coef' : results.params, 'vif': vifs })","c30b9336":"X_incl_const = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit() \n\norg_coef= pd.DataFrame({'coef' : results.params, 'p-value' : round(results.pvalues,2)})\n\nprint('BIC is ', results.bic)\nprint('r-squared is ', results.rsquared)","018e400f":"X_incl_const = sm.add_constant(X_train)\nX_incl_const = X_incl_const.drop(['INDUS'], axis=1)\n\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit()\n\ncoef_minus_indus = pd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})\n\nprint('BIC is', results.bic)\nprint('r-squared is', results.rsquared)","30a9f630":"# Reduced model #2 excluding INDUS and AGE\nX_incl_const = sm.add_constant(X_train)\nX_incl_const = X_incl_const.drop(['INDUS', 'AGE'], axis=1)\n\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit()\n\nreduced_coef = pd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})\n\nprint('BIC is', results.bic)\nprint('r-squared is', results.rsquared)","4bf64866":"frames = [org_coef, coef_minus_indus, reduced_coef] \npd.concat(frames, axis=1, sort=False)","cf934b32":"prices = np.log(data['PRICE']) # Use log prices\nfeatures = data.drop(['PRICE', 'INDUS', 'AGE'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, \n                                                    test_size=0.2, random_state=10)\nX_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit()\n\n# Residuals\n# residuals = y_train - results.fittedvalues\n# results.resid\n\ncorr = round(y_train.corr(results.fittedvalues), 2)\nplt.figure(figsize=(10,6))\nplt.scatter(x=y_train, y=results.fittedvalues, c='navy', alpha=0.4)\nplt.plot(y_train,y_train, color='cyan')\n\nplt.xlabel('Actual log prices $y _i$', fontsize=14)\nplt.ylabel('Prediced log prices $\\hat y _i$', fontsize=14)\nplt.title(f'Actual vs Predicted log prices: $y _i$ vs $\\hat y_i$ (Corr {corr})', fontsize=17)\n\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.scatter(x=np.e**y_train, y=np.e**results.fittedvalues, c='blue', alpha=0.6)\nplt.plot(np.e**y_train, np.e**y_train, color='cyan')\n\nplt.xlabel('Actual prices 000s $y _i$', fontsize=14)\nplt.ylabel('Prediced prices 000s $\\hat y _i$', fontsize=14)\nplt.title(f'Actual vs Predicted prices: $y _i$ vs $\\hat y_i$ (Corr {corr})', fontsize=17)\n\nplt.show()\n\n# Residuals vs Predicted values\nplt.figure(figsize=(10,6))\nplt.scatter(x=results.fittedvalues, y=results.resid, c='navy', alpha=0.6)\n\nplt.xlabel('Predicted log prices $\\hat y _i$', fontsize=14)\nplt.ylabel('Residuals', fontsize=14)\nplt.title('Residuals vs Fitted Values', fontsize=17)\n\nplt.show()\n\n# Mean Squared Error & R-Squared\nreduced_log_mse = round(results.mse_resid, 3)\nreduced_log_rsquared = round(results.rsquared, 3)\n\nprint('MSE', reduced_log_mse)\nprint('RSQUARED',reduced_log_rsquared)","2e50b9bc":"resid_mean = round(results.resid.mean(), 3)\nresid_skew = round(results.resid.skew(), 3)\nplt.figure(figsize=(10,6))\nsns.distplot(results.resid, color='navy')\nplt.title(f'Log price model: residuals Skew ({resid_skew}) Mean ({resid_mean})')\nplt.show()","53559373":"# Original model: normal prices & all features\nprices = data['PRICE']\nfeatures = data.drop(['PRICE'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, \n                                                    test_size=0.2, random_state=10)\n\nX_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit()\n\n# Graph of Actual vs. Predicted Prices\ncorr = round(y_train.corr(results.fittedvalues), 2)\nplt.scatter(x=y_train, y=results.fittedvalues, c='indigo', alpha=0.6)\nplt.plot(y_train, y_train, color='cyan')\n\nplt.xlabel('Actual prices 000s $y _i$', fontsize=14)\nplt.ylabel('Prediced prices 000s $\\hat y _i$', fontsize=14)\nplt.title(f'Actual vs Predicted prices: $y _i$ vs $\\hat y_i$ (Corr {corr})', fontsize=17)\n\nplt.show()\n\n# Residuals vs Predicted values\nplt.scatter(x=results.fittedvalues, y=results.resid, c='indigo', alpha=0.6)\n\nplt.xlabel('Predicted prices $\\hat y _i$', fontsize=14)\nplt.ylabel('Residuals', fontsize=14)\nplt.title('Residuals vs Fitted Values', fontsize=17)\n\nplt.show()\n\n# Residual Distribution Chart\nresid_mean = round(results.resid.mean(), 3)\nresid_skew = round(results.resid.skew(), 3)\n\nsns.distplot(results.resid, color='indigo')\nplt.title(f'Residuals Skew ({resid_skew}) Mean ({resid_mean})')\nplt.show()\n\n# Mean Squared Error & R-Squared\nfull_normal_mse = round(results.mse_resid, 3)\nfull_normal_rsquared = round(results.rsquared, 3)","9cea755d":"# Model Omitting Key Features using log prices\nprices = np.log(data['PRICE'])\nfeatures = data.drop(['PRICE', 'INDUS', 'AGE', 'LSTAT', 'RM', 'NOX', 'CRIM'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, \n                                                    test_size=0.2, random_state=10)\n\nX_incl_const = sm.add_constant(X_train)\nmodel = sm.OLS(y_train, X_incl_const)\nresults = model.fit()\n\n# Graph of Actual vs. Predicted Prices\ncorr = round(y_train.corr(results.fittedvalues), 2)\nplt.scatter(x=y_train, y=results.fittedvalues, c='#e74c3c', alpha=0.6)\nplt.plot(y_train, y_train, color='cyan')\n\nplt.xlabel('Actual log prices $y _i$', fontsize=14)\nplt.ylabel('Predicted log prices $\\hat y _i$', fontsize=14)\nplt.title(f'Actual vs Predicted prices with omitted variables: $y _i$ vs $\\hat y_i$ (Corr {corr})', fontsize=17)\n\nplt.show()\n\n# Residuals vs Predicted values\nplt.scatter(x=results.fittedvalues, y=results.resid, c='#e74c3c', alpha=0.6)\n\nplt.xlabel('Predicted prices $\\hat y _i$', fontsize=14)\nplt.ylabel('Residuals', fontsize=14)\nplt.title('Residuals vs Fitted Values', fontsize=17)\n\nplt.show()\n\n# Mean Squared Error & R-Squared\nomitted_var_mse = round(results.mse_resid, 3)\nomitted_var_rsquared = round(results.rsquared, 3)","ea848837":"pd.DataFrame({'R-Squared': [reduced_log_rsquared, full_normal_rsquared, omitted_var_rsquared],\n             'MSE': [reduced_log_mse, full_normal_mse, omitted_var_mse], \n             'RMSE': np.sqrt([reduced_log_mse, full_normal_mse, omitted_var_mse])}, \n            index=['Reduced Log Model', 'Full Normal Price Model', 'Omitted Var Model'])","3e0df4c7":"# The best model in our models is Reduced Log Models \n\n\n\nlog_prices = np.log(boston_dataset.target)\ntarget = pd.DataFrame(log_prices, columns=['PRICE'])\n\nregr = LinearRegression().fit(features, target) \nfitted_vals = regr.predict(features)\nfitted_vals[0:5]","290d7905":"# Multivariable Regression","6c41ad5e":"## Cleaning Data - Check for missing values","aa876dd4":"## Correlation \n\n### $$ \\rho _{XY} = corr(X,Y) $$\n### $$ -1.0 \\leq \\rho _{XY} \\leq +1.0 $$","5f568cd7":"# Training & Test Dataset Split ","96c621e2":"## Data Transformations\n\nto improve our model we can transform our target and investigate the outputs.  ","eb134d76":"## Visualising Data - Histograms, Distributions and Bar Charts","902865a6":"## Residuals & Residual Plots","89f2a4be":"# Gather Data \n\n[Source: Original research paper](https:\/\/deepblue.lib.umich.edu\/bitstream\/handle\/2027.42\/22636\/0000186.pdf?sequence=1&isAllowed=y) ","25ac955a":"## Regression using log prices","85fe019f":"## p values & Evaluating Coefficients","7962ae77":"## Model Simplification & the BIC","5fb4d3bd":"# Descriptive Statistics","3297bba6":"## Data points and features","39f9594c":"# Data Exploration with Pandas Dataframes","8906747c":"## Testing for Multicollinearity\n\n$$ TAX = \\alpha _0 + \\alpha _1 RM + \\alpha _2 NOX + ... + \\alpha _{12}LSTAT $$\n\n$$ VIF _{TAX} = \\frac{1}{(1 - R _{TAX} ^ 2)} $$"}}