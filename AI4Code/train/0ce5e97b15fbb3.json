{"cell_type":{"aa2cb449":"code","f6afbafa":"code","c99524b6":"code","67f517f1":"code","84875f27":"code","34d1f5b1":"code","18c61031":"code","2ac593c1":"code","1891ef1a":"code","7a99f16c":"code","17faabd5":"code","261c9f5d":"code","288c5b74":"code","57f836b9":"code","62de4405":"code","6ee4237b":"code","edeb6f04":"code","01702c8c":"code","f12bb8a5":"code","c37f64d0":"code","06fe1e90":"code","726bd056":"code","04f9b72e":"code","9d365062":"code","18708478":"code","b82de52e":"code","e29c0800":"code","2da67e0d":"code","aa0ad762":"code","653bf943":"code","839509fb":"code","28acfd14":"code","3c18bbf2":"code","42b8f9db":"code","31f808e5":"code","e8c081af":"code","9879313f":"code","db4a6d13":"markdown","5ffd8761":"markdown","174836b3":"markdown","b7bc7d2c":"markdown","5a934ec3":"markdown","060cef80":"markdown","b6fee152":"markdown","1ce4f849":"markdown"},"source":{"aa2cb449":"import os\nfrom pathlib import Path\nimport PIL\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization, Flatten, Dense, Dropout,Input, Add, ReLU, GlobalAveragePooling2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\n\nSEED = 15","f6afbafa":"train_dir = Path('\/kaggle\/input\/devnagri-script-classification\/Data\/Train\/')\ntest_dir = Path('\/kaggle\/input\/devnagri-script-classification\/Data\/Test\/')\n\ndatagen = ImageDataGenerator(rescale=1.\/255,\n                            validation_split = 0.25,\n                                    fill_mode='nearest')\nTarget_Size = (64,64)\n\n# Generate Train Data\ntrain_data = datagen.flow_from_directory(\n        train_dir,\n        target_size=Target_Size,\n        batch_size=64,\n        shuffle = True,\n        subset='training',\n        class_mode='categorical')\n\n# Generate Validation data\nval_data = datagen.flow_from_directory(\n        train_dir,\n        target_size=Target_Size,\n        shuffle = True,\n        batch_size=64,\n        subset='validation',\n        class_mode='categorical')\n\n\nlabels = train_data.class_indices\n\nclasses = list(labels)\n\nX,y = next(iter(train_data))","c99524b6":"# Generate Test Data\ntest_list = tf.io.gfile.listdir('\/kaggle\/input\/devnagri-script-classification\/Data\/Test\/')\ntest_data = []\nfor f in test_list:\n    img = tf.keras.preprocessing.image.load_img('\/kaggle\/input\/devnagri-script-classification\/Data\/Test\/' + f, color_mode = \"rgb\", target_size = Target_Size)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = img\/255\n    test_data.append(img)\n","67f517f1":"plt.figure(figsize = (12, 10))\nfor i in range(12):\n    ax = plt.subplot(3,4,i+1)\n    img = X[i]\n    plt.imshow(img)\n    plt.title(classes[y[i].argmax()])","84875f27":"kernel_init = tf.keras.initializers.glorot_uniform()\nbias_init = tf.keras.initializers.Constant(value=0.2)","34d1f5b1":"def model5():\n    \n    inputs = Input(shape = (64,64,3))\n\n    \n    x = Conv2D(16, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(inputs)\n    x = Conv2D(16, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x_srt = x\n    \n    x = Conv2D(32, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(32, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(32, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(32, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(32, kernel_size = (7,7), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    \n    x_srt = Conv2D(32, kernel_size = (5,5), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x_srt)\n\n    x = Add()([x, x_srt])\n    x = ReLU()(x)\n    \n    x = MaxPool2D(pool_size = (2,2), strides = 2)(x)\n    \n    x_srt2 = x\n    \n    x = Conv2D(64, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(64, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(64, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(64, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n\n    \n    x = Conv2D(64, kernel_size = (7,7), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    \n    x_srt2 = Conv2D(64, kernel_size = (5,5), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x_srt2)\n\n    x = Add()([x, x_srt2])\n    x = ReLU()(x)\n    \n    x = MaxPool2D(pool_size = (2,2), strides = 2)(x)\n    \n    x_srt3 = x\n    \n    x = Conv2D(128, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(128, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(128, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(128, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n\n    \n    x = Conv2D(128, kernel_size = (7,7), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    \n    x_srt3 = Conv2D(128, kernel_size = (5,5), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x_srt3)\n    \n    x = Add()([x, x_srt3])\n    x = ReLU()(x)\n    \n    x = MaxPool2D(pool_size = (2,2), strides = 2)(x)\n    \n    x_srt4 = x\n    \n    x = Conv2D(256, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(256, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(256, kernel_size = (7,7), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = Conv2D(256, kernel_size = (5,5), padding = 'same', activation = 'relu', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n    x = BatchNormalization()(x)\n\n    \n    x = Conv2D(256, kernel_size = (7,7), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x)\n\n    x_srt4 = Conv2D(256, kernel_size = (5,5), padding = 'same', kernel_initializer = kernel_init, bias_initializer = bias_init)(x_srt4)\n    \n    x = Add()([x, x_srt4])\n    x = ReLU()(x)\n    \n    x = GlobalAveragePooling2D()(x)\n    \n    x = Flatten()(x)\n    \n    x = Dense(256, activation = 'relu')(x)\n    outputs = Dense(46, activation = 'softmax')(x)\n    \n    \n    model = Model(inputs = inputs, outputs = outputs)\n    \n    return model\n    ","18c61031":"model5 = model5()\nmodel5.summary()","2ac593c1":"model5.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=4)\n\nhistory5 = model5.fit(train_data, validation_data=val_data, epochs=20, callbacks=callback, shuffle = True)","1891ef1a":"accuracy = history5.history['accuracy']\nval_accuracy  = history5.history['val_accuracy']\n\nloss = history5.history['loss']\nval_loss = history5.history['val_loss']\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.plot(val_accuracy, label=\"Validation accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.plot(val_loss, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","7a99f16c":"pred_5 = model5.predict(tf.convert_to_tensor(test_data))","17faabd5":"pred_5","261c9f5d":"pred__5 = np.argmax(pred_5, axis = 1)\npred__5","288c5b74":"labels_class = dict([(key, value) for key, value in labels.items()])\nlabels_class","57f836b9":"pred5 = list(map(labels_class.get, pred__5))","62de4405":"from tensorflow.keras.layers import DepthwiseConv2D","6ee4237b":"def expansion_block(x,t,filters,block_id):\n    prefix = 'block_{}_'.format(block_id)\n    total_filters = t*filters\n    x = Conv2D(total_filters,1,padding='same',use_bias=False, kernel_initializer = kernel_init, name = prefix +'expand')(x)\n    x = BatchNormalization(name=prefix +'expand_bn')(x)\n    x = ReLU(name = prefix +'expand_relu')(x)\n    return x\n\ndef depthwise_block(x,stride,block_id):\n    prefix = 'block_{}_'.format(block_id)\n    x = DepthwiseConv2D(3,strides=(stride,stride),padding ='same', kernel_initializer = kernel_init, use_bias = False, name = prefix + 'depthwise_conv')(x)\n    x = BatchNormalization(name=prefix +'dw_bn')(x)\n    x = ReLU(name=prefix +'dw_relu')(x)\n    return x\n\ndef projection_block(x,out_channels,block_id):\n    prefix = 'block_{}_'.format(block_id)\n    x = Conv2D(filters = out_channels,kernel_size = 1,padding='same',use_bias=False, kernel_initializer = kernel_init,name= prefix + 'compress')(x)\n    x = BatchNormalization(name=prefix +'compress_bn')(x)\n    return x","edeb6f04":"def Bottleneck(x,t,filters, out_channels,stride,block_id):\n    y = expansion_block(x,t,filters,block_id)\n    y = depthwise_block(y,stride,block_id)\n    y = projection_block(y, out_channels,block_id)\n    if y.shape[-1]==x.shape[-1]:\n        y = Add()([x,y])\n    return y","01702c8c":"def Model6():\n    inputs = Input(shape = (64,64,3))\n\n    x = Conv2D(32,kernel_size=3,strides=(2,2),padding = 'same', kernel_initializer = kernel_init)(inputs)\n    x = BatchNormalization(name='conv1_bn')(x)\n    x = ReLU(6, name = 'conv1_relu')(x)\n\n    # 13 Bottlenecks\n\n    x = depthwise_block(x,stride=1,block_id=1)\n    x = projection_block(x, out_channels=16,block_id=1)\n\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 2,block_id = 2)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 24, stride = 1,block_id = 3)\n\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 2,block_id = 4)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 5)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 32, stride = 1,block_id = 6)\n\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 2,block_id = 7)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 8)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 9)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 64, stride = 1,block_id = 10)\n\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 11)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 12)\n    x = Bottleneck(x, t = 6, filters = x.shape[-1], out_channels = 96, stride = 1,block_id = 13)\n\n\n    #1*1 conv\n    x = Conv2D(filters = 1280,kernel_size = 1,padding='same',use_bias=False, name = 'last_conv')(x)\n    x = BatchNormalization(name='last_bn')(x)\n    x = ReLU(6,name='last_relu')(x)\n\n    #AvgPool 7*7\n    x = GlobalAveragePooling2D(name='global_average_pool')(x)\n    \n    x = Dense(2048, activation = 'relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1024, activation = 'relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512, activation = 'relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(256, activation = 'relu')(x)\n\n    output = Dense(n_classes,activation='softmax')(x)\n\n    model = Model(inputs, output)\n\n    return model","f12bb8a5":"model6 = Model6()\nmodel6.summary()","c37f64d0":"model6.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=4)\n\nhistory6 = model6.fit(train_data, validation_data=val_data, epochs=10, callbacks=callback, shuffle = True)","06fe1e90":"accuracy = history6.history['accuracy']\nval_accuracy  = history6.history['val_accuracy']\n\nloss = history6.history['loss']\nval_loss = history6.history['val_loss']\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.plot(val_accuracy, label=\"Validation accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.plot(val_loss, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","726bd056":"pred_6 = model6.predict(tf.convert_to_tensor(test_data))\npred6 = np.argmax(pred_6, axis = 1)\npred6","04f9b72e":"labels_class = dict([(key, value) for key, value in labels.items()])\npred6 = list(map(labels_class.get, final))\nresult = pd.DataFrame({'id': test_list,'category': pred6})\nresult.to_csv('result8.csv',index= False)\nresult","9d365062":"pred_6","18708478":"def model7():\n    \n    model = Sequential([\n    Conv2D(filters=96, kernel_size=(10,10), activation='relu', input_shape=(64,64,3)),\n    BatchNormalization(),\n    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    Conv2D(filters=256, kernel_size=(5,5), activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    Conv2D(filters=384, kernel_size=(3,3), activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    Conv2D(filters=384, kernel_size=(3,3), activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding=\"same\"),\n    BatchNormalization(),\n    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    Flatten(),\n    Dense(2048, activation='relu'),\n    Dropout(0.5),\n    Dense(2048, activation='relu'),\n    Dropout(0.5),\n    Dense(46, activation='softmax')\n])\n    \n    return model","b82de52e":"model7 = model7()\nmodel7.summary()","e29c0800":"model7.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=4)\n\nhistory7 = model7.fit(train_data, validation_data=val_data, epochs=10, callbacks=callback, shuffle = True)","2da67e0d":"accuracy = history7.history['accuracy']\nval_accuracy  = history7.history['val_accuracy']\n\nloss = history7.history['loss']\nval_loss = history7.history['val_loss']\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.plot(val_accuracy, label=\"Validation accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.plot(val_loss, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","aa0ad762":"pred_7 = model7.predict(tf.convert_to_tensor(test_data))\npred7 = np.argmax(pred_7, axis = 1)\npred7","653bf943":"from tensorflow.keras.layers import concatenate, AvgPool2D\nimport tensorflow.keras.backend as K","839509fb":"def model8(filters = 32):\n    \n    #batch norm + relu + conv\n    def bn_rl_conv(x,filters,kernel=1,strides=1):\n        \n        x = BatchNormalization()(x)\n        x = ReLU()(x)\n        x = Conv2D(filters, kernel, strides=strides,padding = 'same')(x)\n        return x\n    \n    def dense_block(x, repetition):\n        \n        for _ in range(repetition):\n            y = bn_rl_conv(x, 4*filters)\n            y = bn_rl_conv(y, filters, 3)\n            x = concatenate([y,x])\n        return x\n        \n    def transition_layer(x):\n        \n        x = bn_rl_conv(x, K.int_shape(x)[-1] \/\/2 )\n        x = AvgPool2D(2, strides = 2, padding = 'same')(x)\n        return x\n    \n    inputs = Input (shape = (64,64,3))\n    \n    x = Conv2D(64, 7, padding = 'same')(inputs)\n    x = MaxPool2D(3, strides = 1, padding = 'same')(x)\n    \n    for repetition in [6,12,24,16]:\n        \n        d = dense_block(x, repetition)\n        x = transition_layer(d)\n        \n    x = GlobalAveragePooling2D()(d)\n    \n    output = Dense(46, activation = 'softmax')(x)\n    \n    model = Model(inputs, output)\n    return model","28acfd14":"model8 = model8()\nmodel8.summary()","3c18bbf2":"model8.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=4)\n\nhistory8 = model8.fit(train_data, validation_data=val_data, epochs=1, callbacks=callback, shuffle = True)","42b8f9db":"accuracy = history8.history['accuracy']\nval_accuracy  = history8.history['val_accuracy']\n\nloss = history8.history['loss']\nval_loss = history8.history['val_loss']\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.plot(val_accuracy, label=\"Validation accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.plot(val_loss, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","31f808e5":"pred_8 = model8.predict(tf.convert_to_tensor(test_data))\npred8 = np.argmax(pred_8, axis = 1)\npred8","e8c081af":"fiin = 0.35 * (pred_5 + pred_7) + 0.15 * (pred_8 + pred_6)\npred10 = np.argmax(fiin, axis = 1)\npred10","9879313f":"labels_class = dict([(key, value) for key, value in labels.items()])\npred10 = list(map(labels_class.get, pred10))\nresult = pd.DataFrame({'id': test_list,'category': pred10})\nresult.to_csv('result10.csv',index= False)\nresult","db4a6d13":"### MobileNet","5ffd8761":"### DenseNet","174836b3":"###  ResNet","b7bc7d2c":"###  AlexNet","5a934ec3":"### Ensembling","060cef80":"Due to less time, could only train 1 epoch.","b6fee152":"Plotting some images and their classes","1ce4f849":"The data contains total 78200 images with 46 classes, each class contain 1700 images.\nLets first load the dataset"}}