{"cell_type":{"b226dc39":"code","def43b2b":"code","7188ba90":"code","f65048a4":"code","6003100e":"code","3d82fdb7":"code","42ed8a9a":"code","7c6f849c":"code","ab123124":"code","4e546c56":"code","d59d059f":"code","e2f67924":"code","30570be7":"code","bcc77137":"code","ef6a4af1":"code","f1b5feb9":"code","533aaa8e":"code","4011f733":"markdown","16598cc1":"markdown","e5ecec81":"markdown","b4400c26":"markdown","6ae130a4":"markdown","ef2f74d0":"markdown","407d7889":"markdown","b3f87d11":"markdown","6e498b15":"markdown","bd30aeeb":"markdown","cbcb6164":"markdown","fbd07311":"markdown","63593d8d":"markdown","a925e1fc":"markdown","a10ee738":"markdown"},"source":{"b226dc39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","def43b2b":"# Reading the train and the test data.\ntrainData = pd.read_csv('..\/input\/train.csv')\ntestData = pd.read_csv('..\/input\/test.csv')\n\n# Displaying a sample of the train data to get more detailed info\ntrainData.head()","7188ba90":"trainData.describe()","f65048a4":"trainData.dtypes","6003100e":"trainData.apply(lambda x: x.isnull().any())","3d82fdb7":"pd.DataFrame({'percent_missing': trainData.isnull().sum() * 100 \/ len(trainData)})","42ed8a9a":"pd.DataFrame({'percent_unique': trainData.apply(lambda x: x.unique().size\/x.size*100)})","7c6f849c":"# Names of the features extarcted from the data\nselFeatures = list(trainData.columns.values)\n# Removing the target variable from the column values\ntargetCol = 'Survived'\nselFeatures.remove(targetCol)\n\n# Removing features with unique values\nfor i in selFeatures:\n    if trainData.shape[0] == len(pd.Series(trainData[i]).unique()) :\n        selFeatures.remove(i)\n        \n# Removing features with high percentage of missing values\nselFeatures.remove('Cabin')","ab123124":"import seaborn as sns\nsns.set(style=\"ticks\")\nplotFeatures = [x for x in selFeatures]\nplotFeatures.append(\"Survived\")\nsns.pairplot(trainData[plotFeatures], hue=\"Survived\")","4e546c56":"# Also removing cabin and ticket features for the initial run.\nselFeatures.remove('Ticket')\n        \nprint(\"Target Class: '\"+ targetCol + \"'\")\nprint('Features to be investigated: ')\nprint(selFeatures)","d59d059f":"def handle_categorical_na(df):\n    ## replacing the null\/na\/nan values in 'Cabin' attribute with 'X'\n#     df.Cabin = df.Cabin.fillna(value='X')\n#     ## Stripping the string data in 'Cabin' and 'Ticket' features of numeric values and duplicated characters\n#     df.Cabin = [''.join(set(filter(str.isalpha, s))) for s in df.Cabin]\n#     df.Ticket = [''.join(set(filter(str.isalpha, s))) for s in df.Ticket]\n#     ## replacing the '' values in 'Ticket' attribute with 'X'\n#     df.Ticket.replace(to_replace='',value='X',inplace=True)\n    ## Imputing the null\/na\/nan values in 'Age' attribute with its mean value \n    df.Age.fillna(value=df.Age.mean(),inplace=True)\n    ## replacing the null\/na\/nan values in 'Embarked' attribute with 'X'\n    df.Embarked.fillna(value='X',inplace=True)\n    return df","e2f67924":"from sklearn.model_selection import train_test_split\nseed = 7\nnp.random.seed(seed)\nX_train, X_test, Y_train, Y_test = train_test_split(trainData[selFeatures], trainData.Survived, test_size=0.2)\n\nX_train = handle_categorical_na(X_train)\nX_test = handle_categorical_na(X_test)\n\n## using One Hot Encoding for handling categorical data\nX_train = pd.get_dummies(X_train,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\nX_test = pd.get_dummies(X_test,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])","30570be7":"common_col = [x for x in X_test.columns if x in X_train.columns]\nX_test = X_test[common_col]\n\nmissing_col = [x for x in X_train.columns if x not in X_test.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    X_test.insert(X_test.shape[1], val, pd.Series(np.zeros(X_test.shape[0])))","bcc77137":"def rf_hyperparameter_optimization():\n    from sklearn.model_selection import RandomizedSearchCV\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    max_features = ['auto', 'sqrt']\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    min_samples_split = [2, 5, 10]\n    min_samples_leaf = [1, 2, 4]\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n\n    from sklearn.ensemble import RandomForestRegressor\n    # Using the random grid to search for best hyperparameters\n    # Creating the base model to tune\n    rf = RandomForestRegressor()\n    # Random search of parameters, using 5 fold cross validation, \n    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, Y_train)\n    return rf_random.best_params_\n\n## based on the hyper parameter optimization, the below model is built.\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=1400, min_samples_split=5, min_samples_leaf=4, max_features= 'sqrt', max_depth= 80, bootstrap= True)\nrf_model.fit(X_train,Y_train)\n# Fetching predictions\nY_pred = rf_model.predict(X_test)\n# Calculating the test accuracy\nfrom sklearn import metrics\nscore_rf = metrics.accuracy_score(Y_test, Y_pred)\nprint(\"Test Accuracy:\",score_rf)","ef6a4af1":"from sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm","f1b5feb9":"in_shape = X_train.shape[1]\n\ndef create_model(optimizer='Adam', neurons=50):\n    # Initialize the constructor\n    model = Sequential()\n    # Input - Layer\n    model.add(Dense(neurons, input_dim=in_shape, activation=activation))\n    # Hidden - Layers\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    # Output- Layer\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\ndef nn_hyperparameter_optimization():\n    model = KerasClassifier(build_fn=create_model, verbose=0)\n    # defining the grid search parameters\n    neurons = [65, 75, 85]\n    batch_size= [10, 20, 30, 40]\n    epochs= [10, 20, 30, 40]\n    optimizer = ['RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n    dropout_rate = [0.1, 0.2, 0.3, 0.4, 0.5]\n    param_grid = dict(neurons=neurons,\n                      optimizer=optimizer,\n                      batch_size=batch_size,\n                      epochs=epochs,\n                      activation=activation,\n                      dropout_rate=dropout_rate)\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n    grid_result = grid.fit(X_train, Y_train)\n\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    return grid_result\n## based on the hyper parameter optimization, the below model is built.\nnn_model = Sequential()\n# Input - Layer\nnn_model.add(Dense(65, input_dim=in_shape, activation='relu'))\n# Hidden - Layers\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\n# Output- Layer\nnn_model.add(Dense(1, activation='sigmoid'))\nnn_model.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\nnn_model.fit(X_train, Y_train,\n          batch_size=20,\n          epochs=20,\n          verbose=1,\n          validation_data=(X_test, Y_test))\n\nscore = nn_model.evaluate(X_test, Y_test, verbose=2)\nscore_nn = score[1]\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","533aaa8e":"xTest = testData[selFeatures]\nxTest = handle_categorical_na(xTest)\n## using One Hot Encoding for handling categorical data\nxTest = pd.get_dummies(xTest,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\ncommon_col = [x for x in xTest.columns if x in X_train.columns]\nxTest = xTest[common_col]\nmissing_col = [x for x in X_train.columns if x not in xTest.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    xTest.insert(xTest.shape[1], val, pd.Series(np.zeros(xTest.shape[0])))\ncol_names = xTest.columns\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nxTest = my_imputer.fit_transform(xTest)\nxTest = pd.DataFrame(xTest)\nxTest.columns = col_names\n\nsubmission = pd.DataFrame()\n## Comparing and submitting the best result\nif score_nn>score_rf:\n    predictions = nn_model.predict_classes(xTest)\n    predictions = [x[0] for x in predictions]\nelse:\n    predictions = rf_model.predict(xTest)\nsubmission = pd.DataFrame({'PassengerId': testData.PassengerId, 'Survived': predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","4011f733":"**Submit Predictions**","16598cc1":"Comparing the accuracy of neural network and random forest models and submitting the predictions from the best model.","e5ecec81":"1.2 Summary of the data:","b4400c26":"**3. Data Preparation**\n\n**3.1 Feature Selection**\n\nSelecting the columns required for building the model based on EDA.","6ae130a4":"**2.1 Data Types:**\n\nLet us take a look at the type of data being handled.","ef2f74d0":"**1. Data Understanding**\nFirst let us take a look at how our data looks like, to get more details on the characteristics of the attributes\/features.\n\n**1.1 Peek into the Data:**\n","407d7889":"From the results of sections 2.2 and 2.3, we see that the features\/attributes Cabin, PassengerId and Name can be eliminated from the required features to build the model since, the feature Cabin has 77.1% of missing values, and the features PassengerId\/Name has 100% unique features.","b3f87d11":"**4. Modeling**\n\nUsing **Random Forest** to build a model. The model uses Random Search to find the best hyperparameter values.\n\nThe optimized parameters are:\n\nNumber of trees\nNumber of features to consider at every split\nMaximum number of levels in tree\nMinimum number of samples required to split a node\nMinimum number of samples required at each leaf node\nMethod of selecting samples for training each tree","6e498b15":"**2.3 Unique Values:**\n\nChecking for the amount of unique values in each feature.","bd30aeeb":"**1.3 Some insights :**\n\nThe data set consists of 12 attributes (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\n\nPassengerId - Numeric data, it is unique for each passenger, not useful.\n\nSurvived - Target data to be predicted, with incators 0 (Passenger Died) or 1 (Passenger Survived).\n\nPclass - Numeric data, has 3 categories (1st, 2nd or 3rd), useful.\n\nName - String data type, mostly unique for each passenger, not useful.\n\nSex - Categorical data, has 2 categories (male, female), useful.\n\nAge - Numeric data, max age is 80, it contains few nulls (can be imputed), useful.\n\nSibSp - Numeric data, max number of siblings \/ spouses aboard is 8, useful.\n\nParch - Numeric data, max number of parents \/ children aboard is 6, useful.\n\nTicket - Categorical data, it has a prefix to the number, it can be useful.\n\nFare - Numeric data, higher the class, higher is the fare (need to check), max fare is 512,32 (currency is $ I guess) , useful.\n\nCabin - Categorical data, it contains few nulls (can be imputed), it can be useful.\n\nEmbarked - Categorical data, with 3 categories, C = Cherbourg, Q = Queenstown, S = Southampton, it contains few nulls (can be imputed), useful.\n\nThe attribute 'Survived' would be the attribute to be predicted, i.e, target attribute.\n\nBased on the values in the target attribute, i.e., 0\/1 which states if the passenger survived or not, it is a Binary Classification.","cbcb6164":"**2.4 Visualizations:**\n\nVisualizing the data using interactive plots. Starting with a matrix scatter plot showing the relation between the features that will be used for the training. Followed by plots to check the survival rate based on features.","fbd07311":"****Titanic: Machine Learning from Disaster****","63593d8d":"**2. Exploratory Data Analysis**","a925e1fc":"**2.2 Missing Data:**\n\nChecking and calculating the amount of missing values in the dataset.\n\nThese missing values will be handled later.","a10ee738":"Using **Sequential model **of Keras API to build a neural network. The model is tuned and optimezed using Grid Serach.\n\nThe optimized parameters are:\n\nNumber of neurons in input and hidden layers\nActivation type\nOptimizer\nEpochs\nBatch size\nDropout ratio\nDue to increased processing time, the best parameters were found and used."}}