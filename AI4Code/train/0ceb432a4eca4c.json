{"cell_type":{"7c77e112":"code","3ca4cb40":"code","c130928c":"code","cb7d120f":"code","450a8f7c":"code","82fe5cfa":"code","69e68d97":"code","73ff4658":"code","04b982c2":"code","5dd164bd":"code","3e84525c":"code","ad517c00":"code","effcb04b":"code","55aebae3":"code","52b68488":"code","73b46899":"code","bb2bcf51":"code","cc6eec0d":"code","0574375d":"code","aff2fd0d":"markdown","176ac583":"markdown","ef92ca7c":"markdown","c54272e9":"markdown","c9d009ec":"markdown","1939669a":"markdown","b961d60f":"markdown","832fc106":"markdown"},"source":{"7c77e112":"import gc\nimport time\nimport string\nimport warnings\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import pca\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfTransformer  \nfrom sklearn.feature_extraction.text import CountVectorizer  \nfrom tqdm import tqdm_notebook as tqdm\n\nimport nltk\nimport gensim\nimport gensim.models as w2v\nimport lightgbm as lgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.set_option('max_rows', 500)\npd.set_option('max_columns', 500)\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')","3ca4cb40":"df = pd.read_csv('\/kaggle\/input\/apple-iphone-se-reviews-ratings\/APPLE_iPhone_SE.csv', index_col=False)\nprint('the shape of input data: ' + str(df.shape))\ndf.head()","c130928c":"# stats of Ratings \nprint('stats of Ratings\\n')\nprint(df['Ratings'].value_counts())\nprint('\\n')\nprint(df['Ratings'].value_counts(normalize=True))","cb7d120f":"plt.figure(figsize=(5, 4))\nsns.barplot(df['Ratings'].value_counts(normalize=True).index, df['Ratings'].value_counts(normalize=True).values)","450a8f7c":"# stats of Comment \nprint('stats of Comment\\n')\nprint(df['Comment'].value_counts().head(25))\nprint('\\n')\nprint(df['Comment'].value_counts(normalize=True).head(25))","82fe5cfa":"# some simple stats of corpus\nprint('the median and STD of all comments is: ' + \n      str(np.mean([len(x) for x in df['Comment'].to_list()])) + ' and ' + \n      str(np.std([len(x) for x in df['Comment'].to_list()])))\nprint('the median and STD of all reviews is: ' + \n      str(np.mean([len(x) for x in df['Reviews'].to_list()])) + ' and ' + \n      str(np.std([len(x) for x in df['Reviews'].to_list()])))","69e68d97":"def data_preprocessing (text, is_stem=False):\n    # text lower\n    text = text.lower()\n    text = text.strip()\n    # eliminating punctuation\n    for c in string.punctuation:\n        text = text.replace(c, '')\n    '''\n    # eliminating numbers \n    for i in range(10):\n        text = text.replace(str(i), '')\n    '''\n    # tokenize\n    word_list = nltk.word_tokenize(text)\n    # filting stop words \n    word_filtered = [x for x in word_list if x not in nltk.corpus.stopwords.words('english')]\n    # filting word's stem\n    if is_stem: \n        ps = nltk.stem.porter.PorterStemmer()\n        word_filtered = [ps.stem(x) for x in word_filtered]\n        wl = nltk.stem.WordNetLemmatizer()\n        word_filtered = [wl.lemmatize(x) for x in word_filtered]\n    return word_filtered\n\n\ndef dataset_generation(df):\n    df_preprocessed = pd.DataFrame()\n    df_preprocessed['Ratings'] = df['Ratings']\n    comment_preprocessed, reviews_preprocessed = [], []\n    for idx in tqdm(range(len(df))):\n        text_comment = data_preprocessing(df['Comment'].iloc[idx], is_stem=True)\n        text_review = data_preprocessing(df['Reviews'].iloc[idx], is_stem=False)\n        comment_preprocessed.append(text_comment)\n        reviews_preprocessed.append(text_review)\n    df_preprocessed['Comment'] = comment_preprocessed\n    df_preprocessed['Reviews'] = reviews_preprocessed\n    return df_preprocessed","73ff4658":"# dataset generation \ndf_preprocessed = dataset_generation(df)\ndf_preprocessed.head()","04b982c2":"# some simple stats of Comments\nset_comments = set([i for tmp in df_preprocessed['Comment'].to_list() for i in tmp])\nprint('the num of unique word in comments is ' + str(len(set_comments)))\ndf_comments_encoded = np.zeros((len(df_preprocessed), len(set_comments)))\ndf_comments_encoded = pd.DataFrame(df_comments_encoded, columns=set_comments, dtype=np.int)\n\n# Multi-hot encoding for Comments\nfor idx in tqdm(range(len(df_preprocessed))):\n    tmp_comment = df_preprocessed['Comment'].iloc[idx]\n    for ele in tmp_comment:\n        df_comments_encoded.iloc[idx][ele] = np.int(1)","5dd164bd":"# combination of df_preprocessed and comments after Multi-hot encoded\ndf_preprocessed = pd.concat([df_preprocessed, df_comments_encoded], axis=1)\nfeatures_comments = list(df_comments_encoded)","3e84525c":"# obtain word's vec by local training\nw2v_model = w2v.Word2Vec(sentences=df_preprocessed['Reviews'].to_list(), \n                         vector_size=128, window=5, min_count=3, sg=0)","ad517c00":"# obtain word's vec from Google W2V without local training\nwv_pretrain = gensim.models.KeyedVectors.load_word2vec_format('\/kaggle\/input\/word2vec-google\/GoogleNews-vectors-negative300.bin', binary=True)","effcb04b":"def sen2vec_tfidf(model, sentence, vector_size, word_weight, word_dict):\n    vec = np.zeros(vector_size)\n    for word in sentence:\n        try:\n            vec += model[word] * word_weight[word_dict[word]]\n        except:\n            continue\n    return vec\n\n\ndef sen2vec(model, sentence, vector_size):\n    vec = np.zeros(vector_size)\n    count = 0\n    for word in sentence:\n        try:\n            vec += model[word]\n            count += 1\n        except:\n            continue\n    return vec\/count\n\n\ndef vec_obtain(model, df, vector_size, is_tfidf=False):\n    vecs = np.zeros((len(df), vector_size))\n    if is_tfidf:\n        tfidf, word_dict = tf_idf_weighted([' '.join(x) for x in df['Reviews'].to_list()])\n        for idx in tqdm(range(len(df))):\n            vec = sen2vec_tfidf(model, df['Reviews'].iloc[idx], vector_size, tfidf[idx, :], word_dict)\n            vecs[idx, :] = vec\n    else:\n        for idx in tqdm(range(len(df))):\n            vec = sen2vec(model, df['Reviews'].iloc[idx], vector_size)\n            vecs[idx, :] = vec\n    vecs = pd.DataFrame(vecs, columns=['vec_' + str(x) for x in range(vector_size)])\n    return vecs\n\n\ndef tf_idf_weighted(corpus_list):\n    vectorizer = CountVectorizer()  \n    tmp = vectorizer.fit_transform(corpus_list)\n    word = vectorizer.get_feature_names()  \n    \n    word_dict = {}\n    for idx, w in enumerate(word):\n        word_dict[w] = idx\n    \n    transformer = TfidfTransformer()\n    tfidf = transformer.fit_transform(tmp) \n    return tfidf.toarray(), word_dict","55aebae3":"vecs = vec_obtain(wv_pretrain, df_preprocessed, wv_pretrain.vector_size, is_tfidf=False)\nfeatures_vec = list(vecs)","52b68488":"# combination of features\ndf_train = pd.concat([df_preprocessed, vecs], axis=1)\nfeatures = features_comments + features_vec\ndf_train = df_train[features + ['Ratings']]\nprint('the shape of df_train is: ' + str(df_train.shape))","73b46899":"# dataset spliting \ndf_train, df_test = train_test_split(df_train, test_size=0.20, shuffle=True, random_state=2021)\ndf_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\nprint('the shape of train set: ' + str(df_train.shape))\nprint('the shape of test set: ' + str(df_test.shape))\nprint('\\n')\nprint(df_train['Ratings'].value_counts(normalize=True))\nprint(df_test['Ratings'].value_counts(normalize=True))\n\n# rejusting label\ndf_train['label'] = df_train['Ratings'].apply(lambda x: x-1)\ndf_test['label'] = df_test['Ratings'].apply(lambda x: x-1)","bb2bcf51":"def lgb_kfold_prediction(df_train, df_test, params, features, features_categorical=None, Kfolds=5, seed=2020):\n    kf = KFold(n_splits=Kfolds, shuffle=True, random_state=seed)\n    oob_pred = np.zeros((len(df_train), 1))\n    test_pred = np.zeros((len(df_test), 5))\n    features_importance = np.zeros((len(features), Kfolds))\n    for fold, (train_idx, vali_idx) in enumerate(kf.split(df_train[features], df_train[target])):\n        # dataset splitting \n        X_train, X_vali = df_train[features].iloc[train_idx, :].reset_index(drop=True), df_train[features].iloc[vali_idx, :].reset_index(drop=True)\n        y_train, y_vali = df_train[target].iloc[train_idx].reset_index(drop=True), df_train[target].iloc[vali_idx].reset_index(drop=True)\n        # lgb dataset \n        train = lgb.Dataset(X_train, label=y_train, categorical_feature=features_categorical)\n        vali = lgb.Dataset(X_vali, label=y_vali, categorical_feature=features_categorical)\n        lgb_model = lgb.train(params, train, num_boost_round = 5000, valid_sets=[train, vali], verbose_eval=100, early_stopping_rounds=100)\n        # prediction \n        vali_pred = np.argmax(lgb_model.predict(X_vali), axis=1).reshape(-1, 1)\n        oob_pred[vali_idx] = vali_pred\n        test_pred += lgb_model.predict(df_test[features])\n        features_importance[:, fold] = lgb_model.feature_importance(importance_type='gain')\n        print('+++++ %d th fold kappa is %f +++++' %(fold, cohen_kappa_score(y_vali, vali_pred, weights='quadratic')))\n    features_importance = pd.DataFrame(features_importance, columns=list(range(Kfolds)))\n    features_importance['features'] = features\n    return oob_pred, np.argmax(test_pred\/5, axis=1), features_importance","cc6eec0d":"seed = 2021\ntarget = 'label'\nparams = {\n    'objective': 'multiclass',\n    'num_class': 5,\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':512,\n    'learning_rate': 0.025,\n    'subsample': 0.80,\n    'subsample_freq': 4,\n    'feature_fraction': 1.0,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'seed':seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'n_jobs':-1,\n    'verbose': -1}\n\n\noob_pred, test_pred, features_importance = lgb_kfold_prediction(df_train, df_test, params, features, features_comments, 5, seed)\n\n\nprint('the confusion matrix of OOB: \\n')\nprint(str(confusion_matrix(df_train[target], oob_pred)))\nprint('the accuracy of OOB: \\n')\nprint(str(np.sum(df_train[target]==oob_pred.ravel())\/len(oob_pred)*100))\nprint('the weighted kappa is: \\n')\nprint(str(cohen_kappa_score(df_train[target], oob_pred.ravel(), weights='quadratic')))","0574375d":"print('the confusion matrix of test: \\n')\nprint(str(confusion_matrix(df_test[target], test_pred)))\nprint('the accuracy of test: \\n')\nprint(str(np.sum(df_test[target]==test_pred)\/len(df_test)*100))\nprint('the weighted kappa is: \\n')\nprint(str(cohen_kappa_score(df_test[target], test_pred, weights='quadratic')))","aff2fd0d":"### Data loading ","176ac583":"### GBDT modeling for Multi-classification","ef92ca7c":"### Data Preprocessing","c54272e9":"##### This is the end of this notebook","c9d009ec":"### Hi everyone! If you like this notebook, please vote it~\n#### I'm a beginner and learner in data science, and this is my baseline. If anyone have questions or want to discuss with me, please leave your comments.\n#### This notebook will be continuously updated, I will try more methods in field of NLP as soon as possible~ \n#### The main solutions of this notebook are listed as follows:\n* Each comment text has relative few words, so I use word bag to generate features (also can be seen as Multi-hot encoder)\n* Reviews text are used to obtain word's vectors as features by pretrainined Word2Vec model (GoogleNews), or we can train W2V by this dataset locally\n* The features from Comment and Reviews are combined and lightGBM is used for modeling\n* Accuracy ratio, cohen kappa and confusion matrix are used to evalute the prediction performance of our model","1939669a":"### Pretraining: Word2Vec\n* the input fromat of W2V is 2-D list, the outer ele is each sentence, and the inner ele is each word\n* the label of Ratings (1-5), five ranks ","b961d60f":"### Simple EDA","832fc106":"### Dataset generation and spliting "}}