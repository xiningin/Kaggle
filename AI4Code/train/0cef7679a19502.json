{"cell_type":{"cbe2ad69":"code","fe039921":"code","ffa4463b":"code","3b07df79":"code","0fa6856a":"code","d1d6f43e":"code","ae7021ae":"code","df39cc62":"code","e7687fad":"code","639916ea":"code","5b3646e1":"code","7f73e9f7":"code","67112b69":"code","2b030de6":"code","c39110db":"code","4119c6ec":"code","1e6d319b":"code","9e78e1cf":"code","08efbd5b":"code","7122f429":"code","1738f004":"code","7f57c79c":"code","790cea75":"code","12844664":"code","5cd02f6d":"code","42da397a":"code","7652c0a6":"code","a9b66d1f":"markdown","3db63453":"markdown","69384003":"markdown","b4667159":"markdown","7f335f98":"markdown","5a754e23":"markdown","6bdc6424":"markdown","cdfa81b2":"markdown","768ca1ba":"markdown","35bef4c5":"markdown","ef2cd731":"markdown","5c9922fa":"markdown","7c5037d3":"markdown","917faea6":"markdown","842c4c51":"markdown"},"source":{"cbe2ad69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe039921":"train_df_all  = pd.read_csv(\"\/kaggle\/input\/i-love-ai\/train.csv\")\ntest_df_all   = pd.read_csv(\"\/kaggle\/input\/i-love-ai\/test.csv\")\nsubmit_df     = pd.read_csv(\"\/kaggle\/input\/i-love-ai\/submit_sample.csv\")\n\ntrain_df_all.info()\ntrain_df_all.describe()","ffa4463b":"len(train_df_all['identifier'].unique())","3b07df79":"len(test_df_all['identifier'].unique())","0fa6856a":"train_df_list = [pd.DataFrame(columns = train_df_all.columns) for i in range(len(train_df_all['identifier'].unique()))] # 71\uac1c\uc758 \ube48 DataFrame\uc744 \uac00\uc9c0\uac8c \ub420 \uac83\ntest_df_list = [pd.DataFrame(columns = test_df_all.columns) for i in range(len(test_df_all['identifier'].unique()))] # 14\uac1c\uc758 \ube48 DataFrame\uc744 \uac00\uc9c0\uac8c \ub420 \uac83\n\nidentifier_begin = 15 # train_df\uc758 identifier\ub294 15\ubd80\ud130 \uc2dc\uc791\ud568, test\ub294 1\ubd80\ud130 \uc2dc\uc791\n\n# for i, v in enumerate(train_df_all['identifier']):\n    # train_df_list[v - begin_identifier].append(train_df_all.iloc[i])          # \ubc29\ubc95 1 \uac00\uc7a5 \ub290\ub9bc\n    # pd.concat([train_df_list[v - begin_identifier], train_df_all.iloc[i]])    # \ubc29\ubc95 2 \ub290\ub9bc\n    \n# train \ub370\uc774\ud130\uc5d0 \ub300\ud574\nfor k, group in train_df_all.groupby(['identifier']):        \n    train_df_list[k-identifier_begin] = group\n    \n# test \ub370\uc774\ud130\uc5d0 \ub300\ud574\nfor k, group in test_df_all.groupby(['identifier']):    \n    test_df_list[k-1] = group","d1d6f43e":"for train_df in train_df_list:\n    train_df['SampleTimeFine'] = (train_df['SampleTimeFine'] - train_df['SampleTimeFine'].min()) \/ 1e+6\n\nfor test_df in test_df_list:\n    test_df['SampleTimeFine'] = (test_df['SampleTimeFine'] - test_df['SampleTimeFine'].min()) \/ 1e+6","ae7021ae":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","df39cc62":"import torch.nn as nn\nfrom torch.nn import Linear, ReLU, BatchNorm1d, Dropout\nfrom torch.nn.init import orthogonal_\n\n# input_dim  = len(columns)\noutput_dim = 1 # regression\n\n# NeuralNet\nclass NeuralNet(nn.Module):\n    def __init__(self, input_dim):\n        super(NeuralNet, self).__init__()\n        \n        self.input_dim = input_dim\n        units = [input_dim, 512, 512, 512, output_dim]\n\n        # define Linear\n        self.Linear1 = Linear(units[0], units[1], bias = True)\n        self.Linear2 = Linear(units[1], units[2], bias = True)\n        self.Linear3 = Linear(units[2], units[3], bias = True)\n        self.Linear4 = Linear(units[3], units[4], bias = True)                \n\n        # weight initialization\n        orthogonal_(self.Linear1.weight)\n        orthogonal_(self.Linear2.weight)\n        orthogonal_(self.Linear3.weight)\n        orthogonal_(self.Linear4.weight)\n        \n        # BatchNormalization\n        self.bn1 = BatchNorm1d(units[1])\n        self.bn2 = BatchNorm1d(units[2])\n        self.bn3 = BatchNorm1d(units[3])\n        \n        # activation function & dropout\n        self.relu = ReLU()\n        self.dropout = Dropout(0.2)\n\n    def forward(self, x):\n        x = self.dropout(self.relu(self.bn1(self.Linear1(x))))\n        x = self.dropout(self.relu(self.bn2(self.Linear2(x))))\n        x = self.dropout(self.relu(self.bn3(self.Linear3(x))))\n        x = self.Linear4(x)\n        return x","e7687fad":"!pip install livelossplot","639916ea":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom livelossplot import PlotLosses\n\n# \ubaa8\ub378 \ud6c8\ub828\ubd80\ud130 submit \uc81c\ucd9c\uae4c\uc9c0\nclass Model(): # train, test, submit_df, column\n    def __init__(self, train_df_all, test_df_all, submit_df, columns):\n        self.train_df_all, self.test_df_all, self.submit_df, self.columns = train_df_all, test_df_all, submit_df, columns\n        self.NeuralNet = NeuralNet(len(columns)).to(device)\n        \n            \n    # \uc804\ucc98\ub9ac\n    def scailing(self, train_df_all, test_df_all, columns): # train, test, \uc801\uc6a9\ud560 \uceec\ub7fc\n        scaler = StandardScaler()\n        train_df_all[columns] = scaler.fit_transform(train_df_all[columns])\n        test_df_all[columns]  = scaler.transform(test_df_all[columns])\n        return train_df_all, test_df_all\n    \n    # \ud6c8\ub828-\uac80\uc99d \uc138\ud2b8 \ubd84\ub9ac\n    def get_train_validation(self, train_df_all, columns): #train, \uc801\uc6a9\ud560 \uceec\ub7fc\n#         y = train_df_all['Camera_S_Z'].values\n        y = train_df_all['Camera_S_Z'].values.reshape(-1,1)\n        train_x, val_x, train_y, val_y = train_test_split(train_df_all[columns].values, y)\n        \n        return train_x, val_x, train_y, val_y\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    def fit(self, lr = 1e-3, batch_size = 128, nb_epochs = 30):\n        train_df_all, test_df_all = self.scailing(self.train_df_all, self.test_df_all, self.columns)\n        train_x, val_x, train_y, val_y = self.get_train_validation(train_df_all, self.columns)\n        test_x = test_df_all[self.columns].values\n        \n        lr = lr\n        batch_size = batch_size\n        nb_epochs = nb_epochs\n\n        train_x_tensor = torch.Tensor(train_x).to(device)\n        val_x_tensor = torch.Tensor(val_x).to(device)\n        self.test_x_tensor = torch.Tensor(test_x).to(device)  # predict\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uac83\uc784.\n\n        train_y_tensor = torch.Tensor(train_y).to(device)\n        val_y_tensor = torch.Tensor(val_y).to(device)\n\n        optimizer = torch.optim.Adam(self.NeuralNet.parameters(), lr = lr, weight_decay=1e-4)\n        criterion = torch.nn.L1Loss().to(device) # MAE LOSS\n\n        from torch.utils.data import TensorDataset, DataLoader\n        data_loader = DataLoader(TensorDataset(train_x_tensor, train_y_tensor),\n                                 batch_size=batch_size,\n                                 shuffle = True,\n                                 drop_last = True)\n\n        liveloss = PlotLosses()\n        logs = {}\n        \n        for epoch in range(nb_epochs):\n            self.NeuralNet.train()\n            for X,Y in data_loader:\n                # train                \n                H = self.NeuralNet(X)\n                cost = criterion(H, Y)\n\n                optimizer.zero_grad()\n                cost.backward()\n                optimizer.step()\n\n            # validation\n            H = self.NeuralNet(train_x_tensor)    \n            self.NeuralNet.eval()\n            v_H = self.NeuralNet(val_x_tensor)\n\n            cost = criterion(H, train_y_tensor)    \n            v_cost = criterion(v_H, val_y_tensor)\n            \n            logs['train_loss'] = cost.item()\n            logs['val_loss'] = v_cost.item()\n\n            liveloss.update(logs)\n            liveloss.draw()\n\n    # \uc608\uce21\ud55c \uac12\uc73c\ub85c submit_df\ub97c \ucc44\uc6b0\uace0 submit.csv\ub85c \uc800\uc7a5\n    def predict_and_filled_submit(self): \n        test_df_all, submit_df = self.test_df_all.copy(), self.submit_df.copy()\n        \n        # make column same as submit_df\n        new_column = 'identifier_PacketCounter'\n        test_df_all.insert(0, new_column, test_df_all['identifier'].astype(str) + \"_\" + test_df_all['PacketCounter'].astype(int).astype(str))\n        test_df_all = test_df_all.drop(columns = ['identifier', 'PacketCounter'])\n        \n        # predict\n        self.NeuralNet.eval()\n        test_df_all['Camera_S_Z'] = self.NeuralNet(self.test_x_tensor).cpu().detach().numpy() \n\n        Right = test_df_all[['identifier_PacketCounter', 'Camera_S_Z']]\n        Left  = submit_df\n\n        Left = Left.set_index('identifier_PacketCounter').join(Right.set_index('identifier_PacketCounter'), lsuffix='_').drop(columns = 'Camera_S_Z_').reset_index()\n\n        submit_df['Camera_S_Z'] = Left['Camera_S_Z'].fillna(0)\n\n        submit_df.to_csv(\"submit.csv\", mode='w', index= False)  \n        \n        return submit_df","5b3646e1":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolumns = ['SampleTimeFine', 'Euler_X','Euler_Y','Euler_Z','FreeAcc_X','FreeAcc_Y','FreeAcc_Z','Camera_S_Z']\nplt.figure(figsize=(10,10))\nsns.heatmap(train_df_all[columns].corr(), annot=True, fmt=\".01f\")\nplt.show()","7f73e9f7":"columns = ['Euler_X','Euler_Y','Euler_Z','FreeAcc_X','FreeAcc_Y','FreeAcc_Z']","67112b69":"model = Model(train_df_all, test_df_all, submit_df, columns)\nmodel.fit() # default = lr = 1e-3, batch_size = 256, nb_epochs = 10","2b030de6":"columns = ['FreeAcc_Z']","c39110db":"model = Model(train_df_all, test_df_all, submit_df, columns)\nmodel.fit() # default = lr = 1e-3, batch_size = 256, nb_epochs = 10","4119c6ec":"# reference : https:\/\/stackoverflow.com\/questions\/25191620\/creating-lowpass-filter-in-scipy-understanding-methods-and-units\nfrom scipy.signal import butter, lfilter, freqz\n\ndef butter_lowpass(cutoff=0.8, fs=30.0, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\ndef butter_lowpass_filter(data, cutoff=0.8, fs=30.0, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n# ***\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc2dc \uc870\uc808\ud574\uc57c \ud558\ub294 \ubd80\ubd84***\norder = 6\nfs = 30.0      # sample rate, Hz (\uc218\uc815\ud560 \ud544\uc694 \uc5c6\uc74c)\ncutoff = 0.8   # desired cutoff frequency of the filter, Hz\n\ndef get_filter(df, order=6, fs=30.0, cutoff=0.8):\n    filter = butter_lowpass_filter(df['FreeAcc_Z'], cutoff, fs, order)\n    return filter\n\ni = 0\nfilter = get_filter(train_df_list[i], order, fs, cutoff)\nplt.figure(figsize=(20,5))\nplt.plot(train_df_list[i]['FreeAcc_Z'], 'b-', label='original')\nplt.plot(filter, 'g-', linewidth=2, label='filtered')\nplt.xlabel('Time [sec]')\nplt.grid()\nplt.legend()\n","1e6d319b":"# LPF \uc5bb\uae30\nidentifiers = np.append(test_df_all['identifier'].unique(),[train_df_all['identifier'].unique()])\nfor i in identifiers:\n    if i >= 15: # train\n        train_df_list[i-identifier_begin]['filter'] = get_filter(train_df_list[i-identifier_begin])\n    else: # test\n        test_df_list[i-1]['filter'] = get_filter(test_df_list[i-1])","9e78e1cf":"# \ubd84\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud558\ub098\ub85c \ud569\uce58\uae30\ntrain_filter = np.array([])\ntest_filter = np.array([])\n\nfor i in identifiers:\n    if i >= 15: # train\n        train_filter = np.append(train_filter, train_df_list[i-identifier_begin]['filter'])\n    else: # test\n        test_filter = np.append(test_filter, test_df_list[i-identifier_begin]['filter'])\n\ntrain_df_all['filter'] = train_filter\ntest_df_all['filter'] = test_filter","08efbd5b":"columns = ['filter']","7122f429":"model = Model(train_df_all, test_df_all, submit_df, columns)\nmodel.fit() # default = lr = 1e-3, batch_size = 256, nb_epochs = 10","1738f004":"# SVR reference  : https:\/\/bskyvision.com\/163\nidentifier = 0\nsvr = train_df_list[identifier].copy()\n\nfrom sklearn.svm import SVR\n\ndef get_SVR(df, C=2, gamma=3):\n    X = df['SampleTimeFine']\n    X = np.reshape(np.array(X), (-1,1))\n    y = df['FreeAcc_Z']\n    svr_rbf = SVR(kernel='rbf', C=C, gamma = gamma)    \n    svr = svr_rbf.fit(X,y).predict(X)\n    return svr\n\n#***\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc2dc \uc870\uc808\ud574\uc57c \ud558\ub294 \ubd80\ubd84***\nC = 2 \ngamma = 3 \n\nsvr = get_SVR(train_df_list[identifier], C, gamma)\n\nplt.figure(figsize=(20,5))\nplt.plot(train_df_list[identifier]['FreeAcc_Z'], label = 'original')\nplt.plot(svr, label = 'svr')\nplt.title('FreeAcc_Z')\nplt.legend()\n","7f57c79c":"# SVR \uc5bb\uae30\nidentifiers = np.append(test_df_all['identifier'].unique(),[train_df_all['identifier'].unique()])\nfor i in identifiers:\n    if i >= 15: # train\n        train_df_list[i-identifier_begin]['svr'] = get_SVR(train_df_list[i-identifier_begin])\n    else: # test\n        test_df_list[i-1]['svr'] = get_SVR(test_df_list[i-1])","790cea75":"# \ubd84\ub9ac\ub41c \ub370\uc774\ud130\ub97c \ud558\ub098\ub85c \ud569\uce58\uae30\ntrain_svr = np.array([])\ntest_svr = np.array([])\n\nfor i in identifiers:\n    if i >= 15: # train\n        train_svr = np.append(train_svr, train_df_list[i-identifier_begin]['svr'])\n    else: # test\n        test_svr = np.append(test_svr, test_df_list[i-identifier_begin]['svr'])\n\ntrain_df_all['svr'] = train_svr\ntest_df_all['svr'] = test_svr","12844664":"columns = ['svr']","5cd02f6d":"model = Model(train_df_all, test_df_all, submit_df, columns)\nmodel.fit() # default = lr = 1e-3, batch_size = 256, nb_epochs = 10","42da397a":"model.predict_and_filled_submit()","7652c0a6":"sns.heatmap(train_df[['FreeAcc_Z','svr','filter','Camera_S_Z']].corr(), annot=True, fmt='.1f')","a9b66d1f":"#### baseline \ucf54\ub4dc\ub294 1\ubc88 \ubc29\ubc95\uc744 \uc774\uc6a9\ud558\uc600\uc9c0\ub9cc\n#### 2) \uc13c\uc11c \uac00\uc18d\ub3c4\ub85c \uce74\uba54\ub77c \uac00\uc18d\ub3c4\ub97c \ud559\uc2b5\ud55c \ub4a4 \uc774\uc911 \uc801\ubd84\n#### 3) \uc13c\uc11c \uac00\uc18d\ub3c4\ub97c \uc774\uc911\uc801\ubd84\ud55c \ub4a4 \uce74\uba54\ub77c \uc704\uce58\ub97c \ud559\uc2b5 \ub4f1\uc758 \ubc29\ubc95\uc740 \uc5b4\ub5a8\uae4c\uc694?","3db63453":"### \ub2f5\uc548 \uc81c\ucd9c(\ubc29\ubc95 2-3\uc73c\ub85c \uc81c\ucd9c\ud558\uc600\uc74c)","69384003":"## \ub2e4\ub978 \ubc29\ubc95?\n### \ub354 \uc88b\uc740 \ubc29\ubc95\uc774 \uc788\uc744\uae4c\uc694?","b4667159":"# \uc0c1\uad00\uad00\uacc4 \ubd84\uc11d","7f335f98":"# \ubc29\ubc95 2-3. SVR \ucc98\ub9ac\ub41c FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n### Support Vector Regression\uc744 \uc774\uc6a9\ud558\uc5ec FreeAcc_Z\uc758 \ub178\uc774\uc988\ub97c \uc81c\uac70\ud55c\ub4a4 \ud2b9\uc131\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","5a754e23":"### \u03bcs \ub2e8\uc704\uc778 SampleTimeFine \uc744 sec \ub2e8\uc704\ub85c \ubc14\uafbc\ub2e4.","6bdc6424":"# \ubc29\ubc95 2-2. LPF \ucc98\ub9ac\ub41c FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n### Low Pass Filter\ub97c \uc774\uc6a9\ud558\uc5ec FreeAcc_Z\uc758 \ub178\uc774\uc988(\uace0\uc8fc\ud30c \uc131\ubd84)\uc744 \uc81c\uac70\ud55c\ub4a4 \ud2b9\uc131\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","cdfa81b2":"#### LPF \uc801\uc6a9 \uc608\uc2dc","768ca1ba":"## FreeAcc_Z \uac00 Camera_S_Z\uc640 \uac00\uc7a5 \ud070 \uc0c1\uad00\uad00\uacc4(\uc74c\uc758 \uc0c1\uad00\uad00\uacc4)\ub97c \uac00\uc9d0\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n## \uc5ec\uae30\uc11c \ubc29\ubc95\uc774 \ub098\ub269\ub2c8\ub2e4.\n#### \ubc29\ubc95 1. \ubaa8\ub4e0 \ud2b9\uc131\uc744 \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n#### \ubc29\ubc95 2. FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n#### \ubc29\ubc95 2-2. LPF \ucc98\ub9ac\ub41c FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n#### \ubc29\ubc95 2-3. SVR \ucc98\ub9ac\ub41c FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21","35bef4c5":"# identifier  \ubcc4\ub85c \ub098\ub208\ub2e4.\n### train_df_list\ub97c \ub9cc\ub4e4\uace0 \uac01\uac01\uc758 identifier\ub97c \ub123\ub294\ub2e4.\n","ef2cd731":"# \ubaa8\ub378 \uc124\uacc4","5c9922fa":"#### SVR \uc801\uc6a9 \uc608\uc2dc","7c5037d3":"# \ubc29\ubc952. FreeAcc_Z \ud2b9\uc131\ub9cc \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n### \uc0ac\uc6a9\ud558\ub294 \ud2b9\uc131 'FreeAcc_Z'","917faea6":"# \ubc29\ubc95 1. \ubaa8\ub4e0 \ud2b9\uc131\uc744 \uc774\uc6a9\ud574\uc11c Camera_S_Z\ub97c \uc608\uce21\n### \uc0ac\uc6a9\ud558\ub294 \ud2b9\uc131 'Euler_X','Euler_Y','Euler_Z','FreeAcc_X','FreeAcc_Y','FreeAcc_Z'","842c4c51":"![\ubc29\ubc95\ub4e4.png](attachment:be8aef5e-59e3-4363-968f-7de881689663.png)"}}