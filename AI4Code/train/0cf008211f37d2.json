{"cell_type":{"eb5325f7":"code","203d8d6d":"code","f78e530b":"code","08e152ed":"code","038fb4c9":"code","44e60c85":"code","bede6616":"code","a01f76c8":"code","1a002988":"code","3ae2f7f1":"code","46060b03":"code","632e9d67":"code","99bfedfe":"code","aa5dbba9":"code","3e82cf4e":"code","d4d79244":"code","43ec6ef3":"code","9c7677dc":"code","5b978855":"code","7d4638d5":"code","bbd2116e":"code","32e976d8":"code","1c332cf5":"code","19c0fd11":"code","1135999a":"code","0fdc9323":"code","04a9560e":"code","660ebc11":"code","fdf3e795":"code","af4ab637":"code","f490d69f":"code","c5f8ff46":"code","9ac63817":"code","db072d20":"code","31bf4b3f":"code","5e81d0a5":"code","4499ccc9":"markdown","1d43e999":"markdown","5c99f2d2":"markdown","8ef237fe":"markdown","b88ec7aa":"markdown","ee1a5390":"markdown","3e7b1b0b":"markdown","2386deaf":"markdown"},"source":{"eb5325f7":"import pandas as pd\nimport numpy as np\nimport re\nfrom gensim.models import KeyedVectors\nimport gensim.downloader as api\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn.functional import softmax\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","203d8d6d":"CUDA_ENABLED=True","f78e530b":"train_df = pd.read_csv('..\/input\/stanford-natural-language-inference-39\/train.csv', sep='\\t')\nval_df = pd.read_csv('..\/input\/stanford-natural-language-inference-39\/val.csv', sep='\\t')\ntest_df = pd.read_csv('..\/input\/stanford-natural-language-inference-39\/test.csv', sep='\\t')\ntrain_df.head()","08e152ed":"train_df.sentence1.sample(10)","038fb4c9":"def brief_clean(txt):\n    return re.sub(\"[^A-Za-z']+\", ' ', str(txt)).lower().replace(\"'\", '')","44e60c85":"train_df['sentence1_clean'] = train_df.sentence1.apply(brief_clean)\nval_df['sentence1_clean'] = val_df.sentence1.apply(brief_clean)\ntest_df['sentence1_clean'] = test_df.sentence1.apply(brief_clean)\n\ntrain_df['sentence2_clean'] = train_df.sentence2.apply(brief_clean)\nval_df['sentence2_clean'] = val_df.sentence2.apply(brief_clean)\ntest_df['sentence2_clean'] = test_df.sentence2.apply(brief_clean)","bede6616":"train_df.Category.head(5)","a01f76c8":"train_df.Category.unique()","1a002988":"train_df = train_df[train_df.Category!='-']\nval_df = val_df[val_df.Category!='-']","3ae2f7f1":"train_df.Category.unique()","46060b03":"category_encoder = LabelEncoder()\ntrain_df['target'] = category_encoder.fit_transform(train_df.Category)\nval_df['target'] = category_encoder.transform(val_df.Category)","632e9d67":"train_df.target.head(5)","99bfedfe":"max_vocab_size = 100000","aa5dbba9":"#w2v_model = api.load('word2vec-google-news-300')","3e82cf4e":"w2v_model = KeyedVectors.load_word2vec_format('..\/input\/google-news-w2v\/Google.bin', binary=True, limit=max_vocab_size)","d4d79244":"w2v_model['cat']","43ec6ef3":"embeddings = w2v_model.vectors[:max_vocab_size,:]\nembeddings = np.concatenate((np.zeros((1,300)), embeddings))\nembeddings.shape","9c7677dc":"word2id = {k:i+1 for i,k in enumerate(w2v_model.index2word) if i <max_vocab_size}","5b978855":"print('word id: {}'.format(word2id['cat']))\nprint('word vector:', embeddings[word2id['cat']])","7d4638d5":"#Indexing the sentences words by the w2v dictionary\ndef preprocess_sentence(sentence, word2id, other_id=0):\n    sentence = sentence.split(' ')\n    sentence = np.array([word2id[c] if c in word2id else other_id for c in sentence])\n    return sentence","bbd2116e":"train_df['x1'] = train_df.sentence1_clean.apply(lambda x: preprocess_sentence(x, word2id))\ntrain_df['x2'] = train_df.sentence2_clean.apply(lambda x: preprocess_sentence(x, word2id))\n\nval_df['x1'] = val_df.sentence1_clean.apply(lambda x: preprocess_sentence(x, word2id))\nval_df['x2'] = val_df.sentence2_clean.apply(lambda x: preprocess_sentence(x, word2id))\n\ntest_df['x1'] = test_df.sentence1_clean.apply(lambda x: preprocess_sentence(x, word2id))\ntest_df['x2'] = test_df.sentence2_clean.apply(lambda x: preprocess_sentence(x, word2id))","32e976d8":"class Dataset(data.Dataset):\n  def __init__(self, df, CUDA_ENABLED=True):\n        self.length = len(df)\n        self.X1 = df.x1\n        self.X2 = df.x2\n        self.cuda = CUDA_ENABLED\n        if 'target' in df.columns:\n            self.has_target = True\n            self.target = df.target\n        else:\n            self.has_target = False\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return self.length\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n        x1 = torch.LongTensor(self.X1.iloc[index])\n        x2 = torch.LongTensor(self.X2.iloc[index])\n        if self.cuda:\n            x1= x1.cuda()\n            x2= x2.cuda()\n        if self.has_target:\n            y = self.target.iloc[index]\n            return x1, x2, y\n        else:\n            return x1, x2","1c332cf5":"train_ds = Dataset(train_df, CUDA_ENABLED)\nval_ds = Dataset(val_df, CUDA_ENABLED)\ntest_ds =  Dataset(test_df, CUDA_ENABLED)","19c0fd11":"def colllate_tow_elements(samples):\n    X1 = []\n    X2 = []\n    for x1, x2 in samples:\n        X1.append(x1)\n        X2.append(x2)\n    return X1, X2\ndef colllate_three_elements(samples):\n    X1 = []\n    X2 = []\n    Y = []\n    for x1, x2, y in samples:\n        X1.append(x1)\n        X2.append(x2)\n        Y.append(y)\n    return X1, X2, Y\n    ","1135999a":"train_generator = data.DataLoader(train_ds, shuffle=True, batch_size=32, collate_fn=colllate_three_elements, drop_last=True)\nval_generator = data.DataLoader(val_ds, shuffle=True, batch_size=32, collate_fn=colllate_three_elements, drop_last=True)\ntest_generator = data.DataLoader(test_ds, shuffle=False, batch_size=32, collate_fn=colllate_tow_elements, drop_last=False)","0fdc9323":"for X1, X2, y in train_generator:\n    print (len(X1), len(X2), len(y))\n    print (X1[0], X2[0], y)\n    break","04a9560e":"#this is the class for my MLP classifier, with w2v-initialized embedding layer\nclass TwoInputsMLPClassifier(torch.nn.Module):\n\n    def __init__(self, n_vocab, embed_init, embedding_dim, hidden_dim, n_traget):\n        super(TwoInputsMLPClassifier, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.n_hidden = hidden_dim\n        self.n_vocab = n_vocab\n        self.n_target = n_traget\n        self.word_embeddings = nn.Embedding(n_vocab, embedding_dim)\n        self.word_embeddings.weight.data.copy_(torch.from_numpy(embed_init))\n        self.h1 = nn.Linear(self.embedding_dim, self.n_hidden)\n        self.h2 = nn.Linear(self.n_hidden, self.n_hidden)\n        self.h3 = nn.Linear(2*self.n_hidden, self.n_hidden)\n        self.h2traget = nn.Linear(self.n_hidden, self.n_target)\n\n    def forward(self, x1, x2):    \n        #embedding every sentence (average over words)\n        embeds1 = [torch.mean(self.word_embeddings(s), dim=0) for s in x1]\n        embeds2 = [torch.mean(self.word_embeddings(s), dim=0) for s in x2]\n        \n        #represent the batch as a 2-dim matrix (rater than list of tensors)\n        embeds1 = torch.stack(embeds1)\n        embeds2 = torch.stack(embeds2)\n        \n        #MLP of 2-layers for each embedded sentence\n        o1 = torch.tanh(self.h1(embeds1))\n        o2 = torch.tanh(self.h1(embeds2))\n        o1 = torch.tanh(self.h2(o1))\n        o2 = torch.tanh(self.h2(o2))\n        \n        #concat the representation of the two sentences togther\n        #insert into a two layer MLP (without any activation in the end)\n        o = torch.cat([o1, o2], dim=1)\n        o = torch.tanh(self.h3(o))\n        out = self.h2traget(o)\n\n        return out","660ebc11":"#the main train loop\ndef train(model, train_generator, val_generator,epochs=1, batch_size=32, lr=0.001, print_every=100):\n\n    model.train()\n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    counter = 0\n    for e in range(epochs):\n        for x1, x2, y in train_generator:\n            counter += 1\n\n            model.zero_grad()\n            \n            output = model(x1, x2)\n            \n            y = torch.LongTensor(y)\n            if CUDA_ENABLED:\n                y = y.cuda()\n            loss = criterion(output, y)\n            loss.backward()\n            opt.step()\n            \n            if counter % print_every == 0:\n                val_losses = []\n                val_preds = []\n                val_trues = []\n                model.eval()\n                for x1, x2, y in val_generator:                    \n\n                    output = model(x1, x2)\n                    val_preds.append(softmax(output, dim=1).cpu().detach().numpy())\n                    val_trues.append(y)\n\n                    y = torch.LongTensor(y)\n                    if CUDA_ENABLED:\n                        y = y.cuda()\n                    val_loss = criterion(output, y)\n                    val_losses.append(val_loss.item())\n          \n                \n                model.train() \n                val_preds = np.concatenate(val_preds)\n                val_trues = np.concatenate(val_trues)\n                print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n                      \"Val Acc: {:.4f}\".format(accuracy_score(val_trues, np.argmax(val_preds,axis=1)))\n                     )\n            \n","fdf3e795":"model = TwoInputsMLPClassifier(n_vocab=embeddings.shape[0], embed_init=embeddings, embedding_dim=embeddings.shape[1], \n                               hidden_dim=100, n_traget=len(category_encoder.classes_))\nif CUDA_ENABLED:\n    model.cuda()","af4ab637":"train(model, train_generator, val_generator)","f490d69f":"test_preds = []\nmodel.eval()\nfor x1, x2, in test_generator:                    \n    output = model(x1, x2)\n    test_preds.append(softmax(output, dim=1).cpu().detach().numpy())\ntest_preds = np.concatenate(test_preds)","c5f8ff46":"test_preds.shape","9ac63817":"final_preds = np.argmax(test_preds, axis=1)","db072d20":"final_preds = category_encoder.inverse_transform(final_preds)","31bf4b3f":"test_df['Category'] = final_preds","5e81d0a5":"test_df[['Id', 'Category']].to_csv('submission.csv', index=False)","4499ccc9":"# Create predictions","1d43e999":"# Loading and cleaning the data","5c99f2d2":"# Preparing the data for a model","8ef237fe":"ptretrained weights download link:\nhttps:\/\/drive.google.com\/file\/d\/0B7XkCwpI5KDYNlNUTTlSS21pQmM\/edit?usp=sharing","b88ec7aa":"### cleaning the text","ee1a5390":"### embedding the Category","3e7b1b0b":"# Load pretrained w2v weights (model) using gensim","2386deaf":"# Building and training a model"}}