{"cell_type":{"52fe305e":"code","830cef59":"code","8be7b6e9":"code","8c89161b":"code","906e116c":"code","6c104079":"code","395de92f":"code","0fe872dd":"code","aac13496":"code","a5cb194c":"code","51a5842d":"code","b1c60d31":"code","66b3b133":"code","4a364040":"code","7c6be833":"code","5435cf59":"code","66b67dc3":"code","a636e0c9":"code","dc268e2e":"code","0b107a6f":"code","e0a12b1a":"code","09dee995":"code","23a6370b":"code","b730d75a":"code","85c8afbe":"code","bd6df281":"markdown","5462632b":"markdown","91c46245":"markdown","6cbffdea":"markdown","a801eaa4":"markdown","2dda4699":"markdown","454355b4":"markdown","b768ddcf":"markdown","54ef74ea":"markdown","b2ace6b0":"markdown","7cb44182":"markdown","4e345354":"markdown","783c61ae":"markdown","886754a3":"markdown","b4abdc20":"markdown","a3be7f17":"markdown","e23b0f66":"markdown","7c207aea":"markdown","c4dc1382":"markdown","e8718f2b":"markdown","c5460c1b":"markdown","4fad5da3":"markdown","e9437fda":"markdown","5b97abe7":"markdown","c1a2bb82":"markdown","d3702706":"markdown","10ffb478":"markdown"},"source":{"52fe305e":"import pandas as pd \nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\n%matplotlib inline","830cef59":"!pip install openpyxl\ndata = pd.read_excel('..\/input\/baylor-religion-survey\/Baylor Religion Survey Wave IV (2014).XLSX') #Import the data\ndata_sne = data[['Q23A','Q23C','Q23G']]  #Keep the three metrics answers\ndata_sne.columns = ['Devil','Hell','Demon']","8be7b6e9":"from sklearn.decomposition import PCA\n\n# PCA\ndata_sne_d = data_sne.dropna() \npca = PCA(n_components=1, svd_solver='full') \npca.fit(data_sne_d) \nprint(\"Explained variance ratio:\", \"{:.2f}\".format(pca.explained_variance_ratio_[0]),'\\n')\n\n# Create the index\ndata['Supernatural_Evil'] = data_sne[['Devil','Hell','Demon']].mean(axis=1) \ndata_sne = data[['Supernatural_Evil']] \nprint('The new index mean is:', \"{:.2f}\".format(data['Supernatural_Evil'].mean())) ","8c89161b":"#### Dependent Variables ####\ndata_d = data[['Q73A','Q73E','Q73G','Q73H','Q73C','Q73F','Q73B','Q73D']] #Data Selection\ndata_d.columns = ['Semi_Auto_Guns','High_Capacity_Ammo_Clips','Civilian_Handguns','Concealed_Carry_Laws',\\\n                  'Armed_Security','Teachers_Guns','Gun_Safety_Programm','Mental_Health_Screening']\ndata_d = data_d.replace(2.0,0) #Data preparation\nprint('=' * 28,'Dependent Variables','=' * 28)\nprint(data_d.describe().T)\nprint('='*78)","906e116c":"#### Independent Variables (Part 1) #####\n\n#Religious Variables\ndata_rv = data[['Q4','Q17','Q17','Q17']] #Data Selection\ndata_rv.columns = ['Attendance','Bible','Biblical_Inerrancy','Biblical_Literalism']\ns1 = data_rv.Bible.replace([1,2,3,4,8],[0,0,1,1,1])\ns2 = data_rv.Biblical_Inerrancy.replace([1,2,3,4,8],[0,1,0,0,0])\ns3 = data_rv.Biblical_Literalism.replace([1,2,3,4,8],[1,0,0,0,0])\ndata_rv = pd.concat([data_rv.Attendance,s1,s2,s3], axis=1)\nprint('=' * 27,'Religious Variables','=' * 25)\nprint(data_rv.describe().T)\nprint('='*73,'\\n')\n\n#Religious Affiliation \ndata_ra = data[['RELTRAD','RELTRAD','RELTRAD','RELTRAD','RELTRAD','RELTRAD']] #Data Selection\ndata_ra.columns = ['Conservative_Protestant','Mainline_Protestant','Black_Protestant',\\\n                   'Catholic','Other_Religion','No_Affiliation']\ns1 = data_ra.Conservative_Protestant.replace([1,2,3,4,5,6,7],[1,0,0,0,0,0,0])\ns2 = data_ra.Mainline_Protestant.replace([1,2,3,4,5,6,7],[0,1,0,0,0,0,0])\ns3 = data_ra.Black_Protestant.replace([1,2,3,4,5,6,7],[0,0,1,0,0,0,0])\ns4 = data_ra.Catholic.replace([1,2,3,4,5,6,7],[0,0,0,1,0,0,0])\ns5 = data_ra.Other_Religion.replace([1,2,3,4,5,6,7],[0,0,0,0,1,1,0])\ns6 = data_ra.No_Affiliation.replace([1,2,3,4,5,6,7],[0,0,0,0,0,0,1])\ndata_ra = pd.concat([s1,s2,s3,s4,s5,s6], axis=1)\nprint('=' * 26,'Religious Affiliation','=' * 27)\nprint(data_ra.describe().T)\nprint('='*77)","6c104079":"#### Independent Variables (Part 2) #####\n\n#Political Ideology\ndata_med = data[['Q31']]\ndata_med.columns = ['Political_Ideology']\nprint('=' * 25,'Political Ideology','=' * 26)\nprint(data_med.describe().T)\nprint('='*71,'\\n')\n\n#Age \ndata_age = data[['AGE']]\ndata_age.columns = ['Age']\ndata_age = data_age.loc[(data_age[\"Age\"] >= 19) & (data_age[\"Age\"] <=99)]\nprint('=' * 29,'Age','=' * 29)\nprint(data_age.describe().T)\nprint('='*63,'\\n')\n\n#Sex  \ndata_sex = data[['I_GENDER','I_GENDER']]\ndata_sex.columns = ['Female','Male']\ns2 = data_sex.Male.replace([1,2],[1,0])\ns1 = data_sex.Female.replace([1,2],[0,1])\ndata_sex = pd.concat([s1,s2], axis=1)\nprint('=' * 27,'Sex','=' * 27)\nprint(data_sex.describe().T)\nprint('='*59,'\\n')\n\n#Race  \ndata_r = data[['Q88A','Q89','Q88B','Q88F','Q88D','Q88C','Q88E']]\ndata_r.columns = ['White','Hispanic','African_American','Other_Race','Asian','American_Indian','Hawain']\ndata_r = data_r.dropna(how='all')\ns1 = data_r.White.replace([1,2,np.nan],[1,0,0])\ns2 = data_r.Hispanic.replace([1,2,3,4,5,8,np.nan],[0,1,1,1,1,0,0])\ns3 = data_r.African_American.replace([1,2,np.nan],[1,0,0])\ns4 = data_r.Other_Race.replace([1,2,np.nan],[1,0,0])\ns5 = data_r.Asian.replace([1,2,np.nan],[1,0,0])\ns6 = data_r.American_Indian.replace([1,2,np.nan],[1,0,0])\ns7 = data_r.Hawain.replace([1,2,np.nan],[1,0,0])\n #White\nconditions = [(s1==1) & (s2==1),\n              (s1==1) & (s6==1)]\nchoices = [1,1]\ns8 = pd.Series(np.select(conditions, choices), name = \"White_Hisp_Indian\")\ns9 = pd.Series(s1-s8, name = 'White')\n #Other_Race\ns10 = pd.Series(np.where((s4==1)|(s6==1), 1,0), name = 'Other_Race')\ndata_r = pd.concat([s9,s2,s3,s10], axis=1)\nprint('=' * 31,'Race','=' * 32)\nprint(data_r.describe().T)\nprint('='*69,'\\n')\n\n#Education  \ndata_ed = data[['Q90','Q90','Q90','Q90','Q90']]\ndata_ed.columns = ['Less_High_School','High_School','Some_College','College','Post_Graduate']\ns1 = data_ed.Less_High_School.replace([1,2,3,4,5,6,7],[1,1,0,0,0,0,0])\ns2 = data_ed.High_School.replace([1,2,3,4,5,6,7],[0,0,1,0,0,0,0])\ns3 = data_ed.Some_College.replace([1,2,3,4,5,6,7],[0,0,0,1,1,0,0])\ns4 = data_ed.College.replace([1,2,3,4,5,6,7],[0,0,0,0,0,1,0])\ns5 = data_ed.Post_Graduate.replace([1,2,3,4,5,6,7],[0,0,0,0,0,0,1])\ndata_ed = pd.concat([s1,s2,s3,s4,s5], axis=1)\nprint('=' * 29,'Education','=' * 29)\nprint(data_ed.describe().T)\nprint('='*70,'\\n')\n\n#Household Income  \ndata_hi = data[['Q95']]\ndata_hi.columns = ['Household_Income']\nprint('=' * 26,'Household Income','=' * 25)\nprint(data_hi.describe().T)\nprint('='*69,'\\n')\n\n#Marital Status \ndata_ms = data[['I_MARITAL','I_MARITAL']]\ndata_ms.columns = ['Single','Married']\ns1 = data_ms.Single.replace([1,2,3,4],[1,0,1,1])\ns2 = data_ms.Married.replace([1,2,3,4],[0,1,0,0])\ndata_ms = pd.concat([s1,s2], axis=1)\nprint('=' * 22,'Marital Status','=' * 22)\nprint(data_ms.describe().T)\nprint('='*60,'\\n')\n\n#Children  \ndata_ch = data[['Q93','Q93']]\ndata_ch.columns = ['No_kids_at_home','Kids_at_home']\ns2 = data_ch.Kids_at_home.replace([1,2,3,4,5,6,0],[0,0,0,0,0,0,1])\ns1 = data_ch.No_kids_at_home.replace([1,2,3,4,5,6,0],[1,1,1,1,1,1,0])\ndata_ch = pd.concat([s1,s2], axis=1)\nprint('=' * 28,'Children','=' * 29)\nprint(data_ch.describe().T)\nprint('='*67,'\\n')\n\n#Area demographics \ndata_ad = data[['Q80','Q80']]\ndata_ad.columns = ['Small_Town','Urban_Area']\ns1 = data_ad.Small_Town.replace([1,2,3,4,8],[0,0,1,1,np.nan])\ns2 = data_ad.Urban_Area.replace([1,2,3,4,8],[1,1,0,0,np.nan])\ndata_ad = pd.concat([s1,s2], axis=1)\nprint('=' * 21,'Area Demographics','=' * 21)\nprint(data_ad.describe().T)\nprint('='*61,'\\n')\n\n#Region \ndata_re = data[['STATE','STATE']]\ndata_re.columns = ['South','Other_Region']\n\ns1 = data_re.South.replace(['AL','AR','DE','DC','FL','GA','KY','LA','MD','MS','NC','OK','SC','TN','TX','VA','WV',\\\n                       'AK','AZ', 'CA','CO','CT','HI','ID','IL','IN','IA','KS','ME','MA','MI','MN','MO','MT',\\\n                       'NE','NV','NH','NJ','NM','NY','ND','OH','OR','PA','RI','SD','UT','VT','WA','WI','WY',],\\\n                       [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\\n                       0,0,0,0,0,0,0,0])\ns2 = data_re.Other_Region.replace(['AL','AR','DE','DC','FL','GA','KY','LA','MD','MS','NC','OK','SC','TN','TX','VA','WV',\\\n                       'AK','AZ', 'CA','CO','CT','HI','ID','IL','IN','IA','KS','ME','MA','MI','MN','MO','MT',\\\n                       'NE','NV','NH','NJ','NM','NY','ND','OH','OR','PA','RI','SD','UT','VT','WA','WI','WY',],\\\n                       [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\\\n                       1,1,1,1,1,1,1,1])\ndata_re = pd.concat([s1,s2], axis=1)\nprint('=' * 29,'Region','=' * 28)\nprint(data_re.describe().T)\nprint('='*65,'\\n')","395de92f":"# Combine the dataframes from independent variables\ndata_frames = [data_sne, data_rv, data_ra, data_med, data_age, data_sex, data_r,\\\n               data_ed, data_hi, data_ms, data_ch, data_ad, data_re]\ndf_ind_na = pd.concat(data_frames,axis=1)\n\n# Drop no needed columns (Independant Variables)\ndf_ind_na = df_ind_na.drop(['Bible','No_Affiliation','Male','White','High_School','Single','No_kids_at_home',\\\n                    'Other_Region','Small_Town'],axis=1)","0fe872dd":"# Multiple imputation via Iterative Imputer (Independent Variables dataframe)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimputer = IterativeImputer(max_iter = 20, random_state = 5, imputation_order = 'roman')\ndf_ind = pd.DataFrame(np.round(imputer.fit_transform(df_ind_na)), columns = list(df_ind_na.columns))","aac13496":"# Combine dataframe of Independent Variables with Dependent\ndf = pd.concat([data_d,df_ind],axis=1)\n\n# Create a dataset for each gun policy \/\/ drop N\/A values from dependent variables\ndf1 = df.dropna(subset = ['Semi_Auto_Guns'])\ndf2 = df.dropna(subset = ['High_Capacity_Ammo_Clips'])\ndf3 = df.dropna(subset = ['Civilian_Handguns'])\ndf4 = df.dropna(subset = ['Concealed_Carry_Laws'])\ndf5 = df.dropna(subset = ['Armed_Security'])\ndf6 = df.dropna(subset = ['Teachers_Guns'])\ndf7 = df.dropna(subset = ['Gun_Safety_Programm'])\ndf8 = df.dropna(subset = ['Mental_Health_Screening'])\n\n#Create the two models parameters\nall_columns = list(df.columns) \nd_var = all_columns[0:8]\nmodel_1 = all_columns[8:17]\nmodel_2 = all_columns[8:32]\ndf_all = (df1,df2,df3,df4,df5,df6,df7,df8)","a5cb194c":"from statsmodels.formula.api import logit\n\n#Logistic Regression \ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    formula1 = gun_pol + \" ~ \" + '+'.join(model_1)\n    data1 = df_all[i]\n    mod1 = logit(formula = formula1, data = data1).fit(disp=False)\n    coef1 = np.exp(mod1.params)\n    pval1 = mod1.pvalues\n    df_est.append(coef1)\n    df_est.append(pval1)\n    i=i+1\n\n#Results dataframe creation\ndf10 = pd.concat([df_est[0],df_est[1]], axis=1)\ndf10.columns = ['Coefficient','p_value']\ndf20 = pd.concat([df_est[2],df_est[3]], axis=1)\ndf20.columns = ['Coefficient','p_value']\ndf30 = pd.concat([df_est[4],df_est[5]], axis=1)\ndf30.columns = ['Coefficient','p_value']\ndf40 = pd.concat([df_est[6],df_est[7]], axis=1)\ndf40.columns = ['Coefficient','p_value']\ndf50 = pd.concat([df_est[8],df_est[9]], axis=1)\ndf50.columns = ['Coefficient','p_value']\ndf60 = pd.concat([df_est[10],df_est[11]], axis=1)\ndf60.columns = ['Coefficient','p_value']\ndf70 = pd.concat([df_est[12],df_est[13]], axis=1)\ndf70.columns = ['Coefficient','p_value']\ndf80 = pd.concat([df_est[14],df_est[15]], axis=1)\ndf80.columns = ['Coefficient','p_value']\n\nd1 = {'Semi_Auto_Guns':df10, 'High_Capacity_Ammo_Clips':df20,'Civilian_Handguns':df30,'Concealed_Carry_Laws':df40,\\\n     'Armed_Security':df50,'Teachers_Guns':df60,'Gun_Safety_Programm':df70,'Mental_Health_Screening':df80}\n\ndf_model1 = pd.concat(d1.values() , axis=1, keys=d1.keys())\ndf_model1.drop(index=df_model1.index[0], axis=0, inplace=True)\ndf_model1.style.format('{:.3f}').applymap(lambda x: f\"color: {'brown' if x>0 and x<=0.001 else 'black'}\")","51a5842d":"#Logistic Regression \ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    formula2 = gun_pol + \" ~ \" + '+'.join(model_2)\n    data2 = df_all[i]\n    mod2 = logit(formula = formula2, data = data2).fit(disp=False)\n    coef2 = np.exp(mod2.params)\n    pval2 = mod2.pvalues\n    df_est.append(coef2)\n    df_est.append(pval2)\n    i=i+1\n    \n#Results dataframe\ndf11 = pd.concat([df_est[0],df_est[1]], axis=1)\ndf11.columns = ['Coefficient','p_value']\ndf22 = pd.concat([df_est[2],df_est[3]], axis=1)\ndf22.columns = ['Coefficient','p_value']\ndf33 = pd.concat([df_est[4],df_est[5]], axis=1)\ndf33.columns = ['Coefficient','p_value']\ndf44 = pd.concat([df_est[6],df_est[7]], axis=1)\ndf44.columns = ['Coefficient','p_value']\ndf55 = pd.concat([df_est[8],df_est[9]], axis=1)\ndf55.columns = ['Coefficient','p_value']\ndf66 = pd.concat([df_est[10],df_est[11]], axis=1)\ndf66.columns = ['Coefficient','p_value']\ndf77 = pd.concat([df_est[12],df_est[13]], axis=1)\ndf77.columns = ['Coefficient','p_value']\ndf88 = pd.concat([df_est[14],df_est[15]], axis=1)\ndf88.columns = ['Coefficient','p_value']\n\nd2 = {'Semi_Auto_Guns':df11, 'High_Capacity_Ammo_Clips':df22,'Civilian_Handguns':df33,'Concealed_Carry_Laws':df44,\\\n     'Armed_Security':df55,'Teachers_Guns':df66,'Gun_Safety_Programm':df77,'Mental_Health_Screening':df88}\n\ndf_model2 = pd.concat(d2.values() , axis=1, keys=d2.keys())\ndf_model2.drop(index=df_model2.index[0], axis=0, inplace=True)\ndf_model2.style.format('{:.3f}').applymap(lambda x: f\"color: {'brown' if x>0 and x<0.001 else 'black'}\")","b1c60d31":"import statsmodels.formula.api as smf\n\n#Linear Regression coefficients \/\/\/ Model 1\ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    formula1_4 = gun_pol + \" ~ \" + '+'.join(model_1)\n    data1_4 = df_all[i]\n    mod1_4 = smf.ols(formula = formula1_4, data = data1_4).fit(disp=False)\n    coef1_4 = mod1_4.params\n    df_est.append(coef1_4)\n    df_coef1_4 = pd.concat(df_est, axis=1)\n    i=i+1\ndf_coef1_4.drop(labels = ['Intercept'], inplace = True)\ndf_coef1_4.columns=list(d_var)\n\n#Linear Regression coefficients \/\/\/ Model 2\ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    formula2_4 = gun_pol + \" ~ \" + '+'.join(model_2)\n    data2_4 = df_all[i]\n    mod2_4 = smf.ols(formula = formula2_4, data = data2_4).fit(disp=False)\n    coef2_4 = mod2_4.params\n    df_est.append(coef2_4)\n    df_coef2_4 = pd.concat(df_est, axis=1)\n    i=i+1\ndf_coef2_4.drop(labels = ['Intercept'], inplace = True)\ndf_coef2_4.columns=list(d_var)","66b3b133":"# Standar Deviation of Dependent Variables\ndf_sd_d=[]\ni = 0\nfor gun_pol in list(d_var):\n    data = df_all[i]\n    sd_d = data[gun_pol].std()\n    df_sd_d.append(sd_d)\n    i=i+1\n\n# Standar Deviation of Independent Variables \/\/\/ Model 1\ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    data = df_all[i]\n    sd_ind = data.std().loc['Supernatural_Evil':'Other_Religion']\n    df_est.append(sd_ind)\n    df_sd_i = pd.concat(df_est, axis=1)\n    i=i+1\ndf_sd_i.columns=list(d_var)\ndf_sd_i\n\n# Standar Coefficients \/\/\/ Model 1\nsc1 = df_sd_i.mul(df_coef1_4)\nsc_1 = sc1.div(df_sd_d,axis=1)\nsc_1.abs().style.format('{:.3f}').highlight_max(color = 'coral', axis = 0)","4a364040":"# Plotting \/\/ Model 1\n\nsc1 = sc_1.abs().Semi_Auto_Guns.sort_values(ascending=False)\nsc2 = sc_1.abs().High_Capacity_Ammo_Clips.sort_values(ascending=False)\nsc3 = sc_1.abs().Civilian_Handguns.sort_values(ascending=False)\nsc4 = sc_1.abs().Concealed_Carry_Laws.sort_values(ascending=False)\nsc5 = sc_1.abs().Armed_Security.sort_values(ascending=False)\nsc6 = sc_1.abs().Teachers_Guns.sort_values(ascending=False)\nsc7 = sc_1.abs().Gun_Safety_Programm.sort_values(ascending=False)\nsc8 = sc_1.abs().Mental_Health_Screening.sort_values(ascending=False)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(4, 2, sharey=True, figsize=(18,28))\nfig.subplots_adjust(hspace = 0.7)\n\nsc1.plot(kind='bar', ax=axes[0,0], title = 'Semi_Auto_Guns').grid(axis = 'y')\nsc2.plot(kind='bar', ax=axes[0,1], title = 'High_Capacity_Ammo_Clips').grid(axis = 'y')\nsc3.plot(kind='bar', ax=axes[1,0], title = 'Civilian_Handguns').grid(axis = 'y')\nsc4.plot(kind='bar', ax=axes[1,1], title = 'Concealed_Carry_Laws').grid(axis = 'y')\nsc5.plot(kind='bar', ax=axes[2,0], title = 'Armed_Security').grid(axis = 'y')\nsc6.plot(kind='bar', ax=axes[2,1], title = 'Teachers_Guns').grid(axis = 'y')\nsc7.plot(kind='bar', ax=axes[3,0], title = 'Gun_Safety_Programm').grid(axis = 'y')\nsc8.plot(kind='bar', ax=axes[3,1], title = 'Mental_Health_Screening').grid(axis = 'y')\n\nplt.show()","7c6be833":"# Standar Deviation of Independent Variables \/\/\/ Model 2\ndf_est=[]\ni = 0\nfor gun_pol in list(d_var):\n    data = df_all[i]\n    sd_ind = data.std().loc['Supernatural_Evil':'South']\n    df_est.append(sd_ind)\n    df_sd_i = pd.concat(df_est, axis=1)\n    i=i+1\ndf_sd_i.columns=list(d_var)\n\n# Standar Coefficients \/\/\/ Model 2\nsc2 = df_sd_i.mul(df_coef2_4)\nsc_2 = sc2.div(df_sd_d,axis=1)\nsc_2.abs().style.format('{:.3f}').highlight_max(color = 'coral', axis = 0)","5435cf59":"# Plotting \/\/ Model 2\n\nsc1 = sc_2.abs().Semi_Auto_Guns.sort_values(ascending=False)\nsc2 = sc_2.abs().High_Capacity_Ammo_Clips.sort_values(ascending=False)\nsc3 = sc_2.abs().Civilian_Handguns.sort_values(ascending=False)\nsc4 = sc_2.abs().Concealed_Carry_Laws.sort_values(ascending=False)\nsc5 = sc_2.abs().Armed_Security.sort_values(ascending=False)\nsc6 = sc_2.abs().Teachers_Guns.sort_values(ascending=False)\nsc7 = sc_2.abs().Gun_Safety_Programm.sort_values(ascending=False)\nsc8 = sc_2.abs().Mental_Health_Screening.sort_values(ascending=False)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(4, 2, sharey=True, figsize=(18,28))\nfig.subplots_adjust(hspace = 0.7)\n\nsc1.plot(kind='bar', ax=axes[0,0], title = 'Semi_Auto_Guns').grid(axis = 'y')\nsc2.plot(kind='bar', ax=axes[0,1], title = 'High_Capacity_Ammo_Clips').grid(axis = 'y')\nsc3.plot(kind='bar', ax=axes[1,0], title = 'Civilian_Handguns').grid(axis = 'y')\nsc4.plot(kind='bar', ax=axes[1,1], title = 'Concealed_Carry_Laws').grid(axis = 'y')\nsc5.plot(kind='bar', ax=axes[2,0], title = 'Armed_Security').grid(axis = 'y')\nsc6.plot(kind='bar', ax=axes[2,1], title = 'Teachers_Guns').grid(axis = 'y')\nsc7.plot(kind='bar', ax=axes[3,0], title = 'Gun_Safety_Programm').grid(axis = 'y')\nsc8.plot(kind='bar', ax=axes[3,1], title = 'Mental_Health_Screening').grid(axis = 'y')\n\nplt.show()","66b67dc3":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","a636e0c9":"X = df1.loc[:,model_2].values\ny = df1['Semi_Auto_Guns'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","dc268e2e":"X = df2.loc[:,model_2].values\ny = df2['High_Capacity_Ammo_Clips'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","0b107a6f":"X = df3.loc[:,model_2].values\ny = df3['Civilian_Handguns'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","e0a12b1a":"X = df4.loc[:,model_2].values\ny = df4['Concealed_Carry_Laws'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, \n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","09dee995":"X = df5.loc[:,model_2].values\ny = df5['Armed_Security'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","23a6370b":"X = df6.loc[:,model_2].values\ny = df6['Teachers_Guns'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)\n","b730d75a":"X = df7.loc[:,model_2].values\ny = df7['Gun_Safety_Programm'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","85c8afbe":"X = df8.loc[:,model_2].values\ny = df8['Mental_Health_Screening'].values\n\n#Step 1a (Selection of parameters to search for)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]# Number of trees in random forest\nmax_features = ['auto', 'sqrt']# Number of features to consider at every split\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]# Maximum number of levels in tree\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]# Minimum number of samples required to split a node\nmin_samples_leaf = [1, 2, 4]# Minimum number of samples required at each leaf node\nbootstrap = [True, False]# Method of selecting samples for training each tree\nrandom_grid = {'n_estimators': n_estimators, \n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}   # Create the random grid\n\n#Step 1b (Select best parameters)\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50,\n                               cv = 3, verbose=1, random_state=42, n_jobs = -1)\nrf_random.fit(X, y)\n\n#Step 2 (Run Random Forest for best parameters)\nbest_model_random = rf_random.best_estimator_\nbest_model_random.fit(X,y)\n\n#Step 3 (Evaluate model)\nscores = cross_val_score(best_model_random, X, y, cv=5, scoring='accuracy')\nprint('='*23)\nprint('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\nprint('='*23,'\\n')\n\n# Step 4 (Importance features)\nimportances = best_model_random.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_model_random.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nprint('='*10,'Importance Features','='*10,'\\n')\nfor f in range(X.shape[1]):\n    print(\"%d. Variable %d %s (%f)\" % (f + 1, indices[f], model_2[indices[f]], importances[indices[f]]))\n    \n# Plot the feature importances of the RF Classifier\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n        tick_label=[model_2[x] for x in indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X.shape[1]])\ng = plt.xticks(rotation=90)","bd6df281":"### Semi Auto Weapon Ban","5462632b":"## Part 4: Additional Estimations of the Strength of Predictors\n---\nIn addiion to logistic regression, we will run a complementary series of linear regressions to estimate the strength of the various predictors. In this way we will obtain standardized coefficients.\n\nStandardization of the coefficient is usually done to answer the question of which of the independent variables have a greater effect on the dependent variable in a multiple regression analysis where the variables are measured in different units of measurement. The higher the absolute value of the standardized coefficient, the stronger the effect. \n\n### Linear Regression","91c46245":"### Required Gun Safety Programms","6cbffdea":"### More Armed Security at Schools ","a801eaa4":"### Support for Concealed Carry","2dda4699":"## Part 5: Compare with Decision Trees \/ Random Forest Estimators\n---\nHaving worked with logistic and linear regression models, we will carry out predictions by using Decision Trees \/ Random Forests. Then, we will check if the predictors, primarily the belief in supernatural evil, make also a strong showing with the best model we have found. We will train our models with all available variables.\n\nThe steps we will follow are:\n* Select parameters for Random Forest Classifier\n* Run and fit the model to RF Classifier\n* Check the accuracy of our model\n* Find and plot the importance feature of each predictor","454355b4":"### Conclusion\n\nOur models score in accuracy between **0.65** and **0.93**. The best one is for *Gun Safety Programm* prediction and the worst one is the prediction for *Support for Concealed Carry*. \n\nThe *belief in supernatural evil* is among the top five predictors in all models. Apart from the latest, also strong predictors are the *age*, *political ideology*, *gender*, *household income* and *attendance*.","b768ddcf":"#### Results\n\nIn the second model we add all the covariates plus the political ideology in the first one. Political ideology and sex are the most significant statistically predictors now. So, we will compare how the the belief in supernatural evil affected by the political ideology in the gun policies. \n\nWe see a decline in support for banning *semi-automatic weapons* (***OR 0.73 -> 0.82***), *high-capacity ammunition magazines* (***OR 0.75 -> 0.84***), *civilian ownership of handguns* (***OR 0.68 -> 0.73***), with the effect of political ideology.\n\nOn the other hand the approval of *concealed carry of handguns* (***OR 1.5 -> 1.39***), *placing more policy and security at schools* (***OR 1.55 -> 1.33***), *arming teachers* (***OR 1.41 -> 1.30***) is a pattern that persists with adjustments of the political ideology.\n\nAs the same from above we will consider neligible the association between the belief in supernatural evil and the *support for more gun safety programms* <br>(***OR = 1.04***) and the *better mental health screening* of gun buyers (***OR = 0.83***) \n\n### Economist\n\n<code>A recent study by Christopher Ellison, Benjamin Dowd-Arrow, Amy Burdette and three other sociologists delves into an important but overlooked motivating factor that highlights the role of religion. A survey of 1,572 American adults found that, apart from religious denomination or religious conservatism, belief in the devil, demons and hell is a strong predictor of eight pro-gun beliefs, including arming teachers, carrying concealed firearms and bearing high-capacity defensive weapons. A Catholic who believes in supernatural evil is more likely to hold pro-gun views than a Protestant who does not believe that Satan is corrupting souls, and vice versa.\nThe analysis, which controls for political ideology and other demographic factors, found that each step up on a four-point scale measuring the strength of belief in supernatural evil correlated with 32% more support for arming teachers, and a 38% rise in backing for carrying concealed weapons. The effect of belief in supernatural evil on support for the right to carry concealed guns was roughly the same as having conservative politics or not possessing a college degree, but smaller than the effect of gender.<code>\n    \nAs we see from our findings and the research paper we don't agree with the last point of view about the support for the right to carry concealed guns. *Supernatural evil beliefs*, *Political ideology* and *Gender* are the most statistically significant predictors. ","54ef74ea":"## Part 2: Variables Selection\n---\nApart from the belief in supernatural evil metric, we will use several other variables to control our estimates. We will derive descriptive statitistics in them. \n<br>Firstly, we will select and prepare the dependent variables and secondly the independent ones.\n\n### Dependent Variables\n  * Ban on Semi-Auto Guns\n  * Ban on High-Capacity Ammo Clips \n  * Banning Civilian Handguns\n  * Support for Concealed Carry Laws \n  * More Armed Security at Schools \n  * More Teachers\/Faculty having Guns \n  * More Gun Safety Programs \n  * Expanded Mental Health Screening\n","b2ace6b0":"#### Model 2","7cb44182":"### Independent Variables (*Part 2*)\n  * Political Ideology\n  * Age\n  * Sex\n    * Female\n    * Male\n  * Race\n    * White\n    * Hispanic\n    * African American\/Black \n    * Other\n  * Education\n    * Less Than High School \n    * High School or Equivalent \n    * Some College\n    * College Degree \n    * Post-graduate Degree \n  * Household Income\n  * Marital Status\n    * Not Partnered\/Single \n    * Married\/Cohabitating\n  * Children\n    * No kids under 18 in home \n    * Kids under 18 in home\n  * Area Demographics\n    * Small Town\/Rural\n    * Urban Area\n  * Region\n    * South\n    * Other Region","4e345354":"### Civilian Hand Gun Possession Ban","783c61ae":"### Standard Coefficients\nTo calculate the standar coefficients we will take the standard deviation of the predictor variable, divide by the standard deviation of the response (Dependent Variable) and multiply by the regression coefficient for the predictor under consideration.\n\n#### Model 1\n","886754a3":"### Independent Variables (*Part 1*)\n   * Religious Variables\n     * Attendance\n     * Bible (Human Error, History and Fables) \n     * Biblical Inerrancy\n     * Biblical Literalism\n   * Religious Affiliation\n     * Conservative Protestant \n     * Mainline Protestant \n     * Black Protestant \n     * Catholic\n     * Other\n     * No Affiliation","b4abdc20":"### Logistic Regression \n\nWe will use Logistic Regression to estimate predicting support for the gun policies by supernatural evil presented in odds ratios. \n\n#### Model 1","a3be7f17":"### Single Metric (*test & creation*) \n\nWe will use the **Principal Component Analysis (PCA)** to reduce the dimensionality of our dataset (*Devil, Hell, Demon*) into one dimension. Then we will check how much information (variance) contains our new single metric.","e23b0f66":"### Conclusion\n\nAs we see from the results the new index (*Superntaural Evil*) can be combined from the three questions and keep the ***90%*** of the information. The new index ranges from 1 to 4 with a mean of *3.08*, indicating significant levels of overall belief in supernatural evil among the BRS respondents.","7c207aea":"## Part 1: Belief in Supernatural Evil Metric\n---\nFrom the study, we expect that belief in supernatural evil will be associated with more liberal gun policy attitudes. First, we expect that belief in supernatural evil will be associated with more liberal gun policy attitudes. Second, given the associations between supernatural evil belief and political ideology  and the increasing politicization of firearms policy issues, we expect that the associations described above will be partly mediated by political ideology. <br>\n\nIn order to measure the belief in supernatural evil, we will use the answers to three questions asked by the participants in the survey:\n\n* Whether the respondent believes in the devil.\n\n* Whether the respondent believes in hell.\n\n* Whether the respondent believes in demons.\n\nThen, we will test if we can combine the answers to these three questions to a single metric (*Supernatural Evil*). \n\n---","c4dc1382":"#### Results\n\nAs mentioned above the coefficients are represent in odd ratios (OR). Also we respresent the p-values of each independent variable and highlight the ones that are statistically significant (***p \u2264 0.001***). On this model We will focus primarly on supernatural evil metric as it is statistically significant in most of results.\n\nHaving consider only the *religious predictors* in this model, we see that each-one unit increment in the strenght of one's belief in supernatural evil is associated with *27%* decline in support for *banning semi-automatic weapons* (***OR = 0.73***). A similar pattern occurs in the opinion from *ban high capacity ammunition magazines* (***OR = 0.75***) and *civilian ownership of handguns* (***OR = 0.68***). \n\nSupernatural evil beliefs are posivitely associated with approval of laws allowing *concealed carry of handguns* (***OR = 1.5***), *the placement of more police and security in schools* (***OR = 1.55***), *arming teachers* (***OR = 1.41***).\n\nWe will consider neglible the association between the belief in supernatural evil and the *support for more gun safety programms* (***OR = 0.89***) and the *better mental health screening* of gun buyers (***OR = 0.74***) due to the high p-values.","e8718f2b":"# _Belief in Supernatural Evil and Guns_<br>\nIn this assignment we will replicate a study of belief in supernatural evil and attitudes towards guns in the United States. \nThe study is:\n\n>Christopher G. Ellison, Benjamin Dowd-Arrow, Amy M. Burdette, Pablo E. Gonzalez, Margaret S. Kelley, Paul Froese<br \/> \n\"**Peace through superior firepower: Belief in supernatural evil and attitudes toward gun policy in the United States**\"<br \/>\n>Social Science Research, Volume 99, 2021, https:\/\/doi.org\/10.1016\/j.ssresearch.2021.102595.\n\n---\n\n\n ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ee\/Gun_outline.svg)\n \n\n---\n >Panagiotis Giannopoulos,<br \/>\n >MSc in Data Science <br \/>\n >Athens University of Economics and Business <br \/>\n pgiannopoulos@aueb.gr","c5460c1b":"### Expanded Mental Health Screening","4fad5da3":"#### Results\n\nIn model 2, political ideology has the most strength in all occasions. Supernatural evil beliefs, age, gender and kids under 18 at home variables also consider critical for our model.","e9437fda":"### Data Preparation\n\nTo test these hypotheses, we use data from **Wave Four of the Baylor Religion Survey (BRS)**, which was conducted in January of 2014. Briefly, the BRS is a national random sample of 1572 non-institutionalized respondents ages 18 and over who reside in the continental United States.","5b97abe7":"### High Capacity Magazine Ban","c1a2bb82":"#### Results \nAs we can see from the plots, supernatural evil belief have the greater effect of all religious variables in all occasions expect from the *gun safety program*. It is something that we expect and it is approved also in this part.   \n\n\n#### Model 2","d3702706":"### More Teachers\/Faculty with Guns","10ffb478":"## Part 3: Predict Support for Various Gun Policies\n---\nIn this part, we will proceed to predict support for various gun policies from the metric of supernatural evil, controlling for background variables. The gun policies are our dependent variables from *Part 2*. \n\nGun policies:\n\n* Semi-Auto Weapons Ban\n\n* High-Capacity Magazine Ban\n\n* Cilivian Hand Gun Possession Ban\n\n* Support for Concealed Carry\n\n* More Armed Security at Schools\n\n* More Teachers \/ Faculty with Guns\n\n* Required Gun Safety Programs\n\n* Expanded Mental Health Screening\n\nFor each gun-related policy outcome we estimate two models: (a) one model that includes all religious predictors; and (b) a second model, adding all the covariates and political Ideology to the initial model.\n\nFinally, the research was reported in ***The Economist***, on November 6, 2021, under the title \"Belief in supernatural evil is a strong predictor of pro-gun beliefs\" (United States section), available at https:\/\/www.economist.com\/united-states\/2021\/11\/06\/belief-in-supernatural-evil-is-a-strong-predictor-of-pro-gun-beliefs. We will compare the reporting with the findings in our research.\n\n### Data Preparation\n\n* Combine the dataframes from independent variables\n* Drop the no needed columns for our analysis\n* Impute missing values via ***Multiple Imputation***\n* Create a dataset for each gun policy "}}