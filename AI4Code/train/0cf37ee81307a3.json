{"cell_type":{"9d85505a":"code","3068547a":"code","08c94f6e":"code","4f2b0156":"code","903dfca5":"code","840442a9":"code","eeb17550":"code","59f2c641":"code","7754596c":"code","ddf4ba0d":"code","409a5501":"code","b11aa873":"code","35519106":"code","3ea53467":"code","1db3e48d":"code","f3e332fd":"code","000b3303":"code","e08647a3":"code","d2d1004b":"code","3c47d24c":"code","e130d133":"code","d82dcec7":"markdown","3d4e6867":"markdown","39992439":"markdown","7e1d150c":"markdown","b1081238":"markdown","3005af9f":"markdown","a14cd397":"markdown","2a46b04a":"markdown","cae4082f":"markdown","a77f13b1":"markdown","a5fe7a0c":"markdown","1c57d981":"markdown","7fee98cc":"markdown","c1a7d5bc":"markdown","fe6e838a":"markdown"},"source":{"9d85505a":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\n#import sklearn.cluster.hierarchical as hclust\nfrom sklearn import preprocessing\nimport seaborn as sns","3068547a":"df = pd.read_csv('..\/input\/College.csv')","08c94f6e":"print(df.shape)\ndf.head()","4f2b0156":"#exclude the categorical column and the college names\nfeatures = df.drop(['Private', 'Unnamed: 0'],axis=1)","903dfca5":"features['Acceptperc'] = features['Accept'] \/ features['Apps']\nfeatures['Enrollperc'] = features['Enroll'] \/ features['Accept']","840442a9":"features.describe()","eeb17550":"scaler = preprocessing.MinMaxScaler()\nfeatures_normal = scaler.fit_transform(features)","59f2c641":"pd.DataFrame(features_normal).describe()","7754596c":"inertia = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(features_normal)\n    kmeanModel.fit(features_normal)\n    inertia.append(kmeanModel.inertia_)","ddf4ba0d":"# Plot the elbow\nplt.plot(K, inertia, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.show()","409a5501":"kmeans = KMeans(n_clusters=4).fit(features_normal)","b11aa873":"labels = pd.DataFrame(kmeans.labels_) #This is where the label output of the KMeans we just ran lives. Make it a dataframe so we can concatenate back to the original data\nlabeledColleges = pd.concat((features,labels),axis=1)\nlabeledColleges = labeledColleges.rename({0:'labels'},axis=1)","35519106":"labeledColleges.head()","3ea53467":"sns.lmplot(x='Top10perc',y='S.F.Ratio',data=labeledColleges,hue='labels',fit_reg=False)","1db3e48d":"sns.pairplot(labeledColleges,hue='labels')","f3e332fd":"labeledColleges['Constant'] = \"Data\" #This is just to add something constant for the strip\/swarm plots' X axis. Can be anything you want it to be.","000b3303":"sns.stripplot(x=labeledColleges['Constant'],y=labeledColleges['Top10perc'],hue=labeledColleges['labels'],jitter=True)","e08647a3":"sns.swarmplot(x=labeledColleges['Constant'],y=labeledColleges['Top10perc'],hue=labeledColleges['labels'])","d2d1004b":"f, axes = plt.subplots(4, 5, figsize=(20, 25), sharex=False) #create a 4x5 grid of empty figures where we will plot our feature plots. We will have a couple empty ones.\nf.subplots_adjust(hspace=0.2, wspace=0.7) #Scooch em apart, give em some room\n#In this for loop, I step through every column that I want to plot. This is a 4x5 grid, so I split this up by rows of 5 in the else if statements\nfor i in range(0,len(list(labeledColleges))-2): #minus two because I don't want to plot labels or constant\n    col = labeledColleges.columns[i]\n    if i < 5:\n        ax = sns.stripplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],jitter=True,ax=axes[0,(i)])\n        ax.set_title(col)\n    elif i >= 5 and i<10:\n        ax = sns.stripplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],jitter=True,ax=axes[1,(i-5)]) #so if i=6 it is row 1 column 1\n        ax.set_title(col)\n    elif i >= 10 and i<15:\n        ax = sns.stripplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],jitter=True,ax=axes[2,(i-10)])\n        ax.set_title(col)\n    elif i >= 15:\n        ax = sns.stripplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],jitter=True,ax=axes[3,(i-15)])\n        ax.set_title(col)","3c47d24c":"f, axes = plt.subplots(4, 5, figsize=(20, 25), sharex=False) \nf.subplots_adjust(hspace=0.2, wspace=0.7)\nfor i in range(0,len(list(labeledColleges))-2):\n    col = labeledColleges.columns[i]\n    if i < 5:\n        ax = sns.swarmplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],ax=axes[0,(i)])\n        ax.set_title(col)\n    elif i >= 5 and i<10:\n        ax = sns.swarmplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],ax=axes[1,(i-5)])\n        ax.set_title(col)\n    elif i >= 10 and i<15:\n        ax = sns.swarmplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],ax=axes[2,(i-10)])\n        ax.set_title(col)\n    elif i >= 15:\n        ax = sns.swarmplot(x=labeledColleges['Constant'],y=labeledColleges[col].values,hue=labeledColleges['labels'],ax=axes[3,(i-15)])\n        ax.set_title(col)","e130d133":"colleges = df['Unnamed: 0']\ncolleges = pd.concat((colleges,labels),axis=1)\ncolleges = colleges.rename({'Unnamed: 0':'College',0:'Cluster'},axis=1)\nsortcolleges = colleges.sort_values(['Cluster'])\npd.set_option('display.max_rows', 1000)\nsortcolleges","d82dcec7":"**<font size=5>Visualizing and Understanding K-Means Clusters<\/font>**\n\nThe purpose of this notebook is to analyze the 1995 U.S. News and World Report college statistics dataset using K-means clustering. In this notebook I generate the clusters and then look at a couple different ways of visualizing and understanding the cluster output. Let's begin with the usual: import statements, data load, quick look at the dataset.","3d4e6867":"The elbow method is subjective but it looks like 4 might be the pivot point we're looking for. Let's try 4 clusters.","39992439":"This is nice for scanning by eye and seeing what variables give you nice separation and getting a sense for what happened in the clusters, but there's a lot going on and it's hard to get a quick answer to questions like \"what features tend to define cluster 0? How about cluster 3?\" Let's try visualizing each variable separately using strip plots and swarm plots.","7e1d150c":"So, if you were looking for a college in 1995, you could scan these clusters and get a sense for which cluster might offer what you're looking for. Do you want a more exclusive school? Look for clusters that plot higher in Top10perc and Top25perc. But cluster 1 schools are also more expensive, with higher out-of-state tuition and room and board costs. Maybe you're looking for a big school - look for clusters with higher numbers of full-time undergrad students. When you find a cluster you like, you can see the college list here:","b1081238":"Now all of our variables are scaled to be distributed between 0 and 1.","3005af9f":"There are three columns - 'Apps', 'Accept', and 'Enroll' - that can be collapsed into percentages if we choose. The absolute numbers could be informative - maybe a high number of acceptances means we are looking at a very large school, for example. However, if two schools have an \"Accept\" of, say, 1000, this could mean very different things if \"Apps\" was 10,000 (10% acceptance rate) versus 2,000 (50%). So let's create a % accepted column (Accept \/ Apps) and % enroll column (Enroll \/ Accept).","a14cd397":"Here we plotted the Top 10 Percent column (\" Pct. new students from top 10% of H.S. class\") versus the Student\/Faculty ratio column and color-coded each data point by the cluster to which it was assigned. You can start to get the sense of which clusters have lower student\/faculty ratios or are more selective in the students they accept. However, we can't see 4 clearly distinct clusters just by plotting these two variables; we have 17 other variables contributing to the separation that we have to consider to get the full picture. We can't plot all 19 variables together on one plot like the one above. We could plot every variable against every other variable:","2a46b04a":"How many clusters should we group these colleges into? We can use the elbow method to decide. Plot the sum of squared distances of the data points from their cluster's center for increasing numbers of clusters and see if you can find a clear cluster number where the decrease in distortion starts to level off. A quick tutorial that worked me through this part of the code is [here](https:\/\/pythonprogramminglanguage.com\/kmeans-elbow-method\/).","cae4082f":"**<font size=5>K-Means Clustering<\/font>**","a77f13b1":"**<font size=5>Features<\/font>**\n\nNote that there's a categorical variable in our data - 'Private'. Categorical variables are tricky for clustering. You can't cluster off a categorical variable, so you'd have to do some kind of mapping to it. This can be intuitive for ordinal data, but for non-ordinal categorical variables, assigning numerical values can impact the clusters in ways not meaningful about the underlying data. 'Private' is a binary variable, yes or no, but mapping 0 or 1 would have outsized impact on clustering, since each point would be all the way at the min or the max of this variable while other variables will be continuous. For now, we will disregard this variable.","a5fe7a0c":"The Kaggle [site](https:\/\/www.kaggle.com\/flyingwombat\/us-news-and-world-reports-college-data\/home) has the descriptions of each data column, copied here for easy reference:\n* \"Private A factor with levels No and Yes indicating private or public university\n* Apps Number of applications received\n* Accept Number of applications accepted\n* Enroll Number of new students enrolled\n* Top10perc Pct. new students from top 10% of H.S. class\n* Top25perc Pct. new students from top 25% of H.S. class\n* F.Undergrad Number of fulltime undergraduates\n* P.Undergrad Number of parttime undergraduates\n* Outstate Out-of-state tuition\n* Room.Board Room and board costs\n* Books Estimated book costs\n* Personal Estimated personal spending\n* PhD Pct. of faculty with Ph.D.\u2019s\n* Terminal Pct. of faculty with terminal degree\n* S.F.Ratio Student\/faculty ratio\n* perc.alumni Pct. alumni who donate\n* Expend Instructional expenditure per student\n* Grad.Rate Graduation rate\"","1c57d981":"Let's look at all the features. ","7fee98cc":"This is a strip plot. Seaborn plots one data point for each row and we've color coded the points by the cluster to which they were assigned. Adding jitter fans out the points horizontally. In a strip plot, the points can overlap. In a swarm plot (below), the points cannot overlap.","c1a7d5bc":"**Normalization**\n\nNote that the different categories have different ranges. If we don't normalize them, then columns with wider ranges will have disproportionate contributions to cluster separations.","fe6e838a":"**<font size=5>Visualization<\/font>**\n    \n*(Nota bene: I'm plotting the original data in these visualizations, not their normalized scaled versions. We clustered based on the normalized data but I wanted to see how that translates to the colleges' actual stats)*\n\nThe original dataset had 18 features. We dropped one and added two more, so we clustered on 19. We have 5 clusters of points in 19-dimensional space, which is hard to visualize. If we only had two attributes, we could look at how the clusters separate like this:"}}