{"cell_type":{"066c5717":"code","8b7d1ba4":"code","ad3e3dad":"code","dbe6e036":"code","eeb486d1":"code","85875cdc":"code","ccd8d19e":"code","7551a5dd":"code","4d91a2e7":"code","2996d56d":"code","10ceeabf":"code","22bddcf2":"code","d975d5dd":"code","b050d433":"code","a59a093a":"code","5c78acc3":"code","c542f8a3":"code","a65b7747":"code","a344dbda":"code","755ce2ba":"markdown","95b6299a":"markdown","1c8032ae":"markdown","8b67d9d1":"markdown","bdb0f657":"markdown","b7bb47ef":"markdown","cb836e9f":"markdown","0c3288db":"markdown","e24b6f2c":"markdown","ca1d7d20":"markdown","633dcfc4":"markdown","089f8712":"markdown","7bdb2438":"markdown","98992e63":"markdown","cca5464a":"markdown","ba5a41db":"markdown","fcf66613":"markdown","f7c79466":"markdown","962b94b7":"markdown","ccb2ae2e":"markdown","22e337c4":"markdown","9fb4f355":"markdown","ea19823b":"markdown","d6f4164f":"markdown"},"source":{"066c5717":"# standard imports\nimport time\nimport random\nimport os\nfrom IPython.display import display\nimport numpy as np\nimport pandas as pd\n\n# pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\n# imports for preprocessing the questions\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# cross validation and metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\n\nfrom torch.optim.optimizer import Optimizer\n# progress bars\nfrom tqdm import tqdm\ntqdm.pandas()","8b7d1ba4":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint('Train data dimension: ', train_df.shape)\ndisplay(train_df.head())\nprint('Test data dimension: ', test_df.shape)\ndisplay(test_df.head())","ad3e3dad":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","dbe6e036":"def threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.01 for i in range(100)]):\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","eeb486d1":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","85875cdc":"embed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 # max number of words in a question to use","ccd8d19e":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x","7551a5dd":"train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\ntest_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n\n# fill up the missing values\nx_train = train_df[\"question_text\"].fillna(\"_##_\").values\nx_test = test_df[\"question_text\"].fillna(\"_##_\").values\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\n# Pad the sentences \nx_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)\n\n# Get the target values\ny_train = train_df['target'].values","4d91a2e7":"def load_glove(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.0053247833,0.49346462\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","2996d56d":"# missing entries in the embedding are set using np.random.normal so we have to seed here too\nseed_everything()\n\nglove_embeddings = load_glove(tokenizer.word_index)\nparagram_embeddings = load_para(tokenizer.word_index)\n\nembedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\nnp.shape(embedding_matrix)","10ceeabf":"splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=10).split(x_train, y_train))","22bddcf2":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","d975d5dd":"class NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        hidden_size = 40\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout2d(0.1)\n        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n        self.gru_attention = Attention(hidden_size * 2, maxlen)\n        \n        self.linear = nn.Linear(320, 16)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(16, 1)\n    \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = torch.squeeze(\n            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n        \n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, _ = self.gru(h_lstm)\n        \n        h_lstm_atten = self.lstm_attention(h_lstm)\n        h_gru_atten = self.gru_attention(h_gru)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_gru, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_gru, 1)\n        \n        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out","b050d433":"batch_size = 512 # how many samples to process at once\nn_epochs = 6 # how many times to iterate over all samples","a59a093a":"# code inspired from: https:\/\/github.com\/anandsaha\/pytorch.cyclic.learning.rate\/blob\/master\/cls.py\nclass CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","5c78acc3":"\ndef f1_smart(y_true, y_pred):\n    thresholds = []\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        res = metrics.f1_score(y_true, (y_pred > thresh).astype(int))\n        thresholds.append([thresh, res])\n        #print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n\n    thresholds.sort(key=lambda x: x[1], reverse=True)\n    best_thresh = thresholds[0][0]\n    best_f1 = thresholds[0][1]\n    # print(\"Best threshold: \", best_thresh)\n    return  best_f1, best_thresh\n\ndef f1_smart_torch(y_true, y_pred):\n    y_true = y_true.cpu().data.numpy()\n    return f1_smart(y_true, y_pred)\n\n# save and load model functions\ndef save(m, info):\n    torch.save(info, 'best_model.info')\n    torch.save(m, 'best_model.m')\n    \ndef load():\n    m = torch.load('best_model.m')\n    info = torch.load('best_model.info')\n    return m, info","c542f8a3":"# matrix for the out-of-fold predictions\ntrain_preds = np.zeros((len(train_df)))\n# matrix for the predictions on the test set\ntest_preds = np.zeros((len(test_df)))\n\n# always call this before training for deterministic results\nseed_everything()\n\nx_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\n# For every epoch keep the best_threshold and best f1 here\nbest_thresholds = []\nbest_f1 = []\n\nfor i, (train_idx, valid_idx) in enumerate(splits):    \n    # split data in train \/ validation according to the KFold indeces\n    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n    \n    model = NeuralNet()\n    # make sure everything in the model is running on the GPU\n    model.cuda()\n\n    # define binary cross entropy loss\n    # note that the model returns logit to take advantage of the log-sum-exp trick \n    # for numerical stability in the loss\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n    #optimizer = torch.optim.Adam(model.parameters())\n    base_lr, max_lr = 0.0001, 0.003\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    # infinity\n    min_val_loss = 1000000000\n    step_size=300\n    best_thresh = 0\n    max_f1 = 0\n    \n    for epoch in range(n_epochs):\n        # set train mode of the model. This enables operations which are only applied during training like dropout\n        start_time = time.time()\n        # initialize the scheduler\n        \n        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n               step_size=step_size, mode='exp_range',\n               gamma=0.99994)\n        \n        model.train()\n        \n        avg_loss = 0.  \n        step = 0\n        \n        for x_batch, y_batch in tqdm(train_loader, disable=True):\n            # update learning rate\n            if scheduler:\n                scheduler.batch_step()\n            step += 1\n            # Forward pass: compute predicted y by passing x to the model.\n            y_pred = model(x_batch)\n            # debug code\n            #if step>20:\n            #    continue\n            # Compute and print loss.\n            loss = loss_fn(y_pred, y_batch)\n\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n            \n        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n        model.eval()\n        \n        # predict all the samples in y_val_fold batch per batch\n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(test_df)))\n        \n        avg_val_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n        # save model\n        \n        val_f1, threshold = f1_smart_torch(y_val_fold, valid_preds_fold )\n        print('Optimal F1: {} at threshold: {}'.format(val_f1, threshold))\n        \n        if avg_val_loss < min_val_loss:\n            print(\"Saving model with minimum Loss\")\n            save(m=model, info={'epoch': epoch, 'val_loss': avg_val_loss, 'val_f1': val_f1})\n            min_val_loss = avg_val_loss\n            # Note that our best thresh and f1 are based on min loss\n            best_thresh = threshold\n            max_f1 = val_f1\n            \n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n    \n    best_thresholds.append(best_thresh)\n    best_f1.append(max_f1)\n    \n    #Load Best Model\n    model,_ = load()\n    \n    # predict all samples in the test set batch per batch\n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold \/ len(splits)","a65b7747":"search_result = threshold_search(y_train, train_preds)\nsearch_result","a344dbda":"submission = test_df[['qid']].copy()\nsubmission['prediction'] = test_preds > search_result['threshold']\nsubmission.to_csv('submission.csv', index=False)","755ce2ba":"# Utility functions","95b6299a":"There have been many problems with reproducibility of neural networks in this competition. See for example [this post in the discussion forum](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/73341).\n\nThere, it is recommended to \n\n> Rerun the experiment 10 - 15 times and average the scores to get the progress of your model.\n\nThat's obviously not a good solution because when optimizing the architecture of our NN we would of course like to be as fast as possible while still being sure that our model actually improves. Deviations of up to 0.01 in the F1 score are too large to be even remotely sure of that.\n\nThe problem lies within CuDNN. CuDNN's implementation of GRU and LSTM is [much faster](https:\/\/chainer.org\/general\/2017\/03\/15\/Performance-of-LSTM-Using-CuDNN-v5.html) than the regular implementation but they do not run deterministically in TensorFlow and Keras. In this competition were speed is essential you can not afford to keep determinism  by using the regular implementation of GRU and LSTM.\n\n## PyTorch to the rescue!\n\nIn PyTorch, CuDNN determinism is a one-liner: `torch.backends.cudnn.deterministic = True`. This already solves the problem everyone has had so far with Keras. But that's not the only advantage of PyTorch. PyTorch is:\n\n- significantly faster than Keras and TensorFlow. Again, speed is important in this competition so this is great.\n- has a more pythonic API. I hate working with TensorFlow because there are seemingly tens of thousands of ways to do simple things. PyTorch has (in most cases) one obvious way and is by far not as convoluted as TensorFlow.\n- is executed eagerly. There is no such thing as an execution graph in PyTorch. That makes it much easier to try new things and interact with PyTorch in a notebook.\n\nKeras solves some of these problems with TensorFlow but it has a high-level API. I think that when doing research, it is often preferable to be able to interact with the model on a low-level. And you will see that the lower level API still doesn't make it complicated to work with PyTorch.","1c8032ae":"Finally submit the predictions with the threshold we have just found.","8b67d9d1":"# Creating the embeddings matrix","bdb0f657":"First, search for the best threshold:","b7bb47ef":"# Loading the data","cb836e9f":"# Training","0c3288db":"First, define 5-Fold cross-validation. The `random_state` here is important to make sure this is deterministic too.","e24b6f2c":"Now define the neural network. Defining a neural network in PyTorch is done by defining a class. This is almost as intuitive as Keras. The main difference is that you have one function (`__init__`) where it is defined which layers there are in the network and another function (`forward`) which defines the flow of data through the net.\n\nI replicated the architecture used in [@Shujian Liu's kernel](https:\/\/www.kaggle.com\/shujian\/single-rnn-with-4-folds-clr) in the network.","ca1d7d20":"This kernel is intended as a demonstration of PyTorch. I did not spend any time tuning anything, I just ported the models to PyTorch. So, ways to improve this kernel are:\n\n- Tune the architecture! Now that training is deterministic it should be much less frustrating\n- Increase the number of folds in K-Fold cross-validation. Now that training is faster we can fit more folds into the kernel.\n- Or, keep the folds the same and increase the number of epochs \/ decrease the learning rate to improve the model at the cost of more time to train. \n- Load the weights with the best validation score after training (implement the equivalent of `ModelCheckpoint` in PyTorch). I am not sure if this will improve the score because you might overfit to the validation data. - **DONE**\n- Use a PyTorch implementation of CLR (cyclic learning rate). That seemed to make the model converge faster in some other kernels. - **DONE**","633dcfc4":"# Preface\n\nForked from https:\/\/www.kaggle.com\/bminixhofer\/deterministic-neural-networks-using-pytorch\nand inputs from https:\/\/www.kaggle.com\/dannykliu\/lstm-with-attention-clr-in-pytorch\n\nWanted to provide a good easy to understand training loop with all bells and whistles.\nLet me know how we can improve the training loop further.","089f8712":"Standard preprocessing procedure. This is not the point of this kernel so I have copied it from [this great kernel](https:\/\/www.kaggle.com\/gmhost\/gru-capsule).","7bdb2438":"`seed_torch` sets the seed for numpy and torch to make sure functions with a random component behave deterministically. `torch.backends.cudnn.deterministic = true` sets the CuDNN to deterministic mode. \n\nThis function allows us to run experiments 100% deterministically.","98992e63":"That seems inline with the score from the replicated Keras kernel!","cca5464a":"Another step that many others have already done. Again, the same progress as in the kernel from above.","ba5a41db":"# Processing input","fcf66613":"Now it gets interesting. First, I ported the Attention mechanism many others have used in this competition to PyTorch. I am not sure where the Keras snippet originated from, so I am going to give credit to the [kernel where I have first seen it](https:\/\/www.kaggle.com\/shujian\/single-rnn-with-4-folds-clr).","f7c79466":"## Ways to improve this kernel","962b94b7":"# Evaluation","ccb2ae2e":"# Imports","22e337c4":"Function to search for best threshold regarding the F1 score given labels and predictions from the network.","9fb4f355":"Sigmoid function in plain numpy.","ea19823b":"# Defining the model","d6f4164f":"Now we can already train the network. Unfortunately, we do not have an API as high-level as keras's `.fit` in PyTorch. However, the code is still not too complicated and I have added comments where necessary."}}