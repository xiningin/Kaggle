{"cell_type":{"4b3f9396":"code","eea0b320":"code","88b853ad":"code","6a139425":"code","b41889aa":"code","3b03113d":"code","4a6a1a8a":"code","6c5bc97d":"code","bfc55bc0":"code","ba97b235":"code","d6111d7d":"code","84d1877d":"code","fe22c670":"code","1cbfde17":"code","860d47b3":"code","02354277":"code","6515c70c":"code","d0873b3f":"code","6784c453":"code","cef62f3b":"code","cf5b33c5":"code","418e1995":"code","b756444a":"code","2f261b22":"code","a2fc3123":"code","09fbf8a0":"code","4fca7f25":"code","5a04e11e":"code","74ac3072":"code","43f9e8f2":"code","85d93c90":"code","74f49852":"code","e6615706":"code","f5d20da2":"code","08d08c5b":"code","a0bc07a4":"code","a43b05e5":"code","96e4a84c":"code","d9b2e4b4":"code","2673fa0c":"code","5595056b":"code","ae6808bc":"code","a660a5ec":"code","cce8ebc9":"code","b0fa4371":"code","d527bc3b":"code","369048d7":"code","b6938a0b":"code","f0f754f2":"code","b8ef4767":"code","96abf5a8":"code","d57e5b83":"code","aa25cedf":"code","761cd890":"code","82638380":"code","c0b21a24":"code","a4149781":"code","5a1149ab":"code","c506ae69":"code","b9a64a5f":"code","faa7f091":"code","5360c181":"code","d86d05f6":"code","f267298e":"code","90c2dc79":"code","313149d7":"code","f8e08f43":"code","edd4ec3a":"code","c03029c2":"code","e12105d9":"code","edf8ed84":"code","1202fa15":"code","a30ffde1":"code","b1a0510a":"code","01b8398a":"code","9c96d834":"code","193a31e1":"code","1c4def0a":"code","55388ac2":"code","c16e7c89":"code","4f3edbb9":"code","c1b27750":"code","5bf5ec43":"code","86bc5fc5":"code","2a59e979":"code","4ee65a94":"code","70415bef":"code","115c9fcb":"code","6766cc0c":"code","ef1eeb4e":"code","433d4757":"code","0d5f3fad":"code","ce3b4fd7":"code","300f75be":"code","37540dbd":"code","35f58e03":"code","94f92473":"code","2c283cee":"code","9db28181":"code","4ee36623":"code","0f75c04f":"code","2fd6315c":"code","8b40f9dd":"code","bfea370b":"code","ce546673":"code","325f3e42":"code","a2e99562":"code","0d5e460a":"code","fa5341e4":"code","17076f8e":"code","7f047b15":"code","e95a3824":"code","41592477":"code","93c73c12":"code","df25ad7c":"code","6b5357a8":"code","a1ca5d96":"code","0472d702":"code","1df62b18":"code","427c2214":"code","0dd63484":"markdown","8eb91612":"markdown","b20813f0":"markdown","8edc7c0f":"markdown","d2c53215":"markdown","b2b59a9c":"markdown","602a7218":"markdown","19faf158":"markdown","45ef7f41":"markdown","d6fd6a94":"markdown","c5289baf":"markdown","48d3355b":"markdown","47c89647":"markdown","198322ff":"markdown","14634a71":"markdown","64eb5f45":"markdown","80db1c12":"markdown","19e3dbf8":"markdown","352befbd":"markdown","b52b03ec":"markdown","9f6b7c3f":"markdown","d79a0b91":"markdown","127111d0":"markdown","55bc2768":"markdown","2e6dd21b":"markdown","62adae42":"markdown","d4428ab5":"markdown","433d47bb":"markdown","ca74e757":"markdown","ee904480":"markdown","d2cad7e4":"markdown","cb627fd3":"markdown","1276efe8":"markdown","7d34da65":"markdown","7ebc5d29":"markdown","d8d70e0d":"markdown","e6717499":"markdown","24aac713":"markdown","16321ca6":"markdown","50478dec":"markdown","b5a6596f":"markdown","42267bc7":"markdown","2f86a53c":"markdown","55116846":"markdown","812f8e64":"markdown","0b504766":"markdown","8344034d":"markdown","37523435":"markdown","a7a71142":"markdown","6b0b6a4b":"markdown","a0670368":"markdown","92ac768f":"markdown","7c4fa87b":"markdown"},"source":{"4b3f9396":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nfrom scipy.optimize import fmin as scip_fmin\n\n# Visialisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\n\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n\n# Models\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier","eea0b320":"data_dir = '..\/input\/tabular-playground-series-apr-2021'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","88b853ad":"RANDOM_SEED = 42","6a139425":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","b41889aa":"seed_everything()","3b03113d":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","4a6a1a8a":"train_df.columns","6c5bc97d":"train_df.sample(10)","bfc55bc0":"train_df.describe().T","ba97b235":"test_df.describe().T","d6111d7d":"train_df.nunique()","84d1877d":"test_df.nunique()","fe22c670":"nulls_train = np.sum(train_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()\nprint('There are', len(nullcols_train), 'features with missing values in the training data.')\nprint(f'Columns containing nulls are: {list(nullcols_train.index)}')","1cbfde17":"nulls_test = np.sum(test_df.isnull())\nnullcols_test = nulls_test.loc[(nulls_test != 0)].sort_values(ascending=False)\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_test.index, y=nullcols_test)\nplt.ylabel(\"Null Count\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()\nprint('There are', len(nullcols_test), 'features with missing values in the test data.')\nprint(f'Columns containing nulls are: {list(nullcols_test.index)}')","860d47b3":"nulls_train = np.sum(train_df.isnull())\nnullcols_train = nulls_train.loc[(nulls_train != 0)].sort_values(ascending=False)\nnullcols_train = nullcols_train.apply(lambda x: 100*x\/train_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_train.index, y=nullcols_train)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","02354277":"nulls_test = np.sum(test_df.isnull())\nnullcols_test = nulls_test.loc[(nulls_test != 0)].sort_values(ascending=False)\nnullcols_test = nullcols_test.apply(lambda x: 100*x\/test_df.shape[0])\n\nbarplot_dim = (15, 8)\nax = plt.subplots(figsize=barplot_dim)\nsns.barplot(x=nullcols_test.index, y=nullcols_test)\nplt.ylabel(\"Null %\", size=20);\nplt.xlabel(\"Feature Name\", size=20);\nplt.show()","6515c70c":"train_df['Cabin_Available'] = train_df['Cabin'].apply(lambda x: 0 if pd.isnull(x) else 1)\ntest_df['Cabin_Available'] = test_df['Cabin'].apply(lambda x: 0 if pd.isnull(x) else 1)","d0873b3f":"train_df['Cabin'].fillna('No Cabin', inplace=True)\ntest_df['Cabin'].fillna('No Cabin', inplace=True)","6784c453":"train_df['Ticket_Available'] = train_df['Ticket'].apply(lambda x: 0 if pd.isnull(x) else 1)\ntest_df['Ticket_Available'] = test_df['Ticket'].apply(lambda x: 0 if pd.isnull(x) else 1)","cef62f3b":"train_df['Ticket'].fillna('Missing Ticket', inplace=True)\ntest_df['Ticket'].fillna('Missing Ticket', inplace=True)","cf5b33c5":"train_df['Age_Available'] = train_df['Age'].apply(lambda x: 0 if pd.isnull(x) else 1)\ntest_df['Age_Available'] = test_df['Age'].apply(lambda x: 0 if pd.isnull(x) else 1)","418e1995":"train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].mean(), inplace=True)","b756444a":"train_df['Embarked_Available'] = train_df['Embarked'].apply(lambda x: 0 if pd.isnull(x) else 1)\ntest_df['Embarked_Available'] = test_df['Embarked'].apply(lambda x: 0 if pd.isnull(x) else 1)","2f261b22":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace=True)","a2fc3123":"train_df['Fare_Available'] = train_df['Fare'].apply(lambda x: 0 if pd.isnull(x) else 1)\ntest_df['Fare_Available'] = test_df['Fare'].apply(lambda x: 0 if pd.isnull(x) else 1)","09fbf8a0":"train_df['Fare'] = train_df.groupby('Pclass').Fare.transform(lambda x: x.fillna(x.mean()))\ntest_df['Fare'] = test_df.groupby('Pclass').Fare.transform(lambda x: x.fillna(x.mean()))","4fca7f25":"np.sum(train_df.isnull())","5a04e11e":"np.sum(test_df.isnull())","74ac3072":"ax = plt.subplots(figsize=(12, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='Survived', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Survived\", size=20);","43f9e8f2":"def plot_cat_distribution(cat, train_df=train_df):\n    ax = plt.subplots(figsize=(12, 6))\n    sns.set_style('whitegrid')\n    sns.countplot(x=cat, data=train_df);\n    plt.ylabel('No. of Observations', size=20);\n    plt.xlabel(cat+' Count', size=20);\n    plt.show()\n    \ndef plot_cat_response(cat, train_df=train_df):\n    ax = plt.subplots(figsize=(8, 5))\n    sns.set_style('whitegrid')\n    sns.countplot(x=cat, hue='Survived', data=train_df);\n    plt.show()","85d93c90":"plot_cat_distribution('Pclass')","74f49852":"plot_cat_response('Pclass')","e6615706":"plot_cat_distribution('Sex')","f5d20da2":"plot_cat_response('Sex')","08d08c5b":"g = sns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",\n                data=train_df, kind=\"count\");\ng.fig.set_size_inches(10,5)","a0bc07a4":"g = sns.displot(data=train_df, x=\"Age\", hue=\"Survived\", kind=\"kde\");\ng.fig.set_size_inches(10,5)","a43b05e5":"g = sns.catplot(x='Survived', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","96e4a84c":"g = sns.catplot(x='Sex', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","d9b2e4b4":"g = sns.catplot(x='Sex', y='Age', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","2673fa0c":"g = sns.catplot(x='Pclass', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","5595056b":"g = sns.catplot(x='Pclass', y='Age', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","ae6808bc":"plot_cat_distribution('SibSp')","a660a5ec":"plot_cat_response('SibSp')","cce8ebc9":"# Response Rate\nv = train_df.groupby('SibSp').Survived.value_counts().unstack()\nv['Ratio'] = v[1]\/v[0]\nv.reset_index(inplace=True)","b0fa4371":"v['Ratio'].mean()","d527bc3b":"ax = plt.subplots(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='SibSp', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","369048d7":"g = sns.catplot(x='SibSp', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","b6938a0b":"g = sns.catplot(x='SibSp', y='Age', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(12,5)","f0f754f2":"g = sns.catplot(x='Pclass', y='Age', hue='SibSp', kind='box', data=train_df);\ng.fig.set_size_inches(12,5)","b8ef4767":"plot_cat_distribution('Parch')","96abf5a8":"plot_cat_response('Parch')","d57e5b83":"# Response Rate\nv = train_df.groupby('Parch').Survived.value_counts().unstack()\nv['Ratio'] = v[1]\/v[0]\nv.reset_index(inplace=True)","aa25cedf":"ax = plt.subplots(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nsns.barplot(x='Parch', y='Ratio', data=v.sort_values(by=['Ratio'], ascending=False));","761cd890":"g = sns.catplot(x='Parch', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","82638380":"g = sns.catplot(x='Parch', y='Age', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(12,5)","c0b21a24":"g = sns.catplot(x='Pclass', y='Age', hue='Parch', kind='box', data=train_df);\ng.fig.set_size_inches(14,5)","a4149781":"g = sns.displot(data=train_df, x=\"Fare\", hue=\"Survived\", kind=\"kde\");\ng.fig.set_size_inches(10,5)","5a1149ab":"g = sns.catplot(x='Survived', y='Fare', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","c506ae69":"g = sns.catplot(x='Sex', y='Fare', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","b9a64a5f":"g = sns.catplot(x='Sex', y='Fare', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","faa7f091":"g = sns.catplot(x='Pclass', y='Fare', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","5360c181":"g = sns.catplot(x='Pclass', y='Fare', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","d86d05f6":"plot_cat_distribution('Embarked')","f267298e":"g = sns.catplot(x='Embarked', y='Fare', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","90c2dc79":"g = sns.catplot(x=\"Embarked\", hue=\"Pclass\",\n                data=train_df, kind=\"count\");\ng.fig.set_size_inches(10,5)","313149d7":"plot_cat_response('Embarked')","f8e08f43":"g = sns.catplot(x='Embarked', y='Fare', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","edd4ec3a":"g = sns.catplot(x='Embarked', y='Age', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","c03029c2":"g = sns.catplot(x='Embarked', y='Age', hue='Survived', kind='box', data=train_df);\ng.fig.set_size_inches(10,5)","e12105d9":"g = sns.catplot(x=\"Embarked\", hue=\"Pclass\", col=\"Survived\",\n                data=train_df, kind=\"count\");\ng.fig.set_size_inches(10,5)","edf8ed84":"NUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.Survived.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","1202fa15":"train_df.nunique()","a30ffde1":"drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin']","b1a0510a":"train_df['Fare'] = np.log(train_df['Fare'])\ntest_df['Fare'] = np.log(test_df['Fare'])","01b8398a":"train_df['Clubbed_Class'] = train_df['Pclass'].apply(lambda x: 'Economy' if x == 3 else 'Highend')\ntest_df['Clubbed_Class'] = test_df['Pclass'].apply(lambda x: 'Economy' if x == 3 else 'Highend')","9c96d834":"train_df['Pclass_Sex'] = train_df['Pclass'].astype(str) + '_' + train_df['Sex']\ntest_df['Pclass_Sex'] = test_df['Pclass'].astype(str) + '_' + test_df['Sex']","193a31e1":"train_df['Age_Bin_10'] = pd.cut(train_df['Age'], bins=10, labels=False)\ntrain_df['Age_Bin_50'] = pd.cut(train_df['Age'], bins=50, labels=False)\ntest_df['Age_Bin_10'] = pd.cut(test_df['Age'], bins=10, labels=False)\ntest_df['Age_Bin_50'] = pd.cut(test_df['Age'], bins=50, labels=False)","1c4def0a":"train_df['Fare_Bin_10'] = pd.cut(train_df['Fare'], bins=10, labels=False)\ntrain_df['Fare_Bin_50'] = pd.cut(train_df['Fare'], bins=50, labels=False)\ntest_df['Fare_Bin_10'] = pd.cut(test_df['Fare'], bins=10, labels=False)\ntest_df['Fare_Bin_50'] = pd.cut(test_df['Fare'], bins=50, labels=False)","55388ac2":"train_df['Age_Bin_10_Sex'] = train_df['Age_Bin_10'].astype(str) + '_' + train_df['Sex']\ntrain_df['Age_Bin_50_Sex'] = train_df['Age_Bin_50'].astype(str) + '_' + train_df['Sex']\ntest_df['Age_Bin_10_Sex'] = test_df['Age_Bin_10'].astype(str) + '_' + test_df['Sex']\ntest_df['Age_Bin_50_Sex'] = test_df['Age_Bin_50'].astype(str) + '_' + test_df['Sex']","c16e7c89":"train_df['Age_Bin_10_Class'] = train_df['Age_Bin_10'].astype(str) + '_' + train_df['Pclass'].astype(str)\ntrain_df['Age_Bin_50_Class'] = train_df['Age_Bin_50'].astype(str) + '_' + train_df['Pclass'].astype(str)\ntest_df['Age_Bin_10_Class'] = test_df['Age_Bin_10'].astype(str) + '_' + test_df['Pclass'].astype(str)\ntest_df['Age_Bin_50_Class'] = test_df['Age_Bin_50'].astype(str) + '_' + test_df['Pclass'].astype(str)","4f3edbb9":"train_df['Ticket_len'] = train_df['Ticket'].str.len()\ntest_df['Ticket_len'] = test_df['Ticket'].str.len()\n\ntrain_df['Ticket_type'] = train_df['Ticket'].str.replace('\\d+', '')\ntrain_df['Ticket_type'] = train_df['Ticket_type'].apply(lambda x: 'Num' if x=='' else x[:3])\ntest_df['Ticket_type'] = test_df['Ticket'].str[:3].replace('\\d+', '')\ntest_df['Ticket_type'] = test_df['Ticket_type'].apply(lambda x: 'Num' if x=='' else x[:3])","c1b27750":"train_df['Cabin_type'] = train_df['Cabin'].map(lambda x: str(x)[0])\ntest_df['Cabin_type'] = test_df['Cabin'].map(lambda x: str(x)[0])","5bf5ec43":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1","86bc5fc5":"train_df['IsAlone'] = train_df['FamilySize'] <= 1\ntest_df['IsAlone'] = test_df['FamilySize'] <= 1","2a59e979":"train_df['IsAgeInt'] = (train_df['Age'] == train_df['Age'].map(np.floor)).astype(int)\ntest_df['IsAgeInt'] = (test_df['Age'] == test_df['Age'].map(np.floor)).astype(int)","4ee65a94":"train_df['Last_Name'] = train_df['Name'].map(lambda x: x.split(', ')[0])\ntrain_df['First_Name'] = train_df['Name'].map(lambda x: x.split(', ')[1])\n\ntest_df['Last_Name'] = test_df['Name'].map(lambda x: x.split(', ')[0])\ntest_df['First_Name'] = test_df['Name'].map(lambda x: x.split(', ')[1])","70415bef":"name_cols = ['Last_Name', 'First_Name']\ntrain_df[name_cols] = train_df[name_cols].apply(lambda x: x.mask(x.map(x.value_counts())< (0.001*train_df.shape[0]), 'RARE'))\n\nfor col in name_cols:\n    vals = list(train_df[col].unique())\n    test_df[col] = test_df[col].apply(lambda x: 'RARE' if x not in vals else x)","115c9fcb":"train_df.drop(drop_columns, axis=1, inplace=True)","6766cc0c":"target_col = ['Survived']\nskip_cols = ['kfold']\nnumerical_cols = [\n    'Age', 'SibSp', 'Parch', 'Fare','Age_Bin_10',\n    'Age_Bin_50', 'Fare_Bin_10', 'Fare_Bin_50',\n    'Ticket_len', 'FamilySize', 'IsAgeInt'\n]\ncategorical_cols = []\nfor col in train_df.columns:\n    if col not in (target_col + skip_cols + numerical_cols):\n        categorical_cols.append(col)","ef1eeb4e":"train_df[categorical_cols].head()","433d4757":"non_numeric_cat_cols = []\nfor col in categorical_cols:\n    if (train_df[col].dtypes == object) or (train_df[col].dtypes == bool):\n        non_numeric_cat_cols.append(col)","0d5f3fad":"def label_enc(train_df, test_df, features):\n    lbl_enc = preprocessing.LabelEncoder()\n    full_data = pd.concat(\n        [train_df[features], test_df[features]],\n        axis=0\n    )\n    \n    for col in (features):\n        print(col)\n        if train_df[col].dtype == 'object':\n            lbl_enc.fit(full_data[col].values)\n            train_df[col] = lbl_enc.transform(train_df[col])\n            test_df[col] = lbl_enc.transform(test_df[col])\n            \n    return train_df, test_df","ce3b4fd7":"def one_hot_enc(train_df, test_df, features):\n    OH_enc = preprocessing.OneHotEncoder(sparse=False)\n    OH_cols_train = pd.DataFrame(OH_enc.fit_transform(train_df[features]))\n    OH_cols_test = pd.DataFrame(OH_enc.transform(test_df[features]))\n    \n    OH_cols_train.index = train_df[features].index\n    OH_cols_test.index = test_df[features].index\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    train_df = pd.concat([train_df, OH_cols_train], axis=1)\n    test_df = pd.concat([test_df, OH_cols_test], axis=1)\n    \n    return train_df, test_df","300f75be":"def catboost_enc(train_df, test_df, features):\n    cb_enc = ce.CatBoostEncoder(cols=features)\n    cb_enc.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(cb_enc.transform(train_df[features]).add_suffix('_cb'))\n    test_df = test_df.join(cb_enc.transform(test_df[features]).add_suffix('_cb'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","37540dbd":"def hash_enc(train_df, test_df, features, components=400):\n    hash_enc = ce.HashingEncoder(cols=features, n_components=components)\n    hash_enc.fit(train_df[features])\n    \n    train_df = train_df.join(hash_enc.transform(train_df[features]).add_suffix('_hash'))\n    test_df = test_df.join(hash_enc.transform(test_df[features]).add_suffix('_hash'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","35f58e03":"def target_enc(train_df, test_df, features):\n    targ_enc = ce.TargetEncoder(cols=features)\n    targ_enc.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(targ_enc.transform(train_df[features]).add_suffix('_targ'))\n    test_df = test_df.join(targ_enc.transform(test_df[features]).add_suffix('_targ'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","94f92473":"def helmhert_enc(train_df, test_df, features):\n    helm_enc = ce.HelmertEncoder(cols=features)\n    helm_enc.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(helm_enc.transform(train_df[features]).add_suffix('_helm'))\n    test_df = test_df.join(helm_enc.transform(test_df[features]).add_suffix('_helm'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","2c283cee":"def looe_enc(train_df, test_df, features):\n    loo_enc = ce.LeaveOneOutEncoder(cols=features)\n    loo_enc.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(loo_enc.transform(train_df[features]).add_suffix('_looe'))\n    test_df = test_df.join(loo_enc.transform(test_df[features]).add_suffix('_looe'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","9db28181":"def woe_enc(train_df, test_df, features):\n    WOE_encoder = ce.WOEEncoder(cols=features)\n    WOE_encoder.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(WOE_encoder.transform(train_df[features]).add_suffix('_woe'))\n    test_df = test_df.join(WOE_encoder.transform(test_df[features]).add_suffix('_woe'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","4ee36623":"def mee_enc(train_df, test_df, features):\n    MEE_encoder = ce.MEstimateEncoder(cols=features)\n    MEE_encoder.fit(train_df[features], train_df['Survived'])\n    \n    train_df = train_df.join(MEE_encoder.transform(train_df[features]).add_suffix('_mee'))\n    test_df = test_df.join(MEE_encoder.transform(test_df[features]).add_suffix('_mee'))\n    \n    train_df = train_df.drop(features, axis=1)\n    test_df = test_df.drop(features, axis=1)\n    \n    return train_df, test_df","0f75c04f":"low_cardinality_cols = []\nhigh_cardinality_cols = []\n\nfor feat in categorical_cols:\n    if train_df[feat].nunique() < 5:\n        low_cardinality_cols.append(feat)\n    else:\n        high_cardinality_cols.append(feat)\n        \nprint(f'Low Cardinality Cols: {low_cardinality_cols}')\nprint(f'High Cardinality Cols: {high_cardinality_cols}')","2fd6315c":"train_df, test_df = one_hot_enc(train_df, test_df, low_cardinality_cols)","8b40f9dd":"train_df, test_df = looe_enc(train_df, test_df, high_cardinality_cols)","bfea370b":"cols = list(train_df.columns)\nfeatures = [feat for feat in cols if feat not in skip_cols+target_col]","ce546673":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","325f3e42":"ufs = UnivariateFeatureSelction(\n    n_features=0.8,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df[target_col].values.ravel())\nselected_features = ufs.return_cols(train_df[features])","a2e99562":"def get_stacking():\n    level0 = []\n    level0.append(('gauss', GaussianNB()))\n    level0.append(('lr', LogisticRegression(solver='liblinear')))\n    level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier(n_estimators = 500,\n                                                random_state=42)))\n    level0.append(('xgb', xgb.XGBClassifier(max_depth=7,\n                                            n_estimators=1000,\n                                            colsample_bytree=0.8,\n                                            subsample=0.8,\n                                            learning_rate=0.1,\n                                            tree_method='gpu_hist',\n                                            gpu_id=0)))\n    level0.append(('lgbm', LGBMClassifier(metric='binary_logloss',\n                                          objective='binary',\n                                          learning_rate=0.01,\n                                          seed=42,\n                                          n_estimators=1000)))\n    level0.append(('cbc', CatBoostClassifier(verbose=0,\n                                             n_estimators=1000,\n                                             eval_metric='AUC',\n                                             task_type='GPU',\n                                             devices='0',\n                                             random_seed=42)))\n    level1 = LogisticRegression()\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model","0d5e460a":"def get_voting(vote_type='hard'):\n    models = list()\n    models.append(('gauss', GaussianNB()))\n    models.append(('lr', LogisticRegression(solver='liblinear')))\n    models.append(('knn', KNeighborsClassifier()))\n    models.append(('rf', RandomForestClassifier(n_estimators = 500,\n                                                random_state=42)))\n    models.append(('xgb', xgb.XGBClassifier(max_depth=7,\n                                            n_estimators=1000,\n                                            colsample_bytree=0.8,\n                                            subsample=0.8,\n                                            learning_rate=0.1,\n                                            tree_method='gpu_hist',\n                                            gpu_id=0)))\n    models.append(('lgbm', LGBMClassifier(metric='binary_logloss',\n                                          objective='binary',\n                                          learning_rate=0.01,\n                                          seed=42,\n                                          n_estimators=1000)))\n    models.append(('cbc', CatBoostClassifier(verbose=0,\n                                             n_estimators=1000,\n                                             eval_metric='AUC',\n                                             task_type='GPU',\n                                             devices='0',\n                                             random_seed=42)))\n    \n    ensemble = VotingClassifier(estimators=models, voting=vote_type)\n    return ensemble","fa5341e4":"def get_models():\n    models = dict()\n    models['gauss'] = GaussianNB()\n    models['lr'] = LogisticRegression(solver='liblinear')\n    models['knn'] = KNeighborsClassifier()\n    models['cart'] = DecisionTreeClassifier()\n    models['rf'] = RandomForestClassifier(n_estimators = 500,\n                                          random_state=42,\n                                          n_jobs=-1)\n    models['xgb'] = xgb.XGBClassifier(max_depth=7,\n                                      n_estimators=1000,\n                                      colsample_bytree=0.8,\n                                      subsample=0.8,\n                                      nthread=-1,\n                                      learning_rate=0.1,\n                                      tree_method='gpu_hist',\n                                      gpu_id=0)\n    models['lgbm'] = LGBMClassifier(metric='binary_logloss',\n                                    objective='binary',\n                                    seed=42,\n                                    learning_rate=0.01,\n                                    n_estimators=1000)\n    models['cbc'] = CatBoostClassifier(verbose=0,\n                                       n_estimators=1000,\n                                       eval_metric='AUC',\n                                       task_type='GPU',\n                                       devices='0',\n                                       random_seed=42)\n    models['stacking'] = get_stacking()\n    models['voting_soft'] = get_voting(vote_type='soft')\n    models['voting_hard'] = get_voting(vote_type='hard')\n    \n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","17076f8e":"%%time\n\nX = train_df[selected_features]\ny = train_df[target_col]\n\nmodels = get_models()\nresults = []\nnames = []\n\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} : {round(np.mean(scores),3)} ({round(np.std(scores),3)})')","7f047b15":"ax = plt.subplots(figsize=(12, 6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","e95a3824":"%%time\n\nmodels = get_models()\nclf = models['stacking']\nX = train_df[selected_features]\ny = train_df[target_col]\n\nclf.fit(X, y)\npreds = clf.predict(test_df[selected_features])\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = preds","41592477":"submission.head()","93c73c12":"submission.to_csv('Baseline_Stacked.csv', index=False)","df25ad7c":"%%time\n\nmodels = get_models()\nclf = models['lgbm']\nX = train_df[selected_features]\ny = train_df[target_col]\n\nclf.fit(X, y)\npreds = clf.predict(test_df[selected_features])\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = preds","6b5357a8":"submission.to_csv('Baseline_LGBM.csv', index=False)","a1ca5d96":"%%time\n\nmodels = get_models()\nclf = models['cbc']\nX = train_df[selected_features]\ny = train_df[target_col]\n\nclf.fit(X, y)\npreds = clf.predict(test_df[selected_features])\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = preds","0472d702":"submission.to_csv('Baseline_CBC.csv', index=False)","1df62b18":"%%time\n\nmodels = get_models()\nclf = models['lr']\nX = train_df[selected_features]\ny = train_df[target_col]\n\nclf.fit(X, y)\npreds = clf.predict(test_df[selected_features])\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test_df['PassengerId']\nsubmission['Survived'] = preds","427c2214":"submission.to_csv('Baseline_LogReg.csv', index=False)","0dd63484":"Great, it looks like we successfully managed to impute all the null values. Now moving on to EDA...\n\n# EDA\n\n## 1. Class Imbalance","8eb91612":"Some features can be dropped from the dataset, like:-\n1. We can drop the 'PassengerId' and 'Name' features because their cardinality is so high comapred to the data that, we necessarily will not learn anything from these features. Also The features practically is not relevant to any physical parameter affecting the survival probability.\n2. The features 'Ticket' and 'Cabin' have some potentially useful information like Cabin number etc, but after retrival of those information these columns can be dropped because their cardinality is just too high for the data.","b20813f0":"**Observation:-**  \n* There were more male passengers on the ship ac comapred to females. But there is not a very high bias towards any specific gender.\n* As expected, the probability of survival as a female is mugh higher as comapred to male passengers.\n* Females from Class-1 had the highest chances of survival followed by Class-2 and then Class-3.\n\n## 6. Age\nAge is a continuous variable and one which can again be of high importance. Because in case of evacuation children and old people would be given preference. Let's check what the data says...","8edc7c0f":"**Observations:-**  \n* Most of the people were travelling without Siblings\/Spouse.\n* Survival rate of passengers having 2 Sibling\/Spouse were the highest.\n* Older and very young passengers were generally travelling without any Siblings\/Spouse.\n* Older people travelling with < 3 siblings\/spouse had higher chances of survival.\n\n## 8. Parch\nParch is a integer feature which specifies the number of parents \/ children of the passenger aboard the Titanic. This might have an influence on survival because usually any parent will try to keep their children safe in case of a disaster. Also while evacuating any children, a parent will be accompany them, so that also adds to the equation of survival...  \nWe can treat this a ordinal categorical features because there are only finite number of parents \/ children a passenger could have aboard the ship.","d2c53215":"Looking at some sample rows...","b2b59a9c":"Also, we can explore the cardinality of each feature as shown below...","602a7218":"# Approach:-\nMy advice to beginners will be to follow the following workflow for approaching any tabular data for competition:-  \n1. Read the Problem Statement and understand the requirements carefully.\n2. READ THE PROBLEM STATEMENT AND REQUIREMENTS AGAIN.\n3. Look carefully at the Data and do EDA (It will help largely during feature engineering and Inference). It is worth the time.\n4. Decide the problem category (Classification\/Regression).\n5. Spit data into Kfolds before doing Feature engineering. Because it's very easy to leak data\/contaminate the validation set during Feature Engineering. Then the validation set is no longer representative of real data.\n6. Build Basic model and record the performance.\n7. Try to improve the performance by Feature engineering, better encoding, synthetic feature creation etc.\n8. Do feature selection to only use the important\/relevant features.\n9. If that saturates or starts to drop, go back and try using other models and see whichone works better.\n10. Tune the hyperparameters for the models which seem to work good as per your observations.\n11. Select some of the best models from previous step and do an ensemble\/stacking-blending.\n12. Submit to the leaderboard and gaze the difference in CV and LB. If it is huge, then most likely there was some overfitting and leakage across folds. Try to identify and rectify the same.\n13. When done, resubmit and you should see a close result.\n14. Not satisfied with result\/Trying to get better rank? Head over to the Discussion Forum and read through interesting discussions and see what others are trying to do. If happy, implement them and gradually start improving your scores and skills.\n\n**Happy Kaggling!**\n\n# Why this Competition?\nThis competition provides an unique oppertunity for Data Science beginners to participate in a Hackathon style challenge. It also provides the unique oppertunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- **Classification**.  \n\nThis competition has the right mix to Catergorical and Numerical features we might expect in a practical problem and this helps us know how to leverage both of thhem in conjugation for a Classification task.\n\n# Problem Statement\nThe goal of this competition is to provide a fun, and approachable for anyone, tabular dataset. These competition will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.  \n\nThe dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN.  \n\nSo we are sort of dealing with a variation of actual real-world data and here as Data Scientists are expected to predict the Binary Classification based on these features.\n\n## Data Description:-\nThere are 6 categorical features and 4 continuous features in the dataset. The label binary class.  \n\n`survival` : Survival  \n`pclass` : Ticket class  \n`sex` : Sex  \n`Age` : Age in years  \n`sibsp` : # of siblings \/ spouses aboard the Titanic  \n`parch` : # of parents \/ children aboard the Titanic  \n`ticket` : Ticket number  \n`fare` : Passenger fare  \n`cabin` : Cabin number  \n`embarked` : Port of Embarkation\n\n## Expected Outcome:-\n* Build a model to predict if a person survived this tragic incident or not, given the information above.\n* Grading Metric: **Accuracy**\n\n## Problem Category:-\nFrom the data and objective its is evident that this is a **Binary Classification Problem** in the **Tabular Data** format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey:-","19faf158":"There are more adults on the ship as compared to childeren","45ef7f41":"## 4. Logistic Regression","d6fd6a94":"**Observations:-**  \n* Most passengers were travelling without any parents\/children.\n* Passengers having 1\/3\/5 parents\/children are more likely to survive than others.\n* Age has very little effect on the number of parents\/children accompanying a passenger.\n\n## 9. Fare\nFare is a continuous feature that specifies the total fare paid by the passenger for their place on the boat. It should be highly correlated with Class and total number of people on the same ticket.","c5289baf":"We can also concatenate Age bins with Class features indicating something like \"Young Class-3 passenger\".","48d3355b":"For basic benchmarking, let's catboost encode the non numeric categorical variables...","47c89647":"## 2. Ticket\nAround 5% of the values in ticket number is missing.  \nGenerally speaking, ticket number should be very unique to each passenger. But it might also be the case that people booked tickets for their friends and family and multiple people have boarded using the same ticket number. Thus there will not be any stright forward method to inpute this value as well. This similar to Cabin we will treat each missing class as a new category and also create another feature that tells where the values were imputed synthetically...","198322ff":"**Observations:-**  \n* Maximum number of passengers were travelling on class 3 tickets.\n* It seems that Class 1 and 2 passengers had a much higher chance of survival than Class 3 passengers.\n\n## 4. Name\nName is again something which is most likely unique to the passenger and less\/no effect on the probability of the survival of the person. Any correlation might be coincidental and not necessarily part of the signal. So we can drop this feature later.\n\n## 5. Sex\nAs obvious as it is, this feature tells us the gender of the passangers. This one might be very significant because in case of evacuation weomen and children are given higher preference. Let's see if the data agrees...","14634a71":"## 4. Embarked\nEmbarked is a categorical feature and almost 0.2% of the values are missing.  \nTo keep things simple we can impute embarked with the most popular embarked station. And we will store the missing information in another feature column.","64eb5f45":"## 5. Fare\nFare is a continuos feature and close to 0.1% values are missing.  \nIt will be a fair asumption to think that fare will be hugely related to ticket class. So we can impute the missing values with the average fare of that class...","80db1c12":"# Features Selection\nWe need to select only the important features for better performance of the model. As unnecessary in best case scenario will not add to any productive calculation of the algorithm or in worst case scenario 'confuse' the model.  \n\nTo do the same let's create a wrapper class that has all the built in statistical tests required to perform feature selection and takes some basic inputs from user and spits out the required features.","19e3dbf8":"As found out during EDA, the Fare feature has a very long tail. So as decided, we will take a log transform of the same to shorten the tail and potentially eliminate any ill effects of huge outliers.","352befbd":"# Feature Engineering","b52b03ec":"Let's look at some basic descriptive analysis...","9f6b7c3f":"We can create a synthetic feature which clubs Class-1 and Class-2 together but kepps class-3 separate, because as seen from EDA, the response from Class-1 and 2 were very similar.","d79a0b91":"**Observations:-**  \n* There are more adults on the ship as compared to childeren.\n* The average age of surviving passengers is more than deceased ones, stating that infact children and older people weere evacuated first from the ship. And just because the population of older people is larger than children, the average age is on the higher side.\n* On an average, the age of females on the boat was higher than males.\n* Older females had the highest chances of survival followed by older men.\n* The average age in Class-1 is highest, followed by Class-2 and then Class-3.\n* There are children in all 3 classes of the ship.\n* Class-2 has the oldest preson, followed by Class-3 and then Class-1.\n* Older people from Class-1 and Class-2 had better chances of survival as compared to their younger counterparts. But in Class-3, actually younger people had better chances of survival than older people.\n\n## 7. SibSp\nSibSp is a integer feature which specifies the number of siblings \/ spouses of the passenger aboard the Titanic. This might have an influence on survival because usually a human will also try to keep their relatives safe in case of a disaster.  \nWe can treat this a ordinal categorical features because there are only finite number of Siblings\/Spouse a passenger could have aboard the ship.","127111d0":"## 3. Age\nAge is a continuous feature and almost 3% of those values are missing.  \nTo keep things simple we can impute the age by the mode of all the available ages. And like earlier we will track the rows which had missing values.","55bc2768":"The average age in Class-1 is highest, followed by Class-2 and then Class-3.  \nThere are children in all 3 classes of the ship.  \nClass-2 has the oldest preson, followed by Class-3 and then Class-1.","2e6dd21b":"# Submission Prediction\n\n## 1. Stacked","62adae42":"Older females had the highest chances of survival followed by older men.","d4428ab5":"We can concatenate Pclass and Sex feature to form a synthetic feature which signifies something like \"Class-1 Male Passenger\".","433d47bb":"Passenger ages and Fares can be bucketed and dictretized into groups.","ca74e757":"# Models Benchmarking\nWe will spawn some of the most popular classifiers here and try to benchmark their performance against one another for this dataset.","ee904480":"## 2. PassengerID\nThe data description says it is an unique identifier for each row and does not hold any significant information regarding the passenger. Thus we can plan to drop this feature later while modelling.  \n\n## 3. Pclass\nThis feature signifies the ticket class of the passenger. It is available as a categorical feature with 3 distinct values:- 1st Class, 2nd Class and 3rd Class. Let's look at their distribution and response towards Survival...","d2cad7e4":"We can concatenate Age bins with sex features indicating something like \"Old Female\".","cb627fd3":"Let's see what columns we have in the training data.","1276efe8":"The average age of surviving passengers is more than deceased ones, stating that infact children and older people weere evacuated first from the ship. And just because the population of older people is larger than children, the average age is on the higher side.","7d34da65":"Similarly we can detemine if the person was travelling alone...","7ebc5d29":"We can create a synthetic variable stating the family size on board the Titanic for rach passenger...","d8d70e0d":"The distribution has a very long tail, we probably need to tranform this while feature engineering.","e6717499":"Okay, now that we have addressed all the null columns, let's have a sanity check to ensure that we did not miss anything...","24aac713":"**Observations:-**  \n* Probability distribution of Fare has a very long tail. Needs to be transformed during feature enginering.\n* People who paid higher fare, also had a higher chance of survival.\n* Average fare paid by Male passengers was lower than Female passengers.\n* Higher fare males had a higher chance of survival than lower fare males. But in case of females there was not a big difference.\n* Naturally, Class-1 had the highest fare followed by Class-2 and then Class-3.\n* Higher fare peoples among Class-1 and Class-2 had higher chances of curvival as compared to their lower fare counterparts. However in Class-3, Fare did not matter much in terms of survival probability.\n\n## 10. Embarked\nThis feature defines the port of Embarkation. We have 3 options in this category:- (C = Cherbourg, Q = Queenstown, S = Southampton).  \nThis feature might be important because it will also affect the order of filling of compartments. And due to compartment locations it might have an effect on the survival probability. This combined with Class would act like a spatial proxy for the location of the passenger inside the ship.","16321ca6":"We can create a feature that takes in if the age is in int or fraction...","50478dec":"Ok, so this is a fairly balanced dataset. We have to keep this in mind while developing our models later.  \nNow let's move on to understanding each individual feature through EDA and some visualizations... Before that l;et's create some helper functions that eases the EDA plotting process.","b5a6596f":"We can extract various ticket related information from the ticket number...","42267bc7":"We can extract various cabin related information from the cabin number...","2f86a53c":"## 3. CatBoost","55116846":"On an average, the age of females on the boat was higher than males.","812f8e64":"We can create a feature that captures the name and surname separately...","0b504766":"# Nulls Imputation\nLet's have a basic look around the data we have at hand first","8344034d":"If we convert those numbers into percentage, we can have a basic idea on how much gaps require filling in each feature or whether it is significant to fill or not...","37523435":"**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","a7a71142":"## 1. Cabin\nWe can see that the Cabin feature has quite a bit of missing values as compared to others (~ 68%).  \nAlso cabin is a categorical feature thus inputing this will be a tricky one. But to keep things simple, let's assume that the people who do not have a cabin number, did not have Cabins and we will treat this as a new category and also create one more feature that captures that those values are synthetically imputed...","6b0b6a4b":"**Observations:-**  \n* Most of the passengers embarked the ship from Southampton.\n* The average fare for people boarding from Southampton is lowest while Cherbourg and Queenstown are similar.\n* As a result, most of the people bording from Southampton are Class-3 passengers while the majority of Cherbourg and Queenstown passengers belong to Class-1 or 2.\n* Similarly, the survival probability of passengers from Cherbourg and Queenstown is higher as comapred to passengers from Southampton.\n* People embarking from Southampton who paid a higher fare had a better chance of survival than people with lower fares. But the same can not be said about the people from Cherbourg and Queenstown.\n* The Average age of people embarking from Southampton is lowest followed by Cherbourg and Queenstown.\n\n# KFold Splits  \nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not risk any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","a0670368":"## 2. LGBM","92ac768f":"# Feature Encoding","7c4fa87b":"As we can see from the descriptive analysis before, there are some features which have NaN values both in train and test set. So let's plot what is the situation for all the features..."}}