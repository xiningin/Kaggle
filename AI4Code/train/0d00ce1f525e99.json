{"cell_type":{"9e014767":"code","8550dc36":"code","150807cf":"code","29fb2775":"code","4ebdc308":"code","3d111338":"code","38315396":"code","77575a35":"code","698cc805":"code","fa6e7c72":"code","6325d296":"code","93ec1f46":"code","771ed144":"code","f7c6637e":"code","a937d33d":"code","f1f75953":"code","099dda47":"code","672341c6":"code","fbb6cf71":"code","44aaff0d":"code","9d71adb6":"markdown","b5fd9f1d":"markdown","9b4ca54b":"markdown","14f4ed13":"markdown","fc3133f5":"markdown","b07da688":"markdown","328d37a6":"markdown","1a1f2015":"markdown","5738641d":"markdown","1f52ae02":"markdown","c3a944ce":"markdown","3e6d5a6f":"markdown","2e9cb70c":"markdown","41d8ee89":"markdown"},"source":{"9e014767":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score","8550dc36":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\")","150807cf":"train.head()","29fb2775":"test.head()","4ebdc308":"train.pop(\"id\")\ntest_ids = test.pop(\"id\")","3d111338":"train_targets = train.pop(\"claim\")","38315396":"train_targets.head()","77575a35":"sns.countplot(train_targets)","698cc805":"train_desc = train.describe()\ntrain_desc.transpose()","fa6e7c72":"test_desc = test.describe()\ntest_desc.transpose()","6325d296":"desc_delta = train_desc - test_desc\ndesc_delta.transpose()","93ec1f46":"for data in [train, test]:\n    data['n_nans'] = data.isnull().sum(axis=1)\n    data['std'] = data.std(axis=1)\n    data['var'] = data.var(axis=1)","771ed144":"train.head()","f7c6637e":"test.head()","a937d33d":"columns = list(train.columns)\nfor item in train.columns:\n    if abs(train[item].max()) \/ (abs(train[item].min()) + 10e-10) > 20:\n        train[item] = np.sign(train[item]) * np.log2(np.abs(train[item]) + 1)\n        test[item] = np.sign(test[item]) * np.log2(np.abs(test[item]) + 1)\n    train_mean = train[item].mean()\n    train_std = train[item].std()\n    train[item] = (train[item] - train_mean) \/ train_std\n    test[item] = (test[item] - train_mean) \/ train_std\n    # Missing Value Imputation seems to have a bad effect to final results\n    #train[item].replace(np.NAN, train[item].mean(), inplace=True)\n    #test[item].replace(np.NAN, test[item].mean(), inplace=True)\n","f1f75953":"def evaluate(valid_targets, probs, name):\n    y_pred = np.array(probs > 0.5, dtype=int)\n    acc = accuracy_score(valid_targets, y_pred)\n    loss = log_loss(valid_targets, y_pred)\n    auc = roc_auc_score(valid_targets, probs)\n    print(\"Accuracy score: %.2f\"%(acc))\n    print(\"Log loss: %.2f\"%(loss))\n    print(\"AUC score:\", auc)\n    print(\"Classification report:\")\n    print(classification_report(valid_targets, y_pred))\n    return {\n        \"name\": name, \n        \"accuracy_score\": acc, \n        \"log_loss\": loss, \n        \"auc\": auc\n    }","099dda47":"from sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\ncats = []\nindex = 1\nfor train_indices, valid_indices in kfold.split(train, train_targets):\n    print(\"Training with Fold %d\"%(index))\n    train_features = train.iloc[train_indices]\n    train_labels = train_targets.iloc[train_indices]\n    valid_features = train.iloc[valid_indices]\n    valid_labels = train_targets.iloc[valid_indices]\n    cat_params = {\n        'iterations': 15000, \n        'loss_function': 'Logloss', \n        'depth': 8, \n        'task_type' : 'GPU',\n        'use_best_model': True,\n        'eval_metric': 'AUC',\n        'early_stopping_rounds': 1000,\n        'learning_rate': 0.03,\n        'border_count': 32,\n        'l2_leaf_reg': 3,\n        \"verbose\": 1000\n    }\n    cat = CatBoostClassifier(\n        **cat_params\n    )\n    cat.fit(train_features, train_labels, eval_set=[(valid_features, valid_labels)])\n    cats.append(cat)\n    probs = cat.predict_proba(valid_features)[:, 1]\n    result_cat = evaluate(valid_labels, probs, \"catboost\")\n    print(result_cat)\n    index += 1","672341c6":"probs_list = []\nfor cat in cats:\n    probs = cat.predict_proba(test)[:, 1]\n    probs_list.append(probs)\nprobs_array = np.array(probs_list)\nmean_probs = probs_array.mean(axis=0)","fbb6cf71":"mean_probs.shape","44aaff0d":"submission = pd.DataFrame({\"id\": list(test_ids), \"claim\": mean_probs})\nsubmission.to_csv(\"submission.csv\", index=False)","9d71adb6":"## EDA & Data Preprocessing","b5fd9f1d":"## Feature Scaling","9b4ca54b":"## Using CatBoost","14f4ed13":"The labels looks balanced. ","fc3133f5":"## Import datasets","b07da688":"### Add extra features","328d37a6":"## Import Packages","1a1f2015":"# CatBoost Tabular Playground Prediction(Sep 2021)\n## Table of Contents\n- Import Packages\n- Import Datasets\n- EDA & Preprocessing\n- Model Development & Evaluation\n- Submission","5738641d":"### Evaluation Method","1f52ae02":"### Get Train data Targets","c3a944ce":"### Drop ID columns","3e6d5a6f":"## Model Development & Evaluation\n","2e9cb70c":"Let's see statistic info between train set and test set and compare their differences, which are very small execpt for their total numbers.  Some of the parameters has a great difference, so I will apply log transformation to reduce their skewness.","41d8ee89":"## Submisssion"}}