{"cell_type":{"0bd49f43":"code","67d010dd":"code","df2846d1":"code","90a18a2c":"code","29e645c0":"code","06b71461":"code","fffe6131":"code","e8db95ab":"code","2ba110a0":"code","cecab641":"code","93b832e5":"code","99e48ae9":"code","4dc1cbf8":"code","0962d3b0":"markdown","f4d05b99":"markdown","791effd0":"markdown","e878e0c2":"markdown","ba1a0542":"markdown","8383774c":"markdown","e76d914a":"markdown","ce59abd9":"markdown","3bfe1ce9":"markdown","a16bc3bf":"markdown"},"source":{"0bd49f43":"import random\nimport copy\nfrom collections import deque\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\nimport gym\nfrom gym import spaces\nfrom gym.spaces import Box\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\nfrom kaggle_environments import make, evaluate\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","67d010dd":"%%writefile greedy-goose.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\n\n\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\n    raise TypeError(str(action) + \" is not a valid Action.\")\n\n    \n\n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n\n\n#This class encapsulates mos of the low level Hugry Geese stuff    \nclass BornToNotMedalv2:    \n    def __init__(self):\n        self.DEBUG=False\n        self.rows, self.columns = -1, -1        \n        self.my_index = -1\n        self.my_head, self.my_tail = -1, -1\n        self.geese = []\n        self.heads = []\n        self.tails = []\n        self.food = []\n        self.cell_states = []\n        self.actions = [action for action in Action]\n        self.previous_action = None\n        self.step = 1\n\n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n \n\n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n\n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n    \n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)\n        \n        \n    def preprocess_env(self, observation, configuration):\n        observation = Observation(observation)\n        configuration = Configuration(configuration)\n        \n        self.rows, self.columns = configuration.rows, configuration.columns        \n        self.my_index = observation.index\n        self.hunger_rate = configuration.hunger_rate\n        self.min_food = configuration.min_food\n\n        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n        self.my_body = [pos for pos in observation.geese[self.my_index]]\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.GOOSE.value*3):\n                    new_dead_ends.append(pos_i)                                    \n        \n                \n    def strategy_random(self, observation, configuration):\n        if self.previous_action!=None:\n            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n        else:\n            action = rand.choice([action for action in Action])\n        self.previous_action = action\n        return action.name\n                        \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied) \n    \n                        \n    def strategy_random_avoid_collision(self, observation, configuration):\n        dead_end_cell = False\n        free_cell = True\n        actions = [action \n                   for action in Action \n                   for future_position in [self._translate(self.my_head, action)]\n                   if self.valid_position(future_position)] \n        if self.previous_action!=None:\n            actions = [action for action in actions if action!=opposite(self.previous_action)] \n        if actions==[]:\n            dead_end_cell = True\n            actions = [action \n                       for action in Action \n                       for future_position in [self._translate(self.my_head, action)]\n                       if self.free_position(future_position)]\n            if self.previous_action!=None:\n                actions = [action for action in actions if action!=opposite(self.previous_action)] \n            #no alternatives\n            if actions==[]:\n                free_cell = False\n                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n\n        action = rand.choice(actions)\n        self.previous_action = action\n        if self.DEBUG:\n            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n            if free_cell:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n            else:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n        return action.name\n    \n    \n    def strategy_greedy_avoid_risk(self, observation, configuration):        \n        actions = {  \n            action: self._min_distance_to_food(future_position)\n            for action in Action \n            for future_position in [self._translate(self.my_head, action)]\n            if self.safe_position(future_position)\n        }\n  \n        if self.previous_action!=None:\n            actions.pop(opposite(self.previous_action), None)\n        if any(actions):\n            action = min(actions.items(), key=lambda x: x[1])[0]\n            self.previous_action = action\n            if self.DEBUG:\n                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n            return action.name\n        else:\n            return self.strategy_random_avoid_collision(observation, configuration)\n    \n    \n    #Redefine this method\n    def agent_strategy(self, observation, configuration):\n        action = self.strategy_greedy_avoid_risk(observation, configuration)\n        return action\n    \n    \n    def agent_do(self, observation, configuration):\n        self.preprocess_env(observation, configuration)\n        move = self.agent_strategy(observation, configuration)\n        self.step += 1\n        #if self.DEBUG:\n        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n        return move\n\n    \n    \ndef agent_singleton(observation, configuration):\n    global gus    \n    \n    try:\n        gus\n    except NameError:\n        gus = BornToNotMedalv2()\n            \n    action = gus.agent_do(observation, configuration)\n\n    \n    return action\n\n\n    ","df2846d1":"%%writefile choose_random.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\n\nimport random\nimport copy\noppo_dict = {1:3, 2:4, 3:1, 4:2}\nlast_action = None\naction_list = [1,2,3,4]\ndef agent(obs, conf):\n    global last_action\n    action = random.randint(0, 2)\n    allow = copy.deepcopy(action_list)\n    if last_action is not None:\n        allow.remove(last_action)\n    \n    action = allow[action]\n    last_action = oppo_dict[action]\n    return Action(action).name","90a18a2c":"class GeeseEnv(gym.Env):\n    def __init__(self, opponent=[\"random\"], debug=False):\n        \"\"\"Hungry Geese Environment\n\n        Args:\n            opponent (list, optional): Survived Agent. Defaults to [\"random\"].\n            debug (bool, optional): Whether to output an debug. Defaults to False.\n        \"\"\"\n        super(GeeseEnv, self).__init__()\n        \n        # Number of environments in the vectorized environment.\n        self.num_envs = 1\n\n        # self.num_previous_observations = 1\n        self.debug=debug\n\n        # Permitted actions\n        # ['NORTH', 'EAST', 'SOUTH', 'WEST']\n        self.actions = [action.name for action in Action]\n        # Defined Action Space(Must)\n        self.action_space = spaces.Discrete(len(self.actions))\n\n        # Environment and Configuration\n        self.env = make(\"hungry_geese\", debug=self.debug)\n        self.rows = self.env.configuration.rows\n        self.columns = self.env.configuration.columns\n        self.hunger_rate = self.env.configuration.hunger_rate\n        self.min_food = self.env.configuration.min_food\n\n        # Defined Opponent\n        self.trainer = self.env.train([None, *opponent])\n        \n        # Observation Space(Must?)\n        # Defined value range and shape in output observation \n        self.observation_space = Box(low=0, high=1, shape=(13, 7, 11), dtype=np.uint8)\n        \n        self.length = 1\n    \n    def step(self, action):\n        \"\"\"\n        Input agent action, Output observation after players action\n\n        Args:\n            action (int): \n\n        Returns:\n            np.ndarray: observation (same GeeseNet Input shape)\n            int: reward\n            done: whether end game or not\n            dict: env information\n        \"\"\"\n        action = self.actions[action]\n        obs, reward, done, info = self.trainer.step(action)\n        conv_obs, conv_reward = self.convert(obs)\n        return conv_obs, conv_reward, done, info\n    \n    def reset(self):\n        \"\"\"Reset Environment\n\n        Returns:\n            np.ndarray: observation\n        \"\"\"\n        self.length = 1\n        obs = self.trainer.reset()\n        conv_obs, _ = self.convert(obs)\n        return conv_obs\n    \n    def convert(self, observation):\n        \"\"\"Convert Observation\n\n        Args:\n            observation (dict): Output observation at default environment\n\n        Returns:\n            np.ndarray: converted observation (same GeeseNet Input shape)\n        \"\"\"\n        index = observation[\"index\"]\n        step = observation[\"step\"]\n        geese = observation[\"geese\"]\n        food = observation[\"food\"]\n\n        # remain = len([g for g in geese if len(g) > 0])\n\n        mappings = np.zeros((13, 77), dtype=int)\n\n        # MY GEESE\n        my_geese = geese[index]\n        if len(my_geese) > 0:\n            my_geese_head = my_geese[0]\n            my_geese_body = my_geese\n            my_geese_tail = my_geese[-1]\n\n            mappings[0][my_geese_head] = 1\n            for mgb in my_geese_body:\n                mappings[1][mgb] = 1\n            mappings[2][my_geese_tail] = 1\n\n        # OP GEESE\n        count = 1\n        for i in range(len(geese)):\n            if i == index:\n                continue\n\n            op_geese = geese[i]\n            if len(op_geese) > 0:\n                op_geese_head = op_geese[0]\n                op_geese_body = op_geese\n                op_geese_tail = op_geese[-1]\n\n                mappings[count * 3][op_geese_head] = 1\n                for ogb in op_geese_body:\n                    mappings[count * 3 + 1][ogb] = 1\n                mappings[count * 3 + 2][op_geese_tail] = 1\n\n            count += 1\n\n        # FOOD\n        for f in food:\n            mappings[12][f] = 1\n        \n        length = len(geese[index])\n        if length > self.length:\n            self.length = length\n            reward = 50\n        elif length == self.length:\n            reward = 5\n        else:\n            self.length = length\n            reward = -50\n        \n            \n        remain = len([c for idx, c in enumerate(geese) if (len(c) > 0) and (idx != index)])\n#         reward = length * (3 - remain) - step\/200\n#         if (remain != 0) and (length == 0):\n#             reward = -1\n        if (remain == 0) and (length > 0):\n            reward = 1000\n        elif (remain != 0) and (length == 0):\n            reward = -1000\n            \n            \n        return mappings.reshape(-1, 7, 11), reward","29e645c0":"class Geese:\n    def __init__(self, state_dim, action_dim, save_dir, load_weight):\n        \"\"\"Training Class\n\n        Args:\n            state_dim (np.ndarray): observation shape\n            action_dim (int): action length \n            save_dir (str): save path (log, checkpoint)\n        \"\"\"\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n        \n        self.use_cuda = torch.cuda.is_available()\n        \n        self.net = GeeseNet(self.state_dim, self.action_dim).float()\n#         self.net.load_state_dict(torch.load(load_weight))\n        if self.use_cuda:\n            self.net = self.net.to(device=\"cuda\")\n        \n        # Rate of search to be performed\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.9999\n        self.exploration_rate_min = 0.005\n        self.curr_step = 0\n        \n        self.save_every = 5e5\n            \n        self.memory = deque(maxlen=100000)\n        self.batch_size=32\n\n        self.gamma = 0.9\n\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.025)\n        self.loss_fn = torch.nn.SmoothL1Loss()\n        self.burnin = 1e4  # Minimum number of steps required to train an experience.\n        self.learn_every = 3  # Number of steps to indicate when to update Q_online\n        self.sync_every = 1e4  # Number of steps to indicate when to synchronize Q_target & Q_online\n\n    \n    def act(self, state):\n        \"\"\"Predict action from observation.\n        (Random with a certain probability)\n\n        Args:\n            state (np.ndarray): observation\n\n        Returns:\n            int: action\n        \"\"\"\n        if np.random.rand() < self.exploration_rate:\n            action = np.random.randint(self.action_dim)\n        \n        else:\n            state = state.__array__()\n            if self.use_cuda:\n                state = torch.tensor(state).cuda()\n            else:\n                state = torch.tensor(state)\n            state = state.unsqueeze(0).float()\n            action_values = self.net(state, model=\"online\")\n            action = torch.argmax(action_values, axis=1).item()\n        \n        # update exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n        self.curr_step += 1\n        \n        return action\n\n    def cache(self, state, next_state, action, reward, done):\n        state = state.__array__()\n        next_state = next_state.__array__()\n        if self.use_cuda:\n            state = torch.tensor(state).cuda()\n            next_state = torch.tensor(next_state).cuda()\n            action = torch.tensor([action]).cuda()\n            reward = torch.tensor([reward]).cuda()\n            done = torch.tensor([done]).cuda()\n        else:\n            state = torch.tensor(state)\n            next_state = torch.tensor(next_state)\n            action = torch.tensor([action])\n            reward = torch.tensor([reward])\n            done = torch.tensor([done])       \n\n        self.memory.append((state, next_state, action, reward, done))\n\n    def recall(self):\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n        return state,  next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n    \n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = torch.argmax(next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())\n\n    def save(self):\n        save_path = (\n            self.save_dir \/ f\"geese_net_{int(self.curr_step \/\/ self.save_every)}.chkpt\"\n        )\n        torch.save(\n            self.net.state_dict(),\n            save_path,\n        )\n        print(f\"GeeseNet saved to {save_path} at step {self.curr_step}\")\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step < self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # Sampling Memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimator\n        state = state.float()\n        td_est = self.td_estimate(state, action)\n\n        #Get TD Target\n        next_state = next_state.float()\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Back propagation of loss to Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)","06b71461":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\nfrom kaggle_environments import make\nopponent = ['choose_random.py', 'choose_random.py', 'choose_random.py']\nenv = GeeseEnv(opponent=opponent)\n","fffe6131":"class PartConv2d(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_my_3_1 = nn.Conv2d(3, 6, kernel_size=4)\n        self.conv_my_3_2 = nn.Conv2d(6, 2, kernel_size=2)\n        self.bn_my = nn.BatchNorm2d(2)\n        self.linear_my = nn.Linear(7, 6)\n        \n        self.conv_op_3_1 = nn.Conv2d(3, 6, kernel_size=4)\n        self.conv_op_3_2 = nn.Conv2d(6, 2, kernel_size=2)\n        self.bn_op = nn.BatchNorm2d(2)\n        self.linear_op = nn.Linear(7, 6)\n        \n        self.conv_fd_1_1 = nn.Conv2d(1, 6, kernel_size=4)\n        self.conv_fd_1_2 = nn.Conv2d(6, 2, kernel_size=2)      \n        self.bn_fd = nn.BatchNorm2d(2)\n        self.linear_fd = nn.Linear(7, 6)\n        \n    def forward(self, input):\n        my = input[:, 0:3]\n        op1 = input[:, 3:6]\n        op2 = input[:, 6:9]\n        op3 = input[:, 9:12]\n        food = input[:, 12:]\n        \n        my = self.conv_my_3_1(my)\n        my = self.conv_my_3_2(my)\n        my = self.bn_my(my)\n        \n        my = self.linear_my(my)\n        \n        opes = None\n        for op in [op1, op2, op3]:\n            op = self.conv_op_3_1(op)\n            op = self.conv_op_3_2(op)\n            op = self.bn_op(op)\n            if opes is None:\n                opes = op\n            else:\n                opes += op\n\n        opes = self.linear_op(opes)\n        \n        food = self.conv_fd_1_1(food)\n        food = self.conv_fd_1_2(food)\n        food = self.bn_fd(food)\n        food = self.linear_fd(food)\n        x = torch.cat((my, opes, food), dim=1)\n        return x\n\n    \n\nclass GeeseNet(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        print(input_dim)\n        d, r, c = input_dim\n\n        assert r == 7\n        assert c == 11\n\n        self.online = nn.Sequential(\n            PartConv2d(),\n            nn.LeakyReLU(),\n            nn.Flatten(),\n            nn.Linear(108, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 32),\n            nn.LeakyReLU(),\n            nn.Linear(32, 8),\n            nn.LeakyReLU(),\n            nn.Linear(8, output_dim),\n        )\n\n        self.target = copy.deepcopy(self.online)\n\n        for p in self.target.parameters():\n            p.requires_grad = False\n    \n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)","e8db95ab":"import numpy as np\nimport time, datetime\nimport matplotlib.pyplot as plt\n\n\nclass MetricLogger:\n    def __init__(self, save_dir):\n        self.save_log = save_dir \/ \"log\"\n        with open(self.save_log, \"w\") as f:\n            f.write(\n                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n            )\n        self.ep_rewards_plot = save_dir \/ \"reward_plot.jpg\"\n        self.ep_lengths_plot = save_dir \/ \"length_plot.jpg\"\n        self.ep_avg_losses_plot = save_dir \/ \"loss_plot.jpg\"\n        self.ep_avg_qs_plot = save_dir \/ \"q_plot.jpg\"\n\n        # \u6307\u6a19\u306e\u5c65\u6b74\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # reacord()\u304c\u547c\u3073\u51fa\u3055\u308c\u308b\u305f\u3073\u306b\u8ffd\u52a0\u3055\u308c\u308b\u79fb\u52d5\u5e73\u5747\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # \u73fe\u5728\u306e\u30a8\u30d4\u30bd\u30fc\u30c9\u306e\u6307\u6a19\n        self.init_episode()\n\n        # \u6642\u9593\u3092\u8a18\u9332\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"\u30a8\u30d4\u30bd\u30fc\u30c9\u7d42\u4e86\u6642\u306e\u8a18\u9332\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss \/ self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q \/ self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            f\"Episode {episode} - \"\n            f\"Step {step} - \"\n            f\"Epsilon {epsilon} - \"\n            f\"Mean Reward {mean_ep_reward} - \"\n            f\"Mean Length {mean_ep_length} - \"\n            f\"Mean Loss {mean_ep_loss} - \"\n            f\"Mean Q Value {mean_ep_q} - \"\n            f\"Time Delta {time_since_last_record} - \"\n            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n        )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n            )\n\n        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n            plt.savefig(getattr(self, f\"{metric}_plot\"))\n            plt.clf()\n\nuse_cuda = torch.cuda.is_available()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nsave_dir = Path(\".\")\n\ngeese = Geese(state_dim=(13, 7, 11), action_dim=env.action_space.n, save_dir=save_dir, load_weight=\"..\/input\/geese-weight\/geese_net_0.chkpt\")\n\nlogger = MetricLogger(save_dir)\n\nepisodes = 50000\nfor e in range(episodes):\n\n    state = env.reset()\n\n    while True:\n\n        action = geese.act(state)\n        next_state, reward, done, info = env.step(action)\n        geese.cache(state, next_state, action, reward, done)\n        q, loss = geese.learn()\n        logger.log_step(reward, loss, q)\n        state = next_state\n        if done:\n            break\n\n    logger.log_episode()\n\n    if e % 5000 == 0:\n        logger.record(episode=e, epsilon=geese.exploration_rate, step=geese.curr_step)\ngeese.save()","2ba110a0":"%%writefile dqnv1.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom collections import deque\nimport random\nimport copy\nimport gym\nfrom gym import spaces\nfrom gym.spaces import Box\nfrom pathlib import Path\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\nfrom kaggle_environments import make\nimport os\n\nclass PartConv2d(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_my_3_1 = nn.Conv2d(3, 6, kernel_size=4)\n        self.conv_my_3_2 = nn.Conv2d(6, 2, kernel_size=2)\n        self.bn_my = nn.BatchNorm2d(2)\n        self.linear_my = nn.Linear(7, 6)\n        \n        self.conv_op_3_1 = nn.Conv2d(3, 6, kernel_size=4)\n        self.conv_op_3_2 = nn.Conv2d(6, 2, kernel_size=2)\n        self.bn_op = nn.BatchNorm2d(2)\n        self.linear_op = nn.Linear(7, 6)\n        \n        self.conv_fd_1_1 = nn.Conv2d(1, 6, kernel_size=4)\n        self.conv_fd_1_2 = nn.Conv2d(6, 2, kernel_size=2)      \n        self.bn_fd = nn.BatchNorm2d(2)\n        self.linear_fd = nn.Linear(7, 6)\n        \n    def forward(self, input):\n        my = input[:, 0:3]\n        op1 = input[:, 3:6]\n        op2 = input[:, 6:9]\n        op3 = input[:, 9:12]\n        food = input[:, 12:]\n        \n        my = self.conv_my_3_1(my)\n        my = self.conv_my_3_2(my)\n        my = self.bn_my(my)\n        \n        my = self.linear_my(my)\n        \n        opes = None\n        for op in [op1, op2, op3]:\n            op = self.conv_op_3_1(op)\n            op = self.conv_op_3_2(op)\n            op = self.bn_op(op)\n            if opes is None:\n                opes = op\n            else:\n                opes += op\n\n        opes = self.linear_op(opes)\n        \n        food = self.conv_fd_1_1(food)\n        food = self.conv_fd_1_2(food)\n        food = self.bn_fd(food)\n        food = self.linear_fd(food)\n        x = torch.cat((my, opes, food), dim=1)\n        return x\n\n    \n\nclass GeeseNet(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        print(input_dim)\n        d, r, c = input_dim\n\n        assert r == 7\n        assert c == 11\n\n        self.online = nn.Sequential(\n            PartConv2d(),\n            nn.LeakyReLU(),\n            nn.Flatten(),\n            nn.Linear(108, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 64),\n            nn.LeakyReLU(),\n            nn.Linear(64, 32),\n            nn.LeakyReLU(),\n            nn.Linear(32, 8),\n            nn.LeakyReLU(),\n            nn.Linear(8, output_dim),\n        )\n\n        self.target = copy.deepcopy(self.online)\n\n        for p in self.target.parameters():\n            p.requires_grad = False\n    \n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)\n        \ndef convert_numpy(observation):\n    index = observation[\"index\"]\n    step = observation[\"step\"]\n    geese = observation[\"geese\"]\n    food = observation[\"food\"]\n\n    remain = len([g for g in geese if len(g) > 0])\n\n    mappings = np.zeros((len(geese) * 3 + 1, 77), dtype=int)\n\n    # MY GEESE\n    my_geese = geese[index]\n    if len(my_geese) == 0:\n        return mappings.reshape(-1, 7, 11)\n    my_geese_head = my_geese[0]\n    my_geese_body = my_geese\n    my_geese_tail = my_geese[-1]\n\n    mappings[0][my_geese_head] = 1\n    for mgb in my_geese_body:\n        mappings[1][mgb] = 1\n    mappings[2][my_geese_tail] = 1\n\n    # OP GEESE\n    count = 1\n    for i in range(len(geese)):\n        if i == index:\n            continue\n\n        op_geese = geese[index]\n        if len(op_geese) > 0:\n            op_geese_head = op_geese[0]\n            op_geese_body = op_geese\n            op_geese_tail = op_geese[-1]\n\n            mappings[count * 3][op_geese_head] = 1\n            for ogb in op_geese_body:\n                mappings[count * 3 + 1][ogb] = 1\n            mappings[count * 3 + 2][op_geese_tail] = 1\n\n        count += 1\n\n    # FOOD\n    for f in food:\n        mappings[12][f] = 1\n\n    return mappings.reshape(-1, 7, 11)\n\ndef my_dqn(observation, configuration):\n    global model, obs_prep, last_action, last_observation, previous_observation\n\n    tgz_agent_path = '\/kaggle_simulations\/agent\/'\n    normal_agent_path = '.'\n    model_name = \"geese_net_0\"\n    num_previous_observations = 1\n    epsilon = .1\n    init = False\n    debug = False\n\n    try:\n        model\n    except NameError:\n        init=True\n    else:\n        if model==None:\n            init = True \n            initializing\n    if init:\n        #initializations\n        defaults = [configuration.rows,\n                    configuration.columns,\n                    configuration.hunger_rate,\n                    configuration.min_food]\n\n        model = GeeseNet((13, 7, 11), 4)\n        last_action = -1\n        \n        file_name = os.path.join(normal_agent_path, f'{model_name}.chkpt')\n        if not os.path.exists(file_name):\n            file_name = os.path.join(tgz_agent_path, f'{model_name}.chkpt')\n            \n        model.load_state_dict(torch.load(file_name))\n\n    conv_obs = convert_numpy(observation)\n    tensor_obs = torch.tensor(conv_obs).unsqueeze(0).float()\n    n_out = model(tensor_obs, \"online\") \n    pred = torch.argmax(n_out, axis=1).item()\n    actions = [action.name for action in Action]\n        \n    last_action = actions[pred]\n    return last_action #return action","cecab641":"import kaggle_environments\nfrom kaggle_environments import make, evaluate, utils\n\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run([\"dqnv1.py\", 'choose_random.py', 'choose_random.py', 'choose_random.py'])\nenv.render(mode=\"ipython\", width=800, height=700)","93b832e5":"\n\nresult = evaluate(\n    \"hungry_geese\",\n    [\"dqnv1.py\", 'choose_random.py', 'choose_random.py', 'choose_random.py'],\n    num_episodes=100,\n)","99e48ae9":"result_df = pd.DataFrame(result, columns=[\"Submission\", \"Opponent1\", \"Opponent2\", \"Opponent3\"])\nresult_rank_df = result_df.rank(ascending=False, axis=1, method=\"min\")\n\nsns.heatmap(pd.concat([\n    result_rank_df[\"Submission\"].value_counts(),\n    result_rank_df[\"Opponent1\"].value_counts(),\n    result_rank_df[\"Opponent2\"].value_counts(),\n    result_rank_df[\"Opponent3\"].value_counts()\n], axis=1), cmap='Oranges', annot=True)","4dc1cbf8":"!tar cvzf submission.tar.gz dqnv1.py geese_net_0.chkpt","0962d3b0":"# Supervised","f4d05b99":"# Environment","791effd0":"# Model(NN)","e878e0c2":"# Set Environment","ba1a0542":"# Train","8383774c":"The standard environment, plus a converting process for the observed values.","e76d914a":"Referense: [Train a MARIO-Playing RL Agent](https:\/\/pytorch.org\/tutorials\/intermediate\/mario_rl_tutorial.html)","ce59abd9":"# Trainer","3bfe1ce9":"# Submission","a16bc3bf":"I'm using greedy risk averse goose from: [Greedy-Goose](https:\/\/www.kaggle.com\/victordelafuente\/greedy-risk-averse-improved-dead-end-detection)"}}