{"cell_type":{"ddac8428":"code","6e5ab622":"code","8abb722f":"code","16c7f875":"code","7f7a107e":"code","dddc8bb0":"code","7f66b7b8":"code","1ba80480":"code","018c83b0":"code","f891eed9":"code","d6e2e791":"markdown","8e62f9ee":"markdown","8945f38f":"markdown","3fcb0623":"markdown","ebe84adb":"markdown","4a69d19b":"markdown","45d770f4":"markdown","d48994cd":"markdown","ae595b0e":"markdown","db62a184":"markdown","d42bcad0":"markdown","405a7dd7":"markdown","a4038b8e":"markdown"},"source":{"ddac8428":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.model_selection import KFold\nnp.random.seed(42)","6e5ab622":"# size of the test set (for our purposes we need to know the targets, \n# so this is different from the competition test set)\nTEST_SIZE = 0.9\n\n# number of folds to try out\nN_FOLDS = [3, 5, 7, 10]\n\n# define model parameters\nLEARNING_RATE = 0.1\nN_ESTIMATORS  = [10, 25, 50, 75, 100, 125, 150, 200] # this defines different models\nMAX_DEPTH     = 4\nN_JOBS        = 16\nTREE_METHOD   = 'hist'\nVERBOSITY     = 1","8abb722f":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')","16c7f875":"train.head()","7f7a107e":"ids = train['id'].values\ntrain_ids = np.random.choice(ids, replace=False, size=int((1 - TEST_SIZE) * len(train)))\ntest_ids = np.array([x for x in ids if x not in train_ids])","dddc8bb0":"y_train = train[train['id'].isin(train_ids)]['target'].values\nX_train = train[train['id'].isin(train_ids)].drop(['id', 'target'], axis = 1).values\ny_test  = train[train['id'].isin(test_ids )]['target'].values\nX_test  = train[train['id'].isin(test_ids )].drop(['id', 'target'], axis = 1).values","7f66b7b8":"# Save final AUC for each K-fold and for each XGB model (defined by the number of estimators)\nauc = [[] for x in N_FOLDS]\n\n# Also save the results for out-of-fold part of the train set\n# Three indices: 1. how many K-folds (3, 5, ...) \n#                2. which model (how many estimators) \n#                3. which out-of-fold\noof_auc = [[] for x in N_FOLDS]\n    \n#Iterate over all possible K-fold cross-validation strategies\nfor i, nf in enumerate(N_FOLDS):\n    kf = KFold(n_splits=nf) \n    \n    print(f'\\nRunning {nf}-fold splitting')\n    print('-----')\n\n    # Iterate over XGB models (determined by the number of estimators)\n    for nest in N_ESTIMATORS:\n\n        print(f'Running {nest} estimators')\n\n        # define the model\n        xgb = XGBClassifier(learning_rate = LEARNING_RATE, n_estimators = nest, max_depth = MAX_DEPTH, \n                                n_jobs = N_JOBS, tree_method = TREE_METHOD, verbosity=VERBOSITY, \n                                eval_metric = 'logloss', use_label_encoder = False)\n\n\n        # predictions on the test set - we will average over the K-folds\n        y_test_pred = np.zeros(len(X_test))\n\n        # out-of-fold AUC for the current CV strategy\/model\n        c_oof_auc = []\n\n        # iterate over the K-folds\n        for train_index, valid_index in kf.split(X_train):\n\n            # fit the model\ton the train set\n            model_xgb = xgb.fit(X_train[train_index],y_train[train_index])\n\n            # predict on the test set - average over the K-folds\n            y_test_pred += model_xgb.predict_proba(X_test)[:,1]\/nf\n\n            # predict on the out-of-fold part of the train set\n            y_valid_pred = model_xgb.predict_proba(X_train[valid_index])[:,1]\n            c_oof_auc.append(roc_auc_score(y_train[valid_index], y_valid_pred))\n\n        # Save area under the curve for the final prediction \n        auc[i].append(roc_auc_score(y_test, y_test_pred))\n        # and also for the out-of-fold part of the training set\n        oof_auc[i].append(c_oof_auc)\n        \n        # Keep us informed about what is going on\n        print(f'     auc: {auc[i][-1]}')\n        print(f'     oof auc: {oof_auc[i][-1]}')","1ba80480":"cols = ['r', 'g', 'b', 'k']\n\nfor idx, col in zip(range(4), cols):\n    plt.scatter(N_ESTIMATORS, auc[idx], color = col, label = f'{N_FOLDS[idx]} folds')\n    \nplt.legend()\nplt.ylabel('AUC')\nplt.xlabel('Number of XGBoost estimators');","018c83b0":"for idx, col in zip(range(4), cols):\n    plt.scatter(N_ESTIMATORS, auc[idx] - np.array(auc[-1]), color = col, label = f'{N_FOLDS[idx]} folds')\n    # we convert to np.array as subtracting two lists is not defined\n    \nplt.legend()\nplt.ylabel(f'AUC relative to the {N_FOLDS[-1]}-fold result')\nplt.xlabel('Number of XGBoost estimators');","f891eed9":"cols = ['r', 'g', 'b', 'orange']\n\nfor i, ne in enumerate(N_ESTIMATORS):\n    for j, col in enumerate(cols):\n        if i == 0: #Avoid having too large a legend\n            plt.errorbar(\n                ne + 2*j - 3, \n                np.mean(oof_auc[j][i]), \n                yerr = np.std(oof_auc[j][i]), \n                color = col,\n                label = f'{N_FOLDS[j]} folds'\n            )\n        else:\n            plt.errorbar(\n                ne + 2*j - 3, \n                np.mean(oof_auc[j][i]), \n                yerr = np.std(oof_auc[j][i]), \n                color = col\n            )\n\nplt.scatter(N_ESTIMATORS, auc[-1], color = 'k')\nplt.legend();\nplt.ylabel('AUC')\nplt.xlabel('Number of XGBoost estimators');","d6e2e791":"Separate train and test sets","8e62f9ee":"# Summary","8945f38f":"Here we compare the results on the test set (which do not depend much on number of K-folds, so we just plot one with black points) with the out-of-fold results (color, showing +- one standard deviation).","3fcb0623":"Here we look at the differences. We subtract the result obtained with 10 K-folds and confirm that the differences are really tiny","ebe84adb":"# Investigate the results","4a69d19b":"In this notebook our aim is focusing on K-fold cross validation and getting better intuition for it. We take a bunch of models (all XGBoost, with varying number of estimators) and see how well we can use AUC estimated from the out-of-fold part of the training set to predict AUC actually achieved on the test set.\n\nWe find that the results on the test set depend only very little on the K-fold cross validation we use. The out-of-fold AUC undershoot the test set AUC quite notably and the difference seems to be the largest for the 10-fold CV. This is a bit surprising, though it might be related to the large number of mislabeled samples.","45d770f4":"# Define various parameters of the K-fold CV and models. Plus the test set size.","d48994cd":"# Import libraries","ae595b0e":"We see that the out-of-fold results undershoot the black dots, which represent the test set results. This is not that surprising as each was out-of-fold was trained on a subset of the data only and we expect the out-of-fold results to trail a bit. \n\nWhat is a bit surprising is the magnitude of the difference and the fact that the 10-fold CV leads to the worst out-of-fold result. It also has the largest error standard deviation. \n\nOverall we expected a somewhat tighter relation between the out-of-fold results and the results on the test set, but this might be caused by the large number of mislabelled data that [appears to be present](https:\/\/www.kaggle.com\/motloch\/nov21-mislabeled-25).","db62a184":"# Load the data, separate training and test set","d42bcad0":"# Train various XGBoost classifiers using different number of K-folds","405a7dd7":"First let's compare the final results on the test set. We find that regardless of the cross-validation strategy, we get very similar results! The choice of model matters way more than the number of K-folds we choose.","a4038b8e":"Here we train various XGBoost classifiers using K-fold cross validation. To simulate a set of models, at each number of K-folds we train several XGBoosts each with a different number of estimators.\n\nFor each model and K-fold CV we store average AUC on the test. For later analysis, for each fold we also store AUC for the out-of-fold part of the train set."}}