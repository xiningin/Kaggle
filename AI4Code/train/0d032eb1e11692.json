{"cell_type":{"96b03b90":"code","e617e75e":"code","e458b3e8":"code","e7a21db0":"code","b7389655":"code","1d73622c":"code","48616aa6":"code","c5b1fda0":"code","28e564cd":"code","b55b875d":"code","0c43b542":"code","1a44dff8":"code","33409bd2":"code","87b2e243":"code","7c2bfe4f":"code","f20731d4":"code","2d802985":"code","44386f3b":"code","3929b48d":"code","88417766":"code","ebd0701e":"code","f5f06b16":"code","159f2174":"code","c375de8f":"code","6c5d7321":"code","1adc05a1":"code","533fa822":"code","472921a8":"code","f6627bdf":"code","5b6df3b9":"code","e0bf196e":"code","eb8b1013":"code","6dee787a":"code","68cbb233":"code","11e7e612":"code","5c305924":"code","dc71d70f":"code","c2c1f021":"code","8a950f42":"code","56c02e02":"code","d8691cf0":"code","c5a92667":"code","ad974024":"code","7e3cc196":"code","b4920615":"code","d4c24fa6":"code","4c93c168":"code","1184e606":"code","dbbf1caf":"code","3270247b":"code","33cb2bf4":"code","26b1cff7":"code","c6cc7da9":"code","9f532ad8":"code","e32174d0":"markdown","dcf17e2b":"markdown","27d79d2f":"markdown","3c854f22":"markdown","399a0bc2":"markdown","6987d969":"markdown"},"source":{"96b03b90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e617e75e":"import pandas as pd\nfrom tqdm.notebook import tqdm\nimport string\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom PIL import Image\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport pandas as pd","e458b3e8":"with open('\/kaggle\/input\/dlassignment4d2\/train.en') as f:\n    train_en = f.readlines()\ntrain_en = [x.strip() for x in train_en] \nwith open('\/kaggle\/input\/dlassignment4d2\/train.ta') as f:\n    train_ta = f.readlines()\ntrain_ta = [x.strip() for x in train_ta] \n\nwith open('\/kaggle\/input\/dlassignment4d2\/dev.en') as f:\n    val_en = f.readlines()\nval_en = [x.strip() for x in val_en] \nwith open('\/kaggle\/input\/dlassignment4d2\/dev.ta') as f:\n    val_ta = f.readlines()\nval_ta = [x.strip() for x in val_ta] \n\nwith open('\/kaggle\/input\/finaltest\/test.en') as f:\n    test_en = f.readlines()\ntest_en = [x.strip() for x in test_en] \nwith open('\/kaggle\/input\/finaltest\/test.ta') as f:\n    test_ta = f.readlines()\ntest_ta = [x.strip() for x in test_ta] \n\n","e7a21db0":"print(\"Train eng\",len(train_en))\nprint(\"Train tam\",len(train_ta))\nprint(\"Val eng\",len(val_en))\nprint(\"Val tam\",len(val_ta))","b7389655":"start_token = 'starttoken'\nend_token = 'endtoken'","1d73622c":"tkzr_en = Tokenizer()\n# tkzr_en.fit_on_texts([start_token,end_token])\ntkzr_en.fit_on_texts(train_en+val_en+test_ta)\nvocab_len_en = len(tkzr_en.word_index)+1\n\ntkzr_ta = Tokenizer()\ntkzr_ta.fit_on_texts([start_token,end_token])\ntkzr_ta.fit_on_texts(train_ta+val_ta+test_ta)\nvocab_len_ta = len(tkzr_ta.word_index)+1\n","48616aa6":"vocab_len_ta","c5b1fda0":"# start_token_t = tkzr_en.word_index[start_token]\n# end_token_t = tkzr_en.word_index[end_token]\ntrain_en_t = tkzr_en.texts_to_sequences(train_en)\nval_en_t = tkzr_en.texts_to_sequences(val_en)\ntest_en_t = tkzr_en.texts_to_sequences(test_en)\n# train_en_t = [[start_token_t]+i+[end_token_t] for i in train_en_t]\n\nstart_token_t = tkzr_ta.word_index[start_token]\nend_token_t = tkzr_ta.word_index[end_token]\ntrain_ta_t = tkzr_ta.texts_to_sequences(train_ta)\ntrain_ta_t = [[start_token_t]+i+[end_token_t] for i in train_ta_t]","28e564cd":"f = open(\"\/kaggle\/input\/glove6b200d\/glove.6B.200d.txt\", encoding=\"utf-8\")\ndictionary = set(tkzr_en.word_index.keys())\ndic_em = {}\npbar = tqdm(total=4*10e4)\nwhile True:\n    pbar.update(1)\n    line = f.readline() \n    if not line: \n        break\n    values = line.split()\n    word = values[0]\n    if(word in dictionary):\n        dic_em[word] = np.asarray(values[1:], dtype='float32')\n        dictionary.remove(word)\n# #     embeddings_index[word] = coefs\nf.close()\npbar.close()\n\nfor rem_word in tqdm(dictionary):\n    dic_em[rem_word] = np.ones(200)\n    \ndic_em_csv = pd.DataFrame.from_dict(dic_em,orient='index')\n# dic_em_csv.to_csv('glove_mapping_dl_assignment.csv')","b55b875d":"en_words_set = set(tkzr_en.word_index.keys())\nembedding_weights = np.zeros((vocab_len_en,200))\nfor word in tqdm(tkzr_en.word_index):\n    try:\n        embedding_weights[tkzr_en.word_index[word]] = dic_em_csv.loc[word][:200]\n    except Exception as e:\n        print(word,e)\n        pass","0c43b542":"max_en_len = max(len(i) for i in train_en_t)\nmax_ta_len = max(len(i) for i in train_ta_t)\nprint(max_en_len,max_ta_len)","1a44dff8":"to_categorical([0,1,2,3],4)","33409bd2":"encoder_input_batch = list()\ndecoder_input_batch = list()\ndecoder_output_batch = list()\nfor (i,(en_t,ta_t)) in enumerate(zip(train_en_t,train_ta_t)):\n    encoder_input_batch.append(en_t)\n#     ta_t = pad_sequences(ta_t,max_ta_len,padding='post')\n    decoder_input_batch.append(ta_t)\n#     decoder_output_batch.append(ta_t,num_classes=vocab_len_ta\n    \n    if(i==3):\n        break\nencoder_input_batch = pad_sequences(encoder_input_batch,max_en_len,padding='post')\ndecoder_input_batch = pad_sequences(decoder_input_batch,max_ta_len,padding='post')\nfor ta_item in decoder_input_batch:\n    decoder_output_batch.append(to_categorical(ta_item,num_classes=vocab_len_ta))\n    \n\n","87b2e243":"def generator(batch_size=1):\n    num = 0\n    while(True):\n        encoder_input_batch,decoder_input_batch,decoder_output_batch = list(),list(),list()\n        for (i,(en_t,ta_t)) in enumerate(zip(train_en_t,train_ta_t)):\n            encoder_input_batch.append(en_t)\n            decoder_input_batch.append(ta_t)\n            num+=1\n            if(num==batch_size):\n                encoder_input_batch = pad_sequences(encoder_input_batch,max_en_len,padding='post')\n                decoder_input_batch = pad_sequences(decoder_input_batch,max_ta_len,padding='post')\n                for ta_item in decoder_input_batch:\n                    decoder_output_batch.append(to_categorical(ta_item,num_classes=vocab_len_ta))\n                yield([encoder_input_batch,decoder_input_batch],np.asarray(decoder_output_batch))\n                encoder_input_batch,decoder_input_batch,decoder_output_batch = list(),list(),list()\n                num=0\n","7c2bfe4f":"gen = generator(5)","f20731d4":"t1 = next(gen)","2d802985":"print(t1[0][0].shape)\nprint(t1[0][1].shape)\nprint(t1[1].shape)","44386f3b":"from keras import Sequential\nfrom keras.layers import Input, Dense, Dropout, Conv2D,AveragePooling2D,Flatten, InputLayer, BatchNormalization,LSTM,Embedding,Concatenate,RepeatVector,Attention\nfrom keras.layers.merge import add\nfrom keras.models import Model\nfrom keras.utils import plot_model\nimport tensorflow as tf\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n","3929b48d":"latent_dim = 512\n","88417766":"\nencoder_inp = Input(shape = (max_en_len,))\nencoder_em = Embedding(vocab_len_en, 200,weights=[embedding_weights],trainable=False)(encoder_inp)\nencoder_lstm = LSTM(latent_dim, return_state=True)\nencoder_out,enc_h,enc_c = encoder_lstm(encoder_em)\n\nencoded_state = [enc_h,enc_c]\n\ndecoder_inp = Input(shape = (None,),dtype='int64')\ndecoder_em_layer = Embedding(vocab_len_ta, 200)\ndecoder_em = decoder_em_layer(decoder_inp)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_out,dec_h,dec_c = decoder_lstm(decoder_em, initial_state  = encoded_state)\ndecoder_dense = Dense(vocab_len_ta, activation = 'softmax')\ndecoder_out = decoder_dense(decoder_out)\n\nmodel = Model([encoder_inp, decoder_inp], decoder_out)\n\n# model.compile(optimizer='rmsprop', loss=sparse_loss,target_tensors=[decoder_target])\n# model.compile(optimizer='Adam', loss=)\n\nprint(model.summary())\n","ebd0701e":"plot_model(model,show_shapes=True)","f5f06b16":"opt = Adam(learning_rate=2*1e-4)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy')\nmcp_save = ModelCheckpoint('\/kaggle\/working\/mdl_wts.hdf5', save_best_only=True, monitor='loss', mode='min')\nearly_stopping = EarlyStopping(monitor='loss',min_delta=0,patience=5,verbose=1, mode='auto',restore_best_weights=True)\n\nbatch_size = 8\nhis = model.fit(generator(batch_size),epochs=10,verbose=1,\n#           steps_per_epoch=len(train_en_t)\/\/batch_size\n          steps_per_epoch=21\n         ,callbacks=[mcp_save])","159f2174":"from matplotlib import pyplot as plt\nplt.plot(his.history['loss'])\n# plt.plot(his.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.savefig('\/kaggle\/working\/Q3_model.png', bbox_inches='tight')\nplt.show()","c375de8f":"encoder_model = Model(encoder_inp,encoded_state)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndec_emb2= decoder_em_layer(decoder_inp) # Get the embeddings of the decoder sequence\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\ndecoder_states2 = [state_h2, state_c2]\ndecoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inp] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)","6c5d7321":"plot_model(decoder_model,show_shapes=True)","1adc05a1":"plot_model(encoder_model,show_shapes=True)","533fa822":"tkzr_ta.sequences_to_texts([[1,2,3],[1,2,3,4,5]])","472921a8":"y_pred_all = list()\nfor (i,en_t) in enumerate(tqdm(test_en_t)):\n    X1_test = np.array(pad_sequences([en_t],max_en_len,padding='post'))\n    curr_state = encoder_model.predict(X1_test)\n    inp_word = np.array([[start_token_t]])\n    y_pred_ix = []\n    while True:\n        pred_word,h,c = decoder_model.predict([inp_word]+curr_state)\n        pred_index = np.argmax(pred_word, axis = -1)[0][0]\n        if(pred_index == end_token_t or len(y_pred_ix)>max_ta_len):\n            break\n        y_pred_ix.append(pred_index)\n        inp_word = np.array([[pred_index]])\n        curr_state = [h,c]\n    y_pred_all.append(y_pred_ix)\ny_pred_all_txt = tkzr_ta.sequences_to_texts(y_pred_all)","f6627bdf":"pd.DataFrame({'x':test_en, 'y_pred':y_pred_all_txt, 'y_actual':test_ta}).to_csv('\/kaggle\/working\/final_test_res.csv')\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndf = pd.DataFrame({'x':test_en, 'y_pred':y_pred_all_txt, 'y_actual':test_ta})\ndf.to_csv('\/kaggle\/working\/test_res_final.csv')\n# df['BLEU'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred']), axis=1)\ndf['BLEUone'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(1, 0, 0, 0)), axis=1)\ndf['BLEUtwo'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.5, 0.5, 0, 0)), axis=1)\ndf['BLEUthr'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.33, 0.33, 0.33, 0)), axis=1)\ndf['BLEUfou'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.25, 0.25, 0.25, 0.25)), axis=1)\n\ndf.to_csv('\/kaggle\/working\/test_res-BLEU.csv')\n\n","5b6df3b9":"from keras import Sequential\nfrom keras.layers import Input, Dense, Dropout, Conv2D,AveragePooling2D,Flatten, InputLayer, BatchNormalization,LSTM,Embedding,Concatenate,RepeatVector,Attention\nfrom keras.layers.merge import add\nfrom keras.models import Model\nfrom keras.utils import plot_model\nimport tensorflow as tf\nfrom keras.losses import sparse_categorical_crossentropy,categorical_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n","e0bf196e":"dec_h = tf.zeros(shape=(2,512))\nenc_h = tf.zeros(shape=(2,46,512))\ndec_h = tf.expand_dims(dec_h, 1)\ncon_v = tf.matmul(enc_h,dec_h,transpose_b=True)\nweights = tf.ones(shape=(2,46,1))\ncon_v*=weights\nprint(con_v.shape)\nsof_score = tf.nn.softmax(con_v)\nprint(sof_score.shape)\n(sof_score*enc_h).shape","eb8b1013":"score = tf.matmul(enc_h, dec_h, transpose_b=True)\nscore*=weights\nprint(score.shape)","6dee787a":"class LuongAttention(tf.keras.layers.Layer):\n    def __init__(self, use_scale=False):\n        super(LuongAttention, self).__init__()\n        self.use_scale = use_scale    \n\n    def build(self,input_shape):\n        if self.use_scale:\n            self.scale = self.add_weight(\n              name='scale',\n              shape=(input_shape[1][-2],1),\n              initializer=tf.keras.initializers.Ones(),\n              dtype=self.dtype,\n              trainable=True)\n        else:\n            self.scale = None\n        \n    def call(self, inputs):\n        query = inputs[0]\n        values = inputs[1]\n        # query hidden state shape == (batch_size, hidden size)\n        # query_with_time_axis shape == (batch_size, 1, hidden size)\n        # values shape == (batch_size, max_len, hidden size)\n        # we are doing this to broadcast addition along the time axis to calculate the score\n        query_with_time_axis = tf.expand_dims(query, 1)\n\n        # score shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n        #     score = self.V(tf.nn.tanh(\n        #         self.W1(query_with_time_axis) + self.W2(values)))\n        score = tf.matmul(values, query_with_time_axis, transpose_b=True)\n        if(self.use_scale):\n            score*=self.scale\n\n\n        # attention_weights shape == (batch_size, max_length, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector","68cbb233":"latent_dim = 256","11e7e612":"encoder_inp = Input(shape = (max_en_len,))\n# initial_state = Input(shape = (latent_dim,))#\nencoder_em = Embedding(vocab_len_en, 200,weights=[embedding_weights],trainable=False)(encoder_inp)\nencoder_lstm = LSTM(latent_dim, return_state=True,return_sequences=True)\nencoder_out,enc_h,enc_c = encoder_lstm(encoder_em)\n#                                        ,initial_state=[initial_state,initial_state])#\n\nencoded_state = [encoder_out,enc_h,enc_c]\n\n# model_enc = Model([encoder_inp,initial_state],encoded_state)\nmodel_enc = Model(encoder_inp,encoded_state)\n\n\ndecoder_inp = Input(shape = (1,),dtype='int64')\ndecoder_em_layer = Embedding(vocab_len_ta, 200)\ndecoder_em = decoder_em_layer(decoder_inp)\n\ndecoder_enc_hidden_outs = Input(shape=(max_en_len,latent_dim,))\ndecoder_dec_hidden_out = Input(shape=(latent_dim,))\ndecoder_dec_h = Input(shape=(latent_dim,))\ndecoder_dec_c = Input(shape=(latent_dim,))\n\ndecoder_attention = LuongAttention(use_scale=True)\ncontext_vector = decoder_attention([decoder_dec_hidden_out,decoder_enc_hidden_outs])\ncontext_vector = RepeatVector(1)(context_vector)\n\ncomb_input = Concatenate(axis=-1)( [context_vector,decoder_em])\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_state_inputs = [decoder_dec_h,decoder_dec_c]\ndecoder_out,dec_h,dec_c = decoder_lstm(comb_input, initial_state  = decoder_state_inputs)\ndecoder_dense = Dense(vocab_len_ta, activation = 'softmax')\ndecoder_out = decoder_dense(decoder_out)\n\n# model = Model([encoder_inp, decoder_inp], decoder_out)\n\n# # model.compile(optimizer='rmsprop', loss=sparse_loss,target_tensors=[decoder_target])\n# # model.compile(optimizer='Adam', loss=)\n\n# print(model.summary())\nmodel_dec = Model([decoder_inp,decoder_enc_hidden_outs,decoder_dec_hidden_out,decoder_dec_h,decoder_dec_c],[decoder_out,dec_h,dec_c])\n\nprint(model_enc.summary())\nprint(model_dec.summary())","5c305924":"plot_model(model_enc,show_shapes=True)","dc71d70f":"plot_model(model_dec,show_shapes=True)","c2c1f021":"start_token_t","8a950f42":"optimizer = Adam()","56c02e02":"@tf.function\ndef train_step(inp_enc, inp_dec,targ, enc_hidden):\n    loss = 0\n    dec_h_out = tf.zeros(shape=(BATCH_SIZE,latent_dim))\n\n    with tf.GradientTape() as tape:\n#         print(\"DE\",inp.shape,enc_hidden)\n        enc_outs,enc_h,enc_c= model_enc(inp_enc, enc_hidden)\n        dec_states = [enc_h,enc_c]\n#         dec_input = tf.expand_dims([start_token_t] * BATCH_SIZE, 1)\n        dec_input = inp_dec[:,0]\n\n        # Teacher forcing - feeding the target as the next input\n        for t in range(1, inp_dec.shape[1]):\n#             print(inp_dec.shape[1],t)\n            # passing enc_output to the decoder\n            predictions, dec_h,dec_c = model_dec([dec_input,enc_outs,dec_h_out]+dec_states)\n            dec_states = [dec_h,dec_c]\n            loss += categorical_crossentropy(targ[:, t:t+1,:], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(inp_dec[:, t], 1)\n#         print(\"ones\")\n        batch_loss = (loss \/ int(inp_dec.shape[1]))\n        variables = model_enc.trainable_variables + model_dec.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        optimizer.apply_gradients(zip(gradients, variables))\n\n        return batch_loss","d8691cf0":"EPOCHS = 10\nBATCH_SIZE=2\nsteps_per_epoch = len(train_en_t)\/\/BATCH_SIZE\n# steps_per_epoch = 5\n\n\nfor epoch in tqdm(range(EPOCHS)):\n\n    enc_hidden = tf.zeros(( BATCH_SIZE, latent_dim))\n    train_machine = generator(BATCH_SIZE)\n    total_loss = 0\n\n    for (batch,_) in enumerate(range(steps_per_epoch)):\n        input_batch,target = next(train_machine)\n        inp_enc,inp_dec = input_batch\n        \n#         print(\"batch\",batch)\n        batch_loss = train_step(inp_enc, inp_dec, target,enc_hidden)\n        total_loss += batch_loss\n\n    if batch % 100 == 0:\n        print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss}\")#format(epoch + 1,\n#                                                    batch,\n#                                                    batch_loss.numpy()))\n\n    print(f'Epoch {epoch+1} Loss {(total_loss.numpy()\/steps_per_epoch)}')\n","c5a92667":"y_pred_all = list()\nfor (i,en_t) in enumerate(tqdm(test_en_t)):\n    X1_test = np.array(pad_sequences([en_t],max_en_len,padding='post'))\n    dec_h_out = tf.zeros(( 1, latent_dim))\n    enc_outs_pred,enc_h_pred,enc_c_pred = model_enc(X1_test, enc_hidden)\n#     curr_state = encoder_model.predict(X1_test)\n    curr_state = [enc_h_pred,enc_c_pred]\n    inp_word = np.array([[start_token_t]])\n    y_pred_ix = []\n    while True:\n        pred_word, dec_h,dec_c = model_dec([inp_word,enc_outs_pred,dec_h_out]+curr_state)\n#         pred_word,h,c = decoder_model.predict([inp_word]+curr_state)\n        pred_index = np.argmax(pred_word, axis = -1)[0][0]\n        if(pred_index == end_token_t or len(y_pred_ix)>max_ta_len):\n            break\n        y_pred_ix.append(pred_index)\n        inp_word = np.array([[pred_index]])\n        curr_state = [dec_h,dec_c]\n    y_pred_all.append(y_pred_ix)\ny_pred_all_txt = tkzr_ta.sequences_to_texts(y_pred_all)\n\n\n","ad974024":"pd.DataFrame({'x':test_en, 'y':y_pred_all_txt, 'y_actual':test_ta}).to_csv('\/kaggle\/working\/final_test_res-attention.csv')\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndf = pd.DataFrame({'x':test_en, 'y_pred':y_pred_all_txt, 'y_actual':test_ta})\ndf.to_csv('\/kaggle\/working\/final_test_res-attention.csv')\n# df['BLEU'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred']), axis=1)\ndf['BLEUone'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(1, 0, 0, 0)), axis=1)\ndf['BLEUtwo'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.5, 0.5, 0, 0)), axis=1)\ndf['BLEUthr'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.33, 0.33, 0.33, 0)), axis=1)\ndf['BLEUfou'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.25, 0.25, 0.25, 0.25)), axis=1)\ndf.to_csv('\/kaggle\/working\/final_test_res-attention-BLEU.csv')\n\n","7e3cc196":"# (5, 46)\n# (5, 28)\n# (5, 28, 19254)\ndef generator_transformer(batch_size=1):\n    num = 0\n    while(True):\n        encoder_input_batch,decoder_input_batch,decoder_output_batch = list(),list(),list()\n        for (i,(en_t,ta_t)) in enumerate(zip(train_en_t,train_ta_t)):\n            encoder_input_batch.append(en_t)\n            decoder_input_batch.append(ta_t)\n            num+=1\n            if(num==batch_size):\n                encoder_input_batch = pad_sequences(encoder_input_batch,max_en_len,padding='post')\n                decoder_input_batch = pad_sequences(decoder_input_batch,max_ta_len,padding='post')\n                for ta_item in decoder_input_batch:\n                    decoder_output_batch.append(to_categorical(ta_item,num_classes=vocab_len_ta))\n                yield([encoder_input_batch,decoder_input_batch[:,:-1]],np.asarray(decoder_output_batch)[:,1:,:])\n                encoder_input_batch,decoder_input_batch,decoder_output_batch = list(),list(),list()\n                num=0","b4920615":"gen = generator_transformer()\nt5 = next(gen)\nprint(t5[0][0].shape)\nprint(t5[0][1].shape)\nprint(t5[1].shape)","d4c24fa6":"from keras import Sequential\nfrom keras.layers import Add,Input,Lambda, Dense, Dropout, Conv2D,AveragePooling2D,Flatten, InputLayer, BatchNormalization,LSTM,Embedding,Concatenate,RepeatVector,Attention,LayerNormalization\nfrom keras.layers.merge import add\nfrom keras.models import Model\nfrom keras.utils import plot_model\nimport tensorflow as tf\nfrom keras.losses import sparse_categorical_crossentropy,categorical_crossentropy\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam","4c93c168":"import sys\nimport os\nsys.path.append(os.path.abspath(\"\/kaggle\/input\/kerastransformerfour\/\"))\n# import kaggle.input.keras-transformer.keras_transformer\n# __import__(\"\/kaggle\/input\/keras-transformer\/keras_transformer\")\nfrom keras_transformer.attention import MultiHeadAttention, MultiHeadSelfAttention\nfrom keras_transformer.transformer import TransformerTransition, TransformerBlock\nfrom keras_transformer.position import AddPositionalEncoding\nimport keras_transformer","1184e606":"num_layers = 2\nnum_heads = 4\ndroput_rate=0\n\ntrans_enc_inp = Input(shape=(max_en_len,))\ntrans_enc_em = Embedding(vocab_len_en, 200,weights=[embedding_weights],trainable=False)(trans_enc_inp)\nenc_output = AddPositionalEncoding()(trans_enc_em)\n\n\n# output = trans_enc_pos_inp\nfor _ in range(num_layers):\n    mhsa_out = MultiHeadSelfAttention(num_heads,use_masking=True)(enc_output)\n    add_out = Add()([enc_output,Dropout(droput_rate)(mhsa_out)])\n    norm_out = LayerNormalization()(add_out)\n    transition_out = TransformerTransition(activation='relu')(norm_out)\n    add_out2 = Add()([norm_out,Dropout(droput_rate)(transition_out)])\n    norm_out2 = LayerNormalization()(add_out2)\n    enc_output = LayerNormalization()(norm_out2)\n    \n# EncoderStack = Model(trans_enc_inp,enc_output)\n\ntrans_dec_inp = Input(shape = (max_ta_len-1,),dtype='int64')\ntrans_dec_em = Embedding(vocab_len_ta, 200)(trans_dec_inp)\ndec_output = AddPositionalEncoding()(trans_dec_em)\n\nfor i in range(num_layers):\n    mhsa_dec_out = MultiHeadSelfAttention(num_heads, use_masking = True)(dec_output)\n    add_out_dec = Add()([dec_output,Dropout(droput_rate)(mhsa_dec_out)])\n    norm_out_dec = LayerNormalization()(add_out_dec)\n    corss_att_inp = [enc_output, norm_out_dec]\n    mha_dec_out = MultiHeadAttention(num_heads, use_masking = False)(corss_att_inp)\n    add_out2_dec = Add()([norm_out_dec,Dropout(droput_rate)(mha_dec_out)])\n    norm_out2_dec = LayerNormalization()(add_out2_dec)\n    transition_out_dec = TransformerTransition(activation = 'relu')(mha_dec_out)\n    add_out3_dec = Add()([norm_out2_dec,Dropout(droput_rate)(transition_out_dec)])\n    norm_out3_dec = LayerNormalization()(add_out3_dec)\n    dec_output = LayerNormalization()(norm_out3_dec)\n\n# DecoderStack = Model([input_dec,input_enc],dec_output)\n\noutput = Dense(vocab_len_ta,activation='softmax')(dec_output)\n\n# trans_enc_model = Model(Encoder_inputs,layer_out)\n# trans_enc_model = Model(trans_enc_inp,enc_output)\n# plot_model(trans_enc_model,show_shapes=True)\n\ntransformer_model = Model([trans_enc_inp,trans_dec_inp],output)\nplot_model(transformer_model,show_shapes=True)\n","dbbf1caf":"opt = Adam(learning_rate=9*1e-5)\ntransformer_model.compile(optimizer=opt, loss='categorical_crossentropy')\nmcp_save = ModelCheckpoint('\/kaggle\/working\/mdl_wts-trans.hdf5', save_best_only=True, monitor='loss', mode='min')\nearly_stopping = EarlyStopping(monitor='loss',min_delta=0,patience=5,verbose=1, mode='auto',restore_best_weights=True)\n\nbatch_size = 8\n# his = transformer_model.fit(generator_transformer(batch_size),epochs=10,verbose=1,\n#           steps_per_epoch=len(train_en_t)\/\/batch_size\n# #           steps_per_epoch=21\n#          ,callbacks=[mcp_save]\n#          )","3270247b":"from matplotlib import pyplot as plt\nplt.plot(his.history['loss'])\n# plt.plot(his.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.savefig('\/kaggle\/working\/Q5_model.png', bbox_inches='tight')\nplt.show()","33cb2bf4":"[start_token_t]","26b1cff7":"max_ta_len","c6cc7da9":"y_pred_all = list()\nfor (i,en_t) in enumerate(tqdm(test_en_t)):\n    X1_test = np.array(pad_sequences([en_t],max_en_len,padding='post'))\n    inp_word = np.array([start_token_t])\n    y_pred_ix = []\n    i = 0\n    try:\n        while True:\n#             print(X1_test.shape,i,inp_word,y_pred_ix)\n            pred_words = transformer_model.predict([X1_test,\n                                      pad_sequences([inp_word],max_ta_len-1,padding='post')])\n            pred_index = np.argmax(pred_words, axis = -1)[0][i]\n    #         print(pred_index)\n            if(pred_index == end_token_t or i>=max_ta_len-2):\n                break\n            y_pred_ix.append(pred_index)\n            inp_word = np.append(inp_word,pred_index)\n            i+=1\n    #     print(tkzr_ta.sequences_to_texts([y_pred_ix]))\n        y_pred_all.append(y_pred_ix)\n    except:\n        y_pred_all.append([])\n        print(\"skipped\",tkzr_en.sequences_to_texts([en_t]))\ny_pred_all_txt = tkzr_ta.sequences_to_texts(y_pred_all)","9f532ad8":"from nltk.translate.bleu_score import sentence_bleu\n\ndf = pd.DataFrame({'x':test_en, 'y_pred':y_pred_all_txt, 'y_actual':test_ta})\ndf.to_csv('\/kaggle\/working\/final_test_res-transformer.csv')\ndf['BLEUone'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(1, 0, 0, 0)), axis=1)\ndf['BLEUtwo'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.5, 0.5, 0, 0)), axis=1)\ndf['BLEUthr'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.33, 0.33, 0.33, 0)), axis=1)\ndf['BLEUfou'] = df.apply(lambda row: sentence_bleu(row['y_actual'],row['y_pred'],weights=(0.25, 0.25, 0.25, 0.25)), axis=1)\ndf.to_csv('\/kaggle\/working\/final_test_res-transformer-BLEU.csv')\n\n","e32174d0":"## Model","dcf17e2b":"## Inference","27d79d2f":"# Question 5\n","3c854f22":"# Question 3","399a0bc2":"## Ineference","6987d969":"# Question 4"}}