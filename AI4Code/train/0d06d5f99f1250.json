{"cell_type":{"fddae8ea":"code","8fa31297":"code","5aac1a87":"code","06b0a1a7":"code","15b1aaa2":"code","216a379e":"code","3b66d8d6":"code","89a4b53f":"code","36ec0607":"code","fbc8402f":"code","1193d9c8":"code","0fa69707":"code","48171f2e":"code","c5c3da2f":"code","37c0a198":"code","e576a215":"code","eec5f460":"code","50fec3be":"code","5a61169c":"code","d8ea6428":"code","dc0cf59a":"code","d8974ac6":"code","ba9fecb8":"code","87727856":"code","6145e736":"code","70282c21":"code","35e40dd2":"code","66506f94":"code","c4025b1a":"code","4459c0c3":"code","fa114b44":"code","f02ff53b":"code","6bb2c6aa":"code","e4a719dd":"code","ddf2778b":"code","5d9f73ab":"code","cc36d617":"code","80e6b812":"code","af5855ed":"code","08248f4b":"code","06ed6360":"code","61d122db":"markdown","30b855c3":"markdown","90529c79":"markdown","71583c1c":"markdown","52862e3c":"markdown","cd2f22f8":"markdown","76743561":"markdown","8574f091":"markdown","296ea0a0":"markdown","eb40a2ef":"markdown","253c33f3":"markdown","c93d6f31":"markdown","b7c2d8bc":"markdown","2b8040f3":"markdown","2b3517bc":"markdown","72309c0d":"markdown","6c7ed7ce":"markdown","3ce7e1a9":"markdown","6507416b":"markdown","cc550fa1":"markdown","a2f1c3d4":"markdown","eb056fc9":"markdown"},"source":{"fddae8ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fa31297":"! pip install seaborn==0.11.0","5aac1a87":"import numpy as np \n\nimport pandas as pd \npd.set_option('display.max_columns', None)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(style=\"darkgrid\")\nsns.set_palette(palette = 'pastel')\n# color palette for seaborn\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.simplefilter(action='ignore')","06b0a1a7":"sns.__version__","15b1aaa2":"data = pd.read_csv('\/kaggle\/input\/mental-health-in-tech-survey\/survey.csv')\ndata.head(2)","216a379e":"null_values = data.isnull().sum().sort_values(ascending=False).to_frame()\nnull_values = null_values.loc[null_values[0] != 0]\nax = sns.barplot(x=null_values.index, y=null_values[0], data=null_values,  palette=\"ch:.25\")\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nax = ax.set(ylabel=\"count\", title='Parameters with NULL-values')","3b66d8d6":"treatment_values = data.treatment.value_counts().to_frame()\nfamily_history_values = data.family_history.value_counts().to_frame()\nplot_frame = pd.DataFrame({'answers': ['No', 'Yes'], 'treatment': treatment_values['treatment'], 'family_history': family_history_values['family_history']})\nplot_frame = pd.melt(plot_frame, id_vars='answers', var_name=\"treatment\", value_name=\"family_history\")\nsns.factorplot(x='treatment', y='family_history', hue='answers', data=plot_frame, kind='bar',  palette=\"ch:.25\").set(ylabel='count', xlabel='', title='Answers count')\n\ntreatment_values_yes = data.treatment.loc[data.family_history == 'Yes'].value_counts().to_frame()\ntreatment_values_no = data.treatment.loc[data.family_history == 'No'].value_counts().to_frame()\nplot_frame = pd.DataFrame({'family_history_yes': treatment_values_yes['treatment'], 'family_history_no': treatment_values_no['treatment']})\nplot_frame.index.name = 'answers'\nplot_frame.reset_index(inplace=True)\nplot_frame = pd.melt(plot_frame, id_vars='answers', var_name=\"family_history_yes\", value_name=\"family_history_no\")\nsns.factorplot(x='family_history_yes', y='family_history_no', hue='answers', data=plot_frame, kind='bar',  palette=\"ch:.25\").set(ylabel='count', xlabel='', title='Treatment answers count')\n\nwork_interfere_yes = data.work_interfere.loc[data.treatment == 'Yes'].value_counts().to_frame()\nwork_interfere_no = data.work_interfere.loc[data.treatment == 'No'].value_counts().to_frame()\nplot_frame = pd.DataFrame({'treatment_yes': work_interfere_yes['work_interfere'], 'treatment_no': work_interfere_no['work_interfere']})\nplot_frame.index.name = 'answers'\nplot_frame.reset_index(inplace=True)\nplot_frame = pd.melt(plot_frame, id_vars='answers', var_name=\"treatment_yes\", value_name=\"treatment_no\")\nsns.factorplot(x='treatment_yes', y='treatment_no', hue='answers', data=plot_frame, kind='bar',  palette=\"ch:.25\").set(ylabel='count', xlabel='', title='Work interfere answers count')\n\nwork_interfere_yes = data.work_interfere.loc[data.family_history == 'Yes'].value_counts().to_frame()\nwork_interfere_no = data.work_interfere.loc[data.family_history == 'No'].value_counts().to_frame()\nplot_frame = pd.DataFrame({'family_history_yes': work_interfere_yes['work_interfere'], 'family_history_no': work_interfere_no['work_interfere']})\nplot_frame.index.name = 'answers'\nplot_frame.reset_index(inplace=True)\nplot_frame = pd.melt(plot_frame, id_vars='answers', var_name=\"family_history_yes\", value_name=\"family_history_no\")\nsns.factorplot(x='family_history_yes', y='family_history_no', hue='answers', data=plot_frame, kind='bar',  palette=\"ch:.25\").set(ylabel='count', xlabel='', title='Work interfere answers count')","89a4b53f":"gender_values = data.Gender.value_counts().sort_values(ascending=False).to_frame()\ngender_values = gender_values.rename(columns={'Gender': 'count'})\ntable_gender = gender_values.style.background_gradient(cmap=cmap)\ntable_gender","36ec0607":"# clean \"Gender\" column\n\ndata.Gender = data.Gender.str.lower()\nmale = [\"male\", \"m\", \"male-ish\", \"maile\", \"mal\", \"male (cis)\", \"make\", \"male \", \"man\",\"msle\", \"mail\", \"malr\",\"cis man\", \"cis male\"]\nfemale = [\"cis female\", \"f\", \"female\", \"woman\",  \"femake\", \"female \",\"cis-female\/femme\", \"female (cis)\", \"femail\"]\nother = [\"trans-female\", \"something kinda male?\", \"queer\/she\/they\", \"non-binary\",\"nah\", \"all\", \"enby\", \"fluid\", \n         \"genderqueer\", \"androgyne\", \"agender\", \"male leaning androgynous\", \"guy (-ish) ^_^\", \"trans woman\", \"neuter\", \n         \"female (trans)\", \"queer\", \"ostensibly male, unsure what that really means\", \"p\", \"a little about you\"]\n\ndata.Gender.loc[data.Gender.isin(male)] = 'male'\ndata.Gender.loc[data.Gender.isin(female)] = 'female'\ndata.Gender.loc[data.Gender.isin(other)] = 'others'\n\ngender_values = data.Gender.value_counts().sort_values(ascending=False).to_frame()\ngender_values = gender_values.rename(columns={'Gender': 'count'})\ntable_gender = gender_values.style.background_gradient(cmap=cmap)\ntable_gender","fbc8402f":"from sklearn.ensemble import IsolationForest\n\n\ndef winsorization_outliers(df):\n    out=[]\n    for i in df:\n        q1 = np.percentile(df , 1)\n        q3 = np.percentile(df , 99)\n        if i > q3 or i < q1:\n            out.append(i)\n    print(\"Outliers:\",out)\n    return out\n    \noutliers = winsorization_outliers(data.Age)","1193d9c8":"# drop age-outliers\n\ndata_age = data.loc[~data.Age.isin(outliers)]\nsns.histplot(data=data_age, x=\"Age\")","0fa69707":"import plotly.express as px\n\nfig = px.violin(data_age, y=\"Age\", x=\"treatment\", color=\"Gender\", box=True, points=\"all\")\nfig.show()","48171f2e":"data_outliers = data.loc[data.Age.isin(outliers)]\n\ntreatment_female = data_outliers.treatment.loc[data_outliers.Gender == 'female'].value_counts().to_frame()\ntreatment_male = data_outliers.treatment.loc[data_outliers.Gender == 'male'].value_counts().to_frame()\n\nplot_frame = pd.DataFrame({'treatment_female': treatment_female['treatment'], 'treatment_male': treatment_male['treatment']})\n\nplot_frame.index.name = 'answers'\nplot_frame.reset_index(inplace=True)\nplot_frame = pd.melt(plot_frame, id_vars='answers', var_name=\"treatment_female\", value_name=\"treatment_male\")\nsns.factorplot(x='treatment_female', y='treatment_male', hue='answers', data=plot_frame, kind='bar',  palette=\"ch:.25\").set(ylabel='count', xlabel='', title='Outliers treatment for male\/female')","c5c3da2f":"country_count = data.Country.value_counts().sort_values(ascending=False).to_frame()[:10]\ncountry_count = country_count.rename(columns={'Country': 'count'})\nplt.figure(figsize=(15,5))\nax = sns.barplot(x=country_count.index, y='count', data=country_count,  palette=\"ch:.25\")\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nax = ax.set_title('Top 10 countries')","37c0a198":"state_count = data.state.value_counts().sort_values(ascending=False).to_frame()[:10]\nstate_count = state_count.rename(columns={'state': 'count'})\nplt.figure(figsize=(10,10))\nax = sns.barplot(x=state_count.index, y='count', data=state_count,  palette=\"ch:.25\")\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nax = ax.set_title('Top 10 States')","e576a215":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=data.Country.loc[data.treatment == 'Yes'].value_counts().index.to_list()[:10], values = data.Country.loc[data.treatment == 'Yes'].value_counts()[:10], name=\"Treatment -Yes\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=data.Country.loc[data.treatment == 'No'].value_counts().index.to_list()[:10], values = data.Country.loc[data.treatment == 'No'].value_counts()[:10], name=\"Treatment - No\"),\n              1, 2)\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Country and trearment\",\n   \n    annotations=[dict(text='Yes', x=0.19, y=0.5, font_size=20, showarrow=False),\n                 dict(text='No', x=0.78, y=0.5, font_size=20, showarrow=False)])\nfig.show()\n\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=data.state.loc[data.treatment == 'Yes'].value_counts().index.to_list()[:10], values = data.state.loc[data.treatment == 'Yes'].value_counts()[:10], name=\"Treatment -Yes\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=data.state.loc[data.treatment == 'No'].value_counts().index.to_list()[:10], values = data.state.loc[data.treatment == 'No'].value_counts()[:10], name=\"Treatment - No\"),\n              1, 2)\n\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"State and trearment\",\n   \n    annotations=[dict(text='Yes', x=0.19, y=0.5, font_size=20, showarrow=False),\n                 dict(text='No', x=0.78, y=0.5, font_size=20, showarrow=False)])\nfig.show()","eec5f460":"from category_encoders.ordinal import OrdinalEncoder\n\ndata_encoding = data\nencoder = OrdinalEncoder()\ndata_encoding = encoder.fit_transform(data.drop(['Timestamp', 'comments', 'Age'], axis=1))","50fec3be":"# new DataFrame\ndata_encoding['Age'] = data.Age\ndata_encoding.head()","5a61169c":"corr = data_encoding.corr(method ='spearman')\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(15, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","d8ea6428":"f, ax = plt.subplots(figsize=(25, 1))\ntreatment = corr.sort_values(by=['treatment'], ascending=False).head(1).T\ntreatment = treatment.sort_values(by=['treatment'],ascending=False).T\nsns.heatmap(treatment, cmap=cmap, annot=True)\nplt.show()","dc0cf59a":"ten_corr_features = ['family_history', 'work_interfere', 'care_options', 'obs_consequence', 'benefits', 'mental_health_consequence', \n                     'Country', 'anonymity', 'mental_health_interview', 'leave']","d8974ac6":"print('Top 10 features: \\n', ten_corr_features)","ba9fecb8":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures_norm = MinMaxScaler().fit_transform(data_encoding.drop(['treatment'], axis=1))\nchi_selector = SelectKBest(chi2, k=10)\nchi_selector.fit(features_norm, data_encoding.treatment)\n\nchi_support = chi_selector.get_support()\nchi_feature = data_encoding.drop(['treatment'], axis=1).loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","87727856":"print('Top-10 features: \\n', chi_feature)","6145e736":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=10, verbose=5)\nrfe_selector.fit(features_norm ,data_encoding.treatment)","70282c21":"rfe_support = rfe_selector.get_support()\nrfe_features = data_encoding.drop(['treatment'], axis=1).loc[:,rfe_support].columns.tolist()\nprint('Top-10 features: \\n', rfe_features)","35e40dd2":"! pip install git+https:\/\/github.com\/KamitaniLab\/smlr.git","66506f94":"import smlr\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data_encoding.drop(['treatment'], axis=1), data_encoding.treatment, test_size=0.33, random_state=42)\n\nmodel = smlr.SMLR(max_iter=1000, tol=1e-8, verbose=5)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)","c4025b1a":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, predictions))","4459c0c3":"features_weight = model.coef_[0]\nfeatures_index = [id for id, weight in enumerate(features_weight) if weight!=0]\nsmlr_features = data_encoding.drop(['treatment'], axis=1).iloc[:, features_index].columns.to_list()\nprint('Top SLMR features: \\n', smlr_features)","fa114b44":"! pip install ReliefF","f02ff53b":"from ReliefF import ReliefF\n\nfs = ReliefF(n_neighbors=1, n_features_to_keep=10)\nfeatures = fs.fit_transform(data_encoding.drop(['treatment'], axis=1).to_numpy(), data_encoding['treatment'].to_numpy()).T","6bb2c6aa":"columns = data_encoding.drop(['treatment'], axis=1).columns.to_list()\nreliefF_features = []\nfor feature in features:\n    for column in columns:\n        idx = (data_encoding.drop(['treatment'], axis=1)[column] == feature)\n        if idx.all() == True:\n            reliefF_features.append(column)\n\nprint('Top-10 features: \\n', reliefF_features)","e4a719dd":"feature_names = pd.DataFrame({'features': columns})\nfeature_names = feature_names.set_index('features')\nfeature_names[['correlation', 'chi_2', 'rfe', 'smlr', 'reliefF']] = np.nan","ddf2778b":"def is_important_feature(row, features):\n    names = []\n    for index in row.index.to_list():\n        if index in features:\n            names.append(index)\n        else:\n            names.append(np.NaN)\n    return names","5d9f73ab":"feature_names.correlation = feature_names.apply(lambda row: is_important_feature(row, ten_corr_features))\nfeature_names.chi_2 = feature_names.apply(lambda row: is_important_feature(row, chi_feature))\nfeature_names.rfe = feature_names.apply(lambda row: is_important_feature(row, rfe_features))\nfeature_names.smlr = feature_names.apply(lambda row: is_important_feature(row, smlr_features))\nfeature_names.reliefF = feature_names.apply(lambda row: is_important_feature(row, reliefF_features))","cc36d617":"feature_names","80e6b812":"feature_names['counts'] = 5 - feature_names.isnull().sum(axis=1)","af5855ed":"plt.figure(figsize=(20,5))\nfeatures_counts = feature_names['counts'].sort_values(ascending=False).to_frame()\nax = sns.barplot(x=features_counts.index, y='counts', data=features_counts,  palette=\"ch:.25\")\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nax = ax.set_xticklabels(features_counts.index, rotation = 45, ha=\"right\")","08248f4b":"import plotly.express as px\n\n\nfig = px.parallel_categories(data[['treatment', 'obs_consequence', 'family_history',  \n                                   'work_interfere']])\nfig.show()","06ed6360":"fig = px.parallel_categories(data[['treatment', 'care_options', \n                                   'anonymity', 'Gender', 'benefits', 'mental_health_consequence']])\nfig.show()","61d122db":"Explore important features with count = 4:","30b855c3":"## Country, State and Treatment ","90529c79":"# EDA \nWe have 27 columns (features) and 1259 rows (values - candidates, who answered the questions):","71583c1c":"\n**Feature Selection:**\nTrying different approaches and comparing results:\n\nCorrelation;\nChi-2;\nRFE;\nSMLR;\nReliefF.\n\n**Correlation**\nCorrelation is a simple approach to find linear dependence between features.\n\nIf correlation == |1| - two features have linear dependence;\nIf correlation == 0 - two features have not linear dependence.","52862e3c":"On the bar plot we can see the most important features. They are:\n\nobs_consequence, wich means having heard of or observed negative consequences for coworkers with mental health conditions in workplace;\nfamily history, which means family history of mental illness;\nwork_interfere: \"Is mental health condition affects work?;\ncare options, which means providing options for mental health care by the employer;\nanonymity, which means protecting anonymity if you choose to take advantage of mental health or substance abuse treatment resources;\ncountry;\ngender;\nbenefits, which means providing mental health benefits by the employer;\nmental_health_consequence, which means having negative consequences caused by discussing a mental health issue with your employer.\n\nExplore important features with count = 5:","cd2f22f8":"# Chi-2\nChecks if there is a significant difference between the observed and expected frequencies of two categorical numbers. Thus, the null hypothesis that there is no relationship between the variables is tested:\n\n<center> $X^{2}=\\frac{(\\textrm{Observed frequency} - \\textrm{Expected frequency})^2}{\\textrm{Expected frequency}}$ <\/center>\n\nI want to use top-10 features:","76743561":"Have we got any NULL-values? Let's plot them:","8574f091":"**\"treatment\" analysis from the age-gender point:**","296ea0a0":"# Country, State\n**Exploring country and state columns**","eb40a2ef":"**Checking age values:**\n\nFor the first, let's find outliers with Percentile Capping (Winsorization) method:\n\nIf a value exceeds the value of the 99th percentile and below the 1st percentile of given values are treated as outliers.","253c33f3":"**Relief**\nThis method samples randomly instances from the dataset and updates the relevance of each feature based on the difference between the selected instance and the two nearest instances of the same and opposite classes. If a feature difference is observed in the neighboring instances of the same class ( a \u2018hit\u2019), the feature score decreases, alternatively if the feature value difference is observed with a different score (a \u2018miss\u2019) then the feature score increases.\n\n![](https:\/\/miro.medium.com\/max\/472\/1*EjkXuRcsUqQyQ-sw8uCM8w.png)\n\nThe extended algorithm, ReliefF applies feature weighting and searches for more nearest neighbors.","c93d6f31":"* In both cases, the median man age is higher than female;\n* If treatment was, the max men age is higher than women, and if treatment was not, the max women age is higher than men;\n* Women and others have more cases in treatment-yes category.","b7c2d8bc":"\n<table>\n<thead>\n<tr><th>Feature name<\/th><th>Description<\/th><\/tr>\n<\/thead>\n<tbody>\n<tr><td>Timestamp<\/td><td> - <\/td><\/tr>\n<tr><td>Age<\/td><td> - <\/td><\/tr>\n<tr><td>Gender<\/td><td> - <\/td><\/tr>\n<tr><td>Country<\/td><td> - <\/td><\/tr>\n<tr><td>State<\/td><td> (only for US) <\/td><\/tr>\n<tr><td>Self employed<\/td><td> Are you self-employed? <\/td><\/tr>\n<tr><td>Family history<\/td><td> Family history of mental illness <\/td><\/tr>\n<tr><td>Treatment<\/td><td>Is treatment for a mental health condition was?<\/td><\/tr>\n<tr><td>Work interfere<\/td><td> Is mental health condition affects work? <\/td><\/tr>\n<tr><td>No employees<\/td><td> The number of employees in your company or organization <\/td><\/tr>\n<tr><td>Remote work<\/td><td> Having remote work (outside of an office) at least 50% of the time <\/td><\/tr>\n<tr><td>Tech company<\/td><td> The employer is primarily a tech company\/organization <\/td><\/tr>\n<tr><td>Benefits<\/td><td> Providing mental health benefits by the employer <\/td><\/tr>\n<tr><td>Care options:<\/td><td> Providing options for mental health care by the employer <\/td><\/tr>\n<tr><td>Wellness program<\/td><td> Discussion about mental health as part of an employee wellness program by the employes <\/td><\/tr>\n<tr><td>Seek help<\/td><td> Providing resources by the employer to learn more about mental health issues and how to seek help <\/td><\/tr>\n<tr><td>Anonymity<\/td><td> Protecting anonymity if you choose to take advantage of mental health or substance abuse treatment resources<\/td><\/tr>\n<tr><td>Leave<\/td><td> How easy is it for you to take medical leave for a mental health condition? <\/td><\/tr>\n<tr><td>Mental-health consequence: <\/td><td>  Having negative consequences caused by discussing a mental health issue with your employer<\/td><\/tr>\n<tr><td>Phys-health consequence<\/td><td> Having negative consequences caused by discussing a physical health issue with your employer <\/td><\/tr>\n<tr><td>Coworkers<\/td><td> Would you be willing to discuss a mental health issue with your coworkers?<\/td><\/tr>\n<tr><td>Supervisor<\/td><td> Would you be willing to discuss a mental health issue with your direct supervisor(s)? <\/td>\n<\/tr>\n<tr><td>Mental health interview:<\/td><td> Would you bring up a mental health issue with a potential employer in an interview? <\/td><\/tr>\n<tr><td>Phys health interview<\/td><td> Would you bring up a physical health issue with a potential employer in an interview? <\/td><\/tr>\n<tr><td>Mental vs Physical<\/td><td> Do you feel that your employer takes mental health as seriously as physical health? <\/td><\/tr>\n<tr><td>Obs consequence<\/td><td> Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace? <\/td><\/tr>\n<tr><td>Comments<\/td><td> Any additional notes or comments <\/td><\/tr>\n<\/tbody>\n<\/table>","2b8040f3":"At first, let's exploring mental health in the tech industry in 2014. Then, comparing results with data from 2016 and adding data from word-happiness in 2016.\n\n# **Exploring mental health in the tech industry in 2014**\n\n**Some information about explored data:**\n\n\"This dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace\".\nFeatures:","2b3517bc":"For most common countries:","72309c0d":"**SLMR(Sparse Logistic Regression MULTINOMIAL, sparse multinomial logistic regression):**\n\nThis algorithm implements the l1-regularization using ARD (Automatic relevance determination, automatic determination of relevance ) in the classical multinomial logistic regression. Regularization determines the importance of each feature and nullifies those that are useless for forecasting.","6c7ed7ce":"**Encoding features**\n\nAll of the questions have categorical answers.\n\nIn the future, we could use categorical encoding for them. I use OrdinalEncoder for all columns with the exception of 'Timestamp', 'comments', 'Age'.","3ce7e1a9":"# **Age, Gender and Treatment**\n\nAt the first, we explore the \"Gender\" column;\nAt the second, we explore the \"Age\" column;\nExploring \"treatment\" from age-gender point.\n\n**Gender unique values:**","6507416b":"# RFE\n\n**RFE (Recursive Feature Removal):**\n\nA greedy search algorithm that selects features by recursively defining ever smaller feature sets. It ranks the features according to the order in which they are removed.","cc550fa1":"High correlation is had between ['wellness_program' and 'seek_help'] and ['state' and 'country'];\nFor our target value (treatment) high correlation have 'care_options', 'work interface', 'benefits'.\n\n**Correlation for 'treatment' column:**","a2f1c3d4":"**Dropping outliers and check distribution:**","eb056fc9":"**Treatment, Family history and Work interfere**\nExplore our \"target\" value - treatment.\n\nCandidates answered 'yes', if treatment is for a mental health condition was, and 'no' if treatment was not."}}