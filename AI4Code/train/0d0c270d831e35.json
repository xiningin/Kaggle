{"cell_type":{"03705368":"code","19b52e64":"code","b1a2846b":"code","c67da56e":"code","e0ed9992":"code","e8548095":"code","b9cfb0c6":"code","7d4c03ea":"code","e36c294d":"code","e0fae7b7":"code","2c1452c4":"code","a50f65a9":"code","bf1c9ae0":"code","af089bfd":"code","9dfc1f1e":"code","90c42645":"code","8fdefa8c":"code","9e7cb57e":"markdown","2e171b9d":"markdown","065659f6":"markdown","4cd70dee":"markdown","bceb1d35":"markdown","68e901e7":"markdown","0efdbd16":"markdown","8cc849ca":"markdown","22aac1bc":"markdown","908f46aa":"markdown"},"source":{"03705368":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# Load the data\ndf = pd.read_csv('\/kaggle\/input\/csgo-round-winner-classification\/csgo_round_snapshots.csv')\n\n# Split X and y\ny = df.round_winner\nX = df.drop(['round_winner'], axis=1)\n\n# Drop columns with grenade info\ncols_grenade = 'grenade'\nX = X.drop(X.columns[X.columns.str.contains(cols_grenade)], axis=1)\n\nprint(f\"Total number of samples: {len(X)}\")\n\nX.head()","19b52e64":"# Print a random snapshot as a sample\nsample_index = 25\nprint(df.iloc[sample_index])","b1a2846b":"plt.figure(figsize=(8,6))\nax = sns.countplot(x=\"map\", hue=\"round_winner\", data=df)\nax.set(title='Round winners on each map')\nplt.show()","c67da56e":"plt.figure(figsize=(8,6))\nax = sns.countplot(x=\"map\", hue=\"bomb_planted\", data=df)\nax.set(title='Maps and bomb planted')\nplt.show()","e0ed9992":"plt.figure(figsize=(8,6))\nax = sns.barplot(x=df['round_winner'].unique(), y=df['round_winner'].value_counts())\nax.set(title='Total wins per side', xlabel='Side', ylabel='Wins')\nplt.show()","e8548095":"# Plot the distribution of health\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12,5))\nsns.distplot(df['ct_health'], bins=10, ax=ax1);\nsns.distplot(df['t_health'], bins=10, ax=ax2);","b9cfb0c6":"# Plot the distribution of money\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12,5))\nsns.distplot(df['ct_money'], bins=10, ax=ax1);\nsns.distplot(df['t_money'], bins=10, ax=ax2);","7d4c03ea":"# Plot the distribution of scores\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(12,5))\nsns.kdeplot(df['ct_score'], shade=True, ax=ax1)\nsns.kdeplot(df['t_score'], shade=True, ax=ax2)","e36c294d":"# Plot the distribution of time left\nplt.figure(figsize=(8,6))\nsns.kdeplot(df['time_left'], shade=True)","e0fae7b7":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import power_transform\n\ndef encode_targets(y):\n    encoder = LabelEncoder()\n    encoder.fit(y)\n    y_encoded = encoder.transform(y)\n    return y_encoded\n\ndef encode_inputs(X, object_cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    X_encoded = pd.DataFrame(ohe.fit_transform(X[object_cols]))\n    X_encoded.columns = ohe.get_feature_names(object_cols)\n    X_encoded.index = X.index\n    return X_encoded\n\ndef yeo_johnson(series):\n    arr = np.array(series).reshape(-1, 1)\n    return power_transform(arr, method='yeo-johnson')\n\n# Use OH encoder to encode predictors\nobject_cols = ['map', 'bomb_planted']\nX_encoded = encode_inputs(X, object_cols)\nnumerical_X = X.drop(object_cols, axis=1)\nX = pd.concat([numerical_X, X_encoded], axis=1)\n\n# Use label encoder to encode targets\ny = encode_targets(y)\n\n# Make data more Gaussian-like\ncols = ['time_left', 'ct_money', 't_money', 'ct_health',\n 't_health', 'ct_armor', 't_armor', 'ct_helmets', 't_helmets',\n  'ct_defuse_kits', 'ct_players_alive', 't_players_alive']\nfor col in cols:\n    X[col] = yeo_johnson(X[col])","2c1452c4":"from sklearn.model_selection import train_test_split\nfrom tensorflow import keras\n\n# Make a train, validation and test set\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y,\n stratify=y, test_size=0.1, random_state=0)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full,\n stratify=y_train_full, test_size=0.25, random_state=0)\n\n# Set model parameters\nn_layers = 4\nn_nodes = 300\nregularized = False\ndropout = True\nepochs = 50\n\n# Make a Keras DNN model\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.BatchNormalization())\nfor n in range(n_layers):\n    if regularized:\n        model.add(keras.layers.Dense(n_nodes, kernel_initializer=\"he_normal\",\n         kernel_regularizer=keras.regularizers.l1(0.01), use_bias=False))\n    else:\n        model.add(keras.layers.Dense(n_nodes,\n         kernel_initializer=\"he_normal\", use_bias=False))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation(\"elu\"))\n    if dropout:\n        model.add(keras.layers.Dropout(rate=0.2))\nmodel.add(keras.layers.Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n\n# Make a callback that reduces LR on plateau\nreduce_lr_cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                                                 patience=5, min_lr=0.001)\n\n# Make a callback for early stopping\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n\n# Train DNN.\nhistory = model.fit(np.array(X_train), np.array(y_train), epochs=epochs,\n     validation_data=(np.array(X_valid), np.array(y_valid)),\n      callbacks=[reduce_lr_cb, early_stopping_cb], batch_size=128)","a50f65a9":"model.summary()","bf1c9ae0":"# Evaluate the test set\nmodel.evaluate(X_test, y_test)","af089bfd":"# Plot the loss curves for training and validation.\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values)+1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9dfc1f1e":"# Plot the accuracy curves for training and validation.\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, len(acc_values)+1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, acc_values, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","90c42645":"# Predict the winning teams for ten rounds.\nX_new = X_test[:10]\ny_pred = model.predict_classes(X_new)\nclass_names = ['CT', 'T']\nnp.array(class_names)[y_pred]","8fdefa8c":"# Show the predicated probabilities. Below 0.5 predicts CT, otherwise T.\ny_proba = model.predict(X_new)\ny_proba.round(2)","9e7cb57e":"# Feature encoding\n\nFor feature encoding, we'll perform the usual tricks: Label encode the targets and onehot encode the predictors. In this case, the targets are the round winners and the predictors are the map and the bomb_planted feature. Encoding the former will give us seven additional columns, and the latter will add just two. We apply a yeo_johnson transform to the features, that resemble a skewed Gaussian.","2e171b9d":"Lastly we'll predict the round winner from 10 samples without knowing the winner in advance.","065659f6":"Interestingly, a majority of the snapshots are taken at the start of the round, and slowly fades as we apporach the end of it. This makes sense, since the number of players is narrowed down during a round and only a few games lasts the full duration.","4cd70dee":"The loss and accuracy plots are below. Luckily, they show promise with a diminishing loss and an increasing accuracy as training progresses. There's a lot more potential here though and about ~90% validation accuracy is not out reach. It just takes a lot of training and a suiteable batch size.","bceb1d35":"We get over 80% accuracy with just 50 epochs, which is pretty cool. Early stopping did not come into play, since the validation loss kept improving. Also note there's no overfitting to speak of after enabling dropout. After training, we can print a summary of the model. The notice the batch normalization layers before the activation function. The first layer has the input layer and simply outputs the number of features in the data, here 92.","68e901e7":"# EDA\n\nPlots mostly speak for themselves and are fairly straight-forward. We just visualize some of the data to get a better understanding and spot potential skewness in the data. Plots provide a intuitive basis undertanding of the problem domain and how we should deal with the data.","0efdbd16":"We'll continue with some distribution plots. Note that data look fairly skewed Gaussian, which tells us that a power-transform like yeo-johnson or box-cox would be appropriate. Generally, ML models prefer and assume that the distribution is in the fact Gaussian, so adjusting data is way is a sound pre-process technique.","8cc849ca":"# Evaluation\n\nTo evaluate a DNN, we'll look at the loss and accuracy scores to see how well training's progressed and check if there's any underfit\/overfit. To properly evaluate the model, we'll bring in the yet unseen test set. Afterwards we'll make a few round winner predictions based on the test data. The accuracy for the test set is 80%.","22aac1bc":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F4895752%2Fbf6d1be3b18ade8bd780840fd8f871c1%2FU78nzyG.jpg?generation=1597522833435019&alt=media)\n\n# Introduction\n\nCS:GO is a tactical shooter, where two teams (CT and Terrorist) play for a best of 30 rounds, with each round being 1 minute and 55 seconds. There are 5 players on each team (10 in total) and the first team to reach 16 rounds wins the game. At the start, one team plays as CT and the other as Terrorist. After 15 rounds played, the teams swap side. There are 7 different maps a game can be played on. You win a round as Terrorist by either planting the bomb and making sure it explodes, or by eliminating the other team. You win a round as CT by either eliminating the other team, or by disarming the bomb, should it have been planted.\n\nThe data set consists of ~700 demos from high level tournament play in 2019 and 2020. The total number of snapshots is 122411. This notebook uses a Keres DNN with a Tensorflow backend to predict round winners, either Counter-Terrorist or Terrorist.\n\nTensorFlow is the premier open-source deep learning framework developed and maintained by Google. Although using TensorFlow directly can be challenging, the modern tf.keras API beings the simplicity and ease of use of Keras to the TensorFlow project. Using tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n\nKey takeways:\n\n* Keras can be used to make DNN's that fit the problem well.\n* Model tuning requires a lot of experimentation, but I'll argue my choices.\n* The data requires many epochs for a DNN to learn. I ended at about 250 for an optimal solution.","908f46aa":"# Keras DNN model\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*ytBUCmhkAucJ5imsNfAyfQ.png)\n\nKeras is one of the most popular deep learning libraries in Python for research and development because of its simplicity and ease of use. It uses the Tensorflow backend to build both shallow and deep models without much hazzle. Since there's a lot of data available here, my belief was that neural network were suiteable. We'll go through why the settings and hyperparameters are set the way they are. See more at https:\/\/keras.io\/api\/\n\n**Number of layers\/nodes:** This comes down to the data and what works best. I experimented with both shallow and deep nets and found that between 4-8 layers was suiteable with about 128-300 nodes. I figure it's because there's so many samples, so in order to fit them all, we'll need a big network. <br>\n\n**Learning rate:** We'll initialize the optimizer with the default learning rate and use a callback (ReduceLROnPlateau) to slowly reduce the learning rate when we're at at a plateau. You could try to use learning rate scheduling like 1cycle as well. <br>\n\n**Optimizer:** Based on some testing, the top optimizers for this task are Adam, Adamax and Nadam. They are all adaptive momentum based, which is a method that helps accelerate gradients vectors in the right directions, thus leading to faster converging. Nadam is an Adam optimizer plus the Nesterov momentum trick, more here: \nhttps:\/\/keras.io\/api\/optimizers\/Nadam\/ <br>\n\n**Batch size:** Batch size defines the number of samples that will be propagated through the network. Advantages of using a batch size smaller than number of all samples is that it requires less memory and typically networks trains faster with smaller batches. The downside is that the smaller the batch, the less accurate the estimate of the gradient will be. On Kaggle, a batch size of 128 seems OK, since full batch size is very slow in training.<br>\n\n**Activation function:** These are important to enable non-linear representations of the data. A neural network without an activation function is essentially just a linear regression model, so what it does is a non-linear transformation to the input making it capable to learn and perform more complex tasks. I found ELU wtith he_normal initialization to be good here, which looks a lot like the popular RELU, but tends to converge cost to zero faster and produce more accurate results. More here: https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/activation_functions.html <br>\n\n**Number of epochs:** With full batch size, this was quite high (> 200) for a 4 layer neural network. One approrach is to set epochs high and use early stopping on thation set to stop training, whenever the validation loss\/accuracy stops improving (more next). Another is to simply monitor loss\/accuracy curves for both training and validation and find the right epoch before overftting sets in. With Dropout enabled there was almost no overfitting on this dataset. <br>\n\n**Early stopping:** We set a callback to stop training when some metric stops improving. The default is validation loss. Patience=5 means we'll wait five epochs with no improvement after which training will be stopped.\n\n**Batch normalization:** This can make neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. Adding a BN-layer before each activation function we zero-center and normalize each input, then scales and shifts the result using two new parameter vecetors per layer: One for scaling and the other for shifting. This enables the model to learn optimal scale and mean of each layer's input. It's a farily novel technique that also means we'll get away with not scaling our input (like normally you would with StandardScaler). <br>\n\n**Dropout:** I tested Dropout vs L1\/L2 regularzation and found that Dropout worked better here. At every training step, every neuron (including the input neurons but excluding the  output neurons) has a probability of being temporarily \u201cdropped  out,\u201d meaning it will be entirely ignored during this training step, but it may be active during the  next step. This probability is usually between 0-1 and 0.5. We use 0.2.  <br>\n\nLet's make the model and train it.\n"}}