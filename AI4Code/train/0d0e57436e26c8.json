{"cell_type":{"55f2cb36":"code","89906dfe":"code","cd3920c2":"code","e95d6e68":"code","d16858ac":"code","e91d50fe":"code","08370cdf":"code","3b2157c8":"code","de1a371c":"code","0e15f6d8":"code","037b23e7":"code","fecfe00f":"code","2bda3efb":"code","3f14bfb0":"code","62762382":"code","ddd205bd":"code","715f78fe":"code","c67d541b":"code","404b88ac":"code","4c5cfc7b":"code","db862596":"code","4be21e5e":"code","019d2f70":"code","67136f91":"code","14ce7b17":"code","eb09e8f4":"code","34f6f52f":"code","62f4354f":"code","835d699d":"code","aed66f62":"code","2a938c35":"code","eb3359a4":"code","49dd97a8":"code","141d6067":"code","1043ec37":"code","93d7a333":"code","cf5d8b5b":"code","d311083c":"code","063f554f":"code","22ff9624":"code","3f34f38e":"markdown","cf469e07":"markdown","18a24b50":"markdown","58008c3d":"markdown","22a78d49":"markdown","00a8cfa3":"markdown","3710020e":"markdown","cc57e3fa":"markdown","56a5eb7a":"markdown","d8684ae2":"markdown","ce19b337":"markdown","24ffb465":"markdown","8c9f3b17":"markdown","19b39063":"markdown","b9cfa9b2":"markdown","041cef9a":"markdown","a8f1a7fc":"markdown","517109f3":"markdown","d06cef01":"markdown","ddbfcc61":"markdown","5c827dcc":"markdown","21942783":"markdown","9d37c285":"markdown","c2b0e218":"markdown","f2329a75":"markdown"},"source":{"55f2cb36":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\n<form action=\"javascript:code_toggle()\"><input style = \"float:right\" type=\"submit\" value=\"Toggle code\">''')\n","89906dfe":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline","cd3920c2":"# Setting up visualisations\nsns.set_style(style='white') \nsns.set(rc={\n    'figure.figsize':(12,7), \n    'axes.facecolor': 'white',\n    'axes.grid': True, 'grid.color': '.9',\n    'axes.linewidth': 1.0,\n    'grid.linestyle': u'-'},font_scale=1.5)\ncustom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\nsns.set_palette(custom_colors)","e95d6e68":"#Reading the data from the CSV file. We use a flag IS_FOR_KAGGLE to toggle the path of the data file.\nIS_FOR_KAGGLE = True\n\nif IS_FOR_KAGGLE:\n    trd = pd.read_csv('..\/input\/train.csv')\n    tsd = pd.read_csv('..\/input\/test.csv')\nelse:\n    trd = pd.read_csv('train.csv')\n    tsd = pd.read_csv('test.csv')\n\ntd = pd.concat([trd, tsd], ignore_index=True, sort  = False)","d16858ac":"td.shape","e91d50fe":"td.info()","08370cdf":"pd.DataFrame(td.isnull().sum()).plot.line().set_title(\"Number of missing values in the given features\")\ntd.isnull().sum()","3b2157c8":"sns.heatmap(td.isnull(), cbar = False).set_title(\"Missing values heatmap\")","de1a371c":"td.nunique()","0e15f6d8":"(trd.Survived.value_counts(normalize=True) * 100).plot.barh().set_title(\"Training Data - Percentage of people survived and Deceased\")\n","037b23e7":"fig_pclass = trd.Pclass.value_counts().plot.pie().legend(labels=[\"Class 3\",\"Class 1\",\"Class 2\"], loc='center right', bbox_to_anchor=(2.25, 0.5)).set_title(\"Training Data - People travelling in different classes\")\n","fecfe00f":"pclass_1_survivor_distribution = round((trd[trd.Pclass == 1].Survived == 1).value_counts()[1]\/len(trd[trd.Pclass == 1]) * 100, 2)\npclass_2_survivor_distribution = round((trd[trd.Pclass == 2].Survived == 1).value_counts()[1]\/len(trd[trd.Pclass == 2]) * 100, 2)\npclass_3_survivor_distribution = round((trd[trd.Pclass == 3].Survived == 1).value_counts()[1]\/len(trd[trd.Pclass == 3]) * 100, 2)\npclass_perc_df = pd.DataFrame(\n    { \"Percentage Survived\":{\"Class 1\": pclass_1_survivor_distribution,\"Class 2\": pclass_2_survivor_distribution, \"Class 3\": pclass_3_survivor_distribution},  \n     \"Percentage Not Survived\":{\"Class 1\": 100-pclass_1_survivor_distribution,\"Class 2\": 100-pclass_2_survivor_distribution, \"Class 3\": 100-pclass_3_survivor_distribution}})\npclass_perc_df.plot.bar().set_title(\"Training Data - Percentage of people survived on the basis of class\")\n","2bda3efb":"for x in [1,2,3]:    ## for 3 classes\n    trd.Age[trd.Pclass == x].plot(kind=\"kde\")\nplt.title(\"Age density in classes\")\nplt.legend((\"1st\",\"2nd\",\"3rd\"))","3f14bfb0":"for x in [\"male\",\"female\"]:\n    td.Pclass[td.Sex == x].plot(kind=\"kde\")\nplt.title(\"Training Data - Gender density in classes\")\nplt.legend((\"Male\",\"Female\"))","62762382":"pclass_perc_df","ddd205bd":"fig_sex = (trd.Sex.value_counts(normalize = True) * 100).plot.bar()\nmale_pr = round((trd[trd.Sex == 'male'].Survived == 1).value_counts()[1]\/len(trd.Sex) * 100, 2)\nfemale_pr = round((trd[trd.Sex == 'female'].Survived == 1).value_counts()[1]\/len(trd.Sex) * 100, 2)\nsex_perc_df = pd.DataFrame(\n    { \"Percentage Survived\":{\"male\": male_pr,\"female\": female_pr},  \"Percentage Not Survived\":{\"male\": 100-male_pr,\"female\": 100-female_pr}})\nsex_perc_df.plot.barh().set_title(\"Percentage of male and female survived and Deceased\")\nfig_sex","715f78fe":"pd.DataFrame(td.Age.describe())","c67d541b":"td['Age_Range'] = pd.cut(td.Age, [0, 10, 20, 30, 40, 50, 60,70,80])\nsns.countplot(x = \"Age_Range\", hue = \"Survived\", data = td, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])\n","404b88ac":"sns.distplot(td['Age'].dropna(),color='darkgreen',bins=30)","4c5cfc7b":"td.SibSp.describe()","db862596":"ss = pd.DataFrame()\nss['survived'] = trd.Survived\nss['sibling_spouse'] = pd.cut(trd.SibSp, [0, 1, 2, 3, 4, 5, 6,7,8], include_lowest = True)\n(ss.sibling_spouse.value_counts()).plot.area().set_title(\"Training Data - Number of siblings or spouses vs survival count\")","4be21e5e":"x = sns.countplot(x = \"sibling_spouse\", hue = \"survived\", data = ss, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])\nx.set_title(\"Training Data - survival based on number of siblings or spouses\")","019d2f70":"pd.DataFrame(td.Parch.describe())","67136f91":"pc = pd.DataFrame()\npc['survived'] = trd.Survived\npc['parents_children'] = pd.cut(trd.Parch, [0, 1, 2, 3, 4, 5, 6], include_lowest = True)\n(pc.parents_children.value_counts()).plot.area().set_title(\"Training Data - Number of parents\/children and survival density\")","14ce7b17":"x = sns.countplot(x = \"parents_children\", hue = \"survived\", data = pc, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])\nx.set_title(\"Training Data - Survival based on number of parents\/children\")\n","eb09e8f4":"td['Family'] = td.Parch + td.SibSp\ntd['Is_Alone'] = td.Family == 0","34f6f52f":"td.Fare.describe()","62f4354f":"td['Fare_Category'] = pd.cut(td['Fare'], bins=[0,7.90,14.45,31.28,120], labels=['Low','Mid',\n                                                                                      'High_Mid','High'])","835d699d":"x = sns.countplot(x = \"Fare_Category\", hue = \"Survived\", data = td, palette=[\"C1\", \"C0\"]).legend(labels = [\"Deceased\", \"Survived\"])\nx.set_title(\"Survival based on fare category\")\n","aed66f62":"p = sns.countplot(x = \"Embarked\", hue = \"Survived\", data = trd, palette=[\"C1\", \"C0\"])\np.set_xticklabels([\"Southampton\",\"Cherbourg\",\"Queenstown\"])\np.legend(labels = [\"Deceased\", \"Survived\"])\np.set_title(\"Training Data - Survival based on embarking point.\")","2a938c35":"td.Embarked.fillna(td.Embarked.mode()[0], inplace = True)","eb3359a4":"td['Salutation'] = td.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip()) \ntd.Salutation.nunique()\nwc = WordCloud(width = 1000,height = 450,background_color = 'white').generate(str(td.Salutation.values))\nplt.imshow(wc, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\n\ntd.Salutation.value_counts()","49dd97a8":"grp = td.groupby(['Sex', 'Pclass'])  \ntd.Age = grp.Age.apply(lambda x: x.fillna(x.median()))\n\n#If still any row remains\ntd.Age.fillna(td.Age.median, inplace = True)","141d6067":"sal_df = pd.DataFrame({\n    \"Survived\":\n    td[td.Survived == 1].Salutation.value_counts(),\n    \"Total\":\n        td.Salutation.value_counts()\n})\ns = sal_df.plot.barh()","1043ec37":"td.Cabin = td.Cabin.fillna('NA')","93d7a333":"td = pd.concat([td,pd.get_dummies(td.Cabin, prefix=\"Cabin\"),pd.get_dummies(td.Age_Range, prefix=\"Age_Range\"), pd.get_dummies(td.Embarked, prefix=\"Emb\", drop_first = True), pd.get_dummies(td.Salutation, prefix=\"Title\", drop_first = True),pd.get_dummies(td.Fare_Category, prefix=\"Fare\", drop_first = True), pd.get_dummies(td.Pclass, prefix=\"Class\", drop_first = True)], axis=1)\ntd['Sex'] = LabelEncoder().fit_transform(td['Sex'])\ntd['Is_Alone'] = LabelEncoder().fit_transform(td['Is_Alone'])","cf5d8b5b":"td.drop(['Pclass', 'Fare','Cabin', 'Fare_Category','Name','Salutation', 'Ticket','Embarked', 'Age_Range', 'SibSp', 'Parch', 'Age'], axis=1, inplace=True)\n","d311083c":"# Data to be predicted\nX_to_be_predicted = td[td.Survived.isnull()]\nX_to_be_predicted = X_to_be_predicted.drop(['Survived'], axis = 1)\n\n#Training data\ntrain_data = td\ntrain_data = train_data.dropna()\nfeature_train = train_data['Survived']\nlabel_train  = train_data.drop(['Survived'], axis = 1)\ntrain_data.shape #891 x 28\n\n##Gaussian\nclf = GaussianNB()\nx_train, x_test, y_train, y_test = train_test_split(label_train, feature_train, test_size=0.2)\nclf.fit(x_train,  np.ravel(y_train))\nprint(\"NB Accuracy: \"+repr(round(clf.score(x_test, y_test) * 100, 2)) + \"%\")\nresult_rf=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for GNB is:',round(result_rf.mean()*100,2))\ny_pred = cross_val_predict(clf,x_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix for NB', y=1.05, size=15)","063f554f":"##Random forest\nclf = RandomForestClassifier(criterion='entropy', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\nx_train, x_test, y_train, y_test = train_test_split(label_train, feature_train, test_size=0.2)\nclf.fit(x_train,  np.ravel(y_train))\nprint(\"RF Accuracy: \"+repr(round(clf.score(x_test, y_test) * 100, 2)) + \"%\")\n\nresult_rf=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for Random forest is:',round(result_rf.mean()*100,2))\ny_pred = cross_val_predict(clf,x_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix for RF', y=1.05, size=15)","22ff9624":"result = clf.predict(X_to_be_predicted)\nsubmission = pd.DataFrame({'PassengerId':X_to_be_predicted.PassengerId,'Survived':result})\nsubmission.Survived = submission.Survived.astype(int)\nprint(submission.shape)\nfilename = 'Titanic Predictions.csv'\nsubmission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","3f34f38e":"### Cabin\n\nAssigning NA for non available cabin values. Pulling deck value from Cabin and adding a feature 'Deck'","cf469e07":"### 13. Embarked\n\nEmbarked signifies where the traveler boarded from. There are three possible values for Embark - Southampton,Cherbourg,Queenstown.\n\nIn combined data, more than **70% of the people boarded from Southampton. Just under 20% boarded from Cherbourg and the rest boarded from Queenstown.**\n\nMore People who **boarded from Cherbourg survived than those who died**","18a24b50":"# Encoding & dropping columns\n\nUsing Pandas' get dummies we encoded the categorical data. Later, we drop all the columns we encoded.","58008c3d":"### 6. Sex\n\nRoughly around **65% of the tourists were male** while the remaining 35% were female. However, the percentage of female survivors was higher than the number of male survivors.\n\nMore than 80% male passengers had to die as compared to around 70% female passengers.","22a78d49":"### 8. SibSp\n\nSibSp is the number of siblings or spouse of a person onboard. A maximum of 8 siblings and spouses traveled along with one of the tourists.\n\n**More than 90% people traveled alone or with one of their sibling or spouse**.\n\nThe chances of survival dropped drastically if someone traveled with more than 2 siblings or spouse.","00a8cfa3":"### 10. Ticket\n\n","3710020e":"### 11. Fare\n\nIt is clear that there is a strong correlation between the fare and the survival. **The higher a tourist paid, the higher would be his chances to survive.**","cc57e3fa":"### 7. Age","56a5eb7a":"# Features\n\n### 4. Survived ###\n\nThe horizontal bar plot below shows the percentage of people that survived and percentage of people that unfortunately couldn't make it out alive from the disaster. **More than 60% of the people on the ship died.**","d8684ae2":"### 1. Getting the hang of the data","ce19b337":"### 12. Cabin\n\nAs Cabin has a lot of missing values, we will impute it in following section and then use it for prediction.\n","24ffb465":"<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\ngithub.com\/sumitmukhija\n<\/footer>","8c9f3b17":"### Embarked\n\nSince embarked only has two missing values and the highest number of people boarded the ship from Southampton, the probablity of boarding from Southampton is high. So, we fill the missing values with Southampton. However, instead of manually putting in Southampton, we would find the mode of the Embarked column and substitute missing values with it.","19b39063":"The description of column 'Age' tells us that the **youngest traveler onboard was aged around 2 months** and the **oldest commuter was 80 years**. The **average age of passengers onboard was just under 30 years.** \nHowever, it must be remembered that these observations are with missing values. Since we know that the oldest passenger was 80, we can plot age range and number of people survived or died for that age range.\n\nApparently, higher number of children below age 10 were saved than died. For every other age group, the number of casualities was higher than the number of survivors. More than 140 people within the age group 20 and 30 were dead as compared to just around 80 people of the same age range sustained.","b9cfa9b2":"As the feature 'Ticket' does not provide any additional information, we would not consider this feature","041cef9a":"On diving deeper in to the columns, we see five integer type columns, five object types (strings) and two columns with decimal values. Also, all the columns do not have 1309 values which indicates that there are missing values.","a8f1a7fc":"### 5. Pclass ###\n\nPclass or passenger class represents the traveling class of commuter. There were three classes. In the combined dataset, a clear majority **(709)** traveled in the third class, followed by the second **(277)** and then the first **(323)**. \n\n**The number of passengers in the third class was more than the number of passengers in first and second class combined.**\n\n**With the missing survived values, More than 40% of the first class passengers were rescued.** The pattern differed for the second and third class survivors as roughly around **70% of the second class passengers lost their lives**. The numbers skyrocketed for the third class passengers. More than **80% of the third class passengers couldn't survive** the disaster.\n\n\n","517109f3":"# Prediction\n\nPrediction algorithms used:\n\n1. Gaussian Naive Bayes\n2. Random Forest","d06cef01":"# Data Imputation","ddbfcc61":"### 9. Parch\n\nSimilar to the SibSp, this feature contained the number of parents or children each passenger was traveling with. A maximum of 9 parents\/children traveled along with one of the passenger.\n\nWe will create two new columns, a column named family will have the sum of the number of siblings\/spouse and number of parents\/children. \n\nPeople traveling alone had higher chances of survival. So, we also create a column Is_Alone\n\n","5c827dcc":"The data contains 1309 rows and 12 colunmns. Each row signifies a human on the deck and each column is an attribute corresponding to that human.","21942783":"\n### 3. Number of unique values\n\nTo comprehend the categorical data in the training data, we list down al the columns with the number of unique values they have. We infer that \n\n- survived and Sex can have **two** distinct values\n- Embarked and PClass have **three** distinct values\n\nSo, we have four columns of cateogrical data - **Survived, Sex, Embarked and PClass**.","9d37c285":"### Age\n\nAge has **263 missing values**. To deal with missing values, we first try to categorise the people with their titles. There are **17 different titles** in the training data. We group the titles and sex and then we find the median of all the categories and replace the missing values with the median of that category.","c2b0e218":"# Titanic: Machine Learning from Disaster\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/data\n    ","f2329a75":"### 2. Missing values\n\nThe line plot and the heat map below show the number of missing values in all columns. There are multiple columns that have missing values\n\n- Age \n- Cabin\n- Embarked\n- Fare\n- Survived\n\n**77.5%** of data is NOT available for the column 'Cabin'. Hence, it can be dropped in our analysis and prediction. An alternate approach would have been to predict values for the missing values. However, it is not recommended as the number of values available for Cabin is significantly lower.\n\nIn contrast, Embarked has only 2 missing missing values. As the column contains **0.15%** missing values, the values can be substituted by the mode of the column. \n\nThe column 'Age' on the other hand has 177 missing values which contribute to **20%** on the total expected values. We will impute the age data by categorising people on the basis of their title and finding the median age.\n\nFare has one missing value."}}