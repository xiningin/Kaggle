{"cell_type":{"70e29385":"code","26ee81c9":"code","96853e83":"code","318da279":"code","1e9156e3":"code","7598e0eb":"code","ff57b999":"code","fe373441":"code","ac44d911":"code","c4b51475":"code","2cf59392":"code","ddd34187":"code","94ad4d0e":"code","396a9f1f":"code","5b3cccdb":"code","c005d0cd":"code","1d61bd59":"code","dd6c2f57":"code","d0760193":"code","7cb619f4":"code","1823e12a":"code","486645bb":"code","96e0c7c5":"code","70483f79":"code","9269e0e4":"code","ace4f6da":"code","13d20c51":"code","fb6d8907":"code","db9286cf":"code","fe3c78c9":"code","86d14665":"code","a65f1cae":"code","4e83c5f4":"code","e8fedb67":"code","5d0ad0f5":"code","375e68b2":"code","7f3231c1":"code","01f7bb38":"code","905852b2":"code","801db9d9":"code","9cf9ecf8":"code","d6cc6979":"markdown","6b0504ec":"markdown","e9288f97":"markdown","f561ea17":"markdown","722891f8":"markdown","691c648d":"markdown","6a8d8a7c":"markdown","aede159f":"markdown","47b9ca6e":"markdown","af492501":"markdown","f622782e":"markdown","603f67da":"markdown","bc347b43":"markdown","9157b68c":"markdown","55212806":"markdown","a18c8835":"markdown","d27c4f4c":"markdown","7d6f48eb":"markdown","6ffb6ab0":"markdown","2b2f3483":"markdown","a43abf8d":"markdown","8e51ef77":"markdown","d8766c85":"markdown","5cba9bb0":"markdown"},"source":{"70e29385":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings        \nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport datetime\n\n#plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\n#stats\nimport scipy.stats as stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n#preprocessing\nfrom sklearn import preprocessing\n\n#algorithms\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\n\n#Regressors and stuff\nfrom xgboost import XGBRegressor\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","26ee81c9":"#some useful functions\n#mutual information to compute a utility score for a feature, giving indication of how much potential the feature has\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    figure(num=None, figsize=(15,15), dpi=256, facecolor='w', edgecolor='r')\n    plt.title(\"Mutual Information Scores\")\n    sns.barplot(scores, ticks)\n    plt.show()\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    #\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    # taken from feature engineering tutorial course -- kaggle\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n# remove outliers out of 99th and 1st percentile\ndef Winsorization_outliers(df, column='SalePrice'):\n    start = len(df[column])\n    q1 = np.percentile(df[column] , 1)\n    q3 = np.percentile(df[column] , 99)\n    for i in df[column]:\n        if i > q3 or i < q1:\n            df.drop(df.index[df[column] == i], inplace = True)\n    print(f\"Deleted {start - len(df[column])} outliers\")\n    return df\n\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","96853e83":"train_filename = 'train.csv'\ntest_filename = 'test.csv'\ndirname = dirname + '\/'\ndf_train = pd.read_csv(dirname + train_filename)\n\n# performance influencer #1\ndf_train = Winsorization_outliers(df_train)\ndf_test = pd.read_csv(dirname + test_filename)\ndf = pd.concat([df_train, df_test])\ndf_train.columns = df_train.columns.str.replace(' ', '') \ndf_test.columns = df_test.columns.str.replace(' ', '') \ndf.columns = df.columns.str.replace(' ', '') \ndf","318da279":"print(df.dtypes.to_list())\ndf.info(verbose=True)","1e9156e3":"# before drops and fills\nmissing = missing_percentage(df)\nprint(missing)","7598e0eb":"# this missing values columns fill according information from data description\ndf[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Electrical'] = df['Electrical'].fillna(\"SBrkr\")\ndf['KitchenQual'] = df['KitchenQual'].fillna(\"TA\")\ndf.Exterior1st.fillna('None', inplace=True)   \ndf.Exterior2nd.fillna('None', inplace=True)   \ndf.SaleType.fillna('None', inplace=True)   \ndf.MSZoning.fillna('None', inplace=True)   \n\n# the data description stats that NA refers to \"No Pool\"\ndf[\"PoolQC\"] = df[\"PoolQC\"].fillna(\"None\")\n# Replacing the missing values with 0, since no garage = no cars in the garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df[col] = df[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        df[col] = df[col].fillna('None')\n    # NaN values for these categorical basement df, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        df[col] = df[col].fillna('None')\n\n# this columns with Nones and 0\ndf.MiscFeature.fillna('None', inplace=True)    \ndf.Alley.fillna('None', inplace=True)          \ndf.Fence.fillna('None', inplace=True) \ndf.FireplaceQu.fillna('None', inplace=True)    \ndf.MasVnrType.fillna('None', inplace=True)   \ndf.MasVnrArea.fillna(0, inplace=True)    \ndf.Utilities.fillna(df.Utilities.mode()[0], inplace=True)          \ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.BsmtFinSF2.fillna(0, inplace=True)    \ndf.BsmtFinSF1.fillna(0, inplace=True)\ndf.TotalBsmtSF.fillna(0, inplace=True)   \ndf.BsmtUnfSF.fillna(0, inplace=True)  ","ff57b999":"# add some features\ndf['TotalSF']=df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\ndf['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +\n                                 df['1stFlrSF'] + df['2ndFlrSF'])\n\ndf['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                               df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\ndf['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                              df['EnclosedPorch'] + df['ScreenPorch'] +\n                              df['WoodDeckSF'])\n\ndf['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# more tests on Mutual information shown that age variables give more information than year built\n# This not applies to Age column, but AgeRemove and AgeGarage\ndf['AgeRemod'] = df.YrSold - df.YearRemodAdd\ndf['AgeGarage'] = df.YrSold - df.GarageYrBlt\n\n# For the houses without a Garage, I filled the NANs with zeros, which makes AgeGarage ~ 2000\n# Here I replace their AgeGarage with the maximum value among the houses with Garages\nmax_AgeGarage = np.max(df.AgeGarage[df.AgeGarage < 1000])\ndf['AgeGarage'] = df['AgeGarage'].map(lambda x: max_AgeGarage if x > 1000 else x)\n\n# Some of the values are negative because the work was done after the house \n# was sold. In these cases, I change them to zero to avoid negative ages.\ndf.AgeRemod = df.AgeRemod.map(lambda x: 0 if x < 0 else x)\ndf.AgeGarage = df.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\ndf = df.drop('YearRemodAdd', 1)\ndf = df.drop('GarageYrBlt', 1)","fe373441":"missing = missing_percentage(df)\nprint(missing)","ac44d911":"for i in df_train.select_dtypes('O'):\n    print(f\"{df_train[i].nunique()} -- {i} -- {df_train[i].unique()}\")","c4b51475":"features_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","2cf59392":"df = encode(df)\ndf.select_dtypes(\"O\").shape\n#df.info(verbose=True)","ddd34187":"# let's now perform train and test split\ndef tt_split(data):\n    local_test = data[data.SalePrice.isnull()]\n    local_train = data[data.SalePrice.notna()]\n    return local_train, local_test\n\n#Id columns contains irrelevant and incorrect values due to outlier removal\ndf = df.drop(\"Id\", 1)\ndf_train, df_test = tt_split(df)\n\n#groups = df_train.columns.to_series().groupby(df.dtypes)\n#groups.groups","94ad4d0e":"# get to know some mututal information\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\nmi_scores = make_mi_scores(X, y)\nmi_scores\nplot_mi_scores(mi_scores)","396a9f1f":"# as can be seen some variables just have 0 mutual information. Deleting them \nto_del = mi_scores[mi_scores <= 0]\nto_del = to_del.index.to_list()\ndef drop_uninformative(df, columns):\n    df = df.drop(columns, 1)\n    return df\ndf = drop_uninformative(df, to_del)\ndf_train = drop_uninformative(df_train, to_del)\ndf_test = drop_uninformative(df_test, to_del)\nprint(f\"any of unnesessary columns remain in original dataset: {to_del in df.columns.to_list()}\")","5b3cccdb":"(mu, sigma) = norm.fit(df_train['SalePrice'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(df_train['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","c005d0cd":"print(\"skewness: \" + str(df_train.SalePrice.skew()), \"kurtosis \", str(df_train.SalePrice.kurt()))","1d61bd59":"from scipy.stats import anderson\nresult = anderson(df_train.SalePrice)\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.critical_values[i]\n    if result.statistic < result.critical_values[i]:\n        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n    else:\n        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n","dd6c2f57":"correlation_train=df_train.corr()\nsns.set(font_scale=2)\nplt.figure(figsize = (50,35))\nax = sns.heatmap(correlation_train, annot=True,annot_kws={\"size\": 25},fmt='.1f',cmap='PiYG', linewidths=.5)","d0760193":"sns.set(font_scale=1)\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=df_train,x='OverallQual',y='SalePrice',ax=ax[1])\nsns.violinplot(data=df_train,x='OverallQual',y='SalePrice',ax=ax[2])\nsns.boxplot(data=df_train,x='OverallQual',y='SalePrice',ax=ax[0])\nplt.show()","7cb619f4":"Pearson_GrLiv = 0.7\nfig,ax=plt.subplots(1,2,figsize=(20,10))\nsns.regplot(data=df_train, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2}, ax=ax[0])\nsns.regplot(data=df_train, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2}, ax=ax[1])\nplt.show()","1823e12a":"k = 10 #number of variables for heatmap\ncols = correlation_train.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n","486645bb":"import shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","96e0c7c5":"def get_baseline(df, out=False):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    cv_scores = []\n\n    baseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.', 'ElasticCV', 'RidgeCV', 'LassoCV', 'LGBM_Reg.','SVR',\n                       'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                       'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']\n\n    # Linear Regression\n\n    lreg = LinearRegression()\n    lreg_score = score_dataset(X, y, lreg)\n    cv_scores.append(lreg_score)\n    \n\n    # Bayesian Ridge Regression\n\n    brr = BayesianRidge(compute_score=True)\n    brr_score = score_dataset(X, y, brr)\n    cv_scores.append(brr_score)\n    \n\n    # ElasticCV\n\n    elastic= ElasticNetCV()\n    elastic_score = score_dataset(X, y, elastic)\n    cv_scores.append(elastic_score)\n    \n\n    # Ridge\n\n    ridge = RidgeCV()\n    ridge_score = score_dataset(X, y, ridge)\n    cv_scores.append(ridge_score)\n    \n\n    # LassoCV\n\n    lasso = LassoCV()\n    lasso_score = score_dataset(X, y, lasso)\n    cv_scores.append(lasso_score)\n    \n\n    # Light Gradient Boost Regressor\n\n    l_gbm = LGBMRegressor(objective='regression')\n    l_gbm_score = score_dataset(X, y, l_gbm)\n    cv_scores.append(l_gbm_score)\n    \n\n    # Support Vector Regression\n\n    svr = SVR()\n    svr_score = score_dataset(X, y, svr)\n    cv_scores.append(svr_score)\n    \n\n    # Decision Tree Regressor\n\n    dtr = DecisionTreeRegressor()\n    dtr_score = score_dataset(X, y, dtr)\n    cv_scores.append(dtr_score)\n    \n\n    # Random Forest Regressor\n\n    rfr = RandomForestRegressor()\n    rfr_score = score_dataset(X, y, rfr)\n    cv_scores.append(rfr_score)\n    \n\n    # XGB Regressor\n\n    xgb = XGBRegressor()\n    xgb_score = score_dataset(X, y, xgb)\n    cv_scores.append(xgb_score)\n    \n\n    # Gradient Boost Regressor\n\n    gbr = GradientBoostingRegressor()\n    gbr_score = score_dataset(X, y, gbr)\n    cv_scores.append(gbr_score)\n    \n\n    # Cat Boost Regressor\n\n    catb = CatBoostRegressor()\n    catb_score = score_dataset(X, y, catb)\n    cv_scores.append(catb_score)\n    \n\n    # Stacked Regressor\n\n    stack_gen = StackingRegressor(regressors=(catb, lreg, brr, gbr, lasso, elastic, ridge, svr, dtr, rfr, l_gbm),\n                                  meta_regressor = CatBoostRegressor(),\n                                  use_features_in_secondary = True)\n\n    stack_gen_score = score_dataset(X, y, stack_gen)\n    cv_scores.append(stack_gen_score)\n    \n    \n    if out:\n        print(f\"Baseline score linear: {lreg_score:.7f} RMSLE\")\n        print(f\"Baseline score bayesian: {brr_score:.7f} RMSLE\")\n        print(f\"Baseline score elastic: {elastic_score:.7f} RMSLE\")\n        print(f\"Baseline score ridge: {ridge_score:.7f} RMSLE\")\n        print(f\"Baseline score lasso: {lasso_score:.7f} RMSLE\")\n        print(f\"Baseline score LGBM: {l_gbm_score:.7f} RMSLE\")\n        print(f\"Baseline score svr: {svr_score:.7f} RMSLE\")\n        print(f\"Baseline score decision tree: {dtr_score:.7f} RMSLE\")\n        print(f\"Baseline score random forest: {rfr_score:.7f} RMSLE\")\n        print(f\"Baseline score XGBoost: {xgb_score:.7f} RMSLE\")\n        print(f\"Baseline score Gradient boost: {gbr_score:.7f} RMSLE\")\n        print(f\"Baseline score Cat boost: {catb_score:.7f} RMSLE\")\n        print(f\"Baseline score random forest: {stack_gen_score:.7f} RMSLE\")\n    \n    return baseline_models, cv_scores","70483f79":"baseline_models, cv_scores = get_baseline(df_train)\ndf_scores = pd.DataFrame({\"model\": baseline_models, \"score\" : cv_scores})","9269e0e4":"df_scores","ace4f6da":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef pca_components(df, features):\n    X = df[features]\n    pca_ret, X_pca, loadings = apply_pca(X)\n    return pca_ret, X_pca, loadings\n","13d20c51":"pca_features_1 = ['TotalSF',\n 'GrLivArea',\n 'YearBuilt',\n 'LotArea',\n 'GarageCars',\n 'Total_Bathrooms',\n 'AgeGarage',\n 'AgeRemod',\n 'FullBath',\n 'Total_porch_sf',\n 'TotRmsAbvGrd',\n 'LotFrontage',\n 'OpenPorchSF',\n 'hasfireplace',\n 'Fireplaces',\n]\n","fb6d8907":"pca_1, _, loadings_1 = pca_components(df_train, pca_features_1)\nloadings_1","db9286cf":"plot_variance(pca_1)","fe3c78c9":"def get_inspired(df):\n    df[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    df[\"Feature2\"] = df.GrLivArea \/ df.LotArea\n    df[\"Feature3\"] = (df['1stFlrSF'] + df['2ndFlrSF']) \/ df.TotRmsAbvGrd\n    return df\n\ndf = get_inspired(df)\ndf_train = get_inspired(df_train)","86d14665":"baseline_models, cv_scores = get_baseline(df_train)\ndf_scores = pd.DataFrame({\"model\": baseline_models, \"score\" : cv_scores})","a65f1cae":"df_scores","4e83c5f4":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n\ndf = label_encode(df)\ndf_train = label_encode(df_train)\ndf_train, df_test = tt_split(df)","e8fedb67":"df_train_dummy = pd.get_dummies(df_train)\ndf_test_dummy = pd.get_dummies(df_test)\n\nbaseline_models, cv_scores = get_baseline(df_train_dummy)\ndf_scores = pd.DataFrame({\"model\": baseline_models, \"score\" : cv_scores})","5d0ad0f5":"df_scores","375e68b2":"X_train,X_val,y_train,y_val = train_test_split(df_train_dummy.drop(\"SalePrice\",1),df_train_dummy.SalePrice,test_size = 0.1,random_state=42)","7f3231c1":"x_train = df_train_dummy.drop(\"SalePrice\",1)\ny_train = df_train_dummy.SalePrice","01f7bb38":"xgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)","905852b2":"import optuna\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(x_train, y_train, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\nxgb_params = study.best_params\n","801db9d9":"X_train = df_train_dummy.drop(\"SalePrice\", 1)\ny = df_train_dummy.SalePrice\nX_test = df_test_dummy.drop(\"SalePrice\", 1)\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n","9cf9ecf8":"sample = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\noutput = pd.DataFrame({'Id': sample.Id, 'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)","d6cc6979":"**I want to get the scope on how much values needs to be filled\\\nPretty obvious that some features can be deleted, since we cannot fill up to like 50% of data. That would be a poor choice. Lets delete some features here and now. But before doing that, it would matter to investigate more on features and reasoning for absense of some values.**\n\n* PoolQC: Pool quality -- In this case only 0.48% exist. Althrough this may be due to existence of pools in the area. If there is no pool, then no surprise, there is no data.\n\n*  MiscFeature: Miscellaneous feature not covered in other categories. Too shady and complicated with almost all data absent. Let's delete this one.\n\n* Alley: Type of alley access to property -- here it seems reasonable to use this one, since NA means no access to alley\n\n* Fence: Fence quality -- Let it live, since NA means no fence\n\n* FireplaceQu: Fireplace quality -- same reasoning as for alley and fence\n\n* MasVnrType -- need conversion based on NA\n\n* MasVnrArea -- ned conversion based on NA\n\n* LotFrontage: Linear feet of street connected to property -- first it may seem like missing more than 17% is crusial, but actually 100% of such data have no pool, have pave and all utilities. As it seems to me, this kind of property can not have this frontage by some reason. Need special treatment and strategy.\n\n* GarageYrBlt: Year garage was built -- NA means no garage, but since it is data we need to figuire something for this. Also need some special sort of convertion.\n\nSome features will require mode filling, such as GarageArea, GarageCars, Electrical and basement columns.\nMany other features require more attention, some columns will not be described as meticulously, as this ones\n","6b0504ec":"# **Creating dummies**","e9288f97":"# **Filling missing values**","f561ea17":"**Because of preliminary careful study of others notebooks, I will start with data clearing, imputing and encoding and then move to more extensive data exploration**","722891f8":"# **Distribution normality**","691c648d":"# **Baseline models**","6a8d8a7c":"# **Missing values**","aede159f":"#  **Functions will be used**\n\n**make_mi_scores -- for getting list of columns with the highest ration of mutual information\\\nplot_mi_scores -- visualization mi_scores\\\nscore_dataset -- how well model baseline fits to the data\\\nWinsorization_outliers -- removing everything over 99th and 1st percentile\\\nmissing_percentage -- info about missing values**","47b9ca6e":"# **Baseline models score**","af492501":"**Data distribution is different from normalit has positive kurtosis and skewness. Normality tests can be applied\nH0 theorem - Data normally distributed.**","f622782e":"# **Principal component analysis**","603f67da":"# **Parameters search**","bc347b43":"# **Data loading and outlier removal**\n**Lets get to know with dataset a bit more. As it may seems it contains lots of features.\\\nFirst loading the data.\\\nThen Delete outliers.**","9157b68c":"#  **Hope you like!**\n\n**Hello there! It's my second project about regression tasks. Here I will explore the data and use several model, that can help me to predict house prices. Let's start.**\n\nHere is a dataset about housing prices in IOWA state, containing 79 variables (wow!). More than enough to make great choice. \nI want to test this dataset against several regression techniquest climbing like on ladder and looking for a definitive winner.","55212806":"# **Observation of Overall quality, GrLivArea, Building year vs price**","a18c8835":"**And how many missing we have after cleaning**","d27c4f4c":"**Being Sales as main variable, we want to see spread of this variable over time, histogram prefectly suits for that need**","7d6f48eb":"# **Feature enineering**","6ffb6ab0":"# **Final model**","2b2f3483":"# **Submission**","a43abf8d":"**Choosing XGboost seems a bit confusing, since this particular model did not show best results among all beseline models, but several testing showed that final score is higher**","8e51ef77":"# **Correlation matrix**","d8766c85":"# **Transforming objects to categories**","5cba9bb0":"# **Mutual information**"}}