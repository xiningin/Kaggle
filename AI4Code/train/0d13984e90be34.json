{"cell_type":{"a847c1e6":"code","a5fc421e":"code","f8101264":"code","00708a92":"code","74abdac0":"code","ecbbf2c9":"code","57631b7e":"code","47cc148d":"code","7e3e7435":"code","d9f45446":"code","43962c54":"code","48d09a97":"code","a6776eb6":"code","5da7bd01":"code","af2698f5":"code","2e728ef9":"code","6d2ef302":"code","93f126ac":"code","7c926af5":"code","85a91c7f":"code","de023b7a":"markdown"},"source":{"a847c1e6":"# Creating a toy dataset of types of fruits in the order [Color, Diameter, Label]\nimport numpy as np\nimport pandas as pd\n# toy_dataset = [['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon'], ['Green', 3, 'Apple'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\ntoy_dataset = [['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Red', 1, 'Grape'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\ndata = np.array(toy_dataset)\nprint(data)\nd = pd.DataFrame({'Color':data[:, 0], 'Diameter':data[:, 1], 'Fruit_label':data[:, 2]})\nprint(\"\\n\",d)\n\nprint(\"\\n\", len(toy_dataset))","a5fc421e":"# Assigning labels to each column\nhead = ['Color', 'Diameter', 'Fruit_label']","f8101264":"def unique_values_in_a_col(rows, col):\n    return set([row[col] for row in rows])\n\nprint(unique_values_in_a_col(toy_dataset, 1))","00708a92":"# Function to count the frequency of each type of a label in the dataset\n\ndef count(datast):\n    count = {}  # A dict with key : value as label : freq\n    for row in datast:\n        label = row[-1]   # Since the above dataset has labels in the last column only\n        if label not in count:\n            count[label] = 0\n        count[label] += 1\n    return count\n\n# Example\nprint(count(toy_dataset))","74abdac0":"# Class that asks the question that best splits the dataset\n\nclass Ques:\n    def __init__(self, column, value):\n        self.column = column\n        self.value = value\n    \n    # Function to check if the value passed (ex) is to be split into True or False branch based on the question asked.\n    # Example to demonstrate use is given later\n    def match(self, ex):\n        val = ex[self.column]\n        if type(val) == int:\n            return val >= self.value     # Because one of the type of questions can be if diameter is >= 3\n        else:\n            return val == self.value     # Because another type of questions can be if color matches a particular color\n    \n    # Function to print out the question formulated by values passed in __init__() in a readable format\n    def __repr__(self):\n        det_cond = \"==\"           # Partitioning condition in the tree splitting\n        if type(self.value) == int:\n            det_cond = \">=\"\n        return \"Is %s %s %s ?\" % (head[self.column], det_cond, str(self.value))\n    \n# Examples to show printing of splitting questions and the True\/False branching [match() method]\n\nqs = Ques(1, 3)\nqs","ecbbf2c9":"qs.match(toy_dataset[3])      \n# self.column = 1. So when we provide ex = toy_dataset[3], val becomes ex[3][1].It is = 3. Hence, True.","57631b7e":"''' Function to partition the dataset into True branch end and False branch end. \n                                 _____________\n                                |starting_node|   best split question\n                                      \/   \\\n                            True     \/     \\   False\n                                    \/       \\\n                                Next node   Next node\n                                   or          or\n                                Leaf node   Leaf node     ''' \n\ndef partition(rows, ques):\n    true, false = [], []\n    for row in rows:\n        if ques.match(row):\n            true.append(row)\n        else:\n            false.append(row)\n    return true, false\n\n# Example of a split\nT, F = partition(toy_dataset, Ques(0, 'Red'))\nprint(\"True branch :\", T,\"\\n\\nFalse branch :\", F)","47cc148d":"# Calculating the Gini impurity of list of rows\n\ndef gini_imp(rows):\n    cnts = count(rows)\n    impurity = 1\n    for labl in cnts:\n        p = cnts[labl]\/float(len(rows))\n        impurity -= p**2\n    return impurity\n\n# Example to show calculation of Gini Impurity\nprint(\"Initial impurity (label based, before split): \", gini_imp(toy_dataset))","7e3e7435":"''' Information Gain = uncertainty of starting node - (weighted impurity of the two child nodes)\n\n    Simply, information gain(or just Gain) = G(before split) - sum(weight * G(after split))\n    \nCalculating the information gain of a split. High Gain = most likely split'''\n\ndef info_gain(left_child, right_child, before_split):\n    weight = float(len(left_child))\/(len(left_child) + len(right_child))\n    return before_split - weight*gini_imp(left_child) - (1-weight)*gini_imp(right_child)\n\ntrue, false = partition(toy_dataset, Ques(0, 'Red'))\ninfo_gain(true, false, gini_imp(toy_dataset))","d9f45446":"print(true, \"\\n\\n\", false)","43962c54":"''' And finally. The function that does the best splitting by iterating repetitively over all features to see the \n    possible questions that can be asked and asking that question that gives the highest info gain'''\n\ndef best_split(rows):\n    initial_uncertainty = gini_imp(rows)\n    # No. of coulmns\n    n = len(rows[0]) - 1\n    # Keep track of best information gain\n    best_gain = 0\n    # Keep track of the question that gave the best information gain\n    best_question = None\n    \n    for column in range(n):\n        values = set([row[column] for row in rows])\n        for v in values:\n            question = Ques(column, v)\n            true, false = partition(rows, question)\n            \n            if len(true) == 0 or len(false) == 0:\n                continue\n            inf_gain = info_gain(true, false, gini_imp(rows))\n            if inf_gain >= best_gain:\n                best_gain, best_question = inf_gain, question\n    return best_gain, best_question\n\n\n# Example to find splitting question of starting node of our dataset\nprint(best_split(toy_dataset))","48d09a97":"''' A class to define the leaf nodes of a tree. A leaf node is basically the count of a particular label at a specific row \n    from the training data that satisfies the conditions to be a leaf node.'''\n\nclass Leaf:\n    def __init__(self, rows):\n        self.predictions = count(rows)","a6776eb6":"''' Now, the class to create a splitting node or the Decsision Node'''\n\nclass Dec_node:\n    def __init__(self, question, true, false):\n        self.question = question\n        self.true = true\n        self.false = false","5da7bd01":"''' Finally, the we write the function to build the tree. First we do the start split, to decide the root of the tree. \n    Obviously it is split into the True and False branches. Then, we recursively build the tree on true branch and the false\n    branch. This is continued till information gain at the node are = 0. They are assigned as Leaf nodes.'''\n\ndef build_Tree(rows):\n    # We are finding the first best split question to zero down on the root node.\n    i_gain, ques = best_split(rows)\n    # Next we see if information gain is zero. If yes, no split occurs, Leaf Node is assigned. (Base condition)\n    \n    if i_gain == 0:\n        return Leaf(rows)\n    \n    # However, if gain is not zero, we split the dataset into the true and false branches. The nodes at the end of the branches\n    # become decision nodes...the function build_Tree is again called recursively on these decision nodes.\n  \n    true_rows, false_rows = partition(rows, ques)\n    \n    true_node = build_Tree(true_rows)\n    false_node = build_Tree(false_rows)\n    \n    return Dec_node(ques, true_node, false_node)","af2698f5":"''' Now we write the function to print the tree. '''\n\ndef print_Tree(node, spacing = \"\"):\n    # First we check the base condition,i.e, if we've reached a Leaf\n    if isinstance(node, Leaf):\n        print(spacing + \"Predict\", node.predictions)\n        return\n    \n    # If not a leaf node,\n    \n    print(spacing + str(node.question))\n    \n    # The True branch\n    print(spacing + \"--> True\")\n    print_Tree(node.true, spacing + \" \")\n    \n    # The False branch\n    print(spacing + \"--> False\")\n    print_Tree(node.false, spacing + \" \")","2e728ef9":"this_tree = build_Tree(toy_dataset)\nprint_Tree(this_tree)","6d2ef302":"''' Creating the classifier '''\n\ndef classify(row, node):\n    if isinstance(node, Leaf):\n        return node.predictions\n    if node.question.match(row):\n        return classify(row, node.true)\n    else:\n        return classify(row, node.false)","93f126ac":"classify(toy_dataset[0], this_tree)","7c926af5":"def print_l(counts):\n    \"\"\" Print the predictions at a leaf, in percentage \"\"\"\n    total = sum(counts.values()) * 1.0\n    probs = {}\n    for lbl in counts.keys():\n        probs[lbl] = str(int(counts[lbl] \/ total * 100)) + \"%\"\n    return probs\n\n# Example\nprint_l(classify(toy_dataset[1], this_tree))","85a91c7f":"# Now we use this self-made decision true on a test sample for prediction\n\ntest_data = [['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon'], ['Green', 3, 'Apple'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\n\nfor row in test_data:\n    print(\"Actual: %s. Predicted: %s\" % (row[-1], print_l(classify(row, this_tree))))","de023b7a":"## Decision Tree from scratch"}}