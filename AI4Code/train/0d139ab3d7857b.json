{"cell_type":{"532f5033":"code","120c11c2":"code","027d15bd":"code","f75f91d7":"code","c0d8c0a6":"code","045a2e18":"code","ff27d6a5":"code","6dd3be28":"code","333e53e6":"code","e5909771":"code","d11c4e7b":"code","c0430ae3":"code","1a1490f2":"code","b0e6606b":"code","801b8fed":"code","f506e14e":"markdown","493a9da4":"markdown","b62375e2":"markdown","714f9968":"markdown","16edcbb4":"markdown","b13cd112":"markdown","368c9f67":"markdown","850f9239":"markdown","4c121d18":"markdown","aacc5e4e":"markdown","5424f545":"markdown","905aec56":"markdown","edb2321a":"markdown","c58cb0bc":"markdown"},"source":{"532f5033":"#Aruchomu's original notebook is written in tensorflow v1\n!pip install gast==0.2.2\n!pip uninstall -y tensorflow\n!pip install tensorflow-gpu==1.14","120c11c2":"import numpy as np, pandas as pd, tensorflow as tf #For dataframe processing and Deep Learning\nfrom PIL import Image, ImageDraw, ImageFont        #'Pillow' library for image processing\nfrom IPython.display import display                #To display images in notebook\nfrom seaborn import color_palette                  #For bounding boxes colors\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\nprint(tf.__version__)","027d15bd":"''' DEFAULT YOLO HYPERPARAMS '''\n_BATCH_NORM_DECAY=0.9           #Momentum (Beta) value to compute weighted moving average\n_BATCH_NORM_EPSILON=1e-5        #Added in denominator to handle case of divide by 0\n#moving_avg= beta*moving_avg + (1-beta)*current_avg\n\n_LEAKY_RELU=0.1                 #Leakage (Alpha) value to calculate minimum instead of 0 as in regular RELU function\n#a=max(alpha*z,z)\n\n_ANCHORS=[(10,13),(16,30),(33,23),\n          (30,61),(62,45),(59,119),\n          (116,90),(156,198),(373,326)]\n                                #3 pairs of (anchor1, anchor2, anchor3)\n                                #total 9 anchors each of (anchorW, anchorH) values that'll be multiplied with exponent of bh, bw predicted values\n    \n_MODEL_SIZE=(416,416)           #Input size of model","f75f91d7":"def batch_norm(inputs,trainable,data_format):\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format=='channels_first' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n        scale=True, training=trainable\n    )\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    ''' \n        RESNET IMPLEMENTATION ON TENSORFLOW ALSO MENTIONS THIS\n        RETURNS A TENSOR WITH SAME FORMAT AS INPUT, BUT WITH PADDING\n    '''\n    pad_total = kernel_size - 1\n    pad_beg = pad_total \/\/ 2\n    pad_end = pad_total - pad_beg\n    if data_format == 'channels_first':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                        [pad_beg, pad_end],\n                                        [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                        [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n        inputs=inputs, filters=filters, kernel_size=kernel_size,\n        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n        use_bias=False, data_format=data_format)\n\n\n\ndef darknet53_residual_block(inputs, filters, trainable, data_format, strides=1):\n    ''' \n        input => [ CONV2D --> BN->LRELU --> CONV2D --> BN->LRELU ] => y+input \n    '''\n    residualConnection_input=inputs\n    \n    inputs=conv2d_fixed_padding(inputs,filters,1,data_format,strides)\n    inputs=batch_norm(inputs,trainable,data_format)    \n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    \n    inputs=conv2d_fixed_padding(inputs,filters*2,3,data_format,strides)\n    inputs=batch_norm(inputs,trainable,data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    return tf.add(inputs,residualConnection_input)","c0d8c0a6":"''' TESTING INDIVIDUAL COMPONENT FUNCTIONS \ntf.reset_default_graph()\nx=tf.placeholder(shape=(1,64,64,3),dtype=tf.float32)\n\nres=batch_norm(inputs=tf.to_float(x),trainable=False,data_format='channels_last')\nprint(res,\"\\n\") #1,7,7,3\n\nres=conv2d_fixed_padding(inputs=res,filters=10,kernel_size=3,data_format='channels_last')\nprint(res.shape) #1,7,7,10\n\n\nres=darknet53_residual_block(inputs=res,filters=5,trainable=False,data_format='channels_last')\nprint(res.shape)\n\nr1,r2,res2=darknet53(inputs=res,trainable=False,data_format='channels_last')\nprint(\"Without Eval res2=\",res2)\n\n\nip=np.random.randn(1,64,64,3)\nprint('ip.shape=',ip.shape)\n\nwith tf.Session() as s:\n    s.run(tf.global_variables_initializer())\n    s.run(res, feed_dict={x:ip})\n    print(\"After Eval res2=\",res)\n    '''","045a2e18":"def darknet53(inputs,trainable,data_format):\n    '''\n    C=Normal Conv block, R=Residual Block, layer xNum=Num occurences of layer\n    \n    C -> C -> R -> C -> R x2 -> C -> R x8 -> C -> R x8 -> C -> R x4\n    \n    '''\n    \n    inputs=conv2d_fixed_padding(inputs,filters=32, kernel_size=3,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    inputs=conv2d_fixed_padding(inputs,filters=32*2, kernel_size=3,strides=2, data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    inputs=darknet53_residual_block(inputs,filters=32,trainable=trainable,data_format=data_format)\n    #R\n    \n    inputs=conv2d_fixed_padding(inputs,filters=32*2*2,kernel_size=3,strides=2,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    for _ in range(2):\n        inputs=darknet53_residual_block(inputs,filters=32*2, trainable=trainable,data_format=data_format)\n        #R x2\n        \n    inputs=conv2d_fixed_padding(inputs,filters=32*2*2*2,kernel_size=3,strides=2,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    for _ in range(8):\n        inputs=darknet53_residual_block(inputs,filters=32*2*2, trainable=trainable, data_format=data_format)\n        #R x8\n        \n    route1=inputs\n    \n    inputs=conv2d_fixed_padding(inputs,filters=32*2*2*2*2,kernel_size=3,strides=2,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    \n    \n    for _ in range(8):\n        inputs=darknet53_residual_block(inputs,filters=32*2*2*2, trainable=trainable, data_format=data_format)\n        #R x8\n    route2=inputs\n    \n    inputs=conv2d_fixed_padding(inputs,filters=32*2*2*2*2*2,kernel_size=3,strides=2,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    #C\n    \n    for _ in range(4):\n        inputs=darknet53_residual_block(inputs,filters=32*2*2*2*2,trainable=trainable,data_format=data_format)\n        #R x4\n        \n    return route1,route2,inputs","ff27d6a5":"def yolo_conv_block(inputs, filters, trainable, data_format):\n    inputs=conv2d_fixed_padding(inputs,filters=filters, kernel_size=1,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n\n    inputs=conv2d_fixed_padding(inputs,filters=2*filters,kernel_size=3,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    \n    inputs=conv2d_fixed_padding(inputs,filters=filters, kernel_size=1,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n\n    inputs=conv2d_fixed_padding(inputs,filters=2*filters,kernel_size=3,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    \n    inputs=conv2d_fixed_padding(inputs,filters=filters, kernel_size=1,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n\n    route=inputs\n    \n    inputs=conv2d_fixed_padding(inputs,filters=2*filters,kernel_size=3,data_format=data_format)\n    inputs=batch_norm(inputs,trainable=trainable,data_format=data_format)\n    inputs=tf.nn.leaky_relu(features=inputs, alpha=_LEAKY_RELU)\n    \n    return route,inputs","6dd3be28":"def yolo_layer(inputs, n_classes, anchors, image_size, data_format):\n    '''\n        1)   EACH ANCHOR-BOX PREDICTION NEEDS TO BE OF LENGTH = (5 + numClasses)\n        1.1) HENCE SEMI-FINAL FEATURE MAP IS GENERATED OF DIMENSIONS = <m, W, H, numAnchors*(5 + numClasses)>\n        2)   SET grid_shape = <W,H>\n        3)   inputs NEEDS TO BE OF SHAPE = <m, W, H, numAnchors*(5 + numClasses)>\n        4)   inputs ARE RESHAPED TO < -1, (numAnchors*W*H), (5+numClasses) >\n        5)   SET strides = int(imgW\/W , imgH\/H)\n    '''\n    num_anchors=len(anchors)\n    inputs=tf.layers.conv2d(inputs=inputs, filters=num_anchors*(5+n_classes) ,\n                            kernel_size=1, strides=1, use_bias=True ,data_format=data_format)\n    \n    shape=inputs.get_shape().as_list() #get_shape() is used to return dynamic shape that is computed at runtime\n    \n    grid_shape=shape[2:4] if data_format=='channels_first' else shape[1:3]\n    \n    if data_format=='channels_first':\n        inputs=tf.transpose(inputs,perm=[0,2,3,1])\n    \n    inputs=tf.reshape(inputs,[ -1, num_anchors*grid_shape[0]*grid_shape[1] , 5+n_classes ])\n    \n    strides=(image_size[0]\/\/grid_shape[0] , image_size[1]\/\/grid_shape[1])\n    \n    \n    '''\n       6)   SPLIT UP inputs TO GIVE (x,y) box_centers COORDINATES, (xmax,ymax) box_shapes COORDINATES, OBJECTNESS confidence SCORE\n            AND classes ARRAY OF OBJECT IDENTIFIERS\n            box_centers will have shape = <m, numAnchors*H*W, 2>\n            box_shapes will have shape =  <m, numAnchors*H*W, 2>\n            confidence will have shape =  <m, numAnchors*H*W, 1>\n            classes will have shape =     <m, numAnchors*H*W, nClasses>\n       7)   SET x = [0,1,2,...W]\n            AND y = [0,1,2,...H]\n       8)   SET x_offset, y_offset = MESHGRID OF (x,y)\n       8.1) x_offset WILL HAVE SHAPE <H, W>\n            AND y_offset WILL HAVE SHAPE <H, W>\n       9)   RESHAPE x_offset TO HAVE SHAPE = <H*W, 1>\n            RESHAPE y_offset TO HAVE SHAPE = <H*W, 1>\n       10)  CONCAT x_offset, y_offset TO GET xy_offset: A SERIES OF (x,y) COORDINATES OF SHAPE = <H*W, 2>\n       11)  USE tf.tile TO REPEAT THE SERIES OF (x,y) COORDINATES numAnchor TIMES ALONG THE SECOND DIMENSION\n            xy_offset IS NOW OF SHAPE = <H*W, 2*3>   assuming anchors are 3\n       12)  RESHAPE xy_offset TO BE OF SHAPE = <1, H*W*3, 2>\n    '''\n    \n    \n    box_centers, box_shapes, confidence, classes = tf.split(inputs, [2,2,1,n_classes], axis=-1)\n    \n    \n    x=tf.range(grid_shape[0],dtype=tf.float32)  #range of width\n    y=tf.range(grid_shape[1],dtype=tf.float32)  #range of height\n    \n    x_offset,y_offset=tf.meshgrid(x,y)\n    x_offset=tf.reshape(x_offset,(-1,1))\n    y_offset=tf.reshape(y_offset,(-1,1))\n    \n    #x_offset, y_offset are flattened tensors\n    xy_offset=tf.concat([x_offset,y_offset],axis=-1) #(x,y) coordinate pairs from 0-W and 0-H respectively\n    xy_offset=tf.tile(xy_offset,[1,num_anchors])     #used to repeat xy_offset 'n_anchors' times along 2nd dimension\n    xy_offset=tf.reshape(xy_offset,[1,-1,2])         #(x,y) coordinate pairs from 0 to W and 0 to H respectively, repeated nAnchor times\n    \n    \n    '''\n       13)  SET (x,y) box_centers = sigmoid([x,y])\n       14)  SET box_centers = (box_centers + xy_offset) * strides\n       15)  USE tf.tile TO REPEAT THE SERIES OF anchors (H*W) TIMES ALONG THE FIRST DIMENSION\n       16)  SET box_shapes = e^box_shapes * anchors\n            ie: box_width = e^box_width * anchorWidth\n                box_height = e^box_height * anchorHeight\n       17)  SET confidence = sigmoid (confidence)\n       18)  SET classes = sigmoid(classes)\n       19)  RETURN [((box_centers + xy_offset) * strides)  ,  e^box_shapes * duplicated(anchors)  ,  sigmoid(confidence)  ,  sigmoid(classes)]\n    '''\n    \n    box_centers=tf.nn.sigmoid(box_centers)\n    box_centers=(box_centers+xy_offset)*strides\n    \n    anchors=tf.tile(anchors,[grid_shape[0]*grid_shape[1],1])\n    box_shapes=tf.exp(box_shapes) * tf.to_float(anchors)\n    \n    confidence=tf.nn.sigmoid(confidence)\n    \n    classes=tf.nn.sigmoid(classes)\n    \n    inputs=tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)\n    \n    return inputs","333e53e6":"def upsample(inputs,out_shape,data_format):\n    if data_format=='channels_first':\n        inputs=tf.transpose(inputs,[0,2,3,1])\n        new_height=out_shape[3]\n        new_width=out_shape[2]\n    else:\n        new_height=out_shape[2]\n        new_width=out_shape[1]\n        \n    inputs=tf.image.resize_nearest_neighbor(images=inputs, size=(new_height,new_width))\n    \n    if data_format=='channels_first':\n        inputs=tf.transpose(inputs,[0,3,1,2])\n    \n    return inputs","e5909771":"def build_boxes(inputs):\n    print(\"YOU ARE NOW BUILDING BOXES\")\n    print(inputs)\n    center_x, center_y, width, height, confidence, classes = tf.split(inputs, [1,1,1,1,1,-1],axis=-1)\n    print('center_x=', center_x)\n    print('center_y=', center_y)\n    print('width=', width)\n    print('height=', height)\n    print('confidence=', confidence)\n    print('classes=',classes)\n    topleft_x = center_x-(width\/2)\n    topleft_y = center_y-(height\/2)\n    bottomright_x=center_x+(width\/2)\n    bottomright_y=center_y+(height\/2)\n    \n    boxes=tf.concat([topleft_x,topleft_y,bottomright_x,bottomright_y,confidence,classes],axis=-1)\n    return boxes\n\n\n\ndef non_max_suppression(inputs, n_classes, max_output_size, iou_threshold, confidence_threshold):\n    '''\n    PERFORM NONMAX SUPPRESSION FOR EACH CLASS\n        max_output_size: Max number of boxes to be selected for each class\n    RETURN A LIST OF class-to-boxes DICTIONARIES FOR EACH SAMPLE IN THE BATCH\n    '''\n    batch=tf.unstack(inputs)   #for inputs of shape (Batchsize,B,C,D) returns list of unstacked tensors of shape (B,C,D)\n    boxes_dicts=[]\n    for boxes in batch:\n        boxes=tf.boolean_mask(boxes,boxes[:,4]>confidence_threshold) #internally creates a mask of condition, applies it to 'boxes' and returns values which satisfy True condition\n        classes=tf.argmax(boxes[:,5:],axis=-1)  #select the position of maximum value in array of classes\n        classes=tf.expand_dims(tf.to_float(classes),axis=-1) #add a dimension to the above 'classes' tensor at last dimension\n        boxes=tf.concat([boxes[:,:5],classes],axis=-1)\n        \n        boxes_dict=dict()\n        for cls in range(n_classes):\n            mask=tf.equal(boxes[:,5],cls)\n            mask_shape=mask.get_shape()\n            if mask_shape.ndims!=0:\n                class_boxes=tf.boolean_mask(boxes,mask)\n                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,[4,1,-1],axis=-1)\n                boxes_conf_scores=tf.reshape(boxes_conf_scores,[-1]) #flatten conf_scores completely\n                indices=tf.image.non_max_suppression(boxes=boxes_coords,scores=boxes_conf_scores,\n                                                     max_output_size=max_output_size,iou_threshold=iou_threshold)\n                class_boxes=tf.gather(class_boxes,indices)\n                boxes_dict[cls]=class_boxes[:,:5]\n                \n            \n        boxes_dicts.append(boxes_dict)\n    return boxes_dicts","d11c4e7b":"class Yolo_v3:\n    \n    def __init__(self, n_classes, model_size, max_output_size, iou_threshold, confidence_threshold, data_format=None):\n        '''\n           max_output_size IS THE MAX NUM OF BOXES TO BE SELECTED PER CLASS\n        '''\n        if not data_format:\n            if tf.test.is_gpu_available():\n                data_format='channels_first'\n            else:\n                data_format='channels_last'\n            \n        self.n_classes=n_classes\n        self.model_size=model_size\n        self.max_output_size=max_output_size\n        self.iou_threshold=iou_threshold\n        self.confidence_threshold=confidence_threshold\n        self.data_format=data_format\n        \n    \n    def __call__(self,inputs,trainable):\n        with tf.variable_scope('yolo_v3_model'):\n            if self.data_format=='channels_first':\n                inputs=tf.transpose(inputs,[0,3,1,2])  #bring the channels-dimension first, right after the batch-dimension\n            inputs=inputs\/255.\n            \n            route1,route2,inputs=darknet53(inputs=inputs, trainable=trainable, data_format=self.data_format)\n            \n            route,inputs=yolo_conv_block(inputs=inputs, filters=512, trainable=trainable, data_format=self.data_format)\n            \n            detect1=yolo_layer(inputs=inputs, n_classes=self.n_classes, anchors=_ANCHORS[6:9], image_size=self.model_size, data_format=self.data_format)\n            #finished 1st detection layer\n            \n            \n            inputs=conv2d_fixed_padding(inputs=route, filters=256, kernel_size=1, data_format=self.data_format)\n            inputs=batch_norm(inputs=inputs, trainable=trainable, data_format=self.data_format)\n            inputs=tf.nn.leaky_relu(features=inputs,alpha=_LEAKY_RELU)\n            \n            upsample_size=route2.get_shape().as_list()\n            inputs=upsample(inputs=inputs,out_shape=upsample_size,data_format=self.data_format)\n            \n            axis=1 if self.data_format=='channels_first' else 3  #just set a main variable 'axis' as either 1 or 3 based on 'data_format'\n            \n            inputs=tf.concat([inputs,route2],axis=axis)\n            route,inputs=yolo_conv_block(inputs=inputs, filters=256, trainable=trainable, data_format=self.data_format)\n            \n            detect2=yolo_layer(inputs=inputs, n_classes=self.n_classes, anchors=_ANCHORS[3:6], image_size=self.model_size, data_format=self.data_format)\n            #finished 2nd detection layer\n            \n            \n            inputs=conv2d_fixed_padding(inputs=route, filters=128, kernel_size=1, data_format=self.data_format)\n            inputs=batch_norm(inputs=inputs, trainable=trainable, data_format=self.data_format)\n            inputs=tf.nn.leaky_relu(features=inputs,alpha=_LEAKY_RELU)\n            \n            upsample_size=route1.get_shape().as_list()\n            inputs=upsample(inputs=inputs, out_shape=upsample_size, data_format=self.data_format)\n            \n            inputs=tf.concat([inputs,route1],axis=axis)\n            \n            route,inputs=yolo_conv_block(inputs=inputs, filters=128, trainable=trainable, data_format=self.data_format)\n            \n            detect3=yolo_layer(inputs=inputs, n_classes=self.n_classes, anchors=_ANCHORS[0:3], image_size=self.model_size, data_format=self.data_format)\n            #finished 3rd detection layer\n            \n            inputs=tf.concat([detect1,detect2,detect3],axis=1)\n            print(\"\\n\\nYOU ARE NOW GOING TO BUILD BOXES\\n\\n\")\n            inputs=build_boxes(inputs)\n            \n            boxes_dicts=non_max_suppression(inputs=inputs, n_classes=self.n_classes, max_output_size=self.max_output_size, iou_threshold=self.iou_threshold, confidence_threshold=self.confidence_threshold)\n            \n            return boxes_dicts","c0430ae3":"def draw_boxes(img_names,boxes_dicts,class_names,model_size):\n    colors = ((np.array(color_palette(\"hls\", len(class_names))) * 255)).astype(np.uint8)\n    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names, boxes_dicts):\n        img = Image.open(img_name)\n        draw = ImageDraw.Draw(img)\n        font = ImageFont.truetype(font='\/kaggle\/input\/data-for-yolo-v3-kernel\/futur.ttf', size=(img.size[0] + img.size[1]) \/\/ 100)\n        \n        resize_factor = (img.size[0] \/ model_size[0], img.size[1] \/ model_size[1])\n        for cls in range(len(class_names)):\n            boxes = boxes_dict[cls]\n            if tf.size(boxes) != 0:\n                color = colors[cls]\n                for box in boxes:\n                    xy, confidence = box[:4], box[4]\n                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n                    x0, y0 = xy[0], xy[1]\n                    \n                    thickness = (img.size[0] + img.size[1]) \/\/ 200\n                    for t in np.linspace(0, 1, thickness):\n                        xy[0], xy[1] = xy[0] + t, xy[1] + t          #adjusting top left x and y\n                        xy[2], xy[3] = xy[2] - t, xy[3] - t          #adjusting bottom right x and y\n                        draw.rectangle(xy, outline=tuple(color))\n                    \n                    text = '{} {:.1f}%'.format(class_names[cls], confidence * 100)\n                    \n                    text_size = draw.textsize(text, font=font)\n                    \n                    draw.rectangle( [x0, y0 - text_size[1], x0 + text_size[0], y0],   fill=tuple(color))\n                    \n                    draw.text((x0, y0 - text_size[1]), text, fill='black', font=font)\n        display(img)\n\n        \ndef load_images(img_names, model_size):\n    imgs=[]\n    for imgname in img_names:\n        img=Image.open(imgname)\n        img=img.resize(size=model_size)\n        img=np.array(img,dtype=np.float32)\n        img=np.expand_dims(img,axis=0)\n        imgs.append(img)\n    \n    imgs=np.concatenate(imgs)\n    return imgs\n\n\ndef load_class_names(filename):\n    with open(filename,'r') as f:\n        class_names=f.read().splitlines()\n    return class_names","1a1490f2":"def load_weights(variables,filename):\n    with open(filename,'rb') as f:\n        np.fromfile(f,dtype=np.int32,count=5)  #skip the first 5 lines\n        weights=np.fromfile(f,dtype=np.float32) #remainder of file is 'weights'\n        assign_operations=[]\n        pointer=0\n        \n        #Load weights for Darknet-53\n        #Each conv layer has batch normalization\n        for i in range(52):\n            conv_var=variables[5*i]\n            gamma, beta, mean, variance=variables[(5*i + 1) : (5*i + 5)]\n            batch_norm_variables=[beta, gamma, mean, variance]\n            \n            for var in batch_norm_variables:\n                shape=var.shape.as_list()\n                num_params=np.prod(shape)  #totalParams=shape[0]*shape[1]\n                var_weights=weights[pointer : pointer+num_params].reshape(shape)\n                pointer+=num_params\n                assign_operations.append(tf.assign(var,var_weights))\n                \n            shape=conv_var.shape.as_list()\n            num_params=np.prod(shape)\n            var_weights=weights[pointer : pointer+num_params].reshape((shape[3],shape[2],shape[0],shape[1]))\n            var_weights=np.transpose(var_weights,(2,3,1,0))\n            pointer+=num_params\n            assign_operations.append(tf.assign(conv_var,var_weights))\n            \n        #Load weights for the Yolo-layers\n        #7th , 15th and 23rd layers don't have Batch-norm layer, but have use_biases=True since we use the default Conv2D function\n        ranges=[range(0,6) , range(6,13) , range(13,20)]\n        unnormalized=[6,13,20]\n        \n        for j in range(3):\n            for i in ranges[j]: #for yolo-layers (0 -> 5)\n                current=52*5 + 5*i + j*2\n                conv_var=variables[current]\n                gamma, beta, mean, variance=variables[current+1 : current+5]\n                batch_norm_variables=[beta, gamma, mean, variance]\n                \n                for var in batch_norm_variables:\n                    shape=var.shape.as_list()\n                    num_params=np.prod(shape)\n                    var_weights=weights[pointer : pointer+num_params].reshape(shape)\n                    pointer+=num_params\n                    assign_operations.append(tf.assign(var,var_weights))\n                \n                shape=conv_var.shape.as_list()\n                #print(conv_var)\n                num_params=np.prod(shape)\n                var_weights=weights[pointer : pointer+num_params].reshape((shape[3],shape[2],shape[0],shape[1]))\n                var_weights=np.transpose(var_weights,(2,3,1,0))\n                pointer+=num_params\n                assign_operations.append(tf.assign(conv_var,var_weights))\n                \n            bias =   variables[52*5 + 5*unnormalized[j] + 2*j + 1]\n            shape=bias.shape.as_list()\n            num_params=np.prod(shape)\n            var_weights=weights[pointer : pointer+num_params].reshape(shape)\n            pointer+=num_params\n            assign_operations.append(tf.assign(bias,var_weights))\n            \n            conv_var=variables[52*5 + 5*unnormalized[j] + 2*j]\n            shape=conv_var.shape.as_list()\n            num_params=np.prod(shape)\n            var_weights=weights[pointer : pointer+num_params].reshape((shape[3],shape[2],shape[0],shape[1]))\n            var_weights=np.transpose(var_weights,(2,3,1,0))\n            pointer+=num_params\n            assign_operations.append(tf.assign(conv_var,var_weights))\n            \n    return assign_operations","b0e6606b":"images=['\/kaggle\/input\/data-for-yolo-v3-kernel\/office.jpg','\/kaggle\/input\/yolo-sample-images\/Manly_beach.jpg','..\/input\/yolo-sample-images\/Madame_Toussades-III.jpg']\nfor img in images:\n    #display(Image.open(img))\n    pass\nbatch_size = len(images)\nbatch = load_images(images, model_size=_MODEL_SIZE)\nclass_names = load_class_names('\/kaggle\/input\/data-for-yolo-v3-kernel\/coco.names')\nn_classes = len(class_names)\nmax_output_size = 10\niou_threshold = 0.5\nconfidence_threshold = 0.5\ntf.reset_default_graph()\nmodel = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n                max_output_size=max_output_size,\n                iou_threshold=iou_threshold,\n                confidence_threshold=confidence_threshold)\n\n#tf.compat.v1.disable_eager_execution()\n\ninputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\ndetections=model(inputs, False)\n\nmodel_vars=tf.global_variables(scope='yolo_v3_model')\nassign_ops=load_weights(model_vars,'\/kaggle\/input\/data-for-yolo-v3-kernel\/yolov3.weights')\nprint(\"\\n\\n\")\n\nwith tf.Session() as s:\n    s.run(assign_ops)\n    computed_detections_from_tensors=s.run(detections, feed_dict={inputs:batch})\ncomputed_detections_from_tensors","801b8fed":"draw_boxes(images,computed_detections_from_tensors,class_names,_MODEL_SIZE)","f506e14e":"# DETECTION LAYERS\nTo actually predict bounding boxes and class with respect to anchors.\n\nThe actual authors of Yolo implemented detection layers at 3 scales.\n\nAfter performing a final Convolution of inputs to give a tensor containing box-center and box-shape info, we need to scale\/transform these values so that they can be plotted against real images. Hence you'll see the x_offset, y_offset coming into the picture.\n\n\n![image.png](attachment:image.png)\n\n> inputs Shape = <m, W, H, (numAnchors)(5 + numClasses)>\n\n> grid_shape   = <m, W, H>","493a9da4":"# OVERALL YOLO MODEL\nNow we'll use all of the above predefined modular components.\n\n### Reminder of Yolo-v3 network structure:\n![Yolo-v3 network structure](https:\/\/www.researchgate.net\/publication\/335865923\/figure\/fig1\/AS:804106595758082@1568725360777\/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg)","b62375e2":"# DEFAULT INITIAL CONFIGURATIONS\n\n### 1) BATCH NORMALIZATION\nBatch Normalization helps model converge faster by stabilizing distribution of input to current layer.\n\n### 2) LEAKY RELU\nUsed instead of RELU to avoid \"neuron dying\" when many activations become 0\n\n### 3) ANCHORS\n3 anchors used for each detection layer at one specific scale (as per original Yolo implementation)\n\n### 4) MODEL SIZE\nInputs of the model needs to be a 416x416 image","714f9968":"![image.png](attachment:image.png)\n\n\nReshape this 'inputs' dataframe\n> inputs new Shape = <m, (nAnchors)(W)(H), (5+nClasses)>\n\n> strides = (ImgSizeW\/W , ImageH\/H)\n\n'strides' acts as a rescaling factor","16edcbb4":"# UPSAMPLE LAYER\nTo concat with the shortcut output of Darknet-53 before applying detection at a different scale, we need to upsample the feature map.\n\nThis standalone function will come in handy when we're trying to concat main-input with earlier skip-connection, in order to pass the combination to a detection layer.","b13cd112":"# DRAWING BOXES USING PREDICTIONS\n\ndraw_boxes() uses params: input image_paths, boxes_dictionaries got from model's predictions, static class-names, and default image-size\n\n* For each class in the list of class-names:\n\n    * Extract the boxes predicted for that class from the boxes_dictionary that model predicted\n    * If boxes are predicted:\n        * From each box extract array of coordinates xy (that we got from build_boxes() function) , and confidence-score\n        > xy will be list of coordinates [topleftx, toplefty, bottomrightx, bottomrighty]\n        * Scale each of these co-ordinates using resizing factor (imageW\/W , imageH\/H)\n        * Calculate 't' values of thickness between 0,1 using np.linspace()\n        > for example the 't' values could be [0, 0.25, 0.5, 0.75, 1.0] if thickness=2\n        * Draw rectangles using these 't' values to mimic thick dark lines\n        * Print the text using some simple draw functions","368c9f67":"![image.png](attachment:image.png)\n\n> box_centers Shape = <m, (nAnchors)(W)(H), 2>\n\n> box_shapes Shape  = <m, (nAnchors)(W)(H), 2>\n\n> confidence Shape  = <m, (nAnchors)(W)(H), 1>\n\n> classes Shape     = <m, (nAnchors)(W)(H), nClassses>\n\nAll of these 4 tensors are extracted from the 3rd dimension of inputs: ie along the vertical edge in above image\n\nThe final 'xy_offset' is a list of (x,y) coordinate pairs from [0 to W] and [0 to H] respectively, repeated nAnchor times. It's essentially a list of grid points repeated nAnchor times.\n\n> xy_offset Shape = <1, (numAnchors)(W)(H), 2>\n\nbox_centers = (box_centers+xy_offset)* strides\n\nanchors = duplicate([(W)(H),1])\n> anchors Shape = <1, (numAnchors)(W)(H), 2 >\n\nbox_shapes = e^box_shapes * anchors\n> box_shapes =    <m,(numAnchors)(W)(H), 2>","850f9239":"# OVERALL DARKNET-53 MODEL\nNow that we have our standalone residual block's code set up, we can create the 53 layered Darknet-53.\n\nWe'll in fact be creating 52 layers since the final [AvgPool -> FullyConnected] layer is not required for detection. We'll be appending custom yolo-layers instead.\n\n![Darknet-53 high level architecture](https:\/\/miro.medium.com\/max\/792\/1*7u6XWGYl7lLgc0EcKG1NMw.png)\n\nWe'll later be importing the weights of Darknet-53 that the original paper's authors had trained on ImageNet.","4c121d18":"# FINALLY RUN THE MODEL ***Phew***\n\n### NOTES-\n* If you don't use tf.reset_default_graph() you might get an error with global-variable name issues: if you've executed code blocks arbitrarily thereby creating a tensorflow graph already","aacc5e4e":"# NON-MAX SUPPRESSION\nThis is a key feature in the Yolo algorithm to get rid of prediction boxes with confidence_score < threshold, and other boxes for the same object.\n\nCreate a modular function to extract top-leftmost (x,y) co-ordinates, and the bottom-rightmost (x,y) co-ordinates.","5424f545":"# INTRODUCTION\nThis notebook is inspired from [Aruchomu's YOLO v3 Object Detection notebook](https:\/\/www.kaggle.com\/aruchomu\/yolo-v3-object-detection-in-tensorflow)\n\nI've been wanting to implement Yolo in an interactive hands-on manner after completing Andrew Ng's course, and I intend to guide you through this simple yet ground-breaking methodology.\n\nLater, in a future notebook, I'll try to predict bounding boxes on a real problem statement, possibly [KBhartiya's Swimming Pool and Car Detection dataset](https:\/\/www.kaggle.com\/kbhartiya83\/swimming-pool-and-car-detection) using Tensorflow v2.\n\n![](https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2.png)\n\n### Yolo-v3 network structure:\n![Yolo-v3 network structure](https:\/\/www.researchgate.net\/publication\/335865923\/figure\/fig1\/AS:804106595758082@1568725360777\/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg)\n\n\n### Notes- \n* Refer to this notebook for a walkthrough of in-depth code functionality. I feel Tensorflow v2 will be slightly more abstract.\n* Aruchomu's notebook was implemented in Tensorflow version 1.12.0\n* If we simply try to backtrack to this version, the GAST library breaks down due to compatibility issues in internal Abstract Syntax Trees.\n* Just follow the below cell and you should be fine!","905aec56":"# LOAD MODEL-WEIGHTS\nNow that we have a model skeleton, we can load the official Yolo v3 model-weights.\n\nFlowchart of how weights are stored in file:\n![Flowchart of how weights are stored in file](https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/weights.jpg)","edb2321a":"# SUBSEQUENT YOLO RELATED CONV LAYERS\nYOLO requires a set of conv layers after the base Darknet-53 model, to process the features extracted and map them to bounding boxes (localization) and object classes (identification).\n\n![image.png](attachment:image.png)","c58cb0bc":"# DARKNET-53 COMPONENTS\nThe original paper uses Darknet-53 model, and we'll use the same for extracting features of the image.\n\nWe'll create modular components of the entire network one by one.\n\nDarknet-53 is similar to ResNet: It uses residual-connections to flow lower layer information to flow into current layer.\nThis helps prevent vanishing gradient, and also these connections don't affect the identity function of the entire layer.\n\n### Notes-\n* The fixed_padding() function below, is a pre-requisite for padding input feature-maps, and is not entirely the same as padding='SAME' in the conv2d layer! \n* It'll be used to perform padding, when input feature-map needs to be convolved with a stride>1 ."}}