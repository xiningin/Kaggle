{"cell_type":{"1c7d21e4":"code","f2745842":"code","66aa2833":"code","a8db343d":"code","31934b23":"code","9a310292":"code","477dab1b":"code","5ddb810a":"code","cd2e5eb1":"code","623ff559":"code","e3f662cf":"markdown","48cd5643":"markdown","c33384af":"markdown","5235dc10":"markdown","6f4ddb56":"markdown","e9ee7683":"markdown"},"source":{"1c7d21e4":"!pip install mglearn","f2745842":"import mglearn\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,train_test_split,KFold,ShuffleSplit,GroupKFold,LeaveOneOut\nfrom sklearn.datasets import load_iris,make_blobs\nfrom sklearn.linear_model import LogisticRegression","66aa2833":"logreg = LogisticRegression(max_iter=1000)","a8db343d":"mglearn.plots.plot_stratified_cross_validation()","31934b23":"kfold = KFold(n_splits=3)\niris = load_iris()\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))","9a310292":"loo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))","477dab1b":"mglearn.plots.plot_shuffle_split()","5ddb810a":"iris = load_iris()\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))","cd2e5eb1":"mglearn.plots.plot_group_kfold()","623ff559":"# create synthetic dataset\nX, y = make_blobs(n_samples=12, random_state=0)\n# assume the first three samples belong to the same group,\n# then the next four, etc.\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\nscores = cross_val_score(logreg, X, y, groups=groups, cv=GroupKFold(n_splits=3))\nprint(\"Cross-validation scores:\\n{}\".format(scores))","e3f662cf":"# ShuffleSplit","48cd5643":"# LeaveOneOut","c33384af":"Difference in KFold and ShuffleSplit output\n\nKFold will divide your data set into prespecified number of folds, and every sample must be in one and only one fold. A fold is a subset of your dataset.\n\nShuffleSplit will randomly sample your entire dataset during each iteration to generate a training set and a test set. The test_size and train_size parameters control how large the test and training test set should be for each iteration. Since you are sampling from the entire dataset during each iteration, values selected during one iteration, could be selected again during another iteration.\n\n## Summary: ShuffleSplit works iteratively, KFold just divides the dataset into k folds.\n\nDifference when doing validation\n\nIn KFold, during each round you will use one fold as the test set and all the remaining folds as your training set. However, in ShuffleSplit, during each round n you should only use the training and test set from iteration n. As your data set grows, cross validation time increases, making shufflesplits a more attractive alternate. If you can train your algorithm, with a certain percentage of your data as opposed to using all k-1 folds, ShuffleSplit is an attractive option.","5235dc10":"# KFold","6f4ddb56":"# GroupKFold","e9ee7683":"# Common Model"}}