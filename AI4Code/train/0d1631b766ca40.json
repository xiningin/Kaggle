{"cell_type":{"b9229eb0":"code","9b1c1115":"code","7800da48":"code","d76a4ed0":"code","318201bb":"code","abefee99":"code","ebc68aba":"code","4adecb49":"code","702e190a":"code","86e7fee0":"code","c4e23ab2":"code","cf1a970b":"code","2eebd25c":"code","f1a8aa45":"code","eaf616f7":"code","f5e0371e":"code","445a9374":"markdown","49df9c80":"markdown","3d43c1bb":"markdown","f71d5bd4":"markdown","08df60a0":"markdown","11417e9b":"markdown","a5c0692f":"markdown","c8ff1600":"markdown","24f408fe":"markdown"},"source":{"b9229eb0":"!pip install efficientnet","9b1c1115":"import re\nimport math\nimport random\nimport numpy as np \nimport pandas as pd \nimport os\nimport shutil\nimport json\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras.backend as K\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom efficientnet.keras import EfficientNetB1 as EfficientNet\nimport gc\nprint(\"Using TensorFlow version %s\" % tf.__version__)","7800da48":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","d76a4ed0":"data_path = KaggleDatasets().get_gcs_path(\"cassava-leaf-disease-classification\")\ndata_path","318201bb":"VALIDATION_SIZE = 0.2\nBATCH_SIZE = 16 * REPLICAS\nORIGINAL_WIDTH = 800\nORIGINAL_HEIGHT = 600\nIMG_SIZE = 600\nCHANNELS = 3\nN_CLASSES = 5\n\nn_models = 5","abefee99":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=CHANNELS)\n    image = (tf.cast(image, tf.float32) \/ 255.0)\n    image = tf.image.resize(image, [ORIGINAL_HEIGHT, ORIGINAL_WIDTH])\n    image = tf.reshape(image, [ORIGINAL_HEIGHT, ORIGINAL_WIDTH , CHANNELS])\n    return image\n\ndef normalize(x):\n    x = tf.image.resize(x, [ORIGINAL_HEIGHT, ORIGINAL_WIDTH])\n    x = tf.reshape(x, [ORIGINAL_HEIGHT, ORIGINAL_WIDTH, CHANNELS])\n    return x","ebc68aba":"def read_tfrecord(example):\n    TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string), \n        'target': tf.io.FixedLenFeature([], tf.int64), \n    }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    return decode_image(example['image']), tf.cast(example['target'], tf.int32)\n\ndef load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(lambda x: read_tfrecord(x), num_parallel_calls=AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","4adecb49":"@tf.function\ndef data_aug(x: tf.Tensor) -> tf.Tensor:\n    print(\"lol\",x, \"lol\")\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.2, lambda: tf.image.random_crop(x, [int(ORIGINAL_HEIGHT*0.8), int(ORIGINAL_WIDTH*0.8), 3]), lambda: x)\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.1, lambda: tf.image.random_flip_left_right(x), lambda: x)\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.1, lambda: tf.image.random_flip_up_down(x), lambda: x)\n    \n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.7, lambda: tf.image.random_saturation(x, 0.6, 1.6), lambda: x)\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.7, lambda: tf.image.random_brightness(x, 0.05), lambda: x)\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.7, lambda: tf.image.random_contrast(x, 0.7, 1.3), lambda: x)\n    x = tf.cond(tf.random.uniform([], 0, 1) > 0.5, lambda: tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)), lambda: x)\n    x = normalize(x)\n    print(\"lol2\",x, \"lol2\")\n    return x","702e190a":"DATASET_FILES = tf.io.gfile.glob(data_path + '\/train_tfrecords\/ld_train*.tfrec')\ndataset_size = count_data_items(DATASET_FILES)\ndataset = load_dataset(DATASET_FILES)\ndataset = dataset.shuffle(2048).cache()\nsplit = int(dataset_size * 0.2)\ntrain_size = int(dataset_size * 0.8)\n\nprint(F\"Dataset size: {dataset_size}, split: {split}\")","86e7fee0":"!pip install efficientnet","c4e23ab2":"def n_fold_dataset(augment = True, validation=True, train=True, index=0):\n    validation_start = split * index\n    validation_end = validation_start + split\n    if validation:\n        validation_dataset = dataset.skip(validation_start)\n        validation_dataset = validation_dataset.take(split)\n        validation_dataset = validation_dataset.map(lambda x,y: (normalize(x), y))\n        validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO).cache().repeat()\n    \n    if train:\n        train_dataset_1 = dataset.take(validation_start)\n        train_dataset_2 = dataset.skip(validation_end)\n        train_dataset_2 = train_dataset_2.take(dataset_size - validation_end)\n        train_dataset = train_dataset_1.concatenate(train_dataset_2).cache()\n        train_dataset = train_dataset.map(lambda x, y: (data_aug(x), y), num_parallel_calls=AUTO)\n        train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(AUTO).repeat()\n    \n    if not train:\n        return validation_dataset\n    if not validation:\n        return train_dataset\n    return train_dataset, validation_dataset","cf1a970b":"models = []\nwith strategy.scope():\n    for i in range(n_models):\n        inputs = layers.Input(shape=(ORIGINAL_HEIGHT, ORIGINAL_WIDTH, 3))\n        \n        model = Sequential([\n            EfficientNet(include_top=False,weights='imagenet', input_tensor=inputs, drop_connect_rate=0.3),\n            layers.GlobalAveragePooling2D(),\n            layers.BatchNormalization(),\n            layers.Dropout(0.3),\n            layers.Dense(5, activation=\"softmax\")\n        ])\n        model.compile(loss=losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n        models.append(model)\n        \nmodels[0].summary()","2eebd25c":"def get_callbacks(i):   \n    \n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2,\n                              verbose=1, mode='auto', min_delta=0.00001,\n                              cooldown=0, min_lr=0)\n\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 8 , verbose = 1, restore_best_weights = True)\n    model_cp = ModelCheckpoint(F'EfficientNetB3_tl_best_weights_{i}.h5', \n                                 save_best_only = True, \n                                 save_weights_only = True,\n                                 monitor = 'val_loss', \n                                 mode = 'min', verbose = 1)\n    \n    return [early_stopping, model_cp, reduce_lr]","f1a8aa45":"histories = []\nfor i, model in enumerate(models):\n    print(F\"Training model: {i}\")\n    train_dataset, validation_dataset = n_fold_dataset(augment = True, index=i)\n\n    history = model.fit(train_dataset,\n          steps_per_epoch=train_size\/\/BATCH_SIZE,\n          validation_steps=split\/\/BATCH_SIZE,\n          validation_data=validation_dataset,\n          batch_size=BATCH_SIZE, epochs=100, \n          callbacks=get_callbacks(i))\n    histories.append(history)\n    gc.collect()","eaf616f7":"accuracies = []\nfor i in range(n_models):\n    validation_dataset = n_fold_dataset(train=False, index=i)\n    accuracies.append(model.evaluate(validation_dataset, steps=split\/\/BATCH_SIZE)[1])\n        \nprint(\"Accuracy:\", sum(accuracies) \/ len(accuracies) )","f5e0371e":"fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(16, 12))\n\nfor i, history in enumerate(histories):\n    history_df = pd.DataFrame(history.history)\n    history_df[['loss', 'val_loss']].plot(ax=axes[i,0])\n    history_df[['accuracy', 'val_accuracy']].plot(ax=axes[i,1])","445a9374":"# N fold cross dataset","49df9c80":"# Hardware configuration","3d43c1bb":"# Loading work path","f71d5bd4":"# Parameters","08df60a0":"# Plot all models","11417e9b":"Data augmentation","a5c0692f":"# Loading transfert learning EfficientNet model","c8ff1600":"# Dependencies","24f408fe":"# Utils"}}