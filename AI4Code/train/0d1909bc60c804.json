{"cell_type":{"80cf6df1":"code","7e6ac1e4":"code","927b44dd":"code","63f0a673":"code","e26cc919":"code","c8df3f3f":"code","353b23ac":"code","4d4d103e":"code","cb8666ad":"code","7879fd9a":"code","20d7b2dc":"code","241b5a09":"code","42a0bcf6":"code","d0422ec3":"code","4e0d2f1b":"code","5abacc23":"code","c18b7833":"code","596128b0":"code","5046575e":"code","ae26b47f":"code","e01d2d35":"code","16114b28":"code","d868810c":"code","d29b3352":"code","f724d674":"code","fb684963":"code","08ebb252":"code","92b6bfb4":"code","ff8a7695":"code","27e0ba6a":"code","0f075598":"code","20dc4bef":"code","7fb80ab6":"code","0da25739":"code","6d6119fc":"code","5ec98345":"code","b58fce8e":"code","21b478d0":"code","ccc6209d":"code","5c8494a6":"markdown","b3173ee9":"markdown","5976e322":"markdown","d4b7d9c1":"markdown","7d5f7dda":"markdown","8e205384":"markdown","50e574fa":"markdown","49a8a26e":"markdown","bf302c62":"markdown","c1fef495":"markdown","7c40e29a":"markdown","273915fc":"markdown","be10eb7d":"markdown","0c080961":"markdown","23bd49d5":"markdown","4953bce1":"markdown","9a463df2":"markdown","a86ef894":"markdown","c61b901f":"markdown","ba19c51f":"markdown","1e10e22e":"markdown","b51e4d8b":"markdown","90f5a61a":"markdown","544c9596":"markdown","a7d19a28":"markdown","cf49a8a8":"markdown","d2358617":"markdown","44a82dce":"markdown","e54d5b99":"markdown","f9e617f1":"markdown","9e0eea19":"markdown","4a2ddc24":"markdown","42dd1747":"markdown","3ff369ba":"markdown","51dbded9":"markdown","bdc1187d":"markdown","b85ff896":"markdown"},"source":{"80cf6df1":"import os\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn","7e6ac1e4":"def autolabel(arrayA):\n    ''' label each colored square with the corresponding data value. \n    If value > 20, the text is in black, else in white.\n    '''\n    arrayA = np.array(arrayA)\n    for i in range(arrayA.shape[0]):\n        for j in range(arrayA.shape[1]):\n                plt.text(j,i, \"%.2f\"%arrayA[i,j], ha='center', va='bottom',color='w')\n\ndef hist_it(feat):\n    '''Plot histogram of features with label'''\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.8)\n    feat[Y==1].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.5)\n    plt.ylim((0,1))\n    \ndef gt_matrix(feats,sz=16):\n    '''Plot heatmap of features with values greater than other features'''\n    a = []\n    for i,c1 in enumerate(feats):\n        b = [] \n        for j,c2 in enumerate(feats):\n            mask = (~train[c1].isnull()) & (~train[c2].isnull())\n            if i>=j:\n                b.append((train.loc[mask,c1].values>=train.loc[mask,c2].values).mean())\n            else:\n                b.append((train.loc[mask,c1].values>train.loc[mask,c2].values).mean())\n\n        a.append(b)\n\n    plt.figure(figsize = (sz,sz))\n    plt.imshow(a, interpolation = 'None')\n    _ = plt.xticks(range(len(feats)),feats,rotation = 90)\n    _ = plt.yticks(range(len(feats)),feats,rotation = 0)\n    autolabel(a)","927b44dd":"def hist_it(feat):\n    '''Plot histogram with 100 bins'''\n    plt.figure(figsize=(16,4))\n    feat[Y==0].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    feat[Y==1].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5)\n    plt.ylim((0,1))","63f0a673":"!ls ..\/input","e26cc919":"PATH = '..\/input\/'","c8df3f3f":"train = pd.read_csv(PATH + 'train.csv.zip')\n\n# reduce size of data to prevent kernel crashes\nSAMPLE_SIZE = 1000\nrand_idx = np.random.randint(0, len(train), size=SAMPLE_SIZE)\ntrain = train.iloc[rand_idx,]\ntrain.shape, train.head()\n\nY = train.target\nY.head()","353b23ac":"# Loading too much data might cause the kerenel to crash\n# Convert this to a code cell if you want to risk it\ntest = pd.read_csv(PATH + 'test.csv.zip')\n\n# reduce size of data to prevent kernel crashes\nrand_idx = np.random.randint(0, len(test), size=SAMPLE_SIZE)\ntest = test.iloc[rand_idx,]\ntest_ID = test.ID\ntest_ID.head()","4d4d103e":"import gc; gc.collect()","cb8666ad":"train.shape, train.head()","7879fd9a":"# Loading too much data might cause the kerenel to crash\n# Convert this to a code cell if you want to risk it\ntest.shape, test.head()","20d7b2dc":"# Number of NaNs for each object\ntrain.isnull().sum(axis=1).head(12)","241b5a09":"# Number of NaNs for each column\ntrain.isnull().sum(axis=0).head(12)","42a0bcf6":"# combining datasets seems to crash the kernel\ntraintest = pd.concat([train, test], axis = 0)\ntraintest.shape","d0422ec3":"# `dropna = False` makes nunique treat NaNs as a distinct value\nfeats_counts = train.nunique(dropna = False)\nfeats_counts.sort_values()[:10]","4e0d2f1b":"constant_features = feats_counts.loc[feats_counts==1].index.tolist()\nprint (constant_features)\n\n# Loading too much data might cause the kerenel to crash\ntraintest = traintest.drop(constant_features, axis = 1)\ntraintest.shape","5abacc23":"traintest = traintest.fillna('NaN')\ntraintest.head()","c18b7833":"train_enc =  pd.DataFrame(index = train.index)\n\nfor col in tqdm_notebook(traintest.columns):\n    train_enc[col] = train[col].factorize()[0]\n\ntrain_enc.shape, train_enc.head()","596128b0":"# train_enc[col] = train[col].map(train[col].value_counts())","5046575e":"dup_cols = {}\n\nfor i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n    for c2 in train_enc.columns[i + 1:]:\n        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n            dup_cols[c2] = c1","ae26b47f":"dup_cols.items()","e01d2d35":"# might have to install cPickel\n#!pip install cPickle\n#import cPickle as pickle\n#pickle.dump(dup_cols, open('dup_cols.p', 'w'), protocol=pickle.HIGHEST_PROTOCOL)","16114b28":"traintest = traintest.drop(dup_cols.keys(), axis = 1)\ntraintest.shape, traintest.head()","d868810c":"nunique = train.nunique(dropna=False)\nnunique[:10]","d29b3352":"plt.figure(figsize=(14,4))\nplt.hist(nunique.astype(float)\/train.shape[0], bins=80, orientation='horizontal');","f724d674":"mask = (nunique.astype(float)\/train.shape[0] > 0.8)\ntrain.loc[:train.index[5], mask]","fb684963":"train_idx_orig = train.index\ntrain = train.reset_index(drop=True)\nY = Y.reset_index(drop=True)\ntrain.head(), Y.head()","08ebb252":"mask = (nunique.astype(float)\/train.shape[0] < 0.8) & (nunique.astype(float)\/train.shape[0] > 0.4)\ntrain.loc[:10, mask]","92b6bfb4":"train['VAR_0015'].value_counts()","ff8a7695":"cat_cols = list(train.select_dtypes(include=['object']).columns)\nnum_cols = list(train.select_dtypes(exclude=['object']).columns)","27e0ba6a":"train = train.replace('NaN', -999)","0f075598":"# select first few numeric features\nfeats = num_cols[:32]\n\n# build 'mean(feat1 > feat2)' plot\ngt_matrix(feats,16)","20dc4bef":"hist_it(train['VAR_0002'])\nplt.ylim((0,0.05))\nplt.xlim((-10,1010));","7fb80ab6":"hist_it(train['VAR_0003'])\nplt.ylim((0,0.03))\nplt.xlim((-10,1010));","0da25739":"train['VAR_0002'].value_counts()[:10]","6d6119fc":"train['VAR_0003'].value_counts()[:10]","5ec98345":"train['VAR_0004_mod50'] = train['VAR_0004'] % 50\nhist_it(train['VAR_0004_mod50'])\nplt.ylim((0,0.6))","b58fce8e":"train.loc[:,cat_cols].head().T","21b478d0":"date_cols = [u'VAR_0073','VAR_0075',\n             u'VAR_0156',u'VAR_0157',u'VAR_0158','VAR_0159',\n             u'VAR_0166', u'VAR_0167',u'VAR_0168',u'VAR_0169',\n             u'VAR_0176',u'VAR_0177',u'VAR_0178',u'VAR_0179',\n             u'VAR_0204',\n             u'VAR_0217']\n\nfor c in date_cols:\n    train[c] = pd.to_datetime(train[c],format = '%d%b%y:%H:%M:%S')\n    test[c] = pd.to_datetime(test[c],  format = '%d%b%y:%H:%M:%S')","ccc6209d":"c1 = 'VAR_0217'\nc2 = 'VAR_0073'\n\n# mask = (~test[c1].isnull()) & (~test[c2].isnull())\n# sc2(test.ix[mask,c1].values,test.ix[mask,c2].values,alpha=0.7,c = 'black')\n\nmask = (~train[c1].isnull()) & (~train[c2].isnull())\nplt.figure(figsize=(14,4))\nplt.scatter(train.loc[mask,c1].values,train.loc[mask,c2].values, c=train.loc[mask,'target'].values, \n            alpha=.5);","5c8494a6":"Let's take a looks at the features with a huge number of unique values:","b3173ee9":"We found 5 constant features. Let's remove them.","5976e322":"First we schould look for a constant features, such features do not provide any information and only make our dataset larger. ","d4b7d9c1":"Our conclusion: there are no floating point variables, there are some counts variables, which we will treat as numeric. \n\nAnd finally, let's pick one variable (in this case 'VAR_0015') from the third group of features.","7d5f7dda":"and build a histogram of those values","8e205384":"# Dataset cleaning","50e574fa":"Let's take a look at categorical features we have.","49a8a26e":"We see that one date is strictly greater than the other, so the difference between them can be a good feature. Also look at horizontal line there -- it also looks like NaN, so I would rather create a new binary feature which will serve as an idicator that our time feature is NaN.","bf302c62":"## Remove duplicated features","c1fef495":"There are almost 2000 anonymized variables! It's clear, some of them are categorical, some look like numeric. Some numeric feateures are integer typed, so probably they are event conters or dates. And others are of float type, but from the first few rows they look like integer-typed too, since fractional part is zero, but pandas treats them as `float` since there are NaN values in that features.   \n\nFrom the first glance we see train has one more column `target` which we should not forget to drop before fitting a classifier. We also see `ID` column is shared between train and test, which sometimes can be succesfully used to improve the score.","7c40e29a":"Fill NaNs with something we can find later if needed.","273915fc":"Don't forget to save them, as it takes long time to find these.","be10eb7d":"# Springleaf EDA\nNotebook used in data exploration section of Coursera course on competitive data science: \n- https:\/\/www.coursera.org\/learn\/competitive-data-science\/lecture\/nLD7Y\/springleaf-competition-eda-i","0c080961":"## Remove constant features","23bd49d5":"Let's examine the number of unique values.","4953bce1":"`VAR_0200`, `VAR_0237`, `VAR_0274` look like some georgraphical data thus one could generate geography related features, we will talk later in the course.\n\nThere are some features, that are hard to identify, but look, there a date columns `VAR_0073` -- `VAR_0179`, `VAR_0204`, `VAR_0217`. It is useful to plot one date against another to find relationships. ","9a463df2":"Now let's encode each feature, as we discussed. ","a86ef894":"Indeed, we see interesting patterns here. There are blocks of geatures where one is strictly greater than the other. So we can hypothesize, that each column correspondes to cumulative counts, e.g. feature number one is counts in first month, second -- total count number in first two month and so on. So we immediately understand what features we should generate to make tree-based models more efficient: the differences between consecutive values.","c61b901f":"We see there is something special about 12, 24 and so on, sowe can create another feature x mod 12. ","ba19c51f":"These look like counts too. First thing to notice is the 23th line: 99999.., -99999 values look like NaNs so we should probably built a related feature. Second: the columns are sometimes placed next to each other, so the columns are probably grouped together and we can disentangle that.      ","1e10e22e":"It is also useful to know if there are any NaNs in the data. You should pay attention to columns with NaNs and the number of NaNs for each row can serve as a nice feature later.","b51e4d8b":"All 1932 columns are anonimized which makes us to deduce the meaning of the features ourselves. We will now try to clean the dataset. \n\nIt is usually convenient to concatenate train and test into one dataframe and do all feature engineering using it.","90f5a61a":"Drop from traintest.","544c9596":"## VAR_0004","a7d19a28":"Probably the first thing you check is the shapes of the train and test matrices and look inside them.","cf49a8a8":"Just by reviewing the head of the lists we immediately see the patterns, exactly 56 NaNs for a set of variables, and 24 NaNs for objects. ","d2358617":"# Categorical features","44a82dce":"# Go through some features","e54d5b99":"The values are not float, they are integer, so these features are likely to be even counts. Let's look at another pack of features.","f9e617f1":"We could also do something like this:","9e0eea19":"# Read the data","4a2ddc24":"# Data overview","42dd1747":"Let's calculate how many times one feature is greater than the other and create cross tabel out of it. ","3ff369ba":"## Determine data types","51dbded9":"## VAR_0002, VAR_0003 ","bdc1187d":"Let's replace NaNs with something first.","b85ff896":"The resulting data frame is very very large, so we cannot just transpose it and use .duplicated. That is why we will use a simple loop."}}