{"cell_type":{"fdadd133":"code","7b07740d":"code","9c35ff70":"code","a491accd":"code","cb0f6fd3":"code","af7e224a":"code","7965538b":"code","85559ae9":"code","d068adba":"code","e32f5f67":"code","fda001ef":"code","18d1d531":"code","76491530":"code","462303d2":"code","558098f4":"code","e7024516":"code","2f66a5a2":"code","e13e1092":"code","bcc9d338":"code","48544b14":"code","0f70113f":"code","68ee71fc":"code","129d8f55":"code","44afd18a":"code","35216eea":"code","ecf1585c":"code","6970c472":"code","59ece44d":"code","863def40":"code","69666a97":"code","f06cf475":"code","cbccd431":"code","2d357b9a":"code","761de643":"code","2b268e58":"code","e6a11014":"code","d3e81495":"code","3900bb80":"code","6271227e":"markdown","7109513a":"markdown","26d50f15":"markdown","2f517f7c":"markdown","ce1b9bbb":"markdown","290db7d7":"markdown","f1b3e3bb":"markdown","03543214":"markdown","667ac353":"markdown","fd01a646":"markdown","45b9f225":"markdown","09b99b55":"markdown","7470ae62":"markdown","86868214":"markdown","23298bb1":"markdown","84fab065":"markdown","d0fee8d2":"markdown","173e0f42":"markdown","96353ba8":"markdown","9297adc6":"markdown","26f900a8":"markdown","bcf68994":"markdown","454d28d4":"markdown","97518a3f":"markdown","68e64758":"markdown","27f4daff":"markdown","ca5558a7":"markdown","e8e46f37":"markdown","07e7ea90":"markdown","00b9395f":"markdown","b438012c":"markdown","8c66e289":"markdown","bc2ea3a0":"markdown","8d89b52d":"markdown","1951edcd":"markdown","ed3f8b42":"markdown"},"source":{"fdadd133":"import matplotlib.pyplot as plt\nfrom pandas import read_csv\nimport seaborn as sns\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport re\nimport os\n\ntrain_df = read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.columns.tolist())#column names\nprint(train_df.shape) #Gives num_rows, num_cols\nn = train_df.shape[0] #save number of observations","7b07740d":"train_df.head(6)","9c35ff70":"print(train_df.isna().sum())","a491accd":"#unique values or range for feature set\nprint('Genders:', train_df['Sex'].unique())\nprint('Embarked:', train_df['Embarked'].unique())\nprint('Pclass:', train_df['Pclass'].unique())\nprint('Survived:', train_df['Survived'].unique())\nprint('SibSp Range:', train_df['SibSp'].min(),'-',train_df['SibSp'].max())\nprint('Parch Range:', train_df['Parch'].min(),'-',train_df['Parch'].max())\nprint('Family size range', (train_df['Parch']+train_df['SibSp']).min(),'-',(train_df['Parch']+train_df['SibSp']).max())\nprint('Fare Range:', train_df['Fare'].min(),'-',train_df['Fare'].max())\nprint('Total percent survived: %.2f' % (100*(train_df['Survived']==1).sum()\/n))","cb0f6fd3":"#indexes of passengers that survived the disaster\ndidSurvive = train_df['Survived'] == 1","af7e224a":"train_df.groupby(\"Sex\")[\"Age\"].describe()","7965538b":"train_df.groupby(\"Sex\")[\"Age\"].median()","85559ae9":"#Null ages are filled by gender-based median of age.\ntrain_df['Age'] = train_df[\"Age\"].fillna(train_df.groupby(\"Sex\")[\"Age\"].transform('median'))","d068adba":"sns.distplot(train_df['Age'], bins = 9,kde=False);\nsns.distplot(train_df[didSurvive]['Age'], bins = 9,kde=False);\nplt.ylabel('Count'); plt.legend(['Total Travellers','Survived Travellers']);","e32f5f67":"###Gender analysis\n#check male and female percentage\nprint('Gender Data: Percent travellers');\ngenderPercent = 100*train_df.groupby('Sex')['Sex'].count()\/n;\nprint(genderPercent);\nprint('\\nGender Data: Percent survived travellers')\n#Check percent survivabiliy based on sex \ngenderX = 100*train_df.groupby('Sex')['Survived'].sum()\/n\nprint(genderX);","fda001ef":"fig, ax =plt.subplots(1,2)\nfig.tight_layout()\n#Gender survivability\nsns.countplot(x=\"Sex\", data=train_df, ax=ax[0],order=['male','female']);\nsns.countplot(x=\"Sex\", data=train_df[didSurvive],ax=ax[1],order=['male','female']);\nplt.ylim(0,600)\nax[0].set_title('Total Travellers');\nax[1].set_title('Survived Travellers');","18d1d531":"isMale = train_df['Sex'] == 'male'\nisFemale = train_df['Sex'] == 'female'\n\nfig, ax =plt.subplots(1,2)\nfig.tight_layout()\n\nsns.distplot(train_df[isMale]['Age'],kde=False,bins=9,ax=ax[0])\nsns.distplot(train_df[isFemale]['Age'],kde=False,bins=9,ax=ax[0])\n\nax[0].set_ylabel('Count'); \nax[0].set_title('Total travellers')\n\nsns.distplot(train_df[isMale&didSurvive]['Age'],kde=False,bins=9,ax=ax[1])\nsns.distplot(train_df[isFemale&didSurvive]['Age'],kde=False,bins=9,ax=ax[1])\nax[1].set_ylabel('Count');\nax[1].set_title('Survived travellers')\n\nplt.legend(['Male','Female']);","76491530":"train_df.groupby('Pclass')['Fare'].describe()","462303d2":"#Fare Analysis\nsns.distplot(train_df['Fare'],kde=False);\nsns.distplot(train_df[didSurvive]['Fare'],kde=False);\nplt.ylabel('Count'); plt.legend(['Total Travellers','Survived Travellers']);","558098f4":"famSize = train_df['SibSp']+train_df['Parch'];\nsns.scatterplot(x=famSize,y=train_df['Fare']);","e7024516":"sns.scatterplot(x=train_df['Pclass'],y=train_df['Fare']);","2f66a5a2":"#Travel Class Analysis\n#Class data\nprint('Travel class Data: Percent travellers')\npclassPercent = 100*train_df.groupby('Pclass')['Pclass'].count()\/n\nprint(pclassPercent)\nprint('\\nTravel class Data: survival percentage')\npclassX = 100*train_df.groupby('Pclass')['Survived'].sum()\/n\nprint(pclassX)\nfig, ax =plt.subplots(1,2)\nfig.tight_layout()\nsns.countplot(x=\"Pclass\", data=train_df,ax=ax[0]);\nsns.countplot(x=\"Pclass\", data=train_df[didSurvive],ax=ax[1]);\nax[0].set_title('Total Travellers')\nax[1].set_title('Survived Travellers')\nplt.ylim(0,500);","e13e1092":"train_df['Cabin'].unique().size","bcc9d338":"train_df['Deck']= train_df['Cabin'].astype(str).str[0]\nprint(train_df['Deck'].unique())\nsns.countplot(x='Deck', hue='Pclass',data=train_df);","48544b14":"#Travel Companion Analysis\nhasParch = train_df['Parch'] > 0 #is traveling with parent\nhasSibsp = train_df['SibSp'] > 0 #is traveling with sibling or spouse\nhasFamily = train_df['Parch'] + train_df['SibSp'] > 0#is traveling with family\nisAlone = train_df['Parch'] + train_df['SibSp'] == 0#is traveling alone\n\nprint(\"Has parent or child aboard: %.2f\" % (100*sum(hasParch)\/n))\nprint(\"Has sibling or spouse aboard: %.2f \" % (100*sum(hasSibsp)\/n))\nprint('Is traveling alone:  %.2f '% (100*sum(isAlone)\/n))\nprint('Has family survived:  %.2f '% ( 100*sum(hasFamily&didSurvive)\/n))\nprint('Alone survived:  %.2f '% (100*sum(isAlone&didSurvive)\/n))\n\nfig, ax =plt.subplots(1,2)\nfig.tight_layout()\nfamilySize = train_df['SibSp'] + train_df['Parch']\nsns.countplot(x=familySize,ax=ax[0])\nax[0].set_xlabel('Family Size')\nsns.countplot(x=familySize[didSurvive],ax=ax[1])\nax[0].set_title('Total Travellers')\nax[1].set_title('Survived Travellers')\nax[1].set_xlabel('Family Size')\nplt.ylim(0,600);","0f70113f":"train_df['Titles'] = train_df['Name'].apply(lambda x: re.search(' [A-z][a-z]+\\.',x).group(0))\ntitleOrder = train_df['Titles'].unique()\nprint(titleOrder)","68ee71fc":"sns.countplot(x=\"Titles\", data=train_df,order=titleOrder);\nplt.xticks(rotation=45)\nplt.title('Total Travellers')\nplt.figure()\nplt.xticks(rotation=45)\nsns.countplot(x=\"Titles\", data=train_df[didSurvive],order=titleOrder);\nplt.title('Survived Travellers')\nplt.ylim(0,600);","129d8f55":"#Embarkment Analysis\nprint('Travel embarkment Data: Percent travellers')\nembarkedPercent = 100*train_df.groupby('Embarked')['Embarked'].count()\/n\nprint(embarkedPercent)\nprint('\\nTravel embarkment Data: survival percentage')\nembarkedX = 100*train_df.groupby('Embarked')['Survived'].sum()\/n\nprint(embarkedX);","44afd18a":"#Fill null values by 'S', which has the highest frequency\ntrain_df['Embarked'] = train_df[\"Embarked\"].fillna('S')","35216eea":"fig, ax =plt.subplots(1,2);fig.tight_layout();\nsns.countplot(x=\"Embarked\", data=train_df,ax=ax[0],order=['S','C','Q']);\nsns.countplot(x=\"Embarked\", data=train_df[didSurvive],ax=ax[1],order=['S','C','Q']);\nax[0].set_title('Total Travellers');\nax[1].set_title('Survived Travellers');\nplt.ylim(0,500);","ecf1585c":"train_df[\"Sex\"] = train_df[\"Sex\"].astype('category')\ntrain_df[\"SexCode\"] = train_df[\"Sex\"].cat.codes\nprint(train_df[['Sex','SexCode']].head(5))","6970c472":"#categorize 3 embarked stations to\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].astype('category')\ntrain_df[\"EmbarkedCode\"] = train_df[\"Embarked\"].cat.codes\nprint(train_df[['Embarked','EmbarkedCode']].head(5))","59ece44d":"train_df[\"FamSize\"] = (train_df['Parch'] + train_df['SibSp'])#is traveling with family\nprint(train_df[['Parch', 'SibSp', 'FamSize']].head(5))","863def40":"train_df[\"Titles\"] = train_df[\"Titles\"].astype('category')\ntrain_df[\"TitleCode\"] = train_df[\"Titles\"].cat.codes\nprint(train_df[['Name', 'Titles','TitleCode']].head(10))","69666a97":"#Make similar changes to the Prediction DATA\npredict_df = read_csv('\/kaggle\/input\/titanic\/test.csv')\npredict_df['Age'] = predict_df[\"Age\"].fillna(predict_df.groupby(\"Sex\")[\"Age\"].transform(\"median\"))\n\npredict_df[\"Sex\"] = predict_df[\"Sex\"].astype('category')\npredict_df[\"SexCode\"] = predict_df[\"Sex\"].cat.codes\n\npredict_df[\"Embarked\"] = predict_df[\"Embarked\"].astype('category')\npredict_df[\"EmbarkedCode\"] = predict_df[\"Embarked\"].cat.codes\n\npredict_df[\"FamSize\"] = predict_df['Parch'] + predict_df['SibSp']\n\npredict_df['Titles'] = predict_df['Name'].apply(lambda x: re.search(' [A-z][a-z]+\\.',x).group(0))\npredict_df[\"Titles\"] = predict_df[\"Titles\"].astype('category')\npredict_df[\"TitleCode\"] = predict_df[\"Titles\"].cat.codes\n#Replace missing fare value by the median of Fare in similar class\npredict_df['Fare'] = predict_df[\"Fare\"].fillna(predict_df.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"))","f06cf475":"features= ['Pclass', 'SexCode','Age','EmbarkedCode','FamSize','TitleCode','Fare']\nx = train_df[features];\ny = train_df['Survived'];","cbccd431":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 20,shuffle=True)","2d357b9a":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\n#using svm for classification\nsvm = SVC(kernel='linear')\nsvm.fit(x_train,y_train)\nscores = cross_val_score(svm,x,y)#get cross validation score\nprint(\"SVM Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","761de643":"from sklearn.linear_model import LogisticRegression\n#Using logistic regression\nLR = LogisticRegression(max_iter=200).fit(x_train, y_train)\nscores = cross_val_score(LR,x,y)\nprint(\"LR Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","2b268e58":"from sklearn.ensemble import RandomForestClassifier\n#Using random forest\nRF = RandomForestClassifier(max_depth=100, n_estimators=200)\nRF.fit(x_train, y_train)\nscores = cross_val_score(RF,x,y)\n#print(scores)\nprint(\"RF Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\ny_pred = RF.predict(x_test) #using random forest classifier\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Oranges\");","e6a11014":"from sklearn.neural_network import MLPClassifier\n#using neural network classifier\nNN = MLPClassifier(random_state = 4, max_iter = 5000)\nNN.fit(x_train, y_train)\nscores = cross_val_score(NN,x,y)\nprint(\"NN Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","d3e81495":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn import tree\n#I experiemented with split criteria and method, and ended up using the default values.\nclf = tree.DecisionTreeClassifier(criterion='gini',splitter='random')\nclf = clf.fit(x_train,y_train)\nscores = cross_val_score(clf,x,y)\nprint(\"DT Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n#y_predict= clf.predict(x_predict) #using logistic regression classifier","3900bb80":"x_predict= predict_df[features]\ny_predict= RF.predict(x_predict) #using random forest classifier\npredict_titanic = {'PassengerID': predict_df['PassengerId'],'Survived': y_predict}\ndf = pd.DataFrame(predict_titanic, columns= ['PassengerID', 'Survived'])\ndf.to_csv('\/kaggle\/working\/y_predict.csv',sep=',',index=False)","6271227e":"# Embarkment Analysis","7109513a":"# Travel class analysis","26d50f15":"# Introduction\n\nThis notebook is an attempt at getting my feet wet with Python as well as data science - I would love feedback and advice. Since this is my first attempt, I will do some basic data cleaning and feature engineering inspired from other notebooks. Finally, I will simply run few classifiers, e.g., SVM, linear regression, random forest, and neural network, to compare their accuracies. This notebook doesn't compare pros and cons of these classifiers, mostly because it is an experiment in python as well as data science. I will also not run analysis for varying model parameters.\n\nThe projects aims to determine whether a passenger on the doomed Titanic ship will surivive or not. The dataset provided includes training data and testing data. I will be calling the testing data (test.csv) as prediction data for my sanity. Further, I will be splitting training data into training and testing (20%) for cross validation.\n\nJust like with any data science project, I will first explore and visualize the data, followed by adding new features (columns), running some models, and finally predicting the data. It might be possible to obtain better accuracy\/prediction by tweaking the model parameters, however it is not in the scope of this notebook. \n\n\n# Data Exploration","2f517f7c":"# Gender Analysis","ce1b9bbb":"**Predicting survivability**","290db7d7":"**Neural network**","f1b3e3bb":"Cabin values are mainly provided for passengers in travel class 1 (blue bars are prominent throughout). Further, Travellers in class 2 and class 1 have Cabin information missing (Deck = 'n'). Since, we are already including Pclass as a model training feature, I will drop Cabin for our survivability prediction.","03543214":"**Random Forests**","667ac353":"Some insight in Fare information:\n\n1. A higher family size doesn't necessarily indicate higher Fare.\n2. Passengers in class 2 and 3 paid a fare of under 100 bucks.\n3. Most passengers paid under 50 bucks of fare.\n\nIt seems that passenger fare depends on their travel class and the data is clean, numerical, and easy to incorporate in our model.","fd01a646":"As can be seen by above plots, most passengers have prefix Mr. (male travellers) and most survived passengers have prefixes Mrs. and Miss  (female travellers).","45b9f225":"For each feature, we determine the range of values in the dataset.\n\n1. There are 3 travel classes and 2 genders. \n2. Survived column is binary indicating if the passenger survived (1) or not (0). \n3. Embarked is the station of boarding and there are 3 such ports. \n4. Passengers are traveling alone (family size = 0) or with families as large as 10 members.\n5. 38% of the passengers survived the disaster.","09b99b55":"While, name itself is not relevant for our model, we extract the title or prefix of passenger Name and categorize them. A title reflects the gender and might also indicate social standing of a passenger. The prefix is extracted using regular expression: [A-z][a-z]+. as name title starts with a space, an upper case alphabet ([A-z]), followed by multiple lower case alphabets ([a-z]+), and a dot (.). There are a total of 17 unique titles and no null data.","7470ae62":"> Last, we extract the title or prefix of Passenger Name and categorize them. The prefix is extracted using regular expression: [A-z][a-z]+\\. as name title starts with a space, an upper case alphabet ([A-z]), followed by multiple lower case alphabets ([a-z]+), and a dot (\\.). ","86868214":"Some insight in to gender data:\n\n1. There are 65% and 35% male and female passengers on board the Titanic. \n2. Most females are in their 20s and males in their 30s.\n3. 26% of the females and 12% of the males survived the disaster, indicating that females have higher chance of survival.\n4. Infants, regardless of gender, and males and females in between ages 20-40 have high chance of survival,","23298bb1":"**Decision Tree**","84fab065":"A quick peak reveals, there are 12 features (columns) in the dataset and 891 observations (rows). The *Survived* column, is our *y* variable or the feature that we must predict in the prediction dataset.  The values in the data appear below, obtained using the head() function. \n\nSome initial observations and thoughts: \n\n1. Age and Cabin features have null entries.\n2. Name of a person may not affect survivability of a person. However, their title might as it is an indication of social standing. For instance, a Countess may have better chances of survival than a person with title Mrs. \n3. A value of 0 in SibSp (#sibling and spouse on board) and 0 in Parch (#parents and children on board) indicates that a person is traveling alone.\n4. Travel fare could be related to travel class. A passenger with higher socio-economic standing may choose to travel in class 1, which presumably has higher fare. For instance, Passenger ID = 1 travels in Pclass =3, with Fare = 7.25; Passenger ID = 2 travels in Pclass = 1, with Fare = 71.28.\n5. If you watched the movie Titanic, you might remember that women and children were prioritized duing rescue operations. It begs the question, will age and sex (and therefore, their title) of a person matter to their survivability?","d0fee8d2":"Insights:\n\n1. 72% of passengers and 25% of the survived passengers embarked from station 'S'. \n2. We fill null embarkment values with 'S'","173e0f42":"**Support vector machine**","96353ba8":"> We add a new column called FamilySize, which is obtained by linear combination (i.e. addition) of the columns SibSp and Parch.\n        \n        familySize = (train_df['Parch'] + train_df['SibSp'])\n  \n  If familySize = 0, the passenger is traveling alone.","9297adc6":"To ease our analysis and indexing, we predetermine the indexes of all the passengers that survived the disaster. We now explore the impact on each feature on passenger survivability.","26f900a8":"Upon data explorations, we select travel class, gender, age, embarkment station, family size, title, and fare to train our model. We divide our training data into training (80%) and testing (20%) data.","bcf68994":"> Embarked column is first converted into a category and new feature EmbarkedCode is created where \n\n$$ \\text{EmbarkedCode} = \\begin{cases} 0 &\\mbox{if } \\text{Embarked = 'S'}\\\\\n                                  1 & \\mbox{if } \\text{Embarked = 'Q'}\\\\\n                                  2 & \\mbox{if } \\text{Embarked = 'C'} \\end{cases}.$$","454d28d4":"# Titanic model training, testing, and predicting","97518a3f":"# Name prefix (Title) analysis","68e64758":"# **Basic Feature Engineering**\n \nSince, classification models work with numeric data, we assign numeric labels to categorical data: Gender, Embarked, and Name title. Thus, we engineer new features in to our dataset.\n\n> Gender is encoded by replacing male entries by 1 and female entries by 0. A new feature called SexCode is added to the dataframe.\n\n$$ \\text{sexCode} = \\begin{cases} 1 &\\mbox{if } \\text{Sex = 'male'}\\\\\n                                  0 & \\mbox{if } \\text{Sex = 'female'} \\end{cases}.$$","27f4daff":"Insights in gender and Age:\n\n1. Median age of males and females is 27y and 29y respectively. Since several age values are missing,they are filled by the median of age based on gender. For instance, if the missing value is associated with a male passenger then it is replaced by the median of male age value (29).\n2. A decimal age value indicates children below age 1.","ca5558a7":"Insights:\n\n1. 55% of the passengers travelled in Class 3, 21% in Class 2, and 24% in Class 3.\n2. 13% of the survived passengers traveled in Class 3, 15% in Class 1. \n\nIt seems that Passengers in Class 1 have a slight edge in surviving the disaster.","e8e46f37":"**Logistic Regression**","07e7ea90":"##### The standard sequence of training a data is\n\n1. Create the object of the classifier (*clsf*) you wish to use.\n2. Invoke the method *clsf*.fit() with training data obtained due to train_test_split() and necessary parameters.\n3. Perform cross validation on testing data obtained using train_test_split() method.\n4. Check scores like accuracy, precision, recall.\n5. If model is acceptable, invoke *clsf*.predict() method on the classifier object with prediction data.\n\nI will use 5 classifiers for this work.","00b9395f":"**Prediction data exploring and cleaning**","b438012c":"Lets check for null values. Some Age data is missing, which we can fill by mean or median of the data. Couple of embarkement station data is missing, which we can fill by station with high frequency. Further, majority of the Cabin data is missing. We need to look further into Cabin data to determine how to (and if we can) add missing values.","8c66e289":"# Fare Analysis","bc2ea3a0":"# Travel companion analysis\n\nInsights: \n\n1. 60% of the passengers are traveling alone.\n2. 20% of the passengers traveling with family and 18% of the lonely travellers survived the disaster.","8d89b52d":"Using seaborn to perform most of the plotting in this notebook, we plot the histogram of passenger age and histogram of survived passenger age. Some insights: \n\n1. The majority of the passengers are in the range 20-40 years, with almost 39% in their 30s. \n2. Survivabilty is spread across all ages with passengers under 40 having higher chance.","1951edcd":"# Age Analysis\n ","ed3f8b42":"# **Cabin Analysis**\n\nSince, majority of the cabin data is missing, it is worth checking if it provides any value to our prediction model. Cabin values have several unique (148) string values with first character indicating the Deck on the titanic ship. We can extract the Deck using regular expression, and check against travel class to see where most data is missing. This decision to check against travel class was determined by observing the values externally."}}