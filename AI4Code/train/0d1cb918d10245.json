{"cell_type":{"905f40e4":"code","661289d6":"code","abb2031e":"code","f6f88fcc":"code","3590349f":"code","c7533513":"code","e237505f":"code","6fc96408":"code","a73e4f5f":"code","35a8d36e":"code","4a0fb073":"code","adccdac2":"code","e3e12b99":"code","f3cb6354":"code","8b31788f":"code","bffb19c6":"code","44755dd6":"code","744fac09":"code","e0138ed1":"code","63cb8d1a":"code","308f6275":"markdown","307f0058":"markdown","bf21a73c":"markdown","86dcf485":"markdown","532bc43e":"markdown","f18ac461":"markdown","1864c233":"markdown","d0806a57":"markdown","31eab8e4":"markdown","3ab4c385":"markdown","09a13162":"markdown","1c9d5934":"markdown","b6a2ca89":"markdown"},"source":{"905f40e4":"from nltk.tokenize import TreebankWordTokenizer\nfrom collections import Counter\n\nsentence = \"The faster Harry got to the store, the faster harry, the faster would get home.\"\n","661289d6":"tokenizer = TreebankWordTokenizer()\ntokens = tokenizer.tokenize(sentence.lower())\ntokens","abb2031e":"bag_of_words = Counter(tokens)\nbag_of_words","f6f88fcc":"bag_of_words.most_common(4)","3590349f":"times_harry_appears = bag_of_words['harry']\/len(bag_of_words)\nround(times_harry_appears, 4)","c7533513":"times_faster_appears = bag_of_words['faster']\/len(bag_of_words)\nround(times_faster_appears, 4)","e237505f":"!pip install nlpia\nfrom nlpia.data.loaders import kite_text","6fc96408":"tokens = tokenizer.tokenize(kite_text.lower())","a73e4f5f":"tokens[:10]","35a8d36e":"# Fetch frequency of the data\ntoken_counts = Counter(tokens)\ntoken_counts","4a0fb073":"# Remove stop words and punctuation \nimport nltk\nnltk.download('stopwords', quiet=True)","adccdac2":"stopwords = nltk.corpus.stopwords.words('english')\nkite_intro_tokens = [val for val in tokens if val not in stopwords]","e3e12b99":"kite_intro_counts = Counter(kite_intro_tokens)\nkite_intro_counts","f3cb6354":"# TF\nkite_intro_vector = []\ndoc_length = len(kite_intro_counts)\n\nfor key, val in kite_intro_counts.items():\n    kite_intro_vector.append(val\/doc_length)\n    ","8b31788f":"kite_intro_vector[:10]","bffb19c6":"from nlpia.data.loaders import kite_history ","44755dd6":"kite_history_token = tokenizer.tokenize(kite_history.lower())\n# Remove stop words \nkite_history_token = [val for val in kite_history_token if val not in stopwords]\nkite_history_counts = Counter(kite_history_token)","744fac09":"print(f\"# of words in kite intro: {len(kite_intro_counts)}\")\nprint(f\"# of words in kite intro: {len(kite_history_count)}\")","e0138ed1":"kite_intro_tf = kite_intro_counts[\"kite\"]\/len(kite_intro_counts)\nkite_history_tf = kite_history_count[\"kite\"]\/len(kite_history_count)\nprint(f\" TF for 'kite' word in intro document: {round(kite_intro_tf, 4)}\")\nprint(f\" TF for 'kite' word in history document: {round(kite_history_tf, 4)}\")","63cb8d1a":"number_docs = 2\nintro_idf = number_docs\/1 if (kite_intro_counts['kite'])!=0 else 0 + 1 if (kite_history_count['kite'])!=0 else 0\nhistory_idf = number_docs\/1 if (kite_intro_counts['kite'])!=0 else 0 + 1 if (kite_history_count['kite'])!=0 else 0\nprint(f\" IDF for 'kite' word in intro document: {round(intro_idf, 4)}\")\nprint(f\" IDF for 'kite' word in history document: {round(history_idf, 4)}\")","308f6275":"#### Revisiting bag of words\n\nLet's take a simple sentence and tokenize and fins the bag of words. ","307f0058":"The above calculation indicates the normalized term frequency. \n\n**Why normalize the term frequency ?**\n\nLet's take an example the word \"dog\" appears 3 times in the \"A\" document, and 100 times in the \"B\" document. By frequency, we can conclude that word \"Dog\" is more important to the \"B\" document. But the fact is \"A\" document is of 30 words whereas document \"B\" is of 580000 words. Since the overall word count for the documents that we are comparing are different, frequency is not the right way to measure the importance of words. Using normalized frequency we can get relative importance of words for the documents.  \n\n*TF(\"dog\", document A)* = 3\/30 = .1\n\n*TF(\"dog\", document B)* = 100\/580000 = 0.00017\n\n#### More Example\n\nLet's look at the bigger piece of text. i.e **kite intro text**","bf21a73c":"Does the above term frequency(TF) mean that the word \"kite\" is more important for the intro document than the history? Ideally, the kite word importance must be the same as the documents were about kite. \n\nA good way to think about is the Inverse document frequency. i.e Total number of documents to the number of documents in which the word occurs.  ","86dcf485":"### TF-IDF - *Term Frequency times Inverse Document Frequency*\n\n**Term Frequncy** - Counts of each word in a document\n\n**Inverse Document Frequency** - word count by number of documents in which the word occurs","532bc43e":"As I read the [book](https:\/\/www.amazon.in\/Natural-Language-Processing-Action-Understanding\/dp\/1617294632) **Natural Langauge Processing In Action** , will be summarizing the every chapters in the subsequent notebooks. This is the third notebook for Chapter 3: Math with words(TF-IDF vectors)\n\n![Screenshot 2021-09-13 at 2.05.52 PM.png](attachment:22cc7830-ebed-4a81-a734-2549f0b145b4.png)","f18ac461":"Ignoring the stop words and puntuations, we can see that words \"faster\" and \"harry\" represents the meaning of the sentence. Let's calculate the Term Frequency(TF) for these most frequency words in the sentence","1864c233":"In a short document like this even unordered words carry the meaning information about documents. Let't find the most common words","d0806a57":"Above are relative frequency of the words in the document. To compare the word importance across the documents, let's also consider the history of kites as separate document and calculate TF-IDF","31eab8e4":"TF-IDF is a numerical vector representation of a document. When there are multiple such documents by computing the cosine similarities between these TF-IDF vectors we can identify how similar the documents are. There are a lot of other normalized TF-IDF computation to smoothen the term frequency do find it in the [link](https:\/\/livebook.manning.com\/book\/natural-language-processing-in-action\/chapter-3\/v-1\/173)  ","3ab4c385":"# [ Chapter 1: Packets of Thought(NLP Overview)](https:\/\/www.kaggle.com\/shilpagopal\/nlp-in-action-chapter-1-overview)\n# [Chapter 2: Build your Vocabulary(Word Tokenization)](https:\/\/www.kaggle.com\/shilpagopal\/nlp-in-action-chapter-2-word-tokenization)\n# Chapter 3: Math with words(TF-IDF vectors)\n\nHaving collected and counted words and bucketed them into stems or lemmas, now it's time to understand which words are most important in a document and across the documents. Using the \"importance\" value, we can find the relevant documents in a corpus based on the keyword importance within each document. \n\nTo achieve this, we need to convert the integers representing counts and binary bit vectors from Chapter 2 into continuous numbers. The goal is to find the numerical representation of words that somehow captures the importance or information content of the words they represent. \n\nThere are three ways to achieve this. However, two of the approaches we have discussed in the last chapter. \n* Bag of words - Vectors of word counts or frequency\n* Bags of n-grams - Count of word pairs, triplets and more\n* TF-IDF - word scores that represents the importance","09a13162":"For a given term **t** in a given document **d** of a corpus **D**\n\n$tf(t,d)= \\frac {count(t)}{count(d)}$\n\n$idf(t,D)= log \\frac {number of documents}{number of documents conpatining t}$\n\n$tfidf(t, d, D)= tf(t,d) * idf(t, D)$\n","1c9d5934":"There is one issue with IDF computation, assume that there is a corpus of 1 million documents. when you search for the \"cat\" it returns 1 document\n\nIDF (\"cat\") = 1000000\/1 = 1000000\n\nAnd for \"dog\" it returns 10 document\n\nIDF (\"dog\") = 1000000\/10 = 100000\n\nThat's a huge difference. Though these two words happen to have similar importance. For Zipf's law(It states that, given the corpus of natural language utterances, the frequency of any word is inversely proportional to the rank in the frequency table) to holds good, when you compare frequency of two words, more frequent word should have an exponentially higher frequency than the lesser frequency word. Hence we should scale the word frequency by log().\n\nIDF (\"cat\") = log(1000000\/1) = 6\n\nIDF (\"dog\") = log(1000000\/10) = 5","b6a2ca89":"Let's look at the term frequency of word \"kite\""}}