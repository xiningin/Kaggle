{"cell_type":{"3fd6addc":"code","f105263a":"code","226ddf0b":"code","749758dd":"code","dcc6c876":"code","e2987ae5":"code","6e2780e4":"code","481a734d":"code","8161d40a":"code","ab9f31c7":"code","683bf6ce":"code","0d5b226a":"code","017c07d3":"code","8400791a":"code","1c1fad1a":"code","ddd601ce":"code","97089c20":"code","374b1cb8":"code","3d899092":"markdown","54def0cf":"markdown"},"source":{"3fd6addc":"# ==========================================================\n# import libraries\n# ==========================================================\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nimport time\nimport os\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nLOG_DIR = f\"{int(time.time())}\"","f105263a":"# ==========================================================\n# define functions\n# ==========================================================\ndef build_model(hp):\n    input_layer = Input(shape=(data_x_train_scaled.shape[1]), name='Input_Layer')\n    common_path = Dense(units=hp.Int(f\"dense_1_cp\", 16, 128, 16), activation='relu', name='first_dense')(input_layer)\n    common_path = Dropout(hp.Choice('drouput_rate_1', values=[0., 0.1, 0.2, 0.3]))(common_path)\n    common_path = Dense(units=hp.Int(f\"dense_2_cp\", 16, 128, 16), activation='relu', name='second_dense')(common_path)\n    common_path = Dropout(hp.Choice('drouput_rate_2', values=[0., 0.1, 0.2, 0.3]))(common_path)\n\n    # path 1\n    first_output = Dense(units='1', name='first_output_last_layer')(common_path)\n    \n    # for the number of hidden layers --> needs to be debugged cause sometimes I got nan as loss\n    # first_output_path = Dense(units=hp.Int(f\"dense_0_p1\", 16, 128, 16), activation='relu',\n    #                          name='first_output_dense0')(common_path)\n    # for i in range(hp.Int('n_layers_p1', 1, 4)):\n    #    first_output_path = Dense(units=hp.Int(f\"dense_{i+1}_p1_\", 16, 128, 16), activation='relu')(first_output_path)\n    # first_output = Dense(units='1', name='first_output_last_layer')(first_output_path)\n\n    # path 2\n    second_output_path = Dense(units=hp.Int(f\"dense_1_p2\", 16, 128, 16), activation='relu',\n                               name='second_output_dense0')(common_path)\n    second_output_path = Dropout(hp.Choice('drouput_rate_3', values=[0., 0.1, 0.2, 0.3]))(second_output_path)\n    second_output = Dense(units='1', name='second_output_last_layer')(second_output_path)\n    #  for the number of hidden layers --> needs to be debugged cause sometimes I got nan as loss\n    # second_output_path = Dense(units=hp.Int(f\"dense_0_p2\", 16, 128, 16), activation='relu',\n    #                           name='second_output_dense0')(common_path)\n    # for i in range(hp.Int('n_layers_p2', 1, 4)):\n    #     second_output_path = Dense(units=hp.Int(f\"dense_{i+1}_p2_\", 16, 128, 16), activation='relu')(second_output_path)\n    # second_output = Dense(units='1', name='second_output_last_layer')(second_output_path)\n\n    #\n    model = Model(inputs=input_layer, outputs=[first_output, second_output])\n    \n    model.compile(optimizer=tf.keras.optimizers.SGD(hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n                  loss={'first_output_last_layer': 'mse', 'second_output_last_layer': 'mse'},\n                  metrics={'first_output_last_layer': tf.keras.metrics.RootMeanSquaredError(),\n                           'second_output_last_layer': tf.keras.metrics.RootMeanSquaredError()})\n    return model","226ddf0b":"# ==========================================================\n# data preparation\n# ==========================================================\ndata = pd.read_csv('..\/input\/eergy-efficiency-dataset\/ENB2012_data.csv')\ndata_x = data.iloc[:, :-2]\ndata_y = data.iloc[:, -2:]\n\ndel data\n\n#\ndata_x_train_scaled, data_x_test_scaled, data_y_train, data_y_test = \\\n    train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n\n#\nsc = StandardScaler()\ndata_x_train_scaled = sc.fit_transform(data_x_train_scaled)\ndata_x_test_scaled = sc.transform(data_x_test_scaled)\n\n#\ndata_x_train_scaled, data_x_test_scaled, data_y_train, data_y_test = \\\n    np.array(data_x_train_scaled), np.array(data_x_test_scaled), np.array(data_y_train), \\\n    np.array(data_y_test)\n\n#\ndata_y_train = (data_y_train[:, 0], data_y_train[:, 1])\ndata_y_test = (data_y_test[:, 0], data_y_test[:, 1])","749758dd":"# 0.0001 and 200 epochs --> good results","dcc6c876":"# ==========================================================\n# data prediction\n# ==========================================================\ninput_layer = Input(shape=(data_x_train_scaled.shape[1]), name='Input_Layer')\ncommon_path = Dense(units='128', activation='relu', name='First_Dense')(input_layer)\ncommon_path = Dropout(0.3)(common_path)\ncommon_path = Dense(units='128', activation='relu', name='Second_Dense')(common_path)\ncommon_path = Dropout(0.3)(common_path)\n\nfirst_output = Dense(units='1', name='First_Output__Last_Layer')(common_path)\n\nsecond_output_path = Dense(units='64', activation='relu', name='Second_Output__First_Dense')(common_path)\nsecond_output_path = Dropout(0.3)(second_output_path)\nsecond_output = Dense(units='1', name='Second_Output__Last_Layer')(second_output_path)\n\n#\nmodel = Model(inputs=input_layer, outputs=[first_output, second_output])\nprint(model.summary())\n\n#\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.00001)\n\n#\nmodel.compile(optimizer=optimizer,\n              loss={'First_Output__Last_Layer': 'mse', 'Second_Output__Last_Layer': 'mse'},\n              metrics={'First_Output__Last_Layer': tf.keras.metrics.RootMeanSquaredError(),\n                       'Second_Output__Last_Layer': tf.keras.metrics.RootMeanSquaredError()})","e2987ae5":"plot_model(model)","6e2780e4":"# train the model\nearlyStopping_callback = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=10,\n                              verbose=1) \n\nhistory = model.fit(x=data_x_train_scaled, y=data_y_train, verbose=0, epochs=500, batch_size=10,\n                    validation_split=0.3, callbacks=earlyStopping_callback)","481a734d":"# display training history\nprint(history.history.keys())","8161d40a":"# summarize history for the first output rmse\nplt.plot(history.history['First_Output__Last_Layer_root_mean_squared_error'])\nplt.plot(history.history['val_First_Output__Last_Layer_root_mean_squared_error'])\nplt.title('model\\'s rmse for the first output')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the second output accuracy\nplt.plot(history.history['Second_Output__Last_Layer_root_mean_squared_error'])\nplt.plot(history.history['val_Second_Output__Last_Layer_root_mean_squared_error'])\nplt.title('model\\'s rmse for the second output')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","ab9f31c7":"# summarize history for total loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Total Loss')\nplt.ylabel('total loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the first output loss\nplt.plot(history.history['First_Output__Last_Layer_loss'])\nplt.plot(history.history['val_First_Output__Last_Layer_loss'])\nplt.title('the first output Loss')\nplt.ylabel('y1 loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the second output loss\nplt.plot(history.history['Second_Output__Last_Layer_loss'])\nplt.plot(history.history['val_Second_Output__Last_Layer_loss'])\nplt.title('the second output Loss')\nplt.ylabel('y2 loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","683bf6ce":"# Test the model\ntotal_loss, first_output_loss, second_output_loss, first_output_rmse, second_output_rmse = np.round(model.evaluate(\n    x=data_x_test_scaled, y=data_y_test, verbose=0), 3)\nprint(\n    \"Loss = {}, Y1_loss = {}, Y1_rmse = {}, Y2_loss = {}, Y2_rmse = {}\".format(total_loss, first_output_loss,\n                                                                             first_output_rmse, second_output_loss,\n                                                                             second_output_rmse))","0d5b226a":"LOG_DIR = f\"{int(time.time())}\"","017c07d3":"# ==========================================================\n# Hyperparameter tuning using Keras Tuner\n# ==========================================================\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=12,\n    executions_per_trial=1,\n    directory=LOG_DIR\n)\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(x=data_x_train_scaled,\n             y=data_y_train,\n             verbose=1,\n             epochs=500,\n             batch_size=10,\n             validation_split=0.3,\n             callbacks=[stop_early])","8400791a":"tuner.results_summary()","1c1fad1a":"# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The best result is as follow:\n    dense_1_cp: {best_hps.get('dense_1_cp')},\n    drouput_rate_1: {best_hps.get('drouput_rate_1')},\n    dense_2_cp: {best_hps.get('dense_2_cp')},\n    drouput_rate_2: {best_hps.get('drouput_rate_2')},\n    dense_1_p2: {best_hps.get('dense_1_p2')},\n    drouput_rate_3: {best_hps.get('drouput_rate_3')},\n    learning_rate: {best_hps.get('learning_rate')}\n\"\"\")","ddd601ce":"# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel_tuned = tuner.hypermodel.build(best_hps)\nhistory = model_tuned.fit(x=data_x_train_scaled, y=data_y_train, verbose=0, epochs=500, batch_size=10,\n                    validation_split=0.3, callbacks=earlyStopping_callback)","97089c20":"# Test the model\ntotal_loss_tunned, first_output_loss_tunned, second_output_loss_tunned, first_output_rmse_tunned, second_output_rmse_tunned = np.round(model_tuned.evaluate(\n    x=data_x_test_scaled, y=data_y_test, verbose=0), 3)\nprint(\n    \"Loss = {}, Y1_loss = {}, Y1_rmse = {}, Y2_loss = {}, Y2_rmse = {}\".format(total_loss_tunned, first_output_loss_tunned,\n                                                                             first_output_rmse_tunned, second_output_loss_tunned,\n                                                                             second_output_rmse_tunned))","374b1cb8":"print(f'Results Before Tunning:\\n Test Set RMSE: Heating Load:{np.round(first_output_rmse, 3)}, Cooling Load:{np.round(second_output_rmse, 3)}\\n')\nprint(f'Results After Tunning:\\n Test Set RMSE: Heating Load:{np.round(first_output_rmse_tunned, 3)}, Cooling Load:{np.round(second_output_rmse_tunned, 3)}\\n')\nprint(f'{np.round((first_output_rmse - first_output_rmse_tunned)*100\/first_output_rmse)}% Improvement in predicting Heating Load')\nprint(f'{np.round((second_output_rmse - second_output_rmse_tunned)*100\/second_output_rmse)}% Improvement in predicting Cooling Load')","3d899092":"y1 Heating Load\n\ny2 Cooling Load","54def0cf":"y1 Heating Load\n\ny2 Cooling Load"}}