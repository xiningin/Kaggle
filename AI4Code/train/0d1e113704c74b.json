{"cell_type":{"4d8498fb":"code","041e361d":"code","3f6c731e":"code","0cbef485":"code","b3e29087":"code","365a9a8a":"code","9d313e89":"code","928eadc5":"code","6b322c44":"code","e702d315":"code","bd12647f":"code","8cd7d14a":"code","527ed4b5":"code","b0603c6d":"code","6094459a":"code","a8b149c6":"code","39522791":"code","17ab05d1":"code","b51d5eb6":"code","9c682ece":"code","40bd93d2":"code","ae9b4c01":"code","c81b22fd":"code","5ed740a0":"code","33d68db4":"code","7c1163af":"code","5cd43e3d":"code","651241c0":"code","9db5aa00":"code","fae691fb":"code","78a7ff4f":"code","0b9b9e91":"markdown","aeca584e":"markdown","f0cc6469":"markdown","7f39df4a":"markdown","38d08b13":"markdown","1c7ae12d":"markdown","8b28b812":"markdown","cc2621ac":"markdown"},"source":{"4d8498fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","041e361d":"df = pd.read_csv(\"\/kaggle\/input\/german-credit\/german_credit_data.csv\")","3f6c731e":"df.head()","0cbef485":"df.columns","b3e29087":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['Sex'] = le.fit_transform(df['Sex'])\ndf.Housing = le.fit_transform(df.Housing)","365a9a8a":"df.head()","9d313e89":"pd.value_counts(df.Housing)","928eadc5":"pd.value_counts(df['Saving accounts'])","6b322c44":"pd.value_counts(df['Checking account'])","e702d315":"df.isnull().sum()","bd12647f":"data = df.copy(deep=True)\ndata = data.dropna(how='any')","8cd7d14a":"data.isnull().sum()","527ed4b5":"data.head()","b0603c6d":"data.Duration.unique()","6094459a":"smalldata = data[['Age', 'Credit amount', 'Duration']]\nsmalldata.head()","a8b149c6":"from mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = Axes3D(fig)\n\n# data for 3d plot\nax.scatter(smalldata['Age'], smalldata['Credit amount'], smalldata['Duration'])","39522791":"#Standardizing data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstd_data = scaler.fit_transform(smalldata)\nstd_data","17ab05d1":"mean_vec = np.mean(std_data,axis=0)\ncov_mat = (std_data - mean_vec).T.dot((std_data - mean_vec)) \/ (std_data.shape[0]-1)\n\n#or we can directly use numpy library method\nnp.cov(std_data.T)","b51d5eb6":"#eigen decompostion of the covariance matrix\neig_vals, eig_vectors = np.linalg.eig(cov_mat)\n\nprint(\"Eigen values:\", eig_vals)\nprint(\"Eigen vectors:\", eig_vectors)","9c682ece":" # correlation matrix\n# in most datasets especially financial datasets eigendecomposition on covariance matrix and\n# correlation matrix yields the same results since correlation matrix can be\n# understood as the normalized covariance matrix\n# so here is the eigendecompostion on correlation matrix\ncorr_mat = np.corrcoef(std_data.T)\neig_vals, eig_vectors = np.linalg.eig(corr_mat)\nprint(\"Eigen Values:\", eig_vals)\nprint(\"Eigen vectors:\", eig_vectors)","40bd93d2":"# eigendecomposition of raw data based on correlation matri\ncorr_mat_raw = np.corrcoef(std_data.T)\neig_vals, eig_vecs = np.linalg.eig(corr_mat_raw)\nprint('Eigenvals are:', eig_vals)\nprint('eigenvectors are:', eig_vecs)","ae9b4c01":"# while eigendecompostion of covariance matrix is more intutive, most implementations use singular value decompostion to increase computational efficiency\nu,s,v = np.linalg.svd(std_data.T)\nu","c81b22fd":"# to see which compinents can be dropped we have to check eigen values\n# eigenvectors with least eigen values have the least information about the distribution\n# so we first rank the vectors from highest to lowest eigenvalues then select the first k\n","5ed740a0":"# make a list of (eigenvalue, eigenvectors) tuples\neig_pairs = [(eig_vals[i], eig_vecs[i]) for i in range(len(eig_vals))]\n# eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:i]) for i in range(len(eig_vals))]\n\n# print(\"Eig pairs are\\n\",eig_pairs )\n# sort the eigenvalue, eigenvector tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()\n\nfor i in eig_pairs:\n    print(i[0])","33d68db4":"import matplotlib.pyplot as plt\ntot = sum(eig_vals)\nvar_exp = [(i\/tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\nx_coordinates = ('PC1', 'PC2', 'PC3')\ny_pos = np.arange(len(x_coordinates))\n\nplt.bar(y_pos, var_exp, align='center')\nplt.ylabel('Explained var in %')\nplt.xticks(y_pos, x_coordinates)\nplt.plot(cum_var_exp, 'r')\nplt.show()","7c1163af":"eig_pairs","5cd43e3d":"# making the new projection matrix \npro_mat = np.hstack((eig_pairs[0][1].reshape(3,1),\n                   eig_pairs[1][1].reshape(3,1)))\n\nprint('Projection Matrix :\\n', pro_mat)","651241c0":"pro_mat.shape","9db5aa00":"std_data","fae691fb":"new_mat = std_data.dot(pro_mat)\nnew_mat","78a7ff4f":"X = [new_mat[i][0] for i in range(len(new_mat))]\ny = [new_mat[i][1] for i in range(len(new_mat))]\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.scatter(X, y)","0b9b9e91":"# Singular value decompostion","aeca584e":"Now we have this small dataset where each rocord is a 3 dimensional vector","f0cc6469":"3d plot of the data","7f39df4a":"# selecting principal components","38d08b13":"This plot shows that around 58% variance can be explained by first component and around 30% variance is explained by second component. Thus they both cover around 88% of the variance and third component can be dropped without losing too much information.","1c7ae12d":"Estimating the importance of each feature vector using explined variance","8b28b812":"# Projection onto new feature space","cc2621ac":"here we see that the three approaches give same eigen vectors and eigen values, these approaches are:\n- Eigendecompostion of covariance  matrix after standardization of data\n- Eigendecomppostion of correlation matrix after standardization of data\n- Eigendecompostion of correlation matrix"}}