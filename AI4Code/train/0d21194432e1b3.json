{"cell_type":{"92c2f77e":"code","3ce3254d":"code","1eab1d65":"code","b16bc0c2":"code","cc13fdaa":"code","3bcc33b2":"code","96e97000":"code","55abff17":"code","1cf11b7f":"code","cb5705d7":"code","44903aee":"code","e7576073":"code","18c45ad9":"code","439e677e":"code","adcf9c59":"code","cb5effd6":"code","a2421fc3":"code","07b42de7":"code","fd283d77":"code","3dd18d36":"code","184a9778":"code","98b56b9b":"code","8a96fce5":"code","44b76ce4":"code","e7343a6b":"code","f1875aba":"code","59b88a16":"code","d9542734":"code","725f56b2":"code","e950eb25":"code","75c5a0c7":"code","b202b14a":"code","3d3fc390":"code","d98bb284":"code","726209eb":"code","12e5e743":"code","592053d1":"code","63c276a7":"markdown","9cb3fb0c":"markdown","928c1017":"markdown","d2d367a5":"markdown","05442d3a":"markdown","bc4abdfa":"markdown","cca65546":"markdown","5c63f972":"markdown","f3cd9489":"markdown","b1b0d0cb":"markdown","212d028c":"markdown","be56d13e":"markdown","7cd13b6b":"markdown","084bfaca":"markdown","81cf7e9a":"markdown","e8ad8db9":"markdown","2c6e5747":"markdown","8d602ed6":"markdown"},"source":{"92c2f77e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3ce3254d":"#import some of the necessary library \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n","1eab1d65":"#read the training data into train_df variable\nfilepath1 = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\" \ntrain_df = pd.read_csv(filepath1)\n\n#read the test data into test_df variable\nfilepath2 = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntest_df = pd.read_csv(filepath2)","b16bc0c2":"#print the first five rows of train data \ntrain_df.head()","cc13fdaa":"#print the first five rows of test data \ntest_df.head()","3bcc33b2":"#check the shape of training data\ntrain_df.shape","96e97000":"#check the shape of test data\ntest_df.shape","55abff17":"#let's look at the what are the columns we have\ntrain_df.columns","1cf11b7f":"test_df.columns","cb5705d7":"#to see the stastics summary of your train dataset\ntrain_df.describe()","44903aee":"len(train_df.columns)","e7576073":"train_df.SalePrice.describe()","18c45ad9":"#let's training id's and test id's for future references. As you know  for any machine learning based problem the Id doesn't make a feature, so we are going to drop the ID column from our train and test dataframe.\ntrain_ID = train_df[\"Id\"]\ntest_ID = test_df[\"Id\"]\ntrain_df.drop(\"Id\",axis=1,inplace=True)\ntest_df.drop(\"Id\",axis=1,inplace=True)","439e677e":"#get the number of missing data point each column\nmiss_val= train_df.isnull().sum().sort_values(ascending=False)\nmissing_df = pd.DataFrame({'Feature':miss_val.index,'Count':miss_val.values})\nmissing_df.head(20)","adcf9c59":"#let's see some graph\nfig,ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nplt.title(\"count of missing data per features\")\nsns.barplot(x=\"Feature\",y=\"Count\",data=missing_df,edgecolor='b',palette='cool')","cb5effd6":"total_cells = np.product(train_df.shape)\ntotal_cells","a2421fc3":"#total missing values \ntotal_missing = miss_val.sum()\ntotal_missing","07b42de7":"total_percent = (total_missing\/total_cells)*100\nprint(total_percent)","fd283d77":"#get numericals columns which have missing values\nmissing_num = [var for var in train_df.columns if train_df[var].isnull().sum() > 0 \n              and train_df[var].dtypes == 'float']","3dd18d36":"missing_num","184a9778":"for col in ('LotFrontage', 'MasVnrArea', 'GarageYrBlt'):\n    train_df[col] = train_df[col].fillna(train_df[col].mean())","98b56b9b":"#get categoricals features which have missing values\nmissing_categorical = [var for var in train_df.columns if train_df[var].isnull().sum()>0 \n                       and train_df[var].dtypes == 'O']","8a96fce5":"missing_categorical","44b76ce4":"train_df[\"GarageType\"].value_counts()","e7343a6b":"#handling the categoricals missing values with mode \nfor col in ('Alley',\n 'MasVnrType',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinType2',\n 'Electrical',\n 'FireplaceQu',\n 'GarageType',\n 'GarageFinish',\n 'GarageQual',\n 'GarageCond',\n 'PoolQC',\n 'Fence',\n 'MiscFeature'):\n    train_df[col] = train_df[col].fillna(train_df[col].mode()[0])","f1875aba":"#drop the utilities column\ntrain_df = train_df.drop(\"Utilities\",axis=1)\ntest_df = test_df.drop(\"Utilities\",axis=1)","59b88a16":"train_df.shape","d9542734":"test_df.shape","725f56b2":"X = train_df.copy()\nX.drop(\"SalePrice\",axis=1,inplace=True)\ny = train_df['SalePrice']","e950eb25":"#cheking the data type of each columns\nX.dtypes","75c5a0c7":"#label encoding for categoricals features\nfor colname in X.select_dtypes(\"object\"):\n    X[colname],_=X[colname].factorize()\n    \n#all discrete features should now have integer dtypes\ndiscrete_features = X.dtypes ==int","b202b14a":"X.head()","3d3fc390":"discrete_features","d98bb284":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X,y,discrete_features):\n    mi_scores = mutual_info_regression(X,y,discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores,name=\"MI Scores\",index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\nmi_scores = make_mi_scores(X,y,discrete_features)\nmi_scores[::10]","726209eb":"mi_scores.sort_values(ascending=False).plot.bar(figsize=(15,8))","12e5e743":"corremap = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corremap, vmax=0.9, square=True)","592053d1":"sns.set()\ncolumns = ['OverallQual', 'GrLivArea', 'TotalBsmtSF','GarageCars', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt','SalePrice']\nsns.pairplot(train_df[columns], size = 2)\nplt.show();","63c276a7":"We are creating this Notebook to illustrate that how you can \"Approach almost any Regression Problem\"\n\nA Comprehensive Checklist for Solving Any Regression Problem:\n\n* Data Fetching\n* Understanding the Data\n* Checking the Skwewness of the Output Variable\n* Performing Log Transformation (if required)\n* Exploratory Data Analysis\n* Analysing Correlation\n* Finding out Important Predictors\n* Data Cleaning: -\n  1. Missing Values\n  2. Outliers\n* Feature Engineering: -\n    1. Categorical Feature Encoding\n\nThe above mentioned checklist is very importnant for solving any regression based problem. The similar kind of checklist can also be prepared for other Machine Learning based problems. I will cover them in my upcoming kernals.\n\nLet's begin!","9cb3fb0c":"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.","928c1017":"### Objective:","d2d367a5":"### Handling the missing values in the categorical variables","05442d3a":"# Continue....................","bc4abdfa":"It might be helpful to see what percentage of the values in our dataset were missing to give us a better sense of the scale of this problem**","cca65546":"## Mutual information\nMutual information can help you to understand the relative potential of a feature as a predictor of the target.\nIt's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.","5c63f972":"### Let's Do Some Real \"Work\"","f3cd9489":"### Data Cleaning\nHow many missing data point do we have?","b1b0d0cb":"**Thus,**\n\n* Total Observations in the Training Data : - 1460\n* Total Features in the Training Data : - 79, Excluding Id Column and Dependent Variable i.e. SalePrice","212d028c":"### Let us first import the training and the test data.","be56d13e":"Percent of data that is missing","7cd13b6b":"### Let's handle the missing values\n1. let's handle the missing values in numerical data\n","084bfaca":"**Description of the SalePrice: -**\n\n1. Mean Value      -->  180921.195890\n2. Std. Deviation  -->  79442.502883\n3. Min Value       -->  34900.00\n4. Max Value       -->  755000.00","81cf7e9a":"With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","e8ad8db9":"we have 81 features or variable and 1460 rows or records in the training dataset","2c6e5747":"# Competition Description","8d602ed6":"we almost have same columns in the train and test dataset expect the saleprice column in the test dataset"}}