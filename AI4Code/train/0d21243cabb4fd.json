{"cell_type":{"e5063bcf":"code","124053a3":"code","9a2e417f":"code","0cbfea98":"code","2e523be9":"code","24360604":"code","6d384188":"code","51375563":"code","051bda1e":"code","04e38690":"code","d98d0457":"code","1e20d747":"code","d9f641b0":"code","570b7fc4":"code","caf7b4b7":"code","da592c72":"code","3a636bb4":"code","dedbc80a":"code","a7500c03":"code","06084792":"code","679262e7":"code","3ac897ba":"code","7fbca526":"code","02bb7e4e":"code","a833e845":"code","b5aabd8d":"code","ffbe0577":"code","9dedca11":"code","5704308c":"code","55c79a76":"code","9fa2f833":"code","48d0927d":"code","26aeebf8":"code","11ee6ead":"code","284913c4":"code","0ff1b233":"code","1306fb40":"code","5f415cd5":"code","63c0bc63":"code","b611b540":"code","30da1809":"code","d35cb455":"code","7c342c58":"code","2e42c4c0":"code","f1da83f3":"code","ad1a8957":"code","208cb802":"markdown","2ac65f5e":"markdown","2983bd0a":"markdown","b9977c48":"markdown","1a717c51":"markdown","91e0eeaf":"markdown","00dc4e83":"markdown","c17b9d6f":"markdown","dea9387c":"markdown","9edcd324":"markdown","ebf0c565":"markdown","c7ed145d":"markdown"},"source":{"e5063bcf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","124053a3":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n#VALIDATION\nVALID_SIZE = 0.30 # simple validation using train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = True\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/creditcardfraud\/creditcard.csv\"\n","9a2e417f":"#Read the data\ndata_df = pd.read_csv(PATH)","0cbfea98":"print(\"Credit Card Fraud Detection data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])","2e523be9":"#Glimpse the data\n#We start by looking to the data features (first 5 rows).\ndata_df.head()","24360604":"#Let's look into more details to the data.\ndata_df.describe()","6d384188":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","51375563":"\"\"\"Data unbalance\nLet's check data unbalance with respect with target value, i.e. Class.\"\"\"\ntemp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\nplt.figure(figsize = (6,6))\nplt.title('Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'Class', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","051bda1e":"\"\"\"Data exploration\nTransactions in time\"\"\"\nclass_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\nclass_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\nplt.figure(figsize = (14,4))\nplt.title('Credit Card Transactions Time Density Plot')\nsns.set_color_codes(\"pastel\")\nsns.distplot(class_0,kde=True,bins=480)\nsns.distplot(class_1,kde=True,bins=480)\nplt.show()","04e38690":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show();","d98d0457":"tmp = data_df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","1e20d747":"class_1.describe()","d9f641b0":"\"\"\"The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n\nLet's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days).\"\"\"\n\nfraud = data_df.loc[data_df['Class'] == 1]\nf, ax = plt.subplots(figsize=(14, 6))\ns = sns.regplot(x='Time', y='Amount',data=fraud, fit_reg=False,ax=ax, scatter_kws={'s':5})\nplt.title('Credit Card Fraudulent Transactions Amount vs. Time')\nplt.show()","570b7fc4":"#Features Correlation\nplt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","caf7b4b7":"s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","da592c72":"s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","3a636bb4":"#Features density plot\n\nvar = data_df.columns.values\n\ni = 0\nt0 = data_df.loc[data_df['Class'] == 0]\nt1 = data_df.loc[data_df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","dedbc80a":"#Define predictors and target values\n#Let's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.\n\ntarget = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","a7500c03":"train_df, val_df = train_test_split(data_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","06084792":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","679262e7":"clf.fit(train_df[predictors], train_df[target].values)","3ac897ba":"preds = clf.predict(val_df[predictors])","7fbca526":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   \n","02bb7e4e":"#Confusion Matrix\ncm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","a833e845":"roc_auc_score(val_df[target].values, preds)","b5aabd8d":"\"\"\"AdaBoostClassifier\nAdaBoostClassifier stands for Adaptive Boosting Classifier [5].\n\nPrepare the model\nLet's set the parameters for the model and initialize the model.\"\"\"\n\nclf = AdaBoostClassifier(random_state=RANDOM_STATE,\n                         algorithm='SAMME.R',\n                         learning_rate=0.8,\n                             n_estimators=NUM_ESTIMATORS)","ffbe0577":"#FIT THE MODEL\nclf.fit(train_df[predictors], train_df[target].values)","9dedca11":"preds = clf.predict(val_df[predictors])","5704308c":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","55c79a76":"cm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","9fa2f833":"#area under the curve\nroc_auc_score(val_df[target].values, preds)","48d0927d":"clf = CatBoostClassifier(iterations=500,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='AUC',\n                             random_seed = RANDOM_STATE,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = VERBOSE_EVAL,\n                             od_wait=100)","26aeebf8":"clf = CatBoostClassifier(iterations=500,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='AUC',\n                             random_seed = RANDOM_STATE,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = VERBOSE_EVAL,\n                             od_wait=100)","11ee6ead":"clf.fit(train_df[predictors], train_df[target].values,verbose=True)","284913c4":"preds = clf.predict(val_df[predictors])","0ff1b233":"#Features Importance\ntmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","1306fb40":"cm = pd.crosstab(val_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","5f415cd5":"#Area under the curve\nroc_auc_score(val_df[target].values, preds)","63c0bc63":"# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(val_df[predictors], val_df[target].values)\n\n#What to monitor (in this case, **train** and **valid**)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","b611b540":"#Train the model\nmodel = xgb.train(params, \n                dtrain, \n                MAX_ROUNDS, \n                watchlist, \n                early_stopping_rounds=EARLY_STOP, \n                maximize=True, \n                verbose_eval=VERBOSE_EVAL)","30da1809":"#Plot variable importance\nfig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","d35cb455":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':150, # because training data is extremely unbalanced \n         }","7c342c58":"dtrain = lgb.Dataset(train_df[predictors].values, \n                     label=train_df[target].values,\n                     feature_name=predictors)\ndel train_df\ngc.collect()\n\ndvalid = lgb.Dataset(val_df[predictors].values,\n                     label=val_df[target].values,\n                     feature_name=predictors)\ndel val_df\ngc.collect()","2e42c4c0":"\"\"\"Run the model\"\"\"\nevals_results = {}\n\nmodel = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)\n\ndel dvalid\ngc.collect()","f1da83f3":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nlgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \nplt.show()","ad1a8957":"kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\nfor train_index, test_index in kf.split(data_df):\n    train_X, valid_X = data_df.iloc[train_index], data_df.iloc[test_index]\n\n    dtrain = lgb.Dataset(train_X[predictors].values, label=train_X[target].values,\n                     feature_name=predictors)\n\n    dvalid = lgb.Dataset(valid_X[predictors].values, label=valid_X[target].values,\n                     feature_name=predictors)\n\n    evals_results = {}\n    model =  lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)","208cb802":"Again Confusion matrix\nLet's visualize the confusion matrix.","2ac65f5e":"# Conclusions\nWe investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. We then investigated two predictive models. We started with RandomForrestClassifier, for which we obtained an AUC scode of 0.895. We followed with an AdaBoostClassifier model, with lower AUC score (0.842). We then followed with an CatBoostClassifier, with the AUC score after training 500 iterations 0.88. We then presented the data to a LightGBM model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With both methods obtained values of AUC for the validation set around 0.98.","2983bd0a":"**Confusion matrix\nLet's visualize the confusion matrix.**","b9977c48":"Type I error and Type II error\nWe need to clarify that confussion matrix are not a very good tool to represent the results in the case of largely unbalanced data, because we will actually need a different metrics that accounts in the same time for the selectivity and specificity of the method we are using, so that we minimize in the same time both Type I errors and Type II errors.\n\nNull Hypothesis (H0) - The transaction is not a fraud.\nAlternative Hypothesis (H1) - The transaction is a fraud.\n\nType I error - You reject the null hypothesis when the null hypothesis is actually true.\nType II error - You fail to reject the null hypothesis when the the alternative hypothesis is true.\n\nCost of Type I error - You erroneously presume that the the transaction is a fraud, and a true transaction is rejected.\nCost of Type II error - You erroneously presume that the transaction is not a fraud and a ffraudulent transaction is accepted.\n\nThe following image explains what Type I error and Type II error are:\n\n\nAnd this alternative image explains even better:\n\n\nLet's calculate the ROC-AUC score [4].\n\nArea under curve","1a717c51":"As expected, there is no notable correlation between features V1-V28. There are certain correlations between some of these features and Time (inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V1 and V5).\n\nLet's plot the correlated and inverse correlated values on the same graph.\n\nLet's start with the direct correlated values: {V20;Amount} and {V7;Amount}.","91e0eeaf":"# LightGBM\nLet's continue with another gradient boosting algorithm, LightGBM [8] [9].\n\nDefine model parameters\nLet's set the parameters for the model.","00dc4e83":"RandomForestClassifier\nDefine model parameters\nLet's set the parameters for the model.\n\nLet's run a model using the training set for training. Then, we will use the validation set for validation.\n\nWe will use as validation criterion GINI, which formula is GINI = 2 * (AUC) - 1, where AUC is the Receiver Operating Characteristic - Area Under Curve (ROC-AUC) [4]. Number of estimators is set to 100 and number of parallel jobs is set to 4.\n\nWe start by initializing the RandomForestClassifier.","c17b9d6f":"# XGBoost\n# XGBoost is a gradient boosting algorithm [7].\n\nLet's prepare the model.\n\nPrepare the model\nWe initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.","dea9387c":"### Prepare the model\n\nLet's set the parameters for the model and initialize the model.","9edcd324":"# Training and validation using cross-validation\nLet's use now cross-validation. We will use cross-validation (KFolds) with 5 folds. Data is divided in 5 folds and, by rotation, we are training using 4 folds (n-1) and validate using the 5th (nth) fold.","ebf0c565":"For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1, V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V25, V26, V28 have similar profiles for the two values of Class.\n\nIn general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.","c7ed145d":"We can confirm that the two couples of features are correlated (the regression lines for Class = 0 have a positive slope, whilst the regression line for Class = 1 have a smaller positive slope).\n\nLet's plot now the inverse correlated values."}}