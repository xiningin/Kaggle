{"cell_type":{"ba6b3ce5":"code","1cf40b40":"code","85f26908":"code","e1f2bbc0":"code","7120f5bb":"code","f15b676a":"code","f347ae8b":"code","2db0e233":"code","f650603d":"code","bbfff253":"code","29c70f6c":"code","1a779103":"code","20a3054f":"markdown","5bba01c0":"markdown","e3009bec":"markdown","e493709a":"markdown","5dd3aa1e":"markdown","57a57044":"markdown","0c496f24":"markdown","f358fbaa":"markdown","cc869887":"markdown","f09e612e":"markdown","2cad05d7":"markdown","5ec7cc6f":"markdown","fbc51815":"markdown","56a2d51f":"markdown","fa53d059":"markdown","50fb21fc":"markdown"},"source":{"ba6b3ce5":"!pip install cdqa","1cf40b40":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport nltk\nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom copy import deepcopy","85f26908":"# Paths to json files\npath_1 = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\npath_2 = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\npath_3 = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\npath_4 = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\n\n# List of folder names\nfolder_names = ['biorxiv_medrxiv','comm_use_subset']\nfolder_paths = [path_1, path_2]","e1f2bbc0":"# This piece of code was adopted from the original source at:\n# https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\/notebook \n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:# First, for each query the system arranges all the scientific papers within the corpus in the relevant order.\n# Second, the system analize texts of top N the mosr relevant papers to answer to the query in the best way.\n            name_ls.append(name)\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    for section, text in texts:\n        texts_di[section] += text\n    body = \"\"\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)\n\ndef load_files(dirname, filename=None):\n    filenames = os.listdir(dirname)\n    raw_files = []\n    if filename:\n        filename = dirname + filename\n        raw_files = [json.load(open(filename, 'rb'))]\n    else:\n        #for filename in tqdm(filenames):\n        for filename in filenames:\n            filename = dirname + filename\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)\n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    #for file in tqdm(all_files):\n    for file in all_files:\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df = clean_df.drop(columns=['authors','affiliations','bibliography',\n                                      'raw_authors','raw_bibliography'])\n    return clean_df","7120f5bb":"def get_corpus(folder_paths = folder_paths):\n    num_of_papers = {}\n    corpus = pd.DataFrame(columns=['paper_id','title','abstract','text'])\n    for i in range(len(folder_paths)):\n        filenames = os.listdir(folder_paths[i])\n        print('Reading {0} json files from folder {1} ...'.format(len(filenames), folder_names[i]))\n        num_of_papers[folder_names[i]] = len(filenames)\n        files = load_files(folder_paths[i])\n        df = generate_clean_df(files)\n        corpus = pd.concat([corpus, df], ignore_index=True, sort=False)\n    print('Corpus includes {0} scientific articles.'.format(len(corpus)))\n    return corpus, num_of_papers\n\ncorpus, num_of_papers = get_corpus()","f15b676a":"# This processing algorithm can originaly be found at:\n# https:\/\/github.com\/nilayjain\/text-search-engine\n\ninverted_index = defaultdict(list)\nnum_of_documents = len(corpus)\nvects_for_docs = []  # we will need nos of docs number of vectors, each vector is a dictionary\ndocument_freq_vect = {}  # sort of equivalent to initializing the number of unique words to 0\n\n# It updates the vects_for_docs variable with vectors of all the documents.\ndef iterate_over_all_docs():\n    print('Processing corpus...')\n    for i in range(num_of_documents):\n        if np.mod(i, 1000) == 0:\n            print('{0} of {1}'.format(str(i).zfill(len(str(num_of_documents))),num_of_documents))\n        doc_text = corpus['title'][i] + ' ' + corpus['abstract'][i] + ' ' + corpus['text'][i]\n        token_list = get_tokenized_and_normalized_list(doc_text)\n        vect = create_vector(token_list)\n        vects_for_docs.append(vect)\n    print('{0} of {1}'.format(num_of_documents, num_of_documents))\n\ndef create_vector_from_query(l1):\n    vect = {}\n    for token in l1:\n        if token in vect:\n            vect[token] += 1.0\n        else:\n            vect[token] = 1.0\n    return vect\n\ndef generate_inverted_index():\n    count1 = 0\n    for vector in vects_for_docs:\n        for word1 in vector:\n            inverted_index[word1].append(count1)\n        count1 += 1\n\ndef create_tf_idf_vector():\n    vect_length = 0.0\n    for vect in vects_for_docs:\n        for word1 in vect:\n            word_freq = vect[word1]\n            temp = calc_tf_idf(word1, word_freq)\n            vect[word1] = temp\n            vect_length += temp ** 2\n        vect_length = sqrt(vect_length)\n        for word1 in vect:\n            vect[word1] \/= vect_length\n\ndef get_tf_idf_from_query_vect(query_vector1):\n    vect_length = 0.0\n    for word1 in query_vector1:\n        word_freq = query_vector1[word1]\n        if word1 in document_freq_vect:\n            query_vector1[word1] = calc_tf_idf(word1, word_freq)\n        else:\n            query_vector1[word1] = log(1 + word_freq) * log(\n                num_of_documents)\n        vect_length += query_vector1[word1] ** 2\n    vect_length = sqrt(vect_length)\n    if vect_length != 0:\n        for word1 in query_vector1:\n            query_vector1[word1] \/= vect_length\n\ndef calc_tf_idf(word1, word_freq):\n    return log(1 + word_freq) * log(num_of_documents \/ document_freq_vect[word1])\n\ndef get_dot_product(vector1, vector2):\n    if len(vector1) > len(vector2):\n        temp = vector1\n        vector1 = vector2\n        vector2 = temp\n    keys1 = vector1.keys()\n    keys2 = vector2.keys()\n    sum = 0\n    for i in keys1:\n        if i in keys2:\n            sum += vector1[i] * vector2[i]\n    return sum\n\ndef get_tokenized_and_normalized_list(doc_text):\n    tokens = nltk.word_tokenize(doc_text)\n    ps = nltk.stem.PorterStemmer()\n    stemmed = []\n    for words in tokens:\n        stemmed.append(ps.stem(words))\n    return stemmed\n\ndef create_vector(l1):\n    vect = {}  # this is a dictionary\n    global document_freq_vect\n    for token in l1:\n        if token in vect:\n            vect[token] += 1\n        else:\n            vect[token] = 1\n            if token in document_freq_vect:\n                document_freq_vect[token] += 1\n            else:\n                document_freq_vect[token] = 1\n    return vect\n\ndef get_result_from_query_vect(query_vector1):\n    parsed_list = []\n    for i in range(num_of_documents - 0):\n        dot_prod = get_dot_product(query_vector1, vects_for_docs[i])\n        parsed_list.append((i, dot_prod))\n        parsed_list = sorted(parsed_list, key=lambda x: x[1])\n    return parsed_list\n\niterate_over_all_docs()\ngenerate_inverted_index()\ncreate_tf_idf_vector()","f347ae8b":"# The End-To-End Closed Domain Question Answering System is used here.\n# It is available at: https:\/\/pypi.org\/project\/cdqa\/\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import download_model, download_bnpp_data\nfrom cdqa.pipeline.cdqa_sklearn import QAPipeline\n\ndownload_bnpp_data(dir='.\/data\/bnpp_newsroom_v1.1\/')\ndownload_model(model='bert-squad_1.1', dir='.\/models')","2db0e233":"def find_relevant_articles(query=None, top_n_papers=20, min_n_papers=3):\n    if query == None:\n        query = input('Please enter your query...')\n    print('\\n\\n'+'*'*34+' PROCESSING NEW QUERY '+'*'*34+'\\n')   \n    query_list = get_tokenized_and_normalized_list(query)\n    query_vector = create_vector_from_query(query_list)\n    get_tf_idf_from_query_vect(query_vector)\n    result_set = get_result_from_query_vect(query_vector)\n    papers_info = {'query':query, 'query list':query_list, 'query vector':query_vector,\n                   'id':[], 'title':[], 'abstract':[], 'text':[], 'weight':[], 'index':[]}\n    for i in range(1, top_n_papers+1):\n        tup = result_set[-i]\n        papers_info['id'].append(corpus['paper_id'][tup[0]])\n        papers_info['title'].append(corpus['title'][tup[0]])\n        papers_info['abstract'].append(corpus['abstract'][tup[0]])\n        papers_info['text'].append(corpus['text'][tup[0]])\n        papers_info['weight'].append(tup[1])\n        papers_info['index'].append(tup[0])\n    colms = ['date', 'title', 'category', 'link', 'abstract', 'paragraphs']\n    df = pd.DataFrame(columns=colms)\n    for i in range(len(papers_info['text'])):\n        papers_info['text'][i] = papers_info['text'][i].replace('\\n\\n', ' ')\n        CurrentText = papers_info['text'][i]\n        CurrentText = CurrentText.split('. ')\n        #CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", \"None\", CurrentText]\n        CurrentList = [\"None\", papers_info['title'][i], \"None\", \"None\", papers_info['abstract'][i], CurrentText]\n        CurrentList = np.array(CurrentList)\n        CurrentList = CurrentList.reshape(1, CurrentList.shape[0])\n        CurrentList = pd.DataFrame(data = CurrentList, columns=colms)\n        df = pd.concat([df, CurrentList], ignore_index=True)\n    df = filter_paragraphs(df)\n    # Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n    cdqa_pipeline = QAPipeline(reader='models\/bert_qa.joblib')\n    # Fitting the retriever to the list of documents in the dataframe\n    cdqa_pipeline.fit_retriever(df=df)\n    # Sending a question to the pipeline and getting prediction\n    query = papers_info['query']\n    prediction = cdqa_pipeline.predict(query=query)\n    for i in range(top_n_papers):\n        if papers_info['title'][i] == prediction[1]:\n            pid = papers_info['id'][i]\n    response = {query:{'id':pid,'title':prediction[1],'answer':prediction[0],'summary':prediction[2],\n                       'important papers':{'id':papers_info['id'],'title':papers_info['title']}}}\n    print('QUERY: {0}\\n'.format(query))\n    print('ANSWER MINED FROM PAPER: {0}\\n'.format(prediction[0]))\n    print('PAPER TITLE: {0}\\n'.format(prediction[1]))\n    print('PARAGRAPH IN PAPER: {0}\\n'.format(prediction[2]))\n    show_paper = np.min([min_n_papers, top_n_papers])\n    print('\\nTOP {0} MOST RELEVANT PAPERS RELATED TO THE QUERY:\\n'.format(show_paper))\n    for i in range(show_paper):\n        print('PAPER #{0}. \\nID: {1} \\nTITLE: {2}\\n'.format(i+1, papers_info['id'][i], papers_info['title'][i]))\n    return response, papers_info, prediction, result_set, df","f650603d":"# List of queries\nqueries = ['What is range of incubation period for coronavirus SARS-CoV-2 COVID-19 in humans',\n           'What is optimal quarantine period for coronavirus COVID-19',\n           'What is effective quarantine period for coronavirus COVID-19',\n           'What is percentage of death cases for coronavirus SARS-CoV-2 COVID-19',\n           'What is death rate for coronavirus COVID-19 and air pollution',\n           'At which temperature coronavirus COVID-19 can survive',\n           'How long coronavirus SARS-CoV-2 can survive on plastic surface',\n           'What are risk factors for coronavirus COVID-19',\n           'What is origin of coronavirus COVID-19',\n           'At which temperature coronavirus cannot survive']","bbfff253":"for query in queries:\n    response, papers_info, prediction, result_set, df = find_relevant_articles(query, top_n_papers=50)","29c70f6c":"query = 'What is coronavirus SARS-CoV-2'","1a779103":"find_relevant_articles(query=query, top_n_papers=50, min_n_papers=5);","20a3054f":"## Step 1.   Extraction of data from json files to dataframe format","5bba01c0":"# Query Based Relevant Arrangement of Medical Papers and Getting Suitable Answer","e3009bec":"First, for each query the system arranges all the scientific papers within the corpus in the relevant order.\n\nSecond, the system analize texts of top N the mosr relevant papers to answer to the query in the best way.","e493709a":"## Acknowledgements:\nI am grateful to [xhlulu](https:\/\/www.kaggle.com\/xhlulu) for the [useful notebook](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\/notebook).\n\nI am thankful to [Ivan](https:\/\/www.kaggle.com\/ivanbagmut), [Sergey](https:\/\/www.kaggle.com\/sergeypashchenko), and [Lvennak](https:\/\/www.kaggle.com\/lvennak) for fruitful discussions on COVID-19.\n","5dd3aa1e":"## How to use the system","57a57044":"## Step 6. Getting practical answers and the most relevant papers (query based approach)","0c496f24":" Install the [End-To-End Closed Domain Question Answering System](https:\/\/pypi.org\/project\/cdqa\/)","f358fbaa":"## Step 0. Set up packages","cc869887":"## Step 4. Using pretrained BERT model","f09e612e":"Below one can see a list of 10 queries and answers, which have been found by the system due to text mining. ","2cad05d7":"import the required modules:","5ec7cc6f":"## Step 3. Processing of Corpus","fbc51815":"## Step 5. Search of the most relevant articles and competent answer on the query","56a2d51f":"    2. Call the function find_relevant_articles().\n\n    For example,","fa53d059":"## Step 2. Corpus formation","50fb21fc":"    When Steps 0-6 have been completed with a corpus of scientific papers, the system is ready to process your queries. To get an answer to a query, follow two steps: \n\n    1. Input any query in the form of string type variable.\n    \n    For example,"}}