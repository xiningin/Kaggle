{"cell_type":{"dec0272f":"code","45c38454":"code","ddafa387":"code","f05480d9":"code","140db57f":"code","ef807fd1":"code","19f1817b":"code","920352e9":"code","91553597":"code","4a42a5aa":"code","20567c87":"code","7f2a22ec":"code","0cc7294d":"code","40731833":"code","3357b60a":"code","1506158e":"code","7114214a":"code","676f8156":"code","ebf4c7aa":"code","359074f9":"code","fb2cf16d":"markdown","3a035192":"markdown","ee08ea3d":"markdown","51026373":"markdown","9e0445d2":"markdown","91a87a0c":"markdown","6257fd9c":"markdown","538401ee":"markdown"},"source":{"dec0272f":"import numpy as np # linear algebra\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, f1_score, make_scorer\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import Normalizer, LabelEncoder\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import models\nfrom tensorflow.keras import Sequential\n","45c38454":"y=[]\nX=[]\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/yale-face-database\/data'):\n    \n    \n    for filename in filenames:\n        if filename!=\"Readme.txt\":\n            b=plt.imread(os.path.join(dirname, filename))\n       \n            try:\n                b= cv2.cvtColor(b,cv2.COLOR_GRAY2RGB) \n            except:\n                pass\n\n\n\n            b=cv2.resize(b, (224,224),interpolation = cv2.INTER_AREA)\n\n            if b.shape==(224,224,3) :\n\n                y.append(int(filename[7:9]))\n                X.append(b)\n\n\n\n\n            else:\n                print(face.shape)","ddafa387":"#sort according to the person\ndata=[]\nfor i in range(1,16):\n    person=[]\n    for u in range(len(y)):\n        if y[u]==i:\n            \n            person.append(X[u])\n    data.append([person, i*len(person)])\n    print(str(len(person))+\" images  for class \"+ str(i))","f05480d9":"\nimport random \nrandom.seed(42)\nnewX1=[]\nnewX2=[]\nnewY=[]\nfor i in range(len(data)):\n    parts= int(len(data[i][0])\/2)\n    \n    #positive sampling\n    for u in range(parts):\n        \n        newX1.append(data[i][0][u])\n        newX2.append(data[i][0][u+parts])\n        newY.append(0)\n        \n        \n    \n    #negative sampling\n    for uu in range(parts):\n        \n        numbers = list(range(0,i)) + list(range(i+1,11))\n        r = random.choice(numbers)\n        g = random.randint(0,len(data[r][0])-1)\n        newX1.append(data[i][0][uu])\n        newX2.append(data[r][0][g])\n        newY.append(1)\n\nprint(\"Target values count: \"+ str(np.unique(newY, return_counts=True)))","140db57f":"fig=plt.figure(figsize=(15, 15))\ncolumns = 7\nrows = 7\nfor i in range(1, columns*rows +1):\n    ax=fig.add_subplot(rows, columns, i)\n    \n    plt.imshow(np.concatenate([newX1[i-1],newX2[i-1]], axis=1))\n    ax.title.set_text(newY[i-1])\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.show()","ef807fd1":"c = list(zip(newX1, newX2, newY))\n\nrandom.shuffle(c)\n\na, b, y = zip(*c)\n    ","19f1817b":"Xpca=np.array(a)\nXpca=Xpca.reshape(Xpca.shape[0],Xpca.shape[1]*Xpca.shape[2]*Xpca.shape[3])\nfrom sklearn.decomposition import PCA\n\nfaces_pca = PCA(n_components=0.9)\nfaces_pca.fit(Xpca)\nfig, axes = plt.subplots(2,5,figsize=(9,3),\n subplot_kw={'xticks':[], 'yticks':[]},\n gridspec_kw=dict(hspace=0.01, wspace=0.01))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(np.interp(faces_pca.components_[i], (min(faces_pca.components_[i]),max(faces_pca.components_[i])), (0, 255)).reshape(224,224,3).astype(\"int\"))\n","920352e9":"base_model = models.load_model('..\/input\/facenet\/facenet_keras.h5')\n\nbase_model.load_weights(\"..\/input\/facenet\/facenet_keras_weights.h5\")\nbase_model.trainable=False\n","91553597":"output1=base_model.predict(np.array(a)\/255)\noutput2=base_model.predict(np.array(b)\/255)","4a42a5aa":"indexes1=[i for i,x in enumerate(y) if x == 1]\nindexes0=[i for i,x in enumerate(y) if x == 0]","20567c87":"arr=[]\nsu=[]\nfor s in range(len(output1)):\n    oo = np.abs(np.subtract(np.array(output1[s]),np.array(output2[s])))\n    arr.append(oo)\n    su.append(oo.sum())\n    \na = np.array(su)\nsu1=list(a[indexes1])\nsu0=list(a[indexes0])\n\nfig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Sum differences\")\naxs[0].plot(list(range(75)),su1, list(range(75)),su0)\naxs[0].legend([\"different people\", \"same person\"])\n#axs[0].title(\"Euclidean distance\")\naxs[1].plot(list(range(150)),su)\naxs[1].legend([\"overall variation\"])\n\n    ","7f2a22ec":"from scipy import stats\nimport statistics\nprint(stats.describe(su))\nprint(\"median \"+str(statistics.median(su)))","0cc7294d":"\n\nscorer = make_scorer(f1_score, average='macro')\nX_train, X_test,y_train,y_test= train_test_split(arr,y,test_size=0.2, random_state=42)\n\ntuned_parameters = [{'kernel': ['rbf','poly','linear'], 'gamma': [1e-3, 1e-4, 1e-5],\n                     'C': [1, 10, 100, 1000]}]\nscores = ['precision', 'recall']\n#for score in scores:\n\nclf = GridSearchCV(SVC(), tuned_parameters, scoring=scorer)\nclf.fit(Normalizer().fit(X_train).transform(X_train), y_train)\nparams=clf.best_params_\nprint(clf.best_params_)\ncv= SVC(C=params[\"C\"], gamma= params[\"gamma\"], kernel=params[\"kernel\"])\ncv.fit(Normalizer().fit(X_train).transform(X_train), y_train)\n\ny_pred = cv.predict(Normalizer().fit(X_test).transform(X_test))\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()","40731833":"import scipy\neuc=[]\nfor s in range(len(output1)):\n    oo = scipy.spatial.distance.euclidean(np.array(output1[s]),np.array(output2[s]))\n    euc.append(oo)\na = np.array(euc)\neuc1=list(a[indexes1])\neuc0=list(a[indexes0])\n\nfig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Euclidean distance\")\naxs[0].plot(list(range(75)),euc1, list(range(75)),euc0)\naxs[0].legend([\"different people\", \"same person\"])\n#axs[0].title(\"Euclidean distance\")\naxs[1].plot(list(range(150)),euc)\naxs[1].legend([\"overall difference\"])\n","3357b60a":"\nprint(stats.describe(euc))\nprint(\"median \"+str(statistics.median(euc)))","1506158e":"cos=[]\nfor s in range(len(output1)):\n    oo = scipy.spatial.distance.cosine(np.array(output1[s]),np.array(output2[s]))\n    cos.append(oo)\na = np.array(cos)\ncos1=list(a[indexes1])\ncos0=list(a[indexes0])\nfig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Cosine distance\")\naxs[0].plot(list(range(75)), cos1, list(range(75)),cos0)\n#axs[0].title(\"Euclidean distance\")\naxs[1].plot(list(range(150)),cos)\n","7114214a":"print(stats.describe(cos))\nprint(\"median \"+str(statistics.median(cos)))","676f8156":"c = list(zip(newX1, newX2, newY))\n\nrandom.shuffle(c)\n\na, b, y = zip(*c)\n    ","ebf4c7aa":"features=Model(base_model.input, base_model.output)\nimport tensorflow.keras.backend as K\nimport tensorflow\n\n\ndef distance(vecs):\n    x, y = vecs\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    \n    return K.abs(x-y)\n\nv=Input((224,224,3))\np=Input((224,224,3))\n\nfeaturesA=features(v)\nfeaturesB=features(p)\ndistance= Lambda(distance)([featuresA,featuresB])\n\nx= Dense(96, activation=\"relu\")(distance)\nx= Dropout(0.3)(x)\nx= Dense(64)(x)\noutputs = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=[v,p],outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\nmodel.summary()\nhistory=model.fit([np.array(a)[:120]\/255, np.array(b)[:120]\/255],np.array(y)[:120],validation_data=([np.array(a)[120:]\/255, np.array(b)[120:]\/255],np.array(y)[120:]), epochs=10, batch_size=16)\n","359074f9":"fig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Overfitting analysis\")\naxs[0].plot(list(range(1,11)), history.history['val_accuracy'], list(range(1,11)), history.history['accuracy'])\n\naxs[0].title.set_text(\"Accuracy\")\naxs[0].legend([\"validation accuracy\", \"training accuracy\"])\naxs[1].plot(list(range(1,11)), history.history['val_loss'], list(range(1,11)), history.history['loss'])\naxs[1].title.set_text('Loss')\naxs[1].legend([\"validation loss\", \"trainig loss\"])","fb2cf16d":"## Data Preprocessing\n\nThe model requires images 160x160x3, passing images of shape 224x224x3 gives better result.<br>\nThe data processed for binary classification: every sample contains two images with corresponding binary target\n","3a035192":"# Face Verification Methods\n\n\nThe kernel gives some overview of some basic methods for face verification. It uses FaceNet model developed by Hiroki Taniai and availble in [this repository](https:\/\/drive.google.com\/open?id=1pwQ3H4aJ8a6yyJHZkTwtjcL4wYWQb7bn)","ee08ea3d":"### PCA dimention reduction and eigenfaces visualization\n\nPCA is a basic method used for image feature extraction.Best results visualizing eigenfaces achieved when applied on 2D images.","51026373":"### FaceNet Features Extraction\n\nWhile PCA can be used for feature extraction, CNN is considered more powerful tool to extract image features","9e0445d2":"### Siamese Network\n\nThe CNN model implements a logic similar to the one demonstrated inthe previous sections\n- The features are extracted from images using FaceNet\n- The features are subtracted to get a single tensor\n- ANN used for the final classification","91a87a0c":"### Replacing sum by cosine and euclidean distance \n","6257fd9c":"### Features Subtraction\n\n- get absolute values of features subtracted between two sample images \n- sum all the absolute values for each array\n- if there is a visual difference between positive and negative samples","538401ee":"### SVM Classifier \nVisually there is a difference between similar images is lower than the difference between the images that contains the different people. We apply SVM to see accuracy of the model."}}