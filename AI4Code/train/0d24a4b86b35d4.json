{"cell_type":{"6ac624d7":"code","999af60b":"code","6d00c9ca":"code","fee79600":"code","9d58f46f":"code","64a5f821":"code","bd042a97":"code","76c61999":"code","38ea4ba1":"code","e38b9458":"code","49f9b8a7":"code","3474bf42":"code","8d6f468f":"code","2ba02b38":"code","6c90429c":"code","39e6021b":"code","3854dd64":"code","a0ecd5f8":"code","f5a84290":"code","cc2bccd2":"code","0c00d835":"code","d7e992fe":"code","68da0c54":"code","f4971260":"code","793b1f68":"markdown","dd05100f":"markdown","304a3b62":"markdown","a89c46cf":"markdown","7d842005":"markdown","73813387":"markdown","bbf927d5":"markdown","687dc853":"markdown","7854ed20":"markdown","afd81993":"markdown","9dea0704":"markdown","9de15a7e":"markdown","9bf26c10":"markdown","ec0f91fa":"markdown","55e139f6":"markdown","7f50f746":"markdown","b9f80674":"markdown","b45673f6":"markdown","f3a488eb":"markdown","7bd9158d":"markdown"},"source":{"6ac624d7":"# \u52a0\u8f7d\u5fc5\u8981\u7684\u7a0b\u5e8f\u5305\n# PyTorch\u7684\u7a0b\u5e8f\u5305\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# \u6570\u503c\u8fd0\u7b97\u548c\u7ed8\u56fe\u7684\u7a0b\u5e8f\u5305\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n\n# \u52a0\u8f7d\u673a\u5668\u5b66\u4e60\u7684\u8f6f\u4ef6\u5305\nfrom sklearn.decomposition import PCA\n\n#\u52a0\u8f7dWord2Vec\u7684\u8f6f\u4ef6\u5305\nimport gensim as gensim\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.word2vec import LineSentence\n\n#\u52a0\u8f7d\u2018\u7ed3\u5df4\u2019\u4e2d\u6587\u5206\u8bcd\u8f6f\u4ef6\u5305\n\nimport jieba\n\n#\u52a0\u8f7d\u6b63\u5219\u8868\u8fbe\u5f0f\u5904\u7406\u7684\u5305\nimport re\n\n%matplotlib inline","999af60b":"#\u8bfb\u5165\u539f\u59cb\u6587\u4ef6\n\n#f = open(\"..\/input\/\u4e09\u4f53.txt\", 'r', encoding='utf-8')\n# \u82e5\u60f3\u52a0\u5feb\u8fd0\u884c\u901f\u5ea6\uff0c\u4f7f\u7528\u4e0b\u9762\u7684\u8bed\u53e5\uff08\u9009\u7528\u4e86\u4e09\u4f53\u7684\u5176\u4e2d\u4e00\u7ae0\uff09\uff1a\nf = open(\"..\/input\/wordvector\/3body.txt\", 'r', encoding='utf-8') \ntext = str(f.read())\nf.close()\n\ntext","6d00c9ca":"# \u5206\u8bcd\ntemp = jieba.lcut(text)\nprint(len(temp))\nwords = []\nfor i in temp:\n    #\u8fc7\u6ee4\u6389\u6240\u6709\u7684\u6807\u70b9\u7b26\u53f7\n    i = re.sub(\"[\\s+\\.\\!\\\/_,$%^*(+\\\"\\'\u201c\u201d\u300a\u300b?\u201c]+|[+\u2014\u2014\uff01\uff0c\u3002\uff1f\u3001~@#\uffe5%\u2026\u2026&*\uff08\uff09\uff1a]+\", \"\", i)\n    if len(i) > 0:\n        words.append(i)\nprint(len(words))\nwords","fee79600":"# \u6784\u5efa\u4e09\u5143\u7ec4\u5217\u8868.  \u6bcf\u4e00\u4e2a\u5143\u7d20\u4e3a\uff1a ([ i-2\u4f4d\u7f6e\u7684\u8bcd, i-1\u4f4d\u7f6e\u7684\u8bcd ], \u4e0b\u4e00\u4e2a\u8bcd)\n# \u6211\u4eec\u9009\u62e9\u7684Ngram\u4e2d\u7684N\uff0c\u5373\u7a97\u53e3\u5927\u5c0f\u4e3a2\ntrigrams = [([words[i], words[i + 1]], words[i + 2]) for i in range(len(words) - 2)]\n# \u6253\u5370\u51fa\u524d\u4e09\u4e2a\u5143\u7d20\u770b\u770b\nprint(trigrams[:3])","9d58f46f":"# \u5f97\u5230\u8bcd\u6c47\u8868\nvocab = set(words)\nprint(len(vocab))\n# \u4e24\u4e2a\u5b57\u5178\uff0c\u4e00\u4e2a\u6839\u636e\u5355\u8bcd\u7d22\u5f15\u5176\u7f16\u53f7\uff0c\u4e00\u4e2a\u6839\u636e\u7f16\u53f7\u7d22\u5f15\u5355\u8bcd\n#word_to_idx\u4e2d\u7684\u503c\u5305\u542b\u4e24\u90e8\u5206\uff0c\u4e00\u90e8\u5206\u4e3aid\uff0c\u53e6\u4e00\u90e8\u5206\u4e3a\u5355\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\n#word_to_idx\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u5f62\u5982\uff1a{w:[id, count]}\uff0c\u5176\u4e2dw\u4e3a\u4e00\u4e2a\u8bcd\uff0cid\u4e3a\u8be5\u8bcd\u7684\u7f16\u53f7\uff0ccount\u4e3a\u8be5\u5355\u8bcd\u5728words\u5168\u6587\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\nword_to_idx = {} \nidx_to_word = {}\nids = 0\n\n#\u5bf9\u5168\u6587\u5faa\u73af\uff0c\u6784\u5efa\u8fd9\u4e24\u4e2a\u5b57\u5178\nfor w in words:\n    cnt = word_to_idx.get(w, [ids, 0])\n    if cnt[1] == 0:\n        ids += 1\n    cnt[1] += 1\n    word_to_idx[w] = cnt\n    idx_to_word[ids] = w","64a5f821":"word_to_idx","bd042a97":"class NGram(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGram, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  #\u5d4c\u5165\u5c42\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128) #\u7ebf\u6027\u5c42\n        self.linear2 = nn.Linear(128, vocab_size) #\u7ebf\u6027\u5c42\n\n    def forward(self, inputs):\n        #\u5d4c\u5165\u8fd0\u7b97\uff0c\u5d4c\u5165\u8fd0\u7b97\u5728\u5185\u90e8\u5206\u4e3a\u4e24\u6b65\uff1a\u5c06\u8f93\u5165\u7684\u5355\u8bcd\u7f16\u7801\u6620\u5c04\u4e3aone hot\u5411\u91cf\u8868\u793a\uff0c\u7136\u540e\u7ecf\u8fc7\u4e00\u4e2a\u7ebf\u6027\u5c42\u5f97\u5230\u5355\u8bcd\u7684\u8bcd\u5411\u91cf\n        #inputs\u7684\u5c3a\u5bf8\u4e3a\uff1a1*context_size\n        embeds = self.embeddings(inputs)\n        #embeds\u7684\u5c3a\u5bf8\u4e3a: context_size*embedding_dim\n        embeds = embeds.view(1, -1)\n        #\u6b64\u65f6embeds\u7684\u5c3a\u5bf8\u4e3a\uff1a1*embedding_dim\n        # \u7ebf\u6027\u5c42\u52a0ReLU\n        out = self.linear1(embeds)\n        out = F.relu(out)\n        #\u6b64\u65f6out\u7684\u5c3a\u5bf8\u4e3a1*128\n        \n        # \u7ebf\u6027\u5c42\u52a0Softmax\n        out = self.linear2(out)\n        #\u6b64\u65f6out\u7684\u5c3a\u5bf8\u4e3a\uff1a1*vocab_size\n        log_probs = F.log_softmax(out, dim = 1)\n        return log_probs\n    def extract(self, inputs):\n        embeds = self.embeddings(inputs)\n        return embeds","76c61999":"losses = [] #\u7eaa\u5f55\u6bcf\u4e00\u6b65\u7684\u635f\u5931\u51fd\u6570\ncriterion = nn.NLLLoss() #\u8fd0\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff08\u5e38\u7528\u4e8e\u591a\u5206\u7c7b\u95ee\u9898\u7684\u76ee\u6807\u51fd\u6570\uff09\nmodel = NGram(len(vocab), 10, 2) #\u5b9a\u4e49NGram\u6a21\u578b\uff0c\u5411\u91cf\u5d4c\u5165\u7ef4\u6570\u4e3a10\u7ef4\uff0cN\uff08\u7a97\u53e3\u5927\u5c0f\uff09\u4e3a2\noptimizer = optim.SGD(model.parameters(), lr=0.001) #\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f5c\u4e3a\u4f18\u5316\u5668\n\n#\u5faa\u73af100\u4e2a\u5468\u671f\nfor epoch in range(100):\n    total_loss = torch.Tensor([0])\n    for context, target in trigrams:\n\n        # \u51c6\u5907\u597d\u8f93\u5165\u6a21\u578b\u7684\u6570\u636e\uff0c\u5c06\u8bcd\u6c47\u6620\u5c04\u4e3a\u7f16\u7801\n        context_idxs = [word_to_idx[w][0] for w in context]\n        \n        # \u5305\u88c5\u6210PyTorch\u7684Variable\n        context_var = torch.tensor(context_idxs, dtype = torch.long)\n\n        # \u6e05\u7a7a\u68af\u5ea6\uff1a\u6ce8\u610fPyTorch\u4f1a\u5728\u8c03\u7528backward\u7684\u65f6\u5019\u81ea\u52a8\u79ef\u7d2f\u68af\u5ea6\u4fe1\u606f\uff0c\u6545\u800c\u6bcf\u9694\u5468\u671f\u8981\u6e05\u7a7a\u68af\u5ea6\u4fe1\u606f\u4e00\u6b21\u3002\n        optimizer.zero_grad()\n\n        # \u7528\u795e\u7ecf\u7f51\u7edc\u505a\u8ba1\u7b97\uff0c\u8ba1\u7b97\u5f97\u5230\u8f93\u51fa\u7684\u6bcf\u4e2a\u5355\u8bcd\u7684\u53ef\u80fd\u6982\u7387\u5bf9\u6570\u503c\n        log_probs = model(context_var)\n\n        # \u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u540c\u6837\u9700\u8981\u628a\u76ee\u6807\u6570\u636e\u8f6c\u5316\u4e3a\u7f16\u7801\uff0c\u5e76\u5305\u88c5\u4e3aVariable\n        loss = criterion(log_probs, torch.tensor([word_to_idx[target][0]], dtype = torch.long))\n\n        # \u68af\u5ea6\u53cd\u4f20\n        loss.backward()\n        \n        # \u5bf9\u7f51\u7edc\u8fdb\u884c\u4f18\u5316\n        optimizer.step()\n        \n        # \u7d2f\u52a0\u635f\u5931\u51fd\u6570\u503c\n        total_loss += loss.data\n    losses.append(total_loss)\n    print('\u7b2c{}\u8f6e\uff0c\u635f\u5931\u51fd\u6570\u4e3a\uff1a{:.2f}'.format(epoch, total_loss.numpy()[0]))","38ea4ba1":"# \u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u63d0\u53d6\u6bcf\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\nvec = model.extract(torch.tensor([v[0] for v in word_to_idx.values()], dtype = torch.long))\nvec = vec.data.numpy()\n\n# \u5229\u7528PCA\u7b97\u6cd5\u8fdb\u884c\u964d\u7ef4\nX_reduced = PCA(n_components=2).fit_transform(vec)\n\n\n# \u7ed8\u5236\u6240\u6709\u5355\u8bcd\u5411\u91cf\u7684\u4e8c\u7ef4\u7a7a\u95f4\u6295\u5f71\nfig = plt.figure(figsize = (30, 20))\nax = fig.gca()\nax.set_facecolor('white')\nax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.4, color = 'black')\n\n\n# \u7ed8\u5236\u51e0\u4e2a\u7279\u6b8a\u5355\u8bcd\u7684\u5411\u91cf\nwords = ['\u667a\u5b50', '\u5730\u7403', '\u4e09\u4f53', '\u8d28\u5b50', '\u79d1\u5b66', '\u4e16\u754c', '\u6587\u660e', '\u592a\u7a7a', '\u52a0\u901f\u5668', '\u5e73\u9762', '\u5b87\u5b99', '\u4fe1\u606f']\n\n# \u8bbe\u7f6e\u4e2d\u6587\u5b57\u4f53\uff0c\u5426\u5219\u65e0\u6cd5\u5728\u56fe\u5f62\u4e0a\u663e\u793a\u4e2d\u6587\nzhfont1 = matplotlib.font_manager.FontProperties(fname='..\/input\/wordvector\/\u534e\u6587\u4eff\u5b8b.ttf', size=16)\nfor w in words:\n    if w in word_to_idx:\n        ind = word_to_idx[w][0]\n        xy = X_reduced[ind]\n        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'red')\n        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'black')","e38b9458":"# \u5b9a\u4e49\u8ba1\u7b97cosine\u76f8\u4f3c\u5ea6\u7684\u51fd\u6570\ndef cos_similarity(vec1, vec2):\n    \n    norm1 = np.linalg.norm(vec1)\n    norm2 = np.linalg.norm(vec2)\n    norm = norm1 * norm2\n    dot = np.dot(vec1, vec2)\n    result = dot \/ norm if norm > 0 else 0\n    return result\n    \n# \u5728\u6240\u6709\u7684\u8bcd\u5411\u91cf\u4e2d\u5bfb\u627e\u5230\u4e0e\u76ee\u6807\u8bcd\uff08word\uff09\u76f8\u8fd1\u7684\u5411\u91cf\uff0c\u5e76\u6309\u76f8\u4f3c\u5ea6\u8fdb\u884c\u6392\u5217\ndef find_most_similar(word, vectors, word_idx):\n    vector = vectors[word_to_idx[word][0]]\n    simi = [[cos_similarity(vector, vectors[num]), key] for num, key in enumerate(word_idx.keys())]\n    sort = sorted(simi)[::-1]\n    words = [i[1] for i in sort]\n    return words\n\n# \u4e0e\u667a\u5b50\u9760\u8fd1\u7684\u8bcd\u6c47\nfind_most_similar('\u667a\u5b50', vec, word_to_idx)","49f9b8a7":"# \u8bfb\u5165\u6587\u4ef6\u3001\u5206\u8bcd\uff0c\u5f62\u6210\u4e00\u53e5\u4e00\u53e5\u7684\u8bed\u6599\n# \u6ce8\u610f\u8ddf\u524d\u9762\u5904\u7406\u4e0d\u4e00\u6837\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u6211\u4eec\u4e00\u884c\u4e00\u884c\u5730\u8bfb\u5165\u6587\u4ef6\uff0c\u4ece\u800c\u81ea\u7136\u5229\u7528\u884c\u5c06\u6587\u7ae0\u5206\u5f00\u6210\u201c\u53e5\u5b50\u201d\nf = open(\"..\/input\/wordvector\/\u4e09\u4f53.txt\", 'r', encoding='utf-8')\nlines = []\nfor line in f:\n    temp = jieba.lcut(line)\n    words = []\n    for i in temp:\n        #\u8fc7\u6ee4\u6389\u6240\u6709\u7684\u6807\u70b9\u7b26\u53f7\n        i = re.sub(\"[\\s+\\.\\!\\\/_,$%^*(+\\\"\\'\u201d\u201d\u300a\u300b]+|[+\u2014\u2014\uff01\uff0c\u3002\uff1f\u3001~@#\uffe5%\u2026\u2026&*\uff08\uff09\uff1a\uff1b\u2018]+\", \"\", i)\n        if len(i) > 0:\n            words.append(i)\n    if len(words) > 0:\n        lines.append(words)\n    ","3474bf42":"# \u8c03\u7528Word2Vec\u7684\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002\n# \u53c2\u6570\u5206\u522b\u4e3a\uff1asize: \u5d4c\u5165\u540e\u7684\u8bcd\u5411\u91cf\u7ef4\u5ea6\uff1bwindow: \u4e0a\u4e0b\u6587\u7684\u5bbd\u5ea6\uff0cmin_count\u4e3a\u8003\u8651\u8ba1\u7b97\u7684\u5355\u8bcd\u7684\u6700\u4f4e\u8bcd\u9891\u9608\u503c\nmodel = Word2Vec(lines, size = 20, window = 2 , min_count = 0)","8d6f468f":"model.wv.most_similar('\u667a\u5b50', topn = 20)","2ba02b38":"# \u5c06\u8bcd\u5411\u91cf\u6295\u5f71\u5230\u4e8c\u7ef4\u7a7a\u95f4\nrawWordVec = []\nword2ind = {}\nfor i, w in enumerate(model.wv.vocab):\n    rawWordVec.append(model[w])\n    word2ind[w] = i\nrawWordVec = np.array(rawWordVec)\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)","6c90429c":"# \u7ed8\u5236\u661f\u7a7a\u56fe\n# \u7ed8\u5236\u6240\u6709\u5355\u8bcd\u5411\u91cf\u7684\u4e8c\u7ef4\u7a7a\u95f4\u6295\u5f71\nfig = plt.figure(figsize = (15, 10))\nax = fig.gca()\nax.set_facecolor('white')\nax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.3, color = 'black')\n\n\n# \u7ed8\u5236\u51e0\u4e2a\u7279\u6b8a\u5355\u8bcd\u7684\u5411\u91cf\nwords = ['\u667a\u5b50', '\u5730\u7403', '\u4e09\u4f53', '\u8d28\u5b50', '\u79d1\u5b66', '\u4e16\u754c', '\u6587\u660e', '\u592a\u7a7a', '\u52a0\u901f\u5668', '\u5e73\u9762', '\u5b87\u5b99', '\u8fdb\u5c55','\u7684']\n\n# \u8bbe\u7f6e\u4e2d\u6587\u5b57\u4f53\uff0c\u5426\u5219\u65e0\u6cd5\u5728\u56fe\u5f62\u4e0a\u663e\u793a\u4e2d\u6587\nzhfont1 = matplotlib.font_manager.FontProperties(fname='..\/input\/wordvector\/\u534e\u6587\u4eff\u5b8b.ttf', size=16)\nfor w in words:\n    if w in word2ind:\n        ind = word2ind[w]\n        xy = X_reduced[ind]\n        plt.plot(xy[0], xy[1], '.', alpha =1, color = 'green')\n        plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = 'blue')","39e6021b":"# \u52a0\u8f7d\u8bcd\u5411\u91cf\nword_vectors = KeyedVectors.load_word2vec_format('..\/input\/chinese-word-vector\/vectors.bin', binary=True, unicode_errors='ignore')\nlen(word_vectors.vocab)","3854dd64":"# PCA\u964d\u7ef4\nrawWordVec = []\nword2ind = {}\nfor i, w in enumerate(word_vectors.vocab):\n    rawWordVec.append(word_vectors[w])\n    word2ind[w] = i\nrawWordVec = np.array(rawWordVec)\nX_reduced = PCA(n_components=2).fit_transform(rawWordVec)","a0ecd5f8":"# \u67e5\u770b\u76f8\u4f3c\u8bcd\nword_vectors.most_similar('\u7269\u7406', topn = 20)","f5a84290":"# \u7ed8\u5236\u661f\u7a7a\u56fe\n# \u7ed8\u5236\u6240\u6709\u7684\u8bcd\u6c47\nfig = plt.figure(figsize = (30, 15))\nax = fig.gca()\nax.set_facecolor('black')\nax.plot(X_reduced[:, 0], X_reduced[:, 1], '.', markersize = 1, alpha = 0.1, color = 'white')\n\nax.set_xlim([-12,12])\nax.set_ylim([-10,20])\n\n\n# \u9009\u62e9\u51e0\u4e2a\u7279\u6b8a\u8bcd\u6c47\uff0c\u4e0d\u4ec5\u753b\u5b83\u4eec\u7684\u4f4d\u7f6e\uff0c\u800c\u4e14\u628a\u5b83\u4eec\u7684\u4e34\u8fd1\u8bcd\u4e5f\u753b\u51fa\u6765\nwords = {'\u5f90\u9759\u857e','\u5434\u4ea6\u51e1','\u7269\u7406','\u7ea2\u697c\u68a6','\u91cf\u5b50'}\nall_words = []\nfor w in words:\n    lst = word_vectors.most_similar(w)\n    wds = [i[0] for i in lst] # \u8bcd\n    metrics = [i[1] for i in lst] # \u76f8\u4f3c\u5ea6\n    wds = np.append(wds, w)\n    all_words.append(wds)\n\n\nzhfont1 = matplotlib.font_manager.FontProperties(fname='..\/input\/wordvector\/\u534e\u6587\u4eff\u5b8b.ttf', size=16)\ncolors = ['red', 'yellow', 'orange', 'green', 'cyan', 'cyan']\nfor num, wds in enumerate(all_words):\n    for w in wds:\n        if w in word2ind:\n            ind = word2ind[w] # \u83b7\u53d6index\n            xy = X_reduced[ind] # \u83b7\u53d6\u6bcf\u4e2aword\u7684\u4e8c\u7ef4\u5750\u6807\n            plt.plot(xy[0], xy[1], '.', alpha =1, color = colors[num])\n            plt.text(xy[0], xy[1], w, fontproperties = zhfont1, alpha = 1, color = colors[num])\nplt.savefig('88.png',dpi =600)","cc2bccd2":"# \u5973\u4eba\uff0d\u7537\u4eba\uff1d\uff1f\uff0d\u56fd\u738b\nwords = word_vectors.most_similar(positive=['\u5973\u4eba', '\u56fd\u738b'], negative=['\u7537\u4eba'])\nwords","0c00d835":"# \u5317\u4eac\uff0d\u4e2d\u56fd\uff1d\uff1f\uff0d\u4fc4\u7f57\u65af\nwords = word_vectors.most_similar(positive=['\u5317\u4eac', '\u4fc4\u7f57\u65af'], negative=['\u4e2d\u56fd'])\nwords","d7e992fe":"# \u81ea\u7136\u79d1\u5b66\uff0d\u7269\u7406\u5b66\uff1d\uff1f\uff0d\u653f\u6cbb\u5b66\nwords = word_vectors.most_similar(positive=['\u81ea\u7136\u79d1\u5b66', '\u653f\u6cbb\u5b66'], negative=['\u7269\u7406\u5b66'])\nwords","68da0c54":"# \u738b\u83f2\uff0d\u7ae0\u5b50\u6021\uff1d\uff1f\uff0d\u6c6a\u5cf0\nwords = word_vectors.most_similar(positive=['\u738b\u83f2', '\u6c6a\u5cf0'], negative=['\u7ae0\u5b50\u6021'])\nwords","f4971260":"# \u5c3d\u53ef\u80fd\u591a\u5730\u9009\u51fa\u6240\u6709\u7684\u8d27\u5e01\nwords = word_vectors.most_similar(positive=['\u7f8e\u5143', '\u82f1\u9551'], topn = 100)\nwords = word_vectors.most_similar(positive=['\u7f8e\u5143', '\u82f1\u9551', '\u65e5\u5143'], topn = 100)\n#words = word_vectors.most_similar(positive=['\u7f8e\u5143', '\u82f1\u9551', '\u65e5\u5143'], negative = ['\u539f\u6cb9\u4ef7\u683c', '7800\u4e07'], topn = 100)\nwords","793b1f68":"### 2\u3001\u6784\u9020\u6a21\u578b\u5e76\u8bad\u7ec3","dd05100f":"#### 2). \u7c7b\u6bd4\u5173\u7cfb\u5b9e\u9a8c","304a3b62":"# \u8bcd\u6c47\u7684\u661f\u7a7a\u2014\u2014\u8bcd\u5411\u91cf\uff08Word Vector\uff09\u6280\u672f\n\n\u5728\u8fd9\u8282\u8bfe\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u901a\u8fc7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5f97\u5230\u5355\u8bcd\u7684\u5411\u91cf\u8868\u8fbe\u3002\n\n\u9996\u5148\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684NGram\u8bed\u8a00\u6a21\u578b\uff0c\u6839\u636eN\u4e2a\u5386\u53f2\u8bcd\u6c47\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\uff0c\u4ece\u800c\u5f97\u5230\u6bcf\u4e00\u4e2a\u5355\u8bcd\u7684\u5411\u91cf\u8868\u793a\u3002\u6211\u4eec\u7528\u5c0f\u8bf4\u300a\u4e09\u4f53\u300b\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u8bcd\u5411\u91cf\u5d4c\u5165\u6548\u679c\u3002\n\n\u5176\u6b21\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528\u6210\u719f\u7684Google\u5f00\u53d1\u7684Word2Vec\u5305\u6765\u8fdb\u884c\u5927\u89c4\u6a21\u8bed\u6599\u7684\u8bcd\u5411\u91cf\u8bad\u7ec3\uff0c\u4ee5\u53ca\u5982\u4f55\u52a0\u8f7d\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\uff0c\u4ece\u800c\u5229\u7528\u8fd9\u4e9b\u8bcd\u5411\u91cf\u6765\u505a\u4e00\u4e9b\u7b80\u5355\u7684\u8fd0\u7b97\u548c\u6d4b\u8bd5\u3002\n\n\u672c\u6587\u4ef6\u662f\u96c6\u667aAI\u5b66\u56edhttp:\/\/campus.swarma.org \u51fa\u54c1\u7684\u201c\u706b\u70ac\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u201d\u7b2cVI\u8bfe\u7684\u914d\u5957\u6e90\u4ee3\u7801","a89c46cf":"#### 4). \u5c06\u6bcf\u4e2a\u5355\u8bcd\u8fdb\u884c\u7f16\u7801\uff0c\u6784\u9020\u8bcd\u5178","7d842005":"#### 2). \u4e34\u8fd1\u8bcd\u5411\u91cf","73813387":"### 3. \u7ed3\u679c\u5c55\u793a\n#### 1). \u5c06\u5411\u91cf\u6295\u5f71\u5230\u4e8c\u7ef4\u5e73\u9762\u8fdb\u884c\u53ef\u89c6\u5316","bbf927d5":"#### 1). \u6784\u9020NGram\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\n\n\u6211\u4eec\u6784\u9020\u4e86\u4e00\u4e2a\u4e09\u5c42\u7684\u7f51\u7edc\uff1a\n\n1\u3001\u8f93\u5165\u5c42\uff1aembedding\u5c42\uff0c\u8fd9\u4e00\u5c42\u7684\u4f5c\u7528\u662f\uff1a\u5148\u5c06\u8f93\u5165\u5355\u8bcd\u7684\u7f16\u53f7\u6620\u5c04\u4e3a\u4e00\u4e2aone hot\u7f16\u7801\u7684\u5411\u91cf\uff0c\u5f62\u5982\uff1a001000\uff0c\u7ef4\u5ea6\u4e3a\u5355\u8bcd\u8868\u5927\u5c0f\u3002\n\u7136\u540e\uff0cembedding\u4f1a\u901a\u8fc7\u4e00\u4e2a\u7ebf\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u6620\u5c04\u5230\u8fd9\u4e2a\u8bcd\u7684\u5411\u91cf\u8868\u793a\uff0c\u8f93\u51fa\u4e3aembedding_dim\n\n2\u3001\u7ebf\u6027\u5c42\uff0c\u4eceembedding_dim\u7ef4\u5ea6\u5230128\u7ef4\u5ea6\uff0c\u7136\u540e\u7ecf\u8fc7\u975e\u7ebf\u6027ReLU\u51fd\u6570\n\n3\u3001\u7ebf\u6027\u5c42\uff1a\u4ece128\u7ef4\u5ea6\u5230\u5355\u8bcd\u8868\u5927\u5c0f\u7ef4\u5ea6\uff0c\u7136\u540elog softmax\u51fd\u6570\uff0c\u7ed9\u51fa\u9884\u6d4b\u6bcf\u4e2a\u5355\u8bcd\u7684\u6982\u7387","687dc853":"NGram\u8bcd\u5411\u91cf\u6a21\u578b\u7684\u539f\u7406\u662f\u5229\u7528\u4e00\u4e2a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6765\u6839\u636e\u524dN\u4e2a\u5355\u8bcd\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\uff0c\u4ece\u800c\u5f97\u5230\u6bcf\u4e2a\u5355\u8bcd\u7684\u8bcd\u5411\u91cf","7854ed20":"\u672c\u6587\u4ef6\u662f\u96c6\u667aAI\u5b66\u56edhttp:\/\/campus.swarma.org \u51fa\u54c1\u7684\u201c\u706b\u70ac\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u201d\u7b2cIII\u8bfe\u7684\u914d\u5957\u6e90\u4ee3\u7801","afd81993":"#### 1). \u8bfb\u5165\u539f\u59cb\u6587\u4ef6\uff0c\u7b5b\u6389\u6240\u6709\u6807\u70b9\u7b26\u53f7","9dea0704":"1). \u5927\u89c4\u6a21\u8bcd\u5411\u91cf\u53ef\u89c6\u5316\n\n\u8be5\u4e2d\u6587\u8bcd\u5411\u91cf\u5e93\u662f\u7531\u5c39\u76f8\u5fd7\u63d0\u4f9b\uff0c\u8bad\u7ec3\u8bed\u6599\u6765\u6e90\u4e3a\uff1a\u5fae\u535a\u3001\u4eba\u6c11\u65e5\u62a5\u3001\u4e0a\u6d77\u70ed\u7ebf\u3001\u6c7d\u8f66\u4e4b\u5bb6\u7b49\uff0c\u5305\u542b1366130\u4e2a\u8bcd\u5411\u91cf\uff0c\n\u4e0b\u8f7d\u5730\u5740\u4e3a\uff1a\u94fe\u63a5\uff1ahttp:\/\/pan.baidu.com\/s\/1gePQAun \u5bc6\u7801\uff1akvtg\n\n\u672c\u6587\u4ef6\u662f\u96c6\u667aAI\u5b66\u56edhttp:\/\/campus.swarma.org \u51fa\u54c1\u7684\u201c\u706b\u70ac\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u201d\u7b2cVI\u8bfe\u7684\u914d\u5957\u6e90\u4ee3\u7801","9de15a7e":"Word2Vec\u662fGoogle\u63a8\u51fa\u7684\u4e00\u4e2a\u5f00\u6e90\u7684\u8bcd\u5411\u91cf\u8ba1\u7b97\u5de5\u5177\uff0c\u5b83\u88ab\u5185\u5d4c\u5230\u4e86gensim\u8f6f\u4ef6\u5305\u91cc\u3002\n\u5728\u672c\u8bfe\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e3b\u8981\u5c55\u793a\u4e86\u81ea\u5df1\u8c03\u7528Word2Vec\u8bad\u7ec3\u8bcd\u5411\u91cf\u548c\u8bfb\u53d6\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u8bcd\u5411\u91cf\u6765\u8fdb\u884c\u4e00\u4e9b\u7b80\u5355\u7684\u63a8\u7406","9bf26c10":"## \u4e8c\u3001Word2Vec","ec0f91fa":"### 2\u3001\u8c03\u7528\u4e00\u4e2a\u73b0\u6210\u7684\u8bcd\u5411\u91cf","55e139f6":"### 1. \u81ea\u5df1\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u8bed\u6599","7f50f746":"## \u4e00\u3001NGram \u8bcd\u5411\u91cf\u6a21\u578b","b9f80674":"### 1. \u6587\u672c\u9884\u5904\u7406\n\n\u6211\u4eec\u4ee5\u5218\u6148\u6b23\u8457\u540d\u7684\u79d1\u5e7b\u5c0f\u8bf4\u300a\u4e09\u4f53\u300b\u4e3a\u4f8b\uff0c\u6765\u5c55\u793a\u5229\u7528NGram\u6a21\u578b\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u65b9\u6cd5\n\n\u9884\u5904\u7406\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\n\n1\u3001\u8bfb\u53d6\u6587\u4ef6\n\n2\u3001\u5206\u8bcd\n\n3\u3001\u5c06\u8bed\u6599\u5212\u5206\u4e3aN\uff0b1\u5143\u7ec4\uff0c\u51c6\u5907\u597d\u8bad\u7ec3\u7528\u6570\u636e\n\n\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5e76\u6ca1\u6709\u53bb\u9664\u6807\u70b9\u7b26\u53f7\uff0c\u4e00\u662f\u4e3a\u4e86\u7f16\u7a0b\u7b80\u6d01\uff0c\u800c\u662f\u8003\u8651\u5230\u5206\u8bcd\u4f1a\u81ea\u52a8\u5c06\u6807\u70b9\u7b26\u53f7\u5f53\u4f5c\u4e00\u4e2a\u5355\u8bcd\u5904\u7406\uff0c\u56e0\u6b64\u4e0d\u9700\u8981\u989d\u5916\u8003\u8651\u3002","b45673f6":"#### 2)\u3001\u5206\u8bcd\uff0c\u5e76\u53bb\u6389\u6807\u70b9\u7b26\u53f7","f3a488eb":"#### 3)\u3001\u6784\u5efaN+1\u5143\u7ec4\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u5bf9","7bd9158d":"#### 2). \u5f00\u59cb\u8bad\u7ec3\u6a21\u578b"}}