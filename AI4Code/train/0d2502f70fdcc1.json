{"cell_type":{"69c357f5":"code","40f922ce":"code","ecf164ce":"code","e588d4fe":"code","f1219a0a":"markdown","541b751e":"markdown","afe0d464":"markdown","11af56fb":"markdown"},"source":{"69c357f5":"import math\nfrom keras.callbacks import LambdaCallback\nimport keras.backend as K\n\n\nclass LRFinder:\n    \"\"\"\n    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n    See for details:\n    https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs['loss']\n        self.losses.append(loss)\n\n        # Check whether the loss got too large or NaN\n        if math.isnan(loss) or loss > self.best_loss * 4:\n            self.model.stop_training = True\n            return\n\n        if loss < self.best_loss:\n            self.best_loss = loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n        num_batches = epochs * x_train.shape[0] \/ batch_size\n        self.lr_mult = (float(end_lr) \/ float(start_lr)) ** (float(1) \/ float(num_batches))\n\n        # Save weights into a file\n        self.model.save_weights('tmp.h5')\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n        self.model.fit(x_train, y_train,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=[callback])\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights('tmp.h5')\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n        \"\"\"\n        Plots the loss.\n        Parameters:\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n        \"\"\"\n        plt.ylabel(\"loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n\n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        \"\"\"\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) \/ sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"rate of loss change\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n        plt.ylim(y_lim)","40f922ce":"import pandas as pd\nimport numpy as np\n\ndef read_vectors(filename):\n    return np.fromfile(filename, dtype=np.uint8).reshape(-1,401)\n\nsnk = np.vstack(tuple(read_vectors(\"..\/input\/snake-eyes\/snakeeyes_{:02d}.dat\".format(nn))\n                      for nn in range(2)))\nsnk_y = snk[:,0]\nsnk_X = snk[:,1:]\n\nimport matplotlib.pyplot as plt\nplt.title(f'class == {snk_y[0]}')\nplt.imshow(snk_X[0].reshape(20,20), cmap='Greys_r')","ecf164ce":"import keras\nfrom keras.layers import *\nfrom keras.models import Sequential\n\ninput_shape = (20, 20, 1)\nbatch_size = 128\nnum_classes = 12\nepochs = 5\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(),\n              metrics=['accuracy'])","e588d4fe":"lr_finder = LRFinder(model)\nlr_finder.find(snk_X.reshape((200000, 20, 20, 1)),\n               keras.utils.np_utils.to_categorical(snk_y)[:, 1:], \n               start_lr=10e-5, end_lr=1, batch_size=500, epochs=1)\nlr_finder.plot_loss(n_skip_beginning=20, n_skip_end=5)","f1219a0a":"## Finding an optimal learning rate with lr_finder","541b751e":"`lr_finder` is a utility function included in the neural network library that comes complimentary with the `fast.ai` course. The function was initially specified as a \"useful heuristic\" in [\"Cyclic learning rates for training neural networks\"](https:\/\/arxiv.org\/abs\/1506.01186)&mdash;although the subject of the paper is somewhat different from `lr_finder`.\n\nThe function has a Keras implementation in the form of [`keras_lr_finder`](https:\/\/github.com\/surmenok\/keras_lr_finder). Implementing the LR finder algorithm isn't difficult. It trains the model one at a time, resetting the fitted result back to the original random values after each batch is fitted, then incrementing the learning rate in the search space logorithmically on its path to the maximum learning rate. The number of data points tried in the logorithmic search space is equivalent to `epochs * n_batches`. Here is the implementation, ripped directly from the helper library:","afe0d464":"## Demo\n\nLet's demonstrate how this works using the snake eyes dataset.","11af56fb":"The point of optimality in the initial training rate is the point on the training rate plot where the slope is steepest in the downwards direction. That's because this plot shows the loss after one epoch. Points before the steepest slope are training too slowly. Points after the steepest slope are at risk of training too quickly: usually, but not always (it didn't happen in this demo case), they will fall off the mountain in terms of loss because they jump past the point of optimality.\n\nIn this case we see that a good initial learning weight value is halfway between 10e-3 and 10e-2 (0.01 and 0.001). That's around 0.005.\n\nThe learning rate that you derive this way is probably only good as a starting point. You can still benefit from the usual downtuning procedures for optimizing the learning rate later in model training.\n\nTo learn more about learning rate tuning read the blog post \"[Estimating an optimal learning rate for a deep neural network](https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)\"."}}