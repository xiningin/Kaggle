{"cell_type":{"1b850b92":"code","8a976069":"code","d7240a80":"code","964c6041":"code","76c1b96d":"code","32f99c58":"code","555f2cb0":"code","0a687ad9":"code","2b715bb0":"code","847013c9":"code","af40a01e":"code","d9bf9477":"code","2c3ae792":"code","71ba68b9":"code","a5f07212":"code","d3dbfeb8":"code","ef2f06de":"code","a5287e04":"code","ede3d72a":"code","06e0d664":"code","4ac3d1f1":"code","b0dac672":"code","eadff765":"code","0e2dec5c":"code","40309308":"code","c2da6033":"code","c46c2174":"code","9f0e4682":"code","0718ca79":"code","ee66741f":"code","fed0ba88":"code","e8eb4192":"code","e1037cf5":"code","6801ce0f":"code","fd916c2e":"code","9ec27824":"code","cb09a975":"code","7d49ff49":"code","8992c15a":"code","d9e35aef":"code","2d68685a":"code","1801e8b3":"code","16a976fb":"code","ee8d4b05":"code","42eab937":"code","d39b12a4":"code","b6dad8ea":"code","4a08abbe":"code","5d3e54ef":"code","ca068b01":"code","2c3cc954":"code","62d4fe2b":"code","29e561ac":"code","af921242":"code","02dbb517":"code","8e5add73":"code","987defe1":"code","e5cf7ea8":"code","ace47c49":"code","afdc48b5":"markdown","3000c44f":"markdown","54a33fd1":"markdown","2da9f8d7":"markdown","dc38d0f8":"markdown","59dad7fe":"markdown","e9f410fb":"markdown","fe0248b0":"markdown","33b74da6":"markdown","876543ee":"markdown","f2a4af9c":"markdown","3081b93d":"markdown","8d31f8a3":"markdown","8071b20e":"markdown","fc0c1217":"markdown","b1722f80":"markdown","f15ce034":"markdown","4d4b1302":"markdown","6bfbcbe4":"markdown","c87f9cec":"markdown","db9d594c":"markdown","b24ff9ba":"markdown","2a5ab92a":"markdown","464f9ee4":"markdown","c1fe70ec":"markdown","e5d93ca4":"markdown","09243bf0":"markdown"},"source":{"1b850b92":"# Main Libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport math\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# import operator for dictionary sorting operations\nimport operator\n\n# preprocessing imports\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\n\n# linear regression models\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR, LinearSVR\nfrom xgboost import XGBRegressor\n\n# \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\n# stacking\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n# ignore warnings\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\nwarnings.filterwarnings(\"ignore\", category=Warning)\nprint('Warnings will be ignored!')\n\n# suppress scientific notation\npd.set_option('display.float_format', lambda x: '%.5f' % x)","8a976069":"# create dataframes for exploration\norig = pd.read_csv('..\/input\/train.csv')\ntest_orig = pd.read_csv('..\/input\/test.csv')","d7240a80":"orig.head(20)","964c6041":"# quick glance at columns\norig.info()","76c1b96d":"# check out the nulls\nprint(orig.isnull().sum().sort_values(ascending=False).to_string())","32f99c58":"print(test_orig.isnull().sum().sort_values(ascending=False).to_string())","555f2cb0":"# print value counts for all 'objects' with more than 1 null value\ndef object_vcs_and_nulls(df):\n  for i in df:\n    if df[i].dtype == 'O':\n      if df[i].isnull().sum() > 0:\n        print(df[i].value_counts())  \n        print(\"Number of Null Values: \" + str(df[i].isnull().sum()))\n        print(\"Percentage of Nulls = \" + str(np.round((df[i].isnull().sum() \/ 14.60), 2)) + \"%\")\n        print(\"\\n\")\n      \nobject_vcs_and_nulls(orig)","0a687ad9":"def mean_prices_categorical_var(df):\n  for i in df:\n    if df[i].dtype == 'O':\n      print(\"=\" * 20)\n      print(orig.groupby(i, as_index=True)['SalePrice'].mean() )  \n      print(\"\\n\")\n      print(df[i].value_counts()) \n      print(\"\\n\")\n      \nprint(mean_prices_categorical_var(orig))","2b715bb0":"# import data sets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","847013c9":"# drop houses w\/ over 4000 sq ft of GrLivArea\ntrain = train.drop(train[train[\"GrLivArea\"] > 4000].index, inplace=False)","af40a01e":"# combine dataframes for simplicity\nfull = pd.concat([train,test], ignore_index=True)\n\n# fix a few incorrect values; these values either had typos or had 'garage built' after house was sold\nfull['GarageYrBlt'][2588] = 2007\nfull['GarageYrBlt'][2545] = 2007\nfull['YearBuilt'][2545] = 2007","d9bf9477":"def feature_transformations(df):\n  \n  # drop nulls\n  df['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)\n  df['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\n  df['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)\n  df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)\n  df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\n  df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)\n  df['GarageCars'] = df['GarageCars'].fillna(0)\n  df['GarageArea'] = df['GarageArea'].fillna(0) \n  \n  # feature transformations\n  df['TotalLivingSF'] = df['GrLivArea'] + df['TotalBsmtSF'] - df['LowQualFinSF']\n  df['TotalMainBath'] = df['FullBath'] + (df['HalfBath'] * 0.5)\n  df['TotalBath'] = df['TotalMainBath'] + df['BsmtFullBath'] + (df['BsmtHalfBath'] * 0.5)\n  df['AgeSold'] = df['YrSold'] - df['YearBuilt']\n  df['TotalPorchSF'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']\n  df['TotalSF'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF']\n  df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GarageArea']\n  \n  # garage year built nulls and transformation\n  df['GarageYrBlt'] = df['GarageYrBlt'].replace(np.nan, 1900)\n  df['GarageAgeSold'] = df['YrSold'] - df['GarageYrBlt']\n  \n  # other age\n  df['LastRemodelYrs'] = df['YrSold'] - df['YearRemodAdd']\n  df['LastRemodelYrs'] = df['LastRemodelYrs'].replace(-1, 0)\n  df['LastRemodelYrs'] = df['LastRemodelYrs'].replace(-2, 0)\n  \n  \n  return df\n  \nfull = feature_transformations(full)\ntrain = feature_transformations(train)","2c3ae792":"def mean_price_map(feature):\n  \n  le = LabelEncoder()    \n  feature = le.fit_transform(feature)  \n  mean_prices = train.groupby(feature, as_index=True)['SalePrice'].mean()\n  mean_price_length = len(mean_prices)\n  numbers = np.linspace(0, mean_price_length, (mean_price_length+1))\n  mean_price_dict = dict(zip(numbers, mean_prices))\n  return mean_price_dict\n    \n  \ndef median_price_map(feature):\n  \n  le = LabelEncoder()    \n  feature = le.fit_transform(feature)  \n  med_prices = train.groupby(feature, as_index=True)['SalePrice'].median()\n  med_price_length = len(med_prices)\n  numbers = np.linspace(0, med_price_length, (med_price_length+1))\n  med_price_dict = dict(zip(numbers, med_prices))\n  return med_price_dict\n\n\ndef mean_square_footage(feature):\n  \n  le = LabelEncoder()    \n  feature = le.fit_transform(feature)  \n  mean_sqft = train.groupby(feature, as_index=True)['TotalLivingSF'].mean()\n  mean_sqft_length = len(mean_sqft)\n  numbers = np.linspace(0, mean_sqft_length, (mean_sqft_length+1))\n  mean_sqft_dict = dict(zip(numbers, mean_sqft))\n  return mean_sqft_dict","71ba68b9":"def df_transform(df):\n  \n  # neighborhood\n  le = LabelEncoder()\n  df['Neighborhood'] = le.fit_transform(df['Neighborhood'])\n  df['NhoodMedianPrice'] = df['Neighborhood'].map(median_price_map(train['Neighborhood']))\n  df['NhoodMeanPrice'] = df['Neighborhood'].map(mean_price_map(train['Neighborhood']))\n  df['NhoodMeanSF'] = df['Neighborhood'].map(mean_square_footage(train['Neighborhood']))\n  df['NhoodMeanPricePerSF'] = df['NhoodMeanPrice'] \/ df['NhoodMeanSF']\n  df['ProxyPrice'] = df['NhoodMeanPricePerSF'] * df['TotalSF']\n\n# fixer upper score\n  df['FxUp_SaleCond'] = df.SaleCondition.map({'Partial': 0, 'Normal': 0, 'Alloca': 0, 'Family': 0, \n                                              'Abnorml': 3, 'AdjLand': 0, np.nan: 0}).astype(int)\n  df['FxUp_Foundation'] = df.Foundation.map({'PConc':0, 'Wood':0, 'Stone':0, 'CBlock':1, 'BrkTil': 0, 'Slab': 2, np.nan: 0}).astype(int)\n  df['FxUp_HeatingQC'] = df.HeatingQC.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':2, 'Po': 5, np.nan: 0}).astype(int)\n  df['FxUp_Heating'] = df.Heating.map({'GasA':0, 'GasW':0, 'OthW':2, 'Wall':3, 'Grav': 4, 'Floor': 4, np.nan: 0}).astype(int)\n  df['FxUp_CentralAir'] = df.CentralAir.map({'Y':0, 'N':6, np.nan: 0}).astype(int)\n  df['FxUp_GarageQual'] = df.GarageQual.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':1, 'Po': 3, np.nan: 0}).astype(int)\n  df['FxUp_PavedDrive'] = df['PavedDrive'].map({'Y':0, 'P':0, 'N':2, np.nan: 0}).astype(int)\n  df['FxUp_Electrical'] = df.Electrical.map({'SBrkr':0, 'FuseA':2, 'FuseF':2, 'FuseP':2, 'Mix': 4, np.nan: 0}).astype(int)\n  df['FxUp_MSZoning'] = df.MSZoning.map({'FV':0, 'RL':0, 'RM':0, 'RH':0, 'C (all)':3 , np.nan: 0}).astype(int)\n  df['FxUp_Street'] = df.Street.map({'Pave':0, 'Grvl':3, np.nan: 0}).astype(int)\n  df['FxUp_OverallQual'] = df.OverallQual.map({1: 5, 2: 5, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0})\n  df['FxUp_KitchenQual']= df.KitchenQual.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':1, 'Po': 4, np.nan: 0}).astype(int)\n  \n  df['FixerUpperScore'] = (45 - df['FxUp_SaleCond'] - df['FxUp_Foundation'] - df['FxUp_HeatingQC'] \n                           - df['FxUp_Heating'] - df['FxUp_CentralAir'] - df['FxUp_GarageQual'] - df['FxUp_PavedDrive'] -\n                           df['FxUp_Electrical'] - df['FxUp_MSZoning'] - df['FxUp_Street'] - df['FxUp_OverallQual'] - df['FxUp_KitchenQual'])\n  \n  \n  # map MSSubClass\n  df['MSSubClass'] = df['MSSubClass'].astype(str)\n  df['MSSubClass'] = df.MSSubClass.map({'180':1, '30':2, '45':2, '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n  \n  # LotAreaCut\n  df[\"LotAreaCut\"] = pd.qcut(df.LotArea,10)\n  df[\"LotAreaCut\"] = df.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n  \n\n \n  \n  # drop cfeatures  \n  # drop 'RoofMat1' as 98% of values are 'CompShq' and other values have too small sample size\n  # drop Exterior2nd as problematic in df and not necessarily as important\n  df = df.drop(['RoofMatl', 'Exterior2nd'], axis=1)\n  \n  \n  # assign labels for alley\n  df['Alley'] = df['Alley'].replace('Pave', 1)\n  df['Alley'] = df['Alley'].replace('Grvl', 0)\n  df['Alley'] = df['Alley'].replace(np.nan, 2)\n  df['Alley'] = df['Alley'].astype(int)\n  \n  # assign labels for MasVnrType\n  df['MasVnrType'] = df['MasVnrType'].replace('Stone', 4)\n  df['MasVnrType'] = df['MasVnrType'].replace('BrkFace', 3)\n  df['MasVnrType'] = df['MasVnrType'].replace('BrkCmn', 2)\n  df['MasVnrType'] = df['MasVnrType'].replace('CBlock', 1)\n  df['MasVnrType'] = df['MasVnrType'].replace('None', 0)\n  df['MasVnrType'] = df['MasVnrType'].replace(np.nan, 0)\n  df['MasVnrType'] = df['MasVnrType'].astype(int)\n  \n  # masonry veneer area\n  df['MasVnrArea'] = df['MasVnrArea'].replace(np.nan, 0)\n#   df['MasVnrArea'] = df['MasVnrArea'].astype(int)\n  \n  # assign value labels for basement features\n  df['BsmtQual'] = df.BsmtQual.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n  df['BsmtQual'] = df['BsmtQual'].astype(int)\n  \n  df['BsmtCond'] = df.BsmtCond.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n  df['BsmtCond'] = df['BsmtCond'].astype(int)\n  \n  df['BsmtExposure'] = df.BsmtExposure.map({'Gd':4, 'Av':3, 'Mn':2, 'No': 1, np.nan: 0})\n  df['BsmtExposure'] = df['BsmtExposure'].astype(int)\n  \n  df['BsmtFinType1'] = df.BsmtFinType1.map({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ': 2, 'Unf': 1, np.nan: 0})\n  df['BsmtFinType1'] = df['BsmtFinType1'].astype(int)\n  \n  df['BsmtFinType2'] = df.BsmtFinType2.map({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ': 2, 'Unf': 1, np.nan: 0})\n  df['BsmtFinType2'] = df['BsmtFinType2'].astype(int)  \n  \n  # electrical mapping; replace nulls with \"3\" for standard breaker (mode)\n  df['Electrical'] = df.Electrical.map({'SBrkr':3, 'FuseA':2, 'FuseF':1, 'FuseP':0, 'Mix': 1, np.nan: 3})\n  df['Electrical'] = df['Electrical'].astype(int)\n  \n  # fireplace mapping\n  df['FireplaceQu'] = df.FireplaceQu.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n  df['FireplaceQu'] = df['FireplaceQu'].astype(int)\n  \n  # garage features\n  df['GarageType'] = df.GarageType.map({'2Types':5, 'Attchd':4, 'Basment':3, 'BuiltIn':4, 'CarPort': 1, 'Detchd': 2,np.nan: 0})\n  df['GarageType'] = df['GarageType'].astype(int)\n  \n  df['GarageFinish'] = df.GarageFinish.map({'Fin':3, 'RFn':2, 'Unf':1, np.nan: 0})\n  df['GarageFinish'] = df['GarageFinish'].astype(int)\n  \n  df['GarageQual'] = df.GarageQual.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n  df['GarageQual'] = df['GarageQual'].astype(int)\n  \n  df['GarageCond'] = df.GarageCond.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n  df['GarageCond'] = df['GarageCond'].astype(int)\n  \n  # miscellenous feature mapping\n  df['PoolQC'] = df.PoolQC.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, np.nan: 0})\n  df['PoolQC'] = df['PoolQC'].astype(int)\n  \n  df['Fence'] = df.Fence.map({'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, np.nan: 0})\n  df['Fence'] = df['Fence'].astype(int)\n  \n  df['MiscFeature'] = df.MiscFeature.map({'Shed':1, 'Elev':0, 'Gar2':0, 'Othr':0, 'TenC':0, np.nan: 0})\n  df['MiscFeature'] = df['MiscFeature'].astype(int)\n  df['Shed'] = df['MiscFeature']\n  df = df.drop(['MiscFeature'], axis=1)\n  \n\n  \n  # fill in remaining nulls\n  df['LotFrontage'] = df['LotFrontage'].replace(np.nan, 0)\n  \n  # deal with categorial variables\n  df['MSZoning'] = df.MSZoning.map({'FV':4, 'RL':3, 'RM':2, 'RH':2, 'C (all)':1 , np.nan: 3})\n  df['MSZoning'] = df['MSZoning'].astype(int)\n  \n  df['Street'] = df.Street.map({'Pave':1, 'Grvl':0, np.nan: 1}) \n  df['Street'] = df['Street'].astype(int)\n  \n  # assign value of 0 to regular lots; 1 for all categories of irregular\n  df['LotShape'] = df.LotShape.map({'Reg':0, 'IR1':1, 'IR2':1, 'IR3':1, np.nan: 1}) \n  df['LotShape'] = df['LotShape'].astype(int)\n  \n  # assign value of '3' to hillside, '2' to level, low, and nulls, '1' to banked\n  df['LandContour'] = df.LandContour.map({'HLS':3, 'Bnk':1, 'Lvl':2, 'Low':2, np.nan: 2}) \n  df['LandContour'] = df['LandContour'].astype(int)\n  \n  # only 1 entry w\/ public utilities\n  df['Utilities'] = df.Utilities.map({'AllPub':1, 'NoSeWa':0, np.nan: 2}) \n  df['Utilities'] = df['Utilities'].astype(int)\n  \n  # mode = inside\n  df['LotConfig'] = df.LotConfig.map({'CulDSac':2, 'FR3':1, 'FR2':1, 'Corner':1, 'Inside':0, np.nan: 0}) \n  df['LotConfig'] = df['LotConfig'].astype(int)\n  \n  # land slope, mode = Gtl\n  df['LandSlope'] = df.LandSlope.map({'Sev':2, 'Mod':1, 'Gtl':0, np.nan: 0}) \n  df['LandSlope'] = df['LandSlope'].astype(int)\n  \n  # proxmmity to conditions\n  df['Condition1'] = df.Condition1.map({'PosA':5, 'PosN':4, 'RRNe':3, 'RRNn':3, \n                                        'Norm':2, 'Feedr':0, 'Artery':0, 'RRAn':1, 'RRAe':0, np.nan: 2}) \n  df['Condition1'] = df['Condition1'].astype(int)\n  \n  df['Condition2'] = df.Condition1.map({'PosA':5, 'PosN':4, 'RRNe':3, 'RRNn':3, \n                                        'Norm':2, 'Feedr':0, 'Artery':0, 'RRAn':1, 'RRAe':0, np.nan: 2}) \n  df['Condition2'] = df['Condition1'].astype(int)\n\n  # \n  df['BldgType'] = df.BldgType.map({'1Fam':4, 'TwnhsE':3, 'Twnhs':2, 'Duplex':1, '2fmCon':0, np.nan: 4}) \n  df['BldgType'] = df['BldgType'].astype(int)\n  \n  df['HouseStyle'] = df.HouseStyle.map({'2.5Fin':7, '2Story':6, '1Story':5, 'SLvl':4, \n                                        '2.5Unf':3, '1.5Fin':2, 'SFoyer':1, '1.5Unf':0, np.nan: 5}) \n  df['HouseStyle'] = df['HouseStyle'].astype(int)\n  \n  # gabel and hip most common roof styles by far; guess on value of others\n  df['RoofStyle'] = df.RoofStyle.map({'Hip':2, 'Shed':2, 'Gable':1, 'Mansard':1, 'Flat':1, 'Gambrel':0, np.nan: 1}) \n  df['RoofStyle'] = df['RoofStyle'].astype(int)\n  \n  df['Exterior1st'] = df.Exterior1st.map({'Stone': 8, 'CemntBd': 7, 'VinylSd': 6, 'BrkFace': 5, \n                                        'Plywood': 4, 'HdBoard': 3, 'Stucco': 2, 'ImStucc': 2, \n                                        'WdShing': 1, 'Wd Sdng': 1, 'MetalSd': 1, 'BrkComm': 0, \n                                        'CBlock': 0, 'AsphShn': 0, 'AsbShng': 0, np.nan: 3}) \n  df['Exterior1st'] = df['Exterior1st'].astype(int)\n  \n    \n#   df['Exterior2nd'] = df.Exterior2nd.map({'Stone': 8, 'CmntBd': 7, 'VinylSd': 6, 'BrkFace': 5, \n#                                         'Plywood': 4, 'HdBoard': 3, 'Stucco': 2, 'ImStucc': 2, \n#                                         'Wd Shng': 1, 'Wd Sdng': 1, 'MetalSd': 1, 'Brk Cmn': 0, \n#                                         'CBlock': 0, 'AsphShn': 0, 'AsbShng': 0, 'Other': 3, np.nan: 3}) \n#   df['Exterior2nd'] = df['Exterior2nd'].astype(int)\n  \n  df['ExterQual'] = df.ExterQual.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, np.nan: 1})\n  df['ExterQual'] = df['ExterQual'].astype(int)\n  \n  df['ExterCond'] = df.ExterCond.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 1})\n  df['ExterCond'] = df['ExterCond'].astype(int)\n  \n  df['Foundation'] = df.Foundation.map({'PConc':3, 'Wood':2, 'Stone':2, 'CBlock':2, 'BrkTil': 1, 'Slab': 0, np.nan: 2})\n  df['Foundation'] = df['Foundation'].astype(int)\n  \n  df['Heating'] = df.Heating.map({'GasA':2, 'GasW':1, 'OthW':0, 'Wall':0, 'Grav': 0, 'Floor': 0, np.nan: 2})\n  df['Heating'] = df['Heating'].astype(int)\n  \n  df['HeatingQC'] = df.HeatingQC.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 1})\n  df['HeatingQC'] = df['HeatingQC'].astype(int)\n  \n  df['CentralAir'] = df.CentralAir.map({'Y':1, 'N':0, np.nan: 1})\n  df['CentralAir'] = df['CentralAir'].astype(int)\n  \n  df['KitchenQual'] = df.KitchenQual.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 2})\n  df['KitchenQual'] = df['KitchenQual'].astype(int)\n  \n  df['Functional'] = df.Functional.map({'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, \n                                        'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0, np.nan: 7}) \n  df['Functional'] = df['Functional'].astype(int)\n  \n  df['PavedDrive'] = df['PavedDrive'].map({'Y':2, 'P':1, 'N':0, np.nan: 2})\n  df['PavedDrive'] = df['PavedDrive'].astype(int)\n  \n  df['SaleType'] = df.SaleType.map({'New': 2, 'WD': 1, 'CWD': 1, 'Con': 1, 'ConLI': 1, \n                                        'ConLD':1, 'ConLw':1, 'COD': 0, 'Oth': 0, np.nan: 1}) \n  df['SaleType'] = df['SaleType'].astype(int)\n  \n  df['SaleCondition'] = df.SaleCondition.map({'Partial': 5, 'Normal': 4, 'Alloca': 4, \n                                              'Family': 2, 'Abnorml': 1, 'AdjLand': 0, np.nan: 4})\n  df['SaleCondition'] = df['SaleCondition'].astype(int)\n  \n  df['SeasonSold'] = df.MoSold.map({1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3, 12:0})\n  df['SeasonSold'] = df['SeasonSold'].astype(int)\n  \n  \n#   # Rashid transformations\n  df['TotalHouseOverallQual'] = df['TotalSF'] * df['OverallQual']\n#   df['ZoningPrice'] = df['MSSubCl_MeanPPSF'] * df['TotalSF']\n  df['Functional_OverallQual'] = df['Functional'] * df['OverallQual']\n  df['TotalSF_LotArea'] = df['TotalSF'] * df['LotArea']\n  df['TotalSF_Condition'] = df['TotalSF'] * df['Condition1']\n\n\n\n   \n  return df","a5f07212":"full = df_transform(full)","d3dbfeb8":"n_train=train.shape[0]\ndf = full[:n_train]","ef2f06de":"explore1 = df[['SalePrice', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'Exterior1st', 'MasVnrType', 'MasVnrArea', 'ExterQual',\n       'ExterCond']]\n\nexplore2 = df[['SalePrice', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n       'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF',\n       'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n       'KitchenQual', 'TotRmsAbvGrd']]\n\nexplore3 = df[['SalePrice', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', \n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive',\n       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscVal', 'MoSold',\n       'YrSold', 'SaleType', 'SaleCondition', 'Shed']]\n\nexplore4 = df[['SalePrice', 'NhoodMedianPrice', 'PoolArea', 'PoolQC', 'Fence', 'MiscVal', 'MoSold',\n       'YrSold', 'SaleType', 'SaleCondition', 'Shed']]","a5287e04":"corr1 = explore1.corr()\nplt.figure(figsize = (16,12))\nsns.heatmap(corr1, vmin=-1, vmax=1, \n            xticklabels=corr1.columns.values,\n            yticklabels=corr1.columns.values, cmap='Spectral_r')","ede3d72a":"corr2 = explore2.corr()\nplt.figure(figsize = (16,12))\nsns.heatmap(corr2, vmin=-1, vmax=1, \n            xticklabels=corr2.columns.values,\n            yticklabels=corr2.columns.values, cmap='Spectral_r')","06e0d664":"corr3 = explore3.corr()\nplt.figure(figsize = (16,12))\nsns.heatmap(corr3, vmin=-1, vmax=1, \n            xticklabels=corr3.columns.values,\n            yticklabels=corr3.columns.values, cmap='Spectral_r')","4ac3d1f1":"corr4 = explore4.corr()\nplt.figure(figsize = (16,12))\nsns.heatmap(corr4, vmin=-1, vmax=1, \n            xticklabels=corr4.columns.values,\n            yticklabels=corr4.columns.values, cmap='Spectral_r')","b0dac672":"corr_list = sorted(df.corr().to_dict()['SalePrice'].items(), key=lambda x: x[1], reverse=True)\ncorr_list","eadff765":"from scipy.stats import skew\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in full.drop(['FxUp_SaleCond'], axis=1):\n    if full[i].dtype in numeric_dtypes: \n        numerics2.append(i)\n\nskew_features = full[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})\nskews = skews.drop(['SalePrice'], axis=0)","0e2dec5c":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skew_features[skew_features > 0.75]\nskew_index = high_skew.index\n   \nfor i in high_skew.index:\n    full[i]= boxcox1p(full[i], boxcox_normmax(full[i]+1))\n        \nskew_features2 = full[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews2 = pd.DataFrame({'skew':skew_features2})\nprint(skews2.to_string())","40309308":"# cut out a few features that seemed unuseful\nfull_condensed = full[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr',\n       'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n       'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2',\n       'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st',\n       'Fence', 'FireplaceQu', 'Fireplaces', 'Foundation', 'FullBath',\n       'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish',\n       'GarageQual', 'GarageType', 'GrLivArea', 'HalfBath',\n       'Heating', 'HeatingQC', 'HouseStyle', 'KitchenAbvGr',\n       'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig', 'LotFrontage',\n       'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning', 'MasVnrArea',\n       'MasVnrType', 'MiscVal', 'MoSold', 'OpenPorchSF',\n       'OverallCond', 'OverallQual', 'PavedDrive', 'PoolArea', 'PoolQC',\n       'RoofStyle', 'SaleCondition', 'SalePrice', 'SaleType', 'ScreenPorch',\n       'Street', 'TotRmsAbvGrd', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF',\n       'TotalLivingSF', 'TotalMainBath',\n       'TotalBath', 'AgeSold', 'TotalPorchSF', 'TotalSF', 'TotalArea',\n       'GarageAgeSold', 'LastRemodelYrs', 'NhoodMedianPrice', 'NhoodMeanPrice',\n       'NhoodMeanSF', 'NhoodMeanPricePerSF', 'ProxyPrice', \n       'FixerUpperScore', 'LotAreaCut', 'Shed', 'SeasonSold']]\n\nn_train=train.shape[0]\n\ntrain = full_condensed[:n_train]\ntest = full_condensed[n_train:]\n\nscaler = RobustScaler()\n\nX = train.drop(['SalePrice'], axis=1)\ntest_X = test.drop(['SalePrice'], axis=1)\n\ny = train['SalePrice']\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train['SalePrice'])\ntest_X_scaled = scaler.transform(test_X)\n\nX_scaled_df = pd.DataFrame(X_scaled)\ny_log_df = pd.DataFrame(y_log)\ntest_X_scaled_df = pd.DataFrame(test_X_scaled)","c2da6033":"lasso_fi=Lasso(alpha=0.001)\nlasso_fi.fit(X_scaled,y_log)\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso_fi.coef_}, index=X.columns)\nFI_sorted = FI_lasso.sort_values(\"Feature Importance\",ascending=False)\nprint(FI_sorted.to_string())\nFI_sorted.index","c46c2174":"# Root mean squared error (RMSE)\ndef rmse(y_pred, y_test):\n  return np.sqrt(mean_squared_error(y_test, y_pred))\n\n\nclass CvScore(object):\n  def __init__(self, list, name_list, X, y, folds=5, score='neg_mean_squared_error', seed=66, split=0.33):\n    self.X = X\n    self.y = y\n    self.folds = folds\n    self.score = score\n    self.seed = seed\n    self.split = split\n    self.model = list[0]\n    self.list = list\n    self.name = name_list[0]\n    self.name_list = name_list\n    \n  def cv(self):\n    cv_score = cross_val_score(self.model, self.X, self.y, cv=self.folds, scoring=self.score)\n    score_array = np.sqrt(-cv_score)\n    mean_rmse = np.mean(score_array)\n    print(\"Mean RMSE: \", mean_rmse)\n    \n  def cv_list(self):\n    for name, model in zip(self.name_list, self.list):\n      cv_score = cross_val_score(model, self.X, self.y, cv=self.folds, scoring=self.score)\n      score_array = np.sqrt(-cv_score)\n      mean_rmse = np.mean(score_array)\n      std_rmse = np.std(score_array)\n      print(\"{}: {:.5f}, {:.4f}\".format(name, mean_rmse, std_rmse))","9f0e4682":"lr = LinearRegression()\nridge = Ridge(alpha=40)\nlasso = Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=True, positive=False, precompute=False, random_state=66,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso_lars = LassoLarsIC()\n\n\nrfr = RandomForestRegressor()\netr = ExtraTreesRegressor(max_depth=40, max_features='auto', n_estimators=900)\ngbr = GradientBoostingRegressor()\nxgb = XGBRegressor(max_depth=2, n_estimators=400, colsample_bytree=0.5)\n\nsvr = SVR(C=1, gamma=0.00001, epsilon=0.0)\nlinear_svr = LinearSVR(C=10)\n\n# en = ElasticNet(alpha=0.004, l1_ratio=0.3, max_iter=10000)\nen = ElasticNet(alpha=0.005, l1_ratio=0.1, max_iter=1000)\n\nbr = BayesianRidge()\nkr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n\nada = AdaBoostRegressor(Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=True, positive=False, precompute=False, random_state=66,\n   selection='cyclic', tol=0.0001, warm_start=False))\n\nregression_list = [lr, ridge, lasso, lasso_lars, rfr, etr, gbr, xgb, svr, linear_svr, en, br, kr]\nname_list = [\"Linear\", \"Ridge\", \"Lasso\", \"Lasso Lars\", \"Random Forest\", \"Extra Trees\", \"Grad Boost\", \"XGBoost\", \"SVR\", \"LinSVR\", \n             \"ElasticNet\", \"Bayesian Ridge\", \"Kernel Ridge\"]","0718ca79":"v1 = CvScore(regression_list, name_list, X_scaled, y_log)\nv1.cv_list()","ee66741f":"full_v2 = full[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr',\n       'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n       'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2',\n       'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st',\n       'Fence', 'FireplaceQu', 'Fireplaces', 'Foundation', 'FullBath',\n       'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish',\n       'GarageQual', 'GarageType', 'GrLivArea', 'HalfBath',\n       'Heating', 'HeatingQC', 'HouseStyle', 'KitchenAbvGr',\n       'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig', 'LotFrontage',\n       'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning', 'MasVnrArea',\n       'MasVnrType', 'MiscVal', 'MoSold', 'OpenPorchSF',\n       'OverallCond', 'OverallQual', 'PavedDrive', 'PoolArea', 'PoolQC',\n       'RoofStyle', 'SaleCondition', 'SalePrice', 'SaleType', 'ScreenPorch',\n       'Street', 'TotRmsAbvGrd', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF',\n       'TotalLivingSF', 'TotalMainBath',\n       'TotalBath', 'AgeSold', 'TotalPorchSF', 'TotalSF', 'TotalArea',\n       'GarageAgeSold', 'LastRemodelYrs', 'NhoodMedianPrice', 'NhoodMeanPrice',\n       'NhoodMeanSF', 'NhoodMeanPricePerSF',  \n       'FixerUpperScore', 'LotAreaCut', 'Shed', 'SeasonSold']]\n\nn_train=train.shape[0]\n\ntrain_v2 = full_v2[:n_train]\ntest_v2 = full_v2[n_train:]\n\nscaler = RobustScaler()\n\nX_v2 = train_v2.drop(['SalePrice'], axis=1)\ntest_X_v2 = test_v2.drop(['SalePrice'], axis=1)\ny_v2 = train_v2['SalePrice']\n\nX_scaled_v2 = scaler.fit(X_v2).transform(X_v2)\ny_log_v2 = np.log(train_v2['SalePrice'])\ntest_X_scaled_v2 = scaler.transform(test_X_v2)","fed0ba88":"v2 = CvScore(regression_list, name_list, X_scaled_v2, y_log_v2)\nv2.cv_list()","e8eb4192":"full_v3 = full[['OverallQual', 'ProxyPrice', 'GrLivArea', '2ndFlrSF', '1stFlrSF',\n       'TotalBsmtSF', 'OverallCond', 'NhoodMeanPricePerSF', 'MSZoning',\n       'Functional', 'GarageCars', 'LotArea', 'KitchenQual', 'SaleType',\n       'Fireplaces', 'Condition1', 'SaleCondition', 'HeatingQC', 'GarageArea',\n       'BldgType', 'BsmtExposure', 'Exterior1st', 'TotalBath', 'WoodDeckSF',\n       'ExterQual', 'Foundation', 'FixerUpperScore', 'TotRmsAbvGrd',\n       'GarageFinish', 'LotConfig', 'RoofStyle', 'FireplaceQu', 'TotalPorchSF', 'SalePrice']]\n\nn_train=train.shape[0]\n\ntrain_v3 = full_v3[:n_train]\ntest_v3 = full_v3[n_train:]\n\nscaler = RobustScaler()\n\nX_v3 = train_v3.drop(['SalePrice'], axis=1)\ntest_X_v3 = test_v3.drop(['SalePrice'], axis=1)\ny_v3 = train_v3['SalePrice']\n\nX_scaled_v3 = scaler.fit(X_v3).transform(X_v3)\ny_log_v3 = np.log(train_v3['SalePrice'])\ntest_X_scaled_v3 = scaler.transform(test_X_v3)","e1037cf5":"v3 = CvScore(regression_list, name_list, X_scaled_v3, y_log_v3)\nv3.cv_list()","6801ce0f":"full_v4 = full[['OverallQual', 'ProxyPrice', 'GrLivArea', '2ndFlrSF', '1stFlrSF',\n       'TotalBsmtSF', 'OverallCond', 'NhoodMeanPricePerSF', 'MSZoning',\n       'Functional', 'GarageCars', 'LotArea', 'KitchenQual', 'SaleType',\n       'Fireplaces', 'Condition1', 'SaleCondition', 'HeatingQC', 'GarageArea',\n       'BldgType', 'BsmtExposure', 'Exterior1st', 'TotalBath', 'WoodDeckSF',\n       'ExterQual', 'Foundation', 'FixerUpperScore', 'TotRmsAbvGrd',\n       'GarageFinish', 'FireplaceQu', 'TotalPorchSF', 'SalePrice']]\n\nn_train=train.shape[0]\n\ntrain_v4 = full_v4[:n_train]\ntest_v4 = full_v4[n_train:]\n\nscaler = RobustScaler()\n\nX_v4 = train_v4.drop(['SalePrice'], axis=1)\ntest_X_v4 = test_v4.drop(['SalePrice'], axis=1)\ny_v4 = train_v4['SalePrice']\n\nX_scaled_v4 = scaler.fit(X_v4).transform(X_v4)\ny_log_v4 = np.log(train_v4['SalePrice'])\ntest_X_scaled_v4 = scaler.transform(test_X_v4)","fd916c2e":"v4 = CvScore(regression_list, name_list, X_scaled_v4, y_log_v4)\nv4.cv_list()","9ec27824":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(\"Best parameters found: \", grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","cb09a975":"# param_grid={'C':[0.01, 0.1, 1, 10, 100], 'gamma':[0.00001, 0.0001, .001, .01, .1], 'epsilon': np.linspace(0,1,10)}\n# grid(SVR()).grid_get(X_scaled_v3,y_log_v3,param_grid)","7d49ff49":"# param_grid={'C':[0.01, 0.1, 0.25, 0.5, 0.75, 1, 5, 10, 100]}\n# grid(LinearSVR()).grid_get(X_scaled_v3,y_log_v3,param_grid)","8992c15a":"# param_grid={'alpha':[0.00001, 0.0001, 0.01, 0.1, 0.25, 0.5, 0.75, 1]}\n# grid(Lasso()).grid_get(X_scaled_v3,y_log_v3,param_grid)","d9e35aef":"# param_grid={'alpha':np.linspace(0.2, 1.0, 9), 'kernel':[\"polynomial\"], 'degree':[1,2,3],'coef0': np.linspace(2,10,9)}\n# grid(KernelRidge()).grid_get(X_scaled_v3,y_log_v3,param_grid)","2d68685a":"# # Extreme Gradient Boost\n# xgbm_param_grid = {\n#      'colsample_bytree': np.linspace(0.5, 1.0, 5),\n#      'n_estimators':[200, 400, 600, 800, 1000],\n#      'max_depth': [1, 2, 5, 10],\n# }\n\n# grid(XGBRegressor()).grid_get(X_scaled_v3,y_log_v3,xgbm_param_grid)","1801e8b3":"# # Random Forest Regression\n# rfr_param_grid = {\n#      'n_estimators':[100, 200, 400, 600, 800],\n#      'max_depth': [80, 120, 160, 200]\n# }\n\n# grid(RandomForestRegressor()).grid_get(X_scaled_v3,y_log_v3,rfr_param_grid)","16a976fb":"# # Extra Trees Regression\n# etr_param_grid = {\n#      'n_estimators':[400, 600, 800, 900, 1000],\n#      'max_depth': [5, 10, 20, 40, 50, 60]\n# }\n\n# grid(ExtraTreesRegressor()).grid_get(X_scaled_v3,y_log_v3,etr_param_grid)","ee8d4b05":"lr = LinearRegression()\nridge = Ridge(alpha=40)\n\n# Best parameters found:  {'alpha': 0.0001} 0.11187841593433792\nlasso = Lasso(alpha=0.0001)\n\nlasso_lars = LassoLarsIC()\n\n# Best parameters found:  {'max_depth': 120, 'n_estimators': 800} 0.13093022119435033\nrfr = RandomForestRegressor(max_depth=120, n_estimators=800)\n\n# Best parameters found:  {'max_depth': 60, 'n_estimators': 600} 0.12379414612361438\netr = ExtraTreesRegressor(max_depth=60, max_features='auto', n_estimators=600)\n\ngbr = GradientBoostingRegressor()\n\n# Best parameters found:  {'colsample_bytree': 0.5, 'max_depth': 2, 'n_estimators': 600}\nxgb = XGBRegressor(max_depth=2, n_estimators=600, colsample_bytree=0.5)\n\n# Best parameters found:  {'C': 10, 'epsilon': 0.0, 'gamma': 0.001} 0.10962793797503995\nsvr = SVR(C=10, gamma=0.001, epsilon=0.0)\n\n# Best parameters found:  {'C': 0.75} 0.11327745125722458\nlinear_svr = LinearSVR(C=0.75)\n\n# en = ElasticNet(alpha=0.004, l1_ratio=0.3, max_iter=10000)\nen = ElasticNet(alpha=0.005, l1_ratio=0.1, max_iter=1000)\n\nbr = BayesianRidge()\n\n# Best parameters found:  {'alpha': 1.0, 'coef0': 6.0, 'degree': 2, 'kernel': 'polynomial'} 0.11116062462422147\nkr = KernelRidge(alpha=1.0, kernel='polynomial', degree=2, coef0=6.0)\n\n\nregression_list = [lr, ridge, lasso, lasso_lars, rfr, etr, gbr, xgb, svr, linear_svr, en, br, kr]\nname_list = [\"Linear\", \"Ridge\", \"Lasso\", \"Lasso Lars\", \"Random Forest\", \"Extra Trees\", \"Grad Boost\", \"XGBoost\", \"SVR\", \"LinSVR\", \n             \"ElasticNet\", \"Bayesian Ridge\", \"Kernel Ridge\"]","42eab937":"v3 = CvScore(regression_list, name_list, X_scaled_v3, y_log_v3)\nv3.cv_list()","d39b12a4":"# define cross validation strategy\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse\n\n\nclass AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","b6dad8ea":"# even weighted model\nvar = 6\n\nw1 = 1\/var\nw2 = 1\/var\nw3 = 1\/var\nw4 = 1\/var\nw5 = 1\/var\nw6 = 1\/var\n\nweight_avg = AverageWeight(mod = [en, lasso, ridge, xgb, svr, kr],weight=[w1,w2,w3,w4,w5,w6])\n\nscore = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())","4a08abbe":"# even weighted model\nvar = 5\n\nw1 = 1\/var\nw2 = 1\/var\nw3 = 1\/var\nw4 = 1\/var\nw5 = 1\/var\n\nweight_avg = AverageWeight(mod = [en, lasso, ridge, svr, kr],weight=[w1,w2,w3,w4,w5])\n\nscore = rmse_cv(weight_avg,X_scaled_v3,y_log_v3)\nprint(score.mean())","5d3e54ef":"# weighted towards higher scoring models\n\nw1 = .1\nw2 = .25\nw3 = .05\nw4 = .1\nw5 = .25\nw6 = .25\n\nweight_avg = AverageWeight(mod = [en, lasso, ridge, xgb, svr, kr],weight=[w1,w2,w3,w4,w5,w6])\n\nscore = rmse_cv(weight_avg,X_scaled_v3,y_log_v3)\nprint(score.mean())","ca068b01":"class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","2c3cc954":"a = Imputer().fit_transform(X_scaled)\nb = Imputer().fit_transform(y_log.values.reshape(-1,1)).ravel()\n\nstack_model = stacking(mod=[lasso, ridge, xgb, en, kr],meta_model=svr)\n\nscore = rmse_cv(stack_model,a,b)\nprint(score.mean())","62d4fe2b":"# This is the final model I use; the 6-model even-weighted ensemble seemed to work best\n\n# even weighted model\nvar = 6\n\nw1 = 1\/var\nw2 = 1\/var\nw3 = 1\/var\nw4 = 1\/var\nw5 = 1\/var\nw6 = 1\/var\n\nfinal_model = AverageWeight(mod = [en, lasso, ridge, xgb, svr, kr],weight=[w1,w2,w3,w4,w5,w6])\nfinal_model = final_model.fit(a,b)","29e561ac":"pred = np.exp(final_model.predict(test_X_scaled))\nprint(pred)\n\nid = test_orig['Id']\nresult=pd.DataFrame({'Id': id, 'SalePrice':pred})\nresult.to_csv(\"submission.csv\",index=False)","af921242":"full_v6 = full[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr',\n       'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n       'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2',\n       'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st',\n       'Fence', 'FireplaceQu', 'Fireplaces', 'Foundation', 'FullBath',\n       'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish',\n       'GarageQual', 'GarageType', 'GarageYrBlt', 'GrLivArea', 'HalfBath',\n       'Heating', 'HeatingQC', 'HouseStyle', 'KitchenAbvGr',\n       'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig',\n       'LotFrontage', 'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning', 'MasVnrArea',\n       'MasVnrType', 'MiscVal', 'MoSold', 'Neighborhood', 'OpenPorchSF',\n       'OverallCond', 'OverallQual', 'PavedDrive', 'PoolArea', 'PoolQC',\n       'RoofStyle', 'SaleCondition', 'SalePrice', 'SaleType', 'ScreenPorch',\n       'Street', 'TotRmsAbvGrd', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF',\n       'TotalLivingSF', 'TotalMainBath',\n       'TotalBath', 'AgeSold', 'TotalPorchSF', 'TotalSF', 'TotalArea',\n       'GarageAgeSold', 'LastRemodelYrs', 'NhoodMedianPrice', 'NhoodMeanPrice',\n       'NhoodMeanSF', 'NhoodMeanPricePerSF', 'ProxyPrice',  \n       'FixerUpperScore', 'LotAreaCut', 'Shed', 'SeasonSold',\n       'TotalHouseOverallQual', 'Functional_OverallQual', 'TotalSF_LotArea',\n       'TotalSF_Condition']]\n\ntrain = full_v6[:n_train]\ntest = full_v6[n_train:]\n\nX_v6 = train.drop(['SalePrice'], axis=1)\ntest_X_v6 = test.drop(['SalePrice'], axis=1)\ny_v6 = train['SalePrice']\n\nX_scaled_v6 = scaler.fit(X_v6).transform(X_v6)\ny_log_v6 = np.log(train['SalePrice'])\ntest_X_scaled_v6 = scaler.transform(test_X_v6)\n","02dbb517":"lr = LinearRegression()\nridge = Ridge(alpha=40)\n\n# Best parameters found:  {'alpha': 0.0001} 0.11187841593433792\nlasso = Lasso(alpha=0.0001)\n\nlasso_lars = LassoLarsIC()\n\n# Best parameters found:  {'max_depth': 120, 'n_estimators': 800} 0.13093022119435033\nrfr = RandomForestRegressor(max_depth=120, n_estimators=800)\n\n# Best parameters found:  {'max_depth': 60, 'n_estimators': 600} 0.12379414612361438\netr = ExtraTreesRegressor(max_depth=60, max_features='auto', n_estimators=600)\n\ngbr = GradientBoostingRegressor()\n\n# Best parameters found:  {'colsample_bytree': 0.5, 'max_depth': 2, 'n_estimators': 600}\nxgb = XGBRegressor(max_depth=2, n_estimators=600, colsample_bytree=0.5)\n\n# Best parameters found:  {'C': 100, 'epsilon': 0.0, 'gamma': 0.001} \nsvr = SVR(C=100, gamma=0.0001, epsilon=0.0)\n\n# Best parameters found:  {'C': 0.5} \nlinear_svr = LinearSVR(C=0.5)\n\n# en = ElasticNet(alpha=0.004, l1_ratio=0.3, max_iter=10000)\nen = ElasticNet(alpha=0.005, l1_ratio=0.1, max_iter=1000)\n\nbr = BayesianRidge()\n\n# Best parameters found:  {'alpha': 1.0, 'coef0': 6.0, 'degree': 2, 'kernel': 'polynomial'} 0.11116062462422147\nkr = KernelRidge(alpha=1.0, kernel='polynomial', degree=2, coef0=6.0)\n\n\nregression_list = [lr, ridge, lasso, lasso_lars, rfr, etr, gbr, xgb, svr, linear_svr, en, br, kr]\nname_list = [\"Linear\", \"Ridge\", \"Lasso\", \"Lasso Lars\", \"Random Forest\", \"Extra Trees\", \"Grad Boost\", \"XGBoost\", \"SVR\", \"LinSVR\", \n             \"ElasticNet\", \"Bayesian Ridge\", \"Kernel Ridge\"]","8e5add73":"v6 = CvScore(regression_list, name_list, X_scaled_v6, y_log_v6)\nv6.cv_list()","987defe1":"# even weighted model\nvar = 6\n\nw1 = 1\/var\nw2 = 1\/var\nw3 = 1\/var\nw4 = 1\/var\nw5 = 1\/var\nw6 = 1\/var\n\nweight_avg_v6 = AverageWeight(mod = [en, lasso, ridge, xgb, svr, kr],weight=[w1,w2,w3,w4,w5,w6])\n\nscore = rmse_cv(weight_avg_v6,X_scaled_v6,y_log_v6)\nprint(score.mean())","e5cf7ea8":"a = Imputer().fit_transform(X_scaled_v6)\nb = Imputer().fit_transform(y_log_v6.values.reshape(-1,1)).ravel()\n\nfinal_model_v2 = AverageWeight(mod = [en, lasso, ridge, xgb, svr, kr],weight=[w1,w2,w3,w4,w5,w6])\nfinal_model_v2 = final_model_v2.fit(a,b)","ace47c49":"pred_v2 = np.exp(final_model_v2.predict(test_X_scaled_v6))\nprint(pred_v2)\n\nid = test_orig['Id']\nsubmission_v2 =pd.DataFrame({'Id': id, 'SalePrice':pred_v2})\nsubmission_v2.to_csv(\"submission_v2.csv\",index=False)","afdc48b5":"# Feature Importance\n\nFeature importance for the lasso model","3000c44f":"# Next Steps\n\nI'm still debating whether to try to improve my score more. Since this is a long-running dataset with numerous kernels, it can be quite difficult to get a top 5% score with original work at this point, since many entries are simply copy of high-scoring entries. However, there are a few ways I could explore to improve my score further:\n\n**Experiment more with features** I'm particularly of the view that the garage features may not be adequately capturing some elements and some feature engineering might be in order there. \n\n**Work more on stacking.** I ended up using a model that evenly weighted predictions from 6 different models. Others had more success with more complex stacking techniques. I could work on trying to improve the score that way. \n\nBut I might instead work on some other datasets to expand my experience. ","54a33fd1":"Let's drop the extreme outliers","2da9f8d7":"We once again see minor improvements from narrowing our feature set. In particular, we see significant improvement with the Kernel Ridge model, which ended up being part of my final ensemble. Also, our Linear Model is back to normal. One final attempt at narrowing features even further. ","dc38d0f8":"# Correlation","59dad7fe":"# Skewed Features\n\nCredit here goes to [Laurenstc](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe), whose work I borrowed from. ","e9f410fb":"The correlation heatmaps are interesting, but ultimately not that insightful given our large number of features. I wanted a better way to visualize correlations. I came up with this, admittedly, complex piece of code that took the correlations, put them into a dictionary, and turned them into a sorted list of sets of the feature and correlation with 'SalePrice'","fe0248b0":"# First Model","33b74da6":"**Feature Transformations**\n* First we'll impute 0 values for variables where nulls likely mean absence of the attribute (garage \/ basement)\n* Next,  we'll calculate several aggregate variables on square footage and baths\n* We'll change 'YearBuilt' to an 'AgeSold' variable which is slightly more accurate\n* We'll add a 'TotalArea' feature that includes the garage\n* Finally, had to change s few values for the year of remodeling as they were listed as remodeled after house sold","876543ee":"**Mean and Median Price Functions**\n\nI created some functions to create mean and median price variables, as well as mean square footage, to calculate mean price per square foot","f2a4af9c":"# Hyperparameter Tuning\n\nWe'll try a little bit of hyperparameter tuning. Note that I already did a bit in a previous notework (not shown here), which is where some of the values come from, but I'm going to do another round. The SVR model, in particular, is very sensitive to hyperparameters. Others, such as the Lasso, don't seem nearly that sensitive and I only found that my scores would deviate by small fractions of a percent. \n\nSince tuning hyperparameters tends to have long run times for everytime I commit, I've commented these entries out, but I leave the code so you can see the work. ","3081b93d":"# Final Ensemble","8d31f8a3":"# Scaling and Feature Selection","8071b20e":"My linear model went bonkers in this one. I had occasional issues with certain models behaving oddly, but since I ended up not relying on Linear() for my ensemble, I'm just going to ignore it. Otherwise, there was a slight improvement in results. \n\nWe'll take another stab at narrowing down features","fc0c1217":"# One Last Run","b1722f80":"This time, our scores move up slightly, so I'm going to go with my third set of features which seemed to score the best. ","f15ce034":"# Feature Engineering with Ensemble to Predict Housing Prices\n\nThis is my first major Kaggle dataset. As a former investment manager who invested in Real Estate Investment Trusts (\"REITs\"), housing price prediction was a natural place to start my Kaggle journey.  \n\nMy strategy for this dataset was to do heavy feature engineering and test out a wide variety of regression models. After finding multiple successful models, my plan was to experiment with ensembling them to boost my score further. This notebook showcases some of my efforts.\n\nWhen tackling a dataset, I start with my own original work. Then, I like to read other kernels to get ideas on how to improve my model. While getting a high score is important, ultimately, finding reproducible techiques for future datasets is what's most vital to me. \n\nFirst, I'll give credit where it's due. There were several other kernels that helped me along the way. \n\n## Kernels That Influenced My Work\n\n1. ***Seringe:*** [Stacked Regressions](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard). Excellent kernel on stacking and a fantastic read overall. \n\n2. ***Harun-Ur-Rashid:*** [House Price Prediction from Bangladesh](https:\/\/www.kaggle.com\/harunshimanto\/house-price-prediction-from-bangladesh). While Rashid's is one of the lesser known kernels for this dataset, it influenced my thinking more than any other kernel. In particular, I adopted his approach to ensembling and stacking to create my best models. I also borrowed some elements of his feature engineering, albeit to mixed results. Also, I find some people's coding styles mesh better with my own, and Rashid's way of thinking about coding appealed to me for this reason. \n\n3. ***Laurenstc:*** [Advanced FE](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe). I borrowed Laurenstc's approach to dealing with skewed features. \n\n4. ***juliencs.*** [A Study on Regression Applied to Ames Dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset). I used juliencs's technique for mapping out values. \n\n## Updates\n*11 Sep 2018.* Ran a broader model (submission_v2) for the test data. Found that while narrowing down my features helped in validation scoring, it hurt on the test data, suggesting the narrower feature sets were more likely to result in overfitting. \n\n## Table of Contents\n\n1. Imports\n2. Data Exploration\n3. Build DataFrame \/ Feature Engineering\n4. Correlation Analysis\n5. Dealing with Skewed Features\n6. Scaling and Feature Exploration\n7. First Model\n8. Feature Experimentation\n9. Final Ensemble\n10. Applying to Test Data\n11. Submission #2: A Broader Model \/ Dealing with Overfitting Issues","4d4b1302":"First we'll create a train and test dataframe, distinct from my \"orig\" dataframes","6bfbcbe4":"This submission ended up doing better than the previous one, so the broader dataset seems to be the way to go. ","c87f9cec":"This will make it easier to get a quick sense of what the 'mode' is for most of these variables, as we'll use the mode to impute values for the nulls in many cases.  The next thing I want to do is get a sense of how to value these categorical features. There's no perfect way, but one thing we can do is look at the mean prices associated with each value. \n\nThe function below looks at the mean values, and also shows the value counts. The value counts will help us determine if the data is meaningful or not. For variables with a small sample size (e.g. only 2 entries have it), I relied more on some quick Internet research to try to get a sense of value.","db9d594c":"# Test Data","b24ff9ba":"# Narrowing Down Features\n\nHere, I test to see whether narrowing down features improves my score. I'm starting with the features that seemed least utilized in the Lasso model.","2a5ab92a":"## Transformations\n\nThis next chunk of code requires heavy explanation. I favor making all of my changes inside one or a small number of functions for various reasons. There are, however, a wide multitude of changes here, so of which may be difficult to comprehend. \n\n1. **Neighborhood Mean Features**\n\nThe first thing I did here was create a variety of neighborhood mean related measures, such as neighborhood mean price and mean square footage. I used these two features to also calculate neighborhood mean price per square foot. Finally, I created a 'ProxyPrice' feature which takes the neighborhood mean price per square foot and multiplies it by the livable square footage of the house. \n\n2. **Fixer-Upper Metrics**\n\nThis is probably the most confusing thing I did and can be difficult to explain with the limitations of the medium. These metrics were added after I finished my first set of regression models. I went back and examined the characteristics of price predictions that were most inaccurate. I found that one of the common themes was that many of my \"poorer predictions\" on the high-side (i.e. my model's predictions were much higher than the real selling price) had a lot of attributes associated with fixer-upper homes. I found several elements associated with fixer-upper homes and assigned values to them. The final fixer upper score simply takes the maximum number of 'fixer-upper' points and subtracts the totals, so that a score of 0 is high in 'fixer-upper' status, while a score of 45 indicates a home with few known issues. \n\n3. **MSSubClass and LotAreaCut**\n\nThis was something I borrowed from Rashid's kernel, but it did not appear to have a significant impact, so it could probably be deleted as well. \n\n4. **Assigning Integer Values for Featuers**\n\nFor a wide variety of features, I assigned integer values based on mean prices and personal research. 0 is always the lowest value and the high values can range anywhere from \"1\" to \"8\" depending on the details of the feature. \n\n5. **Added Features from Rashid Kernel**\n\nI added a few engineered features from the Rahman kernel.","464f9ee4":"# Data Exploration\n\n* Create dataframe for data exploration\n* View head of the data\n* Examine nulls\n* Look at how categorical variables compare to our dependent variable 'SalePrice'","c1fe70ec":"# Build DataFrame\n\nNow that I've explored the data, I'm going to create my dataframe through a series of functions. I'll do the following. \n\n* Fill in null values\n* Feature engineer new variables","e5d93ca4":"We can see that some values have a lot of nulls, such as PoolQC, MiscFeature, Alley, and Fence. Then, we have several values (particularly related to the garage and basement) where anywhere from 2% - 6% of the values are nulls. \n\nLet's create a function to get a better sense of values in the categorical features with nulls. Almost all of our nulls are in categorical variables, which are currently classified as 'objects' in Pandas.","09243bf0":"# More Testing \/ Dealing with Overfitting\n\nIt's become apparent that some of my models have issues with overfitting, so I wanted to explore a bit more. In particular, while narrowing features down helped in predicting my validation dataset, it hurt in my test submissions. So here's an attempt to reverse course and go back to the broader feature set. "}}