{"cell_type":{"03ee67ee":"code","682ad7e6":"code","a9a61e7d":"code","112c6d9a":"code","e3941d9e":"code","db2b5943":"code","8d45f5cc":"code","ece07374":"code","81d38697":"code","f46a030b":"code","bb212119":"code","70e7c9e0":"code","a48f032c":"code","92bdff64":"code","cf03b4bf":"code","70d1a988":"code","39a74f16":"code","ce3ab712":"code","fd1f9199":"code","33cf4a93":"code","a9c77ea8":"code","404adb18":"code","3baa99e3":"code","00b4de26":"code","65aa1015":"code","f65619fe":"code","5d291923":"code","93493ee3":"code","8ca5d763":"code","65f02324":"code","2b46b7b8":"code","2bbee6e8":"code","ff5bf45a":"code","df3a78cc":"code","808de8a0":"code","b1030c3d":"code","ca2c23c6":"code","5bee7007":"code","eda27056":"code","d65a3c5e":"code","b63d5f5f":"code","bf6f2eed":"code","1e840f36":"code","853a007c":"code","02bb647e":"code","ad041113":"markdown","0c8f2227":"markdown","790d3212":"markdown","4b972cd5":"markdown","baa620db":"markdown","ca31f83e":"markdown","de1879d4":"markdown","4c74d3bc":"markdown","e27408be":"markdown","cb731fec":"markdown","6b328a38":"markdown","5203bb1c":"markdown","7ac83a9e":"markdown","62adf31d":"markdown","9eb8aa6a":"markdown","5f3812bf":"markdown"},"source":{"03ee67ee":"import re\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom pandas import read_csv\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n\nfiledir = \"\"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        filedir = os.path.dirname(os.path.join(dirname, filename))","682ad7e6":"train_data = pd.read_csv(\"{}\/train.csv\".format(dirname),index_col=0)\ntest_data = pd.read_csv(\"{}\/test.csv\".format(dirname),index_col=0)\ngender_data = pd.read_csv(\"{}\/gender_submission.csv\".format(dirname),index_col=0)\ntest_data[\"Survived\"] = gender_data[\"Survived\"]","a9a61e7d":"train_data.info()","112c6d9a":"test_data.info()","e3941d9e":"# may be name is not relation.\n# may be name and ticket is not relation.\ndrop_col = [\"Name\",\"Ticket\"]\ntrain_data_p= train_data.drop(drop_col,axis=1)\ntest_data_p = test_data.drop(drop_col,axis =1 )","db2b5943":"train_data_p.head()","8d45f5cc":"all_passenger = len(train_data_p)\ndata = (train_data_p[\"Survived\"]==1).sum()\nprint(\"The ratio of survivors to the whole is {0:3.1f}%.\".format(data*100\/all_passenger))\n","ece07374":"plt.tight_layout() \nall_passenger = len(train_data_p)\nall_suvr = (train_data_p[\"Survived\"]==1).sum()\nprint(\"Survival rate (no bias by P class):{0:3.1f}%\".format(all_suvr*100\/all_passenger))\nfor pcls in [1,2,3]:\n    data = (train_data_p[\"Pclass\"]==pcls).sum()\n    surv = ((train_data_p[\"Pclass\"]==pcls)&(train_data_p[\"Survived\"]==1) ).sum()\n    print(\"\"\"Pclass:{0} :\n        Percentage of Pclass{0} among all passengers: \\t{3:3.1f}%\n        Survival Rate of Pclass{0}: \\t\\t\\t{1:3.1f}%\n        Survival Rate(among all passengers): \\t\\t{2:3.1f}% \n        \"\"\".format(pcls,surv\/data*100,surv*100\/all_suvr,data*100\/all_passenger))\n\n_ = plt.hist(\n    [train_data_p[train_data_p[\"Survived\"]==1][\"Pclass\"], \n     train_data_p[train_data_p[\"Survived\"]!=1][\"Pclass\"]],\n     label=[\"Survied\",\"Dead\"], color= [\"red\",\"blue\"],\n     stacked=True,\n)\nplt.legend()\nplt.xlabel(\"Pclass\")\nplt.ylabel(\"Count\")\nplt.xticks([1,2,3],[\"Pclass{}\".format(i) for i in range(1,4) ])\nplt.show()    \n\n_ = plt.hist(\n    [train_data_p[train_data_p[\"Pclass\"]==1][\"Survived\"], \n     train_data_p[train_data_p[\"Pclass\"]==2][\"Survived\"],\n     train_data_p[train_data_p[\"Pclass\"]==3][\"Survived\"]\n    ],\n     label=[\"Pclass1\",\"Pclass2\",\"Pclass3\"], color= [\"orange\",\"blue\",\"green\"],\n     stacked=True,\n)\nplt.legend()\nplt.xlabel(\"Survived\")\nplt.xticks([0,1],[\"Dead\",\"Survied\"])\nplt.ylabel(\"Count\")\n","81d38697":"female_all = (train_data_p[\"Sex\"] == \"female\").sum()\nmale_all =(train_data_p[\"Sex\"] == \"male\").sum()\nprint(\"male,female {} :{}\".format(male_all,female_all))\nall_surv = (train_data_p[\"Survived\"] == 1).sum()\nmale_surv = ((train_data_p[\"Survived\"] == 1)&(train_data_p[\"Sex\"] == \"male\")).sum()\nprint(\"Female`s Survival: {0:3.1f} %\".format( 100*(all_suvr-male_surv)\/all_surv))\nfor sex in [\"male\",\"female\"]:\n    data = (train_data_p[\"Sex\"]==sex).sum()\n    surv = ((train_data_p[\"Sex\"]==sex )& (train_data_p[\"Survived\"]==1 )).sum()\n    print(\"Sex:{} \\t Survived : {:3.1f} %\".format(sex,(surv\/data)*100))\n\n_ = plt.hist(\n    [\n        train_data_p[train_data_p[\"Survived\"]==1][\"Sex\"] ,  \n        train_data_p[train_data_p[\"Survived\"]!=1][\"Sex\"] ,  \n    ],\n    label=[\"Survived\",\"Dead\"], color= [\"orange\",\"blue\"],\n    stacked=True,\n)\n\nplt.legend()\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Count\")\nplt.title(\"\")\nplt.show()","f46a030b":"_hist = plt.hist(    \n    [\n        train_data_p[train_data_p[\"Survived\"]==1][\"Age\"] ,  \n        train_data_p[train_data_p[\"Survived\"]!=1][\"Age\"] ,  \n    ],\n     range=(0, 100), bins=20,\n    label=[\"Survived\",\"Dead\"], color= [\"orange\",\"blue\"],\n    stacked=True,\n)\nplt.legend()\nplt.xlabel(\"Age\")\n_ = plt.ylabel(\"Count\")","bb212119":"rate =(_hist[0][0] \/(_hist[0][1]))*100\nfor i in range(len(rate)):\n    print(\"Age : {:3} ~ {:3} {:3.1f} %\".format(i*5,(i+1)*5,rate[i]))\n","70e7c9e0":"tmp = train_data_p.copy()\ntmp[\"Cabin\"] = train_data_p[\"Cabin\"].str.extract('(?P<cabin>^.)',expand=False).fillna(\"Nan\")\n\n_ = plt.hist(\n    [\n        tmp[(tmp[\"Survived\"]==1) & (tmp[\"Cabin\"]!=\"Nan\")][\"Cabin\"] ,  \n        tmp[(tmp[\"Survived\"]!=1) & (tmp[\"Cabin\"]!=\"Nan\")][\"Cabin\"] ,  \n    ],\n    label=[\"Survived\",\"Dead\"], color= [\"orange\",\"blue\"],\n    stacked=True,rwidth=0.8\n)\n_= plt.legend()\n_= plt.xlabel(\"Cabin\")\n_= plt.ylabel(\"Count\")\n","a48f032c":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup","92bdff64":"intLookUp = IntegerLookup()","cf03b4bf":"total_data=train_data_p.append(test_data_p)\n# fix Age \ntotal_data.Age = np.where(total_data.Age.isnull() == True, -1 ,total_data.Age\/\/10*10)\n# fix Fare\nfare_max = total_data.Fare.max()\ntotal_data.Fare = np.where(total_data.Fare.isnull() == 1 ,-1,total_data.Fare\/fare_max)\n# Fix Cabin\ntotal_data[\"Cabin\"] = total_data[\"Cabin\"].str.extract('(?P<cabin>^.)',expand=False).fillna(\"Nan\")\n#fix Embarked \ntotal_data.Embarked = total_data.Embarked.fillna(\"Nan\")\n\n# \u3068\u308a\u3042\u3048\u305avalue\u3068\u3057\u3066\u6271\u3046\ntotal_data.Parch = total_data.Parch \/ total_data.Parch.max()\ntotal_data.SibSp = total_data.SibSp \/ total_data.SibSp.max()\n\ntotal_data.head()","70d1a988":"# Since \"Sex\", \"Cabin\", \"Embarked\", and \"Pclass\" are categories, the data is in one-hot format.\nmake_one_hot=pd.get_dummies(total_data,columns=[\"Age\",\"Sex\",\"Cabin\",\"Embarked\",\"Pclass\"],)\nmake_one_hot = make_one_hot.dropna()","39a74f16":"#drop Cabin_Nan,Embarked_Nan\nmake_one_hot =  make_one_hot.drop(\"Age_-1.0\",axis=1)\nmake_one_hot =  make_one_hot.drop(\"Age_70.0\",axis=1)\nmake_one_hot =  make_one_hot.drop(\"Age_80.0\",axis=1)\nmake_one_hot =  make_one_hot.drop(\"Cabin_Nan\",axis=1)\nmake_one_hot =  make_one_hot.drop(\"Embarked_Nan\",axis=1)\n\nmake_one_hot.info()","ce3ab712":"# Separate training data and verification data.\ntrain_data = make_one_hot[(make_one_hot.index <= 891)]\ntest_data  = make_one_hot[~(make_one_hot.index <= 891) ]","fd1f9199":"# Convert to numpy format.\ntr_data=train_data.drop(\"Survived\",axis=1).to_numpy()\ntr_lable = train_data[\"Survived\"].to_numpy()\nval_data_n =  test_data.drop(\"Survived\",axis=1).to_numpy()\nval_label = test_data[\"Survived\"].to_numpy()","33cf4a93":"# start ML. use Tensorflow\nimport matplotlib.pylab as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds","a9c77ea8":"train_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_lable))\ntest_dataset = tf.data.Dataset.from_tensor_slices((val_data_n, val_label))\n\nBATCH_SIZE = 200\nSHUFFLE_BUFFER_SIZE = 300\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\ntest_dataset = test_dataset.batch(BATCH_SIZE)\n","404adb18":"# Hyper parameter\u306efine tune\u3092\u884c\u3046\nimport kerastuner as kt\nimport IPython\n","3baa99e3":"# GPU setting\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","00b4de26":"# Already completed\u3000at Version 32\ndef model_builder(hp):\n    model = keras.Sequential()\n\n    # input layer\n    hp_input_layer = hp.Int(\"InputParam\",min_value=32, max_value=64,step=4)\n    model.add(\n        keras.layers.Dense(hp_input_layer, \n                           activation='relu',\n                           input_shape=tr_data.shape[1:],  #  input_data shape\n                           name=\"input_layer\")\n    )\n\n    # layer1\n    hp_layer_units1 = hp.Int('units1', min_value = 32, max_value = 64, step = 4)\n    hp_reg_rate1 = hp.Choice('reg_rate1', values = [1e-2,1e-3,1e-4 ]) \n    hp_drop_rate1 = hp.Choice('drop_rate1', values = [0.2,0.3,0.5]) \n    \n    model.add(keras.layers.Dropout(hp_drop_rate1))\n    model.add(\n        keras.layers.Dense(\n            hp_layer_units1 ,\n            activation='relu',\n            kernel_regularizer=keras.regularizers.l2(hp_reg_rate1))\n    )\n\n    hp_layer_units2 = hp.Int('units2', min_value = 32, max_value = 64, step = 4)\n    hp_reg_rate2 = hp.Choice('reg_rate2', values = [1e-2,1e-3,1e-4]) \n    hp_drop_rate2 = hp.Choice('drop_rate2', values = [0.2,0.3,0.5]) \n\n    model.add(keras.layers.Dropout(hp_drop_rate2))\n    model.add(\n        keras.layers.Dense(\n            hp_layer_units2 ,\n            activation='relu',\n            kernel_regularizer=keras.regularizers.l2(hp_reg_rate2))\n    )    \n\n        \n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2,1e-3,1e-4 ]) \n    model.compile(loss=\"binary_crossentropy\",\n                  optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n                  metrics=[tf.keras.metrics.BinaryAccuracy() #\"accuracy\",\n                          ])\n    return model","65aa1015":"!rm -rf .\/my_dir\ntuner = kt.Hyperband(model_builder,\n                     objective = 'binary_accuracy', \n                     max_epochs = 50,\n                     directory = 'my_dir',\n                     project_name = 'intro_to_kt')","f65619fe":"class ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_binary_accuracy', \n                                              factor=0.5, patience=5, \n                                              min_lr=0.00005, verbose=1)\n","5d291923":"tuner.search(train_dataset,\n             batch_size=BATCH_SIZE,\n             epochs = 50, \n             validation_data = test_dataset, \n             callbacks = [ClearTrainingOutput(),early_stop,reduce_lr]\n            )\n","93493ee3":"# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\nfrom pprint import pprint\nprint(\"tuned model parameter----------\")\npprint(best_hps.values)\nprint(\"----------\")\nmodel = tuner.hypermodel.build(best_hps)\nmodel.summary()","8ca5d763":"hist = model.fit(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    epochs=200,\n    validation_data=test_dataset,\n    callbacks = [ClearTrainingOutput(),early_stop,reduce_lr],\n    verbose =2,\n                )","65f02324":"history_dict = hist.history\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)","2b46b7b8":"# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'b', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n","2bbee6e8":"plt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n","ff5bf45a":"count = 0\npred=model.predict(val_data_n)\n\nfor xi in  range(len(pred)):\n    if ((1 if  pred[xi][0]>0.5 else 0) !=  val_label[xi] ):\n        #print( \"index : {}\".format(xi) , (1 if  pred[xi][0]>0.5 else 0) ==  val_label[xi] )\n        count +=1\n        \nprint(\"False: \",count, \" total : \",len(pred))\nresults = model.evaluate(val_data_n,  val_label, verbose=2)","df3a78cc":"indexs = test_data.index.to_numpy()\npred_data = test_data.drop(\"Survived\",axis=1).to_numpy()","808de8a0":"pred_result=model.predict(pred_data)\nwith open(\".\/result.csv\", mode='w') as f:\n    f.write(\"PassengerId,Survived\\n\")\n    for (num,predict) in zip(indexs,pred_result):\n        #print(\"{},{}\".format(num,1 if predict[0]>0.5 else 0))\n        f.write(\"{},{}\\n\".format(num,1 if predict[0]>0.5 else 0))","b1030c3d":"!rm -rf .\/my_dir\n!head -n 5 result.csv","ca2c23c6":"model = tuner.hypermodel.build(best_hps)\nmodel.summary()","5bee7007":"# Separate training data and verification data.\ntrain_data = make_one_hot #[(make_one_hot.index <= 891)]\ntest_data  = make_one_hot[~(make_one_hot.index <= 891) ]\n\nhist = model.fit(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    epochs=200,\n    validation_data=test_dataset,\n    callbacks = [ClearTrainingOutput(),early_stop,reduce_lr],\n    verbose =2,\n                )","eda27056":"history_dict = hist.history\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)","d65a3c5e":"# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'b', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n","b63d5f5f":"plt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n","bf6f2eed":"from pprint import pprint \ncolums=test_data.drop(\"Survived\",axis=1).columns\ndata=test_dataset.take(1)\narr , label=next(iter(data))","1e840f36":"x = arr\ny = label\nwith tf.GradientTape() as tape:\n    tape.watch(x) # x \u3092\u8a18\u9332\n    pred = model(x)\n    # print(\"label :{}\".format(y))\n    # print(pred)\ngrads = tape.gradient(pred, x)\nmean_value = np.mean(grads,axis=0)\n# \u5024\u306e\u4f4e\u3044\u9805\u76ee\u3092 0 \u306b\u7f6e\u304d\u63db\u3048\u308b\n# mean_value[np.where(np.abs(mean_value) <0.05)] = 0","853a007c":"plt.figure(figsize=(20,10))\nplt.bar([i for i in range(len(colums))],mean_value,width = 1.)\nplt.ylabel(r\"$\\frac{\\partial y}{\\partial x_i}$\",fontsize=18)\nplt.xlabel(r\"$columns: x_i$\")\nplt.grid()\n_ = plt.xticks([i for i in range(len(colums))],colums,rotation=-90,fontsize=18)","02bb647e":"# ","ad041113":"# Search for parameters suitable for the model.\nI have no idea what the appropriate parameter value is, so I start with the Hyperparameter tune.","0c8f2227":"\nI haven't overfitted, but I'm not sure if it's improving.\u3000","790d3212":"# Titanic - Machine Learning from Disaster ","4b972cd5":"- From the above results, looking at the survival rate by age, it seems that the survival rate of young people aged 0 to 15 seems to be significantly higher.","baa620db":"---\n\n# The above is the tendency seen from the data. From this, machine learning is started using Tensorflow.","ca31f83e":"# Is Gender or Age Related to This Case?\n- Compare the survival rates of men and women.\n- As can be seen from the graph, the survival rate of women is more than 50% higher than that of men.\n\n| Sex |Survived rate|\n|:--|--:|\n| male | 18.9% |\n| female | 74.2% |","de1879d4":"# Is the ticket class related to the survival rate in this case?\n- The data set for this contest has three ticket classes, P-Class 1, 2, and 3. Let's compare whether this difference affects survival.","4c74d3bc":"# To create a better model.\n\n- From the notebook below, I suspect that there may be a difference between the training data and the validation data data.\n\n[An extensive data journey on the Titanic (Top 5%)](https:\/\/www.kaggle.com\/stefanschulmeister87\/an-extensive-data-journey-on-the-titanic-top-5)\n\n---\n\n- First, try training the model by including the validation data in the training data.  \n  By doing so, I thought that it would be possible to improve from the state of overfitting.","e27408be":"## From the above result\nFrom the above calculations, we found that the survival rate of each Pclass was higher than that of Pclass2 and Pclass1.  \nAlso, in terms of the overall survival rate, \nPclass1 accounted for about 25% of passengers, while it accounted for about 40% of survivors, \nsuggesting that the survival rate of Pclass1 class was high.  \nIn addition, Pclass3 accounts for nearly half of the passengers, \nwhile Pclass3 has a survival rate of only about 24%, \nwhich is lower than the 38.4% survival rate for this accident.\n\nIf the difference in Pclass does not affect the survival rate, it can be predicted that the survival rate will be about 38.4% for each class. However, from this result, it is considered that the difference in Pclass affects the survival rate.\n\n|Pclass|Rate(*1)|Survival rate(*2)|Survival rate(Among all survivors)(*3)|\n|:--|--:|--:|--:|\n| Pclass 1 | 24.2%|63.0% |  39.8% |\n| Pclass 2 | 20.7%|47.3% |25.4% |\n| Pclass 3 | 55.1%| 24.2% | 34.8% |\n\n- *1 :Percentage of passengers in each class to total\n- *2 :Survival rate for each class\n- *3 :Percentage of survivors classified by pclass\n\n","cb731fec":"---\n\n# use GradientTape","6b328a38":"# I don't know how to evaluate the name and ticket, so delete it from the data.","5203bb1c":"# Generate the determined model.","7ac83a9e":"# First, prepare the dataset.(Work in progress)\nPreliminary research has shown that it probably contains nulls and that it contains data in non-numeric categories.  \nTherefore, change the dataset to one-hot format by deleting or deleting it in a format that can handle null.","62adf31d":"## Gender differences may have a significant impact on survival.\nAs you can see from the graph above, women have significantly higher survival rates than men.\nI think this was a major factor in separating life and death.","9eb8aa6a":"# Is age life-threatening in this case?\n- Group by 5 years and create a histogram.","5f3812bf":"# Conclusion\n(For the time being, I wrote it intuitively, and it is necessary to consider whether it is correct as a way of thinking.)"}}