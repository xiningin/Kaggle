{"cell_type":{"10c153ea":"code","a0d83340":"code","2b6ecda0":"code","c25e6b1d":"code","2dc5d75d":"code","7d9fc70f":"code","51c2cb1a":"code","116edee9":"code","2ae16a42":"code","5d1de3ad":"code","33cae0cf":"code","3848ba2e":"code","0c089f45":"code","fda86300":"code","9d5daa25":"markdown","bbd944b8":"markdown","ef1122e2":"markdown","32da22c8":"markdown","41d823bd":"markdown","7fe1aaa0":"markdown"},"source":{"10c153ea":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\n\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.simplefilter('ignore')","a0d83340":"N_SPLITS = 5\nN_ESTIMATORS = 25001\nEARLY_STOPPING_ROUNDS = 3048 # very important, well protects against overfitting\nVERBOSE = 1000 # faster and more clearly\nSEED = 42","2b6ecda0":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","c25e6b1d":"INPUT = \"..\/input\/tabular-playground-series-sep-2021\/\"\n\ntrain = pd.read_csv(INPUT + \"train.csv\")\ntest = pd.read_csv(INPUT + \"test.csv\")\nsubmission = pd.read_csv(INPUT + \"sample_solution.csv\")\n\n","2dc5d75d":"features = [col for col in test.columns if 'f' in col]\nTARGET = 'claim'\n\ntarget = train[TARGET].copy()","7d9fc70f":"train['n_missing'] = train[features].isna().sum(axis=1)\n# train['mean'] = train[features].mean(axis=1)\n# train['median'] = train[features].median(axis=1)\n# train['std'] = train[features].std(axis=1)\ntrain['min'] = train[features].min(axis=1)\n# train['max'] = train[features].max(axis=1)\ntrain['sem']= train[features].sem(axis=1)\ntrain['quantile'] = train[features].quantile(axis = 1)\n\ntest['n_missing'] = test[features].isna().sum(axis=1)\n# test['mean'] = test[features].mean(axis=1)\n# test['median'] = test[features].median(axis=1)\n# test['std'] = test[features].std(axis=1)\ntest['min'] = test[features].min(axis=1)\n# test['max'] = test[features].max(axis=1)\ntest['sem']= test[features].sem(axis=1)\ntest['quantile'] = test[features].quantile(axis=1)\n\n# features += ['n_missing','mean','median','std','min','max','sem','quantile']\nfeatures += ['n_missing','min','sem','quantile']\nn_missing = train['n_missing'].copy()","51c2cb1a":"# fill_value_dict = {\n#     'f1': 'Mean', \n#     'f2': 'Median', \n#     'f3': 'Median', \n#     'f4': 'Median', \n#     'f5': 'Mode', \n#     'f6': 'Mean', \n#     'f7': 'Median', \n#     'f8': 'Median', \n#     'f9': 'Median', \n#     'f10': 'Median', \n#     'f11': 'Mean', \n#     'f12': 'Median', \n#     'f13': 'Mean', \n#     'f14': 'Median', \n#     'f15': 'Mean', \n#     'f16': 'Median', \n#     'f17': 'Median', \n#     'f18': 'Median', \n#     'f19': 'Median', \n#     'f20': 'Median', \n#     'f21': 'Median', \n#     'f22': 'Mean', \n#     'f23': 'Mode', \n#     'f24': 'Median', \n#     'f25': 'Median', \n#     'f26': 'Median', \n#     'f27': 'Median', \n#     'f28': 'Median', \n#     'f29': 'Mode', \n#     'f30': 'Median', \n#     'f31': 'Median', \n#     'f32': 'Median', \n#     'f33': 'Median', \n#     'f34': 'Mean', \n#     'f35': 'Median', \n#     'f36': 'Mean', \n#     'f37': 'Median', \n#     'f38': 'Median', \n#     'f39': 'Median', \n#     'f40': 'Mode', \n#     'f41': 'Median', \n#     'f42': 'Mode', \n#     'f43': 'Mean', \n#     'f44': 'Median', \n#     'f45': 'Median', \n#     'f46': 'Mean', \n#     'f47': 'Mode', \n#     'f48': 'Mean', \n#     'f49': 'Mode', \n#     'f50': 'Mode', \n#     'f51': 'Median', \n#     'f52': 'Median', \n#     'f53': 'Median', \n#     'f54': 'Mean', \n#     'f55': 'Mean', \n#     'f56': 'Mode', \n#     'f57': 'Mean', \n#     'f58': 'Median', \n#     'f59': 'Median', \n#     'f60': 'Median', \n#     'f61': 'Median', \n#     'f62': 'Median', \n#     'f63': 'Median', \n#     'f64': 'Median', \n#     'f65': 'Mode', \n#     'f66': 'Median', \n#     'f67': 'Median', \n#     'f68': 'Median', \n#     'f69': 'Mean', \n#     'f70': 'Mode', \n#     'f71': 'Median', \n#     'f72': 'Median', \n#     'f73': 'Median', \n#     'f74': 'Mode', \n#     'f75': 'Mode', \n#     'f76': 'Mean', \n#     'f77': 'Mode', \n#     'f78': 'Median', \n#     'f79': 'Mean', \n#     'f80': 'Median', \n#     'f81': 'Mode', \n#     'f82': 'Median', \n#     'f83': 'Mode', \n#     'f84': 'Median', \n#     'f85': 'Median', \n#     'f86': 'Median', \n#     'f87': 'Median', \n#     'f88': 'Median', \n#     'f89': 'Median', \n#     'f90': 'Mean', \n#     'f91': 'Mode', \n#     'f92': 'Median', \n#     'f93': 'Median', \n#     'f94': 'Median', \n#     'f95': 'Median', \n#     'f96': 'Median', \n#     'f97': 'Mean', \n#     'f98': 'Median', \n#     'f99': 'Median', \n#     'f100': 'Mode', \n#     'f101': 'Median', \n#     'f102': 'Median', \n#     'f103': 'Median', \n#     'f104': 'Median', \n#     'f105': 'Median', \n#     'f106': 'Median', \n#     'f107': 'Median', \n#     'f108': 'Median', \n#     'f109': 'Mode', \n#     'f110': 'Median', \n#     'f111': 'Median', \n#     'f112': 'Median', \n#     'f113': 'Mean', \n#     'f114': 'Median', \n#     'f115': 'Median', \n#     'f116': 'Mode', \n#     'f117': 'Median', \n#     'f118': 'Mean'\n# }\n\n# for col in tqdm(features):\n#     if fill_value_dict.get(col)=='Mean':\n#         fill_value = train[col].mean()\n#     elif fill_value_dict.get(col)=='Median':\n#         fill_value = train[col].median()\n#     elif fill_value_dict.get(col)=='Mode':\n#         fill_value = train[col].mode().iloc[0]\n    \n#     train[col].fillna(fill_value, inplace=True)\n#     test[col].fillna(fill_value, inplace=True)\n\n# train.dropna(inplace=True)\n# test.dropna(inplace=True)","116edee9":"pipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler()) #StandardScaler RobustScaler\n])\ntrain[features] = pipeline.fit_transform(train[features])\ntest[features] = pipeline.transform(test[features])","2ae16a42":"train.shape, test.shape","5d1de3ad":"xgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': N_ESTIMATORS, \n    'learning_rate': 0.01, \n    'gamma': 0.25, \n    'max_depth': 4, \n    'min_child_weight': 378, \n    'subsample': 0.63, \n    'colsample_bytree': 0.77, \n    'colsample_bylevel': 0.87, \n    'lambda': 0.05, \n    'alpha': 10\n}\n","33cae0cf":"xgb_oof = np.zeros(train.shape[0])\nxgb_pred = np.zeros(test.shape[0])\nxgb_importances = pd.DataFrame()\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=n_missing)):\n    print(f\">>> fold {fold} >>>\")\n    X_train = train[features].iloc[trn_idx]\n    y_train = target.iloc[trn_idx]\n    X_valid = train[features].iloc[val_idx]\n    y_valid = target.iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = XGBClassifier(**xgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],                \n        verbose=VERBOSE,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS # very important, well protects against overfitting\n    )    \n    df_tmp = pd.DataFrame()\n    df_tmp['fold'] = fold\n    df_tmp['seed'] = SEED\n    xgb_importances = xgb_importances.append(df_tmp)\n    xgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    xgb_pred += model.predict_proba(X_test)[:, -1] \/ N_SPLITS\n\n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, xgb_oof[val_idx])\n    print(f\"fold {fold} - xgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof xgb roc = {roc_auc_score(target, xgb_oof)}\")","3848ba2e":"submission[TARGET] = xgb_pred\nsubmission.to_csv(\"submission.csv\", index=False)","0c089f45":"train.isna().sum()","fda86300":"train.describe()","9d5daa25":"# Datasets\n---","bbd944b8":"#  XGBClassifier\n---","ef1122e2":"Idea taken from https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm\nMissing feature values are replaced depending on the type of distribution.","32da22c8":"# Libraries\n---","41d823bd":"# Submission\n---","7fe1aaa0":"# Parameters\n---"}}