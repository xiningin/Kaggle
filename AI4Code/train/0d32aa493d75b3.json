{"cell_type":{"459e7bcd":"code","8ee05af4":"code","d05bc2b3":"code","2da8583f":"markdown"},"source":{"459e7bcd":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets","8ee05af4":"# Configurations\n# Number of folds for training\nFOLDS = 5\n# Max length\nMAX_LEN = 250\n# Get the trained model we want to use\nMODEL = '..\/input\/tfroberta-base'\n# Let's load our model tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)","d05bc2b3":"# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_test, MAX_LEN):\n    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_test\n\n# Function to build our model\ndef build_roberta_base_model(max_len = MAX_LEN):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    return model\n\n# Function for inference\ndef roberta_base_inference():\n    # Read our test data\n    df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n    # Get text features\n    x_test = df['excerpt']\n    # Encode our text with Roberta tokenizer\n    x_test = encode_texts(x_test, MAX_LEN)\n    # Initiate an empty vector to store prediction\n    predictions = np.zeros(len(df))\n    # Predict with the 5 models (5 folds training)\n    for i in range(FOLDS):\n        print('\\n')\n        print('-'*50)\n        print(f'Predicting with model {i + 1}')\n        # Build model\n        model = build_roberta_base_model(max_len = MAX_LEN)\n        # Load pretrained weights\n        model.load_weights(f'..\/input\/commonlit-readability-roberta-base\/Roberta_Base_123_{i + 1}.h5')\n        # Predict\n        fold_predictions = model.predict(x_test).reshape(-1)\n        # Add fold prediction to the global predictions\n        predictions += fold_predictions \/ FOLDS\n    # Save submissions\n    df['target'] = predictions\n    df[['id', 'target']].to_csv('submission.csv', index = False)\n    return df\n\ndf = roberta_base_inference()\ndf.head()","2da8583f":"# Comments\n\nHere you can find a simple baseline model: https:\/\/www.kaggle.com\/ragnar123\/commonlit-readability-roberta-tf\n\nThis model has an out of folds root mean squared error of 0.5097\n\nHere is the training script:\n\nI tried to use tpu for training but the results are no good, hope this tensorflow baseline help to start this competition."}}