{"cell_type":{"bae59216":"code","9b9abc93":"code","e93a4cb7":"code","7e3c75ce":"code","a9dce3af":"code","a806dbc4":"code","715e4ec7":"code","09f23378":"code","3146b285":"code","a3281959":"code","47619fac":"code","2b9553e0":"code","4ae79985":"code","e44e0775":"code","afd1394d":"code","0f431d0e":"code","35d1f299":"code","76f00f41":"code","5d29387f":"code","cec6602d":"code","22011992":"code","738a709b":"code","ca512f67":"code","cdbd102b":"code","03935c9a":"code","23f01db1":"code","3be114ac":"code","7ec3d5da":"markdown","9613a64f":"markdown","ff084c77":"markdown","b1cde018":"markdown","4a6f6493":"markdown","9951484c":"markdown","aa307326":"markdown"},"source":{"bae59216":"import operator\nfrom functools import reduce\nimport jieba\nimport gensim\nimport json\nimport numpy as np\n#from keras.layers import Input\nfrom keras.models import Model,Sequential\nfrom keras.layers import InputLayer,Embedding,LSTM,Dense,TimeDistributed,SimpleRNN,Input\nfrom keras.optimizers import SGD,Adam,Adadelta,RMSprop\nwith open('..\/input\/input11\/minipaperoutq1.txt','r') as lyrics:\n    raw_text_w2v=lyrics.readlines()\n    #print(load_file)\n    #songs = json.load(load_file)\nraw_text_w2v = [i.strip('\\n') for i in raw_text_w2v]\nlyrics = jieba.lcut_for_search(str(raw_text_w2v))\n#print(raw_text_w2v)\n#raw_text_w2v = ' '.join(raw_text_w2v)#\u7528\u7a7a\u683c\u5206\u5f00","9b9abc93":"'''\nbatch_size = 64  \nepochs = 9\nlatent_dim = 256  \nembedding_size = 128\nfile_name = '..\/input\/poetry.txt'\n'''","e93a4cb7":"\nraw_text = []\nraw_text_w2v = []\nfor lyric in lyrics:\n    if '\u3001' in lyric:\n        continue\n    if '\u25a1' in lyric:\n        continue\n    #lyric = lyric + '\\n'\n    raw_text.extend([word for word in lyric])\n    raw_text_w2v.append([word for word in lyric])\n","7e3c75ce":"#print(l[:1])","a9dce3af":"'''\nl=raw_text_w2v\nword=reduce(operator.add,reduce(operator.add,l))\nword=word.replace('\uff0c',',')        \nword=word.split(',')\n#word=l[0]\nprint(word[:10])\n'''","a806dbc4":"\n'''\nfor i in range(len(word)+1):\n    if i == len(word)-1:\n        break\n    word[i]=word[i]+'\uff0c'+word[i+1]\nprint(word[:5])\n'''","715e4ec7":"#print(word[:3])","09f23378":"#print(type(raw_text_w2v[1]))","3146b285":"#for line in raw_text_w2v:\n #   print(line)","a3281959":"'''\ninput_texts = []\ntarget_texts = []\ninput_vocab = set()\ntarget_vocab = set()\n#with open(file_name, 'r', encoding='utf-8') as f:\n#    lines = f.readlines()\nfor line in word:\n    # \u5c06\u8bd7\u53e5\u7528\u9017\u53f7\u5206\u5f00\n    line_sp = line.strip().split('\uff0c')\n    # \u5982\u679c\u8bd7\u4e2d\u4e0d\u542b\u9017\u53f7\uff0c\u8fd9\u53e5\u8bd7\u6211\u4eec\u5c31\u4e0d\u7528\u4e86\n    if len(line_sp) < 2:\n        continue\n    # \u4e0a\u53e5\u4e3ainput_text\uff0c\u4e0b\u53e5\u4e3atarget_text\n    input_text, target_text = line_sp[0], line_sp[1]\n    # \u5728\u4e0b\u53e5\u524d\u540e\u5f00\u59cb\u5b57\u7b26\u548c\u7ed3\u675f\u5b57\u7b26\n    target_text = '\\t' + target_text[:-1] + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    # \u7edf\u8ba1\u8f93\u5165\u4fa7\u7684\u8bcd\u6c47\u8868\u548c\u8f93\u51fa\u4fa7\u7684\u8bcd\u6c47\u8868\n    for ch in input_text:\n        if ch not in input_vocab:\n            input_vocab.add(ch)\n    for ch in target_text:\n        if ch not in target_vocab:\n            target_vocab.add(ch)\n\n# \u5efa\u7acb\u5b57\u5178\u548c\u53cd\u5411\u5b57\u5178\ninput_vocab = dict([(char, i) for i, char in enumerate(input_vocab)])\ntarget_vocab = dict([(char, i) for i, char in enumerate(target_vocab)])\nreverse_input_char_index = dict((i, char) for char, i in input_vocab.items())\nreverse_target_char_index = dict((i, char) for char, i in target_vocab.items())\n\n# \u8f93\u5165\u4fa7\u8bcd\u6c47\u8868\u5927\u5c0f\nencoder_vocab_size = len(input_vocab)\n# \u6700\u957f\u8f93\u5165\u53e5\u5b50\u957f\u5ea6\nencoder_len = max([len(sentence) for sentence in input_texts])\n# \u8f93\u51fa\u4fa7\u8bcd\u6c47\u8868\u5927\u5c0f\ndecoder_vocab_size = len(target_vocab)\n# \u6700\u957f\u8f93\u51fa\u53e5\u5b50\u957f\u5ea6\ndecoder_len = max([len(sentence) for sentence in target_texts])\nprint(encoder_vocab_size)\nprint(encoder_len)\nprint(decoder_vocab_size)\nprint(decoder_len)\nprint(input_vocab)\nprint(input_texts[:100]) \n'''","47619fac":"#print(raw_text_w2v[0:100])","2b9553e0":"\nmodelx = gensim.models.word2vec.Word2Vec(raw_text_w2v, size=300, min_count=1)\nmodelx.save(\"w2v.model\")\nmodelx = gensim.models.Word2Vec.load(\".\/\" + \"w2v.model\")\nall_word_vector = modelx[modelx.wv.vocab]\nall_word_vector = np.append(all_word_vector, [np.zeros(300)], axis = 0)\nprint(all_word_vector.shape)\n","4ae79985":"modelx.most_similar('\u597d', topn=10)","e44e0775":"vocab_inv = list(modelx.wv.vocab)\nvocab = {x:index for index,x in enumerate(vocab_inv)}","afd1394d":"\n\ndef build_matrix(text, vocab, length, step):\n    M = []\n    for word in text:\n        index = vocab.get(word)\n        if (index is None):\n            M.append(len(vocab)) \n        else:\n            M.append(index)\n    num_sentences = len(M) \/\/ length\n    M = M[: num_sentences * length] \n    M = np.array(M)\n    X = []\n    Y = []\n    for i in range(0, len(M) - length, step):\n        X.append(M[i : i + length])\n        Y.append([[x] for x in M[i + 1 : i + length + 1]])\n    return np.array(X), np.array(Y)\n\n","0f431d0e":"\nseq_length = 4\n\nX, Y = build_matrix(raw_text, vocab, seq_length, seq_length)\n\nprint(\"\u7b2c150\u4e2a\u8f93\u5165\u77e9\u9635\uff1a\",X[150])\nprint(\"\u7b2c150\u4e2a\u8f93\u51fa\u77e9\u9635\uff1a\\n\",Y[150])","35d1f299":"model = Sequential()\nmodel.add(InputLayer(input_shape=(None, )))\nmodel.add(Embedding(input_dim=len(vocab)+1,output_dim=300,trainable=True,weights=[all_word_vector]))\n# \u8bcd\u5d4c\u5165\u5c42\uff0cinput_dim\u8868\u793a\u8bcd\u5178\u7684\u5927\u5c0f\uff0c\u6bcf\u4e2a\u8bcd\u5bf9\u5e94\u4e00\u4e2aoutput_dim\u7ef4\u7684\u8bcd\u5411\u91cf\uff0cweights\u4e3a\u5148\u524d\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\nmodel.add(LSTM(units=300,return_sequences=True)) \n# LSTM\u5c42\uff0c\u8bed\u8a00\u5904\u7406\u5c42\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a(seq_length,300)\nmodel.add(TimeDistributed(Dense(units=len(vocab)+1,activation=\"softmax\")))\n# \u8f93\u51fa\u7684300\u7ef4\u5411\u91cf\u9700\u8981\u7ecf\u8fc7\u4e00\u6b21\u7ebf\u6027\u53d8\u6362\uff08\u4e5f\u5c31\u662fDense\u5c42\uff09\u8f6c\u5316\u4e3alen(vocab)+1\u7ef4\u7684\u5411\u91cf\uff0c\u7528softmax\u53d8\u6362\u5c06\u5176\u8f6c\u5316\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u7b2ci\u7ef4\u8868\u793a\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684\u8bcd\u662fi\u53f7\u5355\u8bcd\u7684\u6982\u7387\nmodel.compile(optimizer=Adam(lr=0.001),loss='sparse_categorical_crossentropy')\n# \u4f18\u5316\u5668\u4e3aAdam\uff0c\u635f\u5931\u51fd\u6570\u4e3a\u4ea4\u53c9\u71b5\nmodel.summary()\n","76f00f41":"'''\nencoder_input_data = np.zeros((len(input_texts), encoder_len), dtype='int')\ndecoder_input_data = np.zeros((len(input_texts), decoder_len), dtype='int')\ndecoder_target_data = np.zeros((len(input_texts), decoder_len, 1), dtype='int')\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t] = input_vocab[char]\n    for t, char in enumerate(target_text):\n        decoder_input_data[i, t] = target_vocab[char]\n        if t > 0:\n            decoder_target_data[i, t - 1, 0] = target_vocab[char]\n            \nprint(encoder_input_data.shape)\nprint(decoder_input_data.shape)\nprint(encoder_input_data[:100])\nprint(decoder_input_data.shape)\nprint(decoder_target_data.shape)\n'''","5d29387f":"'''\n# \u7f16\u7801\u5668\u8f93\u5165\u5c42\nencoder_inputs = Input(shape=(None,))\n# \u7f16\u7801\u5668\u8bcd\u5d4c\u5165\u5c42\nencoder_embedding = Embedding(input_dim=encoder_vocab_size, output_dim=embedding_size, trainable=True)(encoder_inputs)\n# \u7f16\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u5c42\nencoder = LSTM(latent_dim, return_state=True)\n# \u7f16\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u8f93\u51fa\u662f\u4e00\u4e2a\u4e09\u5143\u7ec4(encoder_outputs, state_h, state_c)\n# encoder_outputs\u662f\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u6bcf\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u6784\u6210\u7684\u5e8f\u5217\n# state_h\u548cstate_c\u662f\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u72b6\u6001\u548c\u7ec6\u80de\u72b6\u6001\nencoder_outputs, state_h, state_c = encoder(encoder_embedding)\n# \u6211\u4eec\u4f1a\u628astate_h\u548cstate_c\u4f5c\u4e3a\u89e3\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7684\u521d\u59cb\u72b6\u6001\uff0c\u4e4b\u524d\u6211\u4eec\u6240\u8bf4\u7684\u72b6\u6001\u5411\u91cf\u7684\u4f20\u9012\u5c31\u662f\u8fd9\u6837\u5b9e\u73b0\u7684\nencoder_states = [state_h, state_c]\n\n# \u89e3\u7801\u5668\u7f51\u7edc\u5efa\u6784\n\n# \u89e3\u7801\u5668\u8f93\u5165\u5c42\ndecoder_inputs = Input(shape=(None,))\n# \u89e3\u7801\u5668\u8bcd\u5d4c\u5165\u5c42\ndecoder_embedding = Embedding(input_dim=decoder_vocab_size, output_dim=embedding_size, trainable=True)(decoder_inputs)\n# \u89e3\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u5c42\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n# \u89e3\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7684\u8f93\u51fa\u4e5f\u662f\u4e09\u5143\u7ec4\uff0c\u4f46\u6211\u4eec\u53ea\u5173\u5fc3\u4e09\u5143\u7ec4\u7684\u7b2c\u4e00\u7ef4\uff0c\u540c\u65f6\u6211\u4eec\u5728\u8fd9\u91cc\u8bbe\u7f6e\u4e86\u89e3\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7684\u521d\u59cb\u72b6\u6001\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n# \u89e3\u7801\u5668\u8f93\u51fa\u7ecf\u8fc7\u4e00\u4e2a\u9690\u5c42softmax\u53d8\u6362\u8f6c\u6362\u4e3a\u5bf9\u5404\u7c7b\u522b\u7684\u6982\u7387\u4f30\u8ba1\ndecoder_dense = Dense(decoder_vocab_size, activation='softmax')\n# \u89e3\u7801\u5668\u8f93\u51fa\u5c42\ndecoder_outputs = decoder_dense(decoder_outputs)\n# \u603b\u6a21\u578b\uff0c\u63a5\u53d7\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8f93\u5165\uff0c\u5f97\u5230\u89e3\u7801\u5668\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u8f93\u51fa\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\nmodel.summary()\n'''","cec6602d":"model.fit(X,Y,batch_size=512,epochs=100,verbose=1)\n#model.fit([encoder_input_data, decoder_input_data],decoder_target_data, batch_size=64, epochs=200, validation_split=0.2)","22011992":"model.save('model.h5')","738a709b":"\nst = '\u4e0e\u4f60\u65e0\u5173\uff0c'\nprint(st,end = \"\")\n\nvocab_inv.append('')\n\ni = 0\nwhile i < 200:\n    X_sample = np.array([[vocab.get(x, len(vocab)) for x in st]]) \n    out = model.predict(X_sample) \n    out_2 = out[:,-1:,:] \n    out_3 = out_2[0][0]\n    out_4 = (-out_3).argsort() \n    pdt = out_4[:3]\n    pb = [out_3[index] for index in pdt]\n    if vocab['\uff0c'] in pdt:\n        ch = '\uff0c'\n    else:\n        ch = vocab_inv[np.random.choice(pdt, p=pb\/sum(pb))]\n    print(ch,end='')\n    st = st + ch\n    if vocab[ch] != len(vocab) and ch != '\uff0c' and ch != '\u3002' and ch != '\\n':\n        i += 1\n","ca512f67":"'''\n# \u7b2c\u4e00\u4e2a\u9ed1\u76d2\uff0c\u7f16\u7801\u5668\uff0c\u7ed9\u5b9aencoder_inputs\uff0c\u5f97\u5230encoder\u7684\u72b6\u6001\nencoder_model = Model(encoder_inputs, encoder_states)\n# \u7b2c\u4e8c\u4e2a\u9ed1\u76d2\uff0c\u89e3\u7801\u5668\n# \u89e3\u7801\u5668\u63a5\u53d7\u4e09\u4e2a\u8f93\u5165\uff0c\u4e24\u4e2a\u662f\u521d\u59cb\u72b6\u6001\uff0c\u4e00\u4e2a\u662f\u4e4b\u524d\u5df2\u7ecf\u751f\u6210\u7684\u6587\u672c\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n# \u89e3\u7801\u5668\u4ea7\u751f\u4e09\u4e2a\u8f93\u51fa\uff0c\u4e24\u4e2a\u5f53\u524d\u72b6\u6001\uff0c\u4e00\u4e2a\u662f\u6bcf\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\uff0c\u5176\u4e2d\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u53ef\u4ee5\u7528\u6765\u8ba1\u7b97\u4e0b\u4e00\u4e2a\u5b57\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n'''","cdbd102b":"'''\ndef decode_sequence(input_seq):\n    # \u5148\u628a\u4e0a\u53e5\u8f93\u5165\u7f16\u7801\u5668\u5f97\u5230\u7f16\u7801\u7684\u4e2d\u95f4\u5411\u91cf\uff0c\u8fd9\u4e2a\u4e2d\u95f4\u5411\u91cf\u5c06\u662f\u89e3\u7801\u5668\u7684\u521d\u59cb\u72b6\u6001\u5411\u91cf\n    states_value = encoder_model.predict(input_seq)\n    # \u521d\u59cb\u7684\u89e3\u7801\u5668\u8f93\u5165\u662f\u5f00\u59cb\u7b26'\\t'\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_vocab['\\t']\n\n    stop_condition = False\n    decoded_sentence = ''\n    # \u8fed\u4ee3\u89e3\u7801\n    while not stop_condition:\n        # \u628a\u5f53\u524d\u7684\u89e3\u7801\u5668\u8f93\u5165\u548c\u5f53\u524d\u7684\u89e3\u7801\u5668\u72b6\u6001\u5411\u91cf\u9001\u8fdb\u89e3\u7801\u5668\n        # \u5f97\u5230\u5bf9\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684\u9884\u6d4b\u548c\u65b0\u7684\u89e3\u7801\u5668\u72b6\u6001\u5411\u91cf\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n        # \u91c7\u6837\u51fa\u6982\u7387\u6700\u5927\u7684\u90a3\u4e2a\u5b57\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684\u8f93\u5165\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n        # \u5982\u679c\u91c7\u6837\u5230\u4e86\u7ed3\u675f\u7b26\u6216\u8005\u751f\u6210\u7684\u53e5\u5b50\u957f\u5ea6\u8d85\u8fc7\u4e86decoder_len\uff0c\u5c31\u505c\u6b62\u751f\u6210\n        #if (sampled_char == '\\n' ):\n         #   stop_condition = True\n        # \u5426\u5219\u6211\u4eec\u66f4\u65b0\u4e0b\u4e00\u4e2a\u65f6\u523b\u7684\u89e3\u7801\u5668\u8f93\u5165\u548c\u89e3\u7801\u5668\u72b6\u6001\u5411\u91cf\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n    return decoded_sentence\n    '''","03935c9a":"'''\nfor seq_index in range(200, 300):\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)\n'''","23f01db1":"#print(encoder_input_data)","3be114ac":"#num=45\n\n#num = encoder_input_data[num: num + 1]\n\n#for a in range(20):\n#    word=decode_sequence(num)  \n#    print(word)  \n\n#    num=np.array([[input_vocab[i] for i in word]])\n    #print(num)\n        ","7ec3d5da":"\u642d\u5efa\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1a","9613a64f":"\u6b4c\u8bcd\u751f\u6210\uff1a","ff084c77":"1. \u5bfc\u5165\u51fd\u6570&\u8f7d\u5165\u6570\u636e\u96c6\uff1a","b1cde018":"\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff1a\u6279\u6b21\u5927\u5c0f\uff1a512\uff1b\u8bad\u7ec3\u8f6e\u6570\uff1a40","4a6f6493":"\u5c06\u62c6\u5206\u5b8c\u7684\u6570\u636e\u9001\u7ed9word2vec\u6a21\u578b\u8bad\u7ec3\u8bcd\u5411\u91cf\uff1a","9951484c":"\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u5236\u4f5c\u8bad\u7ec3\u6570\u636e\u2014\u2014\u8bad\u7ec3\u6570\u636e\u662f\u4e00\u4e2a\u8f93\u5165\u8f93\u51fa\u5bf9\uff0c\u6839\u636e\u6211\u4eec\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\uff0c\u8f93\u5165\u662f (wi,wi+1,\u2026,wi+length) \u7684\u8bdd\uff0c\u8f93\u51fa\u662f (wi+1,wi+2,\u2026,wi+length+1)","aa307326":"\u6570\u636e\u521d\u59cb\u5316\u2014\u2014\u53bb\u9664\u6587\u672c\u4e2d\u7684\u4e71\u7801\u5e76\u628a\u6587\u672c\u5206\u89e3\u6210\u5355\u72ec\u7684\u5b57\uff1a"}}