{"cell_type":{"64c6aac3":"code","cbfcdf8c":"code","2d0e412d":"code","59ecb110":"code","cb6b518f":"code","52aa08ca":"code","9061c99f":"code","4f0124b1":"code","a31abbb3":"code","e1fb69d6":"code","31689852":"code","3ae0707f":"code","585c2f9f":"code","2ad9bd40":"code","86f45803":"code","1a6c9201":"code","13c85024":"code","c2aac4f5":"code","5ebc6a8c":"code","7ca024e9":"code","f2203953":"code","3651e906":"markdown","29fda58a":"markdown","bb191ad8":"markdown","e9837467":"markdown","f63c5453":"markdown","a32efa63":"markdown","6095954e":"markdown","7797d2c4":"markdown","ca413cc7":"markdown","e0f2a99f":"markdown","ba58a5fb":"markdown","f9dd00f8":"markdown","8884bb4c":"markdown","325410af":"markdown","687559ea":"markdown","fe0386d1":"markdown","6dd9e69b":"markdown","8521090d":"markdown","69ed3952":"markdown","eab16581":"markdown","5b7daec5":"markdown","08479577":"markdown"},"source":{"64c6aac3":"# Math and Image\nimport numpy as np               # Math\nimport matplotlib.pyplot as plt  # Plot\nimport cv2                       # OpenCV\nimport PIL                       # Data Augmentation\nimport albumentations            # Data Augmentation\nfrom albumentations.pytorch import ToTensorV2\n# Operating System\nimport os\nimport time\nimport copy\n# Utilities\nimport pandas as pd  # Handling CSV files\nimport random\nimport json\nimport tqdm          # Testing\nimport sklearn       # label encoding and metrics\nfrom sklearn import preprocessing\n# Torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n# TensorFlow\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets\n# preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved\n# outside of the current session will list all files under the input directory\n\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)","cbfcdf8c":"# CPU Details - Number & Model\n!lscpu | grep \"CPU(s):\"\n!lscpu | grep Hz\n# Total Memory\n!cat \/proc\/meminfo | grep MemTotal\n# CUDA Check\nprint(\"Is CUDA Available?\",\n      torch.cuda.is_available())\nif torch.cuda.is_available() == True: \n    print(\"Current CUDA device:\",\n        torch.cuda.get_device_name(0))\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device: \",DEVICE)","2d0e412d":"def seed_fix(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_fix()\nprint(\"Seed fixed\")","59ecb110":"ROOT_DATA   = \"\/kaggle\/input\/herbarium-2021-fgvc8\/\"\nDATA_TRAIN  = ROOT_DATA + \"train\/\"\nDATA_TEST   = ROOT_DATA + \"test\/\"\nROOT_OUTPUT = \"\/kaggle\/working\/\"\nMETA        = \"metadata.json\"\nBATCH_SIZE  = 128    # number of training examples utilized in one iteration\nBATCH_EVAL  = 128\nSHUFFLE     = True\nEPOCHS      = 1\nLEARN_RATE  = 4e-4\nIMG_HEIGHT  = 224\nIMG_WIDTH   = 224\nNUM_CLASSES = None   # define below\nNUM_WORKERS = 4\nPRE_TRAINED = False\n\nPATH_SAVE_MODEL = \"\/kaggle\/working\/ResNet50_da_run-09.pth\"\n\nESTIMATED_MAX_TRAINING_TIME = 480 # hours (8h * 60 = 480min, leaving 1h to the test time)\n\nprint(ROOT_DATA)","cb6b518f":"# Training dataset\nwith open(os.path.join(DATA_TRAIN,META),\"r\",encoding=\"ISO-8859-1\") as file:\n    meta_train = json.load(file)\n    print(\"Number of images (training dataset): \",\n          len(meta_train[\"images\"]),)\n    for i in list(meta_train.keys()):\n        print(\"  - sample and number of elements in category {}: \".format(i),\n              len(list(meta_train[i])),)\n        print(\"\\t[0] \",\n              list(meta_train[i])[0], end=\"\\n\")\n\nNUM_CLASSES   = len(meta_train[\"categories\"])\nNUM_IMG_TRAIN = len(meta_train[\"annotations\"])\n\n# Validation dataset\nwith open(os.path.join(DATA_TEST,META),\"r\",encoding=\"ISO-8859-1\") as file:\n    meta_test = json.load(file)\n    print(\"\\nNumber of images (training dataset): \",\n          len(meta_test[\"images\"]),)\n    for i in list(meta_test.keys()):\n        print(\"  - sample and number of elements in category {}: \".format(i),\n              len(list(meta_test[i])),)\n        print(\"\\t[0] \",\n              list(meta_test[i])[0], end=\"\\n\")\n\nNUM_IMG_TEST  = len(meta_test[\"images\"]) \n\n# Print variables\nprint(\"\\n\\n\"\n      \"Number of images for training: \",NUM_IMG_TRAIN)\nprint(\"Number of images for testing : \",NUM_IMG_TEST)\nprint(\"Number of classes            : \",NUM_CLASSES)","52aa08ca":"# Process metadata json for training images into a DataFrame\ntrain_img = pd.DataFrame(meta_train[\"images\"])\ntrain_ann = pd.DataFrame(meta_train[\"annotations\"]).drop(columns=\"image_id\")\ntrain_df  = train_img.merge(train_ann,on=\"id\") # Performs a database-style joint\n\n# Check number of classes\nprint(\"Number of classes (expected): \",NUM_CLASSES)\nprint(\"Number of classes (computed): \",\n      train_df[\"category_id\"].max() - train_df[\"category_id\"].min()+1)\nprint(\"\\nShape training dataframe    :\",train_df.shape)\n\n# Process metadata json for test images into a DataFrame\ntest_df = pd.DataFrame(meta_test[\"images\"])","9061c99f":"sample_submission = pd.read_csv(ROOT_DATA + \"sample_submission.csv\")","4f0124b1":"# Fit the label encoder instance\nlabel_encoder = preprocessing.LabelEncoder()\nlabel_encoder.fit(train_df[\"category_id\"])\n\n# Transform labels to normalized encoding\ntrain_df[\"category_id_le\"] = label_encoder.transform(train_df[\"category_id\"])\nclass_map = dict(sorted(train_df[[\"category_id_le\",\"category_id\"]].values.tolist()))\n\nprint(\"Labels converted to normalized encoding\")","a31abbb3":"class TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, df, labels, transform=None):\n        self.df = df\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.df[\"file_name\"].values[idx]\n        file_path = DATA_TRAIN + file_name\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels.values[idx]\n        \n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n        \n        return img, label","e1fb69d6":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.df[\"file_name\"].values[idx]\n        file_path = DATA_TEST + file_name\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n        \n        return img","31689852":"def get_transforms(*, data: str):\n    assert data in (\"train\",\"test\")\n    \n    if data == \"train\":\n        return albumentations.Compose([\n            albumentations.Resize(IMG_HEIGHT,IMG_WIDTH),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            #albumentations.HorizontalFlip(p=0.25),\n            #albumentations.VerticalFlip(p=0.25),\n            #albumentations.Rotate(limit=10,p=0.05),\n            ToTensorV2(),\n        ])\n\n    elif data == \"test\":\n        return albumentations.Compose([\n            albumentations.Resize(IMG_HEIGHT,IMG_WIDTH),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","3ae0707f":"train_dataset = TrainDataset(\n    train_df,train_df[\"category_id_le\"],\n    transform=get_transforms(data=\"train\"))\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,shuffle=SHUFFLE,num_workers=NUM_WORKERS)\nprint(\"Train data loader created\")","585c2f9f":"test_df.head(n=5)","2ad9bd40":"test_dataset = TestDataset(\n    test_df,\n    transform=get_transforms(data=\"test\"))\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=BATCH_EVAL,shuffle=False,num_workers=NUM_WORKERS)\nprint(\"Test data loader created\")","86f45803":"def train_model(model,dataloader,criterion,optimizer,num_epochs=1):\n    since = time.time()\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        model.train()\n        \n        running_loss = 0.0\n        running_corrects = 0\n        \n        #for inputs, labels in tqdm.tqdm(enumerate(dataloader)):\n        i = 0\n        len_dataset = len(dataloader.dataset)\n        for inputs, labels in dataloader:\n            since_tmp = time.time()\n            \n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            optimizer.zero_grad()\n            \n            with torch.set_grad_enabled(True):\n                outputs = model(inputs)\n                loss = criterion(outputs,labels)\n                \n                _,preds = torch.max(outputs,1)\n                \n                loss.backward()\n                optimizer.step()\n              \n            loss_tmp = loss.item()\n            pred_tmp = torch.sum(preds == labels.data)\n            \n            running_loss += loss_tmp * inputs.size(0)\n            running_corrects += pred_tmp\n            \n            time_elapsed_tmp = time.time() - since_tmp\n            \n            print('  loss: {:.04f} corr: {:d} ({:d}\/{:d}) completed in {:.0f}m {:.03f}s'.format(\n                loss_tmp, pred_tmp,(i+1)*BATCH_SIZE,len_dataset,\n                time_elapsed_tmp \/\/ 60, time_elapsed_tmp % 60))\n            \n            if ((time.time() - since) \/\/ 60 > ESTIMATED_MAX_TRAINING_TIME):\n                break;\n            \n            i += 1\n            \n        epoch_loss = running_loss \/ len_dataset\n        epoch_acc = running_corrects.double() \/ len_dataset\n        \n        torch.save(model.state_dict(),PATH_SAVE_MODEL)\n        print(PATH_SAVE_MODEL)\n        print('Loss: {:.04f} Acc: {:.04f}'.format(epoch_loss, epoch_acc))\n        \n        print()\n            \n        if ((time.time() - since) \/\/ 60 > ESTIMATED_MAX_TRAINING_TIME):\n            break;\n        \n    time_elapsed = time.time() - since\n    print('Loss: {:.04f} Acc: {:.04f}'.format(epoch_loss, epoch_acc))\n    print('Training complete in {:.0f}m {:.03f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    \n    #model.load_state_dict(best_model_wts)\n    return model","1a6c9201":"model = torchvision.models.resnet50(pretrained=PRE_TRAINED)\nmodel.avgpool = torch.nn.AdaptiveAvgPool2d(1)\nmodel.fc = torch.nn.Linear(model.fc.in_features,NUM_CLASSES)","13c85024":"model.load_state_dict(torch.load(\"..\/input\/herbarium-2021-rbs\/ResNet50_da_run-08.pth\"))","c2aac4f5":"model.to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE, amsgrad=False)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, 'min', factor=0.75, patience=5, verbose=True, eps=1e-6)\n\ncriterion = torch.nn.CrossEntropyLoss()","5ebc6a8c":"model = train_model(model,train_loader,criterion,optimizer,EPOCHS)","7ca024e9":"model.eval()\nmodel.to(DEVICE)\n\npredictions = np.zeros((len(test_dataset)))\n\nfor i,images in tqdm.tqdm(enumerate(test_loader)):\n    images = images.to(DEVICE)\n    with torch.no_grad():\n        y_predictions = model(images)\n\n    predictions[i*BATCH_EVAL:(i+1)*BATCH_EVAL] = y_predictions.to(DEVICE).cpu().numpy().argmax(1)","f2203953":"test_df[\"preds\"] = predictions.astype(int)\nsubmission = sample_submission.merge(\n    test_df.rename(columns = {\"id\":\"Id\"})[[\"Id\",\"preds\"]],on=\"Id\"\n).drop(columns=\"Predicted\")\nsubmission[\"Predicted\"] = submission[\"preds\"].map(class_map)\nsubmission = submission.drop(columns=\"preds\")\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head()","3651e906":"## Datasets\n\n### Training Dataset","29fda58a":"### Use existent trained model","bb191ad8":"### Process training and evaluation metadata\n\nMerge training images and annotations as a dataframe. The database-based joint operation merge is performed on the ids \"image_id\" (from images dataframe) and \"id\" (from annotations dataframe).\n\nThe problem with the validation data is that the `.json` does not provide the labels to check if the model is classifying correctly or not the validation images.","e9837467":"### Dataloaders","f63c5453":"### Construct model","a32efa63":"## Inference","6095954e":"im = PIL.Image.open(DATA_TRAIN + train_df.file_name[0])\nnewsize=(IMG_HEIGHT,IMG_WIDTH)\nim=im.resize(newsize)\nim=im.transpose(PIL.Image.FLIP_LEFT_RIGHT)\nim=im.transpose(PIL.Image.FLIP_TOP_BOTTOM)\nim=im.rotate(10)\nim.save(\"original-image_left_2right_top2bottom_rotate-10.png\")\nim","7797d2c4":"### Train","ca413cc7":"### Generic information","e0f2a99f":"### Test Dataset","ba58a5fb":"# Herbarium 2021 - Half-Earth Challenge - FGVC8 - ResNet\n\n**Author:** Ricardo B. Sousa (ORCID: [0000-0003-4537-5095](https:\/\/orcid.org\/0000-0003-4537-5095))\n\n**Affilitation:**: Faculty of Engineering of the University of Porto, INESC TEC - Instituto de Engenharia de Sistemas e Computadores, Tecnologia e Ci\u00eancia\n\n**Scope of this work:** Project of the curricular unit Computacional Vision of PDEEC@FEUP (Doctoral Program in Electrical and Computer Engineering)","f9dd00f8":"### Configuration","8884bb4c":"### Image Augmentation\n\n**Note:** uncomment the `albumentations.HorizontalFlip` + `albumentations.VerticalFlip` + `albumentations.Rotate` to have data augmentation with geometric transformations.","325410af":"## Create dataset","687559ea":"## Model","fe0386d1":"## Submit","6dd9e69b":"# Check One Sample\n\n**Note:** change the type of the next cell from Markdown to Code to have an example of the data augmentation techniquies used in this work.","8521090d":"## Prepare Submission","69ed3952":"### Random Seed\n\n**Note:** change the type of the next cell from Code to Markdown when using data augmentation (if you train 1 epoch per session with seed fixed, the same images would be augmented in all epochs; the purpose is augmenting online all the images if possible, and not only a subset and always the same subset).","eab16581":"### PC Configuration","5b7daec5":"## Label Encoder","08479577":"## Setup\n\n### Load libraries"}}