{"cell_type":{"2b053330":"code","abe2075a":"code","a7a09a36":"code","1616e62e":"code","efe56d47":"code","33ddf381":"code","dc28dfe1":"code","57c3b7f8":"code","858dfac0":"code","8a9b6dbc":"code","ab60369b":"code","10ce4bcb":"code","5b1f45e3":"code","b8bc45ea":"code","3c8dc0ad":"code","87c9fda0":"code","4e131410":"code","95431e34":"code","e1b58bdb":"code","c502df9c":"code","e23660c5":"code","8f3faa45":"code","57f5d223":"code","ad153b73":"code","60281229":"code","72ef55b6":"code","565d1c19":"code","849a3b0d":"code","28ba31fa":"code","3d56bf77":"code","7222f3c9":"code","d11ae485":"code","d522646d":"code","8dbe4e58":"code","7ef37653":"code","29f93268":"code","b3186419":"code","cee0d3e7":"code","5391da11":"code","94bb038c":"code","1284142d":"code","339454c6":"code","bb55171f":"code","d0538853":"code","94602306":"code","8d37a527":"code","e69d87da":"code","ea18bca1":"code","c1b49aa3":"code","a3c7843f":"code","c41c2352":"code","9fb3b9eb":"markdown","103c9367":"markdown","24ba526d":"markdown"},"source":{"2b053330":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","abe2075a":"a1 = pd.read_csv('..\/input\/articles1.csv',index_col=0)\na2 = pd.read_csv('..\/input\/articles2.csv',index_col=0)\na3 = pd.read_csv('..\/input\/articles3.csv',index_col=0)","a7a09a36":"df = pd.concat([a1,a2,a3])","1616e62e":"# save memory\ndel a1, a2, a3","efe56d47":"df.head()","33ddf381":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\ndf.publication.value_counts().plot(kind='bar')","dc28dfe1":"doc = df.loc[0,'content']","57c3b7f8":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en')","858dfac0":"doc = nlp(doc)","8a9b6dbc":"displacy.render(doc,style='ent',jupyter=True)","ab60369b":"from tqdm import tqdm, tqdm_notebook","10ce4bcb":"nlp = spacy.load('en',\n                 disable=['parser', \n                          'tagger',\n                          'textcat'])","5b1f45e3":"frames = []\nfor i in tqdm_notebook(range(1000)):\n    doc = df.loc[i,'content']\n    text_id = df.loc[i,'id']\n    doc = nlp(doc)\n    ents = [(e.text, e.start_char, e.end_char, e.label_) \n            for e in doc.ents \n            if len(e.text.strip(' -\u2014')) > 0]\n    frame = pd.DataFrame(ents)\n    frame['id'] = text_id\n    frames.append(frame)","b8bc45ea":"npf = pd.concat(frames)","3c8dc0ad":"npf.head()","87c9fda0":"npf.columns = ['Text','Start','Stop','Type','id']","4e131410":"plt.figure(figsize=(10,7))\nnpf.Type.value_counts().plot(kind='bar')","95431e34":"orgs = npf[npf.Type == 'ORG']","e1b58bdb":"plt.figure(figsize=(10,7))\norgs.Text.value_counts()[:15].plot(kind='bar')","c502df9c":"orgs.groupby(['id','Text']).size()","e23660c5":"nlp = spacy.load('en')\ndoc = 'Google to buy Apple'\ndoc = nlp(doc)\ndisplacy.render(doc,style='dep',jupyter=True, options={'distance':120})","8f3faa45":"for chunk in doc.noun_chunks:\n    print(chunk.text,'|' , chunk.root.text,'|', chunk.root.dep_,'|',\n          chunk.root.head.text)","57f5d223":"for token in doc:\n    print(token.text,'|', token.lemma_,'|', token.pos_,'|', token.tag_,'|', token.dep_,'|',\n          token.shape_,'|', token.is_alpha,'|', token.is_stop)","ad153b73":"import spacy\nimport random","60281229":"TRAIN_DATA = [\n    ('Who is Shaka Khan?', {\n        'entities': [(7, 17, 'PERSON')]\n    }),\n    ('I like London and Berlin.', {\n        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n    })\n]","72ef55b6":"nlp = spacy.load('en')","565d1c19":"# create the built-in pipeline components and add them to the pipeline\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe('ner')\n    nlp.add_pipe(ner, last=True)\n# otherwise, get it so we can add labels\nelse:\n    ner = nlp.get_pipe('ner')","849a3b0d":"# add labels\nfor _, annotations in TRAIN_DATA:\n    for ent in annotations.get('entities'):\n        ner.add_label(ent[2])","28ba31fa":"n_iter = 5\n\n# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    optimizer = nlp._optimizer \n    if not nlp._optimizer:\n        optimizer = nlp.begin_training()\n    \n    for itn in range(n_iter):\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        for text, annotations in TRAIN_DATA:\n            nlp.update(\n                [text],  # batch of texts\n                [annotations],  # batch of annotations\n                drop=0.5,  # dropout - make it harder to memorise data\n                sgd=optimizer,  # callable to update weights\n                losses=losses)\n        print(losses)","3d56bf77":"# test the trained model\nfor text, _ in TRAIN_DATA:\n    doc = nlp(text)\n    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n    print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n","7222f3c9":"import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en')\nmatcher = Matcher(nlp.vocab)","d11ae485":"pattern = [{'LOWER': 'hello'}, {'IS_PUNCT': True}, {'LOWER': 'world'}]\n\n\nmatcher.add('HelloWorld', None, pattern)\n\ndoc = nlp(u'Hello, world! Hello world!')\nmatches = matcher(doc)","d522646d":"matches","8dbe4e58":"doc[0:3]","7ef37653":"df.title = df.title.fillna('')","29f93268":"np.where(df.content.str.contains('iPhone'))","b3186419":"df.loc[14]","cee0d3e7":"import spacy\nfrom spacy.matcher import Matcher\n\nnlp = spacy.load('en')\nmatcher = Matcher(nlp.vocab)","5391da11":"# Get the hash of the word 'PRODUCT'. This is required to set an entity.\nPRODUCT = nlp.vocab.strings['PRODUCT']\n\ndef add_product_ent(matcher, doc, i, matches):\n    # Get the current match and create tuple of entity label, start and end.\n    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n    match_id, start, end = matches[i]\n    doc.ents += ((PRODUCT, start, end),)\n\npattern1 = [{'LOWER': 'iphone'}]\npattern2 = [{'ORTH': 'iPhone'}, {'IS_DIGIT': True}]\n\nmatcher.add('iPhone', add_product_ent,pattern1, pattern2)","94bb038c":"matches = matcher(doc)","1284142d":"displacy.render(doc,style='ent',jupyter=True)","339454c6":"def matcher_component(doc):\n    matches = matcher(doc)\n    return doc","bb55171f":"nlp.add_pipe(matcher_component,last=True)","d0538853":"doc = nlp(df.content.iloc[14])\n#matcher(doc)","94602306":"displacy.render(doc,style='ent',jupyter=True)","8d37a527":"import re","e69d87da":"pattern = 'NL[0-9]{9}B[0-9]{2}'","ea18bca1":"my_string = 'ING Bank N.V. BTW:NL003028112B01'","c1b49aa3":"re.findall(pattern,my_string)","a3c7843f":"match = re.search(pattern,my_string)","c41c2352":"match.span()","9fb3b9eb":"# Fine tuning SpaCy NER","103c9367":"# Regex","24ba526d":"# Rule based matching"}}