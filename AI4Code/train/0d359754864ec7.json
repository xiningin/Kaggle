{"cell_type":{"f3375174":"code","28247099":"code","1715d4cd":"code","80670d7e":"code","7e438955":"code","224130b8":"code","efd52a2c":"code","27f5c225":"code","5edd0099":"code","b8f5b23a":"code","7ea51fbc":"code","ec0c8dd8":"code","f72da6d4":"code","796e2d26":"code","2a8241ac":"code","9b6225cd":"markdown","30911472":"markdown","0c29859c":"markdown","8c1cdbf4":"markdown","c8794908":"markdown","4c3d9d2f":"markdown","a7adcb26":"markdown","1da6de64":"markdown","110da3bb":"markdown","8cff8d11":"markdown","7560788c":"markdown","aa915fc2":"markdown","53c8ab66":"markdown","2990e82a":"markdown","eabc013e":"markdown"},"source":{"f3375174":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ntrainFile = '\/kaggle\/input\/tweet-sentiment-extraction\/train.csv'\ntestFile = '\/kaggle\/input\/tweet-sentiment-extraction\/test.csv'\nsample = '\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv'\noutTrain = 'train_submission.csv'\noutFile = 'submission.csv'\n\ndf_train = pd.read_csv(trainFile, delimiter=',', dtype=str)\ndf_test = pd.read_csv(testFile, delimiter=',', dtype=str)\ndf_list = [df_train[['textID', 'text', 'sentiment']], df_test]\ndf_all = pd.concat(df_list, ignore_index=True)\nsentList = df_all['sentiment'].unique()","28247099":"# Model, Vectorization, Test, and Tune Flags\ntune = False\ntest = False\nmethod = False # True selects MultinomialNB, False selects SVM\n# end of training data \/ start of testing data\ntrainIdx = 27481\n\n# Tuning variables\nif method:\n    # Training Accuracy: 0.7071\n    # Testing Accuracy: 0.3212\n    # Jaccard Score (on Training): 0.5207\n    max_df = 1.0\n    min_df = 9  # maybe try higher?\n    max_feat = 2000\n    alpha = 6.0\nelse:\n    # Training Accuracy: 0.7459\n    # Testing Accuracy:  0.3356\n    # Jaccard Score (on Training): 0.5030\n    max_df = 1.0     # 0.1\n    min_df = 12      # 14\n    max_feat = 3000  # 2000\n    c = 0.7          # 0.7\n","1715d4cd":"def cleanText(text):\n    '''\n    Some useful hints of cleaning text up using regex:\n    https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions\n    '''\n    import re\n    import string\n    # convert text to lowercase and remove spaces at begining and end of string\n    text = str(text).strip().lower()\n    # remove links\n    text = re.sub(r'http[s]?:\/\/\\S+|www\\.\\S+', '', text)\n    # remove HTML tags\n    text = re.sub(r'<.*?>', \"\", text)\n    # remove the character [\\]\n    text = re.sub(r\"\\\\\", \"\", text)\n    # remove the character [']\n    text = re.sub(r\"\\'\", \"\", text)\n    # remove the character [\"]\n    text = re.sub(r\"\\\"\", \"\", text)\n    # remove text in square brackets []\n    text = re.sub(r'\\[.*?\\]', '', text)\n    # remove words that have numbers\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    # remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # remove punctuation characters\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    return text","80670d7e":"def vectorizeIt(df, textCol, trainIdx, maxFeat=2000, maxDF=1.0, minDF=0.0):\n    from sklearn.feature_extraction.text import CountVectorizer\n    import pandas as pd\n    import numpy as np\n    # this vectorizer will skip stop words\n    vectorizer = CountVectorizer(stop_words=\"english\", \\\n        preprocessor=cleanText, max_features=maxFeat, \\\n        max_df=maxDF, min_df=minDF)\n    # drop nan's\n    index = list(np.where(df[textCol].isnull())[0])\n    new_df = df[textCol].dropna(axis=0)\n    # fit the vectorizer on the text\n    vectorizer.fit(new_df)\n    # get the vocabulary\n    inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n    vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]\n    wc = vectorizer.fit_transform(new_df)\n    del new_df\n    idxTr = [i for i in index if i < trainIdx]\n    sz = trainIdx - len(idxTr)\n    xTrain = wc[:sz]\n    xTest = wc[sz:]\n    return index, vocabulary, xTrain, xTest","7e438955":"def sentArray(df, textCol):\n    import numpy as np\n    import pandas as pd\n    y = np.zeros(shape=(df[textCol].size), dtype=np.int)\n    words = df[textCol].unique()\n    for i in range(len(df)):\n       s = df[textCol].iloc[i]\n       t = np.where(words==s)\n       y[i] = t[0]\n    return y","224130b8":"def jaccard(str1, str2):\n    import re\n    # make non strings string (e.g., nan)\n    str1 = str(str1)\n    str2 = str(str2)\n    # remove the character [\"]\n    if len(str1) > 0:\n        str1 = re.sub(r\"\\\"\", \"\", str1)\n    if len(str2) > 0:\n        str2 = re.sub(r\"\\\"\", \"\", str2)\n    # https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/overview\/evaluation\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if len(c) > 0:\n        jacc = float(len(c)) \/ (len(a) + len(b) - len(c))\n    else:\n        jacc = 0.0\n    return jacc","efd52a2c":"def plotIt(x, y, name, title, xLabel, yLabel, label1, y2=None, label2=None):\n    import matplotlib.pyplot as plt\n    if len(y2) > 0:\n        plt.plot(x, y, 'bo-', label=label1)\n        plt.plot(x, y2, 'rs-', label=label2)\n    else:\n        plt.plot(x, y, 'bo-')\n    plt.xlabel(xLabel)\n    plt.ylabel(yLabel)\n    plt.title(title)\n    plt.legend()\n    plt.savefig(name + '.png', format='png')\n    plt.close()","27f5c225":"def yPrep(index, trainIdx, train, test):\n    import numpy as np\n    # Convert sentiment values to y array of integers\n    idxTr = [i for i in index if i < trainIdx]\n    yTr = sentArray(train, 'sentiment')\n    yTrain = np.delete(yTr, idxTr)\n    idxTe = [i - trainIdx for i in index if i > trainIdx]\n    yTe = sentArray(test, 'sentiment')\n    yTest = np.delete(yTe, idxTe)\n    trainIdx = trainIdx - len(idxTr)\n    return trainIdx, yTrain, yTest, idxTr, idxTe","5edd0099":"def multiNB(alpha, xTrain, xTest, yTrain, yTest):\n    from sklearn.naive_bayes import MultinomialNB\n    '''\n    https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n    alpha:  float, default=1.0\n            Additive (Laplace\/Lidstone) smoothing parameter (0 for no smoothing).\n    '''\n    # Send through sklearn mulinomial Naive Bayes classifier\n    clf = MultinomialNB(alpha=alpha)\n    clf.fit(xTrain,yTrain)\n    accTrain = clf.score(xTrain, yTrain)\n    accTest = clf.score(xTest, yTest)\n    return clf, accTrain, accTest","b8f5b23a":"def tuneNB(all, train, test, trainIdx, max_feat, max_df, min_df, alpha):\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    print(\"Tune MultinomialNB...\")\n    # set the defaults\n    cvSz = 5 # iterations of cross validation\n    # flags for individual tuning\n    tune_max_df = True\n    tune_min_df = True\n    tune_max_feat = True\n    tune_alpha = True\n    # max_df\n    if tune_max_df:\n        max_df = np.arange(start=1.0, stop=0.0, step=-0.1)\n        accTrainAll = np.zeros(shape=(cvSz, len(max_df)))\n        accValidAll = np.zeros(shape=(cvSz, len(max_df)))\n        for j in range(len(max_df)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df[j], minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=0.33, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = multiNB(alpha, xTr, \\\n                    xVld, yTr, yVld)\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'NB_max_df_v_acc'\n        title = 'MultinomialNB: max_df vs. Accuracy'\n        xLabel = 'max_df'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(max_df, accTrain, name, title, xLabel, yLabel, label1, accValid,\\\n            label2)\n        max_df = max_df[np.argmax(accValid)]\n        print(\"max_df: \" + \"{:.3f}\".format(max_df))\n    # min_df\n    if tune_min_df:\n        min_df = np.arange(start=1, stop=20, step=1)\n        accTrainAll = np.zeros(shape=(cvSz, len(min_df)))\n        accValidAll = np.zeros(shape=(cvSz, len(min_df)))\n        for j in range(len(min_df)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df, minDF=min_df[j])\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=0.33, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i, j], accValidAll[i,j] = multiNB(alpha, xTr, \\\n                    xVld, yTr, yVld)\n        # get mean score\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'NB_min_df_v_acc'\n        title = 'MultinomialNB: min_df vs. Accuracy'\n        xLabel = 'min_df'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(min_df, accTrain, name, title, xLabel, yLabel, label1, accValid,\\\n            label2)\n        min_df = min_df[np.argmax(accValid)]\n        print(\"min_df: \" + \"{:.3f}\".format(min_df))\n    # max_feat\n    if tune_max_feat:\n        max_feat = np.arange(start=1000, stop=26000, step=1000)\n        accTrainAll = np.zeros(shape=(cvSz, len(max_feat)))\n        accValidAll = np.zeros(shape=(cvSz, len(max_feat)))\n        for j in range(len(max_feat)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat[j], maxDF=max_df, minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=0.33, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = multiNB(alpha, xTr, \\\n                    xVld, yTr, yVld)\n        # get mean score\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'NB_feat_v_acc'\n        title = 'MultinomialNB: max_feat vs. Accuracy'\n        xLabel = 'max_feat'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(max_feat, accTrain, name, title, xLabel, yLabel, label1, \\\n            accValid, label2)\n        max_feat = max_feat[np.argmax(accValid)]\n        print(\"max_feat: \" + \"{:.2f}\".format(max_feat))\n    # alpha\n    if tune_alpha:\n        alpha = np.arange(start=1.0, stop=21.0, step=1.0)\n        accTrainAll = np.zeros(shape=(cvSz, len(alpha)))\n        accValidAll = np.zeros(shape=(cvSz, len(alpha)))\n        for j in range(len(alpha)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df, minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=0.33, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = multiNB(alpha[j], \\\n                    xTr, xVld, yTr, yVld)\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'NB_alpha_v_acc'\n        title = 'MultinomialNB: alpha vs. Accuracy'\n        xLabel = 'alpha'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(alpha, accTrain, name, title, xLabel, yLabel, label1, accValid,\\\n            label2)\n        alpha = alpha[np.argmax(accValid)]\n        print(\"alpha: \" + \"{:.2f}\".format(alpha))","7ea51fbc":"def buildSelTextNB(df, test, idxList, pred, bow, featLogProb, sentList):\n    import pandas as pd\n    '''\n    neutral == 0\n    negative == 1\n    positive == 2\n    '''\n    print(\"Build MultinomialNB output...\")\n    # create a return DataFrame\n    if test:\n        retDF = pd.DataFrame(columns=['textID', 'selected_text', 'sentiment'])\n        retDF['textID'] = df['textID']\n    else:\n        retDF = pd.DataFrame(columns=['textID', 'selected_text'])\n        retDF['textID'] = df['textID']\n    j = 0\n    for i in range(len(retDF)):\n        # add nan's back in as neutral\n        if i in idxList:\n            aword = str(df['text'].iloc[i])\n            if aword == 'nan':\n                retDF['selected_text'].iloc[i] = \\\n                    '\"' + '\"'\n            else:\n                retDF['selected_text'].iloc[i] = \\\n                    '\"' + aword + '\"'\n            if test: retDF['sentiment'].iloc[i] = 'neutral'\n        elif (pred[j] == 0):\n            retDF['selected_text'].iloc[i] = \\\n                '\"' + df['text'].iloc[i] + '\"'\n            if test: retDF['sentiment'].iloc[i] = 'neutral'\n            j = j + 1 # advance the prediction index\n        else: # Else add selected_text back in based on probability of feature\n            outstring = \"\"\n            idxA = pred[j]\n            words = df['text'].iloc[i].split()\n            for word in words:\n                wordCheck = cleanText(word)\n                if wordCheck in bow:\n                    probWord = (featLogProb)[idxA, bow.index(wordCheck)]\n                    probNeu = (featLogProb)[0, bow.index(wordCheck)]\n                    probNeg = (featLogProb)[1, bow.index(wordCheck)]\n                    probPos = (featLogProb)[2, bow.index(wordCheck)]\n                    # check the word probabilities\n                    if (((idxA == 1) and (probWord > probPos)) or \\\n                        ((idxA == 2) and (probWord > probNeg))):\n                        if len(outstring) == 0:\n                            outstring = word\n                        else:\n                            outstring = outstring + \" \" + word\n            outstring = '\"' + outstring + '\"'\n            retDF['selected_text'].iloc[i] = outstring\n            if test: retDF['sentiment'].iloc[i] = sentList[idxA]\n            j = j + 1 # advance the prediction index\n    return retDF","ec0c8dd8":"def svmClass(xTrain, xTest, yTrain, yTest, c=1.0):\n    from sklearn import svm\n    '''\n    https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n    C:      float, default=1.0\n            Regularization parameter. The strength of the regularization\n            is inversely proportional to C. Must be strictly positive.\n            The penalty is a squared l2 penalty.\n    kernel: {\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\n            Specifies the kernel type to be used in the algorithm. It must be \n            one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 or a \n            callable. If none is given, \u2018rbf\u2019 will be used. If a callable is \n            given it is used to pre-compute the kernel matrix from data \n            matrices; that matrix should be an array of \n            shape (n_samples, n_samples).\n    gamma:  {\u2018scale\u2019, \u2018auto\u2019} or float, default=\u2019scale\u2019\n            Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n            -   if gamma='scale' (default) is passed then it uses \n                1 \/ (n_features * X.var()) as value of gamma,\n            -   if \u2018auto\u2019, uses 1 \/ n_features.\n    '''\n    # not sure how to get feature liklihood out of rbf\n    # clf = svm.SVC(C=c, kernel='rbf', gamma='scale')\n    clf = svm.SVC(C=c, kernel='linear')\n    clf.fit(xTrain, yTrain)\n    accTrain = clf.score(xTrain, yTrain)\n    accTest = clf.score(xTest, yTest)\n    return clf, accTrain, accTest","f72da6d4":"def tuneSVM(xTrain, xTest, yTrain, yTest):\n    import numpy as np\n    from math import floor\n    from sklearn.model_selection import train_test_split\n    print('Tune SVM...')\n    # set the defaults\n    vldSz = 0.129 # percent of training data to make validation set\n    cvSz = floor(1.0\/vldSz) # iterations of cross validation\n    # flags for individual tuning\n    tune_max_df = True\n    tune_min_df = True\n    tune_max_feat = True\n    tune_c = True\n    # max_df\n    if tune_max_df:\n        max_df = np.arange(start=1.0, stop=0.0, step=-0.1)\n        accTrainAll = np.zeros(shape=(cvSz, len(max_df)))\n        accValidAll = np.zeros(shape=(cvSz, len(max_df)))\n        for j in range(len(max_df)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df[j], minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=vldSz, random_state=i, shuffle=True)\n                # Send through sklearn SVM classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = svmClass(xTr, xVld, \\\n                    yTr, yVld, c)\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'SVM_max_df_v_acc'\n        title = 'SVM: max_df vs. Accuracy'\n        xLabel = 'max_df'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(max_df, accTrain, name, title, xLabel, yLabel, label1, accValid, label2)\n        max_df = max_df[np.argmax(accValid)]\n        print(\"max_df: \" + \"{:.3f}\".format(max_df))\n    # min_df\n    if tune_min_df:\n        min_df = np.arange(start=1, stop=20, step=1)\n        accTrainAll = np.zeros(shape=(cvSz, len(min_df)))\n        accValidAll = np.zeros(shape=(cvSz, len(min_df)))\n        for j in range(len(min_df)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df, minDF=min_df[j])\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=vldSz, random_state=i, shuffle=True)\n                # Send through sklearn SVM classifier\n                clf, accTrainAll[i, j], accValidAll[i,j] = svmClass(xTr, xVld, \\\n                    yTr, yVld, c)\n        # get mean score\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'SVM_min_df_v_acc'\n        title = 'SVM: min_df vs. Accuracy'\n        xLabel = 'min_df'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(min_df, accTrain, name, title, xLabel, yLabel, label1, accValid, label2)\n        min_df = min_df[np.argmax(accValid)]\n        print(\"min_df: \" + \"{:.3f}\".format(min_df))\n    # max_feat\n    if tune_max_feat:\n        max_feat = np.arange(start=1000, stop=26000, step=1000)\n        accTrainAll = np.zeros(shape=(cvSz, len(max_feat)))\n        accValidAll = np.zeros(shape=(cvSz, len(max_feat)))\n        for j in range(len(max_feat)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat[j], maxDF=max_df, minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=vldSz, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = svmClass(xTr, xVld, \\\n                    yTr, yVld, c)\n        # get mean score\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'SVM_feat_v_acc'\n        title = 'SVM: max_feat vs. Accuracy'\n        xLabel = 'max_feat'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(max_feat, accTrain, name, title, xLabel, yLabel, label1, accValid, label2)\n        max_feat = max_feat[np.argmax(accValid)]\n        print(\"max_feat: \" + \"{:.2f}\".format(max_feat))\n    # c\n    if tune_c:\n        # c = np.logspace(start=-3, stop=1, num=5, base=10.0, dtype=np.float)\n        c = np.arange(start=0.1, stop=1.1, step=0.1)\n        accTrainAll = np.zeros(shape=(cvSz, len(c)))\n        accValidAll = np.zeros(shape=(cvSz, len(c)))\n        for j in range(len(c)):\n            for i in range(cvSz):\n                tIdx = trainIdx\n                # Create bag of words and word count\n                index, bow, xTrain, xTest = vectorizeIt(all, 'text', trainIdx, \\\n                    maxFeat=max_feat, maxDF=max_df, minDF=min_df)\n                # Convert sentiment values to y array of integers\n                tIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n                    tIdx, train, test)\n                # create cross validation data\n                xTr, xVld, yTr, yVld = train_test_split(xTrain, yTrain, \\\n                    test_size=vldSz, random_state=i, shuffle=True)\n                # Send through sklearn mulinomial Naive Bayes classifier\n                clf, accTrainAll[i,j], accValidAll[i,j] = svmClass(xTr, xVld, \\\n                    yTr, yVld, c[j])\n        accTrain = accTrainAll.mean(axis=0)\n        accValid = accValidAll.mean(axis=0)\n        # plot the results\n        name = 'SVM_c_v_acc'\n        title = 'SVM: c vs. Accuracy'\n        xLabel = 'c'\n        yLabel = 'Accuracy'\n        label1 = 'Train'\n        label2 = 'Valid'\n        plotIt(c, accTrain, name, title, xLabel, yLabel, label1, accValid, label2)\n        c = c[np.argmax(accValid)]\n        print(\"c: \" + \"{:.2f}\".format(c))","796e2d26":"def buildSelTextSVM(df, test, idxList, pred, bow, sentList, y, w, b, x):\n    import pandas as pd\n    import numpy as np\n    '''\n    neutral == 0\n    negative == 1\n    positive == 2\n    '''\n    print('Build SVM output...')\n    # ||w||\n    w_norm = np.zeros(shape=(w.shape[0]))\n    for i in range(w.shape[0]):\n        w_norm[i] = np.linalg.norm(w[i])\n    dist = np.zeros(shape=(x.shape[0]), dtype=np.float64)\n    vect = np.zeros(shape=(x.shape), dtype=np.float64)\n    for i in range(len(dist)):\n        idx = pred[i]\n        # Geometric Distance: d = w_t x + b \/ ||w||\n        dist[i] = (y[i,idx] * (np.dot(w[idx].T, x[i]) + b[idx])) \/ w_norm[idx]\n        # vector for each feature?\n        vect[i] = dist[i] * (w[pred[i]] \/ w_norm[idx])\n    # create a return DataFrame\n    if test:\n        retDF = pd.DataFrame(columns=['textID', 'selected_text', 'sentiment'])\n        retDF['textID'] = df['textID']\n    else:\n        retDF = pd.DataFrame(columns=['textID', 'selected_text'])\n        retDF['textID'] = df['textID']\n    j = 0\n    for i in range(len(retDF)):\n        # add nan's back in as neutral\n        if i in idxList:\n            aword = str(df['text'].iloc[i])\n            if aword == 'nan':\n                retDF['selected_text'].iloc[i] = \"\"\n            else:\n                retDF['selected_text'].iloc[i] = aword\n            if test: retDF['sentiment'].iloc[i] = 'neutral'\n        elif (pred[j] == 0):\n            retDF['selected_text'].iloc[i] = df['text'].iloc[i]\n            if test: retDF['sentiment'].iloc[i] = 'neutral'\n            j = j + 1 # advance the prediction index\n        else: # Else add selected_text back in based on distance from hyperplane\n            outstring = \"\"\n            idxA = pred[j]\n            words = df['text'].iloc[i].split()\n            for word in words:\n                wordCheck = cleanText(word)\n                if wordCheck in bow:\n                    iBow = bow.index(wordCheck)\n                    if (vect[j, iBow] >= 0.0):                    \n                        if len(outstring) == 0:\n                            outstring = word\n                        else:\n                            outstring = outstring + \" \" + word\n            retDF['selected_text'].iloc[i] = outstring\n            if test: retDF['sentiment'].iloc[i] = sentList[idxA]\n            j = j + 1 # advance the prediction index\n    return retDF","2a8241ac":"### Method 1: Multinomial Naive Bayes\nif method:\n    print('Multinomial Naive Bayes Classifier')\n    # tune hyperparameters\n    if tune:\n        tuneNB(df_all, df_train, df_test, trainIdx, max_feat, max_df, \\\n            min_df, alpha)\n    # Perform calculation and output to file\n    else:\n        # Create bag of words and word count\n        index, bow, xTrain, xTest = vectorizeIt(df_all, 'text', \\\n            trainIdx, maxFeat=max_feat, maxDF=max_df, minDF=min_df)\n        # Convert sentiment values to y array of integers\n        trainIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n            trainIdx, df_train, df_test)\n        # Send through sklearn mulinomial Naive Bayes classifier\n        clf, accTrain, accTest = multiNB(alpha, xTrain, xTest, yTrain, yTest)\n        predTrain = clf.predict(xTrain)\n        predTest = clf.predict(xTest)\n        print(\"Training Accuracy: \" + \"{:.4f}\".format(accTrain))\n        print(\"Testing Accuracy: \" + \"{:.4f}\".format(accTest))\n        # Determine selected_text\n        testRet_df = buildSelTextNB(df_test, test, idxTe, predTest, bow, \\\n            clf.feature_log_prob_, sentList)\n        if test:\n            trainRet_df = buildSelTextNB(df_train, test, idxTr, predTrain, \\\n                bow, clf.feature_log_prob_, sentList)\n        # Write sample_submission\n        testRet_df.to_csv(outFile, index=False)\n        if test:\n            trainRet_df.to_csv(outTrain, index=False)\n            sz = len(df_train)\n            scoreArr = np.zeros(shape=(sz))\n            # calculate Jaccard Score of training data\n            for i in range(len(df_train)):\n                scoreArr[i] = jaccard(df_train['selected_text'].iloc[i], \\\n                    trainRet_df['selected_text'].iloc[i])\n            print('Jaccard Score: ' + \"{:.4f}\".format(np.mean(scoreArr)))\n### Method 2: SVM\nelse:\n    print('Support Vector Machine Classifier')\n    print(\"max_df:   \" + \"{:3.5f}\".format(max_df))\n    print(\"min_df:   \" + \"{:8d}\".format(min_df))\n    print(\"max_feat: \" + \"{:7.1f}\".format(max_feat))\n    print(\"c:        \" + \"{:3.5f}\".format(c))\n    if tune:\n        tuneSVM(df_all, df_train, df_test, trainIdx, max_feat, max_df, \\\n            min_df, c)\n    else:\n        index, bow, xTrain, xTest = vectorizeIt(df_all, 'text', \\\n            trainIdx, maxFeat=max_feat, maxDF=max_df, minDF=min_df)\n        # Convert sentiment values to y array of integers\n        trainIdx, yTrain, yTest, idxTr, idxTe = yPrep(index, \\\n            trainIdx, df_train, df_test)\n        # Send through sklearn SVM classifier\n        clf, accTrain, accTest = svmClass(xTrain, xTest, yTrain, yTest, c)\n        predTrain = clf.predict(xTrain)\n        predTest = clf.predict(xTest)\n        print(\"Training Accuracy: \" + \"{:.4f}\".format(accTrain))\n        print(\"Testing Accuracy: \" + \"{:.4f}\".format(accTest))\n        coef = clf.coef_.toarray()\n        b = clf.intercept_\n        # Determine selected_text\n        y = clf.decision_function(xTest)\n        testRet_df = buildSelTextSVM(df_test, test, idxTe, predTest, bow, \\\n            sentList, y, coef, b, xTest.toarray())\n        if test:\n            y = clf.decision_function(xTrain)\n            trainRet_df = buildSelTextSVM(df_train, test, idxTr, predTrain, \\\n                bow, sentList, y, coef, b, xTrain.toarray())\n        # Write sample_submission\n        testRet_df.to_csv(outFile, index=False)\n        if test:\n            trainRet_df.to_csv(outTrain, index=False)\n            sz = len(df_train)\n            scoreArr = np.zeros(shape=(sz))\n            # calculate Jaccard Score of training data\n            for i in range(len(df_train)):\n                scoreArr[i] = jaccard(df_train['selected_text'].iloc[i], \\\n                    trainRet_df['selected_text'].iloc[i])\n            print('Jaccard Score: ' + \"{:.4f}\".format(np.mean(scoreArr)))","9b6225cd":"## Build Output Dataframe\nThis function builds the DataFrame which is used to write the submission.csv and train_submission.csv.  It also still needs a lot of work in the logic that selects the positive and negative sentiment words but it seems to be doing better than the Naive Bayes output code.","30911472":"# First Kaggle Competition\nThis is my first Kaggle competition.  In lieu of a fianl examination for our Oregon State University machine learning course (CS-434), we have been given the opportunity of participating in this competition.  This is also my first Notebook so please feel free to give me pointers.\n\nFor this competition I've tried to employ two methods utilizing scikit-learn (sklearn) Multinomial Naive Bayes and Support Vector Machine (SVM).  I will try to reproduce the code in this notebook.\n\n# Acknowledgements\n* OSU CS-434 Instructor and TAs, a lot of this code comes from sample code provided in class.\n* scikit-learn, https:\/\/scikit-learn.org\/stable\/about.html#citing-scikit-learn\n* Raenish David, https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions\n* Article: https:\/\/medium.com\/@bedigunjit\/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34, written by Gunjit Bedi, entitled \"A guide to Text Classification (NLP) using SVM and Naive Bayes with Python\" on November 9, 2018\n\n# Input","0c29859c":"## Calculate the Jaccard Score\nI used the training data to try and tune the vectorization parameters to improve the Jaccard score.  I made some assumptions from the code posted (https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/overview\/evaluation) in that occasionally the intersection of the ground truth and predicted value was zero, so I added a check for that.  Also this strips away the quotations from the selected text DataFrame values.","8c1cdbf4":"## Generate Output DataFrame\nThis function generates the output DataFrame used to write the submission.csv and the train_submission.csv.  This function also needs a lot of work, specifically in the logic that selects words for positive and negative sentiment.","c8794908":"## Plots for Tuning\nThis is just a simple ploting function for tuning.","4c3d9d2f":"## Eliminate Null Indicies from y Data\nWhen we remove the null value (index 314) from the training data it needs to be removed from the sentiment array too.  This also assumes that it's possible the testing data may have null's too, so this tries to account for that.","a7adcb26":"# Support Vector Machine (SVM)\n## Helper Function\nThis function calls the sklearn SVM classifier.  I initially tried to use the 'rbf' kernel, which had excellent training accuracy results, but I did not know how to pull the feature weights to support word selection.  So I switched to a linear model which provides (coef_ and decision_function) to calculate the distance of the point from the decision boundary.","1da6de64":"## Tuning Function\nThis function is similar to the Multinomial Naive Bayes tuning execept alpha is replaced by c.","110da3bb":"# Multinomial Naive Bayes\n## Helper Function\nThis function calls the sklearn multinomial Naive Bayes classifier.  It returns the accuracy of the training and testing along with the object (clf) itself for use of other functions and variables (e.g., feature_log_prob_).","8cff8d11":"# Utility Functions\nThis is just a collection of functions that are used by the main program and both methods.\n## Text Cleanup\nThis function mainly uses regex to remove characters that are not always helpful for determining sentiment.  Unfortunately, it also removes some punctuation (e.g., \\*\\*\\*\\*\\*) which may be helful for determining sentiment.  So lots of room for improvement.","7560788c":"# Constants","aa915fc2":"## Generate the y values\nThis function creates an array of integer values coresponding to the sentiment values (0 neutral, 1 negative, 2 positive).","53c8ab66":"## Tuning Function\nThis function was used to attempt to tune the hyperparameters, mainly alpha, but it was also used to get some of the vectorization numbers as well.  This function employs sklearn model selection to generate validation data.","2990e82a":"## Vectorization\nThis function converts the input DataFrame to a bag of words, removes nulls and stores the index of nulls, and vectorizes the text. ","eabc013e":"# Main Driver\nThis calls either the multinomialNB or the SVM classifier based on the method flag.  If the tuning flag is set it will only perform the tuning functions for the method selected.  The test flag is for trying to improve the selected text output."}}