{"cell_type":{"78cfca5d":"code","ce500da1":"code","461dacec":"code","7018ffcd":"code","12a5425c":"code","84cb9607":"code","d52d6ddc":"code","7d8b855d":"code","6811a52f":"code","cc0f4b67":"code","3d333edf":"code","a09d8b77":"code","fe305a4c":"code","d54205a1":"code","2fd4f28b":"code","1d088140":"code","878dfe50":"code","cf68ebfc":"code","60978808":"code","272d2dcb":"code","58b193cc":"code","9a7f6a26":"code","0fd287c1":"code","b96f342c":"code","f7ba2420":"code","7f1f0fe2":"code","f273b445":"code","5b795b1e":"code","95255964":"code","453e1a7a":"code","101f4a11":"code","59f5397e":"code","f27729b5":"code","764a33f5":"code","4d178227":"code","2a031f60":"code","752b7532":"code","4e8fd648":"code","2d312425":"code","ea9521a1":"code","c5951d7c":"code","19c93dda":"code","62fd2ae8":"code","f0c2a9ae":"code","e1dafd9f":"code","df828225":"code","9df8291a":"code","0b99e16d":"code","22f7eba9":"code","2a30768d":"code","9aaeab4f":"code","8ea197d0":"code","7e174f51":"code","7f412939":"code","618e39bc":"code","bbc073d1":"code","38aa538e":"code","5f0325b7":"code","72a02f27":"markdown","0c693f9f":"markdown","6ae71412":"markdown","77b407c1":"markdown","26ff3a75":"markdown","def9e621":"markdown","c457b057":"markdown","ac041203":"markdown","6c5ffe41":"markdown","a808262b":"markdown","2cfe84a7":"markdown"},"source":{"78cfca5d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport pandas as pd\nimport json\nimport requests\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 14\n\nimport datetime \nfrom pandas.plotting import register_matplotlib_converters\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 14\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport datetime as dt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn.model_selection import TimeSeriesSplit\nplt.style.use('ggplot')\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# retrieve json file for Live Traffic\n#url = \"https:\/\/api.midway.tomtom.com\/ranking\/liveHourly\/POL_wroclaw\"\nlive = \"https:\/\/api.midway.tomtom.com\/ranking\/live\/POL_wroclaw\"\nwro_live = requests.get(live)\nwro_json = wro_live.json()\n\n\n# retrieve json file for Weather\nweather = \"https:\/\/api.weather.midway.tomtom.com\/weather\/live\/POL_wroclaw\"\n\nweather_req = requests.get(weather)\nweather_json = weather_req.json()\n\npd.set_option(\"display.max_rows\", False)\n","ce500da1":"#wro_json","461dacec":"#wro_json[\"data\"][0][\"TrafficIndexLive\"]","7018ffcd":"# create empty lists of append data for Live Traffic\njams_delay = []\ntraffic_index_live = []\nupdate_time = []\njams_length = []\njams_count = []\ntraffic_index_week = []\ntime = []\n\n\ncount = len(wro_json[\"data\"])-1\n\n# append each item in the json file to the empty lists\ni=0\nwhile i<=count:\n    jams_delay.append(wro_json[\"data\"][i][\"JamsDelay\"],)\n    traffic_index_live.append(wro_json[\"data\"][i][\"TrafficIndexLive\"],)\n    update_time.append(wro_json[\"data\"][i][\"UpdateTime\"])\n    jams_length.append(wro_json[\"data\"][i][\"JamsLength\"],)\n    jams_count.append(wro_json[\"data\"][i][\"JamsCount\"],)\n    #traffic_index_week.append(wro_json[\"data\"][i][\"TrafficIndexWeekAgo\"],)\n    #traffic_index_week.append(wro_json[\"data\"][i][\"UpdateTimeWeekAgo\"],)\n    time.append(wro_json[\"data\"][i][\"UpdateTime\"])\n    i+=1\n    \n# create dataframe with the traffic data \n#df = pd.DataFrame({\"Live Traffic\":live_traffic,\"Jams Delay\":jams_delay,\"Jams Length\":jams_length,\"Jams Count\":jams_count,\"Traffic Index Week Ago\":traffic_index_week_ago,\"Update Time Week Ago\":update_time_week_ago}, index=time)\ndf = pd.DataFrame({\"Delay\":jams_delay,\"Traffic\":traffic_index_live  ,\"Date\":update_time,\"Length\":jams_length,\"Count\":jams_count}, index=time)\n#df.update_time = pd.to_datetime(df.update_time, unit=\"ms\")\ndf.index = pd.to_datetime(df.index, unit=\"ms\")\ndf['Date'] = pd.to_datetime(df['Date'], unit=\"ms\")\ndf.index.name = \"Time\"\n#df.head()","12a5425c":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.2)","84cb9607":"#train_df","d52d6ddc":"#test_df","7d8b855d":"corr =  df.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corr)","6811a52f":"from pandas.plotting import register_matplotlib_converters\n\ndf.index = df['Date'] # indexing the Datetime to get the time period on the x-axis. \n#df=df.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Traffic Jams in Wroclaw') \nplt.title('Jams Count') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Number of Jams\") \nplt.legend(loc='best')","cc0f4b67":"df.index = df['Date'] # indexing the Datetime to get the time period on the x-axis. \n#df=df.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \n\nplt.figure(figsize=(25,8))\nsns.barplot(x=df.index.date, y=df[\"Count\"])\nplt.title(\"Traffic Jams in Wroclaw\")\nplt.xticks(rotation=90)\n","3d333edf":"train = train_df\ntest = test_df","a09d8b77":"# Making copy of dataset\n\ntrain_original=train.copy() \ntest_original=test.copy()","fe305a4c":"#print(train_original.head())\n#print (test_original.head())","d54205a1":"#train.info(), test.info()","2fd4f28b":"#df = df.rename(columns={'Jams Delay': 'Jams_Delay', 'Live Traffic': 'Live_Traffic','Update Time': 'Update_Time', 'Jams Length': 'Jams_Length', 'Jams Count': 'Jams_Count'})\n#df.head()","1d088140":"import datetime \n\ntrain['Date'] = pd.to_datetime(train.Date,format='%d-%m-%Y %H:%M',infer_datetime_format=True) \ntest['Date'] = pd.to_datetime(test.Date,format='%d-%m-%Y %H:%M', infer_datetime_format=True) \ntest_original['Date'] = pd.to_datetime(test_original.Date,format='%d-%m-%Y %H:%M', infer_datetime_format=True) \ntrain_original['Date'] = pd.to_datetime(train_original.Date,format='%d %m %Y %H:%M',  infer_datetime_format=True)","878dfe50":"#train_original.head()","cf68ebfc":"for i in (train, test, test_original, train_original):\n    i['year']=i.Date.dt.year \n    i['month']=i.Date.dt.month \n    i['day']=i.Date.dt.day\n    i['Hour']=i.Date.dt.hour ","60978808":"train['dow']=train['Date'].dt.dayofweek \ntemp = train['Date']","272d2dcb":"def applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0 \ntemp2 = train['Date'].apply(applyer) \ntrain['weekend']=temp2","58b193cc":"from pandas.plotting import register_matplotlib_converters\n\ntrain.index = train['Date'] # indexing the Datetime to get the time period on the x-axis. \n#df=train.drop('ID',1)           # drop ID variable to get only the Datetime on x-axis. \nts = df['Count'] \nplt.figure(figsize=(16,8)) \nplt.plot(ts, label='Jams Count') \nplt.title('Time Series') \nplt.xlabel(\"Time(year-month)\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best')","9a7f6a26":"train.groupby('year')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Yearly Jams Count')","0fd287c1":"train.groupby('month')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7), title='Monthly Jams Count')","b96f342c":"#=train.groupby(['year', 'month'])['Count'].mean() \n#temp.plot(figsize=(15,5), title= 'Jams Count(Year& Month)', fontsize=14)","f7ba2420":"train.groupby('day')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Daily Jams in Wroclaw')","7f1f0fe2":"train.groupby('Hour')['Count'].mean().plot.bar(color='m', figsize=(10,7),fontsize=14,title='Hourly Traffic Jams in Wroclaw')","f273b445":"train.groupby('weekend')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7),title='Weekend Traffic Jams Analysis')","5b795b1e":"train.groupby('dow')['Count'].mean().plot.bar(fontsize=14,figsize=(10,7), title='Traffic Jams based on the day of week')","95255964":"train[\"WeekOfYear\"]=train.index.weekofyear\n\nweek_num=[]\ncount_index = []\nlength_index = []\ndelay_index = []\n\nw=1\nfor i in list(train[\"WeekOfYear\"].unique()):\n    count_index.append(train[train[\"WeekOfYear\"]==i][\"Count\"].iloc[-1])\n    length_index.append(train[train[\"WeekOfYear\"]==i][\"Length\"].iloc[-1])\n    \n    week_num.append(w)\n    w=w+1\n\nplt.figure(figsize=(8,5))\nplt.plot(week_num,count_index,linewidth=3)\nplt.plot(week_num,length_index,linewidth=3)\n\nplt.ylabel(\"Number of Traffic Jams\")\nplt.xlabel(\"Jams Length\")\nplt.title(\"Correlation of Traffic Jams and Lengths along the week.\")\nplt.xlabel","453e1a7a":"week_num=[]\ncount_index = []\nlength_index = []\ndelay_index = []\n\nw=1\nfor i in list(train[\"WeekOfYear\"].unique()):\n    count_index.append(train[train[\"WeekOfYear\"]==i][\"Count\"].iloc[-1])\n    length_index.append(train[train[\"WeekOfYear\"]==i][\"Length\"].iloc[-1])\n    \n    week_num.append(w)\n    w=w+1\n\n\n\n\nplt.figure(figsize=(25,8))\nplt.plot(df[\"Count\"],marker=\"o\",label=\"Number of Jams\")\nplt.plot(df[\"Length\"],marker=\"*\",label=\"Length\")\nplt.plot(df[\"Delay\"],marker=\"^\",label=\"Delays\")\nplt.ylabel(\"Number of Traffic Jams\")\nplt.xlabel(\"Dates\")\nplt.xticks(rotation=90)\nplt.title(\"Progress of traffic jams over time\")\nplt.legend()\nplt.savefig('004br.png')","101f4a11":"#train=train.drop('ID',1)","59f5397e":"train.Timestamp = pd.to_datetime(train.Date,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Hourly time series \nhourly = train.resample('H').mean() \n# Converting to daily mean \ndaily = train.resample('D').mean() \n# Converting to weekly mean \nweekly = train.resample('W').mean() \n# Converting to monthly mean \nmonthly = train.resample('M').mean()","f27729b5":"fig, axs = plt.subplots(4,1) \nhourly.Count.plot(figsize=(15,8), title= 'Hourly', fontsize=14, ax=axs[0]) \ndaily.Count.plot(figsize=(15,8), title= 'Daily', fontsize=14, ax=axs[1])\nweekly.Count.plot(figsize=(15,8), title= 'Weekly', fontsize=14, ax=axs[2]) \nmonthly.Count.plot(figsize=(15,8), title= 'Monthly', fontsize=14, ax=axs[3]) ","764a33f5":"train.shape, test.shape","4d178227":"test.Timestamp = pd.to_datetime(test.Date,format='%d-%m-%Y %H:%M') \ntest.index = test.Timestamp  \n# Converting to daily mean \ntest = test.resample('D').mean() ","2a031f60":"train.Timestamp = pd.to_datetime(train.Date,format='%d-%m-%Y %H:%M') \ntrain.index = train.Timestamp \n# Converting to daily mean \ntrain = train.resample('D').mean()","752b7532":"#Train=train.loc['2012-08-25':'2014-06-24'] \n#valid=train.loc['2014-06-25':'2014-09-25']","4e8fd648":"Train=train.loc['2020-05-01':'2020-12-31'] \nvalid=train.loc['2020-05-09':'2020-05-15']","2d312425":"Train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='train') \nvalid.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14, label='valid') \nplt.xlabel(\"Datetime\") \nplt.ylabel(\"Passenger count\") \nplt.legend(loc='best') \nplt.show()","ea9521a1":"dd= np.asarray(Train.Count) \ny_hat = valid.copy() \ny_hat['naive'] = dd[len(dd)-1] \nplt.figure(figsize=(12,8)) \nplt.plot(Train.index, Train['Count'], label='Train') \nplt.plot(valid.index,valid['Count'], label='Valid') \nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast') \nplt.legend(loc='best') \nplt.title(\"Naive Forecast\") \nplt.show()","c5951d7c":"# calculating RMSE to check the accuracy of our model on validation data set.\n\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt \nrms = sqrt(mean_squared_error(valid.Count, y_hat.naive)) \nprint(rms)","19c93dda":"# Considering rolling mean for last 10, 20, 50 days and visualize the results.\n\ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(10).mean().iloc[-1] # average of last 10 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 10 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(20).mean().iloc[-1] # average of last 20 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 20 observations') \nplt.legend(loc='best') \nplt.show() \ny_hat_avg = valid.copy() \ny_hat_avg['moving_avg_forecast'] = Train['Count'].rolling(50).mean().iloc[-1] # average of last 50 observations. \nplt.figure(figsize=(15,5)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast using 50 observations') \nplt.legend(loc='best') \nplt.show()","62fd2ae8":"# RMSE value for Moving Average \n\n#rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.moving_avg_forecast)) \n#print(rms)","f0c2a9ae":"#Here the predictions are made by assigning larger weight to the recent values and lesser weight to the old values.\n\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt \n\ny_hat_avg = valid.copy() \nfit2 = SimpleExpSmoothing(np.asarray(Train['Count'])).fit(smoothing_level=0.6,optimized=False) \ny_hat_avg['SES'] = fit2.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['SES'], label='SES') \nplt.legend(loc='best') \nplt.show()","e1dafd9f":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.SES)) \nprint(rms)","df828225":"import statsmodels.api as sm \nsm.tsa.seasonal_decompose(Train.Count).plot() \nresult = sm.tsa.stattools.adfuller(train.Count) \nplt.show()","9df8291a":"y_hat_avg = valid.copy() \nfit1 = Holt(np.asarray(Train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1) \ny_hat_avg['Holt_linear'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot(Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear') \nplt.legend(loc='best') \nplt.show()","0b99e16d":"# Calculating the RMSE of the model\n\nrms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_linear)) \nprint(rms)","22f7eba9":"y_hat_avg.Holt_linear.head()","2a30768d":"valid.Count.shape, y_hat_avg.Holt_linear.shape","9aaeab4f":"y_hat_avg = valid.copy() \nfit1 = ExponentialSmoothing(np.asarray(Train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit() \ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid)) \nplt.figure(figsize=(16,8)) \nplt.plot( Train['Count'], label='Train') \nplt.plot(valid['Count'], label='Valid') \nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter') \nplt.legend(loc='best') \nplt.show()\n","8ea197d0":"rms = sqrt(mean_squared_error(valid.Count, y_hat_avg.Holt_Winter)) \nprint(rms)","7e174f51":"predict=fit1.forecast(len(test))","7f412939":"train_log_moving_avg_diff = Train_log - moving_avg","618e39bc":"train_log_moving_avg_diff.dropna(inplace = True) \n#test_stationarity(train_log_moving_avg_diff)","bbc073d1":"train_log_diff = Train_log - Train_log.shift(1) \ntest_stationarity(train_log_diff.dropna())","38aa538e":"from statsmodels.tsa.seasonal import seasonal_decompose \ndecomposition = seasonal_decompose(pd.DataFrame(Train_log).Count.values, freq = 7) \n\ntrend = decomposition.trend \nseasonal = decomposition.seasonal \nresidual = decomposition.resid \n\nplt.subplot(411) \nplt.plot(Train_log, label='Original') \nplt.legend(loc='best') \nplt.subplot(412) \nplt.plot(trend, label='Trend') \nplt.legend(loc='best') \nplt.subplot(413) \nplt.plot(seasonal,label='Seasonality') \nplt.legend(loc='best') \nplt.subplot(414) \nplt.plot(residual, label='Residuals') \nplt.legend(loc='best') \nplt.tight_layout() \nplt.show()","5f0325b7":"train_log_decompose = pd.DataFrame(residual) \ntrain_log_decompose['date'] = Train_log.index \ntrain_log_decompose.set_index('date', inplace = True) \ntrain_log_decompose.dropna(inplace=True) \ntest_stationarity(train_log_decompose[0])","72a02f27":"Split dataframe in train and test","0c693f9f":"# Simple Exponential Smoothing\n\nIn this technique, I assign larger weights to more recent observations than to observations from the distant past.<br>\nThe weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations.<br>\n\nNOTE - If I give the entire weight to the last observed value only, this method will be similar to the naive approach. So, I can say that naive approach is also a simple exponential smoothing technique where the entire weight is given to the last observed value.","6ae71412":"Number of Jams","77b407c1":"**Number of Traffic Jams in Wroclaw during the last 7 days**","26ff3a75":"Now we will decompose the time series into trend and seasonality and will get the residual which is the random variation in the series.\n\n# Removing Seasonality\nBy seasonality, we mean periodic fluctuations. A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).<br>\nSeasonality is always of a fixed and known period.<br>\nWe will use seasonal decompose to decompose the time series into trend, seasonality and residuals.","def9e621":"Delays by Traffic Jams in Wroclaw","c457b057":"Duration of the Traffic Jams in Wroclaw during the last days","ac041203":"# # Exploratory Data Analysis","6c5ffe41":"Basic Modelling","a808262b":"We can infer that the fit of the model has improved as the rmse value has reduced.\n\n\n# Holt\u2019s Linear Trend Model\n\n- It is an extension of simple exponential smoothing to allow forecasting of data with a trend.<br>\n- This method takes into account the trend of the dataset. The forecast function in this method is a function of level and trend.<br>\n\nFirst, lets visualize the trend, seasonality and error in the series and then decompose the time series in four parts.<br>\n\n- Observed, which is the original time series.<br>\n- Trend, which shows the trend in the time series, i.e., increasing or decreasing behaviour of the time series.<br>\n- Seasonal, which tells us about the seasonality in the time series.<br>\n- Residual, which is obtained by removing any trend or seasonality in the time series.<br>","2cfe84a7":"Moving Average"}}