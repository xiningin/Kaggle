{"cell_type":{"cef13b58":"code","fed94a89":"code","943d546a":"code","9c67b7fd":"code","58292493":"code","b1bb167b":"code","de4db811":"code","6b85d8af":"code","1cfcb700":"code","ac21e582":"code","c0e4cfad":"code","5bd8529b":"code","7ac124a6":"code","98c60016":"code","84a7da39":"code","d0331ec0":"code","6a056034":"code","d67988dd":"code","de606894":"code","a9f7fd6e":"code","67d995a7":"code","a74432ce":"code","8b1e935b":"code","f45eb234":"code","48f9fb05":"code","dc4d7739":"code","df12bfce":"markdown","86f4d744":"markdown","09a0434e":"markdown","6fba969c":"markdown","e9da8766":"markdown","850d5837":"markdown","fa9428ae":"markdown","4625c28d":"markdown","86a7ffb0":"markdown","2a8245c5":"markdown","bbd3f78c":"markdown","41c62f14":"markdown","27daafee":"markdown","cafb4392":"markdown","99b71f41":"markdown","ea8542c7":"markdown","f93be976":"markdown","2df2eb09":"markdown","4ce03dc1":"markdown","5922be56":"markdown","ebf167ff":"markdown","a907a3ee":"markdown","3875f08f":"markdown"},"source":{"cef13b58":"#packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.over_sampling import SMOTE\nimport imblearn\nimport seaborn as sns","fed94a89":"#Reading \ntrain = pd.read_csv('..\/input\/train\/train.csv')\ntest = pd.read_csv('..\/input\/test\/test.csv')\nprint(train.head())\ntrain.describe()","943d546a":"print(train.isnull().sum(0))\ntest.isnull().sum(0)","9c67b7fd":"target = train.loc[:,'AdoptionSpeed']\nsubtrain = train.drop(columns=['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'])\nMI_clas = mutual_info_classif(subtrain, target, random_state = 123)\nMI_reg = mutual_info_regression(subtrain, target, random_state = 123)\nidx_clas = np.argsort(MI_clas)[::-1]\nidx_reg = np.argsort(MI_reg)[::-1]\ncolumn_names = subtrain.columns.values\nMI = pd.DataFrame(data = {'Column_clas': column_names[idx_clas], 'MI_clas': MI_clas[idx_clas], 'Column_reg': column_names[idx_reg], 'MI_reg': MI_reg[idx_reg] })\n","58292493":"#Show mutual information\nMI","b1bb167b":"# Plot target variables\ntarget.value_counts().sort_index(ascending=False).plot(kind='barh',figsize=(15,6))\nplt.title('Adoption Speed (Target Variable)', fontsize='xx-large')","de4db811":"sme = SMOTETomek(random_state=42)\nX_res, y_res = sme.fit_resample(subtrain, target.values)","6b85d8af":"y_res.shape\nunique, counts = np.unique(y_res, return_counts=True)\ncount = dict(zip(unique, counts))\nplt.bar(range(len(count)), list(count.values()), align='center')\nplt.xticks(range(len(count)), list(count.keys()))\nplt.title('Balance dataset with SMOTETomek')","1cfcb700":"MI_clas = mutual_info_classif(X_res, y_res, random_state = 123)\nMI_reg = mutual_info_regression(X_res, y_res, random_state = 123)\nidx_clas = np.argsort(MI_clas)[::-1]\nidx_reg = np.argsort(MI_reg)[::-1]\ncolumn_names = subtrain.columns.values\nMI = pd.DataFrame(data = {'Column_clas': column_names[idx_clas], 'MI_clas': MI_clas[idx_clas], 'Column_reg': column_names[idx_reg], 'MI_reg': MI_reg[idx_reg] })","ac21e582":"MI","c0e4cfad":"subtrain[[\"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \n          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"State\"]] = subtrain[[\"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \n          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"State\"]].astype('category')\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(subtrain, target, test_size=0.33, random_state=42)\n\nclf1 = sklearn.ensemble.RandomForestClassifier(n_estimators=200, max_depth=20,\n                             random_state=0)\nclf1.fit(X_train, y_train)\npred = clf1.predict(X_test)\nrf = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF classifier: ', rf)","5bd8529b":"importance = clf1.feature_importances_\nidx = np.argsort(importance)[::-1]\ncolumn_names = subtrain.columns.values\nMI = pd.DataFrame(data = {'Column_clas': column_names[idx], 'RF_importance': importance[idx]})\nMI","7ac124a6":"clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=20,\n                             random_state=0)\nclf.fit(X_train.loc[:, ['Age', 'PhotoAmt']], y_train)\npred = clf.predict(X_test.loc[:, ['Age', 'PhotoAmt']])\nprint('RF using only Age and PhotoAmt: ', sklearn.metrics.cohen_kappa_score(y_test, pred))","98c60016":"sme = SMOTETomek(random_state=42)\nX_train_B, y_train_B = sme.fit_resample(X_train, y_train)\n\nclf1 = sklearn.ensemble.RandomForestClassifier(n_estimators=200, max_depth=30,\n                             random_state=0)\nclf1.fit(X_train_B, y_train_B)\npred = clf1.predict(X_test)\nsmotetomek = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF-SMOTETomek classifier: ', smotetomek)","84a7da39":"sme = SMOTEENN(random_state=42)\nX_train_B, y_train_B = sme.fit_resample(X_train, y_train)\n\nclf1 = sklearn.ensemble.RandomForestClassifier(n_estimators=200, max_depth=30,\n                             random_state=0)\nclf1.fit(X_train_B, y_train_B)\npred = clf1.predict(X_test)\nsmoteenn = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF-SMOTENN classifier: ', smoteenn)","d0331ec0":"sme = SMOTE(random_state=42)\nX_train_B, y_train_B = sme.fit_resample(X_train, y_train)\n\nclf1 = sklearn.ensemble.RandomForestClassifier(n_estimators=200, max_depth=30,\n                             random_state=0)\nclf1.fit(X_train_B, y_train_B)\npred = clf1.predict(X_test)\nsmote = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF-SMOTE  classifier: ', smote)","6a056034":"clf1 = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators = 200, max_depth=30, random_state=0)\nclf1.fit(X_train, y_train)\npred = clf1.predict(X_test)\nunder = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF - undersampling: ', under)","d67988dd":"clf1 = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators = 200, max_depth=20, random_state=0)\nclf1.fit(X_train_B, y_train_B)\npred = clf1.predict(X_test)\nunder_smote = sklearn.metrics.cohen_kappa_score(y_test, pred)\nprint('RF - undersampling and SMOTE: ', under_smote)","de606894":"importance = clf1.feature_importances_\nidx = np.argsort(importance)[::-1]\ncolumn_names = subtrain.columns.values\nMI = pd.DataFrame(data = {'Column_clas': column_names[idx], 'RF-under-smote_importance': importance[idx]})\nMI","a9f7fd6e":"count = {'SMOTEENN': smoteenn, 'SMOTETomek': smotetomek, 'SMOTE': smote, 'RF': rf, 'under': under, 'under-SMOTE': under_smote}\nplt.barh(range(len(count)), list(count.values()), align='center')\nplt.yticks(range(len(count)), list(count.keys()))\nplt.title('Metrics')","67d995a7":"age = pd.DataFrame(data = {'Age' : subtrain['Age'].values, 'target': target.values})\n# Iterate through the five airlines\nfor speed in [0, 1, 2, 3, 4]:\n    # Subset to the airline\n    subset = age[age['target'] == speed]\n    \n    # Draw the density plot\n    sns.distplot(subset['Age'], hist = False, kde = True,\n                 kde_kws = {'linewidth': 3}, label = speed)\n    \n# Plot formatting\nplt.legend(prop={'size': 16}, title = 'Age')\nplt.title('Density Plot')\nplt.xlabel('Age')\nplt.ylabel('Density')\nplt.xlim(0, 50)\n","a74432ce":"photo = pd.DataFrame(data = {'photo' : subtrain['PhotoAmt'].values, 'target': target.values})\n# Iterate through the five airlines\nfor speed in [0, 1, 2, 3, 4]:\n    # Subset to the airline\n    subset = photo[photo['target'] == speed]\n    \n    # Draw the density plot\n    sns.distplot(subset['photo'], hist = False, kde = True,\n                 kde_kws = {'linewidth': 3}, label = speed)\n    \n# Plot formatting\nplt.legend(prop={'size': 16}, title = 'Photo amount')\nplt.title('Density Plot')\nplt.xlabel('Photos')\nplt.ylabel('Density')\nplt.xlim(0, 20)\n","8b1e935b":"photo_age = pd.DataFrame(data = {'Age': subtrain['Age'].values, 'Photos' : subtrain['PhotoAmt'].values, 'target': target.values})\nax = sns.scatterplot(x= \"Age\", y=\"Photos\", data=photo_age, hue=\"target\", palette='Spectral')","f45eb234":"test2 = test.drop(columns=['Name', 'RescuerID', 'Description', 'PetID'])\nsme = SMOTE(random_state=42)\nX_balance, y_balance =  sme.fit_resample(subtrain, target)\nclf1 = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators = 200, max_depth=30, random_state=0)\nclf1.fit(X_balance, y_balance)\npred = clf1.predict(test2)\n\n#Write submission\nsubmission_df = pd.DataFrame(data={'PetID' : test['PetID'], \n                                   'AdoptionSpeed' : pred})\nsubmission_df.to_csv('under-SMOTE.csv', index=False)","48f9fb05":"clf1 = sklearn.ensemble.RandomForestClassifier(n_estimators = 200, max_depth=30, random_state=0)\nclf1.fit(subtrain, target)\ntest2[[\"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \n          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"State\"]] = test2[[\"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\", \n          \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\", \"Health\", \"State\"]].astype('category')\npred = clf1.predict(test2)\n\n#Write submission\nsubmission_df = pd.DataFrame(data={'PetID' : test['PetID'], \n                                   'AdoptionSpeed' : pred})\nsubmission_df.to_csv('RF.csv', index=False)","dc4d7739":"sme = SMOTE(random_state=42)\nX_balance, y_balance =  sme.fit_resample(subtrain, target)\nclf1 = sklearn.ensemble.RandomForestClassifier(n_estimators = 200, max_depth=30, random_state=0)\nclf1.fit(X_balance, y_balance)\npred = clf1.predict(test2)\n\n#Write submission\nsubmission_df = pd.DataFrame(data={'PetID' : test['PetID'], \n                                   'AdoptionSpeed' : pred})\nsubmission_df.to_csv('SMOTETomek.csv', index=False)\n","df12bfce":"Clearly, the dataset is imbalanced. The class 0 is minority. This could affect us because the prediction will be likely to avoid the 0 class. We can solve this problem by some technique, for example, using SMOTE or bagging.  Let's see how we can balance data with SMOTETomek (it creates data of the minority class with SMOTE and clean data with Tomek-Links).","86f4d744":"In the imbalanced dataset we have not been successful at regression so classification seems better.  Furthermore, we can see the feature importance with RF, this classifier tells which are the variables that it rely strongly for the classification. Et Voil\u00e0, the **Age** and **PhotoAmt** are the most important as we guessed.  However, the color comes to be important. We can think, how good will be the Random Forest with only these two features?","09a0434e":"* We have seen that statiscally (intrisic to data) and with a Random Forest classifier the most important features are **Age** and **PhotoAmt**.  Although these features need others for a more accurate prediction this features are the most discriminative.\n* The data is imbalanced and this could be disadventageous for a tree based model like Random Forest. Though RF does not seem very harmed by this imbalanced scenario, we have tried SMOTE based methods for dealing with this obtaining a very good result combining SMOTE with a balanced RF. Furthemore, we have explored other SMOTE method which integrates noise removal where SMOTETomek have a very good result.\n* *Which information have we extracted?* pets with a short age and little amount of photos are less likely to be adopted in a short range of time.  Looking both features together we can see that older pets have less photos and effectively they are adopted more slowly. So, PetFinder.my, PLEASE, for every old pet that you have TAKE MORE PHOTOS OF THEM and put more *emphasis* on them. ","6fba969c":"## Dataset Description","e9da8766":"## Is the dataset balanced?","850d5837":"Statiscally the most significant features are **Age**, **Breed1**, **Sterilized**, **State** and **PhotoAmt**.  On the other hand, **Health**, **Gender**, **Color1**, **VideoAmt** and **Color3**. These statements can be possible and it is likelihood to a reality could be. We can not conclude anything right now but a very first sight intution.","fa9428ae":"SMOTE is a method for imbalanced dataset. This method create synthetic data of minority classes rebalancing all of them. Variants of this methods like SMOTETomek and SMOTEENN include noise removal simultaneosly. In this case, SMOTEENN is not a very good option because removes to many data that it consider noise imbalacing data again.","4625c28d":"### Kappa-metrics of all methods","86a7ffb0":"The quality of the model have decreased so although they are important there are not the only one to take into account for an accurate prediction.","2a8245c5":"With the balanced dataset, **Age** and **PhotoAmt** seem still important but the **Color** seem to have acquired a bigger importance.","bbd3f78c":"This is a modified version of RF which balance data in every sample of each subclassifier with an undersampling.","41c62f14":"## Test Prediction (Submission)","27daafee":"How about combining SMOTE and Balanced Random Forest?","cafb4392":"We can see that the target variable is **Adoption Speed**.  Other variables are the following:\n\n*Data Fields*\n\n**PetID** - Unique hash ID of pet profile\n\n**AdoptionSpeed** - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n\n**Type** - Type of animal (1 = Dog, 2 = Cat)\n\n**Name** - Name of pet (Empty if not named)\n\n**Age** - Age of pet when listed, in months\n\n**Breed1** - Primary breed of pet (Refer to BreedLabels dictionary)\n\n**Breed2** - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n\n**Gender** - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n\n**Color1** - Color 1 of pet (Refer to ColorLabels dictionary)\n\n**Color2** - Color 2 of pet (Refer to ColorLabels dictionary)\n\n**Color3** - Color 3 of pet (Refer to ColorLabels dictionary)\n\n**MaturitySize** - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n\n**FurLength** - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n\n**Vaccinated** - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Dewormed** - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Sterilized** - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n\n**Health** - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n\n**Quantity** - Number of pets represented in profile\n\n**Fee** - Adoption fee (0 = Free)\n\n**State** - State location in Malaysia (Refer to StateLabels dictionary)\n\n**RescuerID** - Unique hash ID of rescuer\n\n**VideoAmt** - Total uploaded videos for this pet\n\n**PhotoAmt** - Total uploaded photos for this pet\n\n**Description** - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.","99b71f41":"## Mutual Information (feature importance)","ea8542c7":"We have an interesting competition where we have different type of data available. There are tabular data with general information of each instance in addition to images, sentiment (text) and metadata of the images. I think the best option for the competion is to combine them all but for a first approach I decide to only use the general information of the pets.","f93be976":"## Conclusions","2df2eb09":"The very first thing that every data scientist have to think is: are there any missing values or our data is complete? \nWith a fast check we can see that there are missing values at **Name** and **Description**. In this kernel we will not discuss about description so we only drop this column. For the **Name**, is the **Name** important?","4ce03dc1":"## Balanced Radom Forest (SMOTETomek, SMOTEENN and SMOTE)","5922be56":"Let's see statistically about how important is each feature. We use the mutual information that returns a value between 0 and 1 indicating how each variable is dependent of the target variable.  Notice that this metric is intrinsic of the data and not have to affect to the behaviour of a particular classifier. For this, we compute both for classification case and the regression case. \n\n**Exactly**: we are tackling a ordinal regression. It is a classification problem where the classes are ordered. It is not exactly either classification or classification.","ebf167ff":"Let's see the Random Forest classification. In future kernel I want to explore regression and ordinal regression.","a907a3ee":"## Balanced Random Forest (undersampling)","3875f08f":"## Random Forest classification"}}