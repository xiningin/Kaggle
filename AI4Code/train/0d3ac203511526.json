{"cell_type":{"e614b1da":"code","f7c5908f":"code","050a7546":"code","a82aaeaf":"code","91d6dce8":"code","e40674e4":"code","0e896760":"code","ff1c6498":"code","173c4a34":"code","ce6a0af4":"code","d0e814e1":"code","c7b30599":"code","31607dc4":"code","6258c889":"code","110222c4":"code","bf77bcc2":"markdown","5e2ad8f6":"markdown","5977a4cf":"markdown","f170cb9b":"markdown","3889e1b2":"markdown","2e508f7d":"markdown","3688585c":"markdown","b47e48e9":"markdown","e7eeee30":"markdown","50187caa":"markdown","0ca87317":"markdown","2b8f80a2":"markdown","a70bb12f":"markdown","7975e842":"markdown","69a346bc":"markdown","f11aa8b3":"markdown","08e79141":"markdown"},"source":{"e614b1da":"pip install lightautoml","f7c5908f":"# Standard python libraries\nimport os\nimport time\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Imports from our package\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.pipelines.features.base import EmptyFeaturePipeline\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBAdvancedPipeline, LGBSimpleFeatures\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\nfrom lightautoml.pipelines.selection.permutation_importance_based import NpPermutationImportanceEstimator, \\\n    NpIterativeFeatureSelector\n\nfrom lightautoml.transformers.base import LAMLTransformer, SequentialTransformer, UnionTransformer, ColumnsSelector\nfrom lightautoml.pipelines.utils import get_columns_by_role\nfrom lightautoml.dataset.roles import NumericRole\nfrom lightautoml.pipelines.features.base import FeaturesPipeline, TabularDataFeatures\nfrom lightautoml.reader.base import PandasToPandasReader","050a7546":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 3 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\nTARGET_NAME = 'TARGET' # Target column name","a82aaeaf":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","91d6dce8":"%%time\n\ndata = pd.read_csv('..\/input\/lama-datasets\/sampled_app_train.csv')\ndata.head()","e40674e4":"%%time\n\ndata['BIRTH_DATE'] = (np.datetime64('2018-01-01') + data['DAYS_BIRTH'].astype(np.dtype('timedelta64[D]'))).astype(str)\ndata['EMP_DATE'] = (np.datetime64('2018-01-01') + np.clip(data['DAYS_EMPLOYED'], None, 0).astype(np.dtype('timedelta64[D]'))\n                    ).astype(str)\n\ndata['constant'] = 1\ndata['allnan'] = np.nan\n\ndata['report_dt'] = np.datetime64('2018-01-01')\n\ndata.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)","0e896760":"%%time\n\ntrain_data, test_data = train_test_split(data, \n                                         test_size=TEST_SIZE, \n                                         stratify=data[TARGET_NAME], \n                                         random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_data.shape, test_data.shape))","ff1c6498":"train_data.head()","173c4a34":"%%time\n\ntask = Task('binary', )\nreader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)","ce6a0af4":"%%time\n\nroles = {'target': TARGET_NAME,\n         DatetimeRole(base_date=True, seasonality=(), base_feats=False): 'report_dt',\n         }","d0e814e1":"\nclass GroupByTransformer(LAMLTransformer):\n\n    _fit_checks = ()\n    _transform_checks = ()\n    _fname_prefix = 'grb'\n\n\n    @property\n    def features(self):\n        \"\"\"Features list.\"\"\"\n\n        return self._features\n\n    def __init__(self):\n\n        super().__init__()\n        self.dicts = {}\n\n\n\n    def fit(self, dataset):\n\n        # set transformer names and add checks\n        for check_func in self._fit_checks:\n            check_func(dataset)\n        # set transformer features\n\n        # convert to accepted dtype and get attributes\n        dataset = dataset.to_pandas()\n        df = dataset.data\n        cat_cols = get_columns_by_role(dataset, 'Category')\n        num_cols = get_columns_by_role(dataset, 'Numeric')\n\n        feats = []\n        for cat in cat_cols:\n            for num in num_cols:  \n                feature = f'{self._fname_prefix}__{cat}_delta_mean_{num}'\n                _dict = df[[cat, num]].groupby(cat)[num].mean().to_dict()\n                self.dicts[feature] = {'cat': cat, 'num': num, 'values': _dict}\n                feats.append(feature)\n            \n        self._features = feats\n        return self\n\n\n    def transform(self, dataset):\n\n        # checks here\n        super().transform(dataset)\n        # convert to accepted dtype and get attributes\n        dataset = dataset.to_pandas()\n        df = dataset.data\n\n        # transform\n        roles = NumericRole()\n        outputs = []\n        for feat, value in self.dicts.items():\n            cat, num = value['cat'], value['num']\n            new_arr = (df[num] - df[cat].map(value['values'])).values.reshape(-1, 1)\n            output = dataset.empty().to_numpy()\n            output.set_data(new_arr, [f'{self._fname_prefix}__{cat}_delta_mean_{num}'], roles)\n            outputs.append(output)\n        # create resulted\n        return dataset.empty().to_numpy().concat(outputs)\n    \n    \nclass GroupByPipeline(FeaturesPipeline, TabularDataFeatures):\n\n\n    def __init__(self, feats_imp = None, top_category: int = 3, top_numeric: int = 3, **kwargs):\n        \"\"\"\n\n        \"\"\"\n        super().__init__(feats_imp=feats_imp)\n        self.top_category = top_category\n        self.top_numeric = top_numeric\n\n    def create_pipeline(self, train):\n\n\n        transformer_list = []\n\n        categories = get_columns_by_role(train, 'Category')\n        numerics = get_columns_by_role(train, 'Numeric')\n        cat_feats_to_select = []\n        num_feats_to_select = []\n        if len(categories) > self.top_category:\n            cat_feats_to_select = self.get_top_categories(train, self.top_category)\n        elif len(categories) > 0:\n            cat_feats_to_select = categories\n            \n        if len(numerics) > self.top_numeric:\n            num_feats_to_select = self.get_top_numeric(train, self.top_numeric)\n        elif len(numerics) > 0:\n            num_feats_to_select = numerics\n        \n        if (len(cat_feats_to_select) > 0) and (len(num_feats_to_select) > 0):\n            cat_processing = [\n\n                ColumnsSelector(keys=cat_feats_to_select + num_feats_to_select),\n                GroupByTransformer(),\n\n            ]\n            cat_processing = SequentialTransformer(cat_processing)\n            transformer_list.append(cat_processing)\n            \n        return UnionTransformer(transformer_list)\n    \n    def get_top_numeric(self, train, top_n = 5):\n\n        nums = get_columns_by_role(train, 'Numeric')\n        if len(nums) == 0:\n            return []\n\n        df = pd.DataFrame({'importance': 0, 'cardinality': 0}, index=nums)\n        # importance if defined\n        if self.feats_imp is not None:\n            feats_imp = pd.Series(self.feats_imp.get_features_score()).sort_values(ascending=False)\n            df['importance'] = feats_imp[feats_imp.index.isin(nums)]\n            df['importance'].fillna(-np.inf)\n\n        # check for cardinality\n        df['cardinality'] = -self.get_uniques_cnt(train, nums)\n        # sort\n        df = df.sort_values(by=['importance', 'cardinality'], ascending=[False, self.ascending_by_cardinality])\n        # get top n\n        top = list(df.index[:top_n])\n\n        return top","c7b30599":"#post selection\n\nmodel0 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n)\n\npie = NpPermutationImportanceEstimator()\nselector = ImportanceCutoffSelector(LGBSimpleFeatures(), model0, pie, cutoff=-99999)","31607dc4":"%%time\n\npipe = LGBAdvancedPipeline(top_intersections=2).append(GroupByPipeline(selector, 5, 5))\n\nmodel = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS})\n\n\npipeline = MLPipeline([\n    (model),\n], pre_selection=selector, features_pipeline=pipe, post_selection=None)","6258c889":"%%time \nstart = time.time()\n\nautoml = AutoML(reader, [\n    [pipeline],\n], skip_conn=False, verbose=0)\n\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\ntime_automl = time.time() - start","110222c4":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values,\n                                           oof_pred.data[:, 0])))\ntest_automl = roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])\nprint('TEST score: {}'.format(test_automl))\n","bf77bcc2":"Cell below shows some user feature preparations to create task more difficult (this block can be omitted if you don't want to change the initial data):","5e2ad8f6":"# Step 0.1. Import necessary libraries ","5977a4cf":"# Step 0.2. Parameters ","f170cb9b":"#  ==== Custom AutoML pipeline ====\n\n\n## Step 1. Create Task and Reader","3889e1b2":"## Step 7. Predict to test data and check scores","2e508f7d":"Roles setup here set target column and base date, which is used to calculate date differences:","3688585c":"# Step 0. Install LAMA","b47e48e9":"## Step 6. Create AutoML.","e7eeee30":"## Step 4. Create feature selector.","50187caa":"# Step 0.6. (Optional) Data splitting for train-test ","0ca87317":"## Step 5. Create pipelines.","2b8f80a2":"## Step 2. Setup columns roles","a70bb12f":"## Step 3. Create custom transformer and feature pipeline.","7975e842":"# Step 0.3. Fix torch number of threads and numpy seed ","69a346bc":"Block below can be omitted if you are going to train model only or you have specific train and test files:","f11aa8b3":"# Step 0.5. (Optional) Some user feature preparation ","08e79141":"# Step 0.4. Example data load "}}