{"cell_type":{"a9fb197d":"code","72264bb0":"code","9780c26a":"code","e1e0854c":"code","03761c3e":"code","9b6f8772":"code","00f44899":"code","633734fe":"code","f49370a2":"code","4e8e7489":"code","b97b07f2":"code","cb5c31cc":"code","8919d109":"code","544df3bb":"code","4346dbf7":"code","402edd98":"code","8ea65f4e":"code","0f3f20c1":"code","f766c07c":"code","4b7edacd":"code","4d806a84":"code","5056e01e":"code","747e1c1f":"code","41d2edea":"code","77a65a8b":"code","be0c6289":"markdown","acff3695":"markdown","65bbce70":"markdown","15d58cfc":"markdown","6fb72695":"markdown","fe8b1f89":"markdown","f5e2ee26":"markdown","30b8337f":"markdown","f3c804b9":"markdown","99252554":"markdown","8031eb2b":"markdown","66770c9a":"markdown","82fa9969":"markdown","34fb8792":"markdown","83a596d1":"markdown","347ec2d8":"markdown"},"source":{"a9fb197d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O and data manipulation \n\n#visulaizations \nimport matplotlib.pyplot as plt   \nimport seaborn as sns\n%matplotlib inline\n\n#\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","72264bb0":"# let's define some helping functions \ndef cor_map(df):\n    ## objective : drawing a heating map of correlation between the numerical features and each other\n    ## input : data \n    \n    cor=df.corr()\n    _,ax=plt.subplots(figsize=(12,10))\n    cmap=sns.diverging_palette(192,6,as_cmap=True)\n    _=sns.heatmap(cor,cmap=cmap,ax=ax,square=True,annot=True)\n    \ndef print_coef(model,x):\n    ## objective : printing the coefficients of each feature to determine it's importance \n    ## input: ML model , X(all data except the target variable) \n    coeff_df = pd.DataFrame(x.columns)\n    coeff_df.columns = ['Variable']\n    coeff_df[\"Coeff\"] = pd.Series(model.coef_)\n    coeff_df.sort_values(by='Coeff', ascending=True)\n    print(coeff_df)\n    \ndef metrics(y_true,y_pred):\n    ## objective: printing the R^2, mean squared error and mean absolute error \n    ## input: the actual target variable Series and the predicted target variable Series \n    print(\"R2 score:\",r2_score(y_true,y_pred))\n    print('mean squared error',mean_squared_error(y_true,y_pred))\n    print(\"mean absolute error\",mean_absolute_error(y_true,y_pred))","9780c26a":"# loading the data \ndata=pd.read_csv('..\/input\/diamonds.csv')\ndata.head()","e1e0854c":"# removing the unwanted column \ndata.drop(['Unnamed: 0'],axis=1,inplace=True)\ndata.head()\n","03761c3e":"# show the classes of every categorical feature \nfor i in data.select_dtypes(include=['O']).columns:\n    print(i,data[i].unique())","9b6f8772":"#check data types and  if there's no missing data\ndata.info()","00f44899":"x=data.drop('price',axis=1)\ny=data['price']","633734fe":"data.describe()","f49370a2":"data.describe(include=['O'])","4e8e7489":"cor_map(data)","b97b07f2":"print(data[['cut','price']].groupby('cut',as_index=False).mean().sort_values(by='price',ascending=False))\nprint(data[['color','price']].groupby('color',as_index=False).mean().sort_values(by='price',ascending=False))\nprint(data[['clarity','price']].groupby('clarity',as_index=False).mean().sort_values(by='price',ascending=False))","cb5c31cc":"#how many exactly missing data we have\ndata.loc[(data['x']==0)|(data['y']==0)|(data['z']==0)].shape[0]","8919d109":"# we will exclude them from the dataset since 20 aren't important\ndata=data.loc[(data['x']!=0)&(data['y']!=0)&(data['z']!=0.0)]","544df3bb":"data['p\/ct']=data['price']\/data['carat']\nprint(data[['cut','p\/ct']].groupby('cut',as_index=False).mean().sort_values(by='p\/ct',ascending=False))\nprint(data[['color','p\/ct']].groupby('color',as_index=False).mean().sort_values(by='p\/ct',ascending=False))\nprint(data[['clarity','p\/ct']].groupby('clarity',as_index=False).mean().sort_values(by='p\/ct',ascending=False))","4346dbf7":"data['cut']=data['cut'].map({'Ideal':1,'Good':2,'Very Good':3,'Fair':4,'Premium':5})\ndata['color']=data['color'].map({'E':1,'D':2,'F':3,'G':4,'H':5,'I':6,'J':7})\ndata['clarity']=data['clarity'].map({'VVS1':1,'IF':2,'VVS2':3,'VS1':4,'I1':5,'VS2':6,'SI1':7,'SI2':8})\ndata.head()","402edd98":"#also we can merge the thre dimensions into volume \ndata['volume']=data['x']*data['y']*data['z']\ndata['table*y']=data['table']*data['y']\ndata['depth*y']=data['depth']*data['y']","8ea65f4e":"data['cut\/wt']=data['cut']\/data['carat']\ndata['color\/wt']=data['color']\/data['carat']\ndata['clarity\/wt']=data['clarity']\/data['carat']","0f3f20c1":"data.drop(['carat','cut','color','clarity','depth','table','x','y','z','p\/ct'],axis=1,inplace=True)\ncor_map(data)","f766c07c":"X_train,X_test,y_train,y_test=train_test_split(data.drop(['price'],axis=1),data['price'],test_size=0.25,random_state=1)\nscale = StandardScaler()\nX_train_scaled = scale.fit_transform(X_train)\nX_test_scaled = scale.transform(X_test)\nprint(\"The shape of the train set\",X_train.shape)\nprint(\"The shape of the test set\",X_test.shape)","4b7edacd":"reg_all=LinearRegression()\nreg_all.fit(X_train_scaled,y_train) #fitting the model for the x and y train\n\npred=reg_all.predict(X_test_scaled) #predicting y(the target variable), on x test\n\n# Rsquare=reg_all.score(X_test,y_test)\nR2=r2_score(y_test,pred)\n# print(\"Rsquare: %f\" %(Rsquare))\n# print(\"R2:\",R2)\nprint ('=========\\nTest results')\nmetrics(y_test,pred)\nprint ('=========\\nTrain results')\nmetrics(y_train,reg_all.predict(X_train_scaled))\nprint_coef(reg_all,X_train)","4d806a84":"kn_model=KNeighborsRegressor(n_neighbors=3)\nkn_model.fit(X_train_scaled,y_train)\npred=kn_model.predict(X_test_scaled)\nprint ('=========\\nTest results')\nmetrics(y_test,pred)\nprint ('=========\\nTrain results')\nmetrics(y_train,kn_model.predict(X_train_scaled))","5056e01e":"gbr = GradientBoostingRegressor(random_state=0)\ngbr.fit(X_train_scaled,y_train)\npred=gbr.predict(X_test_scaled)\nprint ('=========\\nTest results')\nmetrics(y_test,pred)\nprint ('=========\\nTrain results')\nmetrics(y_train,gbr.predict(X_train_scaled))","747e1c1f":"xgb = XGBRegressor(random_state=0,n_jobs=-1)\nxgb.fit(X_train_scaled,y_train)\npred=xgb.predict(X_test_scaled)\nprint ('=========\\nTest results')\nmetrics(y_test,pred)\nprint ('=========\\nTrain results')\nmetrics(y_train,xgb.predict(X_train_scaled))","41d2edea":"\n\nclf = XGBRegressor(random_state=0,n_jobs=-1)\ncv_sets = ShuffleSplit(X_train.shape[0], n_iter =10,test_size = 0.20, random_state = 7)\nparameters = {'n_estimators':list(range(100,1000,100)),\n#              'max_depth':np.linspace(1,32,32,endpoint=True,dtype=np.int),\n             'learning_rate':[0.05,0.1,0.25,0.5,0.75],}\n#              'reg_lambda':[1,10,15,20,25]}\nscorer=make_scorer(r2_score)\ngrid_obj=GridSearchCV(clf, parameters, scoring=scorer,verbose=1,cv=cv_sets)\ngrid_obj= grid_obj.fit(X_train_scaled,y_train)\nclf_best = grid_obj.best_estimator_\nprint(clf_best)\nclf_best.fit(X_train_scaled,y_train)","77a65a8b":"print(clf_best)\nprint ('=========\\nTrain results')\nmetrics(y_train,clf_best.predict(X_train_scaled))\nprint ('=========\\nTest results')\nmetrics(y_test,pred)","be0c6289":"## analysis","acff3695":"## Grid Search with cross validation","65bbce70":"#### 4- XGBoost ","15d58cfc":"* **depth and table have mean nearly equal to the median, which indicates that their distribution is nearly normal with small standard deviation --> this could means that these features independently could be with little effect on the target value since the variation is very small **\n* **carat and the three dimensions have high standard deviation which indicates that there is high variance in these features thus higher effection on the target value **\n* **the three dimensions has zero values !!!??? which is very weird it's impossible to have any dimension in diamond equal to zero so it must be error in entry (<span style='color:red'>complete feature<\/span>)**","6fb72695":"## What is the Daimond ?\nIt's a precious stone consisting of a clear and colourless crystalline form of pure carbon, the hardest naturally occurring substance.\n\n## Why are the diamonds so rare ? \nDiamonds are one of the hardest materials found on Earth. Other than that, they hold no unique distinctions. All gem grade materials are rare, composing just a tiny fraction of the Earth. However, among gems, diamonds are actually the most common.\n\n## Objective \nWe need to predict the value of the diamonds from their features\n\n## Why the problem need to be solved \nThe price of the diamond is hard to figure for normal buyer as it depends on many factors you can see two diamonds with ths same weight(carat) but stil very diffferent on price scale .  \n\" Take a look at [this diamond from Amazon](https:\/\/www.amazon.com\/gp\/product\/B072L343M2\/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=thediamondpro-20&creative=9325&linkCode=as2&creativeASIN=B072L343M2&linkId=b169c65833f9b1074b4c82cb125f7e58) and then take a look this [diamond from Blue Nile](https:\/\/www.bluenile.com\/diamond-details\/LD09889381?click_id=962062758). They\u2019re both one carat diamonds. Does it make sense that they\u2019re the same size, yet one costs \\$1,179 and the other \\$16,500? To make it even crazier, the Blue Nile diamond may actually be a better value!\" [The diamonds pro](https:\/\/www.diamonds.pro\/education\/diamond-prices\/)  \n\nTo avoid being scammed or failed to have a good baragain, a good price prediction may come in handy ","fe8b1f89":"## PreProcessing & Feature extraction ","f5e2ee26":"#### 3- Gradient Boosting ","30b8337f":"#### 1- Linear Regression ","f3c804b9":"## Topics \n0. Loading Libraries and helping functions \n1. Exploring Dataset \n2. Analysis \n3. Preprocessing \n4. Feature Engineering \n5. Model \n6. Tuning \n7. Evaluation ","99252554":"## 0) Loading libraries and helping functions ","8031eb2b":"* **price has extremly high correlation with carat and dimensions, and that's somehow rational as the size of the diamond increases, it's price increases **\n* **as we predicted table and depth has small effect on the price, also they're negatively correlated, in other words somehow inversely proportional **\n* **all the relations between the categorical features and the price seem not reasonable, diamonds with fair cut has higher mean price than that with ideal cut, the same way with the colors and clarity, we could explain this by saying that other factors has much higher effect on the price than them, and since we know that the size of the diamond had the highest correlation rate among the numerical data, then maybe we need to generate a price for every carat(200 mg) of diamond.(<span style='color:indigo'>generate new feature<\/span>)**","66770c9a":"**There is no missing data, we have 7 numerical features and 3 categorical features **","82fa9969":" #### 2- K nearest neighbor","34fb8792":"## Training and Evaluation ","83a596d1":"## Features:\n* **carat**: Diamond carat weight is the measurement of how much a diamond weighs. A metric \u201ccarat\u201d is defined as 200 milligrams\n* **cut**: diamond cut is a style or design guide used when shaping a diamond for polishing,  \n    it's classes from best to worst are : Ideal,Premium,Very Good, Good,Fair\n\n* **color**: the color of the diamond, The less body color in a white diamond, the more true color it will reflect, and thus the greater its value  \n    it's classes from best to worst: D,E,F,G,H,I,J\n\n* **clarity**:Clarity refers to the degree to which these imperfections are present. Diamonds which contain numerous or significant inclusions or blemishes have less brilliance because the flaws interfere with the path of light through the diamond.  \n    it's classes from the best to the worst: IF, VVS2,VVS1,VS2,VS1,SI2,SI1,I1\n\n* **depth**: depth % :The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n\n* **x**,**y**,**z**: Length,width,height\n* **price**: Is the price of the diamond\n* **table**: table%: The width of the diamond's table expressed as a percentage of its average diameter\n<img src=\"https:\/\/www.lumeradiamonds.com\/images\/diamond-education\/depth_table.gif\" \/>","347ec2d8":"**so now officially we predict prict\/carat and by multiplying it with carat it will return to price, or we could instead multipy each class by carat as a weight!!!**"}}