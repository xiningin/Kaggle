{"cell_type":{"d9614707":"code","419763a1":"code","1b3d96df":"code","bed8fbe8":"code","e0bf9f61":"code","b8db78d6":"code","dd239bd0":"code","8cef4e97":"code","9344e52c":"code","839d0103":"code","0fa7a25c":"code","283993a5":"code","5badc525":"markdown","98af9f2e":"markdown","755c4152":"markdown","5b5549cc":"markdown","79110146":"markdown","caa3fd5c":"markdown","1e1385a9":"markdown","1a68d995":"markdown","24da9f40":"markdown"},"source":{"d9614707":"import random, os, pickle, scipy, math, time, joblib\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import signal\nfrom scipy import interpolate\n\nfrom matplotlib import pyplot as plt\nfrom tqdm.auto import tqdm\n\nfrom sklearn.preprocessing import RobustScaler","419763a1":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest  = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\n\n# This will be explained later\nCHOP = 33","1b3d96df":"RC_TimeConstant = {\n    (R, C): 400 * R * C\n    for R in [5,20,50]\n    for C in [10,20,50]\n}\nRC_Intercept = train.groupby(['R','C']).pressure.mean().to_dict()","bed8fbe8":"def numpy_ewma_vectorized_v2(data, window):\n    # https:\/\/stackoverflow.com\/questions\/42869495\/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm\n        \n    alpha = 2 \/(window + 1.0)\n    alpha_rev = 1-alpha\n    n = data.shape[0]\n\n    pows = alpha_rev**(np.arange(n+1))\n\n    scale_arr = 1\/pows[:-1]\n    offset = data[0]*pows[1:]\n    pw0 = alpha*alpha_rev**(n-1)\n\n    mult = data*pw0*scale_arr\n    cumsums = mult.cumsum()\n    out = offset + cumsums*scale_arr[::-1]\n    return out","e0bf9f61":"def autocorr(x):\n    # https:\/\/stackoverflow.com\/questions\/643699\/how-can-i-use-numpy-correlate-to-do-autocorrelation\n    result = np.correlate(x, x, mode='full')\n    result = result[result.shape[0]\/\/2:] \/ x.var() \/ len(x)\n    result[np.isnan(result)] = 0\n    result[np.isinf(result)] = 0\n    return result","b8db78d6":"def rolling_window(a, window):\n    # https:\/\/rigtorp.se\/2011\/01\/01\/rolling-statistics-numpy.html\n    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n    strides = a.strides + (a.strides[-1],)\n    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)","dd239bd0":"def integrate(x, v, a, dt):\n    # https:\/\/en.wikipedia.org\/wiki\/Fourth,_fifth,_and_sixth_derivatives_of_position\n    outp = x + v * dt + 0.5 * a * dt ** 2\n    return np.concatenate(([outp[0]], outp[:-1]))","8cef4e97":"scalers = []","9344e52c":"def featurize(df, no_labels=False, transform=True, verbose=False):\n    global scalers\n    \n    # Don't mess it up!\n    df = df.copy()\n    \n    if not no_labels:\n        pressure = df.pressure.values\n        delta_pressure = np.concatenate(([0],np.diff(pressure)))\n        delta_delta_pressure = np.concatenate(([0],np.diff(delta_pressure)))\n        \n    drop_cols = [\n        col for col in df if col in ['id','breath_id','pressure']\n    ]\n    df = df[['u_out','R','C','time_step','u_in']].values.reshape(-1, 80, 5)\n    \n    # I transpose the df so that I can do sample[feature], rather than sample[:, feature]\n    # We will un-do this operation at the end...\n    df = df.transpose(0,2,1)\n    \n    new_cols = 41\n    enew_cols = 1\n    out  = np.zeros((df.shape[0], new_cols, df.shape[2]), dtype=np.float32)*np.nan\n    eout = np.zeros((df.shape[0], enew_cols), dtype=np.int64)\n    \n    feature_uout = 0\n    feature_R = 1\n    feature_C = 2\n    feature_ts = 3\n    feature_uin = 4\n\n    mask = (df[:, feature_uout] == 0) & (df[:, feature_ts] >= 0)\n    \n    #######################################################\n    # Embedding Features:\n    # MODE: https:\/\/www.kaggle.com\/marutama\/eda-about-pressure-with-colored-charts\n    eout[:, 0] = ((df[:, feature_uin, -1] > 4.8) & (df[:, feature_uin, -1] < 5.1)).astype(np.int64)\n    \n    # I don't embed R or C. I just pass those in as continuous values. \n    # That was probably a mistake..\n    \n    #######################################################\n\n    it = tqdm(df) if verbose else df\n    for idx, sample in enumerate(it):\n        \n        # CACHE:\n        cached_uin = sample[feature_uin].copy()\n        \n        # Before anything else:\n        # https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/282370\n        sample[feature_uin] = np.abs(sample[feature_uin])**0.25 * np.sign(sample[feature_uin])\n        \n        feature_num = 0\n        # 5. Transformation\n        # The resistance of the valve is R_in \u221d 1\/d 4 (Poiseuille\u2019s law) where d, the opening of the valve\n        # https:\/\/arxiv.org\/abs\/2102.06779\n        out[idx, feature_num] = np.sqrt(np.abs(sample[feature_uin])) * np.sign(sample[feature_uin])\n        feature_num += 1\n        \n        # 6. ts derivative (delta_ts)\n        out[idx, feature_num] = np.concatenate(([0],np.diff(sample[feature_ts])))\n        out[idx, feature_num, 0] = out[idx, feature_num, 1:].min()\n        feature_delta_ts = feature_num\n        feature_num += 1\n\n        # 7. u_in derivative (delta_uin)\n        out[idx, feature_num] = np.concatenate(([0],np.diff(sample[feature_uin])))\n        out[idx, feature_num, 0] = out[idx, feature_num, 1:].min()\n        feature_uin_derivative = feature_num\n        feature_num += 1\n        \n        # 8. u_in 2nd order derivative (delta_delta_uin)\n        out[idx, feature_num] = np.concatenate(([0],np.diff(out[idx, feature_uin_derivative])))\n        out[idx, feature_num, 0] = out[idx, feature_num, 1:].min()\n        feature_uin_derivative_derivative = feature_num\n        feature_num += 1\n        \n        # 9. u_in ema3,4\n        out[idx, feature_num] = numpy_ewma_vectorized_v2(sample[feature_uin], 3)\n        feature_num += 1\n        out[idx, feature_num] = numpy_ewma_vectorized_v2(sample[feature_uin], 4)\n        feature_num += 1\n        \n        # 10. feature_uin_derivative ema3\n        out[idx, feature_num] = numpy_ewma_vectorized_v2(out[idx, feature_uin_derivative], 3)\n        feature_num += 1\n        \n        # 11. u_in autocorr\n        out[idx,feature_num] = autocorr(sample[feature_uin])\n        feature_num += 1\n        \n        # 12. integrated uin w.r.t. time\n        out[idx,feature_num] = np.cumsum(sample[feature_uin] * out[idx, feature_delta_ts])\n        feature_uin_integrated = feature_num\n        feature_num += 1\n        \n        # 13. d_uin \/ d_ts\n        out[idx,feature_num] = out[idx, feature_uin_derivative] \/ out[idx, feature_delta_ts]\n        out[idx,feature_num][np.isnan(out[idx,feature_num])] = 0  # TODO: Check if this is a good empty fill....\n        out[idx,feature_num][np.isinf(out[idx,feature_num])] = 0\n        feature_num += 1\n        \n        ######################################\n        window = 4\n        rolling = rolling_window(sample[feature_uin], window)\n        \n        # 14. uin_max\n        tmp = rolling.max(axis=-1)\n        out[idx,feature_num] = np.concatenate(([tmp[0]]*(window-1), tmp))\n        feature_num += 1\n        \n        # 15. uin_std\n        tmp = rolling.std(axis=-1)\n        tmp[np.isnan(tmp)] = 0\n        out[idx,feature_num] = np.concatenate(([tmp[0]]*(window-1), tmp))\n        feature_num += 1\n        \n        # 16. uin_mean - do we need this? similar to ema...\n        tmp = rolling.mean(axis=-1)\n        out[idx,feature_num] = np.concatenate(([tmp[0]]*(window-1), tmp))\n        feature_num += 1\n        \n        # 17. uin_slopes\n        tmp = sample[feature_ts, window-1:].reshape(-1,1)\n        tmp = ((tmp*rolling).mean(axis=1) - tmp.mean()*rolling.mean(axis=1)) \/ ((tmp**2).mean() - (tmp.mean())**2)\n        out[idx,feature_num] = np.concatenate(([tmp[0]]*(window-1), tmp))\n        feature_num += 1\n        \n        #############################\n            \n        # 18. uin_first value\n        out[idx,feature_num] = sample[feature_uin, 0] * np.ones(sample.shape[-1])\n        feature_num += 1\n        \n        # 19. sum(uin[44:])\n        out[idx,feature_num] = sample[feature_uin, 44:].sum() * np.ones(sample.shape[-1])\n        feature_num += 1\n        \n        # 20. ts_last_value\n        out[idx,feature_num] = sample[feature_ts, -1] * np.ones(sample.shape[-1])\n        feature_num += 1\n        \n        # 21\/ u_in right before first u_out=1\n        tmp_last_val = np.nonzero(sample[feature_uin] * (1-sample[feature_uout]))[0]\n        if len(tmp_last_val) > 0:\n            out[idx,feature_num] = sample[feature_uin, tmp_last_val[-1]] * np.ones(sample.shape[-1])\n        else:\n            out[idx,feature_num] = 0\n        feature_num += 1\n        \n        # 22. u_in_mean <-- where u_out = 0\n        out[idx,feature_num] = (sample[feature_uin] * (1-sample[feature_uout])).sum() \/ (1-sample[feature_uout]).sum() * np.ones(sample.shape[-1])\n        feature_num += 1\n\n        # 23. uin_max\n        out[idx,feature_num] = sample[feature_uin].max() * np.ones(sample.shape[-1])\n        feature_num += 1\n\n        # 24. uin_std\n        out[idx,feature_num] = sample[feature_uin].std() * np.ones(sample.shape[-1])\n        feature_num += 1\n        \n        # 25. uin_skew\n        out[idx,feature_num] = scipy.stats.skew(sample[feature_uin]) * np.ones(sample.shape[-1])\n        feature_num += 1\n        \n        #############################\n        # 26. uin_dist2mean\n        out[idx,feature_num] = sample[feature_uin].mean() - sample[feature_uin]\n        feature_uin_dist2mean = feature_num\n        feature_num += 1\n        \n        # 27. Dist2slope integration...\n        out[idx,feature_num] = np.cumsum(out[idx, feature_uin_dist2mean] * out[idx, feature_delta_ts]) \n        feature_num += 1\n        \n        # 28-30. uin_lags (3)\n        feature_uin_lags = feature_num\n        for lag in range(1,4):\n            out[idx,feature_num] = np.concatenate((\n                [0]*lag,\n                sample[feature_uin,:-lag]\n            ))\n            feature_num += 1\n            \n        # 31-33. uin_integrated_lags (3)\n        for lag in range(1,4):\n            out[idx,feature_num] = np.concatenate((\n                [0]*lag,\n                out[idx, feature_uin_integrated,:-lag]\n            ))\n            feature_num += 1\n            \n        # 34-36. uin_derivative_lags (3)\n        feature_uin_derivative_lags = feature_num\n        for lag in range(1,4):\n            out[idx,feature_num] = np.concatenate((\n                [0]*lag,\n                out[idx, feature_uin_derivative,:-lag]\n            ))\n            feature_num += 1\n        \n        # 37-39. uin_seeks (3)\n        feature_uin_seeks = feature_num\n        for seek in range(1,4):\n            out[idx,feature_num] = np.concatenate((\n                sample[feature_uin,seek:],\n                [0]*seek\n            ))\n            feature_num += 1\n            \n        \n        # 40. Physics Integration\n        out[idx,feature_num] = integrate(\n            x=sample[feature_uin],\n            v=out[idx, feature_uin_derivative],\n            a=out[idx, feature_uin_derivative_derivative],\n            dt=out[idx, feature_delta_ts],\n        )\n        feature_num += 1\n        \n        # # Real Physics: Inhale Volume\n        # # https:\/\/www.kaggle.com\/motloch\/vpp-pip-analysis-and-new-features\n        # R = sample[feature_R, 0]\n        # C = sample[feature_C, 0]\n        # inhale_factor = np.exp(-sample[feature_ts] \/ RC_TimeConstant[(R,C)])\n        # vf = cached_uin.cumsum() * R \/ inhale_factor  #  <-- feature1\n        # out[idx,feature_num] = vf \/ 450# + RC_Intercept[(R,C)]\n        # feature_num += 1\n\n        # 41-42. Weird uin cumsums\n        # I forgot who I stole these from\n        # Find the index of the first u_out=1\n        if len(tmp_last_val) ==0:\n            # We cannot compute this feature...\n            out[idx,feature_num+0] = 0\n            out[idx,feature_num+1] = 0\n        else:\n            idx_end = 1 + tmp_last_val[-1]\n            rev_cumsum = sample[feature_uin, :idx_end].cumsum()[::-1]\n            \n            out[idx,feature_num+0, :idx_end] = rev_cumsum - sample[feature_uin, :idx_end]\n            out[idx,feature_num+1, :idx_end] = rev_cumsum.sum() - sample[feature_uin, :idx_end]\n            out[idx,feature_num+0, idx_end:] = out[idx,feature_num+0, idx_end-1]\n            out[idx,feature_num+1, idx_end:] = out[idx,feature_num+1, idx_end-1]\n        feature_num += 2\n        \n        # 43. uin * timestep\n        out[idx,feature_num] = sample[feature_uin] * sample[feature_ts]\n        feature_num += 1\n        \n        # 44. deltats**2\n        out[idx,feature_num] =  out[idx, feature_delta_ts] ** 2\n        feature_num += 1\n        \n\n    # For my models, I only train on the first ~33 samples\n    # Chop off u_out\n    df = np.concatenate((df[:, 1:],out), axis=1).astype(np.float32)\n    df = df.transpose(0,2,1)\n    df = df[:,:CHOP] #chop\n    mask = mask.reshape(-1, 80).astype(np.float32)\n    mask = mask[:, :CHOP] #chop\n    \n    # My models regress pressure, and its first and second derivatives\n    if not no_labels:\n        pressure = pressure.reshape(-1, 80).astype(np.float32)\n        delta_pressure = delta_pressure.reshape(-1, 80).astype(np.float32)\n        delta_delta_pressure = delta_delta_pressure.reshape(-1, 80).astype(np.float32)\n        \n        pressure = pressure[:, :CHOP] #chop\n        delta_pressure = delta_pressure[:, :CHOP] #chop\n        delta_delta_pressure = delta_delta_pressure[:, :CHOP] #chop\n        \n    else:\n        pressure = 1\n        delta_pressure = 1\n        delta_delta_pressure = 1\n    \n    ###\n    # Baked Transform\n    if transform:\n        for col in range(df.shape[-1]):\n            df[:,:,col] = scalers[col].transform(df[:,:,col].reshape(-1,1)).reshape(df[:,:,col].shape)\n\n    return df, eout, pressure, delta_pressure, delta_delta_pressure, mask","839d0103":"df, eout, pressure, delta_pressure, delta_delta_pressure, mask = featurize(train.append(test), transform=False, verbose=True)\n\nprint(df.shape)\nprint(eout.shape)\nprint(pressure.shape)\nprint(delta_pressure.shape)\nprint(delta_delta_pressure.shape)\nprint(mask.shape)","0fa7a25c":"plt.title('Main Mode')\nplt.hist(eout[:,0].flatten(), bins=60)\nplt.show()\n\nplt.title('Pressure')\nplt.hist(pressure[:,0].flatten(), bins=100)\nplt.show()\n\nplt.title('dPressure')\nplt.hist(delta_pressure[:,0].flatten(), bins=100)\nplt.show()\n\nplt.title('ddPressure')\nplt.hist(delta_delta_pressure[:,0].flatten(), bins=100)\nplt.show()","283993a5":"scalers = []\nfor col in tqdm(range(df.shape[-1])):\n    scalers.append(\n        RobustScaler().fit(df[:,:,col].reshape(-1,1))\n    )\n    \n    plt.title(str(col))\n    plt.hist(df[:,:,col].flatten(), bins=100)\n    plt.show()","5badc525":"# Featurizer","98af9f2e":"# Feature Inspection","755c4152":"# Methods","5b5549cc":"# Target Inspection","79110146":"< 2min. Not bad Kaggle. This is fast enough that we can actually perform augmentations on uin and ts and regenerate the dataset live......... but that's for another notebook.","caa3fd5c":"Some of these features look like they could benefit from `** 0.x` or even `np.log1p` type transformations (especially the last feature, which you really have to zoom into). Explore around! Also, when you wish to apply the features to a train or test, or a subset fold of train, leave `transform=True` to apply RobustScaler.","1e1385a9":"# Notes","1a68d995":"I was not able to discover the magic this time, as my data analysis skills (or lack thereof) still leave much to be desired. However, I was able to engineer a pretty sweet set of features. I think many of these were already publicly shared, but some of them are my own novel feature sets.\n\nI do all my FE using numpy. On my machine it takes about a minute to create the 40-some features. Let's see how long it takes Kaggle Kernels...","24da9f40":"# Run it "}}