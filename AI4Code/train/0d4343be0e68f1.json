{"cell_type":{"5050fcde":"code","30c2e244":"code","e08b3e6a":"code","9ded7d9c":"code","c3e036ca":"code","a28cd8ba":"code","ef0abc97":"markdown","8681231b":"markdown","4bcd7fcc":"markdown","6a4e14d1":"markdown","b5bd7198":"markdown","b40969c4":"markdown","59f83cbc":"markdown","f403ea8a":"markdown","1be2d0ff":"markdown","50aef7ed":"markdown","93394ee2":"markdown","b6fcb1ff":"markdown","56cb9792":"markdown","5edaf860":"markdown"},"source":{"5050fcde":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error","30c2e244":"class LinearRegression:\n    \n    def __init__(self, iterations=10000, learning_rate=0.1):\n        self.lr = learning_rate\n        self.iterations = iterations\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for iteration in range(self.iterations):\n            y_predicted = self.predict(X)\n            loss = y_predicted - y\n\n            # Gradients\n            dw = X.T @ loss\n            db = loss.mean(axis=0)\n            \n            dw = dw \/ n_samples\n\n            # Update weights\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n\n    def predict(self, X):\n        return X @ self.weights + self.bias","e08b3e6a":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nX_train, X_test, y_train, y_test = train_test_split(train_data[['LotArea']], train_data['SalePrice'], test_size=0.25)","9ded7d9c":"model = LinearRegression(iterations=10000, learning_rate=0.000000001)\nmodel.fit(X_train, y_train)\nmean_squared_log_error(y_test, model.predict(X_test))","c3e036ca":"class StandardScaler:\n    \n    def __init__(self):\n        self.max_value = None\n        self.min_value = None\n    \n    def fit(self, X):\n        self.max_value = X.max()\n        self.min_value = X.min()\n    \n    def transform(self, X):\n        return (X - self.min_value) \/ (self.max_value - self.min_value)\n    \n    def inverse_transform(self, X):\n        \"\"\"\n        Scale back the data to the original representation\n        \"\"\"\n        return (X * (self.max_value - self.min_value)) + self.min_value","a28cd8ba":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n\nmodel = LinearRegression(iterations=10000, learning_rate=0.1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nmean_squared_log_error(y_test, y_pred)","ef0abc97":"# What we are trying to prove \ud83e\udd47\n\nScaling data can have a great impact on the result. In the picture above you can see obviously that the path on the left is much longer than that on the right. The scaling is applied to the left to become the right one.\n\n\n<div style=\"width:100%;text-align: center;\">\n    <img src=\"https:\/\/miro.medium.com\/max\/600\/1*yi0VULDJmBfb1NaEikEciA.png\" width=\"600px\"\/> \n<\/div>","8681231b":"# Derivative with respect to $w_i$\n\n$$\\large\n\\frac{dMSE}{dw_i} = 2 \\cdot x_i \\cdot (bias + \\langle w, x \\rangle)\n$$  \n\nSince $y_{pred}$ is $bias + \\langle w, x \\rangle$ we can say that: \n\n$$\\large\n\\frac{dMSE}{dw_i} = 2 \\cdot x_i \\cdot (y_{pred} - y_{real})\n$$\n\nSince having or not having 2 does not matter for optimization we can ommit it for convinient use  \n\n$$\\large\n\\frac{dMSE}{dw_i} = x_i \\cdot (y_{pred} - y_{real})\n$$","4bcd7fcc":"# Linear Regression \ud83d\udcbb","6a4e14d1":"# Derivative with respect to $bias$\n\n$$\\large\n\\frac{dMSE}{dw_i} = 2 \\cdot (bias + \\langle w, x \\rangle)\n$$ \n\nSince $y_{pred}$ is $bias + \\langle w, x \\rangle$ we can say that:  \n\n$$\\large\n\\frac{dMSE}{dw_i} = 2 \\cdot (y_{pred} - y_{real})\n$$\n\nSince having or not having 2 does not matter for optimization we can ommit it for convinient use   \n\n$$\\large\n\\frac{dMSE}{dw_i} = y_{pred} - y_{real}\n$$","b5bd7198":"# Fit model with scaling features \ud83d\udcc8","b40969c4":"# Scaler class \ud83d\udcbf\n\nMachine learning algorithm that works on numbers does not know what that number represents. A weight of 10 grams and a price of 10 dollars are two different things, but for mode it is the just the same numbers. \n\nThe problem is that if 1 feature is much bigger than other feature, then the assumption algorithm makes that since first feature is bigger than second one, than it is more important.\n\nWe can solve this problem by getting all features in same not big range. Let's say $-1 \\leq x_i \\leq 1$, where $x_i$ is a feature\n\nThere are many scalers, but we will be using Min-Max scaler.\n\n$$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n\nMinMaxScaler scales all the data features in the range $[0, 1]$","59f83cbc":"# Load and split data \ud83d\udcf1\n\nIn case of simplicity we will be using only 1 feature - LotArea.","f403ea8a":"As we expected, better score for lower time!","1be2d0ff":"# Fit model without scaling features \ud83d\udcc9\n\nIn case that features, such as SalePrice and LotArea are quite big, we need to use extra small learning rate. It means that it will take so much time to find the local minimum! Let's see","50aef7ed":"# Preface \u231a\nThis notebook was initially created not for achiving the best public score, but for clear understanding of need in the scaling features.\n\nSklearn automatically scale out data when we are creating LinearRegression model, but why do we have need of it? Let's create our custom LinearRegression","93394ee2":"<h1 style='text-align: center'>Importance of scaling data<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist'> Code on GitHub! <\/a><\/p>","b6fcb1ff":"It took us around 1 minute and we only get around 0.7 mean squared log error. That's very bad.","56cb9792":"Still sklearn's Linear Regression has in-built feature scaling in Linear Regression Class we can not turn it off and see the difference, so we need to implement our own Linear Regression class!\n\n# Loss function\n\nWe will minimize mean squared root erorr which is sum of squared difference between model output $y_{pred}$ and actual value $y$:  \n\n$$\\large\ny_{pred} = bias + \\langle x, w \\rangle\n$$\n\n$\\text{}$\n\n$$\\large\nMSE = \\frac{1}{n}\\sum_{i=0}^n{(y_{pred} - y_{real})^2}\n$$\n\nLet's calculate gradients with respect to $w$ and $bias$\n\nsince $y_{pred}$ is $bias + \\langle w, x \\rangle$ we can say that:\n\n$$\\large\nMSE = \\frac{1}{l} \\sum_{i=0}^l {( bias + \\langle w, x \\rangle  - y_{real})^2}\n$$\n\nNow let's take the partition derivative of $w_i$ and $bias$","5edaf860":"# Materials\n\n* [Gradient Descent in Practice I - Feature Scalin](https:\/\/www.coursera.org\/learn\/machine-learning\/lecture\/xx3Da\/gradient-descent-in-practice-i-feature-scaling)\n* [All about Feature Scaling](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)"}}