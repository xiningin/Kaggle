{"cell_type":{"f915b1a0":"code","92f3be0a":"code","44a46920":"code","648aa171":"code","810e85a1":"code","27ef0922":"code","891093bf":"code","b53fac5b":"code","6b44874d":"code","cde84c33":"code","262e5211":"code","bc991275":"code","4d596db7":"code","4090757b":"code","9836ed47":"code","83b0c5b6":"code","9a4dcc7e":"code","e7a9cc24":"code","6f7bc030":"code","47c6ce06":"code","99daa020":"code","dbc08e1e":"code","73c0c67c":"code","8b1faeb2":"code","076d2cdf":"code","dd537758":"code","16488319":"code","a756d197":"code","b3d9658e":"code","fcfbc8ff":"code","15b8d2d2":"code","1bba6bb4":"code","7263f1c8":"code","9585aab9":"code","6d7182b8":"code","51d9ef0e":"code","3186f8f1":"code","021a0b31":"code","b8e1bfd0":"code","f57d5304":"code","7b476c44":"code","1e3e45ca":"code","99f02029":"code","749fd9d5":"code","2a871ad6":"code","feb8c18d":"code","328403c7":"code","11d618f2":"code","5c79dcb1":"code","4c2a34f0":"code","8c4d646a":"code","bcbb1bfd":"code","8b1ac62a":"code","f28f4786":"code","0ed18173":"code","b81e4e41":"code","681e6a7e":"code","6a316527":"code","29697ff2":"code","2d124c35":"code","927ef630":"code","8ddcde5b":"code","722c039c":"code","414f34ff":"code","2a130d2e":"code","e54cb682":"code","e7120200":"code","0184938d":"code","f32d4b81":"code","1e9de986":"code","f4b75035":"code","be4e869f":"code","33184deb":"code","79708bf1":"code","5e195d81":"code","89bcc86e":"code","5f3e4af8":"code","9cfd8cd1":"code","295a4db2":"code","71ca9ff3":"markdown","272aa4b8":"markdown","a15f0e43":"markdown","c5229616":"markdown","079c72bc":"markdown","e0d23b18":"markdown","e740d5c2":"markdown","d201873c":"markdown","6ebcc23f":"markdown","47cde39b":"markdown","c42b623f":"markdown","2eaf57c7":"markdown","a830a4f5":"markdown","c077b39f":"markdown","17a1520a":"markdown","cbed2e23":"markdown","67b99a3e":"markdown","bc12acd1":"markdown","b9aba4e1":"markdown","e7ce26b6":"markdown","e3e1714e":"markdown","d4aac3cf":"markdown","34b1e831":"markdown","4669b368":"markdown","3472ca12":"markdown","8b2bb2dd":"markdown","4d8d7c33":"markdown","0aac651a":"markdown","4ede8c2b":"markdown","4cb847e4":"markdown","0e6adfe6":"markdown","356e6d16":"markdown","1bd59ca8":"markdown","7f465c28":"markdown","0b364e23":"markdown","fb47aa78":"markdown","47c67336":"markdown","46eea87f":"markdown","6c5cb350":"markdown","482f0d00":"markdown","4384d02c":"markdown","84ad5e26":"markdown","29bfbc2e":"markdown","49335d5e":"markdown","524e312d":"markdown","1a6672b1":"markdown","e8641559":"markdown","7d85e46a":"markdown","cfa420db":"markdown","362d96ae":"markdown","b1b23ec8":"markdown","3d155375":"markdown","6570f88e":"markdown","b9a762e3":"markdown","3a2c1182":"markdown","21478f8b":"markdown","31c96069":"markdown","34598193":"markdown","177b2164":"markdown","a8892a6d":"markdown","b78e8af3":"markdown","33143c05":"markdown","c275b263":"markdown","f5b41791":"markdown","129d4556":"markdown","35a585a5":"markdown","a9c7b6e2":"markdown"},"source":{"f915b1a0":"# BEFORE RUNNING, uncomment to install extra packages and models \n\n\n! pip install flair\n\n! python -m spacy download en_core_web_md\n\n! pip install scispacy\n\n! pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_craft_md-0.2.4.tar.gz\n","92f3be0a":"\n\n# data manip\nimport json\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nfrom collections import Counter\nfrom ast import literal_eval\n\n# visuals\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode\nimport plotly.io as pio\nfrom spacy import displacy\nfrom tqdm import tqdm_notebook\nfrom termcolor import colored\n# from tsne import bh_sne\n\n\n\n\n# system nav\nimport os\nimport sys\nimport pickle\n\n# nlp\nfrom string import punctuation\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n#feature engineering discarded\n# from flair.embeddings import FlairEmbeddings, DocumentPoolEmbeddings, Sentence, BertEmbeddings\n\n# ML model\nimport torch\nfrom flair.embeddings import DocumentPoolEmbeddings, Sentence, WordEmbeddings\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.decomposition import PCA\n","44a46920":"NO_NER = True","648aa171":"# spacy models used\nEN_MODEL = \"en_core_web_md\"\nSCI_NER = \"en_ner_craft_md\"\n\n# BERTEmbeddings\nSCI_BERT = \"allenai\/scibert_scivocab_uncased\" # not used, for faster processing I've switched to glove static embeddings\n\n# paths to CORD data\nMETADATA = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\nDATA_DIR = \"\/kaggle\/input\/CORD-19-research-challenge\/\"\n\n# the list of 20 biomedical files and the directory to the manually labeled dataset\nBIOMED_DATA = \"\/kaggle\/input\/extra-data\/labeled_biomed_data.csv\"\nSOCIOETHICS_DATA_DIR = \"\/kaggle\/input\/arxiv-data\"\n\n# outdirectory for models and saved model input\nOUTDIR = \"\/kaggle\/working\/\"\nINDIR = \"\/kaggle\/input\/\"\nLIST_DF_DIR = \"list-dfs-3\"\nEXTRA_DATA = \"extra-data\"\n\n\n# we won't consider some of the numerical entity or person types of the generic pipeline\nUNWANTED_ENTS = [\"CARDINAL\", \"PERCENT\", \"PERSON\", \"DATE\", \"TIME\", \"MONEY\", \"ORDINAL\", \"QUANTITY\"]\n\n\n\n# if running from PC\nif not os.path.isdir(os.path.join(OUTDIR, LIST_DF_DIR)):\n    os.mkdir(os.path.join(OUTDIR, LIST_DF_DIR))\nif not os.path.isdir(os.path.join(OUTDIR, EXTRA_DATA)):\n    os.mkdir(EXTRA_DATA)","810e85a1":"print(len(os.listdir(os.path.join(INDIR, LIST_DF_DIR))))\nprint(os.listdir(os.path.join(INDIR, EXTRA_DATA)))","27ef0922":"if NO_NER:\n    with open(os.path.join(INDIR, EXTRA_DATA, \"bio_sci_ner.pkl\"), \"rb\") as f:\n        bio_sci_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"bio_gen_ner.pkl\"), \"rb\") as f:\n        bio_gen_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"ethics_sci_ner.pkl\"), \"rb\") as f:\n        ethics_sci_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"ethics_gen_ner.pkl\"), \"rb\") as f:\n        ethics_gen_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"ex_sci_ner.pkl\"), \"rb\") as f:\n        ex_sci_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"ex_gen_ner.pkl\"), \"rb\") as f:\n        ex_gen_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"df_ner.pkl\"), \"rb\") as f:\n        df_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"df_sci_ner.pkl\"), \"rb\") as f:\n        df_sci_ner = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"df_lemmas.pkl\"), \"rb\") as f:\n        df_lemmas = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"df_tok_counts.pkl\"), \"rb\") as f:\n        df_tok_counts = pickle.load(f)\n","891093bf":"if not NO_NER:\n    NLP = spacy.load(EN_MODEL)\n    NER = spacy.load(SCI_NER)","b53fac5b":"df_meta = pd.read_csv(METADATA)\ndf_meta[\"full_text_file\"].isna().sum()","6b44874d":"print(\"Shape before reduced data \", df_meta.shape)\ndf_meta = df_meta.dropna(subset = ['abstract', \"full_text_file\", \"sha\"])\n# df = df.loc[df['has_full_text'] == True]  # this seems to have been taken out of the original data\ndf_meta['publish_year'] = df_meta['publish_time'].apply(lambda x: x[:4] \n                                              if pd.isna(x) == False \n                                              else None)\ndf_meta = df_meta.loc[df_meta['publish_year'].isin(['2020', '2019'])]\nprint(\"Shape after reduced \", df_meta.shape)\n","cde84c33":"FULL_TEXT_ARTICLES = [os.path.join(DATA_DIR, indir, indir, 'pdf_json', split.strip())+'.json' \n                      for indir, sha in zip(df_meta['full_text_file'].to_list(), \n                                            df_meta['sha'].to_list()) \n                      for split in sha.split(';')]\npaths_not_exist = [x for x in FULL_TEXT_ARTICLES if not os.path.exists(x)]\nif len(paths_not_exist) == 0:\n    print('All of the total', \n          len(FULL_TEXT_ARTICLES), \n          'paths exist in the directory')\nelse:\n    print(len(paths_not_exist), \n          ' paths will be removed from total ', \n          len(FULL_TEXT_ARTICLES))\n    FULL_TEXT_ARTICLES = set(FULL_TEXT_ARTICLES) - set(paths_not_exist)\nprint('First ten article paths:')\nFULL_TEXT_ARTICLES[:10]","262e5211":"biomed_df = pd.read_csv(BIOMED_DATA, sep = ',')\nBIOMED_ARTICLES = biomed_df['path'].to_list()\nfor title in biomed_df.title.to_list():\n    print(title)\nBIOMED_ARTICLES","bc991275":"SOCIOETHICS_ARTICLES = [os.path.join(SOCIOETHICS_DATA_DIR, path) for path in os.listdir(SOCIOETHICS_DATA_DIR) if path.endswith('.txt')]\n\nSOCIOETHICS_ARTICLES","4d596db7":"df_journals = df_meta['journal'].dropna()\njournal_counts = Counter(df_journals).most_common(20)\nlabels = [x[0] for x in journal_counts]\ndf_journals_pieplot = pd.DataFrame(data= [{\"journal\": journal, \"count\": count} for journal, count in journal_counts], index = labels)\ndf_journals_pieplot.plot.pie(y = 'count', figsize = (15,15))\ndf_journals.describe()","4090757b":"# get spacy doc for all abstracts in metadata subset\nif not NO_NER:\n    df_meta['abstract_spacy_doc'] = df_meta['abstract'].apply(lambda x: NLP(x) if pd.isna(x) == False else None)","9836ed47":"if not NO_NER:\n    df_meta['tok_count'] = df_meta['abstract_spacy_doc'].apply(lambda x: len(x))\n    df_tok_counts = df_meta['tok_count'].to_list()\n\nsns.distplot( df_tok_counts)\n","83b0c5b6":"if not NO_NER:\n    df_meta['lemmas'] =  df_meta['abstract_spacy_doc'].apply(lambda x: [_.lemma_ for _ in x if _.lemma_ not in STOP_WORDS and _.text not in punctuation])\n    all_lemmas = Counter([x for k, v in df['lemmas'].iteritems() for x in v])\n    df_lemmas = pd.DataFrame(data = [{\"token\" : tok, \"count\" : count} for tok, count in all_lemmas.most_common(20)])\ndf_lemmas\n","9a4dcc7e":"#taking a look at normal spaCy pipeline NER\n\nif not NO_NER:\n    df_meta['ner'] = df_meta['abstract_spacy_doc'].apply(lambda x: [(_.text, _.label_) for _ in x.ents if _.label_ not in UNWANTED_ENTS])\n    all_ner = Counter([x for k, v in df['ner'].iteritems() for x in v])\n    df_ner = pd.DataFrame(data = [{\"Entity\" : tok[0], \"Type\": tok[1], \"Count\" : count} for tok, count in all_ner.most_common(20)])\ndf_ner","e7a9cc24":"# now let's try sciSpaCy NER\nif not NO_NER:\n    df_meta['sci_ner'] = df_meta['abstract'].apply(lambda x : [(_.text, _.label_) for _ in NER(x).ents] )","6f7bc030":"if not NO_NER:\n    sci_ner = Counter([x for k, v in df['sci_ner'].iteritems() for x in v])\n    df_sci_ner = pd.DataFrame(data = [{\"Entity\" : tok[0], \"Type\" : tok[1], \"Count\" : count} for tok, count in sci_ner.most_common(20)])\ndf_sci_ner","47c6ce06":"labeled_data_extract = \"\"\"In the age of social media, disasters and epidemics usher not only a devastation and affliction in the physical world, but also prompt a deluge of information, opinions, prognoses and advice to billions of internet users. The coronavirus epidemic of 2019-2020, or COVID-19, is no exception, with the World Health Organization warning of a possible \"infodemic\" of fake news. In this study, we examine the alternative narratives around the coronavirus outbreak through advertisements promoted on Facebook, the largest social media platform in the US. Using the new Facebook Ads Library, we discover advertisers from public health and non-profit sectors, alongside those from news media, politics, and business, incorporating coronavirus into their messaging and agenda. We find the virus used in political attacks, donation solicitations, business promotion, stock market advice, and animal rights campaigning. Among these, we find several instances of possible misinformation, ranging from bioweapons conspiracy theories to unverifiable claims by politicians. As we make the dataset available to the community, we hope the advertising domain will become an important part of quality control for public health communication and public discourse in general.\"\"\"\n\nif not NO_NER:\n    ex_gen_ner = NLP(labeled_data_extract)\ndisplacy.render(ex_gen_ner, style='ent')\n","99daa020":"if not NO_NER:\n    ex_sci_ner = NER(labeled_data_extract)\ndisplacy.render(ex_sci_ner, style='ent')","dbc08e1e":"def process_bibs(all_jsons, t):\n    \"\"\"Get indexed dictionary of set of bib_refs per article and index map to reference title strig\n    \"\"\"\n    all_bib_set = set()\n    titles = set()\n    article_bib_set = dict()\n    counts = dict()\n    title2path = dict()\n    # get set of title strings, bib refs and the dictionary of bib ref (string) per article \n    for file in all_jsons:\n        with open(file) as f:\n            article = json.load(f)\n            bibs = article['bib_entries']\n            title = article['metadata']['title'].lower()\n            if title not in title2path:\n                title2path[title] = [file]\n            else:\n                title2path[title].append(file)\n            titles.add(title)\n            article_bib_set[title] = set()\n            for entry in bibs:\n                article_bib_set[article['metadata']['title'].lower()].add(bibs[entry]['title'].lower())\n                all_bib_set.add(bibs[entry]['title'].lower())\n                if bibs[entry]['title'].lower() not in counts:\n                    counts[bibs[entry]['title'].lower()] = 1\n                else:\n                    counts[bibs[entry]['title'].lower()] +=1\n    #first index CORD article titles                           \n    bib2id = {bib:idx for idx, bib in enumerate(titles)}\n    # to index bib refs, first subtract titles from bib refs\n    all_bib_set = all_bib_set-titles\n    # add indexed bib refs\n    bib2id.update({bib:idx+len(titles) for idx, bib in enumerate(all_bib_set)})\n    # index the dictionary of sets of bib refs\n    indexed_article_bib_set = {bib2id[k]:{bib2id[x] for x in v if counts[x]>t} for k, v in article_bib_set.items() }\n    # return inverse index map for the interactive plot legend\n    id2bib = {v:k for k,v in bib2id.items()}\n    return indexed_article_bib_set, id2bib, title2path\n\n\ndef get_edges(bibset):\n    edges = []\n    for i, vs in bibset.items():\n        for j, vals in bibset.items():\n            if i in vals:\n                edges.append((j, i)) #be sure that directed graph points towards article being referenced\n    return edges\n\n\n# this code is from tutorial https:\/\/plotly.com\/ipython-notebooks\/network-graphs\/\ndef plot_interactive_graph(G, pos, id2bib, renderer = \"browser\"):\n    edge_x = []\n    edge_y = []\n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.append(x0)\n        edge_x.append(x1)\n        edge_x.append(None)\n        edge_y.append(y0)\n        edge_y.append(y1)\n        edge_y.append(None)\n\n    edge_trace = go.Scatter(\n        x=edge_x, y=edge_y,\n        line=dict(width=0.5, color='#888'),\n        hoverinfo='none',\n        mode='lines')\n\n    node_x = []\n    node_y = []\n    node_text = []\n    for node in G.nodes():\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)\n        node_text.append(id2bib[node])\n        \n    node_trace = go.Scatter(\n        x=node_x, y=node_y,\n        mode='markers',\n        hoverinfo='text',\n        marker=dict(\n            showscale=True,\n            colorscale='Viridis',\n            reversescale=True,\n            color=[],\n            size=10,\n            colorbar=dict(\n                thickness=15,\n                title='Bib References',\n                xanchor='left',\n                titleside='right'\n            ),\n            line_width=2))\n\n    node_adjacencies = []\n    for node, adjacencies in enumerate(G.adjacency()):\n        node_adjacencies.append(len(adjacencies[1]))\n\n    node_trace.marker.color = node_adjacencies\n    node_trace.text = node_text\n    if renderer == \"notebook_mode\":\n        init_notebook_mode(connected=True)\n    if renderer == 'browser':\n        pio.renderers.default = \"browser\"\n\n    fig = go.Figure(data=[edge_trace, node_trace],\n             layout=go.Layout(\n                title='<br>Bibliographic references',\n                titlefont_size=16,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    text=\"Python code: <a href='https:\/\/plotly.com\/ipython-notebooks\/network-graphs\/'> plotly.com <\/a>\",\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002 ) ],\n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\n    fig.show()\n    \ndef get_top_edges(edges, id2bib, title2path, all_jsons, top_k = 30):\n    # we get the set of ids which are also in our reduced corpus\n    title_ids = [idx for idx in id2bib if id2bib[idx] in title2path]\n    # get only ends of nodes as these are articles being refered TO\n    total_counts = [x[1] for x in edges if x[1] in title_ids]\n    # then get counts in our bib graph and return top k\n    counter = Counter(total_counts)\n#     print(counter.most_common(top_k))\n    return [(id2bib[x[0]],title2path[id2bib[x[0]]]) for x in counter.most_common(top_k)]\n        \n","73c0c67c":"# set threshold for number of occurences in within the bib references\n# I'll set it to 6 to have a simpler looking plot\nt = 6\n\n# if this is run from offline, set renderer kwarg as \"browser\"\n# if run on kaggle, set renderer kwarg as \"notebook_mode\"\nrenderer = \"notebook_mode\"\n\nbibset, id2bib, title2path = process_bibs(FULL_TEXT_ARTICLES, t)\nedges = get_edges(bibset)\n# use the networkx package to instanciate a plot and get positions\nG = nx.Graph()\nG.add_edges_from(edges)\npos=nx.spring_layout(G)\n\n#plot interactive graph via plotly package\nplot_interactive_graph(G, pos, id2bib, renderer = renderer)","8b1faeb2":"\ntop_k = get_top_edges(edges, id2bib, title2path, FULL_TEXT_ARTICLES, top_k = 30)\n# top_k is a list of tuples of the title, and list of paths assiciated\n# as some articles have multiple paths indicated in metadata\ntop_k\n","076d2cdf":"# define  a function to easily recover and read any article\n\ndef get_paragraphs(file, read_only = True):\n    # this method is used to recover and read top_k referenced files \n    # saved as a list of tuples --> [('title':['path1,', 'path2'])]\n    # path is accesses as such\n    if read_only:\n        path = file[1][0]\n    # if not used to read, the function takes just a path\n    if not read_only:\n        path = file\n    text = []\n    with open(path) as f:\n        article = json.load(f)\n        text.append(article['metadata']['title'])\n        for para in article['abstract']:\n            text.append(para['text'])\n        for para in article['body_text']:\n            text.append(para['text'])\n    if read_only == True:\n        for para in text:\n            print(para, '\\n')\n    if read_only == False:\n        return text","dd537758":"\n# get_paragraphs(top_k[2], read_only = True)","16488319":"# ATTENTION this list might be different from the current data . It is based on the data I have downloaded,\n# the paths and titles I've saved in the file directory BIOMED_DATA = \"\/kaggle\/input\/extradata2\/labeld_biomed_data.csv\"\n\n# 25 articles which are purely biomedical talk\nbiomedical_articles = [top_k[0], \n                       top_k[2],\n                       top_k[4],\n                       top_k[5],\n                       top_k[6],\n                       top_k[7],\n                       top_k[8],\n                       top_k[9],\n                       top_k[10],\n                       top_k[13],\n                       top_k[14],\n                       top_k[15],\n                       top_k[16],\n                       top_k[17],\n                       top_k[18],\n                       top_k[19],\n                       top_k[20],\n                       top_k[21],\n                       top_k[22],\n                       top_k[23],\n                       top_k[24],\n                       top_k[26],\n                       top_k[27],\n                       top_k[28],\n                       top_k[29]\n                      ]\nimport pprint\n\npp = pprint.PrettyPrinter( indent = 4 )\nprint(' total number of articles : ', len(biomedical_articles))\npp.pprint(biomedical_articles)","a756d197":"# DONT RUN UNLESS ARTICLES READ \/ SELECTED . \n\n# THIS IS ALREADY DONE ON DOWNLOADED DATA AND INDICES WILL BE DIFFERE\n\n# df_biomed_paths = pd.DataFrame(data = [{\"title\" :x[0], \"path\":x[1][0]} for x in biomedical_articles])\n# df_biomed_paths.to_csv(os.path.join(OUTDIR, \"labeled_biomed_data.csv\"), sep = ',')","b3d9658e":"# to be looked at for further annotation after first roundof experiments\narticles_containing_socioethical_talk = [\n    # But as shown in this study, it is still crucial to isolate \n    # patients and trace and quarantine contacts as early as possible \n    top_k[1], \n    # further international seeding and subsequent local establishment of epidemics might become inevitable\n    top_k[3],\n    # heightened public awareness and impressively strong interventional...\n    top_k[11],\n    # quarantine and integrated interventions will have a major impact on its future trend\n    top_k[12],\n    # importance for healthcare settings and for travelers\n    top_k[25],\n]","fcfbc8ff":"\n\ndef get_paragraphs_(path):\n    neg_labeled = []\n    pos_labeled = []\n    with open(path, 'r', encoding = 'utf-8') as f:\n        article = literal_eval(f.read())\n        pos_labeled.append(article['metadata']['abstract'][0]['text'])\n        pos_labeled.extend([x['text'] for x in article['metadata']['body_text'] if x['label']])\n        neg_labeled.extend([x['text'] for x in article['metadata']['body_text'] if not x['label']])\n    return pos_labeled, neg_labeled","15b8d2d2":"pos_text, neg_text = get_paragraphs_(\"\/kaggle\/input\/arxiv-data\/arxiv01.txt\")\nprint('Example of the first three paragraphs labeled text:')\npos_text[:3]","1bba6bb4":"def get_entity_dfs(articles_paragraphs):\n    ents_sci = [ent.text  for article in articles_paragraphs for para in article for ent in NER(para).ents]\n    counts_sci = Counter(ents_sci)\n    sci_ner = pd.DataFrame(data = [{\"entity\": ent, \"count\":count} for ent, count in counts_sci.most_common(15) ])\n    ents_gen = [ent.text for article in articles_paragraphs for para in article for ent in NLP(para).ents if ent.label_ not in UNWANTED_ENTS]\n    counts_gen = Counter(ents_gen)\n    gen_ner = pd.DataFrame(data= [{\"entity\": ent, \"count\":count} for ent, count in counts_gen.most_common(15) ])\n    return sci_ner, gen_ner","7263f1c8":"\nethics_articles = [get_paragraphs_(path)[0] for path in SOCIOETHICS_ARTICLES]","9585aab9":"if not NO_NER:\n    ethics_sci_ner, ethics_gen_ner = get_entity_dfs(ethics_articles)","6d7182b8":"ethics_sci_ner","51d9ef0e":"ethics_gen_ner ","3186f8f1":"if not NO_NER:\n    biomed_articles = [get_paragraphs(path, read_only = False) for path in BIOMED_ARTICLES]\n\n    bio_sci_ner, bio_gen_ner = get_entity_dfs(biomed_articles)","021a0b31":"bio_sci_ner","b8e1bfd0":"bio_gen_ner","f57d5304":"### the following code was used to save the dataframes used for demo\n\nif not NO_NER:\n    dfs_to_save = {\"bio_sci_ner.pkl\":bio_sci_ner, \n                   \"bio_gen_ner.pkl\": bio_gen_ner, \n                   \"ethics_sci_ner.pkl\":ethics_sci_ner, \n                   \"ethics_gen_ner.pkl\":ethics_gen_ner, \n                   \"ex_sci_ner.pkl\": ex_sci_ner, # these are spacy docs\n                   \"ex_gen_ner.pkl\": ex_gen_ner, # these are spacy docs\n                   \"df_ner.pkl\" : df_ner, \n                   \"df_sci_ner.pkl\":df_sci_ner, \n                   \"df_lemmas.pkl\":df_lemmas,\n                   \"df_tok_counts.pkl\":df_tok_counts  # this is actually a list of ints...\n                  }\n    def save_pickle(fn, obj):\n        with open(os.path.join(OUTDIR, EXTRA_DATA, fn), \"wb\") as f:\n            pickle.dump(obj, f)\n\n\n    for fn, obj in dfs_to_save.items():\n        save_pickle(fn, obj)","7b476c44":"def get_row_data(path):\n    \"\"\"return list of dictionaries one per paragraph with doc index, text, and doc path\n    \n    doc_index is the index within the document that the paragraph falls. \n    Title is doc index 0\n    abstract paragraphs start at doc index 1 and paragraphs extend this indexation. \n    \n    \"\"\"\n    row_data = []\n    count = 0\n    with open(path) as f:\n        article = json.load(f)\n        paragraph = dict()\n        paragraph[\"text\"] = article['metadata']['title']\n        paragraph[\"doc_index\"] = count\n        paragraph[\"path\"] = path\n        row_data.append(paragraph)\n        count +=1\n        for para in article['abstract']:\n            paragraph = dict()\n            paragraph[\"text\"] = para['text']\n            paragraph[\"doc_index\"] = count\n            paragraph[\"path\"] = path\n            row_data.append(paragraph)\n            count += 1\n        for para in article['body_text']:\n            paragraph = dict()\n            paragraph[\"text\"] = para['text']\n            paragraph[\"doc_index\"] = count\n            paragraph[\"path\"] = path\n            row_data.append(paragraph)\n            count += 1\n    return row_data\n\n\ndef get_unlabeled_df(paths):\n    all_data = []\n    for path in paths:\n        row_data = get_row_data(path)\n        all_data.extend(row_data)\n    df = pd.DataFrame(data = all_data)\n    df = df.sample(frac=1) # we don't reset the index \n    return df\n\n\ndef split_unlabeled(unlabeled_df, num_split = 10):\n    unlabeled_df\n    seq_len = round(len(unlabeled_df)\/num_split)\n    list_dfs = []\n    offset = 0\n    for _ in range(num_split):\n        if _ == num_split-1:\n            list_dfs.append(unlabeled_df.iloc[offset:]) ## .reset_index()# witholding reset index\n        else:\n            popped_df = unlabeled_df.iloc[offset:offset+seq_len, :]  #.reset_index()  #holding aside reset index\n            list_dfs.append(popped_df)\n            offset += seq_len\n    return list_dfs\n\ndef get_sci_ents(document):\n    sci_ents = [ent.text for ent in NER(document).ents]\n    # not sure if it's a great idea to do this but it seems the best mitigation\n    # we need to have vectors of the same length, and worst case this will just pull\n    # together texts in final feature vector space which have no interesting entities\n    if not sci_ents:\n        return  None\n    return sci_ents\n    \n    \ndef get_gen_ents(document):   \n    gen_ents = [ent.text for ent in NLP(document).ents if ent.label_ not in UNWANTED_ENTS]\n    if not gen_ents:\n        return None\n    return gen_ents\n\ndef get_train_data(bio_paths, ethics_paths):\n    neg_labeled = []\n    pos_labeled = []\n    for path in bio_paths:\n        neg = get_paragraphs(path, read_only = False)\n        neg_labeled.extend(neg)\n    for path in ethics_paths:\n        pos, neg = get_paragraphs_(path) ## function defined in 1.2.6\n        pos_labeled.extend(pos)\n        neg_labeled.extend(neg)\n    len_pos = len(pos_labeled)\n    len_neg = len(neg_labeled)\n    # here we return the entitites for the labeled train data \n    # data structure is a list of tupples of 2 lists of entities\n    # one list for scispacy entities and one list of general entities\n    x_train = pos_labeled\n    x_train.extend(neg_labeled)\n    y_train = [1 for _ in range(len_pos)]\n    y_train.extend([0 for _ in range(len_neg)])\n    return x_train, y_train\n","1e3e45ca":"if not NO_NER:\n    unlabeled_paths = set(FULL_TEXT_ARTICLES) - set(BIOMED_ARTICLES)\n    print('Total number of unlabeled articles : ', len(unlabeled_paths))\n\n\n\n    selected = sample(list(unlabeled_paths), 15)\n\n    unlabeled_paths = unlabeled_paths - set(selected)\n\n    UNLABELED_DF = get_unlabeled_df(unlabeled_paths)\n    DF_FOR_TEST = get_unlabeled_df(selected)\n    LEN_UNLABELED = len(UNLABELED_DF)\n    print('Total number of paragraphs in unlabeled data : ', LEN_UNLABELED)","99f02029":"if not NO_NER:\n    x_train, y_train = get_train_data(BIOMED_ARTICLES, SOCIOETHICS_ARTICLES)\n    # convert to numpy array and save y_train data\n    y_train = np.array(y_train)\n    np.save(os.path.join(OUTDIR, EXTRA_DATA, 'y_train.npy'), y_train)\n    # instanciate global variable length to retrieve after vstack with unlabeled + PCA\n    LEN_X_TRAIN = len(x_train)\n    print(\"Total number of paragraphs in labeled train data : \", LEN_X_TRAIN)\nif NO_NER:\n    y_train = np.load(os.path.join(INDIR, EXTRA_DATA, 'y_train.npy'))","749fd9d5":"if not NO_NER:\n    x_train_sci = [get_sci_ents(text) for text in x_train]\n    x_train_gen = [get_gen_ents(text) for text in x_train]","2a871ad6":"if not NO_NER:\n    with open(os.path.join(OUTDIR, EXTRA_DATA, \"x_train_sci.pkl\"), \"wb\") as f:\n        pickle.dump(x_train_sci, f)\n    with open(os.path.join(OUTDIR, EXTRA_DATA, \"x_train_gen.pkl\"), \"wb\") as f:\n        pickle.dump(x_train_gen, f)","feb8c18d":"# define the number of times we would like to split the data\nif not NO_NER:\n    num_split = 100\n\n    LIST_DFS = split_unlabeled(UNLABELED_DF, num_split = num_split)","328403c7":"def get_string_int(i):\n    i += 1\n    if i in list(range(10)):\n        return '00'+str(i)\n    if i in [i+10 for i in range(90)]:\n        return '0'+str(i)\n    if i >= 100:\n        return str(i)\n\n\nif not NO_NER:\n    for i, df in enumerate(LIST_DFS):\n        print(\"processing ulabeled dataframe \", i+1, '\\n', \"=\"*10)\n        df[\"sci_ner\"] = df['text'].apply(get_sci_ents)\n        df[\"gen_ner\"] = df['text'].apply(get_gen_ents)\n        df = df.dropna(subset = ['sci_ner', 'gen_ner'])\n        string_int = get_string_int(i)\n        df.to_csv(ox.path.join(OUTDIR, LIST_DF_DIR, '{}_unlabeled_ner.csv'.format(string_int)), sep = ',')","11d618f2":"if not NO_NER:\n    DF_FOR_TEST[\"sci_ner\"] = DF_FOR_TEST['text'].apply(get_sci_ents)\n    DF_FOR_TEST[\"gen_ner\"] = DF_FOR_TEST['text'].apply(get_gen_ents)\n    DF_FOR_TEST.to_csv(os.path.join(OUTDIR, LIST_DF_DIR, 'DF_FOR_TEST_ner.csv'), sep = ',')","5c79dcb1":"\n    ## discarded document embedding models for general entities and scientific entities\n    ## this configuration is ideal in terms of domain adaptation, but was not running properly on kaggle\n# sci_embeddings = BertEmbeddings(SCI_BERT)\n# gen_embeddings = BertEmbeddings()# default goes to bert-large-uncased\n\n# flair_embedding_forward = FlairEmbeddings('news-forward')\n# flair_embedding_backward = FlairEmbeddings('news-backward')\n\n\n# SCI_DOC_EMBED = DocumentPoolEmbeddings([sci_embeddings, flair_embedding_backward, flair_embedding_forward])\n# GEN_DOC_EMBED = DocumentPoolEmbeddings([gen_embeddings, flair_embedding_backward, flair_embedding_forward])\n\n\n\n#     document pooling operation to create a document level embedding\nembeddings = WordEmbeddings('glove') \nDOC_EMBED = DocumentPoolEmbeddings([embeddings], fine_tune_mode='nonlinear')\n\n## define the PCA model  ## for this version, we use PCA for 2d visualisation, setting components to 2 \nPCA_MODEL = PCA(n_components = 2)\n\n\n\n\n","4c2a34f0":"def embed_doc(document, doc_embed_model):\n    sentence = ' '.join(document)\n    sentence = Sentence(sentence)\n    doc_embed_model.embed(sentence)\n    embedding = sentence.get_embedding()\n    return embedding.cpu().numpy()\n\n    \ndef pca_doc(pca_model, all_embed):\n    X = pca_model.fit_transform(all_embed)\n    return pca_model, X","8c4d646a":"if NO_NER:\n    with open(os.path.join(INDIR, EXTRA_DATA, \"x_train_sci\"), \"rb\") as f:\n        x_train_sci = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"x_train_gen\"), \"rb\") as f:\n        x_train_gen = pickle.load(f)","bcbb1bfd":"def drop_x_train_vals(x_sci, x_gen, y_train):\n    x_train_sci = []\n    x_train_gen = []\n    y_train_ = []\n    for s, g, y in zip(x_sci, x_gen, y_train):\n        if s and g :\n            x_train_sci.append(s)\n            x_train_gen.append(g)\n            y_train_.append(y)\n    return x_train_sci, x_train_gen, y_train_\n\nx_train_sci, x_train_gen, y_train = drop_x_train_vals(x_train_sci, x_train_gen, y_train)","8b1ac62a":"LEN_X_TRAIN = len(x_train_sci)\nprint(\"length train data after reduce : \", len(y_train))","f28f4786":"with torch.no_grad():\n    x_train_sci_embed = [embed_doc(ents, DOC_EMBED) for ents in x_train_sci if ents]","0ed18173":"with torch.no_grad():\n        x_train_gen_embed = [embed_doc(ents, DOC_EMBED) for ents in x_train_gen if ents]","b81e4e41":"np.save(os.path.join(OUTDIR, EXTRA_DATA, \"x_train_sci_embed.pkl\"), np.array(x_train_sci_embed))\nnp.save(os.path.join(OUTDIR, EXTRA_DATA, \"x_train_gen_embed.pkl\"), np.array(x_train_gen_embed))","681e6a7e":"if NO_NER:\n    LIST_DFS = [pd.read_csv(os.path.join(INDIR, LIST_DF_DIR, path)) \n                for path in sorted(os.listdir(os.path.join(INDIR, LIST_DF_DIR))) \n                if path.endswith('.csv')\n                and not path.startswith('DF')]\n# reset the lengths of dfs to \nLENGTHS = [len(df) for df in LIST_DFS]\nLENGTHS","6a316527":"for i, df in enumerate(LIST_DFS):\n    print(\"processing ulabeled dataframe {}\/{}  \".format(i+1,len(LIST_DFS)), '\\n', \"=\"*30, '\\n')\n    with torch.no_grad(): # necessary to be able to save as numpy array as well as not take up GPU memory, as the doc pooling does not need bookkeeping\n        sci_ner_embed = [embed_doc(x, DOC_EMBED) if x else np.array([]) for x in df[\"sci_ner\"].to_list() ]\n        gen_ner_embed = [embed_doc(x, DOC_EMBED) if x else np.array([]) for x in df[\"gen_ner\"].to_list()]\n    string_int = get_string_int(i)\n    sci_ner_embed = np.array(sci_ner_embed)\n    np.save(os.path.join(OUTDIR, LIST_DF_DIR, '{}unlabeled_sci_embed.npy'.format(string_int)), sci_ner_embed) \n    np.save(os.path.join(OUTDIR, LIST_DF_DIR, '{}unlabeled_gen_embed.npy'.format(string_int)), gen_ner_embed)\n    del sci_ner_embed\n    del gen_ner_embed\n    torch.cuda.empty_cache()","29697ff2":"sci_embeddings = [os.path.join(OUTDIR, LIST_DF_DIR, path) for path in sorted(os.listdir(os.path.join(OUTDIR, LIST_DF_DIR))) if path.endswith(\"sci_embed.npy\")]\ngen_embeddings = [os.path.join(OUTDIR, LIST_DF_DIR, path) for path in sorted(os.listdir(os.path.join(OUTDIR, LIST_DF_DIR))) if path.endswith(\"gen_embed.npy\")]","2d124c35":"\nall_sci = []\nall_gen = []\nfor sci_embed, gen_embed in zip(sci_embeddings, gen_embeddings):\n    sci_embed_ = np.load(sci_embed)\n    gen_embed_ = np.load(gen_embed)\n    all_sci.extend(sci_embed_)\n    all_gen.extend(gen_embed_)\nEMBEDDED_UNLABELED = np.hstack((np.array(all_sci), np.array(all_gen)))\nall_sci.extend(x_train_sci_embed)\nall_gen.extend(x_train_gen_embed)\n\nall_sci = np.array(all_sci)\nprint(all_sci.shape)\nall_gen = np.array(all_gen)\nprint(all_gen.shape)\n\nall_embed = np.hstack((all_sci, all_gen))               \nnp.save(os.path.join(OUTDIR, EXTRA_DATA, 'all_embed.npy'), all_embed)\n","927ef630":"# for now only using PCA for plot, it would have been interesting otherwise to PCA output from Flair contextual pooled embeddings, as dimensions are very high --> ~4k\nPCA_MODEL_, plot_points = pca_doc(PCA_MODEL, all_embed)","8ddcde5b":"def plot_pca(X, y_train):\n    x = [_[0] for _ in X[sum(LENGTHS):]]\n    y = [_[1] for _ in X[sum(LENGTHS):]]\n    sns.scatterplot(x = x, y= y, hue = y_train, alpha = .5)    \n    ","722c039c":"plot_pca(plot_points, y_train)","414f34ff":"\nX = list(all_embed)\n\nif len(X) != LEN_X_TRAIN+sum(LENGTHS):\n    print(\"dimension difference\", len(X), \"is unequal to \", LEN_X_TRAIN+sum(LENGTHS))\n\n\nX_train = X[sum(LENGTHS):]\nX_train = np.array(X_train)\nnp.save(os.path.join(OUTDIR, 'X_train.npy'), X_train)\n","2a130d2e":"# liblinear solver is good for small datasets according to documentation\n\n# f1 scoring in order to get a better idea of if our model \n\n# class_weight should be set at values representative of the data, but for now this is the only solution to get model to not only predict one of two classes\n\n\nml_model = LogisticRegressionCV(cv = 5, max_iter = 100, solver = 'liblinear', class_weight = {0:0.5, 1:0.5}, scoring = 'f1')","e54cb682":"def semi_sup(model, x_train, y_train, embedded_unlabeled, batch_size = 200, threshold = .65):\n    \"\"\"Semisupervised training\n    \n    :param model: the sklearn compatible machine learning model used\n    :param x_train: the labeled train input, list or array of np arrays (features)\n    :param y_trian: the train labels np array of labels\n    :param x_test: the labeled test input, list or array of np arrays (features)\n    :param embedded_unlabeled: all unlabled feature vectors\n    :param batch_size: number of times to split the data\n    :param threshold: the threshold above which we decide to keep a predicted label as a pseudo label\n    \n    :return:\n        best semi supervised ML model\n        prints of scores\n    \n    \"\"\"\n    \n    stacked = False\n    num_batches = round(len(embedded_unlabeled)\/batch_size)\n    split_embedded_unlabeled = [embedded_unlabeled[i*batch_size : i*batch_size+batch_size] for i in range(num_batches)]\n    original_x_train = x_train\n    original_y_train = y_train\n    f1_scores = []\n    models_to_evaluate = []\n    for num_iter, unlabeled_x in enumerate(split_embedded_unlabeled):\n        # we have already droped Nones so to get the input data we just need to convert to np array\n        model.fit(x_train, y_train)\n        preds = model.predict(unlabeled_x)\n        probas = model.predict_proba(unlabeled_x)\n        pseudo_labeled_x = []\n        pseudo_labeled_y = []\n        for i, (pred, proba) in enumerate(zip(preds, probas)):\n            # getting the probability is just the locating in the probas the index of the prediction \n            if proba[pred] > threshold:\n                pseudo_labeled_x.append(unlabeled_x[i])\n                pseudo_labeled_y.append(pred)\n        x_train = np.vstack((x_train, np.array(pseudo_labeled_x)))\n        y_train = np.hstack((y_train, np.array(pseudo_labeled_y)))\n        # here's something perhaps a bit uncouth... I keep pseudo labels as train data for final evaluation\n        # considering that there is not enough labeled data to withold for a test set...\n        if stacked == False:\n            if len(pseudo_labeled_y) > 1:\n                pseudo_x_train_for_eval = np.array(pseudo_labeled_x)\n                pseudo_y_train_for_eval = np.array(pseudo_labeled_y)\n                stacked = True\n        if stacked == True:\n            pseudo_x_train_for_eval = np.vstack((pseudo_x_train_for_eval, np.array(pseudo_labeled_x)))\n            pseudo_y_train_for_eval = np.hstack((pseudo_y_train_for_eval, np.array(pseudo_labeled_y)))\n        model_to_evaluate = model\n        if num_iter % 2 == 0:\n            print(\" iteration # : \", num_iter+1)\n            print(\"size of batch : \", unlabeled_x.shape)\n            print(\"train size : \", x_train.shape)\n            if len([x for x in pseudo_y_train_for_eval if x  == 1]) > 10 and len([x for x in pseudo_y_train_for_eval if x == 0]) > 10:\n                score = evaluate(model, pseudo_x_train_for_eval, pseudo_y_train_for_eval, original_x_train, original_y_train, num_iter)\n                f1_scores.append(score)\n                models_to_evaluate.append(model_to_evaluate)\n                best_score = 0\n                score_decrease = 0\n                for score, model_to_eval in zip(f1_scores, models_to_evaluate):\n                    if score > best_score:\n                        best_score = score\n                        best_model = model_to_eval\n                        score_decrease = 0\n                    else:\n                        score_decrease += best_score - score\n                if score_decrease > .15:\n                    print(\"training stopped, cumulative score decrease greater than 15 % : \", score_decrease)\n                    return best_model\n                if num_iter > 9 and max(f1_scores) in f1_scores[:int(num_iter\/2)] :\n                    print(\"training stopped, max score is in first half iterations  \", num_iter+1, \"iterations\")\n                    return best_model\n            else:\n                print(pseudo_y_train_for_eval)\n                print(\"Not enough predictions in one of the two classes for eval, set threshold to at most .60\")\n    return best_model\n\n              \n              \ndef evaluate(model, pseudo_x, pseudo_y, x_train, y_train, num_iter):\n    model.fit(pseudo_x, pseudo_y)         \n    y_pred = model.predict(x_train)\n    f1 = f1_score(y_train, y_pred, average = 'binary', zero_division = 0 )\n    print('F1 measure after ', num_iter+1, 'iterations : ', f1 )\n    return f1","e7120200":"\ndef get_prediction_probabilities(unlabeled_x, x_train, y_train):\n#     unlabeled_x = np.array(df[\"doc_embed\"].tolist(), dtype=float)\n    unlabeled_index = {i:idx for i, idx in enumerate(df.index.to_list())}\n    ml_model.fit(x_train, y_train)\n    preds = ml_model.predict(unlabeled_x)\n    probas = ml_model.predict_proba(unlabeled_x)\n    proba_dist = []\n    proba_dist_pos = []\n    proba_dist_neg = []\n    for pred, proba in zip(preds, probas):\n        proba_dist.append(proba[pred])\n        if pred:\n            proba_dist_pos.append(proba[pred])\n        if not pred:\n            proba_dist_neg.append(proba[pred])\n    return proba_dist, proba_dist_pos, proba_dist_neg\ndef plot_proba(proba_dist, dist_name):\n    lower_q = np.percentile(np.array(proba_dist), 25)\n    upper_q = np.percentile(np.array(proba_dist), 75)\n    threshold = np.percentile(np.array(proba_dist), 80)\n    sns.distplot(proba_dist)\n    print(\"\"\"It seems like a lot of predicitons are between {}% and {}%... \n          We don't want to keep these 'unsure data' so, we could try {} \n          as a threshold for {} labels\"\"\".format(round(lower_q, 2), round(upper_q, 2), round(threshold, 2), dist_name))\n    return threshold","0184938d":"proba_dist, proba_dist_pos, proba_dist_neg = get_prediction_probabilities(EMBEDDED_UNLABELED[:1000], X_train, y_train)","f32d4b81":"thresh_all = plot_proba(proba_dist, \"all\")","1e9de986":"thresh_pos = plot_proba(proba_dist_pos, \"positive\")","f4b75035":"thresh_neg = plot_proba(proba_dist_neg, \"negative\")","be4e869f":"ml_model_trained = semi_sup(ml_model, X_train, y_train, EMBEDDED_UNLABELED, batch_size = 900, threshold = min([thresh_all, thresh_pos, thresh_neg, 0.6]))\n","33184deb":"with open(os.path.join(OUTDIR, EXTRA_DATA, \"Semi_Sup_Log_Reg\"), \"wb\") as f:   \n    pickle.dump(ml_model_trained, f)","79708bf1":"def transform_paragraph(paragraphs):\n    sci_ner = []\n    gen_ner = []\n    sci_ner_embed = []\n    gen_ner_embed = []\n    for para in paragraphs:\n        sci_ner.append(get_sci_ents(para))\n        gen_ner.append(get_gen_ents(para))\n    for sci, gen in zip(sci_ner, gen_ner):\n        sci_ner_embed.append(embed_doc(sci, DOC_EMBED))\n        gen_ner_embed.append(embed_doc(gen, DOC_EMBED))\n    \n    sci_ner_embed = np.array(sci_ner_embed)\n    gen_ner_embed = np.array(gen_ner_embed)\n    all_embed = np.hstack((sci_ner_embed, gen_ner_embed))\n    return all_embed\n\n\n\n\ndef read_predicted(article_path, model):\n    paragraphs = get_paragraphs_(article_path, read_only = False)\n    doc_reps = transform_paragraph(paragraphs)\n    preds = model.predict(doc_reps)\n    \n    for para, pred in zip(paragraphs, preds):\n        if not pred:\n            print(para, '\\n')\n        if pred:\n            print(colored(para, color = None, on_color = 'on_yellow'), '\\n')\n    \ndef kaggle_demo(article_path, example_sci_ents, example_gen_ents,paragraphs, model):\n    sci_ner_embed = []\n    gen_ner_embed = []\n    para_predicted = []\n    for sci, gen, para in zip(example_sci_ents, example_gen_ents, paragraphs):\n        if sci and gen:\n            with torch.no_grad():\n                sci_ner_embed.append(embed_doc(sci, DOC_EMBED))\n                gen_ner_embed.append(embed_doc(gen, DOC_EMBED))\n                para_predicted.append(para)\n    sci_ner_embed = np.array(sci_ner_embed)\n    gen_ner_embed = np.array(gen_ner_embed)\n    all_embed = np.hstack((sci_ner_embed, gen_ner_embed))\n    \n    preds = model.predict(all_embed)\n    for para, pred in zip(para_predicted, preds):\n        if not pred:\n            print(para, '\\n\\n')\n        \n        if pred:\n            print(colored(para, color = None, on_color = 'on_yellow'), '\\n\\n')\n    ","5e195d81":"article_path = \"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/02201e4601ab0eb70b6c26480cf2bfeae2625193.json\"\n## some processing done on my machine to run NER pipelines\nif not NO_NER:\n    paragraphs = get_paragraphs(article_path, read_only = False)\n\n    example_sci_ents = get_sci_ents(paragraphs)\n    example_gen_ents = get_gen_ents(paragraphs)\n\n    with open(os.path.join(OUTDIR, \"example_sci_ents.pkl\"), \"wb\") as f:\n        pickle.dump(example_sci_ents, f)\n    with open(os.path.join(OUTDIR, \"example_gen_ents.pkl\"), \"wb\") as f:\n        pickle.dump(example_gen_ents, f)","89bcc86e":"if NO_NER:\n    with open(os.path.join(INDIR, EXTRA_DATA, \"example_sci_ents.pkl\"), \"rb\") as f:\n        example_sci_ents = pickle.load(f)\n    with open(os.path.join(INDIR, EXTRA_DATA, \"example_gen_ents.pkl\"), \"rb\") as f:\n        example_gen_ents = pickle.load(f)\n        \n        \nparagraphs = get_paragraphs(article_path, read_only = False)\nkaggle_demo(article_path, example_sci_ents, example_gen_ents, paragraphs, ml_model_trained)","5f3e4af8":"df_for_kaggle_demo = pd.read_csv(os.path.join(INDIR, LIST_DF_DIR, \"DF_FOR_TEST_ner.csv\"))\narticle_paths = set(df_for_kaggle_demo[\"path\"].to_list())","9cfd8cd1":"article_paths","295a4db2":"for article in article_paths:\n    print(\"processing article, \", article)\n    df_article = df_for_kaggle_demo.loc[df_for_kaggle_demo[\"path\"] == article].sort_values(\"doc_index\", axis = 0)\n    df_article = df_article.dropna(subset = [\"sci_ner\", \"gen_ner\"])\n    sci_ents = [literal_eval(x) for x in df_article.sci_ner.to_list()]\n    gen_ents = [literal_eval(x) for x in df_article.gen_ner.to_list()]\n    paragraphs = [x for x in df_article.text.to_list()]\n    kaggle_demo(article_path, sci_ents, gen_ents, paragraphs, ml_model_trained)\n    print(\"===================================================================\", '\\n'*3)\n    ","71ca9ff3":"---","272aa4b8":"#### Ethics data paths (description of selection in introduction ","a15f0e43":"#### read metadata","c5229616":"#### check directories for data","079c72bc":"# 2. Feature Engineering","e0d23b18":"NOTE : we set the threshold at highest .60. Otherwise the model will not have enough labeled data for a particular class. \n\nOur model has a big tendency to label positive. In order to control this, I've set class_weight to .5 for both classes\n","e740d5c2":"Plot bibliographic references","d201873c":"---","6ebcc23f":"\nThe following articles were not used, because they \ncould potentially contain discussion of socioethical concerns.\n\n\nCommented lines above each article are exerpts which \ngave reason to exclude the article from labeled biomed articles.\n\n","47cde39b":"This is the execution of a semi-supervised model which leverages appropriate available resources for scientific text -- via sciSpaCy NER pipeline and a possibility to opt for using SciBERT embeddings (although avoided for dataprocessing) -- in aims to detect discussion of ethical concerns in a given article. ","c42b623f":"\n\n### 1. Data\n1. Subset of CORD-19 filtered for full-text articles publish date >= 2019 (total after reduce: 3864) \n2. Labeled dataset: \n    - 25 articles selected from dataset which discuss only biomedical topics (see Section 1.2)\n    - Manually annotated corpus of 12 scientific articles gathered from arxiv.com and scholar.google.com which treat subjects relevant to task description, labeled at paragraph-level as : 1 for content related to ethics and social concerns,   0 for content related to methodology, general information or any other conten. Data made public under dataset \"\/kaggle\/input\/archiv-data\" \n    \n    \n**General effort to raise ethical concerns regarding COVID**\n\n- [Oliver et al. 2020](https:\/\/arxiv.org\/abs\/2003.12347)  (Discussion of ethical concerns about contact tracing)\n- [Cho 2020](https:\/\/arxiv.org\/abs\/2003.11511) (Discussion of ethical concerns of app like Singapore's TraceTogether)\n\n**Access to information about COVID-19 (inspired by third bulletpoint)**\n\n- [Visscher 2020](https:\/\/arxiv.org\/abs\/2003.08824) (Provides an epidemological model for lay-people and policy makers)\n- [Fang 2020](https:\/\/arxiv.org\/abs\/2003.12143) (Provides the Chicago public some analysis of spread of disease)        \n\n**Cooperation between global networks, including WHO**\n\n- [Greene et al 2019](https:\/\/onlinelibrary.wiley.com\/doi\/full\/10.1002\/emp2.12040) (General review of Public Health efforts and network of global organizations including WHO)    \n\n**Efforts to develop assessment framework of public health measures**\n\n- [Shirasaki 2016](https:\/\/arxiv.org\/abs\/1610.03600) (Assessment of importance of contact tracing)\n- [Canetti et al 2020](https:\/\/arxiv.org\/abs\/2003.13670) (Development of a contact tracing model which does not store users' geolocation information)        \n\n**Mental Health of healthcare workers**\n\n- [Kang et al 2020](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0889159120303482) (Study of mental health effects during crisis and mitigation efforts)    \n\n**Social media, Misinformation, Anxiety**\n\n- [Mejova 2020](https:\/\/arxiv.org\/abs\/2003.00923) (Study of search engines and quality of information, specifically related to negativity about vaccines)\n- [Ghezzi 2019](https:\/\/arxiv.org\/abs\/1912.00898) (Study of role of Facebook ads during crisis)\n- [Thelwall 2020](https:\/\/arxiv.org\/abs\/2003.11090) (Study of tendencies accross genders of social media content, related to anxiety about COVID-19)\n- [Sharma 2020](https:\/\/arxiv.org\/abs\/2003.12309) (Analysis of misinformation in Tweets about COVID-19)\n\n        \n### 2. Feature Engineering\n\n\nThe idea is to create a document represenation which will make a distinction between purely biomedical talk and social and ethical talk via:\n\n\n1. Leverage sciSpaCy's NER pipeline ('en_ner_craft_md') in parallel with the normal language model ('en_core_web_md'), to detect 2 distinguished sets of entities per paragraph:\n    - a set of biomedical domain specific entities using the spacy NER model trained on CRAFT corpus \n    - another set of entities presumabley non biomedical, detected by normal spacy pipeline\n2. Flair DocumentPooling with glove word embeddings as paragraph representation\n    - separate document embedding of each set of entities, concatenated\n\n### 3. Model\n- Pseudo-labeling Semi-supervised ML model \n- binary classification of presence (or not) of discussion of ethical concerns \n\n\n### 4. Demonstration\n- A method to print the text of a given article with highlighted exerpts which the model predicts to be pertinent to ethics\n    \n\n\n### Attributions\n\n- Document representation has been inspired by findings from [this masters thesis](http:\/\/kth.diva-portal.org\/smash\/record.jsf?pid=diva2%3A1378586&dswid=-9770) which suggests that representing a document via flair DocumentPooling of embeddings of named entities provides to be an effective technique for clustering unstructured lifesciences articles. \n\n- The semi-supervised machine learning model is adapted\/inspired by [this tutorial](https:\/\/github.com\/anirudhshenoy\/pseudo_labeling_small_datasets\/blob\/master\/pseudo_label-Logistic_reg.ipynb) \n\n- The plotly bibiographic reference graph is adapted from [plotly tutorial code](https:\/\/plotly.com\/ipython-notebooks\/network-graphs\/)\n\n\n\n### NOTE about running program\nI've saved all output from this workflow to two directories --> \"\/kaggle\/input\/list-dfs-3\" and \"\/kaggle\/input\/extrad-data\" made public for the task, in order to allow to run this file without running NER extraction, which takes about 2 hours\n- To run with output from spacy pipelines I've used from my machine set NO_NER to True\n","2eaf57c7":"Import packages","a830a4f5":"#### instanciate spacy models","c077b39f":"# 1.1.4 NER in abstracts\n\nA demonstration of information abstracted for document representations ","17a1520a":"#### The following code is if used with spacy pipelines","cbed2e23":"---","67b99a3e":"#### the following code is for example using the preprocessed spacy ner","bc12acd1":"---","b9aba4e1":"## BEFORE RUNNING Set demonstration variables\n- To run whole file with spacy pipeline, set NO_NER to False\n- To run with output from spacy pipelines I've used from my machine set NO_NER to True","e7ce26b6":"# 1. Data exploration and visualisation","e3e1714e":"# CORD-19 Ethics Exploration","d4aac3cf":"# 3. Model","34b1e831":"... so the model predicts all paragraphs here to be labeled positive\n\nLet's try some more articles","4669b368":"# 3.1 Define semi Supervised model","3472ca12":"# 1.1.2 Number of tokens in abstract\n\nTo get an idea of how many tokens we will be processing ...","8b2bb2dd":"#### Save X train ","4d8d7c33":"#### Load pickles for demo","0aac651a":"#### Load labeled data\n\nThe process for labeling biomedical data is described in 1.2","4ede8c2b":"# 1.2.6.1 NER in socio-ethical discussions","4cb847e4":"# 4.1 Demonstration Conclusion\n\n- It seems that the model, trianed on only the labeled data and not on pseudo labels, will highlight at random. But this is only the beginning of an approach that could be very well improved by different adaptation to feature engineering. \n\n- Please let me know what you think!","0e6adfe6":"This representation is quite underwhelming, although there might be some sort of relationship with being related to ethics and being on the periphery of the space. \n\nThe intuition to explain this would be that the majority of the articles have  content pertaining to biomedical analysis, while the articles we are targeting will have some entities which will be further away from the centroid of the entire vector space\n\nThis is somewhat coherent with the visualisation of the bibilographic references \n\nPerhaps it could be improved by adding verbs or other tokens to the document representation ...","356e6d16":"---","1bd59ca8":"# 4. Demonstration\n\nA demonstration of the models' detection of ethical concerns. \n\nFirst article highlights all text","7f465c28":"#### get paths to full text articles ","0b364e23":"### Requirements\n\n\n1. Data manipulation\n\n - pandas==0.25.1\n - numpy==1.17.2\n - networkx==2.2\n\n2. Visualisation\n\n - seaborn==0.10.0\n - matplotlib==3.1.1\n - plotly==4.5.4\n - plotly-orca==1.2.1\n \n3. NLP\n - scispacy==0.2.4\n - spacy==2.2.4\n - scispacy==0.2.4\n - en_core_web_md==2.2.5\n - en_ner_craft_md==0.2.4\n\n4. Neural Net\n - flair==0.4.5\n - torch==1.4.0\n - sklearn==0.21.3\n\n\n","fb47aa78":"#### reduce data to full text articles from 2019-2020","47c67336":"# 1.2.3 Labeled Biomed Data curation\n\nThe following method was used to curate the list of biomedical articles\n\nProceedure:\n1. read articles one by one and filter for biomedical language:\n    - I generally witheld articles which contain some language which would confuse the model\n    - Especially talking about contact tracing as this is a hot topic in COVID ethics\n2. collect list of purely biomedical articles for labeled data (biomed in cell above) \n\n","46eea87f":"---","6c5cb350":"# 2.1 NER extraction\n\n- get_row_data : takes path to ulabeled data file and collects data to be stored in ulabeled_df\n- get_ulabeled_df : method to load all raw text from unlabeled data in a single dataframe\n- split_ulabeled_df : method to split unlabeled df into K number of dfs for processing\n- get_sci_ents: takes a document and returns scispacy named entities\n- get_gen_ents: takes a documnet and returns general named entities\n- get_train_data: returns a list of strings (documents) and associated labeled from labeled data","482f0d00":"---","4384d02c":"# 3.4 Model Conclusion\n\n- I've tested the model on the original train data, using pseudo labels as training input, and this is probably not the most kosher thing to do, but I've found it to be interesting for this particular case\n\n- F1 scores only diminish with more collected pseudo-labeled data, so we can potentially improve this by any of the following:\n\n    - augment original training data\n    - adapt feature vectors to include verbs\n    - change task to be only document representatios, although this would require even more training data","84ad5e26":"# 1.2.1 Bibliography reference graph plot\n\nA fun visualisation of the articles which make references amongst themselves\n\nI've used this visual as a basis to select the articles in the labeled data (15 articles purely biomedical talk)\n\nThe idea is that we would be mostly interested in incorporating articles referenced the most in our labeled data, as they are perhaps more reliable, and the language used is presumably representative of a given class\/category ","29bfbc2e":"# 1.2.6.2 NER in biomedical data","49335d5e":"---","524e312d":"# 1.2.4 Save Biomed Data","1a6672b1":"# 1.1 Preliminary Statistics","e8641559":"---","7d85e46a":"# 1.2 Labeled input: selection and statistics","cfa420db":"---","362d96ae":"---","b1b23ec8":"---","3d155375":"Get entities in iterations so as not to loose data if kernel interrupts","6570f88e":"... SciSpaCy is much more effective at picking up biomedical domain entities","b9a762e3":"# 1.2.5 Witheld Data","3a2c1182":"#### reduce x_train to docs only containing both general and scientific entities","21478f8b":"# 3.3 Train and save model","31c96069":"---","34598193":"# 2.2 Document embeddings","177b2164":"# 2.5 Conclusion Feature Engineering:\n\nPhew that was a lot of processing! \n\nFeature Engineering Considerations:\n\n- This took a lot of computational power to process only ~ 4-5k articles\n- A simpler mechanism might be used to extract entity mentions and detect general vs biomedical categories\n- There is potentially an overlap between entity mentions in my processing, where the general spacy pipeline might pick up on the same mention as scispacy! I would have prefered to create a set and separate the sci vs gen mentions...\n- It would be nice to be able to represent the document which has only entities picked up from either of the NER pipelines, so far we only process those which have entities from both...\n- In light of this it might have been more sound to just concat both sets of entities and do a single document pooling\n- In fact, it might have been best after all to do an embedding representation at document-level and not at paragraph-level. \n- That said, lets see the results of this work!\n\n\nOnward: The Semi-supervised ML Model!","a8892a6d":"# 1.1.3 Top n frequent lemmas","b78e8af3":"# 1.3 Data Exploration Conclusion:  \n\n1. Visualisation of data sources and bibliographic references have allowed to better target training data\n2. There is a stark difference between the enties in the general ner detection here! good sign. this will help in feature engineering \n4. This all in hopes that the distinction between entity sets will help better represent a document\n","33143c05":"# 1.1.1 Pie chart of top 20 journals\n\nWhere is the data coming from?","c275b263":"... but for extracts that are more focused on ethics and social topics, the general pipeline picks up more meaningful entities","f5b41791":"# 2.4 Concat output, plot 2d PCA, and save","129d4556":"# 3.2 Deciding a prediction probability threshold","35a585a5":"# 1.2.2 Find top k referenced articles\n\nSearch through these edges and find the most referred to among the titles in our directory\n\nThe goal is to chose 20 articles from top 30 which seem unrelated to \nthe task of finding ethical and social discussions \n","a9c7b6e2":"# 1.2.6 NER in labeled data"}}