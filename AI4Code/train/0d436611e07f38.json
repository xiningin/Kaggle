{"cell_type":{"806d176c":"code","fe2ca482":"code","9ee0296e":"code","81910f34":"code","2052e724":"code","51cc27aa":"code","5763f364":"code","e6fa6eb4":"code","c5d5f6d6":"code","d9f2e790":"code","ebad6c69":"code","047a63e0":"code","50dbf4e6":"code","fcc042d4":"code","0a4b75ee":"code","d4ee0e86":"code","259e482c":"code","99642606":"code","e53e1108":"code","571d04fa":"code","30e97730":"code","2d3d675e":"code","24452bb9":"code","46718767":"markdown","7d74e73d":"markdown","103948bd":"markdown","5c9b6276":"markdown","58655010":"markdown","ffaf3799":"markdown","c7457360":"markdown","34e493a4":"markdown","96f44939":"markdown","94c7fedd":"markdown","b3fa5eb1":"markdown"},"source":{"806d176c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fe2ca482":"grid_df = pd.read_pickle('\/kaggle\/input\/m5-simple-fe\/grid_part_1.pkl')","9ee0296e":"grid_df.head()","81910f34":"grid_df = grid_df[['id','d','sales']].pivot(index='id',columns='d').reset_index()\nids = pd.DataFrame(grid_df['id'])","2052e724":"grid_df = grid_df['sales'].iloc[:,:1913]\n\n# values > 0 for 1, missing for nan, 0 for -1\ngrid_df = pd.DataFrame(np.where(grid_df.isnull(),np.nan,\n                                np.where(grid_df > 0, 1, -1)))\n\ngrid_df.columns = [f'd_{i}' for i in range(1,1914)]","51cc27aa":"grid_df.head()","5763f364":"d1_peak = grid_df.notnull().sum(axis=1)\ncluster = d1_peak.copy()","e6fa6eb4":"plt.figure(figsize=(12,6))\nsns.kdeplot(d1_peak)\nplt.plot([925,925],[0,0.0030]); plt.plot([1300,1300],[0,0.0030]); plt.plot([1700,1700],[0,0.0030])\nplt.text(x=700,y=0.002,s='Cluster 1'); plt.text(x=1020,y=0.002,s='Cluster 2')\nplt.text(x=1420,y=0.002,s='Cluster 3'); plt.text(x=2000,y=0.002,s='Cluster 4')\nplt.title('# of Nan distribution')\nplt.show()","c5d5f6d6":"c1_mask = (d1_peak <= 925)\nc2_mask = (d1_peak > 925) & (d1_peak <= 1300)\nc3_mask = (d1_peak > 1300) & (d1_peak <= 1700)\nc4_mask = (d1_peak > 1700)\n\ncluster[c1_mask] = 1\ncluster[c2_mask] = 2\ncluster[c3_mask] = 3\ncluster[c4_mask] = 4","d9f2e790":"grid_df['cluster'] = cluster\n\n# missing values for 0\ngrid_df = grid_df.fillna(0)","ebad6c69":"grid_df['cluster'].value_counts()","047a63e0":"A = np.array([1,0,0,0,1,0])\nB = np.array([1,1,0,0,0,0])","50dbf4e6":"np.sum(A == B) \/ len(A)","fcc042d4":"cluster_df = grid_df[grid_df['cluster'] == 1]\ncluster_array = cluster_df.values\ncluster_array = np.where(cluster_array == 0, np.nan, cluster_array)","0a4b75ee":"length = cluster_array.shape[0] \nfor i in tqdm(range(0, int(length\/10))):\n    for j in range(i, length):\n        np.sum(cluster_array[i,:-1] == cluster_array[j,:-1])","d4ee0e86":"def Clustering(cluster_lv1_name, cluster_lv2_num):\n    \n    cluster_df = grid_df[grid_df['cluster'] == cluster_lv1_name]\n    \n    if cluster_lv2_num == 1:\n        print('Pass : Cluster', cluster_lv1_name)\n        \n    else:\n        print('Making dist_matrix : Cluster', cluster_lv1_name)\n        cluster_array = cluster_df.values\n        dist_matrix = np.dot(cluster_array, cluster_array.T)\n\n        ## this part, linkage, takes about 30 minutes.\n        ## If you have another idea for reducing running time,\n        ## Please advise me !\n        Z = linkage(dist_matrix, method='ward')\n        cluster_num = fcluster(Z, t=cluster_lv2_num, criterion='maxclust')\n        cluster_df['cluster'] = cluster_df['cluster'].astype(str) + '_' + cluster_num.astype(str)\n\n    return cluster_df","259e482c":"plan_clustering = {\n    #cluster_lv1_name : how many cluster_lv2 to make\n    1:1,\n    2:1,\n    3:1,\n    4:4\n}","99642606":"%time\ndf_list = list()\nfor lv1, lv2 in plan_clustering.items():\n    df_name = f'cluster_{lv1}_df'\n\n    cluster_df = Clustering(cluster_lv1_name = lv1, cluster_lv2_num = lv2)\n    globals()[df_name] = cluster_df\n    \n    df_list += [cluster_df['cluster']]","e53e1108":"cls_total = pd.concat(df_list)","571d04fa":"cluster_df = pd.concat([ids, cls_total], axis=1)","30e97730":"cluster_df","2d3d675e":"cluster_df['cluster'].value_counts()","24452bb9":"cluster_df.to_pickle('zero_one_cluster.pkl')","46718767":"It takes 90 sec for 500 instances to calculate. \n\ncluster 1,2,3 have about 5000 instances respectively, cluster 4 has 15000 instances.\n\nIt will takes about an hour only to calculate distance matrix. \n\nSo, i found out another way.","7d74e73d":"As you can see on above graph, cluster 4 consists an half of all ids.\n\nSo, I tried to level 2 group only for cluster 4.\n\nBefore level 2 cluster, i will show you very simple example for how Jaccard similarity is calculated.","103948bd":"# Cluster zero \/ one sales pattern\n\nHi all :)\n\nI am novice at Data Science and analysis. If you have any suggestion or advice, Please comment it. It will relly help me!!\n\nI have tried to make some clusters with zero \/ one sales pattern based on the concept of **Jaccard similarity**, inspired by [this amazing kernel](https:\/\/www.kaggle.com\/jpmiller\/grouping-items-by-stockout-pattern)\n\nThe output of clusters for all ids might (i think) lead to feature engineering or new cross validation strategy or another new insights!\n\n\nThe logic is\n\n1. Level 1 clustering using kernel density estimation based on missing values\n    - A wide range of missing values count for all ids => To compare zero \/ one sales pattern, grouping ids which have similar missing distribution into one cluster.\n\n2. Substitute original sales value\n    - Substitute nan for 0, 0 for -1, values > 0 for 1.\n    \n3. Level 2 clustering\n    - Hierarchy Clustering level 1 clusters into more groups.\n    \n    \nThen, enjoy kaggle ~!","5c9b6276":"### 2. the index similar to Jaccard \n\n$\nIndex = \\frac{sum(A==B)~ -~ sum(A!=B)}{len(A)}\n$\n\nI used this index by matrix inner product.\n\nAs i noted above, cluster 4 has 15000 instances, so i made level 2 cluster only for cluster 4","58655010":"### Jaccard similarity calculation\n\nIf there are two binary feature A,B\n\n`A = [1,0,0,0,1,0]`\n\n`B = [1,1,0,0,0,0]`\n\nLet's calculate Jaccard similarity","ffaf3799":"I used `grid_part_1.pkl` file from [this great kernel](https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe). ","c7457360":"I tried two similarity, Jaccard and the score similar to Jaccard.\n\nLets take a look at these examples.","34e493a4":"Transform to original data set form","96f44939":"Plot kde plot of the number of missing values over all ids\n\nI set threshold below based on Heuristic. You can change it.","94c7fedd":"### 1. Jaccard similarity clustering","b3fa5eb1":"There are 4 same values over 6 on each index.\n\nSo, Jaccard similarity between A and B becomes 0.67\n\nSo simple and intuitive definition. Then Lets move onto level 2 cluster"}}