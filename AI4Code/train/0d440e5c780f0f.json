{"cell_type":{"81ca5956":"code","5c5a95b1":"code","e760c915":"code","2efc08f2":"code","a3a98f48":"code","63ff5ab5":"code","02223402":"code","1de58bea":"code","1cfa1e4c":"code","1e07cf4c":"code","8be53a37":"code","2422dfe3":"code","8a14c80c":"code","fd57c28b":"code","95376cb9":"code","e0de0090":"code","330d2a25":"code","f81bc125":"code","ca075528":"code","2ad0440f":"code","153ca44d":"code","454c568e":"code","184be9c6":"code","8f9a309d":"code","d5e567f8":"code","315414d3":"code","1e9cc750":"code","2983e44d":"code","033933ff":"code","8d08a169":"code","73692869":"code","1386f973":"code","ac5128e9":"code","75da43fb":"code","0bed812b":"code","09e7b91a":"code","030cc4bd":"code","700574dd":"code","4ff30c38":"code","863b9116":"code","6b3543a2":"code","e4fb1156":"code","c101d134":"code","b1e79210":"code","d64e2f16":"code","e5e80b72":"code","4901bbf8":"code","f6a6164c":"code","b87e36db":"code","1d401e3d":"code","a6005c2d":"code","b18c5ef6":"code","be236872":"code","8706a2e2":"code","244b106f":"code","69829c43":"code","6dc00678":"code","fbaa1f69":"code","8e4bf45a":"code","d54cf3a4":"code","fc609bd0":"code","f674b8e8":"code","23e2c7d3":"code","e5e49e4e":"code","2baa0efe":"code","5e9f1c64":"code","cdaf5c8b":"code","0df6c185":"markdown","5fcb9405":"markdown","0015e8d3":"markdown","632bf0b9":"markdown","3b6390df":"markdown","1110cf48":"markdown","853ef38f":"markdown","9a7ca9c2":"markdown","c0e789c0":"markdown","76097c70":"markdown","3b77e646":"markdown","dec66794":"markdown","38213733":"markdown","71ce6929":"markdown","1a9bc05b":"markdown","b5170989":"markdown","c50b4d83":"markdown","a8ec7f5d":"markdown","cf4eb2b0":"markdown","e6ad3c59":"markdown","934bd802":"markdown","0c065d8f":"markdown","28a6d7d6":"markdown","e29c9f26":"markdown","b4b5c697":"markdown","f8743043":"markdown","93133560":"markdown","42548367":"markdown","a5efe535":"markdown","31e5afac":"markdown","3a6a324f":"markdown","e3eb54aa":"markdown","9f565540":"markdown","aac24a60":"markdown","c3a35975":"markdown"},"source":{"81ca5956":"# Data set can be found here!\n\n#                           https:\/\/www.kaggle.com\/leonardopena\/top-spotify-songs-from-20102019-by-year","5c5a95b1":"#first import of packages \n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\n\nimport sklearn\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, silhouette_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\n\n%matplotlib inline","e760c915":"\ndf = pd.read_csv(\"..\/input\/top-spotify-songs-from-20102019-by-year\/top10s.csv\", encoding='ISO-8859-1') \n\n\n\ndf.head()","2efc08f2":"#Renaming the columns\n\ndf.rename(columns={'title':'Track Name','artist':'Artist Name','bpm':'Beats Per Minute','top genre':'Genre','nrgy':'Energy','dnce':'Danceability', 'dB':'Loudness dB','spch':'Speechiness','live':'Liveness','val':'Valence','dur':'Length','acous':'Acousticness','pop':'Popularity'},inplace=True)\ndf.head()","a3a98f48":"# The datatypes of the different columns\n\nprint(df.dtypes)","63ff5ab5":"# Get initial descriptive statistics on the columns\n\npd.set_option('precision', 3)\ndf.describe()","02223402":"#Calculating the number of songs of each genre\n\nprint(type(df['Genre']))\n\npopular_genre = df.groupby('Genre').size()\n\npopular_genre = popular_genre.sort_values(ascending=False)\n\npopular_genre\n\ngenre_list = df['Genre'].values.tolist()\n\ngenre_top20 = popular_genre[0:20,]\n\ngenre_top20 = genre_top20.sort_values(ascending=True)\n\ngenre_top20 = pd.DataFrame(genre_top20, columns = [ 'Number of Songs'])\n\ngenre_top20.head()","1de58bea":"plt.figure(figsize=(16,8))\n\n\nax = sns.barplot(x = 'Number of Songs' , y = genre_top20.index , data = genre_top20, orient = 'h', palette = sns.color_palette(\"muted\", 20), saturation = 0.8)\n\nplt.title(\"Top 20 Genres of the Decade... That's a lot of Pop!\",fontsize=30)\nplt.xlabel('Number of Songs', fontsize=25)\nplt.ylabel('Genre', fontsize=10)\n\nxticks = [0, 10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340]\n\nplt.xticks(xticks, size=20,rotation=45)\nplt.yticks(size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","1cfa1e4c":"#Pie chart to show top 20 genres\n\nlabels = genre_top20.index\nsizes = genre_top20.values\n\nexplode = (  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.1, 0.1)\n\nplt.figure(figsize = (10,10))\n\nplt.pie(sizes, labels = labels, explode = explode)\n\nplt.title(\"Top 20 Genres on the List\", fontsize=16)\n\nautopct=('%1.1f%%')\nplt.axis('equal')\n\nplt.show()","1e07cf4c":"#Calculating the least popular genres\n\n\ngenre_bot29 = popular_genre[21:,]\n\ngenre_bot29 = genre_bot29.sort_values(ascending=True)\n\ngenre_bot29 = pd.DataFrame(genre_bot29, columns = [ 'Number of Songs'])\n\ngenre_bot29.head()","8be53a37":"#Pie chart to show bottom 35 genres\n\nlabels = genre_bot29.index\nsizes = genre_bot29.values\n\nexplode = ( 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1)\n\nplt.figure(figsize = (10,10))\n\nplt.pie(sizes, labels = labels, explode = explode)\n\nplt.title(\"Least Popular Genres on the Top 50 List\", fontsize=16)\n\nautopct=('%1.1f%%')\nplt.axis('equal')\n\nplt.show()","2422dfe3":"#Calculating the number of songs by each of the artists\n\n\npopular_artist = df.groupby('Artist Name').size()\n\npopular_artist = popular_artist.sort_values(ascending=False)\n\npopular_artist\n\nartist_list=df['Artist Name'].values.tolist()\n\nartist_top25 = popular_artist[0:25,]\n\nartist_top25 = artist_top25.sort_values(ascending=True)\n\nartist_top25 = pd.DataFrame(artist_top25, columns = [ 'Number of Songs'])\n\nartist_top25.head() ","8a14c80c":"plt.figure(figsize=(16,8))\n\n\nax = sns.barplot(x = 'Number of Songs' , y = artist_top25.index , data = artist_top25, orient = 'h', palette = sns.color_palette(\"muted\", 25), saturation = 0.8)\n\nplt.title(\"Top 25 Artists of the Decade\",fontsize=30)\nplt.xlabel('Number of Songs on Top 50 List', fontsize=25)\nplt.ylabel('Artist', fontsize=10)\n\nxticks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n\nplt.xticks(xticks, size=20,rotation=45)\nplt.yticks(size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","fd57c28b":"# Songs that made the top 50 list twice\n\nplt.figure(figsize=(16,8))\n\ntracks = pd.value_counts(df['Track Name']).iloc[:18].index\n\nsns.countplot(df['Track Name'], order = tracks, orient = 'h', palette = sns.color_palette(\"magma\", 25), saturation =0.7)\n\nplt.title('Songs That Made the Top 50 List on Two Different Years',fontsize=30)\nplt.xlabel('Track', fontsize=25)\nplt.ylabel('Number of Years in Top 50 List', fontsize=25)\n\nplt.xticks(size=20,rotation=90)\nplt.yticks( [0, 1, 2]  , size=20)\nsns.despine(bottom=True, left=True)\n\n\nplt.show","95376cb9":"# Plotting a histogram to show the spread of Popularity since we notice some strange stats worth investigating\n\nplt.hist(df['Popularity'],bins=100)\n\nplt.show()","e0de0090":"# Investigating low popularity\n\nlow_pop = df[df['Popularity'] <= 20]\n\nlow_pop","330d2a25":"# Inspect bad data...  How can the popularity be 0 if these are top 50 songs?\n\ndf.loc[df['Popularity']==0]","f81bc125":"# drop bad data\n\ndf = df.drop(df.index[[50, 138, 267, 362, 442]])\n\ndf = df.reset_index()\n\n# check it's gone\n\ndf.iloc[[50, 138, 267, 362, 442]]","ca075528":"# clean up index\n\ndf = df.drop('index', axis=1)\ndf = df.drop('Unnamed: 0', axis=1)\n\n\ndf.head()","2ad0440f":"# Get descriptive statistics on the columns to see the change\n\npd.set_option('precision', 3)\n\ndf.describe()","153ca44d":"# get df ready for scatter matrix\n\ndf_features = df.drop(df.columns[[0, 1, 2, 3]], axis =1)\n\ndf_features.head()","454c568e":"# Normalize the data with Min\/Max\n\ndf_norm = df_features\n\nscaler = MinMaxScaler() \n\ndf_norm = scaler.fit_transform(df_norm)\n\ndf_norm = pd.DataFrame(df_norm, columns = df_features.columns)\n\ndf_norm.describe()","184be9c6":"# Plotting a histogram to show the difference (note the x-axis)\n\nplt.hist(df_features['Loudness dB'], bins=10)     #original data\nplt.show()\n\n\nplt.hist(df_norm['Loudness dB'], bins=10)          #standardized data\nplt.show()","8f9a309d":"#Fitting the PCA algorithm with our Data\n\npca = PCA().fit(df_norm)","d5e567f8":"#Plotting the Cumulative Summation of the Explained Variance\n\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Spotify Data Explained Variance')\nplt.show()","315414d3":"# print the explained variance for each component\n\nexplained_variance = pca.explained_variance_ratio_\n\nprint(explained_variance)","1e9cc750":"# how much variance can be explained for 8 components\n\nprint('The explained variance for this many components is:  ',explained_variance[0:8].sum())","2983e44d":"# visually inspect pca\n\nmap = pd.DataFrame(pca.components_, columns=df_norm.columns)\nplt.figure(figsize=(12,6))\nsns.heatmap(map, cmap='gist_earth_r')","033933ff":"# choose number of components\n\npca = PCA(n_components = 8)\n\ndata_pca = pca.fit_transform(df_norm)","8d08a169":"# Visualizing the relationship between all features\n\nscatter_matrix(df_norm)\n\nplt.gcf().set_size_inches(30, 30)\n\nplt.show()","73692869":"# Use a spearman correlation to measure the relationship between features\n\npd.set_option('display.width', 100)\npd.set_option('precision', 3)\n\ncorrelation = df_norm.corr(method='spearman')\n\nprint(correlation)","1386f973":"# heatmap of the correlation to visualize the relationships between features\n\nplt.figure(figsize=(10,10))\nplt.title('Correlation heatmap')\n\nsns.heatmap(correlation, annot = True, vmin=-1, vmax=1, cmap=\"YlGnBu\", center=1)","ac5128e9":"# Analysing the relationship between Danceablity and Valence\n\nfig = plt.subplots(figsize = (10,10))\n\nsns.regplot(x = 'Valence', y = 'Danceability', data = df_norm, color = 'olive')\n\nsns.kdeplot(df_norm['Valence'], df_norm['Danceability'])\n\nprint('The spearman correlation is:  ',correlation['Danceability']['Valence'])","75da43fb":"# Analysing the relationship between valence vs popularity\n\n\nf, ax = plt.subplots(figsize=(6, 6))\n\ncmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True, start=2.8, rot=.1)\n\nsns.kdeplot(df['Valence'], df['Popularity'], cmap=cmap, n_levels=16, shade=True);","0bed812b":"# Analysing the relationship between valence vs energy\n\nsns.jointplot(x=df['Valence'], y=df['Energy'], data=df, kind=\"kde\", color='lightblue');","09e7b91a":"# Analysing the relationship between valence vs length\n\n\nsns.jointplot(df['Valence'], df['Length'], kind=\"hex\", color=\"#4CB391\")","030cc4bd":"# Analysing the relationship between loudness vs danceability\n\nsns.catplot(y=\"Danceability\", x=\"Loudness dB\", kind = \"swarm\", data = df_features, palette = 'rocket_r')","700574dd":"# Analysing the spread of popularity throught the years\n\nsns.catplot(y = \"Popularity\", x = \"year\", kind = \"box\", data = df, palette = 'seismic')\n\n","4ff30c38":"# PairGrid to analyze trends over the years\n\nsns.set()\n\ng = sns.PairGrid(df, y_vars = ['Beats Per Minute', 'Energy', 'Danceability', 'Loudness dB', \n                               'Liveness', 'Valence', 'Length', 'Acousticness', 'Speechiness', 'Popularity'] , x_vars = ['year'], aspect = 4)\n\ng = g.map(sns.lineplot, color=\"blue\")\n\n# Adjust the tick positions and labels\n\ng.set(xticks=[2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])\n\n\n# Adjust the arrangement of the plots\n\ng.fig.subplots_adjust(wspace=.02, hspace=.02);","863b9116":"# insert year cloumn into features df\n\ndf_features.insert(0, 'year', df['year'])\n\n\ndf_features.head()","6b3543a2":"# setup features and target\n\n\nX = df_features[['year']]\ny = data_pca\n\n# can switch variable z to y to see effect of all features on predicition (also change y to z)\nz = df_features[['Beats Per Minute', 'Energy', 'Danceability', 'Loudness dB', 'Liveness', 'Valence', 'Length', 'Acousticness', 'Speechiness', 'Popularity']]\n","e4fb1156":"# Train, Test, Split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n\n# Instantiate model\nmlr = LinearRegression()\n\n# Fit Model\nmlr.fit(X_train, y_train)\n\n# Predict\ny_pred = mlr.predict(X_test)\n\n\n# RMSE\nprint('The Root Mean Squared Error for this model is:  ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","c101d134":"# K-Fold Cross Val \n\nmlr = LinearRegression()\n\n\nmlr.fit(X, y)\n\n\nmse = cross_val_score(mlr, X, y, scoring='neg_mean_squared_error', cv=10)\n\n\n# fix the sign of MSE scores\nmse_scores = -mse\n\n\n# convert from MSE to RMSE\nrmse_scores = np.sqrt(mse_scores)\n\n\n# calculate the average RMSE\nprint('The Root Mean Squared Error for this model is:  ', rmse_scores.mean())\n","b1e79210":"# Ridge Regression and GridSearchCV\n\nridge = Ridge()\n\nparams = { 'alpha' : [ 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 40, 80, 100, 1000, 10000 ]  }\n\nrr = GridSearchCV(ridge, params, scoring = 'neg_mean_squared_error', cv=10)\n\nrr.fit(X, y)\n\nprint(rr.best_params_)\nprint(rr.best_score_)\n\nrr_mse = -(rr.best_score_)\n\nrr_rmse = np.sqrt(rr_mse)\n\nprint('The Root Mean Squared Error for this model is:  ', rr_rmse)","d64e2f16":"# Lasso Regression and GridSearchCV\n\nlasso = Lasso()\n\nparams = { 'alpha' : [ 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20, 40, 80, 100, 1000, 10000 ]  }\n\nlr = GridSearchCV(lasso, params, scoring = 'neg_mean_squared_error', cv=10)\n\nlr.fit(X, y)\n\nprint(lr.best_params_)\nprint(lr.best_score_)\n\nlr_mse = -(lr.best_score_)\n\nlr_rmse = np.sqrt(lr_mse)\n\nprint('The Root Mean Squared Error for this model is:  ', lr_rmse)\n\n","e5e80b72":"# predict a hit song in 2020's features\n\nhit = mlr.predict([[2020]])\n\nhit","4901bbf8":"# reverse pca\n\nhit = pca.inverse_transform(hit)\n\nhit","f6a6164c":"# reverse normalization\n\nhit = scaler.inverse_transform(hit)\n    \nhit","b87e36db":"# get the features of the prediciton into a dataframe\n\nhit = pd.DataFrame(hit)\n\nhit = hit.drop(columns = 9, axis=1)\n\nhit\n","1d401e3d":"# make a prediction for 2020 using the machine learning classifier KNN\n\n\nknn = KNeighborsClassifier(n_neighbors = 1)\n\nknn.fit(df_features[['Beats Per Minute','Energy','Danceability','Loudness dB','Liveness','Valence', 'Length', 'Acousticness', 'Speechiness']], df_features.index)\n\ny_pred = knn.predict(hit)\n\ny_pred = pd.DataFrame(y_pred)\n\ny_pred","a6005c2d":"# look up the index\n\nwinner = df.iloc[[388]]\n\nwinner","b18c5ef6":"# Finding out the skew for each feature\n\nskew = df_features.skew()\n\nprint(skew)","be236872":"# scale the data\n\nscaler = StandardScaler()\n\ndf_scaled = scaler.fit_transform(df_features)\n\ndf_scaled = pd.DataFrame(df_scaled)\n\ndf_scaled.head()","8706a2e2":"# Plot to show the difference\n\nplt.hist(df_features['Speechiness'], bins=10)                    #original data\nplt.show()\n\nplt.hist(df_scaled.iloc[8], bins=10)                            #standardized data\nplt.show()","244b106f":"# choose the best number of clusters using elbow method and inertia\n\nk = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\ninertias = []\n\nfor i in k:\n    km = KMeans(n_clusters=i, max_iter=1000, random_state=42)\n    km.fit(df_scaled)\n    inertias.append(km.inertia_)\n\nplt.plot(k, inertias)\nplt.xlabel(\"Value for k\")\nplt.ylabel(\"Inertias\")\nplt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nplt.show()","69829c43":"k = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n\nscore=[]\n\nfor n_cluster in k:\n    kmeans = KMeans(n_clusters=n_cluster).fit(df_scaled)\n    silhouette_avg = silhouette_score(df_scaled, kmeans.labels_)\n    score.append(silhouette_score(df_scaled, kmeans.labels_))\n    \n    print('Silhouette Score for %i Clusters: %0.4f' % (n_cluster, silhouette_avg))","6dc00678":"# plot cluster options\n\nplt.plot(k, score, 'o-')\nplt.xlabel(\"Value for k\")\nplt.ylabel(\"Silhouette score\")\nplt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nplt.show()","fbaa1f69":"# set number of clusters\n\nkclusters = 8\n\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, init='k-means++', random_state=42).fit(df_scaled)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]","8e4bf45a":"# add clustering labels to dataframe\n\ndf.insert(0, 'Playlist Number', kmeans.labels_)\n\ndf.head()    # check out the Cluster Labels column!","d54cf3a4":"df.loc[df['Playlist Number'] == 0, df.columns[[1, 2]]]","fc609bd0":"df.loc[df['Playlist Number'] == 1, df.columns[[1, 2]]]","f674b8e8":"df.loc[df['Playlist Number'] == 2, df.columns[[1, 2]]]","23e2c7d3":"df.loc[df['Playlist Number'] == 3, df.columns[[1, 2]]]","e5e49e4e":"df.loc[df['Playlist Number'] == 4, df.columns[[1, 2]]]","2baa0efe":"df.loc[df['Playlist Number'] == 5, df.columns[[1, 2]]]","5e9f1c64":"df.loc[df['Playlist Number'] == 6, df.columns[[1, 2]]]","cdaf5c8b":"df.loc[df['Playlist Number'] == 7, df.columns[[1, 2]]]","0df6c185":"### Standardization of the data to fix skew and get mean=0 and std=1 in order to help with clustering\n","5fcb9405":"# Part 3 - Multiple Linear Regression to make a prediction for 2020","0015e8d3":"# Part 1 - EDA and Data Visualization","632bf0b9":"## Finding a song from our data to serve as an exemplar for the predicted values by using the machine learning technique, K-Nearest Neighbor Classifier","3b6390df":"## Analyzing the features and their interdependencies","1110cf48":"## Predicition","853ef38f":"### Import packages and read data into a dataframe","9a7ca9c2":"###  _**Sadder songs do have less energy though!**_","c0e789c0":"## Analyzing the trends over the years","76097c70":"### _**It looks like the K-Fold Cross Validation MLR model is the best!**_","3b77e646":"# Playlist #8","dec66794":"# Part 4 - Make Playlists based off of the feature characteristics using Machine Learning technique K-Means Clustering","38213733":"###  _**My hypothesis that popular songs are more depressing was wrong!**_","71ce6929":"## Choose the Best Multiple Linear Regression Model","1a9bc05b":"###  _**In general hit songs are getting slower, less energy, way more danceable, a little louder, slighly happier, way shorter, way more acoustic, with less lyrics!**_","b5170989":"## Playlist #2","c50b4d83":"## Investigating  bad data","a8ec7f5d":"###  _**The most recent songs are the most popular!  ...this makes me question the Popularity score**_","cf4eb2b0":"## PCA (Principle Component Analysis) to reduce the feature columns","e6ad3c59":"# Playlist #7","934bd802":"## Playlist #4","0c065d8f":"# Spotify Top Tracks of the Decade","28a6d7d6":"## Playlist #5","e29c9f26":"# Part 2 - Statistical EDA, Normalization, and PCA","b4b5c697":"# Playlist #6","f8743043":"## Playlist #1","93133560":"## Categorical EDA with bar graphs to see most popular Artists, Genres, and Tracks","42548367":"###  _**Happier songs are more danceable!**_","a5efe535":"#  _**There you have it... If you wish to write a hit song in 2020 you can write one just like Close by Nick Jonas !**_  ","31e5afac":"## Playlist #3","3a6a324f":"## Find the appropriate amount of clusters\n","e3eb54aa":"##  In this exercise I decided to use a data set from Spotify that contained the top 50 songs for each year from 2010-2019.  As a musician I was curious to see what a hit song in 2020 might look like (and how to write one!), and also to test my hypothesis that popular songs are getting more sad sounding.  Then for fun I decided to see if I could use Machine Learning to make playlists based off of their feature characteristics.","9f565540":"## Normalization of the data to get the values between 0 and 1 in order to help with  PCA and regression analysis","aac24a60":"##  _**And again, in 2020 hit songs are getting slower, less energy, way more danceable, a little louder, slightly happier, way shorter, way more acoustic, with less lyrics!**_","c3a35975":"# Now all we have to do is name these playlists... I'll leave that up to you!"}}