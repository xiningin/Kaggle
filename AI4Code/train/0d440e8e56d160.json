{"cell_type":{"7e731d69":"code","3b6b74c0":"code","1ceea906":"code","ffa7c91b":"code","7090351c":"code","7870703b":"code","c2efd8ff":"markdown","baa29f75":"markdown","aa370ec6":"markdown","415905ba":"markdown","fd0c8f8c":"markdown","d72d47db":"markdown","aede54b2":"markdown"},"source":{"7e731d69":"!git clone --depth 1 --branch \"v1.0-alpha\" https:\/\/github.com\/crunchiness\/lernd.git\n%cd lernd\n!pip install ordered-set==4.0.1","3b6b74c0":"import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom IPython.core.display import clear_output\nfrom matplotlib import pyplot as plt\n\nfrom lernd.classes import ILP, LanguageModel, ProgramTemplate\nfrom lernd.lernd_loss import Lernd\nfrom lernd.lernd_types import Constant, RuleTemplate\nfrom lernd.main import generate_weight_matrices, extract_definitions, print_valuations\nfrom lernd.util import get_ground_atom_probs, ground_atom2str, softmax, str2ground_atom, str2pred\n\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\n\ntarget_pred = str2pred('even\/1')\nzero_pred = str2pred('zero\/1')\nsucc_pred = str2pred('succ\/2')\npreds_ext = [zero_pred, succ_pred]\nconstants = [Constant(str(i)) for i in range(11)]\nlanguage_model = LanguageModel(target_pred, preds_ext, constants)\n\n# Program template\naux_pred = str2pred('pred\/2')\naux_preds = [aux_pred]\nrules = {\n    target_pred: (RuleTemplate(0, False), RuleTemplate(1, True)),\n    aux_pred: (RuleTemplate(1, False), None)\n}\nforward_chaining_steps = 6\nprogram_template = ProgramTemplate(aux_preds, rules, forward_chaining_steps)\n\n# ILP problem\nground_zero = str2ground_atom('zero(0)')\nbackground = [ground_zero] + [str2ground_atom(f'succ({i},{i + 1})') for i in range(10)]\npositive = [str2ground_atom(f'even({i})') for i in range(0, 11, 2)]\nnegative = [str2ground_atom(f'even({i})') for i in range(1, 10, 2)]\nilp_problem = ILP(language_model, background, positive, negative)","1ceea906":"print('Background axioms:')\nprint(', '.join(map(ground_atom2str, background)))\n\nprint('\\nPositive examples:')\nprint(', '.join(map(ground_atom2str, positive)))\n\nprint('\\nNegative examples:')\nprint(', '.join(map(ground_atom2str, negative)))","ffa7c91b":"lernd_model = Lernd(ilp_problem, program_template, mini_batch=0.3)\nweights = generate_weight_matrices(lernd_model.clauses)\n\nlosses = []\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.5)\n\nfor i in range(1, 501):\n    loss_grad, loss, valuation, full_loss = lernd_model.grad(weights)\n    optimizer.apply_gradients(zip(loss_grad, list(weights.values())))\n    loss_float = float(full_loss.numpy())\n    mb_loss_float = float(loss.numpy())\n    losses.append(loss_float)\n    if i % 10 == 0:\n        print(f'Step {i} loss: {loss_float}, mini_batch loss: {mb_loss_float}\\n')\n        fig, axs = plt.subplots(ncols=3, gridspec_kw={'width_ratios': [1, 3, 0.2]})\n        fig.subplots_adjust(top=0.8, wspace=0.6)\n        fig.suptitle(f'Softmaxed weight matrices at step {i}', fontsize=16)\n        im0 = axs[0].pcolormesh(softmax(weights[aux_pred]).numpy(), cmap='viridis', vmin=0, vmax=1)\n        axs[0].set_title('Auxiliary predicate')\n        im1 = axs[1].pcolormesh(np.transpose(softmax(weights[target_pred]).numpy()), cmap='viridis', vmin=0, vmax=1)\n        axs[1].set_title('Target predicate')\n        fig.colorbar(im0, cax=axs[2])\n        plt.show()\n        if i != 500:\n            clear_output(wait=True)","7090351c":"fig, ax = plt.subplots()\nax.plot(losses)\nax.set_title('Loss')\nax.set_xlabel('Step')\nax.set_ylabel('Value')","7870703b":"extract_definitions(lernd_model.clauses, weights)\nground_atom_probs = get_ground_atom_probs(valuation, lernd_model.ground_atoms)\nprint_valuations(ground_atom_probs)","c2efd8ff":"Let's see the background knowledge and examples:","baa29f75":"In the `intro_predecessor.ipynb` notebook, the $predecessor$ problem is quite simple and does not showcase the\ncapabilities of **Lernd**.\n\nFor a more difficult problem, let's solve $even$ (this time without noise). **Lernd** should come up with the definition of predicate $even$ given\njust a successor predicate $succ$ and a few examples.\n\nThis is not a trivial problem, because an auxiliary predicate has to be invented and the solution has recursion in it.\nAn example solution could be:\n\n$target(A) \\leftarrow zero(A)$\n\n$target(A) \\leftarrow target(B),pred(B,A)$\n\n$pred(A,B) \\leftarrow succ(A,C),succ(C,B)$\n\nHere $pred$ is the auxiliary predicate, true if $A+2=B$. There may be variations.\n\n\nLet's define the problem:","aa370ec6":"Let's plot the loss to see how it went.","415905ba":"**Lernd** is my implementation of $\\partial$ILP framework. The original $\\partial$ILP paper $-$\n[Learning Explanatory Rules from Noisy Data](https:\/\/arxiv.org\/abs\/1711.04574).","fd0c8f8c":"Finally let's extract the results:","d72d47db":"I found that **Lernd** is able to solve this problem in 500 training steps 78% of the time (i. e. loss lower than 1e-4 was\nreached in 78 tries, out of a 100, and in 74 cases loss was exactly 0). Other times system gets stuck in a local\nminimum, weights oscillating. This could potentially be solved by splitting background axioms and examples into possible\nworlds, as suggested in the original paper.\n\nIn the next notebook we'll run **Lernd** on data with mislabelled examples.","aede54b2":"We can now run **Lernd**.\n\nNote that in this problem mini batching is used (taking random 30% of examples at every training step). This has a\ndramatic effect on performance as it helps to avoid local minima."}}