{"cell_type":{"5852de2c":"code","de604a48":"code","1b96ae5b":"code","8fb29dcd":"code","3d927a9e":"code","668ed656":"code","a7bd8cb2":"code","88445337":"code","fbe19560":"code","5ed7139e":"code","7b33fa19":"code","e9999640":"code","1ee63638":"code","4bee6d5e":"code","764ebf68":"code","4620a15b":"code","8d5ec240":"code","2076e48c":"code","6ea26808":"code","67d22664":"code","cf63d5ff":"code","7135064c":"code","13a3330b":"code","3f382739":"code","1bcf5ded":"code","3b71d2bd":"code","01c969fc":"code","93aaaa6f":"code","4312e9c0":"code","fccd1042":"markdown","3c75d7b7":"markdown","4d8f8825":"markdown","4fb5e451":"markdown","8e0d8bf9":"markdown","86695da2":"markdown","2b998320":"markdown","8e6538b7":"markdown","bd22d2bc":"markdown","8a024966":"markdown","42ee8db9":"markdown","ccbd347b":"markdown","9df16e53":"markdown","9757986b":"markdown"},"source":{"5852de2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de604a48":"data=pd.read_csv('\/kaggle\/input\/jobs-on-naukricom\/home\/sdf\/marketing_sample_for_naukri_com-jobs__20190701_20190830__30k_data.csv')","1b96ae5b":"data.head(10)","8fb29dcd":"data['Role'].value_counts()","3d927a9e":"#There are 649 different Role categories let's generalize it a bit \ndata['Role'] = data['Role'].str.lower()\ndata_job=data['Role'].astype(str) \n#Remove Special Chars from Role Category and splitting each word \ndata_job=data_job.str.replace('[+-\/\\,.()@:|;&_~]', ' ')    \ndata_job1=data_job.str.split(expand=True)\n\n#Getting the splitted words in one column for finding the frequencies for each unique word\ny=1\nfor x in range(0, 10):\n    dj1=data_job1[x].dropna()\n    dj1=dj1.to_frame(name=\"A\") \n    dj2=data_job1[y].dropna()\n    dj2=dj2.to_frame(name=\"A\") \n    dj3=dj1.append(dj2,ignore_index=True)\n    if x == 0:\n          dj4=dj3  \n    else:\n          dj4=dj4.append(dj3,ignore_index=True)\n    y=y+1\n\n#Getting the frequency of each unique word and sorting based on the frequencies.\ndj5 = dj4['A'].value_counts().rename_axis('unique_values').reset_index(name='counts')\ndj5[\"Rank\"] = dj5[\"counts\"]. rank(method='first',ascending=False) \n\n\n#Considering only the words which have the frequency 1000  or greater.\ndjX=dj5.loc[dj5['counts'] >= 1000]\n#making the list of considered words only \nsearchfor=djX.unique_values.tolist()\n\n#Creating a generalized job category with the most occouring categories\n#The loop is reversed because if any category has two words the word with highest frequency will be considered\ndata['Job_Cat']='other'\nindices = []    \nfor x in reversed(searchfor):\n    indices = []    \n    for i, elem in enumerate(data['Role'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Job_Cat')]=x\n            \n#data['Job_Cat'].value_counts()\n\nless_words=data['Job_Cat'].value_counts()\nless_words=less_words.reset_index()\nless_words=less_words.loc[less_words['Job_Cat'] < 100]\nless_words=less_words['index'].tolist()\n\nfor x in less_words:\n    indices = []    \n    for i, elem in enumerate(data['Job_Cat'].astype(str)):\n        if x == elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Job_Cat')]='other'\n    \ndata.loc[data['Job_Cat']=='nan', 'Job_Cat']= 'other'\n\ndata['Job_Cat'].value_counts()","668ed656":"data['Role Category'].value_counts()","a7bd8cb2":"#data['Role Category'].value_counts().head(10)\n\ndj5=data['Role Category'].value_counts().head(30)\ndj5=dj5.index.tolist()\n\ndata['New_Role_Category']='Other'\nfor x in dj5:\n    indices = []    \n    for i, elem in enumerate(data['Role Category'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('New_Role_Category')]=x    \n\ndata['New_Role_Category'].value_counts()","88445337":"#Removing the Blankspaces from start and end of the location and making it consistent using UpperCase\ndata['New_Location']=data['Location'].str.strip()\ndata['New_Location'] = data['New_Location'].str.upper()\n\n#Getting Indexes of locations where there are multiple locations (i.e. if Comma(,) is present)\nindices=[]\nfor i, elem in enumerate(data['New_Location'].astype(str)):\n        if ',' in elem:\n            indices.append(i)\n#Updaing the new location values in those rows with 'Multi' category\ndata.iloc[indices,data.columns.get_loc('New_Location')]='Multi'\ndata['New_Location'].value_counts()\n\n#taking the top 10 locations along with multi, so top 11 locations\nlocations=data['New_Location'].value_counts().head(11)\nlocations=locations.index.tolist()\n\n# using the top 10 locations to genralize the locations. \n#Here we have used in opreator for cases like 'MUMBAI (WADALA)' will be considered as 'MUMBAI'\ndata['final_location']='OTHER'\nfor x in locations:\n    indices = []    \n    for i, elem in enumerate(data['New_Location'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('final_location')]=x\n\n\ndata['final_location'].value_counts()","fbe19560":"data['New_Job_Salary']=data['Job Salary'].str.strip()\ndata['New_Job_Salary'] = data['New_Job_Salary'].str.upper()\n\ndata.loc[(data['New_Job_Salary']=='OPENINGS: 1') | (data['New_Job_Salary']=='NOT DISCLOSED')\n,'New_Job_Salary']= 'NOT DISCLOSED BY RECRUITER'\n\ndata.loc[(data['New_Job_Salary']=='BEST IN THE INDUSTRY') \n| (data['New_Job_Salary']=='OPENINGS: 2')\n| (data['New_Job_Salary']=='BEST IN INDUSTRY')\n,'New_Job_Salary']= 'NOT DISCLOSED BY RECRUITER'\n\ndata.loc[data['New_Job_Salary']==',', 'New_Job_Salary']= ''\ndata.loc[data['New_Job_Salary']=='PA.', 'New_Job_Salary']= ''\n\ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace(',', '')    \ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace('PA.', '')    \ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace('INR.', '')    \n\ndata['New_Job_Salary'] =data['New_Job_Salary'].str.extract(r'(\\d+ - \\d+)')\n\ndata[['Min_Salary','Max_Salary']]=data['New_Job_Salary'].str.split('-',1,expand=True)\ndata['Min_Salary']=data['Min_Salary'].str.strip()\ndata['Max_Salary']=data['Max_Salary'].str.strip()\n\ndata[['New_Job_Salary','Min_Salary','Max_Salary']].head(10)","5ed7139e":"data['Industry_new']=data['Industry'].str.replace(',\\s',',')\ndata[['Industry_new','X']]=data['Industry_new'].str.split(',',1,expand=True)\ndata['Industry_new']=data['Industry_new'].str.strip()\n\n\nR=data['Industry_new'].value_counts().head(20)\nR.reset_index()\nR=R.drop(columns=['Industry_new'])    \n    \ndata['Industry_Final']='Other'\nindices = []    \nfor x in reversed(R.index):\n    indices = []    \n    for i, elem in enumerate(data['Industry'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Industry_Final')]=x\n    \n    \ndata['Industry_Final'].value_counts()\n","7b33fa19":"data['Experience']=data['Job Experience Required'].str.strip()\ndata['Experience'] =data['Experience'].str.extract(r'(\\d+ - \\d+)')\n\ndata['Experience'].value_counts().sum()\n\ndata[['Min_Ex','Max_Ex']]=data['Experience'].str.split('-',1,expand=True)\ndata['Min_Ex']=data['Min_Ex'].str.strip()\ndata['Max_Ex']=data['Max_Ex'].str.strip()","e9999640":"import plotly.express as px\n\ndata['Key Skills']=data['Key Skills'].str.strip()\ndata['Key Skills'] = data['Key Skills'].str.upper()\ndata_skill=data['Key Skills'].str.split('|',expand=True)\n\ndata_skill=data['Key Skills'].str.split('|',expand=True)\ndata_skill['Industry']=data['Industry_Final']\nIndustries= data['Industry_Final'].value_counts().rename_axis('industry').reset_index(name='counts').head(5)\nfor K in Industries['industry']:\n    data_IT=data_skill.iloc[np.where(data_skill['Industry']==K)]\n    data_IT=data_IT.reset_index(drop=True)\n    y=1\n    length=len(data_IT.columns)\n    length=length-1\n    for x in range(length):\n        djj1=data_IT[x].dropna()\n        djj1=djj1.to_frame(name=\"A\") \n        djj2=data_IT[y].dropna()\n        djj2=djj2.to_frame(name=\"A\") \n        djj3=djj1.append(djj2,ignore_index=True)\n        if x == 0:\n            djj4=djj3  \n        else:\n            djj4=djj4.append(djj3,ignore_index=True)\n            y=y+1\n    djj4['A'] = djj4['A'].str.upper()\n    djj4['A'] = djj4['A'].str.strip()\n    djj6= djj4['A'].value_counts().rename_axis('unique_values').reset_index(name='counts').head(20)\n    djj6=djj6.sort_values(by='unique_values')\n    fig3 = px.line_polar(djj6,r='counts', theta='unique_values', line_close=True)\n    fig3.update_traces(fill='toself')  \n    fig3.update_layout(\n    height=400,\n    title_text=K)\n    fig3.show()\n    ","1ee63638":"data1=data[['Uniq Id', 'Job_Cat', 'New_Role_Category','final_location',\n            'Industry_Final', 'Min_Salary', 'Max_Salary', 'Min_Ex','Max_Ex']]\ndata1","4bee6d5e":"data1.head(5)","764ebf68":"import matplotlib.pyplot as plt\n\ndata_loc=data1['final_location'].value_counts()\ndata_loc = data1['final_location'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)\n","4620a15b":"data_loc=data1['Industry_Final'].value_counts()\ndata_loc = data1['Industry_Final'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)","8d5ec240":"data_loc=data1['Job_Cat'].value_counts()\ndata_loc = data1['Job_Cat'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)","2076e48c":"table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False,margins=True,margins_name='Total')\n#table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False)\ntable = table.drop('Total')\ntable=table.sort_values(by=['Total'],ascending=False)\ntable","6ea26808":"table1 = pd.crosstab(data1['Industry_Final'], data1['final_location'], dropna=False,margins=True,margins_name='Total')\n#table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False)\ntable1 = table1.drop('Total')\ntable1=table1.sort_values(by=['Total'],ascending=False)\ntable1","67d22664":"table2 = pd.crosstab(data1['Job_Cat'], data1['Industry_Final'], dropna=False,margins=True,margins_name='Total')\ntable2 = table2.drop('Total')\ntable2=table2.sort_values(by=['Total'],ascending=False)\ntable2","cf63d5ff":"dataSaL=data1.dropna(subset=['Min_Salary','Max_Salary'])\n\ndataSaL.S_Max_sal=0\ndataSaL.S_Min_sal=0\ndataSaL=dataSaL.reset_index(drop=True)\n\ndataSaL['Min_Salary']= dataSaL[\"Min_Salary\"].astype(int) \ndataSaL['Max_Salary']= dataSaL[\"Max_Salary\"].astype(int)\n\n\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(0, 250000), 'S_Min_sal']= 'A. 0L - 2.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(250001, 500000), 'S_Min_sal']= 'B. 2.5L - 5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(500001, 750000), 'S_Min_sal']= 'C. 5L - 7.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(750001, 1000000), 'S_Min_sal']= 'D. 7.5L - 10L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1000001, 1250000), 'S_Min_sal']= 'E. 10L - 12.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1250001, 1500000), 'S_Min_sal']= 'F. 12.5L - 15L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1500001, 2000000), 'S_Min_sal']= 'G. 15L - 20L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(2000001, 3000000), 'S_Min_sal']= 'H. 20L - 30L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(3000001, 5000000), 'S_Min_sal']= 'I. 30L - 50L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(5000001, 10000000), 'S_Min_sal']='J. 50L - 1CR'\n\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(0, 250000), 'S_Max_sal']= 'A. 0L - 2.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(250001, 500000), 'S_Max_sal']= 'B. 2.5L - 5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(500001, 750000), 'S_Max_sal']= 'C. 5L - 7.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(750001, 1000000), 'S_Max_sal']= 'D. 7.5L - 10L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1000001, 1250000), 'S_Max_sal']= 'E. 10L - 12.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1250001, 1500000), 'S_Max_sal']= 'F. 12.5L - 15L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1500001, 2000000), 'S_Max_sal']= 'G. 15L - 20L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(2000001, 3000000), 'S_Max_sal']= 'H. 20L - 30L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(3000001, 5000000), 'S_Max_sal']= 'I. 30L - 50L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(5000001, 10000000), 'S_Max_sal']='J. 50L - 1CR'\n\ndataSaL['S_Max_sal'].value_counts()\n","7135064c":"FL=dataSaL.groupby('final_location').median()\nFL.sort_values(by='Min_Salary',ascending=False)","13a3330b":"IF=dataSaL.groupby('Industry_Final').median()\nIF.sort_values(by='Min_Salary',ascending=False)","3f382739":"JC=dataSaL.groupby('Job_Cat').median()\nJC.sort_values(by='Min_Salary',ascending=False)","1bcf5ded":"tablesal = pd.crosstab(dataSaL[\"S_Max_sal\"], dataSaL[\"S_Min_sal\"], dropna=False,margins=True,margins_name='Total')\ntablesal","3b71d2bd":"dataSaL2=dataSaL.dropna(subset=['Min_Ex','Max_Ex'])\ndataSaL2=dataSaL2.reset_index(drop=True)\n\n\ndataSaL2['Min_Ex']= dataSaL2[\"Min_Ex\"].astype(int) \n\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]== 0 ,'S_Min_Ex']= 'A. 0 Year'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]== 1 , 'S_Min_Ex']= 'B. 1 Year'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==2, 'S_Min_Ex']= 'C. 2 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==3, 'S_Min_Ex']= 'D. 3 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==4, 'S_Min_Ex']= 'E. 4 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==5, 'S_Min_Ex']= 'F. 5 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(6, 7), 'S_Min_Ex']='G. 6-7 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(8, 10), 'S_Min_Ex']='H. 8-10 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(11, 12), 'S_Min_Ex']='I. 11-12 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(13, 15), 'S_Min_Ex']='J. 13-15 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"] > 15, 'S_Min_Ex']='K. 15+ Years'\n\ndataSaL2['S_Min_Ex'].value_counts()\n","01c969fc":"tablesalEx = pd.crosstab(dataSaL2[\"Job_Cat\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","93aaaa6f":"tablesalEx = pd.crosstab(dataSaL2[\"final_location\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","4312e9c0":"tablesalEx = pd.crosstab(dataSaL2[\"Industry_Final\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","fccd1042":"# **Cities, Industries and Job Types With Most Opportunities**","3c75d7b7":"# **Handling Industry**\n\n* Lets Generalize the Industries also.","4d8f8825":"# Experience And Salaries","4fb5e451":"There are 206 distinct role categories lets genralize them too.","8e0d8bf9":"We have generalized the Roles to a point from which it is easy to analyse.","86695da2":"# CrossTabs For Detailed Analysis","2b998320":"**Handling Salaries**","8e6538b7":"# **Key Skills required for top 5 Industries**","bd22d2bc":"**Handling Experience**","8a024966":"# There are 649 different Roles let's generalize them a bit\n\n* We would be taking the top chunk of most occouring words in thr 'Role' field and use them to generalize the Roles.\n\n* Same is applied for other String Columns","42ee8db9":"# **Generalizing the Role Category**","ccbd347b":"#  Get the Required New Fields in new dataset ","9df16e53":"Let's Clean the Salary field and split them.","9757986b":"**Generalizing the Locations**"}}