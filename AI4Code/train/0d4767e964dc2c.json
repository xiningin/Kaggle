{"cell_type":{"f7f0828a":"code","2c271995":"code","1460914a":"code","337831b4":"code","a2e900c1":"code","4487f3f6":"code","8c43b7c7":"code","ca30e31d":"code","27a1625c":"code","2fd8dca1":"code","9b4da1fb":"code","f33c3da0":"code","d519bf5f":"code","96bbbca5":"markdown","2fb86dd0":"markdown","f119a2e7":"markdown","798e91be":"markdown","0abd3acf":"markdown","094bce85":"markdown","f9244aa5":"markdown","4976ba56":"markdown","5cc0aaa4":"markdown"},"source":{"f7f0828a":"import numpy as np\nimport pandas as pd\nimport nltk","2c271995":"#nltk.download('gutenberg')\ncorpuses = [' '.join(list(nltk.corpus.gutenberg.words(corpus))) for corpus in nltk.corpus.gutenberg.fileids()][:2]\n#corpus = list(nltk.corpus.gutenberg.words('carroll-alice.txt'))\n#corpus_string = ' '.join(corpus)\n\nprint(len(corpuses), 'corpuses found')\nprint(sum([len(x) for x in corpuses]), 'characters in corpuses')","1460914a":"PADDING_CHAR = -1\nUNKNOWN_CHAR = -2","337831b4":"characters = sorted(list(set(''.join(corpuses))))\nchar_indices = dict((c, i) for i, c in enumerate(characters))\nindices_char = dict((i, c) for i, c in enumerate(characters))\n\nchar_indices[''] = PADDING_CHAR\nindices_char[PADDING_CHAR] = ''\nindices_char[UNKNOWN_CHAR] = ''\n\nprint(len(characters), 'unique characters in the corpus')","a2e900c1":"def string_to_indices(string, ind_map=char_indices):\n    return [ind_map[c] if c in ind_map else UNKNOWN_CHAR for c in string]\n\ndef indices_to_string(indices, char_map=indices_char):\n    return ''.join([char_map[i] for i in indices])\n\ndef char_to_probabilities(char, chars=characters):\n    return [1 if c==char else 0 for c in chars]","4487f3f6":"SEQUENCE_LENGTH = 40\nSTEP = 1\n\nsentences = []\nnext_chars = []\nfor corpus_string in corpuses:\n    for i in range(0, len(corpus_string) - 1, STEP):\n        sentences.append(corpus_string[i: i + SEQUENCE_LENGTH])\n        next_chars.append(corpus_string[min(i + SEQUENCE_LENGTH, len(corpus_string) - 1)])\n\nprint(len(sentences), 'training data points')","8c43b7c7":"def prepare_text_indices(text):\n    ind_text = string_to_indices(text)\n    if len(ind_text) < SEQUENCE_LENGTH:\n        ind_text = ([PADDING_CHAR] * (SEQUENCE_LENGTH - len(ind_text))) + ind_text\n    elif len(ind_text) > SEQUENCE_LENGTH:\n        ind_text = ind_text[-SEQUENCE_LENGTH:]\n    return np.array(ind_text) \/ len(characters)","ca30e31d":"X_train = np.array([prepare_text_indices(s) for s in sentences])\nX_train = X_train.reshape((X_train.shape[0], SEQUENCE_LENGTH, 1))\nY_train = np.array([char_to_probabilities(c) for c in next_chars])","27a1625c":"from keras.models import Sequential, load_model\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM, Dropout\nfrom keras.layers import TimeDistributed\nfrom keras.layers.core import Dense, Activation, Dropout, RepeatVector\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\n\ndef build_model(input_shape, output_shape, weight_path=None):\n    model = Sequential()\n    model.add(LSTM(512, input_shape=input_shape, return_sequences=True, dropout=0.25))\n    #model.add(LSTM(512, return_sequences=True, dropout=0.25))\n    model.add(LSTM(512, dropout=0.25))\n    model.add(Dense(output_shape))\n    model.add(Activation('softmax'))\n    \n    if weight_path != None:\n        model.load_weights(weight_path)\n    \n    optimizer = Adam(learning_rate=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    return model","2fd8dca1":"# Get model architecture\nmodel = build_model((SEQUENCE_LENGTH, 1), len(characters), weight_path='..\/input\/predictive-text-model-weights\/weights-512-512.hdf5')\n\n# Fit model and save best weights\ncheckpoint = ModelCheckpoint('weights-512-512.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min')\nmodel.fit(X_train, Y_train, batch_size=512, epochs=50, shuffle=True, callbacks=[checkpoint])\n\n# Load trained weights\nmodel = build_model((SEQUENCE_LENGTH, 1), len(characters), weight_path='weights-512-512.hdf5')","9b4da1fb":"def make_char_prediction(text, model=model, pred_length=SEQUENCE_LENGTH):\n    ind_text = prepare_text_indices(text)\n        \n    X_text = np.array(ind_text).reshape((1, pred_length, 1))\n    pred = model.predict(X_text)\n    top_pred = np.argmax(pred)\n    char_pred = characters[top_pred]\n    return char_pred\n\ndef make_word_prediction(text, model=model, min_length=2, pred_length=SEQUENCE_LENGTH):\n    pred_text = ''\n    while True:\n        x = (text + pred_text)[-SEQUENCE_LENGTH:]\n        pred = make_char_prediction(x, model)\n        pred_text += pred\n        if len(pred_text) >= min_length and (pred == ' '):\n            return pred_text","f33c3da0":"test_texts = [\"I\\'ve believed as many as six impossible \",\n              \"no use going back to yesterday, because \",\n              \"A pessimist sees the difficulty in every\",\n              \"sometimes you just have to create a rand\",\n              \"The score was nil nil until the striker \",\n              \"if I could dream of such things then the\",\n              \"We should follow the path on the left as\",\n              \"Let\\'s write a short program to display a\",\n              \"no\"]\n\ntest_preds = []\ntest_preds_long = []\nfor text in test_texts:\n    test_preds.append(make_word_prediction(text, model=model))\n    test_preds_long.append(make_word_prediction(text, model=model, min_length=10))\n\npd.DataFrame({'Text' : test_texts, 'Prediction' : test_preds, 'Long Prediction' : test_preds_long})","d519bf5f":"long_test_text = 'We should follow the path on the left as'\nnew_long_test_text = long_test_text + make_word_prediction(long_test_text, model=model, min_length=20)\n\nprint('Original:', long_test_text)\nprint('New:     ', new_long_test_text)","96bbbca5":"The following cells will be used to build and train our model. The first cell defines our model architecture. It also allows us to load model weights if we have saved them previously. This method is used in the second cell, where we train our model and then reload the best weights (in terms of training loss).","2fb86dd0":"# Make Predictions\nWe are going to create two functions that will allow us to make predictions. The first function will take our text and predict the next character. The second function will take our text and predict the next word (or word ending) by making use of our first function.","f119a2e7":"# Build Model","798e91be":"Now we will take a few examples and make some predictions.","0abd3acf":"# Import Corpuses\n\nNext we will import the corpuses that we are going to use to train our model. These corpuses are taken from Project Gutenberg.","094bce85":"This initial concept is based upon [this Medium post](https:\/\/medium.com\/@curiousily\/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218) by Venelin Valkov. The model from the article has been modified in this notebook to allow multiple LSTM layers, and to accept prediction inputs of less than the maximum sequence length.\n\n - Use a pre-trained model so that training starts from a better position","f9244aa5":"We will need a unique list of characters that are found in our corpus. We will also need two dictionaries that allow us to convert our characters into unique integers and vice versa, so they will also be created below. We define two extra characters that can be used by our model; the first is a padding character so that we can take smaller inputs, while the second is an unknown character incase we come across characters that were not in our training data.","4976ba56":"The following method allows us to convert our text into a list of indices that can be understood by our model. We will then prepare our sentences and characters from our corpus so they can be used to train our model.","5cc0aaa4":"# Prepare Training Data\n\nNow we will split our corpus into training data, where each data point has a size SEQUENCE_LENGTH. In this case we will use a sequence length of 40, meaning our predictions will be based on 40 previous characters. STEP allows us to choose how many data points we have, as each data point will be separated by STEP characters. Setting STEP to 1 allows us to select every possible point."}}