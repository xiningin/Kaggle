{"cell_type":{"4b55cf2f":"code","b0b94241":"code","aeeacf1a":"code","1d512007":"code","b8c43a4c":"code","d71fe030":"code","bf6ddc2c":"code","325c598a":"code","b09b880b":"code","136d89c6":"code","f28709c9":"code","5d710605":"code","873de799":"code","b64364eb":"code","7d1066d1":"code","1cd337eb":"code","0ccf3b80":"markdown","fdbda303":"markdown","fa927ee6":"markdown","32120b75":"markdown","0fbae5c4":"markdown","a8d36b55":"markdown","0220ff74":"markdown","26b0bd08":"markdown","a5868235":"markdown"},"source":{"4b55cf2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0b94241":"df = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv',\n                encoding = 'latin-1')\nprint('dataset shape: ', df.shape)\ndf.head(3)","aeeacf1a":"df.info()","1d512007":"# label \uac12\uc778 target \uc18d\uc131 \uac12 \ubd84\ud3ec \uc54c\uc544\ubcf4\uae30\nprint(df['TARGET'].value_counts())\nunsatisfied_cnt = df[df['TARGET'] == 1].TARGET.count()\ntotal_cnt = df.TARGET.count()\nprint('unsatisfied \ube44\uc728\uc740 {0:.2f}'.format((unsatisfied_cnt \/ total_cnt)))","b8c43a4c":"df.describe()","d71fe030":"# \ubd84\ud3ec\ub97c \uc0b4\ud3b4\ubcfc \ub54c `var3` \ubcc0\uc218\uc758 min \uac12\uc5d0\uc11c \uc774\uc0c1\uce58 \ubc1c\uacac\n# \ub610\ud55c `ID` feature\ub294 \ub2e8\uc21c \uc2dd\ubcc4\uc790\uc774\uae30 \ub54c\ubb38\uc5d0 \uc0ad\uc81c\n\ndf['var3'].replace(-999999, df['var3'].mode()[0], inplace=True) # \ucd5c\ube48\uac12\uc73c\ub85c \ub300\uccb4\ndf.drop('ID', axis=1, inplace=True)\n\n# feature\uc640 label set \ubd84\ub9ac\nX_features = df.iloc[:, :-1]\ny_label = df.iloc[:, -1]\nprint('feature data shape: {0}'.format(X_features.shape))","bf6ddc2c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features,\n                                                   y_label,\n                                                   test_size = .2, \n                                                   random_state = 0)\n\ntrain_cnt = y_train.count()\ntest_cnt = y_test.count()\nprint('train set shape: {0}, test set shape: {1}'.format(X_train.shape, X_test.shape))\n\nprint('train set label \uac12 \ubd84\ud3ec \ube44\uc728')\nprint(y_train.value_counts() \/ train_cnt)\nprint('test set label \uac12 \ubd84\ud3ec \ube44\uc728')\nprint(y_test.value_counts() \/ test_cnt)","325c598a":"# xgboost \ud559\uc2b5 \ubaa8\ub378\uc744 \uc0dd\uc131\ud558\uace0 \uc608\uce21 \uacb0\uacfc\ub97c ROC AUC\ub85c \ud3c9\uac00\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nxgb = XGBClassifier(n_estimators = 500,\n                   random_state = 156)\n\nxgb.fit(X_train, y_train,\n       early_stopping_rounds = 100,\n       eval_metric = 'auc', \n       eval_set = [(X_train, y_train), (X_test, y_test)])\n\nxgb_roc_score = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1],\n                             average = 'macro')\nprint('ROC AUC: {0: 4f}'.format(xgb_roc_score))","b09b880b":"# XGboost hyper-parameter \ud29c\ub2dd \uc218\ud589\nfrom sklearn.model_selection import GridSearchCV\n\n# hyper-parameter test\uc758 \uc218\ud589 \uc18d\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 n_estimator\ub97c 100\uc73c\ub85c \uac10\uc18c\nxgb = XGBClassifier(n_estimator = 100)\n\nparams = {\n    'max_depth' : [5, 7],\n    'min_child_weight' : [1, 3],\n    'colsample_bytree' : [0.5, 0.75]\n}\n\ngridcv = GridSearchCV(xgb, param_grid = params, cv = 3)\ngridcv.fit(X_train, y_train,\n          early_stopping_rounds = 30,\n          eval_metric = 'auc', \n          eval_set = [(X_train, y_train), (X_test, y_test)])\nprint('GridSearchCV \ucd5c\uc801 \ud30c\ub77c\ubbf8\ud130: ', gridcv.best_params_)\n\nxgb_roc_score = roc_auc_score(y_test,\n                             gridcv.predict_proba(X_test)[:, 1],\n                             average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","136d89c6":"# \uc774\uc804\uc5d0 \uc218\uc815\ud55c \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub354\ud574 \ub2e4\ub978 \ud30c\ub77c\ubbf8\ud130 \uc218\uc815\uc744 \uc9c4\ud589\nxgb = XGBClassifier(n_estimator = 1000,\n                   random_state = 156, \n                   learning_rate = .02,\n                   max_depth = 7,\n                   min_child_weight = 3,\n                   colsample_bytree = .75,\n                   reg_alpha = .03)\n\nxgb.fit(X_train, y_train,\n       early_stopping_rounds = 200,\n       eval_metric = 'auc', \n       eval_set = [(X_train, y_train), (X_test, y_test)])\n\nxgb_roc_score = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1],\n                             average = 'macro')\n\nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","f28709c9":"# feature importance \ud655\uc778\ud558\uae30\nfrom xgboost import plot_importance\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nplot_importance(xgb, ax=ax,\n               max_num_features = 20, \n               height = 0.4)","5d710605":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(n_estimator = 500)\nevals = [(X_test, y_test)]\n\nlgbm.fit(X_train, y_train,\n        early_stopping_rounds = 100,\n        eval_metric = 'auc',\n        eval_set = evals,\n        verbose = True)\nlgbm_roc_score = roc_auc_score(y_test, lgbm.predict_proba(X_test)[:, 1],\n                               average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","873de799":"# \uc774\uc804\uc5d0 xgboost\ubcf4\ub2e4 \uac12\uc774 \uac10\uc18c \n# hyper-parameter \uc870\uc815 \uc9c4\ud589\n\nlgbm = LGBMClassifier(n_estimators = 200)\n\nparams = {\n    'num_leaves' : [32 ,64],\n    'max_depth' : [128, 160],\n    'min_child_samples' : [60, 100],\n    'subsample' : [0.8, 1]\n}\n\ngridcv = GridSearchCV(lgbm, param_grid = params, cv = 3)\ngridcv.fit(X_train, y_train,\n          early_stopping_rounds = 30,\n          eval_metric = 'auc',\n          eval_set = [(X_train, y_train), (X_test, y_test)])\nprint('GridSearchCV \ucd5c\uc801 \ud30c\ub77c\ubbf8\ud130: ', gridcv.best_params_)\nlgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1],\n                              average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","b64364eb":"lgbm = LGBMClassifier(n_estimator = 1000,\n                     num_leaves = 32,\n                     sumbsample = 0.8,\n                     min_child_samples = 100,\n                     max_depth = 128)\nevals = [(X_test, y_test)]\nlgbm.fit(X_train, y_train,\n        early_stopping_rounds = 100,\n        eval_metric = 'auc',\n        eval_set = evals,\n        verbose = True)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm.predict_proba(X_test)[:, 1],\n                               average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","7d1066d1":"# \uc81c\ucd9c\ud558\uae30\ntest = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv', encoding = 'latin-1')\ntest_id = test['ID']\ntest.drop('ID', axis=1, inplace = True)\ntest.head(3)","1cd337eb":"pred = lgbm.predict_proba(test)[:, 1]\n\nsubmission = pd.DataFrame({'ID' : test_id, 'TARGET' : pred})\nsubmission.to_csv('submission.csv', index=False)\n\nprint('completed!')","0ccf3b80":"## [Tutorial] Santander Customer Satisfaction \uc0b0\ud0c4\ub370\ub974 \uc740\ud589 \uace0\uac1d \ub9cc\uc871 \uc608\uce21\n### \ucc45 <\ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \uc644\ubcbd \ub9c8\uc2a4\ud130> \ud544\uc0ac \ucf54\ub4dc\uc785\ub2c8\ub2e4.","fdbda303":"\ucd5c\uc801 \uac12\uc740 84.42%\ub85c \ub098\ud0c0\ub0a9\ub2c8\ub2e4.","fa927ee6":"ROC AUC\uac12\uc774 84.42%\ub85c \ud5a5\uc0c1\ub41c \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucd5c\uc801 \ud30c\ub77c\ubbf8\ud130\ub97c \uc801\uc6a9\ud558\uc5ec \uc7ac\ud559\uc2b5 \ud6c4, \uce21\uc815 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","32120b75":"\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\ub85c \uc608\uce21 \uc2dc ROC AUC\ub294 \uc57d 84% \uc785\ub2c8\ub2e4.","0fbae5c4":"---","a8d36b55":"LightGBM \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\ud558\uae30","0220ff74":"\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \ucd94\uac00\ud558\uc5ec \uc218\uc815\ud558\ub2c8 84.2% \uc77c\ubd80 \ud5a5\uc0c1\ud558\uc600\uc2b5\ub2c8\ub2e4.","26b0bd08":"\ud559\uc2b5\uacfc \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8 \ubaa8\ub450 `TARGET`\uc758 \uac12\uc758 \ubd84\ud3ec\uac00 \uc6d0\ubcf8 \ub370\uc774\ud130\uc640 \uc720\uc0ac\ud558\uac8c \uc804\uccb4 \ub370\uc774\ud130\uc758 4% \uc815\ub3c4\ub85c \ubd88\ub9cc\uc871 \uac12\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc84c\uc2b5\ub2c8\ub2e4.","a5868235":"\uc774\uc804 \uc608\uc81c\uc5d0\uc11c hyper-parameter\ub97c \uc801\uc6a9\ud55c \uc774\ud6c4 84.18%\ub85c \uc870\uae08 \uac1c\uc120\ub418\uc5c8\uc2b5\ub2c8\ub2e4."}}