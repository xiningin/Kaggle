{"cell_type":{"9b5f639f":"code","f8695b0a":"code","d09ad122":"code","a92bc375":"code","a769d809":"code","97f7c86c":"code","a79381e8":"code","91933091":"code","7b770c18":"code","cdcbd82e":"code","6c76fcd5":"code","70409c3d":"code","727de8f1":"code","b2c3aa50":"code","685246fa":"code","f0899026":"code","443dfb0a":"code","80783037":"code","b640c414":"code","591f2ec5":"code","3b1fa4c9":"code","d5af7a45":"code","39182edd":"code","41dde016":"code","647154fe":"code","2ffed6c3":"code","3e2f6b0f":"code","1759368c":"code","89cdf34a":"code","6150bac5":"code","8681d622":"code","8d1ae3b5":"code","de6129d7":"code","2b8a6750":"code","850c7523":"code","ab72da33":"code","2f2ba355":"code","5c7cced4":"code","3fa0f77f":"code","6cd87c09":"code","c2e1726b":"code","e8d140a4":"code","48b7b555":"code","c6867c53":"code","04e315fa":"code","7688cd2d":"code","d9c9cfa9":"code","57c70347":"code","441d64ee":"code","23bacfae":"code","57d6eb97":"code","fb729add":"markdown","575e864a":"markdown","50d1ef73":"markdown","9ff87a65":"markdown","9bf7a63d":"markdown","4a13793b":"markdown","79ce7484":"markdown","22236294":"markdown","0456f9a7":"markdown","687a1e4e":"markdown","d718b4e7":"markdown","f0ea5269":"markdown","34fda894":"markdown","e7b11754":"markdown","481724f1":"markdown","c043848c":"markdown","78c07591":"markdown","bf506928":"markdown","473e8d30":"markdown","7d37bee7":"markdown","6a8c0619":"markdown"},"source":{"9b5f639f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint(\"Setup Complete\")\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8695b0a":"full_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n#full_train.head()","d09ad122":"full_train.describe()","a92bc375":"full_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n#full_test.head()","a769d809":"full_test.describe()","97f7c86c":"## merge data for features explorations\nfull_train[\"DataType\"]=1\nfull_test[\"DataType\"]=2\nfull_data = full_train.append(full_test)\nfull_data.describe()","a79381e8":"full_data.describe(include=[\"object\"])","91933091":"full_data.info()","7b770c18":"clean_full_data = full_data.copy()\n\nlabel_encoder = LabelEncoder()\nclean_full_data[\"Sex\"] = label_encoder.fit_transform(clean_full_data[\"Sex\"])\nlabel_encoder = LabelEncoder()\nclean_full_data[\"Embarked_backup\"] = label_data[\"Embarked\"]\nclean_full_data[\"Embarked\"] = label_encoder.fit_transform(clean_full_data[\"Embarked\"].astype(str))\n\n#Embarked 0-C \/ 1-Q \/ 3-S\n\nlabel_data.head()","cdcbd82e":"clean_full_data[[\"Age\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]].corr()","6c76fcd5":"## Try to extract salutation.\nclean_full_data[[\"Salutation\"]]=pd.DataFrame(clean_full_data.apply(lambda x: x.Name.split(\",\")[1].split(\".\")[0], axis=1),columns=[\"Salutation\"])\n#print(label_data.groupby(['Salutation'])['Age'].agg(['count','mean', 'median', 'min', 'max']).round(1))\n#print(label_data.groupby(['Pclass', 'Salutation'])['Age'].agg(['count','mean', 'median', 'min', 'max']).round(1))\n#print(label_data.groupby(['Pclass', 'Sex'])['Age'].agg(['count','mean', 'median', 'min', 'max']).round(1))\n#print(label_data.groupby(['Sex', 'Salutation']).size())","70409c3d":"fillna_data=clean_full_data.groupby(['Pclass', 'Salutation'])['Age'].agg(['mean'])\nfillna_data","727de8f1":"fillna_data2=label_data.groupby(['Pclass', 'Sex'])['Age'].agg(['mean'])\nfillna_data2","b2c3aa50":"clean_full_data[[\"Age_backup\"]]=label_data[[\"Age\"]]\nclean_full_data2 = pd.merge(left=clean_full_data, right=fillna_data, how='left', left_on=['Pclass', 'Salutation'], right_on=['Pclass', 'Salutation'])\n\nclean_full_data2[\"Age\"] = clean_full_data2.apply(\n    lambda row: row['mean'] if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)\nclean_full_data2[\"Age\"] = clean_full_data2.apply(\n    lambda row: 22.185329 if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)\n\n\nclean_full_data2.describe()","685246fa":"clean_full_data.groupby(['Pclass'])['Fare'].agg(['mean'])","f0899026":"clean_full_data2[\"Fare\"] = clean_full_data2.apply(\n    lambda row: 13.302889 if np.isnan(row['Fare']) else row['Fare'],\n    axis=1\n)","443dfb0a":"clean_full_data2.describe()","80783037":"#label_data2.tail(10)","b640c414":"#Get Final Data\nfinal_full_train=clean_full_data2.loc[clean_full_data2['DataType'] == 1]\nfinal_full_test=clean_full_data2.loc[clean_full_data2['DataType'] == 2]\n\n#features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\"]\n#y = final_full_train.Survived\n#X = final_full_train[features]\n#train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n\n","591f2ec5":"final_full_train[[\"Survived\", \"Age\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]].corr()","3b1fa4c9":"#data_check = final_full_train[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]]\n#data_check[data_check.isnull().any(axis=1)]","d5af7a45":"final_full_train[\"cntPartners\"] = final_full_train.SibSp + final_full_train.Parch\nfinal_full_train[\"Age_group\"] = final_full_train.Age.round(-1)","39182edd":"final_full_train.describe()","41dde016":"features_all = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\", \"cntPartners\", \"Age_group\"]\n\ny = final_full_train.Survived\nX = final_full_train[features_all]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n","647154fe":"def model_trial(features_trial, model_a, title):\n    i = 1\n    #i_best = 0\n    \n    for feature in features_trial:\n        model_a.fit(train_X[feature], train_y)\n        #print(train_X[feature])\n        val_predictions = model_a.predict(val_X[feature])\n        val_mae = mean_absolute_error(val_predictions, val_y)\n        \n        print(i, \"Validation MAE for \", title, \" Model: \",  (val_mae))\n        \n        if i == 1:\n            i_best = 1\n            val_mae_best = val_mae\n        elif val_mae < val_mae_best:\n            i_best = i\n            val_mae_best = val_mae\n            \n        i += 1\n        \n    print(\"Best feature is \", i_best)   \n    \n    return i_best","2ffed6c3":"features_trial = []\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"cntPartners\", \"Embarked\", \"Fare\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age_group\", \"cntPartners\", \"Embarked\", \"Fare\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"cntPartners\", \"Embarked\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"cntPartners\", \"Embarked\"])\nfeatures_trial.append([\"Pclass\",  \"Age_group\", \"cntPartners\"])\nfeatures_trial.append([\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Embarked\", \"Fare\"])","3e2f6b0f":"model_trial(features_trial, DecisionTreeRegressor(random_state=0), \"Decision Tree\")","1759368c":"model_trial(features_trial, RandomForestRegressor(random_state=0), \"Random Forest\")","89cdf34a":"model_trial(features_trial, RandomForestClassifier(random_state=0), \"Random Forest\")","6150bac5":"model_trial(features_trial, RadiusNeighborsClassifier(radius=30), \"KNN\")","8681d622":"model_trial(features_trial, LogisticRegression(random_state=0), \"Logistic Regression\")","8d1ae3b5":"final_feature = features_trial[2]\nfinal_model = RandomForestClassifier(random_state=0)\nfinal_model.fit(X[final_feature], y)\nfinal_val_predictions = final_model.predict(X[final_feature])\nfinal_val_mae = mean_absolute_error(final_val_predictions, y)\n        \nprint(\"Validation MAE for Final Model: \",  final_val_mae)","de6129d7":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X[final_feature], train_y)\n    preds_val = model.predict(val_X[final_feature])\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","2b8a6750":"for max_leaf_nodes in [5, 48, 49, 50, 51, 52, 53, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: \", max_leaf_nodes,\" Mean Absolute Error:  \", my_mae)","850c7523":"my_maes=[]\nfor max_leaf_nodes in range(2,500,10):\n    my_mae = get_mae(max_leaf_nodes + 1, train_X, val_X, train_y, val_y)\n    my_maes.append(my_mae)\n    \n#sns.lineplot(x=my_maes, y=range(2,5000,10))","ab72da33":"sns.lineplot(x=range(2,500,10), y=my_maes)","2f2ba355":"sns.lineplot(x=range(2,100,10), y=my_maes[:10])","5c7cced4":"my_maes=[]\nfor max_leaf_nodes in range(50,90):\n    my_mae = get_mae(max_leaf_nodes + 1, train_X, val_X, train_y, val_y)\n    my_maes.append(my_mae)\n    \nsns.lineplot(x=range(50,90), y=my_maes)","3fa0f77f":"final_feature = features_trial[2]\nfinal_model = RandomForestClassifier(max_leaf_nodes=84, random_state=0)\nfinal_model.fit(X[final_feature], y)\nfinal_val_predictions = final_model.predict(X[final_feature])\nfinal_val_mae = mean_absolute_error(final_val_predictions, y)\n        \nprint(\"Validation MAE for Final Model: \",  final_val_mae)","6cd87c09":"final_full_test[\"cntPartners\"] = final_full_test.SibSp + final_full_test.Parch\nfinal_full_test[\"Age_group\"] = final_full_test.Age.round(-1)\n\npredictions = final_model.predict(final_full_test[final_feature]).astype(int)\noutput = pd.DataFrame({'PassengerId': final_full_test.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c2e1726b":"print(full_train.groupby(['Pclass'])['Survived','Age'].agg(['mean', 'median']).round(1))","e8d140a4":"print(full_train.groupby(['Sex', 'Pclass'])['Survived','Age'].agg(['mean', 'median']).round(1))","48b7b555":"sns.lineplot(x=full_train['Age'].round(), y=full_train['Survived'])","c6867c53":"model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)","04e315fa":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","7688cd2d":"sns.barplot(x=full_train['SibSp'].round() + full_train['Parch'].round(), y=full_train['Survived'])","d9c9cfa9":"sns.barplot(x=full_train['SibSp'].round() + full_train['Parch'].round(), y=full_train['Survived'])","57c70347":"sns.distplot(full_train['Age'])","441d64ee":"full_train['Age'].round(-1).value_counts()","23bacfae":"#plot_age = sns.kdeplot(full_train['Age'].loc[full_train['Survived']==1], label = 'Suvived').set_xlabel('Age')\n#plot_age = sns.kdeplot(full_train['Age'].loc[full_train['Survived']==0], label = 'Not Suvived')\nsns.barplot(x=full_train['Age'].round(-1), y=full_train['Survived'])","57d6eb97":"sns.barplot(x=full_train['Age'].round(-1), y=full_train['Survived'])","fb729add":"# Part 2 Data Clean","575e864a":"Decision tree model","50d1ef73":"Based on the correlation table, we can tell higher correlation between Age and Pclass.<br><br>\nReview data again, we can assume the salutation in name can be related to age as well.","9ff87a65":"Random forest model","9bf7a63d":"**Variable Notes *(From Kaggle)***<br>\n<br>\npclass: A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n<br>\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n<br>\nsibsp: The dataset defines family relations in this way...<br>\nSibling = brother, sister, stepbrother, stepsister<br>\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n\nparch: The dataset defines family relations in this way...<br>\nParent = mother, father<br>\nChild = daughter, son, stepdaughter, stepson<br>\nSome children travelled only with a nanny, therefore parch=0 for them.<br><br>\n\n*Plan to encode Sex as 0-femal, 1-male.*\n","4a13793b":"There's missing value for cases with Pclass=3 and Salutation=\"Ms\".\n\"Ms\" is a generic salutation for female. So, let's encode it with the mean for female pclass=3 i.e. 22.2","79ce7484":"**Part 2.1 Categorical Variables Handling**","22236294":"Overfitting Check","0456f9a7":"# Part 3. Data Exploration","687a1e4e":"## Part 1.3 Get Cleaned Data","d718b4e7":"Let's focus on the high correlation above 0.5<br>\nvs Survived: Sex<br>\n<br>\nthe rest:<br>\nPclass vs. Fare<br>","f0ea5269":"# Part 1. Preparations","34fda894":"Logistic Regression","e7b11754":"## Part 3. Build Model","481724f1":"Add some more features:","c043848c":"After trying several methods, we can find out random forest classifier with Feature2 is the best.","78c07591":"There's one missing for Fare as well. And, we already noted from correlation table that fare is highly collerated with Pclass.<br>\nSo, we can recode based on Pclass.","bf506928":"Part 3.1 Explore potential features","473e8d30":"Below is some backup codes.","7d37bee7":"**Part 2.2 Missing Value Handling**\n\nWe can see there're missing and invalid values in Age column. ","6a8c0619":"Correlate vs. Age"}}