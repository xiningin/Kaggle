{"cell_type":{"cee4b4b0":"code","96b87053":"code","0bed9cc2":"code","a9801202":"code","032a46dc":"code","553dad2e":"code","77d61151":"code","3feabc3a":"code","02699fce":"code","5534594f":"code","ee70f5be":"code","dd65faa2":"markdown","11b2e6ab":"markdown","9da8be5a":"markdown","7652e8cf":"markdown","7785737b":"markdown","1fa38d04":"markdown","86b69e4c":"markdown","b98b3b58":"markdown","2758ea01":"markdown","f48745d5":"markdown"},"source":{"cee4b4b0":"import re\nimport unicodedata\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom lime import lime_text\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nfrom sklearn import preprocessing\nfrom nltk.tokenize import word_tokenize \nfrom sklearn.pipeline import make_pipeline\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n\nfrom sklearn.svm import SVC","96b87053":"\nC=10000\nnfold=10\n\nnumber_of_feature = 7000\n\ndataset_path = \"..\/input\/indian-parliament\/rajyasabha_questions_and_answers.xlsx\"\nsheet_name=\"rajyasabha_questions_and_answer\"\n\ntext=\"question_description\"\nreview=\"ministry\"\n\nlanguage=\"english\"","0bed9cc2":"df = pd.read_excel(dataset_path, sheet_name=sheet_name)\nprint(f'Found {len(df)} texts.')\n\nprint(f'{df[review].isnull().sum()} document(s) with no classification removed')\ndf=df[pd.notnull(df[review])]\n\nprint(f'{df[text].isnull().sum()} document(s) with no text removed')\ndf=df[pd.notnull(df[text])]\n\nle = preprocessing.LabelEncoder()\nle.fit(df[review])\ndf[review]=le.transform(df[review])\n\nclasses = [int(c) for c in df[review]]\ndocuments = [d for d in df[text]]\n\n###### Print dataset ###################\ndf = df[[text,review ]]\ndf.columns = ['sentiment', 'review']\ndf.head(3)","a9801202":"y = np.bincount(classes)\nx = np.arange(len(y))\nfig, ax = plt.subplots(figsize=(10,8))\nplt.bar(x, y,width=0.7)\nax.set_xticks(x)\nax.set_aspect('auto')\nplt.show()","032a46dc":"def preprocessor(text):\n    text = re.sub('<[^>]*>', ' ', str(text))\n    text=re.sub('\\d+',' ',str(text))\n    text=re.sub('[\uffab\uffbb]','',str(text))\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n                           str(text))\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' +\n            ' '.join(emoticons).replace('-', ''))\n    return text\n\ndef strip_accents(text):\n    \n    try:\n        text = unicode(text, 'utf-8')\n    except (TypeError, NameError): # unicode is a default on python 3 \n        pass\n    text = unicodedata.normalize('NFD', text)\n    text = text.encode('ascii', 'ignore')\n    text = text.decode(\"utf-8\")\n    return str(text)\n\nstop=set(stopwords.words(language))\n\ndef tokenizer_porter(text):\n    word_tokens = word_tokenize(text)\n    stemmer = SnowballStemmer(language, ignore_stopwords=True)\n    return [stemmer.stem(word) for word in word_tokens]","553dad2e":"class IG():\n    def __init__(self,k_features):\n        self.k_features = k_features\n        \n\n    \n    def fit(self, X, y):\n        def _calIg():\n            entropy_x_set = 0\n            entropy_x_not_set = 0\n            for c in classCnt:\n                probs = classCnt[c] \/ float(featureTot)\n                entropy_x_set = entropy_x_set - probs * np.log(probs)\n                probs = (classTotCnt[c] - classCnt[c]) \/ float(tot - featureTot)\n                entropy_x_not_set = entropy_x_not_set - probs * np.log(probs)\n            for c in classTotCnt:\n                if c not in classCnt:\n                    probs = classTotCnt[c] \/ float(tot - featureTot)\n                    entropy_x_not_set = entropy_x_not_set - probs * np.log(probs)\n            return entropy_before - ((featureTot \/ float(tot)) * entropy_x_set\n                             +  ((tot - featureTot) \/ float(tot)) * entropy_x_not_set)\n        \n        tot = X.shape[0]\n        classTotCnt = {}\n        entropy_before = 0\n        for i in y:\n            if i not in classTotCnt:\n                classTotCnt[i] = 1\n            else:\n                classTotCnt[i] = classTotCnt[i] + 1\n        for c in classTotCnt:\n            probs = classTotCnt[c] \/ float(tot)\n            entropy_before = entropy_before - probs * np.log(probs)\n\n        nz = X.T.nonzero()\n        pre = 0\n        classCnt = {}\n        featureTot = 0\n        information_gain = []\n        for i in range(0, len(nz[0])):\n            if (i != 0 and nz[0][i] != pre):\n                for notappear in range(pre+1, nz[0][i]):\n                    information_gain.append(0)\n                ig = _calIg()\n                information_gain.append(ig)\n                pre = nz[0][i]\n                classCnt = {}\n                featureTot = 0\n            featureTot = featureTot + 1\n            yclass = y[nz[1][i]]\n            if yclass not in classCnt:\n                classCnt[yclass] = 1\n            else:\n                classCnt[yclass] = classCnt[yclass] + 1\n        ig = _calIg()\n        information_gain.append(ig)\n        self.information_gain_a=np.asarray(information_gain)\n        self.indices_ = np.argsort(self.information_gain_a)[-self.k_features:]\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n    def score_(self):\n        return self.information_gain_a;","77d61151":"tfidf = TfidfVectorizer(strip_accents=strip_accents,\n                        lowercase=False,\n                        preprocessor=preprocessor,\n                        stop_words=stop,\n                        min_df = 4\n                       )\n\n    \nig=IG(k_features=number_of_feature)","3feabc3a":"pipe_lr = make_pipeline(                        \n                        tfidf,\n                        ig,\n                        SVC\n                        (kernel=\"linear\", C=C)\n                        )","02699fce":"import warnings\nimport numpy as np\nwarnings.filterwarnings('ignore')\nnp.seterr(divide='ignore', invalid='ignore')","5534594f":"####### Setting up Cross-Validation #####\nX=np.array(documents)\ny=np.array(classes)\nkfold = StratifiedKFold(n_splits=nfold,shuffle=True,random_state=1).split(X, y)\n\n####### Define Variables for Metrics #####\naccuracys = []\nscores= []\nmetriche = np.zeros((nfold,4,len(np.unique(classes))))\ntarget_names=list(map(str,np.unique(classes)))\n\n####### Cross-Validation Loop ############\n\nfor k, (train, test) in enumerate(kfold):\n    \n    \n    pipe_lr.fit(X[train], y[train])\n    y_pred=pipe_lr.predict(X[test])\n\n    \n    ####### Compute Accuracy ##########\n    accuracy = pipe_lr.score(X[test], y[test])\n    accuracys.append(accuracy)\n    \n    ####### Compute Precision,Recall,F-Score ############\n    score=precision_recall_fscore_support(y_true=y[test], y_pred=y_pred, average=\"weighted\")\n    scores.append(score[0:3])\n    \n    print('--------------- Fold: %2d ---------------------'% (k+1))\n    print()\n    print(\"Accuracy:\",  round(accuracy,2))\n    print(\"Detail:\")\n    print(metrics.classification_report(y[test], y_pred))\n    \n    dizionario=metrics.classification_report(y[test], y_pred, target_names=target_names,output_dict=True)\n    for k_d,(m_id, m_info) in enumerate(dizionario.items()):\n        if k_d<len(np.unique(classes)):\n            for j_d,key in enumerate(m_info):\n                metriche[k,j_d,k_d]=m_info[key]\n        else:\n            break\n    \n    ## Plot confusion matrix\n    conf_mat = confusion_matrix(y[test], y_pred)\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(conf_mat, annot=True, fmt='d', ax=ax, cbar=False,cmap=plt.cm.Blues)\n    ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=target_names, \n           yticklabels=target_names, title=\"Confusion matrix\")\n    plt.yticks(rotation=0)\n    \n    print()\n    \n    plt.show()\n    \n    \n\n    \narr = np.array(scores)","ee70f5be":"print(\"Overall results of the cross-validation procedure\")\nprint()\nprint(\"Level 1\")\nprint()\n\nprint('\\nCV accuracy: %.2f +\/- %.2f max: %.2f' % (np.mean(accuracys)*100, np.std(accuracys)*100,np.max(accuracys)*100))\nprint('\\nCV precision: %.2f +\/- %.2f max: %.2f' % (np.mean(arr[:,0])*100, np.std(arr[:,0])*100,np.max(arr[:,0])*100))\nprint('\\nCV recall: %.2f +\/- %.2f max: %.2f' % (np.mean(arr[:,1])*100, np.std(arr[:,1])*100,np.max(arr[:,1])*100))\nprint('\\nCV f1: %.2f +\/- %.2f max: %.2f' % (np.mean(arr[:,2])*100, np.std(arr[:,2])*100,np.max(arr[:,2])*100))\n\nprint()\nprint(\"Level 2\")\nprint()\nprint(f\"{'Class':^7} | {'precision':^9}{'':^6} | {'recall':^10}{'':^5} | {'f1-measure':^6}{'':^5} | {'support':^9}\")\nfor i in range(len(np.unique(classes))):\n    print(f\"{i :^7} | {np.mean(metriche[:,0,i])*100:^5.2f} +\/-{np.std(metriche[:,0,i])*100:^6.2f} | {np.mean(metriche[:,1,i])*100:^5.2f} +\/-{np.std(metriche[:,1,i])*100:^6.2f} | {np.mean(metriche[:,2,i])*100:^5.2f} +\/-{np.std(metriche[:,2,i])*100:^6.2f} | {np.mean(metriche[:,3,i]):^9.2f}\")","dd65faa2":"## Load Dataset","11b2e6ab":"## Import Libraries","9da8be5a":"## Set Hyparameters","7652e8cf":"## Define Preprocessor","7785737b":"# Text Classification in Cross-Validation using SVM Algorithm","1fa38d04":"## Results Output","86b69e4c":"## Define Infogain Class","b98b3b58":"## Class Distribution Diagram","2758ea01":"## Cross-Validation Loop","f48745d5":"## Define Pipeline "}}