{"cell_type":{"aef69cce":"code","d96f983f":"code","196967ce":"code","c894a896":"code","d94af9d8":"code","14b12ffd":"code","393780ad":"code","728975f5":"code","82e689fd":"code","9d2f68f4":"code","e9c41cb6":"code","fcb1e720":"code","ddf5c0d4":"code","d862e36b":"code","f85a5ef3":"code","44e43abc":"code","f4b08a81":"code","ba38c226":"code","ce4a9e1c":"code","0598850e":"code","2ecd61ed":"code","07921d23":"code","aefb36f7":"code","1fc7ebcd":"code","2483fe5a":"code","c7281f16":"code","467d13be":"code","d0967f82":"code","db80bae8":"code","d6bd5c3a":"code","29c902fe":"code","079652ed":"code","be421c47":"code","ff7c8caa":"code","c09bbd20":"code","dab2342c":"code","52211fcb":"code","50f0909d":"code","59269dcc":"code","5ea22dca":"code","372b6d43":"code","029283fc":"code","7edcf2bf":"code","90f70bac":"code","65f1ea22":"code","2e0a8bb4":"code","b109971d":"code","c22a3905":"code","0e52499f":"code","bb1ee664":"code","88793275":"code","bd73c099":"code","ec4dd1d5":"code","f61d6b5f":"code","2fffd9e5":"code","feaae9b6":"code","1604931c":"code","d9b6cbf5":"code","e0e42abf":"code","7d52de20":"markdown","a5f3d190":"markdown","51507579":"markdown","d52878d8":"markdown","cf592cff":"markdown","2383deef":"markdown","234c561e":"markdown","537e02ea":"markdown","dfb8d71c":"markdown","b10ab940":"markdown","b7c958c9":"markdown","f9eefde3":"markdown","575f8e53":"markdown","4ba84aa9":"markdown"},"source":{"aef69cce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d96f983f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\nfrom sklearn.manifold import TSNE\nimport matplotlib.patches as mpatches\nimport time\nimport collections\nfrom collections import Counter\n\nfrom sklearn.model_selection import (GridSearchCV,train_test_split, cross_val_predict, cross_val_score, \n                                     KFold, StratifiedKFold, ShuffleSplit,learning_curve,\n                                     RandomizedSearchCV)\nfrom sklearn.metrics import (confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,\n                             make_scorer,classification_report,roc_auc_score,roc_curve,\n                             average_precision_score,precision_recall_curve)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.metrics import classification_report_imbalanced\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import RandomOverSampler\n\n#from imblearn.under_sampling import ClusterCentroids\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.combine import SMOTEENN\n\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_SEED = 101\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nimport collections\nfrom mpl_toolkits import mplot3d","196967ce":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","c894a896":"df.describe()","d94af9d8":"df.columns","14b12ffd":"# The classes are heavily imbalanced\nprint('Normal', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Anomalies', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","393780ad":"sns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: Normal || 1: Anomaly)', fontsize=14)","728975f5":"df.isnull().sum()","82e689fd":"def check_impute_variance(ser,var):\n    mean = ser.mean()\n    median = ser.median()\n    \n    mean_ser = ser.fillna(mean)\n    median_ser = ser.fillna(median)\n    \n    var_original = ser.std()**2\n    var_mean = mean_ser.std()**2\n    var_median = median_ser.std()**2\n    \n    fig = plt.figure(figsize = (10,5))\n    ax = fig.add_subplot(111)\n    ax = sns.kdeplot(ser.dropna(), color=\"Red\", shade = True, label=\"Original Variance : %.2f\"%(var_original))\n    ax = sns.kdeplot(mean_ser, color=\"Blue\", shade= True, label=\"Mean Variance : %.2f\"%(var_mean))\n    ax = sns.kdeplot(median_ser, color=\"Green\", shade = True, label=\"Median Variance : %.2f\"%(var_median))\n    ax.set_xlabel(var)\n    ax.set_ylabel(\"Frequency\")\n    ax.legend(loc=\"best\")\n    ax.set_title('Frequency Distribution of {}'.format(var), fontsize = 15)","9d2f68f4":"for col in df.columns[:-1]:\n    check_impute_variance(df[col], col)","e9c41cb6":"g = sns.distplot(df[\"Time\"].dropna(), color=\"m\", label=\"Skewness : %.2f\"%(df[\"Time\"].skew()))\ng = g.legend(loc=\"best\")","fcb1e720":"def impute_na_num(ser,var):\n    mean = ser.mean()\n    median = ser.median()\n    \n    mean_ser = ser.fillna(mean)\n    median_ser = ser.fillna(median)\n    \n    var_original = ser.std()**2\n    var_mean = mean_ser.std()**2\n    var_median = median_ser.std()**2\n    \n    if((var_mean < var_original) | (var_median < var_original)):\n        if(var_mean < var_median):\n            return mean_ser\n        else:\n            return median_ser\n    else:\n        return median_ser","ddf5c0d4":"na_cols = df.isnull().sum()[df.isnull().sum().values > 0].index","d862e36b":"for col in na_cols:\n    df[col] = impute_na_num(df[col],col)","f85a5ef3":"df.isnull().sum()","44e43abc":"for col in df.columns[:-1]:\n    fig = plt.figure(figsize = (12,5))\n    ax = fig.add_subplot(111)\n    ax = sns.kdeplot(df[col][(df[\"Class\"] == 0)], color=\"Red\", shade = True)\n    ax = sns.kdeplot(df[col][(df[\"Class\"] == 1)], color=\"Blue\", shade= True)\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Frequency\")\n    ax.legend(df.Class.unique(),loc=\"best\")\n    ax.set_title('Frequency Distribution of {}'.format(col), fontsize = 15)","f4b08a81":"X = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","ba38c226":"rus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(original_Xtrain,original_ytrain)\n\nnew_df = pd.concat([pd.DataFrame(X_rus, columns=df.columns[:-1]),\n                    pd.DataFrame(y_rus, columns=[\"Class\"])],\n                   axis=1)\n\n# Shuffle dataframe rows\nnew_df = new_df.sample(frac=1, random_state=RANDOM_SEED)\n\nnew_df.head()","ce4a9e1c":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\n\n\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","0598850e":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (won't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('Under Sample Correlation Matrix \\n (will use for reference)', fontsize=14)\nplt.show()","2ecd61ed":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","07921d23":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","aefb36f7":"us_outlier_cols = ['V19','V17','V16','V14','V12','V11','V10','V4','V2']","1fc7ebcd":"from scipy.stats import norm\n\nfor col in us_outlier_cols:\n    fig = plt.figure(figsize = (10,5))\n    ax = fig.add_subplot(111)\n    anoms = new_df[col].loc[new_df['Class'] == 1].values\n    sns.distplot(anoms,ax=ax, fit=norm, color='#FB8861')\n    ax.set_title('{} Distribution \\n (for Anamalies)'.format(col), fontsize=14)\nplt.show()","2483fe5a":"def handle_outliers(df,var,target,target_val,tol):\n    anoms = df[var].loc[df[target] == target_val].values\n    q25, q75 = np.percentile(anoms, 25), np.percentile(anoms, 75)\n    print('Outliers handling for: {}'.format(var))\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    iqr = q75 - q25\n    print('IQR {}'.format(iqr))\n    \n    cut_off = iqr * tol\n    lower, upper = q25 - cut_off, q75 + cut_off\n    print('Cut Off: {}'.format(cut_off))\n    print('{} Lower: {}'.format(var,lower))\n    print('{} Upper: {}'.format(var,upper))\n    \n    outliers = [x for x in anoms if x < lower or x > upper]\n\n    print('Number of Outliers in feature {} for Anomalies: {}'.format(var,len(outliers)))\n\n    print('{} outliers:{}'.format(var,outliers))\n\n\n    df = df.drop(df[(df[var] > upper) | (df[var] < lower)].index)\n\n    print('----' * 25)\n    print('\\n')\n    print('\\n')","c7281f16":"for col in us_outlier_cols:\n    handle_outliers(new_df,col,'Class',1,1.5)","467d13be":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","d0967f82":"# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","db80bae8":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","d6bd5c3a":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","29c902fe":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","079652ed":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"AdaBoost Classifier\": AdaBoostClassifier()\n}","be421c47":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","ff7c8caa":"# Logistic Regression Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, n_jobs=-1)\n\nt0 = time.time()\ngrid_log_reg.fit(X_train, y_train)\n\nt1 = time.time()\nprint(\"Grid Search took {:.2} s for Logistic Regression\".format(t1 - t0))\n\nlog_reg = grid_log_reg.best_estimator_\n\ngrid_log_reg.best_params_","c09bbd20":"# kNeighbours Classifier Parameters\nknn_params = {\"n_neighbors\": list(range(2,15,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knn = GridSearchCV(KNeighborsClassifier(), knn_params, n_jobs=-1)\n\nt0 = time.time()\ngrid_knn.fit(X_train, y_train)\n\nt1 = time.time()\nprint(\"Grid Search took {:.2} s for kNN\".format(t1 - t0))\n\nknn = grid_knn.best_estimator_\n\ngrid_knn.best_params_","dab2342c":"# DecisionTree Classifier Parameters\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \n               \"max_depth\": list(range(2,6,1)),\n               \"min_samples_leaf\": list(range(5,7,1)),\n               'max_features': ['auto', 'sqrt', 'log2']}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params, n_jobs=-1)\n\nt0 = time.time()\ngrid_tree.fit(X_train, y_train)\n\nt1 = time.time()\nprint(\"Grid Search took {:.2} s for Decision Tree\".format(t1 - t0))\n\ntree_clf = grid_tree.best_estimator_\n\ngrid_tree.best_params_","52211fcb":"# Random Forest Classifier Parameters\nrf_params = {'n_estimators' : [50,100,150,200],\n             'criterion' : [\"gini\",\"entropy\"],\n             'max_features': ['auto', 'sqrt', 'log2'],\n             'class_weight' : [\"balanced\", \"balanced_subsample\"]}\ngrid_rf = GridSearchCV(RandomForestClassifier(), rf_params, n_jobs=-1)\n\nt0 = time.time()\ngrid_rf.fit(X_train, y_train)\n\nt1 = time.time()\nprint(\"Grid Search took {:.2} s for Random Forest\".format(t1 - t0))\n\nrf_clf = grid_rf.best_estimator_\n\ngrid_rf.best_params_","50f0909d":"# AdaBoost Classifier Parameters\nadb_params = {'n_estimators' : [25,50,75,100],\n              'learning_rate' : [0.001,0.01,0.05,0.1,1,10],\n              'algorithm' : ['SAMME', 'SAMME.R']}\ngrid_adb = GridSearchCV(AdaBoostClassifier(), adb_params, n_jobs=-1)\n\nt0 = time.time()\ngrid_adb.fit(X_train, y_train)\n\nt1 = time.time()\nprint(\"Grid Search took {:.2} s for AdaBoost\".format(t1 - t0))\n\nadb_clf = grid_adb.best_estimator_\n\ngrid_adb.best_params_","59269dcc":"estimators = [log_reg,knn,tree_clf,rf_clf,adb_clf]","5ea22dca":"# Check for Overfitting Case\n\nfor est in estimators:\n    est_score = cross_val_score(est, X_train, y_train, cv=10, n_jobs=-1)\n    print('{} Cross Validation Score: '.format(type(est).__name__), round(est_score.mean() * 100, 2).astype(str) + '%')","372b6d43":"# Implementing NearMiss Technique \n# Distribution of NearMiss\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(original_Xtrain, original_ytrain)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))","029283fc":"us_clf_results = {}\nus_clf_results['Time'] = {}\nus_clf_results['Accuracy'] = {}\nus_clf_results['Precision'] = {}\nus_clf_results['Recall'] = {}\nus_clf_results['F1_score'] = {}\nus_clf_results['Auc_Roc'] = {}\n\nsss = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=False)\n\nfor key, classifier in classifiers.items():\n    \n    # We will undersample during cross validating\n    undersample_accuracy = []\n    undersample_precision = []\n    undersample_recall = []\n    undersample_f1 = []\n    undersample_auc = []\n    \n    cv_time = []\n    idx=1\n    \n    # Cross Validating the right way\n    for train, test in sss.split(original_Xtrain, original_ytrain):\n        print(\"Cross Validation Split - {} for {}\".format(idx,key))\n        undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), classifier)\n        \n        start_time = time.time()\n        undersample_model = undersample_pipeline.fit(original_Xtrain[train], original_ytrain[train])\n        elapsed_time = time.time() - start_time\n        \n        cv_time.append(elapsed_time)\n        undersample_prediction = undersample_model.predict(original_Xtest)\n    \n        undersample_accuracy.append(undersample_pipeline.score(original_Xtest, original_ytest))\n        undersample_precision.append(precision_score(original_ytest, undersample_prediction))\n        undersample_recall.append(recall_score(original_ytest, undersample_prediction))\n        undersample_f1.append(f1_score(original_ytest, undersample_prediction))\n        undersample_auc.append(roc_auc_score(original_ytest, undersample_prediction))\n        \n        idx = idx + 1\n        print(\"---\"*40)\n        print(\"\\n\")\n    \n    us_clf_results['Time'][key] = cv_time\n    us_clf_results['Accuracy'][key] = undersample_accuracy\n    us_clf_results['Precision'][key] = undersample_precision\n    us_clf_results['Recall'][key] = undersample_recall\n    us_clf_results['F1_score'][key] = undersample_f1\n    us_clf_results['Auc_Roc'][key] = undersample_auc","7edcf2bf":"for key, us_res in us_clf_results.items():\n    df_results = (pd.DataFrame(us_res).unstack().reset_index())\n    plt.figure()\n    sns.boxplot(y='level_0', x=0, data=df_results)\n    sns.despine(top=True, right=True, left=True)\n    plt.xlabel(key)\n    plt.ylabel('')\n    plt.title('Results for {} from cross-validation with under-sampling across multiple models'.format(key))","90f70bac":"def plot_learning_curve(estimators, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n        \n    for estimator in estimators:\n        fig = plt.figure(figsize = (15,5))\n        ax = fig.add_subplot(111)\n        if ylim is not None:\n            plt.ylim(*ylim)\n        \n        train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, \n                                                                cv=cv, n_jobs=n_jobs, \n                                                                train_sizes=train_sizes)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        ax1.fill_between(train_sizes, \n                         train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, \n                         alpha=0.1,\n                         color=\"#ff9124\")\n        ax1.fill_between(train_sizes, \n                         test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, \n                         alpha=0.1, \n                         color=\"#2492ff\")\n        ax.plot(train_sizes, \n                train_scores_mean, \n                'o-', \n                color=\"#ff9124\",\n                label=\"Training score\")\n        ax.plot(train_sizes, \n                test_scores_mean, \n                'o-', \n                color=\"#2492ff\",\n                label=\"Cross-validation score\")\n        ax.set_title(\"{} Learning Curve\".format(type(estimator).__name__), fontsize=14)\n        ax.set_xlabel('Training size')\n        ax.set_ylabel('Score')\n        ax.grid(True)\n        ax.legend(loc=\"best\")\n    plt.show()","65f1ea22":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=RANDOM_SEED)\nplot_learning_curve(estimators, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=-1)","2e0a8bb4":"# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, n_jobs=-1,\n                             method=\"decision_function\")\n\nknn_pred = cross_val_predict(knn, X_train, y_train, cv=5, n_jobs=-1)\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5, n_jobs=-1)\n\nrf_pred = cross_val_predict(rf_clf, X_train, y_train, cv=5, n_jobs=-1)\n\nadb_pred = cross_val_predict(rf_clf, X_train, y_train, cv=5, n_jobs=-1)","b109971d":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNN: ', roc_auc_score(y_train, knn_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\nprint('Random Forest Classifier: ', roc_auc_score(y_train, rf_pred))\nprint('AdaBoost Classifier: ', roc_auc_score(y_train, adb_pred))","c22a3905":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknn_fpr, knn_tpr, knn_threshold = roc_curve(y_train, knn_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\nrf_fpr, rf_tpr, rf_threshold = roc_curve(y_train, rf_pred)\nadb_fpr, adb_tpr, adb_threshold = roc_curve(y_train, adb_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, \n                             knn_fpr, knn_tpr,\n                             tree_fpr, tree_tpr,\n                             rf_fpr, rf_tpr,\n                             adb_fpr, adb_tpr):\n    plt.figure(figsize=(15,5))\n    plt.title('ROC Curve \\n Classifiers (Under-Sampling)', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knn_fpr, knn_tpr, label='KNN Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knn_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, rf_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend(loc='best')\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, \n                             knn_fpr, knn_tpr, \n                             tree_fpr, tree_tpr,\n                             rf_fpr, rf_tpr,\n                             adb_fpr, adb_tpr)\nplt.show()","0e52499f":"os_clf_results = {}\nos_clf_results['Time'] = {}\nos_clf_results['Accuracy'] = {}\nos_clf_results['Precision'] = {}\nos_clf_results['Recall'] = {}\nos_clf_results['F1_score'] = {}\nos_clf_results['Auc_Roc'] = {}\n\nsss = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=False)\n\nfor key, classifier in classifiers.items():\n    \n    # We will undersample during cross validating\n    oversample_accuracy = []\n    oversample_precision = []\n    oversample_recall = []\n    oversample_f1 = []\n    oversample_auc = []\n    \n    cv_time = []\n    idx=1\n    \n    # Cross Validating the right way\n    for train, test in sss.split(original_Xtrain, original_ytrain):\n        print(\"Cross Validation Split - {} for {}\".format(idx,key))\n        pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority',random_state=RANDOM_SEED), \n                                            classifier)\n        \n        start_time = time.time()\n        model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n        elapsed_time = time.time() - start_time\n        \n        cv_time.append(elapsed_time)\n        oversample_prediction = model.predict(original_Xtest)\n    \n        oversample_accuracy.append(pipeline.score(original_Xtest, original_ytest))\n        oversample_precision.append(precision_score(original_ytest, oversample_prediction))\n        oversample_recall.append(recall_score(original_ytest, oversample_prediction))\n        oversample_f1.append(f1_score(original_ytest, oversample_prediction))\n        oversample_auc.append(roc_auc_score(original_ytest, oversample_prediction))\n        \n        idx = idx + 1\n        print(\"---\"*40)\n        print(\"\\n\")\n    \n    os_clf_results['Time'][key] = cv_time\n    os_clf_results['Accuracy'][key] = oversample_accuracy\n    os_clf_results['Precision'][key] = oversample_precision\n    os_clf_results['Recall'][key] = oversample_recall\n    os_clf_results['F1_score'][key] = oversample_f1\n    os_clf_results['Auc_Roc'][key] = oversample_auc","bb1ee664":"for key, os_res in os_clf_results.items():\n    df_results = (pd.DataFrame(os_res).unstack().reset_index())\n    plt.figure()\n    sns.boxplot(y='level_0', x=0, data=df_results)\n    sns.despine(top=True, right=True, left=True)\n    plt.xlabel(key)\n    plt.ylabel('')\n    plt.title('Results for {} from cross-validation with over-sampling across multiple models'.format(key))","88793275":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","bd73c099":"from imblearn.keras import BalancedBatchGenerator\ntraining_generator = BalancedBatchGenerator(X, y, sampler=NearMiss(), batch_size=10, random_state=(0))","ec4dd1d5":"def make_model(n_features):\n    model = Sequential()\n    model.add(Dense(200, input_shape=(n_features,),\n              kernel_initializer='glorot_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(100, kernel_initializer='glorot_normal', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(50, kernel_initializer='glorot_normal', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.15))\n    model.add(Dense(25, kernel_initializer='glorot_normal', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model","f61d6b5f":"import time\nfrom functools import wraps\n\n\ndef timeit(f):\n    @wraps(f)\n    def wrapper(*args, **kwds):\n        start_time = time.time()\n        result = f(*args, **kwds)\n        elapsed_time = time.time() - start_time\n        print('Elapsed computation time: {:.3f} secs'\n              .format(elapsed_time))\n        return (elapsed_time, result)\n    return wrapper","2fffd9e5":"@timeit\ndef fit_predict_imbalanced_model(X_train, y_train, X_test, y_test):\n    model = make_model(X_train.shape[1])\n    model.fit(X_train, y_train, epochs=2, verbose=1, batch_size=100)\n    y_pred = model.predict_proba(X_test, batch_size=100)\n    return roc_auc_score(y_test, y_pred)","feaae9b6":"@timeit\ndef fit_predict_balanced_us_model(X_train, y_train, X_test, y_test):\n    model = make_model(X_train.shape[1])\n    training_generator = BalancedBatchGenerator(X_train, y_train,\n                                                batch_size=100,\n                                                random_state=RANDOM_SEED)\n    model.fit_generator(generator=training_generator, epochs=10, verbose=1)\n    y_pred = model.predict_proba(X_test, batch_size=100)\n    return roc_auc_score(y_test, y_pred)","1604931c":"@timeit\ndef fit_predict_balanced_os_model(X_train, y_train, X_test, y_test):\n    model = make_model(X_train.shape[1])\n    training_generator = BalancedBatchGenerator(X_train, y_train,\n                                                sampler=RandomOverSampler(),\n                                                batch_size=1000,\n                                                random_state=RANDOM_SEED)\n    model.fit_generator(generator=training_generator, epochs=5, verbose=1)\n    y_pred = model.predict_proba(X_test, batch_size=1000)\n    return roc_auc_score(y_test, y_pred)","d9b6cbf5":"skf = StratifiedKFold(n_splits=5)\n\ncv_results_imbalanced = []\ncv_time_imbalanced = []\ncv_results_balanced_us = []\ncv_time_balanced_us = []\ncv_results_balanced_os = []\ncv_time_balanced_os = []\nidx=1\n\nfor train_idx, valid_idx in skf.split(original_Xtrain, original_ytrain):\n    print(\"Cross Validation Split: {}\".format(idx))\n    print(\"---\"*40)\n    \n    X_local_train = original_Xtrain[train_idx]\n    y_local_train = original_ytrain[train_idx]\n    X_local_test = original_Xtrain[valid_idx]\n    y_local_test = original_ytrain[valid_idx]\n\n    elapsed_time, roc_auc = fit_predict_imbalanced_model(\n        X_local_train, y_local_train, original_Xtest, original_ytest)\n    cv_time_imbalanced.append(elapsed_time)\n    cv_results_imbalanced.append(roc_auc)\n\n    elapsed_time, roc_auc = fit_predict_balanced_us_model(\n        X_local_train, y_local_train, original_Xtest, original_ytest)\n    cv_time_balanced_us.append(elapsed_time)\n    cv_results_balanced_us.append(roc_auc)\n    \n    elapsed_time, roc_auc = fit_predict_balanced_os_model(\n        X_local_train, y_local_train, original_Xtest, original_ytest)\n    cv_time_balanced_os.append(elapsed_time)\n    cv_results_balanced_os.append(roc_auc)\n    \n    idx = idx + 1\n    print(\"---\"*40)\n    print(\"\\n\")","e0e42abf":"df_results = (pd.DataFrame({'Balanced model Under-Sampled': cv_results_balanced_us,\n                            'Balanced model Over-Sampled': cv_results_balanced_os,\n                            'Imbalanced model': cv_results_imbalanced})\n              .unstack().reset_index())\ndf_time = (pd.DataFrame({'Balanced model Under-Sampled': cv_time_balanced_us,\n                         'Balanced model Over-Sampled': cv_time_balanced_os,\n                         'Imbalanced model': cv_time_imbalanced})\n           .unstack().reset_index())\n\nplt.figure()\nsns.boxplot(y='level_0', x=0, data=df_time)\nsns.despine(top=True, right=True, left=True)\nplt.xlabel('time [s]')\nplt.ylabel('')\nplt.title('Computation time difference using a random under-sampling')\n\nplt.figure()\nsns.boxplot(y='level_0', x=0, data=df_results, whis=10.0)\nsns.despine(top=True, right=True, left=True)\nax = plt.gca()\nax.xaxis.set_major_formatter(\n    plt.FuncFormatter(lambda x, pos: \"%i%%\" % (100 * x)))\nplt.xlabel('ROC-AUC')\nplt.ylabel('')\nplt.title('Difference in terms of ROC-AUC using a random under-sampling')","7d52de20":"<h2> Class Imbalance Exploration <\/h2>\n<a id=\"basic\"><\/a><br><br>\nIn this phase, I will explore the class distribution for <b>Normal<\/b> and <b>Anomalies<\/b>.","a5f3d190":"##  Correlating: \n<a id=\"correlating\"><\/a>\nNow that we have our dataframe correctly balanced, we can go further with our <b>analysis<\/b> and <b>data preprocessing<\/b>.","51507579":"### Classification (Over-Sampling):\n<a id=\"os_modelling\"><\/a>\n<b>Over-Sampling<\/b> : SMOTE stands for Synthetic Minority Over-sampling Technique.  Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\". <br><br>\n\n\n<b> Understanding SMOTE: <\/b>\n<ul>\n<li> <b> Solving the Class Imbalance: <\/b> SMOTE creates synthetic points from the minority class in order to reach an equal balance between the minority and majority class. <\/li>\n<li><b>Location of the synthetic points: <\/b>   SMOTE picks the distance between the closest neighbors of the minority class, in between these distances it creates synthetic points. <\/li>\n<li> <b>Final Effect:  <\/b> More information is retained since we didn't have to delete any rows unlike in random undersampling.<\/li>\n<li><b> Accuracy || Time Tradeoff: <\/b> Although it is likely that SMOTE will be more accurate than random under-sampling, it will take more time to train since no rows are eliminated as previously stated.<\/li>\n\n<\/ul>\n\n### Cross Validation Overfitting Mistake:\n## Overfitting during Cross Validation:  \nIn our undersample analysis I want to show you a common mistake I made that I want to share with all of you. It is simple, if you want to undersample or oversample your data you should not do it before cross validating. Why because you will be directly influencing the validation set before implementing cross-validation causing a \"data leakage\" problem. <b>In the following section you will see amazing precision and recall scores but in reality our data is overfitting!<\/b>\n### The Wrong Way:\n<img src=\"https:\/\/www.marcoaltini.com\/uploads\/1\/3\/2\/3\/13234002\/2639934.jpg?401\"><br>\n\nAs mentioned previously, if we get the minority class and create the synthetic points before cross validating we have a certain influence on the \"validation set\" of the cross validation process. Remember how cross validation works, let's assume we are splitting the data into 5 batches, 4\/5 of the dataset will be the training set while 1\/5 will be the validation set. The test set should not be touched! For that reason, we have to do the creation of synthetic datapoints \"during\" cross-validation and not before, just like below: <br>\n\n\n### The Right Way:\n<img src=\"https:\/\/www.marcoaltini.com\/uploads\/1\/3\/2\/3\/13234002\/9101820.jpg?372\"> <br>\nAs you see above, SMOTE occurs \"during\" cross validation and not \"prior\" to the cross validation process. Synthetic data are created only for the training set without affecting the validation set.","d52878d8":"<h2> Classification (UnderSampling):  <\/h2>\n<a id=\"us_modelling\"><\/a>\nI will train a few types of classifiers and decide which classifier will be more effective in detecting <b>anomalies<\/b>.\n\n## Learning Curves:\n<ul>\n<li>The <b>wider the  gap<\/b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)<\/b>.<\/li>\n<li> If the score is <b>low in both training and cross-validation sets<\/b> this is an indication that our model is <b>underfitting (high bias)<\/b><\/li>","cf592cff":"## Conclusion: \n<a id=\"conclusion\"><\/a>\n<b> The Results of my experiments are as follows: <\/b><br>\n<ul>\n    <li> <b>Logistic Regression<\/b> classifier gives me best performance with <b>under-sampled<\/b> data <\/li>\n    <li> <b>Random Forest<\/b> classifier gives me best performance with <b>over-sampled<\/b> data <\/li>\n    <li> <b>Neural Networks<\/b> trained with imbalanced as well as with balanced samples shows me that with <b>Over-sampled batches the performance increases<\/b>.<\/li>\n<\/ul>\n","2383deef":"<h1 align=\"center\"> Anomaly Detection <\/h1>\n\n<h2> Introduction <\/h2>\nIn this kernel we will use various predictive models to see how accurate they  are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n\n\n<h2> Our Goals: <\/h2>\n<ul>\n<li> Clean and explore the data to understand the imbalanced class distribution of the target variable. <\/li>\n<li> Create correct sampling of the data and eradicate the imbalances. <\/li>\n<li> Perform supervised learning and determine which classifiers has highest anomaly detection. <\/li>\n<li> Perform semi-supervised learning with neural networks and compare the anomaly detection capabilities to our best classifier. <\/li>\n<li> Understand the findings in the models and summarize. <\/li>\n<\/ul>\n\n\n<h2> Outline: <\/h2>\n\nI. <b>Preprocessing<\/b><br>\na) [Class Imbalance Exploration](#basic)<br>\nb) [Missing Value Imputaion](#missing)<br>\nc) [Frequency Distribution](#freqplots)<br>\nd) [Splitting](#splitting)<br>\ne) [Random Under-Sampling](#rusampling)<br>\nf) [Correlating](#correlating)<br>\ng) [Outlier Detection](#outlier)<br>\n\nII. <b>Modelling<\/b><br>\na) [Dimensionality Reduction and Clustering (t-SNE)](#clustering)<br>\nb) [Classification (UnderSampling)](#us_modelling)<br>\nc) [Classification (OverSampling)](#os_modelling)<br>\nd) [Neural Networks with UnderSampling and OverSampling](#neural_networks)<br><br>\n\nIII. <b>Conclusion<\/b><br>\n[Conclusion](#conclusion)","234c561e":"## Outlier Detection:\n<a id=\"outlier\"><\/a>\n\nMy main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.  <br><br>\n\n\n### Interquartile Range Method:\n<ul>\n<li> <b>Interquartile Range (IQR): <\/b> We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.  <\/li>\n<li> <b>Boxplots: <\/b> Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme). <\/li>\n<\/ul>\n\n### Outlier Removal Tradeoff:\nWe have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.  <br><br>\n\n<b>The Tradeoff: <\/b>\nThe lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.","537e02ea":"## Neural Networks with UnderSampling and OverSampling: \n<a id=\"neural_networks\"><\/a>\nIn this section I will implement a simple Neural Network with <b> Balanced Batch Generators <\/b>in order to see  which of the two sampling mechanisms produce better training and anomaly detection. <br><br>\n","dfb8d71c":"## Random Under-Sampling:\n<a id=\"rusampling\"><\/a>\n\nHere we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset <\/b> and thus avoiding our models to overfitting.\n\n**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss<\/b>","b10ab940":"<h3> Correlation Matrices <\/h3>\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\n<b>Note: <\/b> We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.","b7c958c9":"<h2>Dimensionality Reduction and Clustering: <\/h2>\n<a id=\"clustering\"><\/a>\n","f9eefde3":"<h2> Frequency Distribution <\/h2>\n<a id=\"freqplots\"><\/a><br>\nI will check frequency distribution of each plot, with respect to target classes in order to understand feature importances\/selection","575f8e53":"### Splitting the Data \n<a id=\"splitting\"><\/a>\nBefore proceeding with the <b> Random UnderSampling technique<\/b> we have to separate the orginal dataframe. <b> Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.<\/b> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.  ","4ba84aa9":"<h2> Missing Value Imputation <\/h2>\n<a id=\"missing\"><\/a>\n\n<ul>\n    <li> I will check for missing values in all of the variables <\/li>\n    <li> Check for lower variance while mean and median imputation <\/li>\n    <li> Impute missing values with lowest variance method <\/li>\n    <li> Confirm imputation <\/li>\n<\/ul>"}}