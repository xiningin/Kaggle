{"cell_type":{"a4a1a018":"code","3baca8b9":"code","0d99a13a":"code","756d75eb":"code","c8e2bb3c":"code","7bce22dc":"code","3ff1033b":"code","f225bec0":"code","e56c917e":"code","389036d1":"code","67082836":"code","1a14c260":"code","8af8240c":"code","143e0115":"code","819163f1":"code","cb74d8bc":"code","9a91593d":"code","a55fa0af":"code","b96bd972":"code","419ff8e9":"code","4f954e16":"code","80367e33":"code","10e0e1db":"code","51f821cb":"code","463ee668":"code","8645086c":"code","8f1006b1":"code","021a4404":"code","60c7657c":"code","ee3e461f":"code","56030f04":"code","77eaa5bd":"code","0d3b03de":"code","77a0f182":"code","6415d864":"code","c016814e":"code","ba94f4cc":"code","a331ab5a":"code","d19eb5cd":"code","4a8613eb":"code","156b2bab":"code","826f1726":"code","bb9df0db":"code","d7a04431":"code","6a3cf2d1":"code","a6abba81":"code","49fd8a73":"code","176a646f":"code","160bb02f":"code","2f834b13":"code","d43c3bef":"code","b3e3da50":"markdown","c46b9c94":"markdown","3d867f92":"markdown","7fca5e73":"markdown","e8b6c8d0":"markdown","15754c39":"markdown","4e25e955":"markdown","e1248028":"markdown"},"source":{"a4a1a018":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3baca8b9":"import time\nimport tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom shutil import copy, rmtree, copytree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2\nprint (\"all imported\")","0d99a13a":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","756d75eb":"!cd \/kaggle\/input\/mafood121\/MAFood121\/","c8e2bb3c":"! ls MAFood121\/annotations","7bce22dc":"! cd MAFood121\/annotations","3ff1033b":"! cd MAFood121\/annotations\nos.chdir(\"\/kaggle\/input\/mafood121\/MAFood121\/annotations\")\nf = open('cuisinestyles.txt', 'r')\nf_content = f.read()\nprint(f_content)\nf.close()","f225bec0":"f = open('cuisinestyles.txt', 'r')\nf_content = f.read()\nfull_cuisines_list = f_content.split(\"\\n\")\nf.close()\n\ng = open('dishes.txt', 'r')\ng_content = g.read()\nfull_dishes_list = g_content.split(\"\\n\")\ng.close()\nprint(full_cuisines_list)\nprint(full_dishes_list)","e56c917e":"cuisines = [\"sty_american\", \"sty_japanese\", \"sty_italian\"]\ncuisines_i = [full_cuisines_list.index(x) for x in cuisines if x in full_cuisines_list]\nprint(cuisines_i)\ndishes = [\"d_lobster_roll_sandwich\", \"d_onion_rings\", \"d_prime_rib\",\"d_takoyaki\", \"d_ramen\", \"d_sushi\", \"d_lasagna\", \"d_bruschetta\", \"d_gnocchi\"]\ndishes_i = [full_dishes_list.index(x) for x in dishes if x in full_dishes_list]\nprint(dishes_i)","389036d1":"f = open('train_lbls_d.txt', 'r')\nf_content = f.read()\ntrain_data_list = f_content.split(\"\\n\")\nf.close()\n# print(data_list)\nfinal_train_data =[]\nfor i in range(len(train_data_list)):\n    num = train_data_list[i]\n    if num != \"\":\n        if int(num) in dishes_i:\n            final_train_data.append(i)\n\nfinal_train_data = np.array(final_train_data)\nprint(\"Total number of train images available for 9 dishes is\", final_train_data.size)\n\nf = open('train.txt', 'r')\nf_content = f.read()\ntrain_image_addr = f_content.split(\"\\n\")\nf.close()\n\ntrain_image_addr = np.array(train_image_addr)\n\nfiltered_train_image_addr = train_image_addr[final_train_data]\nprint(final_train_data[:10])\nprint(\"Available training data\", final_train_data.size)\nprint(filtered_train_image_addr[:10])\n","67082836":"os.chdir('\/kaggle\/input\/mafood121\/MAFood121\/annotations')\nf = open('train_lbls_cs.txt', 'r')\nf_content = f.read()\ntrain_cuisine_labels = f_content.split(\"\\n\")\nf.close()\ntrain_cuisine_labels = np.array(train_cuisine_labels)\nfiltered_train_cuisine_labels = train_cuisine_labels[final_train_data]\nfiltered_train_cuisine_labels = filtered_train_cuisine_labels.astype(np.int32)\n\ncuisine_sep_train_image_addr = []\nfor cuisine in cuisines_i:\n    cuisine_sep_train_image_addr.append(filtered_train_image_addr[filtered_train_cuisine_labels == cuisine]) \n\nprint(filtered_train_cuisine_labels[:10])\nprint(cuisine_sep_train_image_addr)\n\nf = open('train_lbls_d.txt', 'r')\nf_content = f.read()\ntrain_dish_labels = f_content.split(\"\\n\")\nf.close()\ntrain_dish_labels = np.array(train_dish_labels)\nfiltered_train_dish_labels = train_dish_labels[final_train_data]\nfiltered_train_dish_labels = filtered_train_dish_labels.astype(np.int32)\nprint(filtered_train_dish_labels[:10])","1a14c260":"f = open('test_lbls_d.txt', 'r')\nf_content = f.read()\ndata_list = f_content.split(\"\\n\")\nf.close()\n# print(data_list)\nfinal_test_data =[]\nfor i in range(len(data_list)):\n    num = data_list[i]\n    if num != \"\":\n        if int(num) in dishes_i:\n            final_test_data.append(i)\n\nfinal_test_data = np.array(final_test_data)\nprint(\"Total number of test images available for 9 dishes is\", final_test_data.size)\n\nf = open('test.txt', 'r')\nf_content = f.read()\ntest_image_addr = f_content.split(\"\\n\")\nf.close()\n\ntest_image_addr = np.array(test_image_addr)\n\nfiltered_test_image_addr = test_image_addr[final_test_data]\nprint(final_test_data[:10])\nprint(\"Available test data\", final_test_data.size)\nprint(filtered_test_image_addr[:10])","8af8240c":"os.chdir('\/kaggle\/input\/mafood121\/MAFood121\/annotations')\nf = open('test_lbls_cs.txt', 'r')\nf_content = f.read()\ntest_cuisine_labels = f_content.split(\"\\n\")\nf.close()\ntest_cuisine_labels = np.array(test_cuisine_labels)\nfiltered_test_cuisine_labels = test_cuisine_labels[final_test_data]\nfiltered_test_cuisine_labels = filtered_test_cuisine_labels.astype(np.int32)\nprint(filtered_test_cuisine_labels[:10])\n\ncuisine_sep_test_image_addr = []\nfor cuisine in cuisines_i:\n    cuisine_sep_test_image_addr.append(filtered_test_image_addr[filtered_test_cuisine_labels == cuisine])\nprint(cuisine_sep_test_image_addr)\n\nf = open('test_lbls_d.txt', 'r')\nf_content = f.read()\ntest_dish_labels = f_content.split(\"\\n\")\nf.close()\ntest_dish_labels = np.array(test_dish_labels)\nfiltered_test_dish_labels = test_dish_labels[final_test_data]\nfiltered_test_dish_labels = filtered_test_dish_labels.astype(np.int32)\nprint(filtered_test_dish_labels[:10])","143e0115":"os.chdir('\/kaggle\/input\/mafood121\/MAFood121\/annotations')\nf = open('val_lbls_d.txt', 'r')\nf_content = f.read()\ndata_list = f_content.split(\"\\n\")\nf.close()\n# print(data_list)\nfinal_val_data =[]\nfor i in range(len(data_list)):\n    num = data_list[i]\n    if num != \"\":\n        if int(num) in dishes_i:\n            final_val_data.append(i)\n\nfinal_val_data = np.array(final_val_data)\nprint(\"Total number of validation images available for 9 dishes is\", final_val_data.size)\n\nf = open('val.txt', 'r')\nf_content = f.read()\nval_image_addr = f_content.split(\"\\n\")\nf.close()\n\nval_image_addr = np.array(val_image_addr)\n\nfiltered_val_image_addr = val_image_addr[final_val_data]\nprint(final_val_data[:10])\nprint(\"Available validation data\", final_val_data.size)\nprint(filtered_val_image_addr[:10])","819163f1":"os.chdir('\/kaggle\/input\/mafood121\/MAFood121\/annotations')\nf = open('val_lbls_cs.txt', 'r')\nf_content = f.read()\nval_cuisine_labels = f_content.split(\"\\n\")\nf.close()\nval_cuisine_labels = np.array(val_cuisine_labels)\nfiltered_val_cuisine_labels = val_cuisine_labels[final_val_data]\nfiltered_val_cuisine_labels = filtered_val_cuisine_labels.astype(np.int32)\nprint(filtered_val_cuisine_labels[:10])\n\ncuisine_sep_val_image_addr = []\nfor cuisine in cuisines_i:\n    cuisine_sep_val_image_addr.append(filtered_val_image_addr[filtered_val_cuisine_labels == cuisine])\n#print(cuisine_sep_val_image_addr)\n\nf = open('val_lbls_d.txt', 'r')\nf_content = f.read()\nval_dish_labels = f_content.split(\"\\n\")\nf.close()\nval_dish_labels = np.array(val_dish_labels)\nfiltered_val_dish_labels = val_dish_labels[final_val_data]\nfiltered_val_dish_labels = filtered_val_dish_labels.astype(np.int32)\nprint(filtered_val_dish_labels[:10])","cb74d8bc":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest, collective=False):\n  if os.path.exists(dest):\n    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n  os.makedirs(dest)\n  if collective:\n    for food_item in food_list :\n        print(\"Copying images into\",os.path.join(dest,os.path.basename(food_item)))\n        copy(os.path.join(src,food_item), os.path.join(dest,os.path.basename(food_item)))\n  else:\n      for food_item in food_list :\n        print(\"Copying images into\",food_item)\n        os.makedirs(os.path.dirname(os.path.join(dest,food_item)), exist_ok=True)\n        copy(os.path.join(src,food_item), os.path.join(dest,food_item))","9a91593d":"# picking 3 food items and generating separate data folders for the same\n\nsrc = '\/kaggle\/input\/mafood121\/MAFood121\/images\/'\ndest_train = 'train_mini'\ndest_test = 'test_mini'\nos.chdir('\/kaggle\/working')\nprint(\"Creating train data folder with new classes\")\ndataset_mini(filtered_train_image_addr, src, dest_train)","a55fa0af":"os.chdir('\/kaggle\/working')\ndataset_mini(filtered_test_image_addr, src, dest_test)","b96bd972":"os.chdir('\/kaggle\/working')\nprint(cuisines)\nfor cuisine,cuisine_list in zip(cuisines, cuisine_sep_train_image_addr):\n    dataset_mini(cuisine_list,src,dest='hierarchical\/cuisine_train\/'+cuisine,collective=True)","419ff8e9":"os.chdir('\/kaggle\/working')\nprint(cuisines)\nfor cuisine,cuisine_list in zip(cuisines, cuisine_sep_test_image_addr):\n    dataset_mini(cuisine_list,src,dest='hierarchical\/cuisine_test\/'+cuisine,collective=True)","4f954e16":"os.chdir('\/kaggle\/working')\nprint(cuisines)\nfor cuisine,cuisine_list in zip(cuisines, cuisine_sep_train_image_addr):\n    print(\"Cuisine is\", cuisine)\n    dataset_mini(cuisine_list,src,dest='hierarchical\/per_cuisine\/train\/'+cuisine)","80367e33":"os.chdir('\/kaggle\/working')\nprint(cuisines)\nfor cuisine,cuisine_list in zip(cuisines, cuisine_sep_test_image_addr):\n    print(\"Cuisine is\", cuisine)\n    dataset_mini(cuisine_list,src,dest='hierarchical\/per_cuisine\/test\/'+cuisine)","10e0e1db":"os.chdir('\/kaggle\/working\/hierarchical\/per_cuisine\/test')\n!ls\n\n","51f821cb":"# Visualize the data, showing one image per dish\nrows = 3\ncols = 3\nfig, ax = plt.subplots(rows, cols, figsize=(10,10))\nfig.suptitle(\"Showing one image from each dish\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\ndata_dir = \"\/kaggle\/input\/mafood121\/MAFood121\/images\/\"\nfood_id = 0\nn = 0\nfor dish in dishes_i:\n    i = int(n\/rows)\n    j = n%rows\n    index = np.where(filtered_train_dish_labels == dish)[0][0]\n    selected_image = filtered_train_image_addr[index]\n\n# for i in range(rows):\n#   for j in range(cols):\n#     try:\n#       food_selected = foods_sorted[food_id] \n#       food_id += 1\n#     except:\n#       break\n#     if food_selected == '.DS_Store':\n#         continue\n#     food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n#     food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n    img = plt.imread(os.path.join(data_dir,selected_image))\n    img = cv2.resize(img, dsize=(299, 299), interpolation=cv2.INTER_CUBIC)\n    ax[i][j].imshow(img)\n    ax[i][j].set_title(dish, pad = 10)\n    n+=1\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()","463ee668":"start = time.time()\n\nK.clear_session()\nos.chdir('\/kaggle\/working\/')\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'hierarchical\/cuisine_train\/'\nvalidation_data_dir = 'hierarchical\/cuisine_test\/'\nnb_train_samples = 1644 #75750\nnb_validation_samples = 332 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n# x1 = resnet.output\n# x1 = GlobalAveragePooling2D()(x1)\n# x1 = Dense(128,activation='relu')(x1)\n# x1 = Dropout(0.2)(x1)\n\n# predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x1)\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=10,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_3class.hdf5')","8645086c":"class_map_3 = train_generator.class_indices\nclass_map_3","8f1006b1":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()","021a4404":"plot_accuracy(history,'MAFood121-Inception')\nplot_loss(history,'MAFood121-Inception')","60c7657c":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('best_model_3class.hdf5',compile = False)","ee3e461f":"def predict_class(model, images, show = True):\n  for img in images:\n    img = image.load_img(img, target_size=(224, 224))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255.                                      \n\n    pred = model.predict(img)\n    index = np.argmax(pred)\n    cuisines.sort()\n    prob = {}\n    print(pred.shape)\n    for i in range(len(pred[0])):\n        prob[cuisines[i]] = pred[0][i]\n    pred_value = cuisines[index]\n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.show()\n        print(\"Prediction probabilities are\", prob)\n        print(\"Prediction is\",pred_value)","56030f04":"if not os.path.exists('\/kaggle\/working\/internet_images\/'):\n    os.makedirs('\/kaggle\/working\/internet_images\/')\nos.chdir('\/kaggle\/working\/internet_images\/')\n!wget -O primerib.jpg https:\/\/i.pinimg.com\/originals\/1d\/34\/c3\/1d34c35bbbafdbb7f673e761b7edf122.jpg\n!wget -O takoyaki.jpg https:\/\/iamafoodblog.b-cdn.net\/wp-content\/uploads\/2012\/07\/takoyaki-recipe-4792w-500x500.jpg\n!wget -O gnocchi.jpg https:\/\/www.kikkoman.eu\/fileadmin\/_processed_\/a\/e\/csm_WEB_Spicy_tomato_and_sausage_gnocchi_with_fennel_and_spinach_b1e3a765d7.jpg","77eaa5bd":"images = []\nimages.append('primerib.jpg')\nimages.append('gnocchi.jpg')\n# images.append('omelette.jpg')\nimages.append('takoyaki.jpg')\npredict_class(model_best, images, True)","0d3b03de":"cuisines = [\"sty_american\", \"sty_japanese\", \"sty_italian\"]\ncuisines_i = [full_cuisines_list.index(x) for x in cuisines if x in full_cuisines_list]\nprint(cuisines_i)\ndishes = [\"d_lobster_roll_sandwich\", \"d_onion_rings\", \"d_prime_rib\",\"d_takoyaki\", \"d_ramen\", \"d_sushi\", \"d_lasagna\", \"d_bruschetta\", \"d_gnocchi\"]\ndishes_i = [full_dishes_list.index(x) for x in dishes if x in full_dishes_list]\nprint(dishes_i)","77a0f182":"print(len(cuisine_sep_train_image_addr[0]))\nprint(len(cuisine_sep_test_image_addr[0]))\n\nprint(len(cuisine_sep_train_image_addr[1]))\nprint(len(cuisine_sep_test_image_addr[1]))\n\nprint(len(cuisine_sep_train_image_addr[2]))\nprint(len(cuisine_sep_test_image_addr[2]))","6415d864":"# American cuisine classifier\n\nK.clear_session()\nos.chdir('\/kaggle\/working\/')\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'hierarchical\/per_cuisine\/train\/sty_american\/'\nvalidation_data_dir = 'hierarchical\/per_cuisine\/test\/sty_american\/'\nnb_train_samples = 543 #75750\nnb_validation_samples = 107 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n# x1 = resnet.output\n# x1 = GlobalAveragePooling2D()(x1)\n# x1 = Dense(128,activation='relu')(x1)\n# x1 = Dropout(0.2)(x1)\n\n# predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x1)\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_american.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_american.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=10,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_american.hdf5')","c016814e":"american_map = train_generator.class_indices\namerican_map","ba94f4cc":"# Italian cuisine classifier\n\nK.clear_session()\nos.chdir('\/kaggle\/working\/')\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'hierarchical\/per_cuisine\/train\/sty_italian\/'\nvalidation_data_dir = 'hierarchical\/per_cuisine\/test\/sty_italian\/'\nnb_train_samples = 542 #75750\nnb_validation_samples = 134 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n# x1 = resnet.output\n# x1 = GlobalAveragePooling2D()(x1)\n# x1 = Dense(128,activation='relu')(x1)\n# x1 = Dropout(0.2)(x1)\n\n# predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x1)\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_italian.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_italian.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=10,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_italian.hdf5')","a331ab5a":"italian_map = train_generator.class_indices\nitalian_map","d19eb5cd":"os.chdir('\/kaggle\/working\/hierarchical\/per_cuisine\/test\/sty_japanese')\n!ls\n","4a8613eb":"# Japanese cuisine classifier\n\nK.clear_session()\nos.chdir('\/kaggle\/working\/')\nn_classes = 3\nimg_width, img_height = 299, 299\ntrain_data_dir = 'hierarchical\/per_cuisine\/train\/sty_japanese\/'\nvalidation_data_dir = 'hierarchical\/per_cuisine\/test\/sty_japanese\/'\nnb_train_samples = 559 #75750\nnb_validation_samples = 91 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n# x1 = resnet.output\n# x1 = GlobalAveragePooling2D()(x1)\n# x1 = Dense(128,activation='relu')(x1)\n# x1 = Dropout(0.2)(x1)\n\n# predictions = Dense(3,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x1)\n\n\ninception = InceptionV3(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(n_classes,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\ncheckpointer = ModelCheckpoint(filepath='best_model_japanese.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_japanese.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=10,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_japanese.hdf5')","156b2bab":"japanese_map = train_generator.class_indices\njapanese_map","826f1726":"cuisines = [\"sty_american\", \"sty_japanese\", \"sty_italian\"]\ncuisines_i = [full_cuisines_list.index(x) for x in cuisines if x in full_cuisines_list]\nprint(cuisines_i)\ndishes = [\"d_lobster_roll_sandwich\", \"d_onion_rings\", \"d_prime_rib\",\"d_takoyaki\", \"d_ramen\", \"d_sushi\", \"d_lasagna\", \"d_bruschetta\", \"d_gnocchi\"]\ndishes_i = [full_dishes_list.index(x) for x in dishes if x in full_dishes_list]\nprint(dishes_i)","bb9df0db":"print(\"Time taken is\", time.time()- start)","d7a04431":"def predict_class(model, images, show = True):\n  for img in images:\n    os.chdir('\/kaggle\/working\/internet_images\/')\n    img = image.load_img(img, target_size=(224, 224))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255. \n    \n    \n    os.chdir('\/kaggle\/working\/')\n    pred = model.predict(img)\n    index = np.argmax(pred)\n    cuisines.sort()\n    prob = {}\n    for i in range(len(pred[0])):\n        prob[cuisines[i]] = pred[0][i]\n    pred_value = cuisines[index]\n    dishes = [\"d_lobster_roll_sandwich\", \"d_onion_rings\", \"d_prime_rib\",\"d_takoyaki\", \"d_ramen\", \"d_sushi\", \"d_lasagna\", \"d_bruschetta\", \"d_gnocchi\"]\n    american_dishes = dishes[0:3]\n    japanese_dishes = dishes[3:6]\n    italian_dishes = dishes[6:9]\n    american_dishes.sort()\n    japanese_dishes.sort()\n    italian_dishes.sort()\n    \n    if pred_value == \"sty_american\":\n        model_dish = load_model('best_model_american.hdf5',compile = False)\n        am_pred = model_dish.predict(img)\n        index = np.argmax(am_pred)\n        prob = {}\n        for i in range(len(am_pred[0])):\n            prob[american_dishes[i]] = am_pred[0][i]\n        dish_pred_value = american_dishes[index]\n        \n    if pred_value == \"sty_italian\":\n        model_dish = load_model('best_model_italian.hdf5',compile = False)\n        it_pred = model_dish.predict(img)\n        index = np.argmax(it_pred)\n        prob = {}\n        for i in range(len(it_pred[0])):\n            prob[italian_dishes[i]] = it_pred[0][i]\n        dish_pred_value = italian_dishes[index]\n        \n    if pred_value == \"sty_japanese\":\n        model_dish = load_model('best_model_japanese.hdf5',compile = False)\n        jap_pred = model_dish.predict(img)\n        index = np.argmax(jap_pred)\n        prob = {}\n        for i in range(len(jap_pred[0])):\n            prob[japanese_dishes[i]] = jap_pred[0][i]\n        dish_pred_value = japanese_dishes[index]\n        \n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.show()\n        print(\"Cuisine prediction is\",pred_value)\n        print(\"Dish prediction is\", dish_pred_value)","6a3cf2d1":"# Loading the best saved model to make predictions\nos.chdir('\/kaggle\/working\/')\nK.clear_session()\nmodel_cuisine = load_model('best_model_3class.hdf5',compile = False)","a6abba81":"images = []\nimages.append('primerib.jpg')\nimages.append('gnocchi.jpg')\nimages.append('takoyaki.jpg')\npredict_class(model_cuisine, images, True)","49fd8a73":"def hier_evaluate(image_addr_list,cuisine_val,dish_val):\n    cuisine_pred_list=[]\n    dish_pred_list=[]\n    os.chdir(\"\/kaggle\/working\")\n    model_cuisine = load_model('best_model_3class.hdf5',compile = False)\n    model_american = load_model('best_model_american.hdf5',compile = False)\n    model_japanese = load_model('best_model_japanese.hdf5',compile = False)\n    model_italian = load_model('best_model_italian.hdf5',compile = False)\n    \n    # Due to Keras sorting labels alphabetically\n    cuisine_mapping = [0,2,1]\n    am_mapping = [62,75,85]\n    ja_mapping = [86,105,107]\n    it_mapping = [11,39,60]\n    \n    for image_addr in image_addr_list:\n        os.chdir('\/kaggle\/input\/mafood121\/MAFood121\/images\/')\n        img = image.load_img(image_addr, target_size=(224, 224))\n        img = image.img_to_array(img)                    \n        img = np.expand_dims(img, axis=0)         \n        img \/= 255. \n        cuisine_pred = cuisine_mapping[np.argmax(model_cuisine.predict(img)[0])]\n        cuisine_pred_list.append(cuisine_pred)\n        if(cuisine_pred == 0):\n            dish_pred = am_mapping[np.argmax(model_american.predict(img)[0])]\n            dish_pred_list.append(dish_pred)\n        if(cuisine_pred == 1):\n            dish_pred = ja_mapping[np.argmax(model_japanese.predict(img)[0])]\n            dish_pred_list.append(dish_pred)\n        if(cuisine_pred == 2):\n            dish_pred = it_mapping[np.argmax(model_italian.predict(img)[0])]\n            dish_pred_list.append(dish_pred)\n        \n    return np.array(cuisine_pred_list), np.array(dish_pred_list)","176a646f":"pred = hier_evaluate(filtered_val_image_addr,filtered_val_cuisine_labels,filtered_val_dish_labels)","160bb02f":"cuisine_pred_list = pred[0]\ndish_pred_list = pred[1]\ncuisine_val = filtered_val_cuisine_labels\ndish_val = filtered_val_dish_labels\n\ncuisine_acc = np.sum(cuisine_pred_list == cuisine_val)\/cuisine_val.size\ndish_acc = np.sum(dish_pred_list[cuisine_pred_list == cuisine_val] == dish_val[cuisine_pred_list == cuisine_val])\/dish_val[cuisine_pred_list == cuisine_val].size\ntotal_acc = np.sum(dish_pred_list == dish_val)\/dish_val.size                                                                                                                             \nprint(\"Cuisine accuracy is \",cuisine_acc)\nprint(\"Dish accuracy is \",dish_acc)\nprint(\"Total accuracy is \",total_acc)","2f834b13":"unique, counts = np.unique(cuisine_val, return_counts=True)\ntotal_per_cuisine = dict(zip(unique, counts))\nprint(total_per_cuisine)","d43c3bef":"am_cuisine_correct = np.sum(cuisine_pred_list[cuisine_pred_list == cuisine_val] == total_per_cuisine[0])\nam_cuisine_acc = am_cuisine_correct\/am_cuisine_len\nprint(am_cuisine_acc)","b3e3da50":"## Training model for cuisine detection","c46b9c94":"### For testing purposes, we want to train only on limited dataset, so we choose 3 cusines and 3 dishes each from the 3 cuisines.( In total, 9 dishes to be classified).\n\n\nWe choose american, japanese and italian as our test cuisines.\n\n- For American, we consider lobster_roll_sandwich, onion_rings and prime_rib\n- For Japanese we choose takoyaki, ramen and sushi\n- For Italian we choose lasagna, bruschetta and gnocchi","3d867f92":"## Prediction using the hierarchical structure","7fca5e73":"## Creating mini training dataset","e8b6c8d0":"## Creating train_mini and test_mini datasets","15754c39":"## Creating mini test dataset","4e25e955":"## See how many training and test images we have for each cuisine","e1248028":"## Creating mini validation dataset\n"}}