{"cell_type":{"6586a401":"code","9c1e418b":"code","15dfb569":"code","15b809fc":"code","8a054b90":"code","091eb766":"code","de050950":"code","f5beb6e9":"code","3490e7fa":"code","25a7361d":"code","477186e9":"code","e986f926":"code","4b487a00":"markdown","18e2bae0":"markdown","ec2ae7df":"markdown","6e01c3d0":"markdown","51f6c10b":"markdown","4ec42495":"markdown","a0c23d51":"markdown","e74132ec":"markdown","e83c4790":"markdown","e6dec2a7":"markdown","b51839de":"markdown","58af26e9":"markdown","075e8b8b":"markdown","f79e5897":"markdown","c0616494":"markdown"},"source":{"6586a401":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c1e418b":"#Importing the required modules for EDA and ETL\nimport os\nimport glob #glob is a tool to index and list multiple files for convenient reading\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as style\n\nstyle.use('Solarize_Light2')\ngl = glob.glob('..\/input\/twitter-dateset-collected-during-covid19-pandemic\/data_twitter\/*.csv')\nprint('There are {} files total'.format(len(gl)))","15dfb569":"#Reading in and concatenating tweet data into a single dataframe\nli = [pd.read_csv(t) for t in gl]\nli = pd.concat(li)\nli = li.iloc[:,1:4]\nli.columns = ['time','content','location']\n\nli.info()","15b809fc":"from nltk.tokenize import TweetTokenizer \nfrom nltk.corpus import stopwords\n\ntwe = TweetTokenizer()\n#Filtering the data to two large scale indian Cities Chennai and Bangalore\nche = li[li['location'].isin([\"Chennai\",\"Bangalore\"])]\nche.time =  pd.to_datetime(che.time)\n\n#Creating a new list of tokenized tweets for cleaning\nchetoken = [twe.tokenize(t) for t in che.content]\nstop = stopwords.words('english') #Removing Stop Words\nchetoken = [[t for t in g if t not in stop] for g in chetoken]\nchetoken = [[t for t in g if t.isalpha()] for g in chetoken] #Removing non alpha numeric characters\nchetoken = [' '.join(t) for t in chetoken]\n\n\nchetoken[:5] #Previewing the first five cleaned tweets","8a054b90":"#Plotting the number of daily tweets over observation period by city\nvis1 = che[['time','location','content']].groupby(['time','location']).agg('count')\nvis1 = vis1.reset_index()\nplt.figure(figsize=(25,15))\n\nax1 = sns.relplot(data=vis1,x='time',y='content',hue='location',kind='line',style='location')\nax1.set_xticklabels( rotation=40, ha=\"right\")\nax1.fig.suptitle('Number of Daity Tweets by City')\nax1.set_xlabels(\"Date\")\nax1.set_ylabels('Number of COVID-19 tweets')\n\n\n","091eb766":"#Importing the packages necessary to build a wordcloud\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n\n#Consilidating our corpus into a single text string for processing\nsingletext = ''.join(map(str, chetoken))\n\n#Creating the mask(wordcloud shape) in the form of a COVID protien spike\ncovidmask =  np.array(Image.open('..\/input\/covidimg\/covidspike.jpg'))\n\n#Putting the wordcloud together\nwordcloud1 = WordCloud(width = 700,height=700,colormap='GnBu',\n                       mask=covidmask,max_words=400,background_color = 'white')\nwordcloud1.generate(singletext)\nplt.figure(figsize=(35,23))\nplt.imshow(wordcloud1, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()","de050950":"#Building a binomial classifier with the following packages for Logistic Regression\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(strip_accents='ascii',lowercase=True,stop_words='english')\nX_train, X_test, y_train, y_test = train_test_split(chetoken, che.location, test_size=0.25)\ntrvect = vectorizer.fit_transform(X_train)\ntevect = vectorizer.transform(X_test)\n\ntrvect","f5beb6e9":"lr = LogisticRegression(C=10,max_iter=3000)\nlr.fit(trvect,y_train)\ny_pred = lr.predict(tevect)\n\nprint('The Accuracy Score is : {0:2.2%}'.format(accuracy_score(y_test,y_pred)))\nconfusion_matrix(y_test,y_pred)","3490e7fa":"classification_report(y_test,y_pred,output_dict=True)","25a7361d":"#Building a dataframe of top words associated by city of origin\nfeatures = dict(zip(vectorizer.get_feature_names(),np.exp(lr.coef_).reshape(-1,1)))\nfeatures = pd.DataFrame(features).T\nfeatures.columns = ['Odds']\nfeatures = features.sort_values('Odds')\n\n#Streamlining the features to the top 25 region specific words per city\nbow = vectorizer.vocabulary_\ntopwords = pd.concat([features.head(25), features.tail(25)])\ntopwords['frequency'] = [trvect[bow.get(t)].sum() for t in topwords.index]\ntopwords['city'] = np.where(topwords.Odds < 1,'Bangalore','Chennai')\n\n#Examining Last 10 words of DataFrame\ntopwords.tail(10)","477186e9":"#Similarly examining First 10 words of DataFrame\ntopwords.head(10)","e986f926":"topwords['logodds'] = np.log(topwords.Odds)\n\nplt.figure(figsize=(20,10))\nax2 = sns.scatterplot(x='frequency',y = 'logodds',hue='city',data=topwords, palette=\"Set2\")\nax2.set_title('Most Region specific words used by Chennaites and Bangaloreans')\nfor line in range(0,topwords.shape[0]):\n     ax2.text(topwords.frequency[line]+0.2, topwords.logodds[line], topwords.index[line],\n              horizontalalignment='left', size='small', color='black')","4b487a00":"---\n\n# 4. Building a WordCloud with the entirety of Tweets\n<a class=\"anchor\" id=\"section4\"><\/a>\n\n\nTo get an idea of what insights lay in the content of the tweets a WordCloud is a great way to sum up observations of most frequently occuring terms.\n\nWe accomplish this by using the tokenized list of tweets we generated earlier `chetoken` and proceeding to flatten it.\n\nThen we upload a picture of a COVID protein spike and use it as a mask to shape our wordcloud","18e2bae0":"We can readily obeserve the word **lockdown** is featured prominently in the wordcloud along with phrases and words like positive case, govt and COVID.\n\nThis observation further points to what the tweet volume from mid April to mid May could have been about.","ec2ae7df":"\n---\n\n# 6. Building a top words data frame\n<a class=\"anchor\" id=\"section6\"><\/a>\n\nWe'll be extracting the log odds from the `coef_` method of `LogisticRegression` for this one.\nFeature names can be gathered directly from the `CountVectorizer` object we created. \n\nFrom there on, we create a dataframe of the top 25 region specific words per city and move on to visualzation and interpretation. We narrowed the data down to 25 words per city as we're dealing with over 50,000 features in the entire corpus.\n","6e01c3d0":"---\n\n# 7. Scatterplot and Closing Thoughts\n<a class=\"anchor\" id=\"section7\"><\/a>\n\nWe're going to use a the `topwords` dataframe we looked at above to construct an annotated Scatterplot to better visualize the previously tabulated results.\n\nNote that the original dataset had tweets from ALL Indian Cities which were filtered to Chennai and Bangalore. \n\nThe code written through this kernel can be converted to a function to return region specific terms for any given pair of Cities or locales.\n\nThanks for hopping along for the ride!Feel free to comment as always and, Happy Coding!","51f6c10b":"\n---\n","4ec42495":"Here we've got a list of Bangalore specific words, the Odds look different than they do from Chennai because they are the inverse of the above odds we looked at. (i.e if any of these words are used in a tweet, the Odds of it eminating from Chennai are 1:100)\n\nThis happens because Tweets labelled 'Chennai' comprise the positive class of the Classifier","a0c23d51":"# List of Contents:\n\n[1. Input and EDA](#section1)\n\n[2. Tokenization and Cleaning](#section2)\n\n[3. Plotting Number of Tweets per day](#section3)\n\n[4. Building a WordCloud](#section4)\n\n[5. Logistic Regression](#section5)\n\n[6. Topwords DataFrame](#section6)\n\n[7. Scatterplot and Closing Thoughts](#section7)\n","e74132ec":"---\n\n# 3. Plotting Number of Tweets per day by City\n<a class=\"anchor\" id=\"section3\"><\/a>\n","e83c4790":"# 1. Input and EDA\n<a class=\"anchor\" id=\"section1\"><\/a>\nWe are given the following variables in the data:\n\n* Tweet ID\n* Content\n* Location\n\n\nThe scope of the analysis here will be to join the twitter data from all 100 files and perform NLP techniques to discern **which words in the content correspond to a particular region.**\n\nBut first lets get started with some EDA on the overall dataset.","e6dec2a7":"The `glob` function can be used to aggregate various input files that can then be looped over in the system by means of a dictionary or list comprehension.\n\n","b51839de":"Since we're dealing with the odds in a binary classifier, we have a scenario where if any of the above words are used in a tweet, the odds are over 100:1 that the **tweet originated in Chennai as opposed to Bangalore** (i.e the probability of the tweet being from Chennai divided by the probability of it not being from Chennai)\n\nAs for the content, the words refer to names of local areas within the city or other Named Entities, so the information checks out content-wise.","58af26e9":"It appears that the timeseries of tweets posted in both cities are correlated and appear to peak between 15-APR and 15-MAY.\n\nThis is contextually relevant as India was subject to stringent lockdown measuers beginning 23-MAR and the lockdowns\/curfews were extended in April, so it's possible that we're looking at some public consternation here. Let's put together a WordCloud to glance at the national sentiment.","075e8b8b":"We've correctly classified about 72% of 39,000+ tweets by location as eminating from Chennai or Bangalore. Those are fair results, now on to the fun stuff of mining the data for insights.","f79e5897":"---\n\n# 2. Tokenization and Cleaning\n<a class=\"anchor\" id=\"section2\"><\/a>\n\nThe `nltk` package offers a neat `TweetTokenizer` function to break up an induvidual tweet into tokens. \n\nFrom there we will rid the data of special and non alphanumeric characters to prep it for visualization. Since we're dealing with 800,000+ tweets we are going to limit the scope of this analysis to two major cities [Chennai](https:\/\/en.wikipedia.org\/wiki\/Chennai) and [Bangalore](https:\/\/en.wikipedia.org\/wiki\/Bangalore) and compare their features and trends.","c0616494":"\n---\n\n# 5. Using Logistic Regression to uncover Region Specific words \n<a class=\"anchor\" id=\"section5\"><\/a>\n\nThis will follow the same ML pipeline as your vanilla Binary Classification logistic regression with one twist:\n\nWe use a `CountVectorizer` sparse matrix as the feature to train and test the data instead of a pandas dataframe or series.\n"}}