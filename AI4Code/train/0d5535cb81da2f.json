{"cell_type":{"4a523fca":"code","44d5b37f":"code","984fbbfa":"code","1eae3d4a":"code","186ac0cd":"code","cf8fe3b5":"code","b8c47950":"code","a16daad4":"code","b8a905df":"code","bcc8f396":"code","7eb2326e":"code","a8d65404":"code","57895b18":"code","3b5d13b9":"code","57bb274c":"code","f2e53dbb":"code","b70f6108":"code","bca9ae0c":"code","b11795e8":"code","f2d355dc":"code","24d7e3d8":"code","a389c9fe":"code","a3e21002":"code","eacb3dc0":"code","224c6789":"code","a36f14d3":"code","f5144566":"code","b35d22d2":"code","d2929ac7":"code","9a24581d":"code","dae7fdef":"code","bc4d60a7":"code","7bb62482":"code","0ea93951":"code","cb41b771":"code","24071efd":"code","f84a8b02":"code","ccfca9b6":"code","f543f8c9":"code","1a14ca12":"code","2f4ce0f3":"code","da95fb93":"code","fdf525d3":"code","6bab4179":"code","792cebf8":"code","7c82ee8c":"code","c0db8164":"code","fc69589e":"code","1a791c7a":"code","d4c897e5":"code","2c87b809":"code","9c94971d":"code","753d79d4":"markdown","2fb604af":"markdown","e12b9ad3":"markdown","51e4eee5":"markdown","8b2eb5ad":"markdown","a01ff8b5":"markdown","b208006f":"markdown","cc7bcb01":"markdown","c409a71a":"markdown","a70ccaf0":"markdown","c1409294":"markdown","5726a03e":"markdown","557420f4":"markdown","ba3df301":"markdown","ef2943e9":"markdown","0edbd610":"markdown","72310883":"markdown","dc6588bb":"markdown","b80ce943":"markdown","efc66a7b":"markdown","24a6a6f7":"markdown","9c1b6b80":"markdown","8c10ac62":"markdown","b875f75a":"markdown","f1f4b656":"markdown","7c478471":"markdown","1b756204":"markdown","efaa5c8b":"markdown","bc59abbf":"markdown","6922898a":"markdown","aeec9dc2":"markdown","0e43dd7e":"markdown","63c58202":"markdown","064b7217":"markdown","84817ed0":"markdown"},"source":{"4a523fca":"# Standard library imports\nimport pickle\nimport time # Show the execution time on specific sections\n\n# Third-party imports\nimport keras\nimport matplotlib.patches as mptc\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport seaborn as sns\nimport sklearn.ensemble as skl_esm\nimport sklearn.svm as svm\nimport sklearn.metrics as skl_mtc\nimport sklearn.model_selection as skl_mod_sel\nimport sklearn.preprocessing as skl_pre\nimport xgboost as xgb\n\n%matplotlib inline","44d5b37f":"def bins_labels(bins, **kwargs):\n    '''Plot histogram helper function\n    \n    The code was extracted from Stack Overflow, answer by @Pietro Battiston:\n    https:\/\/stackoverflow.com\/questions\/23246125\/how-to-center-labels-in-histogram-plot\n    \n    Parameters\n    ----------\n    bins : list from start to end by given steps\n        description -> The xticks to fit.\n        format -> range(start, end, step)\n        options -> No apply\n    '''\n    bin_w = (max(bins) - min(bins)) \/ (len(bins) - 1)\n    plt.xticks(np.arange(min(bins)+bin_w\/2, max(bins), bin_w), bins, **kwargs)\n    plt.xlim(bins[0], bins[-1])\n\ndef plot_histogram(dataframe, header, bins=range(0, 380, 10)):\n    '''Plot custom histogram\n\n    Parameters\n    ----------\n    dataframe : Pandas Dataframe\n        description -> The dataframe with the attribute to plot\n        format -> Quantitative attribute\n        options -> No apply\n\n    header : String\n        description -> The attribute to plot\n        format -> 'header_name'\n        options -> No apply\n\n    bins : List from start to end by given steps\n        description -> The xticks to fit.\n        format -> range(start, end, step)\n        options -> No apply\n    '''\n    fig = plt.figure(figsize=(16,5))\n    ax = fig.add_subplot(111)\n    plt.hist(dataframe[header], bins=bins, rwidth= 0.9)\n    title = plt.title(f'\"{header}\" attribute histogram')\n    title.set_color('gray')\n    title.set_size(14)\n    label_y = plt.ylabel('Frequency')\n    label_y.set_color('gray')\n    label_y.set_size(12)\n    label_x = plt.xlabel('Values')\n    label_x.set_color('gray')\n    label_x.set_size(12)\n    plt.xticks(list(bins))\n    bins_labels(bins, fontsize=10, color='gray')\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    plt.axis()\n    plt.show();\n\ndef preprocess(dataframe, min_data, scaler_object):\n    '''Make the data processing steps in new datasets\n    \n    Parameters\n    ----------\n    dataframe : Pandas Dataframe\n        description -> The raw data to process\n        format -> Original headers and data type\n                  of \"concrete_data.csv\"\n        options -> No apply\n\n    min_data : Pandas Series\n        description -> Values to use in the \n                       logarithmic transform\n        format -> Pandas Series indicating \n                  the header and min value\n                  Example: \"Cement\": 123.0\n        options -> No apply\n\n    scaler_object : Sklearn standard scaler object\n        description -> StandardScaler fit to \n                       training dataset\n        format -> No apply\n        options -> No apply\n\n    Returns\n    -------\n    processed_data : Numpy matrix\n        description -> The cleaned and processed data\n        format -> No apply\n        options -> No apply\n    '''\n    # Create the new attributes\n    dataframe['Cement Ratio'] = dataframe['Cement']\/dataframe['Water']\n    dataframe['Aggregates Ratio'] = dataframe['Coarse Aggregate']\/dataframe['Fine Aggregate']\n\n    # Do the logarithmic transformation\n    attributes_to_transform = list(dict(min_data).keys())\n    dataframe[attributes_to_transform] = dataframe[attributes_to_transform].apply(\n        lambda x: np.log10(x + 1 - min_data),\n        axis=1\n    )\n\n    # Drop unnecessary attributes\n    dataframe.drop(\n    columns=['Cement', 'Coarse Aggregate', 'Fine Aggregate'],\n    inplace=True\n    )\n\n    # Do the standard scaling\n    processed_data = scaler_object.transform(dataframe)\n\n    return processed_data\n\ndef train_predict(learner, sample_size, X_train, y_train, X_validation, y_validation):\n    results = {}\n    start = time.time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time.time() # Get end time\n    \n    results['train_time'] = end - start\n    \n    start = time.time() # Get start time\n    predictions_validation = learner.predict(X_validation)\n    predictions_train = learner.predict(X_train)\n    end = time.time() # Get end time\n    \n    results['pred_time'] = end - start\n    \n    results['r2_train'] = skl_mtc.r2_score(y_train, predictions_train)\n    results['r2_validation'] = skl_mtc.r2_score(y_validation, predictions_validation)\n    results['mse_train'] = skl_mtc.mean_squared_error(y_train, predictions_train)\n    results['mse_validation'] = skl_mtc.mean_squared_error(y_validation, predictions_validation)\n    \n    #print(f\"{learner.__class__.__name__} trained on {sample_size} samples.\")\n    \n    return results\n\ndef evaluate(results):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \"\"\"\n  \n    # Create figure\n    fig, ax = plt.subplots(2, 3, figsize = (15,10))\n\n    # Constants\n    bar_width = 0.3\n    colors = ['#B70E0B','#159B0B','#0F1EE5']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'r2_train', 'mse_train', 'pred_time', 'r2_validation', 'mse_validation']):\n            for i in np.arange(3):\n                # Creative plot code\n                ax[j\/\/3, j%3].bar(i+k*bar_width, results[learner][i][metric], width=bar_width, color=colors[k])\n                ax[j\/\/3, j%3].text(i+k*bar_width-bar_width*0.25, results[learner][i][metric], f'{results[learner][i][metric]:.2f}', fontsize=8)\n                ax[j\/\/3, j%3].set_xticks([0.45, 1.45, 2.45])\n                ax[j\/\/3, j%3].set_xticklabels([\"25%\", \"50%\", \"100%\"])\n                ax[j\/\/3, j%3].set_xlabel(\"Training Set Size\")\n                ax[j\/\/3, j%3].set_xlim((-0.1, 3.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"R2\")\n    ax[0, 2].set_ylabel(\"MSE\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"R2\")\n    ax[1, 2].set_ylabel(\"MSE\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"R2 on Training Subset\")\n    ax[0, 2].set_title(\"MSE on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"R2 on Validation Set\")\n    ax[1, 2].set_title(\"MSE on Validation Set\")\n\n    # Create patches for the legend\n    patches = []\n    for index, learner in enumerate(results.keys()):\n        patches.append(mptc.Patch(color=colors[index], label=learner))\n\n    plt.legend(\n        handles=patches,\n        loc='lower center',\n        borderaxespad=-6,\n        ncol=3\n    )\n    \n    # Aesthetics\n    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize=16)\n    plt.tight_layout()\n    plt.show()","984fbbfa":"concrete_df = pd.read_csv('..\/input\/regression-with-neural-networking\/concrete_data.csv')\nconcrete_df.head()","1eae3d4a":"concrete_df.info()","186ac0cd":"concrete_df.describe()","cf8fe3b5":"# Only to show the execution time per code block\nstart_time = time.time()\n\naxs = pd.plotting.scatter_matrix(concrete_df, figsize=(15, 15))\n\nfor ax in axs[:,0]: # the left boundary\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    ax.xaxis.label.set_color('gray')\n    ax.yaxis.label.set_color('gray')\n\nfor ax in axs[-1,:]: # the lower boundary\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    ax.xaxis.label.set_color('gray')\n    ax.yaxis.label.set_color('gray')\n\nplt.show();\nprint(f'{time.time() - start_time:.2f} seconds on executing this code block.')","b8c47950":"ax = concrete_df.plot.box(figsize=(16, 5), grid=True);\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')","a16daad4":"plot_histogram(concrete_df, 'Age', bins=range(0, 380, 10))","b8a905df":"plot_histogram(concrete_df, 'Blast Furnace Slag', bins=range(0, 380, 10))","bcc8f396":"plot_histogram(concrete_df, 'Fly Ash', bins=range(0, 220, 10))","7eb2326e":"plot_histogram(concrete_df, 'Superplasticizer', bins=range(0, 45, 5))","a8d65404":"concrete_df, input_validation = skl_mod_sel.train_test_split(\n    concrete_df.copy(),\n    test_size=0.2,\n    random_state=42\n)\n\ninput_validation, input_test = skl_mod_sel.train_test_split(\n    input_validation.copy(),\n    test_size=0.5,\n    random_state=42\n)","57895b18":"new_attributes = ['Cement Ratio', 'Aggregates Ratio']\n\nconcrete_df[new_attributes[0]] = concrete_df['Cement']\/concrete_df['Water']\nconcrete_df[new_attributes[1]] = concrete_df['Coarse Aggregate']\/concrete_df['Fine Aggregate']","3b5d13b9":"print(concrete_df[new_attributes].corrwith(concrete_df['Strength']))","57bb274c":"print(concrete_df[\n    ['Cement', 'Water', 'Coarse Aggregate', 'Fine Aggregate']\n].corrwith(concrete_df['Strength']))","f2e53dbb":"ax = concrete_df[new_attributes].plot.box(figsize=(16, 5), grid=True);\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')","b70f6108":"# Only to show the execution time per code block\nstart_time = time.time()\n\naxs = pd.plotting.scatter_matrix(concrete_df, figsize=(15, 15))\n\nfor ax in axs[:,0]: # the left boundary\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    ax.xaxis.label.set_color('gray')\n    ax.yaxis.label.set_color('gray')\n\nfor ax in axs[-1,:]: # the lower boundary\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    ax.xaxis.label.set_color('gray')\n    ax.yaxis.label.set_color('gray')\n\nplt.show();\nprint(f'{time.time() - start_time:.2f} seconds on executing this code block.')","bca9ae0c":"attributes_to_transform = [\n    'Cement',\n    'Blast Furnace Slag',\n    'Fly Ash',\n    'Superplasticizer',\n    'Age',\n    'Cement Ratio'\n]\nprint(concrete_df[attributes_to_transform].corrwith(concrete_df['Strength']))","b11795e8":"transformations_df = pd.DataFrame(columns=attributes_to_transform)\nmin_data = concrete_df[attributes_to_transform].min()\n\ntransformations_df = concrete_df[attributes_to_transform].apply(\n    lambda x: np.log10(x + 1 - min_data),\n    axis=1\n)","f2d355dc":"print(transformations_df.corrwith(concrete_df['Strength']))","24d7e3d8":"attributes_to_transform = ['Blast Furnace Slag', 'Age']\nmin_data = concrete_df[attributes_to_transform].min()\n\nconcrete_df[attributes_to_transform] = concrete_df[attributes_to_transform].apply(\n    lambda x: np.log10(x + 1 - min_data),\n    axis=1\n)","a389c9fe":"print(concrete_df.corrwith(concrete_df['Strength']))","a3e21002":"plt.subplots(figsize=(8,8))\nax = sns.heatmap(concrete_df.corr(), annot=True, cbar=False);\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')","eacb3dc0":"scaler = skl_pre.StandardScaler()\nada_model = skl_esm.AdaBoostRegressor(random_state=42)\n\nada_model.fit(\n    scaler.fit_transform(concrete_df.drop(columns=['Strength'])),\n    concrete_df['Strength']\n)","224c6789":"features = concrete_df.drop(columns=['Strength']).columns\nimportances = ada_model.feature_importances_\nindex = np.argsort(importances)\n\nfig = plt.figure(figsize=(16,8))\nax = fig.add_subplot(111)\ntitle = plt.title('Feature Importance')\ntitle.set_color('gray')\ntitle.set_size(14)\nplt.barh(range(len(index)), importances[index], color='green', align='center')\nplt.yticks(range(len(index)), [features[i] for i in index])\nlabel_x = plt.xlabel('Relative Importance')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\n\nfor idx, value in enumerate(index):\n    plt.text(\n        importances[value] + 0.0025,\n        idx,\n        f'{importances[value]:.3f}',\n        fontsize=12,\n        color='gray'\n    )\n\nplt.show();","a36f14d3":"concrete_df.drop(\n    columns=['Cement', 'Coarse Aggregate', 'Fine Aggregate'],\n    inplace=True\n)","f5144566":"# Check the new data contribution\nscaler = skl_pre.StandardScaler()\nada_model = skl_esm.AdaBoostRegressor(random_state=42)\nada_model.fit(scaler.fit_transform(concrete_df.drop(columns=['Strength'])), concrete_df['Strength'])\nfeatures = concrete_df.drop(columns=['Strength']).columns\nimportances = ada_model.feature_importances_\nindex = np.argsort(importances)\n\nfig = plt.figure(figsize=(16,8))\nax = fig.add_subplot(111)\ntitle = plt.title('Feature Importance')\ntitle.set_color('gray')\ntitle.set_size(14)\nplt.barh(range(len(index)), importances[index], color='green', align='center')\nplt.yticks(range(len(index)), [features[i] for i in index])\nlabel_x = plt.xlabel('Relative Importance')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\n\nfor idx, value in enumerate(index):\n    plt.text(\n        importances[value] + 0.0025,\n        idx,\n        f'{importances[value]:.3f}',\n        fontsize=12,\n        color='gray'\n    )\n\nplt.show();","b35d22d2":"output_train = concrete_df['Strength'].copy()\noutput_validation = input_validation['Strength'].copy()\noutput_test = input_test['Strength'].copy()","d2929ac7":"# Fit the scaler and scale the input_data\ninput_train = concrete_df.drop(columns=['Strength']).copy()\nscaler_object = skl_pre.StandardScaler()\nscaler_object.fit(input_train)\ninput_train = scaler_object.transform(input_train)","9a24581d":"input_validation = preprocess(\n    input_validation.drop(columns=['Strength']),\n    min_data,\n    scaler_object\n)\n\ninput_test = preprocess(\n    input_test.drop(columns=['Strength']),\n    min_data,\n    scaler_object\n)","dae7fdef":"model = keras.models.Sequential()\nmodel.add(keras.layers.Dense(5000, activation='relu', input_dim=7))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(500, activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(50, activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(1, kernel_initializer='uniform'))","bc4d60a7":"model.compile(\n    loss=keras.losses.MeanSquaredError(),\n    optimizer=keras.optimizers.Nadam(\n        learning_rate=0.0005,\n        beta_1=0.8,\n        beta_2=0.999)\n)","7bb62482":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    verbose=1,\n    patience=20,\n    mode='auto',\n    restore_best_weights=True)","0ea93951":"reduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=10,\n    verbose=1,\n    mode='auto',\n    min_delta=0.0005,\n    cooldown=0,\n    min_lr=1e-6\n)","cb41b771":"start_time = time.time()\n\nhistory = model.fit(\n    input_train,\n    output_train,\n    epochs=500,\n    batch_size=32,\n    verbose=0,\n    use_multiprocessing=True,\n    validation_data=(input_validation, output_validation),\n    callbacks=[early_stopping, reduce_lr]\n)\n\nprint(f\"\\nCode block time execution: {time.time() - start_time} seconds\")","24071efd":"ax = pd.DataFrame(history.history).plot(figsize=(12,6))\nplt.grid(True)\n#plt.gca().set_ylim(0.4, 0.9)\nlabel_y = plt.ylabel('Value')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('Epoch')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\nplt.show();","f84a8b02":"result = np.sqrt(model.evaluate(input_validation, output_validation))\nprint(f'\\nLoss in validation set:\\n{np.round(result, 4)}')","ccfca9b6":"predict_output = model.predict(input_validation)\nresult = skl_mtc.r2_score(output_validation, predict_output)\nprint(f'R2-score in validation set: {np.round(result, 4)}')","f543f8c9":"result = np.sqrt(model.evaluate(input_test, output_test))\nprint(f'\\nLoss in test set:\\n{np.round(result, 4)}')","1a14ca12":"predict_output = model.predict(input_test)\nresult = skl_mtc.r2_score(output_test, predict_output)\nprint(f'R2-score in test set: {np.round(result, 4)}')","2f4ce0f3":"# Save the best model\nmodel.save('best_model.h5')","da95fb93":"# Save preprocessing data\npickle.dump(min_data, open('min_data.sav', 'wb'))\npickle.dump(scaler_object, open('scaler_object.sav', 'wb'))","fdf525d3":"model_test = keras.models.load_model('best_model.h5')\npredict_output = model_test.predict(input_test)\nresult = skl_mtc.r2_score(output_test, predict_output)\nprint(f'R2-score in test set: {np.round(result, 4)}')","6bab4179":"model_1 = svm.SVR()\nmodel_2 = skl_esm.RandomForestRegressor(random_state=42)\nmodel_3 = xgb.XGBRegressor(random_state=42)\n\nsamples_100 = len(input_train)\nsamples_50 = round(len(input_train)*0.5)\nsamples_25 = round(len(input_train)*0.25)\n\nresults = {}\nfor model in [model_1, model_2, model_3]:\n    model_name = model.__class__.__name__\n    results[model_name] = {}\n    for index, samples in enumerate([samples_25, samples_50, samples_100]):\n        results[model_name][index] = \\\n        train_predict(model, samples, input_train, output_train, input_validation, output_validation)","792cebf8":"evaluate(results)","7c82ee8c":"train =  np.concatenate((input_train, input_validation))\nlabels = np.concatenate((output_train, output_validation))","c0db8164":"model_3","fc69589e":"model = xgb.XGBRegressor(n_jobs=4, random_state=42, verbosity=1)\n\nscorer = {\n    'R2': skl_mtc.make_scorer(skl_mtc.r2_score),\n    'MSE': skl_mtc.make_scorer(skl_mtc.mean_squared_error)\n}\n\nparameters = {\n    'n_estimators': [10, 50, 100, 200, 300, 400, 500],\n    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'gamma': [0, 0.001, 0.01, 0.1],\n    'min_child_weight': [1, 0.5],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4],\n    'booster': ['dart', 'gbtree'],\n    'tree_method': ['hist', 'exact', 'approx']\n}\n\ngrid_obj = skl_mod_sel.GridSearchCV(\n    estimator=model,\n    param_grid=parameters,\n    scoring=scorer,\n    refit='R2'\n)\n\nstart_time = time.time()\ngrid_obj.fit(train, labels)\nprint(f'{time.time() - start_time:.2f} seconds on executing this code block.')\n\nbest_model = grid_obj.best_estimator_","1a791c7a":"# The best model is loaded here from file on my local machine.\nbest_model = xgb.XGBRegressor()\nbest_model.load_model('..\/input\/xgboost-model-concrete-problem\/xgboost_model.json')","d4c897e5":"#model_3.fit(input_train, output_train)\npredictions = model_3.predict(input_test)\nbest_predictions = best_model.predict(input_test)","2c87b809":"best_model","9c94971d":"print('Optimized Model with Neural Networks.')\nprint(f'R2-score in test set: {skl_mtc.r2_score(output_test, predict_output):.4f}')\nprint(f'RMSE on test set: {np.sqrt(skl_mtc.mean_squared_error(output_test, predict_output)):.4f}\\n\\n')\nprint(\"Unoptimized XGBosst Model regressor.\")\nprint(f\"R2 score on test data: {skl_mtc.r2_score(output_test, predictions):.4f}\")\nprint(f\"RMSE on tes data: {np.sqrt(skl_mtc.mean_squared_error(output_test, predictions)):.4f}\\n\\n\")\nprint(\"Optimized XGBoost Model regressor.\")\nprint(f\"R2 score on the testing data: {skl_mtc.r2_score(output_test, best_predictions):.4f}\")\nprint(f\"RMSE on the testing data: {np.sqrt(skl_mtc.mean_squared_error(output_test, best_predictions)):.4f}\")","753d79d4":"# 1. The Data","2fb604af":"Compare the linear correlation of the new attributes to the original attribute components (**Cement**, **Water**, **Coarse Aggregate** and **Fine Aggregate**)","e12b9ad3":"## 1.1. Exploration\n\nPrevisualize the first rows in the dataframe.","51e4eee5":"## 1.3. Split the dataset\n\nIt is essential to separate the dataset in *training* (80%), *validation* (10%), and *testing* (10%) sets before continuing to next steps. The purpose is to keep the testing and validation sets unseen even in the preprocessing process. It helps to generalize and get realistic results.","8b2eb5ad":"## 1.2. Visualization\n\nPlot histograms, boxplots, and any additional graph that help to identify possible patterns, outliers and understand the information better.","a01ff8b5":"The new attributes have a positive correlation, but only **Cement Ratio** had an increase in the absolute value.  Before choosing which attributes to drop is necessary to apply other preprocessing techniques.","b208006f":"# Import required libraries\n\nKeep all the import library code on a single cell to keep the notebook's organization, additional dowloads and setup on this section.","cc7bcb01":"## 2.1. Logarithmic transform\n\nA dataset may sometimes contain at least one feature whose values tend to lie near a single number,  also have a non-trivial number of vastly larger or smaller values (skewed continuous attributes).  \n\nAlgorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the concrete strength dataset, some features fit this description:\n\n* **Cement**\n* **Blast Furnace Slag** \n* **Fly Ash**\n* **Superplasticizer**\n* **Age**\n* **Cement Ratio**\n\nIn the skewed attributes apply a logarithmic transformation,  so the high values and tiny values do not negatively affect the performance of a learning algorithm. This method reduces the effect caused by *outliers*.\n\nTo apply the logarithmic transformation correctly, the suggestions given on [ResearchGate portal](https:\/\/www.researchgate.net\/post\/how_to_transform_data_with_negative_values) help to handle the negative values in the skewed attributes, particularly the logarithmic transformation suggested by **Guido Bongi** from the *Italian National research Council*:\n\n$$Transform_{log}(X) = log(X + 1 - min(X))$$\n\nBefore applying the transformation, check the linear correlation to the attribute **Strength**, apply the transformation and review the linear correlation changes to decide wich transformed attributes to keep.","c409a71a":"Use the function **preprocess** to get the format of the *input_train* in the *input_validation* and *input_test*.","a70ccaf0":"The results showed all the attributes have a considerable contribution.","c1409294":"The final results showed that Neural Networks are not necessary the best way to solve this problem, using other methods related to Machine Learning gave better performance and more consistent and replicable results.","5726a03e":"The linear correlations results showed only the attributes **Blast Furnace Slag** and **Age** got and improvement after the logarithmic transformation, so this transformation is kept only in those attributes.","557420f4":"Check the dataset to identify possible missing values and the data type on each column.","ba3df301":"As the testing dataset showed, the model is capable to generalize well to unseen data, so this model could be apply to other data that has never been seing. After this point, save the model and useful data to keep the results.","ef2943e9":"## 2.2. Feature Importance\n\nThis method is useful to identify attributes that has a tiny contribution in the dataset, those attributes could be dropped.","0edbd610":"The created attribute **Cement Ratio** has higher correlation to other attributes than only the **Cement** attribute, and viceversa, the **Cement** attribute has higher correlation to different attributes.\n\nThe attribute **Aggregates Ratio** has tiny correlations, also they are smaller than the **Fly Ash** attribute.","72310883":"# Function definitions\n\nAll the functions are declared here with corresponding docstrings and references (if apply).","dc6588bb":"The results showed the attribute **Aggregates Ratio** has a higher contribution than the **Coarse Aggregates** and **Fine Aggregates** alone. The same case occurs with the **Cement Ratio**, it has a higher contribution, so the base attributes **Cement**, **Coarse Aggregate** and **Fine Aggregate** are dropped.\n\nThe attribute **Water** is kept because after the attribute creations their contribution did not drop drastically, so it is possible to have a correlation to other attributes.","b80ce943":"# 7. Final scores","efc66a7b":"# 3. Split data\n\nBefore keep going to the model training and testing, do the preprocessing process to the testing dataset and apply the same standarization (use the same values on the training set).","24a6a6f7":"The reduce learning method helps to adjust the learning rate during the training process.","9c1b6b80":"> The following code block has to be exeuted on a local machine, the total time required on the grid search was around **10 hours and 30 minutos**, instead executing here, the resulted model is loaded here from local machine.","8c10ac62":"The optimizer **Nadam** got better performance, so it is selected.","b875f75a":"## 1.4. Attribute combinations\n\nIt is useful to try out some attribute combinations. For example, check the ratio of components versus the amount of water and similar combinations. The list below shows the attributes to probe and their description, then perform a linear correlation to check if the new attributes are useful.\n\n* **Cement Ratio**: Cement amount per part of water (**Cement**\/**Water**).\n\n* **Aggregates Ratio**: Coarse aggregate per part of fine aggregate (**Coarse Aggregate**\/**Fine Aggregate**).","f1f4b656":"# 5. Model selection\n\nThe neural networs model approach was taken only as indicated on this current Kaggle dataset, but neural networks does not always fit to any problem, so it is important to make a correct model selection process and then tunning the selected model, the following code blocks are intended to make a model selection and verify if it is possible to get a better performance.\n\nOn this dataset the models to try are:\n\n   * Support Vector Machines\n   * Random Forest\n   * XGBoost","7c478471":"Based on the graphs, **XGBoost** is the appropiate model to choose, it no requires high time training and perform well to outliers. ","1b756204":"## 4.1.  Evaluation\n\nUse the test set to validate the model to unseen data in training. (Use *input_test* and *output_test*)","efaa5c8b":"The early stopping method helps to stop the training when the model is not increasing its performance and it also avoids the overfitting.","bc59abbf":"Now it is possible to build a model, tune it and test it.","6922898a":"There are no negative values or high values like **9999** so all the data is valid.","aeec9dc2":"# 6. Model tunning\n\nThe next step is tunning the selected model, on this part a Grid Search is performed.","0e43dd7e":"The *pandas* method called **info** showed all the attributes have a quantitative data type. There are no missing values, however, be careful to invalid numeric values (like negative numbers or high values, sometimes those numeric representations mean the real value is missing). \n\nThe *pandas* method called **describe** helps to identify the mentioned cases, it does not need any additional parameter because there are no categorical columns.","63c58202":"# 2. Preprocessing data","064b7217":"# 4. Using Neural Networks","84817ed0":"The boxplot helps to identify the outliers; all the values are in a similar range, only the attribute **Age** has several outliers, so one possible preprocessing method could eliminate the outliers on the training set or apply a preprocessing transformation to reduce their outlier effects.\n\nThe attributes called **Blast Furnace Slag**, **Fly Ash**, **Superplasticizer** and **Age** seems to be highly skewed, so check the available data description before doing any preprocessing data decisions (A *Concrete_Readme.txt* was found in [UCI.edu](https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/concrete\/compressive\/), it has a similar dataset so that Readme was used alongside the Kaggle dataset description to full fill the attributes information.\n\nThe mentioned Readme has the next dataset description and copyright notes: \n\n>Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n>\n>\n>NOTE: Reuse of this database is unlimited with retention of copyright notice for Prof. I-Cheng Yeh and the following published paper:\n>\n>I-Cheng Yeh, \"Modeling of strength of high performance concrete using artificial \nneural networks,\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998)\n\n* [0] **Cement**: Quantitative attribute, it represents the kilograms of cement used in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [1] **Blast Furnace Slag**: Quantitative attribute, it represents the blast furnace slag in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [2] **Fly Ash**: Quantitative attribute, it represents the fly ash in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [3] **Water**: Quantitative attribute, it represents the water in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [4] **Superplasticizer**: Quantitative attribute, it represents the superplasticizer in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [5] **Coarse Aggregate**: Quantitative attribute, it represents the coarse aggregate (particles above 4.75mm) in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [6] **Fine Aggregate**: Quantitative attribute, it represents the fine aggregate (sand in kilograms in one cubic meter of mixture (*kg\/m<sup>3<\/sup>*). **No missing values**.\n\n\n* [7] **Age**: Quantitative attribute, it represents the age or time in days until the concrete needs repairing  (1-365). **No missing values**.\n\n\n* [8] **Strength**: Quantitative attribute, it represents the concrete strength (MPa). **No missing values**.\n\nUsing the information above including the graphics, the attributes **Blast Furnace Slag**, **Fly Ash**, **Superplasticizer** and **Age** could be preprocessed to reduce the skewed data, use the following histograms to check that possible step."}}