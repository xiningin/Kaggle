{"cell_type":{"1c72fda5":"code","45239e6a":"code","9f81ba27":"code","a57b1e65":"code","47d914c2":"code","474575e7":"code","33cf4f66":"code","471fc499":"code","8f81faa4":"code","9ade5933":"code","6f68a293":"code","ead56c8f":"code","23ec8da8":"code","5d17cef5":"code","631e8379":"code","7a581eba":"code","03124292":"code","a6eced6d":"code","98db3bf6":"code","bda84692":"code","b63f052a":"code","5943c10e":"code","232b4f61":"code","6fa9297f":"code","ebdcf228":"code","dc73e44a":"code","48192157":"code","31d0688d":"code","432d17d5":"code","95415ae2":"code","f92f0c94":"markdown","df2c6043":"markdown","f0ca61c8":"markdown","e74e58f2":"markdown","3b3c51fa":"markdown","5036d4d4":"markdown","21e41317":"markdown"},"source":{"1c72fda5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45239e6a":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.feature_selection import RFECV \n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nimport optuna\n\nimport graphviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9f81ba27":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","a57b1e65":"# Age fillna with mean age for each class\ntrain_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())\ntest_df['Age'] = test_df['Age'].fillna(train_df['Age'].mean())\n\n# Cabin, fillna with 'N' and take first letter\ntrain_df['Cabin'] = train_df['Cabin'].fillna('N').map(lambda x: x[0].strip())\ntest_df['Cabin'] = test_df['Cabin'].fillna('N').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \ntrain_df['Ticket'] = train_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\ntest_df['Ticket'] = test_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = train_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ntrain_df['Fare'] = train_df['Fare'].fillna(train_df['Pclass'].map(fare_map['Fare']))\ntest_df['Fare'] = test_df['Fare'].fillna(test_df['Pclass'].map(fare_map['Fare']))\n\n# Embarked, fillna with 'N' value\ntrain_df['Embarked'] = train_df['Embarked'].fillna('N')\ntest_df['Embarked'] = test_df['Embarked'].fillna('N')\n\n# Creating family Size Variable\ntrain_df['family_size'] = train_df['SibSp'] + train_df['Parch']\ntest_df['family_size'] = test_df['SibSp'] + test_df['Parch']","47d914c2":"label_col = ['Pclass', 'Sex', 'Cabin', 'Embarked','Ticket']\n\nlc = LabelEncoder()\nfor col in label_col:\n    train_df[col] = lc.fit_transform(train_df[col])\n    test_df[col] = lc.transform(test_df[col])","474575e7":"features = ['Age','Fare','Pclass', 'Sex', 'Cabin', 'Embarked','Ticket','SibSp','Parch']","33cf4f66":"sc = StandardScaler()\ntrain_df[features] = sc.fit_transform(train_df[features])\ntest_df[features] = sc.transform(test_df[features])","471fc499":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LogisticRegression(max_iter=500)\n    model.fit(X_train, y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]    \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","8f81faa4":"oof_lr = y_oof\npred_lr = y_pred","9ade5933":"params = {'max_depth': 10, 'min_samples_split': 818, 'min_samples_leaf': 35}","6f68a293":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = DecisionTreeClassifier(**params)\n    model.fit(X_train , y_train) \n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","ead56c8f":"oof_dt = y_oof\npred_dt = y_pred","23ec8da8":"params = {'n_estimators': 400,\n    'max_depth': 80,\n    'min_samples_split': 41,\n    'min_samples_leaf': 36}","5d17cef5":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = RandomForestClassifier(**params)\n    model.fit(X_train , y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","631e8379":"oof_rf = y_oof\npred_rf = y_pred","7a581eba":"params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 1000,\n    'objective': 'binary',\n    'learning_rate': 0.01,\n    'reg_lambda': 2.343120757686286,\n    'reg_alpha': 9.978349316502767,\n    'colsample_bytree': 0.4,\n    'min_child_samples': 100,\n    'max_depth': 70,\n    'num_leaves': 50,\n    'device_type' : 'gpu'\n}","03124292":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LGBMClassifier(**params)\n    model.fit(X_train , y_train , eval_set = [(X_valid , y_valid)] ,eval_metric='logloss', early_stopping_rounds = 300 , verbose = False)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","a6eced6d":"oof_lgbm = y_oof\npred_lgbm = y_pred","98db3bf6":"model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[9]),\n    layers.Dropout(0.5),\n    layers.Dense(16, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid'),\n])","bda84692":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","b63f052a":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=20,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","5943c10e":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = train_df[features]\ntest = test_df[features]\ntarget = train_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=100, callbacks=[early_stopping], verbose=0,)\n    y_oof[valid_index] = model.predict(X_valid).flatten()\n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict(test).flatten()  \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","232b4f61":"oof_ann = y_oof\npred_ann = y_pred","6fa9297f":"oof_df = pd.DataFrame()\noof_df['oof_lr'] = oof_lr\noof_df['oof_dt'] = oof_dt\noof_df['oof_rf'] = oof_rf\noof_df['oof_lgbm'] = oof_lgbm\noof_df['oof_ann'] = oof_ann","ebdcf228":"corrMatrix_oof = oof_df.corr()\nsns.heatmap(corrMatrix_oof, annot=True)\nplt.show()","dc73e44a":"pred_df = pd.DataFrame()\npred_df['pred_lr'] = pred_lr\npred_df['pred_dt'] = pred_dt\npred_df['pred_rf'] = pred_rf\npred_df['pred_lgbm'] = pred_lgbm\npred_df['pred_ann'] = pred_ann","48192157":"corrMatrix_pr = oof_df.corr()\nsns.heatmap(corrMatrix_pr, annot=True)\nplt.show()","31d0688d":"oof_df['Survived'] = train_df['Survived']","432d17d5":"n_folds = 10\nkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\ny_pred = np.zeros(test_df.shape[0])\ny_oof = np.zeros(train_df.shape[0])\n\ntrain = oof_df.drop('Survived', axis=1)\ntest = pred_df\ntarget = oof_df['Survived']\nacc_mean = 0\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_oof[valid_index] = model.predict_proba(X_valid)[:,1] \n    accuracy = accuracy_score(y_valid, np.where(y_oof[valid_index]>0.5, 1, 0)) \n    print(\"  Accuracy: {}\".format(accuracy))\n    acc_mean += accuracy\n    y_pred += model.predict_proba(test)[:,1]   \n\ny_pred \/= n_folds\nacc_mean \/= n_folds\n\nprint(\"\")\nprint(\"Mean Accuracy: {}\".format(acc_mean))\nprint(\"Done!\")","95415ae2":"submission['Survived'] = np.where(y_pred>0.5, 1, 0)\nsubmission.to_csv('submission.csv',index=False)","f92f0c94":"# Data Preprocessing","df2c6043":"# ANN Model","f0ca61c8":"# Light GBM","e74e58f2":"# Random Forest Model","3b3c51fa":"# Logistic Regression Model","5036d4d4":"This idea is copied from the following notebook which has both great implemnetation and a much better LB score:\nhttps:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling","21e41317":"# Decision Tree Model"}}