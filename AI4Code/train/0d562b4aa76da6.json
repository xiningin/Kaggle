{"cell_type":{"562bdd5c":"code","97f14977":"code","e73d4d75":"code","281b9e2f":"code","bcb94d24":"code","88af02fc":"code","2f116ebf":"code","6cdeb5c1":"code","e72d7d3c":"markdown","5436bfa8":"markdown","4d75ef22":"markdown"},"source":{"562bdd5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","97f14977":"# util functions to reduce pandas dataframe memory\ndef df_mem_reduce(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\nimport pickle\n\n#util function to dump any object as pickled file\n# ANOTHER APPROACH: pickle.dump(file_, open(filename+'.pkl','wb'), pickle.HIGHEST_PROTOCOL)\ndef dump(file_, filename):\n    with open(filename+'.pkl','wb') as f:\n        pickle.dump(file_, f, pickle.HIGHEST_PROTOCOL)\n\n#util function to load any pickled file dumped by above function\n# ANOTHER APPROACH: pickle.load(open(filename,'rb'))\ndef load(filename):\n    with open(filename,'rb') as f:\n        return pickle.load(f)\n\n\n","e73d4d75":"sell = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nprint(\"%6.2f Mb\" % (sell.memory_usage().sum() \/ 1024**2))\nprint(sell.dtypes)","281b9e2f":"sell_reduced = reduce_mem_usage(sell)\nprint(sell_reduced.dtypes)\nprint(\"%6.2f Mb\" % (sell_reduced.memory_usage().sum() \/ 1024**2))","bcb94d24":"sell_reduced.memory_usage()","88af02fc":"import csv\nimport gzip\nimport json\n\n#util function to read csv file as list of dictionary\ndef read_csv_as_list(filename_to_read):\n    \"\"\"\n    filename_to_read: filename with path to read\n    \"\"\"\n    return [x for x in csv.DictReader(open(filename_to_read,'r'))] # file_ = '*.csv'\n    \n\n#util function to write data in list of dictionary format as csv file\ndef write_list_as_csv(inputList, filename_to_write, columns):\n    \"\"\"\n    inputList: input list of dictionaries to dump\n    filename_to_write: filename with path to write\n    columns: comma separated string containing column names to write in header\n    \"\"\"\n    headerDict = OrderedDict([(x, None) for x in columns.split(',')])\n    with open(filename_to_write,'wb') as fout:\n        dw = csv.DictWriter(fout, delimiter=',',fieldnames=headerDict, quoting =csv.QUOTE_ALL)\n        dw.writeheader()\n        for item in inputList:\n            dw.writerow(item)\n\n\ndef write_list_as_zipped_csv(inputList, filename_to_write):\n    \"\"\"\n    inputList: input list of dictionaries to dump\n    filename_to_write: filename with path to write\n    columns: comma separated string containing column names to write in header\n    \"\"\"\n    with gzip.open(filename_to_write,'w') as outFile:\n        for lJ in inputList:\n            row = json.dumps(lJ)\n            print(row)\n            outFile.write(row.encode())\n            outFile.write('\\n')\n            \n","2f116ebf":"sell_list = read_csv_as_list(filename_to_read='..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nprint(\"%6.2f Mb\" % (sell_list.__sizeof__() \/ 1024**2))","6cdeb5c1":"sell_list[0]","e72d7d3c":"# Testing on M5-sell data","5436bfa8":"## This is the first notebook of a list I will publish containing utility functions. If you like them, Please upvote, it will keep me motivated.\n\nDisclaimer: Some of the codes are written by others and I am just compiling them, with due reference wherever possible.","4d75ef22":"## We can see saving data in list keeps the ram usage low, we will need to use some support functions to process this data which you can find in later notebooks of this series."}}