{"cell_type":{"e4b0ccda":"code","8b42681e":"code","63f512b2":"code","a1322aa8":"code","5a0c102a":"code","0d64f599":"code","3167a509":"code","a618415c":"code","829adcf0":"code","11ef2531":"code","6c0fb75c":"code","ca9b76e7":"code","939ec656":"code","9262bc73":"code","601a2736":"code","91ea4c18":"code","b9b3f561":"code","1654d1aa":"code","e16f48c6":"code","b7fb0bff":"code","8261dfb5":"code","295a4126":"code","18761219":"code","5b171164":"code","4bc7dd93":"code","882e7895":"code","a9e79c55":"code","d2f6fbdd":"code","e075135d":"code","9041a5f3":"markdown","ba87312c":"markdown","a3848998":"markdown","0fe9cc17":"markdown","dc9e855f":"markdown","1433ae13":"markdown","482488a2":"markdown","f7430290":"markdown","3569db7f":"markdown","990ac019":"markdown","bb53424b":"markdown","127151b3":"markdown","0f2083b2":"markdown"},"source":{"e4b0ccda":"import numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom string import punctuation\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten, Conv1D, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport re\n!pip install pymystem3\nfrom pymystem3 import Mystem","8b42681e":"df = pd.read_csv('..\/input\/russian-language-toxic-comments\/labeled.csv')","63f512b2":"df.head(10)","a1322aa8":"df.shape","5a0c102a":"def text_cleaner(text):\n    tokenized_text = word_tokenize(text, language='russian')\n    clean_text = [word.lower() for word in tokenized_text if word not in punctuation and word != '\\n']\n    r = re.compile(\"[\u0430-\u044f\u0410-\u042f]+\")\n    russian_text = ' '.join([w for w in filter(r.match, clean_text)])\n    return russian_text","0d64f599":"df['comment'] = df['comment'].apply(lambda x: text_cleaner(x))","3167a509":"df.loc[df['comment'] == '']","a618415c":"df = df.drop(df[df['comment'] == ''].index)","829adcf0":"df.loc[df['comment'] == '']","11ef2531":"comments = df['comment'].to_numpy()\ncomments","6c0fb75c":"lemmatizator = Mystem()\ntext_for_lemmatization = ' sep '.join(comments)","ca9b76e7":"text_for_lemmatization[:1000]","939ec656":"lemmatizated_text = lemmatizator.lemmatize(text_for_lemmatization)\nlemmatizated_text_list = [word for word in lemmatizated_text if word != ' ' and word != '-']","9262bc73":"lemmatizated_text_list[:10]","601a2736":"lemmatizated_text = ' '.join(lemmatizated_text_list)\nlemmatizated_array = np.asarray(lemmatizated_text.split(' sep '))","91ea4c18":"lemmatizated_array[:10]","b9b3f561":"df['toxic'] = df['toxic'].astype(int)\nlabels = df['toxic'].to_numpy()\nlabels","1654d1aa":"X_train, X_test, y_train, y_test = train_test_split(lemmatizated_array, labels, test_size=0.2, stratify=labels, shuffle=True, random_state=42)","e16f48c6":"X_train","b7fb0bff":"token_counts = Counter()\nfor sent in X_train:\n    token_counts.update(sent.split(' '))\n    \ndict_size = len(token_counts.keys())\ndict_size","8261dfb5":"tokenizer = Tokenizer(num_words=dict_size)\ntokenizer.fit_on_texts(X_train)","295a4126":"X_train_tokenized = tokenizer.texts_to_sequences(X_train)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test)","18761219":"max_comment_length = 250\nX_train_padded = pad_sequences(X_train_tokenized, maxlen=max_comment_length)\nX_test_padded = pad_sequences(X_test_tokenized, maxlen=max_comment_length)","5b171164":"max_features = dict_size\nembedding_dim = 64\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_features, \n                    output_dim=embedding_dim, \n                    input_length=max_comment_length))\nmodel.add(Conv1D(filters=embedding_dim*2, \n                 kernel_size=2, \n                 activation='relu'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.summary()\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=[keras.metrics.Precision(), keras.metrics.Recall()])","4bc7dd93":"%%time\nepochs = 5\n\nhistory = model.fit(X_train_padded, y_train, epochs=epochs, validation_data=(X_test_padded, y_test), batch_size=512)","882e7895":"plt.figure(figsize=(10, 10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a9e79c55":"predictions = model.predict(X_test_padded)\nprint(predictions[5])","d2f6fbdd":"y_test[5]","e075135d":"example = '\u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0437\u0433\u0430'\nclean_example = text_cleaner(example)\nlemm_example = ' '.join(lemmatizator.lemmatize(clean_example))\narray_example = np.array([lemm_example])\nseq_example = tokenizer.texts_to_sequences(array_example)\npad_example = pad_sequences(seq_example, maxlen=max_comment_length)\npred_example = model.predict(pad_example)\nprint(pred_example)","9041a5f3":"Replace class labels with integers","ba87312c":"And now let's take a random example from the Internet, which is not in our dataset","a3848998":"And here it is - the simplest convolutional neural network :)\n\nLet's see if it can handle it. Among the quality metrics, in this case, a high recall is important, because it is better to erroneously react to a normal comment than to skip a toxic one.","0fe9cc17":"First, import the required libraries and load the data.","dc9e855f":"It's time for lemmatization. Stamming is too harsh with words, in cases of profanity and jargon, it can worsen the situation, so I use lemmatization.\n\n\n\nI want to thank the author of this article: https:\/\/habr.com\/ru\/post\/503420\/, his life hack helped speed up the lemmatization, applying the method not to every cell, but to the whole text, separated by separators","1433ae13":"Now split the dataset into training and test data and determine the size of the vocabulary by counting unique words in the training set.\nI count words only in the training set to avoid an obvious data leakage","482488a2":"Hello to all Russian speakers and everyone interested! \n\nRecently in the NLP course, I learned that convolutional neural networks can be used to classify text and decided to check this on this wonderful dataset. Let's see what happens :)","f7430290":"Finally, let's look at the predictions\n\nThe neural network does a good job on the test set","3569db7f":"Let's look at the graph of the network error change by epoch.","990ac019":"Now let's create a function that tokenizes the text, removes punctuation marks and English letters from it, and then apply this function to each cell in the DataFrame.\n\n\nI was given one important advice about this data set, it is better to try not to delete stop words here, they can carry a serious semantic load in the context of comments","bb53424b":"Some comments were last links or sets of numbers, so they should have been left blank after clearing the text. I will remove lines containing empty comments","127151b3":"Now, using the keras framework tokenizer, turn the text into a sequence of integers. After that I select the maximum comment length, all comments longer will be truncated, and those that are shorter will be padded","0f2083b2":"Well, looks good!\nThank You for watching :)"}}