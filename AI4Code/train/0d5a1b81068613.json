{"cell_type":{"3a3bd3a5":"code","311370a0":"code","7fea5d8f":"code","469c1403":"code","f2f064cf":"code","cd22916c":"code","bc4f1290":"code","bf2f064a":"code","c7e61531":"code","3bfdf975":"markdown","e53615ef":"markdown","71b50f67":"markdown","43223566":"markdown","0be8a64a":"markdown","a9ca2f0b":"markdown","ad48b072":"markdown"},"source":{"3a3bd3a5":"from pathlib import Path\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme()","311370a0":"path = Path('\/kaggle\/input\/house-prices-advanced-regression-techniques') # setting the directory path of the data\ntrain_ = pd.read_csv(path.joinpath('train.csv')) # reading the train file\ntest_ = pd.read_csv(path.joinpath('test.csv')) # readin the test file","7fea5d8f":"print(f'Total number of training examples: {train_.shape[0]}') # here f at the start is used to make the f-string a feature in 3.6 and higher.\nprint(f'Total number of test examples: {test_.shape[0]} ')","469c1403":"yr_sold = train_.groupby(['YrSold'], as_index=False).agg(\n                        sale_price_mean = ('SalePrice','mean')\n                        ).sort_values(['sale_price_mean'],ascending=False).reset_index(drop=True)\n# The above command is little heavier let me break it down into small pieces and explain along the way\n# before doing that my purpose what I want to do here.\n#  > I want to see how average sale price of the house is distributed over the years\n# To achieve the above one can use the pd.grouby function avaible in the pandas with the arguments you want to group by.\n# then we can apply the aggregate function available in the pandas in which we can give the columns and operation we want to perform.\n# the I want sort the data frame from the maximum to minimum (thats why ascending is set to False ) values over the years. then reset_index is used to start the index of the rows from zero to num_rows-1.\nplt.figure(figsize=(15,8))\np = sns.lineplot(data=yr_sold, x = 'YrSold', y= 'sale_price_mean', linewidth=3, marker='d',markersize=20)","f2f064cf":"train_.groupby(['YrSold','MoSold'],as_index=False).agg(\n                                        month_sale_price = ('SalePrice', lambda x: np.mean(x)),\n                                    ).groupby(['YrSold'],as_index = False).agg(\n                                            max_avg_ = ('month_sale_price',max),\n                                            MoSold = ('month_sale_price', lambda x: np.arange(1,13)[np.argmax(x)])\n                                                ) #I tried to keep things simple here there are advance options you  can use, here I have hardcoded a little bit to get the index of the max month","cd22916c":"# this same as the above explaine code just to visualize the average saleprice per-month over the dataset\ngrp = train_.groupby(['YrSold', 'MoSold']).agg(month_sale_price = ('SalePrice', np.mean))\ngrp.reset_index(inplace=True)\ngrp['date'] = grp['YrSold'].map(str) + '-' + grp['MoSold'].map(str)\ngrp['date'] = pd.to_datetime(grp['date'],format='%Y-%m').dt.strftime('%m-%Y')\nplt.figure(figsize=(25,10))\nsns.lineplot(data=grp, x='date', y = 'month_sale_price', marker='o', markersize=10)\nplt.xticks(rotation=90)\nplt.show()","bc4f1290":"cols_ = train_.columns[1:-1 ] # to select only the feature columsn (i.e., removing the id and saleprice columns from the list)\ncor_ =  train_[cols_].corr() # finding the correlation\ncor_ = cor_.where(np.tril(np.ones(cor_.shape)).astype('bool')) # taking only the lower part of the correlation matrix\nfig,ax = plt.subplots(1,1,figsize=(15,12))\nsns.heatmap(cor_, ax=ax)","bf2f064a":"cor_stack = cor_.stack();\nhighly_correlated = cor_stack[(abs(cor_stack.values) > 0.5)  & (abs(cor_stack.values)<1)].reset_index().rename(columns={'level_0':'first', 'level_1':'second',0:'coef'})\n# let me write the above line in a simple way to be more understandable for the newbies :)\nfirst_operation = cor_stack[(abs(cor_stack.values)>0.5) & (abs(cor_stack.values)<1)] # first we select those indices which have the correlation coefficient more than 0.5 and less than,the & sign is using to meet both conditions\nfirst_operation = first_operation.reset_index() # the columns of the cor_ matrix are the indices of the first_operation data frame we want them to be columns reset_index is doing this job\n# now let's rename the columns of the first_operation\nfirst_operation.rename(columns={'level_0':'first', 'level_1':'second',0:'coef'},inplace=True)\n# one can check that both highly_correlated and first_operation data frames are same","c7e61531":"# lets sort in the increasing order of the correlation coefficients.\nhighly_correlated.sort_values('coef',ascending=False).reset_index(drop=True)#.iloc[0:3]","3bfdf975":"#### Explaining the correlation matrix\nIn the following I shall select the highly correlated features (say, having correlation coefficient more than 0.5) and try explain based on the domain knowledge. The same kind of expolatory data analysis (EDA) can be done on the other datasets.","e53615ef":"### Using domain knowledge to explain the correlations\n\n<html>\n    <body>\n             <table style=\"width:100%\">\n          <tr>\n            <th>first<\/th>\n            <th>second<\/th>\n            <th>coef<\/th>\n          <\/tr>\n          <tr>\n            <td>GarageArea<\/td>\n            <td>GarageCars<\/td>\n            <td>0.882475<\/td>\n          <\/tr>\n          <tr>\n            <td>GarageYrBlt<\/td>\n            <td>YearBuilt<\/td>\n            <td>0.825667<\/td>\n          <\/tr>\n               <tr>\n                   <td>TotRmsAbvGrd<\/td>\n                   <td>GrLivArea<\/td>\n                   <td>0.825489<\/td>\n                 <\/tr>\n        <\/table> \n            <ul>\n                <li>In the first row <b>coef<\/b> has the value of <b>0.882475<\/b> between <b>GarageArea<\/b> and <b>GarageCars<\/b>, these two features are correlated as the higher the garage area more cars can you park in.\n        <\/li>\n                <li>In the second row <b>coef<\/b> has the value of <b>0.825667<\/b> between <b>GarageYrBlt<\/b> and <b>YearBuilt<\/b>, these two features are highly correlated because it is highly likely that one build a garage at the same time when the house was built.<\/li>\n                <li>Same is true for the third row in which we have <b>0.825489<\/b> value of the <b>coef<\/b> which can be explained in the same way. <\/li>\n        <\/ul>\n        While feeding these features to ML model one can only select the one of among two highly correlated features, because they do not give any extra information to the model.\n    <\/body>\n<\/html>","71b50f67":"# Now look at the month sale for every year","43223566":"Let's see the average house price sales per year and sort them accordingly.\n\nWe can see that the maximum value of the average house sales was in 2007 and minimum was in 2008.","0be8a64a":"# EDA for the beginners\n\nIn this notebook I shall not only do the exploratory data analysis (EDA) but also explain the codes along the way for the beginners. ","a9ca2f0b":"## More to come...","ad48b072":"#### From the above plot we can see that the maximum overall maximum average price of the house is September 2006 (i.e., before the recession period).\n\n#### in 2006, 2007, and 2008 the maximum average price was in the months of September, November, and Octobor, respectively.\n\n#### It is interesting to relate with the recession of 2007-2008."}}