{"cell_type":{"1f927b85":"code","7afd7e17":"code","b2a4a6a4":"code","0a2d18a8":"code","4af847e3":"code","2e4e0432":"code","bcf9fe08":"code","ca7c8658":"code","905283a5":"code","ba708cbc":"code","d449c240":"code","6b2635b7":"code","0cde7b69":"code","545ce9c9":"code","e9a608bc":"code","65db73b3":"code","a51b9ddd":"code","899df8eb":"code","07f75864":"code","022f348d":"code","7f550f43":"code","6bcd645d":"code","e6a6e639":"code","b2f420db":"code","1a5db394":"code","18040ae1":"code","69bedb7b":"code","e11f2d4f":"code","49919c13":"code","1b976a06":"code","eb26e9aa":"code","6040cf1b":"code","978367ca":"code","76811130":"code","ff8c3598":"code","c499c531":"code","f05e3243":"code","afe8f32a":"markdown","ff04bf53":"markdown","0c4f5f8e":"markdown","4f732d8a":"markdown","fdfa4809":"markdown","55a19b35":"markdown","e4eff62c":"markdown"},"source":{"1f927b85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom scipy.stats import skew, norm \nfrom warnings import filterwarnings as filt\n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7afd7e17":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.shape","b2a4a6a4":"df.head()","0a2d18a8":"df.isnull().values.sum()","4af847e3":"from eli5 import show_weights\nfrom eli5.sklearn import PermutationImportance \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold","2e4e0432":"sns.countplot(df.target)","bcf9fe08":"x = df.drop(['target'], axis = 1)\ny = df.target\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)","ca7c8658":"def permImp(x, y):\n    model = RandomForestClassifier().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return show_weights(perm, feature_names = x.columns.tolist())","905283a5":"permImp(x, y)","ba708cbc":"sns.pairplot(df.drop('fbs', axis = 1), hue = 'target')","d449c240":"sns.heatmap(df.corr(), fmt = '.1f', annot = True, cmap = 'gnuplot')","6b2635b7":"from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, Normalizer, MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import plot_tree, DecisionTreeClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve","0cde7b69":"def best_model(x, y):\n    models = [SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n    mnames = ['svm', 'decision tree', 'random forest']\n    scalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler(), PowerTransformer(), Normalizer()]\n    snames = ['none', 'std', 'robust', 'min max', 'power transformer', 'normalizer']\n    scores = [[] for _ in range(len(snames))]\n    iterr = 0\n    print(f'total no. iteration : {len(mnames) * len(snames)}')\n    \n    for model in models:\n        for ind , scaler in enumerate(scalers):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            \n            cv = StratifiedKFold(10, shuffle = True)\n            score = cross_val_score(model, x, y, cv = cv, scoring = 'f1').mean()\n            scores[ind].append(score)\n            \n            iterr += 1\n            print(f'iteration no. :======> {iterr} \/ {len(mnames) * len(snames)}')\n            \n    return pd.DataFrame(scores, index = snames, columns = mnames).T            \n\ndef get_score(xt, yt, xtest, ytest, model, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    print(' Report '.center(70, '='))\n    print()\n    print(f\"training score :==> {model.score(xt, yt)}\")\n    print(f\"testing score  :==> {model.score(xtest, ytest)}\")\n    print(f\"roc auc score  :==> {roc_auc_score(ytest, pred)}\")\n    print()\n    print(classification_report(ytest, pred))\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n    \ndef gridcv(x, y, model, params, scaler = None, fold = 10):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    cv = StratifiedKFold(fold, shuffle = True)\n    clf = GridSearchCV(model, param_grid = params, cv = cv, return_train_score = 'True')\n    clf.fit(x, y)\n    results = pd.DataFrame(clf.cv_results_)\n    return clf, results[['mean_train_score', 'mean_test_score', 'params']]\n\ndef plot_cv(res):\n    sns.lineplot(x = res.index, y = res.mean_train_score)\n    sns.lineplot(x = res.index, y = res.mean_test_score)\n    plt.title('accuracy comparision for train and test set')\n    plt.legend(['training score', 'testing score'])","545ce9c9":"# fbs feats wasnt useful to the model at all , so lets drop it \n\nx_train = x_train.drop(['fbs'], axis = 1)\nx_test = x_test.drop(['fbs'], axis = 1)","e9a608bc":"best_model(x_train, y_train)","65db73b3":"clf = DecisionTreeClassifier()\nclf.fit(x_train, y_train)\nplt.figure(figsize = (18,15))\nplot_tree(clf, filled = True, feature_names = x_train.columns, fontsize=12);","a51b9ddd":"print(f\"training score : {clf.score(x_train, y_train)}\")\nprint(f\"testing score : {clf.score(x_test, y_test)}\")","899df8eb":"path = clf.cost_complexity_pruning_path(x_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","07f75864":"ccp_alphas","022f348d":"impurities","7f550f43":"clf , results = gridcv(x_train, y_train, DecisionTreeClassifier(), {'model__ccp_alpha' : ccp_alphas}, StandardScaler())","6bcd645d":"plot_cv(results)","e6a6e639":"results.sort_values('mean_test_score', ascending = False ).head()","b2f420db":"clf.best_params_","1a5db394":"get_score(x_train, y_train, x_test, y_test, clf)","18040ae1":"clf, results = gridcv(x_train, y_train, RandomForestClassifier(), {'model__ccp_alpha' : ccp_alphas}, StandardScaler())","69bedb7b":"plot_cv(results)","e11f2d4f":"results.sort_values('mean_test_score', ascending = False).head()","49919c13":"get_score(x_train, y_train, x_test, y_test, clf)","1b976a06":"get_score(x_train, y_train, x_test, y_test, SVC(), RobustScaler())","eb26e9aa":"params = {\n        'C' : [0.1, 1, 2, 3, 4, 5, 10, 20, 50, 100, 200],\n        'kernel' : ['rbf', 'poly', 'sigmoid'],\n        'gamma' : ['scale', 'auto'],\n        'class_weight' : [None, 'balanced']\n}\n\npip_params = {f\"model__{key}\" : values for key, values in params.items()}\npip_params","6040cf1b":"clf, results = gridcv(x_train, y_train, SVC(), pip_params, RobustScaler())","978367ca":"plot_cv(results)","76811130":"results.sort_values('mean_test_score', ascending = False).head()","ff8c3598":"clf","c499c531":"results.sort_values('mean_test_score', ascending = False).iloc[0,-1]","f05e3243":"get_score(x_train, y_train, x_test, y_test, SVC(class_weight='balanced', gamma='auto'), RobustScaler())","afe8f32a":"### pruning","ff04bf53":"### ccp alpha for random forest","0c4f5f8e":"### drc without pruning ","4f732d8a":"hence we were able to get low bais and low variance with the help of grid search and ccp alpha","fdfa4809":"the above svc model gave the highest score of 87%","55a19b35":"using random forest and decision tree for testing cost complexity pruning ","e4eff62c":"### svm"}}