{"cell_type":{"9f5f113a":"code","7408684f":"code","6c45a2bd":"code","b4abf13e":"code","68ddfec3":"code","659e1cfa":"code","09c9c734":"code","5e5f2bdf":"code","2a1d16ab":"code","9a688d7e":"code","0a2c6c85":"code","1b41eea4":"code","23fcffab":"code","0a431572":"code","1f64589e":"code","4118d02a":"code","3d0fa15e":"code","1d8284f5":"code","2d00fd3b":"code","40dffcf3":"code","02118868":"code","91835886":"code","9b83f635":"markdown","c2b20855":"markdown","a954edd7":"markdown","40785d25":"markdown","966bc4bb":"markdown","5ec8f664":"markdown","4e4adb59":"markdown","32548905":"markdown","a0697d47":"markdown","f453edd0":"markdown","7624944a":"markdown","212ecbd0":"markdown","3d7849d1":"markdown","f1d22590":"markdown","d346683e":"markdown","81681ba7":"markdown","1e72635c":"markdown","2b3ad40d":"markdown","c2201235":"markdown","7e226399":"markdown"},"source":{"9f5f113a":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nsns.set()\npd.set_option('display.expand_frame_repr', False)","7408684f":"train = pd.read_csv('\/kaggle\/input\/titaniccleaned\/train_cleaned.csv')\ntest = pd.read_csv('\/kaggle\/input\/titaniccleaned\/test_cleaned.csv')\npassengerId = test['PassengerId']\ntest.drop('PassengerId', inplace=True, axis=1)\n\nx_train = train.drop(['Survived'], axis=1)\ny_train = train['Survived']","6c45a2bd":"x_train.head()","b4abf13e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","68ddfec3":"from sklearn.metrics import accuracy_score\n\n\n# Logistic Regression\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\nacc_log = round(lr.score(x_train, y_train) * 100, 2)\n\n# Support Vector Machines\nsvc = SVC()\nsvc.fit(x_train, y_train)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\n\n# Multi Layers Perceptron\nmlp = MLPClassifier()\nmlp.fit(x_train, y_train)\nacc_mlp = round(mlp.score(x_train, y_train) * 100, 2)\n\n# Decision Tree Classifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\nacc_dtc = round(mlp.score(x_train, y_train) * 100, 2)\n\n# Random Forest Classifier\nrfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\nacc_rfc = round(rfc.score(x_train, y_train) * 100, 2)\n\n# Ada Boost Classifiter\nabc = AdaBoostClassifier()\nabc.fit(x_train, y_train)\nacc_abc = round(abc.score(x_train, y_train) * 100, 2)\n\n# Guassian NB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\nacc_gnb = round(gnb.score(x_train, y_train) * 100, 2)\n\n# Bagging Classifier\nbc = BaggingClassifier()\nbc.fit(x_train, y_train)\nacc_bc = round(bc.score(x_train, y_train) * 100, 2)\n\n# Gradient Boosting Classifier\ngbc = GradientBoostingClassifier()\ngbc.fit(x_train, y_train)\nacc_gbc = round(gbc.score(x_train, y_train) * 100, 2)\n\n# K-Neighbors Classifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\n\n# XGboost Classifier\n_xgb = XGBClassifier()\n_xgb.fit(x_train, y_train)\nxgb_preds = _xgb.predict(x_train)\nacc_xgb =  round(accuracy_score(y_train, xgb_preds) * 100, 2)","659e1cfa":"results = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Multi Layers Perceptron (NN)',\n              'Decision Tree Classifier', 'Random Forest', 'AdaBoost Classifier', 'Guassian Naive Bayes', 'Bagging Classifier', \n              'Gradient Boosting Classifier', 'K-Neighbors Classifier', 'XGB'],\n    'Score': [acc_log, acc_svc, acc_mlp, acc_dtc, acc_rfc, acc_abc, acc_gnb, acc_bc, acc_gbc, acc_knn, acc_xgb]\n})\n\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","09c9c734":"import numpy as np\n\nimportances = pd.DataFrame({'feature':train.columns})\n\nimportances_rfc = pd.DataFrame({'feature':x_train.columns, 'RFC': np.round(rfc.feature_importances_, 3)})\nimportances = pd.merge(importances, importances_rfc, on=['feature', 'feature']).set_index('feature')\n\nimportances_gbc = pd.DataFrame({'feature':x_train.columns, 'GBC': np.round(gbc.feature_importances_, 3)})\nimportances = pd.merge(importances, importances_gbc, on=['feature', 'feature']).set_index('feature')\n\nimportances_xgb = _xgb.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())","5e5f2bdf":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(24, 3))\n\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\ndata.plot(kind='bar', title='XGBoost Classifier (weights)', ax=axes[0])\nimportances['RFC'].plot.bar(title='Random Forest Classifier', ax=axes[1])\nimportances['GBC'].plot.bar(title='Gradiant Boosting Classifier', ax=axes[2])\n\nplt.subplots_adjust(top = 0.99, bottom=0.01, hspace=1, wspace=0.2)\nplt.show()","2a1d16ab":"from sklearn.model_selection import train_test_split\n\n\nfeatures = train.drop(['Survived'], axis=1)\nlabels = train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)","9a688d7e":"def print_results(results):\n    \n    best_acc = None\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    params = results.cv_results_['params']\n    for mean, std, params in zip(means, stds, params):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n        if params == results.best_params_:\n            best_acc = round(mean, 3)\n    \n    print('\\nBEST PARAMS: {} | acc: {} (+\/-{})\\n'.format(results.best_params_, best_acc, round(std * 2, 3)))\n    ","0a2c6c85":"X_train = x_train.drop(['Alone', 'Embarked', 'SibSp'], axis=1)\nX_test = x_test.drop(['Alone', 'Embarked', 'SibSp'], axis=1)","1b41eea4":"rf = RandomForestClassifier()\nparameters = {\n    \"n_estimators\": [50, 100, 500],\n    \"max_depth\": [4, 8, 16, None],\n    \"criterion\" : [\"gini\", \"entropy\"],\n    \"min_samples_leaf\" : [5, 10, 25, 50]\n}\n\nrfccv = GridSearchCV(rf, parameters, cv=7, n_jobs=-1)\nrfccv.fit(X_train, y_train.values.ravel())\nprint_results(rfccv)\n","23fcffab":"gbc = GradientBoostingClassifier()\nparameters = {\n    'n_estimators': [10, 50, 250, 500, 1000],\n    'learning_rate': [0.001, 0.01, 1],\n    'max_depth': [1, 5, 10, 20]\n}\n\ngbccv = GridSearchCV(gbc, parameters, cv=7, n_jobs=-1)\ngbccv.fit(X_train, y_train.values.ravel())\nprint_results(gbccv)","0a431572":"_xgb = XGBClassifier(objective= 'binary:logistic', nthread=4, seed=42)\n\nparameters = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\nxgbcv = GridSearchCV(estimator=_xgb, param_grid=parameters, scoring = 'roc_auc', n_jobs=-1, cv=7, verbose=True)\nxgbcv.fit(x_train, y_train.values.ravel())\nprint_results(xgbcv)","1f64589e":"from sklearn.ensemble import VotingClassifier \n\nmodels = {\n    \"rfc\": {\"mdl\":rfccv.best_estimator_},\n    \"gbc\": {\"mdl\":gbccv.best_estimator_},\n    \"xgb\": {\"mdl\":xgbcv.best_estimator_}\n}\n\nestimators = [(mdl_name, models[mdl_name]['mdl']) for mdl_name, model in models.items()]\n\nvot_hard = VotingClassifier(estimators = estimators, voting = 'hard', n_jobs=-1)\nvot_hard.fit(x_train, y_train)\nmodels['vot'] = {'mdl': vot_hard}\n# evaluate_model('Voting Hard', vot_hard, x_test, y_test)","4118d02a":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom time import time\n\ndef evaluate_model(name, model, features, labels):\n    \n    start = time()\n    predicted_values = model.predict(features)\n    end = time()\n    f1 = round(f1_score(labels, predicted_values), 2)\n    accuracy = round(accuracy_score(labels, predicted_values), 2)\n    precision = round(precision_score(labels, predicted_values), 2)\n    recall = round(recall_score(labels, predicted_values), 2)\n\n    print(\"Name: {} | Accuracy: {} | Precision: {} | Recall: {} | F1: {} | Latency: {}ms\".format(name,\n                                                                                        accuracy,\n                                                                                        precision,\n                                                                                        recall,\n                                                                                        f1,      \n                                                                                        round(end - start, 4)))\n    return f1, accuracy, precision, recall\n\n\nfor mdl_name, model in models.items():\n    if mdl_name in ['rfc', 'gbc']:\n        f1, accuracy, precision, recall = evaluate_model(mdl_name, model['mdl'], X_test, y_test)\n    else:\n        f1, accuracy, precision, recall = evaluate_model(mdl_name, model['mdl'], x_test, y_test)\n    models[mdl_name]['f1'] = f1\n    models[mdl_name]['acc'] = accuracy\n    models[mdl_name]['prec'] = precision\n    models[mdl_name]['rec'] = recall","3d0fa15e":"performances = pd.DataFrame({'Performace Measures':['Accuracy', 'Precision', 'Recall', 'F1']})\n\nfor name, model in models.items():\n    name = pd.DataFrame({'Performace Measures':['Accuracy', 'Precision', 'Recall', 'F1'], \n                         name: [model['acc'], model['prec'], model['rec'], model['f1']]})\n    performances = pd.merge(performances, name, \n                            on=['Performace Measures', 'Performace Measures']).set_index('Performace Measures')\n\nperformances\n","1d8284f5":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = models['rfc']['mdl'].predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","2d00fd3b":"models","40dffcf3":"def ensemble(models, x_features, X_features):\n    threshold = 0.3\n    df = pd.DataFrame(columns=['rfc', 'gbc', 'xgb'])\n    f1_com = 0\n    for mdl_name, model in models.items():\n        if mdl_name in ['rfc', 'gbc']:\n            pred = model['mdl'].predict(X_features)\n        else:\n            pred = model['mdl'].predict(x_features)\n        df[mdl_name] = pred * model['f1']\n        f1_com += model['f1']\n        \n    df['Survived'] = round((df['rfc']+ df['gbc'] + df['xgb']) \/ f1_com, 2)\n\n    df.loc[df['Survived'] <= threshold, 'Survived'] = 0,\n    df.loc[df['Survived'] > threshold, 'Survived'] = 1,\n    return df['Survived']\n\n\n# getting the probabilities of our predictions\npredictions = ensemble(models, x_train, X_train)\n\naccuracy = round(accuracy_score(y_train, predictions), 2)\nprecision = round(precision_score(y_train, predictions), 2)\nrecall = round(recall_score(y_train, predictions), 2)\nprint(\"Accuracy: {} | Precision: {} | Recall: {}\".format(accuracy, precision, recall))","02118868":"pred = esemble(models, test)\n\ndata = list(zip(passengerId, pred))\nsubmission = pd.DataFrame(data, columns=['PassengerId', 'Survived'])\nsubmission['Survived'] = submission['Survived'].astype(int)\nsubmission.to_csv('.\/submission.csv', index=False)","91835886":"pred = models['xgb']['mdl'].predict(test)\ndata = list(zip(passengerId, pred))\nsubmission = pd.DataFrame(data, columns=['PassengerId', 'Survived'])\nsubmission.to_csv('.\/submission.csv', index=False)","9b83f635":"### Using Best Selected Model","c2b20855":"# Titanic: Machine Learning from Disaster","a954edd7":"### XGBoost Classifier\nWe'll first train XGB Classifier on all features because they all seems important to XGB","40785d25":"## Weighted Mean Model","966bc4bb":"### Removing Less important features","5ec8f664":"# Loading data\nNote: The cleaned train.csv and test.csv were created in EDA and Feature Engineering Notebook. I'm using these files directly here. You can click on the link below to find that Notebook.\n\nhttps:\/\/www.kaggle.com\/nomihsa965\/titanic-eda-features-engineering","4e4adb59":"### Printing out the Mean Accuracy for every Model provided by sklearn\nWe'll select the top 4 classifiers for further processing","32548905":"RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.(Source: Wikipedia)\n\n<img src=\"https:\/\/camo.githubusercontent.com\/e15a5414be97decd975cfed68e0c0f79e768f7e7\/68747470733a2f2f737461746963312e73717561726573706163652e636f6d2f7374617469632f3530303634353366653462303965663232353262613036382f742f3530393062323439653462303437626135346466643235382f313335313636303131333137352f544974616e69632d537572766976616c2d496e666f677261706869632e6a70673f666f726d61743d3135303077\">","a0697d47":"# Evaluating Models with the Test Data","f453edd0":"### Using Custom Ensembled Model","7624944a":"### Precicion Recall Curve for the Best Selected Model","212ecbd0":"# Splitting Train\/Test Data","3d7849d1":"### Gradient Boosting Classifier","f1d22590":"### Important Features Selection\nLet's visualize the features importance for differnt classifiers. We will remove features do not contribute in predicting the survival rate for every classifier.","d346683e":"### Random Forest Classifier\nLet's first remove less inportant features from our previous observations","81681ba7":"## Making Predictions on Kaggle Test DatA","1e72635c":"# Training Different Classifiers","2b3ad40d":"### Voting Classifier","c2201235":"### Models Performances into DataFrame","7e226399":"# Improving Models' Performance by:\n1. Tuning Hyper-Parameters \n2. Applying K-fold Cross Validation\n\nWe have selected the top-4 models from our previous scores which are Random Forests, Bagging Classifier, Gradient Boosting Classifier and K-Neighbors Classifier"}}