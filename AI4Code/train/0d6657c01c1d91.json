{"cell_type":{"c6c0c37a":"code","a4bbbe83":"code","bae13cd0":"code","43420633":"code","62b8252d":"code","af1372ec":"code","baf2d6f8":"code","33c31969":"code","272bce99":"code","c2483666":"code","9cfa5072":"code","4c40034f":"code","ddd4b3e5":"code","e15fc05d":"code","49f2b02a":"code","12151caa":"markdown","282dd7dd":"markdown","c87d3470":"markdown","4adb56b2":"markdown","aa7305bd":"markdown","d4e6379e":"markdown","1fc892b0":"markdown","da95b5c8":"markdown","12066932":"markdown","51d73243":"markdown","6e53b028":"markdown","19a585c4":"markdown","cbd003e0":"markdown","bf503100":"markdown","5afb5618":"markdown"},"source":{"c6c0c37a":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\n\ndef printFiles(path):\n  return glob.glob(path)\n\nprintFiles('..\/input\/name-languages\/*.txt')","a4bbbe83":"import unicodedata\nimport string\n\nall_let = string.ascii_letters + \" .,;'\"\nn_let = len(all_let)\n\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_let\n    )","bae13cd0":"cat_line = {}\nall_cats = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in printFiles('..\/input\/name-languages\/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_cats.append(category)\n    lines = readLines(filename)\n    cat_line[category] = lines\n\nn_categories = len(all_cats)","43420633":"#Check names in a category\nprint(cat_line['Japanese'][:4])","62b8252d":"import torch\n# Find letter index from all_let, e.g. \"a\" = 0\ndef letterToIndex(letter):\n    return all_let.find(letter)\n\n# Turn a letter into a <1 x n_let> Tensor\ndef letterToTensor(letter):\n    tensor = torch.zeros(1, n_let)\n    tensor[0][letterToIndex(letter)] = 1\n    return tensor\n\n# Turn a line into a <line_length x 1 x n_let>,\n# or an array of one-hot letter vectors\ndef lineToTensor(line):\n    tensor = torch.zeros(len(line), 1, n_let)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor","af1372ec":"print(letterToTensor('K'))\nprint(lineToTensor('Kakinomoto').size())","baf2d6f8":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\nn_hidden = 128\n#Binding model\nrnn = RNN(n_let, n_hidden, n_categories)","33c31969":"input = lineToTensor('Aalsburg')\nhidden = torch.zeros(1, n_hidden)\n\noutput, next_hidden = rnn(input[0], hidden)\nprint(output)","272bce99":"import random\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\ndef randomTrainingExample():\n    category = randomChoice(all_cats)\n    line = randomChoice(cat_line[category])\n    category_tensor = torch.tensor([all_cats.index(category)], dtype=torch.long)\n    line_tensor = lineToTensor(line)\n    return category, line, category_tensor, line_tensor\n\n#Check on a random sample\nfor i in range(10):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    print('category =', category, '\/ line =', line)","c2483666":"def categoryFromOutput(output):\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n    return all_cats[category_i], category_i\n#Check category for an output\nprint(categoryFromOutput(output))","9cfa5072":"criterion = nn.NLLLoss()\n\nlearning_rate = 0.005 \n\ndef train(category_tensor, line_tensor):\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()\n\n    for i in range(line_tensor.size()[0]):\n        output, hidden = rnn(line_tensor[i], hidden)\n\n    loss = criterion(output, category_tensor)\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    for p in rnn.parameters():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return output, loss.item()\n\nimport time\nimport math\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 1000\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output, loss = train(category_tensor, line_tensor)\n    current_loss += loss\n\n    # Print iter number, loss, name and guess\n    if iter % print_every == 0:\n        guess, guess_i = categoryFromOutput(output)\n        correct = '\u2713' if guess == category else '\u2717 (%s)' % category\n        print('%d %d%% (%s) %.4f %s \/ %s %s' % (iter, iter \/ n_iters * 100, timeSince(start), loss, line, guess, correct))\n\n    # Add current loss avg to list of losses\n    if iter % plot_every == 0:\n        all_losses.append(current_loss \/ plot_every)\n        current_loss = 0","4c40034f":"#Visualize Performance\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)","ddd4b3e5":"# Keep track of correct guesses in a confusion matrix\nconfusion = torch.zeros(n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate(line_tensor):\n    hidden = rnn.initHidden()\n\n    for i in range(line_tensor.size()[0]):\n        output, hidden = rnn(line_tensor[i], hidden)\n\n    return output\n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output = evaluate(line_tensor)\n    guess, guess_i = categoryFromOutput(output)\n    category_i = all_cats.index(category)\n    confusion[category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    confusion[i] = confusion[i] \/ confusion[i].sum()\n\n# Set up plot\nfigsize = (10, 10)\nfig = plt.figure(figsize=figsize)\nax = fig.add_subplot(111)\ncax = ax.matshow(confusion.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_cats, rotation=90)\nax.set_yticklabels([''] + all_cats)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()","e15fc05d":"def predict(input_line, n_predictions=3):\n    print('\\n> %s' % input_line)\n    with torch.no_grad():\n        output = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = output.topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_cats[category_index]))\n            predictions.append([value, all_cats[category_index]])","49f2b02a":"predict('Aggelen')\npredict('Accardo')\npredict('Ferreiro')","12151caa":"In the next step, the hyperparameters and the training function will be defined and the RNN model will be trained in 100 epochs.","282dd7dd":"Finally, we will check the predicted likelihoods for the given three names.","c87d3470":"Reading the data","4adb56b2":"The below code snippet will test the on the unseen texts and plot the confusion matrix.","aa7305bd":"# Name Language Prediction using Recurrent Neural Network in PyTorch","d4e6379e":"In the next step, the function modules will be defined to turn the names into tensors to make them compatible with the RNN model.","1fc892b0":"The below lines of codes define function modules to convert Unicode text to equivalent ASCII value.","da95b5c8":"We will check the above module by converting a letter to tensor and a line to tensor.","12066932":"Here, we will demonstrate the implementation of a Recurrent Neural Network (RNN) using PyTorch in the task of multi-class text classification. This RNN model will be trained on the names of the person belonging to 18 language classes. After successful training, the model will predict the language category for a given name that it is most likely to belong. ","51d73243":"We will check the above function for 4 Japanese names.\n","6e53b028":"The below function will print the likelihood of belonging to a language category for the given names.","19a585c4":"This model will be checked on generating tensor output for a name.","cbd003e0":"Now, we will define functions for providing random training examples to the network during training and generating categories for the network outputs.","bf503100":"Using the below code snippet, a function will be defined to build the dictionary of categories and a list of names in every language.","5afb5618":"In the next step, we will define the Recurrent Neural Network model."}}