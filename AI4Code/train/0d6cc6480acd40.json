{"cell_type":{"22fd930d":"code","903435a9":"code","0f84f023":"code","baddbe84":"code","16a619be":"code","97084c6f":"code","a4fc1c29":"code","b27268cf":"code","26eeb6ad":"code","4d2aafc5":"markdown","4374aefc":"markdown","9aa8176e":"markdown","5cc095b1":"markdown","5341f351":"markdown","51838254":"markdown","9f9f7a42":"markdown","099bc0c5":"markdown","5e904674":"markdown","7cabd638":"markdown","5d99d846":"markdown","77a491c8":"markdown"},"source":{"22fd930d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","903435a9":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\n\nX = np.array([[1],[2],[3],[4],[8],[9],[10],[11]])\nY = np.array([0, 0, 0, 0, 1, 1, 1, 1])\nmodel = LinearRegression()\nmodel.fit(X, Y)\nx_values = np.linspace(0, 12, 100)\ny_values = [model.intercept_ + x * model.coef_[0] for x in x_values]\nax[0].plot(x_values, y_values)\nax[0].scatter(X, Y, c='b');\nax[0].title.set_text('\u0420\u0438\u0441. 1')\n\nX = np.array([[1],[2],[3],[4],[8],[9],[10],[11], [18], [19], [20], [21], [22], [23], [24]])\nY = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\nmodel = LinearRegression()\nmodel.fit(X, Y)\nx_values = np.linspace(0, 25, 100)\ny_values = [model.intercept_ + x * model.coef_[0] for x in x_values]\nax[1].plot(x_values, y_values)\nax[1].scatter(X, Y, c='b');\nax[1].title.set_text('\u0420\u0438\u0441. 2')","0f84f023":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n\nx_values = np.linspace(-5, 5, 100)\ny_values =  [sigmoid(x) for x in x_values]\n\nax[0].plot(x_values, y_values);\n\nX = np.array([[-1],[-2],[-3],[-4],[-8],[-9],[5],[8], [12], [13], [14], [15]])\nY = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n\nx_values = np.linspace(-10, 25, 100)\ny_values =  [sigmoid(x) for x in x_values]\n\nax[1].plot(x_values, y_values);\nax[1].scatter(X, [sigmoid(x) for x in X], c='r');","baddbe84":"# hx - sigmoid values between (0, 1)\n\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n\ndef if_y_1(hx):\n    return -np.log(hx)\n\nx_values = np.linspace(0.001, 1, 100)\ny_values = [if_y_1(hx) for hx in x_values]\n\nax[0].plot(x_values, y_values)\nax[0].title.set_text('If y = 1');\nax[0].set_xlabel('h(x) - sigmoid output')\nax[0].set_ylabel('Cost')\n\ndef if_y_0(hx):\n    return -np.log(1 - hx)\n\nx_values = np.linspace(0, 0.999, 100)\ny_values = [if_y_0(hx) for hx in x_values]\n\nax[1].plot(x_values, y_values)\nax[1].title.set_text('If y = 0');\nax[1].set_xlabel('h(x) - sigmoid output')\nax[1].set_ylabel('Cost');","16a619be":"class LogisticRegression:\n    \n    def __init__(self,lr=0.1,n_iters=1000):\n        self.lr = lr\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n    \n    def fit(self,X,y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        for _ in range(self.n_iters):\n            linear_model = X @ self.weights + self.bias\n            hx = self._sigmoid(linear_model)\n            \n            dw = (X.T * (hx - y)).T.mean(axis=0)\n            db = (hx - y).mean(axis=0)\n\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db \n\n    def predict(self,X):\n        linear_model = np.dot(X,self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n        return y_predicted_cls\n  \n    def _sigmoid(self,x):\n        return(1\/(1+np.exp(-x)))","97084c6f":"X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","a4fc1c29":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\n\ncolor = ['blue' if l == 0 else 'green' for l in y_train]\nax[0].scatter(X_train[:, 0], X_train[:, 1], c=color, label='1')\nax[0].title.set_text('Train sample')\n\ncolor = ['blue' if l == 0 else 'green' for l in y_test]\nax[1].scatter(X_test[:, 0], X_test[:, 1], c=color, label='1');\nax[1 ].title.set_text('Test sample')","b27268cf":"model = LogisticRegression()\nmodel.fit(X_train, y_train);","26eeb6ad":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\n\nx_values = np.linspace(X_train[:, 0].min(), X_train[:, 0].max(), 100)\ny_values = [(-model.bias - model.weights[0]*x) \/ model.weights[1] for x in x_values]\ncolor = ['blue' if l == 0 else 'green' for l in y_train]\nax[0].scatter(X_train[:, 0], X_train[:, 1], c=color, label='1')\nax[0].plot(x_values, y_values)\nax[0].title.set_text('Train sample, accuracy: {}'.format(accuracy_score(y_train, model.predict(X_train))))\n\nx_values = np.linspace(X_test[:, 0].min(), X_test[:, 0].max(), 100)\ny_values = [(-model.bias - model.weights[0]*x) \/ model.weights[1] for x in x_values]\ncolor = ['blue' if l == 0 else 'green' for l in y_test]\nax[1].scatter(X_test[:, 0], X_test[:, 1], c=color, label='1');\nax[1].plot(x_values, y_values)\nax[1].title.set_text('Test sample, accuracy: {}'.format(accuracy_score(y_test, model.predict(X_test))))","4d2aafc5":"## Loss function\n\nLet's create some rule to penalize our model for error!\n\nWe know, that $y$ can be 0 or 1. And our $\\hat{y}$ - predicted value is between 0 and 1. If $y$ is similar to $\\hat{y}$ the less error we want to have, and the more $\\hat{y}$ differ from $y$ the more erorr we want to have. Well, it's easy to find a function for that task!\n\n\n$$\\large\nCost(h_{\\theta}(x),y) =  \\begin{equation}\n    \\begin{cases}\n         -log(h_{\\theta}(x)) \\text{ if } y = 1 \\\\\n         -log(1 - h_{\\theta}(x)) \\text{ if } y = 0\n    \\end{cases}\\,\n\\end{equation}\n$$\n\nWhen y = 1, and true class = 1 then the error is 0, because they are the same (that means that we predicted corrrectly). Otherwise, if sigmoid output tends to zero while our true class is 1, the error tends to plus infinity. Pretty logical, by the way.\n\n\nWhen y = 0, and true class = 0, then the error is 0, because they are the same (that means that we predicted corrrectly). Otherwise, if sigmoid output tends to 1 while our true class is 0, the error tends to plus infinity.\n\nIf you don't get idea, look at this plot. It shows how error would grow!","4374aefc":"# Use model","9aa8176e":"Let's fit our model to the data and then predicts its values for test data!","5cc095b1":"<h1 style='text-align: center'>Logistic Regression From Scratch<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist'> Code on GitHub! <\/a><\/p>","5341f351":"# Implementaion Logistic Regression","51838254":"# Why don't we use linear regression for class separation?\n\n(Pic. 1) On axes $x$ feature, and on axes $y$ label of the class. We can see that line doing pretty good on separating these two classes. For example we could say that if value of y > 0.5, then class is positive.\n\n(Pic. 2) Let's add data, that definetely relates to the positive class. And our line will change dramatically. Now, some points that are definetelly has different classes, are different classes with probability of around 0.5. But it is not true!\n\nThat's happens, since linear model does not have purpose to separate data. Its main purpose is to minimize MSE, and to minimize MSE it needs to minimize sum of margins between data and line. \n\nOne more problem here is that for input with value 100 for example we get output of 2 which is weird since our class has labels of 1 and 0 only.\n\n**We need to fit our output data in range between 0 and 1. And sigmoid function will help us!**","9f9f7a42":"# What is Logistic Regression\n\nLogistic regression uses an equation as the representation, very much like linear regression. Input values $X$ are combined linearly using weights or coefficient values to predict an output value $Y$\n\nA key difference from linear regression is that the output value being modeled is a binary value (0 or 1) rather than a numeric value.\n\nLinear Regression models always map a set of $x$ values to a resulting $y$ value on a continuous range. This means that the $y$ value can be -5, 0.4 or 768. How would we use such a Regression model if our yy value is categorical such as a binary value which is either 0 or 1? Is there a way to define a threshold so that a value such as 768 is assigned to the category 1 while a small value such as -5 gets assigned to the category 0?\n\nThat's where Logistic Regression comes into play. With Logistic Regression we can map any resulting $y$ value, no matter its magnitude to a value between 0 and 1.","099bc0c5":"# Gradient of loss function\n\n$$\\large\nh_{\\theta}(x) = \\frac{1}{1 + \\exp(-x)} \n$$\n\n$\\text{}$\n\n$$\\large\nCost(h_{\\theta}(x),y) = -y \\cdot log(h_{\\theta}(x) - (y-1) \\cdot log(1 - h_{\\theta}(x)) \n$$\n  \nParticial derivative with respect to $w_i$ = $(h_{\\theta}(x_i) - y_i)x_i$  \nParticial derivative with respect to $bias$ = $h_{\\theta}(x_i) - y_i$","5e904674":"<div style='text-align: center'>\n    <img src='https:\/\/i.postimg.cc\/C5VCKqsV\/1-e-Mim-R6-Wx-Lvcct-Zsv-Q8-UPIQ.png' width='500' \/>\n<\/div>","7cabd638":"We will rewrite loss function in something more understandble.\n\n$$\\large\nCost(h_{\\theta}(x),y) = -[y=1]log(h_{\\theta}(x) - [y=0]log(1 - h_{\\theta}(x)) \n$$\n\nWe can't differentiate that function so let's do 1 more transformation:\n\n$$\\large\nCost(h_{\\theta}(x),y) = -y \\cdot log(h_{\\theta}(x) - (y-1) \\cdot log(1 - h_{\\theta}(x)) \n$$\n\nWhy and how does it work? \n* y is always 1 or 0\n* If y = 1, then function will look like $-log(h_{\\theta}(x))$\n* If y = 0, then function will look like $-log(1 - h_{\\theta}(x))$","5d99d846":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align: center'>Part 1 \ud83d\udcd3<\/h1>","77a491c8":"# Sigmoid function\n\nWe are using sigmoid for interpretating probability. We need to tune sigmoid that way so that sigmoid can descrive data very well.\n\nThe Sigmoid Function squishes all its inputs (values on the x-axis) between 0 and 1 as we can see on the y-axis in the graph below.\n\nAssume that class is positive if sigmoid is > 0.5, this happend when x > 0.\nIs sigmoid < 0.5, class is negative and x < 0.\n\nWe can see that as $x$ increases towards positive infinity the output gets closer to 1, and as $x$ decreases towards negative infinity the output gets closer to 0."}}