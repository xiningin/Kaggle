{"cell_type":{"ec8949db":"code","60893ebf":"code","0780e471":"code","acb336e6":"code","4c20c090":"code","70451e61":"code","c854d771":"code","8643292a":"code","089cd1ea":"code","3ff03a23":"code","91c286c0":"code","0555b91e":"code","60821ba0":"code","7ecac5ca":"code","d5dba17f":"code","3df4419a":"code","8eb7c1f7":"code","b13b76f8":"code","962eea41":"code","00f645c1":"code","7ae3b4e3":"code","749b98e5":"code","2dc2e68c":"code","8b078ed3":"code","edab998b":"code","34c7d872":"code","fe86937b":"code","2111e5ae":"code","ced9936f":"code","56664752":"code","788a13be":"code","372b618c":"code","7756761d":"code","dfa72d56":"code","cd69c2ff":"code","84753bea":"code","b608a524":"code","e7f8edd6":"code","06bc3c66":"code","1e720478":"code","d693a9da":"code","9e38cb20":"code","395c70a8":"code","929768b0":"code","ae234415":"code","1e3b8c8e":"code","56bf9fdf":"code","46a005e2":"code","aa059d4a":"code","7b1cadc3":"code","348f8401":"code","0ee341f9":"code","94d61c1b":"code","8d62f2f2":"code","5be45277":"code","085bd5d3":"code","da97fcb3":"code","9ccf540e":"code","bf2cde5a":"code","08a0ee0f":"code","ba7d1535":"code","096aa1cd":"code","b7b53015":"code","5c034549":"code","9b966773":"code","d1931204":"code","c5243966":"code","8686bf2a":"code","5f9a3031":"code","d779fb65":"code","79e7db6a":"code","7fd61954":"code","bb14946b":"code","46115ca1":"code","607aaacb":"code","ee55b3ec":"code","c1926e6f":"code","fa6bb725":"code","9d79a709":"code","891d0c02":"code","80a76026":"code","fcd05bb8":"code","fbcad835":"code","e2826a99":"code","8ccecca5":"code","51c02065":"code","009d4c0a":"code","3d2db4df":"code","1e8aa08e":"code","31607c74":"code","9b930fec":"code","3e75b359":"code","5047a4b5":"code","ff716269":"code","7dd87d26":"code","59861641":"code","68d326b9":"code","5b12dac1":"code","92713f6c":"code","02338825":"code","9c280bed":"code","5806100d":"code","f044d6dc":"code","54fd5cf9":"code","d489718e":"code","1291f080":"code","ff0df88e":"code","4f0c4250":"code","2d02af7a":"markdown","74674e02":"markdown","c978f8ba":"markdown","9d194eb3":"markdown","bbe655cf":"markdown","43a5340e":"markdown","05474c0e":"markdown","14db60f4":"markdown","29cdfae4":"markdown","bdf3f1e5":"markdown","18a0e146":"markdown","b60628ba":"markdown","e683628d":"markdown","14943005":"markdown","0487f5e2":"markdown","1aa6e166":"markdown","5bf75cc2":"markdown","ea671fae":"markdown"},"source":{"ec8949db":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ntrain=pd.read_csv(\"\/kaggle\/input\/bigmart-sales-data\/Train.csv\") \nnew_data=pd.read_csv(\"\/kaggle\/input\/bigmart-sales-data\/Test.csv\")","60893ebf":"train.head()","0780e471":"new_data.head()","acb336e6":"print(train.shape)\nprint(new_data.shape)","4c20c090":"print(train.info())\nprint(new_data.info())","70451e61":"categorical_train=[j for j in train if train[j].dtype == 'object']\ncategorical_new_data =[k for k in new_data if new_data[k].dtype == 'object']","c854d771":"for i in categorical_train:\n    columns = train[i].unique()\n    print(i,columns)","8643292a":"for col in categorical_new_data:\n    columns2 = new_data[col].unique()\n    print(i,columns2)","089cd1ea":"train.insert(loc=9,column='current_year',value=2021)\nnew_data.insert(loc=8,column='current_year',value=2021)","3ff03a23":"train['Outlet_age']=train['current_year']- train['Outlet_Establishment_Year']\nnew_data['Outlet_age']=new_data['current_year']- new_data['Outlet_Establishment_Year']","91c286c0":"train=train.drop(['current_year','Outlet_Establishment_Year'],axis=1)\nnew_data=new_data.drop(['current_year','Outlet_Establishment_Year'],axis=1)","0555b91e":"print(train.isnull().sum())\nprint('-------------------------------------')\nprint(new_data.isnull().sum())","60821ba0":"## replacing the duplicate values in 'Item_Weight'\ntrain['Item_Fat_Content']=train['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = False)\nnew_data['Item_Fat_Content']=new_data['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = False) ","7ecac5ca":"train.head()","d5dba17f":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","3df4419a":"sns.heatmap(new_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","8eb7c1f7":"##by looking at the above graph, we can say that not too much data is missing in the same area. they are evenly distrbuted ","b13b76f8":"## imputing missing values for categorical variable 'Outlet_Size'\nprint(train.Outlet_Size.value_counts())\nprint(new_data.Outlet_Size.value_counts())","962eea41":"## imputing categorical variable with the most repeated\nmode=train['Outlet_Size'].mode().values[0]\ntrain['Outlet_Size']=train['Outlet_Size'].replace(np.nan,mode,inplace=False)\nmode1=new_data['Outlet_Size'].mode().values[0]\nnew_data['Outlet_Size']=new_data['Outlet_Size'].replace(np.nan,mode,inplace=False)","00f645c1":"import seaborn as sns\nimport matplotlib as plt\ncorr=train.iloc[:,1:].corr()\ntop_features=corr.index\nsns.heatmap(train[top_features].corr(),annot=True)","7ae3b4e3":"## checking the correlation after imputation of categorical variable to make sure it is not correlating with any other\n## Also we can clearly see that the only variable correlating high with sales is MRP .","749b98e5":"median_train=train['Item_Weight'].median()\nprint(median_train)\nmedian_new_data=new_data['Item_Weight'].median()\nprint(median_new_data)","2dc2e68c":"def impute_nan(train,variable,median_train):\n    train[variable+\"_median\"]=train[variable].fillna(median_train)\n    train[variable+\"_random\"]=train[variable]\n    random_sample=train[variable].dropna().sample(train[variable].isnull().sum(),random_state=0)\n    random_sample.index=train[train[variable].isnull()].index\n    train.loc[train[variable].isnull(),variable+'_random']=random_sample","8b078ed3":"impute_nan(train,'Item_Weight',median_train)\ntrain.head()","edab998b":"## imputed numerical variable with both median and random variable in  two different columns to compare","34c7d872":"import matplotlib.pyplot as plt\n%matplotlib inline","fe86937b":"fig = plt.figure()\nax = fig.add_subplot(111)\ntrain['Item_Weight'].plot(kind='kde', ax=ax)\ntrain.Item_Weight_median.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","2111e5ae":"## we can observe that there is a deviation in the distribution which leads to outilers","ced9936f":"fig = plt.figure()\nax = fig.add_subplot(111)\ntrain['Item_Weight'].plot(kind='kde', ax=ax)\ntrain.Item_Weight_random.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","56664752":"## clearly, random weight imputation is much closer to item weight distribution","788a13be":"fig = plt.figure()\nax = fig.add_subplot(111)\ntrain['Item_Weight'].plot(kind='kde', ax=ax)\ntrain.Item_Weight_median.plot(kind='kde', ax=ax, color='red')\ntrain.Item_Weight_random.plot(kind='kde', ax=ax, color='green')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","372b618c":"## 'Item_weight' and 'Item_weight_random' are in the same distribution and hence we can drop the imputaion with median","7756761d":"def impute_nan(test,variable,median_new_data):\n    new_data[variable+\"_median\"]=new_data[variable].fillna(median_new_data)\n    new_data[variable+\"_random\"]=new_data[variable]\n    random_sample=test[variable].dropna().sample(test[variable].isnull().sum(),random_state=0)\n    random_sample.index=test[test[variable].isnull()].index\n    new_data.loc[new_data[variable].isnull(),variable+'_random']=random_sample","dfa72d56":"impute_nan(new_data,'Item_Weight',median_new_data)\nnew_data.head()","cd69c2ff":"fig = plt.figure()\nax = fig.add_subplot(111)\nnew_data['Item_Weight'].plot(kind='kde', ax=ax)\nnew_data.Item_Weight_median.plot(kind='kde', ax=ax, color='red')\nnew_data.Item_Weight_random.plot(kind='kde', ax=ax, color='green')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","84753bea":"## it worked the same for testing , dropping 'Item_weight_median'","b608a524":"train=train.drop(['Item_Weight_median','Item_Weight'],axis=1)\nnew_data=new_data.drop(['Item_Weight_median','Item_Weight'],axis=1)","e7f8edd6":"# Initially, we check if the data is normally distributed or left\/right skewed in order to avoid outlier","06bc3c66":"sns.distplot(train['Item_Visibility'])","1e720478":"sns.distplot(train['Item_MRP'])","d693a9da":"sns.distplot(train['Item_Outlet_Sales'])","9e38cb20":"## sales are positively skewed , also shows peakness","395c70a8":"sns.distplot(train['Outlet_age'])","929768b0":"import seaborn as sns\nimport matplotlib as plt\ncorr=train.iloc[:,1:].corr()\ntop_features=corr.index\nsns.heatmap(train[top_features].corr(),annot=True)","ae234415":"## in order to check relation between target variable, picking top correlated features with sales","1e3b8c8e":"sns.regplot(x='Item_MRP',y='Item_Outlet_Sales',data=train)","56bf9fdf":"## as correlation said in the heatmap, as the mrp is increasing, sales are gradually increasing which shows good correlation","46a005e2":"sns.regplot(x='Item_Visibility',y='Item_Outlet_Sales',data=train)","aa059d4a":"##Item_visibility does shows correlation in a negative direction","7b1cadc3":"sns.regplot(x='Item_Weight_random',y='Item_Outlet_Sales',data=train)","348f8401":"## very less correlation","0ee341f9":"#ckecking possibilities to reduce dimensionality","94d61c1b":"## removing unnecessary columns based on subjective knowledge\ntrain=train.drop(['Item_Identifier','Outlet_Identifier'],axis=1)\nnew_data=new_data.drop(['Item_Identifier','Outlet_Identifier'],axis=1)","8d62f2f2":"train['Item_Type'].value_counts()","5be45277":"## Item_type has anyway very less correlation with sales and it has too many categorical variables, we can decrease them into categories which can reduce the dimensionality\ntrain['Item_Type']=train['Item_Type'].replace(['Fruits and Vegetables','Snack Foods','Household','Frozen Foods','Dairy','Canned','Baking Goods','Health and Hygiene','Soft Drinks','Meat','Breads','Hard Drinks','Starchy Foods','Breakfast','Seafood','Others'],['edible','edible','non-edible','edible','edible','edible','edible','non-edible','edible','edible','edible','edible','edible','edible','edible','non-edible'],inplace = False)\nnew_data['Item_Type']=new_data['Item_Type'].replace(['Fruits and Vegetables','Snack Foods','Household','Frozen Foods','Dairy','Canned','Baking Goods','Health and Hygiene','Soft Drinks','Meat','Breads','Hard Drinks','Starchy Foods','Breakfast','Seafood','Others'],['edible','edible','non-edible','edible','edible','edible','edible','non-edible','edible','edible','edible','edible','edible','edible','edible','non-edible'],inplace = False)","085bd5d3":"train['Item_Type'].value_counts()","da97fcb3":"new_data.head()","9ccf540e":"train.head()","bf2cde5a":"X=train[['Item_Fat_Content','Item_Visibility','Item_Type','Item_MRP','Outlet_Size','Outlet_Location_Type','Outlet_Type','Outlet_age','Item_Weight_random']]\ny=train['Item_Outlet_Sales']","08a0ee0f":"categorical_columns = X.describe(include='object').columns.to_list()\ncategorical_columns","ba7d1535":"X= pd.get_dummies(X,categorical_columns)","096aa1cd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","b7b53015":"X.head()","5c034549":"categorical_columns2 = new_data.describe(include='object').columns.to_list()\ncategorical_columns2","9b966773":"new_data= pd.get_dummies(new_data,categorical_columns2)","d1931204":"new_data.head()","c5243966":"from sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X_train,y_train)","8686bf2a":"rank=model.feature_importances_","5f9a3031":"feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(18).plot(kind='barh')\nplt.show()","d779fb65":"#Dropping unnecessary features that have below 0% can be done but in this case, since they are giving 0.002% information , i did not want to loose any minute information as well. \n#therefore no dropping is perfomed","79e7db6a":"X_train.shape","7fd61954":"imp_fea=feat_importances.nlargest(18)\nimp_fea","bb14946b":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()","46115ca1":"X_train=sc.fit_transform(X_train)\nX_train=pd.DataFrame(X_train,columns=X_test.columns)\n\nX_test=sc.transform(X_test)\nX_test=pd.DataFrame(X_test,columns=X_train.columns)","607aaacb":"X_train.head()","ee55b3ec":"new_data=sc.fit_transform(new_data)\nnew_data=pd.DataFrame(new_data,columns=X_test.columns)","c1926e6f":"from sklearn.linear_model import LinearRegression\nmodel1 = LinearRegression(normalize=True)\nmodel1.fit(X_train,y_train)","fa6bb725":"y_pred_train_model1 = model1.predict(X_train)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_train,y_pred_train_model1)\nprint(\"r2 score is :\",R2)","9d79a709":"y_pred_test_model1 = model1.predict(X_test)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_test,y_pred_test_model1)\nprint(\"r2 score is :\",R2)","891d0c02":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_train,y_pred_train_model1))\nprint('MSE:', metrics.mean_squared_error(y_train, y_pred_train_model1))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_model1)))","80a76026":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test,y_pred_test_model1))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_test_model1))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_model1)))","fcd05bb8":"from xgboost import XGBRegressor\nmodel2= XGBRegressor(base_score=1, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.15, max_delta_step=0, max_depth=5,\n             min_child_weight=2, monotone_constraints='()',\n             n_estimators=80, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","fbcad835":"model2.fit(X_train,y_train)","e2826a99":"y_pred_train_model2 = model2.predict(X_train)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_train,y_pred_train_model2)\nprint(\"r2 score is :\",R2)","8ccecca5":"print('MAE:', metrics.mean_absolute_error(y_train,y_pred_train_model2))\nprint('MSE:', metrics.mean_squared_error(y_train, y_pred_train_model2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_model2)))","51c02065":"y_pred_test_model2 = model2.predict(X_test)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_test,y_pred_test_model2)\nprint(\"r2 score is :\",R2)","009d4c0a":"print('MAE:', metrics.mean_absolute_error(y_test,y_pred_test_model2))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_test_model2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_model2)))","3d2db4df":"from sklearn.ensemble import GradientBoostingRegressor\nmodel3= GradientBoostingRegressor()","1e8aa08e":"from scipy.stats import uniform as sp_randFloat\nfrom scipy.stats import randint as sp_randInt","31607c74":"## implementing randomised search cv to get the best parameters","9b930fec":"params = {'learning_rate': sp_randFloat(),'subsample'    : sp_randFloat(),'n_estimators' : sp_randInt(100, 1000),'max_depth'    : sp_randInt(4, 10)}","3e75b359":"from sklearn.model_selection import RandomizedSearchCV\nrandm_search = RandomizedSearchCV(estimator=model3, param_distributions = params,\n                               cv = 2, n_iter = 10, n_jobs=-1)\nrandm_search.fit(X_train, y_train)","5047a4b5":"print(\"Best estimators\",randm_search.best_estimator_)\nprint(\"Best score\",randm_search.best_score_)\nprint(\"Best params\",randm_search.best_params_)","ff716269":"model3=  GradientBoostingRegressor(learning_rate=0.0154291815347819, max_depth=9,\n                          n_estimators=165, subsample=0.11550214721325958)","7dd87d26":"model3.fit(X_train,y_train)","59861641":"y_pred_train_model3 = model3.predict(X_train)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_train,y_pred_train_model3)\nprint(\"r2 score is :\",R2)","68d326b9":"y_pred_test_model3 = model3.predict(X_test)\nfrom sklearn.metrics import r2_score\nR2 = r2_score(y_test,y_pred_test_model3)\nprint(\"r2 score is :\",R2)","5b12dac1":"print('MAE:', metrics.mean_absolute_error(y_train,y_pred_train_model3))\nprint('MSE:', metrics.mean_squared_error(y_train, y_pred_train_model3))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_model3)))","92713f6c":"print('MAE:', metrics.mean_absolute_error(y_test,y_pred_test_model3))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_test_model3))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_model3)))","02338825":"## gradient boost regressor and xgboost regressor are giving almost same root mean saure error but, comparitively , GBR is giving slightly more error than xgboost\n## if we observe r2 values of XGBOOST it is overfitting compared to GB REGRESSOR  \n## one thing which is commonly observed is that testing data is only able to give 58% of r2 which means that only 58% is explainable by independant variable\n## although in XGBOOST 70% is explainable is training , it is overfitting in the testing\n## therefore best model which can predict BIGMART SALES would be GRADIENT BOOST REGRESSOR","9c280bed":"import pickle\noutput=open(\"bigmartsales.pickle\",\"wb\")\npickle.dump(model3,output)","5806100d":"sales_pred=open(\"bigmartsales.pickle\",\"rb\")","f044d6dc":"emp=pickle.load(sales_pred)","54fd5cf9":"pred=model3.predict(new_data)","d489718e":"pred","1291f080":"new_data['Outlet_Sales']=pred","ff0df88e":"new_data.head()","4f0c4250":"## we have predicted sales using GRADIENT BOOST REGRESSOR by loading it into pickle and predicting the new data\n## the new_data 'Outlet_Sales' is now loaded into the new_data file","2d02af7a":"Missing Values","74674e02":"MODEL IMPLEMENTATION****","c978f8ba":"## we have to check if the data is missing completely at random or not\n for data missing completely at random, there should be equal probability of missing values for every variable and  there should not be any relationship with other variables \n*  Here, we have 'Item_weight' and 'Outlet_type' which subjectively says that there might not be dependant on each other\n*  Although it needs domain expert knowledge to make a note why the data is missing , in this case I went with basic subjective knowledge saying that there is no relation\n* There any many imputation methods for no relation missing values such as mean , median, mode, random imputation , KNN etc..\n* I went with imputation methods using statistics since and it worked as good as KNN \n* Compared to KNN , statistical methods are preferably choosable since they do not require more computation and time","9d194eb3":"LINEAR REGRESSION","bbe655cf":"Relation of every column with target variable (Analysing data)","43a5340e":"Importing datasets","05474c0e":"SCALING","14db60f4":"FEATURE SELECTION","29cdfae4":"GradientBoostingRegressor","bdf3f1e5":"LINEAR REGRESSION:\n\ntraining:\n* r2 score is : 0.5584145136909324\n* MAE: 848.1920611339497\n* MSE: 1306232.051944958\n* RMSE: 1142.9050931485772\n \ntesting\n* r2 score is : 0.5809991170997183\n* MAE: 791.1141359649645\n* MSE: 1138831.8576089442\n* RMSE: 1067.1606522023496","18a0e146":"XGBOOST","b60628ba":"*Understanding Data Labels *\n\n* Item Identifier : Unique id for item\n* Item Weight : Weight of the item\n* Item Fat Content : Item fat divided into categories\n* Item Visibility : Product Visibility on the storefront(per sft)\n* Item Type : Type of product divided into categories\n* Item Mrp : Price of the Item\n* Outlet Identifier: Unique id for outlet\n* Outlet Establishment Year: Outlet established year\n* Outlet Size: Size of the outlet divided into categories\n* Outlet Location Type: Outlet location divided into categories\n* Outlet type: Outlet type divided into categories\n* Item Outlet Sales: Sales of every item","e683628d":"XGBOOST:\ntraining: \n* r2 score is : 0.7037219067802329\n* MAE: 666.577049444037\n* MSE: 876405.4835396637\n* RMSE: 936.1653078060859\n\ntesting:\n* r2 score is : 0.5887121486767045\n* MAE: 732.0325818719224\n* MSE: 1117868.068659826\n* RMSE: 1057.2928017629865","14943005":"EXPLORATORY DATA ANALYSIS**** and DATA PRE PROCESSING\n\n* Finding number of categorical columns\n* Creating new columns\n* Duplicate values detection\n* Imputing missing values\n* Relation of every column with target variable (Analysing data)\n* Plotting correlation\n* Checking possibilities to reduce dimensionality\n* Handling categorical variable\n* Feature selection\n* scaling the data","0487f5e2":"FINAL RESULTS FOR TRAINING and TESTING DATA","1aa6e166":"GRADIENT BOOSTING REGRESSOR:\ntraining:\n* r2 score is : 0.6737769589742313\n* MAE: 709.2194716729872\n* MSE: 964984.1434611119\n* RMSE: 982.3360644204772\n\ntesting:\n* r2 score is : 0.6013689251080387\n* MAE: 744.5459877561524\n* MSE: 1083467.3291795994\n* RMSE: 1040.8973672651878","5bf75cc2":"* data is right skewed which shows that it is positively skewed\n* Although the data is skewed and is touching the peak points, outlier removal might lead to loosing of sensitive information\n* since this is a sales data , every information which increases\/decreases the sales is equally important ","ea671fae":"handling categorical variables"}}