{"cell_type":{"25a258e3":"code","122cd3bc":"code","3a335789":"code","647c2f5a":"code","e782b1aa":"code","3c4e299b":"code","db00e901":"code","04047df8":"code","00caeb2a":"code","0287abe6":"code","fc9be832":"code","b7b60219":"code","1e87f52c":"code","dc9f7bf8":"code","5abfdc95":"code","ec622d25":"code","3c58af1f":"code","95d278e1":"code","f0f3dd03":"code","59fa5f40":"code","f504fbdc":"code","5003faa2":"code","401345d3":"code","0049bf8e":"code","f3667afc":"code","1a42f0a3":"code","64ea1944":"code","d217e4e4":"code","c8d07dc0":"code","d0a33104":"code","7bbb44f6":"code","ba514a95":"code","cfdc7ba6":"code","e02b94a8":"code","c28555fd":"code","dc6b6653":"code","e7610124":"markdown","69a5b312":"markdown","9d2be304":"markdown","5006db19":"markdown","90d42136":"markdown","ab8dc313":"markdown","7a05a8ef":"markdown","32d45f4c":"markdown","a8f5d929":"markdown","33ca4301":"markdown","69709a2e":"markdown","31f45feb":"markdown","a2a07e7c":"markdown","37a63e24":"markdown","90ceb5c0":"markdown","ed113431":"markdown","d7f4f7f6":"markdown"},"source":{"25a258e3":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","122cd3bc":"df = pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","3a335789":"df.head()","647c2f5a":"ax = sns.heatmap(df.corr(),annot=True,linewidths=0.3)","e782b1aa":"X = df.drop('Y house price of unit area', axis=1)\ny = df['Y house price of unit area']","3c4e299b":"polynomial_convertor = PolynomialFeatures(degree=2 ,include_bias=False)\npoly_features = polynomial_convertor.fit_transform(X)","db00e901":"print(f'The Number of original features are :{X.shape}')","04047df8":"print(f'The Number of features after polynomial :{poly_features.shape}')","00caeb2a":"X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3)","0287abe6":"from IPython.display import Image\nImage(\"..\/input\/picture\/1.png\")","fc9be832":"scaler = StandardScaler()","b7b60219":"sns.kdeplot(X_train[0]);","1e87f52c":"X_train[0]","dc9f7bf8":"# calculate statistics\nscaler.fit(X_train) \n# scale on data set\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","5abfdc95":"#now our features are scaled\nX_train[0] ","ec622d25":"sns.kdeplot(X_train[0]);","3c58af1f":"ridge_model = Ridge(alpha=100) # hyperparameter(lambda) in sklearn is alpha. \nridge_model.fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)","95d278e1":"MAE = mean_absolute_error(y_test, y_pred)\nMSE = mean_squared_error (y_test, y_pred)\nRMSE = np.sqrt(MSE)","f0f3dd03":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","59fa5f40":"l = ridge_model.coef_\nl","f504fbdc":"l = ridge_model.coef_\nplt.figure()\nplt.bar(np.arange(1, len(l) + 1), height=np.abs(l))\nplt.show()","5003faa2":"from sklearn.linear_model import RidgeCV\n\n#scoring calculate based on our metrics. cv=None is equals to leave_one_out technique\nridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0), scoring='r2') \nridge_cv_model.fit(X_train, y_train)","401345d3":"print(f'the best hyperparameters value is: {ridge_cv_model.alpha_}')","0049bf8e":"y_pred_ridge = ridge_cv_model.predict(X_test)","f3667afc":"#Evaluate again\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred_ridge)\nMSE = mean_squared_error (y_test, y_pred_ridge)\nRMSE = np.sqrt(MSE)","1a42f0a3":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","64ea1944":"ridge_cv_model.coef_ #now coefficients are smaller","d217e4e4":"A = ridge_cv_model.coef_\nplt.figure()\nplt.bar(np.arange(1, len(A) + 1), height=np.abs(A))\nplt.show()","c8d07dc0":"from sklearn.linear_model import LassoCV\nlasso_cv_model = LassoCV(eps = 0.1 , n_alphas=100, cv=5) ","d0a33104":"lasso_cv_model.fit(X_train,y_train)","7bbb44f6":"lasso_cv_model.alpha_","ba514a95":"y_pred_lasso = lasso_cv_model.predict(X_test)","cfdc7ba6":"#Evaluate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred_lasso)\nMSE = mean_squared_error (y_test, y_pred_lasso)\nRMSE = np.sqrt(MSE)","e02b94a8":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","c28555fd":"lasso_cv_model.coef_ #eliminated some features","dc6b6653":"B = lasso_cv_model.coef_\nplt.figure()\nplt.bar(np.arange(1, len(B) + 1), height=np.abs(B))\nplt.show()","e7610124":"# **Regularization**","69a5b312":"### two techniques for performing feature scaling\n* **Standardization**: rescale data to have mean = 0 and standard deviation = 1\n* **Normalization**: all data values to be between 0-1","9d2be304":"**Our features expand to 35 after converting to polynomial**","5006db19":"# **Scaling the data**","90d42136":"# **Three types of regularization**:\n\n* L1 regularization(**LASSO** regularization) : add a penatly equal to **absolut value** of the magnitude of coefficients and limit the size of the coefficients and yield **sparse** model. It is like it chooses the important coefficients and gives zero number to another.\n\n\n* L2 regularization(**Ridge** regularization): does not necessarily **eliminate** coefficients. and it is equal to **square value** of the magnitude of coefficients.\n\n\n* combination of L1 & L2(Elastic Net): we have alpha that decide the ratio between them.(0<alpha<=1)\n\n**Note: we do not do Regularization on bias.**\n","ab8dc313":"# Model Evaluation","7a05a8ef":"## Determine the Features and Labels","32d45f4c":"## Split the data to Train and Test","a8f5d929":"# Why do we need scaling?\nMachine learning algorithms just see numbers \u2014 if there was a vast difference in the range it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.\n\nFeature with a higher value range starts dominating and these numbers start playing a more decisive role while training the model,thus feature scaling is needed to bring every feature in the same range without any upfront importance.\n\nAnother reason why feature scaling is applied is that few algorithms like Neural network gradient descent converge much faster with feature scaling than without it.\n\n# Some examples of algorithms where feature scaling matters are:\n\n* K-nearest neighbors (KNN) with a Euclidean distance measure is sensitive to magnitudes and hence     should be scaled for all features to weigh in equally.\n\n* K-Means uses the Euclidean distance measure here feature scaling matters.\n\n* Principal Component Analysis(PCA). PCA tries to get the features with maximum variance, and the     variance is high for high magnitude features and skews the PCA towards high magnitude features.\n\n* We can speed up gradient descent by scaling\n\n* Tree-based algorithms\n","33ca4301":"# Ridge Regression","69709a2e":"# **Cross Validation(CV):**\nIt is an advanced set of methods that systematically splitting data into train and test set\n\nwe can achieve two goals by this method:\n* Train on all the data \n* Evaluate on all the data\n\nIt has a **high accuracy** but **high complexity**.\nthis is known as K-fold cross-validation(usually k=10) \n\nwe have to be aware of data leakage( we can split data set into **Train|Validation|Test**)","31f45feb":"# **Regularization with sickit learn library**\n\n**I used \"[real estate price prediction](https:\/\/www.kaggle.com\/quantbruce\/real-estate-price-prediction)\" as data set.**","a2a07e7c":"# **Why do we use Regularization?**\n*  it is use to **minimize complexity** to overcome **overfitting**(add more bias to **reduce model variance**, this happens because of bias-variance trade off)\n*  penalizing the loss function (it uses a kind of hyperparameter like **lamda** to balance the formula between Regularization & loss function)","37a63e24":"## Import Libraries","90ceb5c0":"## Data overview","ed113431":"## Preprocessing\n**with polynomial features**","d7f4f7f6":"**The above graph shows that Lasso remove some features**"}}