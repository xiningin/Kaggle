{"cell_type":{"9325b293":"code","cd074b1a":"code","c94787ec":"code","affdc7c3":"code","56216609":"code","564eeda4":"code","1e7a9c8a":"code","e6e8ef88":"code","525633e0":"code","55e59335":"code","43c30f3c":"code","7c53967b":"code","b0b6105f":"code","d9116f2d":"code","5ce09191":"code","e7da695f":"code","5a350c58":"code","15b86215":"code","5538424a":"code","61a7882d":"code","ff8fda9b":"code","511c810f":"code","7b85d048":"markdown","781733fd":"markdown","949d620d":"markdown","8c725c8d":"markdown","04b71500":"markdown","44fc0c37":"markdown","14c50af5":"markdown","48220a51":"markdown","a31673f8":"markdown","d4d12372":"markdown","6cf69cfa":"markdown","fc7337e7":"markdown","47644a61":"markdown","24d3903c":"markdown","cd262510":"markdown","6c8f2530":"markdown","08884e13":"markdown","5d926298":"markdown"},"source":{"9325b293":"import pandas as pd\nimport numpy as np\nimport json\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport math\nfrom PIL import Image","cd074b1a":"\noriginal = pd.read_csv(\"\/kaggle\/input\/youtube-new\/USvideos.csv\")\nnew = original.copy()\nwith open(\"\/kaggle\/input\/youtube-new\/US_category_id.json\",\"r\") as category:\n    category = json.load(category)\n### Extract the category information from the JSON file\nvid_cat = []\ncat_id = []\nfor i in category['items']:\n    vid_cat.append(i['snippet']['title'])\n    cat_id.append(int(i['id']))\n    \n### Mapping the category_id\nnew.category_id = original.category_id.map(dict(zip(cat_id,vid_cat)))\nnew.category_id.isnull().sum() # we have no nan values.\n\n### Prepare date type columns\nnew['trending_date'] = pd.to_datetime(new['trending_date'], format='%y.%d.%m')\nnew['publish_time'] = pd.to_datetime(new['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n\n### Add column for publish time\nnew['publish_date'] = new['publish_time'].dt.date\nnew['publish_wd'] = new['publish_time'].dt.weekday\nnew['publish_hr'] = new['publish_time'].dt.hour\nnew['publish_time'] = new['publish_time'].dt.time\n\nnew.head()\n#,'comments_disabled','ratings_disabled'\n#For the purpose of this analysis, some columns are irrelevant, we should remove them\nnew = new.drop(['tags','video_error_or_removed','description'],axis = 1)\n# Remove duplicates in the data\nnew = new.drop_duplicates(keep = 'first')","c94787ec":"new.info() # We do not have any nan.","affdc7c3":"df = new[['category_id','views']].groupby('category_id').aggregate(np.sum).reset_index().sort_values(by ='views', ascending = False)\ndf.views = df.views\/10**6\nplt.figure(figsize = (20,10))\nview_box = sns.barplot(y = 'category_id', x = 'views', data = df, orient = 'h')\nplt.title('Barplot of number of views in each category (Unit: milliions)')\nplt.ylabel('Category')\nplt.xlabel('Views')\n#view_box.set_xticklabels(view_box.get_xticklabels(), rotation=45, horizontalalignment='right')","56216609":"print(new[['views','likes']].corr())\nprint(new[['views','dislikes']].corr())\n","564eeda4":"data_bar = new['publish_wd'].map(dict(zip(range(7),\n        ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday']))).value_counts()\n# Use textposition='auto' for direct text\nfig = go.Figure(data=[go.Bar(\n            x=data_bar.index.values, y=data_bar,\n            textposition='auto',\n        )])\nfig.update_layout(title = \"Number of Videos Published in Each Weekday\",yaxis=dict(\n            title='Videos'))\nfig.show()","1e7a9c8a":"lastdate = max(new.publish_date)-dt.timedelta(days = 7)\nprint(lastdate)","e6e8ef88":"# Load data, define hover text and bubble size, only look at videos with 10M views or above.\ndata = new[['title','channel_title','category_id',\n            'views','publish_wd','publish_hr','likes','dislikes','publish_date']].loc[new.views > 10**6].reset_index()\ndata.publish_wd = data.publish_wd.map(dict(zip(range(7),\n        ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday'])))\ndef bubble_plot(target, plot_title, target_title, data):\n    hover_text = []\n    bubble_size = []\n    for index, row in data.iterrows():\n        hover_text.append(('Title: {title}<br>'+\n                      'Channel: {channel_title}<br>'+\n                      'Category: {category_id}<br>'+\n                      'Views: {views}<br>'+\n                      'Likes: {likes} <br>'+\n                       'Dislikes: {dislikes}<br>'\n                      ).format(title=row['title'],\n                                            channel_title=row['channel_title'],\n                                            category_id=row['category_id'],\n                                            views = row['views'],\n                                            likes = row['likes'],\n                                            dislikes = row['dislikes']))\n        bubble_size.append(row[target]\/row['views'])\n    data['text'] = hover_text\n    data['size'] = bubble_size\n    fig = go.Figure()\n    # Dictionary with dataframes for each weekday\n    weekday = ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday','Saturday','Sunday']\n    wd_data = {wd:data.query(\"publish_wd == '%s'\" %wd)\n                              for wd in weekday}\n    # Create figure\n\n    for key, values in wd_data.items():\n        fig.add_trace(go.Scatter(\n            x=values['views'], y=values[target]\/values['views'],\n            name=key, text=values['text'],\n            marker_size=values['size'],\n            ))\n    # The following formula is recommended by https:\/\/plotly.com\/python\/bubble-charts\/\n    sizeref = 2.*max(data['size'])\/(1000)\n\n\n    # Tune marker appearance and layout\n    fig.update_traces(mode='markers', marker=dict(sizemode='area',\n                                              sizeref=sizeref, line_width=2))\n    fig.update_layout(\n        title=plot_title,\n        xaxis=dict(\n            title='Number of views in millions',\n            gridcolor='white',\n            type='log',\n            gridwidth=2,\n        ),\n        yaxis=dict(\n            title=target_title,\n            gridcolor='white',\n            gridwidth=2,\n        ),\n        paper_bgcolor='rgb(243, 243, 243)',\n        plot_bgcolor='rgb(243, 243, 243)',\n        legend= {'itemsizing': 'constant'}\n    )\n    \n    fig.show()\npd.options.mode.chained_assignment = None \nbubble_plot('likes', \"Like\/View Ratio vs. Number of Views\", \"Like\/View Ratio\", data.loc[data.publish_date >= lastdate])","525633e0":"data.loc[data.publish_date >= lastdate,'publish_wd'].value_counts()","55e59335":"bubble_plot('dislikes', \"Dislike\/View Ratio vs. Number of Views\", \"Dislike\/View Ratio\",data)","43c30f3c":"# Create a dataframe for modeling\nlearn_data = new.loc[(new.comments_disabled) &\n                   (~new.ratings_disabled)].copy()\n# Create a column for number of days a video takes to get on the trending list\nlearn_data['day_to_trend'] = abs(np.subtract(learn_data.trending_date.dt.date,learn_data.publish_date,dtype=np.float32).apply(lambda x: x.days))\nrel_vars = ['views','likes','dislikes','comment_count','publish_wd', 'publish_hr', 'day_to_trend','title']\nlearn_data = learn_data[rel_vars]\nlearn_data.reset_index(inplace=True)\nlearn_data.head()","7c53967b":"from pandas.plotting import scatter_matrix\nscatter_matrix(learn_data[['publish_wd', 'publish_hr', 'day_to_trend']])\nplt.show()\nplt.hist(learn_data['day_to_trend'])\nplt.title(\"Histogram of Original Days to Trend \")\nplt.show()\n\nlearn_data = learn_data.loc[learn_data.day_to_trend <= 14]\nplt.hist(learn_data['day_to_trend'])\nplt.title(\"Histogram of Days to Trend After Removing Values > 7\")\nplt.show()","b0b6105f":"from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nlearn_data.day_to_trend = learn_data.day_to_trend <=7","d9116f2d":"def rfr_model(X, y, my_param_grid = None ):\n# Perform Grid-Search\n    if my_param_grid is None:\n        #the followings are hyperparameter to optimize: max depth of a tree and number of trees in the forest\n        my_param_grid = {\n                'max_depth': range(6,10),\n                'n_estimators': range(155,170),\n                }\n    gsc = GridSearchCV(\n        estimator=RandomForestClassifier(),    \n        param_grid= my_param_grid,\n        cv=5, scoring='accuracy', verbose=0, n_jobs=-1)\n    \n    grid_result = gsc.fit(X, y)\n\n    return grid_result.best_params_,grid_result.best_score_","5ce09191":"X = learn_data[['views','likes','dislikes','publish_wd', 'publish_hr']]\ny = learn_data['day_to_trend']\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=4, test_size = .3)","e7da695f":"print(rfr_model(X_train,y_train)) # ({'max_depth': 9, 'n_estimators': 160}, 0.889456740442656)","5a350c58":"from sklearn.metrics import classification_report\nmy_forest = RandomForestClassifier(max_depth = 9,n_estimators = 160,oob_score = True,warm_start = True )\nmy_forest.fit(X_train,y_train)\nprint(my_forest.oob_score_)# 0.8696883852691218\nprint(my_forest.score( X_test, y_test))# 0.9276315789473685\nprint(my_forest.feature_importances_)\n#print(pd.crosstab(pd.Series(y_train, name='Actual'), pd.Series(my_forest.predict(X_train), name='Predicted')))\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(my_forest.predict(X_test), name='Predicted')))\npred = my_forest.predict(X_test)\nprint(classification_report(y_test, pred))\n","15b86215":"import scikitplot as skplt\nfrom sklearn.metrics import average_precision_score, plot_precision_recall_curve\nprob = my_forest.predict_proba(X_test)\nmyplot = skplt.metrics.plot_roc(y_test, prob)\naverage_precision = average_precision_score(y_test, prob[:,1]) # prob[:,1] is the estimated probability of positive outcome\ndisp = plot_precision_recall_curve(my_forest, X_test,y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))\nscore = metrics.f1_score(np.array(y_test),pred)\nprint('The f1 score for this model is {}'.format(score))","5538424a":"from xgboost import XGBClassifier\nparameters = [{'n_estimators': range(100,150,1)},\n              {'learning_rate': np.arange(0.01,1.0, 0.01)}]\ngbm=XGBClassifier(max_features='sqrt', subsample=0.8, random_state=10)\ngrid_search = GridSearchCV(estimator = gbm, param_grid = parameters, scoring='accuracy', cv = 4, n_jobs=-1)\ngrid_search = grid_search.fit(X_train, y_train)\n#grid_search.cv_results_\n#grid_search.best_params_, grid_search.best_score_\ngrid_search.best_estimator_","61a7882d":"gbm = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.24, max_delta_step=0, max_depth=6,\n              max_features='sqrt', min_child_weight=1, missing=None,\n              monotone_constraints=None, n_estimators=100, n_jobs=0,\n              num_parallel_tree=1, objective='binary:logistic', random_state=10,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n              tree_method=None, validate_parameters=False, verbosity=None)\ngbm.fit(X_train,y_train)\ny_pred = gbm.predict(X_test)\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\nprint(classification_report(y_test, y_pred))","ff8fda9b":"prob = gbm.predict_proba(X_test)\nmyplot = skplt.metrics.plot_roc(y_test, prob)\naverage_precision = average_precision_score(y_test, prob[:,1])\ndisp = plot_precision_recall_curve(gbm, X_test,y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","511c810f":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ntitle = learn_data.loc[learn_data.day_to_trend <= 7].title.copy()\ntitle = \" \".join(x for x in title)\nstopwords = set(STOPWORDS)\nstopwords.update([\"Official\", \"Trailer\"])\n\nmask = np.array(Image.open(\"\/kaggle\/input\/logoyou\/logo.jpg\"))\nwordcloud_usa = WordCloud(stopwords=stopwords, background_color=\"white\", mode=\"RGBA\", max_words=500, mask=mask).generate(title)\nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[10,14])\nplt.imshow(wordcloud_usa.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\nwordcloud = WordCloud(max_words=100, background_color=\"white\", stopwords = stopwords, colormap = 'Reds').generate(title)\nplt.figure(figsize=[10,14])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","7b85d048":"### What we discovered: \n1. The correlation between the view count and like count is 0.85, very high, which confirms our thoughts. If your video can attract a lot of viewers (high view count), it is very likely has a good content (high like count).\n2. The correlation between the view count and dislike count is 0.47, implying that the dislike count would vary together with the view count, too. In other words, popularity does not equate high content quality\/positive viewer reaction. We will look more into this in the following part.","781733fd":"# **Introduction**\nYoutube videos are not only produced by vbloger or their other name, \"Youtubers\". Media corporations including Disney, CNN, BBC, and Hulu also offer some of their material via YouTube as part of the YouTube partnership program. \n\nIf your company, or yourself, a potential million-view youtuber, intend to employ this huge platform to publish your video, it is essential to enhance the content quality, and to increase its visibility. But why Youtube? Because it offers the possibility to monetize your videos, by adding ads during the video progression. With an in-depth analysis of thousands of videos, we could find several keys to increase views, likes, and the most important of all, your incomes.\n\nThe data used in this report can be found at:\nhttps:\/\/www.kaggle.com\/datasnaek\/youtube-new\/ \n\nThe website says that it was last updated on May, 2019; however the latest publish date in the data in 2018\/06\/14","949d620d":"### What we discovered:\nUnlike the Like\/View ratio, which decreases as the number of view increases, the Dislike\/View remains almost the same regardless of the change in views. If your video does not receive favorable reviews in the first couple of days, it may very likely remain so, even though your views increases eventually as time goes. ***If you work in a marketing team and are choosing a channel to carry out your plan with, closely observing a youtuber's newly published videos reviews after their first week is already enough to make your decision. If you are a youtuber, do not experiment new\/unsure content, as \"bad\" videos will likely just stay \"bad\": take it down if you receive too many dislikes in the first three days.  ***","8c725c8d":"## First, let's look at how many views is associated with each category. \nThis number is important, since it tells us about popularity of a video. How can we utilize this piece of information?\nIf you are a Youtuber, it is quite straight forward: the more views, which usually implies the more popular your channel is, the more money you can make from ads. If you are in a marketing team, knowing what type of video people watch the most would help your advertising plan be more effective. Politicians, producers, media companies are other examples who can deploy this information. It is about how to spread out what you want to convey as quick as possible.  ","04b71500":"### Comments:\nWith the Random Forest algorithm, we obtained the parameter estimates that can predict whether or not a video can be on trend within a week with the **90.05% accuracy for the training dataset and 88.5% for the testing dataset**. Since Random Forest algorithm uses a stochastic process to yield a model, what we obtain each time from fitting it to the data will be different. The feature importances indicate that whether or not the comment\/rating section is available does not seem to affect the chance of getting on the trend within one week. It also reveals that the most three important factors are number of views, likes, and dislikes. We will try fitting the model again without these two variables.\n\nAlso, since we are interested in both true positive and true negative guesses, and since we have a some imbalance between the two classes (whether a video gets on the trend within one week), we first use ROC curve to check the performance on both of the two classes. The ROC-AUC of both classes is about 91%.\n\nLet's say, we only want to focus on how good we predict the positive class (or when the positive case is rare in the data). The Precision-Recall Curve should be used instead. The PR-AUC is 96%, meaning that the model seems to predict very well for the positive class. Another way to look at this is using f1-score whose formula is:\n$$f1 = 2\\frac{\\textrm{Precision}\\times \\textrm{Recall}}{\\textrm{Precision}+ \\textrm{Recall}}$$\n\nThis score gives a balance between the precision and recall values. Using this score avoids misleading information from either precision or recall values in certain cases (e.g. data imbalance). Our model f1 is about .94","44fc0c37":"From the histograms of the numerical variables in our dataset, we can see that none of them follow Gaussian distribution. The number of views seem to follow exponential\/gamma distribution, and the target variable seems to only cluster at two locations. **This suggest that we may need to discard the few observations**, and let's narrow down to videos that become trending within two weeks.\n\nSecondly, the scatter plots indicate that **an OLS linear regression will not be a sufficient model**. This is the reason why we should try using more complex learning algorithms like random forest and Xgboost.","14c50af5":"# **Data Exploration**","48220a51":"### Comments: \nIt is reasonable to assume that videos posted on the weekend, when people are off from work and can spend some time on Youtube, will get more views within the first 24 hours than those that are posted on a weekday. However, the histogram shows that most Youtubers do not follow this logic. Most of the videos were published on the weekdays, and surprisingly, not too many videos were publish on Saturday and Sunday. We will evaluate this with the following bubble charts. We will look at the videos that have got than one million view within a week (2018-06-07 or after)","a31673f8":"## Relationship between number of Likes and Views\nThe simplest way is to look at their correlation. View count is very important, how about number of likes? Being able to make someone willing to double-tap on that thumbs-up button may be more crucial than just getting as many views as possible. However, logically, these numbers should vary together. Although, the number of dislikes is in question. Would the dislike count vary together with number of views too? Or if a video is popular, it gets less dislikes?","d4d12372":"### What we discovered:\n1. ***Despite that we can assume both number of views and likes can tell us about how good a video is, the ratio between them may not.*** \nWe showed earlier the correlation between like count and view count is highly positive, meaning they grow together, but this graph reveals that the view count grows much faster than the like count. The bubbles should stay roughly horizonal if they in fact grow with the same speed. Hence, we should not use this ratio to evaluate the content quality of a video.\n2. Double-click on each weekday to observe the impact of publish day on the number of views. Most if not all videos that have more than 8 million views were published on Friday or Monday, and those that were published on Tuesday, cound not reach 8 million views in the latest version of this dataset. If we lower our bar to one million views, it appears that most videos were published on weekends. ***Therefore, half of the Youtubers did the right thing to publish their products on the three \"hot days\", while other half did not have a very great choice. However, this impact is only obvious when the number of views passes 8 million.***","6cf69cfa":"# Predict the number of days to make your video trending\n\n* ***Idea:***  As explained above, making your videos appear on the trending page is very important. If a video takes too long to become trending, there are some factors we should look into. \n* ***Target:*** number of days to make a video trending.\n* ***Predictors:*** publish day (weekday), publish hour(0-24), views, likes, dislikes, and comments.\n* ***Notes:*** It should be noticed here that the number of views, likes, dislikes, and comments are not the values collected after a fixed amount of time since publish date (e.g. 3 days, a week,...). Therefore, they do not have the predicting power we wish. However, we would still proceed the predicting task with our dataset as an experiment.\n* ***Model:*** the models we will examine are Random Forest and Xgboost\n","fc7337e7":"### Comments:\nIt seems that **the XGboost using random forest performs very similarly to the random forest model**. However, it took a longer time to train the original random forest model. **Again, our models are not capable of giving legitimate predictions, because we assumed that the number of views, dislikes, and likes are the values collected after a certain timeframe, which is not true.** When the appropriate data become available, we can be more confident about our results. Also, features like thumbnail picture are more likely to be the good ones in predicting days to trend target. However, this would be a big project by itself and needs contributions from more people who have experience with computer vision\/image processing.","47644a61":"## Random Forest Algorithm\n\n### We will use Random Forest Classifier to predict if the trending day is less than a week","24d3903c":"## Publish Date\n\nWhat day of the week should I publish my video, you might ask? Does it really matter? Isn't it if my video has really good content, the number of views will eventually increase? It is not quite that simple. How important is it to choose a right day, at the right time to post your video? Time is the key point. If your video can get a strong burst of views in the first three days after it comes out, and if its content is excellent, it will get onto the trending list faster. Once your video appears on the trending list, more people will see and click on it, meaning more views, longer time remaining on the trending list and harder for other videos to beat yours.","cd262510":"## XGboost Algorithm\n\n### We will use XGboost Classifier to predict if the trending day is less than a week","6c8f2530":"# Description\n\nThe dataset includes data gathered from **40949 videos** on YouTube that are contained within the trending category each day.\n\nThere are two kinds of data files, one includes comments (JSON) and one includes video statistics (CSV). They are linked by the unique video_id field.\n\n#### The columns in the video file are:\n\n* title\n* channel_title\n* video_id(Unique id of each video)\n* trending_date \n* title\n* channel_title\n* category_id (Can be looked up using the included JSON file)\n* publish_time\n* tags (Separated by | character, [none] is displayed if there are no tags)\n* views\n* likes\n* dislikes\n* comment_count\n* thumbnail_link\n* comments_disabled\n* ratings_disabled\n* video_error_or_removed\n* description\n\n# Data Preparation","08884e13":"### What we discovered: \nMusic, entertainment, film, and comedy are what Americans watch the most, which may not be remarkably surprising, but having a look at real data is apparently much better than a mere guess.","5d926298":"### Let's first take a look at the distribution of our data"}}