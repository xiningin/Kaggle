{"cell_type":{"05dab334":"code","48f4d3a3":"code","10b4b607":"code","95f0f881":"code","5e504e85":"code","91885dd7":"code","341d7fc5":"code","a6da0be0":"code","a23a499e":"code","6b7aef69":"code","7a0bd3e2":"code","5a8f5dd0":"code","4960f50f":"code","8be24d08":"code","13b36e75":"code","77fe029d":"code","49cf4ed9":"code","7273b8af":"code","eea27dd4":"code","0c9a279d":"code","f53e354b":"code","122c4285":"code","ea9840a7":"code","4b3e5548":"code","40650a60":"code","229505cc":"code","b95043c7":"code","b1c17021":"code","2577a475":"code","869ab7c5":"code","5c89d8d2":"code","a6970688":"markdown","a4b51b2b":"markdown","78e74f0f":"markdown","0bcc4709":"markdown","077219d7":"markdown","308d9d7f":"markdown","222fc281":"markdown","81f3a76e":"markdown","fb087744":"markdown","64804766":"markdown","c848ecce":"markdown","68cf6392":"markdown","87891dac":"markdown","b573e54a":"markdown","46292845":"markdown","5ef0c9a5":"markdown","66e6b2d3":"markdown","33a61e48":"markdown","10460e12":"markdown","e0c33855":"markdown","778b2cee":"markdown","6c2112cc":"markdown","a2a97f74":"markdown","9ca2e651":"markdown","696a6e42":"markdown","b8820d8d":"markdown","a179533d":"markdown","b2d2e614":"markdown","c4c7502e":"markdown","d6466f38":"markdown","44289950":"markdown","21542940":"markdown","89495342":"markdown","b2f41e95":"markdown","247ffcb0":"markdown","7661b7e6":"markdown","1c75d1b9":"markdown","fc78e87f":"markdown","4f2d129d":"markdown","ee52b654":"markdown","3b5f1d7b":"markdown"},"source":{"05dab334":"#Import Libraries\nimport numpy as np #linear algebra\nimport pandas as pd #data processing\nimport matplotlib.pyplot as plt #data viz\nimport seaborn as sns #data viz\nfrom sklearn.impute import SimpleImputer #imputes missing vals\nfrom datetime import datetime \nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder #preprocessing\nfrom sklearn.compose import ColumnTransformer #preprocessing\nfrom sklearn.decomposition import PCA #dimensionality reduction\nfrom sklearn.cluster import DBSCAN #clustering\nfrom sklearn.model_selection import train_test_split, GridSearchCV #data split, grid search\nfrom imblearn.over_sampling import SMOTE #balance classes\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC #support vector machine\nfrom sklearn.neighbors import KNeighborsClassifier #knn\nfrom sklearn.naive_bayes import GaussianNB #bayes\nfrom xgboost import XGBClassifier #gradient boosting tree\nfrom sklearn.metrics import accuracy_score, recall_score #calculates accuracy, recall\nfrom sklearn.ensemble import VotingClassifier#ensemble","48f4d3a3":"#Read In Dataset\npd.set_option('display.max_columns', None)\ncustomer_data = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',\n                           delimiter='\\t', index_col='ID')\ncustomer_data.head()","10b4b607":"#Feature Engineering\n#Create 'Age' feature from customer's birth year\ncustomer_data['Age'] = customer_data.Year_Birth.apply(lambda x: 2021 - int(x))\n\n#Create 'Days_Since_Customer' feature from time the customer enrolled\ncustomer_data['Dt_Customer'] = pd.to_datetime(customer_data.Dt_Customer)\nnow = datetime.now()\ncustomer_data['Days_Since_Customer'] = customer_data.Dt_Customer.apply(lambda x: (now - x).total_seconds()\/ (60 * 60 * 24))\n\n#Create 'Fam_Size' feature from the marriage status, number of kids\/teens\nmarital_map = {'Absurd': 1, 'Alone': 1, 'YOLO': 1, 'Single': 1,\n              'Married': 2, 'Together': 2, 'Widow': 1, 'Divorced': 1}\ncustomer_data['Marital_Status'] = customer_data.Marital_Status.map(marital_map) #Maps all singles as 1, couples as 2\ncustomer_data['Num_Kids'] = customer_data.Kidhome.values + customer_data.Teenhome.values\ncustomer_data['Fam_Size'] = customer_data.Marital_Status.values + customer_data.Num_Kids.values\n\n#Create 'Num_Accepted' feature from the sum of previous marketting campaigns that were accepted by the customer\ncustomer_data['Num_Accepted'] = customer_data.AcceptedCmp1.values + customer_data.AcceptedCmp2.values + \\\n                                customer_data.AcceptedCmp3.values + customer_data.AcceptedCmp4.values + \\\n                                customer_data.AcceptedCmp5.values\n\n#Create 'MntTotal' for total amount spent on all items\ncustomer_data['MntTotal'] = customer_data['MntWines'].values + customer_data['MntFruits'].values + \\\n                            customer_data['MntMeatProducts'].values + customer_data['MntFishProducts'].values + \\\n                            customer_data['MntWines'].values + customer_data['MntSweetProducts'].values + \\\n                            customer_data['MntGoldProds'].values\n\n#Drops the unnecessary features from the original dataset\ncustomer_data.drop(['Dt_Customer', 'Year_Birth', 'AcceptedCmp1', 'AcceptedCmp2',\n                    'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Kidhome', 'Teenhome',\n                   'Z_CostContact', 'Z_Revenue', 'Num_Kids', 'Marital_Status'],\n                   axis=1, inplace=True)\ncustomer_data.head() ","95f0f881":"#Prints the size of the dataset\nprint('Dataset Shape:', customer_data.shape)\nprint('-------------------------------')\n#Check if any columns contain null\nprint('Total Nulls Per Column:')\nprint(customer_data.isnull().sum())","5e504e85":"#Imputes the mean\nimputer = SimpleImputer(strategy='mean')\nimputer.fit(customer_data.Income.values.reshape(-1,1))\ncustomer_data['Income'] = imputer.transform(customer_data.Income.values.reshape(-1,1))","91885dd7":"#Calculate percent of responses\npos_resp = customer_data.Response.sum()\ntotal = customer_data.shape[0]\npercent = round((pos_resp \/ total)*100, 2)\n\nprint(pos_resp, 'customers responded to the marketing campaign out of a total of', total, 'cutomers.')\nprint('Percent Responded: ' +  str(percent) + '%')","341d7fc5":"#View feature correlations with the 'Response' column\n#Note: 'Response' will be the target for predictive modeling\nresponse_corr_abs = np.abs(customer_data.corr()['Response']).sort_values(ascending=False)[1:]\nresponse_corr = customer_data.corr()['Response'].sort_values(ascending=False)[1:]\nprint(\"Correlation Coefficients for 'Response'\")\nprint('--------------------------------------------------------')\nprint(response_corr)","a6da0be0":"#Creates function to display kde graphs\ndef display_kdeplot(df, col1, col2):\n    plt.figure(figsize=(8,8))\n    sns.kdeplot(data=df, x=col1, hue=col2, multiple=\"stack\")\n    plt.title(col1 + ' correlation with ' + col2)\n    plt.legend(['Response', 'No Response'])\n    plt.show()\n    print(df[col1].describe())","a23a499e":"for i in range(5):\n    feature_name = response_corr_abs.index[i]\n    display_kdeplot(customer_data, feature_name, 'Response')\n    print('Correlation %:', round(response_corr[feature_name] * 100, 2))\n    print('------------------------------------------------------------------------')","6b7aef69":"#plots the correlation matrix\nplt.figure(figsize=(20,20))\ncmap = sns.color_palette(\"rocket\", as_cmap=True)\nsns.heatmap(customer_data.corr(), annot=True, cmap=cmap, center=0)","7a0bd3e2":"#create function for plotting scatter plot\ndef display_scatterplot(df, col1, col2):\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(data=df, x=col1, y=col2, hue='Response')\n    plt.title(col1 + ' correlation with ' + col2)\n    plt.legend(['Response', 'No Response'])\n    plt.show()","5a8f5dd0":"#Plots 5 features that have a med-high correlation selected from the above correlation matrix\ncorr_features = [['Income', 'MntTotal'], ['Income', 'NumWebVisitsMonth'], ['MntWines', 'MntMeatProducts'],\n['NumStorePurchases', 'MntWines'], ['NumCatalogPurchases', 'MntMeatProducts']]\ncorr_mx = customer_data.corr()\n\nfor corr in corr_features:\n    display_scatterplot(customer_data, corr[0], corr[1])\n    print('Correlation Coefficient %:', round((corr_mx.loc[corr[0], corr[1]]), 2))\n    print('-----------------------------------------------------------')","4960f50f":"#Remove the 'Response' column because it is the target of future predictive model\nX, y = customer_data.drop('Response', axis=1).values, customer_data['Response'].values\n\n#Creates a column transformer that sends 'Education' to be encoded and rest scaled\nct = ColumnTransformer([\n    ('catagoric', OneHotEncoder(), [0]),\n    ('numeric', StandardScaler(), list(range(1, len(X.T))))\n])\n\n#Sends the data through the column transformer\nX_transformed = ct.fit_transform(X)\nprint('Preprocessed Data:')\nprint(X_transformed[0])","8be24d08":"#Create instance of Principal Component Analysis in order to reduce dimensionality while maintaining variance\n#n_components=3 will be chosen in order to visualize the data better\npca = PCA(n_components=3)\n\n#fit to dataset\npca.fit(X_transformed)\n\n#create dimentionality reduced dataset\nX_reduced = pca.transform(X_transformed)\n\nprint('Dimentionality Reduced Data:')\nprint(X_reduced[0])","13b36e75":"#plot the 3d dataset\nfig = plt.figure(figsize=(20,20))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(X_reduced.T[1],X_reduced.T[2],X_reduced.T[0], c=\"blue\")\nax.set_title(\"Customer Data in 3 Dimensions\")\nplt.show()","77fe029d":"#plot the 3d dataset\nfig = plt.figure(figsize=(20,20))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(X_reduced.T[1],X_reduced.T[2],X_reduced.T[0], c=y)\nax.set_title(\"Customer Data in 3 Dimensions with Response\")\nax.legend(['Response'])\nplt.show()","49cf4ed9":"#Create an instance of DBSCAN to create non spherical clusters based on data density\ndb = DBSCAN(eps=0.726, min_samples=26)\n\n#fit to the dimentionality reduced dataset\ndb.fit(X_reduced)\n\n#identify the clusters\nclusters = db.labels_\n\n#display metrics\/sample\nn_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\nn_noise_ = list(clusters).count(-1)\n\nprint('Cluster Predictions')\nprint('-------------------------------')\nprint(\"Number of clusters: %d\" % n_clusters_)\nprint(\"Number of noise points: %d\" % n_noise_)\nprint('Number of points per cluster:')\nfor i in range(n_clusters_):\n    print('Cluster', i, ':', len(clusters[clusters==i]))","7273b8af":"#plot the 3d dataset\nfig = plt.figure(figsize=(20,20))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(X_reduced.T[1],X_reduced.T[2],X_reduced.T[0], \n           c=clusters)\nax.set_title(\"Customer Clusters in 3 Dimensions\")\nax.legend(['Outliers'])\nplt.show()","eea27dd4":"#append the clusters to the original dataset\ncustomer_data['Cluster'] = clusters\n\n#group by cluster and calculate how many responses there were per cluster\ncluster_grp = customer_data.groupby('Cluster').Response.agg(['sum', 'count'])\ncluster_grp['percent_resp'] = cluster_grp['sum'].values \/ cluster_grp['count'].values\ncluster_grp","0c9a279d":"#create new dataframes for each cluster\noutlier = customer_data[customer_data.Cluster == -1]\nclus0 = customer_data[customer_data.Cluster == 0]\nclus1 = customer_data[customer_data.Cluster == 1]\nclus2 = customer_data[customer_data.Cluster == 2]","f53e354b":"#Create function for plotting distributions per cluster\ndef create_kdeplot(col, title):\n    plt.figure(figsize=(12,8))\n    sns.kdeplot(data=outlier, x=col, label='Outliers')\n    sns.kdeplot(data=clus0, x=col, label ='Cluster 0')\n    sns.kdeplot(data=clus1, x=col, label ='Cluster 1')\n    sns.kdeplot(data=clus2, x=col, label ='Cluster 2')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n    \n#Create function for plotting count of discrete values per cluster\ndef create_barplot(col, title):\n    plt.figure(figsize=(8,8))\n    sns.countplot(x=customer_data[col], hue=customer_data[\"Cluster\"])\n    plt.title(title)\n    plt.legend(['Outliers', 'Cluster 0', 'Cluster 1', 'Cluster 2'])\n    plt.show()","122c4285":"#Create list of variables to plot distributions\nvars_to_kdeplot = [('MntTotal', 'Total Spending per Cluster'),\n                ('Fam_Size', 'Family Size per Cluster'),\n                ('NumCatalogPurchases', 'Number of Catalog Purchases per Cluster'),\n                ('NumWebPurchases', 'Number of Website Purchases per Cluster'),\n                ('Age', 'Distribution of Age per Cluster'),\n                ('Num_Accepted', 'Number of Accepted Campaigns per Cluster'),\n                ('Income', 'Distribution of Income per Cluster'),\n                  ('Recency', 'Recency per Cluster'),\n                  ('Days_Since_Customer', 'Days Since Becoming a Customer per Cluster')]\n\n#Create list of variables to plot in bar graph\nvars_to_barplot = [('Education', 'Education Levels per Cluster'),\n                  ('Complain', 'Complaining per Cluster')]\n\n\n#plot all kde plots\nfor var in vars_to_kdeplot:\n    create_kdeplot(var[0], var[1])\n\n#plot all bar plots\nfor var in vars_to_barplot:\n    create_barplot(var[0], var[1])","ea9840a7":"#Split into train (70%) and test (30%)\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=8)\n\n#Split the test set into 2 sets; 1 for test, 1 for validation\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=8)\n\n#Display length of each set\nprint('Length of Each Dataset:')\nprint('Training Set:', len(X_train))\nprint('Validation Set:', len(X_val))\nprint('Test Set:', len(X_test))","4b3e5548":"#Balance the training data set using SMOTE\n#create the SMOTE object\nsm = SMOTE(random_state=8)\n\n#create new training set with SMOTE object\nX_bal, y_bal = sm.fit_resample(X_train, y_train)\n\n#Displays perccent of each class\nprint('Initial Training Set')\nprint('Percent \"Responded\":', y_train.sum()\/len(y_train))\nprint('Balanced Training Set')\nprint('Percent \"Responded\":', y_bal.sum()\/len(y_bal))","40650a60":"#Create a Logistic Regression Model\n#Params to test in grid search\nlr_params = {'solver': ['liblinear'], 'penalty': ['l1'], 'C': [1.0, 0.5, 0.25]}\n\n#grid search\nlr_grid = GridSearchCV(LogisticRegression(), lr_params, cv=3, scoring='recall')\n\n#fit the grid to the training set\nlr_grid.fit(X_bal, y_bal)\n\n#ID the best model\nlr = lr_grid.best_estimator_\n\n#Display Best Parameters\nprint('Best Parameters:', lr_grid.best_params_)\n\n#Display the metrics for the validation set\nlr_preds = lr.predict(X_val)\nlr_val_acc = accuracy_score(y_val, lr_preds)\nlr_val_rec = recall_score(y_val, lr_preds)\nprint('Logistic Regression Model Accuracy:', lr_val_acc)\nprint('Logistic Regression Model Recall:', lr_val_rec)","229505cc":"#Create a Support Vector machine\n#Params to test in grid search\nsvm_params = {'kernel': ['poly', 'rbf'], 'C': [1.0, 0.5, 0.25], 'gamma': ['scale', 'auto']}\n\n#grid search\nsvm_grid = GridSearchCV(SVC(), svm_params, cv=3, scoring='recall')\n\n#fit the grid to the training set\nsvm_grid.fit(X_bal, y_bal)\n\n#ID the best model\nsvm = svm_grid.best_estimator_\n\n#Display Best Parameters\nprint('Best Parameters:', svm_grid.best_params_)\n\n#Display the metrics for the validation set\nsvm_preds = svm.predict(X_val)\nsvm_val_acc = accuracy_score(y_val, svm_preds)\nsvm_val_rec = recall_score(y_val, svm_preds)\nprint('Support Vector Machine Accuracy:', svm_val_acc)\nprint('Support Vector Machine Recall:', svm_val_rec)","b95043c7":"#Create a knn model\n#Params to test in grid search\nknn_params = {'n_neighbors': [7, 9, 11], 'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n             'weights': ['uniform', 'distance']}\n\n#grid search\nknn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=3, scoring='recall')\n\n#fit the grid to the training set\nknn_grid.fit(X_bal, y_bal)\n\n#ID the best model\nknn = knn_grid.best_estimator_\n\n#Display Best Parameters\nprint('Best Parameters:', knn_grid.best_params_)\n\n#Display the metrics for the validation set\nknn_preds = knn.predict(X_val)\nknn_val_acc = accuracy_score(y_val, knn_preds)\nknn_val_rec = recall_score(y_val, knn_preds)\nprint('K Nearest Neighbors Accuracy:', knn_val_acc)\nprint('K Nearest Neighbors Recall:', knn_val_rec)","b1c17021":"#Create a naive bayes model\nnb = GaussianNB()\n\n#fit the model to the training set\nnb.fit(X_bal, y_bal)\n\n#Display the metrics for the validation set\nnb_preds = nb.predict(X_val)\nnb_val_acc = accuracy_score(y_val, nb_preds)\nnb_val_rec = recall_score(y_val, nb_preds)\nprint('Naive Bayes Accuracy:', nb_val_acc)\nprint('Naive Bayes Machine Recall:', nb_val_rec)","2577a475":"#Create a xgboost model\n#Params to test in grid search\nxgb_params = {'n_estimators': [240, 250, 260], 'max_depth': [15, 16, 17],\n             'colsample_bytree': [0.6, 0.7, 0.8, 1.0]}\n\n#grid search\nxgb_grid = GridSearchCV(XGBClassifier(use_label_encoder=False, verbosity=0), xgb_params, cv=3, \n                        scoring='recall')\n\n#fit the grid to the training set\nxgb_grid.fit(X_bal, y_bal)\n\n#ID the best model\nxgb = xgb_grid.best_estimator_\n\n#Display Best Parameters\nprint('Best Parameters:', xgb_grid.best_params_)\n\n#Display the metrics for the validation set\nxgb_preds = xgb.predict(X_val)\nxgb_val_acc = accuracy_score(y_val, xgb_preds)\nxgb_val_rec = recall_score(y_val, xgb_preds)\nprint('Gradient Boosting Tree Accuracy:', xgb_val_acc)\nprint('Gradient Boosting Tree Recall:', xgb_val_rec)","869ab7c5":"#Create ensemble model of all the other models\n#list of models\nmodels = [('logistic_regression', lr), ('support vector machine', svm), \n        ('knn', knn), ('naive_bayes', nb), ('gradient_boost', xgb)]\n\n#Combine models\nensemble_model = VotingClassifier(estimators=models)\n\n#fit the model on the training set\nensemble_model.fit(X_bal, y_bal)\n\n#Display the metrics for the validation set\nensemble_preds = ensemble_model.predict(X_val)\nensemble_val_acc = accuracy_score(y_val, ensemble_preds)\nensemble_val_rec = recall_score(y_val, ensemble_preds)\nprint('Ensemble Model Accuracy:', ensemble_val_acc)\nprint('Ensemble Model Recall:', ensemble_val_rec)","5c89d8d2":"#Display the metrics of the Ensemble model on the test set\ntest_preds = ensemble_model.predict(X_test)\ntest_acc = accuracy_score(y_test, test_preds)\ntest_rec = recall_score(y_test, test_preds)\nprint('Test Set Metrics')\nprint('Ensemble Model Accuracy:', test_acc)\nprint('Ensemble Model Recall:', test_rec)","a6970688":"<h5>Now let us look at where 'Response' would fall in this distribution.<\/h5>","a4b51b2b":"<h5>The outlier customers (-1) have a majority of those customers who responded to the campaign (177). On the other hand, customers of the biggest cluster (0) had only 9% response rate.<\/h5>","78e74f0f":"<h5>Next, I will move onto objective 2, Group similar customers based on traits and behaviors.<\/h5>","0bcc4709":"<a id=\"6\"><\/a>\n<h2>Visualizing the Data<\/h2>","077219d7":"<h5>Finally, I will be covering objective 3, create predictive model to predict which customers will respond to marketting campaigns.<\/h5>","308d9d7f":"<h4>Naive Bayes<\/h4>","222fc281":"<h4>Gradient Boosting Tree<\/h4>","81f3a76e":"<h5>I will start with objective 1, determine customer traits and behaviors.<\/h5>","fb087744":"<a id=\"2\"><\/a>\n<h2>Project Objectives<\/h2>\n<ol>\n    <li>Determine customer traits and behaviors<\/li>\n    <li>Group similar customers based on traits and behaviors<\/li>\n    <li>Create predictive model to predict which customers will respond to marketting campaigns<\/li>\n<\/ol>","64804766":"<h5>Let us take a look at the top 5 most correlated features<\/h5>","c848ecce":"<h5>Thank you for reading through this project. I hope this helps you generate ideas.<\/h5> ","68cf6392":"<h5>Let us plot a very interesting correlated features.<\/h5>","87891dac":"<h5>Now there are 3 distinct clusters of customers along with about 750 outlier customers.<\/h5>","b573e54a":"<h5>The 'Income' column contains 24 missing values. Since this only accounts for 1% of the total dataset, imputing the mean will not have great consequences.<\/h5>","46292845":"<h4>K Nearest Neighbors<\/h4>","5ef0c9a5":"<h5>This model takes the predictions of the previous 5 models and will output the class with the highest amount of votes.\n    The ensemble models strikes a good balance between recall and accuracy (both around 85%). Now, let us see the results on the test set.<\/h5>","66e6b2d3":"<a id=\"12\"><\/a>\n<h2>Split Dataset and Balance Classes<\/h2>","33a61e48":"<h5>For the following models, recall will be a very important metric. This is because we would rather have more False Positives (customers who will not respond to the marketing but was targeted anyway) than False Negatives (customers who would have responded to the add but were not targeted). For these reasons, a balance must be struck between accuracy of the model and the recall of the model.<\/h5>","10460e12":"<h5>There is quite a bit of variance between the largest cluster (cluster 0) and the other cluster. The other clusters and the outliers vary in more subltle ways. Below are some key points for each cluster<\/h5>\n\n<h3>Cluster 0<\/h3>\n<ul>\n    <li>Low Spending<\/li>\n    <li>Lower Income<\/li>\n    <li>Larger Family<\/li>\n    <li>Middle Aged<\/li>\n    <li>Doesn't Respond to Marketing<\/li>\n<\/ul>\n<h3>Cluster 1<\/h3>\n<ul>\n    <li>Moderate Spending<\/li>\n    <li>Smaller Family<\/li>\n    <li>Retirement Age<\/li>\n    <li>High Catalog and Web Purchases<\/li>\n    <li>Long Time Customer<\/li>\n    <li>Moderate Response to Marketing<\/li>\n<\/ul>\n<h3>Cluster 2<\/h3>\n<ul>\n    <li>High Spending<\/li>\n    <li>Smaller Family<\/li>\n    <li>Moderate Catalog and Web Purchases<\/li>\n    <li>Early-Middle Aged<\/li>\n    <li>Slight Response to Marketing<\/li>\n<\/ul>\n<h3>Outliers<\/h3>\n<ul>\n    <li>Variant Spending<\/li>\n    <li>Medium-High Income<\/li>\n    <li>Medium Sized Family<\/li>\n    <li>Moderate Catalog and Web Purchases<\/li>\n    <li>Middle Aged<\/li>\n    <li>High Response to Marketing<\/li>\n<\/ul>","e0c33855":"<a id=\"4\"><\/a>\n<h2>Feature Engineering<\/h2>","778b2cee":"<h4>Logistic Regression<\/h4>","6c2112cc":"<a id=\"13\"><\/a>\n<h2>Create Models<\/h2>","a2a97f74":"<h5>Below displays the correlation matrix.<h5>","9ca2e651":"<h3>About Each Attribute<\/h3>\n<p><strong>People<\/strong><\/p>\n<ul>\n<li>ID: Customer's unique identifier<\/li>\n<li>Year_Birth: Customer's birth year<\/li>\n<li>Education: Customer's education level<\/li>\n<li>Marital_Status: Customer's marital status<\/li>\n<li>Income: Customer's yearly household income<\/li>\n<li>Kidhome: Number of children in customer's household<\/li>\n<li>Teenhome: Number of teenagers in customer's household<\/li>\n<li>Dt_Customer: Date of customer's enrollment with the company<\/li>\n<li>Recency: Number of days since customer's last purchase<\/li>\n<li>Complain: 1 if customer complained in the last 2 years, 0 otherwise<\/li>\n<\/ul>\n<p><strong>Products<\/strong><\/p>\n<ul>\n<li>MntWines: Amount spent on wine in last 2 years<\/li>\n<li>MntFruits: Amount spent on fruits in last 2 years<\/li>\n<li>MntMeatProducts: Amount spent on meat in last 2 years<\/li>\n<li>MntFishProducts: Amount spent on fish in last 2 years<\/li>\n<li>MntSweetProducts: Amount spent on sweets in last 2 years<\/li>\n<li>MntGoldProds: Amount spent on gold in last 2 years<\/li>\n<\/ul>\n<p><strong>Promotion<\/strong><\/p>\n<ul>\n<li>NumDealsPurchases: Number of purchases made with a discount<\/li>\n<li>AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise<\/li>\n<li>AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise<\/li>\n<li>AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise<\/li>\n<li>AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise<\/li>\n<li>AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise<\/li>\n<li>Response: 1 if customer accepted the offer in the last campaign, 0 otherwise<\/li>\n<\/ul>\n<p><strong>Place<\/strong><\/p>\n<ul>\n<li>NumWebPurchases: Number of purchases made through the company\u2019s web site<\/li>\n<li>NumCatalogPurchases: Number of purchases made using a catalogue<\/li>\n<li>NumStorePurchases: Number of purchases made directly in stores<\/li>\n<li>NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month<\/li>\n<\/ul>\n<p>\n    Source: <a href=\"https:\/\/www.kaggle.com\/imakash3011\/customer-personality-analysis\">Kaggle - Customer Personality Analysis<\/a><\/p>","696a6e42":"<a id=\"15\"><\/a>\n<h2>Conclusions on Objective 3<\/h2>\n\n<ul>\n    <li><strong>The final model's accuracy is 85% and recall is 75%.<\/strong> This is a reasonable balance between the two metrics and will allow the store to identify and target the majority of customers who will respond to marketing while not having to spend an excess of resources targeting large amounts of customers who will not respond. If the store was willing to spend a bit more on marketing, the ensemble model could be modified to identify customers who will respond to the add if 2 or more of the 5 weak models vote.<\/li>\n    <li><strong>This dataset may not be complex enough.<\/strong> Customers are complex. There are a variety of reasons why a customer would respond to marketing and the dataset used here only includes a small fraction of all variables that need to be considered. That being said, some more feature that could be useful if provided would be: what items are being marketed for each of the campaigns, times of year of purchases and marketing campaigns, the location of the store(s), and how each marketing campaign was presented to customers (web only?, catalog and web?).<\/li>\n<\/ul>","b8820d8d":"<h5>After using SMOTE the dataset is perfeclty balanced. This will improve the performance of the models in the next section.<\/h5>","a179533d":"<a id=\"7\"><\/a>\n<h2>Conclusions On Objective 1<\/h2>\n<ul>\n    <li><strong>Positive response to previous marketing campaigns<\/strong> was the most correlated with a response to \n        the most recent ad campaign. This shows that possibly the customers are very happy with the marketing campaigns\n        and decide to respond to the next campaign. Or this could be showing a certain group of customers that are more\n        influenced by the campaigns.<\/li>\n    <li><strong>Total amount spent on products<\/strong>, especially wines and meats, are very highly correlated with \n        whether the customer responded to the marketing campaign. However, amount spent on gold, fish, sweets and fish\n        were not as correlated. This could be due to the nature of the most recent marketing campaign - perhaps the store\n        was trying to sell meat and wine in the most recent campaign?<\/li>\n    <li><strong>Catalog purchases correlate with response<\/strong> to the current marketing campaign where as in store, \n        online, and deal purchases have very little to no correlation. This may be due to the medium that the marketing \n        campaign was using - maybe it was not displayed in store\/online but was in all the catalogs? Another possibility\n        is that those customers who perform catalog purchases are more influenced by the campaigns<\/li>\n    <li><strong>Customers with smaller family size responded better<\/strong> to the marketing campaign. Maybe the customers\n        without family had more money to spend on the products in the campaign or the products in the campaign were for \n        signle customers (like alcohol and party supplies). Without further inforamation on the details on the campaign\n        it is hard to say.<\/li>\n    <li><strong>Customers who recently purchased something are likely to respond<\/strong> to the marketing campaign. \n        This is pretty clear - more recent purchases = probable pattern of shopping at the store<\/li>\n    <li><strong>Income and Total Amount Spent are very correlated<\/strong>. Customers who earn more spend more.<\/li>\n    <li>Finally, of note is <strong>Age and Complaining had virtually 0 correlation<\/strong> with response. This shows\n        that the campaign did a good job of catering to all age groups and that customers who complained in the past\n        continued bussiness at the store<\/li>\n<\/ul>","b2d2e614":"<a id=\"11\"><\/a>\n<h2>Conclusions on Objective 2<\/h2>\n\n<ul>\n    <li><strong>The majority of customers are low spenders<\/strong> with big families and not likely to respond \n        to a campaign. This group of customers have lower incomes on average (compared to other customer groups)\n    and probably do not have much money to spend on extra products. Marketing targeting this group will be difficult \n    due to these reasons.<\/li>\n    <li><strong>Customers who deviate from the norm, are more likely to respond to marketing.<\/strong> Amoung\n        these customers are 3 groups: moderately spending elderly customers, high spending middle aged customers, and\n        outlier customers who tend to have higher incomes. Overall, these groups both make more and spend more money \n    than the majority of customers. Marketing targeting these groups have a higher chance of success. This could be\n    accomplished using the web or catalogs.<\/li>\n<\/ul>","c4c7502e":"<a id=\"9\"><\/a>\n<h2>Clustering Customers<\/h2>","d6466f38":"<h5>The data will need some preprocessing before the dimensionality is reduced. 'Education' needs to be encoded and the other\n    columns need to be scaled. Also, the target column 'Repsonse' needs to be removed from the dataset.<\/h5>","44289950":"<a id=\"8\"><\/a>\n<h2>Dimensionality Reduction<\/h2>","21542940":"<a id=\"10\"><\/a>\n<h2>About the Clusters<\/h2>","89495342":"<h5>The reduced dimentionality dataset now has only 3 dimensions. Let us take a look at the dataset now.<\/h5>","b2f41e95":"<a id=\"14\"><\/a>\n<h2>Ensemble the Models<\/h2>","247ffcb0":"<h1>Customer Personality Analysis<\/h1>\n<h2>Table of Contents<\/h2>\n\n* [Problem Statement](#1)\n    \n* [Project Objectives](#2)\n    \n* [Importing Libraries and Read In Dataset](#3)\n    \n* [Feature Engineering](#4)   \n    \n* [Handle Missing Values](#5) \n      \n* [Visualizing the Data](#6)\n\n* [Conclusions on Objective 1](#7)\n    \n* [Dimensionality Reduction](#8)   \n    \n* [Clustering Customers](#9) \n      \n* [About the Clusters](#10)\n\n* [Conclusions on Objective 2](#11)\n    \n* [Split Dataset and Balance Classes](#12)   \n    \n* [Create Models](#13) \n      \n* [Ensemble the Models](#14)\n\n* [Conclusions on Objective 3](#15)","7661b7e6":"<h5>Let us compare the statistics of each cluster by plotting a few variables.<\/h5>","1c75d1b9":"<h5>On initial inspection, it looks like many of the customers who responded were located outside of the dense cluster that holds most customers.<\/h5>","fc78e87f":"<a id=\"3\"><\/a>\n<h2>Importing Libraries and Read In Dataset<\/h2>","4f2d129d":"<h4>Support Vector Machine<\/h4>","ee52b654":"<a id=\"5\"><\/a>\n<h2>Handle Missing Values<\/h2>","3b5f1d7b":"<a id=\"1\"><\/a>\n<h2>Problem Statement<\/h2>\n<p>\n    Customer Personality Analysis is a detailed analysis of a company\u2019s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n\nCustomer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company\u2019s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n<\/p>\n<p>\n    Source: <a href=\"https:\/\/www.kaggle.com\/imakash3011\/customer-personality-analysis\">Kaggle - Customer Personality Analysis<\/a><\/p>"}}