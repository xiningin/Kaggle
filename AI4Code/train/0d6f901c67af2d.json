{"cell_type":{"4dbef606":"code","a9c44f4d":"code","1db6d3b2":"code","94a0c394":"code","0dfecc02":"code","3ee718dd":"code","0c2f62d1":"code","41f5fd7c":"code","f789245b":"code","f03fa6b1":"code","22e42e0a":"code","9a1474cc":"code","deda5ae5":"code","29ac7ac5":"code","42e2b017":"code","3b89dbd6":"code","8364d313":"code","ba87b2d9":"code","6bc81982":"code","60e067fc":"code","707033b2":"code","951f723d":"code","07215cef":"code","caa9311e":"code","944637f4":"code","fa4b8bb0":"code","3fe8ca5d":"code","2d99a448":"code","7fb63511":"code","fa60d302":"code","72f8b2b4":"code","e38b6862":"code","9fb8d58e":"code","bb6441b5":"code","1af8cac2":"code","cdb5a59d":"markdown","0df8e74b":"markdown","7a38dc9a":"markdown","8289a0f9":"markdown","df04f6cf":"markdown","eed32fc6":"markdown","35aa0760":"markdown","3f6e79ad":"markdown","4f63096e":"markdown","197cbeba":"markdown","b19e3e47":"markdown","86626031":"markdown","0e98ebc1":"markdown","18aa0609":"markdown","0f167693":"markdown"},"source":{"4dbef606":"import tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","a9c44f4d":"tf.test.is_built_with_cuda()","1db6d3b2":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","94a0c394":"import numpy as np\nimport os\nfrom skimage.io import imread\nimport datetime\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport abc\nfrom sys import getsizeof\n\nnp.random.seed(30)\nimport random as rn\nrn.seed(30)\nimport cv2\nimport matplotlib.pyplot as plt\n\n# importing some other libraries which will be needed for model building.\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\nfrom keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\nfrom keras.layers.recurrent import LSTM\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras import optimizers\nfrom keras.layers import Dropout","0dfecc02":"project_folder='..\/input\/gesture-recognition\/Project_data'","3ee718dd":"class ModelBuilder(metaclass= abc.ABCMeta):\n    # initialisng the path where project data resides\n    def initialize_path(self,project_folder):\n        self.train_doc = np.random.permutation(open(project_folder + '\/' + 'train.csv').readlines())\n        self.val_doc = np.random.permutation(open(project_folder + '\/' + 'val.csv').readlines())\n        self.train_path = project_folder + '\/' + 'train'\n        self.val_path =  project_folder + '\/' + 'val'\n        self.num_train_sequences = len(self.train_doc)\n        self.num_val_sequences = len(self.val_doc)\n    # initialising the image properties    \n    def initialize_image_properties(self,image_height=100,image_width=100):\n        self.image_height=image_height\n        self.image_width=image_width\n        self.channels=3\n        self.num_classes=5\n        self.total_frames=30\n    # initialising the batch size, frames to sample and the no. of epochs\n    def initialize_hyperparams(self,frames_to_sample=30,batch_size=20,num_epochs=20):\n        self.frames_to_sample=frames_to_sample\n        self.batch_size=batch_size\n        self.num_epochs=num_epochs\n        \n    # MOST IMPORTANT PART HERE - The generator function        \n    def generator(self,source_path, folder_list, augment=False):\n        img_idx = np.round(np.linspace(0,self.total_frames-1,self.frames_to_sample)).astype(int)\n        batch_size=self.batch_size\n        while True:\n            t = np.random.permutation(folder_list)\n            num_batches = len(t)\/\/batch_size\n        \n            for batch in range(num_batches): \n                batch_data, batch_labels= self.one_batch_data(source_path,t,batch,batch_size,img_idx,augment)\n                yield batch_data, batch_labels \n\n            remaining_seq=len(t)%batch_size\n        \n            if (remaining_seq != 0):\n                batch_data, batch_labels= self.one_batch_data(source_path,t,num_batches,batch_size,img_idx,augment,remaining_seq)\n                yield batch_data, batch_labels \n    \n    \n    def one_batch_data(self,source_path,t,batch,batch_size,img_idx,augment,remaining_seq=0):\n    \n        seq_len = remaining_seq if remaining_seq else batch_size\n    \n        batch_data = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels)) \n        batch_labels = np.zeros((seq_len,self.num_classes)) \n    \n        if (augment): batch_data_aug = np.zeros((seq_len,len(img_idx),self.image_height,self.image_width,self.channels))\n\n        \n        for folder in range(seq_len): \n            imgs = os.listdir(source_path+'\/'+ t[folder + (batch*batch_size)].split(';')[0]) \n            for idx,item in enumerate(img_idx):\n                #performing image reading and resizing\n                image = imread(source_path+'\/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'\/'+imgs[item]).astype(np.float32)\n                image_resized=cv2.resize(image,(self.image_height,self.image_width))\n            \n                #normalizing the images\n                batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])\/255\n                batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])\/255\n                batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])\/255\n            \n                if (augment):\n                    shifted = cv2.warpAffine(image, \n                                             np.float32([[1, 0, np.random.randint(-30,30)],[0, 1, np.random.randint(-30,30)]]), \n                                            (image.shape[1], image.shape[0]))\n                    \n                    gray = cv2.cvtColor(shifted,cv2.COLOR_BGR2GRAY)\n\n                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n                    # cropping the images to have the targeted gestures and remove the noise from the images.\n                \n                    \n                    image_resized=cv2.resize((self.image_height,self.image_width))\n                    \n                    #shifted = cv2.warpAffine(image_resized, \n                    #                        np.float32([[1, 0, np.random.randint(-3,3)],[0, 1, np.random.randint(-3,3)]]), \n                    #                        (image_resized.shape[1], image_resized.shape[0]))\n            \n                    batch_data_aug[folder,idx,:,:,0] = (image_resized[:,:,0])\/255\n                    batch_data_aug[folder,idx,:,:,1] = (image_resized[:,:,1])\/255\n                    batch_data_aug[folder,idx,:,:,2] = (image_resized[:,:,2])\/255\n                \n            \n            batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            \n    \n        if (augment):\n            batch_data=np.concatenate([batch_data,batch_data_aug])\n            batch_labels=np.concatenate([batch_labels,batch_labels])\n\n        \n        return(batch_data,batch_labels)\n    \n    \n    def train_model(self, model, augment_data=False):\n        train_generator = self.generator(self.train_path, self.train_doc,augment=augment_data)\n        val_generator = self.generator(self.val_path, self.val_doc)\n\n        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ','').replace(':','_') + '\/'\n    \n        if not os.path.exists(model_name):\n            os.mkdir(model_name)\n        \n        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\n        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n        \n        earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n        callbacks_list = [checkpoint, LR, earlystop]\n\n        if (self.num_train_sequences%self.batch_size) == 0:\n            steps_per_epoch = int(self.num_train_sequences\/self.batch_size)\n        else:\n            steps_per_epoch = (self.num_train_sequences\/\/self.batch_size) + 1\n\n        if (self.num_val_sequences%self.batch_size) == 0:\n            validation_steps = int(self.num_val_sequences\/self.batch_size)\n        else:\n            validation_steps = (self.num_val_sequences\/\/self.batch_size) + 1\n    \n        history=model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n                            callbacks=callbacks_list, validation_data=val_generator, \n                            validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n        return history\n\n        \n    @abc.abstractmethod\n    def define_model(self):\n        pass","0c2f62d1":"# function to plot the training\/validation accuracies\/losses.\n\ndef plot(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n    axes[0].plot(history.history['loss'])   \n    axes[0].plot(history.history['val_loss'])\n    axes[0].legend(['loss','val_loss'])\n    axes[1].plot(history.history['categorical_accuracy'])   \n    axes[1].plot(history.history['val_categorical_accuracy'])\n    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'])","41f5fd7c":"class ModelConv3D1(ModelBuilder):\n    \n    def define_model(self):\n\n        model = Sequential()\n        model.add(Conv3D(16, (3, 3, 3), padding='same',\n                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Flatten())\n        model.add(Dense(128,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n\n        model.add(Dense(64,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.25))\n\n\n        model.add(Dense(self.num_classes,activation='softmax'))\n\n        optimiser = tf.optimizers.Adam()\n        #optimiser = 'sgd'\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","f789245b":"conv_3d1=ModelConv3D1()\nconv_3d1.initialize_path(project_folder)\nconv_3d1.initialize_image_properties(image_height=160,image_width=160)\nconv_3d1.initialize_hyperparams(frames_to_sample=30,batch_size=10,num_epochs=1)\nconv_3d1_model=conv_3d1.define_model()\nconv_3d1_model.summary()","f03fa6b1":"print(\"Total Params:\", conv_3d1_model.count_params())\nhistory_model1 = conv_3d1.train_model(conv_3d1_model)","22e42e0a":"print(\"Memory util is {} Gigs\". format(getsizeof(np.zeros((40,16,30,160,160)))\/(1024*1024*1024)))","9a1474cc":"conv_3d2=ModelConv3D1()\nconv_3d2.initialize_path(project_folder)\nconv_3d2.initialize_image_properties(image_height=160,image_width=160)\nconv_3d2.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=25)\nconv_3d2_model=conv_3d2.define_model()\nconv_3d2_model.summary()","deda5ae5":"print(\"Total Params:\", conv_3d2_model.count_params())\nhistory_model2=conv_3d2.train_model(conv_3d2_model)","29ac7ac5":"plot(history_model2)","42e2b017":"class ModelConv3D4(ModelBuilder):\n    \n    def define_model(self,filtersize=(3,3,3),dense_neurons=64,dropout=0.25):\n\n        model = Sequential()\n        model.add(Conv3D(16, filtersize, padding='same',\n                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(Conv3D(16, filtersize, padding='same',\n                 input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(32, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(Conv3D(32, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(64, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(Conv3D(64, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n\n        model.add(Conv3D(128, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(Conv3D(128, filtersize, padding='same'))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        \n        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n        \n\n        model.add(Flatten())\n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(dropout))\n\n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(dropout))\n\n\n        model.add(Dense(self.num_classes,activation='softmax'))\n\n        optimiser = tf.optimizers.Adam()\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","3b89dbd6":"conv_3d4=ModelConv3D4()\nconv_3d4.initialize_path(project_folder)\nconv_3d4.initialize_image_properties(image_height=120,image_width=120)\nconv_3d4.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=25)\nconv_3d4_model=conv_3d4.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\nconv_3d4_model.summary()","8364d313":"print(\"Total Params:\", conv_3d4_model.count_params())\nhistory_model4=conv_3d4.train_model(conv_3d4_model)","ba87b2d9":"plot(history_model4)","6bc81982":"class RNNCNN1(ModelBuilder):\n    \n    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n\n        model = Sequential()\n\n        model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n                                  input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        \n        model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        \n        model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        \n        model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        \n        model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        \n        #model.add(TimeDistributed(Conv2D(512, (2, 2) , padding='valid', activation='relu')))\n       # model.add(TimeDistributed(BatchNormalization()))\n       # model.add(TimeDistributed(MaxPooling2D((2, 2))))\n\n        model.add(TimeDistributed(Flatten()))\n\n\n        model.add(LSTM(lstm_cells))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(self.num_classes, activation='softmax'))\n        optimiser = tf.optimizers.Adam()\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","60e067fc":"rnn_cnn1=RNNCNN1()\nrnn_cnn1.initialize_path(project_folder)\nrnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\nrnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\nrnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\nrnn_cnn1_model.summary()","707033b2":"print(\"Total Params:\", rnn_cnn1_model.count_params())\nhistory_model8=rnn_cnn1.train_model(rnn_cnn1_model)","951f723d":"plot(history_model8)","07215cef":"# importing the MobileNet model due to it's lightweight architecture and high speed performance as compared \n# to other heavy-duty models like VGG16, Alexnet, InceptionV3 etc. Also, we are now also running on low disk space \n# in the nimblebox.ai platform. \n\nfrom keras.applications import mobilenet","caa9311e":"mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n\nclass RNNCNN_TL(ModelBuilder):\n    \n    def define_model(self,lstm_cells=64,dense_neurons=64,dropout=0.25):\n        \n        model = Sequential()\n        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n        \n        \n        for layer in model.layers:\n            layer.trainable = False\n        \n        \n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        model.add(TimeDistributed(Flatten()))\n\n        model.add(LSTM(lstm_cells))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(self.num_classes, activation='softmax'))\n        \n        \n        optimiser = tf.optimizers.Adam()\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","944637f4":"rnn_cnn_tl=RNNCNN_TL()\nrnn_cnn_tl.initialize_path(project_folder)\nrnn_cnn_tl.initialize_image_properties(image_height=120,image_width=120)\nrnn_cnn_tl.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\nrnn_cnn_tl_model=rnn_cnn_tl.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\nrnn_cnn_tl_model.summary()","fa4b8bb0":"print(\"Total Params:\", rnn_cnn_tl_model.count_params())\nhistory_model16=rnn_cnn_tl.train_model(rnn_cnn_tl_model)","3fe8ca5d":"plot(history_model16)","2d99a448":"from keras.applications import mobilenet\n\nmobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n\nclass RNNCNN_TL2(ModelBuilder):\n    \n    def define_model(self,gru_cells=64,dense_neurons=64,dropout=0.25):\n        \n        model = Sequential()\n        model.add(TimeDistributed(mobilenet_transfer,input_shape=(self.frames_to_sample,self.image_height,self.image_width,self.channels)))\n \n        \n        model.add(TimeDistributed(BatchNormalization()))\n        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n        model.add(TimeDistributed(Flatten()))\n\n        model.add(GRU(gru_cells))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(dense_neurons,activation='relu'))\n        model.add(Dropout(dropout))\n        \n        model.add(Dense(self.num_classes, activation='softmax'))\n        \n        \n        optimiser = tf.optimizers.Adam()\n        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n        return model","7fb63511":"rnn_cnn_tl2=RNNCNN_TL2()\nrnn_cnn_tl2.initialize_path(project_folder)\nrnn_cnn_tl2.initialize_image_properties(image_height=120,image_width=120)\nrnn_cnn_tl2.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\nrnn_cnn_tl2_model=rnn_cnn_tl2.define_model(gru_cells=128,dense_neurons=128,dropout=0.25)\nrnn_cnn_tl2_model.summary()","fa60d302":"print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\nhistory_model17=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model)","72f8b2b4":"plot(history_model17)","e38b6862":"import time\nfrom keras.models import load_model\nmodel = load_model('model_init_2021-12-2410_14_30.351074\/model-00015-0.15974-0.95173-0.13263-0.97000.h5')","9fb8d58e":"test_generator=RNNCNN1()\ntest_generator.initialize_path(project_folder)\ntest_generator.initialize_image_properties(image_height=120,image_width=120)\ntest_generator.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n\ng=test_generator.generator(test_generator.val_path,test_generator.val_doc,augment=False)\nbatch_data, batch_labels=next(g)","bb6441b5":"batch_labels","1af8cac2":"print(np.argmax(model.predict(batch_data[:,:,:,:,:]),axis=1))","cdb5a59d":"### *Importing the necessary libraries*","0df8e74b":"### Problem Statement\nImagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n\nThe gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n \n| Gesture | Corresponding Action |\n| --- | --- | \n| Thumbs Up | Increase the volume. |\n| Thumbs Down | Decrease the volume. |\n| Left Swipe | 'Jump' backwards 10 seconds. |\n| Right Swipe | 'Jump' forward 10 seconds. |\n| Stop | Pause the movie. |\n\nEach video is a sequence of 30 frames (or images).\n\n### Objectives:\n1. **Generator**:  The generator should be able to take a batch of videos as input without any error. Steps like cropping, resizing and normalization should be performed successfully.\n\n2. **Model**: Develop a model that is able to train without any errors which will be judged on the total number of parameters (as the inference(prediction) time should be less) and the accuracy achieved. As suggested by Snehansu, start training on a small amount of data and then proceed further.\n\n3. **Write up**: This should contain the detailed procedure followed in choosing the final model. The write up should start with the reason for choosing the base model, then highlight the reasons and metrics taken into consideration to modify and experiment to arrive at the final model.","7a38dc9a":"## Model 17 - Transfer Learning with GRU and training all weights","8289a0f9":"## Model\nHere you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam.","df04f6cf":"In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error.","eed32fc6":"## Model 8 - CNN- LSTM Model","35aa0760":"## Model 16 - Let us bring in Transfer Learning !! :","3f6e79ad":"### Loading model and Testing","4f63096e":"## Generator\n*This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.*","197cbeba":"### Sample Model","b19e3e47":"## Model 1\n### Base Model - Batch Size = 40 and No. of Epochs = 15","86626031":"> *We can clearly see Model is Overfitting*","0e98ebc1":"##### We can see  val_loss did not improve from 0.28 so earlystopping stops the epoch automatically!! \n- Last Epoch stop on 12\/20!! good job earlystopping ;)\n- Best weights save automatically. The validation accuracy of 28% and training accuracy of 84%. \n- Still Model is Overfitting.","18aa0609":"## Model 4 - \n### Adding more layers - Batch Size = 30 and No. of Epochs = 25","0f167693":"## Model 2  \n### Adding dropout layers - Batch Size = 20 and No. of Epochs = 25"}}