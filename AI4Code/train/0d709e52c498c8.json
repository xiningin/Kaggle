{"cell_type":{"7c69eed3":"code","17132f37":"code","6ece2ff8":"code","1ec06e00":"code","d0eabef6":"code","f79784c2":"code","59a40f77":"code","8b18bafe":"markdown","a1c0f929":"markdown","9767892b":"markdown","0c6a1e10":"markdown","84beb7e1":"markdown","ec544982":"markdown","d7aba618":"markdown","31d9665c":"markdown","325a70d0":"markdown","eb81e751":"markdown"},"source":{"7c69eed3":"import numpy as np\nimport pandas as pd","17132f37":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nsub_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")","6ece2ff8":"train_df.drop(columns=[\"id\"], inplace=True)\ntest_df.drop(columns=[\"id\"], inplace=True)\n\nX = train_df.drop(columns=[\"target\"]).values\ny = train_df[\"target\"].values","1ec06e00":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\n\ndef build_model():\n    model = Sequential([\n        layers.Dense(units=128, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X.shape[1:]),\n        layers.Dense(units=32, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n        layers.Dense(units=32, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n        layers.Dense(units=1, activation=\"sigmoid\")\n    ])\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"binary_crossentropy\",\n        metrics=[\"AUC\"]\n    )\n\n    return model\n\n\nbuild_model().summary()","d0eabef6":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.8,\n    patience=10,\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=60,\n    restore_best_weights=True\n)\n\ncallbacks = [reduce_lr, early_stop]","f79784c2":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n\nEPOCHS = 500\nBATCH_SIZE = 2048\nFOLDS = 7\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\ntest_preds = []\nmean_score = 0\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n\n    scaler = MinMaxScaler()\n\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(test_df)\n\n    model = build_model()\n\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_val, y_val),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[reduce_lr, early_stop],\n        verbose=False\n    )\n\n    y_pred = model.predict(X_val)\n    score = roc_auc_score(y_val, y_pred)\n    mean_score += score\n\n    print(f\"FOLD {fold} | Score: {score}\")\n\n    test_preds.append(model.predict(X_test))\n\n\nprint()\nprint(f\"Mean score of all folds: {mean_score\/FOLDS}\")","59a40f77":"sub_df[\"target\"] = sum(test_preds)\/FOLDS\nsub_df.to_csv(\"submission.csv\", index=False)\n\nsub_df.head()","8b18bafe":"# Introduction","a1c0f929":"I have used the StratifiedKFold validation strategy with 7 folds. To speed up the model training, a batch size of 2048 is used.","9767892b":"# Code","0c6a1e10":"## Some preprocessing","84beb7e1":"While FFNs (Feed Forward Networks) with Batch Normalization holds great potential to harness the many levels of abstract representations that comes with a deep network, the number of layers is limited by SGD. This is because after a certain number of few layers, SGD becomes unstable and the network starts to encounter problems such as vanishing and exploding gradients. Moreover, SGD and regularization techniques like dropout often perturbs Batch Normalization leading to high variance in training error. These problems are solved by Self Normalizing Neural Networks.\n\nSelf-Normalizing Neural Networks (SNNs) are neural networks which automatically keep their activations at zero-mean and unit-variance (per neuron). This is accomplished through the use of SeLU activation function which requires LeCun Normal kernel initialization.\n\nFollowing is an excerpt from the [research paper](https:\/\/arxiv.org\/pdf\/1706.02515.pdf) of Self Normalizing Neural Networks:\n\n> Self-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance\nin their training errors. SNNs push neuron activations to zero mean and unit variance\nthereby leading to the same effect as batch normalization, which enables to robustly learn many\nlayers. SNNs are based on scaled exponential linear units \u201cSELUs\u201d which induce self-normalizing\nproperties like variance stabilization which in turn avoids exploding and vanishing gradients.","ec544982":"## Training Model","d7aba618":"Notice that, the **[LeCun Normal](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers\/LecunNormal)** kernel initializer is used instead of the default one. Although, this network does not contains dropout layers, deep networks with large number of neurons can have [dropout](https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/) layers. However, the authors of the SNN paper have advised not to use this dropout. Instead they have proposed a new dropout technique called **alpha dropout** and have also suggested to use it instead. **[Alpha dropout](https:\/\/keras.io\/api\/layers\/regularization_layers\/alpha_dropout\/)** is available as a layer in keras.\n\nAlthough in this case I have built an SNN with only 3 layers, it is possible to stack many layers in an SNN. I have used 128 neurons in the first hidden layer, 32 neurons in the second hidden layer and 32 in the third hidden layer.","31d9665c":"Defining Various callbacks","325a70d0":"## Building SNN Model","eb81e751":"Seperating features and targets"}}