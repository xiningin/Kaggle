{"cell_type":{"2af6258f":"code","619aa722":"code","feaba0eb":"code","280a663c":"code","185d4cb3":"code","55311007":"code","494e087b":"code","9a9a9901":"code","ca80f2cf":"code","fd0a7aa4":"code","643dd073":"code","cfa13f16":"code","cdb322d5":"code","f595cc53":"code","e40ec474":"code","79937d09":"code","fc248dd6":"code","a848f3fa":"code","f583ca67":"code","d3372705":"code","2d2c7e77":"code","f8e62798":"code","4b076b45":"code","a4388843":"code","9f9c3b68":"code","12de354a":"code","4e9a0527":"code","6412d329":"code","d7d54d2a":"code","7a970e27":"code","1bf890d5":"code","f591fa65":"code","e14c6679":"code","7dd6e277":"code","c60d05da":"code","6518e92f":"code","7687c848":"code","4d6f0600":"code","e205a411":"code","cf93490c":"code","48eca2f1":"code","f9cbfff6":"code","782c90ce":"code","6e24ea7c":"code","af0a307d":"code","702d6cdc":"code","3e3aff42":"code","1bad01e3":"code","a025f3a8":"code","2ec9b483":"code","5b45cca0":"code","de2eb501":"code","5cf48c0d":"code","2113520b":"code","5ec6d45b":"code","f3f06d4d":"code","7476eb62":"code","cefc29ec":"code","bc377b6d":"code","08bbf49c":"code","0c536b5b":"code","1fd2f046":"code","ecf0e8fa":"code","86f13148":"code","3a07bd8b":"code","3832a404":"code","94e0459a":"code","06cf1fac":"code","a4b87c41":"markdown","9ce51f76":"markdown","4f3d0ca0":"markdown","746e0685":"markdown","b0b454c1":"markdown","8b88389b":"markdown","3ee95986":"markdown","38abdf90":"markdown","28ab817f":"markdown","96431788":"markdown","fd0ae6a4":"markdown","26df9d1e":"markdown","17647597":"markdown","a6c87fd8":"markdown","d2f1a3a0":"markdown","cab6e122":"markdown","a76ad0e8":"markdown","42453259":"markdown","881759a6":"markdown","1b46ec36":"markdown","17013d9e":"markdown","6e3fa0f4":"markdown","df087420":"markdown","a9fd8d3e":"markdown","caa6ff32":"markdown","9f720aa6":"markdown","84603e47":"markdown","8b2e8d14":"markdown","8930c789":"markdown","fddcad8e":"markdown","bb6955a9":"markdown","d787fa8e":"markdown","8d30d725":"markdown","4ffdde15":"markdown","ea8ae23a":"markdown","0e99f837":"markdown","520ad87a":"markdown","990070b7":"markdown","d3880fa2":"markdown","b15331ec":"markdown","ccfe9880":"markdown"},"source":{"2af6258f":"#Let us start by importing the libraries \n\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n%matplotlib inline","619aa722":"housing_data = pd.read_table('\/kaggle\/input\/AmesHousing.tsv', delimiter= '\\t')","feaba0eb":"housing_data.head()","280a663c":"#let us create the functions first that we will use in our pipeline\n#for now let us create the functions simply and we can update them in the later cells\n\ndef tranform_features(df): \n    return df\n\n\ndef select_features(df): \n    return df[['Gr Liv Area', 'SalePrice']]\n\n\ndef train_and_test(df):\n    train = df.iloc[0:1460,:]\n    test = df.iloc[1460:,:]\n    \n    selected_columns = train.select_dtypes(include = ['integer','float'])\n    #let'sonly select the columns that we get from the select_features function\n    columns_to_be_selected = select_features(df).columns.values.tolist()\n    selected_columns = selected_columns[columns_to_be_selected]\n    #drop the SalePrice column since it is our target variable\n    selected_columns = selected_columns.drop(columns = ['SalePrice'])\n    final_features = selected_columns.columns.values.tolist()\n    \n    lr = LinearRegression()\n    lr.fit(selected_columns[final_features], train['SalePrice'])\n    predictions = lr.predict(test[final_features])\n    mse = mean_squared_error(test['SalePrice'], predictions)\n    rmse = mse ** (1\/2)\n    \n    return rmse\n    ","185d4cb3":"#Let us quickly test our functions \ntranform_features(housing_data)\nselect_features(housing_data)\ntrain_and_test(housing_data)","55311007":"housing_data.info()","494e087b":"cutoff_point = housing_data.shape[0] * 0.05\ncutoff_point","9a9a9901":"cleaning_data = housing_data.copy()","ca80f2cf":"cleaning_data.isnull().sum()","fd0a7aa4":"missing_data = cleaning_data.isnull().sum()\ncolumns_drop =[]\n\nfor x in missing_data.index.values.tolist(): \n    if missing_data[x] > cutoff_point:\n        columns_drop.append(x)\n    \nprint(columns_drop)\n\n","643dd073":"cleaning_data = cleaning_data.drop(columns = columns_drop)\ncleaning_data.shape","cfa13f16":"categorical_data = cleaning_data.select_dtypes(include=['object'])","cdb322d5":"categorical_data.columns","f595cc53":"categorical_data.isnull().sum()","e40ec474":"categorical_missing_data_columns = categorical_data.isnull().sum()\ncategorical_missing_data_columns = categorical_missing_data_columns.sort_values(ascending=False)\ndrop_columns = categorical_missing_data_columns[categorical_missing_data_columns > 0]\ncols = drop_columns.index.values.tolist()","79937d09":"cleaning_data = cleaning_data.drop(columns = cols)","fc248dd6":"cleaning_data.isnull().sum()","a848f3fa":"numeric_columns = cleaning_data.select_dtypes(include=['float', 'integer'])","f583ca67":"columns_to_impute = numeric_columns.loc[:,(numeric_columns.isnull().sum() > 0)].columns","d3372705":"columns_to_impute","2d2c7e77":"numerical_mode = housing_data[columns_to_impute].mode()\nnumerical_mode","f8e62798":"numerical_mode = numerical_mode.to_dict(orient='records')[0]\nnumerical_mode","4b076b45":"cleaning_data.fillna(numerical_mode, inplace=True)","a4388843":"cleaning_data.isnull().sum()","9f9c3b68":"cleaning_data.info()","12de354a":"cleaning_data[['Overall Qual', 'Overall Cond']].head()","4e9a0527":"cleaning_data[['Year Built', 'Year Remod\/Add']].head()","6412d329":"cleaning_data[['Year Built', 'Year Remod\/Add']].dtypes","d7d54d2a":"cleaning_data['years_until_remod'] = cleaning_data['Year Remod\/Add'] - cleaning_data['Year Built']","7a970e27":"cleaning_data['years_until_remod'].value_counts()","1bf890d5":"cleaning_data[cleaning_data['years_until_remod'] < 0]","f591fa65":"cleaning_data.drop(index = 850, inplace=True)","e14c6679":"cleaning_data.drop(columns=['Year Remod\/Add','Year Built'], inplace=True)","7dd6e277":"cleaning_data[['Order', 'PID']].head()","c60d05da":"cleaning_data.drop(columns = ['Order','PID','Mo Sold','Yr Sold','Sale Type','Sale Condition'], inplace=True)","6518e92f":"cleaning_data.shape","7687c848":"numerical_cols = cleaning_data.select_dtypes(include = ['integer', 'float']) ","4d6f0600":"numerical_cols_corr = numerical_cols.corr()","e205a411":"plt.figure(figsize = (12,10))\nsns.heatmap(numerical_cols_corr)","cf93490c":"cleaning_data.drop(columns = ['Garage Cars', 'TotRms AbvGrd'], inplace=True)","48eca2f1":"cleaning_data.shape","f9cbfff6":"new_numerical_cols = cleaning_data.select_dtypes(include = ['float', 'integer'])\nnew_numerical_cols = new_numerical_cols.corr()\n","782c90ce":"new_numerical_cols['SalePrice'].abs().sort_values()","6e24ea7c":"corr_below = new_numerical_cols['SalePrice'].abs().sort_values()\ncorr_below = corr_below[corr_below < 0.3].index.values.tolist()\ncorr_below","af0a307d":"cleaning_data.drop(columns = corr_below, inplace = True)","702d6cdc":"cleaning_data.shape","3e3aff42":"categorical_cols = cleaning_data.select_dtypes(include=['object'])\ncategorical_cols.shape","1bad01e3":"for x in categorical_cols.columns.values: \n    freq = categorical_cols[x].value_counts()\n    print('The unique values in ', x,' are :')\n    print(freq)\n    print('---------------------------------')","a025f3a8":"unique_heavy_cols = [x for x in categorical_cols.columns if len(categorical_cols[x].value_counts()) > 10]","2ec9b483":"unique_heavy_cols","5b45cca0":"cleaning_data.drop(columns = unique_heavy_cols, inplace=True)","de2eb501":"cleaning_data.shape","5cf48c0d":"categorical_cols.drop(columns = unique_heavy_cols, inplace=True)","2113520b":"categorical_cols.shape","5ec6d45b":"categorical_cols['Street'].head()","f3f06d4d":"for col in categorical_cols.columns: \n    categorical_cols[col] = categorical_cols[col].astype('category')\n    cleaning_data[col] = cleaning_data[col].astype('category')","7476eb62":"categorical_cols.info()","cefc29ec":"print('Unique values :')\nprint(categorical_cols['Street'].value_counts())\nprint('\\n')\nprint('Category data type conversion :')\nprint(categorical_cols['Street'].cat.codes.value_counts())","bc377b6d":"dummy_values = pd.get_dummies(categorical_cols)","08bbf49c":"dummy_values.head()","0c536b5b":"for col in categorical_cols.columns: \n    dummies = pd.get_dummies(cleaning_data[col])\n    cleaning_data = pd.concat([cleaning_data,dummies], axis = 1)\n    del cleaning_data[col]\n    ","1fd2f046":"cleaning_data.head()","ecf0e8fa":"def tranform_features(df):\n    \n    df_copy = df.copy()\n    cutoff = df_copy.shape[0] * 0.05\n    \n    #drop numerical columns with missing data\n    numerical_cols = df_copy.select_dtypes(include = ['integer','float'])\n    missing_numerical = numerical_cols.isnull().sum()\n    drop_numerical_columns = []\n    for x in missing_numerical.index.values.tolist():\n        if missing_numerical[x] > cutoff:\n            drop_numerical_columns.append(x)\n    df_copy.drop(columns = drop_numerical_columns, inplace = True)\n    \n    #drop categorical data with missing values\n    categorical_data = df_copy.select_dtypes(include = ['object'])\n    categorical_missing_data = categorical_data.isnull().sum().sort_values(ascending = False)\n    drop_columns = categorical_missing_data[categorical_missing_data > 0]\n    cat_cols = drop_columns.index.values.tolist()\n    df_copy.drop(columns = cat_cols , inplace=True)\n    \n    #impute the remaining missing values with summary statistic\n    numerical = df_copy.select_dtypes(include = ['integer','float'])\n    numerical_cols_impute = numerical.loc[:,(numerical.isnull().sum() > 0)].columns\n    numerical_mode = df_copy[numerical_cols_impute].mode().to_dict(orient = 'records')[0]\n    df_copy.fillna(numerical_mode, inplace = True)\n    \n    #feature engineering - adding new features\n    df_copy['year_until_remod'] = df_copy['Year Remod\/Add'] - df_copy['Year Built']\n    df_copy.drop(index = 850, inplace = True)\n    df_copy.drop(columns = ['Year Remod\/Add','Year Built'], inplace = True)\n    df_copy.drop(columns = ['Order','PID','Mo Sold','Yr Sold','Sale Type','Sale Condition'], inplace = True)\n    \n    \n    return df_copy","86f13148":"def select_features(df_copy, correlation_cutoff, uniqueness_cutoff):\n    #two factors to consider - Correlation with target column and correlation with other features\n    df_copy2 = df_copy.copy()\n    df_copy2 = df_copy2.drop(columns = ['Garage Cars','TotRms AbvGrd'])\n    \n    #select numerical cols and base on correlation values decide which columns to keep\n    numerical_cols = df_copy2.select_dtypes(include = ['integer','float'])\n    numerical_correlation = numerical_cols.corr()\n    target_correl = numerical_correlation['SalePrice'].abs().sort_values()\n    corr_below_cutoff = target_correl[target_correl < correlation_cutoff].index.values.tolist()\n    df_copy2 = df_copy2.drop(columns = corr_below_cutoff)\n    \n    #select categorical columns and convert them to numerical variables\n    #dropping columns with alot of unique values\n    categorical_only = df_copy2.select_dtypes(include = ['object'])\n    unique_heavy_columns = [col for col in categorical_only.columns if len(categorical_only[col].value_counts()) > uniqueness_cutoff]\n    df_copy2 = df_copy2.drop(columns = unique_heavy_columns)\n    \n    #converting the remaining categorical columns to dummies\n    categorical_columns_only = df_copy2.select_dtypes(include = ['object'])\n    for columns in categorical_columns_only.columns: \n        df_copy2[columns] = df_copy2[columns].astype('category')\n    \n    #converting to dummies\n    for columns in categorical_columns_only.columns:\n        dummies = pd.get_dummies(df_copy2[columns])\n        df_copy2 = pd.concat([df_copy2,dummies], axis = 1)\n        del df_copy2[columns]\n    \n    return df_copy2\n    \n    \n    ","3a07bd8b":"def train_and_test(df_copy2,k):\n    df_copy3 = df_copy2.copy()\n    numeric = df_copy3.select_dtypes(include = ['integer','float'])\n    columns_numeric = numeric.drop(columns = ['SalePrice'])\n    cols = columns_numeric.columns.values.tolist()\n    lr = LinearRegression()\n    \n    \n    if k == 0: \n        train = df_copy3[0:1460]\n        test = df_copy3[1460:]\n        lr.fit(train[cols], train['SalePrice'])\n        predictions = lr.predict(test[cols])\n        mse = mean_squared_error(test['SalePrice'],predictions)\n        rmse = mse ** (1\/2)\n        \n        return rmse\n    \n    if k == 1: \n        shuffle_df = df_copy3.sample(frac = 1)\n        shuffle_df = shuffle_df.reset_index()\n        shuffle_df = shuffle_df.drop(columns = ['index'])\n        \n        #When we call the reset_index function, a new index column is added. The rows are still sorted.\n        #Hence we decided to drop the index column\n        \n        fold_one = shuffle_df[0:1460]\n        fold_two = shuffle_df[1460:]\n        \n        lr.fit(fold_one[cols],fold_one['SalePrice'])\n        predictions_one = lr.predict(fold_two[cols])\n        mse_one = mean_squared_error(fold_two['SalePrice'], predictions_one)\n        rmse_one = mse_one ** (1\/2)\n        \n        lr.fit(fold_two[cols], fold_two['SalePrice'])\n        predictions_two = lr.predict(fold_one[cols])\n        mse_two = mean_squared_error(fold_one['SalePrice'], predictions_two)\n        rmse_two = mse_two ** (1\/2)\n        \n        avg_rmse = np.mean([rmse_one,rmse_two])\n        \n        return avg_rmse\n    \n    else:\n        #if k is more than one, then we perform KFold cross validation\n        kf = KFold(n_splits = k, shuffle = True)\n        mses = cross_val_score(lr,df_copy3[cols], df_copy3['SalePrice'], scoring = 'neg_mean_squared_error', cv = kf)\n        rmses = np.sqrt(np.absolute(mses))\n        avg_rmse_k_fold = np.mean(rmses)\n        \n        return avg_rmse_k_fold\n        ","3832a404":"final_test_data = pd.read_table('\/kaggle\/input\/AmesHousing.tsv', delimiter='\\t')\ntransformed_data = tranform_features(final_test_data)","94e0459a":"selected_features_data = select_features(transformed_data, 0.3, 10)","06cf1fac":"for x in range(0,10,1): \n    print(train_and_test(selected_features_data,k = x))\n    ","a4b87c41":"So how do we use 'Pave' in our model. Well the first step is to convert them to category type variables. When we convert them to categorical data type, pandas automatically assigns a code to each unique value. ","9ce51f76":"_____________________________\n\n**Thank you for taking the time to read this notebook. This is but a learner's attempt at the vast and ever evolving field of Data Scince. I am sure there are ways that can make the model even more efficient or accurate. As I learn to make it better, I would highly appreciate any and all feedback. Your feedback is extremely valuable and will help me learn, grow and evolve into a better Data Science practitioner. **","4f3d0ca0":"Now that we have decided what columns to keep, how we convert them in a way that we can use them in our model. By themselves, these columns cannot be used because they object values. ","746e0685":"Why we used the orient keyword argument : \n\nDataFrame.to_dict(*args, **kwargs)\nConvert DataFrame to dictionary.\n\n**Parameters:**\t\norient : str {\u2018dict\u2019, \u2018list\u2019, \u2018series\u2019, \u2018split\u2019, \u2018records\u2019, \u2018index\u2019}\n\n**Determines the type of the values of the dictionary.**\n\ndict (default) : dict like {column -> {index -> value}}\n\nlist : dict like {column -> [values]}\n\nseries : dict like {column -> Series(values)}\n\nsplit : dict like {index -> [index], columns -> [columns], data -> [values]}\n\nrecords : list like [{column -> value}, ... , {column -> value}]\n\nindex : dict like {index -> {column -> value}}\n\n______","b0b454c1":"<h3> Training and Testing :","8b88389b":"<h3> Categorical columns shorlisting: ","3ee95986":"**Note there are different approaches to go about selecting the columns. We could have used boolean indexing directly and method chaining to make this code more efficient. My approach was to break it down as detailed as possible.**\n\nNow that we have a more managed set of missing values, we can go about imputing them with the most occuring value for that paritcular column. ","38abdf90":"<h2> Predicting House Prices using Linear Regression","28ab817f":"Let's set a cutoff value of unique values <= 10. Any columns with more than 10 unique values will result in a lengthy dummy code. ","96431788":"Before we say that our work is done. Pay close attention to the values. We have managed to convert objects to numericals, but what do our numericals really represent? We are breaking one of the core rules of Linear Regression, that it does not work on nominal type variables. \n\nFor the 'Street' column what do the number 1 and 0 represent. Exclusivity yes, but do they represent a difference ? No. To make sure the difference is between values is represented, we use dummy coding. \n\nDummy coding is essentially assigning a 0 or 1 to an absence or presence of a categorical feature. Let us see it in action. ","fd0ae6a4":"Now that we know our functions work, let us get to the crux of the model. What predictor variables are we going to select? We can answer this question using Feature Engineering. Feature engineering is the process of processing and creating new features. There a few guidelines that we can follow when performing feature engineering, but choosing the right features is still a matter of human ingenuity. \n\nIt is always wise to have a domain expert on board, with whom we can sit down and discuss the different features (features is the same as saying predictor variables). Do some features contribute more towards overall price than others? Can some features be dropped? How can we replace the missing values? All these questions can help increase the accuracy of our model drastically. \n\nSince we do not have a domain expert, let us rather rely on research and logic. The guidelines we want to follow are: \n\n- Any data frame with more than 5% missing values is dropped (not an ideal solution, but for the sake of understanding the model we will follow this guideline)\n- Remove any columns that leak information about the sale \n- Tranform categorical or nominal features into a proper format so that they can be used in our model. Linear Regression does not work on nominal variable types. \n- See if we create new features by combining other features","26df9d1e":"A great way to make the work flow efficient is to create a pipeline. Pipelines to explain it simply are processes \/ functions that are frequently run grouped together. Pipelines can be used to take in multiple inputs, process or transform features and return output. ","17647597":"In the last portion we will update the final function our pipeline, train_and_test(). While simple validation is a good idea when iterating over features, we want to experiment with different values of k. K is basically the number of folds or simply put sections of the data, that we assign to the training and testing set. So if k = 2, then the testing and training data are split as a 50\/50 (k = 2 can also be referred to as holdout validation technique). \n\nSo let us add an input of k to our function that let's us decide the k value. ","a6c87fd8":"See a problem here. The columns by itself are integer types. Our Linear Regression model cannot differentiate date times. So these values will counted as integer values by themselves like 1960, 1961 etc. These can affect the accuracy of our model since the values are not really representing the column.\n\nThe solution to create a column that will better capture the information. From what we can tell, the years till it was last remodeled is a factor that plays into the price. So let us create a column that will tell us the years since the house was last remodeled or any additions were done to it.  ","d2f1a3a0":"Remember categorical columns can contain missing data as well. So let us first check for that.","cab6e122":"We can now drop the 'Year Remod\/Added' and 'Year built' columns. ","a76ad0e8":"Before we begin towards the last phase of the model, training and testing, we first need to update the functions in our pipeline, namely transform_features() and select_features().","42453259":"What we are looking for is collinearity, when one feature is highly correlated with another. The heatmap tells us that the darker shades of red are areas of high correlation. The diagonal represents the correlation of columns with themselves, hence the values of 1. \n\nFrom the heatmap, we can identify that : \n\n- Garage Area and Garage Cars are highly correlated with each other \n- Gr Liv Area and TotRms AbvGrd are highly correlated with each other \n\nThe reason we want to remove columns that showcase collinearity is so that we can remove the risk of duplication which can in turn affect the predicatability of our model. ","881759a6":"We can convert categorical columns to a categorical data type. But the question is that are all categorical columns eligible to be converted to categorical data type?\n\nWe can safely say that all variables that can be classified as nominal (we can tell difference exists but can't tell the size or direction of the difference), are candiates to be converted. \n\nAnother factor that we need to look at is the amount of different values within the column. If a column has too many unique values, how would we go about inputting them into the model? On the other hand if a column has manageable unique values but one unique value occurs more than the others, does that not affect the variance of the distribution?","1b46ec36":"Let us have a quick look whether overall qual corresponds to the same value in overall cond.","17013d9e":"Let's decide on a correlation cutoff value of 0.3. We recommend you run multiple variations of cutoff values and test where our model stops improving. For now we will stick with 0.3.    ","6e3fa0f4":"Let us now move towards selecting numerical features. When choosing features to input into our model, there are factors that we base our decision on :\n\n- Correlation between features and the target column \n- Correlation between features \n\nLet us create a heatmap to check for correlation between our feature columns and the target variable.","df087420":"- Next let's look at the categorical columns","a9fd8d3e":"<h3> Missing Values : ","caa6ff32":"Our next step is to check the correlation of the feature columns with the target variable 'SalePrice'. ","9f720aa6":"We have pretty much finalized the numerical features we will be working with. Let us now focus on categorical columns.","84603e47":"These columns can now be dropped.","8b2e8d14":"We have a row that has value of -1. This can be inaccurate data. Let us drop this particular row. ","8930c789":"<h3> Which columns to select? ","fddcad8e":"<h3> Feature selection : ","bb6955a9":"The best option to deal with missing data is to go back to the original source and look to fill in the missing data. In our case that isn't feasible, so we have to decide whether to \n\n- Drop missing data\n- Impute missing values with another value \n\nBoth approaches have their cons. Dropping missing data reduces the sample size of the data, that in turns makes it less representative. Reducing the sample size means risking your model to be less accurate. \n\nImputing with let's say a summary statistic is in some terms a better option. But the risk we take here is that we may affect the ditribution of data. The Bsmt Qual column has 80 missing values. Replacing it with the most occuring value will definitely affect our distribution. (since we only have 2930 rows).\n\nThe choice is entirely upto us and how we feel we should go about things. A domain expert would help us alot in helping us choose an approach but since that is not an option, we would go with dropping rather than imputing. ","d787fa8e":"So we have cleaned the data for any missing values. But our work here is still not complete. The next step is see if we can combine or create new columns that would represent the data better. ","8d30d725":"By reading the documentation and looking at out dataset we have shortlisted columns that are candidates for being dropped : \n- Order (are basically the index numbers of the original sheet)\n- PID   (some sort of identification code)\n- Mo Sold (we are trying to predict prices. Unless we were accounting for inflation into our final price, the month the house was sold in is not important) \n- Yr Sold (again not relevant to our model) \n- Sale type \n- Sale condition","4ffdde15":"We will use the fillna method to fill in the missing values. Let's take a quick look at its documentation:\n\nDataFrame.fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)[source]\nFill NA\/NaN values using the specified method.\n\nParameters:\t\nvalue : scalar, dict, Series, or DataFrame\nValue to use to fill holes (e.g. 0), alternately a dict\/Series\/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict\/Series\/DataFrame will not be filled. This value cannot be a list.\n\nmethod : {\u2018backfill\u2019, \u2018bfill\u2019, \u2018pad\u2019, \u2018ffill\u2019, None}, default None\nMethod to use for filling holes in reindexed Series pad \/ ffill: propagate last valid observation forward to next valid backfill \/ bfill: use next valid observation to fill gap.\n\naxis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019}\nAxis along which to fill missing values.\n____\n\nIf you notice the values parameter only takes in a scalar, dict, Series or Dataframe. So let us create a dictionary where the key is the column name and the value is the mode of that column. We can then pass it to the fillna() method to replace our missing values. ","ea8ae23a":"- Let us first look at missing values. We set a cutoff point of 5%. So any columns with more than 5% of their data missing will be dropped.","0e99f837":"In this notebook we will look at what is commonly known as parameter machine learning. Unlike the common K-Neighbor model which is part of the instance based learning class, Linear Regression models output a mathematical function that best approximates the relationship between predictor variables and target variable. \n\nWe will be working with a dataset that has been commonly used (but is still very effective for gaining practice), Housing Data for the city of Ames, Iowa from 2006 to 2010. You can find read about the different columns and what data they represent [here](https:\/\/s3.amazonaws.com\/dq-content\/307\/data_description.txt).","520ad87a":"Let's take it step by step. Carefully looking at each column. the approach I ususally take is carefully noting down each column,its attributes, missing values etc. Then evaluating based on guidelines and logic to determine whether this column is an appropriate feature. The whole process cannot be captured in the notebook since it would make it too lengthy.\n\nLogic involves understanding what the column represents, its correlation with the target variable, its correlation with other features. ","990070b7":"No we can see that the values differ. If they were the same, we could have removed one of the columns since they represent the same data. Let's keep moving reading the data documentation to identify any columns that we can transform. ","d3880fa2":"To access the unique codes, we need to use the .cat accessor followed by the .codes accessor.","b15331ec":"Let us now convert all of the remaining categorical columns to their corresponding dummies and then delete the original columns. ","ccfe9880":"<h3> Feature Engineering : \n"}}