{"cell_type":{"5b4122c9":"code","5f70b15e":"code","5b44f84a":"code","a9e092a2":"code","8d08cd6c":"code","298d1103":"code","8c64ca49":"code","b8db2ab8":"code","f791041e":"code","b2c48840":"code","b3588ec4":"code","8c605da8":"code","22d419ec":"code","994edff7":"code","dd33d9cc":"code","71d5d0de":"code","cf6ef890":"code","80fb2d80":"code","fccc3b35":"code","740b8ab7":"code","3674eff3":"code","b0a8d33f":"code","2dffc7b1":"code","edd3243b":"code","6cd63dbb":"code","b9d18e65":"code","ef14bd85":"code","3672b399":"code","b8847d47":"code","ad6198e3":"code","76ec974b":"code","853a82f3":"code","8b3b972e":"code","57f372c3":"code","cba2455a":"code","3719ba50":"code","b80b724e":"code","740790cb":"code","4219ed6e":"code","48d4e2f7":"code","ce47979f":"code","526745bd":"code","737beb6c":"code","bd706956":"code","40c6a52b":"code","078974a3":"code","8f56d61c":"code","e6f3c252":"code","4c04d009":"code","d8dcc32a":"code","d072cf9a":"code","8c91a325":"code","549cf904":"code","dd148e3f":"code","28cd3daa":"code","47681a2f":"code","6356a8e7":"code","62ff0f87":"code","81fd8675":"code","fecc259c":"code","8f39fecd":"code","589c5319":"code","ad5eba66":"code","7f1c4c53":"code","a359f69f":"code","8de4ea4b":"code","9e4ca59e":"code","fc654a73":"code","0d8ca6aa":"markdown","128cb959":"markdown","d243c918":"markdown","c6a65053":"markdown","c16a6e90":"markdown","cfbe1320":"markdown","aa71a639":"markdown","a290e629":"markdown","333640b5":"markdown","ded48a99":"markdown","0a6bab0a":"markdown","5b1588bd":"markdown","8542fbd4":"markdown"},"source":{"5b4122c9":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score","5f70b15e":"os.listdir(\"..\/input\")","5b44f84a":"train_df = pd.read_csv(\"..\/input\/movie-reviews-classification\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/movie-reviews-classification\/test.csv\")","a9e092a2":"train_df.head()","8d08cd6c":"test_df.head()","298d1103":"print(f\"Datase shape: train: {train_df.shape}; test: {test_df.shape}\")","8c64ca49":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","b8db2ab8":"missing_data(train_df)","f791041e":"missing_data(test_df)","b2c48840":"def plot_features_distribution(features, title, df, isLog=False):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    for feature in features:\n        if(isLog):\n            sns.distplot(np.log1p(df[feature]),kde=True,hist=False, bins=120, label=feature)\n        else:\n            sns.distplot(df[feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()\n\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show() \n\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(feature,df):\n    data = df.loc[~df[feature].isnull(), feature].values\n    count = (~df[feature].isnull()).sum()\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    fig.suptitle(\"Prevalent words in {} ({} rows)\".format(feature,count), fontsize=20)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\ndef show_confusion_matrix(valid_y, predicted, size=1, trim_labels=False):\n    mat = confusion_matrix(valid_y, predicted)\n    plt.figure(figsize=(4*size, 4*size))\n    sns.set()\n    target_labels = np.unique(valid_y)\n    if(trim_labels):\n        target_labels = [x[0:70] for x in target_labels]\n    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n                xticklabels=target_labels,\n                yticklabels=target_labels\n               )\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()","b3588ec4":"show_wordcloud('primary_title', train_df)","8c605da8":"show_wordcloud('primary_title', test_df)","22d419ec":"show_wordcloud('original_title', train_df)","994edff7":"show_wordcloud('original_title', test_df)","dd33d9cc":"show_wordcloud('genres', train_df)","71d5d0de":"show_wordcloud('genres', test_df)","cf6ef890":"show_wordcloud('text', train_df)","80fb2d80":"show_wordcloud('text', test_df)","fccc3b35":"plot_features_distribution(['start_year', 'end_year'], 'Running years distribution (train)', train_df)","740b8ab7":"plot_features_distribution(['start_year', 'end_year'], 'Running years distribution (test)', test_df)","3674eff3":"plot_features_distribution(['runtime_minutes'], 'Runtime minutes distribution (train)', test_df)","b0a8d33f":"plot_features_distribution(['runtime_minutes'], 'Runtime minutes distribution (test)', test_df)","2dffc7b1":"print(f\"Movies in train: {train_df.title_id.nunique()} and test: {test_df.title_id.nunique()}\")\nl1 = set(train_df.title_id.unique())\nl2 = set(test_df.title_id.unique())\ncard_int = len(l1.intersection(l2))\nprint(f\"Common movies in train & test: {card_int}\")","edd3243b":"plot_count('is_adult', 'Adult movie (train)', train_df, size=1)","6cd63dbb":"plot_count('is_adult', 'Adult movie (test)', test_df, size=1)","b9d18e65":"data = pd.concat([train_df, test_df])","ef14bd85":"data['text_count'] = data['text'].apply(lambda x: len(x.split(' ')))","3672b399":"plot_features_distribution(['text_count'], 'Text words count (all data)', data)","b8847d47":"data.loc[data['end_year'].isna(), 'end_year'] =  data.loc[data['end_year'].isna(), 'start_year']","ad6198e3":"data['airing'] = data['end_year'] - data['start_year']","76ec974b":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n\"loulla\": \"sad\", \"loullas\": \"sad\", \"medictaon\": \"medication\", \"cannabidol\": \"cannabis\", \"eyedrop\": \"eye drop\",\n\"buttom\" : \"bottom\", \"zerocream\": \"zero cream\", \"bedwash\": \"bed wash\", \"tided\": \"tidied\", \"promted\": \"prompted\",\n\"subcut\": \"sub cut\", \"assited\": \"assisted\", \"upstair\": \"upstairs\", \"conotrane\": \"soothing cream\", \"ryvita\": \"rye bread\",\n\"movicol\": \"purgative\", \"frosties\": \"cereals\", \"finsh\": \"finish\", \"hoovered\": \"hovered\" , \"tangarines\": \"tangerines\", \n\"chocalata\": \"chocolate\", \"commod\": \"commode\", \"aggitated\": \"agitated\", \"enteracted\": \"interacted\", \"downstair\": \"downstairs\",\n\"weelchair\":\"wheelchair\", \"jermey\":\"jeremy\",  \"neice\":\"niece\", \"tangarine\":\"tangerine\",  \"enteracting\":\"interacting\", \n\"pijama\":\"pajamas\", \"toileted\":\"toileting\", \"mantained\":\"maintained\", \"lansoprazole\":\"prevacid\",  \"coffe\":\"coffee\", \n\"smoothy\":\"smoothie\",  \"inlaw\":\"in-laws\", \"catched\":\"caught\",  \"yougart\":\"yogurts\", \"youghut\": \"yogurts\",\"porredges\":\"porridge\",\n\"worktop\": \"table-top\", \"chocalate\": \"chocolate\", \"codamol\": \"codeine\", \"novorapid\": \"insulin\", \n\"reyadh\": \"riyadh\", \"thrn\": \"thorn\", \"promt\": \"prompt\", \"paulous\": \"paulus\", \"zimmerframe\": \"walkers\",\n\"proshield\": \"skin protect\", \"greated\": \"greeted\", \"colegue\": \"coleague\", \"fruite\": \"fruit\", \"asssited\": \"assisted\",\n\"abulaiton\": \"ablution\", \"abulution\":\"ablution\", \"reclimer\": \"recliner\", \"adviced\": \"advised\", \"handwash\": \"hand wash\", \"preapare\":\"prepare\",\n\"arithist\": \"arthritis\", \"comde\":\"commode\", \"stedy\":\"steady\", \"nightware\":\"pajamas\", \"minut\":\"minute\", \"clok\": \"clock\",\n \"erly\":\"early\", \"bowlsmovment\":\"bowels movement\", \"laxido\": \"laxative\", \"cosily\":\"cosy\", \"entrace\":\"entrance\", \n\"skirtings\":\"skirt\", \"dthe\": \"the\", \"walkjng\":\"walking\", \"reafing\":\"reading\", \"assistef\":\"assisted\", \"whashing\": \"washing\",\n\"algesal\":\"painkiller\", \"sitted\":\"seated\", \"evenimg\":\"evening\", \"complan\":\"complain\", \"incase\":\"in case\", \n\"guidence\":\"guidance\", \"apointment\":\"appointment\", \"eaaily\":\"easily\", \"wallking\":\"walking\", \"heractivity\":\"hyperactivity\", \n\"allready\":\"already\", \"intermitently\":\"intermittently\", \"enteract\":\"interact\", \"enteraction\":\"interaction\", \"asention\":\"attention\",                        \n\"asisted\":\"assisted\", \"chganed\":\"changed\", \"kichen\":\"kitchen\", \"backto\":\"back to\", \"pilows\":\"pillows\", \"albertha\":\"alberta\", \n\"sandwitch\":\"sandwich\", \"makeit\":\"make it\", \"nebuliser\":\"breathalyzer\", \"dorrett\": \"dorit\", \"coleague\": \"colleague\",\n\"uostair\":\"upstairs\", \"strenght\": \"strength\", \"branflakes\":\"cereals\", \"jyst\": \"just\", \"ibruphene\":\"ibuprofen\",\n\"comode\":\"commode\",\"wakens\":\"waken\", \"karanji\":\"pastry\", \"moisturising\":\"moisturize\", \"kreamoint\":\"ointment\",\n\"sudocrem\":\"ointment\",\"weat\":\"cool\", \"rubish\":\"rubbish\", \"incon\":\"\", \"dident\":\"didn't\", \"cetraben\":\"ointment\",\n\"alendronic\":\"medicine for osteoporosis\",\"weatbix\":\"cereals\", \"betnovate\":\"lotion\", \"afyer\":\"after\",\n\"specsaver\":\"optical retail chain\", \"appart\":\"apart\",  \"sinamet\":\"treatment for parkinson\", \"spaecially\":\"specially\",\n\"appart\":\"apart\", \"sudocrem\":\"ointment\",  \"lasagnia\":\"lasagna\",  \"assstance\":\"assistance\", \"apploed\":\"applied\",\n\"woth\":\"with\", \"catrione\":\"catherine\", \"shereen\":\"shared\",  \"fortisip\":\"medical yogurt\", \"paracetamols\":\"paracetamol\",\n\"houmus\":\"humus\", \"sudocream\":\"ointment\", \"fishpie\":\"fish pie\",\"christisna\":\"christiana\", \"wendylette\":\"satin sliding sheet\",\n\"osteoporosys\":\"osteoporosis\",\"tranfers\":\"transfers\",\"morethan\":\"more than\",\"ddnt\":\"didn't\",\"whent\":\"went\",\n\"admistered\":\"administered\",\"imto\":\"into\",\"gentlely\":\"gently\",\"ntibiotic\":\"antibiotic\",\n\"nitrofurantoin\":\"ntibiotic for bladder infections\",\"feaces\":\"feces\",\"vacumed\":\"\",\"comunicating\":\"communicating\",\n\"criying\":\"crying\",\"aggitation\":\"agitation\",\"chearful\":\"cheerful\",\"reassureance\":\"reassurance\",\"fewes\":\"fews\",\n\"sheeren\":\"shared\",\"shareen\":\"shared\",\"uptstair\":\"upstairs\", \"counldnt\":\"couldn't\",\"urin\":\"urine\", \"slowdwn\":\"slowdown\",\n\"marrieta\":\"marietta\", \"orenge\":\"orange\", \"lastnight\":\"last night\",  \"diahorhea\":\"diarrhea\", \"personla\":\"personal\",\n\"zerobase\":\"moisturizing cream\",\"alittle\":\"a little\",\"alwys\":\"always\",\"ambulanse\":\"ambulance\",\"olock\":\"door look\",\n\"porredge\":\"porridge\",\"hotchocolate\":\"hot chocolate\",\"tierd\":\"tired\",\"oramorph\":\"morphine\",\"comay\":\"ointment\",\n\"admistrated\":\"administered\", \"dosege\":\"dosage\", \"duvett\":\"bed cover\", \"vesit\":\"visit\", \"eate\":\"eat\", \n\"imodium\":\"diarrhea medicine\",\"iput\":\"input\", \"pyama\":\"pyjamas\", \"shoues\":\"shoes\", \"whem\":\"when\", \"puding\":\"pudding\",\n\"hoited\":\"hoisted\", \"adasa\":\"\", \"mmols\":\"\", \"bshe\":\"\", \"nowvuts\":\"\", \"cean\":\"\", \"vwas\":\"\", \"adesa\":\"\", \"adasa\":\"\",\"rosys\":\"\",\n\"nyself\":\"myself\", \"apoears\":\"appears\",  \"promtped\":\"prompted\", \"tomatoe\":\"tomatoes\", \"omlette\":\"omelette\", \n\"sssisted\":\"asssisted\", \"recipt\":\"receipt\",\"viennetta\":\"ice cream\",\"medicatiom\":\"medication\",\"cefalexin\":\"antibiotic \",\n\"famiky\":\"family\", \"emolient\":\"moisturizing\", \"littlw\":\"little\", \"merveilleux\":\"beautiful\", \"melven\":\"cosmetics\",\n\"progabolin\":\"medicine for epilepsy\", \"carbocestine\": \"medicine for lungs\", \"asssisted\":\"assisted\", \"patrcia\":\"patricia\",\n\"hevis\":\"\", \"bisoprolol\":\"medicine for high blood pressure\", \"dden\":\"\",\"noice\":\"noise\", \"andshe\":\"and she\", \"tiday\":\"today\"                       \n}\n\npuncts = {\"\u2018\": \"'\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', '\u2026': ' '}\npunct_mapping = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\npunct_mapping += '\u00a9^\u00ae` <\u2192\u00b0\u20ac\u2122\u203a \u2665\u2190\u00d7\u00a7\u2033\u2032\u00c2\u2588\u00bd\u00e0\u2026\u201c\u2605\u201d\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a'","853a82f3":"def clean_contractions(text, mapping):\n    '''\n    input: current text, contraction mappings\n    output: modify the comments to use the base form from contraction mapping\n    '''\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef clean_special_chars(text, punct=punct_mapping, mapping=puncts):\n    '''\n    input: current text, punctuations, punctuation mapping\n    output: cleaned text\n    '''\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ') \n    return text","8b3b972e":"import gensim\nimport re\nimport time\n\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nen_stop = set(nltk.corpus.stopwords.words('english'))\n\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n            result.append(token)     \n    return result\n\n#def tokenize(text):\n#    return word_tokenize(text)\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n    \ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)\n\ndef prepare_text(tokens):\n    tokens = [re.sub(r'[^a-zA-Z\\s]', '', token, re.I|re.A) for token in tokens]\n    tokens = [token.lower() for token in tokens if len(token) > 2]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [get_lemma2(token) for token in tokens]\n    return ' '.join(tokens)\n","57f372c3":"data['proc_text'] = data['text'].apply(lambda x: clean_special_chars(x))\ndata['proc_text'] = data['proc_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ndata['proc_text'] = data['proc_text'].apply(lambda x: preprocess(x))\ndata['proc_text'] = data['proc_text'].apply(lambda x: prepare_text(x))","cba2455a":"data['proc_text'][0:10]","3719ba50":"MAX_FEATURES = 200000\ndef tokenize(texts):\n    tokenizer = Tokenizer(num_words=MAX_FEATURES)\n    tokenizer.fit_on_texts(texts)\n    word_index = tokenizer.word_index\n    print(f\"Found {len(word_index)} unique tokens.\")\n    return word_index, tokenizer\n\ndef load_embeddings(file):\n    \"\"\"\n    input: embeddings file\n    output: embedding index\n    \"\"\"\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index\n\n\ndef prepare_embeddings():\n    emb_glove = load_embeddings('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\n    print(f'Embeddings size: {len(emb_glove)}')\n    return emb_glove\n\ndef build_embedding_matrix(word_index, emb_glove):\n    print('build embedding matrix')\n    embeddings_index = emb_glove\n    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n","b80b724e":" word_index, tokenizer = tokenize(data['proc_text'])","740790cb":"def count_vect_feature(feature, df, max_features=750000):\n    start_time = time.time()\n    cv = CountVectorizer(max_features=max_features,\n                             ngram_range=(1, 5),\n                             stop_words='english')\n    X_feature = cv.fit_transform(df[feature])\n    print('Count Vectorizer `{}` completed in {} sec.'.format(feature, round(time.time() - start_time,2)))\n    return X_feature\n\ndef tfidf_feature(feature, df, max_features=25000):\n    start_time = time.time()\n    tfidf = TfidfVectorizer(max_features=max_features,\n                             ngram_range=(1, 3),\n                             stop_words='english')\n    X_feature = tfidf.fit_transform(df[feature])\n    print('TFIDF `{}` completed in {} sec.'.format(feature, round(time.time() - start_time,2)))\n    return X_feature\n","4219ed6e":"data.columns.values","48d4e2f7":"X_text_feature = count_vect_feature('proc_text', data, max_features=750000)","ce47979f":"#X_text_tfidf_feature = tfidf_feature('proc_text', data, max_features=50000)","526745bd":"def handle_missing_inplace(dataset):\n    dataset['genres'].fillna(value='missing', inplace=True)\n    dataset['is_adult'].fillna(value=0, inplace=True)\n    dataset['original_title'].fillna(value='missing', inplace=True)\n    dataset['primary_title'].fillna(value='missing', inplace=True)\n    dataset['runtime_minutes'].fillna(value=0, inplace=True)\n    dataset['start_year'].fillna(value=0, inplace=True)\n    dataset['end_year'].fillna(value=0, inplace=True)","737beb6c":"handle_missing_inplace(data)","bd706956":"X_original_title_feature = count_vect_feature('original_title', data, max_features=20000)","40c6a52b":"X_primary_title_feature = count_vect_feature('primary_title', data, max_features=20000)","078974a3":"X_dummies = csr_matrix(pd.get_dummies(data[['is_adult']],sparse=True).values)","8f56d61c":"X_numeric = csr_matrix(data[['text_count', 'runtime_minutes', 'airing']].values)","e6f3c252":"X_genres_feature = count_vect_feature('genres', data, max_features=15000)","4c04d009":"t_svd = TruncatedSVD(n_components = 50)","d8dcc32a":"X_tsvd_text_feature = t_svd.fit_transform(X_text_feature)","d072cf9a":"X_tsvd_genres_feature = t_svd.fit_transform(X_genres_feature)\nX_tsvd_primary_title_feature = t_svd.fit_transform(X_primary_title_feature)\nX_tsvd_original_title_feature = t_svd.fit_transform(X_original_title_feature)","8c91a325":"sparse_merge = hstack((X_tsvd_text_feature, X_dummies, X_numeric, X_tsvd_genres_feature, \\\n                       X_tsvd_primary_title_feature, X_tsvd_original_title_feature, )).tocsr()","549cf904":"X = sparse_merge[0:train_df.shape[0]]\nX_test = sparse_merge[train_df.shape[0]:]\ny = train_df.polarity.values\nprint(f\"X: {X.shape} test X: {X_test.shape} y: {y.shape}\")\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state = 42) \nprint(f\"train X: {train_X.shape}, valid X: {valid_X.shape}, train y: {train_y.shape}, valid y: {valid_y.shape}\")","dd148e3f":"sparse_merge_all = hstack((X_text_feature, X_dummies, X_numeric, X_genres_feature, \\\n                       X_primary_title_feature, X_original_title_feature, )).tocsr()\nX_nb = sparse_merge_all[0:train_df.shape[0]]\nX_nb_test = sparse_merge_all[train_df.shape[0]:]\ny_nb = train_df.polarity.values\nprint(f\"X: {X.shape} test X: {X_test.shape} y: {y.shape}\")\ntrain_nb_X, valid_nb_X, train_nb_y, valid_nb_y = train_test_split(X_nb, y_nb, test_size = 0.2, random_state = 42) \nprint(f\"train X: {train_X.shape}, valid X: {valid_X.shape}, train y: {train_y.shape}, valid y: {valid_y.shape}\")\nclf_nb = MultinomialNB(fit_prior=True)\nclf_nb.fit(train_nb_X, train_nb_y)\npredicted_valid_nb = clf_nb.predict(valid_nb_X)\nshow_confusion_matrix(valid_nb_y, predicted_valid_nb, size=1)\nprint(classification_report(valid_nb_y, predicted_valid_nb))","28cd3daa":"clf_svc = SVC(kernel='linear')\nclf_svc.fit(train_X, train_y)\nprint('Train SVC completed')","47681a2f":"predicted_valid_svc = clf_svc.predict(X=valid_X)\nprint('Predict SVC completed')\nshow_confusion_matrix(valid_y, predicted_valid_svc, size=1)\nprint(classification_report(valid_y, predicted_valid_svc))\nprint(f\"ROC-AUC: {roc_auc_score(predicted_valid_svc, valid_y)}\")","6356a8e7":"clf_lr = LogisticRegression()\nclf_lr.fit(train_X, train_y)\nprint('Train LR completed')","62ff0f87":"predicted_valid_lr = clf_lr.predict(X=valid_X)\nprint('Predict LR completed')\nshow_confusion_matrix(valid_y, predicted_valid_lr, size=1)\nprint(classification_report(valid_y, predicted_valid_lr))\nprint(f\"ROC-AUC: {roc_auc_score(predicted_valid_lr, valid_y)}\")","81fd8675":"predicted_valid = 0.3 * predicted_valid_nb + 0.3 * predicted_valid_svc + 0.4 * predicted_valid_lr\n#print(classification_report(valid_y, predicted_valid))\n#print(f\"ROC-AUC (blending): {roc_auc_score(predicted_valid, valid_y)}\")","fecc259c":"predicted_valid[0:30]","8f39fecd":"predict_test_nb = clf_nb.predict(X_test)","589c5319":"predict_test_svc = clf_svc.predict(X_test)","ad5eba66":"predict_test_lr = clf_lr.predict(X_test)","7f1c4c53":"predict_test = 0.3 * predict_test_nb + 0.3 * predict_test_svc + 0.4 * predict_test_lr","a359f69f":"submission = pd.read_csv('..\/input\/movie-reviews-classification\/sampleSubmission.csv')\nsubmission['polarity'] = predict_test\nsubmission.to_csv('submission.csv', index=False)","8de4ea4b":"submission['polarity'] = predict_test_svc\nsubmission.to_csv('submission_svc.csv', index=False)","9e4ca59e":"submission['polarity'] = predict_test_lr\nsubmission.to_csv('submission_lr.csv', index=False)","fc654a73":"submission['polarity'] = predict_test_nb\nsubmission.to_csv('submission_nb.csv', index=False)","0d8ca6aa":"# Read the data","128cb959":"## Text pre-processing","d243c918":"## Alternative Model - SCV\n\nWe use as an alternative model SVC.","c6a65053":"# Further improvements\n\n* Pre-process the text data: clean contraction, clean special characters, eliminate stop words, use lematization or stemming;\n* Use different text vectorizing options;\n* Use other models;\n* Add additional features (categorical or numerical);\n* Perform model tuning\n","c16a6e90":"Missing data are ~5% of `end_year` in both train and test as well as ~1% `runtime_minutes`.","cfbe1320":"# Load packages","aa71a639":"# Model\n\nWe create here a more complex model, with multiple features.","a290e629":"## Check the data","333640b5":"## Alternative model - logistic regression","ded48a99":"# Data exploration","0a6bab0a":"## Missing data","5b1588bd":"# Submission\n\nLet's predict the polarity for the test set.","8542fbd4":"# Features engineering\n\nLet's create few additional features."}}