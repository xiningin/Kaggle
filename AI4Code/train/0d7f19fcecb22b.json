{"cell_type":{"dfd0ec09":"code","c4884d9b":"code","cebb8a2a":"code","abfb2b81":"code","f970ba89":"code","467e61c6":"code","3e34cd90":"code","32d2daff":"code","e1b51887":"code","51ef4ae6":"code","537ae069":"code","f0acf1d0":"code","05a3ae1e":"code","82151d47":"code","aee19f2b":"code","2409acd1":"code","664df031":"code","3bc635a3":"code","c793beed":"code","51efb43a":"code","cdff68d0":"code","3363f94a":"code","8e623b7a":"code","132d2ccf":"code","174c2c0c":"code","c5e9b54f":"code","5894fef1":"code","0531bfdf":"code","bbec1d67":"code","49ed5d84":"code","99ae9fd1":"code","9e19b219":"markdown","d03178ef":"markdown","22d9580e":"markdown","cfcd17a2":"markdown","f8159c0f":"markdown","7a754188":"markdown","01a3fbf9":"markdown","d09e4b62":"markdown","630e8c93":"markdown","fe9a860c":"markdown","dd50eace":"markdown","e02f36a7":"markdown","2285db4d":"markdown","7006784d":"markdown","05d8d405":"markdown","2d054c48":"markdown","10561f83":"markdown","f7dd2287":"markdown","34006dca":"markdown","d71eba11":"markdown","784bc14c":"markdown","a275855c":"markdown","94499d2b":"markdown","19a246c7":"markdown","1c5058d2":"markdown","f9a3b0ce":"markdown"},"source":{"dfd0ec09":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","c4884d9b":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()\ntrain.describe()","cebb8a2a":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","abfb2b81":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 6))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"palevioletred\", \"lightsteelblue\"],\n             textprops={\"fontsize\": 14},\n             autopct='%1.1f%%',\n             pctdistance=0.5)\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=14)\nfig.set_facecolor('white')\nplt.show();","f970ba89":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=14, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","467e61c6":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","3e34cd90":"df = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values","32d2daff":"# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=11)\n                                  \n        i+=1","e1b51887":"df = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values","51ef4ae6":"# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"cornflowerblue\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n            \n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","537ae069":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"cornflowerblue\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"salmon\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","f0acf1d0":"import seaborn as sns\n\ndf = train\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","05a3ae1e":"import random\n\n# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\ncolumns = train.drop([\"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n# plt.suptitle(\"Features vs target\", y=0.99)\nplt.show();","82151d47":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","aee19f2b":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","2409acd1":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nSEED = 7770777\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n\nlgb_parameters = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.007899156646724397,\n    \"num_leaves\": 77,\n    \"max_depth\": 77,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 9.562925363678952,\n    \"reg_lambda\": 9.355810045480153,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    \"bagging_seed\": SEED,\n    \"feature_fraction_seed\": SEED,\n    \"seed\": SEED\n}","664df031":"predictions_lgb = pd.DataFrame()\n\ncat_columns = [f\"cat{i}\" for i in range(10)]\n\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nfor k, (tr_id, vl_id) in enumerate(kf.split(X, y)):\n    print(\"=\"*50)\n    print(f\"               KFold{k+1}\")\n    print(\"=\"*50)\n    \n    X_train, X_valid = X.iloc[tr_id, :], X.iloc[vl_id, :]\n    y_train, y_valid = y.iloc[tr_id], y.iloc[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_valid, y_valid)\n    \n    model_lgb = lgb.train(params=lgb_parameters,\n                          train_set=lgb_train,\n                          valid_sets=lgb_val,\n                          num_boost_round=100000,\n                          early_stopping_rounds=200,\n                          verbose_eval=1000)\n    \n    prediction_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n    prediction_lgb = pd.DataFrame(prediction_lgb)\n    \n    predictions_lgb = pd.concat([predictions_lgb, prediction_lgb], axis=1)","3bc635a3":"#model_lgbm = LGBMRegressor(**lgbm_parameters)\n#model_lgbm.fit(X_train, y_train,\n#          eval_set = ((X_valid,y_valid)),\n#          verbose = -1,\n#          early_stopping_rounds = 50,\n#          categorical_feature=object_cols)  \n#preds_valid_lgbm = model_lgbm.predict(X_valid)\n#print(mean_squared_error(y_valid, preds_valid_lgbm, squared=False))","c793beed":"predictions_lgb.head()","51efb43a":"predictions_lgb_mean = predictions_lgb.mean(axis=1)","cdff68d0":"# Fit the model\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","3363f94a":"from xgboost import XGBRegressor\n\n# Define the model's hyperparameters\nxgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.35,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': 4}","8e623b7a":"model_XGB = XGBRegressor(n_estimators=1000, learning_rate=0.05) \nmodel_XGB.fit(X_train, y_train) # Your code here\npredictions_XGB = model_XGB.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB, squared=False))","132d2ccf":"from catboost import CatBoostRegressor\n\nmodel_cat = CatBoostRegressor(\n                          iterations=6800,\n                          learning_rate=0.93,\n                          loss_function=\"RMSE\",\n                          random_state=42,\n                          verbose=0,\n                          thread_count=4,\n                          depth=1,\n                          l2_leaf_reg=3.28,\n                         )\nmodel_cat.fit(X_train, y_train)\npreds_valid_cat = model_cat.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid_cat, squared=False))","174c2c0c":"# Define the model \nmodel_RF = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel_RF.fit(X_train, y_train)\npreds_RF_valid = model_RF.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_RF_valid, squared=False))","c5e9b54f":"# Use the model to generate predictions\n#preds_valid_1 = model_lgb.predict(X_valid)\npreds_valid_2 = model_XGB.predict(X_valid)\npreds_valid_3 = model_cat.predict(X_valid)\npreds_valid = preds_valid_2*0.4 + preds_valid_3*0.6\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","5894fef1":"#valid_lgb = pd.DataFrame(valid_lgb)\ncat_columns = [f\"cat{i}\" for i in range(10)]\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\n# Use the model to generate predictions\npreds_valid_1 = model_lgb.predict(X_valid)\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\npreds_valid_2 = model_XGB.predict(X_valid)\npreds_valid_3 = model_cat.predict(X_valid)\npreds_valid = preds_valid_1*0.5 + preds_valid_2*0.2 + preds_valid_3*0.3\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","0531bfdf":"#valid_lgb = pd.DataFrame(valid_lgb)\ncat_columns = [f\"cat{i}\" for i in range(10)]\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\n# Use the model to generate predictions\npreds_valid_1 = model_lgb.predict(X_valid)\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\npreds_valid_2 = model_XGB.predict(X_valid)\npreds_valid_3 = model_cat.predict(X_valid)\npreds_valid = preds_valid_1*0.75 + preds_valid_2*0.1 + preds_valid_3*0.15\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","bbec1d67":"#valid_lgb = pd.DataFrame(valid_lgb)\ncat_columns = [f\"cat{i}\" for i in range(10)]\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\n# Use the model to generate predictions\npreds_valid_1 = model_lgb.predict(X_valid)\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\npreds_valid_2 = model_XGB.predict(X_valid)\npreds_valid_3 = model_cat.predict(X_valid)\npreds_valid = preds_valid_1*0.78 + preds_valid_2*0.1 + preds_valid_3*0.12\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","49ed5d84":"# Use the model to generate predictions\npredictions_1 = predictions_lgb_mean\npredictions_2 = model_XGB.predict(X_test)\npredictions_3 = model_cat.predict(X_test)\n\npredictions = predictions_1*0.78 + predictions_2*0.1 + predictions_3*0.12\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","99ae9fd1":"#valid_lgb = pd.DataFrame(valid_lgb)\ncat_columns = [f\"cat{i}\" for i in range(10)]\nX[cat_columns] = X[cat_columns].astype(\"category\")\nX_test[cat_columns] = X_test[cat_columns].astype(\"category\")\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\n# Use the model to generate predictions\npredictions_1 = model_lgb.predict(X_test)\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n\npredictions_2 = model_XGB.predict(X_test)\npredictions_3 = model_cat.predict(X_test)\npredictions = predictions_1*0.78 + predictions_2*0.1 + predictions_3*0.12\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission_output.csv', index=False)","9e19b219":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","d03178ef":"## Step 5.4 Random Forest","22d9580e":"### Comparing the datasets length","cfcd17a2":"# Step 4: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","f8159c0f":"Next, we break off a validation set from the training data.","7a754188":"As you can see, target column is very weakly correlated with all features.\n\n#### Let's visualize each feature vs target.","01a3fbf9":"# Step 5: Predict using LightGBM, XGBoost, and CatBoost","d09e4b62":"### Target Distribution","630e8c93":"### Lists of categorical and numerical feature columns","fe9a860c":"#### Calculate the average of the predictions to get the final prediction.","dd50eace":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\nStanding on the shoulder of others (Refrences):\n* https:\/\/www.kaggle.com\/maximkazantsev\/30dml-eda-simple-catboost\n* https:\/\/www.kaggle.com\/charmingmichelleluo\/lightgbm\/\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","e02f36a7":"# Step 3: Exploratory Data Analysis\n\nNow, let's learn more about the data.","2285db4d":"### Plot of all numerical feature values distribution","7006784d":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","05d8d405":"## Step 5.2 XGBoost (Best is: rmse = 0.723090)","2d054c48":"### Combined dataframe ocnaining categorical feature only","10561f83":"## Step 5.1 LightGBM (Best iteration is: rmse = 0.715778)","f7dd2287":"### Calculatin correlation values","34006dca":"### Combined dataframe containing numerical feature only","d71eba11":"### Let's check if the datasets have different amount of categories in categorical features.\n","784bc14c":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","a275855c":"## Step 5.3 Simple CatBoost (Best is: rmse = 0.720703)","94499d2b":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","19a246c7":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","1c5058d2":"#### Reset the Dataset","f9a3b0ce":"### Plot of all numerical feature values distribution"}}