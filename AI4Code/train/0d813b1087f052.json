{"cell_type":{"c0bb0d0b":"code","1625711e":"code","f946ebcf":"code","7206e5bd":"code","f0440714":"code","ffd3475e":"code","723ba287":"code","80d7e48c":"code","4f60ed2d":"code","c4764c81":"code","33d37b28":"code","0bb7e5c1":"code","aea9c3ce":"code","e4118879":"code","5eea019e":"code","240a4d51":"code","f2975f46":"code","3477068c":"code","74b9bda0":"code","1e508e09":"code","69f196fa":"code","ab8405ea":"code","885a2266":"code","a7ebb253":"code","2bd86ffb":"code","1d72ef08":"code","c8aaedef":"code","7e50d5db":"code","3c51d308":"code","42c0c94a":"code","b03858cd":"code","f5ef2629":"code","0e2eae18":"code","cfe9f49d":"code","d3be3e37":"code","f1466037":"code","4150cb65":"code","691c0ef6":"markdown","0f7763fd":"markdown","35354d92":"markdown","67f16fa7":"markdown"},"source":{"c0bb0d0b":"# General imports\nimport os\nimport cv2\nimport glob \nimport json\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import *\nfrom sklearn.tree import *\nfrom sklearn.impute import *\nfrom sklearn.metrics import *\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\nfrom sklearn.decomposition import *\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\n\n\nwarnings.filterwarnings('ignore')\n\nSEED = 42\nnp.random.seed(SEED)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\n\nsns.set_style(\"white\")\nmpl.rcParams['figure.dpi'] = 200\n%matplotlib inline","1625711e":"data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\n\ndata.head()","f946ebcf":"data.describe()","7206e5bd":"TARGET = 'claim'\nIMPUTE_STRATEGY = 'mean'","f0440714":"class Preprocessor:\n\n    def __init__(self, pps):\n        self.pps = pps\n    \n    def fit(self, feat):\n        for pp in self.pps:\n            feat = pp.fit_transform(feat)\n    \n    def transform(self, feat):\n        for pp in self.pps:\n            feat = pp.transform(feat)\n        return feat\n    \n    def fit_transform(self, feat):\n        self.fit(feat)\n        return self.transform(feat)\n\npreprocessor = Preprocessor(pps=[SimpleImputer(missing_values=np.nan, strategy=IMPUTE_STRATEGY),\n                                StandardScaler()])","ffd3475e":"X = data.drop(columns=['id', TARGET])\nY = data[TARGET]\n\nX['mv_row'] = X.isna().sum(axis=1)\nX['min_row'] = X.min(axis=1)\nX['std_row'] = X.std(axis=1)\n\nX = preprocessor.fit_transform(X)","723ba287":"X.shape, Y.shape","80d7e48c":"X_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\nX_test.head()","4f60ed2d":"ids = X_test['id']\nX_test = X_test.drop(columns=['id'])\n\nX_test['mv_row'] = X_test.isna().sum(axis=1)\nX_test['min_row'] = X_test.min(axis=1)\nX_test['std_row'] = X_test.std(axis=1)\n\nX_test = preprocessor.transform(X_test)\n\nX_test.shape","c4764c81":"import optuna\nfrom optuna.samplers import TPESampler\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)","33d37b28":"from xgboost import XGBClassifier\n\n# Setup XGB hyperparameters for exps\ndef get_xgb_hyperparams(trail):\n    xgb_params = {\n        'learning_rate': 0.01,\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'n_estimators': trail.suggest_int('n_estimators', 500, 4000, 100),\n        'reg_lambda': trail.suggest_int('reg_lambda', 1, 100),\n        'reg_alpha': trail.suggest_int('reg_alpha', 1, 100),\n        'subsample': trail.suggest_float('subsample', 0.2, 1.0, step=0.1),\n        'colsample_bytree': trail.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n        'max_depth': trail.suggest_int('max_depth', 3, 10), \n        'min_child_weight': trail.suggest_int('min_child_weight', 2, 10),\n        'gamma': trail.suggest_float('gamma', 0, 20)        \n    }\n    return xgb_params","0bb7e5c1":"# Define objective function\ndef objective_xgb(trail, X, Y, n_splits=5):\n       \n    xgb_params = get_xgb_hyperparams(trail)\n    \n    skfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_preds, total_y = [], []\n    \n    for train_index, val_index in skfolds.split(X, Y):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        xgb_clf = XGBClassifier(**xgb_params)\n    \n        xgb_clf = xgb_clf.fit(x_train, y_train)\n        preds = xgb_clf.predict_proba(x_val)\n        \n        total_preds.extend(preds[:, 1])\n        total_y.extend(y_val)\n    \n    ra_score = roc_auc_score(total_y, total_preds)\n    \n    return ra_score","aea9c3ce":"# Callback function to print log messages when the best trail is updated\n\ndef logging_callback(study, frozen_trail):\n    prev_best = study.user_attrs.get('prev_best', None)\n    if prev_best != study.best_value:\n        study.set_user_attr('prev_best', study.best_value)\n        print(f\"Trail {frozen_trail.number} finished with best value {frozen_trail.value}\")","e4118879":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='maximize', \n                            study_name='xgb_tuning')\nobjc = lambda trail : objective_xgb(trail, X, Y)\n\nstudy.optimize(objc, timeout=60*60, callbacks=[logging_callback])","5eea019e":"print(f\"Best roc_auc value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","240a4d51":"def cross_validate_model(class_name, class_params, X, Y, X_test, n_splits=5):\n    \n    skfolds = StratifiedKFold(n_splits=n_splits, shuffle=False)\n    \n    oof_preds, oof_y = [], []\n    \n    test_preds = np.zeros((X_test.shape[0]))\n    \n    for i, (train_index, val_index) in enumerate(skfolds.split(X, Y)):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n        \n        print(f\"---------- Fold {i+1} Started ----------\")\n        clf = class_name(**class_params)\n    \n        clf = clf.fit(x_train, y_train)\n        preds = clf.predict_proba(x_val)\n        \n        oof_preds.extend(preds[:, 1])\n        oof_y.extend(y_val)\n        \n        test_preds += clf.predict_proba(X_test)[:, 1]\n        \n        ra_score = roc_auc_score(y_val, preds[:, 1])\n    \n        print(f\"ROC AUC of current fold is {ra_score}\")\n        \n        print(f\"---------- Fold {i+1} Ended ----------\")\n    \n    return oof_preds, test_preds \/ n_splits","f2975f46":"# These parameters are obtained using above optimization\nxgb_params = {\n    'n_estimators' : 3600,\n    'reg_lambda' : 3,\n    'reg_alpha' : 26,\n    'subsample' : 0.6000000000000001,\n    'colsample_bytree' : 0.6000000000000001,\n    'max_depth' : 9,\n    'min_child_weight' : 5,\n    'gamma' : 13.054739572819486,\n}\nxgb_params['learning_rate'] = 0.01\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['booster'] = 'gbtree'\n\nxgb_oof, xgb_test = cross_validate_model(XGBClassifier, xgb_params, X, Y, X_test)","3477068c":"df = pd.DataFrame({\n    'id': data['id'],\n    'oof_preds': xgb_oof\n})\n\ndf.to_csv('xgb_oof.csv', index=False)\n\ndf = pd.DataFrame({\n    'id': ids,\n    'claim': xgb_test\n})\n\ndf.to_csv('xgb_preds.csv', index=False)","74b9bda0":"from catboost import CatBoostClassifier\n\n# Setup CatB hyperparameters for exps\ndef get_catb_hyperparams(trail):\n    catb_params = {\n        'loss_function': 'CrossEntropy',\n        'task_type': 'GPU',\n        'bootstrap_type': 'Bernoulli',\n        'iterations': trail.suggest_int('iterations', 2000, 20000),\n        'od_wait': trail.suggest_int('od_wait', 500, 2000),\n        'learning_rate': trail.suggest_uniform('learning_rate', 0.01, 0.3),\n        'reg_lambda': trail.suggest_uniform('reg_lambda', 1e-4, 100),\n        'subsample': trail.suggest_uniform('subsample', 0, 1),\n        'random_strength': trail.suggest_uniform('random_strength', 10, 50),\n        'depth': trail.suggest_int('depth', 1, 15),\n        'min_data_in_leaf': trail.suggest_int('min_data_in_leaf', 1, 30),\n        'leaf_estimation_iterations': trail.suggest_int('leaf_estimation_iterations', 1, 15)\n    }\n    return catb_params","1e508e09":"# Define objective function\ndef objective_catb(trail, X, Y, n_splits=5):\n    \n    catb_params = get_catb_hyperparams(trail)\n    \n    skfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_preds, total_y = [], []\n    \n    for train_index, val_index in skfolds.split(X, Y):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        catb_clf = CatBoostClassifier(**catb_params)\n    \n        catb_clf = catb_clf.fit(x_train, y_train)\n        preds = catb_clf.predict_proba(x_val)\n        \n        total_preds.extend(preds[:, 1])\n        total_y.extend(y_val)\n    \n    ra_score = roc_auc_score(total_y, total_preds)\n    \n    return ra_score","69f196fa":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='maximize', \n                            study_name='catb_tuning')\nobjc = lambda trail : objective_catb(trail, X, Y)\n\nstudy.optimize(objc, timeout=60*15, callbacks=[logging_callback])","ab8405ea":"print(f\"Best ROC AUC value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","885a2266":"# These parameters are obtained using above optimization\ncatb_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\ncatb_oof, catb_test = cross_validate_model(CatBoostClassifier, catb_params, X, Y, X_test)","a7ebb253":"df = pd.DataFrame({\n    'id': data['id'],\n    'oof_preds': catb_oof\n})\n\ndf.to_csv('catb_oof.csv', index=False)\n\ndf = pd.DataFrame({\n    'id': ids,\n    'claim': catb_test\n})\n\ndf.to_csv('catb_preds.csv', index=False)","2bd86ffb":"from lightgbm import LGBMClassifier\n\n# Setup lgbm hyperparameters for exps\ndef get_lgbm_hyperparams(trail):\n    lgbm_params = {\n        \"objective\": \"binary\",\n        \"learning_rate\": 0.008,\n        'device': 'gpu',\n        'n_estimators': trail.suggest_int(\"n_estimators\", 500, 5000),\n        \"num_leaves\": trail.suggest_int(\"num_leaves\", 8, 256),\n        \"min_child_samples\": trail.suggest_int(\"min_child_samples\", 2, 3000),\n        'feature_fraction': trail.suggest_uniform('feature_fraction', 0.25, 0.7),\n        'bagging_fraction': trail.suggest_uniform('bagging_fraction', 0.7, 1.0),\n        'bagging_freq': trail.suggest_int('bagging_freq', 0, 5),\n        'reg_alpha': trail.suggest_int(\"reg_alpha\", 1, 100),\n        'reg_lambda': trail.suggest_int(\"reg_lambda\", 1, 100),\n    }\n    return lgbm_params","1d72ef08":"# Define objective function\ndef objective_lgbm(trail, X, Y, n_splits=5):\n    \n    lgbm_params = get_lgbm_hyperparams(trail)\n    \n    skfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    total_preds, total_y = [], []\n    \n    for train_index, val_index in skfolds.split(X, Y):\n        x_train, x_val = X[train_index], X[val_index]\n        y_train, y_val = Y[train_index], Y[val_index]\n    \n        lgbm_clf = LGBMClassifier(**lgbm_params)\n    \n        lgbm_clf = lgbm_clf.fit(x_train, \n                                y_train,\n                                eval_metric='auc',\n                                eval_set=[(x_val, y_val)],\n                                verbose=1000)\n        preds = lgbm_clf.predict_proba(x_val)\n        \n        total_preds.extend(preds[:, 1])\n        total_y.extend(y_val)\n    \n    ra_score = roc_auc_score(total_y, total_preds)\n    \n    return ra_score","c8aaedef":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=SEED), \n                            direction='maximize', \n                            study_name='lgbm_tuning')\nobjc = lambda trail : objective_lgbm(trail, X, Y)\n\nstudy.optimize(objc, timeout=60*60, callbacks=[logging_callback])","7e50d5db":"print(f\"Best ROC AUC value: {study.best_value}\")\nprint(f\"Best params: \")\nfor param, value in study.best_params.items():\n    print(f\"\\t{param} : {value}\")","3c51d308":"# These parameters are obtained using above optimization\nlgbm_params = {\n    \"objective\": \"binary\",\n    \"learning_rate\": 0.008,\n    'device': 'gpu',\n    'n_estimators': 3205,\n    'num_leaves': 184,\n    'min_child_samples': 63,\n    'feature_fraction': 0.6864594334728974,\n    'bagging_fraction': 0.9497327922401265,\n    'bagging_freq': 1,\n    'reg_alpha': 19,\n    'reg_lambda': 19,\n}\n\nlgbm_oof, lgbm_test = cross_validate_model(LGBMClassifier, lgbm_params, X, Y, X_test)","42c0c94a":"df = pd.DataFrame({\n    'id': data['id'],\n    'oof_preds': lgbm_oof\n})\n\ndf.to_csv('lgbm_oof.csv', index=False)\n\ndf = pd.DataFrame({\n    'id': ids,\n    'claim': lgbm_test\n})\n\ndf.to_csv('lgbm_preds.csv', index=False)","b03858cd":"model_names = ['xgb', 'catb', 'lgbm']\n\noof_data = pd.DataFrame()\n\nfor el in model_names:\n    oof_data[f'{el}_oof'] = pd.read_csv(f'{el}_oof.csv')['oof_preds']\n    \noof_data['claim'] = Y\n\noof_data.head()","f5ef2629":"preds_data = pd.DataFrame()\n\nfor el in model_names:\n    preds_data[f'{el}_preds'] = pd.read_csv(f'{el}_preds.csv')['claim']\n    \npreds_data.head()","0e2eae18":"X = oof_data.drop(columns=['claim']).to_numpy()\nY = oof_data['claim'].to_numpy()\n\nX.shape, Y.shape","cfe9f49d":"x_test = preds_data.to_numpy()\n\nx_test.shape","d3be3e37":"n_splits = 5\nskfolds = StratifiedKFold(n_splits=n_splits, shuffle=False)\n\noof_preds, oof_y = [], []\n\ntest_preds = np.zeros((X_test.shape[0]))\n\nfor i, (train_index, val_index) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[train_index], X[val_index]\n    y_train, y_val = Y[train_index], Y[val_index]\n\n    print(f\"---------- Fold {i+1} Started ----------\")\n    clf = LogisticRegression(random_state=SEED)\n\n    clf = clf.fit(x_train, y_train)\n    preds = clf.predict_proba(x_val)\n\n    oof_preds.extend(preds[:, 1])\n    oof_y.extend(y_val)\n\n    test_preds += clf.predict_proba(x_test)[:, 1]\n\n    ra_score = roc_auc_score(y_val, preds[:, 1])\n\n    print(f\"ROC AUC of current fold is {ra_score}\")\n\n    print(f\"---------- Fold {i+1} Ended ----------\")\n\ntest_preds \/= n_splits","f1466037":"sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub['claim'] = test_preds\n\nsub.head()","4150cb65":"sub.to_csv('submission.csv', index=False)","691c0ef6":"## Stacking","0f7763fd":"## Tuning Light GBM Classifier using Optuna","35354d92":"## Tuning Cat Boost Classifier using Optuna","67f16fa7":"## Tuning XGB Classifier using Optuna"}}