{"cell_type":{"dc584180":"code","277ebb3d":"code","5080f3ff":"code","145727ce":"code","63df5de1":"code","4c173d62":"code","9f6aaab5":"code","2d61255d":"code","6f00a72d":"code","53782cc3":"code","938b9eab":"code","415fe4e8":"code","385c6e1c":"code","bde58240":"code","dd44eda1":"code","cf2e8068":"code","595476bb":"code","aabe55c5":"code","81c97d43":"code","55750c96":"code","a5d4cb67":"code","b45fa31d":"code","01e51a37":"code","927341aa":"code","40ff9c9a":"code","e7ec72d2":"code","633f5835":"code","32e9b640":"code","52a80364":"code","97e1e3d2":"code","ca9c1841":"code","e4678fff":"code","a4ea860a":"code","2d2db3e9":"code","8dc73390":"code","1a4a49b0":"code","d48d974b":"code","7eb8e239":"code","7bd39f36":"code","ade5a45f":"code","01d44699":"code","be5476c3":"code","c40aa1b8":"markdown","59dec1e8":"markdown","31f5a05d":"markdown","3345f658":"markdown","6280a0fa":"markdown","632b5a3c":"markdown","e92d43aa":"markdown","2d1903c4":"markdown","eadc32b1":"markdown","fba7de6c":"markdown","dcd3d760":"markdown","b77ae67f":"markdown","0a087634":"markdown","ecc7d7b7":"markdown","92b68068":"markdown","91fa32ac":"markdown","9f57aebb":"markdown","41470bb1":"markdown","2b4efae1":"markdown","cd313c5c":"markdown","851b0def":"markdown","e3e2c92d":"markdown","c6b8dc23":"markdown","99334963":"markdown","cf3adcdb":"markdown","78e71ed0":"markdown","9335a17b":"markdown"},"source":{"dc584180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","277ebb3d":"import numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt","5080f3ff":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nx_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ny_test = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","145727ce":"print(\"Variables in training dataset: \",train.columns.values)\nprint(\"Variables in input testing dataset: \",x_test.columns.values)\nprint(\"Variable in output testing dataset: \",y_test.columns.values)","63df5de1":"#Seeing the type of data these variables contain\ntrain.head(10)","4c173d62":"x_test.head(10)","9f6aaab5":"train.info()","2d61255d":"x_test.info()","6f00a72d":"#Seeing the trends in the data\ntrain.describe(include='all')","53782cc3":"x_test.describe(include='all')","938b9eab":"sn.barplot(x='Sex',y='Survived', data=train)","415fe4e8":"sn.barplot(x='Pclass',y='Survived', data= train)","385c6e1c":"sn.barplot(x='SibSp',y='Survived', data= train)","bde58240":"sn.catplot(y='Survived',x='Parch',data=train, kind='bar')","dd44eda1":"train.describe(include='all')","cf2e8068":"#Removing the Cabin column since it is mostly empty\ntrain.drop([\"Cabin\"], axis=1, inplace=True)\nx_test.drop([\"Cabin\"], axis=1, inplace=True)","595476bb":"#Removing Ticket column since it doesn't lend something useful for our model\ntrain.drop([\"Ticket\"], axis=1, inplace=True)\nx_test.drop([\"Ticket\"],axis=1, inplace=True)","aabe55c5":"#extract a title for each Name in the train and test datasets\ntrain['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nx_test['Title'] = x_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","81c97d43":"#replace various titles with more common names\ncombine = [train, x_test]\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","55750c96":"#seeing the effect of our Title feature on our Output variable Survived\ntrain[['Title', 'Survived']].groupby(['Title'],as_index = False).mean()","a5d4cb67":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head(10)","b45fa31d":"# fill missing age with mean age group for each title along with gender classification (only where needed)\nmr_age = round(train[train[\"Title\"] == 1][train['Sex']== 'male']['Age'].mean(),0)#Young Adult\nmiss_age = round(train[train[\"Title\"] == 2][train['Sex']== 'female']['Age'].mean(),0) #Student\nmrs_age = round(train[train[\"Title\"] == 3][train['Sex']== 'female']['Age'].mean(),0) #Adult\nmaster_age = round(train[train[\"Title\"] == 4][train['Sex']== 'male']['Age'].mean(),0) #Baby\nroyal_age_male = train[train[\"Title\"] == 5][train['Sex']== 'male']['Age'].mean() #Adult\nroyal_age_female = train[train[\"Title\"] == 5][train['Sex']== 'female']['Age'].mean() #Adult\nrare_age_male = round(train[train[\"Title\"] == 6][train['Sex']== 'male']['Age'].mean(),0) #Adult\nrare_age_female = train[train[\"Title\"] == 6][train['Sex']== 'female']['Age'].mean() #Adult\n\nprint(mr_age,miss_age,mrs_age,master_age,royal_age_male,royal_age_female,rare_age_male,rare_age_female)\nprint(train['Age'].count())\nprint(train.loc[1,'Age'])\n#filling the missing age according to the title and gender they have.\nfor x in range(train['PassengerId'].count()):\n    if np.isnan(train.loc[x,'Age']):\n        if train.loc[x,'Title'] == 1:\n            train.loc[x,'Age'] = mr_age\n        elif train.loc[x,'Title'] == 2:\n            train.loc[x,'Age'] = miss_age\n        elif train.loc[x,'Title'] == 3:\n            train.loc[x,'Age'] = mrs_age\n        elif train.loc[x,'Title'] == 4:\n            train.loc[x,'Age'] = master_age\n        elif train.loc[x,'Title'] == 5 and train.loc[x,'Sex'] == 'male':\n            train.loc[x,'Age'] = royal_age_male\n        elif train.loc[x,'Title'] == 5 and train.loc[x,'Sex'] == 'female':\n            train.loc[x,'Age'] = royal_age_female\n        elif train.loc[x,'Title'] == 6 and train.loc[x,'Sex'] == 'male':\n            train.loc[x,'Age'] = rare_age_male\n        elif train.loc[x,'Title'] == 6 and train.loc[x,'Sex'] == 'female':\n            train.loc[x,'Age'] = rare_age_female\n            \nfor x in range(x_test['PassengerId'].count()):\n    if np.isnan(x_test.loc[x,'Age']):\n        if x_test.loc[x,'Title'] == 1:\n            x_test.loc[x,'Age'] = mr_age\n        elif x_test.loc[x,'Title'] == 2:\n            x_test.loc[x,'Age'] = miss_age\n        elif x_test.loc[x,'Title'] == 3:\n            x_test.loc[x,'Age'] = mrs_age\n        elif x_test.loc[x,'Title'] == 4:\n            x_test.loc[x,'Age'] = master_age\n        elif x_test.loc[x,'Title'] == 5 and x_test.loc[x,'Sex'] == 'male':\n            x_test.loc[x,'Age'] = royal_age_male\n        elif x_test.loc[x,'Title'] == 5 and x_test.loc[x,'Sex'] == 'female':\n            x_test.loc[x,'Age'] = royal_age_female\n        elif x_test.loc[x,'Title'] == 6 and x_test.loc[x,'Sex'] == 'male':\n            x_test.loc[x,'Age'] = rare_age_male\n        elif x_test.loc[x,'Title'] == 6 and x_test.loc[x,'Sex'] == 'female':\n            x_test.loc[x,'Age'] = rare_age_female","01e51a37":"train.drop(['Name'], axis = 1, inplace = True)\nx_test.drop(['Name'], axis = 1, inplace = True)\n\ntrain.head(10)","927341aa":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\nx_test['Sex'] = x_test['Sex'].map(sex_mapping)\n\ntrain.head(10)","40ff9c9a":"#let us first fill the missing values in our Training dataset\nprint(\"Number of people embarking in Southampton (S):\")\ns = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(s)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\nc = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(c)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nq = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(q)\n\n#We can see the most embarked place is Southampton then let's fill our empty data with that value\ntrain = train.fillna({\"Embarked\": \"S\"})\n","e7ec72d2":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": int(1.0), \"C\": int(2.0), \"Q\": int(3.0)}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\nx_test['Embarked'] = x_test['Embarked'].map(embarked_mapping)\n\ntrain.head(10)","633f5835":"#let us fill the missing values of fare feature in Testing data\nfor x in range(len(x_test[\"Fare\"])):\n    if pd.isnull(x_test[\"Fare\"][x]):\n        pclass = x_test[\"Pclass\"][x] #Pclass = 3\n        x_test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#now let us create a Categorical Feature for our feature Fare\ntrain['FareCat'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\nx_test['FareCat'] = pd.qcut(x_test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#now let us delete the Continuous Feature Fare\ntrain.drop(['Fare'], axis = 1, inplace = True)\nx_test.drop(['Fare'], axis = 1, inplace = True)\n\ntrain.head(10)","32e9b640":"#let us check for missing value in both datasets once again\ntrain.info()","52a80364":"x_test.info()","97e1e3d2":"#Let us create our Target and Input data from our Training set\nfeatures = train.drop(['PassengerId','Survived'], axis = 1)\noutcomes = train['Survived']\n\n#let us split our Target and Input data to Training and Testing data for our model\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(features,outcomes,test_size = 0.25, random_state = 0)","ca9c1841":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train, y_train)\ny_pred = log_reg.predict(x_val)\nacc_log_reg = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_log_reg","e4678fff":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ng_naive = GaussianNB()\ng_naive.fit(x_train, y_train)\ny_pred = g_naive.predict(x_val)\nacc_g_naive = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_g_naive","a4ea860a":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(x_train, y_train)\ny_pred = svm.predict(x_val)\nacc_svm = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_svm","2d2db3e9":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_linear_svc","8dc73390":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_perceptron","1a4a49b0":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_val)\nacc_decision_tree = round(accuracy_score(y_pred, y_val)*100 ,2)\nacc_decision_tree","d48d974b":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_val)\nacc_random_forest = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_random_forest","7eb8e239":"#kNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_knn","7bd39f36":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(x_train, y_train)\ny_pred = gbc.predict(x_val)\nacc_gbc = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_gbc","ade5a45f":"# AdaBoost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier()\nada.fit(x_train, y_train)\ny_pred = ada.predict(x_val)\nacc_ada = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_ada","01d44699":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Classifier', 'AdaBoost Classifier'],\n    'Score': [acc_svm, acc_knn, acc_log_reg, \n              acc_random_forest, acc_g_naive, acc_perceptron,acc_linear_svc, acc_decision_tree, acc_gbc, acc_ada]})\nmodels.sort_values(by='Score', ascending=False)","be5476c3":"#set ids as PassengerId and predict survival \nids = x_test['PassengerId']\npredictions = gbc.predict(x_test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","c40aa1b8":"Now let us see the relationship between Survivors and their Gender.","59dec1e8":"Now, let us see the relationship between the the survivors and the number of parent and child they have abroad.","31f5a05d":"## Creating Output File","3345f658":"Let us compare Accuracy of our models","6280a0fa":"### Testing different Models:\n* Logistic Regression\n* Gaussian Naive Bayes\n* Support Vector Machine (SVC, Linear SVC)\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* k-Nearest Neighbours\n* Gradient Boosting Classifier\n* AdaBoost Classifier\n* ","632b5a3c":"Now, let us see the relationship betweeen the Survivors and the number of people he\/she is travelling with.","e92d43aa":"### **Filling the Missing data in Age Column**","2d1903c4":"After seeing the accuracy score i have chosen to go with Gradient Boosting Classifier.","eadc32b1":"Let's look what sets of variable we are working with","fba7de6c":"### Name Feature\nSince we have created the Title column from the Name column therefore we can delete this column","dcd3d760":"# Analysis of Data","b77ae67f":"# Reading the datasets","0a087634":"We will try to fill the missing Age feature for the passengers on the basis of the mean age of the people of the same Title.","ecc7d7b7":"# Exploring the Data<br>\nNow let us explore our dataset to see different relationship between our input variables.","92b68068":"### Sex Feature\nSince our Sex column is still in string and is less useful therefore we will try to convert this feature into categorical","91fa32ac":"### Embarked Feature\nNow we convert our Embarked column data from Categorical String to Categorically Numeric data.","9f57aebb":"**Looking at dataset again**","41470bb1":"### Fare Feature\nNow let us see how can we utilise this feature.","2b4efae1":"train.csv has the output and input variables training data, <br> \ntest.csv has the input variable testing data,<br>\ngender_submission.csv has the output variable testing data <br>","cd313c5c":"# Importing the libraries for our use","851b0def":"# Data Cleaning:","e3e2c92d":"## Inference from the Variables in the dataset\n### Output Variable:\n'Survived' is our output variable or the variable whose values we have to compute using our Logistic Regression Model.<br>\n### Input Variable:\n'PassengerId' is being used as the primary key in the datasets to uniquely identify the attributes associated with any passengers. <br>\n'Pclass' describes the class of the Passengers and is categorical variable with values 1, 2 or 3. <br>\n'Name' describes the name of the Passengers. <br>\n'Sex' is a categorical variable that describes gender of the passengers and has two values 'male' or 'female'. <br>\n'Age' describes the age of the passengers.<br>\n'SibSp' describes the number of Sibllings or Spouses that are travelling with the Passenger.<br>\n'Parch' describes the number of Parents or children that are abroad to the passenger.<br>\n'Ticket' describes the type of the ticket the passenger has.<br>\n'Fare' describes the fare of the ticket of the passenger.<br>\n'Cabin' describes the cabin alloted to the passenger while his\/her stay on titanic.<br>\n'Embarked' describes the port where passengers embarked on the ship and is also categorical. <BR>","c6b8dc23":"# Observations from Visualization\n1. Females had better survival chances than Male Passengers.\n2. People with better economical background or the people in better Passenger class tend more to survive.\n3. People who are travelling with fewer number of people tend to survive more, People travelling alone have better chances of survival than people traveliing with more than 3 people.\n4. People having number of parents or children less than 4 tend to survive more, but still people with neither parents nor child tend to survive less.","99334963":"# Observations:\n1. Here we can see that in our training dataset out of 891 entries in the dataset following columns has missing values:<br>\n    'Age' has 714 values of 891<br>\n    'Cabin' has 204 values of 891<br>\n    'Embarked' has 889 values of 891<br>\n\n2. Here we can see that in our testing dataset out of 418 entries in the dataset following columns has missing values:<br>\n    'Age' has 332 values of 418<br>\n    'Cabin' has 91 values of 418<br>\n\n3. Names in the passengers list are all unique (both in training and testing datasets) and not null.<br>\n4. Number of Males Passengers are greater than the number of female passengers. <br>\n5. Mean Age of Passengers boarded on Titanic is nearly 30 years.<br>\n6. Since 'Cabin' column has the most missing values and will be difficult to fill it with the amount of known data therefore it is better to drop that column from both the datasets.<br>\n7. 'Age' column also has some missing values but 'Age' parameter is very important to predict the survival of a passenger. So we will fill the missing data with the known data.<br>\n8. Only 2 values are missing from the 'Embarked' column in our training dataset, so it would be benificary if we directly dropped these entries.<br>","cf3adcdb":"# Model Selection\nFinally after cleaning our Training and Testing data let us finally fit our data to different models","78e71ed0":"Making data more workable by removing non-usefull features.","9335a17b":"Now, let us see the relationship between Survivors and their Classes."}}