{"cell_type":{"fa43354d":"code","ed203119":"code","51cbcc2e":"code","22af2fe7":"code","cc7482c9":"code","43829296":"code","28b9d911":"code","83d15d2e":"code","36457af0":"code","026257d8":"code","a328917c":"code","33709e20":"code","48b063b0":"code","b099ccd5":"code","7429b89f":"code","0e799f01":"markdown","d3bb3d02":"markdown"},"source":{"fa43354d":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import StandardScaler","ed203119":"torch.manual_seed(1)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    torch.cuda.manual_seed_all(1)\nelse:\n    device = 'cpu'","51cbcc2e":"train = pd.read_csv('..\/input\/nba-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/nba-prediction\/test.csv')","22af2fe7":"train.shape, test.shape","cc7482c9":"train.head()","43829296":"# Null \uac12 \ud655\uc778\ntrain.isnull().sum()","28b9d911":"# \ud574\ub2f9 \ud589 \uc81c\uac70\n#train.dropna(inplace=True)\ntrain.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)","83d15d2e":"# \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44\nX_train = train.drop(['ID', 'Win'], axis=1)\nX_test = test.drop('ID', axis=1)\ny_train = train['Win']\n\n\n# Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Tensor\ub85c \ubcc0\ud658\nX_train = torch.FloatTensor(X_train).to(device)\nX_test = torch.FloatTensor(X_test).to(device)\n\ny_train = torch.LongTensor(y_train.values).to(device)","36457af0":"# Hyperparameter \uc124\uc815\nlearning_rate = 0.1\nn_epochs = 500\ndrop_prob = 0.3\n\n\n# Model \uc124\uc815\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(X_train.shape[1], 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n        self.fc4 = nn.Linear(256, 256)\n        self.fc5 = nn.Linear(256, 256)\n        self.fc6 = nn.Linear(256, 2)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight.data)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc3(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc4(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc5(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc6(out)\n        return out","026257d8":"model = Net().to(device)\n\n\n# optimizer\uc640 Loss Function \uc124\uc815\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 Learning rate Scheduler \uc0ac\uc6a9\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)","a328917c":"## \ud559\uc2b5 ##\n\nfor epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    accuracy = (torch.argmax(H, dim=1) == y_train).float().mean()\n    \n    scheduler.step()\n    \n    if epoch % 20 == 0:\n        print('Epoch {:4d} \/ {}, Loss : {:.4f}, Accuracy : {:.2f} %'.format(\n            epoch, n_epochs, loss.item(), accuracy*100))","33709e20":"with torch.no_grad():\n    model.eval()\n    pred = model(X_test)","48b063b0":"submit = pd.read_csv('..\/input\/nba-prediction\/sample_submit.csv')","b099ccd5":"submit['Win'] = torch.argmax(pred, dim=1).cpu()\nsubmit.head()","7429b89f":"submit.to_csv('submission.csv', index=False)","0e799f01":"## Training","d3bb3d02":"## Data Preparation"}}