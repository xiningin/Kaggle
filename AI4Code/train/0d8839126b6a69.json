{"cell_type":{"02de83ec":"code","dfa2e3de":"code","977e1747":"code","f2321fb4":"code","e2ea10a6":"code","b7d95d67":"code","2cb93f27":"code","b516f45d":"code","a958710b":"code","f8934bf5":"code","31560463":"code","fa1bad90":"code","7a408fca":"code","b4769354":"code","6b707d82":"code","12ab0a62":"code","942bcfa6":"code","88c7049e":"code","4d255d46":"markdown","9fe160dd":"markdown","b9224982":"markdown","740954ae":"markdown","de258f6d":"markdown","f8f5292c":"markdown","cbe4aae9":"markdown","2332d75d":"markdown","0f38fd58":"markdown","d9a1b1db":"markdown","10e9123c":"markdown","4b2272b1":"markdown","db76d170":"markdown","d9ca6294":"markdown","f698a7d9":"markdown","cedb9d4d":"markdown","e111d19f":"markdown","836cc53c":"markdown","6143970c":"markdown","4daf2726":"markdown","ff36e767":"markdown","7d21a4f9":"markdown","5b45cf7e":"markdown","1b976c33":"markdown","78fc362c":"markdown"},"source":{"02de83ec":"# Import essential libraries and modules\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport os\nimport plotly \nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot \ninit_notebook_mode(connected=True)\nfrom IPython.display import Image\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Configure some plot settings\n%config InlineBackend.figure_format = 'retina'\nsns.set()","dfa2e3de":"metadata_path = \"..\/input\/buildingdatagenomeproject2\/metadata.csv\"\nweather_path = \"..\/input\/buildingdatagenomeproject2\/weather.csv\"\nenergy_path = \"..\/input\/buildingdatagenomeproject2\/electricity.csv\"\nmeta_cols = [\"building_id\", \"site_id\", \"primaryspaceusage\", \"sqm\"]\nsite_id = \"Bear\"\n\ndef transform_dataset(metadata_path, weather_path, energy_path, site_id):\n    \n    \"\"\"\n    This functions returns a long form dataset of the chosen site with adjoining metadata, weather, and \n    electricity meter data. \n    \n    metadata_path: path to metadata data set\n    weather_path: path to weather data set\n    energy_path: path to energy data set\n    site_id: selected site id\n    \"\"\"\n    \n    # Load data\n    metadata = pd.read_csv(metadata_path)\n    weather = pd.read_csv(weather_path, parse_dates=[\"timestamp\"])\n    energy = pd.read_csv(energy_path, parse_dates=[\"timestamp\"])\n    \n    # Filter\n    metadata = metadata.loc[metadata.site_id == site_id, meta_cols]\n    weather = weather.loc[weather.site_id == site_id]\n    cols = [\"timestamp\"] + [col for col in energy.columns if site_id in col]\n    energy = energy[cols]\n    \n    # Melt\n    data = energy.melt(id_vars=\"timestamp\", var_name=\"building_id\", value_name=\"meter_reading\")\n    \n    # Merge\n    data = pd.merge(data, metadata, how=\"left\", on=\"building_id\")\n    data = pd.merge(data, weather, how=\"left\", on=[\"timestamp\",\"site_id\"])\n    \n    # Ordinal Encoding of Nominal Categorical Variables\n    encoder = OrdinalEncoder()\n    cols = [\"building_id\",\"site_id\"]\n    encoded_columns = encoder.fit_transform(data[cols])\n    encoded_columns = pd.DataFrame(encoded_columns)\n    encoded_columns.columns = cols\n    data[cols] = encoded_columns\n    \n    # One Hot Encoding of Categories \n    \n    # if cardinality is high\n    # minor = (data.primaryspaceusage.value_counts() < len(data)*0.1)\n    # minor_categories = list(minor[minor].index)\n    # data = data.replace(minor_categories, \"Other\")\n    \n    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)  \n    cols = [\"primaryspaceusage\"]\n    OH_cols = OH_encoder.fit_transform(data[cols])\n    OH_cols = pd.DataFrame(OH_cols)\n    OH_cols.columns = OH_encoder.get_feature_names(cols)\n    data = data.drop(cols, axis=1)\n    data = pd.concat([data, OH_cols], axis=1)\n    \n    return data\n\ndata = transform_dataset(metadata_path, weather_path, energy_path, site_id)\ndata.drop(['site_id'], axis=1, inplace=True) \n\n# Preview some columns \ncols_to_show = ['timestamp','building_id', 'meter_reading','sqm','primaryspaceusage_Education','primaryspaceusage_Lodging\/residential','primaryspaceusage_Public services']\ndata[cols_to_show].head()","977e1747":"df = [] \nfor i in data.primaryspaceusage.unique(): \n    df.append(go.Box(y=data[data.primaryspaceusage == i].meter_reading, name=i)) \niplot(df, show_link=False)","f2321fb4":"data.isna().sum() \/ len(data) * 100","e2ea10a6":"# Drop columns with high percentage of missing values\nto_drop = [\"cloudCoverage\", \"precipDepth6HR\"]\ndata_prep = data.copy()\ndata_prep.drop(to_drop, axis=1, inplace=True)\n\n# Perform imputation for missing values except 'meter_reading'\n\nto_input = [col for col in data_prep.columns if (data_prep[col].isna().any() and data_prep[col].dtype == 'float64')]\nto_input.remove('meter_reading')\n\nmy_imputer = SimpleImputer()\ninput_df = pd.DataFrame(my_imputer.fit_transform(data_prep[to_input]))\n\n# Assign column names\ninput_df.columns = to_input\n\n# Replace columns \ndata_prep[to_input] = input_df\n\ndata_prep.isna().sum()\n\n# Select rows without missing value in 'meter_reading'\ndata_prep.dropna(inplace=True)\ndata_prep.isna().sum()","b7d95d67":"data_prep.describe()","2cb93f27":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error, mean_absolute_error","b516f45d":"# Splitting into training and validation set\n\n# Train set\nX_train = data_prep[data_prep.timestamp < \"2017-01-01\"].set_index(\"timestamp\").drop(\"meter_reading\", axis=1)\ny_train = data_prep[data_prep.timestamp < \"2017-01-01\"].set_index(\"timestamp\").meter_reading\n\n# Validation set\nX_val = data_prep[data_prep.timestamp >= \"2017-07-01\"].set_index(\"timestamp\").drop(\"meter_reading\", axis=1)\ny_val = data_prep[data_prep.timestamp >= \"2017-07-01\"].set_index(\"timestamp\").meter_reading\n\n# Specifying our model\n\nxgb = XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=4, random_state=1, objective=\"reg:squarederror\")\n\n# Train\nxgb.fit(X_train, y_train, verbose=False)\n\n# Compute loss metric\npreds = xgb.predict(X_val)\nscore = mean_squared_error(y_val, preds)\nprint(\"MSE: \", score)","a958710b":"def plot_errors(X_val, y_val, model, plot_intervals=False, plot_anomalies=False):\n\n    \"\"\"\n        y_val - Actual readings\n        preds - Predicted readings\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    preds = pd.DataFrame({\"preds\": model.predict(X_val)}, index=y_val.index)\n    preds_freq = preds.resample('D').mean()\n    y_val_freq = y_val.resample('D').mean()\n    plt.figure(figsize=(15,5))\n    plt.title(\"Prediction and Observed Electricity Reading\")\n    plt.plot(preds_freq, \"g\", label=\"Predicted\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(y_val_freq, preds_freq)\n        deviation = np.std(y_val_freq.values - preds_freq.values)\n        lower_bond = preds_freq - (mae + deviation*1.96)\n        upper_bond = preds_freq + (mae + deviation*1.96)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond \/ Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n    plt.plot(y_val_freq, label=\"Actual\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","f8934bf5":"plot_errors(X_val, y_val, xgb, plot_intervals=True)","31560463":"xgb_importance = xgb.feature_importances_\nimportance_df = pd.DataFrame({'importance': xgb_importance, 'columns': X_val.columns})\n\n# Summarize feature importance\nfor i in range(len(importance_df)-1):\n    print(importance_df['columns'][i] + \"Score: {:0.5f}\".format(importance_df['importance'][i]))","fa1bad90":"importance_df.sort_values('importance',inplace=True)\naxes = importance_df.plot(kind='barh',y='importance',x='columns',color='r', xlabel='')","7a408fca":"cmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nfrom matplotlib.patches import Patch\n\ndef plot_cv_indices(cv, n_splits, X, y, date_col = None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize = (11, 7))\n    \n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker='_', lw=10, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n\n    # Formatting\n    yticklabels = list(range(n_splits+2))\n    \n    if date_col is not None:\n        tick_locations  = ax.get_xticks()\n        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n\n        tick_locations_str = [str(int(i)) for i in tick_locations]\n        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates) ]\n        ax.set_xticks(tick_locations)\n        ax.set_xticklabels(new_labels)\n    \n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[n_splits+0.2, -.2])\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Testing set', 'Training set'], loc=(1.02, .8))\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)","b4769354":"from sklearn.model_selection import TimeSeriesSplit\n\n# Set number of  cross-validation folds\nn_splits = 3\ntscv = TimeSeriesSplit(n_splits)\n\n# Make a new data copy\ndata_cv = data_prep.copy()\ndata_cv = data_cv.set_index('timestamp')\n\ny_cv = data_cv.meter_reading\nX_cv = data_cv.drop('meter_reading', axis=1)\n\nfor fold, (train_index, val_index) in enumerate(tscv.split(X_cv)):\n    print(\"Fold: {}\".format(fold))\n    print(\"TRAIN indices:\", train_index, \"\\n\", \"VALIDATION indices:\", val_index)\n    print(\"\\n\")\n\nplot_cv_indices(tscv, n_splits, X_cv, y_cv)","6b707d82":"# Fitting, Prediction, and Evaluation\nMSE = []\nxgb_cv = XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=4, random_state=1, objective=\"reg:squarederror\")\n\nfor fold, (train_index, val_index) in enumerate(tscv.split(X_cv)):\n    X_train, X_val = X_cv.iloc[train_index], X_cv.iloc[val_index]\n    y_train, y_val = y_cv.iloc[train_index], y_cv.iloc[val_index]\n    xgb_cv.fit(X_train, y_train, verbose=False)\n    preds = my_model.predict(X_val)\n    score = mean_squared_error(y_val, preds)\n    MSE.append(score)\n    print(\"MSE: \", score)","12ab0a62":"print(\"Average MSE: {:0.2f}\".format(np.mean(MSE)))","942bcfa6":"def view_building(data, building_name, time_before):\n    \"\"\" This simple function helps to iterate over the process of generating trendline plots for building(s).\n    \"\"\"\n    temp = pd.DataFrame(data[building_name].truncate(before = time_before)).fillna(method='ffill')\n    temp.plot()\n\n# Sample Run\nview_building(data = electricity, building_name = ['Panther_lodging_Cora','Panther_office_Hannah'], time_before = '2017-01-01')","88c7049e":"def view_site(data, site_name, element, time_before, frequency = 'D', outlier = None):\n    \"\"\" This function helps to iterate over the process of generating trendline plots for site(s).\n    \"\"\"\n    temp = pd.DataFrame(data[data.site_id==site_name].truncate(before = time_before))\n    temp_freq = temp.resample(frequency).mean()\n    if outlier==None:\n        temp_freq_trim = temp_freq\n    else:\n        temp_freq_trim = temp_freq[temp_freq > outlier]\n    temp_freq_fill = temp_freq_trim.fillna(method='ffill')\n    temp_freq_fill[element].plot()\n\n# Sample Run\nview_site(data = weather, site_name = 'Panther', element = 'airTemperature', time_before = '2017-01-01')\nview_site(data = weather, site_name = 'Panther', element = 'airTemperature', time_before = '2017-01-01', frequency = 'H', outlier = -40)","4d255d46":"| Primary Use | Counts | Percentage |\n| --- | --- | --- |\n| Education | 1,175,448 | 72.83% |\n| Entertainment\/public assembly | 157,896 | 9.78% |\n| Public services | 105,264 | 6.52% |\n| Lodging\/residential | 70,176 | 4.35% |\n| Parking | 52,632 | 3.26% |\n| Technology\/science | 35,088 | 2.17% |\n| Utility | 17,544 | 1.09% |","9fe160dd":"Next, we examine the features importance to identify the features relevant for building energy prediction. As seen from our plot on feature importance, identification of primary space use type is most important, followed by the floor area of the building (per sqm), and the id of buildings. Seemingly, weather variables did not have a huge impact. This makes intuitive sense as the spatial scale of analysis is site-based and there might not be enough variation to explain energy fluctation. Notably, this finding coincides with our earlier exploration of electricity and weather data where we found general weak relationship in correlation plots. ","b9224982":"## Context\n<div id=\"Section2\">\n    \nWorld leaders pledged tougher new carbon emission reduction targets at a virtual climate summit on Thursday (April 22), stepping up the fight to limit the temperature rise to 1.5 degrees above pre-industrial levels, instead of the three degrees the planet is currently headed towards.\nAmerica vowed to halve the planet-warming greenhouse gases it emits by 2030. buildings, one of the largest contributors to carbon footprint. Role of education institutes.   Universities, as innovation drivers in science and technology worldwide, should be leading the Great Transformation towards a carbon\u2013neutral society and many have indeed picked up the challenge. (Helmers et al., 2021).\u201cSchools going carbon-neutral provides a great opportunity to demystify carbon neutrality for students and can give them a practical experience through inclusion in curricula and operations of the school.\u201dsaid Niklas Hagelberg, coordinator of the Climate Change Programme at UN Environment. (First in Class, 2019). Group 4 Energy Consultants have been engaged by the U.S Government to study the carbon footprint of educational buildings in the U.S. with the targets of: 1) promoting understanding of factors affecting energy usage, and 2) identifying recommendations to reduce educational building carbon footprint.\n    \n### Factors Affecting Building Energy Consumption\n    \nOngoing research on building energy consumption.\n\n### Building Energy Estimation Methods\n    \nOne of the first steps of energy efficient buildings is to model building energy usage. While this is traditionally modelled through complex and technical methods, machine learning provides potential to augment these approaches. \n    \n---","740954ae":"We perform a simple train-validation split. Given the temporal nature of the data, a random split would not be appropriate due to high autocorrelation. Instead, we treat records before **1 Jan 2017** as our training set and all other records belong in our validation set. We also specify mean squared error as our loss metric. ","de258f6d":"## Conclusion\n<div id=\"Section4\">\n    \nOUR CONCLUSION\n    \n---","f8f5292c":"Next, we examine the distribution of electricity meter readings across primary usage types. As seen from the visualisation, there are a large number of education buildings with energy readings above the benchmark. ","cbe4aae9":"Our data now has 1614048 rows and 15 columns. The site consists of 92 unique buildings and mainly consist of educational buildings.","2332d75d":"### Model Evaluation and Comparison\n\nUltimately, XXX Model returned the best result. \n\nAlso, there is consistency \/ no consistency in feature importance... (compare model feature importance)","0f38fd58":"## Predictive Modelling\n<div id=\"Section4\">\n    \nIn this section, we compare the effectiveness of various machine learning methods to measure feature importance. We consider three models: 1) Random Forest, 2) XGBoost, and 3) LightGBM. \n    \n- Random Forest Regressor\n- XGBoost\n- LightGBM\n---","d9a1b1db":"Next, we proceed to fit our model on each fold of train-validation set and compute the mean squared error for each iteration. The final mean squared error is the average of error from each fold. ","10e9123c":"### Model 3: LightGBM\nWe start by importing our loss metric and RandomForestRegressor class. ","4b2272b1":"---\n\n### Formatting Tips\n\nThis is how we can add hyperlinks [kaggle](https:\/\/www.kaggle.com\/).\n\n**text** bold.\n\n*text* italics.\n\n**_text_** bold and italics.\n\n![sde4.jpg](https:\/\/www.indesignlive.sg\/wp-content\/uploads\/2019\/02\/web_SouthElevation-Credit-Rory-Gardiner.jpg)\n\n<cite>Image credit: Rory Gardiner<\/cite>","db76d170":"## Dataset: Building Data Genome 2 Project\n### Target Task: (\\#2) What are the factors that influence building energy consumption and how can we identify them?\n**Group Members**: HE, Zhi Yin (), WU Chen Xi (), YAP Yu Ming Winston (A0108318J), ZHANG Xin Yuan (),and ZHENG, Xue Hua ()\n\n**Executive Summary**\n\nEducational institutes, as innovation drivers of science and technology worldwide, are in a great position to advance energy efficiency. However, progress on building energy efficiency studies have generally been limited by: 1) scarcity of data on building energy consumption, and 2) highly complex\/technical traditional building energy estimation methods. The advent of new and dynamic streams of building energy data and machine learning methods provide new ways to model building energy consumption. In this report, the team is tasked by California City Hall to explore the energy efficiency of universities in the California area. This is part of their urban strategy to reduce building carbon emission and to promote education on climate change mitigation and adaptation. Particularly,they have become aware of a new, extensive building meter dataset (Building Data Genome 2 Project) and would like to understand how how data science and machine learning methods can be used to predict and model energy consumption of educational institutes. In this report, we start by outlining key movements in climate and sustainability that sets the context for our study. Next, we proceed to examine existing research on factors affecting building energy consumption. Following our review, we conduct an exploratory analysis of our dataset to identify buildings with energy consumption above the benchmark. We then proceed to employ, evalutate, and compare the effectiveness of various machine learning algorithms (XGBoost, RandomForest, SVD, Linear Regression) for building energy estimation before concluding with recommendations to reduce building energy consumption. \n\n\n---\n## Outline\n\n[1. Context](#Section1) \n\n[2. Notebook Preparation](#Section2)\n\n[3. Exploratory Data Analysis](#Section3)\n\n[4. Predictive Modelling](#Section4)\n\n[5. Conclusion](#Section5)\n\n---","d9ca6294":"## Notebook Preparation\n<div id=\"Section1\">\n    \n---\n\nThis section outlines the data loading and preprocessing steps. \n    \n","f698a7d9":"### Prepare Workspace\nStarting off, we load the essential libraries and configure some plot settings. \n","cedb9d4d":"### Loading and Preparing Dataset\nWe begin by joining hourly electricity reading, weather, and meta data. The site (nicknamed \"Bear\") which consists of a cluster of education buildings at the University of California, Berkeley, will be the focus of this analysis.\n\nData preprocessing steps and workflows (with minor adjustments to categorical one-hot encoding) are adapted from [Preprocessing 1 Notebook](https:\/\/www.kaggle.com\/ponybiam\/preprocessing-1-data-transformation\/) by [Pony Biam](https:\/\/www.kaggle.com\/ponybiam) and [Clayton Miller](https:\/\/www.kaggle.com\/claytonmiller). ","e111d19f":"Overall our model reports an MSE of 2342.83 which translates to an error of approximately 20 kWh per hourly reading. Given the diverse range of reading values, our model does a reasonable job of predicting building energy consumption. We take a look at the daily aggregated predictions to see where the model under performs. The model is able to keep up with the capture daily fluctuation and trends. However, towards the period from 11 November 2017 onwards it starts to under-estimate energy consumption. This might be attributed to unforeseen increases in energy usage for the period. ","836cc53c":"### Model 1: Random Forest\nWe start by importing our loss metric and RandomForestRegressor class. ","6143970c":"### Time Series Cross-Validation\n\nPreviously, our plot of prediction against actual measurement revealed that prediction error accrued mainly for later time periods. In particular, our model started to underestimate building energy consumption from 11 November 2017 onwards. In particular, this might be due to an artifact of data splitting where the model can only capture trends from 2016-2017. Time series cross validation splits the dataset into chunks of training and validation sets where training sets precede validation. The use of cross validation methods might help to improve fit by allowing the model to learn patterns from various segments of the data. We begin by defining a function to visualise our train-validation set splits. ","4daf2726":"### Model 2: XGBoost\nWe start by importing our loss metric and XGBRegressor class. ","ff36e767":"For columns with relatively few missing values, we impute the numerical mean. A visual check confirms that columns no longer have missing values. ","7d21a4f9":"Before fitting the model, it is necessary to check for missing values. As seen from the array below, some variables such as `precipDepth6HR` and `cloudCoverage` have a high percentage of missing values. Although there are a large number of missing values for `meter_reading`, it serves as our response variable, hence the rows with missing readings are dropped. ","5b45cf7e":"## Exploratory Data Analysis\n<div id=\"Section3\">\n    \nThis section should inform the subsequent predictive modelling section.\n    \n---","1b976c33":"We first conduct a normal time series split using the standard `TimeSeriesSplit` module from the `sklearn` package. We print out the train-validation sets of each fold and plot them. ","78fc362c":"Surprisingly, the cross-validated average MSE of 20,367.48 is much higher than the previous approach. There could be two explanations for this: 1) the cross-validated approach does not accout well for seasonality where the building energy consumption from preceding quarters do not explain the subsequent quarter well, and 2) the reduction in data size reduces the ability of the model to learn more subtle signals (e.g. weather fluctuations that tend to be constant on a yearly basis). *A cross-validated block approach was also attempted by the team but ultimately discarded as it yielded worse results as compared to the standard time series cross-validation approach."}}