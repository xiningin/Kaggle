{"cell_type":{"37ade648":"code","6a229d84":"code","5c6fc26a":"code","b11f173a":"code","ae535844":"code","2cdab6e8":"code","ebbbd74e":"code","a24a8f07":"code","6d760a2c":"code","6f69577f":"code","411a0428":"code","6851837b":"code","44781441":"code","c7098599":"code","e4cb27c9":"code","f1111de2":"code","b23ddf27":"code","fa625f48":"code","6bf2a28d":"code","428dc067":"code","0be55d1e":"code","26d623a4":"code","ad51b64e":"code","8513c63d":"code","095e6152":"code","f3972c88":"code","733584d6":"code","657b4a95":"code","90265bbc":"code","079541b4":"code","fe87f2d7":"code","6e0ec11c":"code","7999afd2":"code","86fb9822":"code","61182605":"code","569f7239":"code","af112321":"code","1b1bc278":"code","4e5e5c0d":"code","1b818dab":"code","6f19edb6":"code","49724cfe":"code","dee6181c":"code","71bf6e89":"code","18cae4b7":"code","97710403":"code","96beb22e":"code","1e37f390":"code","b0864909":"code","d7917841":"code","5421b601":"code","913eb404":"markdown","402470f7":"markdown","a0919bda":"markdown","8e51c65b":"markdown","3091cc70":"markdown","0b0b93ef":"markdown","da94cfed":"markdown","b09f39cc":"markdown","8a3b6199":"markdown","e57ae678":"markdown","0a8a52ce":"markdown","93014dd2":"markdown","b9437372":"markdown","b2749a1c":"markdown","53e3aa5c":"markdown","ff5d7058":"markdown","123ee4a3":"markdown","9cc77c73":"markdown","96d79f9b":"markdown","7f27b1a7":"markdown","be8285f6":"markdown","e5f65b61":"markdown","22d6b713":"markdown","2038211e":"markdown","0a282361":"markdown","b8449ade":"markdown","370f7713":"markdown","178b36a9":"markdown","36de0825":"markdown","3239e571":"markdown","82244eac":"markdown","c65fed89":"markdown","0a6faa4f":"markdown","12158b54":"markdown","87d122a8":"markdown","fda3116f":"markdown","35a6fc3f":"markdown","fcbdb1ff":"markdown","bb94ac3f":"markdown","6b4ebe9f":"markdown","b0c54934":"markdown","941ddeea":"markdown","6d16d660":"markdown","586c13d2":"markdown","2f24cd12":"markdown","e181aae4":"markdown","033e0920":"markdown","ce858039":"markdown","e5271cfd":"markdown"},"source":{"37ade648":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # for plot visualization\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom scipy.stats import skew\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))","6a229d84":"plt.figure(figsize=(8, 5), dpi=80)\nsns.set_style(\"darkgrid\")","5c6fc26a":"test_dataset = pd.read_csv('..\/input\/test.csv')\ntrain_dataset = pd.read_csv('..\/input\/train.csv', nrows=2_000_000)","b11f173a":"train_dataset.head(5)","ae535844":"train_dataset.tail(5)","2cdab6e8":"train_dataset.dtypes","ebbbd74e":"# lets check current memory usage status\ntrain_dataset.info(memory_usage='deep')","a24a8f07":"for dtype in ['float','int','object']:\n    selected_dtype = train_dataset.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b \/ 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))","6d760a2c":"train_dataset.drop(labels='key', axis=1, inplace=True)\ntest_dataset.drop(labels='key', axis=1, inplace=True)","6f69577f":"# Let's again check the memory usage.\ntrain_dataset.info(memory_usage='deep')","411a0428":"train_dataset.passenger_count = train_dataset.passenger_count.astype(dtype = 'uint8')","6851837b":"train_dataset.pickup_longitude = train_dataset.pickup_longitude.astype(dtype = 'float32')\ntrain_dataset.pickup_latitude = train_dataset.pickup_latitude.astype(dtype = 'float32')\ntrain_dataset.dropoff_longitude = train_dataset.dropoff_longitude.astype(dtype = 'float32')\ntrain_dataset.dropoff_latitude = train_dataset.dropoff_latitude.astype(dtype = 'float32')\ntrain_dataset.fare_amount = train_dataset.fare_amount.astype(dtype = 'float32')","44781441":"# let's again check the memory_usage report\ntrain_dataset.info(memory_usage='deep')","c7098599":"train_dataset.isnull().sum()","e4cb27c9":"print(f'Row count before drop-null operation - {train_dataset.shape[0]}')\ntrain_dataset.dropna(inplace = True)\nprint(f'Row count after drop-null operation - {train_dataset.shape[0]}')","f1111de2":"train_dataset['pickup_datetime'] = pd.to_datetime(arg=train_dataset['pickup_datetime'], infer_datetime_format=True)\ntest_dataset['pickup_datetime'] = pd.to_datetime(arg=test_dataset['pickup_datetime'], infer_datetime_format=True)","b23ddf27":"train_dataset.dtypes","fa625f48":"def add_new_date_time_features(dataset):\n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    dataset['day_of_week'] = dataset.pickup_datetime.dt.dayofweek\n    \n    return dataset\n\ntrain_dataset = add_new_date_time_features(train_dataset)\ntest_dataset = add_new_date_time_features(test_dataset)","6bf2a28d":"train_dataset.describe()","428dc067":"print(f'Rows before removing coordinate outliers - {train_dataset.shape[0]}')\n\ntrain_dataset = train_dataset[train_dataset.pickup_longitude.between(test_dataset.pickup_longitude.min(), test_dataset.pickup_longitude.max())]\ntrain_dataset = train_dataset[train_dataset.pickup_latitude.between(test_dataset.pickup_latitude.min(), test_dataset.pickup_latitude.max())]\ntrain_dataset = train_dataset[train_dataset.dropoff_longitude.between(test_dataset.dropoff_longitude.min(), test_dataset.dropoff_longitude.max())]\ntrain_dataset = train_dataset[train_dataset.dropoff_latitude.between(test_dataset.dropoff_latitude.min(), test_dataset.dropoff_latitude.max())]\n\nprint(f'Rows after removing coordinate outliers - {train_dataset.shape[0]}')","0be55d1e":"train_dataset.describe()","26d623a4":" train_dataset.fare_amount[(train_dataset.fare_amount <= 0) | (train_dataset.fare_amount >= 350)].count()","ad51b64e":"# Let's eliminate these rows\nprint(f'Row count before elimination - {train_dataset.shape[0]}')\ntrain_dataset = train_dataset[train_dataset.fare_amount.between(0, 350, inclusive=False)]\nprint(f'Row count after elimination - {train_dataset.shape[0]}')","8513c63d":"train_dataset.passenger_count[(train_dataset.passenger_count < 1) | (train_dataset.passenger_count > 8)].count()","095e6152":"# Let's eliminate these rows\nprint(f'Row count before elimination - {train_dataset.shape[0]}')\ntrain_dataset = train_dataset[train_dataset.passenger_count.between(0, 8, inclusive=False)]\nprint(f'Row count after elimination - {train_dataset.shape[0]}')","f3972c88":"def degree_to_radion(degree):\n    return degree*(np.pi\/180)\n\ndef calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n    \n    from_lat = degree_to_radion(pickup_latitude)\n    from_long = degree_to_radion(pickup_longitude)\n    to_lat = degree_to_radion(dropoff_latitude)\n    to_long = degree_to_radion(dropoff_longitude)\n    \n    radius = 6371.01\n    \n    lat_diff = to_lat - from_lat\n    long_diff = to_long - from_long\n\n    a = np.sin(lat_diff \/ 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff \/ 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    \n    return radius * c\n","733584d6":"train_dataset['distance'] = calculate_distance(train_dataset.pickup_latitude, train_dataset.pickup_longitude, train_dataset.dropoff_latitude, train_dataset.dropoff_longitude)\ntest_dataset['distance'] = calculate_distance(test_dataset.pickup_latitude, test_dataset.pickup_longitude, test_dataset.dropoff_latitude, test_dataset.dropoff_longitude)","657b4a95":"train_dataset.sort_values(by='distance')","90265bbc":"train_dataset.distance[(train_dataset.distance == 0)].count()","079541b4":"train_dataset[(train_dataset.pickup_latitude != train_dataset.dropoff_latitude) &\n              (train_dataset.pickup_longitude != train_dataset.dropoff_latitude) &\n              (train_dataset.distance == 0)].count()","fe87f2d7":"def add_distances_from_airport(dataset):\n    #coordinates of all these airports\n    jfk_coords = (40.639722, -73.778889)\n    ewr_coords = (40.6925, -74.168611)\n    lga_coords = (40.77725, -73.872611)\n\n    dataset['pickup_jfk_distance'] = calculate_distance(jfk_coords[0], jfk_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_jfk_distance'] = calculate_distance(jfk_coords[0], jfk_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_ewr_distance'] = calculate_distance(ewr_coords[0], ewr_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_ewr_distance'] = calculate_distance(ewr_coords[0], ewr_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    dataset['pickup_lga_distance'] = calculate_distance(lga_coords[0], lga_coords[1], dataset.pickup_latitude, dataset.pickup_longitude)\n    dataset['dropof_lga_distance'] = calculate_distance(lga_coords[0], lga_coords[1], dataset.dropoff_latitude, dataset.dropoff_longitude)\n    \n    return dataset\n\n\ntrain_dataset = add_distances_from_airport(train_dataset)\ntest_dataset = add_distances_from_airport(test_dataset)","6e0ec11c":"sns.distplot(a=train_dataset.fare_amount)","7999afd2":"sns.jointplot(x='distance', y='fare_amount', data=train_dataset)","86fb9822":"g = sns.FacetGrid(train_dataset, col=\"year\", hue=\"passenger_count\")\ng.map(plt.scatter, \"distance\", \"fare_amount\")\ng.add_legend()","61182605":"train_dataset[(train_dataset.distance>90) & (train_dataset.fare_amount<70)]","569f7239":"sns.countplot(x='day_of_week', data=train_dataset)","af112321":"tc = train_dataset.pivot_table(index='day_of_week', columns='month', values='fare_amount')\nsns.heatmap(data = tc)","1b1bc278":"train_dataset['fare_amount'].skew()","4e5e5c0d":"train_dataset['fare_amount'] = np.log1p(train_dataset['fare_amount'])\nsns.distplot(train_dataset['fare_amount'], color='blue')","1b818dab":"selected_predictors = [\n    'pickup_longitude', \n    'pickup_latitude', \n    'dropoff_longitude', \n    'dropoff_latitude',\n    'pickup_jfk_distance',\n    'dropof_jfk_distance',\n    'pickup_ewr_distance',\n    'dropof_ewr_distance',\n    'pickup_lga_distance',\n    'dropof_lga_distance',\n    'hour',\n    'month',\n    'year',\n    'distance'\n]\n\nX = train_dataset.loc[:, selected_predictors].values\ny = train_dataset.iloc[:, 0].values\nX_test_dataset = test_dataset.loc[:, selected_predictors].values\n\n# Since test_dataset is too large, So we are going to keep only 5% of the dataset in test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/20)","6f19edb6":"rand_forest_regressor = RandomForestRegressor()\nrand_forest_regressor.fit(X_train, y_train)\n\ny_rand_forest_predict = rand_forest_regressor.predict(X_test)\nrandom_forest_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_rand_forest_predict)))\nprint(f' Random Forest Mean Squared Error - {random_forest_model_error}')","49724cfe":"# parameters = {\n#                 'learning_rate': [0.07, 0.1, 0.3],\n#                 'max_depth': [3, 5, 7],\n#                 'n_estimators': [200, 400, 500]\n#             }\n\n# XGB_hyper_params = GridSearchCV(estimator=XGB_regressor, param_grid=parameters, n_jobs=-1, cv=5)","dee6181c":"# XGB_hyper_params.fit(X_train[:50_000], y_train[:50_000])\n# # find out the best hyper parameters\n# XGB_hyper_params.best_params_","71bf6e89":"XGB_model = XGBRegressor(learning_rate=0.3, max_depth=6, n_estimators=500)\nXGB_model.fit(X_train, y_train)\ny_XGB_predict = XGB_model.predict(X_test)\n\nXGB_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_XGB_predict)))\n\nprint(f'XGBoost Mean Squared Error - {XGB_model_error}')","18cae4b7":"# let's plot feature_importance again and check if there is any difference or not.\nsns.barplot(y=list(train_dataset.loc[:, selected_predictors].columns), x=list(XGB_model.feature_importances_))","97710403":"lgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=35, n_estimators=300)\n\nlgb_model.fit(X_train, y_train)\ny_LGB_predict = lgb_model.predict(X_test)\n\nLGB_model_error = sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_LGB_predict)))\n\nprint(f'LGBM Mean Squared Error - {LGB_model_error}')","96beb22e":"# ensembled prediction over splitted test data\nensembled_prediction = (0.5*np.expm1(y_XGB_predict))+(0.5*np.expm1(y_LGB_predict))\nensembled_prediction_error = sqrt(mean_squared_error(np.expm1(y_test), ensembled_prediction))\n\nprint(f'Ensembled Mean Squared Error - {ensembled_prediction_error}')","1e37f390":"# making prediction using test_dataset predictors\ny_XGB_predict = np.expm1(XGB_model.predict(X_test_dataset))\n\n# submitting our predictions\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['fare_amount'] = y_XGB_predict\nsubmission.to_csv('xgb_submission.csv', index=False)\nsubmission.head(10)","b0864909":"# making prediction using test_dataset predictors\ny_LGB_predict = np.expm1(lgb_model.predict(X_test_dataset))\n\n# submitting our predictions\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['fare_amount'] = y_LGB_predict\nsubmission.to_csv('lgbm_submission.csv', index=False)\nsubmission.head(10)","d7917841":"# making prediction using test_dataset predictors\n# y_rand_forest_predict = np.expm1(rand_forest_regressor.predict(X_test_dataset))\n\n# # submitting our predictions\n# submission = pd.read_csv('..\/input\/sample_submission.csv')\n# submission['fare_amount'] = y_rand_forest_predict\n# submission.to_csv('random_forest_submission.csv', index=False)\n# submission.head(10)","5421b601":"# submitting our predictions\nensembled_prediction = (0.5*y_XGB_predict)+(0.5*y_LGB_predict)\nsubmission.to_csv('ensembled_submission.csv', index=False)\nsubmission.head(10)","913eb404":"Interesting, here atleas one thing is very clear that these journies happend only in 2009 and 2010. Let's find out some more information about this segment","402470f7":"Here, we are going to perform data cleaning, feature engineering, data visualization for which we will prefer seaborn and will train Random Forest, XGBoost & LGBM models then will compare and apply GridSearchCV parameter selection on the better one to check if it does any more improvement and at last will ensemble best predictions.\n\nThanks to kagglers, I am going through many splendid kernels and learning new techniques and approaches and yes this is my first Kaggle competition submission, will be glad to have your suggestions :)","a0919bda":"<h2>Check for anomalies in dataset<\/h2>","8e51c65b":"<h3>LightGBM<\/h3>","3091cc70":"This is good that we don't have any such row. Lets move forward.\n\nWe should also consider adding few more features which can be distances from popular New York airports:\n\n1. John F. Kennedy International Airport (JFK)\n2. Newark Liberty International Airport (EWR)\n3. LaGuardia Airport (LGA)\n\nI didn't considered this initially but after going through some kernels got that it can be an important feature which can make a reasonable difference in model predictions. So lets proceed and add them.","0b0b93ef":"Well, there are too many.  In above table If you check dropoff and pickup coordinates for same rows, then can notice that latitudes and longitudes are same but fare_amount is non-zero, which can be because of round trips, So it won't be a prudent choice to drop them.\n\nInstead we can remove records that have distance 0 with unequal pickup & dropoff coordinates","da94cfed":"Let's use pickup and dropoff co-ordinate features to add a new feature **distance**, later on it can be used as one of the significant predictor.","b09f39cc":"**Total journies on each weekdays (0 is for Monday)**","8a3b6199":"<h3>Stacked Ensemble<\/h3>","e57ae678":"Co-ordinate values are varying too much. We can take test_dataset coordinate min and max limit to drop the outliers.","0a8a52ce":"**Import required libraries**","93014dd2":"Here we are going to try few best regression models like **Random Forest Regression**, **XGBoost** and **LGBM** models along with gridSearchCV parameter selection and will prefer best two for stacked ensemble technique.\n\nFor cross checking the performance of these models, we are spliting train dataset from out test_dataset itself, since it contains the 'fare_amount' column.\n\nLet's select the predictors from features, since we should not consider all features for training our model. ","b9437372":"So object consumes most of the space. We have already observed that we have two object columns, 'pickup_datetime' and 'key', which is nothing but the same pickup_datetime value, So let's drop key column","b2749a1c":"Great! we have reduced the size of dataset more that 50%.","53e3aa5c":"Let's check first and last 5 records of train_dataset.","ff5d7058":"We are using only first 50,000 records here, otherwise it will take too much time to complete execution.","123ee4a3":"Looking good.\n\nWill do few more improvements later and update ;)","9cc77c73":"**Unvariate distribution of fare_amount**","96d79f9b":"Similarly, passanger_count also contains some outliers, idealy it should now contain values less than 1 or greater than 7","7f27b1a7":"Great! now it's type is changed to the appropriate one and we can generate many more important features from this like day, month, year, weekday, hour etc. Let's do that.","be8285f6":"It appears that we have many 0 values in distance, let's check how many such rows are there","e5f65b61":"**Calculate 'distance' between co-ordinates**","22d6b713":"**Weekdays and Months impact on fare_amount**","2038211e":"Seems fine now. We have also noted that minimum fare_amount is in negative which is not possible, at the same time we can also notice that maximum fare_amount is 500, which is not very resonable. So lets count rows where fare_amount is not between 0 and 350.","0a282361":"Means on Monday and Sunday people tends to travel less.","b8449ade":"If you have observed in unvariate distribution of 'fare_amount', It is currently right skewed","370f7713":"**Let's check for outliers**","178b36a9":"Lets use GridSearchCV for best parameter selection in XGBoost. We are going to provide three parameter sets *learning_rate*, *max_depth* & *n_estimators*, so that we can re-train this models with new best fit parameters.","36de0825":"<h2>Fare Prediction on test_dataset and final submission<\/h2>","3239e571":"<h2>Apply Regression Algorithms<\/h2>","82244eac":"<h3>Random Forest Regression<\/h3>","c65fed89":"Size is considerably reduced now. Later we will change the type of pickup_datetime as well.","0a6faa4f":"**Total distribution of 'fare_amount' and the 'distance'**","12158b54":"**Let's change pickup_datetime type to datetime**","87d122a8":"Better, now we should make prediction using stacked XGBoost and LGBM models.","fda3116f":"**Read test and train dataset**","35a6fc3f":"**Let's check for 'null' in feature values**","fcbdb1ff":"All I could find here is many similiar pickup coordinates (41.366138, -73.137390). There might be some better alternative got available after 2010 for larger journey So people must have started preferring that, again not sure ;)","bb94ac3f":"Let's check the train_dataset sorted by distance values.","6b4ebe9f":"If you see the segment on bottom-right part, you will notice many points that bit uncommon, since their distance is too much but fare is not that much, I guess it might be because of some offer\/discount was going on for long journies, not sure though. So let's plot more detailed plot including some other features as well.","b0c54934":"<h3>XGBoost Regression<\/h3>","941ddeea":"<h2>Handle dataset memory consumption<\/h2>","6d16d660":"(Commented out the above code, since it takes lots of time while commiting)\n\nNow we have best fit parameter values for XGBoost model, So lets go ahead and train this model again.","586c13d2":"**Parameter Estimation using GridSearchCV**","2f24cd12":"We can certainly fix this by using log transformation","e181aae4":"One more thing we can do - change the type of numeric values to subtypes that consumers less memory. For Eg : passenger_count's type is **int64**, which takes 8 bytes and we know that maximum one digint (non-negative) is going to get stored here for which uint is enough.","033e0920":"Here we are going to taked weighted average (as per mean squared error score) of XGBoost and LGBM.","ce858039":"<h2>Data Visualizations<\/h2>","e5271cfd":"So, here we have 3 type of feature values - float64, int64 and object. Let's check how much memory they acquire."}}