{"cell_type":{"779ddf09":"code","67e331c0":"code","b792bba8":"code","761bd291":"code","15ffdb11":"code","bab02e39":"code","3e41aa9e":"code","99e48349":"code","d09a20d7":"code","9b52e2d4":"code","16d876d9":"code","15899273":"code","864af13d":"code","719fe553":"code","01858984":"code","6dabb095":"code","6fd449f9":"code","75224cd2":"code","705db872":"code","97654da1":"code","363f48a4":"code","3f705f44":"code","8ea382f3":"code","4aec78e0":"code","13885a52":"code","5e169c5e":"code","240fa1d6":"code","53cee8b0":"code","7babf4f6":"code","291642ec":"code","daae9b11":"code","304d8afa":"code","5ea16658":"code","2e1419cc":"code","d9df5c69":"code","27ce8032":"code","8b1514d3":"code","a78bff23":"code","d8d4d08f":"code","e9d7e779":"code","69713d37":"code","4b5d3fe5":"code","4a569d88":"code","4f1cdaa0":"code","863f7dde":"code","4d7bbea7":"code","e9a32db3":"code","36a6e26e":"code","edaaa876":"code","26a66563":"code","7b1dd940":"code","8620ea45":"code","87fc31f3":"code","12979c5f":"code","5dea930b":"code","bb853d9d":"code","156c1c7c":"code","4f9bb7b4":"code","d26eabfe":"code","a95a1301":"code","ffea9954":"code","cb47b233":"code","efdd88aa":"code","c92c6e8e":"code","db05c577":"code","f1635867":"code","9a650c88":"code","7b6d6cdc":"code","b7c02a6f":"code","96166cc1":"code","cf409b67":"code","63d8c4bb":"code","fd1c9766":"code","31d3834b":"code","5b30dfab":"code","d550b37e":"code","381fb7ac":"code","51a12f24":"code","38721cc1":"code","42aa6927":"code","6135871e":"code","e55daf07":"code","ca0df25c":"code","94433b6c":"code","cedfe793":"code","3036113a":"code","0d562592":"code","c87beec0":"code","bc5e328d":"markdown","7e5223ba":"markdown","37b00369":"markdown","ad5f32a8":"markdown","a245c9cc":"markdown","c95a0600":"markdown","b9040b55":"markdown","18a2133d":"markdown","9195e639":"markdown","77de5eae":"markdown","de0cf996":"markdown","32278dda":"markdown","8d450d40":"markdown","de978bcf":"markdown","94fdeb37":"markdown","6ede5788":"markdown","f3e1e96d":"markdown","e946cc3b":"markdown"},"source":{"779ddf09":"# Basic\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\n# System\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67e331c0":"users = pd.read_csv('\/kaggle\/input\/bx-csv-dump\/BX-Users.csv', error_bad_lines=False, delimiter=';', encoding = 'ISO-8859-1') #encoding = \"latin-1\"","b792bba8":"books = pd.read_csv('\/kaggle\/input\/bx-csv-dump\/BX-Books.csv', error_bad_lines=False, delimiter=';', encoding = 'ISO-8859-1') #encoding = \"latin-1","761bd291":"ratings = pd.read_csv('\/kaggle\/input\/bx-csv-dump\/BX-Book-Ratings.csv', error_bad_lines=False, delimiter=';', encoding = 'ISO-8859-1')","15ffdb11":"users.shape","bab02e39":"books.shape","3e41aa9e":"ratings.shape","99e48349":"users.columns","d09a20d7":"ratings.columns","9b52e2d4":"books.columns","16d876d9":"data = pd.merge(ratings, users, on='User-ID', how='inner')","15899273":"data = pd.merge(data, books, on='ISBN', how='inner')","864af13d":"# Check\ndata.columns","719fe553":"#data.rename(columns={'Book-Rating':'BookRating', 'User-ID':'UserID'},inplace=True)","01858984":"# Drop (TODO: image analysis?)\n'''to_drop = ['Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n\ndata = data.drop(to_drop, axis=1, inplace=False)'''","6dabb095":"data.shape","6fd449f9":"print('Size of the dataset is: ', data.memory_usage().sum() \/ 1024**2, ' MB')","75224cd2":"# TODO: EDA in Power BI","705db872":"data.shape","97654da1":"data.head(5)","363f48a4":"data.info()","3f705f44":"print('Number of books: ', data['ISBN'].nunique())","8ea382f3":"print('Number of users: ',data['User-ID'].nunique())","4aec78e0":"print('Missing data [%]')\nround(data.isnull().sum() \/ len(data) * 100, 4)","13885a52":"sns.distplot(data['Age'].dropna(), kde=False)","5e169c5e":"print('Number of outliers: ', sum(data['Age'] > 100))","240fa1d6":"data['Book-Rating'] = data['Book-Rating'].replace(0, None)","53cee8b0":"sns.countplot(x='Book-Rating', data=data)","7babf4f6":"print('Average book rating: ', round(data['Book-Rating'].mean(), 2))","291642ec":"# Publication by Year\nyear = pd.to_numeric(data['Year-Of-Publication'], 'coerse').fillna(2099, downcast = 'infer')\nsns.distplot(year, kde=False, hist_kws={\"range\": [1945,2020]})","daae9b11":"country = data['Location'].apply(lambda row: str(row).split(',')[-1])\ndata.groupby(country)['Book-Rating'].count().sort_values(ascending=False).head(10)","304d8afa":"# Cast to numeric\ndata['Year-Of-Publication'] = pd.to_numeric(data['Year-Of-Publication'], 'coerse').fillna(2099, downcast = 'infer')","5ea16658":"data['Book-Rating'] = data['Book-Rating'].replace(0, None)","2e1419cc":"data['Age'] = np.where(data['Age']>90, None, data['Age'])","d9df5c69":"# Categorical feautes\ndata[['Book-Author', 'Publisher']] = data[['Book-Author', 'Publisher']].fillna('Unknown')","27ce8032":"# Check cat features\ndata[['Book-Author', 'Publisher']].isnull().sum()","8b1514d3":"# Age\nmedian = data[\"Age\"].median()\nstd = data[\"Age\"].std()\nis_null = data[\"Age\"].isnull().sum()\nrand_age = np.random.randint(median - std, median + std, size = is_null)\nage_slice = data[\"Age\"].copy()\nage_slice[pd.isnull(age_slice)] = rand_age\ndata[\"Age\"] = age_slice\ndata[\"Age\"] = data[\"Age\"].astype(int)","a78bff23":"# Check Age\ndata['Age'].isnull().sum()","d8d4d08f":"data['Country'] = data['Location'].apply(lambda row: str(row).split(',')[-1])","e9d7e779":"# Drop irelevant\ndata = data.drop('Location', axis=1)","69713d37":"data['Country'].head()","4b5d3fe5":"#TODO: country\/language analysis (Babel lib?)","4a569d88":"#en_list = ['usa', 'canada', 'united kingdom', 'australia']","4f1cdaa0":"#data[data['Country'].isin(en_list)]","863f7dde":"df = data","4d7bbea7":"# Relevant score\ndf = df[df['Book-Rating'] >= 6]","e9a32db3":"# Check\ndf.groupby('ISBN')['User-ID'].count().describe()","36a6e26e":"df = df.groupby('ISBN').filter(lambda x: len(x) >= 5)","edaaa876":"df.groupby('User-ID')['ISBN'].count().describe()","26a66563":"df = df.groupby('User-ID').filter(lambda x: len(x) >= 5)","7b1dd940":"df.shape","8620ea45":"df_p = df.pivot_table(index='ISBN', columns='User-ID', values='Book-Rating')","87fc31f3":"# Select users who liked LOTR\nlotr = df_p.ix['0345339703'] # Lord of the Rings Part 1\nlike_lotr = lotr[lotr == 10].to_frame().reset_index()\nusers = like_lotr['User-ID'].to_frame()","12979c5f":"# Trim original dataset\nliked = pd.merge(users, df, on='User-ID', how='inner')","5dea930b":"rating_count = liked.groupby('ISBN')['Book-Rating'].count().to_frame()","bb853d9d":"rating_mean = liked.groupby('ISBN')['Book-Rating'].mean().to_frame()","156c1c7c":"rating_count.rename(columns={'Book-Rating':'Rating-Count'}, inplace=True)","4f9bb7b4":"rating_mean.rename(columns={'Book-Rating':'Rating-Mean'}, inplace=True)","d26eabfe":"liked = pd.merge(liked, rating_count, on='ISBN', how='inner')","a95a1301":"liked = pd.merge(liked, rating_mean, on='ISBN', how='inner')","ffea9954":"liked['Rating-Mean'] = liked['Rating-Mean'].round(2)","cb47b233":"liked['Rating-Count'].hist()","efdd88aa":"C = liked['Rating-Mean'].mean()\nC","c92c6e8e":"m = rating_count.quantile(.995)[0] # .9\nm","db05c577":"# IMDB formula; source: http:\/\/trailerpark.weebly.com\/imdb-rating.html?source=post_page---------------------------\ndef weighted_rating(x, m=m, C=C):\n    v = x['Rating-Count']\n    R = x['Rating-Mean']\n\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","f1635867":"# Create relevant sub-dataset\nliked_q = liked.copy().loc[liked['Rating-Count'] >= m]\nliked_q.shape","9a650c88":"liked_q['Score'] = liked_q.apply(weighted_rating, axis=1)","7b6d6cdc":"top_r = liked_q[['Book-Title', 'Rating-Mean']]","b7c02a6f":"top_r = top_r.groupby(['Book-Title'])['Rating-Mean'].mean().to_frame()","96166cc1":"top_r.sort_values(by='Rating-Mean', ascending=False).head(10)","cf409b67":"top_p = liked[['Book-Title', 'Rating-Count']]","63d8c4bb":"top_p = top_p.groupby(['Book-Title'])['Rating-Count'].mean().to_frame()","fd1c9766":"top_p.sort_values(by='Rating-Count', ascending=False).head(10)#.plot(kind='barh')","31d3834b":"from tqdm import tqdm\nfrom gensim.models import Word2Vec \nimport random","5b30dfab":"users = df[\"User-ID\"].unique().tolist()\nlen(users)","d550b37e":"# shuffle users ID's\nrandom.shuffle(users)\n\n# extract 90% of customer ID's\nusers_train = [users[i] for i in range(round(0.9*len(users)))]\n\n# split data into train and validation set\ntrain_df = df[df['User-ID'].isin(users_train)]\nvalidation_df = df[~df['User-ID'].isin(users_train)]","381fb7ac":"# list to capture purchase history of the customers\nreads_train = []\n\n# populate the list with the product codes\nfor i in tqdm(users_train):\n    temp = train_df[train_df[\"User-ID\"] == i][\"ISBN\"].tolist()\n    reads_train.append(temp)","51a12f24":"# list to capture purchase history of the customers\nreads_val = []\n\n# populate the list with the product codes\nfor i in tqdm(validation_df['User-ID'].unique()):\n    temp = validation_df[validation_df[\"User-ID\"] == i][\"ISBN\"].tolist()\n    reads_val.append(temp)","38721cc1":"# train word2vec model\nmodel = Word2Vec(window = 10, sg = 1, hs = 0,\n                 negative = 10, # for negative sampling\n                 alpha=0.03, min_alpha=0.0007,\n                 seed = 14)\n\nmodel.build_vocab(reads_train, progress_per=200)\n\nmodel.train(reads_train, total_examples = model.corpus_count, \n            epochs=10, report_delay=1)","42aa6927":"model.init_sims(replace=True)","6135871e":"print(model)","e55daf07":"# extract all vectors\nX = model[model.wv.vocab]\n\nX.shape","ca0df25c":"import umap\n\ncluster_embedding = umap.UMAP(n_neighbors=30, min_dist=0.0,\n                              n_components=2, random_state=42).fit_transform(X)\n\nplt.figure(figsize=(10,9))\nplt.scatter(cluster_embedding[:, 0], cluster_embedding[:, 1], s=3, cmap='Spectral')","94433b6c":"books = train_df[[\"ISBN\", \"Book-Title\"]]\n\n# remove duplicates\nbooks.drop_duplicates(inplace=True, subset='ISBN', keep=\"last\")\n\n# create product-ID and product-description dictionary\nbooks_dict = books.groupby('ISBN')['Book-Title'].apply(list).to_dict()","cedfe793":"# Find LOTR\ndf[df['Book-Title'].str.contains('Lord of the Rings')].sample()","3036113a":"# Check\nbooks_dict['0345339703']","0d562592":"def similar_books(v, n = 15):\n    \n    # extract most similar products for the input vector\n    ms = model.similar_by_vector(v, topn= n+1)[1:]\n    \n    # extract name and similarity score of the similar products\n    new_ms = []\n    for j in ms:\n        pair = (books_dict[j[0]][0], j[1])\n        new_ms.append(pair)\n        \n    return new_ms ","c87beec0":"# Recommend\nsimilar_books(model['0345339703'])","bc5e328d":"# Feature Engineering","7e5223ba":"## Add measures (aggregate) ","37b00369":"# Embedings (word2vec)","ad5f32a8":"## Extract features","a245c9cc":"# Simple recommendation","c95a0600":"## Impute nulls","b9040b55":"## Handle outliers","18a2133d":"# Introduction","9195e639":"# Load libraries","77de5eae":"The idea behind this is that different movies are connected via the same user and thus will have high cosine similarity. What this truely shows is how books \"travels\" - remeber, this is book crossing project. For more information about word2vec see: http:\/\/www.bicorner.net\/?p=194&preview=true","de0cf996":"## Most popular (TOP 10)","32278dda":"# Prepeare dataset","8d450d40":"## Change data-type","de978bcf":"## Best rated (TOP 10)","94fdeb37":"# EDA","6ede5788":"**Problem: Book recommendations - \u201eI like Lord of the Rings, what else should I read?\u201c**\n\nGoal:\n\n- take some data, try some approaches, produce some code, get some results\n\n- then come and show us your solution and have a chat around it - show how you think about a specific problem, how you are able to explain what approach you used and why, think about the limitations of the approach and how things could be improved if there was more time, what you think of the results and if they make sense, etc.\n\nPhilosophy:\n\n- the actual result and code are not that important \u2013 the journey there and potential future paths are more important\n\n- if you are able to follow-through with some ideas then great, if you just start something and have a clear idea on how to proceed that is also useful\n\n- the expectation is that you will spend an evening or two with the task (but there are no bounds to proactivity if you enjoy playing with the problem)\n\nData: - available open dataset: http:\/\/www2.informatik.uni-freiburg.de\/~cziegler\/BX\/\n\n- alternatively feel free to grab any other relevant data set\n\nTools:\n\n- use whatever you are comfortable in (R, Python, Matlab, Java, SQL,\u2026) or feel free to use it as an opportunity to try out a new language\n\n- it is not a contest in finding the best black-box library and blindly using it \u2013 own solutions are preferred even if they are simple\n\n- how to present: up to you \u2013 slides, walking through code, drawing, \u2026 ","f3e1e96d":"# Load data","e946cc3b":"### Select English speaking countries "}}