{"cell_type":{"7d4556f4":"code","969d3390":"code","42c199bc":"code","b2e76973":"code","59aaf5e3":"code","4961a7cf":"code","dc4c62c9":"code","4c75e3ca":"code","cd07669c":"code","ac7652b7":"code","a8b38aab":"code","c72ea268":"code","bb7eef03":"code","7d2d55b4":"code","ef3daeab":"code","729a8d3c":"code","80345be4":"code","467882bc":"code","c20e164c":"code","e0cabeae":"code","71699830":"code","f0a29733":"code","38a89778":"markdown","b60b4a2c":"markdown","7020a45a":"markdown","9decf0ba":"markdown","7acc9f5f":"markdown","00d16d47":"markdown","e25496c4":"markdown","17669935":"markdown","33518b93":"markdown","af2dc44b":"markdown","57db44c6":"markdown","1a3f5791":"markdown","0e4bc2c4":"markdown"},"source":{"7d4556f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier ,VotingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier                \n\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint(os.listdir(\"..\/input\"))","969d3390":"train_data= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col = 0, header=0)\ntest_data= pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col = 0, header=0)\ntrain_data.head()","42c199bc":"test_data.head()","b2e76973":"print('Train:\\n',train_data.info())\nprint('\\nTest:\\n',test_data.info())","59aaf5e3":"print(\"Train:\\n\",train_data.isnull().sum()\/len(train_data)*100)\nprint('\\nTest:\\n',test_data.isnull().sum()\/len(train_data)*100)","4961a7cf":"#Create a new copy of the data\nnew_data = train_data.copy()\n#Divide Age and Fare into 4 groups\nnew_data['Fare'] = pd.cut(new_data['Fare'], 4, labels=[1, 2, 3, 4])\nnew_data['Age'] = pd.cut(new_data['Age'], 4, labels=[1, 2, 3, 4])\n","dc4c62c9":"#Plot survival rates among groups \ncolumns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Age' ,'Fare', 'Embarked', 'Survived']\nfig, ax = plt.subplots(2, 4, figsize=(12, 8))\nfor i, axi in enumerate(ax.flat):\n    if i ==7: \n        temp = new_data['Survived'].value_counts()\/(len(new_data))\n        axi.bar(temp.index, temp.values, label = columns[i], color = ['g', 'b', 'r', 'c', 'm'])\n        axi.legend()\n    else:    \n        temp = new_data.groupby(columns[i])['Survived'].mean()\n        axi.bar(temp.index, temp.values, label = columns[i], color = ['g', 'b', 'r', 'c', 'm'])\n        axi.legend()\n\n","4c75e3ca":"print(train_data.groupby('Pclass').median()['Age'])","cd07669c":"def Titanic_data_cleaning(data):\n    replace_values = data.groupby('Pclass').median()['Age'].sort_index().values\n    data.loc[(data['Age'].isnull() == 1) & (data['Pclass'] == 1), 'Age'] = replace_values[0]               \n    data.loc[(data['Age'].isnull() == 1) & (data['Pclass'] == 2), 'Age'] = replace_values[1]                \n    data.loc[(data['Age'].isnull() == 1) & (data['Pclass'] == 3), 'Age'] = replace_values[2]                \n    \n    data.fillna(value = {'Fare': data['Fare'].median(), 'Embarked': data['Embarked'].mode()[0]}, \n                      inplace = True)\n    data.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace = True)\n\n    std_scaler = StandardScaler()\n    data.loc[:, ['Age', 'Fare']] = std_scaler.fit_transform(data[['Age', 'Fare']])\n\n    data = pd.get_dummies(data)\n        \n    return data","ac7652b7":"train_data = Titanic_data_cleaning(train_data)\ntest_data = Titanic_data_cleaning(test_data)\n","a8b38aab":"print(\"Train:\\n\",train_data.info())\nprint('\\nTest:\\n',test_data.info())","c72ea268":"X_train = train_data.iloc[:, 1:].copy()\ny_train = train_data.iloc[:, 0].copy()\nX_test = test_data.copy()","bb7eef03":"# Stochastic Gradient Descent\nsgd_clf = SGDClassifier(random_state=0)\nsgd_clf.fit(X_train, y_train)\nsgd_clf_score = cross_val_score(sgd_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\nprint(\"Stochastic Gradient Descent score: \", sgd_clf_score)\nprint(sgd_clf.score(X_train, y_train))","7d2d55b4":"# Logistic regression\nlgs_clf = LogisticRegression(C = 10, solver = 'newton-cg' ,random_state=0)\n\n# param_grid = [{'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n#                 'C': [0.001 ,0.01 ,0.1, 1, 10, 100]}]\n# grid_search = GridSearchCV(lgs_clf, param_grid=param_grid, cv = 10)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n# {'C': 10, 'solver': 'newton-cg'}\n\nlgs_clf.fit(X_train, y_train)\nlgs_clf_score = cross_val_score(lgs_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\nprint(\"Logistic regression score: \", lgs_clf_score)\nprint(lgs_clf.score(X_train, y_train))","ef3daeab":"# Support Vector Machine\nsvc_clf = SVC(C = 10, gamma = 'auto' ,kernel = 'rbf', random_state = 0)\n\n# param_grid = [{'gamma': ['scale','auto'], \n#               'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#               'C': [0.001 ,0.01 ,0.1, 1, 10, 100]}]\n\n#grid_search = GridSearchCV(svc_clf, param_grid=param_grid, cv = 10)\n#grid_search.fit(X_train, y_train)\n#grid_search.best_params_\n\n# best params: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n\nsvc_clf.fit(X_train, y_train)\nsvc_clf_score = cross_val_score(svc_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\nprint(\"SVM score: \", svc_clf_score)\nprint(svc_clf.score(X_train, y_train))","729a8d3c":"# Decision Tree \ntree_clf = DecisionTreeClassifier(max_depth = 4, max_features = 6 ,random_state=0)\ntree_clf.get_params()\n\n# param_grid = [{'max_depth': [2, 4, 8, 16, 20], \n#                'max_features': [2, 4, 6, 8, 10]}\n#                ]\n\n# grid_search = GridSearchCV(tree_clf, param_grid=param_grid, cv = 10)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n\n# {'max_depth': 4, 'max_features': 6}\n\ntree_clf.fit(X_train, y_train)\ntree_clf_score = cross_val_score(tree_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n\nprint(\"Decision Tree score: \", tree_clf_score)\nprint(tree_clf.score(X_train, y_train))","80345be4":"# Using voting method to aggregate different algorithms prediction\n\nvoting_clf = VotingClassifier(estimators = [('sgd' ,sgd_clf),('lg',lgs_clf)\n        ,('svc', svc_clf),('tr', tree_clf)], voting = 'hard')\n\nvoting_clf.fit(X_train, y_train)\nvoting_clf_score = cross_val_score(voting_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n\nprint(\"Voting score: \", voting_clf_score)\nprint(voting_clf.score(X_train, y_train))","467882bc":"# Using Bagging to improve decision tree score\nbag_clf = BaggingClassifier(tree_clf, n_estimators = 500, bootstrap=True)\n\nbag_clf.fit(X_train, y_train)\n\nbag_clf_score = cross_val_score(bag_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n\nprint(\"Bagging score: \", bag_clf_score)\nprint(bag_clf.score(X_train, y_train))\n\n","c20e164c":"# Random Forest Classifier\n\nforest_clf = RandomForestClassifier(n_estimators=400,max_depth=8, max_features=4,\n                                    random_state=0)\n\n# This code may take about 30 minutes, depending on your computer.\n# param_grid = [{'n_estimators': [100, 200, 300, 400, 500], \n#               'max_depth': [2, 4, 8, 16, 20], \n#               'max_features': [2, 4, 6, 8, 10]}\n#               ]\n\n# grid_search = GridSearchCV(forest_clf, param_grid=param_grid, cv = 10)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n\n# {'max_depth': 8, 'max_features': 4, 'n_estimators': 400}\n\nforest_clf.fit(X_train, y_train)\n\nforest_clf_score = cross_val_score(forest_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n\nprint(\"Random Forest score: \", forest_clf_score)\nprint(forest_clf.score(X_train, y_train))\n","e0cabeae":"# Gradient Boosting Classifier\n\ngrdb_clf = GradientBoostingClassifier(max_depth = 4, max_features = 10 , n_estimators=101 ,random_state=0)\n\n# param_grid = [{'max_depth': [2, 4, 8, 16, 20], \n#                'max_features': [2, 4, 6, 8, 10]}\n#                ]\n\n# grid_search = GridSearchCV(grdb_clf, param_grid=param_grid, cv = 10)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n\n# {'max_depth': 4, 'max_features': 10}\n\ngrdb_clf.fit(X_train, y_train)\n\ngrdb_clf_score = cross_val_score(grdb_clf, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n\nprint(\"Gradient Boosting score: \", grdb_clf_score)\nprint(grdb_clf.score(X_train, y_train))\n","71699830":"rfs_clf = forest_clf = RandomForestClassifier(n_estimators=110, max_depth= 8, max_features='auto',\n                                    random_state=0, oob_score=False, min_samples_split = 2,\n                                   criterion= 'gini', min_samples_leaf=2, bootstrap=False)\nrfs_clf.fit(X_train, y_train)\ny_f_predict = rfs_clf.predict(X_test)","f0a29733":"output = pd.DataFrame({'PassengerId': test_data.index, \n                       'Survived': y_f_predict})\noutput.to_csv('submission.csv', index=False)","38a89778":"The cabin feature contains 77% and 37% null data in the training and test sets. So, I think we can drop these columns, as they do not add much value. \nThe Fare column has just one value missing(0.11%) in the test set, so it is not a big deal. We will replace this value with the median of the feature. \nThe age column has a 20% missing value in the training set and 10% in the test set. One way to get around this problem is to simply replace the missing value with the median. However, there is a better approach that I will show later on. \nBut, first, let's get some insight from the data and find out the survival rate among different categories of our features.\n","b60b4a2c":"Based on our classifier algorithms, it is evident that Random Forest Classifier and Gradient Boosting Classifier give us the best Cross-validation score. A score of more than 84%, which is considered a pretty good score.","7020a45a":"In the training set, columns Age and Cabin and from test data, columns Age, Fare, and Cabin contain null data. First, I want to know what percentage of these columns has invalid data and how important they are:","9decf0ba":"Let's get some insight from the data. ","7acc9f5f":"By fine tuning Random Forest Classifier, we can improve the accuracy score. \nI will use RandomForest for submission.","00d16d47":"Up to now, our best estimator was SVM. We are trying to test two more classifiers, Random Forest and Gradient Boosting.\n","e25496c4":"The above plot gives us useful information. Unfortunately, more than 60% of the passengers died in the tragic event. Nevertheless, there is some meaningful higher chance of survival among some categories. For example, being a female gives a significantly higher chance of survival than a male (more than 70% compared to less than 20%), or we can see that people in the fourth Fare group all survived. First-class passengers have a significantly higher chance of survival. Male and 3rd class passengers are less likely to survive (poor Jack!). The oldest people have less chance of staying alive and so on. There is more helpful information in other categories as well.","17669935":"Now we define train and test sets. ","33518b93":"For this purpose, I will divide the Age and Fare features, which have continuous data, into four groups. Then, I will calculate the survival rate based on their group for all the categorical features.","af2dc44b":"Now that everything is sorted out, we start feeding our data to different machine learning algorithms. I will use cross-validation score with 10 validation to measure the algorithm's accuracy. Fur tuning the hyperparameter, I will use grid search. However, as the grid search may take a pretty long time to be run in some algorithms, I will comment out the code for the grid search and just show the results.","57db44c6":"Now that we have some understanding of the data let's jump in and clean our data for machine learning algorithms. \nFirst of all, instead of replacing the missing ages with the overall median, the following code shows that each Pclass have a different median age. It's better to replace the missing value based on their Pclass median Age. So, we replace 1st, 2nd and 3rd  class missing ages with 37, 29, 24, respectively.\n","1a3f5791":"We then replace Fare and Embarked missing values with median and most frequent ones. Drop the unwanted columns, Standardise the continuous data (Fare, Age) and encode the categorical features using the one-hot encoder. The following function does the job for us: ","0e4bc2c4":"We see that our data is now clean and have no missing values. "}}