{"cell_type":{"c0608b9e":"code","9794d1dc":"code","8a4cbd4d":"code","316f7728":"code","b7c25c40":"code","86b69f1e":"code","104a983e":"code","f98ea932":"code","49a4ff03":"code","b9f8736e":"code","e039d5e0":"code","a2b2489f":"code","9f19bb66":"code","bb7ce986":"code","79e7662c":"code","d9ca406e":"code","0ed56f7c":"code","3426ceaf":"code","1fb88782":"code","7dd9672b":"code","c9f5701e":"code","7c390a7e":"code","b6ca73f8":"code","e47200bf":"code","1a6f5f4d":"code","775d5195":"code","cd9dcdf7":"markdown","69c247e6":"markdown","5f4bc614":"markdown","f07fa0a2":"markdown","3f988747":"markdown","d2b24db2":"markdown","4b4668cc":"markdown","72a74575":"markdown","76ed9bbd":"markdown","2f526f66":"markdown","63e2c09d":"markdown","4b7ab2d5":"markdown","8233d8b5":"markdown","e5c1b5cf":"markdown","ac8ffc5a":"markdown","6c319726":"markdown","5b1e6e08":"markdown","5342554e":"markdown","9684425d":"markdown","5d3d5d3d":"markdown","aa1480b1":"markdown","38da47cf":"markdown","043cca7b":"markdown","ca277fc4":"markdown","7a1324a7":"markdown","f11791fb":"markdown","ba41c7bc":"markdown","6e9b3975":"markdown","b17d457d":"markdown","88323e77":"markdown","9e5d6012":"markdown","0dc3c7d7":"markdown","7aad015a":"markdown","a0da52c8":"markdown","b246939e":"markdown","f88d488b":"markdown","83508cad":"markdown","af63d7bd":"markdown","bbff3a1d":"markdown","605844ba":"markdown","becf5040":"markdown","07e902ff":"markdown"},"source":{"c0608b9e":"!pip install -U swifter \n!pip install perfplot==0.8.3","9794d1dc":"import pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport multiprocessing\nimport swifter\nimport warnings\nwarnings.filterwarnings('ignore')","8a4cbd4d":"import seaborn as sns\nsns.set(style=\"whitegrid\", font_scale=1.75)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 24}\n\n# prettify plots\nplt.rcParams['figure.figsize'] = [20.0, 5.0]\n    \n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","316f7728":"%%time\ndata = pd.DataFrame()\ndata['col1'] = np.random.normal(size = 15_000_000) # 1.5 to 15 million \ndata['col2'] = np.random.normal(size = 15_000_000) # 1.5 to 15 million \n\n# function taking in data directly as parameters and working directly on them\n# broadcasting of data - leading to vectorization\ndef myfunc(x,y): return y*(x**2+1)","b7c25c40":"%%time\ndf = data.apply(lambda row: myfunc(row.col1,row.col2), axis=1)","86b69f1e":"%%time \nddata = dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","104a983e":"%%time \nddata = dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='threads')","f98ea932":"%%time \nddata = dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='single-threaded')","49a4ff03":"%%time \nddata = dd.from_pandas(data, npartitions=1)\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","b9f8736e":"%%time \nddata = dd.from_pandas(data, npartitions=100)\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","e039d5e0":"%%time\ndf = data.swifter.apply(lambda row: myfunc(row.col1,row.col2), axis=1)","a2b2489f":"%%time\ndf = myfunc(data['col1'], data['col2'])","9f19bb66":"df","bb7ce986":"def setup_smaller_dataframe(n):\n    # num_of_datapoints = 1_500_000 # 1.5 million \n    num_of_datapoints = 15_000 # should be enough to show the performance numbers\n    \n    smaller_data = pd.DataFrame()\n    smaller_data['col1'] = np.random.normal(size = num_of_datapoints)\n    smaller_data['col2'] = np.random.normal(size = num_of_datapoints)\n    return smaller_data","79e7662c":"def using_pandas_apply(data):\n    return data.apply(lambda row: myfunc(row.col1,row.col2), axis=1)","d9ca406e":"def using_dask_many_nparts_processes(data):\n    return dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","0ed56f7c":"def using_dask_many_nparts_threads(data):\n    return dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='threads')","3426ceaf":"def using_dask_many_nparts_single_threaded(data):\n    return dd.from_pandas(data, npartitions=2*multiprocessing.cpu_count())\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler=\"single-threaded\")","1fb88782":"def using_dask_1_nparts_processes(data):\n    return dd.from_pandas(data, npartitions=1)\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","7dd9672b":"def using_dask_100_nparts_processes(data):\n    return dd.from_pandas(data, npartitions=100)\\\n            .map_partitions(lambda df: df.apply((lambda row: myfunc(row.col1,row.col2)), axis=1))\\\n            .compute(scheduler='processes')","c9f5701e":"def using_swifter(data):\n    return data.swifter.apply(lambda row: myfunc(row.col1,row.col2), axis=1)","7c390a7e":"def using_vectorization_method(data):\n    return myfunc(data['col1'], data['col2'])","b6ca73f8":"%%time\nimport perfplot  \nimport pandas as pd\nimport numpy as np\n\nslower_ones = perfplot.bench(\n    setup=setup_smaller_dataframe,\n    kernels=[\n        lambda data: using_pandas_apply(data),\n        lambda data: using_dask_many_nparts_processes(data),\n        lambda data: using_dask_many_nparts_threads(data),\n        lambda data: using_dask_many_nparts_single_threaded(data),\n        lambda data: using_dask_1_nparts_processes(data),\n        lambda data: using_dask_100_nparts_processes(data),\n        lambda data: using_swifter(data),\n        lambda data: data # as good as noop\n    ],\n    labels=['using_pandas_apply', \n            'using_dask_many_nparts_processes', \n            'using_dask_many_nparts_threads',\n            'using_dask_many_nparts_single_threaded',\n            'using_dask_1_nparts_processes',\n            'using_dask_100_nparts_processes',\n            'using_swifter',\n            'noop'\n    ],\n    n_range=[k for k in range(0, 5)],\n    xlabel='iterations',\n    equality_check=None\n)","e47200bf":"plt.rcParams['figure.figsize'] = [20.0, 15.0]\nmatplotlib.rc('font', **font)\nslower_ones.show(logy=True, time_unit=\"ms\")","1a6f5f4d":"%%time\nfaster_ones = perfplot.bench(\n    setup=setup_smaller_dataframe,\n    kernels=[   \n        lambda data: using_swifter(data),\n        lambda data: using_vectorization_method(data),\n        lambda data: data # as good as noop\n    ],\n    labels=[\n        'using_swifter',\n        'using_vectorization_method',\n        'noop'\n    ],\n    n_range=[k for k in range(0, 10)],\n    xlabel='iterations',\n    equality_check=None\n)","775d5195":"plt.rcParams['figure.figsize'] = [20.0, 15.0]\nmatplotlib.rc('font', **font)\nfaster_ones.show(logy=\"auto\", time_unit=\"ms\")","cd9dcdf7":"## Method 2a: Applying mutations to dataframe using `dask` `dataframes`","69c247e6":"- [Original notebook](https:\/\/github.com\/geodra\/Pandas-Tricks) turned into this kernel - Thanks [@geodara](https:\/\/github.com\/geodra)\n- [High Performance Python: 2nd edition](https:\/\/www.amazon.co.uk\/High-Performance-Python-Performant-Programming\/dp\/1492055026\/ref=sr_1_1?dchild=1&keywords=high+performance+python&qid=1598560883&sr=8-1) | on [OReilly's Safari](https:\/\/learning.oreilly.com\/library\/view\/high-performance-python\/9781492055013)\n- [Python programming resources](https:\/\/github.com\/neomatrix369\/awesome-ai-ml-dl\/blob\/master\/Programming-in-Python.md)\n- [A number of Python Performance resources and notes](https:\/\/github.com\/neomatrix369\/awesome-ai-ml-dl\/blob\/master\/Python-Performance.md)\n","5f4bc614":"<a id='smaller-dataset'><\/a>\n\n----------\n\n## Create another dataframe with a bit smaller number of data points\n\n### Running the above processes by comparing them using `timeit` and `perfplot`, `timeit` is built into `perfplot`. We will be repeatedly running the above processes, so a smaller dataset is suitable, to reduce the long wait.","f07fa0a2":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","3f988747":"## Finally the processed dataframe","d2b24db2":"<a id='method-2a'><\/a>\n\n----------\n\n## Method 2a: Applying mutations to dataframe using `dask` `dataframes`\n\n### Passing the columns to mutate as a parameter to the function being used. Using multi-processing to process the records in the dataframe, in addition to using `apply()`. Using `npartitions` and `processes` as the scheduler.","4b4668cc":"##### Measuring performance using `%%time: using `%%time` is less reliable although gives quick results","72a74575":"<a id='method-2e'><\/a>\n\n----------\n\n## Method 2e: Applying mutations to dataframe using `dask` `dataframes`\n\n### Passing the columns to mutate as a parameter to the function being used. Using multi-processing to process the records in the dataframe, in addition to using `apply()`. Using `100` `npartitions` and `processes` as the scheduler.","76ed9bbd":"## Method 2b: Applying mutations to dataframe using `dask` `dataframes`","2f526f66":"<a id='method-1'><\/a>\n\n----------\n\n## Method 1: Applying mutations to dataframe using `pandas` `apply()`\n\n### Passing the columns to mutate as a parameter to the function being used. This applies the mutations usin gthe function provided a row at a time.","63e2c09d":"<a id='method-2c'><\/a>\n\n----------\n\n## Method 2c: Applying mutations to dataframe using `dask` `dataframes`\n\n### Passing the columns to mutate as a parameter to the function being used. Using multi-processing to process the records in the dataframe, in addition to using `apply()`. Using `npartitions` and a `single-threaded` scheduler.","4b7ab2d5":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","8233d8b5":"## Method 3: Applying mutations to dataframe using `swifter`","e5c1b5cf":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","ac8ffc5a":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","6c319726":"## Method 2c: Applying mutations to dataframe using `dask` `dataframes`","5b1e6e08":"<a id='methods-perfplot'><\/a>\n\n----------\n\n## Method 1: Applying mutations to dataframe using `pandas` `apply()`","5342554e":"<a id='method-3'><\/a>\n\n----------\n\n## Method 3: Applying mutations to dataframe using `swifter`\n\n### Passing the columns to mutate as a parameter to the function being used.","9684425d":"<a id='method-4'><\/a>\n\n----------\n\n## Method 4: Applying mutations to dataframe using a vectorization method\n\n### Passing the columns to mutate as a parameter to the function being used.","5d3d5d3d":"<a id='import-libraries'><\/a>\n\n----------\n\n## Import libraries and packages","aa1480b1":"## Method 2d: Applying mutations to dataframe using `dask` `dataframes`\n\n### Passing the columns to mutate as a parameter to the function being used. Using multi-processing to process the records in the dataframe, in addition to using `apply()`. Using `1` `npartitions` and `processes` as the scheduler.","38da47cf":"### We are only using 1\/10 of the amount of data as in the previous section but `perfplot` rinses and repeats for multiple times, this more reliable than using just `%%time` and running it once. For better validation of the hypothesis, we should repeat the experiments multiple times and measure the outcome and then comparisons are more meaningful.","043cca7b":"#### Note: the below will take a good 16-20 minutes to finish, best to let it run as a batch process and then come back to observe the results.","ca277fc4":"<a id='large-dataset'><\/a>\n\n----------\n\n## Create a dataframe with a large number of data points\n","7a1324a7":"<a id='summary'><\/a>\n\n----------\n\n## Summary","f11791fb":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","ba41c7bc":"<a id='resources'><\/a>\n\n----------\n\n## Resources","6e9b3975":"<a id='ToC'><\/a>\n\n----------\n\n# Table of contents\n\n- [Install libraries\/packages](#install-libraries)\n- [Import libraries\/packages](#import-libraries)\n- [Large Dataset](#training-dataset)\n   - [Method 1](#method-1)\n   - [Method 2a](#method-2a)\n   - [Method 2b](#method-2b)\n   - [Method 2c](#method-2c)\n   - [Method 2d](#method-2d)\n   - [Method 2e](#method-2e)\n   - [Method 3](#method-3)\n   - [Method 4](#method-4)\n- [Smaller Dataset](#smaller-dataset) \n   - [All above methods using perfplot](#methods-perfplot)\n- [Summary](#summary)   \n- [Resources](#resources)","b17d457d":"- this kernel is a result of the [ML Tokyo Study group: high-perf-python](https:\/\/machinelearningtokyo.slack.com\/archives\/C01938EJPRA) (it's run on the back of the 2nd edition of book with the same title by Ian Ozsvald and Micha)\n- Performance and measuring performance is a amazing and dynamic topic and task to do, you can see this from our ongoing conversation on these threads: [1](https:\/\/machinelearningtokyo.slack.com\/archives\/C01938EJPRA\/p1598110378026400) | [2](https:\/\/machinelearningtokyo.slack.com\/archives\/C01938EJPRA\/p1598262503066900). Nothing is set in stone and the tables can turn as we measure using one method against the other. Let's measure and not guess, and there's always some observer effect, we need to learn, stay alert and be aware of them.\n- there are a number of gotchas in Python, when you know them you can write neat and highly performant code\n- Pandas has a lot of slow function, there are numpy and other equivalents of them\n- There's a 101 ways to speed up a lot of the Python code especially those written using libraries like Pandas\n- Packages, frameworks and libraries are written daily to speed such older tools \n- Python provides some really neat measuring tools and visualisers\n- See the Resources section for more things to take away and do in your spare time","88323e77":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","9e5d6012":"## Method 4: Applying mutations to dataframe using a vectorization method","0dc3c7d7":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","7aad015a":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","a0da52c8":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","b246939e":"<a id='install-libraries'><\/a>\n\n----------\n\n## Install libraries and packages","f88d488b":"<a id='method-2b'><\/a>\n\n----------\n\n## Method 2b: Applying mutations to dataframe using `dask` `dataframes`\n\n### Passing the columns to mutate as a parameter to the function being used. Using multi-processing to process the records in the dataframe, in addition to using `apply()`. Using `npartitions` and `threads` as the scheduler.","83508cad":"## Method 2d: Applying mutations to dataframe using `dask` `dataframes`","af63d7bd":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","bbff3a1d":"#### The below will take a good 20+ minutes to finish, best to let it run as a batch process and then come back to observe the results.","605844ba":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","becf5040":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents<\/span><\/a>","07e902ff":"## Method 2e: Applying mutations to dataframe using `dask` `dataframes`"}}