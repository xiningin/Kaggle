{"cell_type":{"9db0e64e":"code","c24de11c":"code","adbcaab7":"code","b4131f1e":"code","f491f01c":"code","30a2ea06":"code","28cb10a3":"code","b8fa2ccb":"code","264980b3":"code","bed6819d":"code","ec764465":"code","960f6d2d":"code","0875f9da":"code","8d535b9a":"code","2762fa17":"code","215d7b41":"code","2a80ff54":"code","b49bc515":"code","bf490a4f":"code","11995dbf":"code","331db3ab":"code","7e2644a4":"code","0ff7f604":"code","d7208ffa":"code","31c0a88a":"code","21eebb8c":"code","a4e55a03":"code","6589363a":"code","a314c283":"code","5c1973d5":"code","1286fa98":"code","25b44e5b":"code","b402448c":"code","76fbbe9d":"code","bf317209":"code","85c2dde4":"code","b8d350c4":"code","a8251fdf":"code","a66ff4f5":"code","2fe1a0b8":"code","ec72ef3a":"code","4a9af513":"code","b1274450":"code","af72e707":"markdown","592ac2b6":"markdown","43f4ae6f":"markdown","f6aa1219":"markdown","46a7fbaa":"markdown","122ed4a4":"markdown","43e3f3ca":"markdown","9901dfff":"markdown","bcc00a3a":"markdown","0f6a91d6":"markdown","dd980b92":"markdown","61c90027":"markdown","a514d18a":"markdown","3dfa0a46":"markdown","ad3f0859":"markdown","150091ce":"markdown","8fbfb2f6":"markdown","2c487fbe":"markdown","769fd62a":"markdown","089c52d2":"markdown","d3bdc12f":"markdown","bc890cea":"markdown","609bf6a1":"markdown","34dda68c":"markdown","1c78d8b3":"markdown","50675185":"markdown","570c7e58":"markdown"},"source":{"9db0e64e":"!pip install tensorflow-gpu==1.13.1","c24de11c":"import os\nimport gc\nimport sys\nimport json\nimport glob\nimport random\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport itertools\nfrom tqdm import tqdm\n\nfrom imgaug import augmenters as iaa\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport tensorflow","adbcaab7":"!pip3 show tensorflow-gpu","b4131f1e":"import tensorflow as tf\nwith tf.device('\/gpu:0'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n\nwith tf.Session() as sess:\n    print (sess.run(c))\n","f491f01c":"with tf.Session() as sess:\n  devices = sess.list_devices()\ndevices","30a2ea06":"DATA_DIR = Path(r'\/kaggle\/working\/Mask_RCNN\/fashion_dataset')\nROOT_DIR = Path(r'\/kaggle\/working\/Mask_RCNN')\n\n# For demonstration purpose, the classification ignores attributes (only categories),\n# and the image size is set to 512, which is the same as the size of submission masks\nNUM_CATS = 46\nIMAGE_SIZE = 512","28cb10a3":"!git clone https:\/\/github.com\/IITGuwahati-AI\/Mask_RCNN.git\n\n\n!rm -rf .git # to prevent an error when the kernel is committed\n!rm -rf images assets # to prevent displaying images at the bottom of a kernel","b8fa2ccb":"pwd","264980b3":"sys.path.append(ROOT_DIR)\nfrom mrcnn.config import Config\nfrom mrcnn import utils_for_FGC\nimport mrcnn.model_for_FGC as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model_for_FGC import log","bed6819d":"# !wget --quiet https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n# !ls -lh mask_rcnn_coco.h5\n\n\nCOCO_WEIGHTS_PATH = Path(r'\/kaggle\/working\/Mask_RCNN\/mask_rcnn_coco.h5')\nNUM_CATS = 46\nIMAGE_SIZE = 1024","ec764465":"class FashionConfig(Config):\n    NAME = \"fashion\"\n    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 2 # a memory error occurs when IMAGES_PER_GPU is too high\n    \n    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n    #DETECTION_NMS_THRESHOLD = 0.0\n    \n    # STEPS_PER_EPOCH should be the number of instances \n    # divided by (GPU_COUNT*IMAGES_PER_GPU), and so should VALIDATION_STEPS;\n    # however, due to the time limit, I set them so that this kernel can be run in 9 hours\n    STEPS_PER_EPOCH = 1000\n    VALIDATION_STEPS = 200\n\n    ## My changes CA\n    BACKBONE = 'resnet101'\n    \n    IMAGE_MIN_DIM = 1024\n    IMAGE_MAX_DIM = 1024    \n    IMAGE_RESIZE_MODE = 'square'\n\n    MINI_MASK_SHAPE = (112, 112)  # (height, width) of the mini-mask\n\n    NUM_ATTR = 294\n\n    LOSS_WEIGHTS = {\n        \"rpn_class_loss\": 1.,\n        \"rpn_bbox_loss\": 1.,\n        \"mrcnn_class_loss\": 1.,\n        \"mrcnn_bbox_loss\": 1.,\n        \"mrcnn_mask_loss\": 1.,\n        \"mrcnn_attr_loss\":1.\n    }\n\n\n    \nconfig = FashionConfig()\nconfig.display()","960f6d2d":"with open(DATA_DIR\/\"label_descriptions.json\") as f:\n    label_descriptions = json.load(f)\n\nclass_names = [x['name'] for x in label_descriptions['categories']]\nattr_names = [x['name'] for x in label_descriptions['attributes']]\n","0875f9da":"print(len(class_names),len(attr_names))","8d535b9a":"\nsegment_df = pd.read_csv(DATA_DIR\/\"train_small.csv\")\nsegment_df['AttributesIds'] = segment_df['AttributesIds'].apply(lambda x:tuple([int(i) for i in x.split(',')]))\n","2762fa17":"def pad_tuple_attrs(x):\n    if x!=x:\n        x = []\n    else:\n        x = list(x)\n    for i in range(len(x)):\n        if x[i]>=281 and x[i]<284:\n            x[i] = x[i]-46\n        elif x[i]>284:\n            x[i] = x[i]-47\n    \n    x = tuple(x)\n    return x\n","215d7b41":"\nsegment_df['AttributesIds'] = segment_df['AttributesIds'].apply(pad_tuple_attrs)\n","2a80ff54":"def get_one_hot(targets):\n    targets = np.array(list(targets), dtype = np.int32)\n    nb_classes = 294\n    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n    return res.reshape(list(targets.shape)+[nb_classes]).sum(axis=0)\n","b49bc515":"segment_df['AttributesIds'] = segment_df['AttributesIds'].apply(get_one_hot)","bf490a4f":"segment_df['AttributesIds'].head()","11995dbf":"image_df = segment_df.groupby('ImageId')['EncodedPixels', 'ClassId', 'AttributesIds'].agg(lambda x: list(x))\nsize_df = segment_df.groupby('ImageId')['Height', 'Width'].mean()\nimage_df = image_df.join(size_df, on='ImageId')\n\nprint(\"Total images: \", len(image_df))\nimage_df.head()\n","331db3ab":"def resize_image(image_path):\n    image_path = image_path + \".jpg\"\n    # print(\"image_path\", image_path)\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n    return img","7e2644a4":"class FashionDataset(utils_for_FGC.Dataset):\n\n    def __init__(self, df):\n        super().__init__(self)\n        \n        # Add classes\n        for i, name in enumerate(class_names):\n            self.add_class(\"fashion\", i+1, name)\n        \n        for i, name in enumerate(attr_names):\n            self.add_attribute(\"fashion\", i, name)\n        # Add images \n        for i, row in df.iterrows():\n            self.add_image(\"fashion\", \n                           image_id=row.name, \n                           path=str(DATA_DIR\/'train'\/row.name), \n                           labels=row['ClassId'],\n                           attributes=row['AttributesIds'],\n                           annotations=row['EncodedPixels'], \n                           height=row['Height'], width=row['Width'])\n\n    \n\n    def image_reference(self, image_id):\n        # attr_sublist=[]\n        attr_list=[]\n        info = self.image_info[image_id]\n        for x in info['attributes']:\n            attr_sublist=[]\n            for i, j in enumerate(x):\n                if(j==1):\n                    attr_sublist.append(attr_names[i])\n            attr_list.append(attr_sublist)\n                \n            \n            \n        return info['path'], [class_names[int(x)] for x in info['labels']],attr_list\n    \n    def load_image(self, image_id):\n        return resize_image(self.image_info[image_id]['path'])\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n                \n        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n        labels = []\n        attributes = []\n        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n            annotation = [int(x) for x in annotation.split(' ')]\n            \n            for i, start_pixel in enumerate(annotation[::2]):\n                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n\n            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n            \n            mask[:, :, m] = sub_mask\n            labels.append(int(label)+1)\n            attributes.append(list(info['attributes'][m]))\n            \n        return mask, np.array(labels), np.array([np.array(attr) for attr in attributes])","0ff7f604":"\ndataset = FashionDataset(image_df)\ndataset.prepare()\n\nfor i in range(1):\n    image_id = random.choice(dataset.image_ids)\n    print(dataset.image_reference(image_id))\n    \n    image = dataset.load_image(image_id)\n    mask, class_ids, attr_ids = dataset.load_mask(image_id)\n    # print(\"class_ids\", class_ids)\n    # print(\"attr_ids\", attr_ids)\n    # print(type(attr_ids))\n    visualize.display_top_masks(image, mask, class_ids, attr_ids, dataset.class_names, dataset.attr_names, limit=4)","d7208ffa":"# This code partially supports k-fold training, \n# you can specify the fold to train and the total number of folds here\nFOLD = 0\nN_FOLDS = 3\n\nkf = KFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\nsplits = kf.split(image_df) # ideally, this should be multilabel stratification\n\ndef get_fold():    \n    for i, (train_index, valid_index) in enumerate(splits):\n        if i == FOLD:\n            return image_df.iloc[train_index], image_df.iloc[valid_index]\n        \ntrain_df, valid_df = get_fold()\n\ntrain_dataset = FashionDataset(train_df)\ntrain_dataset.prepare()\n\nvalid_dataset = FashionDataset(valid_df)\nvalid_dataset.prepare()\n","31c0a88a":"train_segments = np.concatenate(train_df['ClassId'].values).astype(int)\nprint(\"Total train images: \", len(train_df))\nprint(\"Total train segments: \", len(train_segments))\n\nplt.figure(figsize=(12, 3))\nvalues, counts = np.unique(train_segments, return_counts=True)\nplt.bar(values, counts)\nplt.xticks(values, class_names, rotation='vertical')\nplt.show()\n\nvalid_segments = np.concatenate(valid_df['ClassId'].values).astype(int)\nprint(\"Total train images: \", len(valid_df))\nprint(\"Total validation segments: \", len(valid_segments))\n\nplt.figure(figsize=(12, 3))\nvalues, counts = np.unique(valid_segments, return_counts=True)\nplt.bar(values, counts)\nplt.xticks(values, class_names, rotation='vertical')\nplt.show()\n\n\ntrain2_segments = np.concatenate(train_df['AttributesIds'].values).astype(int).reshape((-1,))\ntrain2_segments = train2_segments[train2_segments!= -1]\n# print(\"Total train images: \", len(valid_df))\nprint(\"Total train segments: \", len(train2_segments))\n\nplt.figure(figsize=(12, 3))\nvalues, counts = np.unique(train2_segments, return_counts=True)\nplt.bar(values, counts)\nplt.xticks(values, attr_names, rotation='vertical')\nplt.show()\n\ntrain2_segments = np.concatenate(valid_df['AttributesIds'].values).astype(int).reshape((-1,))\ntrain2_segments = train2_segments[train2_segments!= -1]\n# print(\"Total train images: \", len(valid_df))\nprint(\"Total train segments: \", len(train2_segments))\n\nplt.figure(figsize=(12, 3))\nvalues, counts = np.unique(train2_segments, return_counts=True)\nplt.bar(values, counts)\nplt.xticks(values, attr_names, rotation='vertical')\nplt.show()","21eebb8c":"# # Note that any hyperparameters here, such as LR, may still not be optimal\nLR = 1e-4\n\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","a4e55a03":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)\n\nmodel.load_weights(str(COCO_WEIGHTS_PATH), by_name=True, exclude=[\n    'mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', 'mrcnn_mask'])\n","6589363a":"augmentation = iaa.Sequential([\n    iaa.Fliplr(0.5) # only horizontal flip here\n])","a314c283":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=config.LEARNING_RATE*2, # train heads with higher lr to speedup learning\n            epochs=2,\n            layers='heads',\n            augmentation=None)\n\nhistory = model.keras_model.history.history","5c1973d5":"!pip3 show tensorflow","1286fa98":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=LR,\n            epochs=EPOCHS[1],\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","25b44e5b":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=LR\/5,\n            epochs=EPOCHS[2],\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","b402448c":"epochs = range(EPOCHS[-1])\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(131)\nplt.plot(epochs, history['loss'], label=\"train loss\")\nplt.plot(epochs, history['val_loss'], label=\"valid loss\")\nplt.legend()\nplt.subplot(132)\nplt.plot(epochs, history['mrcnn_class_loss'], label=\"train class loss\")\nplt.plot(epochs, history['val_mrcnn_class_loss'], label=\"valid class loss\")\nplt.legend()\nplt.subplot(133)\nplt.plot(epochs, history['mrcnn_mask_loss'], label=\"train mask loss\")\nplt.plot(epochs, history['val_mrcnn_mask_loss'], label=\"valid mask loss\")\nplt.legend()\n\nplt.show()","76fbbe9d":"best_epoch = np.argmin(history[\"val_loss\"]) + 1\nprint(\"Best epoch: \", best_epoch)\nprint(\"Valid loss: \", history[\"val_loss\"][best_epoch-1])","bf317209":"glob_list = glob.glob(f'\/kaggle\/working\/fashion*\/mask_rcnn_fashion_{best_epoch:04d}.h5')\nmodel_path = glob_list[0] if glob_list else ''","85c2dde4":"class InferenceConfig(FashionConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\nassert model_path != '', \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","b8d350c4":"sample_df = pd.read_csv(DATA_DIR\/\"sample_submission.csv\")\nsample_df.head()","a8251fdf":"# Convert data to run-length encoding\ndef to_rle(bits):\n    rle = []\n    pos = 0\n    for bit, group in itertools.groupby(bits):\n        group_list = list(group)\n        if bit:\n            rle.extend([pos, sum(group_list)])\n        pos += len(group_list)\n    return rle","a66ff4f5":"# Since the submission system does not permit overlapped masks, we have to fix them\ndef refine_masks(masks, rois):\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    mask_index = np.argsort(areas)\n    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n    for m in mask_index:\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n        union_mask = np.logical_or(masks[:, :, m], union_mask)\n    for m in range(masks.shape[-1]):\n        mask_pos = np.where(masks[:, :, m]==True)\n        if np.any(mask_pos):\n            y1, x1 = np.min(mask_pos, axis=1)\n            y2, x2 = np.max(mask_pos, axis=1)\n            rois[m, :] = [y1, x1, y2, x2]\n    return masks, rois","2fe1a0b8":"%%time\nsub_list = []\nmissing_count = 0\nfor i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n    image = resize_image(str(DATA_DIR\/'test'\/row['ImageId']))\n    result = model.detect([image])[0]\n    if result['masks'].size > 0:\n        masks, _ = refine_masks(result['masks'], result['rois'])\n        for m in range(masks.shape[-1]):\n            mask = masks[:, :, m].ravel(order='F')\n            rle = to_rle(mask)\n            label = result['class_ids'][m] - 1\n            sub_list.append([row['ImageId'], ' '.join(list(map(str, rle))), label])\n    else:\n        # The system does not allow missing ids, this is an easy way to fill them \n        sub_list.append([row['ImageId'], '1 1', 23])\n        missing_count += 1","ec72ef3a":"submission_df = pd.DataFrame(sub_list, columns=sample_df.columns.values)\nprint(\"Total image results: \", submission_df['ImageId'].nunique())\nprint(\"Missing Images: \", missing_count)\nsubmission_df.head()","4a9af513":"submission_df.to_csv(\"submission.csv\", index=False)","b1274450":"for i in range(9):\n    image_id = sample_df.sample()['ImageId'].values[0]\n    image_path = str(DATA_DIR\/'test'\/image_id)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    result = model.detect([resize_image(image_path)])\n    r = result[0]\n    \n    if r['masks'].size > 0:\n        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n        for m in range(r['masks'].shape[-1]):\n            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n        \n        y_scale = img.shape[0]\/IMAGE_SIZE\n        x_scale = img.shape[1]\/IMAGE_SIZE\n        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n        \n        masks, rois = refine_masks(masks, rois)\n    else:\n        masks, rois = r['masks'], r['rois']\n        \n    visualize.display_instances(img, rois, masks, r['class_ids'], \n                                ['bg']+label_names, r['scores'],\n                                title=image_id, figsize=(12, 12))","af72e707":"Let's visualize class distributions of the train and validation data.","592ac2b6":"# Dowload Libraries and Pretrained Weights","43f4ae6f":"The submission file is created, when all predictions are ready.","f6aa1219":"Finally, it's pleasing to visualize the results! Sample images contain both fashion models and predictions from the Mask R-CNN model.","46a7fbaa":"Afterwards, we reduce LR and train again.","122ed4a4":"The final step is to use our model to predict test data.","43e3f3ca":"Then, load the submission data.","9901dfff":"Segments that contain attributes are only 3.46% of data, and [according to the host](https:\/\/www.kaggle.com\/c\/imaterialist-fashion-2019-FGVC6\/discussion\/90643#523135), 80% of images have no attribute. So, in the first step, we can only deal with categories to reduce the complexity of the task.","bcc00a3a":"[](http:\/\/)My code is largely based on [this Mask-RCNN kernel](https:\/\/www.kaggle.com\/hmendonca\/mask-rcnn-and-coco-transfer-learning-lb-0-155) and borrowed some ideas from [the U-Net Baseline kernel](https:\/\/www.kaggle.com\/go1dfish\/u-net-baseline-by-pytorch-in-fgvc6-resize). So, I would like to thank the kernel authors for sharing insights and programming techniques. Importantly, an image segmentation task can be accomplished with short code and good accuracy thanks to [Matterport's implementation](https:\/\/github.com\/matterport\/Mask_RCNN) and a deep learning line of researches culminating in [Mask R-CNN](https:\/\/arxiv.org\/abs\/1703.06870).\n\nI am sorry that I published this kernel quite late, beyond the halfway of a timeline. I just started working for this competition about a week ago, and to my surprise, the score fell in the range of silver medals at that time. I have no dedicated GPU and no time to further tune the model, so I decided to make this kernel public as a starter guide for anyone who is interested to join this delightful competition.\n\n<img src='https:\/\/i.imgur.com\/j6LPLQc.png'>","0f6a91d6":"Rows with the same image are grouped together because the subsequent operations perform in an image level.","dd980b92":"Let's visualize training history and choose the best epoch.","61c90027":"This section creates a Mask R-CNN model and specifies augmentations to be used.","a514d18a":"Now, the data are partitioned into train and validation sets.","3dfa0a46":"# Make Datasets","ad3f0859":"Here is the main prediction steps, along with some helper functions.","150091ce":"The crucial part is to create a dataset for this task.","8fbfb2f6":"Let's visualize some random images and their masks.","2c487fbe":"# Predict","769fd62a":"# Train","089c52d2":"Welcome to the world where fashion meets computer vision! This is a starter kernel that applies Mask R-CNN with COCO pretrained weights to the task of [iMaterialist (Fashion) 2019 at FGVC6](https:\/\/www.kaggle.com\/c\/imaterialist-fashion-2019-FGVC6).","d3bdc12f":"This cell defines InferenceConfig and loads the best trained model.","bc890cea":"Then, all layers are trained.","609bf6a1":"Here is the custom function that resizes an image.","34dda68c":"First, we train only the heads.","1c78d8b3":"Hope you guys like this kernel. If there are any bugs, please let me know.\n\nP.S. When clicking 'Submit to Competition' button, I always run into 404 erros, so I have to save a submission file and upload it to the submission page for submitting. The public LB score of this kernel is around **0.07**.","50675185":"Mask R-CNN has a load of hyperparameters. I only adjust some of them.","570c7e58":"# Set Config"}}