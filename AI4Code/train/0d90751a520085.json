{"cell_type":{"a2373f0d":"code","102b4d9c":"code","a106a515":"code","b06a2b73":"code","7ddaaae5":"code","7f34796e":"code","18c41daa":"code","340bf933":"code","0e9f56ba":"code","b7dbfd3e":"code","95aeb880":"code","d93ff269":"code","058486d2":"code","1a14aed9":"code","99eaf660":"code","4519bb86":"code","bbcb675c":"code","fb490c51":"code","b1a7c8ff":"code","6040ba31":"code","16856221":"code","a60635a3":"code","ed7b9d46":"code","465469f4":"code","5c5901c8":"code","2b2ec2a3":"code","9ac9b73b":"code","c643f9d7":"code","cd1ce3ac":"code","6602892f":"code","b5a9d642":"markdown","7d9ddfa5":"markdown","0d031c84":"markdown","a2711475":"markdown","aac330f2":"markdown","49b15709":"markdown","2a1fcf09":"markdown","97c7e643":"markdown","488fcea6":"markdown","7a6aaee7":"markdown","4c7db4ea":"markdown","d994c3af":"markdown","14928056":"markdown","5bf7aeee":"markdown","6629682a":"markdown","f8e23af4":"markdown"},"source":{"a2373f0d":"#import the libraries we need\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","102b4d9c":"df = pd.read_csv('..\/input\/cleaned-titanic-dataset\/Cleaned Titanic Dataset.csv')","a106a515":"df.head()","b06a2b73":"#we drop the Unnamed column\n\ndf.drop('Unnamed: 0', axis =1 , inplace= True)","7ddaaae5":"#set the Passenger ID as Index\n\ndf.set_index('PassengerId')","7f34796e":"df.info()","18c41daa":"#X : Features\n#y : Target variable\n\nX = df.drop('Survived', axis = 1)\ny = df['Survived']","340bf933":"#we use train_test_split from sklearn.model_selection to devide dataset to train and test set.\n\nfrom sklearn.model_selection import train_test_split\n\n#train set in a bigger sample of dataset that model uses to learn.\n#test set in smaller sample of dataset that model should be evaluated in.\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state=40)","0e9f56ba":"#we use StandardScaler from sklearn.preprocessing to make dataset values standard\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(X_train)","b7dbfd3e":"scaled_X_train = scaler.transform(X_train)\nscaled_X_test = scaler.transform(X_test)","95aeb880":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors=1) #n_neighbors = k\n\nknn_model.fit(scaled_X_train, y_train)","d93ff269":"y_pred = knn_model.predict(scaled_X_test)","058486d2":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix, precision_score, recall_score","1a14aed9":"confusion_matrix(y_test, y_pred)","99eaf660":"plot_confusion_matrix(knn_model, scaled_X_test, y_test)","4519bb86":"pd.DataFrame({ '':['Accuracy','precision','recall'],\n             'Value': [accuracy_score(y_test, y_pred),precision_score(y_test, y_pred,average='macro')\n              ,recall_score(y_test, y_pred, average='macro')]})","bbcb675c":"print(classification_report(y_test,y_pred))","fb490c51":"#make a list of accuracy with different K values.\ntest_error_rate= [] \n\nfor k in range (1, 15):\n    \n    knn_model = KNeighborsClassifier(n_neighbors = k)\n    knn_model.fit(scaled_X_train, y_train)\n    \n    y_pred_test = knn_model.predict(scaled_X_test)\n    \n    test_error = 1 - accuracy_score(y_test, y_pred_test)\n    test_error_rate.append(test_error)\n    \n","b1a7c8ff":"pd.Series(test_error_rate, index = np.arange(1,15))","6040ba31":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 15), test_error_rate, label='Test Error')\nplt.legend()\nplt.ylabel('1 - Accuracy')\nplt.xlabel('K')","16856221":"scaler = StandardScaler()\nknn = KNeighborsClassifier()\n\noperations= [('scaler', scaler), ('knn', knn)]","a60635a3":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV","ed7b9d46":"pipe = Pipeline(operations)\nk_values= list(range(1, 20))\nparam_grid= {'knn__n_neighbors': k_values}\n\nfull_cv_classifier= GridSearchCV(pipe, param_grid, cv=10, scoring='accuracy')","465469f4":"full_cv_classifier.fit(X_train, y_train)","5c5901c8":"full_cv_classifier.best_estimator_.get_params()","2b2ec2a3":"scaler= StandardScaler()\nKNN_8 = KNeighborsClassifier(n_neighbors = 8)\noperations= [('scaler', scaler), ('KNN_8', KNN_8)]","9ac9b73b":"pipe= Pipeline(operations)","c643f9d7":"pipe.fit(X_train, y_train)","cd1ce3ac":"pipe_pred= pipe.predict(X_test)","6602892f":"print(classification_report(y_test, pipe_pred))","b5a9d642":"# 1. Importing Data and first review\n\n[In last notebook](https:\/\/www.kaggle.com\/javadmaddah\/logistic-regression-on-titanic-dataset-week-5-1) I cleaned dataset and do some exploratory data analysis and now we can use the dataset that was prepared before. You can check my last notebook to see the steps of cleaning data and changing categorical variables to dummy ones.\n\nSo I just import the csv file that I saved from last notebook. ","7d9ddfa5":"Classification models have some different metrics than others. Accurace, precision and recall are 3 famouse metrics for classification models.\n\n![image.png](attachment:image.png)\n\n![image-2.png](attachment:image-2.png)","0d031c84":"This method says that K = 8 is perfect choise. So we can decide to build model with K = 8 or K = 9.","a2711475":"## 4.2 Predict","aac330f2":"# 7. Final model","49b15709":"So, It seems like K = 9 is the best choice with the least error","2a1fcf09":"# 3. Scaling Data\n\nWhen we are working with KNN models we should know that Scaling data is necessary. KNN make decisions by distance between data and different scales reduces model validity.\n\nWe use Standard scaling method.\n\n![image.png](attachment:5ef91ce3-74f0-45cc-81da-a6714269fd98.png)","97c7e643":"Accuracy of the model with K = 8 is 89 %. Compared to logistic regression, which gave us 84% accuracy, the KNN has an error of about 10%.\n\nSo It's more valid model than Logistic regression.","488fcea6":"# 4. Build the model\n## 4.1 Train\n\n\u2018k\u2019 in KNN is a parameter that refers to the number of nearest neighbours to include in the majority of the voting process.","7a6aaee7":"So when K = 1 we have 83% accuracy. we have to check with other K values to find the least error.","4c7db4ea":"A **confusion matrix** is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n![image.png](attachment:image.png)","d994c3af":"## 4.3 Evaluate","14928056":"# 5. Choose the best option for K value with Elbow Method","5bf7aeee":"In last notebook I try to build a model to predict the people who survived from the sinking of the Titanic with \"Logistic Regression.\" ( [Regression Notebook](https:\/\/www.kaggle.com\/javadmaddah\/logistic-regression-on-titanic-dataset-week-5-1) )\n\nNow in this notebook we use K-nearest neighbors (KNN) to build a model to predict same thing. In statistics, the k-nearest neighbors algorithm (KNN) is a non-parametric classification method. The output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\n![image.png](attachment:image.png)\n\n[You can learn more about KNN here.](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)","6629682a":"# 2. Split Data to Train & Test","f8e23af4":"# 6. Creating Pipeline to find K value\nPipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. \n\ncheck out here for more information about Pipeline: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n"}}