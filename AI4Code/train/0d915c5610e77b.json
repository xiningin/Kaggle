{"cell_type":{"078faf17":"code","9afed453":"code","74f847ae":"code","2cd10ad2":"code","63834d3b":"code","39c4156b":"code","e4002781":"code","0e144737":"code","7019e566":"code","f25fc3f0":"code","c6e25e5e":"markdown","0da13313":"markdown","b6c5cb7c":"markdown","c71af17f":"markdown","3a61f73d":"markdown","4fbd1361":"markdown","d2c064b0":"markdown","e44bb77a":"markdown","4a659df7":"markdown","abee90d2":"markdown","00de0a58":"markdown","7cf4f95d":"markdown","e559917b":"markdown","413c07f5":"markdown"},"source":{"078faf17":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply, MaxPooling1D\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import Callback\n\nfrom sklearn.metrics import cohen_kappa_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom keras.constraints import unit_norm\nfrom sklearn.manifold import TSNE","9afed453":"df_train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")\n\ntrain_input = df_train[\"signal\"].values.reshape(-1,5000,1)#number_of_data:1000 x time_step:5000\ntrain_input_mean = train_input.mean()\ntrain_input_sigma = train_input.std()\ntrain_input = (train_input-train_input_mean)\/train_input_sigma\ntest_input = df_test[\"signal\"].values.reshape(-1,5000,1)#\ntest_input = (test_input-train_input_mean)\/train_input_sigma\n\n#train_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,5000,11)\ntrain_target = np.array([[i]*int(train_input.shape[0]\/10) for i in range(10)]).flatten()#batch class\n\nidx = np.arange(train_input.shape[0])\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2, stratify = train_target)\n\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","74f847ae":"def cbr(x, out_layer, kernel, stride, dilation):\n    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = GlobalAveragePooling1D()(x_in)\n    x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n    x = Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = cbr(x_in, layer_n, kernel, 1, dilation)\n    x = cbr(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = Add()([x_in, x])\n    return x  \n\ndef Classifier(input_shape=(None,1)):\n    layer_n = 96\n    kernel_size = 7\n    depth = 3\n\n    input_layer = Input(input_shape)    \n    input_layer_1 = AveragePooling1D(5)(input_layer)\n    input_layer_2 = AveragePooling1D(25)(input_layer)\n    \n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    out_2 = x\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, 1)\n    \n    x = GlobalAveragePooling1D()(x)\n    x = Dense(128, activation=\"linear\")(x)\n    x = Lambda(lambda x: 3*x\/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keepdims=True)+1e-7))(x)#L2\n    out = Dense(10, activation=\"softmax\", kernel_constraint = unit_norm(), name=\"out\")(x)\n\n    model=Model(input_layer, out)\n    \n    return model\n\ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n    return input_data, target_data\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx = np.arange(len(input_dataset))\n    np.random.shuffle(idx)\n    \n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx[i]]\n            target_data = target_dataset[idx[i]]\n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n\n            x.append(input_data)\n            y.append(target_data)\n\n            count+=1\n            if count==batch_size:\n                x = np.array(x, dtype=np.float32)\n                y = np.identity(10)[y].astype(np.float32)\n\n                inputs = x\n                targets = y\n       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets\n    \ndef model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) \/\/ batch_size,\n        callbacks = [lr_schedule],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\ndef lrs(epoch):\n    if epoch<40:\n        lr = learning_rate\n    elif epoch<60:\n        lr = learning_rate\/10\n    else:\n        lr = learning_rate\/100\n    return lr    ","2cd10ad2":"K.clear_session()\nmodel = Classifier()\n#print(model.summary())\n\nlearning_rate=0.0075\nn_epoch=80\nbatch_size=64\n\nlr_schedule = LearningRateScheduler(lrs)\n\nmodel.compile(loss=categorical_crossentropy, \n              optimizer=Adam(lr=learning_rate), \n              metrics=[\"accuracy\"])\n\nhist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)","63834d3b":"from sklearn.manifold import TSNE\ndef tSNE_visualization(model, train_inputs, train_batch, test_inputs, test_batch):\n    inputs = np.concatenate((train_inputs,test_inputs), axis=0)\n    latent_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n    latent_features = latent_model.predict(inputs)\n    \n    tsne_model = TSNE(n_components=2, perplexity=30, n_iter=1000)\n    pred = tsne_model.fit_transform(latent_features)\n    train_pred = pred[:len(train_inputs)]\n    test_pred = pred[len(train_inputs):]\n\n    fig, ax = plt.subplots(2, 1, figsize=(7,14))\n    cmap = plt.get_cmap(\"tab10\")\n    \n    for i in range(10):\n        ax[0].scatter(train_pred[train_batch==i,0], train_pred[train_batch==i,1], alpha=0.5, s=int(40), color=cmap(i), edgecolors=None, label=\"train_batch_No.{}\".format(i))\n        ax[0].legend(bbox_to_anchor=(1.4,1), loc=\"upper right\", borderaxespad=0, fontsize=12)\n        ax[0].set_title('t-SNE Latent space -TRAIN-', fontsize=20)\n    \n    for i in range(4):\n        ax[1].scatter(test_pred[test_batch==i,0], test_pred[test_batch==i,1], alpha=0.5, s=int(40), color=cmap(i), edgecolors=None, label=\"test_batch_No.{}\".format(i))\n        ax[1].legend(bbox_to_anchor=(1.4,1), loc=\"upper right\", borderaxespad=0, fontsize=12)\n        ax[1].set_title('t-SNE Latent space -TEST-', fontsize=20)\n    plt.show()\n\n\ntest_batch = np.array([[i]*int(500000\/5000) for i in range(4)]).flatten()\n\ntSNE_visualization(model, val_input, val_target, test_input, test_batch)\n","39c4156b":"test_class = np.argmax(model.predict(test_input), axis=-1)\n\ncmap = plt.get_cmap(\"tab10\")\n    \nfig, ax = plt.subplots(figsize=(25,8))\nfor i in range(10):\n    ax.plot(np.arange(i*500000,(i+1)*500000), df_train[\"signal\"].values[i*500000:(i+1)*500000], color=cmap(i), label=\"train_batch_No.{}\".format(i))\nax.set_xticks(np.arange(0,5000000,100000))\nax.set_xlim(0,5000000)\nax.grid()\nax.set_title('TRAIN Signal', fontsize=15)\nplt.show()\n\nfig, ax = plt.subplots(2, 1, figsize=(10,16))\n\nfor i in range(4):\n    ax[0].plot(np.arange(i*500000,(i+1)*500000), df_test[\"signal\"].values[i*500000:(i+1)*500000], color=cmap(i), label=\"train_batch_No.{}\".format(i))\n    ax[1].plot(np.arange(i*100,(i+1)*100), test_class[i*100:(i+1)*100], color=cmap(i), label=\"train_batch_No.{}\".format(i))\nax[0].set_xticks(np.arange(0,2000000,100000))\nax[1].set_xticks(np.arange(0,400,20))\nax[0].set_xlim(0,2000000)\nax[1].set_xlim(0,400)\nax[0].grid()\nax[1].grid()\nax[0].set_title('TEST Signal', fontsize=15)\nax[1].set_title('TEST predicted Batch Class', fontsize=15)\nplt.show()","e4002781":"def add_feature(x):\n    x_ = np.roll(x[:,:,0],1)\n    x_[:,0] = 0\n    x_ = (x_[:,:,np.newaxis] - x) \n    x__ = np.roll(x[:,:,0],-1)\n    x__[:,-1] = 0\n    x__ = (x__[:,:,np.newaxis] - x) \n    x = np.concatenate((x,x_,x__), axis=-1)\n    return x\n\ntrain_input = df_train[\"signal\"].values.reshape(-1,5000,1)\ntrain_input = add_feature(train_input)\ntrain_input_mean = np.mean(train_input.reshape(len(df_train),-1), axis=0).reshape(1,1,-1)\ntrain_input_sigma =np.std(train_input.reshape(len(df_train),-1), axis=0).reshape(1,1,-1)\ntrain_input = (train_input-train_input_mean)\/train_input_sigma\ntrain_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,5000,11)\n\ntest_input = df_test[\"signal\"].values.reshape(-1,10000,1)#I like 10000 because the test data changes in 10000 time steps at minimum.\ntest_input = add_feature(test_input)\ntest_input = (test_input-train_input_mean)\/train_input_sigma\n\nbatch_number = np.array([[i]*int(500000\/5000) for i in range(10)]).flatten()\nidx = np.arange(train_input.shape[0])\n\n# select No.0-7 batch\nidx = [idx[i] for i in range(len(batch_number)) if batch_number[i] in [0,1,2,3,4,5,6,7]]\n\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","0e144737":"def aggregation_block(x_shallow, x_deep, deep_ch, out_ch):\n    x_deep = UpSampling1D(5)(x_deep)\n    x_deep = Conv1D(deep_ch, kernel_size=7, strides=1, padding=\"same\")(x_deep)\n    x_deep = BatchNormalization()(x_deep)   \n    x_deep = Activation(\"relu\")(x_deep)\n    x = Concatenate()([x_shallow, x_deep])\n    x = Conv1D(out_ch, kernel_size=1, strides=1, padding=\"same\")(x)\n    x = BatchNormalization()(x)   \n    x = Activation(\"relu\")(x)\n    return x\n\ndef aggregation(skip_connections,output_layer_n,name=\"\"):\n    skip_connections_1=[]\n    n=len(skip_connections)\n    m=0\n    for i in range(n):\n        x= cbr(skip_connections[i], output_layer_n, 1, 1, [1])\n        skip_connections_1.append(x)\n    x_0= cbr(skip_connections[0], output_layer_n, 1, 1, [1])\n    x_0 = aggregation_block(x_0, skip_connections[1], output_layer_n, output_layer_n)\n    x_1= cbr(skip_connections[1], output_layer_n, 1, 1, [1])\n    x_1 = aggregation_block(x_1, skip_connections[2], output_layer_n, output_layer_n)\n    x_0 = aggregation_block(x_0, x_1, output_layer_n, output_layer_n)    \n    x_2= cbr(skip_connections[2], output_layer_n, 1, 1, [1])\n    skip_connections_out=[x_0,x_1,x_2]\n    return skip_connections_out\n\ndef MinPooling1D(x, stride):\n    x = Lambda(lambda x: -x)(x)\n    x = MaxPooling1D(stride)(x)\n    x = Lambda(lambda x: -x)(x)\n    return x\n    \ndef min_max_average_pooling(x_in, stride):\n    x_av = AveragePooling1D(stride)(x_in)\n    x_max =  MaxPooling1D(stride)(x_in)\n    x_min = MinPooling1D(x_in, stride)\n    x_out = Concatenate()([x_av,x_max,x_min])\n    return x_out\n\ndef Unet(input_shape=(None,3)):\n    layer_n = int(64*1.5)\n    kernel_size = 7\n    depth = int(2*1.5)\n    dilations = [1]\n    skip_connections = []\n\n    input_layer = Input(input_shape)\n    \n    input_layer_1 = min_max_average_pooling(input_layer, 5)\n    input_layer_2 = min_max_average_pooling(input_layer, 25)\n    input_layer_3 = min_max_average_pooling(input_layer, 125)\n    \n\n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, dilations)\n    out_0 = x\n    skip_connections.append(x)\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, dilations)\n    out_1 = x\n    skip_connections.append(x)\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, dilations)\n    out_2 = x\n    skip_connections.append(x)\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, dilations)\n\n    x = Concatenate()([x, input_layer_3])\n    x = cbr(x, layer_n*4, kernel_size, 1, 1)\n    x = resblock(x, layer_n*4, kernel_size, dilations)    \n\n    skip_connections=aggregation(skip_connections,layer_n)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, skip_connections[2]])\n    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, skip_connections[1]])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, skip_connections[0]])\n    x = cbr(x, layer_n, kernel_size, 1, 1)    \n\n    x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    out = Activation(\"softmax\", name=\"out\")(x)\n    \n    model=Model(input_layer, out)\n    \n    return model\n\ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n        target_data = target_data[::-1]\n        \n    return input_data, target_data\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x = []\n    y = []\n  \n    count = 0\n    idx = np.arange(len(input_dataset))\n    np.random.shuffle(idx)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx[i]]\n            target_data = target_dataset[idx[i]]\n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n\n            x.append(input_data)\n            y.append(target_data)\n            \n            count += 1\n            if count==batch_size:\n                x = np.array(x, dtype=np.float32)\n                y = np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y\n                \n                x = []\n                y = []\n                count=0\n                yield inputs, targets\n\nclass macroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n\n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        f1_val = f1_score(self.targets, pred, average=\"macro\")\n        print(\"val_f1_macro_score: \", f1_val)\n                \ndef model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) \/\/ batch_size,\n        callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\ndef cos_lrs(epoch):\n    if epoch<200:\n        lr = learning_rate - learning_rate*(1-np.cos(np.pi*epoch\/200))\/2 + learning_rate\/200\n    else:\n        lr = learning_rate\/200\n    return lr\n","7019e566":"K.clear_session()\nmodel = Unet()\n\nn_epoch=230\nbatch_size=64\nlearning_rate=0.00075\nlr_schedule = LearningRateScheduler(cos_lrs)\n\nmodel.compile(loss=categorical_crossentropy, \n              optimizer=Adam(lr=learning_rate), \n              metrics=[\"accuracy\"])\n\nhist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)","f25fc3f0":"pred = np.argmax((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])\/2, axis=2).reshape(-1)\ndf_sub = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(np.round(pred,0), np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)\nprint(df_sub.head())","c6e25e5e":"## Train Classifier","0da13313":"### Submit","b6c5cb7c":"### Preprocessing\nuse only 0-7 batch of training data","c71af17f":"## Load and Split Dataset","3a61f73d":"---","4fbd1361":"## Introduction\nThis kernel analyze the similarity between train and test dataset by using a classifier of the batch numbers.\n* Step 1: Train classifier to predict batch numbers of training dataset.\n* Step 2: Apply the classifier on test dataset.\n\nMost of the code is same as my previous kernel.\n(https:\/\/www.kaggle.com\/kmat2019\/u-net-1d-cnn-with-keras)","d2c064b0":"## Classify Test Data","e44bb77a":"### Define Model, then Training","4a659df7":"## Comparing Train Data to Test Data in Latent Space using t-SNE","abee90d2":"As shown by last plots, each test batch has the characteristics of some training batches.\n\nYou can possibly get high public\/private score using the corresponding training batch.","00de0a58":"## Import Library","7cf4f95d":"I know many people like high (public) scoring kernels.\n---\n\nThanks to the following forum, it proved that the the first 30% of the test data is used to calculate public score.\n\nhttps:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/133142\n\nAccording to the result of similality analysis, **the first 30% of the test data is similar to batch No.0 to 7 of training data.**\n\n**Let's try overfitting to these training data to get high public score!**","e559917b":"## Define Classifier\n* Input: 5000 time steps of \"signal\"\n* Output: Class of the batch number. (First batch is from 0 to 0.5 sec, second batch is from 0.5 to 1 sec, ...)","413c07f5":"The distribution of test data is similar to that of training data than I expected. Most of the test data belongs to either of the training batch?\n\nTest batch No.2 (data from 1.0 to 1.5 sec) shown by green plots are slightly unique comparing to the others."}}