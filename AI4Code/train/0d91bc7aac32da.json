{"cell_type":{"759d06b5":"code","ad146292":"code","e40861c4":"code","9c50799d":"code","ad8692a4":"code","d1214044":"code","4243dc51":"code","217e833b":"code","26e5386a":"code","0d8d17af":"code","dbc4fcdc":"code","a5e560f5":"code","7334fe6a":"code","3b0d95ca":"code","3f98c932":"code","a8586fa2":"code","894dd5c3":"code","d287d082":"code","819c4d9b":"code","6baea60f":"code","079d754a":"code","35df9984":"code","3519f1d7":"code","18064ff1":"code","017b8cd1":"code","515ad5f5":"code","c2d92849":"code","1fe6f672":"code","2f3cc0b5":"code","f85d15f8":"code","7a6a80d4":"code","de71c024":"code","e148edca":"code","1e42521b":"code","0b0ba0cc":"code","1eb12c3c":"code","d34e5487":"code","a762ae15":"code","810beb26":"code","e36ff2b6":"code","8d985198":"code","c46fb87c":"code","5026ad9d":"markdown","d1c5da17":"markdown","6abb80ca":"markdown","6bd0144d":"markdown","32722624":"markdown","ed3ff15d":"markdown","ffbe2d99":"markdown","1aa88dfe":"markdown","4e288b03":"markdown","4cd58fcf":"markdown","b6a0e1d0":"markdown","3c531ec9":"markdown","0d9ec5ab":"markdown","7346c500":"markdown","7b359f47":"markdown"},"source":{"759d06b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n","ad146292":"#ML Librarires \nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport  matplotlib.pyplot as plt\n\n\nplt.style.use('seaborn')\n%matplotlib inline","e40861c4":"df = pd.read_csv('..\/input\/star-type-classification\/Stars.csv')\ndf.head()","9c50799d":"df.shape","ad8692a4":"df.info()","d1214044":"df.describe()","4243dc51":"df=df.sample(frac=1).reset_index(drop=True)","217e833b":"pd.DataFrame(df.isnull().sum(), columns=[\"Null Count\"])","26e5386a":"#Or \n#missing values cheacking\ndf.apply(lambda x: sum(x.isnull()),axis=0)","0d8d17af":"from sklearn import preprocessing","dbc4fcdc":"#Label Encoding\n\nle = preprocessing.LabelEncoder()\n\n# columns to select for encoding\nselected_col = ['Color','Spectral_Class']\n\nle.fit(df[selected_col].values.flatten())\n\ndf[selected_col] = df[selected_col].apply(le.fit_transform)","a5e560f5":"df.info()","7334fe6a":"df.head()","3b0d95ca":"#Plotting data \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\n\nplt.figure(figsize=(20,14))\nsns.heatmap(df.corr(),annot=True,linecolor='green',linewidths=3,cmap = 'plasma')\n#Data Cor-Relation","3f98c932":"#Box Plotting All features distribution corresponding Target column\ni=1\nplt.figure(figsize=(40,40))\nfor c in df.columns[:]:\n    plt.subplot(3,3,i)\n    plt.title(f\"Boxplot of {c}\",fontsize=16)\n    plt.yticks(fontsize=12)\n    plt.xticks(fontsize=12)\n    sns.boxplot(y=df[c],x=df['Type'])\n    i+=1\nplt.show()","a8586fa2":"# the data columns\ncols = ['Temperature', 'L', 'R', 'A_M', 'Color', 'Spectral_Class']\ntarget = [\"Type\"]","894dd5c3":"import scipy.stats as stats\npd.set_option('display.max_columns', None)\nplt.style.use('ggplot')","d287d082":"fig,ax = plt.subplots(3,2, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    skew = df[col].skew()\n    sns.distplot(df[col], ax = ax[i], fit= stats.norm, kde=False, label='Skew = %.3f' %(skew))\n    ax[i].legend(loc='best')\n    i += 1\nplt.show()","819c4d9b":"#checking the target variable countplot\n\nprint(\"class :\", df[\"Type\"].unique())\nprint()\n\nprint(\"Value Count :\\n\",df[\"Type\"].value_counts())","6baea60f":"len(df[\"Type\"].value_counts())","079d754a":"#checking the target variable countplot\n\nplt.figure(figsize=(25,15))\nsns.set_style('white')\nsns.countplot(x='Type', data = df, palette='GnBu')\nsns.despine(left=True)","35df9984":"sns.countplot(df[\"Type\"])\nplt.show()","3519f1d7":"df.iloc[:,:-1].boxplot(figsize=(12,6))\nplt.show()","18064ff1":"fig,ax = plt.subplots(3,2, figsize=(16, 12))\nax = ax.flatten()\ni = 0\nfor col in cols:\n    sns.boxplot(\"Type\", col, ax = ax[i], data=df)\n    ax[i].legend([col], loc='best')\n    i += 1\nplt.tight_layout()\nplt.show()","017b8cd1":"#Scatter Matrix\npd.plotting.scatter_matrix(df.iloc[:,:-1], c=df.iloc[:,-1], figsize=(20, 20), marker='o')\nplt.legend(df[\"Type\"].unique())\nplt.show()","515ad5f5":"#Pairplot\nsns.pairplot(df, hue='Type', diag_kind='hist')\nplt.show()","c2d92849":"#Correlation Plot\nplt.figure(figsize=(8,6))\ncorr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, fmt= '.2f', cmap='YlGnBu', mask=mask)\nplt.show()","1fe6f672":"#A_M,Color and Spectral have no correlation with Type, \n#which means for some type it maybe high for some low causing cancelling effect","2f3cc0b5":"df.groupby(\"Type\")[\"Color\"].mean()","f85d15f8":"df.groupby(\"Type\")[\"R\"].mean()","7a6a80d4":"import statsmodels.api as sm\nimport statsmodels.stats as sms\n\nfor col in cols:\n    data = sm.formula.ols(col+\"~ Type\", data=df).fit()\n    pval = sms.anova.anova_lm(data)[\"PR(>F)\"][0]\n    print(f\"Pval for {col}: {pval}\")","de71c024":"seed = 42\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier,\\\n                            BaggingClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline","e148edca":"# split the data into train and test\ndef split_data(X, Y, seed=42, train_size=0.8):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, train_size=train_size, random_state = seed, stratify=Y)\n    xtrain, xtest = preprocess(xtrain, xtest)\n    return (xtrain, xtest, ytrain, ytest)\n\n# preprocess the data for training\ndef preprocess(x1, x2=None):\n    sc = StandardScaler()\n    x1 = pd.DataFrame(sc.fit_transform(x1), columns=x1.columns)\n    if x2 is not None:\n        x2 = pd.DataFrame(sc.transform(x2), columns=x2.columns)\n        return (x1,x2)\n    return x1\n\n# for model evaluation and training\ndef eval_model(model, X, Y, seed=1):\n    xtrain, xtest, ytrain, ytest = split_data(X, Y)\n    model.fit(xtrain, ytrain)\n    \n    trainpred = model.predict(xtrain)\n    trainpred_prob = model.predict_proba(xtrain)\n    testpred = model.predict(xtest)\n    testpred_prob = model.predict_proba(xtest)\n    \n    print(\"Train ROC AUC : %.4f\"%roc_auc_score(ytrain, trainpred_prob, multi_class='ovr'))\n    print(\"\\nTrain classification report\\n\",classification_report(ytrain, trainpred))\n    \n    ### make a bar chart for displaying the wrong classification of one class coming in which other class\n    \n    print(\"\\nTest ROC AUC : %.4f\"%roc_auc_score(ytest, testpred_prob, multi_class='ovr'))\n    print(\"\\nTest classification report\\n\",classification_report(ytest, testpred))\n    \ndef plot_importance(columns, importance):\n    plt.bar(columns, importance)\n    plt.show()","1e42521b":"#Feature Extraction, Importance & Splitting\n\nY= df['Type']\n\nX = df.drop(['Type'],axis = 1)","0b0ba0cc":"X_sc = preprocess(X)","1eb12c3c":"model_logr = LogisticRegression(random_state=seed,n_jobs=-1)\nmodel_nb = GaussianNB()\nmodel_dt = DecisionTreeClassifier(random_state=seed)\nmodel_dt_bag = BaggingClassifier(model_dt, random_state=seed, n_jobs=-1)\nmodel_ada = AdaBoostClassifier(random_state=seed)\nmodel_gbc = GradientBoostingClassifier(random_state=seed)\nmodel_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=seed)\nmodel_lgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\nmodel_knn = KNeighborsClassifier(n_jobs=-1)\n","d34e5487":"models = []\nmodels.append(('LR',model_logr))\nmodels.append(('NB',model_nb))\nmodels.append(('DT',model_dt))\nmodels.append(('Bag',model_dt_bag))\nmodels.append(('Ada',model_ada))\nmodels.append(('GBC',model_gbc))\nmodels.append(('RF',model_rf))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGBM',model_lgbm))\nmodels.append(('KNN',model_knn))","a762ae15":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","810beb26":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","e36ff2b6":"X = df.drop(['A_M', 'Color', 'Spectral_Class'], axis=1)\nX_sc = preprocess(X)\nY = df['Type']","8d985198":"results = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","c46fb87c":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","5026ad9d":"None of the features are normally distributed and some have outliers\n\nNote: Outlier treatment maybe done to check impact on classification\n\nDeatils: https:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/what-is-skewness-statistics\/","d1c5da17":"Background: The idiomatic way to do this with Pandas is to use the .sample method of your dataframe to sample all rows without replacement https:\/\/stackoverflow.com\/questions\/29576430\/shuffle-dataframe-rows\n\nIf you wish to shuffle your dataframe in-place and reset the index, you could do e.g.\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nHere, specifying drop=True prevents .reset_index from creating a column containing the old index entries.","6abb80ca":"# Data Read","6bd0144d":"Comparison of Models","32722624":"Univariate Box Plot","ed3ff15d":"#Skewness of Data","ffbe2d99":"Bivariate Box plots","1aa88dfe":"Observations:\u00b6\n- L is the main component of Df making more than 70% of composition\n- Combined temp and L make up around 90%\n- A_M, Color, R, spectral  are the least important components\n\nAbove box plot confirms the outliers\n\nI prefer to use models without outlier treatment, in many cases it can improve the model performance.\nBut it also leads to change of information which might alter real\/practical situations","4e288b03":"Running the algorithms","4cd58fcf":"Creating array of models","b6a0e1d0":"Statistical Importance Check for Variable","3c531ec9":"Data Preprocessing & Evaluation Functions","0d9ec5ab":"# Exploratory Data Analysis & Data Pre-Processing\n\nhttps:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15","7346c500":"Only Significant Variables","7b359f47":"# Part 1: Method-ML Algorithams\n\nMore Deatils :\nhttps:\/\/www.dataquest.io\/blog\/top-10-machine-learning-algorithms-for-beginners\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/common-machine-learning-algorithms\/"}}