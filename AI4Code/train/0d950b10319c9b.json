{"cell_type":{"ebb6b243":"code","8de2fab9":"code","25b5c8c7":"code","f019224b":"code","f64d42ea":"code","ba665dfd":"code","123224bc":"code","338b2ee2":"code","10e6d199":"code","41fec342":"code","9c12bcda":"code","4f30038e":"code","3d7b5da9":"code","10312a75":"markdown","685d5462":"markdown","ab3ad1dd":"markdown","8d077d1b":"markdown","65b81684":"markdown"},"source":{"ebb6b243":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport warnings\nimport gc\nfrom bayes_opt import BayesianOptimization\nwarnings.simplefilter('ignore')","8de2fab9":"building = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')\nweather_train = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_train.csv')\ntrain = pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')","25b5c8c7":"train = train.merge(building, on='building_id', how='left')\ntrain = train.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n# Logarithmic transform of target values\ny = np.log1p(train['meter_reading'])\n\ndel building, weather_train, train['meter_reading']\ngc.collect();","f019224b":"# Transforming timestamp to a datetime format\ntrain[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n\n# To save time and memory this dict is going to be used to label encode primary_use feature.\n# This is exactly what LabelEncoder would do to the data.\nle_dict = {'Education': 0,\n           'Office': 6,\n           'Entertainment\/public assembly': 1,\n           'Lodging\/residential': 4,\n           'Public services': 9,\n           'Healthcare': 3,\n           'Other': 7,\n           'Parking': 8,\n           'Manufacturing\/industrial': 5,\n           'Food sales and service': 2,\n           'Retail': 11,\n           'Warehouse\/storage': 15,\n           'Services': 12,\n           'Technology\/science': 13,\n           'Utility': 14,\n           'Religious worship': 10}\n\ntrain['primary_use'] = train['primary_use'].map(le_dict)","f64d42ea":"# Some new features from timestamp\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain[\"day\"] = train[\"timestamp\"].dt.day\ntrain[\"day_of_week\"] = train[\"timestamp\"].dt.weekday\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour","ba665dfd":"# Saving some memory\nd_types = {'building_id': np.int16,\n          'meter': np.int8,\n          'site_id': np.int8,\n          'primary_use': np.int8,\n          'square_feet': np.int32,\n          'year_built': np.float16,\n          'floor_count': np.float16,\n          'air_temperature': np.float32,\n          'cloud_coverage': np.float16,\n          'dew_temperature': np.float32,\n          'precip_depth_1_hr': np.float16,\n          'sea_level_pressure': np.float32,\n          'wind_direction': np.float16,\n          'wind_speed': np.float32,\n          'month': np.int8,\n          'day': np.int16,\n          'hour': np.int16,\n          'day_of_week': np.int8}\n\nfor feature in d_types:\n    train[feature] = train[feature].astype(d_types[feature])\n    \ngc.collect();","123224bc":"# building_id is useless. I am explaining it in my EDA kernel: https:\/\/www.kaggle.com\/nroman\/eda-for-ashrae\ndel train['building_id']","338b2ee2":"# By default dataset is not sorted by time. We want to train on the past data to predict future.\ntrain = train.sort_index(by='timestamp').reset_index(drop=True)\n\n# timestamp is no longer needed\ndel train['timestamp']\n\n# Cut first 80% of the training dataset and the last 20% keep as holdout\ncut_idx = int(len(train) * 0.8)\nX_train, y_train, X_test, y_test = train.iloc[:cut_idx], y.iloc[:cut_idx], train.iloc[cut_idx:], y.iloc[cut_idx:]","10e6d199":"bounds = {\n    'learning_rate': (0.002, 0.2),\n    'num_leaves': (50, 500), \n    'bagging_fraction' : (0.1, 1),\n    'feature_fraction' : (0.1, 1),\n    'min_child_weight': (0.001, 0.5),   \n    'min_data_in_leaf': (20, 170),\n    'max_depth': (-1, 32),\n    'reg_alpha': (0.1, 2), \n    'reg_lambda': (0.1, 2)\n}","41fec342":"def train_model(learning_rate, \n                num_leaves,\n                bagging_fraction, \n                feature_fraction, \n                min_child_weight,\n                min_data_in_leaf,\n                max_depth,\n                reg_alpha,\n                reg_lambda):\n    \n    params = {'learning_rate': learning_rate,\n              'num_leaves': int(num_leaves), \n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'min_child_weight': min_child_weight,   \n              'min_data_in_leaf': int(min_data_in_leaf),\n              'max_depth': int(max_depth),\n              'reg_alpha': reg_alpha, \n              'reg_lambda': reg_lambda,\n              'objective': 'regression',\n              'boosting_type': 'gbdt',\n              'random_state': 47,\n              'verbosity': 0,\n              'metric': 'rmse'}\n    \n    trn_data = lgb.Dataset(X_train, y_train)\n    val_data = lgb.Dataset(X_test, y_test)\n    model = lgb.train(params, trn_data, 5000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=50)\n    # Returning negative rmse because optimizer tries to maximize a function\n    return -model.best_score['valid_1']['rmse']","9c12bcda":"optimizer = BayesianOptimization(f=train_model, pbounds=bounds, random_state=47)\noptimizer.maximize(init_points=10, n_iter=20)","4f30038e":"print('Best RMSE score:', -optimizer.max['target'])","3d7b5da9":"print('Best set of parameters:')\nprint(optimizer.max['params'])","10312a75":"# Fitting an optimizer","685d5462":"# Some manipulations with data","ab3ad1dd":"# Importing libraries and merging a dataset\n\nWe will need only training set to find the optimal parameters since the goal of this kernel is not to make a submission but to optimally tune a model. Also this would allow us not to worry about memory.","8d077d1b":"# Best parameters","65b81684":"And here we go."}}