{"cell_type":{"b1310512":"code","96457544":"code","fc06dbd0":"code","2389bc39":"code","8398da02":"code","115fc645":"code","f01b8f82":"code","d55278da":"code","707a207d":"markdown","1d9c35cb":"markdown","29727649":"markdown","725ae6fe":"markdown","4ca520b4":"markdown","c31ef8b5":"markdown","a7f69575":"markdown","300cf4ca":"markdown"},"source":{"b1310512":"#importing numpy\nimport numpy as np","96457544":"#sigmoid and darivative of sigmoid function\ndef sigdiv(x,deriv=False):\n    if(deriv==True):\n        return x*(1-x)\n    else:\n        return 1\/(1+np.exp(-x))","fc06dbd0":"# input arrays\nX = np.array([[0,0,1],\n            [0,1,1],\n            [1,0,1],\n            [1,1,1]])","2389bc39":"# output array\nY = np.array([[0],\n            [1],\n            [1],\n            [0]])","8398da02":"np.random.seed(0)","115fc645":"# Randomly initializing our weights with mean zero\nW0 = 2*np.random.random((3,4)) - 1\nW1 = 2*np.random.random((4,1)) - 1","f01b8f82":"for j in range(100000):\n    #1 Feed forward through layers 0, 1, and 2\n    l0 = X\n    l1 = sigdiv(np.dot(l0,W0))\n    l2 = sigdiv(np.dot(l1,W1))\n    \n    #2 Calculating error\n    l2_error = Y - l2\n    \n    #printing error\n    if (j% 10000) == 0:\n        print(\"Error iafter \" + str(j) + \" itration :\" + str(np.mean(np.abs(l2_error))))\n    \n    # in what direction is the target value\n    #3 calculating change to made in weights. \n    l2_delta = l2_error*sigdiv(l2,deriv=True)\n    \n    #4 how much did each l1 value contribute to the l2 error (according to the weights)?\n    l1_error = l2_delta.dot(W1.T)\n    \n    # in what direction is the target value\n    # calculating change to made in weights.\n    l1_delta = l1_error * sigdiv(l1,deriv=True)\n    \n    #5 updating weights\n    W1 += l1.T.dot(l2_delta)\n    W0 += l0.T.dot(l1_delta)","d55278da":"#Output after training\nl2","707a207d":"### This imports numpy, which is a linear algebra library. ","1d9c35cb":"A sigmoid function maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks.\nNotice that this function can also generate the derivative of a sigmoid (when deriv=True). One of the desirable properties of a sigmoid function is that its output can be used to create its derivative. If the sigmoid's output is a variable \"out\", then the derivative is simply out * (1-out). This is very efficient. ","29727649":"Inputs          Output<br>                                                                                                                                                                                                                                         \nA--B--C---D                                                                                                                                                                                                                                                   \n0--0---1---0                                                                                                                                                                                                                                                   \n1---1---1----1                                                                                                                                                                                                                                                   \n1---0---1----1                                                                                                                                                                                                                                                     \n0---1---1---0                                                                                                                                                                                                                                                       \n","725ae6fe":"Created by :**MOHIT CHATURVEDI**","4ca520b4":"# Introduction\nThis Notebook impliments the Neural Network to describe the Backpropogation. ","c31ef8b5":"## Training neral network\nThis begins our actual network training code. This for loop \"iterates\" multiple times over the training code to optimize our network to the dataset.","a7f69575":"1. Three layers neural network\n\n   1.1 Input layer L0.                                                                                                                                                                                                                                         \n  \n  1.2 Hidden layer L1.\n   \n   1.2 Output layer L2 \n   .\n2. Error calculation for hidden layer wights. l2_error is the amount hidden layer has missed.\n\n3. Calculating the change to made in W2 weights by finding the derivetive of L2.\nwhen L2_errors are multiplied by L2 derivative then the confident errors are muted beacuse L2_error will have close value to zero.\n\n4. Caculating the error passed by l1 to l2.\n5. updating the weights by adding them(W1,W2) to their respective errors. \n\nThe weights are updated at each itration of loop and neural network learns on each back propogation.\nRemember that the input is used as a single batch.\n","300cf4ca":"It appears to be completely unrelated to column three, which is always 1. However, columns 1 and 2 give more clarity. If either column 1 or 2 are a 1 (but not both!) then the output is a 1. This is our pattern.( A XOR B)\n\nThis is considered a \"nonlinear\" pattern because there isn't a direct one-to-one relationship between the input and output. Instead, there is a one-to-one relationship between a combination of inputs, namely columns 1 and 2."}}