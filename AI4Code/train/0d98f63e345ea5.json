{"cell_type":{"f06939e2":"code","83295b72":"code","4681c14f":"code","6f865841":"code","26dfdb46":"code","1b1248a8":"code","2fd4c5b5":"code","4dff5c4d":"code","b842a1fe":"code","2ee32fe4":"code","651694d6":"code","5af78b07":"code","2871f7ed":"code","eda2999a":"code","d802ab81":"code","5d38c47e":"code","86ef4d50":"markdown","f65f0c1a":"markdown"},"source":{"f06939e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83295b72":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrandom_state = 42\n\nnp.random.seed(random_state)\n\ndata = pd.read_csv('\/kaggle\/input\/diamonds-datamad1021-rev\/train.csv')\n\ndata.drop('id', axis=1, inplace=True)\n\ndata.head(10)","4681c14f":"data.info()","6f865841":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, train_size=0.8, random_state=random_state)\n\ndata['price_cat'] = np.ceil(data['price'] \/ data['price'].mean())\ntrain_idx, test_idx = next(iter(split.split(data, data['price_cat'])))\ntrain_set, test_set = data.loc[train_idx], data.loc[test_idx]\n\ntrain_set.drop('price_cat', axis=1, inplace=True)\ntest_set.drop('price_cat', axis=1, inplace=True)\ndata.drop('price_cat', axis=1, inplace=True)\n\ntrain_features, train_labels = train_set.drop('price', axis=1), train_set['price'].copy()\ntest_features, test_labels = test_set.drop('price', axis=1), test_set['price'].copy()\n\nprint(f'Train set size: {len(train_features)}, test set size: {len(test_features)}')","26dfdb46":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nclass NumericalDataSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attributes):\n        self.attributes = attributes\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X[self.attributes]\n\nnum_columns = list(train_features.select_dtypes(exclude=[object]))\n\nnum_pipeline = Pipeline([\n    ('data_selector', NumericalDataSelector(num_columns)),\n    ('num_transformer', StandardScaler())\n])\n\nnum_data = num_pipeline.fit_transform(train_features)\nnum_data","1b1248a8":"from sklearn.preprocessing import LabelBinarizer\n\nclass CategoricalDataBinarizer(BaseEstimator, TransformerMixin):\n    @staticmethod\n    def transform_column(x, col_name):\n        encoder = LabelBinarizer()\n        col_ds = encoder.fit_transform(x[col_name])\n        col_values = np.unique(x[col_name])\n        col_df = pd.DataFrame(col_ds, columns=col_values)\n        return col_df\n\n\n    def __init__(self, attributes):\n        self.attributes = attributes\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        new_dfs = [self.transform_column(X, col) for col in X[self.attributes]]\n        new_X = pd.concat(new_dfs, axis=1)\n        return new_X.values\n\ncat_columns = list(train_features.select_dtypes(include=[object]))\n\ncat_pipeline = Pipeline([\n    ('cat_binarizer', CategoricalDataBinarizer(cat_columns))\n])\n\ncat_data = cat_pipeline.fit_transform(train_features)\ncat_data","2fd4c5b5":"def cat_get_new_column_names(x, col_names):\n    new_col_names = []\n    for col in col_names:\n        new_col_names += list(np.unique(x[col]))\n    return new_col_names\n\nnew_cat_columns = cat_get_new_column_names(train_set, cat_columns)\nnew_cat_columns","4dff5c4d":"from sklearn.pipeline import FeatureUnion\n\ndiamond_preprocess = FeatureUnion(transformer_list=[\n    ('num_processor', num_pipeline),\n    ('cat_processor', cat_pipeline)\n], verbose=1)\n\ntrain_features_prepared = diamond_preprocess.fit_transform(train_features)\ntest_features_prepared = diamond_preprocess.fit_transform(test_features)\ntrain_features_prepared","b842a1fe":"preprocessed_dataframe = pd.DataFrame(train_features_prepared, columns=num_columns + new_cat_columns)\npreprocessed_dataframe.head()","2ee32fe4":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nregressor = RandomForestRegressor(random_state=random_state).fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","651694d6":"from sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression().fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","5af78b07":"from sklearn.ensemble import GradientBoostingRegressor\n\nregressor = GradientBoostingRegressor(random_state=random_state).fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","2871f7ed":"from sklearn.neighbors import KNeighborsRegressor\n\nregressor = KNeighborsRegressor().fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","eda2999a":"from sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(random_state=random_state).fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","d802ab81":"from sklearn.ensemble import AdaBoostRegressor\n\nregressor = AdaBoostRegressor(random_state=random_state).fit(train_features_prepared, train_labels)\n\npredictions = regressor.predict(test_features_prepared)\n\nnp.sqrt(mean_squared_error(test_labels, predictions))","5d38c47e":"submition_dataset = pd.read_csv('\/kaggle\/input\/diamonds-datamad1021-rev\/test.csv')\nsubmition_ids = submition_dataset['id'].copy()\nsubmition_test = submition_dataset.drop('id', axis=1)\nsubmition_test = diamond_preprocess.fit_transform(submition_test)\n\nsubmition_regressor = RandomForestRegressor(random_state=random_state).fit(train_features_prepared, train_labels)\n\nsubmission_preds = submition_regressor.predict(submition_test)\n\ndf = pd.DataFrame({'id': submition_ids.values, 'price': submission_preds})\n\ndf.to_csv('submission.csv', index=False)","86ef4d50":"I wanted to show the power of sklearn.pipeline and how useful it might be. Feel free to comment if you like it or have something to add. ","f65f0c1a":"That's all. If you found it useful - please upvote;)\nHave a nice day!"}}