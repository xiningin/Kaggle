{"cell_type":{"7d17bf0f":"code","316abee9":"code","d33040ca":"code","ab402404":"code","e755a3e3":"code","103437ea":"code","2ee0652d":"code","13dbf6fe":"code","c4ba3d36":"code","c4e33a4f":"code","600b0767":"code","61c5ffb0":"code","02dab550":"code","0083e8a6":"code","294c44b8":"code","d865b45a":"markdown","6107ab71":"markdown","02b5d312":"markdown","b5c313f6":"markdown","b733f8e5":"markdown","9a062898":"markdown","f4bec274":"markdown","e222f543":"markdown","eb026f2a":"markdown","179d4a5c":"markdown","f7f31047":"markdown","8a7c8a95":"markdown","5b498b8f":"markdown","e948c45c":"markdown","524f2979":"markdown","2ac3255b":"markdown"},"source":{"7d17bf0f":"# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# train est split operator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport tqdm\n\n# the model that will be used\nfrom catboost import CatBoostRegressor\nimport pandas as pd\nimport numpy as np\n#display all columns\npd.set_option(\"display.max_columns\", None)","316abee9":"# read data\ntrain = pd.read_csv('..\/input\/widsdatathon2022\/train.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2022\/test.csv')\n#train = train.drop('id', axis=1)\ntrain.head()\n\nprint(train.shape)\nprint(test.shape)","d33040ca":"target = 'site_eui'\n\n# histogran to evaluate target distribution\ntrain[target].hist(bins=50, density=True)\nplt.ylabel('Number of buildings')\nplt.xlabel('Site Energy Usage Intensity ')\nplt.show()","ab402404":"np.log(train[target]).hist(bins=50, density=True)\nplt.ylabel('Number of buildings')\nplt.xlabel('Site Energy Usage Intensity ')\nplt.show()","e755a3e3":"# Function to calculate missing values by column# Funct \n# from https:\/\/www.kaggle.com\/parulpandey\/starter-code-with-baseline\ndef missing_values_table(df):\n        # Total missing values by column\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values by column\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # build a table with the thw columns\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\n# Missing values for training data\nmissing_values_train = missing_values_table(train)\nmissing_values_train[:20].style.background_gradient(cmap='Reds')","103437ea":"#  let's male a list of discrete variables\nMAX = 30\ndiscrete_vars = [var for var in train.columns if len(train[var].unique()) < MAX ]\n\nplt.figure(figsize=(10, 50))\nsns.set(style=\"darkgrid\")\n\ni=1\nfor col in discrete_vars:\n    plt.subplot(len(discrete_vars), 1, i)\n    sns.boxplot(data=train, x=col, y='site_eui')#.set(title=col)\n    i=i+1\nplt.show()","2ee0652d":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","13dbf6fe":"corr[target].sort_values(key=abs, ascending=False)[:10]","c4ba3d36":"# prepare dataset for training\ntarget = train['site_eui'].values\ndel train['site_eui']\ndel train['id']\n\n# keep. test_ids to use it for submission\ntest_ids = test['id']\ndel test['id']","c4e33a4f":"for var in missing_values_train.index:\n    # compute the imputation value for each var\n    if train[var].dtype!='float':\n        impute_value = train[var].mode()\n    else:\n        impute_value = train[var].mean()\n    # apply the imputation in the train\n    train[var] = train[var].fillna(impute_value)\n    # apply the same imputation in the test\n    test[var] = test[var].fillna(impute_value)","600b0767":"# get discrete end categorical features colums indexes \n# needed later for the cat bosst model\ncats_discrete_idx = np.where(train.dtypes != 'float')[0]\n# create the label\nle = LabelEncoder()\nfor col_idx in cats_discrete_idx:\n    train[col] = le.fit_transform(train.iloc[:, col_idx].astype(str))\n    test[col] = le.fit_transform(test.iloc[:, col_idx].astype(str))\n    ","61c5ffb0":"# prepaere the out of folds predictions \ntrain_oof = np.zeros((train.shape[0],))\ntest_preds = np.zeros(test.shape[0])","02dab550":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n# we. can also use \n\nfor fold, (train_idx, test_idx) in tqdm.tqdm(enumerate(kf.split(train, target))):\n    X_train, X_test = train.iloc[train_idx][test.columns], train.iloc[test_idx][test.columns]\n    y_train, y_test = target[train_idx], target[test_idx]\n    \n    ## config from https:\/\/www.kaggle.com\/nicapotato\/simple-catboost\n    model = CatBoostRegressor(iterations=500,\n                         learning_rate=0.02,\n                         depth=12,\n                         eval_metric='RMSE',\n#                         early_stopping_rounds=42,\n                         random_seed = 23,\n                         bagging_temperature = 0.2,\n                         od_type='Iter',\n                         metric_period = 75,\n                         od_wait=100)\n    # train model\n    model.fit(X_train, y_train,\n                 eval_set=(X_test,y_test),\n                 cat_features=cats_discrete_idx,\n                 use_best_model=True,\n                 verbose=True)\n\n    oof = model.predict(X_test)\n    train_oof[test_idx] = oof\n    test_preds += model.predict(test)\/NUM_FOLDS      \n    print(f\"out-of-folds prdiction ==== fold_{fold} RMSE\",np.sqrt(mean_squared_error(oof, y_test, squared=False)))","0083e8a6":"# save results\nnp.save('train_oof.npy', train_oof)\nnp.save('test_preds.npy', test_preds)","294c44b8":"sub = pd.DataFrame(columns=['id', 'site_eui'])\nsub['id']  = test_ids\nsub['site_eui'] = test_preds\nsub.to_csv('submission_CB1.csv', index=False)","d865b45a":"### Discrete variables\nDiscrete features are varibles that show a finite number of values","6107ab71":"### Missing values imputation\n* Categorical or Discrete variables => **most_frequent** imputation\n* Linear features => **mean imputation**","02b5d312":"The most insteresting correlation to retain is the `energy_star_rating` : **buildings with low energy star rating have higher energy intensity**","b5c313f6":"The distribution is **skewed towards the right**, which can be improved with a mathematical transformation so that it would have a more **Gaussian** like shape","b733f8e5":"### Objective :\nWe aim to predict the energy consumption instensity by building using climate and weather features .\n","9a062898":"### Missing Values","f4bec274":"### Modeling;\n\nIn this notebook we will test out only catboost model, we can enhance performances by stacking it with other models .\n\nI will use cross evaluation method:\n* The training predictions are generated with out-of-folds technique\n\nfor more informqtion qbout this method check out this [article](https:\/\/machinelearningmastery.com\/out-of-fold-predictions-in-machine-learning\/#:%7E:text=An%20out%2Dof%2Dfold%20prediction,example%20in%20the%20training%20dataset.) \n> An out-of-fold prediction is a prediction by the model during the k-fold cross-validation procedure. That is, out-of-fold predictions are those predictions made on the holdout datasets during the resampling procedure. If performed correctly, there will be one prediction for each example in the training dataset\n\nThe test prediction is the mean of CV-models predictions\n\n","e222f543":"The datset contains only 5 features having missing values we need to impute the missing data especially for those with big propotion of missing values.","eb026f2a":"### Correlation analysis","179d4a5c":"The imnplementation code sample was inspired from [Mathurin Ache's notebook starter](https:\/\/www.kaggle.com\/mathurinache\/starter-wids2022\/notebook) **thanks to upvote it**\n\nAnd you can check  for all his great contributions : his profile is fulled of great content","f7f31047":"### Save data and submit file\n","8a7c8a95":"### Label encoding\nwe will apply the label encoding both in the **categorical** features and the **discrete** features (features with finite values number)","5b498b8f":"### Target\nLet's plot target distribution","e948c45c":"We can't extract a valuable informatiom from these box plot all what we can say is that the Energy intensity can show off extreme high values at each modality","524f2979":"### Data Analysis","2ac3255b":"### Feature Engineering"}}