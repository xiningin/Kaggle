{"cell_type":{"087e6bf6":"code","c3293cef":"code","0b19c618":"code","bbf93c23":"code","3215d5d8":"code","704676e5":"code","0ce5a14b":"code","df3561bf":"code","085d6e00":"code","c5bd8b94":"code","b6477720":"code","c1d0dc83":"code","66f6c1ca":"code","8f103984":"code","7a09a57b":"code","7b131493":"code","ad12c824":"code","220e0f3b":"code","3c0c73c0":"code","adf39034":"code","e81176d8":"code","f1363006":"code","bbe439c7":"code","11cc087b":"code","b990ea19":"code","ad75c51a":"code","fcb0be03":"code","865f0b73":"code","00382361":"code","6ad3a371":"code","b21f8c57":"code","4e514954":"code","8128688a":"code","97bcbaf6":"code","d0adc773":"code","dfe1b90b":"code","c69e532c":"code","5c9592ef":"code","66b74b67":"code","c1fdcbd6":"code","21337955":"code","8dabebaf":"code","036f222d":"code","073e6e89":"code","b158da79":"code","261aba62":"code","2bde774a":"code","4f85cade":"code","eeddf04b":"code","f88935d9":"code","37a5a750":"code","fd4951fb":"code","307d19eb":"code","ae740b30":"code","a6a4cd72":"code","9ed8378d":"code","d9dd0909":"code","3a7cca78":"code","739dcc42":"code","49fc091c":"code","5b021d99":"code","bfbbc702":"code","f532b9b2":"code","6c9b2b7e":"code","0838b72f":"code","b8d848d5":"code","7f43f104":"code","dde37519":"code","3e453063":"code","cbfc55b3":"code","e226191c":"code","685fa69a":"code","4e3d3d47":"code","791fd4fb":"code","05413ef3":"code","f6098fc4":"code","c77788fc":"code","bedc7bba":"code","ce7965f4":"code","224b4e1a":"code","b043b233":"code","20a2701d":"code","1a50ecd4":"code","20ec43e9":"code","aa646ff5":"code","6b9ac48c":"code","db7d93e9":"code","8e7eb5dd":"code","cb467e40":"code","fc4a80fb":"code","ebdf7165":"code","5868d4de":"code","71648e57":"code","0e6de333":"code","91eb2ba0":"code","b5b12fc9":"code","25641c2e":"code","36886ebc":"code","b3cf6de1":"code","8ed2cfce":"code","114478c4":"code","78a9c317":"code","239bf012":"code","3cca0fed":"code","44fe64a7":"code","7d4cb95c":"code","d022cbf7":"code","d74a2935":"code","33eda72a":"code","f19cb496":"code","03f53539":"code","0ecfc650":"code","13cc9090":"code","01910184":"code","649e8a42":"code","8fbdd08f":"code","48c86322":"code","f4690a3b":"code","41d6c9b0":"code","1925b123":"code","cbccf7bd":"code","b98fe2ee":"code","3080fc14":"markdown","81ac3d58":"markdown","b1f3fc2f":"markdown","4f024fbd":"markdown","f105af84":"markdown","cd849f22":"markdown","bbd35164":"markdown","9c1e7033":"markdown","bf89d489":"markdown","9075fccb":"markdown","8b349780":"markdown","71493b31":"markdown","d7a03f33":"markdown","e4de5f59":"markdown","62ae3c7d":"markdown","c77d59ea":"markdown","db3c8a32":"markdown","056f8325":"markdown","d0520d77":"markdown","37ef9e22":"markdown","56fd8b5e":"markdown","7529ca3c":"markdown","9eb1d3a7":"markdown","325f8290":"markdown","5487f38f":"markdown","acb9f620":"markdown","7235b071":"markdown","a12c14f5":"markdown","c7e04117":"markdown","96d07ff9":"markdown","a93c4d79":"markdown","2b16aa3d":"markdown"},"source":{"087e6bf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3293cef":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0b19c618":"# Variable\tDefinition\n# User_ID\tUser ID\n# Product_ID\tProduct ID\n# Gender\tSex of User\n# Age\tAge in bins\n# Occupation\tOccupation (Masked)\n# City_Category\tCategory of the City (A,B,C)\n# Stay_In_Current_City_Years\tNumber of years stay in current city\n# Marital_Status\tMarital Status\n# Product_Category_1\tProduct Category (Masked)\n# Product_Category_2\tProduct may belongs to other category also (Masked)\n# Product_Category_3\tProduct may belongs to other category also (Masked)\n# Purchase\tPurchase Amount (Target Variable)","bbf93c23":"data= pd.read_csv(\"\/kaggle\/input\/black-friday\/train.csv\")","3215d5d8":"data.head()","704676e5":"data.shape","0ce5a14b":" # Target-- Purchase\n\n # Data understanding and cleaning\n # EDA\n # Base Model\n # Feature Selection - ANOVA, CHI-SQUARE\n # Try different models\n # Parametric Tuning    - RMSE","df3561bf":"# Checking\ndata.info()","085d6e00":"# Checking the null values in the dataset\ndata.isnull().sum()\/data.shape[0] *100","c5bd8b94":"# Only product_category_1 and product_category_2 have null values\n# Denoting none of the customers have purchased the product- Let's replace that with '0'\ndata['Product_Category_2'].fillna(0,inplace=True)\ndata['Product_Category_3'].fillna(0,inplace=True)","b6477720":"data.isnull().sum()\/data.shape[0] *100","c1d0dc83":"print(\"Total number of USER_ID: \", data['User_ID'].nunique())\n# It seems a repeadted purchases on the same user id as it near to 6000 while the data is for 5 lakhs\n# Other possiblitity only 1% have a unique user_id\ndata['User_ID'].value_counts().plot(kind='hist')","66f6c1ca":"data.groupby(['User_ID'])['Purchase'].sum().sort_values(ascending=False)[:10].plot(kind='bar')\nplt.xlabel(\"User_ID\")\nplt.ylabel(\"sum purchase in millions\")\nplt.show()","8f103984":"# We need to target the user-ID \"1004277\" for more increase in the sales","7a09a57b":"# Product_ID\nprint(\"Total number of product_id :\",data['Product_ID'].nunique())\ndata['Product_ID'].value_counts().plot(kind='hist')  # Only certain Product are contributing more","7b131493":"ss= data['Product_ID'].value_counts()[:10]\nss.plot(kind='bar')# Count wise product_id \nplt.xlabel(\"Product_id\")\nplt.ylabel(\"Count_of_Id\")\nplt.show()","ad12c824":"data.groupby(['Product_ID'])['Purchase'].sum().plot(kind='hist')","220e0f3b":"data.groupby(['Product_ID'])['Purchase'].sum().sort_values(ascending=False)[:10].plot(kind='bar')\nplt.xlabel(\"Product_id\")\nplt.ylabel(\"sum purchase in millions\")\nplt.show()","3c0c73c0":"data_Sex = data.groupby('Gender')['Gender'].count()\ndata_Sex = pd.DataFrame({'Sex':data_Sex.index, 'Count':data_Sex.values})\nplt.pie(data_Sex['Count'],labels = data_Sex['Sex'],autopct='%1.1f%%',shadow=True);\nplt.title('Gender Split in data');\n","adf39034":"print(data.groupby(['Gender'])['Purchase'].sum())\ndata_GP=data.groupby(['Gender'])['Purchase'].sum()\nplt.pie(data_GP,autopct='%1.1f%%',labels=['Female','Male'])\nplt.show()","e81176d8":"sns.countplot(data['Gender'],hue=data[\"Age\"])","f1363006":"# Count of Male and Purchase sum is high  --- so we need to focus on them more","bbe439c7":"sns.countplot(data['Age'])\nplt.title(\"Age distribution\")\nplt.show()","11cc087b":"data_Age = data.groupby('Age')['Age'].count()\ndata_Age = pd.DataFrame({'Age':data_Age.index, 'Count':data_Age.values})\nplt.pie(data_Age['Count'],labels = data_Age['Age'],autopct='%1.1f%%',shadow=True);\nplt.title('Age split in data');\nplt.show()","b990ea19":"data.groupby('Age')['Purchase'].mean().plot()\nplt.xlabel('Age group')\nplt.ylabel('Average_Purchase amount in $')\nplt.title('Age group vs average amount spent')\nplt.show()","ad75c51a":"### If you observe here the puchase in the age group of 51-55 is comparatively higher with only 7%","fcb0be03":"sns.countplot(data['Age'],hue=data[\"Marital_Status\"])\n# 1 married and 0 unmarried","865f0b73":"sns.countplot(data['Age'],hue=data[\"Gender\"])","00382361":"print(\"City wise Contribution\", data['City_Category'].value_counts(normalize=True) *100)\nsns.countplot(data['City_Category'])","6ad3a371":"data.groupby('City_Category')['Purchase'].mean().plot()","b21f8c57":"data['Occupation'].value_counts().plot(kind='bar')","4e514954":"OS= data.groupby(['Occupation'])['Purchase'].mean()\nplt.plot(OS.index,OS.values,'ro-')\nplt.xticks(OS.index)\nplt.xlabel('Occupation types')\nplt.ylabel('Average purchase amount in $')\nplt.title('Average amount of purchase for each Occupation')\nplt.show()","8128688a":"# Inference\n# Number of more counts in occuptation doesn't contribute more in the purchase amount\n# Mean value of purchase value for occuptation 8 & 15 is more compartievly to the number of counts(Heavy Spenders)\n# More effort on the less occupation (8&15) coulld generate more purchases\n# Occupation 11 to 18 looks like a target are to focus in terms of raising puchases\n# On other hand We can concentrate is there a possiblity of increasing the more count occupation to contribute to purchase","97bcbaf6":"plt.figure(figsize=[12,8])\nsns.countplot(data['Occupation'],hue=data[\"Age\"])","d0adc773":"data['Stay_In_Current_City_Years'].value_counts().plot()","dfe1b90b":"data1= data.groupby('Stay_In_Current_City_Years')['Purchase'].sum().reset_index()\ndata2= data['Stay_In_Current_City_Years'].value_counts()\ndata2=pd.DataFrame({\"Stay_In_Current_City_Years\":data2.index, \"Count\":data2.values})\nnw_data = pd.merge(data1,data2,left_on='Stay_In_Current_City_Years',right_on='Stay_In_Current_City_Years',how = 'left');\n\nnw_data = nw_data.sort_values(['Stay_In_Current_City_Years'],ascending=False)[0:10];\nnw_data","c69e532c":"    \nplt.figure(figsize=(16,6));\nplt.grid();\nplt.plot(nw_data['Stay_In_Current_City_Years'],nw_data['Purchase'],'o-');\nplt.xlabel('Stay_In_Current_City_Years');\nplt.ylabel('Total amount it was purchased in 10\\'s of Million $');\nplt.title('Stay wise connection with purchases');\nfor a,b,c in zip(nw_data['Stay_In_Current_City_Years'], nw_data['Purchase'], nw_data['Count']): \n    plt.text(a, b+100000, str(c))  \nplt.show();\n","5c9592ef":"data.groupby(['Gender','Marital_Status'])['Purchase'].count().plot(kind='pie',figsize=(8,8))","66b74b67":"print(\"Count of martial_status\", data.groupby(['Marital_Status'])['Purchase'].count())\nprint(\"Average purchase amount\", data.groupby(['Gender','Marital_Status'])['Purchase'].mean())\ndata.groupby(['Marital_Status'])['Purchase'].mean().plot(kind='bar')\ndata.groupby(['Gender','Marital_Status'])['Purchase'].mean().unstack().plot(kind='bar')\nplt.show()\n","c1fdcbd6":"data['Product_Category_1'].value_counts().plot(kind= 'bar', figsize=(16,5))","21337955":"PC1= data.groupby('Product_Category_1')['Purchase'].mean()\nplt.figure(figsize=(12,8))\nplt.plot(PC1.index,PC1.values,'ro-')\nplt.xticks(PC1.index)\nplt.show()","8dabebaf":"data['Product_Category_2'].value_counts().plot(kind= 'bar', figsize=(16,5))","036f222d":"PC2= data.groupby('Product_Category_2')['Purchase'].mean()\nplt.figure(figsize=(12,8))\nplt.plot(PC2.index,PC2.values,'ro-')\nplt.xticks(PC2.index)\nplt.show()","073e6e89":"data['Product_Category_3'].value_counts().plot(kind= 'bar', figsize=(16,5))","b158da79":"PC3= data.groupby('Product_Category_3')['Purchase'].mean()\nplt.figure(figsize=(12,8))\nplt.plot(PC3.index,PC3.values,'ro-')\nplt.xticks(PC3.index)\nplt.show()","261aba62":"data['Purchase'].plot(kind='hist')","2bde774a":"# For the base level -- Creating a copy droping the null values\ndf= data.copy(deep=True)","4f85cade":"df.info()","eeddf04b":"# Product_categories are int\n#df[['Product_Category_2','Product_Category_3']]=df[['Product_Category_2','Product_Category_3']].astype('int')\n# Stay_in_city\ndf['Stay_In_Current_City_Years'].replace({'4+':4},inplace=True)\n# df['Stay_In_Current_City_Years']=df['Stay_In_Current_City_Years'].astype('int')","f88935d9":"# Gender\ndf['Gender'].replace({\"M\":1,\"F\":0},inplace=True)","37a5a750":"# Age\ndef map_age(age):\n    if age == '0-17':\n        return 0\n    elif age == '18-25':\n        return 1\n    elif age == '26-35':\n        return 2\n    elif age == '36-45':\n        return 3\n    elif age == '46-50':\n        return 4\n    elif age == '51-55':\n        return 5\n    else:\n        return 6","fd4951fb":"df['Age']=df['Age'].apply(map_age)","307d19eb":"print(df['Product_Category_2'].describe())\nprint(\"--------------------------------\")\nprint(df['Product_Category_3'].describe())","ae740b30":"# Mean and Median are some _what close to each other\n# Hence filling the null values with the mean","a6a4cd72":"df['Product_Category_2']=df['Product_Category_2'].fillna(9.0).astype(int)\ndf['Product_Category_3']=df['Product_Category_3'].fillna(13.0).astype(int)","9ed8378d":"df['City_Category'].value_counts()","d9dd0909":"# Mapping the City_Category \n\ndf['City_Category']=df['City_Category'].map({\"B\":1,\"A\":2,\"C\":3})","3a7cca78":"df['City_Category']= df['City_Category'].astype(int)","739dcc42":"df['Stay_In_Current_City_Years']= df['Stay_In_Current_City_Years'].astype(int)","49fc091c":"## Making a copy if these needs to included again\n\nddf=df.copy()","5b021d99":"df = df.drop([\"User_ID\",\"Product_ID\"],axis=1)\n# As it contains more number of unique values","bfbbc702":"# X and Y split -- train_test_split\n\nfrom sklearn.model_selection import train_test_split\nX = df.drop(\"Purchase\",axis=1)\ny = df['Purchase']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","f532b9b2":"# Base Model- Decision Tree\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np","6c9b2b7e":"dtr= DecisionTreeRegressor()","0838b72f":"dtr.fit(X_train,y_train)\nd_predict= dtr.predict(X_test)","b8d848d5":"print(\"RMSE score for Decision Tree : \", np.sqrt(mean_squared_error(y_test,d_predict)))","7f43f104":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split","dde37519":"rfc=RandomForestRegressor(n_estimators=150)\ngbr=GradientBoostingRegressor()\nxg=XGBRegressor()","3e453063":"rfc.fit(X_train, y_train)\nr_predict= rfc.predict(X_test)","cbfc55b3":"gbr.fit(X_train,y_train)\ng_predict= gbr.predict(X_test)","e226191c":"xg.fit(X_train, y_train)\nxg_predict= xg.predict(X_test)","685fa69a":"print(\"RMSE score for Random_Forest : \", np.sqrt(mean_squared_error(y_test,r_predict)))\nprint(\"RMSE score for Gradient Boosting : \", np.sqrt(mean_squared_error(y_test,g_predict)))\nprint(\"RMSE score for Gradient Boosting : \", np.sqrt(mean_squared_error(y_test,xg_predict)))","4e3d3d47":"import statsmodels.api as sm","791fd4fb":"#Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.04):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","05413ef3":"# Back_ward Elimination is predicting all as an important features","f6098fc4":"# RFE method of feature selection","c77788fc":"from sklearn.feature_selection import RFE","bedc7bba":"rfc=RandomForestRegressor()\ngbr=GradientBoostingRegressor()","ce7965f4":"#Initializing RFE model\nrfe = RFE(rfc, 7)","224b4e1a":"#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\n","b043b233":"rfe.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","20a2701d":"X.columns","1a50ecd4":"X= df[['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']]","20ec43e9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","aa646ff5":"rfc.fit(X_train, y_train)\nr_predict= rfc.predict(X_test)","6b9ac48c":"print(\"RMSE score for Random_Forest : \", np.sqrt(mean_squared_error(y_test,r_predict)))","db7d93e9":"gbr.fit(X_train,y_train)\ng_predict= gbr.predict(X_test)","8e7eb5dd":"print(\"RMSE score for Gradient Boosting : \", np.sqrt(mean_squared_error(y_test,g_predict)))","cb467e40":"xg.fit(X_train, y_train)\nxg_predict= xg.predict(X_test)","fc4a80fb":"print(\"RMSE score for XG Boosting : \", np.sqrt(mean_squared_error(y_test,xg_predict)))","ebdf7165":"### Performance level have been improvised with the RFE feature selection without including the product_catergory 2 & 3\n### But thats the worst thing to do and the best thing to do if you want only product category 1 to be compared","5868d4de":"# We have removed the user_id and product_id entirely\n# Lets try to map the frequent used and products rather than eliminating all\n# Reference to other kaggle notebooks","71648e57":"# Mapping the User_ID based on the importance for the top 20 rather than excluding them totally\nuser_ids=ddf['User_ID']\ncounts=user_ids.value_counts().index[:19]\n# important_counts=set(counts.index[:19])\n# user_ids=user_ids.map(lambda user_id:user_id if user_id  in important_counts else 0)\n#from sklearn.preprocessing import OneHotEncoder\n#user_id_encoder=OneHotEncoder(categories ='auto')\n#user_id_encoder.fit(user_ids.values.reshape(-1,1))","0e6de333":"ddf['User_ID']=ddf['User_ID'].map(lambda user_id:user_id if user_id  in counts else 0)","91eb2ba0":"# Product_ID\nproduct_means=ddf.groupby([\"Product_ID\"])[\"Purchase\"].mean()\ntotal_mean=ddf['Purchase'].mean()","b5b12fc9":"product_means.sort_values","25641c2e":"pid=product_means.sort_values(ascending=False).index[:19]","36886ebc":"ddf['Product_ID']=ddf['Product_ID'].map(lambda product_id:product_id if product_id  in pid else 0)","b3cf6de1":"ddf['User_ID']=ddf['User_ID'].astype('object')","8ed2cfce":"nddf= pd.get_dummies(ddf,drop_first=True)","114478c4":"X=nddf.drop(['Purchase'],1)\ny=nddf['Purchase']","78a9c317":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)","239bf012":"rfc=RandomForestRegressor()\ngbr=GradientBoostingRegressor()","3cca0fed":"rfc.fit(X_train, y_train)\nr_predict= rfc.predict(X_test)","44fe64a7":"gbr.fit(X_train,y_train)\ng_predict= gbr.predict(X_test)","7d4cb95c":"# Trying XGboost for the many sparse matrix- tree algorithm don't perform the best\nfrom xgboost import XGBRegressor","d022cbf7":"xg=XGBRegressor()","d74a2935":"xg.fit(X_train, y_train)\nxg_predict= xg.predict(X_test)","33eda72a":"print(\"RMSE score for Random_Forest : \", np.sqrt(mean_squared_error(y_test,r_predict)))\nprint(\"RMSE score for Gradient Boosting : \", np.sqrt(mean_squared_error(y_test,g_predict)))\nprint(\"RMSE score for XG Boosting : \", np.sqrt(mean_squared_error(y_test,xg_predict)))","f19cb496":"## From the normal way of labelling, feature selection and top_20 user_id and product_id\n## We got the best score with respect to validation from --> Normal Labeling the model without user_id and product_id rather than taking dummies on the top 20\n## Hence we use that particular set for training and predicting on the test_data","03f53539":"## Checking the test_data\ntest= pd.read_csv(\"\/kaggle\/input\/black-friday\/test.csv\")","0ecfc650":"test.isnull().sum()","13cc9090":"print(\" Product _category_2 \\n \" ,test['Product_Category_2'].describe())\nprint(\" Product _category_3 \\n \" ,test['Product_Category_3'].describe())","01910184":"test.info()","649e8a42":"# Similary Meand and median are colse by.\n# taking a mid range to fill the null values\ntest['Product_Category_2']=test['Product_Category_2'].fillna(9.0).astype(int)\ntest['Product_Category_3']=test['Product_Category_3'].fillna(13.0).astype(int)","8fbdd08f":"# Stay_In_Current_City_Years\ntest['Stay_In_Current_City_Years']=test['Stay_In_Current_City_Years'].replace({'4+':4})\ntest['Stay_In_Current_City_Years']=test['Stay_In_Current_City_Years'].astype(int)\n# Age\ntest['Age']=test['Age'].apply(map_age)\n# Gender\ntest['Gender'].replace({\"M\":1,\"F\":0},inplace=True)\n# Mapping the City_Category \ntest['City_Category']=test['City_Category'].map({\"B\":1,\"A\":2,\"C\":3})\ntest['City_Category']=test['City_Category'].astype(int)\n\n","48c86322":"test.drop(['User_ID','Product_ID'],axis=1,inplace=True)","f4690a3b":"# Choosing df --> Where USER_ID,PRODUCT_ID are dropped\nfrom sklearn.model_selection import train_test_split\nX = df.drop(\"Purchase\",axis=1)\ny = df['Purchase']","41d6c9b0":"# Model that performed the best is XG_Boost\nxg.fit(X,y)\ntest_predict=xg.predict(test)","1925b123":"test['Purchase']=test_predict","cbccf7bd":"print(\"Purchase distribution for the test data\", sns.distplot(test['Purchase']))","b98fe2ee":"print(\"Final prediction for the test data \\n \")\ntest.head()","3080fc14":"Inference- \n- Purchase peak is arround 5000 and 10000 more in relation to the count","81ac3d58":"Inference:\n- Product_catergory_1 ranges from (1000-20000)\n- In product catergory 1 --> Count wise (5,1,8) contributes to the most, Purchase Amount wise(10,9,7,6) contribues more\n- It may its a costly item ","b1f3fc2f":"Inference:\n\n-Our focus is to increase the price of sales not interest in the count so we need to focus on specific product_id contributing more to the purchases.","4f024fbd":"def user_id_transform(ddf):\n    uid=ddf['User_ID'].map(lambda user_id:user_id if user_id  in important_counts else 0).values.reshape(-1,1)\n    uid=user_id_encoder.transform(uid).toarray()\n    for index,category in enumerate(user_id_encoder.categories_[0]):\n        ddf[str(category)]= uid[:,index]\n    ddf.drop(columns=['User_ID'],inplace=True)\n    return ddf","f105af84":"### Product_ID","cd849f22":"### Testing ","bbd35164":"### User_ID                       \n","9c1e7033":"### Let's try feature selection to check if there is a improvement in the performance of the model","bf89d489":"Inference\n\n- Unmarried count is more with both Male and Female genders - Overall purchase amount is same\n- No special concentration required, show equal importance","9075fccb":"### Purchases","8b349780":"Inference:\n- Product_catergory_3 ranges from (9000-14000)\n- Product_categor_3 have a diverese range of product with the least contribution on \"12\" check on that.","71493b31":"### Age","d7a03f33":"### Preparing the data for the model","e4de5f59":"### Occupation                  ","62ae3c7d":"Inference:\n- Even the number of count in the city b is more, purchase wise City C is contributing more.","c77d59ea":"Inference\n- Occupation 4 which is of more count as more number of youngsters- We can focus on the product of their interest in relation to their occupation\n- Age group\"26-35\" are almost high in every categories as they contribute 40% of the total ages, yet the puchase amount is less if we can attract them we can surely increase the sales by 5-10%","db3c8a32":"### Modelling & Validating","056f8325":"### Trying Other models","d0520d77":"### Product_Categories","37ef9e22":"### Gender","56fd8b5e":"### Stay_In_Current_City_Years","7529ca3c":"Inference:\n- Product_catergory_2 ranges from (7000-16000)\n- In product_2 category \"10\" contributes the wide range of purchase amount","9eb1d3a7":"### Marital_Status","325f8290":"### Performance check","5487f38f":"Inference\n- Here you can observe that the person staying 1 year are on the exploration state\n- As the stay increases the purchase amount decreases maybe they have got all the stuff needed are we need to understand there requirements","acb9f620":"### City Category","7235b071":"- USer_ID and product_ID plays a important part in the domain but the performance of the model was better without them(seems confusing)\n- As a future scope we can analyse the bias and the varaince error of the model with respect to different CrossVallidation for choosing the model\n- Increase the number of USER_ID,Product_id give it a another shot\n- Perform hyper tuning of the model to improvise the performance of the XG_Boost and other models","a12c14f5":"As we can observe there is multicollinearity between product_Catory 1 ,2 and 3","c7e04117":"## Conclusion","96d07ff9":"### Performance check","a93c4d79":"### Futher Analysis","2b16aa3d":"Inference:\n- 26-35 age group where they contribute around 40% and sum of their purchases are more even though they are small- (Unmarried)\n- While Unmarried are more in the contribution"}}