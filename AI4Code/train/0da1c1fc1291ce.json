{"cell_type":{"c26f6a64":"code","571afe94":"code","898f85ae":"code","0f20c413":"code","5cd207cd":"code","bec7b881":"code","015b9162":"code","4891f233":"code","11606470":"code","cd5c9e6d":"code","87219344":"code","5f13ab07":"code","af3a0699":"code","70cc35b9":"code","f6d25ff6":"code","b4142c45":"code","eda1f825":"code","f3fb765d":"code","91b54cb4":"code","55fea7b2":"code","d6edcdd5":"code","b49d4ecb":"code","ec645fc0":"code","eb151051":"code","c12ffe70":"markdown","ebc8a9d4":"markdown","1795240e":"markdown","5c923ee0":"markdown","23e2bc43":"markdown","1df55294":"markdown"},"source":{"c26f6a64":"%matplotlib inline\n\nfrom functools import partial\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nfrom IPython.display import display_html\n\nfrom sklearn.manifold import TSNE\n\nfrom bayes_opt import BayesianOptimization\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\nimport lightgbm as lgb\nimport warnings\nwarnings.simplefilter('ignore', category=FutureWarning)\n\npd.set_option('display.max_columns', 100)\nsns.set_style('whitegrid')","571afe94":"PATH = Path('..\/input')\n[f.name for f in PATH.iterdir()]","898f85ae":"train_df = pd.read_csv(PATH\/'flight_delays_train.csv')\nprint(train_df.shape)\ntrain_df.head()","0f20c413":"test_df = pd.read_csv(PATH\/'flight_delays_test.csv')\nprint(test_df.shape)\ntest_df.head()","5cd207cd":"train_df['dep_delayed_15min'].value_counts(normalize=True)","bec7b881":"def plot_by_target(df, col, plot=sns.countplot, hue='dep_delayed_15min', sharey=False):\n    g = sns.FacetGrid(train_df, col=hue, height=4.5, aspect=1, sharey=sharey)\n    g.map(plot, col)\n    g.set_xticklabels(rotation=90);","015b9162":"plot_by_target(train_df, 'Month')","4891f233":"plot_by_target(train_df, col='DayofMonth')","11606470":"plot_by_target(train_df, 'DayOfWeek')","cd5c9e6d":"plot_by_target(train_df, col='DepTime', plot=plt.hist)","87219344":"plot_by_target(train_df, col='DepTime', plot=partial(sns.boxplot, showfliers=False))","5f13ab07":"plot_by_target(train_df, col='UniqueCarrier')","af3a0699":"plot_by_target(train_df, col='Distance', plot=plt.hist)","70cc35b9":"plot_by_target(train_df, col='Distance', plot=partial(sns.boxplot, showfliers=False))","f6d25ff6":"# Extract the labels\ntrain_y = train_df.pop('dep_delayed_15min')\ntrain_y = train_y.map({'N': 0, 'Y': 1})\n\n# Concatenate for preprocessing\ntrain_split = train_df.shape[0]\nfull_df = pd.concat((train_df, test_df))\n\n# Hour and minute\nfull_df['hour'] = full_df['DepTime'] \/\/ 100\nfull_df.loc[full_df['hour'] == 24, 'hour'] = 0\nfull_df.loc[full_df['hour'] == 25, 'hour'] = 1\nfull_df['minute'] = full_df['DepTime'] % 100\n\n# Season\nfull_df['summer'] = (full_df['Month'].isin(['c-6', 'c-7', 'c-8'])).astype(np.int32)\nfull_df['autumn'] = (full_df['Month'].isin(['c-9', 'c-10', 'c-11'])).astype(np.int32)\nfull_df['winter'] = (full_df['Month'].isin(['c-12', 'c-1', 'c-2'])).astype(np.int32)\nfull_df['spring'] = (full_df['Month'].isin(['c-3', 'c-4', 'c-5'])).astype(np.int32)\n\n# Daytime\nfull_df['daytime'] = pd.cut(full_df['hour'], bins=[0, 6, 12, 18, 23], include_lowest=True)","b4142c45":"# String to numerical\nfor col in ['Month', 'DayofMonth', 'DayOfWeek']:\n    full_df[col] = full_df[col].apply(lambda x: x.split('-')[1]).astype(np.int32) - 1\n\n# Label Encoding\nfor col in ['Origin', 'Dest', 'UniqueCarrier', 'daytime']:\n    full_df[col] = pd.factorize(full_df[col])[0]\n\n# Categorical columns\ncat_cols = ['Month', 'DayofMonth', 'DayOfWeek', 'Origin', 'Dest', 'UniqueCarrier', 'hour', 'summer', 'autumn', 'winter', 'spring', 'daytime']\n\n# Converting categorical columns to type 'category' as required by LGBM\nfor c in cat_cols:\n    full_df[c] = full_df[c].astype('category')\n\n# Split into train and test\ntrain_df, test_df = full_df.iloc[:train_split], full_df.iloc[train_split:]\ntrain_df.shape, train_y.shape, test_df.shape","eda1f825":"def cross_val_scheme(model, train_df, train_y, cv):\n    cv_scores = cross_val_score(model, train_df, train_y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    print(f'CV Scores: {cv_scores}')\n    print(f'CV mean: {cv_scores.mean()} \\t CV Std: {cv_scores.std()}')\n    model.fit(train_df, train_y)\n    feat_imp = pd.DataFrame({'col': full_df.columns.values, 'imp': model.feature_importances_}).sort_values(by='imp', ascending=False)\n    return cv_scores, feat_imp","f3fb765d":"skf = StratifiedKFold(n_splits=5, random_state=7, shuffle=True)\nclf = lgb.LGBMClassifier(random_state=7)\ncv_scores, feat_imp = cross_val_scheme(clf, train_df, train_y, skf)\nplt.figure(figsize=(8, 10))\nsns.barplot(x='imp', y='col', data=feat_imp[1:], orient='h');","91b54cb4":"bay_tr_ix, bay_val_ix = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=7).split(train_df, train_y))[0]","55fea7b2":"def LGB_bayesian(num_leaves, min_data_in_leaf, learning_rate, min_sum_hessian_in_leaf, feature_fraction, lambda_l1,\n                 lambda_l2, min_gain_to_split, max_depth):\n    \n    num_leaves = int(np.round(num_leaves))\n    min_data_in_leaf = int(np.round(min_data_in_leaf))\n    max_depth = int(np.round(max_depth))\n    \n    params = {\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'learning_rate': learning_rate,\n        'max_depth': max_depth,\n        'min_data_in_leaf': min_data_in_leaf,\n        'min_gain_to_split': min_gain_to_split,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'num_leaves': num_leaves,\n        'max_bin': 255,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 6,\n        'save_binary': True,\n        'seed': 7,\n        'feature_fraction_seed': 7,\n        'bagging_seed': 7,\n        'drop_seed': 7,\n        'data_random_seed': 7,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': True,\n        'n_jobs': -1\n    }\n    \n    lgb_train = lgb.Dataset(train_df.loc[bay_tr_ix], train_y.loc[bay_tr_ix], free_raw_data=False)\n    lgb_valid = lgb.Dataset(train_df.loc[bay_val_ix], train_y.loc[bay_val_ix], free_raw_data=False)\n    \n    num_rounds=5000\n    clf = lgb.train(params, lgb_train, num_rounds, valid_sets=[lgb_train, lgb_valid], verbose_eval=250, early_stopping_rounds=50)\n    val_preds = clf.predict(train_df.loc[bay_val_ix], num_iterations=clf.best_iteration)\n    \n    score = roc_auc_score(train_y.loc[bay_val_ix], val_preds)\n    return score\n\n\nbounds_lgb = {\n    'feature_fraction': (0.5, 1),\n    'lambda_l1': (0., 10.),\n    'lambda_l2': (0., 10.),\n    'learning_rate': (0.01, 0.1),\n    'max_depth': (2, 8),\n    'min_data_in_leaf': (5, 30),\n    'min_gain_to_split': (0, 1),\n    'min_sum_hessian_in_leaf': (0.01, 1),\n    'num_leaves': (10, 35)\n}\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_lgb, random_state=7)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=10, n_iter=10, acq='ucb')","d6edcdd5":"LGB_BO.max['target'], LGB_BO.max['params']","b49d4ecb":"def test_kfold(params, train_df, train_y, test_df, cv):\n    test_preds = 0.\n    valid_preds = np.zeros(train_y.shape)\n    \n    for fold, (train_ix, valid_ix) in enumerate(cv.split(train_df, train_y)):\n        print(f\"\\nFOLD: {fold+1} {'='*50}\")\n        X_train, X_valid = train_df.iloc[train_ix], train_df.iloc[valid_ix]\n        y_train, y_valid = train_y.iloc[train_ix], train_y.iloc[valid_ix]\n        \n        lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, free_raw_data=False)\n        \n        clf = lgb.train(params, lgb_train, 5000, valid_sets=[lgb_train, lgb_valid], verbose_eval=250, early_stopping_rounds=50)\n        valid_preds[valid_ix] = clf.predict(train_df.iloc[valid_ix], num_iterations=clf.best_iteration)\n        test_preds += clf.predict(test_df, num_iterations=clf.best_iteration)\n    \n    print(f'Valid CV: {roc_auc_score(train_y, valid_preds)}')\n    test_preds \/= cv.n_splits\n    \n    return test_preds","ec645fc0":"params = {\n    'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n    'learning_rate': LGB_BO.max['params']['learning_rate'],\n    'max_depth': int(np.round(LGB_BO.max['params']['max_depth'])),\n    'min_data_in_leaf': int(np.round(LGB_BO.max['params']['min_data_in_leaf'])),\n    'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n    'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n    'num_leaves': int(np.round(LGB_BO.max['params']['num_leaves'])),\n    'max_bin': 255,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 3,\n    'save_binary': True,\n    'seed': 7,\n    'feature_fraction_seed': 7,\n    'bagging_seed': 7,\n    'drop_seed': 7,\n    'data_random_seed': 7,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'is_unbalance': True,\n    'boost_from_average': True,\n    'n_jobs': -1\n}\n\n# 5-Fold testing\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')    \n    test_preds = test_kfold(params, train_df, train_y, test_df, StratifiedKFold(n_splits=5, random_state=7, shuffle=True))","eb151051":"final_df = pd.DataFrame({'id': range(test_preds.shape[0]), 'dep_delayed_15min': test_preds})\nfinal_df.to_csv('lightgbm_5fold_sub_v2.csv', header=True, index=False)\npd.read_csv('lightgbm_5fold_sub_v2.csv').head()","c12ffe70":"## Creating new features","ebc8a9d4":"## Training and testing","1795240e":"# Submission","5c923ee0":"## Load the data","23e2bc43":"## EDA","1df55294":"## Bayesian optimization"}}