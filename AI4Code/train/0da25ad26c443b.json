{"cell_type":{"40033b3d":"code","d2163da5":"code","f422e8d7":"code","04d23ff8":"code","f73eb3b4":"code","21c128a9":"code","2055e451":"code","4bd07ade":"code","b751f1e7":"code","05b40200":"code","523b3076":"code","c483b3f2":"code","9ab0ff1e":"code","3409bb49":"code","632b66ad":"code","4004465c":"code","f28f0389":"code","223ebf18":"code","fd811fb4":"code","f4762171":"code","3f3a0e2f":"code","4d0dfea0":"code","c81e36e7":"code","27d8091d":"code","454825e0":"code","c30843a8":"code","f645bb54":"code","36ac875b":"code","1134f543":"code","4ba1ccc2":"code","4b23e77d":"code","364df8cc":"code","caee6008":"code","343ad35c":"code","15d1c9e3":"code","7bf51495":"code","97c5935f":"code","0ba328b1":"code","759dc3d2":"code","9a6b97fc":"code","0d7764ce":"code","fc2fbd1d":"code","01d2ed4b":"code","1688cce3":"code","8cac87b9":"code","714b379c":"code","4f43d7ca":"code","3ed7ea3b":"code","f7ce4b24":"code","4f88cd48":"code","99893ca4":"code","44c35070":"code","b903d7c8":"code","7f69f57d":"code","a5b36b2a":"code","e10cef1f":"code","21949396":"code","20fef4dd":"code","90e16cc2":"code","8e2ba895":"code","896b01c1":"code","d996e6fe":"code","999e4e21":"code","22ac58cb":"code","38918799":"code","23d8d624":"code","a257f1ed":"code","db4666b6":"code","c90dfa92":"code","bc838dcc":"code","315d3fd9":"code","ac3fe04a":"code","1467ec37":"code","4c91156f":"code","973781a9":"code","425e5e1b":"code","9cf2461e":"code","52638987":"code","d8ac1214":"code","899da3b7":"code","43243abb":"code","522b2e03":"code","b700a376":"code","20b0f92d":"code","6ac5b0eb":"code","34ab2eef":"code","ed14174a":"code","5981a34b":"code","592d5d01":"code","8df51a13":"code","81dc8377":"code","ced52707":"code","7ce00f44":"code","2ac7bdb0":"code","68bc66a5":"code","73e17d0e":"code","bc540eab":"code","5d149eda":"code","d7583767":"code","a006b6dd":"code","ef5bebe3":"markdown","6a0daf13":"markdown"},"source":{"40033b3d":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nimport catboost\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\n\n\nfrom tqdm import tqdm\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\nimport sys\nimport os\nimport gc\nfrom glob import glob\nimport pickle\nimport json\nimport subprocess\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, RepeatedStratifiedKFold\n","d2163da5":"# !pip install --no-index --find-links ..\/input\/pytorchset timm\n# !pip install --no-index --find-links ..\/input\/pytorchset pytorch-lightning","f422e8d7":"# import torch\n# import torch.optim as optim\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# import torchvision.transforms as T\n# import timm\n# from sklearn.model_selection import StratifiedKFold\n# from torchvision.io import read_image\n# from torch.utils.data import DataLoader, Dataset\n\n# import pytorch_lightning as pl\n# from pytorch_lightning.utilities.seed import seed_everything\n# from pytorch_lightning import callbacks\n# from pytorch_lightning.callbacks.progress import ProgressBarBase\n# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n# from pytorch_lightning.loggers import TensorBoardLogger\n# from pytorch_lightning import LightningDataModule, LightningModule\n\n# from sklearn.metrics import mean_squared_error\n# from scipy.optimize import minimize","04d23ff8":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","f73eb3b4":"# import lightgbm as lgb\n\n# import cupy as cp # linear algebra\n# import cudf as cd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# from cuml.svm import SVR\n# from cuml.decomposition import PCA\n\n# from cuml.metrics import accuracy_score","21c128a9":"DF_CEILING_VALUE = 20000.0\nSHOP_CEILING_VALUE = 999999.9\nITEM_CEILING_VALUE = 999999.9\n","2055e451":"PATH = '..\/input\/competitive-data-science-predict-future-sales\/'\n\ndf = pd.read_csv(PATH + 'sales_train.csv')\ndf_test = pd.read_csv(PATH + 'test.csv')\nsample = pd.read_csv(PATH + 'sample_submission.csv')\nitems = pd.read_csv(PATH + 'items.csv')\nshops = pd.read_csv(PATH + 'shops.csv')\nitem_cats = pd.read_csv(PATH + 'item_categories.csv')","4bd07ade":"data_files_names = [\"df\",\"df_test\",\"sample\",\"items\",\"shops\",\"item_cats\"]\ndata_files = [df,df_test,sample,items,shops,item_cats]","b751f1e7":"df_test = df_test.drop('ID', axis=1)","05b40200":"df","523b3076":"df.loc[df['item_cnt_day'] < 0.0, 'item_cnt_day'] = 0.0","c483b3f2":"df[df['item_cnt_day'] > 100.0]","9ab0ff1e":"df.loc[df['item_cnt_day'] > 100.0, 'item_cnt_day'] = 1.0\nlen(df[df['item_cnt_day']>10.0])","3409bb49":"df[df['item_cnt_day'] > 10.0]","632b66ad":"df.loc[df['shop_id'] == 57, 'shop_id'] = 0\ndf.loc[df['shop_id'] == 58, 'shop_id'] = 1\ndf.loc[df['shop_id'] == 11, 'shop_id'] = 10\ndf.loc[df['shop_id'] == 40, 'shop_id'] = 39\n\ndf_test.loc[df_test['shop_id'] == 57, 'shop_id'] = 0\ndf_test.loc[df_test['shop_id'] == 58, 'shop_id'] = 1\ndf_test.loc[df_test['shop_id'] == 11, 'shop_id'] = 10\ndf_test.loc[df_test['shop_id'] == 40, 'shop_id'] = 39","4004465c":"df['shop_item_id'] = df['shop_id'].astype('str').str.zfill(2) +  df['item_id'].astype('str').str.zfill(5)\ndf['item_category_id'] = pd.merge(df, items, on='item_id',how='left')['item_category_id']\ndf","f28f0389":"df = df.groupby(['date_block_num', 'shop_id','item_id','shop_item_id'], as_index=False\n        ).agg({'item_cnt_day':'sum'}\n        ).rename(columns={'item_cnt_day':'mon_shop_item_cnt'})\ndf","223ebf18":"# Add shop_item_id to df_test\ndf_test['shop_item_id'] = df_test['shop_id'].astype('str').str.zfill(2) + df_test['item_id'].astype('str').str.zfill(5)\ndf['item_category_id'] = pd.merge(df, items, on='item_id',how='left')['item_category_id']\ndf_test","fd811fb4":"count = 0\ndf_ids = df['shop_item_id'].unique()\nrepeat_count = 0\nfor one_id in df_test['shop_item_id'].sample(1000):\n    if one_id in df_ids:\n        count += 1\n    repeat_count += 1\n\n    if repeat_count > 1000:\n        break\nprint(count \/ repeat_count)","f4762171":"plt.figure(figsize=(12,6))\nplt.hist(df['mon_shop_item_cnt'])","3f3a0e2f":"df.loc[df['mon_shop_item_cnt'] > 100.0, 'mon_shop_item_cnt'] = 101.0","4d0dfea0":"print(len(df.loc[df['mon_shop_item_cnt'] > 100.0]), len(df.loc[df['mon_shop_item_cnt'] > 101.0]))","c81e36e7":"# # left 214200\n# transition = df_test\n# for i in range(34):\n#     transition = pd.merge(transition, df[df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i }), how='left')\n# # transition = prepro_transition(transition, DF_CEILING_VALUE)\n# transition = transition.fillna(0)\n# transition","27d8091d":"# Outer -> 520915\ntransition = pd.DataFrame(np.unique(np.concatenate([df['shop_item_id'], df_test['shop_item_id']])), columns=['shop_item_id'])\nfor i in range(34):\n    transition = pd.merge(transition, df[df['date_block_num']==i].drop(['date_block_num', 'shop_id', 'item_id', 'item_category_id'], axis=1).rename(columns={'mon_shop_item_cnt': i}), on='shop_item_id', how='left')\ntransition = transition.fillna(0)\ntransition","454825e0":"plt.figure(figsize=(12,6))\nplt.bar(transition.loc[:, 0:].columns,transition.loc[:, 0:].sum())","c30843a8":"plt.figure(figsize=(12,6))\nplt.hist(transition.loc[:, 0:].T.sum())","f645bb54":"# I think these data is too big but not invalid\ntransition[transition.loc[:, 0:].T.sum() >= 500.0]","36ac875b":"# transition_mean = transition.loc[:, 0:].mean().mean()\n# transition_std = transition.loc[:, 0:].std().std()\ntransition_max = transition.loc[:, 0:].max().max()\ntransition_max","1134f543":"std_transition = transition.copy()\nstd_transition.loc[:, 0:] = (std_transition.loc[:, 0:]) \/ transition_max\nstd_transition","4ba1ccc2":"shops.loc[shops['shop_id'] == 57, 'shop_id'] = 0\nshops.loc[shops['shop_id'] == 58, 'shop_id'] = 1\nshops.loc[shops['shop_id'] == 11, 'shop_id'] = 10\nshops.loc[shops['shop_id'] == 40, 'shop_id'] = 39\nshops","4b23e77d":"shop_df = df.groupby(['date_block_num', 'shop_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'}\n        )\nshop_transition = pd.DataFrame(shops['shop_id'].unique(), columns=['shop_id'])\n\nfor i in range(34):\n    shop_transition = pd.merge(shop_transition, shop_df[shop_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='shop_id', how='left')\nshop_transition = shop_transition.fillna(0)\n\nshop_transition_max = shop_transition.loc[:, 0:].max().max()\nshop_transition.loc[:, 0:] = (shop_transition.loc[:, 0:]) \/ shop_transition_max\n\nshop_transition","364df8cc":"shop_feature = transition.loc[:, ['shop_item_id']].copy()\nshop_feature['shop_id'] = shop_feature['shop_item_id'].str[:2].astype(int)\nshop_feature","caee6008":"shop_feature = pd.merge(shop_feature, shop_transition, on='shop_id', how='left')\nshop_feature = shop_feature.drop('shop_id', axis=1)\nshop_feature","343ad35c":"items","15d1c9e3":"item_df = df.groupby(['date_block_num', 'item_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'})\nitem_transition = pd.DataFrame(items['item_id'].unique(), columns=['item_id'])\n\nfor i in range(34):\n    item_transition = pd.merge(item_transition, item_df[item_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='item_id', how='left')\nitem_transition = item_transition.fillna(0)\n\nitem_transition_max = item_transition.loc[:, 0:].max().max()\nitem_transition.loc[:, 0:] = (item_transition.loc[:, 0:]) \/ item_transition_max\n\nitem_transition","7bf51495":"item_feature = transition.loc[:, ['shop_item_id']].copy()\nitem_feature['item_id'] = item_feature['shop_item_id'].str[2:].astype(int)\nitem_feature = pd.merge(item_feature, item_transition, on='item_id', how='left')\nitem_feature = item_feature.drop('item_id', axis=1)\nitem_feature","97c5935f":"cats_df = df.groupby(['date_block_num', 'item_category_id'], as_index=False\n        ).agg({'mon_shop_item_cnt':'sum'})\ncats_transition = pd.DataFrame(cats_df['item_category_id'].unique(), columns=['item_category_id'])\nfor i in range(34):\n    cats_transition = pd.merge(cats_transition, cats_df[cats_df['date_block_num']==i].drop('date_block_num', axis=1).rename(columns={'mon_shop_item_cnt': i}), on='item_category_id', how='left')\ncats_transition = cats_transition.fillna(0)\n\ncats_transition_max = cats_transition.loc[:, 0:].max().max()\ncats_transition.loc[:, 0:] = (cats_transition.loc[:, 0:]) \/ cats_transition_max\n\ncats_transition","0ba328b1":"cats_feature = transition.loc[:, ['shop_item_id']].copy()\ncats_feature['item_id'] = cats_feature['shop_item_id'].str[2:].astype(int)\ncats_feature['item_category_id'] = pd.merge(cats_feature, items, on='item_id',how='left')['item_category_id']\ncats_feature = pd.merge(cats_feature, cats_transition, on='item_category_id', how='left')\ncats_feature = cats_feature.drop(['item_id','item_category_id'], axis=1)\ncats_feature","759dc3d2":"print(shop_feature.loc[:, 0:].mean().mean(), item_feature.loc[:, 0:].mean().mean())","9a6b97fc":"# Add noise\nshop_feature.loc[:, 0:] += np.random.normal(0, shop_feature.loc[:, 0:].mean().mean() * 0.025, shop_feature.loc[:, 0:].shape)\nitem_feature.loc[:, 0:] += np.random.normal(0, item_feature.loc[:, 0:].mean().mean() * 0.025, item_feature.loc[:, 0:].shape)\ncats_feature.loc[:, 0:] += np.random.normal(0, cats_feature.loc[:, 0:].mean().mean() * 0.025, cats_feature.loc[:, 0:].shape)","0d7764ce":"shop_feature","fc2fbd1d":"features = [std_transition, shop_feature, item_feature, cats_feature]\n# features = [std_transition, item_feature]","01d2ed4b":"pre_ave = 0.3","1688cce3":"# Generate dataset\nindex = std_transition.index\n\ntrain_index = []\nval_index = []\ntest_index = []\n\ntrain_index = index\n# train_index, val_index = train_test_split(index, test_size=0.2)\n# val_index, test_index = train_test_split(val_index, test_size=0.2)","8cac87b9":"# print(len(std_transition),len(train_index), len(val_index))\nprint(len(std_transition),len(train_index), len(val_index), len(test_index) )","714b379c":"std_transition.loc[:20000, 0:].T.sum().sort_values()","4f43d7ca":"std_transition.loc[18252]","3ed7ea3b":"df_test","f7ce4b24":"sub_index = pd.DataFrame(std_transition.index, columns=['index'])\nsub_index['shop_item_id'] = std_transition['shop_item_id']\nsub_index = pd.merge(df_test, sub_index, on='shop_item_id',how='left')\nprint(sub_index.isna().sum())\nsub_index = sub_index.fillna(18252)\nsub_index = sub_index['index']\nsub_index","4f88cd48":"val_index = sub_index\nval_index","99893ca4":"std_pre_ave = (pre_ave \/ transition_max)\n# std_pre_ave = adj_y_val.mean()\nprint(std_pre_ave, pre_ave, transition_max)","44c35070":"class WindowGenerator():\n    def __init__(self, start_month, last_target_month, input_width, train_index=train_index, val_index=val_index, test_index=test_index, sub_index=sub_index, features=features):\n        self.start_month = start_month\n        self.last_target_month = last_target_month\n        \n        # Work out the window parameters.\n        self.input_width = input_width\n        # It includes y value\n        self.total_width = input_width + 1\n        \n        # Store the raw data.\n        self.train_index = train_index\n        self.val_index = val_index\n        self.test_index = test_index\n        self.sub_index = sub_index\n        \n        # Fixed\n        self.features = features\n        self.features_number = len(features)\n        \n        self.repeat_number = self.last_target_month - self.start_month - self.input_width\n    def __repr__(self):\n        return '\\n'.join([\n            f'Start month: {self.start_month}',\n            f'Input window size: {self.input_width}',\n            f'last_target_month: {self.last_target_month}',\n            \n            f'repeat_number: {self.repeat_number}',\n            \n            f'features_number: {self.features_number}',\n        ])\n    \n    def generate_window(self, features_lists, index_i, start_point):\n        one_window = []\n        for window_i in range(self.input_width):\n            one_features = []\n            for feature_i in range(self.features_number):\n                # Array start zero, so I plus 1 to start point.\n                one_features.append(features_lists[feature_i][index_i][start_point + 1 + window_i])\n            one_window.append(one_features)\n\n        return one_window\n    \n    def generate_list(self, index):\n        x_data = []\n        y_data = []\n        \n        index_number = len(index)\n        features_lists = [feature.loc[index].drop('shop_item_id', axis=1).to_numpy().tolist() for feature in features]\n        \n        reduce = 0\n        \n        # (batch, time, features)\n        for index_i in range(index_number):\n            for time_i in range(self.repeat_number):\n                start_point = self.start_month + time_i\n                one_y = features_lists[0][index_i][start_point + self.total_width]\n                \n                if self.repeat_number > 2:\n                    if one_y < std_pre_ave:\n                        if np.random.rand() < 0.3:\n                            continue\n                \n                one_window = self.generate_window(features_lists, index_i, start_point)\n                \n                x_data.append(one_window)\n                y_data.append(one_y)\n\n        return x_data, y_data\n    def generate_sub_x(self):\n        features_lists = [feature.loc[self.sub_index].drop('shop_item_id', axis=1).to_numpy().tolist() for feature in features]\n        index_number = len(self.sub_index)\n        \n        # start_point + self.total_width = 34\n        start_point = 34 - self.total_width\n        \n        sub_x = []\n        # (batch, time, features)\n        for index_i in range(index_number):\n             sub_x.append(self.generate_window(features_lists, index_i, start_point))\n                \n        return sub_x\n    \n    def make_dataset(self, index):\n        x_data, y_data = self.generate_list(index)\n        return tf.Dataset.from_tensor_slices((x_data, y_data))\n\n    def train_list(self):\n        return self.generate_list(self.train_index)\n\n    def val_list(self):\n        return self.generate_list(self.val_index)\n\n    def test_list(self):\n        return self.generate_list(self.test_index)\n    \n    \n    @property\n    def train(self):\n        return self.make_dataset(self.train_index)\n\n    @property\n    def val(self):\n        return self.make_dataset(self.val_index)\n\n    @property\n    def test(self):\n        return self.make_dataset(self.test_index)\n","b903d7c8":"input_width = 29\n\ntrain_window = WindowGenerator(\n    start_month = 30 - (input_width + 1),\n    last_target_month = 32,\n    input_width = input_width,\n)\ntrain_window","7f69f57d":"val_window = WindowGenerator(\n    start_month = 33 - (input_width + 1),\n    last_target_month = 33,\n    input_width = input_width,\n)\nval_window","a5b36b2a":"%%time\nx_train, y_train = train_window.train_list()","e10cef1f":"%%time\nx_val, y_val = val_window.val_list()","21949396":"adj_x_train = np.array(x_train)\nadj_y_train = np.array(y_train)\nadj_y_train = adj_y_train.clip(0, 20.0 \/ transition_max)\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())","20fef4dd":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_train))","90e16cc2":"# One time delete 0.01%\nover_index = np.where(adj_y_train < std_pre_ave)[0]\nprint(over_index)\nprint(len(adj_y_train), len(over_index))\ndrop_rate = 0.035\ndrop_number = int(len(over_index) * drop_rate) if int(len(over_index) * drop_rate) > 1.0 else 1\nprint(drop_number)\n\nadj_area = 0.0001","8e2ba895":"%%time\nfor i in range(100):\n    over_index = np.where(adj_y_train < std_pre_ave)[0]\n    np.random.shuffle(over_index)\n    drop_index = over_index[:drop_number]\n    drop_number = int(drop_number * 0.95)\n    \n    adj_x_train = np.delete(adj_x_train, drop_index, 0)\n    adj_y_train = np.delete(adj_y_train, drop_index)\n    if adj_y_train.mean() + adj_area > std_pre_ave:\n        break\nprint(f'Stop i:{i}')\nprint(adj_y_train.mean(), adj_y_train.std(), adj_y_train.max())","896b01c1":"adj_x_val = np.array(x_val)\nadj_y_val = np.array(y_val)\nadj_y_val = adj_y_val.clip(0, 20.0 \/ transition_max)\nprint(adj_y_val.mean(), adj_y_val.std(), adj_y_val.max())","d996e6fe":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_val))","999e4e21":"# One time delete 0.01%\nover_index = np.where(adj_y_val < std_pre_ave)[0]\nprint(over_index)\nprint(len(adj_y_val), len(over_index))\ndrop_rate = 0.02\ndrop_number = int(len(over_index) * drop_rate) if int(len(over_index) * drop_rate) > 1.0 else 1\nprint(drop_number)\n\nadj_area = 0.0001","22ac58cb":"%%time\nfor i in range(100):\n    over_index = np.where(adj_y_val < std_pre_ave)[0]\n    np.random.shuffle(over_index)\n    drop_index = over_index[:drop_number]\n    drop_number = int(drop_number * 0.95)\n    \n    adj_x_val = np.delete(adj_x_val, drop_index, 0)\n    adj_y_val = np.delete(adj_y_val, drop_index)\n    if adj_y_val.mean() + adj_area > std_pre_ave:\n        break\nprint(f'Stop i:{i}')\nprint(adj_y_val.mean(), adj_y_val.std(), adj_y_val.max())","38918799":"plt.figure(figsize=(12,6))\nplt.hist(np.sqrt(adj_y_val))","23d8d624":"del x_train, y_train, x_val, y_val\ntf.keras.backend.clear_session()\ngc.collect()","a257f1ed":"n_models = []","db4666b6":"tpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    # Enable XLA\n    tf.config.optimizer.set_jit(enabled = \"autoclustering\")\n    strategy = tf.distribute.get_strategy()\n    \n# Set Auto Tune\nAUTOTUNE = tf.data.experimental.AUTOTUNE   ","c90dfa92":"BATCH_SIZE = 2048\nSHUFFLE_BUFFER_SIZE = 100","bc838dcc":"dataset_train = adj_x_train.reshape(adj_x_train.shape + (1,))\ndataset_val = adj_x_val.reshape(adj_x_val.shape + (1,))\ndataset_train = tf.data.Dataset.from_tensor_slices((dataset_train, adj_y_train))\ndataset_val = tf.data.Dataset.from_tensor_slices((dataset_val, adj_y_val))\n\ndataset_train = dataset_train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\ndataset_val = dataset_val.batch(BATCH_SIZE)","315d3fd9":"del adj_x_train, adj_y_train, adj_x_val, adj_y_val\ntf.keras.backend.clear_session()\ngc.collect()","ac3fe04a":"dataset_train","1467ec37":"with strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(input_width, len(features), 1)),\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(32),\n        tf.keras.layers.Dense(32),\n        tf.keras.layers.Dense(units=1)\n    ])\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',\n                                                        mode='min', patience=3)\n\n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.RootMeanSquaredError()])\n\n    # val_performance['Dense'] = dense.evaluate(val_window.val)\n    # performance['Dense'] = dense.evaluate(val_window.test, verbose=0)\n    model.summary()","4c91156f":"model.fit(dataset_train, epochs=30,\n    validation_data=dataset_val,\n    callbacks=[early_stopping],\n    batch_size=BATCH_SIZE,\n)\nn_models.append(model)","973781a9":"del model, dataset_train, dataset_val\ntf.keras.backend.clear_session()\ngc.collect()","425e5e1b":"# dataset_train = tf.data.Dataset.from_tensor_slices((adj_x_train, adj_y_train))\n# dataset_val = tf.data.Dataset.from_tensor_slices((adj_x_val, adj_y_val))\n\n# dataset_train = dataset_train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n# dataset_val = dataset_val.batch(BATCH_SIZE)","9cf2461e":"# model = tf.keras.Sequential([\n# #     tf.keras.layers.ConvLSTM2D(64, (3,3),\n# #         input_shape=(adj_x_train.shape[1], adj_x_train.shape[2], 1),\n# #         padding=\"same\",\n# #         return_sequences=True,\n# #         activation=\"relu\",\n# #     ),\n# #     tf.keras.layers.ConvLSTM2D(64, (3,3), padding=\"same\", return_sequences=True, activation=\"relu\"),\n# #     tf.keras.layers.GlobalAveragePooling2D(),\n#     tf.keras.layers.LSTM(32, return_sequences=True),\n#     tf.keras.layers.LSTM(32, return_sequences=True),\n#     tf.keras.layers.Dense(1)\n# ])\n# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',\n#                                                     mode='min', patience=5)\n\n# model.compile(loss=tf.losses.MeanSquaredError(),\n#             optimizer=tf.optimizers.Adam(),\n#             metrics=[tf.metrics.RootMeanSquaredError()])","52638987":"# model.fit(dataset_train, epochs=30,\n#     validation_data=dataset_val,\n#     callbacks=[early_stopping],\n#     batch_size=BATCH_SIZE,\n# )","d8ac1214":"def flatten_x(x):\n    flat_row = []\n    for one_row in x:\n        temp_flat_row = []\n        for feature_set in one_row:\n            for one_feature in feature_set:\n                temp_flat_row.append(one_feature)\n        flat_row.append(temp_flat_row)\n        \n    return flat_row","899da3b7":"# adj_x_train = flatten_x(adj_x_train)\n# adj_x_val = flatten_x(adj_x_val)","43243abb":"models = []","522b2e03":"# lgb_train = lgb.Dataset(adj_x_train, adj_y_train, free_raw_data=False)\n# lgb_val = lgb.Dataset(adj_x_val, adj_y_val, reference=lgb_train, free_raw_data=False)\n\n\n# lgbm_params = {\n#     'objective': 'mse',\n#     'metric': 'rmse',\n#     \"num_leaves\": 500,\n#     'is_unbalance':True,\n#     'boosting':'gbdt',\n#     \"learning_rate\": 0.01,\n#     'num_boost_round': 10000,\n#     'early_stopping_rounds':200\n# }\n\n# # # optimized by oputuna\n# # lgbm_params = {\n# #     'objective': 'mse',\n# #     'metric' : 'rmse',\n# #     \"num_leaves\": 966,\n# #     \"cat_smooth\": 45.01680827234465,\n# #     \"min_child_samples\": 27,\n# #     \"min_child_weight\": 0.021144950289224463,\n# #     \"max_bin\": 214,\n# #     \"learning_rate\": 0.01,\n# #     \"subsample_for_bin\": 300000,\n# #     \"min_data_in_bin\": 7,\n# #     \"colsample_bytree\": 0.8,\n# #     \"subsample\": 0.6,\n# #     \"subsample_freq\": 5,\n# #     \"n_estimators\": 3000,\n# # }\n","b700a376":"# model = lgb.train(lgbm_params,\n#                   lgb_train,\n#                   valid_names=['train', 'valid'],\n#                   valid_sets=[lgb_train, lgb_val],\n#                   early_stopping_rounds=20,\n#                   verbose_eval=100)\n# models.append(model)","20b0f92d":"# model = catboost.CatBoostRegressor(\n#     iterations=700,\n#     learning_rate=0.02,\n#     depth=12,\n#     eval_metric='RMSE',\n#     random_seed = 23,\n#     bagging_temperature = 0.2,\n#     od_type='Iter',\n#     metric_period = 75,\n#     od_wait=100\n# )\n\n# model.fit(\n#     adj_x_train,\n#     adj_y_train, \n#     eval_set=(adj_x_val,adj_y_val),\n# )\n# models.append(model)","6ac5b0eb":"# model = XGBRegressor(\n#     max_depth=8,\n#     n_estimators=1000,\n#     min_child_weight=300, \n#     colsample_bytree=0.8, \n#     subsample=0.8, \n#     eta=0.3,    \n#     seed=42,\n# )\n# model.fit(\n#     adj_x_train,\n#     adj_y_train, \n#     eval_metric=\"rmse\", \n#     eval_set=[(adj_x_train, adj_y_train), (adj_x_val, adj_y_val)], \n#     verbose=True, \n#     early_stopping_rounds = 10)\n# models.append(model)","34ab2eef":"sub_x = train_window.generate_sub_x()","ed14174a":"# CNN\ndataset_sub = np.array(sub_x)\ndataset_sub = dataset_sub.reshape(dataset_sub.shape + (1,))\ndataset_sub = tf.data.Dataset.from_tensor_slices(dataset_sub)\ndataset_sub = dataset_sub.batch(BATCH_SIZE)\ndataset_sub","5981a34b":"del features, transition, std_transition, shop_transition, item_transition\ntf.keras.backend.clear_session()\ngc.collect()\n\nBATCH_SIZE = 256","592d5d01":"preds = n_models[0].predict(dataset_sub, batch_size=BATCH_SIZE)\n# preds = np.zeros(len(sub_x))\n# for model in n_models:\n#     preds = preds + model.predict(dataset_sub, batch_size=BATCH_SIZE)","8df51a13":"# # Gradient boosting models\n# sub_x = flatten_x(sub_x)\n\n# preds = np.zeros(len(sub_x))\n# for model in models:\n#     preds = preds + model.predict(sub_x)\n# preds = preds \/ len(models)","81dc8377":"# preds = model.predict(sub_x)\npreds = preds * transition_max\npreds = preds.clip(0.0, 20.0)","ced52707":"print(preds.mean(), preds.std(), preds.max())","7ce00f44":"plt.figure(figsize=(12,6))\nplt.hist(preds,bins=50)","2ac7bdb0":"plt.figure(figsize=(12,6))\nplt.hist(preds[(preds>0.0) & (preds<1.0)],bins=50)","68bc66a5":"# stop_unit = 0.01\n\n# adj_area = preds.mean() * 2\n\n# min_unit = 0.01\n# if (preds.mean() > pre_ave):\n#     min_unit *= -1\n# min_rate = 1 + min_unit\n\n# print(adj_area)\n# print(min_rate)","73e17d0e":"# adj_preds = preds.copy()\n# for i in range(1000):\n#     adj_preds *= min_rate\n# #     adj_preds[adj_preds < adj_area] = (adj_preds[adj_preds < adj_area] - pre_ave) * min_rate + pre_ave\n#     if (adj_preds.mean() - stop_unit < pre_ave) and (adj_preds.mean() + stop_unit > pre_ave):\n#         break\n# print(f'Stop i:{i}, mean:{adj_preds.mean()}')","bc540eab":"# plt.figure(figsize=(12,6))\n# plt.hist(adj_preds,bins=50)","5d149eda":"sample['item_cnt_month'] = preds\nsample.to_csv('submission.csv', index=False)","d7583767":"sample.head(50)","a006b6dd":"plt.hist(sample['item_cnt_month'],bins=50)","ef5bebe3":"Preprocessing each data","6a0daf13":"I want to know submission data's charactaristic.\nThus, I submit some single value\n\n### Single value scores\n* 0.2 : 1.220329 and 1.206208.\n* 0.3 : 1.217545 and 1.203262.\n* 0.4 : 1.222959 and 1.208611.\n* 0.5 : 1.23646\n* 0.6 : 1.245039 and 1.235736."}}