{"cell_type":{"50701e5f":"code","406a0bc8":"code","32e7d0de":"code","52af8cc7":"code","20df4578":"code","c5a254b6":"code","8161dd0d":"code","a2ea67dc":"code","e8960d65":"code","b440e32d":"code","ec219bc4":"code","464a82e1":"code","6b6c416d":"code","b5ed87f5":"code","555e8438":"code","3904e985":"code","5ce60500":"code","8f17d20b":"code","91ae53d2":"code","c61d6b9a":"code","d1b92a99":"code","7cf03458":"code","91a4a719":"code","f9cb100e":"code","6bcca5d6":"code","4ca867d0":"code","09f1a32e":"code","13dbfc1e":"code","7f05cdd3":"code","3879b580":"code","862c8994":"code","0cada4b7":"code","0956f7f9":"code","0b96b57b":"code","30f62812":"code","10683f00":"code","037c1994":"code","eec932a1":"code","f877af14":"code","36262258":"code","f6af5bd2":"code","3d40c883":"code","328d5631":"code","b5cf3723":"code","c149b97b":"code","e4dadc7d":"code","3d4ce921":"code","18305bb6":"code","e55d30ec":"markdown","e8ae324f":"markdown","abdde9fb":"markdown","c9e9983c":"markdown","cd4cbf13":"markdown"},"source":{"50701e5f":"# use mlp for prediction on multi-output regression\nfrom numpy import asarray\nfrom sklearn.datasets import make_regression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() ","406a0bc8":"train=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/train.csv\")\ntrain.head(5)","32e7d0de":"test=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/test.csv\")\ntest.head(5)","52af8cc7":"y=train[[\"Price\"]]\nX=train\nX.drop('Price', axis=1, inplace=True) ","20df4578":"val=train[train[\"Levy\"]==\"-\"][\"Levy\"].index\ntrain[\"Levy\"].iloc[val]=0\ntrain[train[\"Levy\"]==\"-\"][\"Levy\"]\n\nval=test[test[\"Levy\"]==\"-\"][\"Levy\"].index\ntest[\"Levy\"].iloc[val]=0\ntest[test[\"Levy\"]==\"-\"][\"Levy\"]","c5a254b6":"li_col=train.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if train[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \nprint(li_cat)    \nfor i in range(len(li_cat)):\n    train[li_cat[i]]=train[li_cat[i]].astype(str)\n    train[li_cat[i]]=label_encoder.fit_transform(train[li_cat[i]]) \n    \n    \nli_col=test.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if test[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \nprint(li_cat)    \nfor i in range(len(li_cat)):\n    test[li_cat[i]]=test[li_cat[i]].astype(str)\n    test[li_cat[i]]=label_encoder.fit_transform(test[li_cat[i]]) ","8161dd0d":"\n\n# # get the dataset\n# def get_dataset():\n# \tX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=3, random_state=2)\n# \treturn X, y\n\n# get the model\ndef get_model(n_inputs, n_outputs):\n\tmodel = Sequential()\n\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n\tmodel.add(Dense(n_outputs, kernel_initializer='he_uniform'))\n\tmodel.compile(loss='mae', optimizer='adam')\n\treturn model\n\n# print(model)","a2ea67dc":"# load dataset\n# X, y = train\nn_inputs, n_outputs = X.shape[1], y.shape[1]\n# get model\nmodel = get_model(n_inputs, n_outputs)\n# fit the model on all data\nmodel.fit(X, y, verbose=0, epochs=100)\n# make a prediction for new data\n# row = [-0.99859353,2.19284309,-0.42632569,-0.21043258,-1.13655612,-0.55671602,-0.63169045,-0.87625098,-0.99445578,-0.3677487]\n# newX = asarray([row])\nyhat = model.predict(test)\nprint('Predicted: %s' % yhat[0])","e8960d65":"# output=\" \"\noutput=pd.DataFrame(yhat)\noutput.columns =['Price']\noutput","b440e32d":"output[output['Price']<0]","ec219bc4":"# output.to_csv(\"Neural_Network.csv\",index=False)","464a82e1":"import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import seed\nseed(1)\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport tensorflow\ntensorflow.random.set_seed(1)\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() ","6b6c416d":"train=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/train.csv\")\ntrain.head(5)","b5ed87f5":"test=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/test.csv\")\ntest.head(5)","555e8438":"val=train[train[\"Levy\"]==\"-\"][\"Levy\"].index\ntrain[\"Levy\"].iloc[val]=0\ntrain[train[\"Levy\"]==\"-\"][\"Levy\"]\n\nval=test[test[\"Levy\"]==\"-\"][\"Levy\"].index\ntest[\"Levy\"].iloc[val]=0\ntest[test[\"Levy\"]==\"-\"][\"Levy\"]","3904e985":"li_col=train.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if train[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \nprint(li_cat)    \nfor i in range(len(li_cat)):\n    train[li_cat[i]]=train[li_cat[i]].astype(str)\n    train[li_cat[i]]=label_encoder.fit_transform(train[li_cat[i]]) \n    \n    \nli_col=test.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if test[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \nprint(li_cat)    \nfor i in range(len(li_cat)):\n    test[li_cat[i]]=test[li_cat[i]].astype(str)\n    test[li_cat[i]]=label_encoder.fit_transform(test[li_cat[i]]) ","5ce60500":"y=train[[\"Price\"]]\nX=train\nX.drop('Price', axis=1, inplace=True) \nX_train, X_val, y_train, y_val = train_test_split(X, y)","8f17d20b":"y_train=np.reshape(y_train, (-1,1))\ny_val=np.reshape(y_val, (-1,1))\nscaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\nprint(scaler_x.fit(X_train))\nxtrain_scale=scaler_x.transform(X_train)\nprint(scaler_x.fit(X_val))\nxval_scale=scaler_x.transform(X_val)\nprint(scaler_y.fit(y_train))\nytrain_scale=scaler_y.transform(y_train)\nprint(scaler_y.fit(y_val))\nyval_scale=scaler_y.transform(y_val)","91ae53d2":"model = Sequential()\nmodel.add(Dense(8, input_dim=17, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(2670, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.summary()","c61d6b9a":"model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nhistory=model.fit(xtrain_scale, ytrain_scale, epochs=30, batch_size=150, verbose=1, validation_split=0.2)\npredictions = model.predict(xval_scale)","d1b92a99":"print(history.history.keys())\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","7cf03458":"# predictions = scaler_y.inverse_transform(predictions)\n# predictions","91a4a719":"model.predict(test)","f9cb100e":"output=pd.DataFrame(model.predict(test))\noutput.columns =['Price']\noutput","6bcca5d6":"output[output['Price']<0]","4ca867d0":"# output.to_csv(\"Neural_Network1.csv\",index=False)","09f1a32e":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras import models,layers\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() ","13dbfc1e":"dataset=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/train.csv\")\ndataset","7f05cdd3":"test=pd.read_csv(r\"..\/input\/machinehackthe-math-company\/test.csv\")\ntest","3879b580":"li_col=dataset.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if dataset[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \n# print(li_cat)    \nfor i in range(len(li_cat)):\n    dataset[li_cat[i]]=dataset[li_cat[i]].astype(str)\n    dataset[li_cat[i]]=label_encoder.fit_transform(dataset[li_cat[i]]) \n    \n\nli_col=test.columns.to_list()\nli_cat=[]\n# print(li_col)\nfor i in range(len(li_col)):\n    if test[li_col[i]].dtypes=='O':\n        li_cat.append(li_col[i])\n        \n# print(li_cat)    \nfor i in range(len(li_cat)):\n    test[li_cat[i]]=dataset[li_cat[i]].astype(str)\n    test[li_cat[i]]=label_encoder.fit_transform(test[li_cat[i]]) ","862c8994":"val=dataset[dataset[\"Levy\"]==\"-\"][\"Levy\"].index\ndataset[\"Levy\"].iloc[val]=0\ndataset[dataset[\"Levy\"]==\"-\"][\"Levy\"]\n\nval=test[test[\"Levy\"]==\"-\"][\"Levy\"].index\ntest[\"Levy\"].iloc[val]=0\ntest[test[\"Levy\"]==\"-\"][\"Levy\"]\n\ny = dataset\nX = dataset\n# X=X.drop(['Price'], axis = 1, inplace = True) \ndf=dataset\ndf.head(5)","0cada4b7":"df","0956f7f9":"train_data,test_data,train_targets,test_targets = train_test_split(df.drop([\"Price\"],axis=1),df[\"Price\"],test_size=0.3,random_state=42)\n# train_data,test_data,train_targets,test_targets = train_test_split(X,y,test_size=0.3,random_state=42)\n","0b96b57b":"#Shapes\ntrain_data.shape\ntest_data.shape","30f62812":"ss = StandardScaler()\ntransformed_train_data = ss.fit_transform(train_data)\ntransformed_test_data = ss.transform(test_data)","10683f00":"transformed_train_data.shape\ntransformed_test_data.shape","037c1994":"transformed_train_data.mean(axis=0)\n","eec932a1":"transformed_train_data.std(axis=0)\n","f877af14":"np.random.seed(6)\nX = np.random.randint(0,1000,(12,6))\nX","36262258":"k = 4\nnum_val_samples = len(X) \/\/ k\nprint(num_val_samples) #3\nprint(X)\nfor i in range(k):\n    print(\"Fold #\",i)\n    X_val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n    X_train_data = np.concatenate(\n        [X[:i * num_val_samples],\n         X[(i + 1) * num_val_samples:]],\n        axis=0)\n    print(\"Val data with fold:\",i)\n    X_val_data\n    print(\"Train data with fold:\",i)\n    X_train_data","f6af5bd2":"def build_nn():\n    network = models.Sequential()\n    network.add(layers.Dense(64, activation='relu',\n                           input_shape=(transformed_train_data.shape[1],)))\n    network.add(layers.Dense(64, activation='relu'))\n    network.add(layers.Dense(1))\n    network.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # MSE Loss & MAE Loss - Regression task\n    return network","3d40c883":"k = 4\nnum_val_samples = len(transformed_train_data) \/\/ k\nnum_epochs = 100\nall_mae_histories = []\n\nfor i in range(k):\n    print('processing fold #', i)\n    # Prepare the validation data: data from partition # k\n    val_data = transformed_train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n\n    # Prepare the training data: data from all other partitions\n    partial_train_data = np.concatenate(\n        [transformed_train_data[:i * num_val_samples],\n         transformed_train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n\n    # Build the Keras model (already compiled)\n    model = build_nn()\n    # Train the model (in silent mode, verbose=0)\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=1, verbose=0)\n    mae_history = history.history['val_mae']\n    all_mae_histories.append(mae_history)","328d5631":"average_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","b5cf3723":"import matplotlib.pyplot as plt\nplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","c149b97b":"\ndef smooth_curve(points, factor=0.9):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n\nsmooth_mae_history = smooth_curve(average_mae_history[10:])\n\nplt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\nplt.xlabel('Epochs')\nplt.ylabel('Validation MAE')\nplt.show()","e4dadc7d":"\n# Get a fresh, compiled model.\nmodel = build_nn()\n# Train it on the entirety of the data.\nmodel.fit(transformed_train_data,train_targets,\n          epochs=43, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(transformed_test_data, test_targets)","3d4ce921":"# output=\" \"\noutput=pd.DataFrame(model.predict(test))\noutput.columns =['Price']\noutput","18305bb6":"# output.to_csv(\"Neural_Network1.csv\",index=False)","e55d30ec":"# approach 1","e8ae324f":"# approach 2","abdde9fb":"https:\/\/github.com\/manifoldailearning\/Deep-Learning-2020\/blob\/master\/Class_9_%26_10_Deep_learning_2020_Regression.ipynb","c9e9983c":"# approach 3","cd4cbf13":"https:\/\/machinelearningmastery.com\/deep-learning-models-for-multi-output-regression\/"}}