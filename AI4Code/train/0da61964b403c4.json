{"cell_type":{"f2118dfc":"code","cf32aca6":"code","da9ae2b8":"code","9da11d95":"code","63401fd9":"code","b48de0a8":"code","1f36727a":"code","57bc300e":"code","e699b59b":"code","e208ab33":"code","9c2c76a7":"code","8c067272":"code","3b10aab5":"code","c26adbc1":"code","0cf58428":"code","d1a41a64":"code","344110c5":"code","614a0d25":"code","aed41d84":"code","d519b2fb":"code","65f97a81":"code","fcd03761":"code","f18b2086":"code","b6132c54":"markdown","59b3c2ba":"markdown","eb22195f":"markdown","cea284b9":"markdown","c350b02e":"markdown","7ff76dd1":"markdown","3317dcb7":"markdown","3eaba7ee":"markdown","a30d7218":"markdown","95296326":"markdown","6e8da4d2":"markdown","84d67451":"markdown","86abe85a":"markdown","20bd4af5":"markdown","2dd9fe0d":"markdown","4c46775f":"markdown","0e16be33":"markdown","60dfdbf6":"markdown","3baef85f":"markdown","2df7f745":"markdown"},"source":{"f2118dfc":"! pip install pyspark pyhash networkx\n! ls \/kaggle\/input\/\n\nfrom pyspark.sql import SparkSession, functions as F\nimport matplotlib.pyplot as plt\n\nspark = (\n    SparkSession.builder\n    .config(\"spark.driver.memory\", \"12g\")\n    .getOrCreate()\n)\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism*2)\nspark.conf.get(\"spark.driver.memory\")","cf32aca6":"input_path = \"\/kaggle\/input\/cord19-parquet\/cord19.parquet\"\ndf = spark.read.parquet(input_path)\ndf.createOrReplaceTempView(\"cord19\")\ndf.printSchema()\n\ninput_path = \"\/kaggle\/input\/cord19-parquet\/metadata.parquet\"\ndf = spark.read.parquet(input_path)\ndf.createOrReplaceTempView(\"metadata\")\ndf.printSchema()\ndf.show(vertical=True, n=1)","da9ae2b8":"query = \"\"\"\nselect\n    paper_id,\n    metadata.title as paper,\n    value.title as citation\nfrom\n    cord19\nlateral view\n    explode(bib_entries) as key, value\n\"\"\"\nreferences = spark.sql(query)\n\nprint(\"writing references to parquet\")\n# A single file reduces parallelism, but is more efficient for output\n%time references.toPandas().to_parquet(\"references.parquet\")\nreferences = spark.read.parquet(\"references.parquet\")\nreferences.createOrReplaceTempView(\"references\")\nreferences.show(n=5)\nprint(f\"references has {references.count()} rows\")","9da11d95":"query = \"\"\"\nselect\n    citation,\n    count(distinct paper) as num_citations\nfrom\n    references\ngroup by\n    citation\norder by\n    num_citations desc\n\"\"\"\nspark.sql(query).show(n=10, truncate=60)\n\nspark.sql(query).groupBy(\"num_citations\").count().toPandas().plot.scatter(\"num_citations\", \"count\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.title(\"degree distribution of citation network w\/o deduplication\")\nplt.show()","63401fd9":"query = \"\"\"\nwith titles as (\n    select paper as title from references\n    union\n    select citation as title from references\n)\nselect\n    title,\n    sha1(title) as title_sha,\n    count(*) as num_cited\nfrom\n    titles\nwhere\n    length(title) > 0\ngroup by\n    title,\n    title_sha\n\"\"\"\ntitles = spark.sql(query)\n\nprint(\"writing to titles to parquet\")\n%time titles.toPandas().to_parquet(\"titles.parquet\")\n\ntitles = spark.read.parquet(\"titles.parquet\")\nprint(f\"there are {titles.count()} distinct titles\")\ntitles.show(n=5, truncate=60)\n\n(\n    titles\n    .withColumn(\"length\", F.expr(\"log2(length(title))\"))\n    .groupBy(\"length\")\n    .count()\n    .select(\"length\")\n).toPandas().hist(\"length\", density=1)\nplt.title(\"histogram of log_2(length)\")\nplt.show()","b48de0a8":"import pyhash\nimport sys\n\nhasher = pyhash.murmur3_32()\nvalue = \"Control of Communicable Diseases Manual\"\nhashed = hasher(value)\nprint(f\"hashed '{value}' into {sys.getsizeof(hashed)} bits: {hashed}\")","1f36727a":"from pyspark.ml.linalg import Vectors, VectorUDT\nimport pyhash\n\n\n@F.udf(VectorUDT())\ndef hashed_shingles(text, k=9):\n    \"\"\"\"Generates a set of hashed k-shingles as a sparse matrix.\n    \n    Text is lower cased before it is shingled. Punctuation is is\n    assumed to be significant.\n\n    The max input dimension is log2(2147483647), or 30.99. This\n    determines the number of buckets. To calculate the empirical\n    limits of a SparseVector, set `num_buckets` to 2**32-1. Then \n    refer to the resulting exception message.\n    \"\"\"\n    num_buckets = 2**24\n    \n    # A standard library alternative is to use \n    # a check summing routine. This will be slower.\n    # import zlib; hasher = zlib.adler32\n    hasher = pyhash.murmur3_32()\n    \n    shingles = (text[i:i+k].lower() for i in range(len(text)-k+1))\n    hashed_shingles = {(hasher(s) % num_buckets, 1) for s in shingles}\n    \n    return Vectors.sparse(num_buckets, hashed_shingles)","57bc300e":"from pyspark.ml.feature import MinHashLSH\n\nnum_shingles=9\nnum_hash_tables=5\nminhasher = MinHashLSH(\n    inputCol=\"features\", \n    outputCol=\"hashes\", \n    numHashTables=num_hash_tables\n)\n\nprepared_titles = (\n    titles\n    .withColumn(\"length\", F.expr(\"length(title)\"))\n    .where(f\"length(title) >= {num_shingles}\")\n    .withColumn(\"features\", hashed_shingles(\"title\"))\n)\n\nmodel = minhasher.fit(prepared_titles)\nhashed_titles = model.transform(prepared_titles)","e699b59b":"threshold = 0.8\nhashed_titles.cache()\n\nann_titles = (\n    model.approxSimilarityJoin(\n        hashed_titles, \n        hashed_titles, \n        threshold,\n        distCol=\"jaccard_distance\"\n    )\n    .where(\"datasetA.title_sha <> datasetB.title_sha\")\n    .orderBy(\"jaccard_distance\")\n)","e208ab33":"edgelist = (\n    ann_titles\n    .select(\n        F.col(\"datasetA.title_sha\").alias(\"src\"), \n        F.col(\"datasetB.title_sha\").alias(\"dst\"), \n        \"jaccard_distance\"\n    )\n)\nedgelist.cache()\n\n%time print(f\"there are {edgelist.count()} edges\")","9c2c76a7":"import networkx as nx\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ncc = []\nfor i in tqdm(range(0, 85, 10)):\n    graph = nx.from_pandas_edgelist(\n        edgelist.where(f\"jaccard_distance <= {i\/100}\").toPandas(),\n        source=\"src\",\n        target=\"dst\",\n        edge_attr=\"jaccard_distance\",\n    )\n    cc.append(\n        dict(\n            threshold=i\/100,\n            nodes=len(graph.nodes),\n            edges=len(graph.edges),\n            connected_components=nx.number_connected_components(graph),\n            max_connected_component=max(map(len, nx.connected_components(graph))),\n            average_clustering=nx.average_clustering(graph),\n        )\n    )\ndf = pd.DataFrame(cc)\ndf","8c067272":"fig, axes = plt.subplots(2, 2, figsize=(12, 8))\ndf.plot(\"threshold\", \"nodes\", ax=axes[0][0])\ndf.plot(\"threshold\", \"edges\", ax=axes[0][1])\ndf.plot(\"threshold\", \"connected_components\", ax=axes[1][0])\ndf.plot(\"threshold\", \"max_connected_component\", ax=axes[1][1])\nfig.suptitle(\"effect of threshold on network construction\")\nplt.show()","3b10aab5":"from pyspark.sql import Window\nfrom pyspark.sql import types as T\n\n\ndef index_with_dedupe_threshold(\n    threshold, edgelist, title_sha, output_path, n=10\n):\n    output_prefix = f\"{output_path}\/t{round(threshold*100):02d}\"\n    graph = nx.from_pandas_edgelist(\n        edgelist.where(f\"jaccard_distance <= {threshold}\").toPandas(),\n        source=\"src\",\n        target=\"dst\",\n        edge_attr=\"jaccard_distance\",\n    )\n    \n    # index of approximate citation to the canonical citation\n    title_index = spark.sparkContext.parallelize(\n        [(max(nodes), list(nodes)) for nodes in nx.connected_components(graph)]\n    ).toDF(\n        schema=T.StructType(\n            [\n                T.StructField(\"citation_id\", T.StringType()),\n                T.StructField(\"approx_citation_ids\", T.ArrayType(T.StringType())),\n            ]\n        )\n    )\n    citation_index = (\n        title_index.withColumn(\"near_duplicates\", F.size(\"approx_citation_ids\"))\n        .withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"near_duplicates\"))))\n    )\n    citation_index.cache()\n    output = f\"{output_prefix}_citation_index.parquet\"\n    citation_index.toPandas().to_parquet(output)\n\n    # map citation_ids to titles\n    indexed_titles = (\n        citation_index\n        .withColumn(\"approx_citation_id\", F.explode(\"approx_citation_ids\"))\n        .drop(\"approx_citation_ids\")\n        .join(\n            title_sha.selectExpr(\"title as approx_citation\", \"title_sha as approx_citation_id\"),\n            on=\"approx_citation_id\",\n        )\n        .join(\n            title_sha.selectExpr(\n                \"title as citation\", \"title_sha as citation_id\"\n            ),\n            on=\"citation_id\",\n        )\n        .withColumn(\n            \"edit_distance\", F.levenshtein(\"citation\", \"approx_citation\")\n        )\n        .orderBy(\"rank\", F.desc(\"edit_distance\"))\n    )\n    indexed_titles.cache()\n    \n    # write out the titles and their edit distances\n    output = f\"{output_prefix}_approx_citation.csv\"\n    (\n        indexed_titles\n        .withColumn(\"secondary_rank\", F.row_number().over(\n                Window.partitionBy(\"rank\").orderBy(F.desc(\"near_duplicates\"))\n            )\n        )\n        # only keep the top 200 results for each cluster\n        .where(\"secondary_rank < 200\")\n        .where(\"rank <= 50\")\n        .select(\"rank\", \"edit_distance\", \"approx_citation\")\n        .orderBy(\"rank\", F.desc(\"edit_distance\"))\n        .toPandas()\n        .to_csv(output)\n    )\n    \n    # write out all of the titles by rank\n    output = f\"{output_prefix}_ranked_titles.csv\"\n    (\n        indexed_titles\n        .groupBy(\"rank\")\n        .agg(F.count(\"*\").alias(\"near_duplications\"), F.max(\"citation\").alias(\"citation\"))\n        .orderBy(\"rank\")\n        .toPandas()\n        .to_csv(output)\n    )\n    \n    # write out stats for each cluster\n    output = f\"{output_prefix}_edit_distance_stats.csv\"\n    stats = (\n        indexed_titles\n        .withColumn(\"x\", F.col(\"edit_distance\"))\n        .groupBy(\"rank\")\n        .agg(\n            F.count(\"x\").alias(\"count\"),\n            F.mean(\"x\").alias(\"mean\"),\n            F.stddev(\"x\").alias(\"stddev\"),\n            F.min(\"x\").alias(\"min\"),\n            F.expr(\"percentile(x, array(0.5))[0] as p50\"),\n            F.expr(\"percentile(x, array(0.9))[0] as p90\"),\n            F.max(\"x\").alias(\"max\"),\n        )\n        .orderBy(\"rank\")\n    )\n    rounded = stats.select([F.round(c, 2).alias(c) for c in stats.columns])\n    rounded.toPandas().to_csv(output)\n    \n    citation_index.unpersist()\n    indexed_titles.unpersist()","c26adbc1":"# ensure output folder exists\n! mkdir -p corrections\n\nfor i in tqdm(range(0, 85, 10)):\n    index_with_dedupe_threshold(i\/100, edgelist, titles, \"corrections\", 10)","0cf58428":"df = (\n    spark.read.csv(\"corrections\/*_edit_distance_stats.csv\", header=True)\n    .withColumn(\"threshold\",\n        F.regexp_extract(F.input_file_name(), \".*t(\\d+)_edit_distance_stats.csv\", 1)\n        .astype(\"float\") \/ 100\n    )\n)\ndf.show(n=5,truncate=False)\noutput = \"edit_distance_stats.csv\"\nprint(f\"writing out {output}\")\ndf.drop(\"_c0\").toPandas().to_csv(output)\n\n(\n    df.groupBy(\"threshold\")\n    .agg(F.expr(\"mean(cast(p50 as float)) as mean_p50\"))\n    .orderBy(\"threshold\")\n).toPandas().plot(\"threshold\", \"mean_p50\")\nplt.show()","d1a41a64":"for threshold in [0.2, 0.5, 0.8]:\n    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n    fig.suptitle(f\"threshold={threshold}\")\n    (\n        df\n        .where(f\"threshold={threshold}\")\n        .selectExpr(\"log2(cast(p50 as float)) as median_edit_distance\")\n        .toPandas().hist(\"median_edit_distance\", bins=10, ax=axes[0], density=1.0)\n    )\n    (\n        df\n        .where(f\"threshold={threshold}\")\n        .selectExpr(\"cast(count as float) as degree\")\n        .groupBy(\"degree\")\n        .count()\n        .toPandas().plot.scatter(\"degree\", \"count\", ax=axes[1], title=\"degree distribution\")\n    )\n    plt.xscale(\"log\")\n    plt.yscale(\"log\")\n    plt.show()","344110c5":"! cp corrections\/t30_citation_index.parquet citation_index.parquet\ncitation_index = spark.read.parquet(\"citation_index.parquet\")\n\nprint(f\"references:\\t{references}\")\nprint(f\"citation_index:\\t{citation_index}\")","614a0d25":"query = \"\"\"\nwith papers as (\n    select distinct\n        paper_id,\n        paper\n    from\n        references\n)\nselect \n    paper_id = sha1(paper) sha_matches_paper_id,\n    count(*) as num_seen\nfrom\n    papers\ngroup by\n    sha_matches_paper_id\n\"\"\"\nspark.sql(query).show(truncate=False)\n\nquery = \"\"\"\nselect distinct\n    paper_id,\n    sha1(paper)\nfrom\n    references\nlimit 5\n\"\"\"\nspark.sql(query).show(truncate=False)\n","aed41d84":"index = citation_index.select(\"citation_id\", F.explode(\"approx_citation_ids\").alias(\"approx_citation_id\"))\ncitations = (\n    references\n    .withColumn(\"paper_sha\", F.sha1(\"paper\"))\n    .withColumn(\"citation_sha\", F.sha1(\"citation\"))\n    # left join, since the citation index only contains duplicates. If the title is unique, we'll use\n    # the title hash as the id for the node.\n    .join(\n        index.selectExpr(\"citation_id as src_id\", \"approx_citation_id as paper_sha\"),\n        on=\"paper_sha\",\n        how=\"left\",\n    )\n    .withColumn(\"src_citation_id\", F.coalesce(\"src_id\", \"paper_sha\"))\n    .join(\n        index.selectExpr(\"citation_id as dst_id\", \"approx_citation_id as citation_sha\"),\n        on=\"citation_sha\",\n        how=\"left\",\n    )\n    .withColumn(\"dst_citation_id\", F.coalesce(\"dst_id\", \"citation_sha\"))\n    .drop(\"paper_sha\", \"citation_sha\")\n    .select(\"paper_id\", \"src_citation_id\", \"dst_citation_id\", \"paper\", \"citation\")\n    # remove self-edges and duplicate edges\n    .where(\"src_citation_id <> dst_citation_id\")\n    .distinct()\n    .orderBy(\"paper_id\", \"src_citation_id\")\n)\n\nprint(\"writing out citations.parquet\")\n%time citations.toPandas().to_parquet(\"citations.parquet\")\n\ncitations.limit(5).toPandas()","d519b2fb":"# We're done with Spark, lets try to free up memory\nspark.stop()\n\n# kill java to make memory immediately available\n! killall -9 java\n\n# Also free up any dangling objects\nimport gc\ngc.collect() ","65f97a81":"import pandas as pd\nimport networkx as nx\n\ncitations = pd.read_parquet(\"citations.parquet\")\ncitations.head()","fcd03761":"pd.set_option('display.max_colwidth', 120)\n\nG = nx.from_pandas_edgelist(citations, source=\"paper\", target=\"citation\")\nprint(nx.info(G))\n%time pr = nx.pagerank(G)\n(\n    pd.DataFrame(pr.items(), columns=[\"citation_id\", \"pagerank\"])\n    .sort_values(by=\"pagerank\", ascending=False)\n    .head(20)\n)","f18b2086":"pd.set_option('display.max_colwidth', 120)\n\nG = nx.from_pandas_edgelist(citations, source=\"src_citation_id\", target=\"dst_citation_id\")\nprint(nx.info(G))\n%time pr = nx.pagerank(G)\npr_df = pd.DataFrame(pr.items(), columns=[\"citation_id\", \"pagerank\"]).set_index(\"citation_id\")\npr_df.to_csv(\"citation_pagerank.csv\")\n(\n    citations[[\"dst_citation_id\", \"citation\"]]\n    .rename(columns={\"dst_citation_id\": \"citation_id\"})\n    .groupby(\"citation_id\")\n    .first()\n    .join(pr_df, on=\"citation_id\", how=\"inner\")\n    .sort_values(by=\"pagerank\", ascending=False)\n    .reset_index()\n    .head(20)[[\"citation\", \"pagerank\"]]\n)","b6132c54":"The degree distribution follows a power distribution, since it is linear on a log-log scale. Now lets create a list of all of the titles and look at the distribution of title lengths. There are about $10^6$ distinct titles.","59b3c2ba":"The 20-minute runtime is well worth the results. The first observation is the sharp increase in the size of the max connected component that occurs at $t=0.7$. The larger the size of the maximum connected component, the more likely there are going to be corrections to titles with similar shingles. The number of edges also increases precipitously with the size of the maximum connected component. They are related because each connected component is a clique, and therefore contains $\\frac{k(k+1)}{2}$ edges where $k$ is the number of nodes in the clique. We see as the threshold passes $0.75$, the number of components starts to decrease. This is the point at which the graph starts to snowball and become an indistinguishable mass. With a threshold of $1.0$, we would expect to see every possible pair of titles, which would consume $O(n^2)$ space.\n\n## Effect of Jaccard distance threshold on title clustering\n\nNow we'll take a look at the quality of the approximate title matching by manually inspecting the results of the largest cliques and a few random ones. We repeat by varying the threshold from 0.8 down to 0.1\n\n### Generating the citation index and diagnostics","eb22195f":"We can use these references to find the most frequently cited papers. We'll also visualize this as the network's degree distribution. A node's degree is defined as the number of edges containing that node.","cea284b9":"### Statistics about average edit distance within clusters.","c350b02e":"## Discussion\n\nThe quality of the citation network when we apply deduplication is subjectively better than the naive citation network. The results of the naive citation network are not very relevant, whereas the results of the deduplicated network are useful to the task on hand: finding relevant knowledge in the literature about the ongoing COVID-19 pandemic. \n\n0. [In Knobil and Neill's Physiology of Reproduction](https:\/\/www.sciencedirect.com\/book\/9780123971753\/knobil-and-neills-physiology-of-reproduction#book-description)\n1. [Guideline for isolation precautions: preventing transmission of infectious agents in health care settings](https:\/\/www.cdc.gov\/infectioncontrol\/pdf\/guidelines\/isolation-guidelines-H.pdf)\n2. [Biology of natural killer cells](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0065277608606641)\n3. [The Molecular Biology of Coronaviruses](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0065352706660053)\n4. [A novel coronavirus associated with severe acute respiratory syndrome.](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/12690092)\n\nThe ranking of these articles reflects the improved connectivity of the network. \n\n# Next Steps\n\nThere are plenty of interesting directions to go with this now. One idea for a future notebook would be to create a citation embedding using a technique such as [node2vec](http:\/\/snap.stanford.edu\/node2vec\/) or [Graph Convolutional Networks](https:\/\/tkipf.github.io\/graph-convolutional-networks\/). Having an embedding would make it easier to incorporate into a recommendation engine. Yet another idea would be to create an author network using LSH for deduplication. The shingling length would likely need to be modified, and perhaps other information like affliation would need to be taken into consideraion.\n\nThere are also quality issues with the network that have not been addressed, such as the inability to distinguish between volumes of journals or small variations in the title. There are also some titles like _\"Submit your next manuscript to BioMed Central and take full advantage of: \u2022 Convenient online submission \u2022 Thorough ...\"_ that make it into the dataset. However, reducing the citation network above into a co-citation network via an embedding would likely reduce the impact of anomalies like this.\n\nLet me know if this notebook has been helpful to you. I hope this notebook can be of use for addressing the tasks at hand.","7ff76dd1":"# Deduping Quality\n\nNow we take a look at clustering titles using the results of the neighborhood search (`ann_titles`).\n","3317dcb7":"## Hashing k-shingles\n\nA k-shingle is a window of k characters in a piece of text. A set of k-shingles is formed by sliding a window over the length of the entire set. The idea is that a near duplicate title will share a large proportion of these k-shingles. The exact measurement we will be using is the [Jaccard distance](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index), which is the normalized overlap of two sets.\n\n$$\nd_{J}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n$$\n\nLooking at the distribution of title lengths above, we see that the mode appears around titles lengths of $2^9$, or 512 characters. We use a shingle size of 9, which should give us good enough coverage to reduce the number of false positives. We do not have access to labeled data, so we will make a judgement call on what \"good enough\" is.\n\n\nThe result is hashed into a fixed number of buckets where the number of buckets is determined by our hashing function. There are limits to the size of a `SparseVector` in the linalg library unfortunately, so we reduce the hash function to a smaller set by taking the modulo of the output. A note from Spark's `FeatureHasher` (which is not to be confused with this application of hashing) notes that taking a modulo of MurmurHash3 will only be evenly distributed among buckets if the number of buckets is a power of 2. Here, we've chosen $2^{24}$ to create a 3 byte hash function. The extra byte in the 32-bit integer will remain unused. ","3eaba7ee":"## Citation network from deduped titles","a30d7218":"# Network analysis at a glance\n\nThis will be brief, and will just show off how to use the dataset.","95296326":"Lets double check that the `paper_id` is really the `sha1` of the paper title.","6e8da4d2":"## Citation network from titles","84d67451":"# Constructing the citation network\n\nNow we build the actual citation network.","86abe85a":"It turns out that the `sha1(paper)` doesn't match. It doesn't matter since we've generated our own `citation_id` for this purpose that is derived from a 20-byte `sha1` function.\n\n## Indexing papers and citations with `citation_index`\n\nWe create the `cord19_citation` table.","20bd4af5":"# CORD-19 Citation Network with Title Deduping\n\nThe CORD-19 citation network contains papers as nodes and paper citations as edges. We construct this using the bibliography section, which contains metadata related to cited papers. One challenge is the lack of a stable identifier for creating nodes in the network. This notebook outlines a method for constructing the citation network through approximate deduplication of titles.\n\n## Available datasets\n\nThis notebook can be added as a data-source for immediate access to the citation network and its intermediate datasets.\n\ndataset | output type | description \n-|-|-\n`references.parquet` | parquet | the flattened citations from the bibliography\n`titles.parquet` | parquet | list of all titles seen across CORD-19 JSON documents\n`corrections\/t{threshold}_citation_index.parquet` | parquet | the index for mapping `sha1` of titles to their `citation_id`\n`corrections\/t{threshold}_ranked_titles.csv` | csv | A list of titles, cluster size, and rank by cluster size.\n`corrections\/t{threshold}_approx_citation.csv` | csv | The top 50 near-duplicate titles by edit distance for the 20 largest clusters\n`corrections\/t{threshold}_edit_distance_stats.csv` | csv | Intercluster edit-distance statistics\n`edit_distance_stats.csv` | csv | Aggregated intercluster edit-distance statistics from all thresholds\n`citation_index.parquet` | parquet | The index of `title_sha` to `citation_id` used for network construction\n`citations.parquet` | parquet | The CORD-19 citation network with deduplication, threshold $t=0.3$\n`citation_pagerank.csv` | csv | A list of citation_ids to pagerank\n\nHere are some datasets that have been obsoleted:\n\ndataset | output type | description \n-|-|-\n`hashed_titles\/`  | partitioned parquet | titles that have been shingled and minhashed using `pyspark.ml.feature.MinHashLSH`\n`ann_titles\/` | partitioned parquet | a self approximate-similarity join of hashed titles with a Jaccard distance threshold of 0.8\n\n### Changelog\n\n#### 2020-03-23 V24\n* Update title to include deduping\n* removed intermediate spark files to save on space and rely on caching mechanism instead\n* edit distance stats keep top 50 results for the 20 largest clusters. 100 rows is sufficient for quality checks\n* renamed `cord19_citations` to citations\n* include `title_sha` and `num_cited` to titles, for convenience\n\n## Construction\n\nOur goal is to create a `citation_id` by looking at titles in the flattened bibliography. The `sha1` hash of the title alone is not a high quality identifier because titles often contain errors in capitalization or punctuation. We also do not have access to a consistent identifier for all of the citations like a DOI. If we do not correct for these errors, we will miss edges when looking at the number of common neighbors between papers. Proper network construction is important to measures like the [co-citation similarity](https:\/\/en.wikipedia.org\/wiki\/Co-citation).\n\nInspecting the titles in the bibliography reveals the questionable quality of using them as nodes in a graph:\n\n\n```\nControl of communicable disease manual\nControl of communicable diseases manual\nControl of Communicable Disease Manual\n```\n\nSometimes, papers are referenced in preprint where the title changes after publication. There may also be typesetting errors in the bibliography. We'll cluster titles that are similar and assign them to a single identifier. The ideal choice of identifier would be the most frequent title in the cluster or the title in its most cited form, but we'll settle for the `max(sha1(title))` of the cluster.\n\nWe could calculate the edit distance (Levenshtein distance) between all pairs of titles to determine clusters. Sets of similar titles will get assigned a `citation_id`. However, calculating distances across all pairs will prohibitively expensive (n=1,042,000). We'll accomplish this efficiently using a class of [Locality Sensitive Hashing (LSH)](http:\/\/www.mmds.org\/mmds\/v2.1\/ch03-lsh.pdf) techniques for approximate similarity joins. We deduplicate titles using the [pyspark.ml.feature.MinHashLSH module in Spark](https:\/\/spark.apache.org\/docs\/latest\/ml-features#minhash-for-jaccard-distance) and generate a lookup table that can be used against the flattened references.\n\nFinally, we'll actually construct the CORD-19 citation network (`cord19_citations`).\n","2dd9fe0d":"We observe the average of the median edit distances increases as the tolerance increased. We also note that the degree distribution for tolerances follow a power distribution. However, we can also make out obvious outliers. It might be useful to plot a goodness of fit for a power distribution. The line fit to a plot of $t=0.8$ on a log-log scale should have a signifint shift in distribution due to the outlier (p=0.05).\n\nThe distribution of of the median of edit distances shifts to the right as the threshold increases. A divergence measure may be good for determining when it changes.\n\n## Notes on title clustering\n\nReading through the results of `corrections\/t{threshold}_approx_citations.csv` gives a sense of how well the title clustering is performing. Ideally, titles assigned to the same cluster should be referencing the same paper, but with minor typos.\n\nAt $t=0.8$, the largest component has 115k nodes, while the second largest has 242. The second title is strange, which looks to be a somewhat random sequence with variations of the following characters.\n\n```\nGlcNAcb1-4Galb1-4GlcNAcb-Sp8\nGalb1-4GlcNAcb1-6(Fuca1-2Galb1-3GlcNAcb1-3)Galb1-4Glc-Sp21\nGalNAca1-3(Fuca1-2)Galb1-3GalNAcb1-3Gala1-4Galb1-4Glc-Sp21\nMana1-3)Mana1-6(Mana1-3)Manb1-4GlcNAcb1-4GlcNAcb-Sp12\nGalNAcb1-4(Neu5Aca2-8Neu5Aca2-8Neu5Aca2-3)Galb1-4Glcb-Sp0\nFuca1-2Galb1-4GlcNAcb1-2Mana-Sp0\n```\n\nLooking at the 3rd and 4th ranked clusters, the threshold has certainly not been set properly for this to be actionable.\n\n\nAt $t=0.7$, the quality of the largest component at 16.9k nodes is still poor. Looking at the second component with 932 nodes:\n\n```\nRapid and sensitive detection of Taura syndrome virus by reverse ...\nNorth American and European porcine reproductive and respiratory synd...\nPorcine reproductive and respiratory syndrome virus antagonizes JAK\/ST...\nPorcine reproductive-respiratory syndrome virus infection predisposes pigs...\n```\n\nAt $t=0.6$, the difference between the size of rank 1 and rank 2 component sizes becomes much smaller. However, the tolerance is too high, causing mismatches between titles. From the largest component:\n\n```\nStaphylococcus aureus\nMeticillin-resistant Staphylococcus aureus (MRSA): screening and decolonisation\nMethicillin resistant Staphylococcus aureus colonization in pigs and pig farmers\n```\n\n\n$t=0.3$ is the first where the titles look consistent within their components. A shortcoming of this technique becomes apparent scrolling through the result. Changes in the year or the volume in the references become indistinguishable.\n\n```\n\"The European Union Summary Report on Trends and Sources of Zoonoses, Zoonotic Agents and Food-borne Outbreaks in 2010\"\n\"The European Union Summary Report on Trends and Sources of Zoonoses, Zoonotic Agents and Food-borne Outbreaks in 2013\"\n\"The European Union Summary Report on Trends and Sources of Zoonoses, Zoonotic Agents and Food-borne Outbreaks in 2012\"\n\"The European Union summary report on trends and sources of zoonoses, zoonotic agents and food-borne outbreaks in (2014)\"\n\"The European Union summary report on trends and sources of zoonoses, zoonotic agents and food-borne outbreaks in (2013)\"\n```","4c46775f":"# Deduping Citations with MinHash\n\nWe use MinHash to cluster approximately-duplicate titles of citations. These clusters are assigned a `citation_id`, which become the nodes of the citation network.\n\nRefer to [Chapter 3 of Mining Massive Datasets](http:\/\/infolab.stanford.edu\/~ullman\/mmds\/ch3n.pdf) for an overview of the techniques used here.\n\n\n## A note on hashing\n\n[MurmurHash3](https:\/\/en.wikipedia.org\/wiki\/MurmurHash) is a fast, non-cryptographic hash. We want to make sure the hashing algorithm is fast since we are digesting quite a bit of text. Here, we use a 4 byte hash to help reduce the size of our characteristic matrix. Since there are a lot of empty entries, we'll use a sparse representation which has natural applications in map-reduce settings.","0e16be33":"## MinHashing citation titles\n\nRunning the Minhashing algorithm is simple, and can be incorporated into a model pipeline. We have to ensure that documents are at least the size of the shingle, otherwise the resulting vector will be empty.","60dfdbf6":"## Approximate nearest-neighbor search of titles\n\nHere, we perform a self-join on the hashed titles with a threshold $t=0.8$. We do this to analyze the effect of the threshold on the quality of the clusters. In practice, we should set this to a reasonable value, say $t=0.3$. Increasing the threshold will increase the runtime of this approximate join.","3baef85f":"We're going to reuse some of the results from [Parquet and BigQuery dataset for CORD-19](https:\/\/www.kaggle.com\/acmiyaguchi\/parquet-and-bigquery-dataset-for-cord-19), which can be found by searching for [`CORD-19 parquet`](https:\/\/www.kaggle.com\/acmiyaguchi\/cord19-parquet) under `+ Add data`. A refresher of PySpark and SQL can be found in [PySpark DataFrame Preprocessing for CORD-19](https:\/\/www.kaggle.com\/acmiyaguchi\/pyspark-dataframe-preprocessing-for-cord-19).","2df7f745":"# Exploring the Flattened Bibliography\n\nFirst thing we need to do is to create a flattened table of all the citations in the bibliography. We register this as our `references` table."}}