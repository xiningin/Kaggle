{"cell_type":{"b7419700":"code","e6ddf779":"code","e223fb74":"code","3699202d":"code","d006e380":"code","2d2b3a3b":"code","012e6d0f":"code","c0f197de":"code","ed0f93d2":"code","b6323731":"code","1cc8bb72":"code","45b94760":"code","29b198ab":"code","4a58ceb6":"code","9078aa00":"code","b6bef087":"code","0ae0d591":"code","02bd30a8":"code","c01ad310":"code","65e20b3b":"code","6f15a4bd":"code","969901c6":"code","c5c5249d":"code","1312e4b2":"code","577ed850":"code","bf2e0f02":"code","d90e0ea5":"code","d5af367e":"code","4a8b5397":"code","5e37e157":"code","8d9ccd14":"code","e33e54e4":"code","314564a3":"code","640f8e0f":"code","e526db09":"code","34afae65":"code","0ecd3a0e":"code","d0f170dc":"code","1e0b3f09":"code","d7c4c393":"code","4c2a21d8":"code","3228429f":"code","d4cda0e1":"code","5c63977e":"code","1f40141d":"code","0d6bc1a5":"code","05b4fe80":"code","bef24006":"code","cb81158c":"code","caee0168":"code","b4520d7e":"code","1182aaad":"code","639ab79a":"code","a84ea83b":"code","2e724c8c":"code","8a83f8dc":"code","a1f05615":"code","b051ccef":"code","34d5facc":"code","20966b8f":"code","f916cd96":"code","10986c2c":"code","a383a635":"code","fa48ef4c":"code","73d13308":"code","55dcf4bb":"code","56fc7701":"code","99ef13fe":"code","6d0cb1f4":"code","ab8d2cf1":"code","fc2a12a2":"code","5910072e":"code","8d228ee9":"code","3cd3a758":"code","18852a61":"code","6a9f31a0":"code","fbe0915b":"code","6687ea3a":"code","f7701da1":"code","ea98d7aa":"code","c75b73e5":"code","e4edc219":"code","fe4e9c31":"code","a16f0610":"code","6b060e8b":"code","cc819e23":"code","c0f44cec":"code","c486d6ae":"code","715f2103":"code","f9ff644a":"code","f26f8a16":"code","3cc7082c":"code","01fb6cae":"code","41f57b19":"code","83b53b4c":"code","98a5ca56":"code","2d6de072":"code","a43eafd0":"code","2514a4f6":"code","2065181c":"code","e9cabe12":"code","f0823e4f":"code","8dad71c9":"code","aa35777e":"code","f7e4ed81":"code","b3404db9":"code","84d1fb42":"code","050dde76":"code","4fe17064":"code","f6a9f55d":"code","5bc0ac59":"code","bce3a8c9":"code","41074549":"code","2e5e8260":"code","22225f5f":"code","d937058d":"code","cbb64e67":"code","5e5be843":"code","65010cac":"code","9a4b0060":"code","05c144fb":"code","80070367":"code","ca05c064":"code","6cb21747":"code","4dddb29e":"code","3d505865":"code","54d09e4c":"markdown","f022dde2":"markdown","0d9a6660":"markdown","7f3567d5":"markdown","a2bbc953":"markdown","b285a5f7":"markdown","a90cdf65":"markdown","8141f008":"markdown","50054847":"markdown","dbbfc56d":"markdown","b5d19859":"markdown","f0e0877d":"markdown","90e37bb9":"markdown","36199f46":"markdown","130fbb1a":"markdown","495500f2":"markdown","682014f8":"markdown","fa2d51f4":"markdown","a7271e85":"markdown","e2257963":"markdown","a16a59a5":"markdown","a7245611":"markdown","bc374eef":"markdown","e0e14339":"markdown","9614bb20":"markdown","b6294d4d":"markdown","dcba56e4":"markdown","d9d29019":"markdown","8c964468":"markdown","5519e5b9":"markdown","51e2ce3f":"markdown","7b4f5c60":"markdown","d4895049":"markdown","d395ad16":"markdown","aa3d0db7":"markdown","674d9cdd":"markdown","baf80501":"markdown","80cbb0ac":"markdown","37bc3a9a":"markdown","5ac225b4":"markdown","2ea4c43e":"markdown","39e1adc7":"markdown","ac5b9985":"markdown","bed4cc7a":"markdown","c99411e5":"markdown","ab7f9659":"markdown","7122616a":"markdown","a727ba92":"markdown","0b4a8174":"markdown","47364517":"markdown","3e108b41":"markdown","23f9308e":"markdown","2b811d7a":"markdown","3b2deebf":"markdown","e40186ed":"markdown","fbcafe7b":"markdown","5592c06a":"markdown","ecdaeebe":"markdown","4c414462":"markdown","c37e4249":"markdown","6bcaa0a8":"markdown","e91be2de":"markdown","2fdc14dc":"markdown","0239cbba":"markdown","e63d2a52":"markdown","13e657b2":"markdown","88e6de5b":"markdown","9d954dbd":"markdown","0512574f":"markdown","a08a0ee2":"markdown","a963cf32":"markdown","34655497":"markdown","7cd32d36":"markdown","58cd95b2":"markdown","ec165e2a":"markdown","3a7eb8a5":"markdown","99067103":"markdown","7e42acf2":"markdown","78d7668c":"markdown","25281f8a":"markdown","eefccf87":"markdown","9a213c10":"markdown","f89b439f":"markdown","ba9ca97d":"markdown","4a111c08":"markdown","6b09e7a6":"markdown","78f6c89c":"markdown","4a95705c":"markdown","26d89a2c":"markdown","bce754b7":"markdown","6d58f99e":"markdown","c542f2b0":"markdown","94d47361":"markdown","f76eb451":"markdown","a03db760":"markdown","64a1f4f8":"markdown","e0e0771d":"markdown","6ee9f1b1":"markdown"},"source":{"b7419700":"import gc # garbage collector\nimport numpy as np # linear algebra\nfrom collections import Counter # for counting commong words\nimport pandas as pd # data processing, JSON file I\/O (e.g. pd.read_json)\nimport matplotlib.pyplot as plt # visualization\nplt.style.use('fivethirtyeight') # use ggplot ploting style\nimport json\nimport seaborn as sns # visualization \nfrom wordcloud import WordCloud, STOPWORDS # this module is for making wordcloud in python\nimport os\nimport re # regular expression\nimport string # for finding punctuation in text\nimport nltk # preprocessing text\nfrom textblob import TextBlob\n# import ploty for visualization\nimport plotly\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import *\nimport plotly.tools as tls\nimport plotly.figure_factory as fig_fact\nplotly.tools.set_config_file(world_readable=True, sharing='public')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","e6ddf779":"df_businesses = pd.read_json('..\/input\/yelp_academic_dataset_business.json', lines=True)","e223fb74":"df_businesses.head()","3699202d":"print('In total there are ', df_businesses.isnull().sum().sum(), ' missing values')\nprint('Missing values on each column:')\ndf_businesses.apply(lambda x: sum(x.isnull()),axis=0)","d006e380":"print(\"1 for Opened Businesses - 0 for Closed Businesses\")\ndf_businesses['is_open'].value_counts()","2d2b3a3b":"print('Let\\'s analyze the ratings of businesses')\ndf_businesses.stars.describe()","012e6d0f":"plt.hist(df_businesses.stars, bins=np.linspace(1,5,10))\nplt.xlabel('Stars')\nplt.ylabel('# of Businesses')\nplt.title('Businesses by Stars')","c0f197de":"print('All businesses: ',df_businesses.shape)\ndf_businesses.fillna('NA', inplace=True)\ndf_businesses = df_businesses[df_businesses['categories'].str.contains('Restaurants')]\nprint('After we have filtered only Restaurants: ',df_businesses.shape)","ed0f93d2":"df_businesses[df_businesses['attributes'].str.contains('NA') | df_businesses['categories'].str.contains('NA') | df_businesses['hours'].str.contains('NA')]","b6323731":"df_businesses","1cc8bb72":"print(\"1 for Opened Restaurants - 0 for Closed Restaurants\")\ndf_businesses['is_open'].value_counts()","45b94760":"print('Let\\'s analyze the ratings of Restaurants')\ndf_businesses.stars.describe()","29b198ab":"print('There are a total of {} states where there are Restaurants'.format(len(df_businesses['state'].unique())))\ndf_businesses['state'].unique()","4a58ceb6":"df_businesses['state'].value_counts()","9078aa00":"df_states = df_businesses.groupby('state').count()\ndf_top_states = df_states['name']\ndf_top_states_sorted = df_top_states.sort_values(ascending = False)\ndf_top_states_sorted[:20].plot(kind = 'bar')","b6bef087":"print('There are a total of {} cities where there are Restaurants'.format(len(df_businesses['city'].unique())))\ndf_businesses['city'].unique()","0ae0d591":"df_businesses['city'].value_counts()","02bd30a8":"df_cities = df_businesses.groupby('city').count()\ndf_top_cities = df_cities['name']\ndf_top_cities_sorted = df_top_cities.sort_values(ascending = False)\ndf_top_cities_sorted[:20].plot(kind = 'bar')","c01ad310":"plt.hist(df_businesses.review_count, bins=range(0,200,5))\nplt.xlabel('Review Count')\nplt.ylabel('# of Restaurants')\nplt.title('Restaurants by Review Count')","65e20b3b":"plt.hist(df_businesses.stars, bins=np.linspace(1,5,10))\nplt.xlabel('Stars')\nplt.ylabel('# of Restaurants')\nplt.title('Restaurants by Stars')","6f15a4bd":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","969901c6":"del df_cities,df_top_cities,df_top_cities_sorted,df_states,df_top_states,df_top_states_sorted\ngc.collect()","c5c5249d":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","1312e4b2":"df_reviews_dir = pd.read_json('..\/input\/yelp_academic_dataset_review.json', chunksize=100000, lines=True)\ndf_tips = pd.read_json('..\/input\/yelp_academic_dataset_tip.json', lines=True)","577ed850":"df_reviews = pd.DataFrame()\ni=0\nfor df in df_reviews_dir:\n    df = df[df['business_id'].isin(df_businesses['business_id'])]\n    df_reviews = pd.concat([df_reviews, df])\n    i=i+1\n    print(i)\n    if i==10: break","bf2e0f02":"print('Reviews: ',df_reviews.shape)\nprint('Tips: ',df_tips.shape)","d90e0ea5":"df_reviews[\"stars\"].value_counts()","d5af367e":"plt.hist(df_reviews.stars, bins=np.linspace(1,5,10))\nplt.xlabel('Stars')\nplt.ylabel('# of Reviews')\nplt.title('Reviews by Stars')","4a8b5397":"df_reviews.describe()","5e37e157":"print('Missing values of Reviews dataset ', df_reviews.isnull().sum().sum())\nprint('Missing values of Tips dataset ', df_tips.isnull().sum().sum())","8d9ccd14":"df_businesses = df_businesses[df_businesses['business_id'].isin(df_reviews['business_id'])]","e33e54e4":"print('Final businesses shape: ', df_businesses.shape)\nprint('Final review shape: ', df_reviews.shape)","314564a3":"df_reviews['name'] = df_reviews['business_id'].map(df_businesses.set_index('business_id')['name'])","640f8e0f":"def preprocess(text):\n    text = re.sub('[^a-z\\s]', '', text.lower())                  # get rid of noise\n    text = [word for word in text.split() if word not in set(stopwords)]  # remove stopwords\n    return ' '.join(text) # then join the text again\n# let's find out which stopwords need to remove. We'll use english stopwords.\ni = nltk.corpus.stopwords.words('english')\n# punctuations to remove\nj = list(string.punctuation)\n# finally let's combine all of these\nstopwords = set(i).union(j)\n\ndf_reviews['cleared_text'] = df_reviews['text'].apply(preprocess)","e526db09":"df_reviews","34afae65":"top_restaurants = df_reviews.name.value_counts().index[:20].tolist()\ndf_top_reviews = df_reviews.loc[df_reviews['name'].isin(top_restaurants)]\ndf_top_reviews.groupby(df_top_reviews.name)['stars'].mean().sort_values(ascending=True).plot(kind='barh',figsize=(12, 10))\nplt.yticks(fontsize=18)\nplt.title('Top rated restaurants on Yelp',fontsize=20)\nplt.ylabel('Restaurants names', fontsize=18)\nplt.xlabel('Ratings', fontsize=18)\nplt.show()","0ecd3a0e":"specific_restaurant_reviews = 'QXAEGFB4oINsVuTFxEYKFQ'\n\ndf_specific_restaurant_reviews = df_reviews[df_reviews['business_id'] == specific_restaurant_reviews]","d0f170dc":"df_specific_restaurant_reviews","1e0b3f09":"def sentiment(text):\n    sentiment = TextBlob(text)\n    return sentiment.sentiment.polarity\n\ndf_top_reviews['sentiment_polarity'] = df_top_reviews['cleared_text'].apply(sentiment)","d7c4c393":"df_top_reviews","4c2a21d8":"df_top_reviews.groupby(df_top_reviews.name)['sentiment_polarity'].mean().sort_values(ascending=True).plot(kind='barh',figsize=(12, 10))\nplt.yticks(fontsize=18)\nplt.title('Top customers satisfied restaurants on Yelp',fontsize=20)\nplt.ylabel('Restaurants names', fontsize=18)\nplt.xlabel('Reviews polarity', fontsize=18)\nplt.show()","3228429f":"df_top_reviews.groupby(df_top_reviews.name)[['useful','funny', 'cool']].mean().sort_values('useful',ascending=True).plot(kind='barh', figsize=(15, 14),width=0.7)\nplt.yticks(fontsize=18)\nplt.title('Top useful, funny and cool restaurants',fontsize=28)\nplt.ylabel('Restaurants names', fontsize=18)\nplt.legend(fontsize=22)\nplt.show()","d4cda0e1":"df_tips['name'] = df_tips['business_id'].map(df_businesses.set_index('business_id')['name'])\ndf_top_tips = df_tips.loc[df_tips['name'].isin(top_restaurants)]\ndf_top_tips['cleared_text'] = df_top_tips['text'].apply(preprocess)","5c63977e":"df_top_tips","1f40141d":"wc = WordCloud(width=1600, height=800, random_state=1, max_words=200000000)\nwc.generate(str(df_top_tips['cleared_text']))\nplt.figure(figsize=(20,10), facecolor='k')\nplt.title(\"Tips for top reviewed restaurant\", fontsize=40,color='white')\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=10)\nplt.show()","0d6bc1a5":"df_little_miss_bbq_only = df_businesses.loc[df_businesses['name'] == \"Little Miss BBQ\"]\ndf_little_miss_bbq_review = df_top_reviews.loc[df_top_reviews['business_id'].isin(df_little_miss_bbq_only.business_id)]","05b4fe80":"wc = WordCloud(width=1600, height=800, random_state=1, max_words=200000000)\nwc.generate(str(df_little_miss_bbq_review['cleared_text']))\nplt.figure(figsize=(20,10), facecolor='k')\nplt.title(\"Customers reviews about 'Little Miss BBQ'\", fontsize=40,color='white')\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=10)\nplt.show()","bef24006":"# convert date column to pandas datatime \ndf_little_miss_bbq_review.date = pd.to_datetime(df_little_miss_bbq_review.date)\ndf_little_miss_bbq_review.groupby(df_little_miss_bbq_review.date.dt.year)['sentiment_polarity'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"Customer satisfaction of 'Little Miss BBQ' in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Polarity of reviews(satisfaction)', fontsize=18)\nplt.show()","cb81158c":"df_little_miss_bbq_review.date = pd.to_datetime(df_little_miss_bbq_review.date)\ndf_little_miss_bbq_review.groupby(df_little_miss_bbq_review.date.dt.year)['stars'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'Little Miss BBQ' ratings in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Stars', fontsize=18)\nplt.show()","caee0168":"df_little_miss_bbq_review.date = pd.to_datetime(df_little_miss_bbq_review.date)\ndf_little_miss_bbq_review.groupby(df_little_miss_bbq_review.date.dt.year)[['useful','funny','cool']].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'Little Miss BBQ' usefulness, funniness and coolness in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.show()","b4520d7e":"del df_little_miss_bbq_only,df_little_miss_bbq_review\ngc.collect()","1182aaad":"df_the_venetian_las_vegas_only = df_businesses.loc[df_businesses['name'] == \"The Venetian Las Vegas\"]\ndf_the_venetian_las_vegas_review = df_top_reviews.loc[df_top_reviews['business_id'].isin(df_the_venetian_las_vegas_only.business_id)]","639ab79a":"wc = WordCloud(width=1600, height=800, random_state=1, max_words=200000000)\nwc.generate(str(df_the_venetian_las_vegas_review['cleared_text']))\nplt.figure(figsize=(20,10), facecolor='k')\nplt.title(\"Customers reviews about 'The Venetian Las Vegas'\", fontsize=40,color='white')\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=10)\nplt.show()","a84ea83b":"# convert date column to pandas datatime \ndf_the_venetian_las_vegas_review.date = pd.to_datetime(df_the_venetian_las_vegas_review.date)\ndf_the_venetian_las_vegas_review.groupby(df_the_venetian_las_vegas_review.date.dt.year)['sentiment_polarity'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"Customer satisfaction of 'The Venetian Las Vegas' in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Polarity of reviews(satisfaction)', fontsize=18)\nplt.show()","2e724c8c":"df_the_venetian_las_vegas_review.date = pd.to_datetime(df_the_venetian_las_vegas_review.date)\ndf_the_venetian_las_vegas_review.groupby(df_the_venetian_las_vegas_review.date.dt.year)['stars'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'The Venetian Las Vegas' ratings in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Stars', fontsize=18)\nplt.show()","8a83f8dc":"df_the_venetian_las_vegas_review.date = pd.to_datetime(df_the_venetian_las_vegas_review.date)\ndf_the_venetian_las_vegas_review.groupby(df_the_venetian_las_vegas_review.date.dt.year)[['useful','funny','cool']].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'The Venetian Las Vegas' usefulness, funniness and coolness in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.show()","a1f05615":"del df_the_venetian_las_vegas_only,df_the_venetian_las_vegas_review\ngc.collect()","b051ccef":"df_juans_flaming_fajitas_cantina_only = df_businesses.loc[df_businesses['name'] == \"Juan's Flaming Fajitas & Cantina\"]\ndf_juans_flaming_fajitas_cantina_review = df_top_reviews.loc[df_top_reviews['business_id'].isin(df_juans_flaming_fajitas_cantina_only.business_id)]","34d5facc":"wc = WordCloud(width=1600, height=800, random_state=1, max_words=200000000)\nwc.generate(str(df_juans_flaming_fajitas_cantina_review['cleared_text']))\nplt.figure(figsize=(20,10), facecolor='k')\nplt.title(\"Customers reviews about 'Juan's Flaming Fajitas & Cantina'\", fontsize=40,color='white')\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=10)\nplt.show()","20966b8f":"# convert date column to pandas datatime \ndf_juans_flaming_fajitas_cantina_review.date = pd.to_datetime(df_juans_flaming_fajitas_cantina_review.date)\ndf_juans_flaming_fajitas_cantina_review.groupby(df_juans_flaming_fajitas_cantina_review.date.dt.year)['sentiment_polarity'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"Customer satisfaction of 'Juan's Flaming Fajitas & Cantina' in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Polarity of reviews(satisfaction)', fontsize=18)\nplt.show()","f916cd96":"df_juans_flaming_fajitas_cantina_review.date = pd.to_datetime(df_juans_flaming_fajitas_cantina_review.date)\ndf_juans_flaming_fajitas_cantina_review.groupby(df_juans_flaming_fajitas_cantina_review.date.dt.year)['stars'].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'Juan's Flaming Fajitas & Cantina' ratings in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Stars', fontsize=18)\nplt.show()","10986c2c":"df_juans_flaming_fajitas_cantina_review.date = pd.to_datetime(df_juans_flaming_fajitas_cantina_review.date)\ndf_juans_flaming_fajitas_cantina_review.groupby(df_juans_flaming_fajitas_cantina_review.date.dt.year)[['useful','funny','cool']].mean().plot(kind='bar', figsize=(12, 7))\nplt.title(\"'Juan's Flaming Fajitas & Cantina' usefulness, funniness and coolness in different years\", fontsize=20)\nplt.xlabel('Year', fontsize=18)\nplt.show()","a383a635":"del df_juans_flaming_fajitas_cantina_only,df_juans_flaming_fajitas_cantina_review,df_top_reviews,df_tips,df_top_tips\ngc.collect()","fa48ef4c":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","73d13308":"del df,df_reviews_dir,df_specific_restaurant_reviews,top_restaurants, specific_restaurant_reviews\ngc.collect()","55dcf4bb":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","56fc7701":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport itertools","99ef13fe":"vectorizer = TfidfVectorizer(ngram_range=(1,3))\n\nvectors = vectorizer.fit_transform(df_reviews['cleared_text'])","6d0cb1f4":"X_train, X_test, y_train, y_test = train_test_split(vectors, df_reviews['stars'], test_size=0.2, random_state=42, shuffle=False)","ab8d2cf1":"from sklearn.svm import LinearSVC\n\nLSVMclassifier = LinearSVC()\n\nLSVMclassifier.fit(X_train, y_train)","fc2a12a2":"LSVMpredictions = LSVMclassifier.predict(X_test)\n\nprint(\"Actual Ratings: \")\nprint(y_test[:10])\nprint(\"Predicted Ratings: \",end = \"\")\nprint(LSVMpredictions[:10])","5910072e":"print('Accuracy score: ', accuracy_score(y_test, LSVMpredictions))","8d228ee9":"print ('Precision: ' + str(precision_score(y_test, LSVMpredictions, average='weighted')))\nprint ('Recall: ' + str(recall_score(y_test, LSVMpredictions, average='weighted')))","3cd3a758":"print(classification_report(y_test, LSVMpredictions))","18852a61":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Greens):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","6a9f31a0":"from sklearn import metrics\nnames = ['1','2','3','4','5']\n\nconfusion_matrix = metrics.confusion_matrix(y_test, LSVMpredictions)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","fbe0915b":"X_null_train, X_full_test, y_null_train, y_full_test = train_test_split(vectors, df_reviews['stars'], test_size=0.999995, random_state=42, shuffle=False)\n\nLSVMfullpredictions = LSVMclassifier.predict(X_full_test)","6687ea3a":"print(\"Actual Ratings: \")\nprint(y_full_test[-10:])\nprint(\"\\nPredicted Ratings: \",end = \"\")\nprint(LSVMfullpredictions[-10:])","f7701da1":"df_LSVM_reviews = df_reviews.copy()\n\ndf_LSVM_reviews.drop(df_LSVM_reviews.head(3).index, inplace=True)\n\ndf_LSVM_reviews['stars'] = LSVMfullpredictions\n\ndf_LSVM_reviews","ea98d7aa":"sentiments = []\nfor star in df_reviews['stars']:\n    if star <= 3:\n        sentiments.append('n')\n    if star > 3:\n        sentiments.append('p')","c75b73e5":"X2_train, X2_test, y2_train, y2_test = train_test_split(vectors, sentiments, test_size=0.20, random_state=42)","e4edc219":"LSVMclassifier2 = LinearSVC()\n\nLSVMclassifier2.fit(X2_train, y2_train)","fe4e9c31":"LSVMpredictions2 = LSVMclassifier2.predict(X2_test)\n\nprint(\"Actual Rating Category: \")\nprint(y2_test[:10])\nprint(\"\\nPredicted Rating Category: \",end = \"\")\nprint(list(LSVMpredictions2[:10]))","a16f0610":"print('Accuracy score: ', accuracy_score(y2_test, LSVMpredictions2))","6b060e8b":"print ('Precision: ' + str(precision_score(y2_test, LSVMpredictions2, average='weighted')))\nprint ('Recall: ' + str(recall_score(y2_test, LSVMpredictions2, average='weighted')))","cc819e23":"print(classification_report(y2_test, LSVMpredictions2))","c0f44cec":"from sklearn import metrics\nnames = ['Negative','Positive']\n\nconfusion_matrix = metrics.confusion_matrix(y2_test, LSVMpredictions2)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","c486d6ae":"x = df_reviews['text'].copy()\ny = df_reviews['stars'].copy()","715f2103":"def text_preprocessing(text):\n    no_punctuation = [ch for ch in text if ch not in string.punctuation]\n    no_punctuation = ''.join(no_punctuation)\n    return [w for w in no_punctuation.split() if w.lower() not in stopwords]","f9ff644a":"from sklearn.feature_extraction.text import CountVectorizer\n\nvector = CountVectorizer(analyzer=text_preprocessing).fit(x)\n\nx = vector.transform(x)","f26f8a16":"X3_train, X3_test, y3_train, y3_test = train_test_split(x, y, test_size=0.20, random_state=0, shuffle=False)","3cc7082c":"from sklearn.naive_bayes import MultinomialNB\n\nNBclassifier = MultinomialNB()\n\nNBclassifier.fit(X3_train, y3_train)","01fb6cae":"NBpredictions = NBclassifier.predict(X3_test)\n\nprint(\"Actual Ratings: \")\nprint(y3_test[:10])\nprint(\"Predicted Ratings: \",end = \"\")\nprint(NBpredictions[:10])","41f57b19":"print('Accuracy score: ', accuracy_score(y3_test, NBpredictions))","83b53b4c":"print ('Precision: ' + str(precision_score(y3_test, NBpredictions, average='weighted')))\nprint ('Recall: ' + str(recall_score(y3_test, NBpredictions, average='weighted')))","98a5ca56":"print(classification_report(y3_test, NBpredictions))","2d6de072":"from sklearn import metrics\nnames = ['1','2','3','4','5']\n\nconfusion_matrix = metrics.confusion_matrix(y3_test, NBpredictions)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","a43eafd0":"X3_null_train, X3_full_test, y3_null_train, y3_full_test = train_test_split(x, y, test_size=0.999995, random_state=0, shuffle=False)\n\nNBfullpredictions = NBclassifier.predict(X3_full_test)","2514a4f6":"print(\"Actual Ratings: \")\nprint(y3_full_test[-10:])\nprint(\"\\nPredicted Ratings: \",end = \"\")\nprint(NBfullpredictions[-10:])","2065181c":"df_NB_reviews = df_reviews.copy()\n\ndf_NB_reviews.drop(df_NB_reviews.head(3).index, inplace=True)\n\ndf_NB_reviews['stars'] = NBfullpredictions\n\ndf_NB_reviews","e9cabe12":"df_copied_reviews = df_reviews.copy()\n\ndf_copied_reviews['stars'][df_copied_reviews.stars == 3] = 1\ndf_copied_reviews['stars'][df_copied_reviews.stars == 2] = 1\ndf_copied_reviews['stars'][df_copied_reviews.stars == 4] = 5","f0823e4f":"rating1 = df_copied_reviews[df_copied_reviews['stars'] == 1]","8dad71c9":"len(rating1)","aa35777e":"rating5 = df_copied_reviews[df_copied_reviews['stars'] == 5][0:216737]","f7e4ed81":"frames = [rating1, rating5]\ndf_copied_reviews = pd.concat(frames)","b3404db9":"x2 = df_copied_reviews['text']\ny2 = df_copied_reviews['stars']","84d1fb42":"from sklearn.feature_extraction.text import CountVectorizer\n\nvector2 = CountVectorizer(analyzer=text_preprocessing).fit(x2)\n\nx2 = vector.transform(x2)","050dde76":"X4_train, X4_test, y4_train, y4_test = train_test_split(x2, y2, test_size=0.20, random_state=0)","4fe17064":"NBclassifier2 = MultinomialNB()\n\nNBclassifier2.fit(X4_train, y4_train)","f6a9f55d":"NBpredictions2 = NBclassifier2.predict(X4_test)\n\nprint(\"Actual Rating Category: \")\nprint(y4_test[:10])\nprint(\"\\nPredicted Rating Category: \",end = \"\")\nprint(list(NBpredictions2[:10]))","5bc0ac59":"print('Accuracy score: ', accuracy_score(y4_test, NBpredictions2))","bce3a8c9":"print ('Precision: ' + str(precision_score(y4_test, NBpredictions2, average='weighted')))\nprint ('Recall: ' + str(recall_score(y4_test, NBpredictions2, average='weighted')))","41074549":"print(classification_report(y4_test, NBpredictions2))","2e5e8260":"from sklearn import metrics\nnames = ['Negative','Positive']\n\nconfusion_matrix = metrics.confusion_matrix(y4_test, NBpredictions2)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(confusion_matrix, classes=names, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","22225f5f":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","d937058d":"del df_copied_reviews,rating1,rating5,sentiments,TfidfVectorizer,CountVectorizer,LinearSVC,MultinomialNB,accuracy_score,precision_score,recall_score,classification_report,confusion_matrix,vectors,vector,vector2,y,y2,y_test,y2_test,y3_test,y4_test,y_full_test,y3_full_test,y_train,y2_train,y3_train,y4_train,y_null_train,y3_null_train,LSVMpredictions,LSVMpredictions2,LSVMfullpredictions,NBpredictions,NBpredictions2,NBfullpredictions,LSVMclassifier,LSVMclassifier2,NBclassifier,NBclassifier2,x,x2,X_test,X2_test,X3_test,X4_test,X_full_test,X3_full_test,X_train,X2_train,X3_train,X4_train,X_null_train,X3_null_train\ngc.collect()","cbb64e67":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","5e5be843":"from sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","65010cac":"def predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_difference = (ratings - mean_user_rating[:, np.newaxis])\n        prediction = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_difference) \/ np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        prediction = ratings.dot(similarity) \/ np.array([np.abs(similarity).sum(axis=1)])\n    return prediction","9a4b0060":"def rmse(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()].flatten()\n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    return sqrt(mean_squared_error(prediction, ground_truth))","05c144fb":"def mae(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()].flatten()\n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    return mean_absolute_error(prediction, ground_truth)","80070367":"def collaborativeFiltering(dataset):\n    reviews_dataframe = dataset.copy()\n\n    print(\"Starting undersampling of the dataset\")\n    \n    # Undersampling the dataset to get a balanced dataset\n    rating1 = reviews_dataframe[reviews_dataframe['stars'] == 1][0:4000]\n    rating2 = reviews_dataframe[reviews_dataframe['stars'] == 2][0:2500]\n    rating3 = reviews_dataframe[reviews_dataframe['stars'] == 3][0:4000]\n    rating4 = reviews_dataframe[reviews_dataframe['stars'] == 4][0:4000]\n    rating5 = reviews_dataframe[reviews_dataframe['stars'] == 5][0:4000]\n    frames = [rating1, rating2, rating3, rating4, rating5]\n    reviews_dataframe = pd.concat(frames)\n    \n    print(\"Completed undersampling the dataset\")\n    \n    # converting user_id and business_id to integers for the matrix\n    reviews_dataframe['user_id'] = pd.factorize(reviews_dataframe.user_id)[0]\n    reviews_dataframe['business_id'] = pd.factorize(reviews_dataframe.business_id)[0]\n    \n    # getting the number unique users and restaurants\n    unique_users = reviews_dataframe.user_id.unique().shape[0]\n    unique_restaurants = reviews_dataframe.business_id.unique().shape[0]\n    \n    train_data, test_data = train_test_split(reviews_dataframe, test_size=0.2)\n    \n    #Create two user-item matrices, one for training and another for testing\n    train_data_matrix = np.zeros((unique_users, unique_restaurants))\n    \n    print(\"Starting creation of user-item matrix\")\n    \n    # train_data_matrix\n    for line in train_data.itertuples():\n         train_data_matrix[line[9], line[1]] = line[6]\n            \n    # test_data_matrix\n    test_data_matrix = np.zeros((unique_users, unique_restaurants))\n    for line in test_data.itertuples():\n        test_data_matrix[line[9], line[1]] = line[6]\n    \n    print(\"Completed creating user-item matrix\")\n    \n    \n    print(\"Starting creation of similarity matrix\")\n    \n    # similarity between users and items\n    user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n    item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')\n    \n    print(\"Completed creating similarity matrix\")\n    \n    \n    print(\"Starting creation of prediction matrix\")\n    \n    item_prediction = predict(train_data_matrix, item_similarity, type='item')\n    user_prediction = predict(train_data_matrix, user_similarity, type='user')\n    \n    print(\"Completed creating prediction matrix\")\n    \n    \n    print('Printing the RMSE and MAE' + '\\n')\n    \n    if dataset.equals(df_reviews):\n        rating_type = 'biased rating'\n    elif dataset.equals(df_LSVM_reviews):\n        rating_type = 'unbiased rating for Linear SVM Dataframe'\n    else:\n        rating_type = 'unbiased rating for Naive Bayes Dataframe'\n    \n    print('Root Mean Square Error while testing the model using ' + rating_type)\n    print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n    print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)) + '\\n')\n\n    print('Root Mean Square Error while training the model using ' + rating_type)\n    print('User-based CF RMSE: ' + str(rmse(user_prediction, train_data_matrix)))\n    print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_data_matrix)) + '\\n')\n    \n    print('Mean Absolute Error while testing the model using ' + rating_type)\n    print('User-based CF MAE: ' + str(mae(user_prediction, test_data_matrix)))\n    print('Item-based CF MAE: ' + str(mae(item_prediction, test_data_matrix)) + '\\n')\n\n    print('Mean Absolute Error while training the model using ' + rating_type)\n    print('User-based CF MAE: ' + str(mae(user_prediction, train_data_matrix)))\n    print('Item-based CF MAE: ' + str(mae(item_prediction, train_data_matrix)) + '\\n')","ca05c064":"collaborativeFiltering(df_reviews)","6cb21747":"collaborativeFiltering(df_LSVM_reviews)","4dddb29e":"collaborativeFiltering(df_NB_reviews)","3d505865":"del df_LSVM_reviews,df_NB_reviews\ngc.collect()","54d09e4c":"Lets check the mean of useful, funny and cool of each Restaurant","f022dde2":"Lets see what we dont need anymore so we can delete them from Memory.","0d9a6660":"# Predicting Ratings from Review text","7f3567d5":"Check the number of Restaurants on each city","a2bbc953":"Lets make predictions for the whole dataset using the classifier that we trained ","b285a5f7":"Classification Report of the Model","a90cdf65":"Importing the libraries","8141f008":"# Analyzing Tip.json\n\nSince Tip dataset doesnt have Business name but only Business id, lets add the name column in Tip dataset too. And then I will select 20 Restaurants which have more tips, and put them in a new dataframe and then I will do text cleansing for Tips. ","50054847":"Accuracy of the model","dbbfc56d":"**1. Collaborative Filtering**","b5d19859":"Prediction Function","f0e0877d":"* Naive Bayes Algorithm using (1,2,3,4 and 5 Ratings)","90e37bb9":"Precision and Recall of the Model","36199f46":"Collaborative filtering for Linear SVM Reviews dataset","130fbb1a":"This is how the Business dataset looks like","495500f2":"# ANALYZING SOME SPECIFIC RESTAURANTS","682014f8":"Precison and Recall of the Model","fa2d51f4":"2. **Naive Bayes Algorithm**","a7271e85":"Accuracy of the model","e2257963":"There are a total of 59371 Restaurants. I also checked for the empty values and I could have removed them but I filled them inplace with NA. Lets see them","a16a59a5":"Now lets see the dataset","a7245611":"Lets see what we dont need anymore so we can delete them from Memory.","bc374eef":"Lets delete the dataframe that we don't need now.","e0e14339":"Now that we have imported the libraries, I am going to read the datasets with pandas.","9614bb20":"Lets save this to a new dataframe called df_NB_reviews. Before saving it lets make a copy of a reviews dataset since we need some modifications on the new dataset and we will need the original dataset later again. We create a copy of dataset and drop first 3 rows, since we couldnt predict ratings for the first 3 rows. After the copy is made change the real stars column with the predicted ones.","b6294d4d":"Because reviews are too big, we will read them in chunks, and make sure we only take reviews for the Restaurants that we filtered earlier. I choose 10 chunks, (larger numbers will give MemoryError).","dcba56e4":"Firstly importing all the libraries that we need","d9d29019":"Classification Report of the Model","8c964468":"Confusion Metrics","5519e5b9":"Lets also see the Reviews with the added column name","51e2ce3f":"# Analyzing Business.json","7b4f5c60":"**Lets analyze Little Miss BBQ Restaurant Reviews and Ratings**","d4895049":"Make sure we only get businesses that are in our review list and delete the rest, since we dont have reviews for them.","d395ad16":"Accuracy of the model","aa3d0db7":"Classification Report of the Model","674d9cdd":"Yelp is a collection of different business in different areas. In Yelp the most popular business is Restaurants business. Yelp has a huge collection of restaurants. I take top 20 most occurrences restaurants and calculate their mean of ratings reviews polarity and plot them and see which is most popular restaurants since it's quite impossible to plot that thousands of business ratings. ","baf80501":"Lets see how the filtered dataset with only Restaurnats looks like","80cbb0ac":"**Lets analyze The Venetian Las Vegas Restaurant Reviews and Ratings**","37bc3a9a":"1. ** Linear SVM Algorithm**","5ac225b4":"Precision and Recall of the Model","2ea4c43e":"On the analysis section that we saw earlier, there are more positives ratings than negatives. Because of that the dataset is unbalanced. To make undersampling of the dataset lets check how many ratings are Negative and select the same amount for Positive.","39e1adc7":"Lets see how many different states are there","ac5b9985":"Collaborative Filtering Algorithm","bed4cc7a":"**Predicting Positive and Negative sentiments **","c99411e5":"Lets make predictions for the whole dataset using the classifier that we trained ","ab7f9659":"Vectorizer - breaks text into single words and bi-grams and then calculates the TF-IDF representation. The 'fit' builds up the vocabulary from all the reviews and the 'transform' turns each indivdual text into a matrix of numbers.","7122616a":"Lets see how many different cities are there","a727ba92":"Making predictions","0b4a8174":"Lets calculate the mean of sentiment polarity for each restaurant","47364517":"Importing the libraries","3e108b41":"Lets get all reviews for top 20 restaurants and calculate the mean of their rating","23f9308e":"Building Multinomial Naive Bayes model and train our classifier","2b811d7a":"Selecting review text and rating from reviews dataset and make a copy of them, since we will vectorize the text, and the original dataset is needed again later.","3b2deebf":"Splitting dataset into Train and test Data where 20% of data is going to be used for testing and 80% for training.","e40186ed":"Lets put them in respective variables rating1 and rating5","fbcafe7b":"Due to Memory Error I am using just 4200 reviews for 1,3,4,5 stars rated and 2500 for 2 stars rated ","5592c06a":"Splitting dataset into Train and test Data where 20% of data is going to be used for testing and 80% for training.","ecdaeebe":"Lets have a look in the dataset","4c414462":"Vectorization - Converting each review into a vector using bag-of-words approach","c37e4249":"Lets calculate the accuracy of our classifier by comparing the predicted ratings and the real ratings, if they are the same, our classifier predicted the ratings correctly. We sum up all of the correct answers and divide by the total number of reviews in our test set","6bcaa0a8":"Vectorization - Converting each review into a vector using bag-of-words approach","e91be2de":"Mean Absolute Error Algorithm","2fdc14dc":"Since there are 216737 Negative ratings from (1 to 3) lets also select only 216737 for Positive (4 and 5)","0239cbba":"Building Multinomial Naive Bayes model and train our classifier","e63d2a52":"Since there are many different Business on this dataset, we only want to work with Restaurants.\nLets filter them out","13e657b2":"Confusion Metrics","88e6de5b":"**Evaluating our classifier**","9d954dbd":"Building Linear SVM model and train our classifier","0512574f":"Root Mean Square Error Algorithm","a08a0ee2":"Lets analyze the columns of reviews","a963cf32":"Confusion Metrics","34655497":"Splitting dataset into Train and test Data where 20% of data is going to be used for testing and 80% for training.","7cd32d36":"Splitting dataset into Train and test Data where 20% of data is going to be used for testing and 80% for training.","58cd95b2":"Since Review dataset doesnt have Business name but only Business id, lets add the name column in Review dataset too, after that I will do a text cleansing and add it to another column called cleared_text. And then I will select 20 Restaurants which have more reviews, and put them in a new dataframe, then I will calculate the mean of Ratings for each Restaurant","ec165e2a":"Making predictions","3a7eb8a5":"Check for df that are on memory and we dont need anymore so we free up some space","99067103":"Making predictions","7e42acf2":"Lets select One specific Restaurant by business_id and see its reviews","78d7668c":"**Lets analyze Juan's Flaming Fajitas & Cantina Restaurant Reviews and Ratings**","25281f8a":"Lets delete this dataframes df_cities,df_top_cities,df_top_cities_sorted,df_states,df_top_states,df_top_states_sorted because we dont need them anymore","eefccf87":"Lets see what we dont need anymore so we can delete them from Memory.","9a213c10":"Classification Report of the Model","f89b439f":"Now lets add sentiment polarity for Restaurants with most Reviews","ba9ca97d":"Lets check the ratings on reviews","4a111c08":"* Naive Bayes Classifier using (1 and 5 Rating: Positive & Negative Reviews)","6b09e7a6":"# Analyzing Review.json and Tip.json","78f6c89c":"Building Linear SVM model and train our classifier","4a95705c":"Precision and Recall of the Model","26d89a2c":"Lets classify Ratings from 1 to 3 as Negatives and Ratings from 4 to 5 as Positives","bce754b7":"Confusion Metrics","6d58f99e":"Making predictions","c542f2b0":"Collaborative filtering for original Reviews dataset","94d47361":"# Recommendation Algorithms","f76eb451":"Lets save this to a new dataframe called df_LSVM_reviews. Before saving it lets make a copy of a reviews dataset since we need some modifications on the new dataset and we will need the original dataset later again. We create a copy of dataset and drop first 3 rows, since we couldnt predict ratings for the first 3 rows. After the copy is made change the real stars column with the predicted ones.","a03db760":"Check the number of Restaurants on each state","64a1f4f8":"Lets see most used words in the tips","e0e0771d":"Now lets see the reviews and ratings for Restaurants","6ee9f1b1":"Collaborative filtering for Naive Bayes Reviews dataset"}}