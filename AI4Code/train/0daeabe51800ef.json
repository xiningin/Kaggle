{"cell_type":{"4d7683b3":"code","d5f891dc":"code","e94baa05":"code","365fea96":"code","0ee40565":"code","090a0879":"code","908a91a9":"code","f6a43065":"code","c6b86b36":"code","b22fe7de":"code","f7aa553a":"code","875d1b87":"code","f496318f":"code","59ceb761":"code","7fbc1b9d":"code","8eba03bf":"code","84ceab72":"code","15554181":"code","952ee24a":"code","5d132091":"code","bc4f9200":"code","58decf44":"code","7c17a6c6":"code","c3b80b9d":"code","ec0ba20b":"code","daaf05e2":"code","6ca1569d":"code","7f20773d":"code","5e38a30c":"code","5110b1c3":"code","a012d8a4":"code","301970d4":"code","36313f6e":"markdown","75e74d5e":"markdown","c682d193":"markdown","901f0781":"markdown","66055c40":"markdown","9ca893f9":"markdown","fd97d325":"markdown","46c5edc7":"markdown","8a3470f1":"markdown","d19d6981":"markdown","50d76fc9":"markdown","274ce3a1":"markdown","18e7bf2c":"markdown","4d14509d":"markdown","f8241333":"markdown","eb4fbd47":"markdown"},"source":{"4d7683b3":"from fastai.imports import *\nfrom fastai.structured import *\nfrom fastai.column_data import *\nfrom torch.nn import functional as F\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","d5f891dc":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# Define custom loss function to account for two ouput nodes\ndef roc_auc_own(y_score, y_true):\n    y_score = np.exp(y_score[:,1])\n    return roc_auc_score(y_true, y_score)","e94baa05":"df_train = pd.read_feather('..\/input\/home-credit-data-processing-for-neural-networks\/tables_merged_train')\ndf_test = pd.read_feather('..\/input\/home-credit-data-processing-for-neural-networks\/tables_merged_test')","365fea96":"df_train.dtypes.value_counts()","0ee40565":"cat_vars = [col for col in df_train if df_train[col].dtype.name != 'float64' and df_train[col].dtype.name != 'float32' and len(df_train[col].unique()) < 150]\ncat_vars.remove('TARGET')","090a0879":"cat_sz = [(c, len(df_train[c].unique())+1) for c in cat_vars]","908a91a9":"cat_vars","f6a43065":"# Train validation-split\ny = np.array(df_train['TARGET'])\ndf_train.drop('TARGET', axis = 1, inplace=True)\ndf_to_nn_train, df_to_nn_valid, y_train, y_valid = train_test_split(df_train, y, test_size=0.33, random_state=23, stratify = y)","c6b86b36":"def preprocess_fast_ai(df_to_nn_train, df_to_nn_valid, cat_vars):\n    # Declare categorical variables\n    for v in cat_vars: df_to_nn_train[v] = df_to_nn_train[v].astype('category').cat.as_ordered()\n    apply_cats(df_to_nn_valid, df_to_nn_train)\n\n    # Deal with missingness and put everything as numbers\n    df, _, nas, mapper = proc_df(df_to_nn_train, do_scale=True, skip_flds=['SK_ID_CURR'])\n    df_valid, _, nas, mapper = proc_df(df_to_nn_valid, do_scale=True, na_dict=nas, mapper=mapper, skip_flds=['SK_ID_CURR'])\n    return df, df_valid","b22fe7de":"df, df_valid = preprocess_fast_ai(df_to_nn_train, df_to_nn_valid, cat_vars)","f7aa553a":"emb_szs = [(c, min(50, (c+1)\/\/2)) for _,c in cat_sz]","875d1b87":"md  = ColumnarModelData.from_data_frames('', trn_df = df, val_df = df_valid, \n                                         trn_y = y_train.astype('int'), val_y = y_valid.astype('int'), \n                                         cat_flds=cat_vars, bs=512, is_reg= False)","f496318f":"class MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n                 y_range=None, use_bn=False, is_reg=True, is_multi=False):\n        super().__init__()\n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont= n_emb, n_cont\n        szs = [n_emb + n_cont] + szs\n        self.lins = nn.ModuleList([\n            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([\n            nn.BatchNorm1d(sz) for sz in szs[1:]])\n        for o in self.lins: kaiming_normal(o.weight.data)\n        self.outp = nn.Linear(szs[-1], out_sz)\n        kaiming_normal(self.outp.weight.data)\n\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn,self.y_range = use_bn,y_range\n        self.is_reg = is_reg\n        self.is_multi = is_multi\n\n    def forward(self, x_cat, x_cont):\n        x = []\n        for i,e in enumerate(self.embs):\n            x.append(e(x_cat[:,i]))\n        x = torch.cat(x, 1)\n        x = self.emb_drop(x)\n        x2 = self.bn(x_cont)\n        x = torch.cat([x, x2], 1)\n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            x = F.relu(l(x))\n            if self.use_bn: x = b(x)\n            x = d(x)\n        x = self.outp(x)\n        x = F.log_softmax(x)\n        return x","59ceb761":"# Define Model\nm = MixedInputModel(emb_szs, n_cont = len(df.columns)-len(cat_vars),\n                   emb_drop = 0.05, out_sz = 2, szs = [500, 250, 250], drops = [0.1, 0.1, 0.1], \n                   y_range = None, use_bn = False, is_reg = False, is_multi = False)\nbm = BasicModel(m.cuda(), 'binary_classifier')","7fbc1b9d":"# Define Learner\nclass StructuredLearner(Learner):\n    def __init__(self, data, models, **kwargs):\n        super().__init__(data, models, **kwargs)\n        self.crit = F.nll_loss\n# Instantiate learner\nlearn = StructuredLearner(md, bm)","8eba03bf":"learn.lr_find(1e-4, 1)\nlearn.sched.plot(100)","84ceab72":"lr = 1e-2\nlearn.fit(lr, 3, metrics=[roc_auc_own])","15554181":"# predictions \nlogpreds = learn.predict() # final output log_softmax\npreds = np.exp(logpreds[:,1])","952ee24a":"logpreds_valid = learn.predict(is_test = False)\npreds_valid = np.exp(logpreds_valid[:,1])\npreds_binary = (preds_valid >= 0.5).astype(np.int)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_valid, preds_binary)\nplot_confusion_matrix(cm, [0, 1])","5d132091":"from sklearn.metrics import classification_report\nprint(classification_report(y_valid,\n                            preds_binary,\n                            target_names= ['0', '1']))","bc4f9200":"from sklearn.metrics import roc_curve\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_valid,\n                                                               preds_valid)\n# Plot ROC curve\nplt.title(\"Receiver Operating Characteristic\")\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()","58decf44":"class ColumnarDataset(Dataset):\n    def __init__(self, cats, conts, y, is_reg, is_multi):\n        n = len(cats[0]) if cats else len(conts[0])\n        self.cats  = np.stack(cats,  1).astype(np.int64)   if cats  else np.zeros((n,1))\n        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n        self.y     = np.zeros((n,1))                       if y is None else y\n        if is_reg:\n            self.y =  self.y[:,None]\n        self.is_reg = is_reg\n        self.is_multi = is_multi\n\n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]\n\n    @classmethod\n    def from_data_frames(cls, df_cat, df_cont, y=None, is_reg=True, is_multi=False):\n        cat_cols = [c.values for n,c in df_cat.items()]\n        cont_cols = [c.values for n,c in df_cont.items()]\n        return cls(cat_cols, cont_cols, y, is_reg, is_multi)\n\n    @classmethod\n    def from_data_frame(cls, df, cat_flds, y=None, is_reg=False, is_multi=False):\n        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y, is_reg, is_multi)\n\nclass ColumnarModelData(ModelData):\n    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None, shuffle=True):\n        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n        super().__init__(path, DataLoader(trn_ds, bs, shuffle=shuffle, num_workers=1),\n            DataLoader(val_ds, bs*2, shuffle=False, num_workers=1), test_dl)\n    @classmethod\n    def from_data_frames(cls, path, trn_df, trn_y, cat_flds, bs, val_df = None, val_y = None,  is_reg = False, is_multi = False, test_df=None):\n        trn_ds  = ColumnarDataset.from_data_frame(trn_df,  cat_flds, trn_y, is_reg, is_multi)\n        val_ds  = ColumnarDataset.from_data_frame(val_df,  cat_flds, val_y, is_reg, is_multi) if val_df is not None else None\n        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds, None,  is_reg, is_multi) if test_df is not None else None\n        return cls(path, trn_ds, val_ds, bs, test_ds=test_ds)\n    \n    @classmethod\n    def from_data_frame(cls, path, val_idxs, df, y, cat_flds, bs, is_reg=True, is_multi=False, test_df=None):\n        ((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idxs, df, y)\n        return cls.from_data_frames(path, trn_df, val_df, trn_y, val_y, cat_flds, bs, is_reg, is_multi, test_df=test_df)","7c17a6c6":"train_df, test_df = preprocess_fast_ai(df_train, df_test, cat_vars)","c3b80b9d":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler()\ndf_resampled, y_resampled = ros.fit_sample(df, y_train)\ndf_resampled = pd.DataFrame(df_resampled, columns = df.columns)\ny_valid.mean(), y_resampled.mean()","ec0ba20b":"md  = ColumnarModelData.from_data_frames('', trn_df = df_resampled, \n                                         val_df = df_valid, trn_y = y_resampled.astype('int'),\n                                         val_y = y_valid.astype('int'), cat_flds=cat_vars, bs=1024, is_reg = False,\n                                         test_df = test_df)\n# Define Learner\nclass StructuredLearner(Learner):\n    def __init__(self, data, models, **kwargs):\n        super().__init__(data, models, **kwargs)\n        self.crit = F.nll_loss\nm = MixedInputModel(emb_szs, n_cont = len(df.columns)-len(cat_vars),\n                   emb_drop = 0.4, out_sz = 2, szs = [1000, 500], \n                   drops = [0.6, 0.6],y_range = None, use_bn = False, is_reg = False)\nbm = BasicModel(m.cuda(), 'binary_classifier')\n# Instantiate learner\nlearn = StructuredLearner(md, bm)","daaf05e2":"learn.lr_find(1e-2, 2)\nlearn.sched.plot(100)","6ca1569d":"lr = 0.1\nlearn.fit(lr, 3, metrics=[roc_auc_own])","7f20773d":"learn.fit(lr, 2, metrics=[roc_auc_own], cycle_len=1, cycle_mult=2)","5e38a30c":"logpreds = learn.predict()\npreds = np.exp(logpreds[:,1])\n\nlogpreds_valid = learn.predict(is_test = False)\npreds_valid = np.exp(logpreds_valid[:,1])\npreds_binary = (preds_valid >= 0.5).astype(np.int)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_valid, preds_binary)\nplot_confusion_matrix(cm, [0, 1])","5110b1c3":"from sklearn.metrics import classification_report\nprint(classification_report(y_valid,\n                            preds_binary,\n                            target_names= ['0', '1']))\n\nfrom sklearn.metrics import roc_curve\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_valid,\n                                                               preds_valid)","a012d8a4":"# Plot ROC curve\nplt.title(\"Receiver Operating Characteristic\")\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()","301970d4":"logpreds = learn.predict(True)\npreds = np.exp(logpreds[:,1])\n\nsubmission = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'],\n              'TARGET': preds})\nsubmission.to_csv('submission.csv', index=False, float_format='%.8f')","36313f6e":"Although our AUC is not that bad, our predictions are extremely naive. They are driven by the huge imbalance in the dataset. This can be seen by the confusion matrix: we are hardly predicting for any of the observations the `1` class. This results in a lousy recall. We need our model to learn better what indentifies the people who belong to the `1` class.","75e74d5e":"Now, let's do some fitting:","c682d193":"Our model is having trouble identifying the people with class `1`. The main problem is that there is hardly any of them in the dataset. Let's change that by oversampling with replacement these people and creating and augmented dataset such that they appear the same number of times as the people with class `0`. ","901f0781":"Let's redefine the data loader with the new observations added. Given that our training stops being representative of our validation, let's do some heavy dropout regularization. ","66055c40":"# Fast.ai\/Pytorch Starter\n\nAlthough most people (including me) seem to be doing much better with Boosting Trees, I think it is worth the time to explore a Neural Network solution to the  Home Credit Default Risk competition. Besides, a Kaggle competition is always a good opportunity to test what one is currently learning. I did two \"cool\"  things in this kernel:\n\n* Categorical Embeddings.\n* Custom loss functions with different weights for each class to try to manage the imbalance in `TARGET`. \n\nI had to tweak the fast.ai library a little bit, but all things considered, it is extraordinary how little code you actually have to write to get some model going. \n\n## Load Data\n\nTo aggregate the various tables available in the competition I followed the next heuristic:\n\n* If the variable was continuous, I aggregated it using its mean. \n* If the variable was categorical, I aggregated it using its mode. If I were to one-hot-encode the variables and aggregate them using their mean, there wouldn't be categorical variables (besides the one in the main table) for which to create categorical embeddings. \n\nThe mode computation is very, very slow, so I did it all of this in another Kaggle kernel.\n\n## Imports\n","9ca893f9":"Besides the embedding, 3 fully connected layers:","fd97d325":"There's no easy way of using the fast.ai library (that I know) to predict structured data in a classification problem. Besides, the fast.ai package that Kaggle is running is not the same as the source code in GitHub. Thus, I read a little bit of the code and tweaked it to create the model that we are going to use.","46c5edc7":"## Pre-processing \n\nThe fast.ai library handles NA values for us. For categorical variables, missing variables are encoded as a level within the categories of their own; in this case, with a zero. For continuous variables, if a given variable has missing variables, we create an extra dummy variable recording which of the observations were missing and, in the original given variable, we impute the missing values with the median. \n\nThus, the algorithm will be able to encode missingness in any way it chooses. Also, we will normalize continuous variables for ease of optimization. ","8a3470f1":"We define our learner's loss function:","d19d6981":"## PyTorch\/Fast.ai\n\nDefine the data loader:","50d76fc9":"Which variables are we going to treat as categorical?","274ce3a1":"## What type of variables do we have?","18e7bf2c":" Even though we are regularizing heavily, the model is still overfitting and the ROC has improved a little bit. Let's try to take this model to the leaderboard.","4d14509d":"## Let's Understand our predictions","f8241333":"## Tackling the imbalance problem\n\nSeems we've exhausted what this model can learn, as the changes from `val_loss` and `roc` have hit decreasing returns and our predictions aren't that intelligent. Let's try to correct for the imbalance in `TARGET`.","eb4fbd47":"The embedding sizes we are going to use for each category:"}}