{"cell_type":{"88fc381c":"code","b5131ff3":"code","347676a2":"code","a9ac0c00":"code","3ee8d131":"code","8afbe973":"code","d09c76ed":"code","d7fdf2a9":"code","23c1ff32":"code","51410483":"code","68593e37":"code","15405cf3":"code","a14b2f2a":"code","bdc2ad27":"markdown","605aaf4a":"markdown","cac2259d":"markdown","ffff74a6":"markdown","2a6e2cc2":"markdown","3fe591d8":"markdown","da5b9e71":"markdown","a382a76b":"markdown","f6ed05fe":"markdown","e0319b48":"markdown","c0415191":"markdown","8f2fbf1a":"markdown","71320928":"markdown","cb90cc13":"markdown"},"source":{"88fc381c":"# import all needed libraries\nimport numpy as np\nimport pandas as pd\nimport spacy\nfrom spacy import displacy\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\n\nimport nltk\nfrom nltk.corpus import words as english_words, stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS \n\nfrom collections import Counter\n\nimport torch\n\nimport re\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","b5131ff3":"# load data\nspeechs = list()\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)\n        text = open(file_path,'r').read()\n        speechs.append(text)\n\nprint(f\"Total Number of Documents : {len(speechs)}\")","347676a2":"def cleansing_text(text: str) -> str:\n    ## replacing the newlines and extra spaces, and change all character to lower case\n    corpus = text.replace('\\n', ' ').replace('\\r', '').replace('  ',' ').lower()\n\n    ## removing everything except alphabets\n    corpus_sans_symbols = re.sub('[^a-zA-Z \\n]', '', corpus)\n\n    ## removing stopwords\n    stop_words = set(w.lower() for w in stopwords.words())\n\n    corpus_sans_symbols_stopwords = ' '.join(filter(lambda x: x.lower() not in stop_words, corpus_sans_symbols.split()))\n    return corpus_sans_symbols_stopwords\n    \npreprocessed_speechs = list(map(cleansing_text, speechs))\n\n# display an example of Tramp Speech\nprint(preprocessed_speechs[0][:100] + '...')\n","a9ac0c00":"stemmer = nltk.PorterStemmer()\ndef stemmer_str(text: str)-> str:\n    corpus_stemmed = ' ' .join (map(lambda str: stemmer.stem(str), text.split()))\n    return corpus_stemmed\n\npreprocessed_speechs_stemmer = list(map(stemmer_str, preprocessed_speechs))\n\nprint(preprocessed_speechs_stemmer[0][:100] + '...')\n","3ee8d131":"all_speechs_str = ' '.join(preprocessed_speechs)\n\nwordcloud = WordCloud(width = 800, height = 800,background_color ='grey', min_font_size = 10).generate(all_speechs_str)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.rcParams.update({'font.size': 25})\nplt.axis(\"off\") \nplt.title('Word Cloud: DJT Rallies ')\nplt.tight_layout(pad = 0) \n  \nplt.show()","8afbe973":"word_freq_count = Counter(' '.join(preprocessed_speechs).split(\" \"))\n\ncommon_words = [word[0] for word in word_freq_count.most_common(20)]\ncommon_counts = [word[1] for word in word_freq_count.most_common(20)]\n\nplt.figure(figsize=(15, 12))\n\nsns.set_style(\"whitegrid\")\nsns_bar = sns.barplot(x=common_words, y=common_counts)\nsns_bar.set_xticklabels(common_words, rotation=45)\nplt.title('Most Common Words in the document')\nplt.show()","d09c76ed":"all_speechs_str = ' '.join(preprocessed_speechs_stemmer)\n\nwordcloud = WordCloud(width = 800, height = 800,background_color ='grey', min_font_size = 10).generate(all_speechs_str)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.rcParams.update({'font.size': 25})\nplt.axis(\"off\") \nplt.title('Word Cloud: DJT Rallies ')\nplt.tight_layout(pad = 0) \n  \nplt.show()\n ","d7fdf2a9":"word_freq_count = Counter(' '.join(preprocessed_speechs_stemmer).split(\" \"))\n\ncommon_words = [word[0] for word in word_freq_count.most_common(20)]\ncommon_counts = [word[1] for word in word_freq_count.most_common(20)]\n\nplt.figure(figsize=(15, 12))\n\nsns.set_style(\"whitegrid\")\nsns_bar = sns.barplot(x=common_words, y=common_counts)\nsns_bar.set_xticklabels(common_words, rotation=45)\nplt.title('Most Common Words in the document')\nplt.show()","23c1ff32":"from itertools import islice\n\ntfidf_vec = TfidfVectorizer(stop_words=\"english\")\ntransformed = tfidf_vec.fit_transform(raw_documents=preprocessed_speechs)\nindex_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\nprint( {k: index_value[k] for k in list(index_value)[:50]})","51410483":"tfidf_vec = TfidfVectorizer(stop_words=\"english\")\ntransformed = tfidf_vec.fit_transform(raw_documents=preprocessed_speechs_stemmer)\nindex_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\nprint( {k: index_value[k] for k in list(index_value)[:50]})","68593e37":"# this function can get any n-grams from a string\n# note that we can use also nltk.bigrams(eng_tokens)\ndef get_n_grams(text: str, n:int):\n    n_grams = list()\n    text_tokens = text.split(' ')\n    for index, token in enumerate(text_tokens):\n        if index+n < len(text_tokens):\n            n_grams.append(tuple(text_tokens[index:index+n]))\n    return n_grams","15405cf3":"#get bi-grams from all speechs\nbi_grams = list()\nfor speech in preprocessed_speechs:\n    bi_grams = bi_grams + get_n_grams(speech, 2)\n\nbi_grams_freq = nltk.FreqDist(bi_grams)\nbi_grams_sorted = sorted(bi_grams_freq , key = bi_grams_freq.__getitem__, reverse = True)\n\n# keep only 20\nbi_grams_sorted = bi_grams_sorted[:20]\n[print(item, ' : ', bi_grams_freq[item]) for item in bi_grams_sorted]\n\nbi_grams_dict = dict()\nfor item in bi_grams_sorted:\n    bi_grams_dict[' '.join(item)] = bi_grams_freq[item]\n \nwordcloud = WordCloud(width = 800, height = 800,background_color ='grey', min_font_size = 10).generate_from_frequencies(bi_grams_dict)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.rcParams.update({'font.size': 25})\nplt.axis(\"off\") \nplt.title('Word Cloud: DJT Rallies ')\nplt.tight_layout(pad = 0) \n  \nplt.show()","a14b2f2a":"#get 3-grams from all speechs\nthree_grams = list()\nfor speech in preprocessed_speechs:\n    three_grams = three_grams + get_n_grams(speech, 3)\n\nthree_grams_freq = nltk.FreqDist(three_grams)\nthree_grams_sorted = sorted(three_grams_freq , key = three_grams_freq.__getitem__, reverse = True)\n\n# keep only 20\nthree_grams_sorted = three_grams_sorted[:20]\n[print(item, ' : ', three_grams_freq[item]) for item in three_grams_sorted]\n\nthree_grams_dict = dict()\nfor item in three_grams_sorted:\n    three_grams_dict[' '.join(item)] = three_grams_freq[item]\n     \nwordcloud = WordCloud(width = 800, height = 800,background_color ='grey', min_font_size = 10).generate_from_frequencies(three_grams_dict)\nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.rcParams.update({'font.size': 25})\nplt.axis(\"off\") \nplt.title('Word Cloud: DJT Rallies ')\nplt.tight_layout(pad = 0) \n  \nplt.show()","bdc2ad27":"## 1.3 Stemming and Lemmatizing","605aaf4a":"Without Stemming","cac2259d":"## 1.1 Load Data","ffff74a6":"With stemming","2a6e2cc2":"# 2. Frequency appearance words, n-grams, phrases:","3fe591d8":"**With Stemmer**","da5b9e71":"# Features of this analysis\n\nThe purpose of this exersise is to carry out an NLP investigation of Donald Trump'ss speeches at ellection rallies during 2019 and 2020. The following tasks is handled :\n\n* most used words, bigrams, 3-grams to display in word-cloud graph\n\n\n","a382a76b":"## 2.1 Words Frequencies","f6ed05fe":"## 1.2 Removing special characters and stopwords(using NLTK)","e0319b48":"## 2.3 Top 20 n-grams frequency","c0415191":"# 1. Pre-processing Speach","8f2fbf1a":"## 2.2 TOP 50 terms using TF-IDF\n\nTF-IDF, short for **term frequency\u2013inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n\n**Without Stemmer**","71320928":"**Word cloud of 3-grams**","cb90cc13":"**Word cloud of bi-grams**"}}