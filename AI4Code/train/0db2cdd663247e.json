{"cell_type":{"7b79f3db":"code","35b51610":"code","cc1e1f9e":"code","523210cd":"code","4c1b1511":"code","c2e4b25a":"code","ee9a43ef":"code","61ab7ef8":"code","1d46dedf":"markdown","0d5ade9b":"markdown","2ea4ad99":"markdown"},"source":{"7b79f3db":"import pandas as pd\nimport sklearn as sk\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn.linear_model import SGDClassifier","35b51610":"\ntrain_data = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        na_values=\"nulo\")\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\",\n        na_values=\"nulo\")\n\n\nx_train1 = train_data[[\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education.num\", \"marital.status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital.gain\", \"capital.loss\", \"hours.per.week\", \"native.country\"] ]\nx_teste = test_data[[\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education.num\", \"marital.status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital.gain\", \"capital.loss\", \"hours.per.week\", \"native.country\"] ]\n\ny_train = train_data[[\"income\"]]\n\n\nframes = [x_train1, x_teste]\nresult = pd.concat(frames, sort = False)\nresult = pd.get_dummies(result)\nx_train = result.head(32560)\n\nframes = [x_teste, x_train1]\nresult = pd.concat(frames, sort = False)\nresult = pd.get_dummies(result)\nx_test = result.head(16280)","cc1e1f9e":"SGD = SGDClassifier()\nparam_grid = {\"loss\" : [\"hinge\", \"modified_huber\", \"log\"],\n              \"penalty\" :   [\"l2\", \"l1\", \"elasticnet\"],\n             }\n\nclf = GridSearchCV(SGD,param_grid=param_grid , cv =6, n_jobs = -1, verbose=2, refit = False)\nclf.fit(x_train, y_train)\nclf.best_score_","523210cd":"'''\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 150)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 40)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 6, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(x_train, y_train)\nrf_random.best_score_\n'''","4c1b1511":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\nDTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\",max_depth = None)\nABC = AdaBoostClassifier(base_estimator = DTC)\n\n# run grid search\ngrid_search_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring = 'roc_auc', cv = 6, n_jobs =  -1, verbose=2)\ngrid_search_ABC.fit(x_train, y_train)\ngrid_search_ABC.best_score_","c2e4b25a":"#rf_random.best_params_\n\n","ee9a43ef":"# treinando com todos os dados:\n\nclassificador = RandomForestClassifier(n_estimators= 1601,\n min_samples_split= 10,\n min_samples_leaf= 2,\n max_features= 'auto',\n max_depth= 63,\n bootstrap= False)\n\nclassificador = classificador.fit(x_train, y_train)","61ab7ef8":"out = classificador.predict(x_test)\ndf = pd.DataFrame(out, columns=['Income'])\ndf.to_csv(\"submission.csv\", index_label = 'Id')","1d46dedf":"Est\u00e1 comentado, mas o resultado foi:\n\n{'n_estimators': 1601,\n 'min_samples_split': 10,\n 'min_samples_leaf': 2,\n 'max_features': 'auto',\n 'max_depth': 63,\n 'bootstrap': False}","0d5ade9b":"Os melhores par\u00e2metros para o melhor classificador est\u00e3o a seguir: (random Forest)","2ea4ad99":"Random Forest demora muito, ent\u00e3o deixei comentado\no resultado foi de 0.8659398034398035"}}