{"cell_type":{"6a3f0c96":"code","95a32863":"code","9b13e251":"code","059d5dae":"code","bc7cf792":"code","11c86aac":"code","543ebfd3":"code","0c45e578":"code","37eea75a":"code","44efe695":"code","ee0d885e":"code","b9af5e9a":"code","2c3dc84e":"code","09b450d9":"code","122a906f":"code","9cc6435e":"code","39f777bd":"code","5dbac244":"code","0d04eebd":"code","ae92c30e":"code","4d1b6631":"code","05ab4c0c":"code","3aae0922":"code","1b974d27":"code","e5877828":"code","9a51b2d7":"code","69c536a1":"code","3e305cc5":"code","b156da51":"code","77cfaba8":"code","50767338":"code","d31728be":"code","b6094504":"code","74e9b72a":"code","52529744":"code","49ad2d51":"code","2968620e":"code","bbff4eff":"code","6916aa5b":"code","b3f23095":"code","7c6108b8":"code","ef6ece9a":"code","27883875":"code","9336f50d":"code","63c5655a":"code","1f00e799":"code","ee9f5aa3":"code","49c8c7b7":"code","6870790c":"code","9f335fee":"code","5a0b08e5":"code","3e7068bb":"code","f9157ca6":"code","2cf750f9":"code","f26de21c":"code","531c141b":"code","0cc8e80b":"code","328ec9de":"code","0de55dfc":"code","9ef1bcfb":"code","4dcae990":"code","12ab4aa2":"code","1a742fee":"code","1881b011":"code","dfc9b4cb":"code","0a1a8650":"code","77fa7c7f":"code","8ea7532f":"code","710019e9":"code","6a4e6cac":"code","86f2f299":"code","b43be8d7":"code","1a9886ac":"code","d2214d29":"code","96c5c589":"markdown","458c0656":"markdown","34d94bed":"markdown","11e9ddc7":"markdown","3130f02b":"markdown","82b4477e":"markdown","a9749e4a":"markdown","af32f701":"markdown","a5c54b0e":"markdown","1ddbbdda":"markdown","13cf0a55":"markdown","31365bad":"markdown","fedcd0a7":"markdown","f19f9d5f":"markdown","2aa57588":"markdown","12d6f087":"markdown","07edd1ad":"markdown","af7df18c":"markdown","0624c962":"markdown","4c5248a5":"markdown","11692c14":"markdown","d937d211":"markdown","825496aa":"markdown","5b1fbd9b":"markdown","245e6c89":"markdown","9f17e2aa":"markdown","5cec8d48":"markdown","99cf5de8":"markdown","456f2f97":"markdown","f7ccfd5a":"markdown","3be8b7d2":"markdown"},"source":{"6a3f0c96":"# CODE TAKEN FROM https:\/\/github.com\/kpe\/bert-for-tf2\/\n# ALL CREDITS TO https:\/\/github.com\/kpe\n# CODE COPIED TO LOCAL FOLDER DUE TO INTERNET RESTRICTIONS\n# NORMALLY THIS CODE WOULD BE AVAILABLE VIA pip install bert-for-tf2\n\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gc\nimport os\nimport warnings\nimport operator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nfrom collections import Counter\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport nltk\nfrom gensim import corpora, models\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom keras.preprocessing.text import Tokenizer\n\npyLDAvis.enable_notebook()\nnp.random.seed(2018)\nwarnings.filterwarnings('ignore')","95a32863":"sample = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/sample_submission.csv')\nsample.head(3)","9b13e251":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntrain.head(3)","059d5dae":"test = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')\ntest.head(3)","bc7cf792":"target_columns = sample.columns.values[1:].tolist()\ntarget_columns","11c86aac":"print(\"Train and test shape: {} {}\".format(train.shape, test.shape))","543ebfd3":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n","0c45e578":"train['question_title'].str.len()","37eea75a":"#Number of characters in the sentence\n\nlengths = train['question_title'].apply(len)\ntrain['lengths'] = lengths\nlengths = train.loc[train['lengths']<4000]['lengths']\nsns.distplot(lengths, color='b')\nplt.show()","44efe695":"question_body=train['question_body'].str.len()\nanswer_body=train['answer'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(question_body,ax=ax1,color='blue')\nsns.distplot(answer_body,ax=ax2,color='green')\nax2.set_title('Distribution for question body')\nax1.set_title('Distribution for answer')\nplt.show()\n","ee0d885e":"words = train['question_body'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\ntrain['words'] = words\n#words = train.loc[train['words']<500]['words']\nsns.distplot(words, color='r')\nplt.show()","b9af5e9a":"answer=train['answer'].apply(lambda x : len(x.split(' ')))\nsns.distplot(answer,color='red')\nplt.gca().set_title('Distribution of no: of words in answer')","2c3dc84e":"avg_word_len = train['answer'].apply(lambda x: 1.0*len(''.join(x.split()))\/(len(x) - len(''.join(x.split())) + 1))\ntrain['avg_word_len'] = avg_word_len\navg_word_len = train.loc[train['avg_word_len']<10]['avg_word_len']\nsns.distplot(avg_word_len, color='g')\nplt.show()","09b450d9":"stopwords=stopwords.words('english')\ntrain['que_stopwords']=train['question_body'].apply(lambda x : [x for x in x.split() if x in stopwords])\ntrain['ans_stopwords']=train['answer'].apply(lambda x: [x for x in x.split() if x in stopwords])","122a906f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nwords=train['que_stopwords'].apply(lambda x : len(x))\nsns.distplot(words,color='green',ax=ax1)\nax1.set_title('Distribution of stopwords in question ')\nwords=train['ans_stopwords'].apply(lambda x: len(x))\nsns.distplot(words,color='blue',ax=ax2)\nax2.set_title('Distribution of stopwords in  Answer')\n","9cc6435e":"\ndef common_ngrams(col,common=10):\n    corpus=[]\n    for question in train[col].values:\n        words=[str(x[0]+' '+x[1]) for x in ngrams(question.split(),2)]\n        corpus.append(words)\n    flatten=[x for one in corpus for x in one]\n    counter=Counter(flatten)\n    most_common=counter.most_common(common)\n    string,value=zip(*(most_common))\n    return string,value\n","39f777bd":"string,value=common_ngrams('question_title')\nplt.figure(figsize=(9,7))\nplt.bar(height=value,x=string,color='green')\nplt.gca().set_xticklabels(string,rotation='45')\nplt.show()","5dbac244":"string,value=common_ngrams('answer')\nplt.figure(figsize=(9,7))\nplt.bar(height=value,x=string,color='green')\nplt.gca().set_xticklabels(string,rotation='45')\nplt.show()","0d04eebd":"plt.figure(figsize=(20,15))\nplt.title(\"Distribution of question_not_really_a_question\")\nsns.distplot(train['question_not_really_a_question'],kde=True,hist=False, bins=120, label='question_not_really_a_question')\nplt.legend(); plt.show()","ae92c30e":"def plot_features_distribution(features, title):\n    plt.figure(figsize=(15,10))\n    plt.title(title)\n    for feature in features:\n        sns.distplot(train.loc[~train[feature].isnull(),feature],kde=True,hist=False, bins=120, label=feature)\n    plt.xlabel('')\n    plt.legend()\n    plt.show()","4d1b6631":"plot_features_distribution(targets, \"Distribution of targets in train set\")","05ab4c0c":"features = ['question_well_written','answer_well_written']\nplot_features_distribution(features, \"Distribution of question_well_written  vs  answer_well_written\")","3aae0922":"def plot_count(feature, title,size=1):\n    f, ax = plt.subplots(1,1, figsize=(10,10))\n    total = float(len(train))\n    g = sns.countplot(train[feature], order = train[feature].round(2).value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2,\n                height + 5,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()   ","1b974d27":"plot_count('question_well_written','question_well_written')","e5877828":"plot_count('question_expect_short_answer','question_expect_short_answer')","9a51b2d7":"plot_count('question_asker_intent_understanding','question_asker_intent_understanding')","69c536a1":"plot_count('question_body_critical','question_body_critical')","3e305cc5":"plot_count('question_conversational','question_conversational')","b156da51":"plot_count('question_fact_seeking','question_fact_seeking')","77cfaba8":"plot_count('question_has_commonly_accepted_answer','question_has_commonly_accepted_answer')","50767338":"plot_count('question_interestingness_others','question_interestingness_others')","d31728be":"plot_count('question_interestingness_self','question_interestingness_self')","b6094504":"plot_count('question_multi_intent','question_multi_intent')","74e9b72a":"plot_count('question_not_really_a_question','question_not_really_a_question')","52529744":"plot_count('question_opinion_seeking','question_opinion_seeking')","49ad2d51":"plot_count('question_type_choice','question_type_choice')","2968620e":"plot_count('question_type_compare','question_type_compare')","bbff4eff":"plot_count('question_type_consequence','question_type_consequence')","6916aa5b":"plot_count('question_type_definition','question_type_definition')","b3f23095":"plot_count('question_type_entity','question_type_entity')","7c6108b8":"plot_count('question_type_instructions','question_type_instructions')","ef6ece9a":"plot_count('question_type_procedure','question_type_procedure')","27883875":"plot_count('question_type_reason_explanation','question_type_reason_explanation')","9336f50d":"plot_count('question_type_spelling','question_type_spelling')","63c5655a":"plot_count('answer_helpful','answer_helpful')","1f00e799":"plot_count('answer_level_of_information','answer_level_of_information')","ee9f5aa3":"plot_count('answer_plausible','answer_plausible')","49c8c7b7":"plot_count('answer_relevance','answer_relevance')","6870790c":"plot_count('answer_satisfaction','answer_satisfaction')","9f335fee":"plot_count('answer_type_instructions','answer_type_instructions')","5a0b08e5":"plot_count('answer_type_procedure','answer_type_procedure')","3e7068bb":"plot_count('answer_type_reason_explanation','answer_type_reason_explanation')","f9157ca6":"plot_count('answer_well_written','answer_well_written')","2cf750f9":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(20,20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","f26de21c":"show_wordcloud(train['question_body'].sample(6079), title = 'Prevalent words in question_body - train data')","531c141b":"show_wordcloud(train.loc[train['question_well_written'] > 0.6]['question_body'].sample(3000), \n               title = 'Prevalent question_body words with question_well_written score > 0.6')","0cc8e80b":"show_wordcloud(train.loc[train['answer_well_written'] > 0.6]['question_title'].sample(3000), \n               title = 'Frequesnt question_title words with answer_well_written score > 0.6')","328ec9de":"\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it's not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match(\"^.*?([A-Za-z0-9_-]+)\/bert_model.ckpt\", init_checkpoint)\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n    ]\n\n    cased_models = [\n        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n        \"multi_cased_L-12_H-768_A-12\"\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = \"False\"\n        case_name = \"lowercased\"\n        opposite_flag = \"True\"\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = \"True\"\n        case_name = \"cased\"\n        opposite_flag = \"False\"\n\n    if is_bad_config:\n        raise ValueError(\n            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n            \"However, `%s` seems to be a %s model, so you \"\n            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n            \"how the model was pre-training. If this error is wrong, please \"\n            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n                                              model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(\"utf-8\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https:\/\/en.wikipedia.org\/wiki\/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter\/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False","0de55dfc":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\n#import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom tensorflow.keras.models import load_model\n\nnp.set_printoptions(suppress=True)","9ef1bcfb":"PATH = '..\/input\/google-quest-challenge\/'\nBERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","4dcae990":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","12ab4aa2":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback","1a742fee":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=5\n\n#outputs = compute_output_arrays(df_train, output_categories)\n#inputs = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","1881b011":"models = []\nfor i in range(5):\n    model_path = f'..\/input\/bertuned-f{i}\/bertuned_f{i}.h5'\n    model = bert_model()\n    model.load_weights(model_path)\n    models.append(model)","dfc9b4cb":"model_path = f'..\/input\/bertf1e15\/Full-0.h5'\nmodel = bert_model()\nmodel.load_weights(model_path)","0a1a8650":"models.append(model)","77fa7c7f":"len(models)","8ea7532f":"test_predictions = []","710019e9":"for model in models:\n    test_predictions.append(model.predict(test_inputs, batch_size=8)) ","6a4e6cac":"test_predictions[i].shape","86f2f299":"final_predictions = np.mean(test_predictions, axis=0)","b43be8d7":"final_predictions.shape","1a9886ac":"df_sub.iloc[:, 1:] = final_predictions\ndf_sub.to_csv('submission.csv', index=False)","d2214d29":"df_sub.head()","96c5c589":"### question_well_written  vs  answer_well_written ","458c0656":"### Distribution of characters in question body & Answer body","34d94bed":"\nWe have a simple bell-shaped normal distribution of the average word length with a mean of around 4.5\n","11e9ddc7":"## Importing Required Libaries","3130f02b":"# part 1 : EDA","82b4477e":"### Getting Basic idea about the data","a9749e4a":"**This kernel is divided into 2 parts**\n\n# part 1 : EDA\n\n# part 2 : Transfer Learning(modeling)","af32f701":"*Using this [kernel](https:\/\/www.kaggle.com\/bibek777\/bert-base-tf2-0-minimalistic-iii\/notebook) i will add my trained models weight *","a5c54b0e":"Reference : [JIGSAW EDA](https:\/\/www.kaggle.com\/gpreda\/jigsaw-eda) \n\n[Jigsaw Competition : EDA and Modeling](https:\/\/www.kaggle.com\/tarunpaparaju\/jigsaw-competition-eda-and-modeling)","1ddbbdda":"### Average Word Length","13cf0a55":"### Lets Plot Feature Distribution ","31365bad":"- hmm,both the distributions are left skewed and almost identical.\n","fedcd0a7":"# part 2 : Transfer Learning(modeling)","f19f9d5f":"### Distribution of stopwords in question & Answer","2aa57588":"### common bigrams in Answer title","12d6f087":"### Distribution of character length in question_title","07edd1ad":"### wordcloud of frequent used words in the question_body.","af7df18c":"*In cell above you can replace question_not_really_a_question with other keywords from targets variable to get exact distribution of that column*","0624c962":"### prevalent words in the train set \n(we will use a 6079 question_body sample and show top 100 words)","4c5248a5":"### Distribution of the number of words in the question_body","11692c14":"\n\nIt looks like we have a unimodal left-skewed distribution of the number of words in the question_body.\n","d937d211":"### frequent used words in question_body for which question_well_written score above 0.6","825496aa":"### Target Features","5b1fbd9b":"### Distribution of the number of words in the Answer","245e6c89":"## What is in this kernel?\nThis kenel is dedicated for doing Exploratory data analysis on Google Q&A competition data.We will be exploring various aspects of the data given which hopefully will be helpful for our fellow kagglers.\n\n<font color=\"Blue\" size=4 >please UPVOTE the kernel if you find it helpful <\/font> ","9f17e2aa":"- Although the lengths seem to be skewed just a bit to the lower lengths.we see another clear peak around the 45-50 character mark.","5cec8d48":"## Ngram analysis","99cf5de8":"### Common bigrams in question title","456f2f97":"Well, as expected majority of questions start or have 'how to' or 'what is' with them.","f7ccfd5a":"**The graphs below are self explanatory,so i won't wspend much time explaining what the graphs below mean**","3be8b7d2":"## Lets see More data distribution"}}