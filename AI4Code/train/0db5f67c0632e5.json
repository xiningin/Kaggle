{"cell_type":{"47a5af4f":"code","3af5f7bf":"code","370321e8":"code","2a0e5712":"code","bc475bc4":"code","86bf5584":"code","b6215e44":"code","3beda17e":"code","01b2be8d":"code","ac8d07e1":"code","48e3893b":"code","1c38310e":"code","04e1aad1":"code","0db05297":"code","d410c2f7":"code","6aade92d":"code","d19efd2f":"code","478112f1":"code","c62c63b9":"code","9aea2805":"code","8349a6bc":"code","157a0414":"code","fabb1ab5":"code","9ca0b704":"code","687845d4":"code","72ef6844":"code","82e06a20":"code","c3198c20":"code","382c2f7c":"code","785fc7df":"code","679d878d":"code","15b6b94a":"code","000081a4":"code","8711a587":"code","9388a25b":"code","8db30c72":"code","abcc6c1b":"code","5a40f297":"code","8abc50bf":"code","69839c70":"code","2e2548aa":"code","58319fef":"code","dae1bbaf":"code","05cc1887":"code","5dd5c1f7":"code","1c27a30a":"code","a5662623":"code","7eb0ddd0":"code","303e643d":"code","5f495fcf":"code","4c417f21":"code","309aa5d2":"markdown","f76bdb33":"markdown","fa15e75d":"markdown","4bbf8c53":"markdown","58b7938c":"markdown","afdb6c8b":"markdown","0e3b0fd0":"markdown","77942aad":"markdown","4703782e":"markdown","262c03ec":"markdown","e039f4f0":"markdown","3c96d9b4":"markdown","27d03cce":"markdown","0ef5a344":"markdown","8cca1141":"markdown","605fc204":"markdown","b89a2654":"markdown","769b0b3b":"markdown","ab0dfb32":"markdown","c7bcab64":"markdown","8c899a3b":"markdown","32c79670":"markdown","30621fe1":"markdown","1686b952":"markdown","06eb3b5f":"markdown","0bc8198a":"markdown","c9e15e93":"markdown","939bedbe":"markdown","cb2b7bd8":"markdown","1df0ff10":"markdown","c95ce2d8":"markdown","d55d3d52":"markdown","aebcb3de":"markdown","ff499f41":"markdown","a6e7617d":"markdown","bc76080d":"markdown","a3f4240c":"markdown","99e14125":"markdown","83254c26":"markdown","362ee7f9":"markdown","d0285b37":"markdown","5d6b86d0":"markdown","725d550d":"markdown","1c939f60":"markdown","df6ca24b":"markdown","9b757a6e":"markdown","46cb8a1c":"markdown","0563c11f":"markdown","d52a08d9":"markdown","d6b9c4b4":"markdown","e0926d78":"markdown","c7ce8485":"markdown","a3b416cb":"markdown","3ddbe719":"markdown","bd134f97":"markdown","3ccf4182":"markdown","6affa03b":"markdown","5d07feae":"markdown","64e1c653":"markdown","a7f0b737":"markdown","1248a88f":"markdown","8b80ba0f":"markdown","fa8d4e86":"markdown","06e7b4fd":"markdown","d95394c7":"markdown","630fe510":"markdown","c8375224":"markdown","2944f6ec":"markdown","f84d17ff":"markdown","048ca41f":"markdown","02d3e28a":"markdown","7391e410":"markdown","8856b24b":"markdown","12c099d4":"markdown","7df8d311":"markdown","2c4e1b0d":"markdown","3579a65f":"markdown","e366cb61":"markdown","1d4c662c":"markdown","17d34076":"markdown","8e3cd5c3":"markdown","3937eb17":"markdown","4e92076b":"markdown","31362fde":"markdown","2ba94a18":"markdown","7b451e13":"markdown","c92256d1":"markdown","8ec3c7d2":"markdown","18e501b5":"markdown","2f80910c":"markdown","c68ffce4":"markdown","003250a8":"markdown","b04d06de":"markdown","ae09f834":"markdown","627612b2":"markdown","dd6204da":"markdown"},"source":{"47a5af4f":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport math as ma\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n\nfrom plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection","3af5f7bf":"data = pd.read_csv('..\/input\/heart.csv')","370321e8":"data.head(5)","2a0e5712":"data.shape","bc475bc4":"data.groupby(\"target\")['age'].count().plot.bar()","86bf5584":"y_value_counts = data['target'].value_counts()\n\nprint(\"Number of people getting heart attack \", y_value_counts[1], \", (\", (y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of not getting heart attack \", y_value_counts[0], \", (\", (y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n","b6215e44":"data.columns.values\n","3beda17e":"data.columns = ['age','sex','chestpain',\n                     'blood_pressure','cholestoral','blood_sugar','ECG','max_heart_rate','angima','oldpeak','slope','coloured_vessels','thal','target']","01b2be8d":"data.head(5)","ac8d07e1":"data.info()","48e3893b":"data['sex'][data['sex'] == 0] = 'Female'\ndata['sex'][data['sex'] == 1] = 'Male'\n\ndata['chestpain'][data['chestpain'] == 0] = 'level_zero'\ndata['chestpain'][data['chestpain'] == 1] = 'level_one'\ndata['chestpain'][data['chestpain'] == 2] = 'level_two'\ndata['chestpain'][data['chestpain'] == 3] = 'level_three'\n\ndata['blood_sugar'][data['blood_sugar'] == 0] = 'low'\ndata['blood_sugar'][data['blood_sugar'] == 1] = 'high'\n\ndata['ECG'][data['ECG'] == 0] = 'normal'\ndata['ECG'][data['ECG'] == 1] = 'wave_abnormality'\ndata['ECG'][data['ECG'] == 2] = 'Estes_criteria'\n\ndata['angima'][data['angima'] == 0] = 'No'\ndata['angima'][data['angima'] == 1] = 'Yes'\n\ndata['slope'][data['slope'] == 0] = 'Upsloping'\ndata['slope'][data['slope'] == 1] = 'Flat'\ndata['slope'][data['slope'] == 2] = 'Downsloping'\n\ndata['thal'][data['thal'] == 0] = 'level_zero'\ndata['thal'][data['thal'] == 1] = 'level_one'\ndata['thal'][data['thal'] == 2] = 'level_two'\ndata['thal'][data['thal'] == 3] = 'level_three'\n\ndata['coloured_vessels'][data['coloured_vessels'] == 0] = 'zero'\ndata['coloured_vessels'][data['coloured_vessels'] == 1] = 'one'\ndata['coloured_vessels'][data['coloured_vessels'] == 2] = 'two'\ndata['coloured_vessels'][data['coloured_vessels'] == 3] = 'three'\ndata['coloured_vessels'][data['coloured_vessels'] == 4] = 'four'\n\ndata.head(5)","1c38310e":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.boxplot(x = 'target', y = 'age', data = data[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['target'] == 0.0]['age'][0:] , label = \"0\", color = 'red')\nsns.distplot(data[data['target'] == 1.0]['age'][0:] , label = \"1\" , color = 'blue' )\nplt.legend()\nplt.show()","04e1aad1":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.boxplot(x = 'target', y = 'blood_pressure', data = data[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['target'] == 0.0]['blood_pressure'][0:] , label = \"0\", color = 'red')\nsns.distplot(data[data['target'] == 1.0]['blood_pressure'][0:] , label = \"1\" , color = 'blue' )\nplt.legend()\nplt.show()","0db05297":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.boxplot(x = 'target', y = 'cholestoral', data = data[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['target'] == 0.0]['cholestoral'][0:] , label = \"0\", color = 'red')\nsns.distplot(data[data['target'] == 1.0]['cholestoral'][0:] , label = \"1\" , color = 'blue' )\nplt.legend()\nplt.show()","d410c2f7":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.boxplot(x = 'target', y = 'max_heart_rate', data = data[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['target'] == 0.0]['max_heart_rate'][0:] , label = \"0\", color = 'red')\nsns.distplot(data[data['target'] == 1.0]['max_heart_rate'][0:] , label = \"1\" , color = 'blue' )\nplt.legend()\nplt.show()","6aade92d":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.boxplot(x = 'target', y = 'oldpeak', data = data[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(data[data['target'] == 0.0]['oldpeak'][0:] , label = \"0\", color = 'red')\nsns.distplot(data[data['target'] == 1.0]['oldpeak'][0:] , label = \"1\" , color = 'blue' )\nplt.legend()\nplt.show()","d19efd2f":"heart_attack = data[data['target']==1]['oldpeak'].values\nno_heart_attack = data[data['target']==0]['oldpeak'].values\n\n\nfrom prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names = [\"Percentile\", \"Heart Attack\", \"No Heart attack\"]\n\nfor i in range(0,101,5):\n    x.add_row([i,np.round(np.percentile(heart_attack,i), 3), np.round(np.percentile(no_heart_attack,i), 3)])\nprint(x)","478112f1":"def stack_plot(data, xtick, col2='heart attack cases', col3='total cases'):\n    ind = np.arange(data.shape[0])\n    \n    plt.figure(figsize=(10,5))\n    p1 = plt.bar(ind, data[col3].values)\n    p2 = plt.bar(ind, data[col2].values)\n\n    plt.ylabel('Number of people')\n    plt.title('Heart Attack Vs No Heart Attack')\n    plt.xticks(ind, list(data[xtick].values))\n    plt.legend((p1[0], p2[0]), ('total cases', 'heart attack cases'))\n    plt.show()","c62c63b9":"def univariate_barplots(data, col1, col2='target', top=False):\n    # Count number of zeros in dataframe python: https:\/\/stackoverflow.com\/a\/51540521\/4084039\n    temp = pd.DataFrame(data.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()\n   \n    # Pandas dataframe grouby count: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n    \n    temp['total'] = pd.DataFrame(data.groupby(col1)[col2].agg({'total':'count'})).reset_index()['total']\n\n    temp['Avg'] = pd.DataFrame(data.groupby(col1)[col2].agg({'Avg':'mean'})).reset_index()['Avg']\n    \n\n    temp.sort_values(by=['total'],inplace=True, ascending=False)\n    \n    if top:\n        temp = temp[0:top]\n    \n    stack_plot(temp, xtick=col1, col2=col2, col3='total')\n    print(temp.head(5))\n","9aea2805":"univariate_barplots(data, 'sex', 'target' , top=False)","8349a6bc":"univariate_barplots(data, 'chestpain', 'target' , top=False)","157a0414":"univariate_barplots(data, 'blood_sugar', 'target' , top=False)","fabb1ab5":"univariate_barplots(data, 'ECG', 'target' , top=False)","9ca0b704":"univariate_barplots(data, 'angima', 'target' , top=False)","687845d4":"univariate_barplots(data, 'slope', 'target' , top=False)","72ef6844":"univariate_barplots(data, 'coloured_vessels', 'target' , top=False)","82e06a20":"univariate_barplots(data, 'thal', 'target' , top=False)","c3198c20":"y = data['target']\ndata.drop(['target'], axis=1, inplace=True)\nX=data\nprint(X.shape)\nprint(y.shape)","382c2f7c":"#Split the data into train and test\n\nX_tr, X_test, y_tr, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)","785fc7df":"print(X_tr.shape)\nprint(y_tr.shape)\nprint(X_test.shape)\nprint(y_test.shape)","679d878d":"from sklearn.preprocessing import StandardScaler\nage_scalar = StandardScaler(with_mean=False)\nage_scalar.fit(X_tr['age'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {age_scalar.mean_[0]}, Standard deviation : {np.sqrt(age_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nTr_age_standardized = age_scalar.transform(X_tr['age'].values.reshape(-1, 1))\nTest_age_standardized = age_scalar.transform(X_test['age'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standarsation\")\nprint(Tr_age_standardized.shape)\nprint(Test_age_standardized.shape)","15b6b94a":"blood_pressure_scalar = StandardScaler(with_mean=False)\nblood_pressure_scalar.fit(X_tr['age'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {blood_pressure_scalar.mean_[0]}, Standard deviation : {np.sqrt(blood_pressure_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nTr_blood_pressure_standardized = blood_pressure_scalar.transform(X_tr['blood_pressure'].values.reshape(-1, 1))\nTest_blood_pressure_standardized = blood_pressure_scalar.transform(X_test['blood_pressure'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standarsation\")\nprint(Tr_blood_pressure_standardized.shape)\nprint(Test_blood_pressure_standardized.shape)","000081a4":"cholestoral_scalar = StandardScaler(with_mean=False)\ncholestoral_scalar.fit(X_tr['age'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {cholestoral_scalar.mean_[0]}, Standard deviation : {np.sqrt(cholestoral_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nTr_cholestoral_standardized = cholestoral_scalar.transform(X_tr['cholestoral'].values.reshape(-1, 1))\nTest_cholestoral_standardized = cholestoral_scalar.transform(X_test['cholestoral'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standarsation\")\nprint(Tr_cholestoral_standardized.shape)\nprint(Test_cholestoral_standardized.shape)","8711a587":"max_heart_rate_scalar = StandardScaler(with_mean=False)\nmax_heart_rate_scalar.fit(X_tr['age'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {max_heart_rate_scalar.mean_[0]}, Standard deviation : {np.sqrt(max_heart_rate_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nTr_max_heart_rate_standardized = cholestoral_scalar.transform(X_tr['max_heart_rate'].values.reshape(-1, 1))\nTest_max_heart_rate_standardized = cholestoral_scalar.transform(X_test['max_heart_rate'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standarsation\")\nprint(Tr_max_heart_rate_standardized.shape)\nprint(Test_max_heart_rate_standardized.shape)","9388a25b":"oldpeak_scalar = StandardScaler(with_mean=False)\noldpeak_scalar.fit(X_tr['oldpeak'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {oldpeak_scalar.mean_[0]}, Standard deviation : {np.sqrt(oldpeak_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nTr_oldpeak_standardized = cholestoral_scalar.transform(X_tr['oldpeak'].values.reshape(-1, 1))\nTest_oldpeak_standardized = cholestoral_scalar.transform(X_test['oldpeak'].values.reshape(-1, 1))\n\nprint(\"Shape of matrix after standarsation\")\nprint(Tr_oldpeak_standardized.shape)\nprint(Test_oldpeak_standardized.shape)","8db30c72":"def One_hot_encoding_tr(col):\n    my_counter = Counter()\n    for word in col.values:\n        my_counter.update(word.split())\n\n    col_dict = dict(my_counter)\n    sorted_col_dict = dict(sorted(col_dict.items(), key=lambda kv: kv[1])) # sort categories in desc order as a dictionary\n\n    from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer(vocabulary=list(sorted_col_dict.keys()), lowercase=False, binary=True)\n    vectorizer.fit(col.values)\n    print(vectorizer.get_feature_names())\n    \n    Tr_col_one_hot = vectorizer.transform(col.values)\n    return Tr_col_one_hot\n\ndef One_hot_encoding_test(col,test_col):\n    my_counter = Counter()\n    for word in col.values:\n        my_counter.update(word.split())\n\n    col_dict = dict(my_counter)\n    sorted_col_dict = dict(sorted(col_dict.items(), key=lambda kv: kv[1])) # sort categories in desc order as a dictionary\n\n    from sklearn.feature_extraction.text import CountVectorizer\n    vectorizer = CountVectorizer(vocabulary=list(sorted_col_dict.keys()), lowercase=False, binary=True)\n    vectorizer.fit(col.values)\n    print(vectorizer.get_feature_names())\n    \n    Tr_col_one_hot = vectorizer.transform(col.values)\n    Test_col_one_hot = vectorizer.transform(test_col.values)\n    return Test_col_one_hot\n\n\n","abcc6c1b":"Tr_sex_one_hot=One_hot_encoding_tr(X_tr['sex'])\nTest_sex_one_hot=One_hot_encoding_test(X_tr['sex'],X_test['sex'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_sex_one_hot.shape)\nprint(Test_sex_one_hot.shape)","5a40f297":"Tr_chestpain_one_hot=One_hot_encoding_tr(X_tr['chestpain'])\nTest_chestpain_one_hot=One_hot_encoding_test(X_tr['chestpain'],X_test['chestpain'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_chestpain_one_hot.shape)\nprint(Test_chestpain_one_hot.shape)","8abc50bf":"Tr_blood_sugar_one_hot=One_hot_encoding_tr(X_tr['blood_sugar'])\nTest_blood_sugar_one_hot=One_hot_encoding_test(X_tr['blood_sugar'],X_test['blood_sugar'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_blood_sugar_one_hot.shape)\nprint(Test_blood_sugar_one_hot.shape)","69839c70":"Tr_ECG_one_hot=One_hot_encoding_tr(X_tr['ECG'])\nTest_ECG_one_hot=One_hot_encoding_test(X_tr['ECG'],X_test['ECG'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_ECG_one_hot.shape)\nprint(Test_ECG_one_hot.shape)","2e2548aa":"Tr_angima_one_hot=One_hot_encoding_tr(X_tr['angima'])\nTest_angima_one_hot=One_hot_encoding_test(X_tr['angima'],X_test['angima'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_angima_one_hot.shape)\nprint(Test_angima_one_hot.shape)","58319fef":"Tr_slope_one_hot=One_hot_encoding_tr(X_tr['slope'])\nTest_slope_one_hot=One_hot_encoding_test(X_tr['slope'],X_test['slope'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_slope_one_hot.shape)\nprint(Test_slope_one_hot.shape)","dae1bbaf":"Tr_coloured_vessels_one_hot=One_hot_encoding_tr(X_tr['coloured_vessels'])\nTest_coloured_vessels_one_hot=One_hot_encoding_test(X_tr['coloured_vessels'],X_test['coloured_vessels'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_coloured_vessels_one_hot.shape)\nprint(Test_coloured_vessels_one_hot.shape)","05cc1887":"Tr_thal_one_hot=One_hot_encoding_tr(X_tr['thal'])\nTest_thal_one_hot=One_hot_encoding_test(X_tr['thal'],X_test['thal'])\nprint(\"Shape of matrix after one hot encoding\")\nprint(Tr_thal_one_hot.shape)\nprint(Test_thal_one_hot.shape)","5dd5c1f7":"from scipy.sparse import hstack\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import linear_model\nimport pylab\nfrom matplotlib.pyplot import figure\n##https:\/\/stackoverflow.com\/questions\/3899980\/how-to-change-the-font-size-on-a-matplotlib-plot\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 12}\n\nplt.rc('font', **font)","1c27a30a":"train = hstack((Tr_age_standardized,Tr_blood_pressure_standardized,Tr_cholestoral_standardized,Tr_chestpain_one_hot,Tr_blood_sugar_one_hot,Tr_ECG_one_hot,Tr_angima_one_hot,\n            Tr_slope_one_hot,Tr_coloured_vessels_one_hot,Tr_thal_one_hot,Tr_max_heart_rate_standardized,Tr_oldpeak_standardized,Tr_sex_one_hot\n             \n           )) \n\ntest = hstack((Test_age_standardized,Test_blood_pressure_standardized,Test_cholestoral_standardized,Test_chestpain_one_hot,Test_blood_sugar_one_hot,Test_ECG_one_hot,Test_angima_one_hot,\n            Test_slope_one_hot,Test_coloured_vessels_one_hot,Test_thal_one_hot,Test_max_heart_rate_standardized,Test_oldpeak_standardized,Test_sex_one_hot\n             \n           )) \nprint(train.shape)\nprint(test.shape)","a5662623":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n\nlamda=list()\ntest_auc=list()\ntraining_auc=list()\ncv_auc=list()\n\niteration=np.arange(1,10)\nfor i in range(1,10):\n\n    k=[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n\n    param_grid = {'alpha':[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n\n    #Hyper parameter Tuning for lamda with 3 fold cross validation\n    clf = linear_model.SGDClassifier(loss='log')\n\n    clf_cv= GridSearchCV(clf,param_grid,cv=3,scoring='roc_auc')\n\n    clf_cv.fit(train.toarray(),y_tr)\n    cv_auc.append(clf_cv.best_score_)\n    \n    #Instantiate Classifier\n    clf = linear_model.SGDClassifier(loss='log',alpha=clf_cv.best_params_.get('alpha'))\n    clf.fit(train.toarray(),y_tr)\n    y_pred_proba_test = clf.predict_proba(test.toarray())[:,1]\n    y_pred_proba_tr = clf.predict_proba(train.toarray())[:,1]\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n    fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_tr, y_pred_proba_tr)\n   \n    test_auc.append(roc_auc_score(y_test,y_pred_proba_test))\n    training_auc.append(roc_auc_score(y_tr,y_pred_proba_tr))\n    lamda.append(clf_cv.best_params_.get('alpha'))\n\nfigure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\npylab.subplot(2,2,1)    \npylab.plot(iteration, training_auc, '-b', label='Training AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Training AUC')\npylab.grid()\n\n\npylab.subplot(2,2,2)\npylab.plot(iteration, test_auc, '-b', label='Test AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('test AUC')\npylab.grid()\n\n\npylab.subplot(2,2,3)\npylab.plot(iteration, cv_auc, '-b', label='CV AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('CV AUC')\npylab.grid()\n\npylab.subplot(2,2,4)\npylab.plot(iteration, lamda, '-b', label='Lambda',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Lambda')\npylab.yscale(\"log\")\npylab.grid()\npylab.show()\n\n","7eb0ddd0":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\nfrom sklearn.neighbors import KNeighborsClassifier\nk_nn=list()\ntest_auc=list()\ntraining_auc=list()\ncv_auc=list()\n\niteration=np.arange(1,10)\n\nk=np.arange(1,70,5)\nparam_grid = {'n_neighbors':np.arange(1,70,5)}\n\nfor i in range(1,10):\n\n\n    #Hyper parameter Tuning for lamda with 3 fold cross validation\n    clf = KNeighborsClassifier(algorithm='brute')\n\n    clf_cv= GridSearchCV(clf,param_grid,cv=3,scoring='roc_auc')\n\n    clf_cv.fit(train.toarray(),y_tr)\n    cv_auc.append(clf_cv.best_score_)\n    \n    #Instantiate Classifier\n    clf = KNeighborsClassifier(n_neighbors=clf_cv.best_params_.get('n_neighbors'),algorithm='brute')\n    clf.fit(train.toarray(),y_tr)\n    y_pred_proba_test = clf.predict_proba(test.toarray())[:,1]\n    y_pred_proba_tr = clf.predict_proba(train.toarray())[:,1]\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n    fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_tr, y_pred_proba_tr)\n   \n    test_auc.append(roc_auc_score(y_test,y_pred_proba_test))\n    training_auc.append(roc_auc_score(y_tr,y_pred_proba_tr))\n    k_nn.append(clf_cv.best_params_.get('n_neighbors'))\n    \nfigure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\npylab.subplot(2,2,1)    \npylab.plot(iteration, training_auc, '-b', label='Training AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Training AUC')\npylab.grid()\n\n\npylab.subplot(2,2,2)\npylab.plot(iteration, test_auc, '-b', label='Test AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('test AUC')\npylab.grid()\n\n\npylab.subplot(2,2,3)\npylab.plot(iteration, cv_auc, '-b', label='CV AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('CV AUC')\npylab.grid()\n\npylab.subplot(2,2,4)\npylab.plot(iteration, k_nn, '-b', label='Number of nearest neighbours',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Number of nearest neighbours')\npylab.grid()\npylab.show()","303e643d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\nfrom sklearn.naive_bayes import MultinomialNB\n\nalpha_list=list()\ntest_auc=list()\ntraining_auc=list()\ncv_auc=list()\n\niteration=np.arange(1,10)\n\nparam_grid = {'alpha':[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n\nfor i in range(1,10):\n\n\n    #Hyper parameter Tuning for lamda with 3 fold cross validation\n    clf = MultinomialNB()\n\n    clf_cv= GridSearchCV(clf,param_grid,cv=3,scoring='roc_auc')\n\n    clf_cv.fit(train.toarray(),y_tr)\n    cv_auc.append(clf_cv.best_score_)\n    \n    #Instantiate Classifier\n    clf = MultinomialNB(alpha=clf_cv.best_params_.get('alpha'))\n    clf.fit(train.toarray(),y_tr)\n    y_pred_proba_test = clf.predict_proba(test.toarray())[:,1]\n    y_pred_proba_tr = clf.predict_proba(train.toarray())[:,1]\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n    fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_tr, y_pred_proba_tr)\n   \n    test_auc.append(roc_auc_score(y_test,y_pred_proba_test))\n    training_auc.append(roc_auc_score(y_tr,y_pred_proba_tr))\n    alpha_list.append(clf_cv.best_params_.get('alpha'))\n    \nfigure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\npylab.subplot(2,2,1)    \npylab.plot(iteration, training_auc, '-b', label='Training AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Training AUC')\npylab.grid()\n\n\npylab.subplot(2,2,2)\npylab.plot(iteration, test_auc, '-b', label='Test AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('test AUC')\npylab.grid()\n\n\npylab.subplot(2,2,3)\npylab.plot(iteration, cv_auc, '-b', label='CV AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('CV AUC')\npylab.grid()\n\npylab.subplot(2,2,4)\npylab.plot(iteration, alpha_list, '-b', label='Alpha',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('alpha')\npylab.grid()\npylab.show()\n    \n","5f495fcf":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\nalpha_list=list()\ntest_auc=list()\ntraining_auc=list()\ncv_auc=list()\n\niteration=np.arange(1,10)\n\nparam_grid = {'alpha':[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n\nfor i in range(1,10):\n\n\n    #Hyper parameter Tuning for lamda with 3 fold cross validation\n    clf = linear_model.SGDClassifier(loss='hinge')\n\n    clf_cv= GridSearchCV(clf,param_grid,cv=3,scoring='roc_auc')\n\n    clf_cv.fit(train.toarray(),y_tr)\n    \n    cv_auc.append(clf_cv.best_score_)\n    \n    #Instantiate Classifier\n    clf = linear_model.SGDClassifier(loss='hinge',alpha=clf_cv.best_params_.get('alpha'))\n    calibrated = CalibratedClassifierCV(clf, method='sigmoid', cv=5)\n    calibrated.fit(train.toarray(),y_tr)\n\n    y_pred_proba_test = calibrated.predict_proba(test.toarray())[:, 1]\n    y_pred_proba_tr = calibrated.predict_proba(train.toarray())[:, 1]\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n    fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_tr, y_pred_proba_tr)\n   \n    test_auc.append(roc_auc_score(y_test,y_pred_proba_test))\n    training_auc.append(roc_auc_score(y_tr,y_pred_proba_tr))\n    alpha_list.append(clf_cv.best_params_.get('alpha'))\n    \n\nfigure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\npylab.subplot(2,2,1)\npylab.plot(iteration, training_auc, '-b', label='Training AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Training AUC')\npylab.grid()\n\n\n\npylab.subplot(2,2,2)\npylab.plot(iteration, test_auc, '-b', label='Test AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('test AUC')\npylab.grid()\n\n\npylab.subplot(2,2,3)\npylab.plot(iteration, cv_auc, '-b', label='CV AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('CV AUC')\npylab.grid()\n\n\npylab.subplot(2,2,4)\npylab.plot(iteration, alpha_list, '-b', label='Alpha',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('alpha')\npylab.yscale(\"log\")\npylab.grid()\npylab.show()\n\n","4c417f21":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n\nfrom sklearn.svm import SVC\nc_list=list()\ngamma_list=list()\ntest_auc=list()\ntraining_auc=list()\ncv_auc=list()\n\niteration=np.arange(1,10)\n\nparam_grid = {'C':[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000],\n              'gamma':[ 0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\n\nfor i in range(1,10):\n\n\n    #Hyper parameter Tuning for lamda with 3 fold cross validation\n    clf = SVC(kernel='rbf',probability=True)\n\n    clf_cv= GridSearchCV(clf,param_grid,cv=3,scoring='roc_auc')\n\n    clf_cv.fit(train.toarray(),y_tr)\n    cv_auc.append(clf_cv.best_score_)\n    \n    #Instantiate Classifier\n    clf = SVC(kernel='rbf', gamma=clf_cv.best_params_.get('gamma'), C=clf_cv.best_params_.get('C'),probability=True)\n    clf.fit(train.toarray(),y_tr)\n    y_pred_proba_test = clf.predict_proba(test.toarray())[:,1]\n    y_pred_proba_tr = clf.predict_proba(train.toarray())[:,1]\n    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_pred_proba_test)\n    fpr_tr, tpr_tr, thresholds_tr = roc_curve(y_tr, y_pred_proba_tr)\n   \n    test_auc.append(roc_auc_score(y_test,y_pred_proba_test))\n    training_auc.append(roc_auc_score(y_tr,y_pred_proba_tr))\n    c_list.append(clf_cv.best_params_.get('C'))\n    gamma_list.append(clf_cv.best_params_.get('gamma'))\n    \nfigure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\npylab.subplot(3,2,1)    \npylab.plot(iteration, training_auc, '-b', label='Training AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('Training AUC')\npylab.grid()\n\n\npylab.subplot(3,2,2)\npylab.plot(iteration, test_auc, '-b', label='Test AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('test AUC')\npylab.grid()\n\n\npylab.subplot(3,2,3)\npylab.plot(iteration, cv_auc, '-b', label='CV AUC',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('CV AUC')\npylab.grid()\n\npylab.subplot(3,2,4)\npylab.plot(iteration, c_list, '-b', label='C',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('c')\npylab.grid()\n\n\npylab.subplot(3,2,5)\npylab.plot(iteration, gamma_list, '-b', label='gamma',linewidth=2.0,marker='o')\npylab.legend(loc='upper right')\npylab.xlabel('Execution number')\npylab.ylabel('gamma')\npylab.grid()\npylab.show()\n    \n\n","309aa5d2":"#### slope","f76bdb33":"#### Oldpeak","fa15e75d":"#### coloured_vessels","4bbf8c53":"So Let's begin by loading some libraries","58b7938c":"## 3. Naive Bayes","afdb6c8b":"#### Chestpain","0e3b0fd0":"#### coloured_vessels","77942aad":"Out of total people who had level two thalassemia had heartattack.","4703782e":"We see that all the features have numerals as their column values. However, one should carefully understand that most of the features are actually categorical features as their values keep repeating. Most of them are ordinal categorical features having a clear order in the categories. For example chestpain feature has 4 values indicating pain levels from 0 to 3. I am changing them so that they are more apparent categorical features. This will help me diferentiate the numerical and categorical features for further analysis. Additionally, I will show one-hot encoding for categorical features in the coming sections for those who are iterested. ","262c03ec":"#### Maximum heart rate","e039f4f0":"Now the data looks better so we can proceed to do some exploratory data analysis for understanding the features in the data","3c96d9b4":"#### ECG","27d03cce":"#### Chestpain","0ef5a344":"In the coming sections, I will execute each model 10 times and store the fitting parameters and score from the model in lists and later I will plot them. The iteration number in the x-axis of plots indicates the execution number of the model. For example, execution number=3 means 3rd time the model is executed.","8cca1141":"Now lets see how categorical features help in performing the classification","605fc204":" Before executing any machine learning algorithm, we should perform scaling of the numerical feature. Scaling in simple terms means bringing all the features to the same level. It is a crucial step because if the numerical features have different scales with one feature having large values and nother features having small values, then their effects on the algorithm will be different. There are some algorithms that can negate the effect of scaling. Nevertheless, it is a good practice to always perform scaling on the numerical features for every machine learning algorithm.  Here I will be doing the standardization which is also called \"mean centering and scaling\".  For standardization, for a given feature or column in the training data, we calculate its mean and standard deviation and then the column values are changed as","b89a2654":"Looking at the 75th percentile, 75% people who had heart attack had old peak less than 1.","769b0b3b":"Now we apply classification algorithm on this small dataset of 303 rows. One simple trick to check the stability of these models is to run each model multiple times and note if the parameters of the model vary a lot. If the training score, test score and fitting parameters of model have big differences in their values during each execution of the model that indicates that the model is unstable and can be overfitting. ","ab0dfb32":"Working with small datasets can be tricky as models can easily overfit. One must be careful in selecting the correct model for a given dataset and not fall for the lucky fit guess by an unstable model.","c7bcab64":"Similarly, we see that naive bayes is also a stable model on this dataset giving a high test score of nearly 0.92","8c899a3b":"Therefore below I am splitting the data only as training and test and I will use GridSearchCV to perform k-fold cross validation.","32c79670":"So, far we have noticed that the linear models such as logistic regression and linear SVM are unstable for this small dataset. The reason could lie in the non-linearity of the dataset. Linear models assume that linear a surface such as a hyperplane will be able to seperate the two classes of data. This may not be true in this dataset. \n\nWe can test this by running a non-linear SVM model using RBF kernel. If we are alble to stabilise our results that means that indeed linearity is the cause. ","30621fe1":"### 1. Numerical features","1686b952":"It should be noted for a given feature in the test data, we should do the feature scale as below","06eb3b5f":"#### cholestoral","0bc8198a":"![image.png](attachment:image.png)","c9e15e93":"Above, we see that non linear SVM with RBF kernel is stable for this dataset giving a high test AUC score of around 0.92 nearly same as Naive Bayes algorithm. ","939bedbe":"# Standardization of Numerical features","cb2b7bd8":"#### Use of K-fold cross validation on small data set.","1df0ff10":"#### thal","c95ce2d8":"# Splitting of Data for test and Training","d55d3d52":"The first important step that I want to mention which will help us in getting around with small data sets is to use K- fold cross validation instead of simple validation. In k-fold cross validation we **do not** explicitly keep aside a part of the data to perform validation. We only separate the whole data into two sets that is training set and test set. Quoting from the Wikipedia \u201cThe training data is randomly sampled into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for evaluating the model, and the remaining k \u2212 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation\u201d. Therefore, the advantage of this method is that we are able to use more data points for training.","aebcb3de":"#### angima","ff499f41":"#### Cholestrol","a6e7617d":"Checking for Null or missing values","bc76080d":"Note that we have used the mean and standard deviation for the **training data** for a given feature of test data set. This is essential to avoid data leakage. Test data is like the unseen data we will encounter after deployment of our model. Therefore do adapt our models for unseen data we do not use any attribute of test data during training phase.","a3f4240c":"In the plots above, we clearly see that the logistic regression model is not stable for this dataset. We see that even the training score varies between 0.65 to 0.9 indicating a high variance in this model or overfitting. Therefore even after performing hyperparameter tuning, we see that we can still overfit our models when we have small datasets. Similarly, lamda which is the regularisation parameter, varies a lot for each execuation of the model. if we had run our model only once and if we had acheived a high score we would have concluded that logistic regression works very well for this model. This is a common mistake. **Therefore we should not rely on a lucky guess in case of small datasets**.\nLogistic regression is often a first choice of model for many machine learning engineers and is considered a flexible model however with this data set, logistic regression is not a good choice.","99e14125":"This is a fairly balanced dataset.","83254c26":"# Data Loading","362ee7f9":"Running the model multiple times is affordable in case of small datasets as the time complexity is not very high","d0285b37":"The golden rule of selecting which model performs best with a given dataset is that there is no such rule. The performance of models is reliant on the dataset itself and one model can outperform others for a different dataset.\nIn this kernel, I will use the heart disease data to describe how to work around the problems of small data. We will try to find out which classification models are unstable for this dataset and how to avoid a lucky good fit from these models.\n","5d6b86d0":"![image.png](attachment:image.png)","725d550d":"#### thal","1c939f60":"## Univariate Analysis","df6ca24b":"Old peak is also an important feature for classification as seen in the level difference of box plots and seperation in the pdfs. It can also be noted that most of people who had heart attack had an old peak value of less than 1. To give the exact numbers I will list down bellow the percentiles for this feature.","9b757a6e":"Blood sugar does not give much information in the classification as nearly 50% of people had attack for both high or low sugar categories. Also we do not have enough data for high blood sugar category to actually make comparision.","46cb8a1c":"We see that the two pdfs on the right curve have a slight seperation. This can also be seen in the box plot on the left. Typically, if one box plot is much higher or lower than another, that indicates that the feature will be important in the classification","0563c11f":"Here I want to check if feature \"age\"  is able to seperate the two classes.","d52a08d9":"Combining all features together using hstack.","d6b9c4b4":"#### oldpeak","e0926d78":"![image.png](attachment:image.png)","c7ce8485":"#### sex","a3b416cb":"For the blood pressure, we see that the pdfs are mostly overlapping and similarly the box plot medians (middle line of the box) have the same level. Therefore this feature might not be very useful in classification.","3ddbe719":"70% of people who did not have exercise induced angima had heartattack","bd134f97":"Here, we should be careful in concluding that majority of people who had heart attack had zero coloured vessels in flouroscopy as majority of data we have has only this category","3ccf4182":"#### Age ","6affa03b":"I am using simple AUC score for evaluating the model as this is a balanced dataset thus AUC score should not be affected.","5d07feae":"## 2. KNN","64e1c653":"Out of the people who had zero level of chest pain only 27% had heart attack as compared to 80% of people who had level two chest pain","a7f0b737":"Now we perform one hot encoding of the categorical features","1248a88f":"## 1. Logistic Regression","8b80ba0f":"# Conclusion","fa8d4e86":"#### angima","06e7b4fd":"I am changing the names of the cloumn below so that I can have good understanding of the features later when I do Exploratory Data Analysis","d95394c7":"# Data Cleaning","630fe510":"Here, we see that score and parmeter of linear SVM also varry while running the model multiple times. However, if we have a look at the scale of the test AUC score we see that this variation is not as much as we saw in logistic regression","c8375224":"#### slope","2944f6ec":"#### sex","f84d17ff":"## 5 Non-linear SVM","048ca41f":"I will perform univariate analysis i.e taking single features and understanding how they help in the classfication task of separating two classes (target 0: no heart attack and target 1: heart attack).","02d3e28a":"#### blood_sugar","7391e410":"For cholestrol, there is only a sight gap in the box plots","8856b24b":"# Understanding the Data","12c099d4":"For the numerical features we can plot pdf and box plots to understand their contribution in the classification task","7df8d311":"This is a very small data. In the sections below we will try to figure out ways to detect unstable model for this data. But before that, lets try to understand the data itself. Lets find out if this a balanced data or not.","2c4e1b0d":"For optimal performance, it is essential to provide a large number of data points to the Machine Learning Models during the training stage. More data points signify more information and knowledge for the model. Consequently, the model can generate a good fit with more confidence. \nHowever, in certain situations such as Medical applications, many a time a data scientist or a machine learning engineer is assigned a small dataset to work with. When the dataset is small, the model can become unstable. The definition of an unstable model is **the one that is unable to give a robust constant result each time the model is executed**. This can also be interpreted as overfitting. That means that the model tries to overfit around whatever a small number of data points it is provided with. As a result, it loses its robustness and can give different results each time it is executed. We cannot trust the output of such an unstable model. \n","3579a65f":"Here we can conclude that 75% of total women in the dataset had heartattack as compared to 45% of total men. Even when we have less number of data for women, we see that this category dominates for class 1.","e366cb61":"Most of personnels (75%) which had downslopping in their peak exercise ST segment had heart attack.","1d4c662c":"### 2. Categorical features","17d34076":"For example, let\u2019s assume one model gives a training AUC score of 0.88 and test AUC score of 0.85 in one run and then in another run it gives a training AUC of 0.65 and test AUC of 0.63 that means the model is not stable. As we will see below that this is indeed the case for several models on this small dataset.","8e3cd5c3":"#### Checking robostness of models for small data sets by executing the same model multiple times","3937eb17":"# Exploratory Data Analysis","4e92076b":"Maximum heart rate is an important feature for classification as seen in the level difference of box plots and less overlap in the pdfs","31362fde":"There are no missing or null values as all the column entrees have 303 non null values","2ba94a18":"# Applying classification Models","7b451e13":"However, it can so happen with small data sets that during the first execution itself the model produces a good fit showing high accuracy score for both training and test phase. Many times, beginners or even experienced machine learning engineers do this make of taking this high accuracy score as the result of the model and declaring it a \u201cwell-performing model\u201d for the given dataset. However, it should be carefully noted that given a small dataset, this result can just be a **\u201clucky good fit\u201d** of an unstable model performing overfitting (see the image above). ","c92256d1":"## 4. Linear Support Vector Machine (SVM)","8ec3c7d2":"#### max_heart_rate","18e501b5":"For categories of people which show wave abnormality in their ECG around 63% people had heartattack as compared to 46% of people who had normal ECG. Since we have very few data points for estes criteria we cannot really conclude anything on that","2f80910c":"#### Blood Pressure","c68ffce4":"Here we see that KNN is a stable model for this data set because the score and parameter value remain constant for this model. It gives a test score of nearly 0.82.","003250a8":"#### blood_sugar","b04d06de":"# One hot encoding of categorical features","ae09f834":"#### ECG","627612b2":"#### blood_pressure","dd6204da":"#### age"}}