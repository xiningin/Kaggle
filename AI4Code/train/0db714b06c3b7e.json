{"cell_type":{"f9a650f8":"code","3767070a":"code","79aeecf0":"code","62fb6367":"code","f67719e7":"code","92c30fc0":"code","753e3536":"code","900553b1":"code","b249abeb":"code","6f0063c4":"code","a1362a3c":"code","132def22":"code","559cbe75":"code","7526535f":"markdown","1d9dc3e6":"markdown","ded9eee8":"markdown","78603b48":"markdown","3e33582a":"markdown","226f30a3":"markdown","013eab79":"markdown","1d0c8f9a":"markdown","c9dedce1":"markdown","265d6c40":"markdown","362cf758":"markdown","28d6ceba":"markdown","7d50b5a9":"markdown"},"source":{"f9a650f8":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Advance mathematics\nimport scipy as sp\n\n# Pretty print dataframes\nimport IPython\nfrom IPython import display\n\n# Misc\nimport random\nimport time","3767070a":"# Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Model Utilities\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Data Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n# Data Visualization Backend\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","79aeecf0":"data_train = pd.read_csv('..\/input\/train.csv')\ndf_val  = pd.read_csv('..\/input\/test.csv')\n\n# Copy and use them in an list to clean both at the same time\ndf_train = data_train.copy(deep = True)\ndfs = [df_train, df_val]\n\n# Preview of data\ndf_train.head()","62fb6367":"print('Train columns with null values:\\n', df_train.isnull().sum())\nprint('--' * 15)\nprint('--' * 15)\n\nprint('Test columns with null values:\\n', df_val.isnull().sum())\nprint('--' * 15)\nprint('--' * 15)","f67719e7":"# We know what we need to clean (or fill) so we do it in both dataframes\nfor dataset in dfs:\n    \n    # Complete numerical missing values with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n    # Complete alphabetical missing values with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n# Drop this columns because PassengerId and Ticket does not gives relevant info\n# about the passenger.\n# The Cabin could be usefull to know the position of the passenger cabin\n# in the boat but it has too much missing values so we drop it in the train data.\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndf_train.drop(drop_column, axis=1, inplace = True)","92c30fc0":"# Check again the missing values in the datasets\nprint('Train columns with null values:\\n', df_train.isnull().sum())\nprint('--' * 15)\nprint('--' * 15)\n\nprint('Test columns with null values:\\n', df_val.isnull().sum())\nprint('--' * 15)\nprint('--' * 15)","753e3536":"# Let's try to extract information to create new features\nfor dataset in dfs:\n    \n    # Family size may be relevant\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    \n    # Passenger is alone or not, depending on the family size\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    \n    # Cut the Fare and the Age in intervals to better understanding\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n    # Get the title of the passenger\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n# Using the common minimial for stats\nstat_min = 10\n\n# Ger rid of strange Titles (count below the minimum)\ntitle_names = (df_train['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\ndf_train['Title'] = df_train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n# Check the data\ndf_train.info()\ndf_train.head()","900553b1":"label = LabelEncoder()\n\nfor dataset in dfs:\n    \n    # Convert to dummy variables those columns with few different values\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\ninput_columns = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ntarget = ['Survived']","b249abeb":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n# Run the models 10x times with train\/val of 60\/30 leaving out the 10%\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )\n\n# Table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# Table to compare MLA predictions vs the expected output\nMLA_predict = df_train[target]\n\nrow_index = 0\nfor alg in MLA:\n\n    # Name of the MLA\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    \n    # Cross Validation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html\n    cv_results = model_selection.cross_validate(alg, df_train[input_columns], df_train[target], cv  = cv_split, return_train_score = True)\n\n    # Store the results in the table\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n    \n    # Save the prediction in the corresponding table\n    alg.fit(df_train[input_columns], df_train[target])\n    MLA_predict[MLA_name] = alg.predict(df_train[input_columns])\n    \n    row_index+=1\n\n# Sort results by descending Test Score\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","6f0063c4":"# Base Tree\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, df_train[input_columns],df_train[target], cv  = cv_split, return_train_score = True)\ndtree.fit(df_train[input_columns], df_train[target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n# Parameters to search best estimator like criterion gini (by default),max depth of the tree and random seed\nparam_grid = {'criterion': ['gini', 'entropy'], 'max_depth': [2,4,6,8,10,None], 'random_state': [0]}\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\n\n# Fit the data in the new estimator\ntune_model.fit(df_train[input_columns], df_train[target])\n\nprint('AFTER DT Parameters: ', tune_model.best_params_)\nprint(\"AFTER DT Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w\/bin score 3*std: +\/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))","a1362a3c":"# Base Data & Model Score\nprint('BEFORE DT RFE Training Shape Old: ', df_train[input_columns].shape) \nprint('BEFORE DT RFE Training Columns Old: ', df_train[input_columns].columns.values)\nprint(\"BEFORE DT RFE Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n# Feature Selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy')\ndtree_rfe.fit(df_train[input_columns], df_train[target])\n\n# Reduce features with the most relevant columns\nX_rfe = df_train[input_columns].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, df_train[X_rfe], df_train[target], cv  = cv_split, return_train_score = True)\n\n# Data & Model Score after feature selection with base model\nprint('AFTER DT RFE Training Shape New: ', df_train[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\nprint(\"AFTER DT RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n# Tune model with the new features\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\nrfe_tune_model.fit(df_train[X_rfe], df_train[target])\n\n# Model Hyper-Params and scores after Tunning\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\nprint(\"AFTER DT RFE Tuned Training w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT RFE Tuned Test w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))","132def22":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(MLA_predict)","559cbe75":"df_val['Survived'] = rfe_tune_model.predict(df_val[X_rfe])\n\n# Submit file\nsubmission = df_val[['PassengerId','Survived']]\nsubmission.to_csv('submit.csv', index=False)\nsubmission.head()","7526535f":"## Tune Model With Feature Selection","1d9dc3e6":"## Data Modelling Libraries","ded9eee8":"## ML Models","78603b48":"## Submission","3e33582a":"This kernel is a summary that I have created to learn the techniques and data science algorithms of another one that has much better content and explanations so if you liked it please give your support to the original kernel in the link below, thank you.\n\nOriginal Kernel: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy","226f30a3":"## Dummy Variables","013eab79":"## Model Tunning","1d0c8f9a":"## Model Selection Finale","c9dedce1":"## Feature Engineering","265d6c40":"## Required Libraries","362cf758":"## Introduction","28d6ceba":"## Play and Clean Data","7d50b5a9":"## Load Data"}}