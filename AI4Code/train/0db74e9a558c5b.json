{"cell_type":{"a0953ce8":"code","bfa5e3ec":"code","f524203e":"code","03342c0c":"code","35d943de":"code","0a9cc409":"code","93968ea0":"code","b8a45e38":"code","cb1da532":"code","02cb44c9":"code","2f240b6a":"code","a6f1c63d":"code","40b9b461":"code","19649571":"code","3a25e63b":"code","1a6fb410":"code","5144e25e":"code","47904141":"code","2ebed3fd":"code","2326c947":"code","d4eab0dc":"code","6ec0f120":"code","d10aefe0":"code","70d629bf":"code","6f7171ab":"code","296629f7":"code","82de1a93":"code","5f518f77":"code","2a1dfc6f":"code","b2b66cc6":"code","f9fdea70":"code","a1aa7d47":"code","6131271c":"code","90aad2da":"code","f9f97a4e":"code","c857a1c2":"code","4b2bc94c":"code","3df74642":"code","5334eabe":"code","ceae7ff8":"code","2029ce34":"code","e28d7a21":"code","ca80706e":"code","9aef466e":"code","92d8c879":"code","9da557e3":"code","45f321cb":"code","3e69601b":"code","799a05ac":"code","32ecee09":"code","36bc111f":"code","a74f8468":"code","a07b3edb":"code","24c8111f":"code","686b6df2":"code","75f88e8a":"code","021e887d":"code","ee6ba704":"code","a6c4b18f":"code","d73003b5":"code","2ee2c2fd":"code","f1618515":"code","5e307af3":"code","09c1e6af":"code","625196af":"code","784f0c15":"code","fb722bb0":"code","7a65f42c":"code","4d395447":"code","f07ee1ed":"code","8cce9d1b":"markdown","0d7a86d1":"markdown","d4082431":"markdown","e9f28360":"markdown","56eb4113":"markdown","92d80bc2":"markdown","a4a7318d":"markdown","cb8241a3":"markdown","7fb5dacb":"markdown","b3eb21fd":"markdown","a5e18a67":"markdown","75f782a6":"markdown","d95b54a8":"markdown","8f8fd7c0":"markdown","93697ee0":"markdown","dcf3cfe4":"markdown","703016aa":"markdown","10cc7391":"markdown","67221b29":"markdown","a2e17363":"markdown","c4d0c758":"markdown","5d0177cd":"markdown","bdebad7f":"markdown","57bd9c7d":"markdown","9ac06977":"markdown","4e313773":"markdown","79cb9594":"markdown","18c58078":"markdown","18dec969":"markdown","66808ad5":"markdown","9bffbe6d":"markdown","ca54306d":"markdown","22bb8270":"markdown","a796cbbc":"markdown","e495ad6a":"markdown","adc175c4":"markdown","86e5c7ca":"markdown","69fbca75":"markdown","510fe45e":"markdown","2dfdc3ce":"markdown"},"source":{"a0953ce8":"import seaborn as sns\nheaders = ['#000000']\nprint(\"Headers Color:\")\nsns.palplot(sns.color_palette(headers))\nprint()","bfa5e3ec":"l_r = ['#3d5a80', '#98c1d9', '#e0fbfc', '#ee6c4d', '#293241']\nprint(\"Linear Regression Colors:\")\nsns.palplot(sns.color_palette(l_r))\nprint()","f524203e":"m_l_r = ['#d9ed92', '#b5e48c', '#99d98c', '#76c893', '#52b69a', '#34a0a4', '#168aad']\nprint(\"Multiple Linear Regression Colors:\")\nsns.palplot(sns.color_palette(m_l_r))\nprint()\n","03342c0c":"p_r = ['#590d22', '#800f2f', '#a4133c', '#c9184a', '#ff4d6d', '#ff758f', '#ff8fa3']\nprint(\"Polynomial Regression Colors:\")\nsns.palplot(sns.color_palette(p_r))\nprint()","35d943de":"b_v_t = ['#582f0e', '#7f4f24', '#936639', '#a68a64', '#b6ad90', '#c2c5aa', '#a4ac86', '#656d4a', '#414833', '#333d29']\nprint(\"Bias-Variance Tradeoff Colors:\")\nsns.palplot(sns.color_palette(b_v_t))\nprint()","0a9cc409":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93968ea0":"df = pd.read_csv('\/kaggle\/input\/ice-cream-revenue\/IceCreamData.csv')\ndf.sample(10)","b8a45e38":"df.info()","cb1da532":"df.describe()","02cb44c9":"plt.figure(dpi = (200))\nsns.pairplot(df)","2f240b6a":"sns.set_theme(style=\"white\")\nplt.figure(dpi = (100))\nsns.jointplot(x = df['Revenue'], y = df['Temperature'], kind='reg', line_kws={\"color\": \"red\"})","a6f1c63d":"plt.figure(figsize = (10,5), dpi = (100))\nsns.scatterplot(x = df['Revenue'], y = df['Temperature'], marker=\"*\")","40b9b461":"from sklearn.model_selection import train_test_split\n\nX = df[['Temperature']]\ny = df['Revenue']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","19649571":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(fit_intercept = True)","3a25e63b":"model.fit(X_train, y_train)","1a6fb410":"pred = model.predict(X_test)","5144e25e":"train_score = model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","47904141":"sns.set_theme(style=\"white\")\nsns.jointplot(x=y_test, y=pred, kind='reg', line_kws={\"color\": \"red\"})","2ebed3fd":"from sklearn.metrics import mean_squared_error\nprint(\"Mean Squared Error:\",mean_squared_error(y_test, pred))","2326c947":"print(\"Root Mean Squared Error:\",np.sqrt(mean_squared_error(y_test, pred)))","d4eab0dc":"from sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error:\",mean_absolute_error(y_test, pred))","6ec0f120":"import sklearn.metrics as m\nprint(\"Mean Absolute Percentage Error:\",np.mean(np.abs( (y_test-pred) \/ y_test))*100)","d10aefe0":"from sklearn.metrics import r2_score\nprint(\"R^2:\",r2_score(y_test, pred))","70d629bf":"import sklearn.metrics as m\nn=X_test.shape[0]\np=X_test.shape[1] - 1\nR2 = m.r2_score(y_test, pred)\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))","6f7171ab":"import sklearn.metrics as metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\nk = X_test.shape[1]\nn = len(X_test)\n\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(metrics.mean_squared_error(y_test, pred))\nMAE = mean_absolute_error(y_test, pred)\nMAPE = np.mean(np.abs( (y_test-pred) \/ y_test))*100\nr2 = r2_score(y_test, pred)\nadj_r2 = 1-(1-r2) * (n-1)\/(n-k-1)","296629f7":"results = [MSE, RMSE, MAE, MAPE, r2, adj_r2]\nmetrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'r2', 'adj_r2']\n\ntable_results = pd.DataFrame({'Metric': metrics, 'Score': results})\ntable_results","82de1a93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f518f77":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndf.sample(10)","2a1dfc6f":"df.info()","b2b66cc6":"df.describe().T","f9fdea70":"df.drop('id', axis = 1, inplace = True)","a1aa7d47":"df_v = df\ndf_v.columns","6131271c":"df_v.drop(['date','zipcode', 'lat', 'long'], axis = 1, inplace = True)","90aad2da":"sns.pairplot(data = df_v)","f9f97a4e":"sns.jointplot(x='sqft_living', y='price', data = df_v, color = 'gray')","c857a1c2":"plt.figure(figsize=(24,16)) \nsns.heatmap(df_v.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","4b2bc94c":"from sklearn.model_selection import train_test_split\n\nX = df.drop([\"price\"],axis =1)\ny = df[\"price\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","3df74642":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","5334eabe":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept = True, normalize = True)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n \ntrain_score = model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","ceae7ff8":"print('Linear Model Coefficient (m): ', model.coef_)\nprint('Linear Model Coefficient (b): ', model.intercept_)","2029ce34":"sns.set_theme(style=\"white\")\nsns.jointplot(x=y_test, y=pred, kind='reg', line_kws={\"color\": \"red\"})","e28d7a21":"import sklearn.metrics as metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\nk = X_test.shape[1]\nn = len(X_test)\n\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(metrics.mean_squared_error(y_test, pred))\nMAE = mean_absolute_error(y_test, pred)\nMAPE = np.mean(np.abs( (y_test-pred) \/ y_test))*100\nr2 = r2_score(y_test, pred)\nadj_r2 = 1-(1-r2) * (n-1)\/(n-k-1)\n\nresults = [MSE, RMSE, MAE, MAPE, r2, adj_r2]\nmetrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'r2', 'adj_r2']\n\ntable_results = pd.DataFrame({'Metric': metrics, 'Score': results})\ntable_results","ca80706e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9aef466e":"df = pd.read_csv('\/kaggle\/input\/manufacturing-cost\/EconomiesOfScale.csv')\ndf.sample(10)","92d8c879":"df.info()","9da557e3":"df.describe()","45f321cb":"sns.set_theme(style=\"white\")\nplt.figure(dpi = (200))\nsns.pairplot(df)","3e69601b":"sns.jointplot(x='Number of Units', y='Manufacturing Cost', data = df)","799a05ac":"sns.set_theme(style=\"white\")\nplt.figure(dpi = (100))\nsns.jointplot(x = df['Number of Units'], y = df['Manufacturing Cost'], kind='reg', line_kws={\"color\": \"red\"})","32ecee09":"plt.figure(figsize = (5,5), dpi = (100))\nsns.scatterplot(x = df['Number of Units'], y = df['Manufacturing Cost'], marker=\"*\")","36bc111f":"from sklearn.model_selection import train_test_split\n\nX = df[['Number of Units']]\ny = df['Manufacturing Cost']\n\n# We are using entire dataset for training\nX_train = X\ny_train = y\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)","a74f8468":"from sklearn.preprocessing import PolynomialFeatures\npoly_regressor = PolynomialFeatures(degree=6)","a07b3edb":"X_columns = poly_regressor.fit_transform(X_train)","24c8111f":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\nmodel.fit(X_columns, y_train)\n\npred = model.predict(poly_regressor.fit_transform(X_train))","686b6df2":"plt.scatter(X_train, y_train, color = 'red')\nplt.plot(X_train, pred, color = 'blue')\nplt.ylabel('Cost Per Unit Sold [dollars]')\nplt.xlabel('Number of Units [in Millions]')\nplt.title('Unit Cost vs. Number of Units [in Millions](Training dataset)')","75f88e8a":"j = 0\nplt.figure(figsize=(150,150))\n\nfor i in range(1,11):\n    poly_regressor = PolynomialFeatures(degree=i)\n    X_columns = poly_regressor.fit_transform(X_train)\n    \n    model = LinearRegression()\n\n    model.fit(X_columns, y_train)\n\n    pred = model.predict(poly_regressor.fit_transform(X_train))\n  \n    plt.subplot(5,2,j+1)\n    plt.scatter(X_train, y_train, color = 'red', s = 115)\n    plt.plot(X_train, pred, color = 'blue',linewidth = 5.5)\n    plt.ylabel('Cost Per Unit Sold [dollars]')\n    plt.xlabel('Number of Units [in Millions]')\n    plt.title(f'Unit Cost vs. Number of Units [in Millions](Training dataset) - Degree =  {i}',color = 'black',fontsize = 75)\n    j += 1","021e887d":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndf.sample(10)","ee6ba704":"df.info()","a6c4b18f":"df.describe()","d73003b5":"df.drop('id', axis = 1, inplace = True)\ndf.drop('date', axis = 1, inplace = True)\ndf.drop('zipcode', axis = 1, inplace = True)\ndf.drop('lat', axis = 1, inplace = True)\ndf.drop('long', axis = 1, inplace = True)","2ee2c2fd":"plt.figure(figsize=(12,8)) \nsns.heatmap(df.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","f1618515":"columns = df.columns.drop('price')\n\nfeatures = columns\nlabel = ['price']\n\nX = df[features]\ny = df[label]","5e307af3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","09c1e6af":"from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha = 0.001, normalize=True)\nridge.fit(X_train, y_train)\n\npred = ridge.predict(X_test)\n\ntrain_score = ridge.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = ridge.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","625196af":"k = X_test.shape[1]\nn = len(X_test)\n\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(mean_squared_error(y_test, pred))\nMAE = mean_absolute_error(y_test, pred)\nr2 = r2_score(y_test, pred)\nadj_r2 = 1-(1-r2) * (n-1)\/(n-k-1)\n\nresults = [MSE, RMSE, MAE, r2, adj_r2]\nmetrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n\ntable_results = pd.DataFrame({'Metric': metrics, 'Score': results})\ntable_results","784f0c15":"plt.plot(y_test, pred, \"o\", color = 'r')\nplt.xlim(0, 3000000)\nplt.ylim(0, 3000000)\n\nplt.xlabel(\"Model Predictions\")\nplt.ylabel(\"True Value (ground Truth)\")\nplt.title('Ridge Regression Predictions')\nplt.show()","fb722bb0":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha = 0.001, normalize=True)\nlasso.fit(X_train, y_train)\n\npred = lasso.predict(X_test)\n\ntrain_score = lasso.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = lasso.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","7a65f42c":"k = X_test.shape[1]\nn = len(X_test)\n\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(mean_squared_error(y_test, pred))\nMAE = mean_absolute_error(y_test, pred)\nr2 = r2_score(y_test, pred)\nadj_r2 = 1-(1-r2) * (n-1)\/(n-k-1)\n\nresults = [MSE, RMSE, MAE, r2, adj_r2]\nmetrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n\ntable_results = pd.DataFrame({'Metric': metrics, 'Score': results})\ntable_results","4d395447":"from sklearn.linear_model import ElasticNet\n\ne_net = ElasticNet(alpha=0.1, l1_ratio=0.5, normalize=False)\ne_net.fit(X_train, y_train)\n\npred = e_net.predict(X_test)\n\ntrain_score = e_net.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = e_net.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","f07ee1ed":"k = X_test.shape[1]\nn = len(X_test)\n\nMSE = mean_squared_error(y_test, pred)\nRMSE = np.sqrt(mean_squared_error(y_test, pred))\nMAE = mean_absolute_error(y_test, pred)\nr2 = r2_score(y_test, pred)\nadj_r2 = 1-(1-r2) * (n-1)\/(n-k-1)\n\nresults = [MSE, RMSE, MAE, r2, adj_r2]\nmetrics = ['MSE', 'RMSE', 'MAE', 'r2', 'adj_r2']\n\ntable_results = pd.DataFrame({'Metric': metrics, 'Score': results})\ntable_results","8cce9d1b":"## Adjusted R Squared\n\nEach additional argument added to a model always increases the R\u00b2 value.\n\nAs the independent variable is added, the model becomes more complex, when the model becomes complex, \"overfitting\" occurs. Hence R-squared increases.\n\nAdjusted R-square comes into play to solve such problems. The adjusted R-square compensates for each independent variable and only increases when each given variable improves the model above what is possible.\n\n![image.png](attachment:247d46e6-0e65-455b-8dc7-c8bf10a39594.png)","0d7a86d1":"<a id = \"3\"><\/a>\n<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">Polynomial Regression<\/h1><\/center>\n\nThe hypothesis function is not necessarily linear if it does not fit the data well.\n\nThe behavior or curve of the hypothesis function can be changed to a quadratic, cubic, or square root function (or any other form).\n\nSometimes, there may be a non-linear relationship between the data. One way to try to explain such a relationship is with a polynomial regression model. Such a model for a single predicted X:\n\n![image.png](attachment:587469c3-82a3-41d7-8299-20265ee490f3.png)\n\nHere h is called the degree of polynomial. For lower orders, the relationship has a specific name (i.e. h = 2 is called quadratic, h = 3 is called cubic, h = 4 is called quartic, etc.). Although this model enables a nonlinear relationship between Y and X, polynomial regression is still considered linear regression as the regression coefficients \u03b21, \u03b22, \u2026, \u03b2h are linear.\n\nTo calculate the above equation, we will only need the response variable (Y) and the predictor variable (X). However, polynomial regression models may also have other predictive variables that can lead to the interaction term. As can be seen, the basic equation for a polynomial regression model above is a relatively simple model, but depending on the situation, the model can grow much larger.","d4082431":"<center><h1 style = \"background:#3d5a80 ;color:white;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","e9f28360":"## Mean Squared Error\n\nMean Square Error (MSE) is the most commonly used regression loss function. MSE is the average loss of frames per sample across the entire dataset. To calculate the MSE, sum all frame losses for individual samples and then divide by the number of samples.\n\n![image.png](attachment:2133e97e-390a-4264-9505-d5a0080a2b08.png)\n\n* x is the property the model uses to predict.\n* The prediction (x) meaning is the predicted value according to the x property.\n* y is the true value.\n* N is the number of samples.","56eb4113":"<center><h1 style = \"background:#ff4d6d ;color:white;border:0;font-weight:bold\">Optimal Degree<\/h1><\/center>","92d80bc2":"<a id = \"1\"><\/a>\n<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">Simple Linear Regression<\/h1><\/center>\n\n## What is linear regression?\nLinear regression analysis is used to estimate the value of one variable relative to the value of another variable. The variable you want to predict is called the dependent variable. The variable you use to predict the value of the other variable is called the independent variable.\n\n## Why is linear regression important?\n\nLinear regression models are relatively simple and provide an easy-to-interpret mathematical formula that can produce predictions. Linear regression can be applied to a variety of fields in business and academic studies.\n\nYou can see linear regression used in everything from the biological, behavioral, environmental and social sciences to business. Linear regression models have become a scientifically proven method and they reliably predict the future. Because linear regression is a well-established statistical procedure, the properties of linear regression models can be well understood and trained very quickly.\n\n## Simple Linear Regression Model\n\nThe simple linear regression model is a method used to estimate the dependent variable with the help of the independent variable when there is a linear relationship between the independent variable and the dependent variable.\n\n\\begin{equation}\nY=\\beta_{0}+\\beta_{1} X+\\epsilon\n\\end{equation}\n\n<SCRIPT SRC='https:\/\/cdn.mathjax.org\/mathjax\/latest\/MathJax.js?config=TeX-AMS-MML_HTMLorMML'><\/SCRIPT>\n<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})<\/SCRIPT>\n\nWe can construct a linear model with the above equation. In this model, <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mi>&#x03B2;<!-- \u03b2 --><\/mi>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mn>0<\/mn>\n    <\/mrow>\n  <\/msub>\n<\/math> is the cutoff point, <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mi>&#x03B2;<!-- \u03b2 --><\/mi>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mn>1<\/mn>\n    <\/mrow>\n  <\/msub>\n<\/math> is the slope of the line, and \u03f5 is the error term.\n\n\n\n","a4a7318d":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Multiple Linear Regression<\/h1><\/center>","cb8241a3":"<center><h1 style = \"background:#582f0e ;color:white;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","7fb5dacb":"<a id = \"7\"><\/a>\n<center><h1 style = \"background:#b6ad90 ;color:white;border:0;font-weight:bold\">Lasso Regression (L1 Regularization)<\/h1><\/center>\n\n* It performs both variable selection and regularization in order to increase the prediction accuracy and interpretability of the model it produces. \n* Just like in ridge regression, the aim is to find the coefficients that minimize the sum of squares error by applying penalties to the coefficients. But unlike ridge regression, it sets the coefficients of irrelevant variables equal to zero.","b3eb21fd":"**METHOD** : *fit(X, y[, sample_weight])* --> It fits linear model.\n\n**X** : {array-like, sparse matrix} of shape (n_samples, n_features)\n\nTraining data.\n\n**y** :array-like of shape (n_samples,) or (n_samples, n_targets)\n\nTarget values. Will be cast to X\u2019s dtype if necessary\n\n**sample_weight** : array-like of shape (n_samples,), default=None\n\nIndividual weights for each sample\n\n**returns** : **self** : returns an instance of self.","a5e18a67":"<center><h1 style = \"background:#d9ed92 ;color:black;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","75f782a6":"<center><h1 style = \"background:#98c1d9 ;color:black;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","d95b54a8":"<center><h1 style = \"background:#936639 ;color:white;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","8f8fd7c0":"**METHOD** : *predict(X, y[, sample_weight])* --> Predicts test values using the linear model.\n\n**X** : array-like or sparse matrix, shape (n_samples, n_features)\n\nTest samples.\n\n**returns** : **C** --> array, shape (n_samples,)\n\nIt returns predicted values.","93697ee0":"<center><h1 style = \"background:#ee6c4d ;color:white;border:0;font-weight:bold\">Linear Regression<\/h1><\/center>\n\n**fit_intercept** : bool, default=True\nWhether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n\n**normalize** : bool, default=False\nThis parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False.\n\n**copy_X** : bool, default=True\nIf True, X will be copied; else, it may be overwritten.\n\n**n_jobs** : int, default=None\nThe number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. \n\n**positive** : bool, default=False\nWhen set to True, forces the coefficients to be positive. This option is only supported for dense arrays.","dcf3cfe4":"## Mean Absolute Error\n\nAbsolute error is the difference between estimated values and actual values. To be exact, it is the mean of the absolute value of each difference between the actual value and the predicted value for that sample across the entire sample of the data set.\n\n\n\n* y is the true value.\n* y^ is the predicted value.\n\n*A lower value indicates better accuracy.*\n\n![image.png](attachment:7054c75f-e697-4d5b-a295-21fb628906d8.png)","703016aa":"## Let's put it all together","10cc7391":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Correlation<\/h1><\/center>","67221b29":"<center><h1 style = \"background:#590d22 ;color:white;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","a2e17363":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","c4d0c758":"<center><h1 style = \"background:#800f2f ;color:white;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","5d0177cd":"# Conclusion\n\nWe have come to the end of the notebook. I covered basic regression techniques and problems in this notebook. I hope you liked it.\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* Waiting for your positive and negative comments. :)\n\nThank you for your time.\n","bdebad7f":"<a id = \"8\"><\/a>\n<center><h1 style = \"background:#c2c5aa ;color:white;border:0;font-weight:bold\">Elastic Net<\/h1><\/center>\n\nThe goal is the same as ridge and lasso regression, but elastic net combines ridge and lasso regression. Ridge regression style penalization and lasso regression style variable selection.","57bd9c7d":"<a id = \"6\"><\/a>\n<center><h1 style = \"background:#a68a64 ;color:white;border:0;font-weight:bold\">Ridge Regression (L2 Regularization)<\/h1><\/center>\n\n* It is used to analyze multivariate regression data. \n* The aim is to find the coefficients that minimize the sum of squares error by applying a penalty to these coefficients. \n* It is resistant to over-fitting. \n* It offers a solution to multidimensionality. \n* It establishes a model with all variables, does not remove irrelevant variables, only brings their coefficients closer to zero.\n* It is necessary to find a good value for alpha (penalty) when building the model.\n","9ac06977":"<a id = \"2\"><\/a>\n<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">Multiple Linear Regression<\/h1><\/center>\n\nIt is an analysis to reveal the relationship between a dependent variable and a series of independent variables associated with it.\n\nMultiple linear regression examines the linear relationship between two or more independent variables and one dependent variable. There is a correlation between dependent and independent variables in multiple regression. Let's denote the independent variables as X and the dependent variables as Y.\n\n$$ {Y = XB + E}.$$\n\n* Y: dependent variable observation vector\n* X: independent variables observation matrix\n* B: coefficients vector\n* E: random error vector\n\n<SCRIPT SRC='https:\/\/cdn.mathjax.org\/mathjax\/latest\/MathJax.js?config=TeX-AMS-MML_HTMLorMML'><\/SCRIPT>\n<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})<\/SCRIPT>\n\n$$ Simple Linear Regression --> {y =  b_0  + b_1 x_1}.$$\n\n$$ Multiple Linear Regression --> {y =  b_0  + b_1 x_1  + b_2 x_2+\u22ef+ b_n x_n}.$$\n\n\nIn multiple linear regression, the degree of effect of each independent variable on the dependent variable is different from each other. Therefore, the coefficient of each variable need not be the same in addition to the equation in simple linear regression.\n\n![image.png](attachment:870d5609-7211-4952-b439-03f67e96b263.png)","4e313773":"<a id = \"5\"><\/a>\n<center><h1 style = \"background:#293241 ;color:white;border:0;font-weight:bold\">Evaluation of Model<\/h1><\/center>","79cb9594":"**NOTE:** All the method explanations were taken from scikit-learn documentation. \n\n*Source for explanations of parameters* : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html","18c58078":"<center><h1 style = \"background:#c9184a ;color:white;border:0;font-weight:bold\">Polynomial Regression<\/h1><\/center>","18dec969":"<center><h1 style = \"background:#a4133c ;color:white;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","66808ad5":"## R square\n\nR\u00b2 is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination or the multiple coefficient of determination for multiple regression. To put it in simpler language, R-square is a measure of fit for linear regression models.\n\n\n\nR-squared does not indicate whether a regression model fits your data adequately. A good model can have a low R-squared value. On the other hand, a biased model can have a high R-squared value!\n\n![image.png](attachment:4f734e46-546a-4f2c-95cb-4b71eb0f42da.png)","9bffbe6d":"<center><h1 style = \"background:#76c893 ;color:black;border:0;font-weight:bold\">Evaluation of Model<\/h1><\/center>","ca54306d":"<center><h1 style = \"background:#b5e48c ;color:black;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","22bb8270":"**METHOD** : *score(X, y[, sample_weight])* --> Return the coefficient of determination <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msup>\n    <mi>R<\/mi>\n    <mn>2<\/mn>\n  <\/msup>\n<\/math> of the prediction.\n\n**X** : array-like of shape (n_samples, n_features)\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.\n\n**y** : array-like of shape (n_samples,) or (n_samples, n_outputs)\n\nTrue values for X.\n\n**returns** : **score** --> float\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msup>\n    <mi>R<\/mi>\n    <mn>2<\/mn>\n  <\/msup>\n<\/math> of self.predict(X) wrt. y.","a796cbbc":"<center><h1 style = \"background:#e0fbfc ;color:black;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","e495ad6a":"## Mean Absolute Percentage Error\n\nIn regression and time series models, mean absolute percent error is frequently used to measure the accuracy of predictions. If there are zero among the real values, the MAPE cannot be calculated as there will be division by zero. Percentage error cannot exceed 100% for very low predictive values, but there is no upper limit for the percentage error for very high predictive values. When MAPE is used to compare the accuracy of estimators, it is biased as it systematically selects a method that is too low for estimates. This small but serious problem can be overcome with an accuracy criterion that finds the ratio of predicted values to their true values. This approach leads to estimates that can be interpreted in terms of the geometric mean.\n\n![image.png](attachment:4136bb30-c14e-488d-8501-6a1961ab783c.png)","adc175c4":"## Root Mean Squared Error\n\nIt is a quadratic metric that measures the magnitude of the error, often used to find the distance between the predictor's predicted values and the actual values of a machine learning model. The standard deviation of the RMSE estimation errors (residues). That is, the residuals are a measure of how far the regression line is from the data points; RMSE is a measure of how far these residues spread. In other words, it tells you how dense that data is around the line that best fits the data. The RMSE value can range from 0 to infinity. Negative oriented scores, i.e. predictors with lower values, perform better. A zero RMSE value means the model made no mistakes. RMSE has the advantage of punishing large errors more so it may be better suited to some situations. RMSE prevents the unwanted use of absolute values in many mathematical calculations.\n\n![image.png](attachment:04720730-1a7b-4188-8d81-55288a5e6d2f.png)","86e5c7ca":"<a id = \"4\"><\/a>\n<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">BIAS - VARIANCE TRADEOFF<\/h1><\/center>\n\n**Bias** is the difference between the models mean estimate and the true value. If the model has high bias, it oversimplifies the training data. This causes a high error rate in training and test data.\n\n**Variance,** on the other hand, is a value that tells the model prediction variability or spread of data for a given data point. If the model has high variance, instead of finding a pattern that can generalize in the train set, it cannot generalize well to data it has never seen before, since it starts to learn or even memorize the observations in this set. This results in low error in the training data but high error in the test data.\n\nIn machine learning, models are fitted to the data, and the loss function is tried to be minimized. If the loss function is very close to 0, the model has a probability of over-fitting, and if it is far from 0, there is a probability of under-fitting. To explain a little more, this is why the loss function is seriously small since the estimated value will be very close to the true value, as the model starts to learn the observations instead of discovering the pattern in the data.\n\n![image.png](attachment:1a34892b-9799-464a-a1d5-81de0f185b85.png)\n\n*Soruce for picture:* https:\/\/medium.com\/greyatom\/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76\n\n\n* **High Bias Low Variance:** Models are consistent but average error rate is high.\n\n* **High Bias High Variance:** Models are both inaccurate and inconsistent.\n\n* **Low Bias Low Variance:** Models are accurate and consistent on average. We strive to achieve this result in our models.\n\n* **Low Bias High Variance:** Models are somewhat accurate but inconsistent on average. When a small change is made in the data set, it causes a large error rate.\n\n![image.png](attachment:f00bd127-a448-45f1-ac55-af3b7effee31.png)\n\n*Soruce for picture:* https:\/\/ahmetkuzubasli.medium.com\/modeliniz-neden-hala-hatal%C4%B1-bias-ve-variance-6368f36de751\n\n## What is the way to find out if we have high bias or high variance?\n\n#### If the model has high bias, the following results can be encountered;\n* The error rate of the training set of the model is high.\n* The error rate of the test data set is similar to the training set.\n\n#### If the model has high variance, the following results can be encountered;\n* The error rate of the training set of the model is low.\n* The error rate of the test data set of the model is high.\n\n\n## How to avoid overfitting?\nIf the model has started to memorize the data set used for training more than necessary, or if the training set is monotonous, the risk of overfitting is high. When we give the test data to this model, which has a high score in the training set, we will probably get a very low score. Because the model memorized the situations in the training set and searches for these situations in the test data set. Very bad prediction scores will be obtained in the test data set, since there will be no memorized cases in the slightest change. Models with overfitting problems have high variance and low bias.\n\nThis usually happens when the model is very complex (i.e. too many features\/variables compared to the number of observations). This model will have very high prediction accuracy on training data, but probably not very accurate on untrained or new data. This problem stems from the model's inability to generalize. Such models learn or explain the \"noise\" in the training data rather than the actual relationships between the variables in the data.\n\nThe overfitting problem can be solved by applying the following methods;\n\n* **Adding more data:** If the training set is uniform, data diversity should be increased by adding more data.\n* **Ensembling:** It is a machine learning method that allows to use separate models together. Thus, the model can work with more complex structured data without overfit.\n* **Reducing the number of features:** Columns that are highly correlated with each other can be deleted or a single variable can be created from these variables by methods such as factor analysis.\n* **Cross Validation:** An important technique in Machine Learning applied to predict model accuracy on unseen data. It does the training by dividing the data set into k pieces, it uses 1 piece from these k pieces for testing, this piece is different from the previous iteration each time, so the model is constantly tested with a new test set.\n* **Regularization:** Regularization is a technique used to reduce the complexity of the model. It does this by penalizing the loss function. In other words, it reduces the effect of these variables by reducing the weight of the variables with high weight in the model. This method helps to solve the over-learning problem. The loss function is the sum of the squares of the difference between the actual value and the predicted value. To reduce the weight of the variables, it is necessary to increase the regularization value. **Lasso** and **Ridge** Regularization methods are techniques used for this purpose.\n\n\n## How to avoid underfitting?\nUnlike overfitting, if a model has underfitting, it means that the model does not fit the training data and therefore misses trends in the data. It also means that the model cannot be generalized for new data. This problem is often the result of a very simple model.\nModels with underfitting problems have a high error rate in both training and test data sets. It has low variance and high bias. Instead of following the training data too closely, these models ignore the lessons learned from the training data and fail to learn the fundamental relationship between inputs and outputs.\n\n* **Simple model structure:** The constructed model is so simple that it cannot fully learn the relationship between input and output data. The high train error shows that the machine learning model is very simple.\n\n* **Missing data:** Another reason for underfitting can be a lack of data. For example, when the results depend on more than one variable, training performance will be low if one tries to learn through only one variable.\n\n* **Insufficient data:** The amount of data must be sufficient if models are expected to yield high success rates. If there is not enough data, the model will have trouble finding a pattern between the input data and the results.\n\n* **Noisy data:** If the data is too noisy, machine learning techniques will have learning difficulties and will not be able to generalize. Noisy data should be reduced so that the model does not think that the data is randomly generated.","69fbca75":"## Let's Start!","510fe45e":"<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">REGRESSION MASTER NOTEBOOK<\/h1><\/center>\n\n\nHey! In this notebook, I have covered regression techniques in detail. I explained the theoretical information about regression techniques, evaluation metrics and bias-variance tradeoff, which is one of the important issues of regression. I tried to explain each technique with different datasets. Three different datasets are used in this notebook:\n\n1. House Sales in King County, USA\n1. Ice Cream Revenue\n1. Manufacturing cost\n\n### In each main topic, I used the same template:\n\n1. Information About Dataset\n1. Data Visualization\n1. Train-Test Split\n1. Model\n1. Evaluation of Model\n\nIn addition, in the Linear Regression section, I gave theoretical information about the metrics used in the evaluation of regression models, as well as coding.\n\n### The main topics are:\n\n1. [Simple Linear Regression](#1)\n   1. [Evaluation Metrics](#5)\n1. [Multiple Linear Regression](#2)\n1. [Polynomial Regression](#3)\n1. [Bias-Variance Tradeoff](#4)\n   1. [Ridge Regression](#6)\n   1. [Lasso Regression](#7)\n   1. [Elastic Net](#8)\n   \n### Before starting:\n* Main headings are indicated with a black background.\n* The following color palettes are used for each main title.","2dfdc3ce":"<center><h1 style = \"background:#7f4f24 ;color:white;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>"}}