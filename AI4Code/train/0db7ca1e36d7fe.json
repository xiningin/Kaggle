{"cell_type":{"3484f0a2":"code","d677de0a":"code","1e25ea1a":"code","1eb8fe36":"code","875d0d67":"code","2f108f93":"code","ad65df26":"code","31734aa2":"code","32ef1272":"code","2f7a46fa":"code","45c267ce":"code","7bc57185":"code","e81196c3":"code","6b614bbb":"code","01926d9a":"code","d064dfe7":"code","60d33d6e":"code","09b95095":"code","392068c5":"code","5fc4760e":"code","f19f0da7":"code","0e904652":"code","4a572f0c":"code","c3e636da":"code","72744f20":"code","829617ea":"code","9413f499":"code","ee67cafa":"code","1fa7800f":"code","648b7fc6":"code","774f4300":"code","17f7adfc":"code","d9e5ba9e":"code","a50fb645":"code","2375f1c4":"code","e4a6a99e":"code","ad4dd7e3":"code","fc741a5d":"code","b634fa1d":"code","00d24c32":"code","5d54d848":"code","f0e6fe54":"code","fb449f81":"code","b3f87aab":"code","fd321884":"code","31971bea":"code","ae442532":"code","fdf1f961":"code","fd81e0e5":"code","0297cfe2":"code","bb19c217":"code","93d2fe80":"code","7a8d807f":"code","d8b8748d":"code","37b7da61":"code","9ddace14":"code","84ea5c0d":"code","2ae614fd":"code","41b7891a":"code","27e2042d":"code","d4bcfe2c":"code","d67b2a65":"markdown","dcfdff5e":"markdown","277c2769":"markdown","9cfdc429":"markdown","e7959701":"markdown","c106a2c4":"markdown","ef944359":"markdown","83822374":"markdown","34c6dece":"markdown","f9b99c7c":"markdown","1c234357":"markdown","c845a192":"markdown","06c7b290":"markdown"},"source":{"3484f0a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d677de0a":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np","1e25ea1a":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","1eb8fe36":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel(\"samples\")\nplt.title(\"distribution\")","875d0d67":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 1][\"text\"].str.len())\nplt.title(\"Disaster tweets length\")","2f108f93":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 0][\"text\"].str.len(), color= 'r')\nplt.title(\"No disaster tweets length\")","ad65df26":"plt.grid()\n\nword1 = train[train[\"target\"] == 1][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)))\nplt.title(\"Disaster tweets length\")","31734aa2":"plt.grid()\n\nword1 = train[train[\"target\"] == 0][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)), color = 'r')\nplt.title(\"Disaster tweets length\")","32ef1272":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","2f7a46fa":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","45c267ce":"corpus = create_corpus(0)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","7bc57185":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"top words 0\")","e81196c3":"corpus = create_corpus(1)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","6b614bbb":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"top words 1\")","01926d9a":"corpus = create_corpus(1)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1\n        \n        \n","d064dfe7":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"Punctuation disaster 1\")","60d33d6e":"corpus = create_corpus(0)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1","09b95095":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"Punctuation disaster 0\")","392068c5":"from collections import Counter","5fc4760e":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","f19f0da7":"plt.title(\"most common words\")\nplt.grid()\nsns.barplot(x = y, y = x)","0e904652":"df = pd.concat([train, test])\ndf.shape","4a572f0c":"df","c3e636da":"import re","72744f20":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)","829617ea":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))","9413f499":"df","ee67cafa":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","1fa7800f":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))","648b7fc6":"df","774f4300":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)","17f7adfc":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))","d9e5ba9e":"df","a50fb645":"def remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","2375f1c4":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))","e4a6a99e":"df","ad4dd7e3":"!pip install pyspellchecker","fc741a5d":"from spellchecker import SpellChecker","b634fa1d":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    \n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","00d24c32":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","5d54d848":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize","f0e6fe54":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if \\\n        ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus","fb449f81":"corpus = create_corpus(df)","b3f87aab":"embedding_dict = {}\n\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","fd321884":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout, Input\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom keras.models import Model","31971bea":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,\n                          maxlen = MAX_LEN, \n                         truncating = 'post', \n                         padding = 'post')","ae442532":"word_index = tokenizer_obj.word_index\nprint('number of unique words: ', len(word_index))","fdf1f961":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","fd81e0e5":"def encode_tweets(tokenizer, tweets, max_len):\n    nb_tweets = len(tweets)\n    tokens = np.ones((nb_tweets,max_len),dtype='int32')\n    masks = np.zeros((nb_tweets,max_len),dtype='int32')\n    segs = np.zeros((nb_tweets,max_len),dtype='int32')\n\n    for k in range(nb_tweets):        \n        # INPUT_IDS\n        tweet = tweets[k]\n        enc = tokenizer.encode(tweet)                   \n        if len(enc) < max_len-2:\n            tokens[k,:len(enc)+2] = [0] + enc + [2]\n            masks[k,:len(enc)+2] = 1\n        else:\n            tokens[k,:max_len] = [0] + enc[:max_len-2] + [2]\n            masks[k,:max_len] = 1 \n    return tokens,masks,segs","0297cfe2":"def build_model(max_len):\n    ids = Input((max_len,), dtype=tf.int32)\n    attention = Input((max_len,), dtype=tf.int32)\n    token = Input((max_len,), dtype=tf.int32)\n    \n    bertweet = TFAutoModel.from_pretrained(\"vinai\/bertweet-base\")\n    x,_ = bertweet(ids,attention_mask=attention,token_type_ids=token)\n\n    out = Dense(1,activation='sigmoid')(x[:,0,:])\n    \n    model = Model(inputs=[ids, attention, token], outputs = out)\n    optimizer = Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model\n\n    \nmodel = build_model(MAX_LEN)\nmodel.summary()","bb19c217":"optimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])","93d2fe80":"test_data = tweet_pad[train.shape[0]:]","7a8d807f":"tokenizer = AutoTokenizer.from_pretrained(\"vinai\/bertweet-base\")","d8b8748d":"train_tokens, train_masks, train_segs = encode_tweets(tokenizer, train[\"text\"].to_list(), MAX_LEN)\ntrain_labels = train[\"target\"]","37b7da61":"train_segs[0]","9ddace14":"\"\"\"X_train, X_test, y_train, y_test = train_test_split(train_data, train[\"target\"].values, test_size = 0.15)\n\"\"\"","84ea5c0d":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\n#CKPT = ModelCheckpoint('.\/ckpt.h5', save_best_only=True, monitor='val_loss', mode='min')\nES = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)","2ae614fd":"hist = model.fit([train_tokens,train_masks,train_segs], train_labels, \n                 batch_size = 32, epochs = 50, \n                 validation_split = 0.1, callbacks= [ES])","41b7891a":"submit = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","27e2042d":"test_tokens, test_masks, test_segs = encode_tweets(tokenizer,test[\"text\"].to_list(), MAX_LEN)","d4bcfe2c":"test[\"target\"] = model.predict([test_tokens, test_masks, test_segs]).round().astype(int)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"submission.csv\",index=False)","d67b2a65":"## Remove emoji","dcfdff5e":"# 1. Quick look","277c2769":"# 2. Create corpus","9cfdc429":"# Data cleaning","e7959701":"This notebook is forked from \nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\/notebook\n\nAnd transformer guide from https:\/\/www.kaggle.com\/massinissaguendoul\/nlp-disaster-tweet","c106a2c4":"## Common words","ef944359":"# Glove vectorization (word2vec)","83822374":"## Spelling checker\n\nAdditional: spelling checker for indonesian dataset","34c6dece":"## punctuation","f9b99c7c":"## remove html tag","1c234357":"## removing URLs","c845a192":"## distribution","06c7b290":"## Remove punctuation"}}