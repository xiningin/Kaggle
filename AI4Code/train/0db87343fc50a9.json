{"cell_type":{"a7faffdf":"code","9f120fe6":"code","853b3633":"code","fc04b26d":"code","dd7ff96c":"code","8e7c5ef8":"code","99a968dd":"code","7f149edd":"code","b2daa2af":"code","142cc8b5":"code","ee68b0eb":"code","eeba0417":"code","9873d265":"code","b4d0695c":"code","b0d8eb4a":"code","97070128":"code","83ac3bd9":"code","88d3710a":"markdown","c8eb8313":"markdown","5d6d1f97":"markdown","b62527cc":"markdown","cf2baecc":"markdown","b20262bd":"markdown","e8d0c397":"markdown","9a96aa83":"markdown","d9ca6524":"markdown","554a7e06":"markdown","bdb5d23c":"markdown","b700debd":"markdown","d06ec4e8":"markdown"},"source":{"a7faffdf":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE","9f120fe6":"df = pd.read_csv('..\/input\/mnist-data\/train.csv')\ndf = df[:1000]","853b3633":"label = df.label\nprint(label)\n#df.drop(\"label\", axis=1, inplace=True)","fc04b26d":"from sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(df)\nstandardized_data.shape","dd7ff96c":"model1 = TSNE(n_components=2, random_state=0, perplexity=30, learning_rate=200, n_iter=1000)\ntsne1 = model1.fit_transform(standardized_data)\nprint(tsne1)\n# fit_transform(self, X[, y])\n# Fit X into an embedded space and return that transformed output.","8e7c5ef8":"reduced_tsne1 = np.vstack((tsne1.transpose(), label)).transpose()\nreduced_tsne1 = pd.DataFrame(data=reduced_tsne1, columns=[\"X\", \"Y\", \"label\"])\nreduced_tsne1.label = reduced_tsne1.label.astype(np.int) #optional(converting label to int)\nprint(reduced_tsne1.head())\nreduced_tsne1.dtypes","99a968dd":"g = sns.lmplot(x='X',y='Y',data=reduced_tsne1, fit_reg=False,hue='label', size=6)\nplt.title(\"tSNE plot 1\",size=25)","7f149edd":"model2 = TSNE(n_components=2, random_state=0, perplexity=50, learning_rate=200, n_iter=2000)\ntsne2 = model2.fit_transform(standardized_data)\nprint(tsne2)","b2daa2af":"reduced_tsne2 = np.vstack((tsne2.transpose(), label)).transpose()\nreduced_tsne2 = pd.DataFrame(data=reduced_tsne2, columns=[\"X\", \"Y\", \"label\"])\nreduced_tsne2.label = reduced_tsne2.label.astype(np.int) #optional(converting label to int)\nprint(reduced_tsne2.head())\nreduced_tsne2.dtypes","142cc8b5":"g = sns.lmplot(x='X',y='Y',data=reduced_tsne2, fit_reg=False,hue='label', size=6)\nplt.title(\"tSNE plot 2\",size=25)","ee68b0eb":"from sklearn.decomposition import PCA","eeba0417":"df = pd.read_csv('..\/input\/mnist-data\/train.csv')\nstandardized_data = StandardScaler().fit_transform(df)\nstandardized_data.shape","9873d265":"pca = PCA(n_components=2)\nX = pca.fit_transform(standardized_data)\nprint(X)","b4d0695c":"label = df.label\ndata = np.vstack((X.transpose(), label)).transpose()\ndata = pd.DataFrame(data=data, columns=['1', '2','label'])\ndata.head()","b0d8eb4a":"g = sns.lmplot(x='1',y='2',data=data,fit_reg=False ,hue='label', size=6)\nplt.xlabel(\"Principal component 1\")\nplt.ylabel(\"Principal component 2\")\nplt.title(\"PCA\",size=25)","97070128":"pca = PCA(n_components=50)\npca_res = pca.fit_transform(df)\nsns.scatterplot(x = pca_res[:,0], y = pca_res[:,1], hue = label, palette = sns.hls_palette(10), legend = 'full')","83ac3bd9":"tsne = TSNE(n_components = 2, random_state=0)\ntsne_res = tsne.fit_transform(pca_res)\n\nsns.scatterplot(x = tsne_res[:,0], y = tsne_res[:,1], hue = label, palette = sns.hls_palette(10), legend = 'full')","88d3710a":"By combining tSNE and PCA, it is found that we can segregate the clusters better.\n- 3 and 8 have separate clusters now. \n- Very few points are spread out and most of them belong to some cluster.","c8eb8313":"## Combining tSNE and PCA","5d6d1f97":"t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation.","b62527cc":"# tSNE","cf2baecc":"PCA converts the correlations or lack-of correlations into a 2D plot.\nThe points that are highly correlated cluster together.The 2 axes are ranked in order of importance. Thus differences among the Principal component 1 are more important than PC 2.","b20262bd":"Interpretation -\nThe clustered groups show the various digits.\nThe blue cluster label 0 is near label 8 on principal component 1 since the digits 0 and 8 are similar. Then 8 is near 6. And similarly, the distances vary according to the correlation between them. Almost all the clusters overlap one another because of the similarities between the numbers, which should not be the case. \n\nOne drawback of PCA is that the linear projection can\u2019t capture non-linear dependencies. ","e8d0c397":"# PCA","9a96aa83":"Steps followed:\n1. Loading data\n2. Taking the first 1000 values for convenience(time consuming process)\n3. Data standardization\n4. Applying tSNE and fitting the model\n5. Converting the fit into dataframe for plotting\n6. Plotting the model","d9ca6524":"### Parameters of tsne\nn_components - int, optional (default: 2)\nDimension of the embedded space.\n\nrandom_state - int, RandomState instance, default=None\nDetermines the random number generator. Pass an int for reproducible results across multiple function calls. \n\nperplexity - float, optional (default: 30)\nThe perplexity is related to the number of nearest neighbors(expected density). \nLarger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. \nDifferent values can result in significanlty different results.\n\nlearning_rate - float, optional (default: 200.0)\nThe learning rate for t-SNE is usually in the range [10.0, 1000.0]. \nIf the learning rate is too high, the data may look like a \u2018ball\u2019 with any point approximately equidistant from its nearest neighbours. \nIf the learning rate is too low, most points may look compressed in a dense cloud with few outliers. \n\nn_iter - int, optional (default: 1000)\nMaximum number of iterations for the optimization. Should be at least 250.","554a7e06":"Conclusion -\ntSNE gives a better visualisation compared to PCA. As shown in the scatter plot, it does not give sufficiently meaningful insights about the different labels.","bdb5d23c":"Steps followed:\n1. Loading data\n2. Standardisation of data\n3. Applying and fitting the model\n4. Plotting the model","b700debd":"The various colours denote the various digits like 0,1,2,3,4...etc.\nPoints corresponding to same digit are clustered together.\n\nWe have applied model 2 below with increased perplexity and number of iterations to improve accuracy.","d06ec4e8":"We observe that the images corresponding to the different digits are separated into different clusters of points. For example, all the orange points are clustered together that represent 1."}}