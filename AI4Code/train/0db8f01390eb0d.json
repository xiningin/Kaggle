{"cell_type":{"014833ac":"code","4389cc72":"code","bc042fe9":"code","6975989a":"code","46c60a58":"code","7859dbf2":"code","53d5e1af":"code","4853b2f8":"code","d6da3e51":"code","1eac9a91":"code","dff02405":"code","5297831c":"code","350206bc":"code","04006988":"code","1dd1017f":"code","cb5c3286":"code","d853847e":"code","396a1142":"code","daeebc8b":"code","75a98cae":"code","c206bdcb":"code","a7813ac6":"code","1cc3b1f0":"markdown","9102c4ee":"markdown","00cf2ba5":"markdown","bcb640ab":"markdown","124a1039":"markdown","76ae5c84":"markdown","b598488f":"markdown","919449ba":"markdown","564f63f4":"markdown","6a103874":"markdown","64e4d492":"markdown","98340f59":"markdown","9a567baf":"markdown","42413cb2":"markdown","a0ae8b11":"markdown","3efec490":"markdown"},"source":{"014833ac":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport collections\nfrom sklearn import preprocessing\n\n%matplotlib inline","4389cc72":"!pip install keras","bc042fe9":"# READ DATA \ntrain_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip')\ntest_df = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip')","6975989a":"# convert TARGET to the numeric\ntrain_df['interest_level'] = train_df['interest_level'].apply(lambda x: 0 if x=='low' \n                                                      else 1 if x=='medium' \n                                                      else 2) \n# REMOVE UNNECESSARY WORDS FROM DESCRIPTION\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"<br \/>\", \"\"))\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"br\", \"\"))\ntrain_df['description'] = train_df['description'].apply(lambda x: x.replace(\"<p><a\", \"\"))\n\n#basic features\ntrain_df['rooms'] = train_df['bedrooms'] + train_df['bathrooms'] \n\n# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# description contains email\nregex = r'[\\w\\.-]+@[\\w\\.-]+'\ntrain_df['has_email'] = train_df['description'].apply(lambda x: 1 if re.findall(regex, x) else 0)\n\n# description contains phone\ntrain_df['has_phone'] = train_df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\n\n# CONVERT LOWER ALL OF WORDS\ntrain_df[[\"features\"]] = train_df[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])","46c60a58":"# REMOVE UNNECESSARY WORDS FROM DESCRIPTION\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"<br \/>\", \"\"))\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"br\", \"\"))\ntest_df['description'] = test_df['description'].apply(lambda x: x.replace(\"<p><a\", \"\"))\n\n#basic features\ntest_df['rooms'] = test_df['bedrooms'] + test_df['bathrooms'] \n\n# count of photos #\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# description contains email\nregex = r'[\\w\\.-]+@[\\w\\.-]+'\ntest_df['has_email'] = test_df['description'].apply(lambda x: 1 if re.findall(regex, x) else 0)\n\n# description contains phone\ntest_df['has_phone'] = test_df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\n\n# CONVERT LOWER ALL OF WORDS\ntest_df[[\"features\"]] = test_df[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])","7859dbf2":"feature_value_train = train_df['features'].tolist()\nfeature_value_test = test_df['features'].tolist()\n\nfeature_value_train\nfeature_value_test\n\nfeature_lst_train = []\nfeature_lst_test = []\n\nfor i in range(len(feature_value_train)):\n    feature_lst_train += feature_value_train[i]\n    \nfor i in range(len(feature_value_test)):\n    feature_lst_test += feature_value_test[i]\n\nuniq_feature_train = list(set(feature_lst_train))\nuniq_feature_test = list(set(feature_lst_test))\n\n\n# see the frequency of each feature\ndef most_common(lst):\n    features = collections.Counter(lst)\n    feature_value = features.keys()\n    frequency = features.values()\n    data = [('feature_value', feature_value),\n            ('frequency', frequency),]    \n    df = pd.DataFrame.from_dict(dict(data))\n    return df.sort_values(by = 'frequency', ascending = False)\n\ndf_features_train = most_common(feature_lst_train)\ndf_features_test = most_common(feature_lst_test)\n\n\ndef newColumn(name, df, series):\n    feature = pd.Series(0,df.index,name = name)# data : 0\n    for row,word in enumerate(series):\n        if name in word:\n            feature.iloc[row] = 1\n    df[name] = feature # feature : series ; value in series : 1 or 0\n    return df\n\n# select features based on frequency\nfacilities = ['elevator', 'cats allowed', 'hardwood floors', 'dogs allowed', 'doorman', 'dishwasher', 'no fee', 'laundry in building', 'fitness center']\nfor name in facilities:\n    train_df = newColumn(name, train_df, train_df['features'])\n    test_df = newColumn(name, test_df, test_df['features'])","53d5e1af":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))","4853b2f8":"train_df['price'] = np.log10(train_df['price'])\ntest_df['price'] = np.log10(test_df['price'])","d6da3e51":"# TRAINING DATASET\ntrain_df.drop('created', axis=1, inplace=True)\ntrain_df.drop('description', axis=1, inplace=True)\ntrain_df.drop('features', axis=1, inplace=True)\ntrain_df.drop('photos', axis=1, inplace=True)\n\n\n# TEST DATASET\ntest_df.drop('created', axis=1, inplace=True)\ntest_df.drop('description', axis=1, inplace=True)\ntest_df.drop('features', axis=1, inplace=True)\ntest_df.drop('photos', axis=1, inplace=True)","1eac9a91":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport optuna\nimport math\n\nX = train_df.drop(['price'], axis = 1)\ny = train_df.price\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = .3,\n                                                    random_state = 5)","dff02405":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_test)\n        return mean_absolute_error(y_test, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"minimize\")\n        study.optimize(self.objective, n_trials=self.trials)\n        return study","5297831c":"def create_model(trial):\n    params = {\n         'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n         'booster':trial.suggest_categorical('booster', ['gbtree', 'dart', 'gblinear']),\n         'learning_rate':trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n         'max_depth':trial.suggest_int(\"max_depth\", 3, 19),\n         'subsample':trial.suggest_uniform(\"subsample\", 0.0, 1.0),\n         'colsample_bytree':trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0),\n    }\n    model = xgb.XGBRegressor(**params)\n    return model\n\noptimizer = Optimizer('mae')\nxgb_opt_study = optimizer.optimize()\nxgb_opt_params = xgb_opt_study.best_params\nxgb_opt = xgb.XGBRegressor(**xgb_opt_params)   # Model\nxgb_opt.fit(X, y)\npreds = xgb_opt.predict(X_test)\n\nprint(\"Number of finished trials: \", len(xgb_opt_study.trials))\nprint(\"Best trial:\")\nxgb_trial = xgb_opt_study.best_trial\n\nprint(\"  Value: {}\".format(xgb_trial.value))\nprint(\"  Params: \")\nfor key, value in xgb_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","350206bc":"def create_model(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'max_depth': trial.suggest_int('max_depth',3 ,19),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n    }\n    model = RandomForestRegressor(**params)\n    return model\n\noptimizer = Optimizer('mae')\nrf_opt_study = optimizer.optimize()\nrf_opt_params = rf_opt_study.best_params\nrf_opt = RandomForestRegressor(**rf_opt_params)\nrf_opt.fit(X, y)\npreds = rf_opt.predict(X_test)\n\nprint(\"Number of finished trials: \", len(rf_opt_study.trials))\nprint(\"Best trial:\")\nrf_trial = rf_opt_study.best_trial\n\nprint(\"  Value: {}\".format(rf_trial.value))\nprint(\"  Params: \")\nfor key, value in rf_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","04006988":"def create_model(trial):\n    params = {\n        'copy_X': trial.suggest_categorical(\"copy_X\", [\"True\", \"False\"]),\n        'fit_intercept': trial.suggest_categorical(\"fit_intercept\", [\"True\", \"False\"]),\n        'n_jobs': trial.suggest_int('n_jobs',-1 ,3),\n    }\n    model = LinearRegression(**params)\n    return model\n\noptimizer = Optimizer('mae')\nlr_opt_study = optimizer.optimize()\nlr_opt_params = lr_opt_study.best_params\nlr_opt = LinearRegression(**lr_opt_params)\nlr_opt.fit(X, y)\npreds = lr_opt.predict(X_test)\n\nprint(\"Number of finished trials: \", len(lr_opt_study.trials))\nprint(\"Best trial:\")\nlr_trial = lr_opt_study.best_trial\n\nprint(\"  Value: {}\".format(lr_trial.value))\nprint(\"  Params: \")\nfor key, value in lr_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","1dd1017f":"train_df['price'].describe()","cb5c3286":"pipeline_models = []\n\nxgb_default = xgb.XGBRegressor()\nrf_default = RandomForestRegressor()\nlr_default = LinearRegression()\nsvm_default = SVR()\n\nmodels = [xgb_default, xgb_opt,\n          rf_default, rf_opt,\n          lr_default, lr_opt,\n          svm_default]\n\nmodel_names = ['XGB Regression (default)', 'XGB Regression (opt)', \n               'Random Forest (default)', 'Random Forest (opt)',\n               'Linear Regression (default)', 'Linear Regression (opt)',\n               'Support Vector Machine (default)']\n\n## Assign each model to a pipeline\nfor name, model in zip(model_names,models):\n    pipeline = (\"Scaled_\"+ name,\n                Pipeline([(\"Scaler\",StandardScaler()),\n                          (name,model)\n                         ]))\n    pipeline_models.append(pipeline)","d853847e":"train_df['price'] = 10 ** train_df['price']\ntest_df['price'] = 10 ** test_df['price']","396a1142":"train_df['price']","daeebc8b":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n## Create a dataframe to store all the models' cross validation score\nevaluate = pd.DataFrame(columns=[\"model\",\"cv_MAE\", \"cv_RMSE\"])\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n## Encoded dataset\nfor name,model in pipeline_models:\n    scores = cross_validate(model, X, y, cv=kfold, n_jobs=-1,\n                         scoring=('neg_root_mean_squared_error', 'neg_mean_absolute_error'))\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv_MAE\"] = round(abs(scores['test_neg_mean_absolute_error']).mean(), 3)\n    evaluate.loc[row,\"cv_RMSE\"] = round(abs(scores['test_neg_root_mean_squared_error']).mean(), 3)","75a98cae":"evaluate","c206bdcb":"## Visualization\nfig, ax = plt.subplots(figsize=(16,9))\n\n## Encoded dataset\nbar = sns.barplot(evaluate[\"model\"], evaluate[\"cv_MAE\"])\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()\/2, height*1.02,height,ha=\"center\")\nax.set_title(\"Cross Validate Score (MAE)\")\nax.set_xticklabels(evaluate[\"model\"].to_list(),rotation =50)","a7813ac6":"## Visualization\nfig, ax = plt.subplots(figsize=(16,9))\n\n## Encoded dataset\nbar = sns.barplot(evaluate[\"model\"], evaluate[\"cv_RMSE\"])\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()\/2, height*1.02,height,ha=\"center\")\nax.set_title(\"Cross Validate Score (RMSE)\")\nax.set_xticklabels(evaluate[\"model\"].to_list(),rotation =50)","1cc3b1f0":"- MAE score visualization","9102c4ee":"* Pipeline for storing models","00cf2ba5":"# TEST DATA FEATURE ENGINEERING","bcb640ab":"# MOST FREQUENT FEATURES EXTRACTION","124a1039":"- XGB REGRESSOR OPTUNA PREDICTION","76ae5c84":"# LABEL ECONDING FOR CATEGORICAL VARIABLES","b598488f":"# TRAIN DATA FEATURE ENGINEERING","919449ba":"# DROP UNNECESSARY COLUMNS","564f63f4":"- RANDOM FOREST OPTUNA PREDICTION","6a103874":"* BEFORE PREDICTION INVERSE LOG10","64e4d492":"* Evaluate scores","98340f59":"Let me show description of price","9a567baf":"### LOGARITHMIC EXPRESSION TO THE PRICE COLUMN","42413cb2":"- LINEAR REGRESSION OPTUNA PREDICTION","a0ae8b11":"# REGRESSION FOR PRICE","3efec490":"- RMSE score visualization"}}