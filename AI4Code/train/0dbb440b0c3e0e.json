{"cell_type":{"fe41bfcc":"code","b77cc877":"code","5bf7502a":"code","e6a0d386":"code","5201043a":"code","6ce991d9":"code","f3cf8f37":"code","48d098f3":"code","68c5109a":"code","dae0063f":"code","bf28f9cd":"code","ec467257":"code","f3059d58":"code","879e7f35":"code","623d1458":"code","f3f0bcae":"code","ba1dbc38":"code","010e020e":"code","85d83f4e":"code","0c6bffcf":"code","513e6122":"code","fbe2cedd":"code","d335b1dd":"code","904dfd2f":"code","a1f32834":"code","60b5b656":"code","99925e76":"code","028587cd":"code","1bd02ad3":"code","4d994cbc":"code","7c8349bf":"code","b2b59eeb":"code","3aa93b56":"code","d290e493":"code","6b18f830":"code","bc732fb6":"code","42d617ce":"code","6f7e1aa8":"code","40da066c":"code","d2ad102b":"code","a94a944a":"code","6bad1058":"code","f9858675":"code","6ee92903":"markdown","5acc492a":"markdown","8627c2b7":"markdown","3169ac32":"markdown","b4f47e8b":"markdown","4b1e6513":"markdown"},"source":{"fe41bfcc":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport re","b77cc877":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\n","5bf7502a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e6a0d386":"nltk.download('stopwords')","5201043a":"from tqdm.notebook import tqdm","6ce991d9":"prefix = '\/kaggle\/input\/nlp-getting-started\/'","f3cf8f37":"\ntest = pd.read_csv(prefix + 'test.csv')","48d098f3":"test.head()\n\n","68c5109a":"test.shape","dae0063f":"train = pd.read_csv(prefix + 'train.csv')","bf28f9cd":"train.head()\n","ec467257":"train.shape","f3059d58":"df = pd.concat([train,test])","879e7f35":"df.shape","623d1458":"!pip install emoji\nimport emoji","f3f0bcae":"def convert_emojis(text):\n    try:\n        text = emoji.demojize(text)\n    except:\n        pass\n    return text","ba1dbc38":"text = \"Hello \ud83d\ude02\"\nconvert_emojis(text)","010e020e":"df['text'] = df['text'].apply(convert_emojis)","85d83f4e":"df","0c6bffcf":"def cleanup_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub('', text)\n\ndf['text'] = df['text'].apply(cleanup_url)","513e6122":"def cleanup_punctuations(text):\n    import string\n    table = str.maketrans('', '', string.punctuation)    \n    return text.translate(table)\n","fbe2cedd":"df['text'] = df['text'].apply(cleanup_punctuations)","d335b1dd":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize(sentence):\n    sentence_words = w_tokenizer.tokenize(sentence)\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        \n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence\n","904dfd2f":"small_df = df[0:10]","a1f32834":"small_df","60b5b656":"df['text'] = df['text'].apply(lemmatize)","99925e76":"df","028587cd":"# just for checkpoint \ndf.to_csv('cleaned.2.csv')","1bd02ad3":"import torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe","4d994cbc":"def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache\/dataset_test.csv',\n                   index=False)","7c8349bf":"def get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","b2b59eeb":"import logging\nimport os\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache\/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache\/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='twitter.27B', dim=200)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","3aa93b56":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","d290e493":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","6b18f830":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects \/ len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects \/ len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Validation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","bc732fb6":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 200\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optim,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","42d617ce":"plt.figure(figsize=(10, 6))\nplt.title('Loss')\nsns.lineplot(range(len(train_loss)), train_loss, label='train')\nsns.lineplot(range(len(val_loss)), val_loss, label='test')","6f7e1aa8":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nsns.lineplot(range(len(train_acc)), train_acc, label='train')\nsns.lineplot(range(len(val_acc)), val_acc, label='test')","40da066c":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","d2ad102b":"sample_submission = pd.read_csv(prefix+'sample_submission.csv')","a94a944a":"sample_submission['target'] = list(map(int, results_target))","6bad1058":"sample_submission.to_csv('submission.csv', index=False)","f9858675":"#download the result CSV here\n\nfrom IPython.display import FileLink\nFileLink('\/kaggle\/working\/submission.csv')","6ee92903":"This is the notebook for Twitter disaster classification","5acc492a":"lemmatize sentence","8627c2b7":"Emoticons may provide signals for real disaster, so we do not remove it, instead we translate to text.","3169ac32":"Glove Embedding","b4f47e8b":"Pytorch part\n","4b1e6513":"cleanup punctuations"}}