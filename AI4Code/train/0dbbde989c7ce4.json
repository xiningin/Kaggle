{"cell_type":{"edf73f93":"code","70a5278a":"code","2d900594":"code","1ad5bc2a":"code","b9dff2f7":"code","f8fee940":"code","472528fc":"code","4c0f1360":"code","fa3cf1bd":"code","bdcac55d":"code","f0495762":"code","eb010b3f":"code","a250814e":"code","c78a2dd2":"code","a92736ab":"code","faf250cd":"code","38eac53f":"code","e75d7734":"code","8f94c0d3":"code","b940a95f":"code","b69927e8":"code","897ecfc1":"code","2d39105f":"code","142a154b":"code","a2046863":"code","3907581a":"code","182fca45":"code","aa0dae6b":"code","44b1b34e":"code","3ddd214b":"code","6d326e32":"code","a3d2ea5a":"code","7e49ef95":"code","13197262":"code","35d8cdc9":"code","aa67a5f0":"code","daf693d9":"code","46382495":"code","b5ca4774":"code","f0b4a9ce":"code","3f925700":"code","afba2d23":"code","d3a29423":"code","9f1a9f94":"code","20c79000":"code","e848865f":"code","ad5818b2":"markdown","f50a558b":"markdown","11408910":"markdown","4b57a037":"markdown","89779293":"markdown","b385fc10":"markdown","e18e7577":"markdown","4a56f1fe":"markdown","71c89015":"markdown","13e53037":"markdown","cb756e98":"markdown","34712c83":"markdown","b7093f0f":"markdown","ab7316dd":"markdown","adc0c857":"markdown","1ed1270b":"markdown","87726b4a":"markdown","0fec7b19":"markdown","01687884":"markdown","922bf490":"markdown","93b14ea5":"markdown","9d4e5326":"markdown","47658b70":"markdown","b3f03bc5":"markdown","18ef203c":"markdown","b63c81eb":"markdown","1ee2f9c2":"markdown","ac13b262":"markdown","543f177c":"markdown"},"source":{"edf73f93":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom transformers import TFLongformerModel\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import mixed_precision\nfrom multiprocessing import cpu_count\n\nimport glob\nimport sys\nimport os\nimport random\nimport logging\nimport math\nimport os\nimport time\n\n# Disable Tensorflow Logs\/Warnings\ntf.get_logger().setLevel(logging.ERROR)\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","70a5278a":"# Seed all random sources\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    \nset_seeds(42)","2d900594":"# Column Data Types\ndtype = {\n    'id': 'string',\n    'discourse_id': np.uint64,\n    'discourse_start': np.uint16,\n    'discourse_end': np.uint16,\n    'discourse_text': 'string',\n    'discourse_type': 'category',\n    'discourse_type_num': 'category',\n    'predictionstring': 'string',\n}\n\ntrain = pd.read_csv('\/kaggle\/input\/feedback-prize-2021\/train.csv', dtype=dtype)\n\ndisplay(train.head())\n\ndisplay(train.info())","1ad5bc2a":"# Get all labels sorted for reproducibility\nLABELS = train['discourse_type'].unique().sort_values().tolist() + ['Not Annotated', 'Padding']\n# Add extra non-annotated and padding label\nN_LABELS = len(LABELS)\n\n# Not Annotated Class\nNA_CLASS = N_LABELS - 2\n\n# Padding Class\nPAD_CLASS = N_LABELS - 1\n\n# Number of Non-Pad Labels\nN_NON_PAD_LABELS = N_LABELS -1\n\nprint(f'N_LABELS: {N_LABELS}, NA_CLASS: {NA_CLASS}, PAD_CLASS: {PAD_CLASS}, N_NON_PAD_LABELS: {N_NON_PAD_LABELS}')\nprint(f'LABELS: {LABELS}')","b9dff2f7":"# Text Token Sequence Size\nSEQ_LENGTH = 4096","f8fee940":"DEBUG = False\n\nif DEBUG:\n    TAKE_N = 2048\nelse:\n    TAKE_N = np.iinfo(np.uint64).max","472528fc":"# === TRAIN ===\nX_train_input_ids = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/train\/train_tokens.npy')[:TAKE_N]\nX_train_attention_masks = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/train\/train_attention_masks.npy')[:TAKE_N]\ny_train = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/train\/train_labels.npy')[:TAKE_N]\n\n# === VAL ===\nX_val_input_ids = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/val\/val_tokens.npy')[:TAKE_N]\nX_val_attention_masks = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/val\/val_attention_masks.npy')[:TAKE_N]\ny_val = np.load('\/kaggle\/input\/feedback-prize-preprocessing-oversampling\/val\/val_labels.npy')[:TAKE_N]\n\nprint(f'X_train_input_ids shape: {X_train_input_ids.shape}, X_train_attention_masks shape: {X_train_attention_masks.shape}, y_train shape: {y_train.shape}')\nprint(f'X_train_input_ids dtype: {X_train_input_ids.dtype}, X_train_attention_masks dtype: {X_train_attention_masks.dtype}, y_train dtype: {y_train.dtype}')\nprint(f'X_val_input_ids shape: {X_val_input_ids.shape}, X_val_attention_masks shape: {X_val_attention_masks.shape}, y_train shape: {y_val.shape}')\nprint(f'X_val_input_ids dtype: {X_val_input_ids.dtype}, X_val_attention_masks dtype: {X_val_attention_masks.dtype}, y_train dtype: {y_val.dtype}')","4c0f1360":"# Hacky way of getting class count dictionary without unnanotated class\nclass_count = pd.Series(y_train.flatten()).value_counts().sort_index()\nLABELS_COUNT_DICT = dict(list(zip(LABELS, class_count)))\n\nCLASS_WEIGHT = dict()\nfor label_idx, (label, label_count) in enumerate(LABELS_COUNT_DICT.items()):\n    # Assign SQRT Class Weights\n    CLASS_WEIGHT[label_idx] = max(LABELS_COUNT_DICT.values()) \/ label_count\n    \n# Make DataFrame\nCLASS_WEIGHT_DF = pd.DataFrame(CLASS_WEIGHT.items(), index=LABELS, columns=['LABEL_INDEX', 'CLASS_WEIGHT'])\n\ndisplay(CLASS_WEIGHT_DF)","fa3cf1bd":"def get_class_weight_mask(CLASS_WEIGHT_DF):\n    class_weight_mask = tf.constant(CLASS_WEIGHT_DF['CLASS_WEIGHT'], dtype=tf.float32)\n    class_weight_mask = tf.expand_dims(class_weight_mask, axis=0)\n    class_weight_mask = tf.repeat(class_weight_mask, repeats=SEQ_LENGTH, axis=0)\n    class_weight_mask = tf.expand_dims(class_weight_mask, axis=0)\n    \n    return class_weight_mask\n\n    \nCLASS_WEIGHT_MASK = get_class_weight_mask(CLASS_WEIGHT_DF)","bdcac55d":"@tf.function(experimental_compile=True)\ndef cross_entropy_class_weight(\n    y_true, # True Labels One Hot Encoded\n    y_pred, # Predicted Labels\n    from_logits=True, # If the Predicted Labels are Logits or Probabilities\n    apply_class_weight=False, # Apply Class Weights on Loss\n    add_l2_loss=False, # Add Weights Loss\n    mask_padding=False, # Mask Padding Tokens in Loss\n        ):\n    y_true_labels = tf.cast(y_true, tf.float32)\n    \n    \n    class_weight = tf.constant(CLASS_WEIGHT_MASK)\n    if from_logits:\n        y_pred = tf.nn.softmax(y_pred, axis=2)\n    \n    # Compute Cross Entropy\n    ce_loss = K.log(1e-9+ y_pred) * y_true_labels\n    # Apply Class Weight\n    if apply_class_weight:\n        ce_loss *= class_weight\n          \n    # Sum Each Token\n    ce_loss = K.sum(ce_loss, axis=2)\n    \n    # Nullify Padding Loss\n    if mask_padding:\n        y_true_attention_mask = tf.cast(y_true_labels[:,:, PAD_CLASS] == 0, tf.float32)\n        ce_loss = ce_loss * y_true_attention_mask\n        # Sum for each sample and compute mean\n        ce_loss = -K.sum(ce_loss) \/ K.sum(y_true_attention_mask)\n    else:\n        # Sum for each sample and compute mean\n        ce_loss = -K.mean(ce_loss)\n    \n    # L2 Loss\n    if add_l2_loss:\n        trainable_weights = [w for w in model.weights if ('bias' not in w.name and 'tf_longformer_model' not in w.name)]\n        l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in trainable_weights])\n    else:\n        l2_loss = tf.constant(0.0, dtype=tf.float32)\n      \n    return ce_loss + l2_loss","f0495762":"F1Score = tfa.metrics.F1Score(num_classes=N_NON_PAD_LABELS, average='macro')\n\n@tf.function()\ndef non_pad_f1(y_true, y_pred):\n    # Output Logits to Probabilities\n    y_pred = tf.nn.softmax(y_pred, axis=2)\n    \n    # Cast labels to float32\n    y_true = tf.cast(y_true, tf.float32)\n        \n    # Filter Pad Tokens\n    non_pad_idxs = tf.where(tf.argmax(y_true, axis=2) != N_LABELS - 1)\n\n    # Remove Non-Pad Row\n    y_true = tf.slice(y_true, [0,0,0], [BATCH_SIZE, SEQ_LENGTH, N_NON_PAD_LABELS])\n    y_pred = tf.slice(y_pred, [0,0,0], [BATCH_SIZE, SEQ_LENGTH, N_NON_PAD_LABELS])\n\n    # Gather Non-Pad Predictions\n    y_true = tf.gather_nd(y_true, non_pad_idxs)\n    y_pred = tf.gather_nd(y_pred, non_pad_idxs)\n\n    return F1Score(y_true, y_pred)","eb010b3f":"CategoricalAccuracy = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n\n@tf.function()\ndef non_pad_categorical_accuracy(y_true, y_pred):\n    # Output Logits to Probabilities\n    y_pred = tf.nn.softmax(y_pred, axis=2)\n    \n    # Cast labels to float32\n    y_true = tf.cast(y_true, tf.float32)\n        \n    # Filter Pad Tokens\n    non_pad_idxs = tf.where(tf.argmax(y_true, axis=2) != N_LABELS - 1)\n\n    # Remove Non-Pad Row\n    y_true = tf.slice(y_true, [0,0,0], [BATCH_SIZE, SEQ_LENGTH, N_NON_PAD_LABELS])\n    y_pred = tf.slice(y_pred, [0,0,0], [BATCH_SIZE, SEQ_LENGTH, N_NON_PAD_LABELS])\n\n    # Gather Non-Pad Predictions\n    y_true = tf.gather_nd(y_true, non_pad_idxs)\n    y_pred = tf.gather_nd(y_pred, non_pad_idxs)\n\n    return CategoricalAccuracy(y_true, y_pred)","a250814e":"@tf.function(experimental_compile=True)\ndef weights_l2(y_true, y_pred):\n    # Exclude bias weights and non-trainable weights\n    trainable_weights = [w for w in model.weights if 'bias' not in w.name and w.trainable]\n    l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in trainable_weights])\n    return l2_loss","c78a2dd2":"def get_model():\n    # Weights Initialization and Activation Dense Layers\n    activation = tf.keras.activations.relu\n    kernel_initializer = tf.keras.initializers.HeUniform\n    \n    # Clear Backend\n    tf.keras.backend.clear_session()\n    \n    # enable XLA optmizations\n    tf.config.optimizer.set_jit(True)\n\n    input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='attention_mask')\n\n    # Longformer-base model\n    longformer = TFLongformerModel.from_pretrained(\n        'allenai\/longformer-base-4096',\n        output_hidden_states=True, # return hidden states, we need the raw output\n        return_dict=True,\n    )\n    \n    # Global and pooler layers are not trained, thus set the weights untrainable\n    for w in longformer.trainable_weights:\n        if 'global' in w.name or 'pooler\/dense' in w.name:\n            w._trainable = False\n    \n    # Get the last hidden state\n    last_hidden_state = longformer(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n    # Output shape (768 for base and 1024 for large)\n    last_hidden_state_shape = last_hidden_state.shape[-1]\n\n    # Two fully connected layers and an output layer\n    fc1 = tf.keras.layers.Dense(last_hidden_state_shape, activation=activation, kernel_initializer=kernel_initializer, name='head\/fc1')(last_hidden_state)\n    fc2 = tf.keras.layers.Dense(last_hidden_state_shape \/\/ 4, activation=activation, kernel_initializer=kernel_initializer, name='head\/fc2')(fc1)\n    output = tf.keras.layers.Dense(N_LABELS, name='head\/classifier')(fc2)\n\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n    # LOSS\n    loss = cross_entropy_class_weight\n\n    # OPTIMIZER\n    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-6)\n\n    # METRICS\n    metrics = [\n        tf.keras.metrics.CategoricalCrossentropy(from_logits=True, name='categorical_crossentropy'),\n        non_pad_categorical_accuracy,\n        non_pad_f1,\n        weights_l2,\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics) \n\n    return model\n\nmodel = get_model()","a92736ab":"# Show model summary\nmodel.summary()","faf250cd":"# Show model architecture\ntf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","38eac53f":"# Training configuration\nBATCH_SIZE = 1\nN_EPOCHS = 1\n\nprint(f'BATCH SIZE: {BATCH_SIZE}, N_EPOCHS: {N_EPOCHS}')","e75d7734":"N_TRAIN_SAMPLES = len(y_train)\nN_VAL_SAMPLES = len(y_val)\n\nTRAIN_STEPS_PER_EPOCH = math.ceil(N_TRAIN_SAMPLES \/ BATCH_SIZE)\nVAL_STEPS_PER_EPOCH = math.ceil(N_VAL_SAMPLES \/ BATCH_SIZE)\n\nprint(f'N_TRAIN_SAMPLES: {N_TRAIN_SAMPLES}, N_VAL_SAMPLES: {N_VAL_SAMPLES}')\nprint(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}, VAL_STEPS_PER_EPOCH: {VAL_STEPS_PER_EPOCH}')","8f94c0d3":"# Labels are sparsely saved to reduce memory usage, but need to be one-hot-encoded for the loss and metrics\ndef one_hot_encode_labels(X, y):\n    y_one_hot = tf.one_hot(y, N_LABELS, on_value=1, off_value=0, axis=2, dtype=tf.uint8)\n\n    return X, y_one_hot","b940a95f":"def get_dataset(X_input_ids, X_attention_masks, y, shuffle_repeat, bs=BATCH_SIZE):\n    # Create dataset from numpy arrays\n    dataset = tf.data.Dataset.from_tensor_slices((\n        # Input\n        { \n            'input_ids': X_input_ids,\n            'attention_mask': X_attention_masks,\n        },\n        # Label\n        y.astype(np.uint8)\n    ))\n    \n    if shuffle_repeat:\n        dataset = dataset.shuffle(len(y))\n        dataset = dataset.repeat()\n    \n    # Batch Samples\n    dataset = dataset.batch(bs, drop_remainder=True)\n    # One Hot Encode Labels\n    dataset = dataset.map(one_hot_encode_labels, num_parallel_calls=cpu_count())\n    # Always have a batch ready\n    dataset = dataset.prefetch(10)\n    \n    return dataset","b69927e8":"# TRAIN DATASET\ntrain_dataset = get_dataset(X_train_input_ids, X_train_attention_masks, y_train, True)\n\n# Example of a batch\ntrain_x, train_y = next(iter(train_dataset))\nprint(f'train_x keys: {list(train_x.keys())}')\nprint(f'train_x input ids shape: {train_x[\"input_ids\"].shape}, train_x attention mask shape: {train_x[\"attention_mask\"].shape}')\nprint(f'train_x input ids dtype: {train_x[\"input_ids\"].dtype}, train_x attention mask dtype: {train_x[\"attention_mask\"].dtype}')\nprint(f'train_y shape: {train_y.shape}, train_y dtype: {train_y.dtype}')","897ecfc1":"# VALIDATION DATASET\nval_dataset = get_dataset(X_val_input_ids, X_val_attention_masks, y_val, False)\n\n# Example of a batch\nval_x, val_y = next(iter(val_dataset))\nprint(f'val_x keys: {list(val_x.keys())}')\nprint(f'val_x input ids shape: {val_x[\"input_ids\"].shape}, val_x attention mask shape: {val_x[\"attention_mask\"].shape}')\nprint(f'val_x input ids dtype: {val_x[\"input_ids\"].dtype}, val_x attention mask dtype: {val_x[\"attention_mask\"].dtype}')\nprint(f'val_y shape: {val_y.shape}, val_y dtype: {val_y.dtype}')","2d39105f":"# Resets all training metrics\ndef metric_reset_states():\n    # Reset Model Metrics\n    for m in model.metrics:\n        m.reset_states()\n        \n    # Reset Metric Computation Metrics\n    F1Score.reset_states()\n    CategoricalAccuracy.reset_states()","142a154b":"# Metrics per Step\nHISTORY_STEP = dict({\n    'loss': [], 'val_loss': [], \n    'mean_grad': [], 'val_mean_grad': [],\n})\n\n# Metrics Rolling Window\nHISTORY_RW = dict({\n    'loss': [], 'val_loss': [], \n    'mean_grad': [], 'val_mean_grad': [],\n})\n\nfor m in model.compiled_metrics._metrics:\n    if '__name__' in vars(m):\n        name = m.__name__\n    else:\n        name = m.name\n        \n    HISTORY_STEP[f'{name}'] = []\n    HISTORY_STEP[f'val_{name}'] = []\n    \n    HISTORY_RW[f'{name}'] = []\n    HISTORY_RW[f'val_{name}'] = []","a2046863":"def log(loss, mean_grad, step, step_total, t_start, metric_postfix, rolling_window=1024):\n    # Add Loss to Metrics History\n    loss = loss.numpy()\n    HISTORY_STEP[f'{metric_postfix}loss'].append(loss)\n    \n    # Only log mean gradiant for train step\n    if mean_grad > 0:\n        HISTORY_STEP[f'{metric_postfix}mean_grad'].append(mean_grad)\n        \n    ms_per_step = int((time.time() - t_start) * 1e3 \/ (step + 1))\n    s_left = int(ms_per_step * (step_total - step - 1) \/ 1e3)\n    # Add steps progress, ms per step speed and estimated seconds left\n    logs = f'{step + 1}\/{step_total} | {ms_per_step}ms\/step, {s_left}s left, '\n    # Log Rolling Mean of loss with window of 100 steps\n    loss_rw = np.mean(HISTORY_STEP[f'{metric_postfix}loss'][-rolling_window:])\n    HISTORY_RW[f'{metric_postfix}loss'].append(loss_rw)\n    logs +=f'{metric_postfix}loss: {loss_rw:.3f}, '\n    \n    if mean_grad > 0:\n        mean_grad_rw = np.mean(HISTORY_STEP[f'{metric_postfix}mean_grad'][-rolling_window:])\n        HISTORY_RW[f'{metric_postfix}mean_grad'].append(mean_grad_rw)\n        logs +=f'mean_grad: {mean_grad_rw:.3e}, '\n    \n    for idx, m in enumerate(model.metrics):\n        if idx > 0:\n            logs += ', '\n        \n        m_result = m.result().numpy()\n        # Add to Metric History\n        HISTORY_STEP[f'{metric_postfix}{m.name}'].append(m_result)\n        # Metric Rolling Mean Window 100\n        m_rw = np.mean(HISTORY_STEP[f'{metric_postfix}{m.name}'][-rolling_window:])\n        HISTORY_RW[f'{metric_postfix}{m.name}'].append(m_rw)\n        \n        # Add to Logs\n        if m.name in ['weights_l2']:\n            logs += f'{metric_postfix}{m.name}: {m_rw:.0f}'\n        else:\n            logs += f'{metric_postfix}{m.name}: {m_rw:.3f}'\n        \n    # Print Logs by overwriting line (\"\\r\" returns print carriage to start of line)\n    print('\\r', logs.ljust(3), end='')\n    sys.stdout.flush()\n    \n    # Reset all Metrics\n    metric_reset_states()","3907581a":"@tf.function(\n    autograph=False,\n    input_signature=[\n        {'input_ids': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.uint16), 'attention_mask': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.int8)},\n    ],\n)\ndef pred(X_batch):\n    return model(X_batch, training=False)","182fca45":"class GradientAccumulation():\n    def __init__(self, n_gradients, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Variable to store the gradients in\n        self.grads_zeros = [tf.Variable(tf.zeros_like(v, dtype=tf.float32), name=v.name, trainable=False) for v in model.trainable_variables if v.trainable]\n        # Counter for Gradient Accumulation Steps\n        self.counter = tf.Variable(0, dtype=tf.float32)\n        # Number of Gradient to Accumulate before performing backpropagation\n        self.n_gradients = tf.constant(n_gradients, dtype=np.float32)\n        # Trainable Variables to obtain the gradients for\n        self.trainable_variables = [v for v in model.trainable_variables if v.trainable]\n        print(f'self.counter: {self.counter.numpy()}, self.n_gradients: {self.n_gradients}')\n    \n    # Backpropagation\n    @tf.function()\n    def backprop(self):\n        # Backpropagation (updating the model weights)\n        model.optimizer.apply_gradients(zip(self.grads_zeros, self.trainable_variables))\n\n        # Reset Gradient Accumulation Counter\n        self.counter.assign(0)\n\n        # Set Accumulated Gradients to Zero\n        for g_z in self.grads_zeros:\n            g_z.assign(tf.zeros_like(g_z))\n            \n    # Empty Function for \"tf.cond\" operator\n    @tf.function()\n    def fun_empty(self):\n        return\n        \n    # Mean Gradients used for metric to monitor gradient\n    @tf.function(jit_compile=True)\n    def get_mean_gradient(self, grads):\n        # Gradient Sum and Count\n        grads_sum = tf.constant(0, tf.float64)\n        grads_count = tf.constant(0, tf.float64)\n\n        for g in [g for g in grads if g is not None]:\n            g_mean = tf.reduce_mean(tf.math.abs(g))\n            g_count = tf.reduce_prod(tf.shape(g))\n            grads_sum += tf.cast(g_mean, tf.float64) * tf.cast(g_count, tf.float64)\n            grads_count += tf.cast(g_count, tf.float64)\n\n        return grads_sum \/ grads_count\n\n    # Training Step (forward step and compute gradients)\n    @tf.function(\n        autograph=False,\n        input_signature=[\n            {'input_ids': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.uint16), 'attention_mask': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.int8)},\n            tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH, N_LABELS], dtype=tf.uint8),\n        ],\n    )\n    def train_step(self, X_batch, y_true_batch):\n        # Forward Step (make prediction)\n        with tf.GradientTape() as tape:\n            y_pred_batch = model(X_batch)\n            loss = model.loss(y_true_batch, y_pred_batch)\n            \n        # Update Train Metrics\n        model.compiled_metrics.update_state(y_true_batch, y_pred_batch)\n\n        # Get Gradients\n        grads = tape.gradient(loss, self.trainable_variables)\n                \n        # Update Accumulation Gradients, divide by n_gradients to compute mean gradients\n        [g_z.assign_add(g \/ self.n_gradients) for g, g_z in zip(grads, self.grads_zeros)]\n        \n        # Increase Gradient Accumulation Counter\n        self.counter.assign_add(1)\n        \n        # If we have enough steps perform backpropagation, otherwise do nothing\n        tf.cond(tf.math.equal(self.counter, self.n_gradients), self.backprop, self.fun_empty)    \n                \n        # Compute Mean Gradients of current forward step for metric\n        mean_grad = self.get_mean_gradient(grads)\n                \n        return loss, mean_grad\n    \n    # Validation Step\n    @tf.function(    \n        autograph=False,\n        input_signature=[\n            {'input_ids': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.uint16), 'attention_mask': tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH], dtype=tf.int8)},\n            tf.TensorSpec(shape=[BATCH_SIZE, SEQ_LENGTH, N_LABELS], dtype=tf.uint8),\n        ],\n    )\n    def val_step(self, X_batch, y_true_batch):\n        # Forward Step with gradient monitoring\n        y_pred_batch = pred(X_batch)\n        loss = model.loss(y_true_batch, y_pred_batch)\n            \n        # Update Train Metrics\n        model.compiled_metrics.update_state(y_true_batch, y_pred_batch)\n                \n        return loss","aa0dae6b":"# Make Training Object and define the number of gradients to accumulate before backpropagation\nGA = GradientAccumulation(n_gradients=8)","44b1b34e":"# Train Step\nt_start_train = time.time()\ntrain_dataset_iter = iter(train_dataset)\n\nfor step_idx, step in enumerate(range(TRAIN_STEPS_PER_EPOCH)):\n    # Get Next Train text and label\n    X_batch, y_true_batch = next(train_dataset_iter)\n    # Perform a train step\n    loss, mean_grad = GA.train_step(X_batch, y_true_batch)\n    # Log training progress\n    log(loss, mean_grad, step, TRAIN_STEPS_PER_EPOCH, t_start_train, '')\n    \nprint('\\n')\n    \n# Validation Step\nt_start_val = time.time()\nval_dataset_iter = iter(val_dataset)\n\nfor step in range(VAL_STEPS_PER_EPOCH):\n    X_batch, y_true_batch = next(val_dataset_iter)\n    loss = GA.val_step(X_batch, y_true_batch)\n    log(loss, 0, step, VAL_STEPS_PER_EPOCH, t_start_val, 'val_')\n    \nprint('\\n')","3ddd214b":"# Save Weights\nmodel.save_weights('model.h5')","6d326e32":"history_train_rows = []\nhistory_val_rows = []\n\nfor k, v in HISTORY_STEP.items():\n    if 'val' not in k:# Training\n        history_train_rows.append({ 'metric': k, 'mean value': np.mean(v) })\n    elif 'mean_grad' not in k: # Validation\n        history_val_rows.append({ 'metric': k, 'mean value': np.mean(v) })\n\nprint('=== TRAIN METRICS ===')\ndisplay(pd.DataFrame.from_dict(history_train_rows))\n\nprint('=== VALIDATION METRICS ===')\ndisplay(pd.DataFrame.from_dict(history_val_rows))","a3d2ea5a":"def plot_history_metric(metric, f_best=np.argmax, include_val=True, y_lim_start=None):\n    # Plot Every Train Step\n    plt.figure(figsize=(20, 8))\n    plt.plot(HISTORY_RW[metric])\n    plt.title(f'Model {metric} Step', fontsize=24, pad=10)\n    plt.ylabel(metric, fontsize=20, labelpad=10)\n    plt.xlabel('step', fontsize=20, labelpad=10)\n    plt.xticks(fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.legend(prop={'size': 18})\n    plt.grid()\n    if y_lim_start is not None:\n        plt.ylim(bottom=0, top=max(HISTORY_RW[metric]) * 1.25)\n    plt.show()\n    \n    if include_val:\n        # Plot Validation as Histogram\n        plt.figure(figsize=(20, 8))\n        plt.title(f'Model val_{metric} Histogram', fontsize=24, pad=10)\n        pd.Series(HISTORY_STEP[f'val_{metric}']).plot(kind='hist', bins=32, color='tab:orange')\n        plt.ylabel(f'val_{metric}', fontsize=20, labelpad=10)\n        plt.xlabel(metric, fontsize=20, labelpad=10)\n        plt.xticks(fontsize=16)\n        plt.yticks(fontsize=16)\n        plt.plot()","7e49ef95":"# Increase Plot DPI\nplt.rcParams['figure.dpi'] = 300","13197262":"plot_history_metric('loss', f_best=np.argmin)","35d8cdc9":"plot_history_metric('mean_grad', include_val=False)","aa67a5f0":"plot_history_metric('categorical_crossentropy', f_best=np.argmin)","daf693d9":"plot_history_metric('non_pad_categorical_accuracy')","46382495":"plot_history_metric('non_pad_f1')","b5ca4774":"plot_history_metric('weights_l2', include_val=False, y_lim_start=0)","f0b4a9ce":"def get_predictions(dataset, total_steps):    \n    y_true = []\n    y_pred = []\n    for idx, (X_batch, y_true_batch) in tqdm(enumerate(dataset), total=total_steps):\n        y_true += np.argmax(y_true_batch, axis=2).flatten().tolist()\n        y_pred += np.argmax(pred(X_batch), axis=2).flatten().tolist()\n    \n    # Make Numpy arrays\n    y_true = np.array(y_true, dtype=np.int8)\n    y_pred = np.array(y_pred, dtype=np.int8)\n    \n    return y_true, y_pred\n\n# Validation Dataset\ny_true_val, y_pred_val = get_predictions(\n    get_dataset(X_val_input_ids, X_val_attention_masks, y_val, False),\n    VAL_STEPS_PER_EPOCH,\n)\n\n# Get Random Subset as prediction takes forever\nnp.random.seed(42)\ntrain_idxs = np.random.choice(a=np.arange(len(y_train)), size=len(y_val), replace=False)\n\n# Train Dataset\ny_true_train, y_pred_train = get_predictions(\n    get_dataset(X_train_input_ids[train_idxs], X_train_attention_masks[train_idxs], y_train[train_idxs], False),\n    VAL_STEPS_PER_EPOCH,\n)","3f925700":"def show_validation_report_per_class(y, y_pred):\n    report = classification_report(y, y_pred, target_names=LABELS, digits=3, output_dict=True)\n    report_df = pd.DataFrame.from_dict(report).T\n    report_df['support'] = report_df['support'].astype(int)\n    \n    pd.set_option('display.precision', 3)\n    \n    # Set empty Table Style\n    report_df = report_df.style.set_properties({})\n    \n    # Cell Styling\n    cells = {\n        'selector': 'td',\n        'props': 'font-size: 14pt',\n    }\n    \n    # Heading Styling\n    headers = {\n        'selector': 'th',\n        'props': 'font-size: 18pt'\n    }\n    \n    # Set Table Styling\n    report_df.set_table_styles([cells, headers])\n    \n    display(report_df)","afba2d23":"show_validation_report_per_class(y_true_val, y_pred_val)","d3a29423":"show_validation_report_per_class(y_true_train, y_pred_train)","9f1a9f94":"def plot_confusion_matrix(y, y_pred, title_prefix):\n    # Confusion matrix\n    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n    cfn_matrix = confusion_matrix(y, y_pred, labels=range(N_LABELS))\n    cfn_matrix = (cfn_matrix.T \/ cfn_matrix.sum(axis=1)).T\n    df_cm = pd.DataFrame(cfn_matrix, index=np.arange(N_LABELS), columns=np.arange(N_LABELS))\n    ax = sns.heatmap(df_cm, cmap='Blues', annot=True, fmt='.3f', linewidths=.5, annot_kws={'size':16})\n    plt.title(f'{title_prefix.upper()} CONFUSION MATRIX', size=32, pad=25)\n    plt.xticks(np.arange(N_LABELS) + 0.50, LABELS, fontsize=18, rotation=30)\n    plt.yticks(np.arange(N_LABELS) + 0.50, LABELS, fontsize=18, rotation=0)\n    plt.xlabel('PREDICTED', fontsize=24, labelpad=10)\n    plt.ylabel('ACTUAL', fontsize=24, labelpad=10)\n    plt.show()","20c79000":"# Validation Confusion Matrix\nplot_confusion_matrix(y_true_val, y_pred_val, 'validation')","e848865f":"# Train Confusion Matrix\nplot_confusion_matrix(y_true_train, y_pred_train, 'train')","ad5818b2":"# Plot Training History","f50a558b":"# Training Loop","11408910":"# Prediction Analysis","4b57a037":"# Logs\n\nCustom log function for the custom train function","89779293":"# Dataset","b385fc10":"# Class Weights\n\nClass weights to make the scale the loss of minority classes, not used for training.","e18e7577":"# History\n\nTraining\/Validation is monitored per step and as moving average.","4a56f1fe":"Hello Fellow Kagglers,\n\nThere are many Longformer notebooks out there, but I could not find one using gradient accumulation so here it is.\n\n**Why do we need Gradient Accumulation?**\n\nWith \"just\" 16GB of GPU memory the Longformer-base model can be trained with a batch size of 1. With gradient accumulation an infinite large batch size can be imitated. This will result in more precise weight updates, as low quality samples, for example by errornous notations, will be averaged out by the high quality samples. With a batch size of 1 these low quality samples would result in poor weight updates. With a large batch size of, for example 8, all 8 samples would have to be of low quality, which is unlikely, to result in the same poor weight update.\n\n**How does Gradient Accumulation Work?**\n\nGradient accumulation is the process of making several forward passes, computing the gradients each forward pass and averaging the gradients before doing a single backpropogation based on the average gradients. The final gradient is thus computed as:\n\n$$average\\ gradient = \\sum^{N}_{i=0} \\frac{gradient_{i}}{N}$$\n\nThis is exactly the same as with an actual mini batch size of N, but now computed over several forward passes instead of 1.\n\n\n[Preprocessing Notebook](https:\/\/www.kaggle.com\/markwijkhuizen\/preprocessing-oversampling)","71c89015":"# Reset Metrics","13e53037":"# Validation Report","cb756e98":"# Train Report","34712c83":"# Train\/Validation Split","b7093f0f":"# Validation Step","ab7316dd":"# Validation Dataset","adc0c857":"# Longformer Model\n\nLongformer model with two fully connected layers","1ed1270b":"# Labels\n\nEach discourse type has a seperate label. In addition a not-annotated label is added for words which are not annotated and and padding label is added.","87726b4a":"# F1 Score Without Padding\n\nThis custom F1 score ignores the padding class","0fec7b19":"# Custom Loss Function\n\nThis custom loss function allows for class weights and L2 regularization","01687884":"# Train Configuration","922bf490":"# Confusion Matrix","93b14ea5":"# Training Metrics","9d4e5326":"# Gradient Accumulation\n\nThis is the actual gradient accumulation training step","47658b70":"# Load Train","b3f03bc5":"# Weights L2 Distance\n\nreturns the L2 distance of all trainable weights ignoring biases","18ef203c":"# Validation Analysis","b63c81eb":"# Categorical Accuracy Without Padding\n\nThis custom categorical accuracy metric ignores the padding class ","1ee2f9c2":"# Train Dataset","ac13b262":"# Seed Everything for Deterministic Behaviour","543f177c":"# Dataset"}}