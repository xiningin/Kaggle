{"cell_type":{"a51f28c5":"code","29625545":"code","3aaaed86":"code","9ac318c4":"code","4c6e4ed7":"code","2168b961":"code","88481aff":"code","39906fd8":"code","650ae868":"code","7ce54b30":"code","42cfdf3d":"code","87d5f475":"code","06bd85b5":"code","da5ca4ea":"code","e9bbb2d2":"code","9a33ac28":"code","9f932d13":"code","c850cf0e":"code","932293c4":"code","d8d32744":"code","7d18d84d":"code","4007a1bf":"code","8bb5b235":"code","015409e9":"code","5397a9b1":"code","d2ec0266":"code","efed0dc1":"code","920015dd":"markdown","f611a384":"markdown","e668d025":"markdown","74c637e1":"markdown","f6bef5d0":"markdown","d9c520b8":"markdown","2e598da7":"markdown","75bd315e":"markdown","edc76b36":"markdown","8d8e8282":"markdown","ed107e7b":"markdown","94b08452":"markdown","ffc49f5c":"markdown"},"source":{"a51f28c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29625545":"import seaborn as sns\nimport tensorflow as tf","3aaaed86":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","9ac318c4":"df.columns","4c6e4ed7":"sns.countplot(df['target'])","2168b961":"\"Percentage Real Tweets = {:.1%}\".format(len(df[df['target']==1]) \/ len(df))\n","88481aff":"\nfrom sklearn.model_selection import train_test_split\n\nX = df['text']\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","39906fd8":"words = []\n\nfor item in df['text']:\n    words.append(len(item.split(' ')))\n    \ndf_wordlengths = pd.DataFrame(words)\ndf_wordlengths.describe()","650ae868":"g = sns.distplot(df_wordlengths,axlabel='Number of words in tweet')","7ce54b30":"train_labels = np.array(y_train,dtype=float) \ntest_labels = np.array(y_test,dtype=float)","42cfdf3d":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#Set Hyper Parameters\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 27 # Mean of Number of Words in Tweets + (2*Std)\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\nsequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences,max_length,truncating=trunc_type,padding=padding_type)\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_padded = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type,padding=padding_type)","87d5f475":"#Build Model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\n\n#Compile Model\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","06bd85b5":"#Fit the model\nnum_epochs = 10\nhistory = model.fit(x=padded,y=train_labels,epochs=num_epochs,validation_data=(test_padded,test_labels))","da5ca4ea":"#Evaluate Accuracy\n\ntest_loss, test_acc = model.evaluate(test_padded,test_labels,verbose=2)\n\nprint('\\nTest Accuracy:',test_acc)","e9bbb2d2":"#Plot the history plot\nimport matplotlib.pyplot as plt\n\ndef plot_graphs(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Ephochs\")\n    plt.ylabel(string)\n    plt.legend([string,'validation_'+string])\n    plt.show()\n\nplot_graphs(history,\"accuracy\")\nplot_graphs(history,\"loss\")","9a33ac28":"predictions = model.predict(test_padded)","9f932d13":"def rounding(x):\n    '''Round results to 0 (No Disaster) or 1 (Real Disaster)'''\n    if x < 0.5:\n        return 0\n    else:\n        return 1\n    \npredictions_final = [rounding(x) for x in predictions]","c850cf0e":"sns.countplot(predictions_final)","932293c4":"for x in range(0,5):\n    print(f'Tweet: {X_test.iloc[x]},\\n Prediction: {predictions[x]},\\n  Target:{test_labels[x]}\\n')","d8d32744":"from sklearn.metrics import classification_report,confusion_matrix","7d18d84d":"print(classification_report(predictions_final,y_test))","4007a1bf":"submission_sequences = tokenizer.texts_to_sequences(df_test['text'])\nsubmission_padded = pad_sequences(submission_sequences,maxlen=max_length,truncating=trunc_type,padding=padding_type)","8bb5b235":"submission_predictions = model.predict(submission_padded)","015409e9":"df_test['target'] = [rounding(x) for x in submission_predictions]","5397a9b1":"df_submission = df_test.drop(['keyword','location','text'],axis=1)","d2ec0266":"df_submission","efed0dc1":"df_submission.to_csv('submission.csv',index=False,header=True)","920015dd":"First, split the training and test set","f611a384":"Overview of results - taking a peak at the tweets and their predictions","e668d025":"Tokenize text and set hyperparameters to be used in the model","74c637e1":"Assess prediction results","f6bef5d0":"# Predict on test data from training set","d9c520b8":"Extract information about length of tweet to use for hyperparameters","2e598da7":"# Import Data","75bd315e":"Plot the accuracy and MSE per epoch","edc76b36":"# Build the Model\n\n1. Build Model\n2. Compile the model\n3. Fit the model\n4. Evaluate Accuracy","8d8e8282":"Convert Labels to numpy arrays","ed107e7b":"Round predictions to zero or one for final output","94b08452":"# Submission\n\nRun the test.csv through the model and output it into an excel for submission","ffc49f5c":"# Tokenize Text\n\nText is tokenized using keras tokenizer below"}}