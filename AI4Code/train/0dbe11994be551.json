{"cell_type":{"cd42bd1d":"code","e7c09bb0":"code","b39b0192":"code","0fcdf3b7":"code","9543e717":"code","03713c81":"code","3404ff01":"code","d8e77b1e":"markdown","885c36e5":"markdown","ae99d721":"markdown","5ffdfbb6":"markdown","590771f3":"markdown","e45212dc":"markdown","7c633b83":"markdown","3d486a9e":"markdown"},"source":{"cd42bd1d":"# Re-loads all imports every time the cell is ran. \n%load_ext autoreload\n%autoreload 2\n\nfrom time import time\n\nimport numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:,.5f}'.format\n\nfrom IPython.display import display\n\n# Sklearn tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Neural Networks\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers.csv_logs import CSVLogger\n\n# Plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e7c09bb0":"class TimeseriesDataset(Dataset):   \n    '''\n    Custom Dataset subclass. \n    Serves as input to DataLoader to transform X \n      into sequence data using rolling window. \n    DataLoader using this dataset will output batches \n      of `(batch_size, seq_len, n_features)` shape.\n    Suitable as an input to RNNs. \n    '''\n    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n        self.X = torch.tensor(X).float()\n        self.y = torch.tensor(y).float()\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])","b39b0192":"class PowerConsumptionDataModule(pl.LightningDataModule):\n    '''\n    PyTorch Lighting DataModule subclass:\n    https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/datamodules.html\n\n    Serves the purpose of aggregating all data loading \n      and processing work in one place.\n    '''\n    \n    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n        super().__init__()\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.X_train = None\n        self.y_train = None\n        self.X_val = None\n        self.y_val = None\n        self.X_test = None\n        self.X_test = None\n        self.columns = None\n        self.preprocessing = None\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n        '''\n        Data is resampled to hourly intervals.\n        Both 'np.nan' and '?' are converted to 'np.nan'\n        'Date' and 'Time' columns are merged into 'dt' index\n        '''\n\n        if stage == 'fit' and self.X_train is not None:\n            return \n        if stage == 'test' and self.X_test is not None:\n            return\n        if stage is None and self.X_train is not None and self.X_test is not None:  \n            return\n        \n        path = '\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt'\n        \n        df = pd.read_csv(\n            path, \n            sep=';', \n            parse_dates={'dt' : ['Date', 'Time']}, \n            infer_datetime_format=True, \n            low_memory=False, \n            na_values=['nan','?'], \n            index_col='dt'\n        )\n\n        df_resample = df.resample('h').mean()\n\n        X = df_resample.dropna().copy()\n        y = X['Global_active_power'].shift(-1).ffill()\n        self.columns = X.columns\n\n\n        X_cv, X_test, y_cv, y_test = train_test_split(\n            X, y, test_size=0.2, shuffle=False\n        )\n    \n        X_train, X_val, y_train, y_val = train_test_split(\n            X_cv, y_cv, test_size=0.25, shuffle=False\n        )\n\n        preprocessing = StandardScaler()\n        preprocessing.fit(X_train)\n\n        if stage == 'fit' or stage is None:\n            self.X_train = preprocessing.transform(X_train)\n            self.y_train = y_train.values.reshape((-1, 1))\n            self.X_val = preprocessing.transform(X_val)\n            self.y_val = y_val.values.reshape((-1, 1))\n\n        if stage == 'test' or stage is None:\n            self.X_test = preprocessing.transform(X_test)\n            self.y_test = y_test.values.reshape((-1, 1))\n        \n\n    def train_dataloader(self):\n        train_dataset = TimeseriesDataset(self.X_train, \n                                          self.y_train, \n                                          seq_len=self.seq_len)\n        train_loader = DataLoader(train_dataset, \n                                  batch_size = self.batch_size, \n                                  shuffle = False, \n                                  num_workers = self.num_workers)\n        \n        return train_loader\n\n    def val_dataloader(self):\n        val_dataset = TimeseriesDataset(self.X_val, \n                                        self.y_val, \n                                        seq_len=self.seq_len)\n        val_loader = DataLoader(val_dataset, \n                                batch_size = self.batch_size, \n                                shuffle = False, \n                                num_workers = self.num_workers)\n\n        return val_loader\n\n    def test_dataloader(self):\n        test_dataset = TimeseriesDataset(self.X_test, \n                                         self.y_test, \n                                         seq_len=self.seq_len)\n        test_loader = DataLoader(test_dataset, \n                                 batch_size = self.batch_size, \n                                 shuffle = False, \n                                 num_workers = self.num_workers)\n\n        return test_loader","0fcdf3b7":"class LSTMRegressor(pl.LightningModule):\n    '''\n    Standard PyTorch Lightning module:\n    https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/lightning_module.html\n    '''\n    def __init__(self, \n                 n_features, \n                 hidden_size, \n                 seq_len, \n                 batch_size,\n                 num_layers, \n                 dropout, \n                 learning_rate,\n                 criterion):\n        super(LSTMRegressor, self).__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n\n        self.lstm = nn.LSTM(input_size=n_features, \n                            hidden_size=hidden_size,\n                            num_layers=num_layers, \n                            dropout=dropout, \n                            batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        # lstm_out = (batch_size, seq_len, hidden_size)\n        lstm_out, _ = self.lstm(x)\n        y_pred = self.linear(lstm_out[:,-1])\n        return y_pred\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        result = pl.TrainResult(loss)\n        result.log('train_loss', loss)\n        return result\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        result = pl.EvalResult(checkpoint_on=loss)\n        result.log('val_loss', loss)\n        return result\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        result = pl.EvalResult()\n        result.log('test_loss', loss)\n        return result","9543e717":"'''\nAll parameters are aggregated in one place.\nThis is useful for reporting experiment params to experiment tracking software\n'''\n\np = dict(\n    seq_len = 24,\n    batch_size = 70, \n    criterion = nn.MSELoss(),\n    max_epochs = 10,\n    n_features = 7,\n    hidden_size = 100,\n    num_layers = 1,\n    dropout = 0.2,\n    learning_rate = 0.001,\n)","03713c81":"seed_everything(1)\n\ncsv_logger = CSVLogger('.\/', name='lstm', version='0'),\n\ntrainer = Trainer(\n    max_epochs=p['max_epochs'],\n    logger=csv_logger,\n    gpus=1,\n    row_log_interval=1,\n    progress_bar_refresh_rate=2,\n)\n\nmodel = LSTMRegressor(\n    n_features = p['n_features'],\n    hidden_size = p['hidden_size'],\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size'],\n    criterion = p['criterion'],\n    num_layers = p['num_layers'],\n    dropout = p['dropout'],\n    learning_rate = p['learning_rate']\n)\n\ndm = PowerConsumptionDataModule(\n    seq_len = p['seq_len'],\n    batch_size = p['batch_size']\n)\n\ntrainer.fit(model, dm)\ntrainer.test(model, datamodule=dm)","3404ff01":"metrics = pd.read_csv('.\/lstm\/0\/metrics.csv')\ntrain_loss = metrics[['train_loss', 'step', 'epoch']][~np.isnan(metrics['train_loss'])]\nval_loss = metrics[['val_loss', 'epoch']][~np.isnan(metrics['val_loss'])]\ntest_loss = metrics['test_loss'].iloc[-1]\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\naxes[0].set_title('Train loss per batch')\naxes[0].plot(train_loss['step'], train_loss['train_loss'])\naxes[1].set_title('Validation loss per epoch')\naxes[1].plot(val_loss['epoch'], val_loss['val_loss'], color='orange')\nplt.show(block = True)\n\nprint('MSE:')\nprint(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\nprint(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\nprint(f'Test loss:  {test_loss:.3f}')","d8e77b1e":"# DataModule","885c36e5":"# Parameters","ae99d721":"# Plot report","5ffdfbb6":"# Prediction task\n\nWe are going to predict hourly levels of global active power one step ahead.","590771f3":"# Train loop","e45212dc":"# TimeseriesDataset\n","7c633b83":"# Model\nImplement LSTM regressor using pytorch-lighting module","3d486a9e":"# Import dependencies"}}