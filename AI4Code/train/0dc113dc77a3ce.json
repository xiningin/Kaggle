{"cell_type":{"c47a0ab2":"code","0f40748e":"code","4d6ecc53":"code","ba5c70ad":"code","3312cf48":"code","6e395b9e":"code","2d45e7d7":"code","023edaaf":"code","f4738053":"code","7482d9fd":"code","f4f37252":"code","03cc3bc8":"code","c1f62dce":"code","3dc5d540":"code","d4dcac1b":"code","00038e0c":"code","cf3fe473":"code","abda8608":"code","7840a24c":"code","74b8f051":"code","d557fedb":"code","fa566456":"markdown"},"source":{"c47a0ab2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f40748e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf_positive=pd.read_csv(\"..\/input\/preprocessed-twitter-tweets\/processedNegative.csv\",squeeze=True).T.reset_index()\ndf_neutral=pd.read_csv(\"..\/input\/preprocessed-twitter-tweets\/processedNeutral.csv\",squeeze=True).T.reset_index()\ndf_negative=pd.read_csv(\"..\/input\/preprocessed-twitter-tweets\/processedPositive.csv\",squeeze=True).T.reset_index()\ndf_positive.head()\n","4d6ecc53":"#Adding Sentiments column\ndf_positive['Sentiments']=1\ndf_neutral['Sentiments']=0\ndf_negative['Sentiments']=-1","ba5c70ad":"#merging dataframes\ndf=pd.concat([df_negative,df_positive,df_neutral],axis=0,ignore_index=True)\ndf.columns=['Tweets','Sentiments']\ndf['Sentiments'].value_counts()","3312cf48":"df.info()","6e395b9e":"#clean data\nfrom  nltk.stem.porter import PorterStemmer\nfrom  nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nps=PorterStemmer()\n\n\ndef cleanText(text):\n  text=re.sub(r'@[A-Za-z0-9]+','',text) # removed @mentions\n  text=re.sub(r'#','',text) #removing symbol #\n  text=re.sub(r'RT[\\s]+','',text) #removing retweets\n  text=re.sub(r'https?:\\\/\\\/\\S+','',text) #removing hyperlinks\n  text=re.sub(r'[^A-Za-z\\s]','',text) #removing special characters\n  text=text.lower()\n  text=text.split()\n  text=[ps.stem(word) for word in text if not word in stopwords.words('english')]\n  text=' '.join(text)\n  return text\n\ndf['Tweets']=df['Tweets'].apply(cleanText)\ndf\n","2d45e7d7":"#Visualize\n\nallWords=\" \".join([sentence for sentence in df['Tweets']])\n\nfrom wordcloud import WordCloud\nwordcloud=WordCloud(width=800,height=500,random_state=42,max_font_size=100).generate(allWords)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","023edaaf":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    df['Tweets'].values, df['Sentiments'].values, test_size=0.25, random_state=42)","f4738053":"#converting string into integer using Tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#instantialing tokenizer\nmax_vocab=500000\ntokenizer=Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)\n","7482d9fd":"#vocabulary of dataset\nwordidx=tokenizer.word_index\nlen(wordidx)","f4f37252":"#converting sentence to sequence\ntrain_seq=tokenizer.texts_to_sequences(X_train)\ntest_seq=tokenizer.texts_to_sequences(X_test)\nlen(train_seq),len(y_train)","03cc3bc8":"#pad sequences to equal length\npad_train=pad_sequences(train_seq)\nT=pad_train.shape[1]\npad_train.shape","c1f62dce":"pad_test=pad_sequences(test_seq,maxlen=T)\npad_test.shape","3dc5d540":"from tensorflow.keras.layers import Embedding, LSTM, Dense,GlobalMaxPool1D,Input\nfrom tensorflow.keras.models import Model\nM=100 # lstm dimension\nD=20  #embedding_features\ni = Input(shape=(T, ))\nx= Embedding(max_vocab+1,D)(i)\nx=LSTM(M,return_sequences=True)(x)\nx=GlobalMaxPool1D()(x)\nx=Dense(32,activation='relu')(x)\nx=Dense(1,activation='softmax')(x)\n\nmodel=Model(i,x)\nprint(model.summary())\nwarnings.filterwarnings(\"ignore\")","d4dcac1b":"#model compile\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nr=model.fit(pad_train,y_train,validation_data=(pad_test,y_test),epochs=100,batch_size=64)","00038e0c":"#plotting the loss and val loss\nplt.plot(r.history['loss'],label='loss')\nplt.plot(r.history['val_loss'],label='Validation loss')\nplt.legend()\n","cf3fe473":"#plotting the accuracy and val accuracy\nplt.plot(r.history['accuracy'],label='accuracy')\nplt.plot(r.history['val_accuracy'],label='Validation accuracy')\nplt.legend()","abda8608":"from tensorflow.keras.layers import Embedding, LSTM, Dense,GlobalMaxPool1D,Input,Bidirectional\nfrom tensorflow.keras.models import Model\nM=100 # lstm dimension\nD=20  #embedding_features\ni = Input(shape=(T, ))\nx= Embedding(max_vocab+1,D)(i)\nx=Bidirectional(LSTM(M,return_sequences=True))(x)\nx=GlobalMaxPool1D()(x)\nx=Dense(32,activation='relu')(x)\nx=Dense(1,activation='softmax')(x)\n\nmodel2=Model(i,x)\nprint(model2.summary())","7840a24c":"#model compile\nmodel2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nr2=model2.fit(pad_train,y_train,validation_data=(pad_test,y_test),epochs=100,batch_size=64)","74b8f051":"#plotting the loss and val loss\nplt.plot(r2.history['loss'],label='loss')\nplt.plot(r2.history['val_loss'],label='Validation loss')\nplt.legend()","d557fedb":"#plotting the accuracy and val accuracy\nplt.plot(r2.history['accuracy'],label='accuracy')\nplt.plot(r2.history['val_accuracy'],label='Validation accuracy')\nplt.legend()","fa566456":" #  **Using LSTM**"}}