{"cell_type":{"41c9a514":"code","401db3bc":"code","6fefa5ed":"code","b826fad5":"code","c5c26203":"code","f432fa5e":"code","6b32c27b":"code","362bf2a7":"code","f75ab3cd":"code","dde315c8":"code","e434ed0c":"code","dd97e3fd":"code","9c74b373":"code","fb00c238":"code","cf2b4ffb":"code","5c36579f":"code","0136c278":"code","99f391dd":"code","0ab65496":"code","68b66f0f":"code","5632c563":"code","b02f7ab9":"code","42b29b33":"code","ef5a0f3e":"code","60b1d68d":"code","8ce024d2":"code","56da8a76":"code","cbf748ce":"markdown","b40761a8":"markdown","f2dbf77c":"markdown","79f05a90":"markdown","b7199f23":"markdown","d6007d41":"markdown","79a0de1d":"markdown","cbdf706b":"markdown","b009a4f9":"markdown","7461d811":"markdown","4801ba0c":"markdown","13571b1c":"markdown","40d81e3e":"markdown","5fd256be":"markdown","69a44e33":"markdown","bf237acc":"markdown","5552694e":"markdown","5bf61f3e":"markdown","dfdcea38":"markdown","6fe43be5":"markdown","857a7bdc":"markdown"},"source":{"41c9a514":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns","401db3bc":"filepath1=\"\/kaggle\/input\/youtube-new\/US_category_id.json\"\ncategory_id_df = pd.read_json(filepath1)\ncategory_id_df.head()","6fefa5ed":"filepath2=\"\/kaggle\/input\/youtube-new\/USvideos.csv\"\nvideos_df = pd.read_csv(filepath2,header='infer')\nvideos_df.head()","b826fad5":"def clean_video_csv(video_df,country_code):\n    \"\"\"\n    This function is to remove unnecessary chars like '\"',',','\\r'which will cause errors when copy csv files into staging table.\n    \n    Parameters:\n    video_df: Dataframe from read_csv file\n    filepath: videos csv filepath\n    \n    Return:\n    video_df: Dataframe which remove unnecessary chars\n    \"\"\"\n    video_df[\"tags\"] = video_df[\"tags\"].apply(lambda x:x.replace('\"',\"\"))\n    video_df[\"title\"] = video_df[\"title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"channel_title\"] = video_df[\"channel_title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\\r',''))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\"',''))\n    video_df[\"country\"] = country_code\n    return video_df\n#Clean videos csv files for selected country code\ncountry_code=['US']\nfor c in country_code:\n    filepath=\"\/kaggle\/input\/youtube-new\/\"+c+\"videos.csv\"\n    video_df = pd.read_csv(filepath,header='infer')\n    savepath = \"\/kaggle\/working\/\"+c+\"videos1.csv\"\n    video_df = clean_video_csv(video_df,c)\n    video_df.to_csv(savepath,index=False)","c5c26203":"def category_extract (df,country_code):\n    \"\"\"\n    The function is to extract category id and category title from category_id json files\n    \n    Parameters:\n    df: Dataframe of read_json file\n    filepath: category_id json filepath\n    \n    Return:\n    category_df: Dataframe with columns: category_id,category_title,category_filename,country_code\n    \n    \"\"\"\n    category_id = []\n    category_title = []\n    for i in range(df.shape[0]):\n        category_id.append(df.iloc[i][\"items\"]['id'])\n        category_title.append(df.iloc[i][\"items\"][\"snippet\"][\"title\"])\n    category_df = pd.DataFrame()\n    category_df[\"category_id\"] = category_id\n    category_df[\"category_title\"] = category_title\n    category_df.insert(category_df.shape[1],\"country_code\",country_code)\n    return category_df\n\n#Extract category title and id from json file of each country\n\ncategory_all = pd.DataFrame()\nfor c in country_code:\n    filepath=\"\/kaggle\/input\/youtube-new\/\"+c+\"_category_id.json\"\n    category_id_df = pd.read_json(filepath)\n    category_all = pd.concat([category_all,category_extract(category_id_df,c)])\n    \n#category_all.tail()\nsavepath = \"\/kaggle\/working\/category_all.csv\"\ncategory_all.to_csv(savepath,index=False)","f432fa5e":"US = pd.read_csv(\"\/kaggle\/working\/USvideos1.csv\")\nUS.head()\n","6b32c27b":"category = pd.read_csv(\"\/kaggle\/working\/category_all.csv\")\ncategory.head()","362bf2a7":"trendingdate_df = US.groupby(\"video_id\").trending_date.describe().reset_index()\ntrendingdate_df.head()","f75ab3cd":"pd.DataFrame(trendingdate_df[\"count\"].astype('int').copy()).describe()","dde315c8":"sns.distplot(trendingdate_df[\"count\"])","e434ed0c":"US1 =US.merge(category,how=\"inner\",left_on=[\"category_id\",\"country\"],right_on=[\"category_id\",\"country_code\"])\nUS1[\"trending_date1\"] = US1[\"trending_date\"].apply(lambda x: pd.Timestamp(int(\"20\"+x[0:2]),int(x[-2:]),int(x[3:5]),0))","dd97e3fd":"columns = [\"video_id\",\"trending_date1\",\"channel_title\",\"publish_time\",\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_title\"]\nUS1 = US1[columns].copy()","9c74b373":"videos = trendingdate_df[trendingdate_df[\"count\"].values>=10].video_id\nUS2 = US1[ US1.video_id.isin(videos.values)]","fb00c238":"def standardize(data):\n    scaler = StandardScaler()\n    scaler = scaler.fit(data)\n    transformed = scaler.transform(data)\n    return scaler,transformed\nscaler_views, US_views = standardize(US2.views.values.reshape(-1,1))\n#scaler_likes, US_likes = standardize(US2.likes.values.reshape(-1,1))\n#scaler_dislikes, US_dislikes = standardize(US2.dislikes.values.reshape(-1,1))\n#scaler_comments, US_comments = standardize(US2.comment_count.values.reshape(-1,1))","cf2b4ffb":"US3 = pd.DataFrame()\nUS3[\"trending_date1\"] = US2[\"trending_date1\"]\nUS3[\"video_id\"] = US2[\"video_id\"]\nUS3[\"views\"] = US_views\n#US3[\"likes\"] = US_likes\n#US3[\"dislikes\"] = US_dislikes\n#US3[\"comment_count\"] = US_comments\nUS3.reset_index(inplace=True)\nUS3.head()","5c36579f":"US3.drop(\"index\",axis=1,inplace=True)\nUS3.head()","0136c278":"\nx=[]\ny=[]\ncategory = []\nfor v in videos:\n    row=[]\n    temp_df = US3[US3[\"video_id\"]==v].sort_values(by=\"trending_date1\")\n    #print (temp_df)\n    seq = temp_df.views[0:9].index #first 9 views as input\n        \n    for s in seq:\n        #print (US3.iloc[s].values[2:])\n        row.append(US3.iloc[s].values[2:])\n    x.append(row)\n    nextstep = temp_df.views[9:10].values # the 10th views as output\n    y.append(nextstep)\n    ","99f391dd":"\nx = np.reshape(x,(len(x),9,1)) #input shape(len(x),timesteps,dimensions)\nprint (x.shape)\ny = np.reshape(y,(-1,1))\nprint (y.shape)\nx = x.astype('float64')\ny = y.astype('float64')","0ab65496":"from sklearn.model_selection import ShuffleSplit,train_test_split\nx_train,x_testall,y_train,y_testall = train_test_split(x,y,test_size=0.4,random_state=42)\nx_val,x_test,y_val,y_test = train_test_split(x_testall,y_testall,test_size=0.5,random_state=42)","68b66f0f":"x_train.shape\nx_val.shape\nx_test.shape","5632c563":"from keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.layers.core import Dense, Activation, Dropout","b02f7ab9":"model = Sequential()\nmodel.add(LSTM(64, return_sequences=True,input_shape=(9, 1),activation='tanh'))#32 is output,  input shape\u662f3\u7ef4: (Batch_size, Time_step, Input_Sizes),\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128,return_sequences=False,activation='tanh'))\nmodel.add(Dense(1, activation='linear'))#1 is output\n\noptimizer = RMSprop(lr=0.005)\nmodel.compile(loss='mean_squared_error', optimizer=optimizer)","42b29b33":"from keras.callbacks import ModelCheckpoint  \ncheckpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit(x_train, y_train, \n          validation_data=(x_val, y_val),\n          epochs=50, batch_size=64, callbacks=[checkpointer], verbose=1)","ef5a0f3e":"model.load_weights('weights.best.from_scratch.hdf5')","60b1d68d":"import math\nfrom sklearn.metrics import mean_squared_error\n\n# make predictions\npredict_train = model.predict(x_train)\npredict_test = model.predict(x_test)\n# invert predictions\npredict_train = scaler_views.inverse_transform(predict_train)\ny_train = scaler_views.inverse_transform(y_train)\npredict_test = scaler_views.inverse_transform(predict_test)\ny_test= scaler_views.inverse_transform(y_test)\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(predict_train, y_train))\nprint('Train Score: %.2f RMSE' % (trainScore))\nvalScore = math.sqrt(mean_squared_error(predict_test, y_test))\nprint('Val Score: %.2f RMSE' % (valScore))","8ce024d2":"predict = scaler_views.inverse_transform(model.predict(x_test))\n\nprint (\"the demo predict is %s\" %(predict.astype('int')[0:10]))\nprint (\"the demo true is %s\" %(y_test.astype('int')[0:10]))\n#print (mean_squared_error(predict,true))","56da8a76":"pd.DataFrame(y_test).describe()","cbf748ce":"The median of y_test is 1352392 , the rmse is about 391828(slightly difference for each training process\n), that means about 50% views have predict error less than 27%.","b40761a8":"### More Test and Discussion\n1. For shorter time sequence, will the prediction do better or worse? \nI have tested timestep=4 and got RMSE on trainset is 817809,on testset is 305374 . We have a better performance in predict shorter time sequence probably because of we have more data of 4 sequence days than 9 sequence days. \n2. If adding likes, dislikes,comment count to training, will the model perform better?\nWe keep timestep =4 and set dimension =4 and add more 3 columns(likes,dislikes,comment_counts) and got RMSE on trainset is 569316 ,on testset is 373221 We got a better performance on trainset while worse performance on testset. \n\nWe come to a conclusion that more training data will do good for the result and adding more dimensions does not necessarily yield in a better result.\n\nThank you for reading. Have fun to try!","f2dbf77c":"The distplot of how many days videos have.","79f05a90":"Since category id in different countries are not the same, we have to extract categoty id and titles from json file of each country and save it as a csv file for later use. ","b7199f23":"We create the dataframe we'll use in building model.","d6007d41":"We extract the first 9days views in x and 10th day views in y as label.","79a0de1d":"Build up Lstm model by Keras. We use mean square error as loss function and RMSprop as optimizer algorithm.","cbdf706b":"about 75% videos have more than 3 trending days. The median days is 6. Laterly, We found we got a better performance in predicting 4 sequence days than 9 sequence days, probably because of we have more data of 4 sequence days. ","b009a4f9":"We shape input x as (len(x),timesteps,dimensions), here timesteps is 9 because we use the first 9 views to predict the 10th views. The dimension is 1 because we only use views to predict.","7461d811":"### Data Modeling","4801ba0c":"There are two group of data files, videos data file and category data file.","13571b1c":"Spliting train and test data.","40d81e3e":"Let's merge US videos dataset and category dataset and transform trending_data into timestamp","5fd256be":"### Data Transformation","69a44e33":"Here we only keep videos which have more than 10 trending days. ","bf237acc":"We extract the columns we'll use ","5552694e":"We'll standardize numerical data. Here we standardize views,likes,dislikes and comment counts with zero mean and unit variace. We'll only use views later.","5bf61f3e":"Let 's see how many trending days videos have. ","dfdcea38":"If you want to load data to relational database for later analysis. You may encounter some problems due to unnecessary chars like '\"' ',' '\\r' in title,description and channel title columns. You have to remove all these problematic chars. Be careful that each country may have different chars need to be removed. We only take US dataset as an example.","6fe43be5":"### Data Read and Exploration","857a7bdc":"### Motivation\nIn this notebook, I'm going to built a LSTM model to predict views trending of vidoes from Trending YouTube Video Statistic dataset. There are lots of data from different countries. I only use US videos as demo. You can also predict likes, dislikes or comment counts in a similar way while I only use views prediction as demo. Feel free to amend the code and try something new. I have tried several senarios and found something interesting. I hope you like it .Let's get started."}}