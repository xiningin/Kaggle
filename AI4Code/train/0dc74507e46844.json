{"cell_type":{"e7e5e6e2":"code","951cc277":"code","2777223c":"code","e131288e":"code","04100420":"code","99a37d01":"code","35b301e4":"code","b3e58fcb":"code","87755d2f":"code","bbfaa5ae":"code","012a6d01":"code","31a3ed94":"code","aaa4a26b":"code","9bb16d88":"code","4f39a487":"code","b68639bf":"code","2b6cbcbe":"code","5b67d68d":"code","ffb17539":"code","0dc93155":"code","bc87c64d":"code","4e023f02":"code","eb4853bd":"code","c620adcc":"code","8c8c393b":"code","69228be0":"code","9f5f3b47":"code","14527aa8":"code","d808bf95":"code","c1782628":"code","816d27dd":"code","3b12aca6":"code","f32f54d1":"code","7e1db26b":"code","39c248ea":"code","ab30e803":"code","641b4477":"code","465302bf":"code","d07f5064":"code","985ee308":"code","e82f35d8":"code","6679e437":"markdown","fc42b7c1":"markdown","9113a0fd":"markdown","be8498ee":"markdown","b1f998fb":"markdown","dfe09b05":"markdown","32fea1f5":"markdown","fb0deb8f":"markdown"},"source":{"e7e5e6e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","951cc277":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","2777223c":"# import datasets\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","e131288e":"train_df.head(20)","04100420":"train_df.claim.nunique()","99a37d01":"train_df.shape","35b301e4":"train_df.isnull().sum()","b3e58fcb":"train_df.describe()","87755d2f":"test_df.describe()","bbfaa5ae":"missing_train_df = pd.DataFrame(train_df.isna().sum(axis=0))\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['count_percent'] = missing_train_df['count']\/train_df.shape[0]\n\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['count_percent'] = missing_test_df['count']\/test_df.shape[0]","012a6d01":"missing_test_df","31a3ed94":"missing_train_row = train_df.drop(['id', 'claim'], axis=1).isna().sum(axis=1)\nmissing_train_feature_numbers = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_feature_numbers.columns = ['no_of_feature', 'count_percent']\n\nmissing_test_row = test_df.drop(['id'], axis=1).isna().sum(axis=1)\nmissing_test_feature_numbers = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_feature_numbers.columns = ['no_of_feature', 'count_percent']","aaa4a26b":"missing_test_feature_numbers","9bb16d88":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","4f39a487":"train_df['num_nulls'].corr(train_df['claim'])","b68639bf":"train_df.claim.value_counts()","2b6cbcbe":"train_df","5b67d68d":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\n\nfeatures = [col for col in train_df.columns if col not in ['claim', 'id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\ntrain_df[features] = pipe.fit_transform(train_df[features])\ntest_df[features] = pipe.transform(test_df[features])","ffb17539":"test_df.isnull().sum()","0dc93155":"train_df","bc87c64d":"target = train_df['claim'].copy()","4e023f02":"train_df.drop(['num_nulls','claim','id'], inplace=True, axis=1)","eb4853bd":"test_df.drop(['num_nulls','id'], inplace=True, axis=1)","c620adcc":"target","8c8c393b":"train_df","69228be0":"test_df","9f5f3b47":"# outlier handling","14527aa8":"# transformations","d808bf95":"# standradization","c1782628":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# create the scaler object\nscaler = StandardScaler()\n\n# Same as previous -  we only fit the training data to scaler\nscaler.fit(train_df)\n\ntrain_scaled = scaler.transform(train_df)\ntest_scaled = scaler.transform(test_df)\n\nstandardized_df_train = pd.DataFrame(train_scaled, columns = train_df.columns)\nstandardized_df_test = pd.DataFrame(test_scaled, columns = test_df.columns)\n\nstandardized_df_train","816d27dd":"# PCA\nfrom sklearn.decomposition import PCA\n# Make an instance of the Model\npca = PCA()\npca.fit(standardized_df_train) \n\nX_train_pca = pca.transform(standardized_df_train)\nX_test_pca = pca.transform(standardized_df_test)","3b12aca6":"explained_variance_ratio=pca.explained_variance_ratio_\nexplained_variance_ratio","f32f54d1":"arr=explained_variance_ratio \nsum = 0;  \nfor i in range(0, 110):    \n   sum=sum+arr[i]   \nprint(\"Sum :\" + str(sum));  ","7e1db26b":"# not much of diamentions can be reduced","39c248ea":"pca = PCA(n_components=110)\npca.fit(standardized_df_train) \n\nX_train_pca = pca.transform(standardized_df_train)\nX_test_pca = pca.transform(standardized_df_test)","ab30e803":"#Implementing Linear Regression\nfrom sklearn import linear_model\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X_train_pca,target)","641b4477":"# Fitting Polynomial Regression to the dataset\n# from sklearn.preprocessing import PolynomialFeatures\n# poly_reg = PolynomialFeatures(degree=4)\n# X_poly = poly_reg.fit_transform(standardized_df_train)\n# pol_reg = LinearRegression()\n# model2=pol_reg.fit(X_poly, target)","465302bf":"predictions = lm.predict(X_test_pca)\ny_hat = pd.DataFrame(predictions, columns=[\"predicted\"])\nprint(y_hat.head(10)) #print predictions for first ten values","d07f5064":"y_hat['predicted'] = y_hat['predicted'].round(decimals = 1)","985ee308":"y_hat","e82f35d8":"submission['claim'] = y_hat\nsubmission.to_csv('submission.csv', index=False)","6679e437":"# Load the Data","fc42b7c1":"# A lot of missing values. Lets see the correlation","9113a0fd":"Lets see why people are obsessed with the null counts","be8498ee":"Good to go.Can optimize later","b1f998fb":"# Lets see data","dfe09b05":"Damn!! thats a large correlation . Need to keep this factor. ","32fea1f5":"# So now the work remaining is the removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","fb0deb8f":"# Lets also look if we have imbalance case"}}