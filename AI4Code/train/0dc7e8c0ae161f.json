{"cell_type":{"13b2b517":"code","74780bb0":"code","d0655de4":"code","9752e40f":"code","8aeab5f8":"code","c299c114":"code","721051b5":"code","852128cc":"code","4e413df8":"code","5be8d3e1":"code","261f21d8":"code","da3d11c8":"code","ad2578a3":"code","c15b1474":"code","27407e91":"code","550ed21f":"code","8e84fd7b":"code","1b945de2":"code","25ca6c0f":"code","bad07209":"code","0743e12b":"code","e9f3c745":"code","4f3a3cec":"code","a3924f5b":"code","01a7dfe8":"code","1dfc7b25":"code","d688b2f6":"code","df94041f":"code","909bf43c":"code","b0628a8f":"code","1ef80651":"code","cc7642ce":"code","383073ac":"code","948e5643":"code","777ab39c":"code","8e91f4e9":"code","d7313fbe":"code","7fe562ea":"code","2d37de78":"code","7fa33bc8":"code","bab3f97c":"code","a5510ddd":"code","89382013":"code","3dbf6180":"code","63fcbdf3":"code","6f0ea333":"code","3dabda16":"code","f81e345f":"code","d881b22f":"code","bb3a9869":"code","d75781c5":"code","94eb23fc":"code","8902bdb3":"code","9082e257":"code","f13dad55":"code","cc77a185":"code","e06e9084":"code","f4141032":"code","2729346a":"code","95041296":"code","f8ca00c3":"code","ad0cf26e":"code","61edade1":"code","004025c2":"code","ed697d8c":"code","43fe7415":"code","a64a6479":"code","3e16aa45":"code","9bb96221":"code","294d428c":"markdown","7751cb92":"markdown","37bce62e":"markdown"},"source":{"13b2b517":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74780bb0":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","d0655de4":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","9752e40f":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.head(15)","8aeab5f8":"df.describe(include='all')","c299c114":"df.info()","721051b5":"df.isnull().sum()","852128cc":"# due to many missing values column 'Cabin' will be dropped\ndf.drop('Cabin',axis=1, inplace=True)\n# PassngerId contains ordinal numbers it should not have influence on Survival rate therefore this column will be dropped.\ndf.drop('PassengerId',axis=1, inplace=True)\n# Ticket number also shouldn't be connected with survival rate therefore this column will be also dropped.\ndf.drop('Ticket',axis=1, inplace=True)","4e413df8":"# Column name contains also a Title, which should tell us more than a name\ndf['Title'] = df['Name'].str.split(',', expand=True)[1].str.split('.',expand=True)[0]\ndf['Title'] = df['Title'].transform(lambda x: x.strip() if x.strip() in ['Mr', 'Miss', 'Mrs', 'Master'] else 'Rare')\ndf['Title'].value_counts()","5be8d3e1":"# After extracting Title from Name, this column might be dropped\ndf.drop('Name',axis=1, inplace=True)","261f21d8":"df[['Title','Sex', 'Age', 'Survived']].groupby(['Title','Sex']).agg({'Age': ['mean', 'min', 'max', 'median', 'count'], 'Survived': ['mean', 'sum']})","da3d11c8":"#Pclass divides the data better than Title, therefore it will be used to recreate missing values in column Age\ndf[['Pclass','Sex', 'Age', 'Survived']].groupby(['Pclass','Sex']).agg({'Age': ['mean', 'min', 'max', 'median', 'count'], 'Survived': ['mean', 'sum']})","ad2578a3":"grid = sns.FacetGrid(df, col='Pclass', row='Sex')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)","c15b1474":"# guess_ages = np.zeros((2,3))\n\n# for row,i in enumerate(sorted(list(df['Sex'].unique()),reverse=False)):\n#     for col,j in enumerate(sorted(list(df['Pclass'].unique()),reverse=False)):\n#         guess_df = df[(df['Sex']==i) & (df['Pclass']==j)]['Age'].dropna()\n#         guess_age = int(round(guess_df.median()))\n#         guess_ages[row,col] = guess_age\n        \n# for row,i in enumerate(sorted(list(df['Sex'].unique()),reverse=False)):\n#     for col,j in enumerate(sorted(list(df['Pclass'].unique()),reverse=False)):\n#         df.loc[(df['Age'].isnull()) & (df['Sex']==i) & (df['Pclass']==j),'Age'] = guess_ages[row,col]\n\ndf['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    \ndf.info()","27407e91":"# the last variable with missing values is Embarked, because there are only 2 missing values we will fill it with the most frequent value\ndf['Embarked'].fillna(df['Embarked'].dropna().mode()[0], inplace=True)\ndf.Embarked.unique()","550ed21f":"df.info()","8e84fd7b":"#Looking at below grouped values we can draw a conclusion that women has very high Survival rate in Pclass 1 and 2 not matter which port they embarked.\ndf[['Pclass','Sex', 'Age', 'Embarked', 'Survived']].groupby(['Pclass','Embarked', 'Sex']).agg({'Survived': ['mean', 'sum', 'count']})","1b945de2":"plt.figure(figsize=[16,12])\n\nplt.subplot(331)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='S') & (df['Pclass']==1)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='S') & (df['Pclass']==1)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(S)\/Pclass(1)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(332)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='S') & (df['Pclass']==2)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='S') & (df['Pclass']==2)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(S)\/Pclass(2)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(333)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='S') & (df['Pclass']==3)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='S') & (df['Pclass']==3)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(S)\/Pclass(3)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(334)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='C') & (df['Pclass']==1)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='C') & (df['Pclass']==1)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(C)\/Pclass(1)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(335)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='C') & (df['Pclass']==2)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='C') & (df['Pclass']==2)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(C)\/Pclass(2)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(336)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='C') & (df['Pclass']==3)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='C') & (df['Pclass']==3)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(C)\/Pclass(3)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(337)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='Q') & (df['Pclass']==1)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='Q') & (df['Pclass']==1)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(Q)\/Pclass(1)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(338)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='Q') & (df['Pclass']==2)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='Q') & (df['Pclass']==2)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(Q)\/Pclass(2)')\nplt.ylabel('Passengers (#)')\nplt.legend()\n\nplt.subplot(339)\nplt.hist(x=[df[(df['Survived']==1) & (df['Embarked']=='Q') & (df['Pclass']==3)]['Sex'], df[(df['Survived']==0) & (df['Embarked']=='Q') & (df['Pclass']==3)]['Sex']],stacked=True, color=['g','r'],label=['Survived', 'Not survived'] )\nplt.title('Embarked(Q)\/Pclass(3)')\nplt.ylabel('Passengers (#)')\nplt.legend()","25ca6c0f":"df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\ndf[['FamilySize','Survived']].groupby('FamilySize').agg({'Survived':['mean','sum','count']})","bad07209":"df['FamilySize'] = df.FamilySize.apply(lambda x: x if x in [1,2,3,4] else '5+').astype(str)\ndf[['FamilySize','Survived']].groupby('FamilySize').agg({'Survived':['mean','sum','count']})","0743e12b":"from sklearn.preprocessing import KBinsDiscretizer\nn_bins=int(2*len(df)**(1\/3))\nqt_fare = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n\nscaled_feature_names = [f\"BIN_{x}\" for x in ['Fare']]\ndf[scaled_feature_names] = qt_fare.fit_transform(df[['Fare']])\ndf[scaled_feature_names] = df[scaled_feature_names].astype(int)","e9f3c745":"#df[['Fare', 'BIN_Fare', 'Survived']].groupby(['BIN_Fare']).agg({'Fare' : ['mean', 'min', 'max', 'count'], 'Survived': 'mean'})","4f3a3cec":"qt_age = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n\nscaled_feature_names = [f\"BIN_{x}\" for x in ['Age']]\ndf[scaled_feature_names] = qt_age.fit_transform(df[['Age']])\ndf[scaled_feature_names] = df[scaled_feature_names].astype(int)","a3924f5b":"#df[['Age', 'Sex', 'BIN_Age', 'Survived']].groupby(['BIN_Age','Sex']).agg({'Age' : ['mean', 'min', 'max', 'count'], 'Survived': ['mean','sum','count']})","01a7dfe8":"df.info()","1dfc7b25":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal = OrdinalEncoder()\nobject_columns = list(df.select_dtypes(include='object').columns)\nordinal_object_columns = [f\"ORD_{x}\" for x in object_columns]\n\ndf[ordinal_object_columns] = ordinal.fit_transform(df[object_columns]).astype(int)","d688b2f6":"final_columns = list(df.select_dtypes(exclude='object').columns)\nfinal_columns","df94041f":"p = sns.pairplot(df[final_columns], hue='Survived', palette='deep', height=1.5)","909bf43c":"def correlation_heatmap(df):\n    _, ax = plt.subplots(figsize=(14,12))\n    colormap = sns.diverging_palette(225, 10, as_cmap=True)\n    \n    _ = sns.heatmap(\n            df.corr(),\n             cmap=colormap,\n             square=True,\n             cbar_kws={'shrink':.9},\n             ax=ax,\n             annot=True,\n             linewidth=0.1,\n             linecolor='white',\n             annot_kws={'fontsize':12}\n    )\n        \ncorrelation_heatmap(df[final_columns])","b0628a8f":"# from final list of column we will exclude Age as based on this column BIN_Age has been created and there is high correlation between these variables\nfinal_columns.remove('Fare')\nfinal_columns.remove('Survived')\nfinal_columns.remove('Age')\ntarget_column = 'Survived'\nprint(\"final_columns:\",final_columns, '','target_column:', target_column, sep='\\n')","1ef80651":"from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_df_train = pd.DataFrame(OH_encoder.fit_transform(df[final_columns]))\nOH_df_train.columns = OH_encoder.get_feature_names(final_columns)\nOH_df_train","cc7642ce":"X_train_OH, X_val_OH, y_train_OH, y_val_OH = model_selection.train_test_split(OH_df_train, df['Survived'], test_size=0.2, random_state=0)\nX_train_OH.shape, y_train_OH.shape, X_val_OH.shape, y_val_OH.shape","383073ac":"df[final_columns] = df[final_columns].astype('float64')\ndf[final_columns]","948e5643":"X_train, X_val, y_train, y_val = model_selection.train_test_split(df[final_columns], df['Survived'], test_size=0.2, random_state=0)\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","777ab39c":"algorithms = [\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n        \n    linear_model.LogisticRegression(solver='liblinear'),\n    \n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    svm.SVC(),\n\n    tree.DecisionTreeClassifier(),\n    \n    XGBClassifier(eval_metric = 'auc')    \n    ]\n\nAlg_columns = ['Algorithm_name', 'Algorithm_Train_Accuracy_Mean', 'Algorithm_Test_Accuracy_Mean']\nAlg_compare = pd.DataFrame(columns = Alg_columns)\n\nAlg_predict = y_train.copy()\n\nrow_index = 0\nfor alg in algorithms:\n    Alg_name = alg.__class__.__name__\n    Alg_compare.loc[row_index, 'Algorithm_name'] = Alg_name\n    \n    cv_results = model_selection.cross_validate(alg, X_train, y_train, cv=10, scoring='accuracy', return_train_score=True)\n    \n    Alg_compare.loc[row_index, 'Algorithm_Train_Accuracy_Mean'] = cv_results['train_score'].mean()\n    Alg_compare.loc[row_index, 'Algorithm_Test_Accuracy_Mean'] = cv_results['test_score'].mean()\n    \n    alg.fit(df[final_columns], df['Survived'])\n    Alg_predict[Alg_name] = alg.predict(X_train)\n    \n    row_index+=1\n    \n    \nAlg_compare.sort_values(by=['Algorithm_Test_Accuracy_Mean'], ascending=False, inplace=True)\nAlg_compare","8e91f4e9":"algorithms = [\n    ensemble.GradientBoostingClassifier(random_state=0),\n    ensemble.RandomForestClassifier(random_state=0),\n        \n    linear_model.LogisticRegression(random_state=0,solver='liblinear'),\n    \n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    svm.SVC(random_state=0),\n\n    tree.DecisionTreeClassifier(random_state=0),\n    \n    XGBClassifier(eval_metric = 'logloss',random_state=0)    \n    ]\n\nAlg_columns = ['Algorithm_name', 'Algorithm_Train_Accuracy_Mean', 'Algorithm_Test_Accuracy_Mean']\nAlg_compare = pd.DataFrame(columns = Alg_columns)\n\nAlg_predict = y_train_OH.copy()\n\nrow_index = 0\nfor alg in algorithms:\n    Alg_name = alg.__class__.__name__\n    Alg_compare.loc[row_index, 'Algorithm_name'] = Alg_name\n    \n    cv_results = model_selection.cross_validate(alg, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n    \n    Alg_compare.loc[row_index, 'Algorithm_Train_Accuracy_Mean'] = cv_results['train_score'].mean()\n    Alg_compare.loc[row_index, 'Algorithm_Test_Accuracy_Mean'] = cv_results['test_score'].mean()\n    \n    alg.fit(OH_df_train, df['Survived'])\n    Alg_predict[Alg_name] = alg.predict(X_train_OH)\n    \n    row_index+=1\n    \n    \nAlg_compare.sort_values(by=['Algorithm_Test_Accuracy_Mean'], ascending=False, inplace=True)\nAlg_compare","d7313fbe":"#base model\ngbc = ensemble.GradientBoostingClassifier(random_state=0)\n\ngbc_base_results = model_selection.cross_validate(gbc, X_train, y_train, scoring='accuracy', cv=10, return_train_score=True)\n\nprint('Before DT Parameteres: ', gbc.get_params())\nprint('Before DT Train score mean: ', round(gbc_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(gbc_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\ngbc_param_grid = {'n_estimators': [50,100,150],\n              'criterion':['friedman_mse', 'mae'],\n              'max_depth': [2,4,6,8,10],\n              'random_state': [0]}\n#best\ngbc_param_grid = {'criterion': ['friedman_mse'], 'max_depth': [2], 'n_estimators': [50], 'random_state': [0]}\n\ngbc_tuned_model = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid=gbc_param_grid, scoring='accuracy', cv=10, return_train_score=True)\ngbc_tuned_model.fit(X_train, y_train)\n\nprint('After DT Parameteres: ', gbc_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(gbc_tuned_model.cv_results_['mean_train_score'][gbc_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(gbc_tuned_model.cv_results_['mean_test_score'][gbc_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'criterion': 'friedman_mse', 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  77.6\n# After DT Test w\/bin score mean:  77.07\n\n# After DT Parameteres:  {'criterion': 'friedman_mse', 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  83.75\n# After DT Test w\/bin score mean:  83.44","7fe562ea":"#base model\nNB = naive_bayes.GaussianNB()\n\nNB_base_results = model_selection.cross_validate(NB, X_train, y_train, scoring='accuracy', cv=10, return_train_score=True)\n\nprint('Before DT Parameteres: ', NB.get_params())\nprint('Before DT Train score mean: ', round(NB_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(NB_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nNB_param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n#best\nNB_param_grid = {'var_smoothing': [0.0008111308307896872]}\n\nNB_tuned_model = model_selection.GridSearchCV(naive_bayes.GaussianNB(), param_grid=NB_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nNB_tuned_model.fit(X_train, y_train)\n\nprint('After DT Parameteres: ', NB_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(NB_tuned_model.cv_results_['mean_train_score'][NB_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(NB_tuned_model.cv_results_['mean_test_score'][NB_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'var_smoothing': 0.0008111308307896872}\n# After DT Training w\/bin score mean:  74.52\n# After DT Test w\/bin score mean:  75.03\n\n# After DT Parameteres:  {'var_smoothing': 0.0008111308307896872}\n# After DT Training w\/bin score mean:  79.53\n# After DT Test w\/bin score mean:  80.07","2d37de78":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)","7fa33bc8":"#base model\nlr = linear_model.LogisticRegression(random_state=0,solver='liblinear')\n\nlr_base_results = model_selection.cross_validate(lr, X_scaled, y_train, scoring='accuracy', cv=10, return_train_score=True)\nlr.fit(OH_df_train, df['Survived'])\n\nprint('Before DT Parameteres: ', lr.get_params())\nprint('Before DT Train score mean: ', round(lr_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(lr_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nlr_param_grid = {'C' : [0.1, 1, 4],\n                    'penalty': ['l1', 'l2'],\n                    'solver' : ['liblinear'],\n                    'random_state': [0]}\n#best\nlr_param_grid = {'C': [1], 'penalty': ['l1'], 'random_state': [0], 'solver': ['liblinear']}\n\nlr_tuned_model = model_selection.GridSearchCV(linear_model.LogisticRegression(), param_grid=lr_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nlr_tuned_model.fit(X_scaled, y_train)\n\nprint('After DT Parameteres: ', lr_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(lr_tuned_model.cv_results_['mean_train_score'][lr_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(lr_tuned_model.cv_results_['mean_test_score'][lr_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'C': 1, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n# After DT Training w\/bin score mean:  73.46\n# After DT Test w\/bin score mean:  72.75\n\n# After DT Parameteres:  {'C': 1, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n# After DT Training w\/bin score mean:  80.81\n# After DT Test w\/bin score mean:  80.35","bab3f97c":"#base model\nxbc = XGBClassifier(eval_metric = 'logloss', use_label_encoder=False, random_state=0)   \n\nxbc_base_results = model_selection.cross_validate(xbc, X_train, y_train, scoring='accuracy', cv=10, return_train_score=True)\nxbc.fit(df[final_columns], df['Survived'])\n\nprint('Before DT Parameteres: ', xbc.get_params())\nprint('Before DT Train score mean: ', round(xbc_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(xbc_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nxbc_param_grid = {'n_estimators': [50,100,150,250,350],\n              'eval_metric' : ['logloss','auc', 'error'],\n              'learning_rate' : [0.1, 0.05],\n              'max_depth': [2,4,6,8,10],\n              'use_label_encoder' : [False],\n              'random_state': [0]}\n#best\nxbc_param_grid = {'eval_metric': ['logloss'], 'learning_rate': [0.05], 'max_depth': [2], 'n_estimators': [10], 'random_state': [0], 'use_label_encoder': [False]}\n\nxbc_tuned_model = model_selection.GridSearchCV(XGBClassifier(), param_grid=xbc_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nxbc_tuned_model.fit(X_train, y_train)\n\nprint('After DT Parameteres: ', xbc_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(xbc_tuned_model.cv_results_['mean_train_score'][xbc_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(xbc_tuned_model.cv_results_['mean_test_score'][xbc_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'eval_metric': 'logloss', 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 350, 'random_state': 0, 'use_label_encoder': False}\n# After DT Training w\/bin score mean:  82.54\n# After DT Test w\/bin score mean:  76.99\n\n# After DT Parameteres:  {'eval_metric': 'logloss', 'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'random_state': 0, 'use_label_encoder': False}\n# After DT Training w\/bin score mean:  83.9\n# After DT Test w\/bin score mean:  83.29","a5510ddd":"#base model\nrf = ensemble.RandomForestClassifier(random_state=0)\n\nrf_base_results = model_selection.cross_validate(rf, X_train, y_train, scoring='accuracy', cv=10, return_train_score=True)\nrf.fit(df[final_columns], df['Survived'])\n\nprint('Before DT Parameteres: ', rf.get_params())\nprint('Before DT Train score mean: ', round(rf_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(rf_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nrf_param_grid = {'n_estimators': [50,100,150],\n              'max_features' : ['auto', 'log2'],\n              'max_depth': [2,4,6,8,10,12],\n              'random_state': [0]}\n#best\nrf_param_grid = {'max_depth': [4], 'max_features': ['auto'], 'n_estimators': [50], 'random_state': [0]}\n\nrf_tuned_model = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid=rf_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nrf_tuned_model.fit(X_train, y_train)\n\nprint('After DT Parameteres: ', rf_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(rf_tuned_model.cv_results_['mean_train_score'][rf_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(rf_tuned_model.cv_results_['mean_test_score'][rf_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'max_depth': 4, 'max_features': 'auto', 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  77.53\n# After DT Test w\/bin score mean:  77.0\n\n# After DT Parameteres:  {'max_depth': 4, 'max_features': 'auto', 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  83.74\n# After DT Test w\/bin score mean:  83.44","89382013":"#base model\ngbc2 = ensemble.GradientBoostingClassifier(random_state=0)\n\ngbc2_base_results = model_selection.cross_validate(gbc2, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n\n\nprint('Before DT Parameteres: ', gbc2.get_params())\nprint('Before DT Train score mean: ', round(gbc2_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(gbc2_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\ngbc2_param_grid = {'n_estimators': [50,100,150],\n              'criterion':['friedman_mse', 'mae'],\n              'max_depth': [2,4,6,8,10],\n              'random_state': [0]}\n#best\ngbc2_param_grid = {'criterion': ['friedman_mse'], 'max_depth': [2], 'n_estimators': [50], 'random_state': [0]}\n\ngbc2_tuned_model = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid=gbc2_param_grid, scoring='accuracy', cv=10, return_train_score=True)\ngbc2_tuned_model.fit(X_train_OH, y_train_OH)\n\nprint('After DT Parameteres: ', gbc2_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(gbc2_tuned_model.cv_results_['mean_train_score'][gbc2_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(gbc2_tuned_model.cv_results_['mean_test_score'][gbc2_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'criterion': 'friedman_mse', 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  78.4\n# After DT Test w\/bin score mean:  76.84\n\n# After DT Parameteres:  {'criterion': 'friedman_mse', 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n# After DT Training w\/bin score mean:  84.18\n# After DT Test w\/bin score mean:  83.01","3dbf6180":"#base model\nSVecM = svm.SVC()\n\nSVecM_base_results = model_selection.cross_validate(SVecM, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n\n\nprint('Before DT Parameteres: ', SVecM.get_params())\nprint('Before DT Train score mean: ', round(SVecM_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(SVecM_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nSVecM_param_grid = {'C' : [0.1, 1, 4, 10],\n                    'kernel': ['rbf', 'poly'],\n                    'gamma': ['scale', 'auto', 1, 10],\n                    'degree' : [2,3,4,5],\n                    'random_state': [0]}\n#best\nSVecM_param_grid = {'C': [1], 'degree': [2], 'gamma': ['scale'], 'kernel': ['rbf'], 'random_state': [0], 'probability' : [True]}\n\nSVecM_tuned_model = model_selection.GridSearchCV(svm.SVC(), param_grid=SVecM_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nSVecM_tuned_model.fit(X_train_OH, y_train_OH)\n\nprint('After DT Parameteres: ', SVecM_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(SVecM_tuned_model.cv_results_['mean_train_score'][SVecM_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(SVecM_tuned_model.cv_results_['mean_test_score'][SVecM_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n\n# After DT Parameteres:  {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 0}\n# After DT Training w\/bin score mean:  78.92\n# After DT Test w\/bin score mean:  76.8\n\n# After DT Parameteres:  {'C': 1, 'degree': 2, 'gamma': 'scale', 'kernel': 'rbf', 'probability': True, 'random_state': 0}\n# After DT Training w\/bin score mean:  84.77\n# After DT Test w\/bin score mean:  83.29","63fcbdf3":"#base model\nlr2 = linear_model.LogisticRegression(random_state=0,solver='liblinear')\n\nlr2_base_results = model_selection.cross_validate(lr2, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n\nprint('Before DT Parameteres: ', lr.get_params())\nprint('Before DT Train score mean: ', round(lr2_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(lr2_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nlr2_param_grid = {'C' : [0.1, 1, 4],\n                    'penalty': ['l1', 'l2'],\n                    'solver' : ['liblinear'],\n                    'random_state': [0]}\n#best\nlr2_param_grid = {'C': [1], 'penalty': ['l1'], 'random_state': [0], 'solver': ['liblinear']}\n\nlr2_tuned_model = model_selection.GridSearchCV(linear_model.LogisticRegression(), param_grid=lr2_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nlr2_tuned_model.fit(X_train_OH, y_train_OH)\n\nprint('After DT Parameteres: ', lr_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(lr2_tuned_model.cv_results_['mean_train_score'][lr2_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(lr2_tuned_model.cv_results_['mean_test_score'][lr2_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'C': 1, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n# After DT Training w\/bin score mean:  79.65\n# After DT Test w\/bin score mean:  76.33\n\n# After DT Parameteres:  {'C': 1, 'penalty': 'l1', 'random_state': 0, 'solver': 'liblinear'}\n# After DT Training w\/bin score mean:  82.96\n# After DT Test w\/bin score mean:  82.45","6f0ea333":"#base model\nknn = neighbors.KNeighborsClassifier()\n\nknn_base_results = model_selection.cross_validate(xbc, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n\n\nprint('Before DT Parameteres: ', xbc.get_params())\nprint('Before DT Train score mean: ', round(knn_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(knn_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nknn_param_grid = {'n_neighbors' : [3,5,7,9],#3,5,7,9\n                  'leaf_size' : [15,20,25,30,35]},#15,20,25,30,35\n#best\nknn_param_grid = {'leaf_size': [20], 'n_neighbors': [9]}\n\nknn_tuned_model = model_selection.GridSearchCV(neighbors.KNeighborsClassifier(), param_grid=knn_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nknn_tuned_model.fit(X_train_OH, y_train_OH)\n\nprint('After DT Parameteres: ', knn_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(knn_tuned_model.cv_results_['mean_train_score'][knn_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(knn_tuned_model.cv_results_['mean_test_score'][knn_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'leaf_size': 20, 'n_neighbors': 7}\n# After DT Training w\/bin score mean:  78.7\n# After DT Test w\/bin score mean:  74.95\n\n# After DT Parameteres:  {'leaf_size': 20, 'n_neighbors': 9}\n# After DT Training w\/bin score mean:  85.03\n# After DT Test w\/bin score mean:  82.45","3dabda16":"#base model\nxbc2 = XGBClassifier(eval_metric = 'logloss', use_label_encoder=False, random_state=0)   \n\nxbc2_base_results = model_selection.cross_validate(xbc2, X_train_OH, y_train_OH, scoring='accuracy', cv=10, return_train_score=True)\n\n\nprint('Before DT Parameteres: ', xbc.get_params())\nprint('Before DT Train score mean: ', round(xbc2_base_results['train_score'].mean()*100,2))\nprint('Before DT Test score mean: ', round(xbc2_base_results['test_score'].mean()*100,2))\nprint('-'*10)\n\n\nxbc2_param_grid = {'n_estimators': [50,100,150,250,350],\n              'eval_metric' : ['logloss','auc', 'error'],\n              'learning_rate' : [0.1, 0.05],\n              'max_depth': [2,4,6,8,10],\n              'use_label_encoder' : [False],\n              'random_state': [0]}\n#best\nxbc2_param_grid = {'eval_metric': ['logloss'], 'learning_rate': [0.05], 'max_depth': [2], 'n_estimators': [100], 'random_state': [0], 'use_label_encoder': [False]}\n\nxbc2_tuned_model = model_selection.GridSearchCV(XGBClassifier(), param_grid=xbc2_param_grid, scoring='accuracy', cv=10, return_train_score=True)\nxbc2_tuned_model.fit(X_train_OH, y_train_OH)\n\nprint('After DT Parameteres: ', xbc_tuned_model.best_params_)\nprint('After DT Training w\/bin score mean: ', round(xbc2_tuned_model.cv_results_['mean_train_score'][xbc2_tuned_model.best_index_]*100,2))\nprint('After DT Test w\/bin score mean: ', round(xbc2_tuned_model.cv_results_['mean_test_score'][xbc2_tuned_model.best_index_]*100,2))\nprint('-'*10)\n\n# After DT Parameteres:  {'eval_metric': 'logloss', 'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 350, 'random_state': 0, 'use_label_encoder': False}\n# After DT Training w\/bin score mean:  77.67\n# After DT Test w\/bin score mean:  77.26\n\n# After DT Parameteres:  {'eval_metric': 'logloss', 'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'random_state': 0, 'use_label_encoder': False}\n# After DT Training w\/bin score mean:  87.42\n# After DT Test w\/bin score mean:  83.3","f81e345f":"print('[OrdinalEncoder] - GradientBoostingClassifier accuracy is',gbc_tuned_model.score(X_val, y_val))\nprint('\\n')\nprint(metrics.classification_report(gbc_tuned_model.predict(X_val), y_val))\nprint('-'*10)\nprint('[OrdinalEncoder] - GaussianNB accuracy is',NB_tuned_model.score(X_val, y_val))\nprint('\\n')\nprint(metrics.classification_report(NB_tuned_model.predict(X_val), y_val))\nprint('-'*10)\nprint('[OrdinalEncoder] - LogisticRegression accuracy is',lr_tuned_model.score(X_val, y_val))\nprint('\\n')\nprint(metrics.classification_report(lr_tuned_model.predict(X_val), y_val))\nprint('-'*10)\nprint('[OrdinalEncoder] - XGBClassifier accuracy is',xbc_tuned_model.score(X_val, y_val))\nprint('\\n')\nprint(metrics.classification_report(xbc_tuned_model.predict(X_val), y_val))\nprint('-'*10)\nprint('[OrdinalEncoder] - RandomForestClassifier accuracy is',rf_tuned_model.score(X_val, y_val))\nprint('\\n')\nprint(metrics.classification_report(rf_tuned_model.predict(X_val), y_val))","d881b22f":"plt.figure(figsize=[30,6])\n\nplt.subplot(151)\nprobs = gbc_tuned_model.predict_proba(X_val)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='GradientBoostingClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(152)\nprobs = NB_tuned_model.predict_proba(X_val)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='GaussianNB (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(153)\nprobs = lr_tuned_model.predict_proba(X_val)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='LogisticRegression (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(154)\nprobs = xbc_tuned_model.predict_proba(X_val)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='XGBClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(155)\nprobs = rf_tuned_model.predict_proba(X_val)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='RandomForestClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')","bb3a9869":"print('[OneHotEncoder] - GradientBoostingClassifier accuracy is',gbc2_tuned_model.score(X_val_OH, y_val_OH))\nprint('\\n')\nprint(metrics.classification_report(gbc2_tuned_model.predict(X_val_OH), y_val_OH))\nprint('-'*10)\nprint('[OneHotEncoder] - SVC accuracy is',SVecM_tuned_model.score(X_val_OH, y_val_OH))\nprint('\\n')\nprint(metrics.classification_report(SVecM_tuned_model.predict(X_val_OH), y_val_OH))\nprint('-'*10)\nprint('[OneHotEncoder] - LogisticRegression accuracy is',lr2_tuned_model.score(X_val_OH, y_val_OH))\nprint('\\n')\nprint(metrics.classification_report(lr2_tuned_model.predict(X_val_OH), y_val_OH))\nprint('-'*10)\nprint('[OneHotEncoder] - KNeighborsClassifier accuracy is',knn_tuned_model.score(X_val_OH, y_val_OH))\nprint('\\n')\nprint(metrics.classification_report(knn_tuned_model.predict(X_val_OH), y_val_OH))\nprint('-'*10)\nprint('[OneHotEncoder] - XGBClassifier accuracy is',xbc2_tuned_model.score(X_val_OH, y_val_OH))\nprint('\\n')\nprint(metrics.classification_report(xbc2_tuned_model.predict(X_val_OH), y_val_OH))","d75781c5":"plt.figure(figsize=[30,6])\n\nplt.subplot(151)\nprobs = gbc2_tuned_model.predict_proba(X_val_OH)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val_OH, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='GradientBoostingClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(152)\nprobs = SVecM_tuned_model.predict_proba(X_val_OH)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val_OH, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='SVC (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(153)\nprobs = lr2_tuned_model.predict_proba(X_val_OH)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val_OH, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='LogisticRegression (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(154)\nprobs = knn_tuned_model.predict_proba(X_val_OH)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val_OH, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='KNeighborsClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\n\nplt.subplot(155)\nprobs = xbc2_tuned_model.predict_proba(X_val_OH)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_val_OH, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, 'c-', label='XGBClassifier (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')","94eb23fc":"#The best looking classifier are: \n## XGBClassifier and RandomForestClassifier from OrdinalEncoder approach\n## KNeighborsClassifier from OneHotEncoder approach\n#and this models will be used for final submission.","8902bdb3":"test.head()","9082e257":"test.info()","f13dad55":"test['Title'] = test['Name'].str.split(',', expand=True)[1].str.split('.',expand=True)[0]\ntest['Title'] = test['Title'].apply(lambda x: x.strip() if x.strip() in ['Mr', 'Miss', 'Mrs', 'Master'] else 'Rare')\ntest.drop(['Cabin', 'Ticket', 'Name'], axis=1, inplace=True)","cc77a185":"test['Pclass'].unique()","e06e9084":"guess_ages = np.zeros((2,3))\n\nfor row,i in enumerate(sorted(list(df['Sex'].unique()),reverse=False)):\n    for col,j in enumerate(sorted(list(df['Pclass'].unique()),reverse=False)):\n        guess_df = test[(df['Sex']==i) & (df['Pclass']==j)]['Age'].dropna()\n        guess_age = int(round(guess_df.median()))\n        guess_ages[row,col] = guess_age\n        \nfor row,i in enumerate(sorted(list(test['Sex'].unique()),reverse=False)):\n    for col,j in enumerate(sorted(list(test['Pclass'].unique()),reverse=False)):\n        test.loc[(test['Age'].isnull()) & (test['Sex']==i) & (test['Pclass']==j),'Age'] = guess_ages[row,col]\n            \n        \ntest.info()","f4141032":"test['Fare'].fillna(test['Fare'].dropna().mean(), inplace=True)","2729346a":"test['FamilySize'] = test['Parch'] + test['SibSp'] + 1\ntest['FamilySize'] = test.FamilySize.apply(lambda x: x if x in [1,2,3,4] else '5+').astype(str)","95041296":"from sklearn.preprocessing import KBinsDiscretizer\n\nn_bins=int(2*len(df)**(1\/3))\n\nqt_fare = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\nscaled_feature_names = [f\"BIN_{x}\" for x in ['Fare']]\nqt_fare.fit(df[['Fare']])\n\ntest[scaled_feature_names] = qt_fare.transform(test[['Fare']])\ntest[scaled_feature_names] = test[scaled_feature_names].astype(int)","f8ca00c3":"qt_age = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n\nscaled_feature_names = [f\"BIN_{x}\" for x in ['Age']]\nqt_age.fit(df[['Age']])\n\ntest[scaled_feature_names] = qt_age.transform(test[['Age']])\ntest[scaled_feature_names] = test[scaled_feature_names].astype(int)","ad0cf26e":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal = OrdinalEncoder()\nobject_columns = list(df.select_dtypes(include='object').columns)\nordinal_object_columns = [f\"ORD_{x}\" for x in object_columns]\nordinal.fit(df[object_columns])\n\ntest[ordinal_object_columns] = ordinal.transform(test[object_columns]).astype(int)","61edade1":"final_columns_test = list(test.select_dtypes(exclude='object').columns)\nfinal_columns_test.remove('PassengerId')\nfinal_columns_test.remove('Age')\nfinal_columns_test.remove('Fare')\ntest[final_columns_test].info()","004025c2":"from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_encoder.fit(df[final_columns])\n\nOH_df_test = pd.DataFrame(OH_encoder.fit_transform(test[final_columns_test]))\nOH_df_test.columns = OH_encoder.get_feature_names(final_columns_test)","ed697d8c":"#test['Survived'] = xbc_tuned_model.predict(test[final_columns_test]) #score = 0.74\ntest['Survived'] = rf_tuned_model.predict(test[final_columns_test]) #score = 0.78229\n#test['Survived'] = gbc_tuned_model.predict(test[final_columns_test]) #score = 0.77511","43fe7415":"submit = test[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/submission.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', test['Survived'].value_counts(normalize = True))\nsubmit.sample(10)","a64a6479":"# from sklearn.preprocessing import OneHotEncoder\n\n# OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n# OH_encoder.fit(df[final_columns])\n\n# OH_df_test = pd.DataFrame(OH_encoder.transform(test[final_columns_test]))\n# OH_df_test.columns = OH_encoder.get_feature_names(final_columns_test)","3e16aa45":"# test['Survived'] = knn_tuned_model.predict(OH_df_test) #score = 0.75","9bb96221":"# submit = test[['PassengerId','Survived']]\n# submit.to_csv(\"..\/working\/submission.csv\", index=False)\n\n# print('Validation Data Distribution: \\n', test['Survived'].value_counts(normalize = True))\n# submit.sample(10)","294d428c":"## OneHotEncoder data","7751cb92":"## OrdinalEncoder data","37bce62e":"### The best model has been created by using random forest algorithm with final score: 0.78229"}}