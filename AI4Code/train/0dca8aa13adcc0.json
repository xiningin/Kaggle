{"cell_type":{"64aa14a2":"code","5fbf77fc":"code","01000956":"code","c4459cfb":"code","779936f2":"code","7ad4ee7b":"code","2deb785d":"code","690bd43c":"code","8cadeef9":"markdown","0de23628":"markdown","a754639b":"markdown","20b662f6":"markdown","7f1631ab":"markdown","68c228cd":"markdown","eb2ae4bb":"markdown","50af3902":"markdown","f3455f90":"markdown","46916cf5":"markdown","424a8074":"markdown"},"source":{"64aa14a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('done')","5fbf77fc":"'''\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef prep_data(raw):\n    y = raw[:, 0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n    \n    x = raw[:,1:]\n    num_images = raw.shape[0]\n    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n    out_x = out_x \/ 255\n    return out_x, out_y\n\nlego_file =\"..\/input\/lego-dataset\/Data science dataset\"\nlego_data = np.loadtxt(lego_file, skiprows=1, delimiter=',')\nx, y = prep_data(lego_data)\n'''","01000956":"num_classes = 2 # Number of options computer can pick.\nresnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nlego_model = Sequential()\nlego_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\nlego_model.add(Dense(num_classes, activation='softmax'))\n\n\nlego_model.layers[0].trainable = False ","c4459cfb":"'''lego_model.add(Conv2D(20, activation='relu', kernel_size=3))\nlego_model.add(Conv2D(20, activation='relu', kernel_size=3))\nlego_model.add(Flatten())\nlego_model.add(Dense(100, activation='relu'))\nlego_model.add(Dense(10, activation='softmax'))'''","779936f2":"lego_model.compile(optimizer='sgd', \n                     loss='categorical_crossentropy', \n                     metrics=['accuracy'])","7ad4ee7b":"data_generator_with_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n                                              horizontal_flip = True,\n                                              width_shift_range = 0.1,\n                                              height_shift_range = 0.1)\n            \ndata_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)","2deb785d":"image_size = 224 #resnet uses an image size of 224.\n\ntrain_generator = data_generator_with_aug.flow_from_directory(\n        directory = '..\/input\/lego-dataset\/Data science dataset\/Train',\n        target_size=(image_size, image_size),\n        batch_size=12,\n        class_mode='categorical')\n\n\nvalidation_generator = data_generator_no_aug.flow_from_directory(\n        directory = '..\/input\/lego-dataset\/Data science dataset\/Val-150photos',\n        target_size=(image_size, image_size),\n        class_mode='categorical')\nfitted = lego_model.fit(train_generator,\n                          epochs=3,\n                          steps_per_epoch=19,\n                          validation_data=validation_generator,\n                          validation_steps=1)","690bd43c":"#history = mode.fit\nfitted = pd.DataFrame(fitted.history)\nfitted.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(fitted['val_loss'].min()))\nvgg16acc = fitted['val_loss'].min()","8cadeef9":"# Data Aug.\nThis changes aspects of the image such as flipping it or moving it around. I dont really need to do this since all my images in the data set are computor generated, all similar and in different positions. ","0de23628":"# Add First Layers.\nTo Start of the model I have started off by classifying everything and starting some layers.","a754639b":"# Conclusion.\nI have succesfully added Layers, compiled the model, augmented the data, catagorised the data and finally graphed the results to create a functioning image classifyer which has a accuracy of a 98.4% accuracy.\n\n#### Results:\nHaving a 100% accuracy as shown above is usually a bad thing however since my images are all computor generated and similar it is expected.\nChallennges, hypothesis is supported by this as it has a high accuracy.\nThe graph is a good representation of the history of the model wich is showing loss from data set.","20b662f6":"# Data Set\nFor an ideal data set to use here we want a data set which contains alot of images assorted into validation and training. I want to gather around 150 for validation and 250 + for training. I could not find a data set which I liked which fit this criteria so I had to modify one to better suit it.","7f1631ab":"# Introduction.\nI am going to use transfer learning to train a neural network, the steps I have taken are as follows:\n- Prepared the data.\n- Added Layers.\n- Compiled the model.\n- Augmented the data.\n- Catagorised the data.\n- And finally graphing the results.\n\n#### Hypothesis:\nThe Data set I have found and are going to use is very well suited to making a image classifyer so I predict that the accuracy will be very high.\n","68c228cd":"# Graph.","eb2ae4bb":"# Catagorising Data.\nIn this step I catagorised data based upon its name and changed image size to 2245 because that is what resnet has it in there.","50af3902":"# Compiling The Model.\nI am about to compile the model with the code below.\nI added a optimiser, a loss function and some metrics.\nLoss function and metrics are added so we can check the models accuracy.\nOptimizers are algorithms which change weights.\nSDG stands for Stochastic gradient descent which is used because it is the more preffered method. ","f3455f90":"# Prepare The Data.\nHere I have some code which would prepare data for its use however alot of what I have done here is completed later on in other steps so it is not really needed which is why I have made it so it is disregarded when run.","46916cf5":"# Add The Rest Of The Layers.\nThis is making a number of layers in the model. I dont need the above since I am not making my own model I am using resnet50,so i have told the computor to disregard it however I could still use it if I wished to do so.","424a8074":"# What Is Transfer Learning?\nTransfer learning is a method of deep learning image classifer where it transfers what it learnt from other problembs results applies it to a new model."}}