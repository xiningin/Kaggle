{"cell_type":{"92cf70b2":"code","df655edd":"code","6d1e8a78":"code","4cd9c350":"code","a0b13d89":"code","dbae2029":"code","20f90944":"code","716e9214":"code","391803db":"code","7cc52de2":"code","b573f6b9":"code","8b13789a":"code","76202b91":"code","b0d7fd9c":"code","27d11fd1":"code","6ccca68f":"code","ff0c2e9a":"code","223d2eae":"code","45d1752b":"code","ed83c397":"code","6ca1704c":"code","b2322289":"code","833473b6":"code","67775ef6":"code","41888202":"code","af90bab0":"code","9a0ac0fe":"code","d1d0751d":"code","099bfafc":"code","4031d80a":"code","8fa9cbfd":"code","5911b73b":"code","5bc9243f":"code","58336d21":"code","4c0911d9":"code","473cbd93":"code","7e025780":"code","6a3c6cec":"code","78297d9d":"code","77a9e7b8":"code","8dc48b4b":"code","788ecd75":"code","a9c5523d":"code","b5de4a54":"code","d128a138":"code","a8f15135":"code","bae3b882":"code","784e909b":"code","d94747f1":"code","342dd66f":"code","f3976fa4":"code","fa7ce969":"code","f245fceb":"code","8e5e8c6d":"code","2d214c0b":"code","d2a6dd45":"code","a10fb223":"code","373f425f":"code","41d36e93":"code","ede84123":"markdown","43c2200f":"markdown"},"source":{"92cf70b2":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Modeling\nimport lightgbm as lgb\n\n# Splitting data\nfrom sklearn.model_selection import train_test_split\n\nN_FOLDS = 5\nMAX_EVALS = 5","df655edd":"features = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\n\n# Sample 17000 rows (10000 for training, 7000 for testing)\nfeatures = features.sample(n = 17000, random_state = 42)\n\n# Only numeric features\nfeatures = features.select_dtypes('number')\n\n# Extract the labels\nlabels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n\n# Split into training and testing data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 7000, random_state = 37)","6d1e8a78":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","4cd9c350":"# Create a training and testing dataset\ntrain_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","a0b13d89":"# Get default hyperparameters\nmodel = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = N_FOLDS, seed = 42)","dbae2029":"print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['auc-mean'])))","20f90944":"from sklearn.metrics import roc_auc_score","716e9214":"# Optimal number of esimators found in cv\nmodel.n_estimators = len(cv_results['auc-mean'])\n\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\n\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","391803db":"def objective(hyperparameters, iteration):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross validation score from a set of hyperparameters.\"\"\"\n    \n    # Number of estimators will be found using early stopping\n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']\n    \n     # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n    \n    # results to retun\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators \n    \n    return [score, hyperparameters, iteration]","7cc52de2":"score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","b573f6b9":"# Create a default model\nmodel = lgb.LGBMModel()\nmodel.get_params()","8b13789a":"# Hyperparameter grid\nparam_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","76202b91":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","b0d7fd9c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Learning rate histogram\nplt.hist(param_grid['learning_rate'], bins = 20, color = 'r', edgecolor = 'k');\nplt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);","27d11fd1":"a = 0\nb = 0\n\n# Check number of values in each category\nfor x in param_grid['learning_rate']:\n    # Check values\n    if x >= 0.005 and x < 0.05:\n        a += 1\n    elif x >= 0.05 and x < 0.5:\n        b += 1\n\nprint('There are {} values between 0.005 and 0.05'.format(a))\nprint('There are {} values between 0.05 and 0.5'.format(b))","6ccca68f":"# number of leaves domain\nplt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\nplt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);","ff0c2e9a":"# Dataframes for random and grid search\nrandom_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","223d2eae":"com = 1\nfor x in param_grid.values():\n    com *= len(x)\nprint('There are {} combinations'.format(com))","45d1752b":"print('This would take {:.0f} years to finish.'.format((100 * com) \/ (60 * 60 * 24 * 365)))","ed83c397":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    \n    # Dataframe to store results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    \n    # https:\/\/codereview.stackexchange.com\/questions\/171173\/list-all-possible-permutations-from-a-python-dictionary-of-lists\n    keys, values = zip(*param_grid.items())\n    \n    i = 0\n    \n    # Iterate through every possible combination of hyperparameters\n    for v in itertools.product(*values):\n        \n        # Create a hyperparameter dictionary\n        hyperparameters = dict(zip(keys, v))\n        \n        # Set the subsample ratio accounting for boosting type\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        \n        # Evalute the hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n        \n        i += 1\n        \n        # Normally would not limit iterations\n        if i > MAX_EVALS:\n            break\n       \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    \n    return results    ","6ca1704c":"grid_results = grid_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","b2322289":"# Get the best parameters\ngrid_search_params = grid_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","833473b6":"pd.options.display.max_colwidth = 1000\ngrid_results['params'].values","67775ef6":"random.seed(50)\n\n# Randomly sample from dictionary\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n# Deal with subsample ratio\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\nrandom_params","41888202":"def random_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n    \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","af90bab0":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","9a0ac0fe":"# Get the best parameters\nrandom_search_params = random_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","d1d0751d":"random_results['params']","099bfafc":"import csv\n\n# Create file and open connection\nout_file = 'random_search_trials.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write column names\nheaders = ['score', 'hyperparameters', 'iteration']\nwriter.writerow(headers)\nof_connection.close()","4031d80a":"def random_search(param_grid, out_file, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization. \n       Writes result of search to csv file every search iteration.\"\"\"\n    \n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(random_params, i)\n        results.loc[i, :] = eval_results\n\n        # open connection (append option) and write results\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow(eval_results)\n        \n        # make sure to close connection\n        of_connection.close()\n        \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n\n    return results ","8fa9cbfd":"def grid_search(param_grid, out_file, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\n       Writes result of search to csv file every search iteration.\"\"\"\n    \n    # Dataframe to store results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    \n    # https:\/\/codereview.stackexchange.com\/questions\/171173\/list-all-possible-permutations-from-a-python-dictionary-of-lists\n    keys, values = zip(*param_grid.items())\n    \n    i = 0\n    \n    # Iterate through every possible combination of hyperparameters\n    for v in itertools.product(*values):\n        # Select the hyperparameters\n        parameters = dict(zip(keys, v))\n        \n        # Set the subsample ratio accounting for boosting type\n        parameters['subsample'] = 1.0 if parameters['boosting_type'] == 'goss' else parameters['subsample']\n        \n        # Evalute the hyperparameters\n        eval_results = objective(parameters, i)\n        \n        results.loc[i, :] = eval_results\n        \n        i += 1\n        \n        # open connection (append option) and write results\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow(eval_results)\n        \n        # make sure to close connection\n        of_connection.close()\n        \n        # Normally would not limit iterations\n        if i > MAX_EVALS:\n            break\n       \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    \n    return results    ","5911b73b":"# MAX_EVALS = 1000\n\n# # Create file and open connection\n# out_file = 'grid_search_trials_1000.csv'\n# of_connection = open(out_file, 'w')\n# writer = csv.writer(of_connection)\n\n# # Write column names\n# headers = ['score', 'hyperparameters', 'iteration']\n# writer.writerow(headers)\n# of_connection.close()\n\n# grid_results = grid_search(param_grid, out_file)\n\n\n# # Create file and open connection\n# out_file = 'random_search_trials_1000.csv'\n# of_connection = open(out_file, 'w')\n# writer = csv.writer(of_connection)\n\n# # Write column names\n# headers = ['score', 'hyperparameters', 'iteration']\n# writer.writerow(headers)\n# of_connection.close()\n\n# random_results = random_search(param_grid, out_file)","5bc9243f":"random_results = pd.read_csv('..\/input\/home-credit-model-tuning\/random_search_trials_1000.csv')\ngrid_results = pd.read_csv('..\/input\/home-credit-model-tuning\/grid_search_trials_1000.csv')","58336d21":"import ast\n\n# Convert strings to dictionaries\ngrid_results['hyperparameters'] = grid_results['hyperparameters'].map(ast.literal_eval)\nrandom_results['hyperparameters'] = random_results['hyperparameters'].map(ast.literal_eval)","4c0911d9":"def evaluate(results, name):\n    \"\"\"Evaluate model on test data using hyperparameters in results\n       Return dataframe of hyperparameters\"\"\"\n        \n    # Sort with best values on top\n    results = results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # Print out cross validation high score\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, results.loc[0, 'score'], results.loc[0, 'iteration']))\n    \n    # Use best hyperparameters to create a model\n    hyperparameters = results.loc[0, 'hyperparameters']\n    model = lgb.LGBMClassifier(**hyperparameters)\n    \n    # Train and make predictions\n    model.fit(train_features, train_labels)\n    preds = model.predict_proba(test_features)[:, 1]\n    \n    print('ROC AUC from {} on test data = {:.5f}.'.format(name, roc_auc_score(test_labels, preds)))\n    \n    # Create dataframe of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(results.loc[0, 'hyperparameters'].keys()))\n\n    # Iterate through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # Put the iteration and score in the hyperparameter dataframe\n    hyp_df['iteration'] = results['iteration']\n    hyp_df['score'] = results['score']\n    \n    return hyp_df","473cbd93":"grid_hyp = evaluate(grid_results, name = 'grid search')","7e025780":"random_hyp = evaluate(random_results, name = 'random search')","6a3c6cec":"import altair as alt\n\nalt.renderers.enable('notebook')","78297d9d":"# Combine results into one dataframe\nrandom_hyp['search'] = 'random'\ngrid_hyp['search'] = 'grid'\n\nhyp = random_hyp.append(grid_hyp)\nhyp.head()","77a9e7b8":"max_random = random_hyp['score'].max()\nmax_grid = grid_hyp['score'].max()\n\nc = alt.Chart(hyp, width = 400, height = 400).mark_circle(size = 150).encode(alt.Y('score', scale = alt.Scale(domain = [0.65, 0.76])),\nx = 'iteration', color = 'search')\n\nc.title = 'Score vs Iteration'\nc","8dc48b4b":"best_grid_hyp = grid_hyp.iloc[grid_hyp['score'].idxmax()].copy()\nbest_random_hyp = random_hyp.iloc[random_hyp['score'].idxmax()].copy()","788ecd75":"\nhyp.sort_values('search', inplace = True)\n\n# Plot of scores over the course of searching\nsns.lmplot('iteration', 'score', hue = 'search', data = hyp, size = 8);\nplt.scatter(best_grid_hyp['iteration'], best_grid_hyp['score'], marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.scatter(best_random_hyp['iteration'], best_random_hyp['score'], marker = '*', s = 400, c = 'orange', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","a9c5523d":"print('Average validation score of grid search =   {:.5f}.'.format(np.mean(grid_hyp['score'])))\nprint('Average validation score of random search = {:.5f}.'.format(np.mean(random_hyp['score'])))","b5de4a54":"# Create bar chart\nbars = alt.Chart(random_hyp, width = 400).mark_bar().encode(x = 'boosting_type', y = alt.Y('count()', scale = alt.Scale(domain = [0, 400])))\n\nbars.title = 'Boosting Type for Random Search'\n\n# Add text for labels\ntext = bars.mark_text(align = 'center', baseline = 'bottom', size = 20).encode(text = 'count()')\n\n# Display\nbars + text","d128a138":"# Bar plots of boosting type\nrandom_hyp['boosting_type'].value_counts().plot.bar(figsize = (14, 6), color = 'blue', title = 'Random Search Boosting Type');","a8f15135":"random_hyp['score'] = random_hyp['score'].astype(float)\nbest_random_hyp = random_hyp.loc[0, :].copy()","bae3b882":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# Density plots of the learning rate distributions \nsns.kdeplot(param_grid['learning_rate'], label = 'Sampling Distribution', linewidth = 4)\nsns.kdeplot(random_hyp['learning_rate'], label = 'Random Search', linewidth = 4)\nplt.vlines([best_random_hyp['learning_rate']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['orange'])\nplt.legend()\nplt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');","784e909b":"# Iterate through each hyperparameter\nfor i, hyper in enumerate(random_hyp.columns):\n    if hyper not in ['boosting_type', 'iteration', 'subsample', 'score', 'learning_rate', 'is_unbalance', 'metric', 'verbose', 'iteration', 'n_estimators', 'search']:\n        plt.figure(figsize = (14, 6))\n        \n        # Plot the random search distribution and the sampling distribution\n        if hyper != 'loss':\n            sns.kdeplot(param_grid[hyper], label = 'Sampling Distribution', linewidth = 4)\n        sns.kdeplot(random_hyp[hyper], label = 'Random Search', linewidth = 4)\n        plt.vlines([best_random_hyp[hyper]],\n                     ymin = 0.0, ymax = 10.0, linestyles = '--', linewidth = 4, colors = ['orange'])\n        plt.legend(loc = 1)\n        plt.title('{} Distribution'.format(hyper))\n        plt.xlabel('{}'.format(hyper)); plt.ylabel('Density');\n        plt.show();","d94747f1":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot('iteration', hyper, data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp['iteration'], best_random_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","342dd66f":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Scatterplot of next four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        sns.regplot('iteration', hyper, data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp['iteration'], best_random_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","f3976fa4":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));\n\nplt.tight_layout()\n\nfig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Scatterplot of next four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i])\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'score', title = 'Score vs {}'.format(hyper));\n\nplt.tight_layout()","fa7ce969":"# Read in full dataset\ntrain = pd.read_csv('..\/input\/home-credit-simple-featuers\/simple_features_train.csv')\ntest = pd.read_csv('..\/input\/home-credit-simple-featuers\/simple_features_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","f245fceb":"train_set = lgb.Dataset(train, label = train_labels)\n\nhyperparameters = dict(**random_results.loc[0, 'hyperparameters'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = N_FOLDS)","8e5e8c6d":"print('The cross validation score on the full dataset = {:.5f} with std: {:.5f}.'.format(\n    cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\nprint('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))","2d214c0b":"# Train the model with the optimal number of estimators from early stopping\nmodel = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]","d2a6dd45":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_simple_features_random.csv', index = False)","a10fb223":"import xgboost as xgb\n\nclf_xgBoost = xgb.XGBClassifier(\n    learning_rate =0.01, n_estimators=1000, max_depth=4, min_child_weight=4, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n            nthread=4, scale_pos_weight=2, seed=27)\n# Fit the models\nclf_xgBoost.fit(train,train_labels)","373f425f":"pred = clf_xgBoost.predict_proba(test)[:, 1]","41d36e93":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': pred})\nsubmission.to_csv('submission_xgboost.csv', index = False)\nsubmission.head()","ede84123":"As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale.","43c2200f":"## Score versus Hyperparameters"}}