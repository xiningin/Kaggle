{"cell_type":{"d81710c4":"code","7cca6c68":"code","b3a78228":"code","a629fe6d":"code","d3849233":"code","200b3aa5":"code","0a6f1f0c":"code","92ac56b3":"code","a2e10097":"code","983b835a":"code","df95df76":"code","77d87776":"code","e923080a":"code","46698d5f":"code","2330278b":"code","710b3cc0":"code","47e7454d":"code","db11be08":"code","efa5be4f":"code","6c70a0bc":"code","2e7c2591":"code","62799cc1":"code","29412b98":"code","a02a3dd3":"code","2584c894":"code","a757e4ec":"code","3d537121":"markdown","4b79a5bf":"markdown","c840b13c":"markdown","c4832702":"markdown","5b3c710e":"markdown","6169c1a4":"markdown","1ac69485":"markdown","8953c8a9":"markdown","71611f9b":"markdown","ed0141cc":"markdown","26000dd1":"markdown","d13e5f30":"markdown","df8197ac":"markdown","4900bdc9":"markdown","7c7ebca7":"markdown","1f40a718":"markdown","851a96e3":"markdown","5f094639":"markdown","05eb56a1":"markdown","136a5458":"markdown","5bb9c33c":"markdown","cb007e6a":"markdown"},"source":{"d81710c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cca6c68":"# Some libraries for visualization purposes\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom pandas.plotting import scatter_matrix\nfrom collections import Counter\n\n# Increase the default plot size and set the color scheme\nplt.rcParams['figure.figsize'] = 8, 5\nplt.style.use(\"fivethirtyeight\")\n\n# Disable warnings \nimport warnings\nwarnings.filterwarnings('ignore')","b3a78228":"# Loading the data\ndf = pd.read_csv('\/kaggle\/input\/palmer-archipelago-antarctica-penguin-data\/penguins_size.csv')\ndf.head()","a629fe6d":"# Info and Describe of the data\nprint(df.shape)\nprint(df.describe())\nprint(df.info())","d3849233":"sns.heatmap(df.isnull());","200b3aa5":"df.dropna(inplace = True)\ndf.info()","0a6f1f0c":"df.head()","92ac56b3":"plt.figure(figsize = (15, 7))\nplt.subplot(1, 3, 1)\nsns.countplot(data=df, x='species')\nplt.subplot(1, 3, 2)\nsns.countplot(data=df, x='sex')\nplt.subplot(1, 3, 3)\nsns.countplot(data=df, x='island')\nplt.tight_layout()\nplt.show()","a2e10097":"print(df.shape)\ndf = df.loc[df.sex != '.', :]\nprint(df.shape)","983b835a":"plt.figure(figsize = (8, 6))\nplt.subplot(2, 2, 1)\nsns.distplot(df['culmen_length_mm'])\nplt.subplot(2, 2, 2)\nsns.distplot(df['culmen_depth_mm'])\nplt.subplot(2, 2, 3)\nsns.distplot(df['flipper_length_mm'])\nplt.subplot(2, 2, 4)\nsns.distplot(df['body_mass_g'])\nplt.tight_layout()\nplt.show()","df95df76":"plt.figure(figsize = (8, 6))\nplt.subplot(2, 2, 1)\nsns.boxplot(df['culmen_length_mm'])\nplt.subplot(2, 2, 2)\nsns.boxplot(df['culmen_depth_mm'])\nplt.subplot(2, 2, 3)\nsns.boxplot(df['flipper_length_mm'])\nplt.subplot(2, 2, 4)\nsns.boxplot(df['body_mass_g'])\nplt.tight_layout()\nplt.show()","77d87776":"sns.heatmap(df.corr(), annot=True)\nplt.show()","e923080a":"sns.pairplot(data=df, hue='species');","46698d5f":"df.head()","2330278b":"df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\ntrain = df[df['is_train'] == True]\ntest = df[df['is_train'] == False]\n\ntrain_x = train[train.columns[:len(train.columns) - 1]]\ntrain_x = train_x.drop('species', axis=1) # Dropping the label\ntrain_y = train['species']\n\n\ntest_x = test[test.columns[:len(test.columns) - 1]]\ntest_x = test_x.drop('species', axis=1) # Dropping the label\ntest_y = test['species']","710b3cc0":"train_x = pd.get_dummies(train_x)\ntest_x = pd.get_dummies(test_x);\n\n# One hot encoding the label data to fit in the model","47e7454d":"print(train_x.shape)\ntrain_x.head()","db11be08":"print(train_y.shape)","efa5be4f":"# Euclidean Distance\ndef euclidean_distance(point1, point2):\n    distance = 0\n    for i in range(point1.shape[0]):\n        distance += np.square(point1[i] - point2[i])\n    return np.sqrt(distance)\n\n# Manhattan Distance\ndef manhattan_distance(point1, point2):\n    distance = 0\n    for i in range(point1.shape[0]):\n        distance += abs(point1[i] - point2[i])\n    return distance","6c70a0bc":"def knn(train_x, train_y, dis_func, sample, k):\n    \"\"\"\n    Parameters:\n    train_x: training samples\n    train_y: corresponding labels\n    dis_func: calculates distance\n    sample: one test sample\n    k: number of nearest neighbors\n    \n    Returns:\n    cl: class of the sample\n    \"\"\"\n    \n    distances = {}\n    for i in range(len(train_x)):\n        d = dis_func(sample, train_x.iloc[i])\n        distances[i] = d\n    sorted_dist = sorted(distances.items(), key = lambda x : (x[1], x[0]))\n    \n    # take k nearest neighbors\n    neighbors = []\n    for i in range(k):\n        neighbors.append(sorted_dist[i][0])\n    \n    #convert indices into classes\n    classes = [train_y.iloc[c] for c in neighbors]\n    \n    #count each classes in top k\n    counts = Counter(classes)\n    \n    #take vote of max number of samples of a class\n    list_values = list(counts.values())\n    list_keys = list(counts.keys())\n    cl = list_keys[list_values.index(max(list_values))]\n    \n    return cl","2e7c2591":"model = knn(train_x, train_y, euclidean_distance, test_x.iloc[3], k=5)\nprint(model)\nprint(test_y.iloc[3])","62799cc1":"# Calculate the accuracy\ndef get_accuracy(test_x, test_y, train_x, train_y, k):\n    correct = 0\n    for i in range(len(test_x)):\n        sample = test_x.iloc[i]\n        true_label = test_y.iloc[i]\n        predicted_label_euclidean = knn(train_x, train_y, euclidean_distance, sample, k)\n        if predicted_label_euclidean == true_label:\n            correct += 1\n    \n    accuracy_euclidean = (correct \/ len(test_x)) * 100\n    \n    correct = 0\n    for i in range(len(test_x)):\n        sample = test_x.iloc[i]\n        true_label = test_y.iloc[i]\n        predicted_label_euclidean = knn(train_x, train_y, manhattan_distance, sample, k)\n        if predicted_label_euclidean == true_label:\n            correct += 1\n    \n    accuracy_manhattan = (correct \/ len(test_x)) * 100\n    \n    print(\"Model accuracy with Euclidean Distance is %.2f\" %(accuracy_euclidean))\n    print(\"Model accuracy with Manhattan Distance is %.2f\" %(accuracy_manhattan))  ","29412b98":"get_accuracy(test_x, test_y, train_x, train_y, k=5)","a02a3dd3":"from sklearn import neighbors\nfrom sklearn.metrics import accuracy_score","2584c894":"classifier = neighbors.KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\nclassifier.fit(train_x, train_y)","a757e4ec":"y_pred = classifier.predict(test_x)\naccuracy = accuracy_score(test_y, y_pred)\nprint('Accuracy with sklearn KNN with the same hyperparameters: {:.4f}'.format(accuracy))","3d537121":"# Exploratory Data Analysis + Feature Engineering\n- Data Visualisation\n- Cleaning the data\n- Feature Engineering\n- Feature Extraction","4b79a5bf":"We are getting the exact same results as Sklearn implementation of KNN proving that our algorithm works.","c840b13c":"None of the columns are corelated as much as 0.95 so we will not be dropping any columns, though we can see that flipper length and body mass is highly corelated. ","c4832702":"### Distance between 2 vectors\n","5b3c710e":"## Multivariate","6169c1a4":"We can see that there are no outliers in our data, so we can make KNN without any issues","1ac69485":"# Summarize the data\n- Load the dataset\n- Get the high level overview of the data","8953c8a9":"# KNN Classifier from scratch\n\nToday, we will build the **K Nearest Neighbors algorithm** just with numpy and pandas library. There will be a lot of things to optimize, by the end of the notebook you'll feel like you have invented the algorithm by yourself. ","71611f9b":"The amount of missing values are very less, so we can drop those rows.","ed0141cc":"## Feature Engineering\n### Categorical values","26000dd1":"### Converting the categorical values in train dataset to one hot encoded","d13e5f30":"Here the the penguins belonging to the island 'Torgersen' is very less.","df8197ac":"# What is KNN\nKNN stands for K-Nearest Neighbors. It's basically a classification algorithm that will make a prediction of a class of a target variable based on it's nearest neighbors. It will calculate the distance from the given point you want to classify your instance based on the majority classes of k-nearest points.","4900bdc9":"We have one more outlier data in 'sex' column.","7c7ebca7":"### Box Plot to check the outliers in the data","1f40a718":"## Checking for coorelations","851a96e3":"Here we can see that we are getting the correct results","5f094639":"## Univariate Analysis","05eb56a1":"### Dist plot to check the variation of the data in the continous variables ","136a5458":"### Count Plots to see the balance of data in categorical values","5bb9c33c":"# Model Building - KNN\n- Split the data\n- Define the distance functions \n- Calculate the distance of test point from all the points in the dataset\n- Sort the distance\n- Take the majority vote \n- Predict the class","cb007e6a":"# Testing the same dataset with the KNN algo from SKlearn"}}