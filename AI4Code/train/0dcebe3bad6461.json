{"cell_type":{"4cbc126b":"code","2555ce32":"code","09e3e3d4":"code","69bb2000":"code","db3d42e5":"code","3d55fc1e":"code","29bd168d":"code","ea0348cf":"code","063cc573":"code","e53f6e1b":"code","40c2ba2f":"code","28525e7b":"code","60b987de":"code","785ecc04":"code","02225a68":"code","5434768b":"code","1557c42c":"code","b56806e2":"code","428e7c9c":"code","4d29e30f":"code","6e6ec51d":"code","d493a2cc":"code","2d835c8a":"code","21af6791":"code","f59d0d52":"code","856e25a5":"code","7ce6cda0":"code","7db61e9d":"code","de2d36e7":"code","0b0fe6ca":"code","3b6fe1c3":"code","f7c482e4":"code","7eff1cb3":"code","06f70dd4":"code","a9c441d5":"code","0619eab1":"code","52b99c8f":"code","021a0530":"code","8463c385":"code","2391e32a":"code","e564a46a":"code","f4f63f50":"code","aec6bb74":"code","24131b97":"code","e3fd535c":"code","9e4f8d93":"code","107f27e3":"code","8d64bce9":"code","06c514ff":"code","4fe7db3f":"code","4703dc7c":"code","d68094ed":"code","66f95ee6":"code","4c880c8d":"code","2cc2656f":"code","c0f8a1de":"code","fa6e5eac":"code","543073cd":"code","924af5b4":"code","35830f5e":"code","66740633":"code","504670ef":"code","9da9fcb5":"markdown","c6d6b120":"markdown","48104aaf":"markdown","6c761c48":"markdown","f1424035":"markdown","0b8a1552":"markdown","c26df6f2":"markdown","0a0635b9":"markdown","d5c822d6":"markdown","bc96d1bf":"markdown","fdf633ee":"markdown","39154dc0":"markdown","8790249b":"markdown","42064b08":"markdown","b191086c":"markdown","746f63e4":"markdown","4dac0d39":"markdown","c48c99f5":"markdown","248839e5":"markdown","bc2563c0":"markdown","42c5e0c3":"markdown","aa43ece3":"markdown","f517ef22":"markdown","3a5ee050":"markdown","bb890835":"markdown","f39be485":"markdown","965bd41d":"markdown","52b7d2fc":"markdown","6896fb40":"markdown","00271fdd":"markdown","812fe9f1":"markdown","87e95b4c":"markdown","d93b71fa":"markdown","48c9c272":"markdown","b23e5957":"markdown","c6f6611a":"markdown","90fcad63":"markdown","25c777c0":"markdown","39d5e315":"markdown","bb4a09e7":"markdown","5fb676e4":"markdown","04e876c3":"markdown","b30d0039":"markdown","58a60f01":"markdown","1c08c0d9":"markdown","9cd5caaf":"markdown","a2dcbbd4":"markdown","d9af586a":"markdown","14617469":"markdown","cdffba79":"markdown","beebcd1c":"markdown","920104c6":"markdown","03f2fa3e":"markdown","8dfc9296":"markdown","ca20f5c9":"markdown","a5c46c0a":"markdown","e7f074d3":"markdown","2721e7db":"markdown","b8bc92eb":"markdown","fb7db714":"markdown","b2c12d4c":"markdown","cf5f068a":"markdown"},"source":{"4cbc126b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2555ce32":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","09e3e3d4":"train.head()","69bb2000":"test.head()","db3d42e5":"train.info()\nprint('_'*40)\ntest.info()","3d55fc1e":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n# Train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n# Test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","29bd168d":"#missing amount for train set\nmissing= train.isnull().sum().sort_values(ascending=False)\npercentage = (train.isnull().sum()\/ train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\nmissing_data.head(3)","ea0348cf":"#missing amount for test set\nmissing= test.isnull().sum().sort_values(ascending=False)\npercentage = (test.isnull().sum()\/ test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\nmissing_data.head(3)","063cc573":"train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","e53f6e1b":"train['Embarked'].value_counts()","40c2ba2f":"isn = pd.isnull(test['Fare'])\ntest[isn]","28525e7b":"average_of_fare = test.groupby('Pclass')['Fare'].mean()\ntest['Fare'].fillna(value= average_of_fare[3], inplace=True)","60b987de":"# combine train and test then find mean age grouped by Pclass\nage_mean_by_pclass = pd.concat([train, test]).groupby('Pclass')['Age'].mean()\nage_mean_by_pclass","785ecc04":"train.loc[train['Age'].isnull(), 'Age'] = train['Pclass'].map(age_mean_by_pclass)\ntest.loc[test['Age'].isnull(), 'Age'] = test['Pclass'].map(age_mean_by_pclass)","02225a68":"# Sex & Age\ng = sns.FacetGrid(train, hue = 'Survived', col = 'Sex', height = 3, aspect = 2)\ng.map(plt.hist, 'Age', alpha = .5, bins = 20)\ng.add_legend()\nplt.show()","5434768b":"train['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","1557c42c":"def age_range(df):\n    df['Age'].loc[df['Age'] <= 16 ] = 0\n    df['Age'].loc[(df['Age'] > 16) & (df['Age'] <= 32)] = 1\n    df['Age'].loc[(df['Age'] > 32) & (df['Age'] <= 48)] = 2\n    df['Age'].loc[(df['Age'] > 48) & (df['Age'] <= 64)] = 3\n    df['Age'].loc[df['Age'] > 64] = 4   \nage_range(train)\nage_range(test)","b56806e2":"train['Cabin'].isnull().sum()","428e7c9c":"train.groupby(train['Cabin'].isnull())['Survived'].mean()","4d29e30f":"# train\ntrain['Cabin'] = train['Cabin'].notnull().astype('int')\n# test\ntest['Cabin']=test['Cabin'].notnull().astype('int')","6e6ec51d":"train_titles, test_titles = set(), set()  # empty sets to save titles\n\nfor train_name, test_name in zip(train['Name'], test['Name']):\n    train_titles.add(train_name.split(',')[1].split('.')[0].strip())\n    test_titles.add(test_name.split(',')[1].split('.')[0].strip())\nprint(train_titles,'\\n', test_titles)","d493a2cc":"def title(df):\n    # all the titles will be replaced to one of the following: 'Mr', 'Ms', Master, 'Officer', 'Royalty', 'Miss'\n    df['Title'] = df['Name'].str.split(', ').str[1].str.split('.').str[0]\n    \n    df['Title'] = df['Title'].replace(['Capt','Col','Major','Dr','Rev'], 'Officer')\n    df['Title'] = df['Title'].replace(['Mme','Ms'], 'Mrs')\n    df['Title'] = df['Title'].replace(['Jonkheer','Don','Dona','Sir','the Countess','Lady'], 'Royalty')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n\ntitle(train)\ntitle(test)","2d835c8a":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","21af6791":"train['FamilySize'].value_counts()","f59d0d52":"train['FamilySize'] = train['FamilySize'].astype(int)\ntest['FamilySize'] = test['FamilySize'].astype(int)\ndef family_range(df):\n    df['FamilySize'].loc[df['FamilySize'] <= 1 ] = 0\n    df['FamilySize'].loc[(df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)] = 1\n    df['FamilySize'].loc[df['FamilySize'] >= 5] = 2   \nfamily_range(train)\nfamily_range(test)","856e25a5":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n# Train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n# Test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","7ce6cda0":"# Train Data\ntrain = pd.get_dummies(train, columns=['Sex','Embarked','Title'],drop_first=True)\n# Test Data\ntest= pd.get_dummies(test, columns=['Sex','Embarked','Title'],drop_first=True)","7db61e9d":"fig=plt.figure(figsize=(18,10))\nax = fig.gca()\nsns.heatmap(train.corr(), annot=True,ax=ax, cmap=plt.cm.YlGnBu)\nax.set_title('The correlations between all numeric features')\npalette =sns.diverging_palette(80, 110, n=146)\nplt.show()","de2d36e7":"corr_matrix = train.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","0b0fe6ca":"_ = sns.countplot(data=train, x='Survived', hue='Pclass')\n_.set_title('Number of Survivals by Class')\n_.set_xlabel('Survival')\n_.set_xticklabels(['Dead', 'Survived'])\n_.legend(['1st class', '2nd class', '3rd class']);","3b6fe1c3":"print('Survival rate per class: ')\ntrain.groupby('Pclass')['Survived'].sum() \/ train['Pclass'].value_counts() # Survive percent by class","f7c482e4":"_ = sns.countplot(data=train, x='Pclass', hue='Sex_male')\n_.set_title('Number of Males and Females by Class')\n_.set_xticklabels(['1st class', '2nd class', '3rd class'])\n_.legend(['Female', 'Male']);","7eff1cb3":"_ = sns.countplot(data=train, x='Survived', hue='FamilySize')\n_.set_title('Number of Deaths by Family Size')\n_.set_xlabel('Survival')\n_.set_xticklabels(['Dead','Survived'])\n_.legend(['Single', 'Small (2 to 4)', 'Large (5 and more)'], title='Family Size');","06f70dd4":"print('Survival rate for people by there Family Size: ')\n(train.groupby('FamilySize')['Survived'].sum() \/ train['FamilySize'].value_counts()).rename({0:'Single', 1:'Small', 2:'Large'})","a9c441d5":"_ = sns.countplot(data=train, x='Survived', hue='Sex_male')\n_.set_title('Number of Survivals and Deaths Per Gender')\n_.set_xlabel('Survival')\n_.set_xticklabels(['Dead','Survived'])\n_.legend(['Female', 'Male']);","0619eab1":"print('Survival rate per gender: ')\n(train.groupby('Sex_male')['Survived'].sum() \/ train['Sex_male'].value_counts()).rename(index={0:'female',1:'male'})","52b99c8f":"_ = sns.countplot(data=train, x='Survived')\n_.set_title('Number of Survivals and Deaths')\n_.set_xlabel('Survival')\n_.set_xticklabels(['Dead','Survived']);","021a0530":"train['Survived'].value_counts()","8463c385":"# majority_count, minority_count = train['Survived'].value_counts()\n# majority = train[train['Survived'] == 0]\n# minority = train[train['Survived'] == 1]\n\n# minority_overSamp = minority.sample(majority_count, replace=True, random_state=55)\n# train_overSamp = pd.concat([majority, minority_overSamp])\n\n# train_overSamp['Survived'].value_counts()","2391e32a":"# majority_count, minority_count = train['Survived'].value_counts()\n# majority = train[train['Survived'] == 0]\n# minority = train[train['Survived'] == 1]\n\n# majority_underSamp = majority.sample(minority_count)\n# train_underSamp = pd.concat([majority_underSamp, minority], axis=0)\n\n# train_underSamp['Survived'].value_counts()","e564a46a":"# from imblearn.over_sampling import SMOTE\n# oversample = SMOTE()\n# X, y = oversample.fit_resample(X, y)","f4f63f50":"# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import chi2\n# ksel = SelectKBest(chi2, k=9) \n# ksel.fit(X, y) \n# new_X = ksel.transform(X)\n# new_testing = ksel.transform(testing)\n\n# ix = ksel.get_support()\n# pd.DataFrame(new_X, columns = X.columns[ix]).head(5)","aec6bb74":"# Train data\nfeatures_drop = ['PassengerId','Name', 'Ticket', 'Survived','SibSp','Parch']","24131b97":"selected_features = [x for x in train.columns if x not in features_drop]","e3fd535c":"# Test data\nfeatures_drop_test = ['PassengerId','Name', 'Ticket','SibSp','Parch']","9e4f8d93":"selected_features_test = [x for x in test.columns if x not in features_drop_test]","107f27e3":"# Train data\nX = train[selected_features]\ny = train['Survived']","8d64bce9":"# Test data\ntesting = test[selected_features_test]","06c514ff":"ss = StandardScaler()\nXs = ss.fit_transform(X)","4fe7db3f":"testing_s = ss.transform(testing)","4703dc7c":"X_train, X_test, y_train, y_test = train_test_split(\n    Xs, y, test_size = .3, random_state= 42, stratify = y) ","d68094ed":"def modeling(model, X_train, y_train, test_data, X_test=None, y_test=None, prefit=False):\n    '''Takes model and data then print model results with some metrics then return predictions'''\n    \n    start = \"\\033[1m\" # to create BOLD print\n    end = \"\\033[0;0m\" # to create BOLD print\n    \n    # Print bold model name \n    model_name = str(model)#.split('(')[0]\n    print(''.join(['\\n', start, model_name, end]))\n    \n    #Fit model\n    if not prefit:\n        model.fit(X_train, y_train)\n    \n    #Accuarcy score    \n    print('Train Score', model.score(X_train, y_train))\n    try:\n        print('Test Score :', model.score(X_test, y_test))\n    \n        #confusion matrix\n        X_test_pred = model.predict(X_test)\n        print('\\nconfusion matrix\\n', confusion_matrix(y_test, X_test_pred))  \n    except: pass\n    \n    #cross val score\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_res = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n    print('\\nCV scores: ', cv_res)\n    print('CV scores mean: ', cv_res.mean())  \n    \n    #predictions\n    y_pred = model.predict(test_data)\n    print('\\nFirst 10 Predictions: \\n', y_pred[:10])\n\n\n    return y_pred","66f95ee6":"# Models\nlogreg = LogisticRegression(max_iter=300)\nknn = KNeighborsClassifier(n_neighbors=7)  \nsvm_lin = svm.SVC(kernel='linear', C=33)\nsvm_poly = svm.SVC(kernel='poly', C=3)\nsvm_rbf = svm.SVC(kernel='rbf', C=33)\n\n# randomF = RandomForestClassifier(max_depth=350, n_estimators=9, max_features=11, random_state=14, min_samples_split=3)\nrandomF = RandomForestClassifier(max_depth=350, random_state=42)\ndtree= DecisionTreeClassifier(random_state=42)\nextree = ExtraTreesClassifier(n_estimators=66, min_samples_split=7, random_state=42)\nxgb_model = xgb.XGBClassifier(colsample_bytree= 0.8, gamma= 1, learning_rate= 0.002,\n                              max_depth= 8, min_child_weight= 1,subsample= 0.8,)\n\n\n\nmodels = [(logreg,'logreg'), (knn,'knn'), (svm_lin,'svm_lin'), (svm_poly,'svm_poly'), (svm_rbf,'svm_rbf'),\n          (randomF,'randomF'), (dtree,'dtree'), (extree,'extree'), (xgb_model,'xgb_model')]\n\npreds = {}    # empty dict to save all models predictions\nfor model, name in models:\n    preds[name] = modeling(model, X_train, y_train, testing_s, X_test, y_test)","4c880c8d":"def g_search(model, param, X_train, y_train, test_data, X_test=None, y_test=None):\n    '''Simple grid search with kfold'''\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    gs = GridSearchCV(model,\n                  param,\n                  scoring='accuracy',\n                  cv=cv,\n                  n_jobs=-1,\n                  verbose=0)\n    gs.fit(X_train, y_train)\n    \n    # Results\n    y_pred = modeling(gs.best_estimator_, X_train, y_train, test_data, X_test, y_test, prefit=True) # print results and return predictions\n    \n    print('Best parameters: ', gs.best_params_)\n    \n    return y_pred","2cc2656f":"# grid search using all the data (Xs, y)\ngrid_logreg_pred = g_search(LogisticRegression(), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\ngrid_knn_pred = g_search(KNeighborsClassifier(), {'n_neighbors': np.arange(1, 100, 1)}, Xs, y, testing_s)\ngrid_svm_lin_pred = g_search(svm.SVC(kernel='linear'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\ngrid_svm_poly_pred = g_search(svm.SVC(kernel='poly'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\ngrid_svm_rbf_pred = g_search(svm.SVC(kernel='rbf'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)","c0f8a1de":"dt = DecisionTreeClassifier()\ndt_en = BaggingClassifier(base_estimator=dt, n_estimators=500, max_features=4, random_state=55)\n\ndt_en_pred = modeling(dt_en, Xs, y, testing_s) ","fa6e5eac":"adaboost = AdaBoostClassifier(n_estimators=67)\nadaboost.fit(X_train, y_train)\n\nadaboost_pred = modeling(adaboost, Xs, y, testing_s) ","543073cd":"xgb_model = xgb.XGBClassifier(\n\n    colsample_bytree= 0.8,\n    gamma= 1,\n    learning_rate= 0.002,\n    max_depth= 8,\n    min_child_weight= 1,\n    subsample= 0.8,\n)\n\nxgb_pred = modeling(xgb_model, Xs, y, testing_s)","924af5b4":"param_grid = {\n                   'n_estimators': np.arange(50,500,20),\n                   'max_depth' : [i for i in range(1,15,1)],\n#                    'gamma': [1,2,3,4],\n#                    'reg_alpha': [0,1,2,3]\n}\nxgb_model = xgb.XGBClassifier(\n\n    colsample_bytree= 0.8,\n    learning_rate= 0.001,\n    min_child_weight= 1,\n    subsample= 0.8,\n)\n\ngrid_xgb_pred = g_search(xgb_model, param_grid, Xs, y, testing_s)","35830f5e":"thesubmission = gender_submission.copy()\nthesubmission['Survived'] = xgb_pred\nthesubmission['Survived'].head(10)","66740633":"thesubmission.to_csv('thesubmission.csv', index=False)","504670ef":"# xgb_model = xgb.XGBClassifier(\n\n#     colsample_bytree= 0.8,\n#     gamma= 1,\n#     learning_rate= 0.002,\n#     max_depth= 8,\n#     min_child_weight= 1,\n#     subsample= 0.8     \n# )\n\n# Train Score 0.8709315375982043\n\n# CV scores:  [0.8547486  0.85393258 0.83707865 0.83707865 0.8258427 ]\n# CV scores mean:  0.8417362375243236\n\n# First 10 Predictions: \n#  [0 1 0 0 1 0 1 0 1 0]\n\n# features_drop = ['PassengerId','Name', 'Ticket', 'Survived','SibSp','Parch']\n\n# X_train, X_test, y_train, y_test = train_test_split(\n#     Xs, y, test_size=.3,random_state=55, stratify=y) \n\n# Kaggle score: 0.79186","9da9fcb5":"### Embarked\nFrom the data description wee see that \"Embarked\" (Port of Embarkation) contains 3 values: C, Q, S which means: C = Cherbourg, Q = Queenstown, S = Southampton<br>\nAnd since there is only 2 null values in the train, We will fill them with the mode","c6d6b120":"## Features Selection","48104aaf":"Now let's take a look at the most important variables, which will have strong linear releationship with \n<b>Survived<\/b> variable .<br><br>","6c761c48":"#### Family size to groups","f1424035":"And from the above titles we can create a dictionary contains all titles meaning, Then we will create a new feature contains titles","0b8a1552":"### Best model parameters","c26df6f2":"We can see below that the data is imbalanced, And we will try different techniques to solve this problem","0a0635b9":"Most of the rows is null, Let's find out if we can gain some benefit","d5c822d6":"We can extract the title from \"Name\" feature","bc96d1bf":"#### Age groups","fdf633ee":"- The graph below shows that the number of deaths in males was a lot more than females","39154dc0":"\n\n# &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Titanic\n\n\n\n","8790249b":"### Dropping Some Features","42064b08":"- ### These datasets include 11 explanatory variables:","b191086c":"**AdaBoost Classifier**","746f63e4":"**The mean age by each Pclass.**","4dac0d39":"# Exploring the Data","c48c99f5":"## Making several new features ","248839e5":"So we can put 0 for null values, and 1 for non null values ","bc2563c0":"### Fare\nThere is only one 'Fare' value missing in the test, And we will fill it with the mean grouped by Pclass","42c5e0c3":"### Submission","aa43ece3":"## Importing packages","f517ef22":"## One Hot Encoding\nCreating Dummies For Categorical Features.","3a5ee050":"It says, 66% of passengers who have non-missing cabin values survived","bb890835":"**Number of Males and Females by Class**","f39be485":"## Check Missing Values","965bd41d":"# Visual Analysis","52b7d2fc":"# Feature Engineering","6896fb40":"- The graph below shows that the death rate of males was higher than females\n- The graph below shows that older passengers had less chance of survival.\n<br><br>\nTherefore we will create age groups","00271fdd":"![kaggle_score.png](attachment:kaggle_score.png)","812fe9f1":"## Smote sampling (worse results)","87e95b4c":"## Oversampling (worse results)\nTrying to balance the data by duplicating some rows for people who survived to be 549 survived and 549 dead","d93b71fa":"**Correlation with the target**","48c9c272":"**Survived Correlation Matrix**","b23e5957":"## Apply modeling","c6f6611a":"**Number of Survivals and Deaths Per Gender**","90fcad63":"- The graph below shows the number of deaths for people by there Family Size","25c777c0":"## Making sure there is no null values","39d5e315":"###  Title Feature","bb4a09e7":"- The graph below shows that most of men were in the 3rd class.  ","5fb676e4":"### Family Size Feature \n(from adding 'SibSp' and  'Parch' togather)","04e876c3":" ## Handle Missing Data","b30d0039":"**Number of Deaths by Family Size**","58a60f01":"**Splitting and Standardizing Train Data to Obtain Test Scores**","1c08c0d9":"**We fill them with the mean age with respect to each Pclass.**","9cd5caaf":"- In this Kaggle competition, we aim to predict which passengers survived the Titanic shipwreck according to economic status (class), sex, age .\n\n- In this competition, we face a binary classification problem and we will try to solve it","a2dcbbd4":"## Undersampling (worse results)\nTrying to balance the data by removing some rows for people who are dead to be 342 survived and 342 dead","d9af586a":"## Introduction","14617469":"### Kaggle Score\n(This is an old score because Titanic validation data on kaggle change every month)\n<br>Old score: 0.80382\n<br>Current: 0.79186","cdffba79":"### Age\nBoth train and test data contains a lot of Age null values, And we will fill them with the mean age with respect to each Pclass, And then we will create age groups","beebcd1c":"Train data have Survived (dependent variable) and other predictor variables.\nTest data include the same variables that in train data, but without Survived (dependent variable) because this data will be submitted to kaggle.","920104c6":"## Loading the Titanic","03f2fa3e":"# Dealing with imbalanced data","8dfc9296":"**Number of Survivals by Class**","ca20f5c9":"**BaggingClassifier with a decision tree base estimator**","a5c46c0a":"### Cabin","e7f074d3":"- The graph below shows that the number of survivals in the  3rd class was lower than 1st and 2nd class.","2721e7db":"**Grid Search XGBOOST**","b8bc92eb":"-  #### Data Dictionary\n\n|Feature|Dataset|Description|\n|-------|---|---|\n|Survival|Train|The number of survived the Titanic shipwreck| \n|Pclass|Train\/Test|Economic status (class)| \n|Sex|Train\/Test|male or female.| \n|Age|Train\/Test|Age in years| \n|Sibsp\/Parch|Train\/Test|The number of siblings, spouses, or children aboard the Titanic.| \n|ticket|Train\/Test|Ticket number.| \n|Fare|Train\/Test|Passenger fare| \n|Cabin|Train\/Test|Cabin number| \n|Embarked|Train\/Test|Port of Embarkation| \n\n","fb7db714":"![g.jpg](attachment:g.jpg)","b2c12d4c":"## XGBoost (best result)","cf5f068a":"# Modeling "}}