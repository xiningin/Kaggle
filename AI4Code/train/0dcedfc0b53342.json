{"cell_type":{"0458a209":"code","4a8d40ee":"code","c6c09fe2":"code","8706df69":"code","7cb6150d":"code","117155f1":"code","ac20c530":"code","e9ce1b9d":"code","e19edfc3":"code","86b75d33":"code","8c5af3f8":"code","6a3e200e":"code","4f9c3cd5":"code","39ea0ae2":"code","3ae182de":"code","ab860c76":"code","96aa0b5e":"code","473d7c02":"code","08d3930f":"code","9cccfa62":"code","007ac47e":"code","3866a31f":"code","8cd70e0b":"code","c96377c6":"code","285760f5":"code","d7069b1b":"code","3a6b2282":"code","015e5514":"code","ddf9db83":"code","bea1e920":"code","c94b19b2":"code","d2d1fb03":"code","4d7fbcee":"code","602542f7":"code","0b8db40b":"code","a436f1e2":"code","5b0e4a30":"code","5ad11f5a":"code","7c43dce3":"code","2eceec42":"code","0602da73":"code","58bd0e6d":"code","e2c9cdb8":"code","a730e4f1":"code","b1311442":"code","49c03786":"code","c377744e":"code","5499bb49":"code","6d55c67e":"code","e298d03a":"code","4930b8fa":"code","8aa7c77c":"markdown","9b127674":"markdown","9534be47":"markdown","90cb1bb3":"markdown","2c09c954":"markdown","9463517d":"markdown","1df0d09d":"markdown","634dd79b":"markdown","69d9deae":"markdown","16e0cbf8":"markdown","02d10e96":"markdown","d1fab02d":"markdown","6eeb34ec":"markdown","2fc0c7ad":"markdown","c9b20e62":"markdown","d0a4716c":"markdown"},"source":{"0458a209":"#import important libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport string\nimport re, os, io\nfrom numpy import array, argmax, random, take\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', None)","4a8d40ee":"!pip install tensorflow-addons==0.12.1 -q","c6c09fe2":"import tensorflow as tf\ntf.__version__","8706df69":"import tensorflow_addons as tfa","7cb6150d":"from tensorflow import keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers","117155f1":"filepath_en = \"..\/input\/teleng\/train.en\"\n# data source: https:\/\/indicnlp.ai4bharat.org\/corpora\/","ac20c530":"filepath_te = \"..\/input\/teleng\/train.te\"\n# data source: https:\/\/indicnlp.ai4bharat.org\/corpora\/","e9ce1b9d":"tel_text  = io.open(filepath_te,encoding='UTF-8').read().strip().split('\\n')\neng_text  = io.open(filepath_en,encoding='UTF-8').read().strip().split('\\n')\nprint(\"Number of Examples in Telugu Language Data:\",len(tel_text))\nprint(\"Number of Examples in English Language Data:\",len(eng_text))","e19edfc3":"sample_tel = tel_text[0:5]\nsample_eng = eng_text[0:5]\nprint(sample_tel)\nprint()\nprint(sample_eng)","86b75d33":"import unicodedata\n\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='BN')","8c5af3f8":"# Remove punctuations, reduce words to lower case,and adding '<start>' and '<end>' tokens to the sentences\n\ndef preprocess_eng(w):\n    #w= unicode_to_ascii(w)\n    w= w.lower().strip()\n    w= re.sub(r'([?.!,\u00bf_])',r' \\1 ',w)\n    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n    w= re.sub(r\"[^a-zA-Z?.!,\u00bf_]+\",\" \",w)\n    w= w.strip()\n    w= '<start> '+ w +' <end>'\n    return w","6a3e200e":"line=[preprocess_eng(i) for i in sample_eng]\ntuple(line)","4f9c3cd5":"def preprocess_tel(w):\n    w= unicode_to_ascii(w)\n    w= re.sub(r'([\\?.!,\u00bf_])',r' \\1 ',w)\n    w= re.sub(r'[\" \"]+', \" \",w) # keep white space\n    w= w.strip()\n    w= '<start> '+ w +' <end>'\n    w= w.replace(\"u'\", \"'\")\n    return w","39ea0ae2":"def create_eng_dataset(filepath,num_examples):\n    #limit num_examples for faster training - len(lines) is full data\n    lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n    lines=[preprocess_eng(i) for i in lines[:num_examples]]\n    return tuple(lines)\n\n\ndef create_tel_dataset(filepath,num_examples):\n    #limit num_examples for faster training - len(lines) is full data\n    lines= io.open(filepath,encoding='UTF-8').read().strip().split('\\n')\n    lines=[preprocess_tel(i) for i in lines[:num_examples]]\n    return tuple(lines)","3ae182de":"i = create_eng_dataset(filepath_en,5)\ni","ab860c76":"len(i)","96aa0b5e":"j = create_tel_dataset(filepath_te,5)\nj","473d7c02":"# tokenize function \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional","08d3930f":"def tokenize(lang):\n    lang_tokenizer = Tokenizer(filters='',oov_token='<OOV>')\n    lang_tokenizer.fit_on_texts(lang)\n    lang_data = lang_tokenizer.texts_to_sequences(lang)\n    lang_data = pad_sequences(lang_data,padding=\"post\")\n    return lang_data, lang_tokenizer","9cccfa62":"def load_dataset(filepath1,filepath2, num_examples=None):\n    # create cleaned input and target sentences\n    input_lang = create_eng_dataset(filepath1,num_examples)\n    targ_lang  = create_tel_dataset(filepath2,num_examples)\n    input_lang, inp_lang_tokenizer  = tokenize(input_lang)\n    target_lang, targ_lang_tokenizer= tokenize(targ_lang)\n    return input_lang,target_lang,inp_lang_tokenizer,targ_lang_tokenizer","007ac47e":"num_examples= 32000\ninput_lang,target_lang,inp_lang_tokenizer,targ_lang_tokenizer = load_dataset(filepath_en,filepath_te,num_examples)","3866a31f":"input_lang.shape,target_lang.shape","8cd70e0b":"input_lang[0].shape","c96377c6":"from sklearn.model_selection import train_test_split\n\ninput_train, input_val, target_train, target_val = train_test_split(input_lang, target_lang, test_size=0.2)","285760f5":"input_train.shape,target_train.shape","d7069b1b":"buffer_size = 64000\nbatch_size  = 64\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((input_train,target_train))\ntrain_dataset = train_dataset.shuffle(buffer_size).batch(batch_size,drop_remainder =True)","3a6b2282":"train_dataset","015e5514":"val_dataset = tf.data.Dataset.from_tensor_slices((input_val,target_val))\nval_dataset = val_dataset.batch(batch_size,drop_remainder =True)","ddf9db83":"ex_input_batch,ex_tar_batch = next(iter(train_dataset))\nex_input_batch.shape,ex_tar_batch.shape","bea1e920":"maxlen_input = ex_input_batch.shape[1]\nmaxlen_output   = ex_tar_batch.shape[1]","c94b19b2":"vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\nvocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n\nembedding_dim = 256\nunits = 512\nnum_examples = 32000\nsteps_per_epoch = num_examples\/\/batch_size\n\nprint(\"max_length_english, max_length_telugu, vocab_size_english, vocab_size_telugu\")\nmaxlen_input, maxlen_output, vocab_inp_size, vocab_tar_size","d2d1fb03":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n          ##-------- LSTM layer in Encoder ------- ##\n        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,return_sequences=True,\n                                               return_state=True,\n                                               recurrent_initializer='glorot_uniform')\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, h, c = self.lstm_layer(x, initial_state = hidden)\n        return output, h, c\n    \n    def initialize_hidden_state(self):\n        return [tf.zeros((self.batch_size, self.enc_units)), tf.zeros((self.batch_size, self.enc_units))] # all zeros","4d7fbcee":"## Test Encoder Stack\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units,batch_size)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_h, sample_c = encoder(ex_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\nprint ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))","602542f7":"import gc\ngc.collect()","0b8db40b":"np.random.seed(42)\ntf.random.set_seed(42)\n\nencoder_embedding_dim = 256\ndecoder_embedding_dim = 256\nunits=512\nbatch_size =64\nmaxlen_output = target_train.shape[1]","a436f1e2":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n        super(Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.units = units\n        \n        # Embedding Layer\n        self.decoder_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        \n        #Final Dense layer on which softmax will be applied\n        self.output_layer = tf.keras.layers.Dense(vocab_size)\n        \n        # Define the fundamental cell for decoder recurrent structure\n        self.decoder_inner_cell = tf.keras.layers.LSTMCell(self.units)\n        \n        # Sampler\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        \n        ## Attention Wrapper ##\n        self.attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(units,memory=None,\n                                                                                memory_sequence_length=self.batch_size*[maxlen_input])\n        self.attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(cell=self.decoder_inner_cell, \n                                                                                     attention_mechanism=self.attention_mechanism,\n                                                                                     attention_layer_size=self.units)\n        \n        ## Basic Decoder ##\n        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,sampler = self.sampler,\n                                               output_layer = self.output_layer)\n        \n         \n        self.inference_decoder = tfa.seq2seq.BasicDecoder(cell=self.attention_decoder_cell,\n                                                          sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n                                                              embedding_fn = self.decoder_embedding),\n                                                          output_layer=self.output_layer,maximum_iterations = 154)\n        \n    def build_initial_state(self, batch_size, encoder_state, Dtype):\n        decoder_initial_state = self.attention_decoder_cell.get_initial_state(batch_size=batch_size, dtype=Dtype)\n        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n        return decoder_initial_state\n        \n    def call(self,decoder_input,initial_state):\n        decoder_embeddings = self.decoder_embedding(decoder_input)\n        outputs, _, _ = self.decoder(decoder_embeddings,initial_state=initial_state,\n                                     sequence_length=self.batch_size*[maxlen_output-1])\n        return outputs","5b0e4a30":"# Test decoder stack\n\ndecoder = Decoder(vocab_tar_size, embedding_dim,512, batch_size)\nsample_x = tf.random.uniform((batch_size, maxlen_output))\ndecoder.attention_mechanism.setup_memory(sample_output) # from encoder\ninitial_state = decoder.build_initial_state(batch_size, [sample_h, sample_c], tf.float32)\n\n\nsample_decoder_outputs = decoder(sample_x, initial_state)\n\nprint(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)","5ad11f5a":"gc.collect()","7c43dce3":"#Optimizer\n\noptimizer = tf.keras.optimizers.Adam()\n\n\ndef loss_function(real, pred):\n    # real shape = (batch_size, maxlen_output)\n    # pred shape = (batch_size, maxlen_output, vocab_tar_size )\n    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    loss = cross_entropy(y_true=real, y_pred=pred)\n    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n    mask = tf.cast(mask, dtype=loss.dtype)  \n    loss = mask* loss\n    loss = tf.reduce_mean(loss)\n    return loss","2eceec42":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","0602da73":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n    \n    with tf.GradientTape() as tape:\n        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n        \n        dec_input = targ[ : , :-1 ] # Ignore <end> token\n        real = targ[ : , 1: ]         # ignore <start> token\n        \n        # Set the AttentionMechanism object with encoder_outputs\n        decoder.attention_mechanism.setup_memory(enc_output)\n        # Create AttentionWrapperState as initial_state for decoder\n        decoder_initial_state = decoder.build_initial_state(batch_size, [enc_h, enc_c], tf.float32)\n        pred = decoder(dec_input, decoder_initial_state)\n        logits = pred.rnn_output\n        loss = loss_function(real, logits)\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss","58bd0e6d":"gc.collect()","e2c9cdb8":"import time\n\nEPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    \n    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n        \n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n    # saving (checkpoint) the model every 2 epochs\n    if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss \/ steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","a730e4f1":"def beam_evaluate_sentence(sentence, beam_width=3):\n    y = str(sentence)\n    y = preprocess_eng(y)\n    sentence= str(y)\n\n    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.strip().split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    inference_batch_size = inputs.shape[0]\n    result = ''\n    \n    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n    \n    dec_h = enc_h\n    dec_c = enc_c\n    \n    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n    end_token = targ_lang_tokenizer.word_index['<end>']\n    \n    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n    decoder.attention_mechanism.setup_memory(enc_out)\n    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 194, 512]] :\", enc_out.shape)\n    \n    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n    decoder_initial_state = decoder.attention_decoder_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n    \n    # Instantiate BeamSearchDecoder\n    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.attention_decoder_cell,beam_width=beam_width, output_layer=decoder.output_layer)\n    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n    \n    # The BeamSearchDecoder object's call() function takes care of everything.\n    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, \n                                                              start_tokens=start_tokens, end_token=end_token, \n                                                              initial_state=decoder_initial_state)\n    \n    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n    return final_outputs.numpy(), beam_scores.numpy()","b1311442":"def beam_translate(sentence):\n    result, beam_scores = beam_evaluate_sentence(sentence)\n    print(result.shape, beam_scores.shape)\n    print()\n    \n    for beam, score in zip(result, beam_scores):\n        print(beam.shape, score.shape)\n        print()\n        \n    output = targ_lang_tokenizer.sequences_to_texts(beam)\n    output = [a[:a.index('<end>')] for a in output]\n    beam_score = [a.sum() for a in score]\n    print('Input: %s' % (sentence))\n    print()\n    \n    for i in range(len(output)):\n        print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))","49c03786":"beam_translate('india also continues to push back economically.')","c377744e":"beam_translate(u'rise again.')\n#\u0c2e\u0c33\u0c4d\u0c32\u0c40 \u0c09\u0c26\u0c2f\u0c3f\u0c38\u0c4d\u0c24\u0c3e\u0c21\u0c41.","5499bb49":"beam_translate(u'I remember my childhood days.')\n#\u0c35\u0c3f\u0c26\u0c4d\u0c2f\u0c3e\u0c30\u0c4d\u0c25\u0c41\u0c32\u0c28\u0c41 \u0c1a\u0c42\u0c38\u0c4d\u0c24\u0c41\u0c02\u0c1f\u0c47 \u0c28\u0c3e\u0c15\u0c41 \u0c1a\u0c3f\u0c28\u0c4d\u0c28\u0c2a\u0c4d\u0c2a\u0c1f\u0c3f \u0c30\u0c4b\u0c1c\u0c41\u0c32\u0c41 \u0c17\u0c41\u0c30\u0c4d\u0c24\u0c41\u0c15\u0c4a\u0c38\u0c4d\u0c24\u0c41\u0c28\u0c4d\u0c28\u0c3e\u0c2f\u0c3f.')","6d55c67e":"def evaluate_sen(sentence):\n    y = str(sentence)\n    y = preprocess_eng(y)\n    sentence= str(y)\n\n    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=maxlen_input,padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    inference_batch_size = inputs.shape[0]\n    result = ''\n    \n    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n    dec_h = enc_h\n    dec_c = enc_c\n    \n    start_tokens = tf.fill([inference_batch_size], targ_lang_tokenizer.word_index['<start>'])\n    end_token = targ_lang_tokenizer.word_index['<end>']\n    \n    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n    \n    # Instantiate BasicDecoder object\n    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.attention_decoder_cell, sampler=greedy_sampler, \n                                                output_layer=decoder.output_layer)\n    \n    # Setup Memory in decoder stack\n    decoder.attention_mechanism.setup_memory(enc_out)\n    \n    # set decoder_initial_state\n    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n    decoder_embedding_matrix = decoder.decoder_embedding.variables[0]\n    \n    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, \n                                     end_token= end_token, initial_state=decoder_initial_state)\n    return outputs.sample_id.numpy()\n\n\ndef translate2(sentence):\n  result = evaluate_sen(sentence)\n  print(result)\n  result = targ_lang_tokenizer.sequences_to_texts(result)\n  print('Input: %s' % (sentence))\n  print('Predicted translation: {}'.format(result))","e298d03a":"translate2(u'rise again.')","4930b8fa":"translate2(u'I remember my childhood days.')","8aa7c77c":"## tfa-addons BasicDecoder for Decoding","9b127674":"#### Sample sentences","9534be47":"### One step training function","90cb1bb3":"## English Language ","2c09c954":"## Model training","9463517d":"## English to Telugu Language Machine Translation Model with \"Luong Attention\" without pre-trained embeddings\nIn this notebook, I am exploring the data sourced from https:\/\/indicnlp.ai4bharat.org to translate the English sequences into Telugu language.\n\nThis one-to-one Translation model.\n\nFor this, I am using keras framework to train the RNNs without pre-trained embeddings with Luong attention.\n\nSteps:\n1. Download the data\n2. Preprocess the data \n3. Create train and test\/valid data sets\n4. Model training\n5. Decoding the outputs of decoder RNN\n6. Performance\/Evaluation of the model\n7. Translation\n\nPlease comment or upvote if you like it!","1df0d09d":"## tfa-addons Beam Search Decoder","634dd79b":"### Example input and target","69d9deae":"### Model saving","16e0cbf8":"## The model successfully translates English to Telugu. However, it's full accurate. There's scope for lot of improvement.\n\nBeam Search Decoder is better than BasicDecoder for decoding.\n\nI believe that pre-trained embeddings would improve the model and reduce the overfitting.","02d10e96":"#### Number of sequences(sentences\/paragraphs) in the data","d1fab02d":"## Telugu Language","6eeb34ec":"### Optimizer and Loss function","2fc0c7ad":"### Preprocess the text data","c9b20e62":"## Decoder Class","d0a4716c":"### Encoder"}}