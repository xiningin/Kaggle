{"cell_type":{"cdb3a61e":"code","c90567b8":"code","d81d8740":"code","3f939112":"code","6b705e17":"code","91dd3031":"code","396fb3df":"code","804b0828":"code","6194e36a":"code","ced985e9":"code","e09369fe":"code","3e0f6d95":"code","89aa0ff6":"code","02d146ae":"code","263f2dac":"code","b7cceabc":"code","69c01dde":"code","a6c6f9e7":"code","410477f5":"code","8e2259e1":"code","f2225cbb":"code","e53defa3":"code","632f82df":"code","5feb9143":"code","35004378":"code","70a0215d":"code","47a0b070":"code","3f270357":"code","ce52dbb1":"code","bff89f1e":"code","5bf5cffe":"code","2f3d4397":"code","d08f636e":"code","4a55d768":"code","76df67f5":"code","86d9ee57":"code","cd0c2370":"code","c5b4c05a":"code","7f7ad9ae":"code","d7b5f0ff":"code","f69bca98":"code","f2d2ae72":"code","86aabe8b":"code","f7349ddb":"code","ad996c21":"code","d46d5955":"code","91bfed8c":"code","107886da":"code","d9a2a66d":"code","1e5bb461":"code","52558886":"code","c99d87bd":"code","c6242835":"code","33738fd1":"code","18f7c84e":"code","1c7d5a66":"code","b6955c2b":"code","306ccce1":"code","bb413001":"code","b6e152da":"code","a3087c4b":"code","8c143450":"code","9285e2c4":"code","1d5a87c5":"code","33b54d98":"code","2a279fc2":"code","cdc81ef5":"code","70c01403":"code","d73a8568":"code","1b4f2152":"code","f08007c2":"code","5b5cc5fb":"markdown","294e148e":"markdown"},"source":{"cdb3a61e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c90567b8":"%%time\nDATASET_COLUMNS = [\"label\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndf = pd.read_csv(\"\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\", encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\ndf.head()","d81d8740":"%%time\ndf.drop(['ids','date','flag','user'],axis=1,inplace=True)","3f939112":"%%time\ndf.head()","6b705e17":"%%time\nimport seaborn as s\ns.countplot(x='label',data=df)","91dd3031":"df","396fb3df":"%%time\ndf['label'].unique()","804b0828":"%%time\ndf[\"label\"].replace({4:1}, inplace=True)","6194e36a":"%%time\ndf","ced985e9":"%%time\ndf.isnull().sum()","e09369fe":"# df['tweet'] = df['tweet'].apply(str)\n# df['label'] = df['label'].apply(str)","3e0f6d95":"df.shape","89aa0ff6":"df.describe()","02d146ae":"%%time\ndf.head()","263f2dac":"df['tweet'].dtype","b7cceabc":"df['label'].dtype","69c01dde":"%%time\ndf['label'] = df['label'].apply(str)","a6c6f9e7":"df['label'].dtype","410477f5":"%%time\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nstopword = set(stopwords.words('english'))\nprint(stopword)","8e2259e1":"# %%time\n# nltk.download('punkt')\n# nltk.download('wordnet')","f2225cbb":"import string","e53defa3":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}","632f82df":"%%time\nimport re\nurlPattern = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\nuserPattern = '@[^\\s]+'\n\ndef process_tweets(tweet):\n    # Lower Casing\n    tweet = tweet.lower()\n    tweet=tweet[0:]\n    # Removing all URls \n    tweet = re.sub(urlPattern,'',tweet)\n    # Removing all @username.\n    tweet = re.sub(userPattern,'', tweet)\n    #Conversion of emoticons to words\n    for emot in EMOTICONS:\n        tweet = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), tweet)\n    #Remove punctuations\n    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n    #tokenizing words\n    tokens = word_tokenize(tweet)\n    #Removing Stop Words\n    final_tokens = [w for w in tokens if w not in stopword]\n    #reducing a word to its word stem \n    wordLemm = WordNetLemmatizer()\n    finalwords=[]\n    for w in final_tokens:\n        if len(w)>1:\n            word = wordLemm.lemmatize(w)\n            finalwords.append(word)\n    return ' '.join(finalwords)","5feb9143":"%%time\ndf = df.sample(frac=1)\ndf = df[:200000]","35004378":"%%time\ndf","70a0215d":"df.shape","47a0b070":"%%time\ndf['temp'] = df['tweet'].apply(lambda x: process_tweets(x))\nprint('Text Preprocessing complete.')","3f270357":"df","ce52dbb1":"%%time\n# DataFrame\nimport pandas as pd \n\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# nltk\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n# tensorflow\nimport tensorflow.compat.v2 as tf\nimport tensorflow_datasets as tfds\n\n# Utility\nimport pandas as pd\nimport numpy as np \nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport string\nimport pickle","bff89f1e":"X = df['temp'].values\ny = df['label'].values","5bf5cffe":"X.shape","2f3d4397":"# from sklearn.model_selection import StratifiedKFold\n# skf=StratifiedKFold(n_splits=4)\n# skf","d08f636e":"print(type(X))","4a55d768":"# words = ''.join(str(df['temp'].tolist()))\n# words","76df67f5":"vector = TfidfVectorizer(ngram_range=(1,2),sublinear_tf=True)\nX = vector.fit_transform(X)\nprint(f'Vector fitted.')\nprint('No. of feature_words: ', len(vector.get_feature_names()))","86d9ee57":"from sklearn.model_selection import cross_val_score\ncross_val_score(LogisticRegression(solver='liblinear'),X,y,cv=5)","cd0c2370":"from sklearn.model_selection import cross_val_score\ncross_val_score(LinearSVC(),X,y,cv=5)","c5b4c05a":"%%time\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(BernoulliNB(),X,y,cv=5)","7f7ad9ae":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=101)","d7b5f0ff":"type(X_test)","f69bca98":"print(X.shape)\nprint(y.shape)","f2d2ae72":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","86aabe8b":"%%time\ndef model_Evaluate(model):\n    #accuracy of model on training data\n    acc_train=model.score(X_train, y_train)\n    #accuracy of model on test data\n    acc_test=model.score(X_test, y_test)\n    \n    print('Accuracy of model on training data : {}'.format(acc_train*100))\n    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n\n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","f7349ddb":"%%time\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'penalty': ['l1','l2'], 'C': [0.01,0.1,1,10,100]}\ngrid_search=GridSearchCV(estimator=LogisticRegression(),\n                        param_grid=grid_values,\n                        scoring='accuracy',\n                        cv=10,\n                        n_jobs=-1)\ngrid_search=grid_search.fit(X_train,y_train)","ad996c21":"grid_search.best_params_","d46d5955":"%%time\n# lg = LogisticRegression(penalty='l2', dual=False, tol=0.01, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight='balanced', random_state=5, solver='lbfgs', max_iter=1000, multi_class='ovr', verbose=0, warm_start=False)\nlg =LogisticRegression(max_iter=100,C=1,penalty='l2')\nlg.fit(X_train, y_train)\nmodel_Evaluate(lg)","91bfed8c":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_values = {'penalty': ['l1','l2'],'C': [0.1, 1, 10, 100, 1000],'fit_intercept':[True,False]} \n  \ngrid = GridSearchCV(estimator=LinearSVC(),param_grid=grid_values)\n  \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","107886da":"grid.best_params_","d9a2a66d":"%%time\nsvm = LinearSVC(C=0.1,fit_intercept=True,penalty='l2')\nsvm.fit(X_train, y_train)\nmodel_Evaluate(svm)","1e5bb461":"BernoulliNB().get_params().keys()","52558886":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_values = {'alpha' : [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0],'fit_prior':[True,False]} \n  \ngrid = GridSearchCV(estimator=BernoulliNB(),param_grid=grid_values)\n  \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","c99d87bd":"grid.best_params_","c6242835":"nb = BernoulliNB(alpha=1.0,fit_prior=False)\nnb.fit(X_train, y_train)\nmodel_Evaluate(nb)","33738fd1":"# %%time\n# from sklearn.model_selection import GridSearchCV\n\n# grid_values = {'n_estimators' :[10,20,30],'criterion':['gini', 'entropy'],'max_depth':[10,20,30,40,50],'max_features' : [\"auto\",\"sqrt\", \"log2\",\"None\"]} \n  \n# grid = GridSearchCV(estimator=RandomForestClassifier(),param_grid=grid_values)\n\n# grid.fit(X_train, y_train)","18f7c84e":"# %%time\n# from sklearn.tree import DecisionTreeClassifier\n# clf_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=10, random_state=13)\n# clf_tree.fit(X_train,y_train)\n# model_Evaluate(clf_tree)","1c7d5a66":"# rf = RandomForestClassifier()\n# rf.fit(X_train, y_train)\n# model_Evaluate(rf)","b6955c2b":"# %%time\n# from sklearn.tree import DecisionTreeClassifier\n# clf_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=10, random_state=1)\n# clf_tree.fit(X_train,y_train)\n# model_Evaluate(clf_tree)","306ccce1":"# %%time\n# from sklearn.model_selection import RandomizedSearchCV\n# grid_values = {'criterion':['gini'],\n#               'max_depth':[1,5,10,None]}\n# grid_search=RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n#                         param_distributions=grid_values,\n#                         scoring='accuracy')\n# grid_search=grid_search.fit(X_train,y_train)","bb413001":"%%time\nfrom sklearn.ensemble import VotingClassifier\nEns = VotingClassifier( estimators= [('LG',lg),('SVM',svm),('NB',nb)], voting = 'hard')\nEns.fit(X_train,y_train)\nmodel_Evaluate(Ens)","b6e152da":"from joblib import dump, load\ndump(Ens, 'tweeter.joblib')","a3087c4b":"import pickle\n\nfile = open('vectoriser.pickle','wb')\npickle.dump(vector, file)\nfile.close()\n\nfile = open('Ens1.pickle','wb')\npickle.dump(Ens, file)\nfile.close()","8c143450":"def load_models():\n    # Load the vectoriser.\n    file = open('vectoriser.pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('Ens1.pickle', 'rb')\n    Ens = pickle.load(file)\n    file.close()\n    return vectoriser, Ens","9285e2c4":"def predict(vectoriser, model, text):\n    # Predict the sentiment\n    processes_text=[process_tweets(sen) for sen in text]\n    textdata = vectoriser.transform(processes_text)\n    print(textdata.shape)\n    sentiment = model.predict(textdata)\n    return sentiment","1d5a87c5":"if __name__==\"__main__\":\n    # Loading the models.\n    vectoriser, Ens1 = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"i am lucky to have this project :)\",\n            \"Work is too hectic.\",\n            \"Mr.Sharama, I am so good\"]\n    \n    df = predict(vectoriser, Ens1, text)\n    print(df)","33b54d98":"# %%time\n# from sklearn.ensemble import VotingClassifier\n# Ens = VotingClassifier(  estimators= [('LG',lg),('RF',rf),('NB',nb),('DT',clf_tree)], voting = 'soft')\n# Ens.fit(X_train , y_train)\n# model_Evaluate(Ens)","2a279fc2":"import pickle\n\nfile = open('vectoriser.pickle','wb')\npickle.dump(vector, file)\nfile.close()\n\nfile = open('logis.pickle','wb')\npickle.dump(lg, file)\nfile.close()","cdc81ef5":"def load_models():\n    # Load the vectoriser.\n    file = open('vectoriser.pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('logis.pickle', 'rb')\n    mod= pickle.load(file)\n    file.close()\n    return vectoriser, Ens","70c01403":"def predict(vector, model, text):\n    # Predict the sentiment\n    processes_text=[process_tweets(sen) for sen in text]\n    textdata = vector.transform(processes_text)\n    sentiment = model.predict(textdata)\n    return sentiment","d73a8568":"if __name__==\"__main__\":\n    # Loading the models.\n    vectoriser, logis = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"i am lucky to have this project :)\",\n            \"Work is too hectic.\",\n            \"Mr.Sharama, I feel so good\"]\n    \n    df = predict(vector, logis, text)\n    print(df.head())","1b4f2152":"# from sklearn.neighbors import KNeighborsClassifier\n# knn = KNeighborsClassifier(n_neighbors=3)\n# knn.fit(X_train,y_train)\n# model_Evaluate(knn)","f08007c2":"# from xgboost import XGBRegressor\n# my_model = XGBRegressor()\n# my_model.fit(X_train, y_train,verbose=False)\n# model_Evaluate(my_model)","5b5cc5fb":"**Pre-Processing**","294e148e":" **Knowing the Dataset**"}}