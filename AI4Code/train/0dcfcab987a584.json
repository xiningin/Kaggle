{"cell_type":{"d9251487":"code","27f751b1":"code","d1bde9ef":"code","7fb094d2":"code","713e032e":"code","327fc572":"code","8d58173a":"code","0d14960a":"code","b3c8b7a2":"code","a945e543":"code","1ec64ad7":"code","ee427130":"code","ab24a062":"code","a99fcce7":"code","f2b8ea76":"code","9d22cee7":"code","54214ce6":"code","ae0beb89":"code","d72a4717":"code","5f8871bf":"code","f595fa21":"code","da6fa9f3":"code","0c193356":"code","8ae671b0":"code","3d044cba":"code","a349e056":"code","a29f7404":"code","8614f60e":"code","2fbb67df":"code","f9ae5a95":"code","9e5d0490":"code","28b3c88e":"code","5d8d364d":"code","0e357d4e":"code","5ea9ef30":"code","faee1a44":"code","e8ccbd2b":"code","fc9248d5":"code","0db11c35":"code","90461950":"code","9c0d8ef0":"code","8521aa91":"code","73236fd9":"code","164dfc49":"code","6b2b9c1e":"code","1b8cc929":"code","ea0462b9":"code","aedd644c":"code","b6fd9ec9":"code","70880279":"code","40696858":"code","08e53e8c":"code","98e4f942":"code","eaca3b99":"code","d75fb274":"code","176dbba7":"code","c33b4b91":"code","9bab255e":"code","236fc3a4":"markdown","06c10cc6":"markdown","2f38e2b3":"markdown","944e1487":"markdown","570cf0cf":"markdown","351448e3":"markdown","80ea1a37":"markdown","20f15389":"markdown","4c24d7d5":"markdown","a52bf617":"markdown","d46e051a":"markdown","eea68be4":"markdown","fe0b04dd":"markdown","220895f7":"markdown","e7228a63":"markdown","aaea7707":"markdown","b4e406ca":"markdown","2d7ab36f":"markdown","852d26d6":"markdown","7ca9a569":"markdown","01fb3a6f":"markdown","45398301":"markdown","ddb63e4d":"markdown","309a7822":"markdown","5973a306":"markdown","d93c2312":"markdown","370e8861":"markdown","5c4ac6f9":"markdown","e319edf5":"markdown","c0ad8f69":"markdown","6d1a7aeb":"markdown","41413d46":"markdown","e3ebecc3":"markdown","e80eeba1":"markdown"},"source":{"d9251487":"#importing neccessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost.sklearn import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n","27f751b1":"sessions=pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/sessions.csv.zip')\ncountries=pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/countries.csv.zip')\nage_gender=pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/age_gender_bkts.csv.zip')\nsubmission=pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/sample_submission_NDF.csv.zip')","d1bde9ef":"#Loading the Data\ntrain= pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/train_users_2.csv.zip')\ntest = pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/test_users.csv.zip')\n","7fb094d2":"train.signup_method.unique()","713e032e":"train.affiliate_channel.unique()","327fc572":"train.affiliate_provider.unique()","8d58173a":"train.first_affiliate_tracked.unique()","0d14960a":"train.first_device_type.unique()","b3c8b7a2":"train.first_browser.unique()","a945e543":"train.signup_app.unique()","1ec64ad7":"countries.sort_values(by='distance_km')","ee427130":"a=train['country_destination'].value_counts() \na=a.drop(['NDF','other'],axis=0) \ndf_value_counts = pd.DataFrame(a)\ndf_value_counts = a.reset_index() \ndf_value_counts.columns = ['country_destination', 'value_counts'] # change column names \nmapdata= pd.merge(df_value_counts, countries,how= 'inner' , on='country_destination')\nmapdata['poppercent']=mapdata['value_counts']\/mapdata['value_counts'].sum() \nmapdata['text']=['United States', 'France', 'Italy','United Kingdom', 'Spanish','Canada' ,'German', 'Dutch','Australia', 'Brazil' ] ","ab24a062":"# Create a world map to show distributions of users \nimport folium\nfrom folium.plugins import MarkerCluster\n#empty map\nworld_map= folium.Map(tiles=\"cartodbpositron\")\nmarker_cluster = MarkerCluster().add_to(world_map)\n#for each coordinate, create circlemarker of user percent\nfor i in range(len(mapdata)):\n        lat = mapdata.iloc[i]['lat_destination']\n        long = mapdata.iloc[i]['lng_destination']\n        radius=5\n        popup_text = \"\"\"Country : {}<br>\n                    %of Users : {}<br>\"\"\"\n        popup_text = popup_text.format(mapdata.iloc[i]['country_destination'],\n                                   mapdata.iloc[i]['poppercent']\n                                   )\n        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n#show the map\nworld_map","a99fcce7":"a=train.groupby('country_destination').count().sort_values(by='id',ascending=True)\na.id\n","f2b8ea76":"countries.dtypes","9d22cee7":"age_gender.drop(age_gender.index[0],inplace=True)\nage_gender","54214ce6":"age_gender.country_destination.unique()","ae0beb89":"age_gender.age_bucket.unique()","d72a4717":"agebtw0n19= age_gender[(age_gender.age_bucket=='0-4') | (age_gender.age_bucket=='5-9') | (age_gender.age_bucket=='10-14')|(age_gender.age_bucket=='15-19')]\nagebtw49n34= age_gender[(age_gender.age_bucket=='45-49') | (age_gender.age_bucket=='40-44') | (age_gender.age_bucket=='35-39') | (age_gender.age_bucket=='30-34')]\nagebtw99n50= age_gender[(age_gender.age_bucket=='95-99') | (age_gender.age_bucket=='90-94') | (age_gender.age_bucket=='85-89') | (age_gender.age_bucket=='80-84') | (age_gender.age_bucket=='75-79') | (age_gender.age_bucket=='70-74') | (age_gender.age_bucket=='70-74') | (age_gender.age_bucket=='65-69') | (age_gender.age_bucket=='60-64') | (age_gender.age_bucket=='55-59') | (age_gender.age_bucket=='50-54') ]\nagebtw20n29= age_gender[(age_gender.age_bucket=='25-29') | (age_gender.age_bucket=='20-24')]\n","5f8871bf":"import seaborn as sns\nax = sns.boxplot(x=agebtw20n29[\"population_in_thousands\"])","f595fa21":"import seaborn as sns\nax = sns.boxplot(x=agebtw99n50[\"population_in_thousands\"])","da6fa9f3":"plt.figure(figsize=(20,5))\nimport seaborn as sns\nage_gender.sort_values(\"age_bucket\", ascending=False,inplace=True)\ncolors=sns.color_palette()\nsns.stripplot(x=\"age_bucket\",y=\"population_in_thousands\",data=age_gender,jitter=True,hue='country_destination',palette='pastel')","0c193356":"actionanditsdetail=pd.crosstab(age_gender.age_bucket, age_gender.country_destination,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionanditsdetail) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')","8ae671b0":"actionanditsdetail=pd.crosstab(age_gender.country_destination, age_gender.gender,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionanditsdetail) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')\n","3d044cba":"sessions=sessions.sort_values('user_id')\nsessions.rename(columns={\"user_id\": \"id\"},inplace=True)\nsessions.sort_values(by='id',inplace=True)\nsessions = sessions[(sessions['secs_elapsed'].notnull()) & (sessions['secs_elapsed'] > 0.0) ]  \nsessions[(sessions['action_detail']=='booking')]","a349e056":"groupedid=pd.DataFrame(sessions.groupby(['id'])['secs_elapsed'].sum())\ngroupedid['hour_elapsed']=groupedid['secs_elapsed'].div(3600).round(decimals=0)\ngroupedid.drop('secs_elapsed',axis=1,inplace=True)\ngroupedid.reset_index(inplace=True)\ngroupedid[(groupedid['id'] == '6udv3scuxe') | (groupedid['id'] == 'yxf0sm9sbw') | (groupedid['id'] == 'nttj7g9av6')] ","a29f7404":"groupedid.hour_elapsed.describe()","8614f60e":"actionndevicetype=pd.crosstab(sessions.action, sessions.device_type,margins=False)\nfrom scipy.stats import chi2_contingency \n\n# defining the table \n\nstat, p, dof, expected = chi2_contingency(actionndevicetype) \n\n# interpret p-value \nalpha = 0.05\nprint(\"p value is \" + str(p)) \nif p <= alpha: \n\tprint('Dependent (reject H0)') \nelse: \n\tprint('Independent (H0 holds true)')","2fbb67df":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(train.country_destination.value_counts())\nsns.countplot(train.country_destination)","f9ae5a95":"import pandas as pd\n#Loading the Data again\ntrain= pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/train_users_2.csv.zip')\ntest = pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/test_users.csv.zip')\n","9e5d0490":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","28b3c88e":"train['first_affiliate_tracked'] = train['first_affiliate_tracked'].fillna('Unknown')\ntest['first_affiliate_tracked'] = test['first_affiliate_tracked'].fillna('Unknown')","5d8d364d":"import seaborn as sns\nsns.boxplot(x=train.age)","0e357d4e":"train.age.describe()","5ea9ef30":"import numpy as np\n#train=train[(train.age > 14) & (train.age < 110)]# modele sadece 14 ya\u015f\u0131ndan b\u00fcy\u00fck ve 110 ya\u015f\u0131ndan k\u00fc\u00e7\u00fckleri dahil edebilirim.\n#test=test[(test.age > 14) & (test.age < 110)]\ntrain[(train.age < 14) & (train.age > 110)]=np.nan\ntest[(test.age < 14) & (test.age > 110)]=np.nan\ntrain['age'].fillna(train['age'].mean(), inplace=True)\ntest['age'].fillna(train['age'].mean(), inplace=True)","faee1a44":"#Converting below columns as categories for plotting in graphs\ncategorical_features = [\n    'affiliate_channel',\n    'affiliate_provider',\n    'first_affiliate_tracked',\n    'first_browser',\n    'first_device_type',\n    'gender',\n    'language',\n    'signup_app',\n    'signup_method',\n    'signup_flow'\n]\n\nfor categorical_feature in categorical_features:\n    train[categorical_feature] = train[categorical_feature].astype('category')\nfor categorical_feature in categorical_features:\n    test[categorical_feature] = test[categorical_feature].astype('category')    ","e8ccbd2b":"train['date_account_created'] = pd.to_datetime(train['date_account_created'])\ntrain['date_first_booking'] = pd.to_datetime(train['date_first_booking'])\ntrain['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active'], format='%Y%m%d%H%M%S')\ntrain['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active']).dt.date\ntest['date_account_created'] = pd.to_datetime(test['date_account_created'])\ntest['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active'], format='%Y%m%d%H%M%S')\ntest['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active']).dt.date\n","fc9248d5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Use seaborn style defaults and set the default figure size\n\ndf = train.groupby(['date_first_booking'])['country_destination'].count().reset_index()\ndf.dropna(axis=0,inplace=True)\nimport plotly.express as px\n\nfig = px.line(df, x='date_first_booking', y=\"country_destination\")\nfig.show()\n","0db11c35":"train.drop(['date_first_booking'], axis=1,inplace=True)\ntest.drop(['date_first_booking'], axis=1,inplace=True)","90461950":"#date_account_created\n\ndac = np.vstack(train.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntrain['dac_year'] = dac[:,0]\ntrain['dac_month'] = dac[:,1]\ntrain['dac_day'] = dac[:,2]\ntrain.drop(['date_account_created'], axis=1,inplace=True)\n#timestamp_first_active\ntfa = np.vstack(train.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntrain['tfa_year'] = tfa[:,0]\ntrain['tfa_month'] = tfa[:,1]\ntrain['tfa_day'] = tfa[:,2]\ntrain.drop(['timestamp_first_active'], axis=1,inplace=True)\n#date_account_created\n\ndac = np.vstack(test.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntest['dac_year'] = dac[:,0]\ntest['dac_month'] = dac[:,1]\ntest['dac_day'] = dac[:,2]\ntest.drop(['date_account_created'], axis=1,inplace=True)\n#timestamp_first_active\ntfa = np.vstack(test.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values)\ntest['tfa_year'] = tfa[:,0]\ntest['tfa_month'] = tfa[:,1]\ntest['tfa_day'] = tfa[:,2]\ntest.drop(['timestamp_first_active'], axis=1,inplace=True)","9c0d8ef0":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder() \n  \ntrain['gender']= le.fit_transform(train['gender'])\ntrain['signup_method']= le.fit_transform(train['signup_method']) \ntrain['first_affiliate_tracked']= le.fit_transform(train['first_affiliate_tracked']) \ntrain['signup_method']= le.fit_transform(train['signup_method']) \ntrain['language']= le.fit_transform(train['language'])\ntrain['affiliate_channel']= le.fit_transform(train['affiliate_channel'])\ntrain['affiliate_provider']= le.fit_transform(train['affiliate_provider'])\ntrain['signup_app']= le.fit_transform(train['signup_app'])\ntrain['first_device_type']= le.fit_transform(train['first_device_type'])\ntrain['first_browser']= le.fit_transform(train['first_browser'])\ntrain['signup_flow']= le.fit_transform(train['signup_flow'])","8521aa91":"le = LabelEncoder() \n  \ntest['gender']= le.fit_transform(test['gender'])\ntest['signup_method']= le.fit_transform(test['signup_method']) \ntest['first_affiliate_tracked']= le.fit_transform(test['first_affiliate_tracked']) \ntest['signup_method']= le.fit_transform(test['signup_method']) \ntest['language']= le.fit_transform(test['language'])\ntest['affiliate_channel']= le.fit_transform(test['affiliate_channel'])\ntest['affiliate_provider']= le.fit_transform(test['affiliate_provider'])\ntest['signup_app']= le.fit_transform(test['signup_app'])\ntest['first_device_type']= le.fit_transform(test['first_device_type'])\ntest['first_browser']= le.fit_transform(test['first_browser'])\ntest['signup_flow']= le.fit_transform(test['signup_flow'])","73236fd9":"train.country_destination.replace('NDF',0,inplace=True)\ntrain.country_destination.replace('US',1,inplace=True)\ntrain.country_destination.replace('other',2,inplace=True)\ntrain.country_destination.replace('FR',3,inplace=True)\ntrain.country_destination.replace('CA',4,inplace=True)\ntrain.country_destination.replace('GB',5,inplace=True)\ntrain.country_destination.replace('ES',6,inplace=True)\ntrain.country_destination.replace('IT',7,inplace=True)\ntrain.country_destination.replace('PT',8,inplace=True)\ntrain.country_destination.replace('NL',9,inplace=True)\ntrain.country_destination.replace('DE',10,inplace=True)\ntrain.country_destination.replace('AU',11,inplace=True)","164dfc49":"from sklearn.model_selection import train_test_split\ny=train['country_destination']\nX=train.drop(['country_destination','id'],axis=1)\nfrom imblearn.combine import SMOTETomek\n# transform the dataset\nsmotetomek = SMOTETomek(sampling_strategy='auto')\n\n# split the dataset into train and test sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.30, random_state=1,shuffle=True,stratify=y)\nX_train, y_train = smotetomek.fit_resample(X_train1, y_train1)","6b2b9c1e":"x=pd.DataFrame(X_train)\nY=pd.DataFrame(y_train)\nresult = pd.concat([x, Y], axis=1, join='inner')\nsns.countplot(result.country_destination)","1b8cc929":"target_names = ['NDF', 'US', 'other', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU']\n#Classifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\nrf=RandomForestClassifier()\n\nrf.fit(X_train,y_train)\n\ny_predrf=rf.predict(X_test1)\nprint(classification_report(y_test1, y_predrf, target_names=target_names))","ea0462b9":"feature_names=list(X_train1.columns.values.tolist()) ","aedd644c":"import pandas as pd\nfeature_imp = pd.Series(rf.feature_importances_,index=feature_names).sort_values(ascending=False)\nfeature_imp\n","b6fd9ec9":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","70880279":"pred_country={0:\"NDF\", 1:\"US\", 2:\"other\", 3:\"FR\", 4:\"CA\", 5:\"GB\", 6:\"ES\", 7:\"IT\", 8:\"PT\", 9:\"DE\", 10:\"NL\", 11:\"AU\"}","40696858":"from xgboost.sklearn import XGBClassifier\nxgb = XGBClassifier()                  \n\nxgb.fit(X_train,y_train)\n\n\ny_predxgb=xgb.predict(X_test1)\n\nprint(classification_report(y_test1, y_predxgb, target_names=target_names))\n","08e53e8c":"predictionsxgb=xgb.predict(test.drop(['id'],axis=1))","98e4f942":"resultsxgb=[]\nfor i in predictionsxgb:\n    resultsxgb.append(pred_country[i])","eaca3b99":"#my_submissionxgb = pd.DataFrame({'id': test.id, 'country':resultsxgb})\n#my_submissionxgb.to_csv('submissionxgb.csv', index=False)","d75fb274":"from sklearn.neural_network import MLPClassifier\n#Generate prediction using Neural Net\n\nmlp = MLPClassifier(activation='identity', solver='sgd',learning_rate='adaptive', alpha=0.0001, batch_size='auto')\nmlp.fit(X_train,y_train)\npredsmlp = mlp.predict(X_test1)\nfrom sklearn import metrics\nprint(classification_report(y_test1, predsmlp, target_names=target_names))","176dbba7":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'solver': ['lbfgs','sgd'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\nmlpgridsearch = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\nmlpgridsearch.fit(X_train,y_train)\npredsgridmlp = mlpgridsearch.predict(X_test1)\nfrom sklearn import metrics\nprint(classification_report(y_test1, predsgridmlp, target_names=target_names))","c33b4b91":"from sklearn.naive_bayes import ComplementNB cnb = ComplementNB() cnb.fit(X_train, y_train) y_predcnb=cnb.predict(X_test1)\n\nfrom sklearn import metrics\n\nprint(classification_report(y_test1, y_predcnb, target_names=target_names))","9bab255e":"import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder from xgboost.sklearn import XGBClassifier\n\n#Loading the Data again train= pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/train_users_2.csv.zip') test = pd.read_csv('..\/input\/airbnb-recruiting-new-user-bookings\/test_users.csv.zip') train['first_affiliate_tracked'] = train['first_affiliate_tracked'].fillna('Unknown') test['first_affiliate_tracked'] = test['first_affiliate_tracked'].fillna('Unknown') train['date_account_created'] = pd.to_datetime(train['date_account_created']) train['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active'], format='%Y%m%d%H%M%S') train['timestamp_first_active'] = pd.to_datetime(train['timestamp_first_active']).dt.date test['date_account_created'] = pd.to_datetime(test['date_account_created']) test['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active'], format='%Y%m%d%H%M%S') test['timestamp_first_active'] = pd.to_datetime(test['timestamp_first_active']).dt.date train.drop(['date_first_booking'], axis=1,inplace=True) test.drop(['date_first_booking'], axis=1,inplace=True)\n\n#date_account_created\n\ndac = np.vstack(train.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) train['dac_year'] = dac[:,0] train['dac_month'] = dac[:,1] train['dac_day'] = dac[:,2] train.drop(['date_account_created'], axis=1,inplace=True)\n\n#timestamp_first_active tfa = np.vstack(train.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) train['tfa_year'] = tfa[:,0] train['tfa_month'] = tfa[:,1] train['tfa_day'] = tfa[:,2] train.drop(['timestamp_first_active'], axis=1,inplace=True)\n\n#date_account_created\n\ndac = np.vstack(test.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) test['dac_year'] = dac[:,0] test['dac_month'] = dac[:,1] test['dac_day'] = dac[:,2] test.drop(['date_account_created'], axis=1,inplace=True)\n\n#timestamp_first_active tfa = np.vstack(test.timestamp_first_active.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) test['tfa_year'] = tfa[:,0] test['tfa_month'] = tfa[:,1] test['tfa_day'] = tfa[:,2] test.drop(['timestamp_first_active'], axis=1,inplace=True) import numpy as np train[(train.age < 14) & (train.age > 110)]=np.nan train['age'].fillna(train['age'].mean(), inplace=True)\n\ntest[(test.age < 14) & (test.age > 110)]=np.nan test['age'].fillna(train['age'].mean(), inplace=True)\n\n#Converting below columns as categories for plotting in graphs categorical_features = [ 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'first_browser', 'first_device_type', 'gender', 'language', 'signup_app', 'signup_method', 'signup_flow' ]\n\nfor categorical_feature in categorical_features: train[categorical_feature] = train[categorical_feature].astype('category') for categorical_feature in categorical_features: test[categorical_feature] = test[categorical_feature].astype('category')\n\n#One-hot-encoding features ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] for f in ohe_feats: train_dummy = pd.get_dummies(train[f], prefix=f) train_cont= train.drop([f], axis=1) train = pd.concat((train_cont, train_dummy), axis=1)\n\n#One-hot-encoding features ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] for f in ohe_feats: test_dummy = pd.get_dummies(test[f], prefix=f) test_cont= test.drop([f], axis=1) test = pd.concat((test_cont, test_dummy), axis=1)\n\n#Splitting train and test\n\nfrom sklearn.model_selection import train_test_split y=train['country_destination'] X=train.drop(['country_destination','id'],axis=1) from imblearn.combine import SMOTETomek\n\ntransform the dataset\nsmotetomek = SMOTETomek(sampling_strategy='auto')\n\nsplit the dataset into train and test sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.30, random_state=1,shuffle=True) X_train, y_train = smotetomek.fit_resample(X_train1, y_train1)\n\n#Classifier\n\nxgb = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints=None, learning_rate=0.3, max_delta_step=0, max_depth=6, min_child_weight=1, monotone_constraints=None, n_estimators=25, n_jobs=0, num_parallel_tree=1, objective='multi:softprob', random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None, seed=0, subsample=0.5, tree_method=None, validate_parameters=False, verbosity=None) xgb.fit(X_train, y_train) target_names = ['NDF', 'US', 'other', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU'] y_predxgb=xgb.predict(X_test1)\n\nfrom sklearn.metrics import classification_report print(classification_report(y_test1, y_predxgb, target_names=target_names))","236fc3a4":"Tablo de\u011ferleri random forest  ve mlp ile k\u0131yaslad\u0131\u011f\u0131mda baya k\u00f6t\u00fc. Ancak t\u00fcm s\u0131n\u0131flara (destinationlara) yap\u0131lacak ilk rezervasyonlar\u0131 mlp'den daha iyi tahmin etti. Random Forest \u0131 ge\u00e7emedi\u011fi i\u00e7in bu algoritmay\u0131 eledim.","06c10cc6":"Rezervasyonlar \u00fczerinde zaman etkisi g\u00f6zard\u0131 edilemez bu nedenle tarih bilgisi bulunduran verileri s\u0131n\u0131fland\u0131rma algoritmas\u0131na dahil etmek modellerimi iyile\u015ftirebilir.","2f38e2b3":"S\u0131n\u0131flar e\u015fit da\u011f\u0131lmam\u0131\u015f. Az\u0131nl\u0131k s\u0131n\u0131flar\u0131n k\u00f6t\u00fc tahmin edilmesine sebep olacak bir problem. 'resampling' teknikleri deneyerek bir denge sa\u011flayabilece\u011fimi d\u00fc\u015f\u00fcn\u00fcyorum.","944e1487":"train ve test i yenilemek istedi\u011fimde notebookun ba\u015f\u0131na gitmemek i\u00e7in buraya ta\u015f\u0131d\u0131m bu kodlar\u0131.","570cf0cf":"Her y\u0131l temmuz ve agustos aylar\u0131nda ilk rezervasyon say\u0131lar\u0131 artm\u0131\u015f. 2011'den 2014' e kadar genel bir art\u0131\u015f var. 2104 y\u0131l\u0131n\u0131n temmuz ay\u0131ndan may\u0131s 2015' e kadar azal\u0131\u015fa ge\u00e7mi\u015f. 2015'in temmuz ve agustos aylar\u0131na dair bir bilgi mevcut de\u011fil.","351448e3":"**** 0.69 submission score (smoote oversampling kulland\u0131\u011f\u0131mda).\n    Farkl\u0131 resampling algoritmalar\u0131n\u0131 deneyerek modelimi en iyi haline getirmeye \u00e7al\u0131\u015fasag\u0131m.\nSmootetomek fonsiyonu scorumu biraz d\u00fc\u015f\u00fcrsede classlar\u0131m\u0131 \u00e7ok g\u00fczel dengeledi. Bu nedenle modelim i\u00e7in SMOTE'dan daha iyi bir se\u00e7enek oldu\u011funu d\u00fc\u015f\u00fcn\u00fcyorum.","80ea1a37":"SMOTETomek- resampling yaparak s\u0131n\u0131flar\u0131 dengeliyorum. Train-test ayr\u0131m\u0131n\u0131 'stratify=y' ile yapt\u0131\u011f\u0131mda  'Stratified Samplimg' di\u011fer ad\u0131yla tabakal\u0131 \u00f6rnekleme yaparak country_destination de\u011fi\u015fkenini temsil eden her bir alt s\u0131n\u0131f\u0131 train ve test verilerine dahik etti\u011fime ein oluyorum.","20f15389":"* Arama Motoru Optimizasyonu (SEO), Web sitelerinin arama motorlar\u0131nda daha iyi performans g\u00f6stermesi i\u00e7in yap\u0131lan \u00e7al\u0131\u015fmalar\u0131n t\u00fcm\u00fcne verilen isimdir.\n\n* Remarketing en yal\u0131n tan\u0131m\u0131 ile, sitenizi ziyaret eden ki\u015filerin etiketlenerek, sitenizden ayr\u0131ld\u0131ktan sonra ziyaret ettikleri di\u011fer sitelerde tekrar reklam\u0131n\u0131z\u0131 g\u00f6rmelerini sa\u011flamakt\u0131r.","4c24d7d5":"60 ya\u015f\u0131ndan b\u00fcy\u00fckler ya\u015flar\u0131 ilerledik\u00e7e daha sakin lokasyonlar\u0131 tercih etmi\u015f. Ama genel olarak ya\u015f, gidilen \u00fclke ve populasyon b\u00fcy\u00fckl\u00fc\u011f\u00fc aras\u0131ndado\u011frudan bir ili\u015fki yok gibi g\u00f6r\u00fcn\u00fcyor. Ama y\u0131l bilgisi sadece 2015 y\u0131l\u0131na ait. Chi squre test katagorik de\u011fi\u015fkenlerimiz aras\u0131nda bir ili\u015fki olup olmad\u0131\u011f\u0131n\u0131 test etmemizi sa\u011flayacakt\u0131r.","a52bf617":"14 ya\u015f\u0131ndan k\u00fc\u00e7\u00fcklerin aileleriyle seyehat edeceklerini varsayarak 14 ya\u015f\u0131ndan k\u00fc\u00e7\u00fckleri modele dahil etmedim. \u00d6nceki ara\u015ft\u0131rmalar\u0131mda k\u00fc\u00e7\u00fck ya\u015ftaki yolcular\u0131n tercihleriyle b\u00fcy\u00fck ya\u015ftaki yolcular\u0131n tercihleri aras\u0131nda anlaml\u0131 bir fark olmad\u0131\u011f\u0131n\u0131 g\u00f6rd\u00fcm. Ayr\u0131ca 110 ya\u015f\u0131ndan b\u00fcy\u00fck insanlar\u0131n ya\u015flar\u0131n\u0131 inceledi\u011fimde anlams\u0131z ya\u015flarda(2014 gibi) insanlar g\u00f6rd\u00fcm. O nedenle sadece 110 ya\u015f\u0131ndan k\u00fc\u00e7\u00fck ve 14 ya\u015f\u0131ndan b\u00fcy\u00fck insanlar i\u00e7in s\u0131n\u0131fland\u0131rma yapmaya karar verdim.","d46e051a":"Device type ve action aras\u0131nda bir ili\u015fki var gibi g\u00f6r\u00fcn\u00fcyor. % 95 g\u00fcven aral\u0131\u011f\u0131yla kullan\u0131c\u0131lar\u0131n hareketleriyle kulland\u0131klar\u0131 alet aras\u0131nda anlaml\u0131 bir ili\u015fki vard\u0131r diyebiliriz.","eea68be4":"# TRAIN DATA EDA","fe0b04dd":"Mean imputation yerine ba\u015fka y\u00f6ntemler de denenebilirdi. Ama age de\u011fi\u015fkeni mean imputaion i\u00e7in uygun bir de\u011fi\u015fkendi. Ayr\u0131ca pratik bir y\u00f6ntem oldu\u011funu d\u00fc\u015f\u00fcnd\u00fcm. Modeli iyile\u015ftirip iyile\u015ftirmeyece\u011fini g\u00f6rmek i\u00e7in ba\u015fka imputation y\u00f6ntemleri de denenebilir. 14 ya\u015f\u0131ndan k\u00fc\u00e7\u00fck \u00e7ocuklar\u0131 dataya dahil ederek \u00e7ocuklu ebebynlerin seyehat tercihleri hakk\u0131nda bir fikir sahibi olunabilirdi belki ama age_gender datas\u0131na bakt\u0131g\u0131mda ya\u015f guruplar\u0131 ve country destinationlar\u0131 aras\u0131nda bir ili\u015fki g\u00f6zlemleyememi\u015ftim. 14 ya\u015f\u0131ndan k\u00fc\u00e7\u00fckleri modele dahil etmek modelim \u00fczerinde b\u00fcy\u00fck farklaryaratmayacakt\u0131r. Ek olarak, 110 ya\u015f\u0131ndan b\u00fcy\u00fck ya\u015flar\u0131 inceledi\u011fimde 2014, 1929 gibi ya\u015flar g\u00f6zlemledim. Bunlar missing gibi g\u00f6r\u00fcnmesede asl\u0131nda kay\u0131p bilgiler. 800 k\u00fc\u015f\u00fcr ki\u015fiye ait bilgileri modelden atmaktansa onlar\u0131 da missing olarak kabul edip imputation uygulamak modellerimi iyile\u015ftirebilir. Sonraki a\u015famalarda modelimi ileriye ta\u015f\u0131mak ad\u0131na deneyece\u011fim bir y\u00f6ntem olacakt\u0131r.","220895f7":"signup_flow de\u011fi\u015fkeni say\u0131sal bir de\u011fermi\u015f gibi dursa da \u00f6nceki notebooklar\u0131mdan birinde grafiksel olarak bu de\u011fi\u015fkene kategorik demenin daha do\u011fru olaca\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnm\u00fc\u015ft\u00fcm. Yar\u0131\u015fmaya dair a\u00e7\u0131klamalar\u0131n yazd\u0131\u011f\u0131 sayfada bunun siteye kay\u0131t olunmadan \u00f6nceki sayfa olduguna dair bir a\u00e7\u0131klama vard\u0131. Bu iki durumu g\u00f6z \u00f6n\u00fcnde bulundurunca signup_flow de\u011fi\u015fkenine kategorik gibi davranmam gerekti\u011fine karar verdim.","e7228a63":"S\u0131n\u0131fland\u0131rma algoritmalar\u0131 kullanaca\u011f\u0131m i\u00e7in one-hot encoding yerine label encoding kullanmay\u0131 tercih ettim. E\u011fer bu veri regresyona uygun olsayd\u0131 one-hot encoding uygulard\u0131m. \u00c7\u00fcnk\u00fc label encoding yapt\u0131g\u0131mda bir de\u011fi\u015fkene ait s\u0131n\u0131f \u00e7ok fazla oldu\u011funda bir s\u0131n\u0131f\u0131n di\u011ferinden daha \u00f6nemli oldu\u011funu varsayabiliyor modelimiz. Say\u0131sal bir b\u00fcy\u00fckl\u00fck olarak alg\u0131l\u0131yor s\u0131n\u0131flara verilen say\u0131lar\u0131. Bu da modellerimizin do\u011frulu\u011funu olumsuz etkileyebilir. Ancak s\u0131n\u0131fland\u0131rma algoritmalar\u0131nda b\u00f6yle bir sorun s\u00f6z konusu de\u011fil. One-hot-encoding uygulamak da yaln\u0131\u015f olmazd\u0131 ancak \u015fimdilik buna gerek oldu\u011funu d\u00fc\u015f\u00fcnm\u00fcyorum.","aaea7707":"#Eklemek istediklerim:","b4e406ca":"Sadece \u0130talya destination\u0131n\u0131n presicion de\u011feri y\u00fcksek \u00e7\u0131kt\u0131. Farkl\u0131 parametler ve one hot encoding kullanarak yeniden XGboost denedi\u011fimde accuracy 0.2 artt\u0131 ama bu model pek do\u011fru s\u0131n\u0131fland\u0131rma yapmad\u0131.","2d7ab36f":"\u0130nsanlar \u00f6ncelikli olarak Amerika i\u00e7inde seyehat etmi\u015fler. En \u00e7ok seyehat edilen(ad\u0131n\u0131 bildi\u011fimiz) ikinci \u00fclke Fransa, \u00fc\u00e7\u00fcnc\u00fc \u00fclke ise \u0130talya. Yani dil ve uzakl\u0131k tercih edilme a\u00e7\u0131s\u0131ndan bir kriterdir diyemeyiz. Language_levenshtein_distance ile country destination aras\u0131ndaki ili\u015fkiyi g\u00f6zlemleyebilece\u011fimiz bir plot \u00e7izerek bu hipotez kontrol edilebilir. Ama yorumlar\u0131mda bir de\u011fi\u015fiklik olmayacakt\u0131r.","852d26d6":"Australya(AU),\u0130ngiltere(GB),Kanada(CA) ve Amerika(US) ingilizce konu\u015fuyor. \u0130nsanlar dillerinin konu\u015fuldu\u011fu yerleri \u00f6ncelikli olarak tercih etmi\u015f olabilir mi? Yoksa Amerikaya veya Amerikaya yak\u0131n yerlerin tercih edilmesi daha olas\u0131 m\u0131?","7ca9a569":"Rezervasyon yapan insanlar\u0131n sitede ge\u00e7irdikleri toplam s\u00fcre i\u00e7in diger insanlar\u0131n ge\u00e7irdikleri s\u00fcrenin %75 inden fazla zaman ge\u00e7irdikleri s\u00f6ylenebilir. S\u00fcre bak\u0131m\u0131ndan \u00fcst \u00e7eyreklikte bulunuyorlar.","01fb3a6f":"\u00d6nceki denemelerimde timestamp_first_active ve date_account_created de\u011fi\u015fkenlerini \u00fczerlerinde hi\u00e7bir de\u011fi\u015fiklik yapmadan silmi\u015ftim. Ama bu iki de\u011fi\u015fkeni b\u00f6l\u00fcp g\u00fcn,ay,y\u0131l \u015feklinde yeni de\u011fi\u015fkenler \u00fcretmek submission scorumu neredeyse hi\u00e7 de\u011fi\u015ftirmedi. En y\u00fcksek scorelar\u0131m\u0131 sadece date_first_booking de\u011fi\u015fkenini kulland\u0131\u011f\u0131mda elde etmi\u015ftim. Bu de\u011fi\u015fkeni g\u00fcn,ay,y\u0131l olacak \u015fekilde 3 e par\u00e7alay\u0131p veri setine eklemi\u015ftin ve ard\u0131ndan kategorik de\u011fi\u015fkenlerime one-hot-endoding uygulam\u0131\u015ft\u0131m, scorelar\u0131m \u00e7ok anlaml\u0131 bir \u015fekilde de\u011fi\u015fmi\u015fti ancak 'Test' verisinde date_first_booking de\u011fi\u015fkeni tamamen bo\u015ftu ayr\u0131ca az\u0131nl\u0131kta olan s\u0131n\u0131flar\u0131n tahmin edilebilirli\u011fi olduk\u00e7a d\u00fc\u015f\u00fckt\u00fc hatta baz\u0131 s\u0131n\u0131flar hi\u00e7 tahmin edilemiyordu. Bu nedenle yar\u0131\u015fmaya herhangi bir y\u00fckleme yapamad\u0131m. Yeni kullan\u0131c\u0131lar\u0131n ilk rezervasyon lokasyonlar\u0131 zamana ba\u011fl\u0131 de\u011fi\u015febilen bir \u015fey. Belki[ bu dataya uygun bir time series ](https:\/\/otexts.com\/fpp2\/hierarchical.html)algoritmas\u0131 bulunabilir ve dataya o \u015fekilde yakla\u015f\u0131labilir.","45398301":"Mlp accuracy olarak XGB den daha y\u00fcksek bir accuray ye sahip olsa da az\u0131nl\u0131k s\u0131n\u0131flar\u0131 tahminlerken \u00e7uvallad\u0131. Rezervasyon yapmayacak kullan\u0131c\u0131lar\u0131 iyi tahmin etti ancak yapanlar\u0131n nereye gidece\u011fini hi\u00e7 tahmin edemedi. Submission scorumsa 0.67'ye y\u00fckseldi. Baz\u0131 parametreleri de\u011fi\u015ftirmenin modelimin \u00fczerinde nas\u0131l bir etki b\u0131rakaca\u011f\u0131n\u0131 g\u00f6zlemlemek istiyorum.","ddb63e4d":"Box plot kodunun \u00fczerinden kendi olu\u015fturdugum ya\u015f gruplar\u0131 i\u00e7in ayr\u0131 ayr\u0131 box plotlar \u00e7izdirdim. Ancak ya\u015f ve gidilecek yerin nufusu aras\u0131nda bir ili\u015fki g\u00f6zlemleyemedim. Ya\u015fl\u0131lar daha sakin yerleri, gen\u00e7ler daha kalabal\u0131k yerleri tercih edebilir gibi bir durum g\u00f6zlemlerim diye bir varsay\u0131mda bulunmu\u015ftum. ","309a7822":"age_gender ve countries datalar\u0131n\u0131 inceledi\u011fimde train datas\u0131yla birle\u015ftirebilece\u011fim bir de\u011fi\u015fken g\u00f6zlemleyemedim. \u0130nsanlar\u0131n tatillerini ge\u00e7irebilece\u011fi yer se\u00e7imlerinde gidilecek \u00fclkenin uzakl\u0131\u011f\u0131, o \u00fclkede konu\u015fulan dilin ingilizce veya ingilizceye yak\u0131n bir dil ailesine sahip olu\u015fu,cinsiyet, ki\u015finin ya\u015f\u0131 ve ya\u015fa ba\u011fl\u0131 olarak gidilen yerin nufus yogunlu\u011fu aras\u0131nda anlaml\u0131 bir ili\u015fki g\u00f6zlemleyemedim. Ama tatile gidilecek d\u00f6neme ba\u011fl\u0131 olarak insanlar \u015fehir d\u0131\u015f\u0131na \u00e7\u0131kmay\u0131 veya Amerika i\u00e7inde gezmeyi tercih etmi\u015f olabilirler.","5973a306":"1. MISSINGNESS","d93c2312":"Train datas\u0131yla birle\u015ftirebilece\u011fimherhengi bir \u00f6zellik g\u00f6zlemleyemedim. Ancak modellerimin do\u011frulugunu artt\u0131rmak i\u00e7in train datas\u0131n\u0131n kolonlar\u0131yla oynay\u0131p datay\u0131 yeniden \u015fekillendirebilece\u011fimi d\u00fc\u015f\u00fcnd\u00fcm.","370e8861":"Sadece 3 ki\u015finin hareketleri booking ile sonu\u00e7lanm\u0131\u015f. 3 \u00fc de windows kullan\u0131c\u0131s\u0131 olarak g\u00f6r\u00fcn\u00fcyor ama bence eylemlerin rezervasyon ile sonu\u00e7lanmas\u0131 detayl\u0131 ara\u015ft\u0131rma gerektirece\u011fi i\u00e7in bilgisayardan islem yap\u0131l\u0131yor olmas\u0131 daha olas\u0131. Daha sonra device type \u0131 bilgisayar olup olmamas\u0131na g\u00f6re yeniden isimlendirebilirim. train datas\u0131na sitede ge\u00e7irilen toplam s\u00fcre bilgisi ekleyip bu veriyi scale edebilirim. Bu iki y\u00f6ntemin modelimi iyile\u015ftirip iyile\u015ftirmedi\u011fine bakabilirim.","5c4ac6f9":"> Using Stratified Sampling technique ensures that there will be selection from each sub-groups and prevents the chance of omitting one sub-group leading to sampling bias","e319edf5":"Resampling yapmak s\u0131n\u0131flar\u0131 daha e\u015fit da\u011f\u0131tt\u0131. ","c0ad8f69":"# LABEL ENCODING","6d1a7aeb":"Random Forest a k\u0131yasla daha iyi bir classification raporum oldu\u011funu s\u00f6yleyebilirim. Resampling uygulad\u0131ktan ve XGBoost yapt\u0131ktan sonra submission scorum 0.59 a d\u00fc\u015ft\u00fc. Ancak Ba\u015flang\u0131\u00e7ta country_destination de\u011fi\u015fkenin i\u00e7indeki veriler e\u015fit \u00f6rne\u011fe sahip de\u011fildi. Bu nedenle veride en s\u0131k bulunan iki s\u0131n\u0131f\u0131 \u00e7ok iyi tahmin ederken di\u011fer s\u0131n\u0131flar\u0131 neredeyse hi\u00e7 tahmin etmiyordu modelim. Bu problemi \u00e7\u00f6zmek i\u00e7in 'resampling' y\u00f6ntemlerini ara\u015ft\u0131rd\u0131m. Undersampling y\u00f6ntemlerini deneyecek kadar b\u00fcy\u00fck bir veri setine sahip olmad\u0131\u011f\u0131m i\u00e7in oversampling y\u00f6ntemlerine y\u00f6neldim. Ancak bu y\u00f6ntemleri denedikten sonra bile 'balanced' bir y de\u011fi\u015fkenine sahip olamad\u0131\u011f\u0131m\u0131 g\u00f6zlemledim. Sonra over ve under sampling y\u00f6ntemlerinin kombinasyonu olarak tan\u0131mlanabilecek y\u00f6ntemlere y\u00f6neldim. S\u0131n\u0131flar\u0131m\u0131 en iyi dengeleyen y\u00f6ntem [SMOTEomek](http:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html#module-imblearn.combine) oldu. ","41413d46":"# MODELLING","e3ebecc3":"Sitede ger\u00e7ekle\u015ftirilen ilk eylemin zaman\u0131n\u0131n ve siteye ilk kay\u0131t olunan tarihin zamanlar\u0131n\u0131n g\u00fcn, ay, y\u0131l bilgilerini kullanarak yeni de\u011fi\u015fkenler \u00fcretmek modelimi iyile\u015ftirse de \u00e7ok b\u00fcy\u00fck farklar yaratmad\u0131. S\u00fcrekli yeni y\u00f6ntemler denedi\u011fim i\u00e7in bu bilgilerin modelden at\u0131lm\u0131\u015f halini sizlerle payla\u015fmad\u0131m. Ayr\u0131ca date_first_booking de\u011fi\u015fkenini veriden att\u0131m. \u00c7\u00fcnk\u00fc zaten ilk booking yap\u0131lan zaman test datas\u0131nda yok. Asl\u0131nda country_destination'\u0131 tahmin etmeden \u00f6nce date_first_booking' i de tahmin etmek gerekiyor olabilir. \u00c7\u00fcnk\u00fc yeni bir m\u00fc\u015fterinin ne zaman gelece\u011fi, geldi\u011finde gitmek isteyebilece\u011fi ilk yeri etkileyen bir fakt\u00f6r. Yani burada asl\u0131nda zamana ba\u011fl\u0131\/d\u00f6nemsel bir tak\u0131m etkiler de s\u00f6z konusu. Ayn\u0131 anda birden fazla de\u011fi\u015fkeni tahminleyen y\u00f6ntem ya da \u00f6nce yeni m\u00fc\u015fterilerin gelecekleri tarihleri tahminleyip sonra 'gelseler, nereye giderlerdi?' sorusunu sormak mant\u0131kl\u0131 olabilir belki.","e80eeba1":"# Computation for the Booking Destination"}}