{"cell_type":{"bccdf72d":"code","61550cb7":"code","f6a07c83":"code","03228e80":"code","844a99af":"code","7a5b74d5":"code","9f0e5f5e":"code","a1727dac":"code","e6446069":"code","3e9c019e":"code","f416fb0b":"code","e3364103":"code","a2f99774":"code","c10b3068":"code","7ee093a6":"code","844abb72":"code","9c40256b":"code","4757f5f9":"code","3530d590":"code","ed57df7e":"code","b2cb4051":"code","cb301b44":"code","33eef153":"code","292724ad":"code","50364cb8":"code","4351b2b6":"code","06f3f9a2":"code","6a586198":"code","afdfc7b2":"code","288676fa":"code","eb3771a5":"code","632e9e3d":"code","8cec48f8":"code","c9b1f9bc":"code","12cba460":"code","9e4a14df":"code","33f5afa8":"code","75f39c26":"code","844b78b5":"code","ea8dbb27":"code","6041e53a":"code","50d488ed":"code","426d8bf8":"markdown","3a16c358":"markdown","d83c0c7c":"markdown","8cee7433":"markdown","791a43d9":"markdown","637e1b13":"markdown","f411b0da":"markdown","5e0c463f":"markdown","f3c83de4":"markdown","31abace4":"markdown","50e89be4":"markdown","0bcd6db0":"markdown","6857f3cb":"markdown","0cd0d8c9":"markdown","20363017":"markdown","309f6c74":"markdown","8b176357":"markdown","20eced36":"markdown"},"source":{"bccdf72d":"#importing the requird liberaries for dataframe and the data visiulazition\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nfrom IPython.display import Image\n\n#importing ML libraries for clustering\n\nimport sklearn\nimport scipy\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","61550cb7":"#reading the input data\ndf = pd.read_csv('..\/input\/onlineretail\/OnlineRetail.csv',sep = \",\",encoding=\"ISO-8859-1\",header = 0)\ndf.head()","f6a07c83":"#describe the dataframe.....so here we can seee the df(dataframe) has 8 rows and 541909 rows (entries)\n# And will get the full information about the dataframe, like:- count of values in all the columns, their data type\ndf.info()","03228e80":"#describing the dataframe\n#with the help of this describe() function will get breakdown of the dataset.\n#here we can see that the Quantity is showing -80995(min) and the max(80995) so it seems there is something wrong with the dataframe so we have\n##perform data cleansing to clean the data.\ndf.describe()","844a99af":"#as we saw above that we have null values in Description and customer id columns in Describe() function.\n#now we are going to see the percentage of null values in our dataset\n\ndf_null = round(100*(df.isnull().sum())\/len(df),2)\ndf_null","7a5b74d5":"#droping rows that has the missing values because we can not replace the null in Customer ID with any value.\ndf = df.dropna()\ndf.shape","9f0e5f5e":"#now we can see that there is no null values in our data\ndf.info()","a1727dac":"#we have to change the data type of the customerid as per Business Understanding\ndf['CustomerID'] = df['CustomerID'].astype(str)","e6446069":"#we have unit price and the qty in our data so we are going to create anothere variable ammount\ndf['Amount'] = df['UnitPrice']*df['Quantity']\n#now grouping the data based on customer id\nrfm_m = df.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","3e9c019e":"#now we are going to count the InvoiceNo variable to create another variable FREQUENCY \nrfm_f= df.groupby('CustomerID')['InvoiceNo'].count()\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","f416fb0b":"#now we are going to merge these to df what we have just created\nrfm = pd.merge(rfm_m,rfm_f,on='CustomerID', how='inner')\nrfm.head()","e3364103":"#to calculate the receancy we need to calculate the latest date in our dataset\n#and before that we need to convert the date in appropriate  format.\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%d-%m-%Y %H:%M')\nl_date = max(df['InvoiceDate'])\nprint(l_date)","a2f99774":"#now we are going to check the difference between the max date and the invoice date.\ndf['Difference'] = l_date - df['InvoiceDate']\ndf.head()","c10b3068":"#now we are calculating the minimum date gap per cuctomer to get the recency.\nrfm_r = df.groupby('CustomerID')['Difference'].min()\nrfm_r =rfm_r.reset_index()\nrfm_r.head()","7ee093a6":"#now we are going to extract number of days only from the difference variable\n#run this code block once otherwise it will throw error because the data will get change once you will run it.\nrfm_r['Difference'] = rfm_r['Difference'].dt.days\n","844abb72":"rfm_r.head()","9c40256b":"#Now we are going to merge this rfm_r to the main rfm dataset\nrfm = pd.merge(rfm,rfm_r,on='CustomerID', how='inner')\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","4757f5f9":"#Outlier analysis of Amount, Frequency & Recency Features\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","3530d590":"#treating outliers\n#as you can see there are outliers in above figure...so we need to treat them.\n# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","ed57df7e":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","b2cb4051":"#converting it to dataframe and adding colummn names\nrfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\nrfm_df_scaled.head()","cb301b44":"# k-means with some arbitrary k\n\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","33eef153":"kmeans.labels_","292724ad":"# Elbow-curve\/SSD\n\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.plot(ssd)","50364cb8":"# Silhouette Analysis\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","4351b2b6":"#so from this output we are going to buildthe model on 3 clusters\n# Final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","06f3f9a2":"kmeans.labels_","6a586198":"#adding clusters to our df\n# assign the label\nrfm['Cluster_Id'] = kmeans.labels_\nrfm.head()","afdfc7b2":"# Box plot to visualize Cluster Id vs Amount\n\nsns.boxplot(x='Cluster_Id', y='Amount', data=rfm)","288676fa":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)","eb3771a5":"# Box plot to visualize Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Id', y='Recency', data=rfm)","632e9e3d":"#single linkage\nImage(\"..\/input\/linkage\/single.png\")","8cec48f8":"# Single linkage: \n\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","c9b1f9bc":"#Complete linkage\nImage(\"..\/input\/linkage\/mplete.png\")","12cba460":"# Complete linkage\n\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","9e4a14df":"# average linkage\nImage(\"..\/input\/linkage\/average.png\")","33f5afa8":"# Average linkage\n\nmergings = linkage(rfm_df_scaled, method=\"average\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","75f39c26":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","844b78b5":"# Assign cluster labels\n\nrfm['Cluster_Labels'] = cluster_labels\nrfm.head()","ea8dbb27":"#Plot Cluster labels vs Amount\n\nsns.boxplot(x='Cluster_Labels', y='Amount', data=rfm)","6041e53a":"#Plot Cluster labels vs Frequency\n\nsns.boxplot(x='Cluster_Labels', y='Frequency', data=rfm)","50d488ed":"#Cluster labels vs Recency\nsns.boxplot(x='Cluster_Labels', y='Recency',data=rfm)","426d8bf8":"### There are 2 types of outliers and we will treat outliers as it can skew our dataset\n* Statistical\n* Domain specific","3a16c358":"## Step 5:- Final Analysis\n\n### Inference:-\n##### K-Means Clustering with 3 Cluster Ids\n\n1. Customers with Cluster Id 1 are the customers with high amount of transactions as compared to other customers.\n2. Customers with Cluster Id 1 are frequent buyers.\n3. Customers with Cluster Id 2 are not recent buyers and hence least of importance from business point of view.\n\n#### Hierarchical Clustering with 3 Cluster Labels\n\n1. Customers with Cluster_Labels 2 are the customers with high amount of transactions as compared to other customers.\n2. Customers with Cluster_Labels 2 are frequent buyers.\n3. Customers with Cluster_Labels 0 are not recent buyers and hence least of importance from business point of view.\n\n","d83c0c7c":"### Step 2:- Data Cleansing","8cee7433":"## Cutting the Dendrogram based on K clusters","791a43d9":"### Step 3:- Data Preparation","637e1b13":"### Complete Linkage\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two furthest points.","f411b0da":"### Finding the Optimal Number of Clusters\n#### Elbow Curve to get the right number of Clusters\n\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.","5e0c463f":"# ****Overview****\nOnline retail is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based registered  online retail. The company mainly sells unique gifts for all occasion. Total of 8 columns & 541910 rows.","f3c83de4":"### Average Linkage:\n\nIn average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the average length each arrow between connecting the points of one cluster to the other.","31abace4":"# Rescaling the Attributes\nIt is extremely important to rescale the variables so that they have a comparable scale.| There are two common ways of rescaling:\n\n1)Min-Max scaling\n\n2)Standardisation (mean-0, sigma-1)\n\nHere, we are going to use Standardisation Scaling.\n\n","50e89be4":" So here we are going to analysis the Customers based on below 3 factors:\n \nR (Recency): Number of days since last purchase\n\nF (Frequency): Number of tracsactions\n\nM (Monetary): Total amount of transactions","0bcd6db0":"## **The steps are broadly divided into:**\n\nStep 1: Reading and Understanding the Data\n\nStep 2: Data Cleansing\n\nStep 3: Data Preparation\n\nStep 4: Model Building\n\nStep 5: Final Analysis","6857f3cb":"# ****Business Goal****\n\nWe are going to create segment for customer with the help of RFM model so that the store can easily target the relevant customers efficiently.","0cd0d8c9":"# Hierarchical Clustering\nHierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\nOR Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering,\n\n* Divisive\n* Agglomerative.\n\n### Single Linkage:\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two closest points.\n","20363017":"## Step 4:- Building the Model\n\n### K-Means Clustering\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\n\nThe algorithm works as follows:\n\n* First we initialize k points, called means, randomly.\n* We categorize each item to its closest mean and we update the mean\u2019s coordinates, which are the averages of the items categorized in that mean so far.\n* We repeat the process for a given number of iterations and at the end, we have our clusters.","309f6c74":"### Step 1 :- Reading and understanding the data.","8b176357":"### Silhouette Analysis :-\n\nSilhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n\n\n                 silhouette score = p\u2212q \/ max(p,q)\n \np  is the mean distance to the points in the nearest cluster that the data point is not a part of\n\nq  is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1.\n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster,\n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","20eced36":"This kernel is based on the assignment by IIITB collaborated with UPGRADE.\n\nIf this Kernel helped you in any way, some UPVOTES would be very much appreciated."}}