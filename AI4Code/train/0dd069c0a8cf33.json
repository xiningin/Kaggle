{"cell_type":{"e185c299":"code","29c81988":"code","49606f51":"code","99761574":"code","86d134d5":"code","9b3f5aba":"code","42d5f74a":"code","14758fc8":"code","1616c6e5":"code","31add40e":"code","a8f40c95":"code","61ff3e97":"code","67e3ac9e":"code","d747a732":"markdown","5ec9df43":"markdown","0664a324":"markdown","8bbafd99":"markdown","be2dd194":"markdown","6aa40c72":"markdown"},"source":{"e185c299":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29c81988":"import torch\nimport torchvision\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image, ImageDraw\nimport xml.etree.ElementTree as ET\n\nimages_dir = '\/kaggle\/input\/object-detection-dataset\/Object Detection Dataset\/train\/'\nannotations_dir = '\/kaggle\/input\/object-detection-dataset\/Object Detection Dataset\/train\/'","49606f51":"sample_id = 41\n\nsample_image_path = f'\/kaggle\/input\/object-detection-dataset\/Object Detection Dataset\/train\/pic 41.jpg'\nsample_annot_path = f'\/kaggle\/input\/object-detection-dataset\/Object Detection Dataset\/train\/pic 41.xml'","99761574":"sample_image = Image.open(sample_image_path)\nsample_image","86d134d5":"with open(sample_annot_path) as annot_file:\n    print(''.join(annot_file.readlines()))","9b3f5aba":"tree = ET.parse(sample_annot_path)\nroot = tree.getroot()\n\nsample_annotations = []\n\nfor neighbor in root.iter('bndbox'):\n    xmin = int(neighbor.find('xmin').text)\n    ymin = int(neighbor.find('ymin').text)\n    xmax = int(neighbor.find('xmax').text)\n    ymax = int(neighbor.find('ymax').text)\n    \n    sample_annotations.append([xmin, ymin, xmax, ymax])\n    \nprint('Ground-truth annotations:', sample_annotations)","42d5f74a":"sample_image_annotated = sample_image.copy()\n\nimg_bbox = ImageDraw.Draw(sample_image_annotated)\n\nfor bbox in sample_annotations:\n    img_bbox.rectangle(bbox, outline=\"white\") \n    \nsample_image_annotated","14758fc8":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n    pretrained=True, progress=True, num_classes=91, pretrained_backbone=True)\n\nmodel","1616c6e5":"model.eval()\n\nnp_sample_image = np.array(sample_image.convert(\"RGB\"))\n\ntransformed_img = torchvision.transforms.transforms.ToTensor()(\n        torchvision.transforms.ToPILImage()(np_sample_image))\n\nresult = model([transformed_img])\n\nresult","31add40e":"COCO_INSTANCE_CATEGORY_NAMES = ['bus',\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', \n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack', 'umbrella', 'N\/A', 'N\/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N\/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table',\n    'N\/A', 'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']","a8f40c95":"bus_id = 1\nbus_boxes = [x.detach().numpy().tolist() for i, x in enumerate(result[0]['boxes']) if result[0]['labels'][i] == bus_id]\nbus_boxes","61ff3e97":"sample_image_annotated = sample_image.copy()\n\nimg_bbox = ImageDraw.Draw(sample_image_annotated)\n\nfor bbox in sample_annotations:\n    img_bbox.rectangle(bbox, outline=\"white\") \n\nfor bbox in bus_boxes:\n    x1, x2, x3, x4 = map(int, bbox)\n    print(x1, x2, x3, x4)\n    img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\") \n\nsample_image_annotated","67e3ac9e":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Mar\u00edlia Prata, @mpwolke Was here' )","d747a732":"#Choose the number that the object figures in the list above. Bus was 7th position resulted in empty brackets. Then I changed its position to number one in the list. And it worked.","5ec9df43":"#Draw Predictions","0664a324":"#Download Faster R-CNN","8bbafd99":"#Thanks to Mateusz Kwasniak https:\/\/www.kaggle.com\/mtszkw\/inference-with-pretrained-faster-r-cnn-pytorch","be2dd194":"#Draw Image With Annotations","6aa40c72":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcS7SDX0jmjh8KJ8fF8EHqHDZ9QioJ4YG0AcUA&usqp=CAU)immersivelimit.com"}}