{"cell_type":{"ba8f5f1f":"code","98646edc":"code","b71fb5a4":"code","b68bd012":"code","9b7362ba":"code","69e25863":"code","93a62c67":"code","3c9ae875":"code","a0892f54":"code","8e8c3007":"code","79ce6a20":"code","096639b1":"code","6f2c4d2d":"code","6b12df25":"code","e6ebf36a":"code","38cb7981":"code","86334a78":"code","584b8433":"code","59ab0c30":"code","efaaef81":"code","f03d27c1":"code","b7dc1302":"code","bce625f6":"code","19efc749":"code","4888e569":"code","c13e9034":"code","c92440aa":"code","0b79d36a":"code","0ed03ace":"code","0928f7b0":"code","88c18f68":"code","aaa7119a":"code","20b0605a":"code","94385bf8":"code","13188fc4":"code","ebe180a8":"code","ebb8af9e":"code","b51e48a6":"code","4436d443":"code","121ae095":"code","86c60384":"code","daca1e1b":"markdown","7d019185":"markdown","b135b8ab":"markdown","6d8ab90b":"markdown","921b9284":"markdown","8678fedf":"markdown","aadad4a1":"markdown","eab29282":"markdown","5cfc5464":"markdown","8ef0afbf":"markdown","a4a90261":"markdown","9f53a9e3":"markdown","fb215e5b":"markdown","3417c54a":"markdown","e4f2d66e":"markdown","bf8a914e":"markdown","86e0470e":"markdown","2d2c611b":"markdown","fb306711":"markdown","5e07e066":"markdown","52d74e97":"markdown","c74559f2":"markdown","09be824a":"markdown","2ec92c29":"markdown","f752e082":"markdown","9c6b3b6c":"markdown","ebc595f7":"markdown","ad8c9586":"markdown","6084572d":"markdown","646c1726":"markdown","60958779":"markdown","d92c7ff3":"markdown","15ad10ea":"markdown"},"source":{"ba8f5f1f":"# Loading libraries needed for analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nrandom_seed = 316\npd.set_option('display.max_columns', None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","98646edc":"train_df = pd.read_csv('..\/input\/shopee-workshop-building-ann\/HR-Employee-Attrition_train.csv')\ntest_df = pd.read_csv('..\/input\/shopee-workshop-building-ann\/HR-Employee-Attrition_test.csv')\nprint('Train data:', train_df.shape)\nprint('Test data:', test_df.shape)","b71fb5a4":"train_df.head()","b68bd012":"train_df.info()","9b7362ba":"test_df.info()","69e25863":"train_df.describe()","93a62c67":"sns.countplot(train_df['Attrition'])\nplt.show()","3c9ae875":"train_df['Attrition'].value_counts(normalize = True)","a0892f54":"col_summary = pd.DataFrame(train_df.columns, columns = ['Column'])\nna_list = []\nunique_list = []\ndtype_list = []\n\nfor col in train_df.columns:\n    na_list.append(train_df[col].isna().sum())\n    unique_list.append(train_df[col].nunique())\n    dtype_list.append(train_df[col].dtype)\n    \ncol_summary['Missing values'] = na_list\ncol_summary['Unique values'] = unique_list\ncol_summary['Data type'] = dtype_list\ncol_summary","8e8c3007":"train_df.drop(['StandardHours', 'Over18', 'EmployeeCount'], axis = 1, inplace = True)\ntest_df.drop(['StandardHours', 'Over18', 'EmployeeCount'], axis = 1, inplace = True)","79ce6a20":"num_cols = train_df._get_numeric_data()\nnum_cols","096639b1":"cat_cols = train_df[[col for col in train_df.columns if col not in num_cols.columns]]\ncat_cols","6f2c4d2d":"id_col = 'EmployeeNumber'\ntarget_col = 'Attrition'","6b12df25":"target_col_mapper = {\n    'Yes': 1,\n    'No': 0\n}\n\ntrain_df['Attrition'] = train_df['Attrition'].map(target_col_mapper)","e6ebf36a":"# add 'age range' column\ntrain_df['age_band'] = pd.cut(train_df['Age'], 4, labels = False)\ntest_df['age_band'] = pd.cut(test_df['Age'], 4, labels = False)\n\n# add 'daily rate range' column\ntrain_df['daily_rate_band'] = pd.qcut(train_df['DailyRate'], 5, labels = False)\ntest_df['daily_rate_band'] = pd.qcut(test_df['DailyRate'], 5, labels = False)\n\n# add 'distance from home range' column\ntrain_df['distance_from_home_band'] = pd.qcut(train_df['DistanceFromHome'], 5, labels = False)\ntest_df['distance_from_home_band'] = pd.qcut(test_df['DistanceFromHome'], 5, labels = False)\n\n# add 'hourly rate range' column\ntrain_df['hourly_rate_band'] = pd.qcut(train_df['HourlyRate'], 5, labels = False)\ntest_df['hourly_rate_band'] = pd.qcut(test_df['HourlyRate'], 5, labels = False)\n\n# add 'monthly income range' column\ntrain_df['monthly_income_band'] = pd.qcut(train_df['MonthlyIncome'], 5, labels = False)\ntest_df['monthly_income_band'] = pd.qcut(test_df['MonthlyIncome'], 5, labels = False)\n\n# add 'monthly rate range' column\ntrain_df['monthly_rate_band'] = pd.qcut(train_df['MonthlyRate'], 5, labels = False)\ntest_df['monthly_rate_band'] = pd.qcut(test_df['MonthlyRate'], 5, labels = False)\n\n# add 'num companies worked range' column\ntrain_df['num_companies_worked_band'] = pd.qcut(train_df['NumCompaniesWorked'], 3, labels = False)\ntest_df['num_companies_worked_band'] = pd.qcut(test_df['NumCompaniesWorked'], 3, labels = False)\n\n# add 'percent salary hike range' column\ntrain_df['percent_salary_hike_band'] = pd.qcut(train_df['PercentSalaryHike'], 3, labels = False)\ntest_df['percent_salary_hike_band'] = pd.qcut(test_df['PercentSalaryHike'], 3, labels = False)\n\n# add 'total working years range' column\ntrain_df['total_working_years_band'] = pd.qcut(train_df['TotalWorkingYears'], 3, labels = False)\ntest_df['total_working_years_band'] = pd.qcut(test_df['TotalWorkingYears'], 3, labels = False)\n\n# add 'training times last year range' column\ntrain_df['training_times_last_year_band'] = pd.qcut(train_df['TrainingTimesLastYear'], 3, labels = False)\ntest_df['training_times_last_year_band'] = pd.qcut(test_df['TrainingTimesLastYear'], 3, labels = False)\n\n# add 'years at company range' column\ntrain_df['years_at_company_band'] = pd.qcut(train_df['YearsAtCompany'], 3, labels = False)\ntest_df['years_at_company_band'] = pd.qcut(test_df['YearsAtCompany'], 3, labels = False)\n\n# add 'years in current role range' column\ntrain_df['years_in_current_role_band'] = pd.qcut(train_df['YearsInCurrentRole'], 3, labels = False)\ntest_df['years_in_current_role_band'] = pd.qcut(test_df['YearsInCurrentRole'], 3, labels = False)\n\n# add 'years since last promotion range' column\ntrain_df['years_since_last_promotion_band'] = pd.qcut(train_df['YearsSinceLastPromotion'], 3, duplicates='drop', labels = False)\ntest_df['years_since_last_promotion_band'] = pd.qcut(test_df['YearsSinceLastPromotion'], 3, duplicates='drop', labels = False)\n\n# add 'years with curr manager range' column\ntrain_df['years_with_curr_manager_band'] = pd.qcut(train_df['YearsWithCurrManager'], 3, labels = False)\ntest_df['years_with_curr_manager_band'] = pd.qcut(test_df['YearsWithCurrManager'], 3, labels = False)","38cb7981":"# add 'no_company_before' column\ntrain_df['no_company_before'] = train_df['NumCompaniesWorked'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['no_company_before'] = test_df['NumCompaniesWorked'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'worked_less_than_a_year' column\ntrain_df['worked_less_than_a_year'] = train_df['TotalWorkingYears'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['worked_less_than_a_year'] = test_df['TotalWorkingYears'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'no_training_last_year' column\ntrain_df['no_training_last_year'] = train_df['TrainingTimesLastYear'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['no_training_last_year'] = test_df['TrainingTimesLastYear'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'less_than_a_year_at_company' column\ntrain_df['less_than_a_year_at_company'] = train_df['YearsAtCompany'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['less_than_a_year_at_company'] = test_df['YearsAtCompany'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'never_promoted' column\ntrain_df['never_promoted'] = train_df['YearsSinceLastPromotion'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['never_promoted'] = test_df['YearsSinceLastPromotion'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'less_than_a_year_with_manager' column\ntrain_df['less_than_a_year_with_manager'] = train_df['YearsWithCurrManager'].apply(lambda x: 'Yes' if x == 0 else 'No')\ntest_df['less_than_a_year_with_manager'] = test_df['YearsWithCurrManager'].apply(lambda x: 'Yes' if x == 0 else 'No')\n\n# add 'no_business_travel' column\ntrain_df['no_business_travel'] = train_df['BusinessTravel'].apply(lambda x: 'Yes' if x == 'Non-Travel' else 'No')\ntest_df['no_business_travel'] = test_df['BusinessTravel'].apply(lambda x: 'Yes' if x == 'Non-Travel' else 'No')\n\n# drop columns\ndrop_cols = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ntrain_df.drop(drop_cols, axis=1, inplace=True)\ntest_df.drop(drop_cols, axis=1, inplace=True)\n\nprint(train_df.shape, test_df.shape)","86334a78":"# nominal data\nnom_cols = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n\n# ordinal data\nord_cols = ['BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']\n\n# additional features\nnom_cols.append('no_company_before')\nnom_cols.append('worked_less_than_a_year')\nnom_cols.append('no_training_last_year')\nnom_cols.append('less_than_a_year_at_company')\nnom_cols.append('never_promoted')\nnom_cols.append('less_than_a_year_with_manager')\nnom_cols.append('no_business_travel')\n\nbinned_feats = ['age_band', 'daily_rate_band', 'distance_from_home_band', 'hourly_rate_band', 'monthly_income_band', 'monthly_rate_band', 'num_companies_worked_band', 'percent_salary_hike_band', 'total_working_years_band', 'training_times_last_year_band', 'years_at_company_band', 'years_in_current_role_band', 'years_since_last_promotion_band', 'years_with_curr_manager_band']\nfor feat in binned_feats:\n    ord_cols.append(feat)","584b8433":"for feat in nom_cols + ord_cols:\n    print('Attrition Correlation by:', feat)\n    print(train_df[[feat, target_col]].groupby(feat, as_index=False).mean().sort_values(by=target_col, ascending=False))\n    print('-'*50)\n\n# for feat in :\n#     display(train_df.groupby(feat)[target_col].mean().to_frame().style.background_gradient(cmap='summer_r'))","59ab0c30":"train_df.corr()[target_col]","efaaef81":"# encode nominal features\ndef cat_to_dummy(train, test):\n    train_d = pd.get_dummies(train)\n    test_d = pd.get_dummies(test)\n    return train_d, test_d\n\ntrain_nom, test_nom = cat_to_dummy(train_df[nom_cols], test_df[nom_cols])\n\n# drop original nominal columns\ntrain_df.drop(nom_cols, axis = 1, inplace = True)\ntest_df.drop(nom_cols, axis = 1, inplace = True)\n\n# concatenate encoded nominal columns\ntrain_df = pd.concat([train_df, train_nom], axis = 1)\ntest_df = pd.concat([test_df, test_nom], axis = 1)\n\ntrain_nom","f03d27c1":"train_df['BusinessTravel'].value_counts()","b7dc1302":"business_travel_mapper = {\n    'Travel_Rarely': 1,\n    'Travel_Frequently': 2,\n    'Non-Travel': 0\n}\n\ntrain_df['BusinessTravel'] = train_df['BusinessTravel'].map(business_travel_mapper)\ntest_df['BusinessTravel'] = test_df['BusinessTravel'].map(business_travel_mapper)","bce625f6":"X_train, X_test, y_train, y_test = train_test_split(train_df.drop([target_col, id_col], axis=1), train_df[target_col], test_size=0.3, random_state=random_seed, stratify=train_df[target_col])\nprint('Train data:', X_train.shape)\nprint('Test data:', X_test.shape)","19efc749":"# scaler = StandardScaler()\n# # scaler = MinMaxScaler(feature_range=(-1,1))\n# # numerical_cols = [ncol for ncol in train_df.columns if ncol not in nom_cols + ord_cols + [target_col, id_col]]\n# # numerical_cols = [ncol for ncol in train_df.columns if ncol not in nom_cols + [target_col, id_col]]\n# numerical_cols = ord_cols\n\n# # fit only on training data\n# X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n# X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n# test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n\n# X_train.head()","4888e569":"train_df.to_csv('train_scaled.csv', index = False)\ntest_df.to_csv('test_scaled.csv', index = False)","c13e9034":"%%time\nstart_time = time.time()\n\nNN = MLPClassifier(max_iter=1000, random_state=random_seed)\nparameter_space = {\n    # 'hidden_layer_sizes': [(64, 64, 64), (63, 44, 63), (63, 63, 63), (50,50,50), (49, 34, 49), (49, 49, 49)], # based on the number of features + 1\n    'hidden_layer_sizes': [(102, 102, 102), (101, 67, 101), (38, 38, 38), (37, 26, 37)],\n    'activation': ['tanh', 'relu','logistic'],\n    'solver': ['adam', 'lbfgs'],\n    'alpha': [0.001, 0.01],\n    'learning_rate': ['constant','adaptive']\n}\n\nscoring = ['f1', 'recall', 'precision', 'accuracy']\nclf = GridSearchCV(NN, parameter_space, n_jobs=-1, scoring=scoring, refit='f1', cv=10)\nclf.fit(X_train, y_train)\nprint('Time taken for training the model:', \"{:.2f}\".format((time.time() - start_time) \/ 60))","c92440aa":"print('Best parameters found:\\n', clf.best_params_)","0b79d36a":"clf.best_estimator_","0ed03ace":"clf.best_score_\n\n# baseline\n# 0.5919221341710098","0928f7b0":"# best scores\ncv_results = pd.DataFrame.from_dict(clf.cv_results_)\ncv_results = cv_results[['mean_test_f1', 'rank_test_f1', 'mean_test_recall', 'rank_test_recall', 'mean_test_precision', 'mean_test_accuracy', 'rank_test_accuracy', 'params', 'std_test_f1', 'std_test_accuracy']]","88c18f68":"# f1\ncv_results.loc[cv_results['rank_test_f1']==1]","aaa7119a":"# recall\ncv_results[cv_results['rank_test_recall']==1]","20b0605a":"# accuracy\ncv_results[cv_results['rank_test_accuracy']==1]","94385bf8":"%%time\nstart_time = time.time()\n\nNN = clf.best_estimator_\ncv_results = cross_validate(NN, X_train, y_train, scoring=scoring, cv=10, return_train_score=True)\n\nprint('CV scores on training set:')\nprint(\"Average Recall score: %0.4f (+\/- %0.4f)\" % (cv_results['train_recall'].mean(), cv_results['train_recall'].std() * 2))\nprint(\"Average F1 score: %0.4f (+\/- %0.4f)\" % (cv_results['train_f1'].mean(), cv_results['train_f1'].std() * 2))\nprint(\"Average Precision score: %0.4f (+\/- %0.4f)\" % (cv_results['train_precision'].mean(), cv_results['train_precision'].std() * 2))\nprint(\"Average Accuracy score: %0.4f (+\/- %0.4f)\" % (cv_results['train_accuracy'].mean(), cv_results['train_accuracy'].std() * 2))\nprint()\nprint('CV scores on test set:')\nprint(\"Average Recall score: %0.4f (+\/- %0.4f)\" % (cv_results['test_recall'].mean(), cv_results['test_recall'].std() * 2))\nprint(\"Average F1 score: %0.4f (+\/- %0.4f)\" % (cv_results['test_f1'].mean(), cv_results['test_f1'].std() * 2))\nprint(\"Average Precision score: %0.4f (+\/- %0.4f)\" % (cv_results['test_precision'].mean(), cv_results['test_precision'].std() * 2))\nprint(\"Average Accuracy score: %0.4f (+\/- %0.4f)\" % (cv_results['test_accuracy'].mean(), cv_results['test_accuracy'].std() * 2))\nprint()\nprint('Time taken for cross-validation:', \"{:.2f}\".format((time.time() - start_time) \/ 60))\n\n# baseline\n# CV scores on test set:\n# Average Recall score: 0.5029 (+\/- 0.2099)\n# Average F1 score: 0.5919 (+\/- 0.2097)\n# Average Precision score: 0.7506 (+\/- 0.3197)\n# Average Accuracy score: 0.8832 (+\/- 0.0732)","13188fc4":"NN.fit(X_train, y_train)","ebe180a8":"def metrics(true, pred):\n    recall = recall_score(true, pred)\n    f1score = f1_score(true, pred)\n    precision = precision_score(true, pred)\n    accuracy = accuracy_score(true, pred)\n    print(f'recall: {recall}, f1-score: {f1score}, precision: {precision}, accuracy: {accuracy}')","ebb8af9e":"y_true, y_pred = y_train, NN.predict(X_train)\nprint('Results on train set:')\nprint(confusion_matrix(y_true, y_pred))\nmetrics(y_true, y_pred)","b51e48a6":"y_true, y_pred = y_test, NN.predict(X_test)\nprint('Results on test set:')\nprint(confusion_matrix(y_true, y_pred))\nmetrics(y_true, y_pred)\n\n# baseline\n# recall: 0.43478260869565216, f1-score: 0.4878048780487805, precision: 0.5555555555555556, accuracy: 0.8467153284671532","4436d443":"pred = NN.predict(test_df.drop([id_col], axis=1))","121ae095":"submission = pd.DataFrame({\n    'EmployeeNumber': test_df[id_col],\n    'Attrition': ['Yes' if i == 1 else 'No' for i in pred]\n})\nsubmission['Attrition'].value_counts()","86c60384":"submission.to_csv('submission.csv', index = False)","daca1e1b":"**Performance of the model on test data**","7d019185":"# Features Correlation with Attrition","b135b8ab":"# Employee Attrition Prediction","6d8ab90b":"**Identify id and target columns**","921b9284":"Since columns **'StandardHours', 'Over18', 'EmployeeCount'** has only one unique value, we could exclude them from our features.","8678fedf":"**Check numerical columns**","aadad4a1":"# Modeling","eab29282":"Around 17 percent of the employees in the dataset leaves the company. \n\nTherefore, the **baseline accuracy** is **83 percent** and our neural network model should definitely beat this baseline benchmark.","5cfc5464":"The problem we are trying to solve in this analysis is employee attrition prediction. Based on the features available, we are going to predict whether an employee leaves the company or not.","8ef0afbf":"**Additional features**","a4a90261":"**Find optimal parameter setting with GridSearch**","9f53a9e3":"The rest of the ordinal features are already in ordinal form so no need to map them.","fb215e5b":"# Data Preprocessing","3417c54a":"**Handle continuous numeric features**","e4f2d66e":"# Cross-validation","bf8a914e":"**Check categorical columns**","86e0470e":"# Extract features","2d2c611b":"From the confusion matrix, we can see that our neural network misclassified 10 'No's and 17 'Yes's\n\n(TN = 218, FN = 17, FP = 10, TP = 29)","fb306711":"Ordinal Encoding: Ordinal features","5e07e066":"**Handle nominal and ordinal variables**","52d74e97":"# References\n* [Shopee Workshop Building ANN - Sample code](https:\/\/www.kaggle.com\/xingyuren\/sample-code)\n* [Cross-Validation for Imbalanced Datasets - Lumiata Tech Blog](https:\/\/medium.com\/lumiata\/cross-validation-for-imbalanced-datasets-9d203ba47e8)","c74559f2":"# Evaluation metrics","09be824a":"One Hot Encoding: Nominal features","2ec92c29":"**Save to file: train and test features**","f752e082":"Encode target column","9c6b3b6c":"**Check columns for missing and unique values**","ebc595f7":"**Performance of the model on training data**","ad8c9586":"**Split data into train and test**","6084572d":"# Exploratory Data Analysis","646c1726":"**Identify nominal and ordinal variables**","60958779":"Choosing the evaluation metrics is quite challenging when dealing with imbalanced datasets. Accuracy is not the right metric as the accuracy of baseline model that classifies everything as overrepresented class is 83%.","d92c7ff3":"**Class distribution**","15ad10ea":"# Feature Scaling"}}