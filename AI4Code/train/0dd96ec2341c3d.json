{"cell_type":{"e5babf1d":"code","e139fa32":"code","5c4ec53e":"code","9dcd3b8c":"code","d89e66f7":"code","4e1c4832":"code","44660b0d":"code","7a2c7001":"code","eead51ee":"code","fc72d8b2":"code","9730daf4":"code","41246d77":"code","b4276669":"code","ff81061c":"code","f1fa8dfe":"code","0a37111c":"code","153d29f2":"code","26250665":"code","a7d6f7c9":"code","3e774d30":"code","769f8fa0":"code","43a9031c":"code","199143ee":"code","46c37f82":"code","c36d86a0":"code","4865b789":"code","a414c2da":"code","77c513a6":"code","a5cd338b":"code","304de565":"code","69f36c97":"markdown","6c760276":"markdown","987e2926":"markdown","b3f213f2":"markdown","3340090d":"markdown","2e46f0e0":"markdown","c7ad943b":"markdown","653181fd":"markdown","dd4ebbd1":"markdown","eb2d6da1":"markdown"},"source":{"e5babf1d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e139fa32":"df= pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","5c4ec53e":"df.describe().T","9dcd3b8c":"df.isnull().sum()","d89e66f7":"from scipy import stats\nz = np.abs(stats.zscore(df))\nprint(z)","4e1c4832":"threshold = 3\nprint(np.where(z > 3))","44660b0d":"print(z[13][9])","7a2c7001":"df_o = df[(z < 3).all(axis=1)]","eead51ee":"df.shape","fc72d8b2":"df_o.shape","9730daf4":"from sklearn.model_selection import train_test_split\nX = df_o.drop(columns = 'quality')\ny = df_o['quality']","41246d77":"X.head()","b4276669":"y.head()","ff81061c":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)","f1fa8dfe":"y_pred = clf.predict(X_test)","0a37111c":"from sklearn import metrics","153d29f2":"print('Accuracy: ', metrics.accuracy_score(y_test,y_pred))","26250665":"df.columns","a7d6f7c9":"import pandas as pd\nfeature_imp = pd.Series(clf.feature_importances_, index=df_o.columns[:11]).sort_values(ascending=False)\nfeature_imp","3e774d30":"%matplotlib inline\nimport seaborn as sns\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","769f8fa0":"#Random Search Cross Validation\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","43a9031c":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 14, num = 7)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","199143ee":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train,y_train)","46c37f82":"rf_random.best_params_","c36d86a0":"def evaluate(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    errors = abs(predictions - y_test)\n    mape = 100 * np.mean(errors \/ y_test)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\nbase_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test,y_test)","4865b789":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","a414c2da":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","77c513a6":"#Grid Search with Cross Validation\n\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [8, 10, 12, 14],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","a5cd338b":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","304de565":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","69f36c97":"### Feature importance visualization","6c760276":"### <a id='grid'>Fine tuning with GridSearchCV <\/a>","987e2926":"### <a id = 'random'>Fine tuning using RandomSearchCV<\/a>","b3f213f2":"## Random forest and Random Search on red wine quality:\n![Random forest: credit: github user kjw0612](https:\/\/camo.githubusercontent.com\/5afa4bc35f56871811e34f442baacc6cc098bd12a9320062846efe1bed011275\/68747470733a2f2f33312e6d656469612e74756d626c722e636f6d2f37393637306561626539336364643434386331356635626362313938643066622f74756d626c725f696e6c696e655f6e386533393859624b76317330347263332e706e67)\nIn this notebook Eswar sai and I have worked on Random forest regression and feature importance based feature selection using it. This is a good notebook for someone who is starting out with random forest. But before all that, <br\/>\n### What Is Random Forest?\nRandom forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.<br\/>\n### How Random Forest Works:\n\nPut simply: random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.<br\/>\n\nOne big advantage of random forest is that it can be used for both classification and regression problems, which form the majority of current machine learning systems. Let's look at random forest in classification, since classification is sometimes considered the building block of machine learning.<br\/>\n\nRandom forest has nearly the same hyperparameters as a decision tree or a bagging classifier. Fortunately, there's no need to combine a decision tree with a bagging classifier because you can easily use the classifier-class of random forest. With random forest, you can also deal with regression tasks by using the algorithm's regressor.<br\/>\n\nRandom forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.<br\/>\n\nTherefore, in random forest, only a random subset of the features is taken into consideration by the algorithm for splitting a node. You can even make trees more random by additionally using random thresholds for each feature rather than searching for the best possible thresholds (like a normal decision tree does).<br\/>\n\nRandom forest was first introduced by Briemann in his paper of 2001. You can read the details of random forest's inner working, which is called the CART algorithm, in the [following blog](https:\/\/shyambhu20.blogspot.com\/2020\/01\/Random-forest-model.html).<br\/>\nNow, let's discuss the contents of this notebook:<br\/>\n### contents:\n(1) [Basic data exploration](#section1)<br\/>\n(2) [Basic Random forest regressor fitting](#section2)<br\/>\n(3) [Fine tuning using RandomizedSearchCV](#random)<br\/>\n(4) [Fine tuning using GridSearchCV](#grid)<br\/>\n<br\/>\nEnjoy reading! and if you like the effort, consider showing your appreciation with a upvote!\nReferences:\n(1) [Description taken from this article](https:\/\/builtin.com\/data-science\/random-forest-algorithm)<br\/>\n","3340090d":"### <a id = 'section1'> Basic data exploration<\/a>","2e46f0e0":"### Z-score based outlier removal","c7ad943b":"### Data loading","653181fd":"### Train-test-split","dd4ebbd1":"#### Here ends our treaties with Random forest. Thanks for reading the notebook. Happy kaggling!","eb2d6da1":"### <a id='section1'>Basic Random forest fitting<\/a>"}}