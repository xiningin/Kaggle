{"cell_type":{"38c5d035":"code","fc793b6e":"code","90169607":"code","d17aef62":"code","e3eca86c":"code","b703f21d":"code","323c7846":"code","19cd8c04":"code","5254ab13":"code","5b4b1138":"code","d445ca10":"code","a407ddfe":"code","5c04d0ec":"code","805402af":"code","964e859d":"code","1239a1d5":"code","8a3c66a9":"code","04172c63":"code","1a049570":"code","ee1cee87":"code","876162f3":"code","19d720d1":"code","96a58768":"code","7946c089":"code","ad8aa749":"code","386badc0":"code","9bebbc5f":"code","34d4a77a":"code","c2cef651":"code","37640824":"code","8c514670":"code","9b866bbc":"code","560e0139":"code","44b726c3":"code","6ff44f08":"code","66f4d072":"code","0e08657c":"code","383e441e":"code","61c31a24":"code","bdaebcdb":"code","374b8cc5":"code","68d7fb44":"code","942a6020":"markdown","16c4e553":"markdown","66a706aa":"markdown","7eb91fe8":"markdown","7d156821":"markdown","9c29cd8a":"markdown","335dd129":"markdown","a1f51fbd":"markdown","36bbf189":"markdown","e7ee24f6":"markdown","3a4947bd":"markdown","7ec4424b":"markdown","212c8307":"markdown","eb8a9e73":"markdown","463cd27d":"markdown","a3ceeb9e":"markdown","a1e2aa3a":"markdown","b17ff45a":"markdown","8f3d79d8":"markdown","9ba4476b":"markdown","e0a81047":"markdown","a3bd2206":"markdown","5aedb61c":"markdown","d8e4513a":"markdown","03d51794":"markdown","c74c98ec":"markdown","c43f6f77":"markdown","50d225e2":"markdown","1361ca1f":"markdown","204b347f":"markdown","b70de80a":"markdown","5642dd53":"markdown","3903feb1":"markdown","18364a9b":"markdown","a034d0a2":"markdown","8d082f28":"markdown","da42b7fc":"markdown","602c7e3e":"markdown"},"source":{"38c5d035":"# from google.colab import drive\n# drive.mount('\/content\/drive')","fc793b6e":"#Loading the neccessary libraries\n%matplotlib inline\n\nimport os, random, cv2, glob, itertools, shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom PIL import Image \n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.autograd import Variable\n\nimport torchvision\nfrom torchvision import datasets, transforms, utils\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","90169607":"windows_path = '''E:\\Data_Cloud\\GoogleDrive\\Dataset'''\ncolab_path = '..\/input\/utkface\/UTKFace'\nbasepath = colab_path\n\n#reassigning the name of the data location\nUTKFace_folder = '..\/input\/utkface\/UTKFace\/trainB_Female'\n","d17aef62":"plt.figure(figsize=(30,30))\n\n#looping over the images and selecting random images to view\nfor i in range(5):\n    file = np.random.choice(os.listdir(UTKFace_folder))\n    image_path= os.path.join(UTKFace_folder, file)\n    image=mpimg.imread(image_path)\n    plt.subplot(1,10,i+1)\n    plt.imshow(image)","e3eca86c":"for i in range(1):\n    file = np.random.choice(os.listdir(UTKFace_folder))\n    image_path = os.path.join(UTKFace_folder, file)\n    image=mpimg.imread(image_path)\n    plt.imshow(image)","b703f21d":"#creating a dataframe selecting gender and ages available\nface_df = pd.DataFrame(glob.glob(os.path.join(UTKFace_folder, '*.jpg')), columns=['names'])\nface_df['male'] = face_df.names.str.contains(r'\\d+_0_[0-4]_\\d+.jpg.chip.jpg$') + 0\nface_df['age'] = face_df.names.str.extract(r'(\\d+)_[0-1]_[0-4]_\\d+.jpg.chip.jpg$').astype(int) \n\n#Limiting Age Group between 20-70\nface_df = face_df[face_df['age'] <= 60] \nface_df = face_df[face_df['age'] >= 20] ","323c7846":"#Retreiving the images and labeling them accordingly\nfor idx, i in enumerate(face_df.sample(6).index):\n  plt.figure(figsize=[8,8])\n  im_row = face_df.loc[i,:]\n  my_image = Image.open(im_row.names).convert('RGB')\n  plt.subplot(3,2,idx+1)\n  plt.imshow(my_image)\n  plt.title(f'Gender {im_row.male} and Age {im_row.age}')","19cd8c04":"to_male = face_df[face_df['male'] == 1]\nto_female = face_df[face_df['male'] == 0]","5254ab13":"for idx, i in enumerate(to_male.sample(3).index):\n  plt.figure(figsize=[8,8])\n  im_row = face_df.loc[i,:]\n  my_image = Image.open(im_row.names).convert('RGB')\n  plt.subplot(3,2,idx+1)\n  plt.imshow(my_image)\n  plt.title(f'Gender {im_row.male} and Age {im_row.age}')","5b4b1138":"for idx, i in enumerate(to_female.sample(3).index):\n  plt.figure(figsize=[8,8])\n  im_row = face_df.loc[i,:]\n  my_image = Image.open(im_row.names)\n  plt.subplot(3,2,idx+1)\n  plt.imshow(my_image)\n  plt.title(f'Gender {im_row.male} and Age {im_row.age}')","d445ca10":"#Code Block only run once to resize images","a407ddfe":"#all_im = glob(os.path.join(basepath, '**\/*'))","5c04d0ec":"# for im in tqdm(all_im):\n#     image = Image.open(im)\n#     image = image.resize((128,128), Image.BILINEAR)\n#     image.save(im)","805402af":"#all_im[-1]","964e859d":"#Creating the Parameters\n\nparameters = {'ngf':32,\n              'ndf':64,\n              'num_epochs':100,\n              'decay_epoch': 100,\n              'lgG':0.0002,\n              'lgD':0.0002,\n              'beta1':.5,\n              'beta2':.9999,\n              'lambdaA':10,\n              'lambdaB':10,\n              'batch_size':32,\n              'pool_size':50, \n              'img_width':256, \n              'img_height':256, \n              'img_depth':3,\n              'input_size':128,\n              'resize_size':128,\n              'crop_size':256,\n              'fliplr':True,\n              'num_resnet':6,\n              'num_workers':2\n              }\n          ","1239a1d5":"#Convert to Numpy \ndef to_numpy(x):\n  return x.data.cpu().numpy()\n\n#use GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","8a3c66a9":"trans = transforms.Compose([\n    #transforms.Resize(size=parameters['input_size']),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n])","04172c63":"def plot_train_result(real_image, gen_image, recon_image, epoch, save=False,  \n                      show=True, fig_size=(15, 15)):\n   \n   fig, axes = plt.subplots(2, 3, figsize=fig_size)\n   \n   imgs = [to_numpy(real_image[0]), to_numpy(gen_image[0]), \n            to_numpy(recon_image[0]),\n            to_numpy(real_image[1]), to_numpy(gen_image[1]), \n            to_numpy(recon_image[1])]\n            \n   for ax, img in zip(axes.flatten(), imgs):\n     \n     ax.axis('off')\n     img = img.squeeze()\n     img = (((img - img.min()) * 255) \/ (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n\n     ax.imshow(img, cmap=None, aspect='equal')\n    \n   plt.subplots_adjust(wspace=0, hspace=0)\n\n   title = 'Epoch {0}'.format(epoch + 1)\n   fig.text(0.5, 0.04, title, ha='center')","1a049570":"class ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        \n        return_images = Variable(torch.cat(return_images, 0))\n        \n        return return_images","ee1cee87":"class DatasetFromFolder(data.Dataset):\n    \n    def __init__(self, image_dir, subfolder='train', transform=None, \n                 resize_scale=None, crop_size=None, fliplr=False):\n        super(DatasetFromFolder, self).__init__()\n        self.input_path = os.path.join(image_dir, subfolder)\n        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n        self.file_num = len(self.image_filenames)\n        self.transform = trans\n        \n        self.resize_scale = resize_scale\n        self.crop_size = crop_size\n        self.fliplr = fliplr\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, index):\n        # index = total_index % self.file_num\n        # Load Images\n        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n        img = Image.open(img_fn).convert('RGB')\n\n        #preprocessing\n        if self.resize_scale:\n            img = img.resize((self.resize_scale, self.resize_scale), \n                             Image.BILINEAR)\n\n        if self.crop_size:\n            x = random.randint(0, self.resize_scale - self.crop_size + 1)\n            y = random.randint(0, self.resize_scale - self.crop_size + 1)\n            img = img.crop((x, y, x + self.crop_size, y + self.crop_size))\n        \n        if self.fliplr:\n            if random.random() < 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img   ","876162f3":"class ConvBlock(torch.nn.Module):\n    \n    #Initialize a dunction\n    def __init__(self, input_size, output_size, kernel_size=3, stride=2,\n                 padding=1, activation='relu', batch_norm=True):\n        super(ConvBlock,self).__init__()\n        \n        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size,\n                                    stride, padding)\n        self.batch_norm = batch_norm\n        self.bn = torch.nn.InstanceNorm2d(output_size)\n        self.activation = activation\n        self.relu = torch.nn.ReLU(True)\n        self.lrelu = torch.nn.LeakyReLU(0.2,True)\n        self.tanh = torch.nn.Tanh()\n    \n    #Defining the forward function\n    def forward(self,x):\n        if self.batch_norm:\n            out = self.bn(self.conv(x))\n        else:\n            out = self.conv(x)\n        \n        if self.activation == 'relu':\n            return self.relu(out)\n        elif self.activation == 'lrelu':\n            return self.lrelu(out)\n        elif self.activation == 'tanh':\n            return self.tanh(out)\n        elif self.activation == 'no_act':\n            return out\n  ","19d720d1":"class DeconvBlock(torch.nn.Module):\n\n    #Initialization Function\n    def __init__(self, input_size, output_size, kernel_size=3, stride=2,\n                 padding=1, output_padding=1, activation='relu',\n                 batch_norm=True):\n      \n        super(DeconvBlock,self).__init__()\n\n        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size,\n                                               kernel_size, stride, padding,\n                                               output_padding)\n        self.batch_norm = batch_norm\n        self.bn = torch.nn.InstanceNorm2d(output_size)\n        self.activation = activation\n        self.relu = torch.nn.ReLU(True)\n    \n    #Forward pass\n    def forward(self,x):\n        if self.batch_norm:\n            out = self.bn(self.deconv(x))\n        else:\n            out = self.deconv(x)\n        if self.activation == 'relu':\n            return self.relu(out)\n        elif self.activation == 'lrelu':\n            return self.lrelu(out)\n        elif self.activation == 'tanh':\n            return self.tanh(out)\n        elif self.activation == 'no_act':\n            return out","96a58768":"class ResnetBlock(torch.nn.Module):\n    def __init__(self,num_filter,kernel_size=3,stride=1,padding=0):\n        super(ResnetBlock,self).__init__()\n\n        conv1 = torch.nn.Conv2d(num_filter, num_filter, kernel_size,\n                                stride, padding)\n        \n        conv2 = torch.nn.Conv2d(num_filter, num_filter, kernel_size,\n                                stride, padding)\n        \n        bn = torch.nn.InstanceNorm2d(num_filter)\n        relu = torch.nn.ReLU(True)\n        pad = torch.nn.ReflectionPad2d(1)\n        \n        self.resnet_block = torch.nn.Sequential(\n            pad,\n            conv1,\n            bn,\n            relu,\n            pad,\n            conv2,\n            bn\n            )\n    def forward(self,x):\n        out = self.resnet_block(x)\n        return out","7946c089":"class Generator(torch.nn.Module):\n    def __init__(self,input_dim,num_filter,output_dim,num_resnet):\n        super(Generator,self).__init__()\n        \n        #Reflection padding\n        self.pad = torch.nn.ReflectionPad2d(3)\n        \n        #Encoder\n        # Input Layer - NN Layer 1\n        self.conv1 = ConvBlock(input_dim, num_filter, kernel_size=7, stride=1, \n                               padding=0)\n        \n        # NN Layer 2\n        self.conv2 = ConvBlock(num_filter, num_filter*2)\n        \n        ## NN Layer 3\n        self.conv3 = ConvBlock(num_filter*2, num_filter*4)\n        \n        #Transformer\n        self.resnet_blocks = []\n        for i in range(num_resnet):\n            self.resnet_blocks.append(ResnetBlock(num_filter*4))\n        self.resnet_blocks = torch.nn.Sequential(*self.resnet_blocks)\n        \n        #Decoder\n        self.deconv1 = DeconvBlock(num_filter*4, num_filter*2)\n        self.deconv2 = DeconvBlock(num_filter*2, num_filter)\n        self.deconv3 = ConvBlock(num_filter, output_dim, kernel_size=7,\n                                 stride=1, padding=0, activation='tanh',\n                                 batch_norm=False)\n    \n    #Forward Function\n    def forward(self,x):\n        \n        #Encoder\n        enc1 = self.conv1(self.pad(x))\n        enc2 = self.conv2(enc1)\n        enc3 = self.conv3(enc2)\n        \n        #Transformer\n        res = self.resnet_blocks(enc3)\n        \n        #Decoder\n        dec1 = self.deconv1(res)\n        dec2 = self.deconv2(dec1)\n        out = self.deconv3(self.pad(dec2))\n        return out\n    \n    #Creating weights inside the Generator\n    \n    # def weights(m, mean=0.0, std = 0.02):\n    #   for m in self.children():\n    #     if isinstance(m, nn.Conv2d):\n    #       nn.init.normal_(m.weight, 0.0, 0.02)\n    #     elif isinstance(m, nn.BatchNorm2d):\n    #       nn.init.normal_(m.weight, 0.0, 0.02)\n    #       nn.init.constant_(m.bias, 0)\n    \n    def normal_weight_init(self,mean=0.0,std=0.02):\n        for m in self.children():\n            if isinstance(m,ConvBlock):\n                torch.nn.init.normal_(m.conv.weight,mean,std)\n            if isinstance(m,DeconvBlock):\n                torch.nn.init.normal_(m.deconv.weight,mean,std)\n            if isinstance(m,ResnetBlock):\n                torch.nn.init.normal_(m.conv.weight,mean,std)\n                torch.nn.init.constant_(m.conv.bias,0)","ad8aa749":"Generator_A = Generator(3, parameters['ngf'], 3, \n                        parameters['num_resnet']).cuda()\nGenerator_A.normal_weight_init(mean=0.0, std=0.02)\nGenerator_A","386badc0":"Generator_B = Generator(3, parameters['ngf'], 3, \n                        parameters['num_resnet']).cuda()\nGenerator_B.normal_weight_init(mean=0.0, std=0.02)\nGenerator_B","9bebbc5f":"class Discriminator(torch.nn.Module):\n    def __init__(self,input_dim,num_filter,output_dim):\n        super(Discriminator,self).__init__()\n\n        #Input - NN Layer 1\n        conv1 = ConvBlock(input_dim,num_filter,kernel_size=4,stride=2,padding=1,activation='lrelu',batch_norm=False)\n        \n        #NN Layer 2\n        conv2 = ConvBlock(num_filter,num_filter*2,kernel_size=4,stride=2,padding=1,activation='lrelu')\n        \n        #NN Layer 3\n        conv3 = ConvBlock(num_filter*2,num_filter*4,kernel_size=4,stride=2,padding=1,activation='lrelu')\n        \n        #NN Layer 4\n        conv4 = ConvBlock(num_filter*4,num_filter*8,kernel_size=4,stride=1,padding=1,activation='lrelu')\n        \n        #Output - NN Layer 5\n        conv5 = ConvBlock(num_filter*8,output_dim,kernel_size=4,stride=1,padding=1,activation='no_act',batch_norm=False)\n        \n        \n        self.conv_blocks = torch.nn.Sequential(\n            conv1,\n            conv2,\n            conv3,\n            conv4,\n            conv5\n            )\n        \n    def forward(self,x):\n        out = self.conv_blocks(x)\n        return out\n\n    #Creating the weights for the Discriminator\n    def normal_weight_init(self, mean=0.0, std=0.02):\n        for m in self.children():\n            if isinstance(m, ConvBlock):\n                torch.nn.init.normal_(m.conv.weight.data, mean, std)\n","34d4a77a":"#Creating the first Discriminator\nDiscriminator_A = Discriminator(3, parameters['ndf'], 1).cuda()\nDiscriminator_A.normal_weight_init(mean=0.0, std=0.02)\nDiscriminator_A","c2cef651":"#Creating the second Discriminator\nDiscriminator_B = Discriminator(3, parameters['ndf'], 1).cuda()\nDiscriminator_B.normal_weight_init(mean=0.0, std=0.02)\nDiscriminator_B","37640824":"train_data_A = DatasetFromFolder(basepath, subfolder='trainA_Male', transform=trans,\n                                resize_scale=parameters['resize_size'], \n                                #crop_size=parameters['crop_size'], \n                                #fliplr=parameters['fliplr']\n                                 )\n\ntrain_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A, \n                                                  batch_size=parameters['batch_size'], \n                                                  num_workers=parameters['num_workers'],\n                                                  pin_memory=True,\n                                                  shuffle=True)\n\ntrain_data_B = DatasetFromFolder(basepath, subfolder='trainB_Female', transform=trans,\n                                 resize_scale=parameters['resize_size'], \n                                 #crop_size=parameters['crop_size'], \n                                 #fliplr=True\n                                 )\n\ntrain_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, \n                                                  batch_size=parameters['batch_size'], \n                                                  num_workers=parameters['num_workers'],\n                                                  pin_memory=True,\n                                                  shuffle=True)\n\n#Load test data\ntest_data_A = DatasetFromFolder(basepath, subfolder='testA_Male', transform=trans)\n\ntest_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, \n                                                 batch_size=parameters['batch_size'], \n                                                 shuffle=False)\n\ntest_data_B = DatasetFromFolder(basepath, subfolder='trainB_Female', transform=trans)\n\ntest_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, \n                                                 batch_size=parameters['batch_size'], \n                                                 shuffle=False)\n","8c514670":"print(train_data_A.resize_scale)","9b866bbc":"test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0)\ntest_real_A_data","560e0139":"test_real_B_data = train_data_B.__getitem__(11).unsqueeze(0)\ntest_real_B_data","44b726c3":"Generator_optimizer = torch.optim.Adam(itertools.chain(Generator_A.parameters(),\n                                                       Generator_B.parameters()),\n                                                       betas=(parameters['beta1'], parameters['beta2']),\n                                                       lr=parameters['lgG'])\n\nDiscriminator_A_optimizer = torch.optim.Adam(itertools.chain(Discriminator_A.parameters(),\n                                                           Discriminator_B.parameters()),\n                                                           betas=(parameters['beta1'], parameters['beta2']),\n                                                           lr=parameters['lgD'])\n\nDiscriminator_B_optimizer = torch.optim.Adam(itertools.chain(Discriminator_A.parameters(),\n                                                           Discriminator_B.parameters()),\n                                                           betas=(parameters['beta1'], parameters['beta2']),\n                                                           lr=parameters['lgD'])\n","6ff44f08":"MSE_Loss = torch.nn.MSELoss().cuda()\nL1_Loss = torch.nn.L1Loss().cuda()","66f4d072":"Discriminator_A_avg_losses = []\nDiscriminator_B_avg_losses = []\nGenerator_A_avg_losses = []\nGenerator_B_avg_losses = []\n\ncycle_A_avg_losses = []\ncycle_B_avg_losses = []","0e08657c":"num_pool = 50\nfake_A_Pool = ImagePool(num_pool)\nfake_B_Pool = ImagePool(num_pool)\n\nstep = 0","383e441e":"scaler = torch.cuda.amp.GradScaler(enabled=True)\nbest_gen = np.inf\n\nfor epoch in range(parameters['num_epochs']):\n    Discriminator_A_losses = []\n    Discriminator_B_losses = []\n    Generator_A_losses = []\n    Generator_B_losses = []\n    cycle_A_losses = []\n    cycle_B_losses = []\n    \n    # Learing rate decay \n    if(epoch + 1) > parameters['decay_epoch']:\n        Discriminatoroptimizer.param_groups[0]['lr'] -= parameters['lgD'] \/ (parameters['num_epochs'] - parameters['decay_epoch'])\n        Discriminator_B_optimizer.param_groups[0]['lr'] -= parameters['lgD'] \/ (parameters['num_epochs'] - parameters['decay_epoch'])\n        G_optimizer.param_groups[0]['lr'] -= parameters['lrG'] \/ (parameters['num_epochs'] - parameters['decay_epoch'])\n        \n    \n    # training \n    for i, (real_A, real_B) in tqdm(enumerate(zip(train_data_loader_A, train_data_loader_B)), total=len(train_data_loader_A)):\n        \n        # input image data\n        real_A = real_A.to(device)\n        real_B = real_B.to(device)\n        \n        # Train The Generator\n\n        # A --> B\n        fake_B = Generator_A(real_A)\n        Discriminator_B_fake_decision = Discriminator_B(fake_B)\n        Generator_A_loss = MSE_Loss(Discriminator_B_fake_decision, Variable(torch.ones(Discriminator_B_fake_decision.size()).cuda()))\n        \n        # Forward Cycle Loss\n        recon_A = Generator_B(fake_B)\n        cycle_A_loss = L1_Loss(recon_A, real_A) * parameters['lambdaA']\n        \n        # B --> A\n        fake_A = Generator_B(real_B)\n        Discriminator_A_fake_decision = Discriminator_A(fake_A)\n        Generator_B_loss = MSE_Loss(Discriminator_A_fake_decision, Variable(torch.ones(Discriminator_A_fake_decision.size()).cuda()))\n        \n        # Backward Cycle Loss\n        recon_B = Generator_A(fake_A)\n        cycle_B_loss = L1_Loss(recon_B, real_B) * parameters['lambdaB']\n        \n        # Back Propagation\n        Generator_loss = Generator_A_loss + Generator_B_loss + cycle_A_loss + cycle_B_loss\n        Generator_optimizer.zero_grad()\n        Generator_loss.backward()\n        Generator_optimizer.step()\n        \n        \n        # Train Discriminator_A \n        Discriminator_A_real_decision = Discriminator_A(real_A)\n        Discriminator_A_real_loss = MSE_Loss(Discriminator_A_real_decision, Variable(torch.ones(Discriminator_A_real_decision.size()).cuda()))\n        \n        fake_A = fake_A_Pool.query(fake_A)\n        \n        Discriminator_A_fake_decision = Discriminator_A(fake_A)\n        Discriminator_A_fake_loss = MSE_Loss(Discriminator_A_fake_decision, Variable(torch.zeros(Discriminator_A_fake_decision.size()).cuda()))\n        \n        # Back propagation\n        Discriminator_A_loss = (Discriminator_A_real_loss + Discriminator_A_fake_loss) * 0.5\n        Discriminator_A_optimizer.zero_grad()\n        Discriminator_A_loss.backward()\n        Discriminator_A_optimizer.step()\n        \n        # Train Discriminator_B \n        Discriminator_B_real_decision = Discriminator_B(real_B)\n        # print('real_A, ',real_A.shape)\n        # print('real_B, ',real_B.shape)\n        # print('dis_A, ',Discriminator_A_real_decision.shape)\n        # print('dis_B, ',Discriminator_B_real_decision.shape)\n        Discriminator_B_real_loss = MSE_Loss(Discriminator_B_real_decision, Variable(torch.ones(Discriminator_B_fake_decision.size()).cuda()))\n        #Discriminator_B_real_loss = MSE_Loss(Discriminator_B_real_decision, Variable(torch.ones(Discriminator_B_real_decision.size()).cuda()))\n        \n       \n        fake_B = fake_B_Pool.query(fake_B)\n        \n        Discriminator_B_fake_decision = Discriminator_B(fake_B)\n        Discriminator_B_fake_loss = MSE_Loss(Discriminator_B_fake_decision, Variable(torch.zeros(Discriminator_B_fake_decision.size()).cuda()))\n        \n        # Back Propagation\n        Discriminator_B_loss = (Discriminator_B_real_loss + Discriminator_B_fake_loss) * 0.5\n        Discriminator_B_optimizer.zero_grad()\n        Discriminator_B_loss.backward()\n        Discriminator_B_optimizer.step()\n        \n        #  Print \n\n\n        # loss values\n        Discriminator_A_losses.append(Discriminator_A_loss.item())\n        Discriminator_B_losses.append(Discriminator_B_loss.item())\n        Generator_A_losses.append(Generator_A_loss.item())\n        Generator_B_losses.append(Generator_B_loss.item())\n        cycle_A_losses.append(cycle_A_loss.item())\n        cycle_B_losses.append(cycle_B_loss.item())\n\n        if i%100 == 0:\n            print('Epoch [%d\/%d], Step [%d\/%d], Discriminator_A_losses: %.4f, Discriminator_B_loss: %.4f, Generator_A_loss: %.4f, Generator_B_loss: %.4f'\n                  % (epoch+1, parameters['num_epochs'], i+1, len(train_data_loader_A), Discriminator_A_loss.item(), Discriminator_B_loss.item(), Generator_A_loss.item(), Generator_B_loss.item()))\n            \n        step += 1\n        \n    Discriminator_A_avg_loss = torch.mean(torch.FloatTensor(Discriminator_A_losses))\n    Discriminator_B_avg_loss = torch.mean(torch.FloatTensor(Discriminator_B_losses))\n    Generator_A_avg_loss = torch.mean(torch.FloatTensor(Generator_A_losses))\n    Generator_B_avg_loss = torch.mean(torch.FloatTensor(Generator_B_losses))\n    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n\n    # Average Loss Values\n    Discriminator_A_avg_losses.append(Discriminator_A_avg_loss.item())\n    Discriminator_B_avg_losses.append(Discriminator_B_avg_loss.item())\n    Generator_A_avg_losses.append(Generator_A_avg_loss.item())\n    Generator_B_avg_losses.append(Generator_B_avg_loss.item())\n    cycle_A_avg_losses.append(cycle_A_avg_loss.item())\n    cycle_B_avg_losses.append(cycle_B_avg_loss.item())\n    \n    # Test Image Results\n    test_real_A = test_real_A_data.cuda()\n    test_fake_B = Generator_A(test_real_A)\n    test_recon_A = Generator_B(test_fake_B)\n\n    test_real_B = test_real_B_data.cuda()\n    test_fake_A = Generator_B(test_real_B)\n    test_recon_B = Generator_A(test_fake_A)\n\n    plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n                            epoch, save=True)\n","61c31a24":" \"\"","bdaebcdb":"best_gen","374b8cc5":"all_losses = pd.DataFrame()\nall_losses['Discriminator_A_avg_losses'] = Discriminator_A_avg_losses\nall_losses['Discriminator_B_avg_losses'] = Discriminator_B_avg_losses\nall_losses['Generator_A_avg_losses'] = Generator_A_avg_losses\nall_losses['Generator_B_avg_losses'] = Generator_B_avg_losses\nall_losses['cycle_A_avg_losses'] = cycle_A_avg_losses\nall_losses['cycle_B_avg_losses'] = cycle_B_avg_losses\n\nprint(all_losses)","68d7fb44":"#save to csv\nall_losses.to_csv('..\/input\/utkface\/UTKFace\/all_losses_ver.csv', index=False)","942a6020":"##Creating the Generator\n\nWe will now create the __Generator__. The Generator will consist of the following:\n1. Encoder\n2. Transformer\n3. Decoder\n\nIn addition, we will also be adding __weights__.\n\nAfter creating the class, we will create the actuall Generator.","16c4e553":"Defining a function which will __plot__ the result from training","66a706aa":"## Retreiving Items\n\nIn this block \/ section we will be retreiving an item for both the real data generated twice. We will also show this data not as an image, but as a Tensor array. We will do this to verify that there are some changes made compared to it's original state.","7eb91fe8":"Our first step is to load the neccessary libraries we will be using for the entire process.","7d156821":"Once again we will verify that the dataframe assigned to the male gender is indeed pulling male images","9c29cd8a":"### Creating the Convolutional Block\n\nThe __convolution__ block will consist of two things.\n\n1. Initialization\n\n2. Forward pass.","335dd129":"### Creating the Dataloader\n\nIn this section, we begin by creating the __dataloader__ which we will use to retrieve the data in parts.","a1f51fbd":"### Creating the Dataset\n\nIn this section, we will now attempt to organize to understand further what the images properties are. It is also in this section that we will separate the images between male and female and remove any images with ages below 20 years old and above 60 years old.\n\nThe images has been pre-labled and we will be using Regex in retrieving image labels.","36bbf189":"### Test Real Data B","e7ee24f6":"We will repeat the step, but this time retrieve images of Females. This is once again to verify that the dataset is indeed retreiving the correct images.","3a4947bd":"#### Generator A","7ec4424b":"## Creating functions\n\nIn the next set, we will create several functions that will be used throughout. The first function will convert our data into Numpy, which we can call at a later point.\n\nWe will also set a variable so we can use a GPU for faster processing.","212c8307":"### Exploring the Images in the Data Folder\n\nNext, we will attempt to view the original images from the source data folder without making any changes. There will be no preprocessing or manipulation at this stage simply viewing the images. \n\nWe will view 5 images from the folder.","eb8a9e73":"#### Generator B\n\nRepeating the previous step, but for Generator B.","463cd27d":"Since we are using two platforms, we will be assigning a directory to the location of the data for each platform. This will make it easier to access the data, depending the platform we will be using.\n\nThe two platforms will be:\n1. A local installation of  Jupyter Notebook - work on local codes. GPU is limited\n2. A Google Colaboratory Notebook - Use of available resources and GPU to run the model","a3ceeb9e":"### Residual Learning Block\n\nIn this secton, we will create a __residual learning__ block or __Resnet__. This class will also contain two functions.\n\n1. Initialization\n\n2. Forward pass","a1e2aa3a":"### Preparing the Dataset\n\nIn thhis block we will be resizing all the images in the Dataset. This process will take some time when resizing around 60,000+ images. From it's orginal image size we will resize them to a 128x128. This will also ensure that the images will be the same size accross. This will limit any mismatch errors.","b17ff45a":"#### Discriminator A","8f3d79d8":"Using the image pool created earliler, we will create a pool of variables which we will be using for training at the next cell block.","9ba4476b":"## Creating the Optimizer\n\nIn this block, we need to create the learning optimizers. We will have one optimizer for the Generator and two optimizers for the Discriminator. The Discriminator has two optimizers as this function checks the data generated by the Generator.","e0a81047":"##Parameters\n\nIn this block, we will create global parameters which we can use at different areas of this project.","a3bd2206":"### De-Convolution Function\n\nIn this class we will create, we will need to bring back the convolution from the results previously processed. There will be two functions inside this class.\n\n1. Initialization\n\n2. Forward pass","5aedb61c":"With the images properly separated, we will first separate the images into two dataframes. One for dataframe for Males and another for Females.","d8e4513a":"# Introduction\n\nIn this project, we will attempt to greate a Cycle Generative Adversarial Network (cycleGAN) in order to manipulate facial images. We will attempt to change the gender for a set of images.","03d51794":"### Creating the Discriminator and Testing the Neural Network\n\nSimilar with the Generator, which requires 2 Neural Networks. A cycleGAN also uses 2 Discriminators.","c74c98ec":"Next, we will define an __ImagePool__ function","c43f6f77":"## Training the model and retreiving Images\n\n","50d225e2":"## Retreiving the Loss results\n\nWe will not retreive the losses generated after learning and save them to a csv file.","1361ca1f":"We will now run the Training sequence in the block. It is also in these block that learning begins and the time to process the results can take several hours. \n\nThere will be two results displayed below.\n1. The epoch number where the learning takes place. This will also include the loss.\n2. The image results on how well it did.","204b347f":"#### Discriminator B","b70de80a":"We will not view the images after the previous process. The purpose of this step is to verify that we are able to successfully separate the images accordingly.\n\n__Gender:__\n\n0 - Female\n\n1 - Male","5642dd53":"### Creating the Generator and Testing the Neural Network\n\nSince we will be using a Cycle GAN, which requires 2 Neural Networks competing with each other, we will create 2 Generators as its requirement. We will also test it by calling it and view the results created earlier.","3903feb1":"## Defining Losses\n\nIn order to make sure that there is learning, we will create cycle losses, which we will pass on at a later time.","18364a9b":"##Creating the Discriminator\n\nThe Discriminator will consist of the following\n1. Initialization\n2. Forward Pass\n3. Weights\n\nAfter creating the class, we will create the actuall Discriminator.\n","a034d0a2":"We will attempt the same procedure, but this time on a single image. We will look closely at just a single random image.","8d082f28":"### Test Real Data A","da42b7fc":"### Transformer\n\nSince we will be using a __transformer__ at a later stage, we will set a transformer variable and assign it procedures which transforms an image size accordingly. This transformer will also convert the image into a Tensor and Normalize it as well.","602c7e3e":"## Training Block \/ DataLoaders\n\nIn this block, we will create the actual training block we will be using to train our model."}}