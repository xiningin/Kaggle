{"cell_type":{"f892e29f":"code","91b189bb":"code","878e782d":"code","d89fe5a0":"code","0440761c":"code","410fc4be":"code","48a7e32a":"code","4a72bc32":"code","d0d3630b":"code","7f4ecaf1":"code","73472d27":"code","12e3d423":"code","86b7093c":"code","caf8d9fe":"code","5a7ae1b2":"code","bde6e518":"code","0884a315":"code","35152c9d":"code","4e485092":"code","3fc09402":"code","99a3ab52":"code","dd23b230":"code","f901dbf5":"code","05755980":"code","ad3a0a2e":"code","66da2f7e":"code","d652dc73":"code","71a6b622":"code","6eaea18b":"code","66f92521":"code","e62d8ce7":"code","93d30cc7":"markdown","3f6e1fb4":"markdown","eac1379c":"markdown","6c1fba2d":"markdown","bca1f416":"markdown","74590157":"markdown","d3305ce5":"markdown","27e841f4":"markdown","51e1c016":"markdown","b9986597":"markdown","fd1b9110":"markdown","ddc26594":"markdown","81863dca":"markdown","28db4879":"markdown","25d2ee98":"markdown","6866a1f4":"markdown","9ecc4fcf":"markdown","ddf03d34":"markdown","318655c9":"markdown","dd3bb029":"markdown","01d96d1b":"markdown","3431bb2a":"markdown","25a7696e":"markdown","f7df599b":"markdown","0fffd06e":"markdown","e50305be":"markdown","44a48e42":"markdown","769b9f09":"markdown","4f614d12":"markdown","35243eec":"markdown","adaf45af":"markdown"},"source":{"f892e29f":"## Import the packages required for this notebook\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os","91b189bb":"## Loading the data from the Kaggle environment\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","878e782d":"# Load Training data\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_df.head()","d89fe5a0":"# Loading test data\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') \ntest_df.head()","0440761c":"## Analysis train data\n\nprint(train_df.shape)\nprint(train_df.dtypes)","410fc4be":"# describe the train dataframe\ntrain_df.describe()","48a7e32a":"## describe the test dataset\n\ntest_df.describe()","4a72bc32":"print(\"Missing Age values: \",train_df['Age'].isnull().sum())\n\n\naverage_age = train_df['Age'].mean()\nprint(\"Average Age on the existing passengers :\",average_age)\n\n\n\n### filling the missing age values with average age.\ntrain_df['Age'] = train_df['Age'].fillna(average_age)\ntest_df['Age'] = test_df['Age'].fillna(average_age)","d0d3630b":"print(train_df['Embarked'].value_counts())\nprint(\"Missing Values: \", train_df['Embarked'].isnull().sum())\n\n\n## using the missing Embarked values to default as \"Q\". This will try to balance out the value in S.\n\ntrain_df['Embarked'] = train_df['Embarked'].fillna(\"Q\")\ntest_df['Embarked'] = train_df['Embarked'].fillna(\"Q\")\n\nprint(train_df['Embarked'].value_counts())","7f4ecaf1":"train_df['Pclass'].value_counts()  ## No missing values, PClass looks good in distribution.","73472d27":"train_df['Sex'].value_counts() ## no null values, features looks ok.","12e3d423":"## creating features on train data\n\ntrain_df['family_size'] = train_df['SibSp'] + train_df['Parch'] + 1\ntrain_df['is_alone'] = np.where(train_df['family_size'] <= 1, 1, 0)\ntrain_df['is_kid'] = np.where(train_df['Age'] <= 16, 1, 0)\n\n\n## creating features on test data\ntest_df['family_size'] = test_df['SibSp'] + test_df['Parch'] + 1\ntest_df['is_alone'] = np.where(test_df['family_size'] <= 1, 1, 0)\ntest_df['is_kid'] = np.where(test_df['Age'] <= 16, 1, 0)","86b7093c":"### Converting the string values to categorical numbers - on training data\n\ntrain_df['Sex_cat'] = train_df['Sex'].map({'male':0, 'female':1}) ## categorical value for sex\ntrain_df['Embarked_cat'] = train_df['Embarked'].map({'S':0,'C':1,'Q':2}) ## categorical value for Embarked\n\ntest_df['Sex_cat'] = test_df['Sex'].map({'male':0, 'female':1}) ## categorical value for sex\ntest_df['Embarked_cat'] = test_df['Embarked'].map({'S':0,'C':1,'Q':2}) ## categorical value for Embarked","caf8d9fe":"train_df.head() ### sampling the dataframe after the transformations applied","5a7ae1b2":"to_drop_cols = ['Sex','Embarked','Cabin','Ticket','Parch','SibSp','Name','Fare']\n\nfor cols in to_drop_cols:\n    print(\"Dropping \",cols)\n    train_df.drop(cols, axis=1, inplace=True)\n    test_df.drop(cols,axis=1, inplace=True)","bde6e518":"train_df.head()","0884a315":"## Selecting the training features.\n\nX = train_df[['Pclass','Age','family_size','is_alone','Sex_cat','Embarked_cat','is_kid']]\ny = train_df['Survived']\n\n## selecting the features for the test predictions. Note: The features are the same as in the training data.\nX_test_df = test_df[['Pclass','Age','family_size','is_alone','Sex_cat','Embarked_cat','is_kid']]","35152c9d":"X.head()","4e485092":"## we will use scikit-learn to split the data. Note: X and y are based of the Training dataset.\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=33) ## 0.2 represents 80\/20 split\n\nprint(X_train.shape)","3fc09402":"##\u00a0converting splitted training dataset\n\nX_train = torch.FloatTensor(X_train.values)\ny_train = torch.LongTensor(y_train.values)\nX_test = torch.FloatTensor(X_test.values)\ny_test = torch.LongTensor(y_test.values)\n\n\n## converting the test dataset.\nX_test_df = torch.FloatTensor(X_test_df.values)","99a3ab52":"X_train.shape ## notice it is now of type torch.","dd23b230":"## define the model\n## the model uses 2 hidden layers with 200, 100 neurons\n\nclass Model(nn.Module):\n    \n    def __init__(self, num_features=7, h1=200, h2=100, out_features=2):\n        super().__init__()\n        \n        self.input = nn.Linear(num_features,h1) ## fully connected linear unit\n        self.h1_layer = nn.Linear(h1,h2)\n        self.out = nn.Linear(h2, out_features)\n        \n    def forward(self, x):\n        \n        x = nn.functional.relu(self.input(x)) \n        x = nn.functional.relu(self.h1_layer(x))\n        x = nn.functional.relu(self.out(x)) ## ReLu activation unit for binary classification.\n        \n        return x","f901dbf5":"## Instantiate the model.\n\nmodel = Model()\nmodel","05755980":"criterion = nn.CrossEntropyLoss() ##\u00a0Using Cross Entropy Loss.","ad3a0a2e":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  ##\u00a0Adam optimizer with learning rate = 0.001","66da2f7e":"trainloader = DataLoader(X_train, batch_size=10, shuffle=True)\ntestloader = DataLoader(X_test, batch_size=10, shuffle=False)","d652dc73":"epochs = 200\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    \n    optimizer.zero_grad() \n    \n    y_predict = model.forward(X_train) ## predicting\n    loss = criterion(y_predict, y_train) ## calculating the loss using the loss function defined above.\n    \n    losses.append(loss)\n    \n    if i%10 == 0:  ##\u00a0printing out the epoch loss result after every 10 iterations.\n        print(f'Epoch: {i} \\t Loss: {loss:0.8f}')\n        \n    \n    loss.backward()\n    optimizer.step()","71a6b622":"###\u00a0plotting the losses\n\nplt.plot(range(epochs), losses)\nplt.title(\"Loss Graph\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","6eaea18b":"correct = 0\n\nwith torch.no_grad():\n    for i, data in enumerate(X_test):\n        y_val_test = model.forward(data)\n        \n        #print(f'Row: {i} \\t {y_val_test} \\t {y_val_test.argmax()} \\t {y_test[i]}')\n        \n        if y_val_test.argmax() == y_test[i]:\n            correct +=1\n\nprint(f'Total Correct Predictions: {correct} out of {len(X_test)}')\nprint(f'Accuracy : {100 * (correct\/len(X_test))}')","66f92521":"predict_test_df = test_df.copy() ##\u00a0making a copy of the test dataframe.\n\nwith torch.no_grad():\n    test_predictions = model(X_test_df) ##\u00a0using the model to make the predictions.\n    pred_max, pred_max_label = test_predictions.topk(1, dim=1)\n    \n    predict_test_df['Survived'] = pred_max_label.numpy().ravel()\n    \n    predict_test_df[['PassengerId','Survived']].to_csv('submission.csv',index=False) ## as per the competition rule.","e62d8ce7":"torch.save(model.state_dict(),'titanic_model.pt') ## this only saves the model state.","93d30cc7":"### 3.1 Features transformation and Extraction","3f6e1fb4":"**b. Feature: EMBARKED**","eac1379c":"Dropping columns that are not required from the data frame.","6c1fba2d":"# 1. General Imports","bca1f416":"**Saving the model**","74590157":"# 6. Making Predictions and Submission","d3305ce5":"# 4. Defining the Model","27e841f4":"2. Analying and understanding the Training data.","51e1c016":"### 3.4 Converting the dataset from dataframe to Tensors\n\nThis step converts the dataframes into tensors so that the pytorch model could process.","b9986597":"# 5. Training Model and Accuracy Check","fd1b9110":"### b. Define the loss function and Optimizer","ddc26594":"# Titanic notebook using pytorch for predicting passenger survival\n\nThis is a sample notebook that tries to solve the problem of predicting whether a passenger survived the infamous accident. The notebook will use the training data provided by the competition and will try to use deep learning neural network framework ([pytorch](https:\/\/pytorch.org)) to solve the problem.\n\nTable of Contents:\n\n1. General Imports\n2. Loading and Analysing the input data\n3. Feature Extraction and preparing the data\n4. Defining the model\n5. Training the Model + Accuracy check\n6. Testing and making predictions + Submission.\n\n*I will try to keep it to the basics and not overload with graphs and charts.*","81863dca":"### a. Using data load to shuffle the data during training process. \n\nThis will try to improve our model's learning process.","28db4879":"**e. Feature Extraction: FAMILY SIZE; IS_ALONE; IS_KID**\n\n- FAMILY_SIZE: Combination of SibSp (Siblings) and Parch (Parents\/Spouse) values to calculate the total family size.\n- IS_ALONE: If FAMILY_SIZE is 1 or 0, then the person is alone.\n- IS_KID: If the Age of the person is less than 16 yrs, is considered as a kid (more of a not an adult)","25d2ee98":"**Observation:** The above step just described the numerical fields of the notebook. You could easily notice the count of Age values is less than the rest of the numerical fields. This means the column: Age has missing values in them.\n\nIn the transformation step, we will address the missing values.","6866a1f4":"**Note:**\n\nAfter our initial analysis, we found that the columns and features which we would be interested in. The below are the list of columns that we have identified as part of this data preparation.\n\nFeatures:\n- PClass\n- Age\n- Family_Size\n- is_alone\n- is_kid\n- Sex\n- Embarked\n\nOutput:\n- Survived\n\nThe features can be improved and a lot can be done with the feature engineering. For the sake of this notebook, we have identified the above 7 features for training our model.","9ecc4fcf":"### c. Accuracy check","ddf03d34":"This model reached an accuracy of almost 82.68%. \nThe result could be improved using better feature engineering and better refined algorithms.\nThis is a sample notebook to help the community understand the entire flow.","318655c9":"Note: If you are using Cross Entropy loss you don't need to do a scaling of your dataset.","dd3bb029":"**d. Feature: SEX**","01d96d1b":"# Conclusion","3431bb2a":"**Note:** The test dataset is more of a prediction dataset. Donot confuse it with the train\/test split dataset. You will notice there is no `Survived` column in the test data. This is part of the competition to predict the Survivals.","25a7696e":"# 3. Feature extraction and preparing the data\n\nThis step will try to do a deepdive analysis and try to come up with new features.","f7df599b":"**c. Feature: PClass**","0fffd06e":"# 2. Loading and Analysing the Input data","e50305be":"a. Loading Training and Test files using Pandas to understand the data.","44a48e42":"**a. Feature: AGE**","769b9f09":"### a. Model definition","4f614d12":"### 3.3 Splitting the dataset for testing\n\nNote: This step splits the **training data** into 80\/20 train\/test split. This will help us identify the accuracy. Do not confuse it with the test dataset.","35243eec":"### 3.2 Categorization and selecting the features","adaf45af":"### b. Training the model"}}