{"cell_type":{"62718a0c":"code","50e33430":"code","3eb5121e":"code","a40a87f3":"code","90b89205":"code","3b33b31d":"code","4cdb4f31":"code","01f711e0":"code","4b9df2a9":"code","c529f0a6":"code","17512f62":"code","a34ca09d":"code","2fc24510":"code","5d4f83a0":"code","ffb399c2":"code","89472bbc":"code","8cc8cd3a":"code","81eb80ea":"code","95f4457b":"code","09813df1":"code","e5bdfe6a":"code","525d1d9d":"code","40db4524":"code","bedc9adb":"code","2d1ad2d7":"code","ff8daba7":"code","86c65ce1":"code","34e020d4":"code","819be227":"code","0ac7fd09":"code","792c30e0":"code","96428d3d":"code","08b04570":"code","45463f3b":"code","e27da19c":"code","a170be0d":"code","447acdcc":"code","22ad3b01":"code","32a0ae54":"code","9bfcf98b":"code","94f8fd70":"code","0c9191c0":"code","2ecc0eb5":"code","75021db6":"code","afcaab5a":"code","ec7e5ddf":"code","882714d9":"code","835f3999":"code","4762153d":"code","0aeb5948":"code","9ca09b2c":"markdown","f5cbfd79":"markdown","acba62e7":"markdown","9f83d367":"markdown","035a3f21":"markdown","edf2306b":"markdown","e022b0f3":"markdown","184322b9":"markdown","7fb1c4dc":"markdown","ca087234":"markdown","60c74ebd":"markdown","1b837d29":"markdown","72e57121":"markdown","894ee176":"markdown","9d7fff03":"markdown","0b7a41a7":"markdown","8bb30c21":"markdown","9086af56":"markdown","e5743edc":"markdown","efbd688b":"markdown","09e6ffa4":"markdown","03b6f6e6":"markdown","8391bdef":"markdown","3eca0738":"markdown","b3238857":"markdown","7c01f66c":"markdown","8bde459e":"markdown","2580065a":"markdown","e0bb2520":"markdown","bc7e7e8d":"markdown","697b2952":"markdown","ed25f2f2":"markdown"},"source":{"62718a0c":"#Data Structures\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\n\n### For installing missingno library, type this command in terminal\n#pip install missingno\n\nimport missingno as msno\n\n#Sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n#Plotting\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n#Others\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","50e33430":"#COMMENT THIS SECTION INCASE RUNNING THIS NOTEBOOK LOCALLY\n\n#Checking the kaggle paths for the uploaded datasets\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3eb5121e":"#INCASE RUNNING THIS LOCALLY, PASS THE RELATIVE PATH OF THE CSV FILES BELOW\n#(e.g. if files are in same folder as notebook, simple write \"train.csv\" as path)\n\ndata = pd.read_csv(\"..\/input\/ml-lab-i-c33\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/ml-lab-i-c33\/test.csv\")\nsample = pd.read_csv(\"..\/input\/ml-lab-i-c33\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/ml-lab-i-c33\/data_dictionary.csv\")\n\nprint(data.shape)\nprint(unseen.shape)\nprint(sample.shape)\nprint(data_dict.shape)","a40a87f3":"data_dict","90b89205":"ids = ['id','circle_id']\ntotal_amounts = [i for i in list(data.columns) if re.search('total.+amt',i)]\ntotal_outgoing_minutes = [i for i in list(data.columns) if re.search('total.+og.+mou',i)]\noffnetwork_minutes = [i for i in list(data.columns) if re.search('offnet',i)]\naverage_revenue_3g = [i for i in list(data.columns) if re.search('arpu.+3g',i)]\naverage_revenue_2g = [i for i in list(data.columns) if re.search('arpu.+2g',i)]\nvolume_3g = [i for i in list(data.columns) if re.search('vol.+3g',i)]\nvolume_2g = [i for i in list(data.columns) if re.search('vol.+2g',i)]\nage_on_network = [i for i in list(data.columns) if re.search('aon',i)]\n\n#Storing them in a single flat list\nvariables = [*ids, \n             *total_amounts, \n             *total_outgoing_minutes, \n             *offnetwork_minutes, \n             *average_revenue_3g, \n             *average_revenue_2g,\n             *volume_3g,\n             *volume_2g,\n             *age_on_network, \n             'churn_probability']\n\ndata = data[variables].set_index('id')","3b33b31d":"data.head()","4cdb4f31":"data.info(verbose=1)","01f711e0":"data.describe(include=\"all\")","4b9df2a9":"data['circle_id'].unique()","c529f0a6":"X = data.drop(['circle_id'],1).iloc[:,:-1]\ny = data.iloc[:,-1]\n\nX.shape, y.shape","17512f62":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a34ca09d":"X_train.head()","2fc24510":"msno.bar(X_train)","5d4f83a0":"msno.matrix(X_train)","ffb399c2":"missing_data_percent = 100*X_train.isnull().sum()\/len(y_train)\nmissing_data_percent","89472bbc":"new_vars = missing_data_percent[missing_data_percent.le(40)].index\nnew_vars","8cc8cd3a":"X_train_filtered = X_train[new_vars]\nX_train_filtered.shape","81eb80ea":"missing_data_percent = X_train_filtered.isnull().any()\nimpute_cols = missing_data_percent[missing_data_percent.gt(0)].index\nimpute_cols","95f4457b":"imp = SimpleImputer(strategy='constant', fill_value=0)\nX_train_filtered[impute_cols] = imp.fit_transform(X_train_filtered[impute_cols])","09813df1":"msno.bar(X_train_filtered)","e5bdfe6a":"X_train_filtered.describe()","525d1d9d":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train_filtered)","40db4524":"def cap_outliers(array, k=3):\n    upper_limit = array.mean() + k*array.std()\n    lower_limit = array.mean() - k*array.std()\n    array[array<lower_limit] = lower_limit\n    array[array>upper_limit] = upper_limit\n    return array","bedc9adb":"X_train_filtered1 = X_train_filtered.apply(cap_outliers, axis=0)\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train_filtered1)","2d1ad2d7":"scale = StandardScaler()\nX_train_filtered2 = scale.fit_transform(X_train_filtered1)","ff8daba7":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = pd.DataFrame(X_train_filtered2, columns=new_vars))","86c65ce1":"plt.figure(figsize=(10,8))\nsns.heatmap(pd.DataFrame(X_train_filtered2, columns=new_vars).corr())","34e020d4":"#Distribution for the churn probability\nsns.histplot(y_train)","819be227":"rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrf.fit(X_train_filtered2, y_train)","0ac7fd09":"feature_importances = pd.DataFrame({'col':new_vars, 'importance':rf.feature_importances_})","792c30e0":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nplt.bar(feature_importances['col'], feature_importances['importance'])","96428d3d":"pca = PCA()\npca_components = pca.fit_transform(X_train_filtered2)\nsns.scatterplot(x=pca_components[:,0], y=pca_components[:,1], hue=y_train)","08b04570":"sns.scatterplot(x=pca_components[:,1], y=pca_components[:,2], hue=y_train)","45463f3b":"rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nrf.fit(pca_components, y_train)\n\nfeature_importances = pd.DataFrame({'col':['component_'+str(i) for i in range(16)], \n                                    'importance':rf.feature_importances_})\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nplt.bar(feature_importances['col'], feature_importances['importance'])","e27da19c":"lr = LogisticRegression(max_iter=1000, tol=0.001, solver='sag')\nlr.fit(pca_components[:,:2], y_train)","a170be0d":"lr.score(pca_components[:,:2], y_train)","447acdcc":"imp = SimpleImputer(strategy='constant', fill_value=0)\nscale = StandardScaler()\npca = PCA(n_components=10)\nlr = LogisticRegression(max_iter=1000, tol=0.001)","22ad3b01":"pipe = Pipeline(steps = [('imputation',imp),\n                         ('scaling',scale),\n                         ('pca',pca),\n                         ('model',lr)])","32a0ae54":"pipe.fit(X_train[new_vars], y_train)","9bfcf98b":"train_score = pipe.score(X_train[new_vars], y_train)\nprint(\"Training accuracy:\", train_score)","94f8fd70":"test_score = pipe.score(X_test[new_vars], y_test)\nprint(\"Test accuracy:\", test_score)","0c9191c0":"confusion_matrix(y_train, pipe.predict(X_train[new_vars]))","2ecc0eb5":"confusion_matrix(y_test, pipe.predict(X_test[new_vars]))","75021db6":"precision_score(y_test, pipe.predict(X_test[new_vars]))","afcaab5a":"recall_score(y_test, pipe.predict(X_test[new_vars]))","ec7e5ddf":"sample.head()","882714d9":"unseen.head()","835f3999":"submission_data = unseen.set_index('id')[new_vars]\nsubmission_data.shape","4762153d":"unseen['churn_probability'] = pipe.predict(submission_data)\noutput = unseen[['id','churn_probability']]\noutput.head()","0aeb5948":"output.to_csv('submission_pca_lr_13jul.csv',index=False)","9ca09b2c":"At this step, you can create a bunch of features based on business understanding, such as \n1. \"average % gain of 3g volume from month 6 to 8\" - (growth or decline of 3g usage month over month?)\n2. \"ratio of total outgoing amount and age of user on network\" - (average daily usage of a user?)\n3. \"standard deviation of the total amount paid by user for all services\" - (too much variability in charges?)\n4. etc..\n\nAnother way of finding good features would be to project them into a lower dimensional space using PCA. PCA creates components which are a linear combination of the features. This then allows you to select components which explain the highest amount of variance.\n\nLets try to project the data onto 2D space and plot. **Note:** you can try TSNE, which is another dimensionality reduction approach as well. Check https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html for moree details.","f5cbfd79":"Let's also summarize the features using the df.describe method:","acba62e7":"# 2. Create X, y and then Train test split\n\nLets create X and y datasets and skip \"circle_id\" since it has only 1 unique value","9f83d367":"For the purpose of this **starter notebook**, we I will restrict the dataset to only a small set of variables. \n\nThe approach I use here is to understand each Acronym, figure our what variable might be important and filter out variable names based on the combinations of acrynoms using REGEX. So, if I want the total minutes a person has spent on outgoing calls, I need acronyms, TOTAL, OG and MOU. So corresponding regex is ```total.+og.+mou```","035a3f21":"# 4. Exploratory Data Analysis & Preprocessing\n\nLets start by analysing the univariate distributions of each feature.","edf2306b":"Splitting train and test data to avoid any contamination of the test data","e022b0f3":"Let's make a confusion matrix to analyze how each class is being predicted by the model.","184322b9":"The goal of this notebook is to provide an overview of how write a notebook and create a submission file that successfully solves the churn prediction problem. Please download the datasets, unzip and place them in the same folder as this notebook.\n\nWe are going to follow the process called CRISP-DM.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/639px-CRISP-DM_Process_Diagram.png\" style=\"height: 400px; width:400px;\"\/>\n\nAfter Business and Data Understanding via EDA, we want to prepare data for modelling. Then evaluate and submit our predictions.","7fb1c4dc":"The submission file should contain churn_probability values that have to be predicted for the unseen data provided (test.csv)","ca087234":"# 5. Feature engineering and selection\n\nLet's understand feature importances for raw features as well as components to decide top features for modelling.","60c74ebd":"# 1. Loading dependencies & datasets\n\nLets start by loading our dependencies. We can keep adding any imports to this cell block, as we write mode and mode code.","1b837d29":"You can now take this file and upload it as a submission on Kaggle.","72e57121":"Finally, lets create a csv file out of this dataset, ensuring to set index=False to avoid an addition column in the csv.","894ee176":"Let's also check which of the components have high feature importances towards the end goal of churn prediction.","9d7fff03":"### 4.1 Handling outliers\n\nThe box plots of these features show there a lot of outliers. These can be capped with k-sigma method.","0b7a41a7":"Lets first select the columns that we want to work with (or create them, if you have done any feature engineering)","8bb30c21":"Next, lets create a new column in the unseen dataset called churn_probability and use the model pipeline to predict the probabilities for this data","9086af56":"Next, we load our datasets and the data dictionary file.\n\nThe **train.csv** file contains both dependent and independent features, while the **test.csv** contains only the independent variables. \n\nSo, for model selection, I will create our own train\/test dataset from the **train.csv** and use the model to predict the solution using the features in unseen test.csv data for submission.","e5743edc":"### 4.2 Feature scaling\n\nLets also scale the features by scaling them with Standard scaler (few other alternates are min-max scaling and Z-scaling).","efbd688b":"Next, we try imputation on variables with any amount of missing data still left. There are multiple ways of imputing data, and each will require a good business understanding of what the missing data is and how you may handle it.\n\nSome tips while working with missing data - \n\n1. Can simply replace missing values directly with a constant value such as 0\n2. In certain cases you may want to replace it with the average value for each column respectively\n3. For timeseries data, you may consider using linear or spline interplolation between a set of points, if you have data available for some of the months, and missing for the others.\n4. You can consider more advance methods for imputation such as MICE.\n\nIn our case, I will just demostrate a simple imputation with constant values as zeros.","09e6ffa4":"Lets also calculate the % missing data for each column:","03b6f6e6":"# 3. Handling Missing data\n\nFirst lets analyse the missing data. We can use missingno library for quick visualizations.","8391bdef":"You can perform feature transformations at this stage. \n\n1. **Positively skewed:** Common transformations of this data include square root, cube root, and log.\n2. **Negatively skewed:** Common transformations include square, cube root and logarithmic.\n\nPlease read the following link to understand how to perform feature scaling and preprocessing : https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n \nLets also plot the correlations for each feature for bivariate analysis.","3eca0738":"We can see a high amount of type 2 error. Due to class imbalance, the model is clearly trying to predict majority of the cases as class 0. Understanding how to handle class imbalance in classification models might be the key to winning this competition :) (hint!)","b3238857":"1. Lets analyze the data dictionary versus the churn dataset.\n2. The data dictonary contains a list of abbrevations which provide you all the information you need to understand what a specific feature\/variable in the churn dataset represents\n3. Example: \n\n> \"arpu_7\" -> Average revenue per user + KPI for the month of July\n>\n> \"onnet_mou_6\" ->  All kind of calls within the same operator network + Minutes of usage voice calls + KPI for the month of June\n>\n>\"night_pck_user_8\" -> Scheme to use during specific night hours only + Prepaid service schemes called PACKS + KPI for the month of August\n>\n>\"max_rech_data_7\" -> Maximum + Recharge + Mobile internet + KPI for the month of July\n\nIts important to understand the definitions of each feature that you are working with, take notes on which feature you think might impact the churn rate of a user, and what sort of analysis could you do to understand the distribution of the feature better.","7c01f66c":"Since too much missing information would make a column not really a great predictor for churn, we drop these columns and keep only the ones which have less than 40% missing data.","8bde459e":"# Telecom Churn Prediction - Starter Notebook\n\n**Author:** Akshay Sehgal (www.akshaysehgal.com)","2580065a":"Let's look at each variable's datatype:","e0bb2520":"# 0. Problem statement\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business\ngoal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n\nIn this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n\n**Customer behaviour during churn:**\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a\nperiod of time (this is especially applicable to high-value customers). In churn prediction, we\nassume that there are three phases of customer lifecycle :\n\n1. <u>The \u2018good\u2019 phase:<\/u> In this phase, the customer is happy with the service and behaves as usual.\n\n2. <u>The \u2018action\u2019 phase:<\/u> The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\n3. <u>The \u2018churn\u2019 phase:<\/u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month (September) is the \u2018churn\u2019 phase.","bc7e7e8d":"The model has 89.8% accuracy, but let's build a pipeline to fit and score the model faster.\n\nThe steps of this pipeline would be the following, but this is only one type of pipeline -\n1. Imputation\n2. Scaling\n3. PCA\n4. Classification model\n\nYou can change this pipeline, add addition transformations, change models, use cross validation or even use this pipeline to work with a Gridsearch.","697b2952":"# 6. Model building\n\nLet's build a quick model with logistic regression and the first 2 PCA components.","ed25f2f2":"# 7. Creating submission file\n\nFor submission, we need to make sure that the format is exactly the same as the sample.csv file. It contains 2 columns, id and churn_probability"}}