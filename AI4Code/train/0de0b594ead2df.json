{"cell_type":{"398bed86":"code","c54e24c2":"code","e384108b":"code","1dc1a553":"code","2a66496d":"code","e1a800d3":"code","86b881e5":"code","97093de8":"code","386ec55c":"code","4f9a5cf1":"code","e38a41a0":"code","54f244f8":"code","6d1c0432":"code","5cf1be7f":"code","2391e82f":"code","ed29b060":"code","e9f56ad1":"code","833cd893":"code","c29b344b":"code","8b4c00e2":"code","1f4dc13a":"code","a0421e5a":"code","c4c19c47":"code","e03cb73f":"code","a4a87de2":"markdown","088a0439":"markdown","ff7d6fa1":"markdown","1e730d06":"markdown","97ec8cfc":"markdown"},"source":{"398bed86":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","c54e24c2":"df=pd.read_csv('..\/input\/fake-news-data\/train.csv')","e384108b":"df.head()","1dc1a553":"#Dropping the NAN values\ndf=df.dropna()","2a66496d":"#Getting the Independant features\nX= df.drop('label', axis= 1)\n\n#Getting the dependant features\ny= df['label']","e1a800d3":"X.shape\n","86b881e5":"y.shape","97093de8":"import tensorflow as tf\ntf.__version__","386ec55c":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense #due to classification\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout","4f9a5cf1":"voc_size=10000","e38a41a0":"m= X.copy()\nm.reset_index(inplace= True)","54f244f8":"import nltk\nimport re\nfrom nltk.corpus import stopwords","6d1c0432":"from nltk.stem.porter import PorterStemmer\ncorpus=[]\nps= PorterStemmer()\n\nfor i in range(len(m)):\n    text= re.sub(r'\\[^a-zA-Z]', ' ', m['title'][i])\n    text= text.lower()\n    text= re.sub(r'\\d+', ' ', text)\n    text= re.sub(r'\\s+', ' ', text)\n    text= [ps.stem(word)for word in text if not word in stopwords.words('english')]\n    text= ''.join(text)\n    corpus.append(text)","5cf1be7f":"onehot= [one_hot(words, voc_size) for words in corpus]\nonehot","2391e82f":"sent_length= 20\nembed_docs= pad_sequences(onehot, padding= 'pre', maxlen= sent_length)\nembed_docs","ed29b060":"len(embed_docs)","e9f56ad1":"#Creating the model\n\nvector_features=40\nmodel= Sequential()\nmodel.add(Embedding(voc_size, vector_features, input_length=sent_length))\nmodel.add(Dropout(0.3))#adding the dropout layer\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))#classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","833cd893":"len(embed_docs), y.shape","c29b344b":"import numpy as np\nX_final=np.array(embed_docs)\ny_final=np.array(y)\nX_final.shape, y_final.shape","8b4c00e2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X_final, y_final, test_size=0.33, random_state=42)","1f4dc13a":"#Finally training\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)","a0421e5a":"y_pred= model.predict_classes(X_test)","c4c19c47":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)# for understaing the better classification and other accuracy parameters","e03cb73f":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","a4a87de2":"**Embedding representation**","088a0439":"Data preprocessing and cleaning -----------","ff7d6fa1":"# **Model Training**","1e730d06":"# **Onehot Representation**","97ec8cfc":" **Performance Metrics and Accuracy**"}}