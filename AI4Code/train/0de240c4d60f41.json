{"cell_type":{"087a3786":"code","252f09cd":"code","a28ceac4":"code","78cd0ce9":"code","5b75ea4e":"code","4f78356b":"code","fc965412":"code","f111339e":"code","1927d8bc":"code","47f76afd":"code","5fcdf176":"code","8a2cea6a":"code","4ba205e9":"code","6cd22105":"code","d3e5acbc":"code","68275e85":"code","f2cbcc2f":"code","feaa46f9":"code","0566eb00":"code","6c085f5c":"code","b68ec026":"code","4db44443":"code","f90947b5":"code","f8d8f6fc":"code","06184ed8":"code","1294722c":"code","ab49e670":"code","5653505f":"code","b494300f":"code","8e9e64b9":"code","29bda149":"code","867f5995":"code","5d5545c6":"code","9da8cbcd":"code","e5f3c1b2":"code","61c878b7":"code","6a5b863a":"code","50af241c":"code","4c919a8e":"code","f603f3d3":"code","06ca0bee":"code","5728143b":"code","ef0254f6":"code","81da3a48":"code","7aece3cb":"code","64025353":"code","1d413ba3":"code","19cf37bc":"code","4fdfac76":"code","40515973":"code","4a6ee2af":"code","0f22e144":"code","644c2837":"code","3f831906":"markdown","31124cdb":"markdown","136d7da9":"markdown","8a5e0342":"markdown","604cf7f2":"markdown","6e713031":"markdown","04d22eed":"markdown","07e95912":"markdown","8b8be60d":"markdown","d9b3d33c":"markdown","2488d9f2":"markdown","6edf8b91":"markdown","43d97e7a":"markdown","e31d65cd":"markdown","83d1c83e":"markdown","9ee5562c":"markdown","034ef125":"markdown","755e8ed9":"markdown","5aa952a0":"markdown","9a46cf62":"markdown","924b45b0":"markdown","7cd3f62c":"markdown","165bd1f1":"markdown","fea7ccc8":"markdown","0231082b":"markdown","9034db46":"markdown","0008ba53":"markdown","a769576f":"markdown","5d84cfd6":"markdown","ee53b5c1":"markdown","8edb6dba":"markdown","a094cd19":"markdown","eae56e66":"markdown","fbe78ca8":"markdown"},"source":{"087a3786":"# data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"darkgrid\")\n%matplotlib inline\n\n# machine learning\n#Let's import them as and when needed\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","252f09cd":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","a28ceac4":"train_data.head()","78cd0ce9":"test_data.head()","5b75ea4e":"train_data.info()\nprint('-'*50)\ntest_data.info()","4f78356b":"train_data.describe()","fc965412":"train_data.describe(include=['O'])","f111339e":"#drop the Cabin and Ticket columns in both dataset. We also don't need the PassengerId in training dataset.\n\ntrain_data.drop(labels = ['PassengerId', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest_data.drop(labels = ['Ticket', 'Cabin'], axis=1, inplace=True)","1927d8bc":"print('Training Data')\nprint(pd.isnull(train_data).sum())\nprint(\"-\"*50)\nprint('Testing Data')\nprint(pd.isnull(test_data).sum())","47f76afd":"sns.distplot(train_data['Age'].dropna())","5fcdf176":"train_data['Age'].fillna(train_data['Age'].median(), inplace= True)\ntest_data['Age'].fillna(test_data['Age'].median(), inplace= True)","8a2cea6a":"train_data['Embarked'].fillna(\"S\", inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace = True)","4ba205e9":"print('Training Data')\nprint(pd.isnull(train_data).sum())\nprint(\"-\"*50)\nprint('Testing Data')\nprint(pd.isnull(test_data).sum())","6cd22105":"train_data.head()","d3e5acbc":"test_data.head()","68275e85":"plt.figure(figsize=(8,4))\nplt.tight_layout()\nsns.barplot(x='Sex', y='Survived', data=train_data)\nplt.title('Distribution of Survival based on Gender')\nplt.show()","f2cbcc2f":"plt.figure(figsize=(8,4))\nplt.tight_layout()\nsns.barplot(x='Pclass', y='Survived', data=train_data)\nplt.title('Distribution of Survival based on Class')\nplt.show()","feaa46f9":"plt.figure(figsize=(12,6))\nplt.tight_layout()\nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=train_data)\nplt.title('Distribution of Survival based on Gender and Class')\nplt.show()","0566eb00":"g = sns.FacetGrid(train_data, col=\"Survived\", margin_titles=True)\ng.map(plt.hist, \"Age\", color=\"steelblue\", bins=20)","6c085f5c":"sns.swarmplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", palette=[\"r\", \"c\", \"y\"], data=train_data)","b68ec026":"sns.pairplot(train_data)","4db44443":"train_data.head()","f90947b5":"test_data.head()","f8d8f6fc":"train_data['Sex'] = train_data['Sex'].map( {'female': 1, 'male': 0}).astype(int)","06184ed8":"train_data.head()","1294722c":"test_data['Sex'] = test_data['Sex'].map( {'female': 1, 'male': 0}).astype(int)","ab49e670":"test_data.head()","5653505f":"train_data['Embarked'] = train_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2}).astype(int)\ntest_data['Embarked'] = test_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2}).astype(int)","b494300f":"train_data.head()","8e9e64b9":"test_data.head()","29bda149":"train_data[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","867f5995":"train_data['Single'] = train_data.FamilySize.apply(lambda x: 1 if x == 1 else 0)\ntest_data['Single'] = test_data.FamilySize.apply(lambda x: 1 if x == 1 else 0)","5d5545c6":"train_data.head()","9da8cbcd":"test_data.head()","e5f3c1b2":"train_data['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False) ##Regular Expression <3\ntest_data['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","61c878b7":"title_list = list(set(train_data['Title']))\ntitle_list","6a5b863a":"mix = [train_data, test_data] ## Just so save ourself some time and repeated code.\n\nfor dt in mix:\n    dt['Title'] = dt['Title'].replace(['Dr', 'Col', 'Sir', 'Countess', 'Jonkheer', 'Lady', 'Don', 'Capt', 'Major', 'Rev', \\\n                                      ], 'Unique')\n    dt['Title'] = dt['Title'].replace('Mlle', 'Miss')\n    dt['Title'] = dt['Title'].replace('Ms', 'Miss')\n    dt['Title'] = dt['Title'].replace('Mme', 'Mrs')","50af241c":"map_title = {'Mrs': 1, 'Miss': 2, 'Mr': 3, 'Master': 4, 'Unique': 5}\n\nfor dt in mix:\n    dt['Title'] = dt['Title'].map(map_title)\n    dt['Title'] = dt['Title'].fillna(0)\n    \ntrain_data.head()","4c919a8e":"train_data = train_data.drop(['Name', 'SibSp', 'Parch'], axis=1)\ntest_data = test_data.drop(['Name', 'SibSp', 'Parch'], axis=1)","f603f3d3":"train_data.head()","06ca0bee":"from sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","5728143b":"# to evaluate model performance, we can use the accuracy_score function.","ef0254f6":"X_train = train_data.drop(labels=['Survived'], axis=1)\ny_train = train_data['Survived']\nX_test = test_data.drop('PassengerId',axis=1).copy()","81da3a48":"X_train.shape,  y_train.shape, X_test.shape\n","7aece3cb":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\naccuracy_log = logreg.score(X_train, y_train)\nprint(\"The accuracy for the Logistic Regression is: \" + str(accuracy_log))","64025353":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\naccuracy_svc = svc.score(X_train, y_train)\nprint(\"The accuracy for Support Vector Machines is:\" + str(accuracy_svc))","1d413ba3":"# K Nearest Neighbours\n\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test)\naccuracy_knn = knn.score(X_train, y_train)\nprint(\"The accuracy of KNN is: \" + str(accuracy_knn))","19cf37bc":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test)\naccuracy_gnb = gaussian.score(X_train, y_train)\nprint(\"The accuracy of Gaussian Naive Bayes is: \" + str(accuracy_gnb))","4fdfac76":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\naccuracy_lsvc = linear_svc.score(X_train, y_train)\nprint(\"The accuracy of linear SVC is: \" + str(accuracy_lsvc))","40515973":"#Decision Tree\n\nd_tree = DecisionTreeClassifier()\nd_tree.fit(X_train, y_train)\nY_pred = d_tree.predict(X_test)\naccuracy_d_tree = d_tree.score(X_train, y_train)\nprint(\"The accuracy of Decision Tree is: \" + str(accuracy_d_tree))","4a6ee2af":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100) #tried with 80, 90, 100 no difference\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)\naccuracy_r_forest = random_forest.score(X_train, y_train)\nprint(\"The accuracy of Random Forest is: \" + str(accuracy_r_forest))","0f22e144":"model_performance = pd.DataFrame({\n    \"Model\": [\"Logistic Regression\", \"Support Vector Machines\", \"K Nearest Neighbours\", \"Gaussian Naive Bayes\",\n             \"Linear SVC\", \"Decision Tree\", \"Random Forest\"],\n    \"Accuracy\": [accuracy_log, accuracy_svc, accuracy_knn, accuracy_gnb, accuracy_lsvc, accuracy_d_tree, accuracy_r_forest]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","644c2837":"submission = pd.DataFrame({\n    \"PassengerId\": test_data[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\n\n#submission.to_csv(\"..\/output\/titanic.csv\", index=False)","3f831906":"### References: \n* [startupsci](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Blog](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)","31124cdb":"Let's see the status of null values now.","136d7da9":"Since, We know from previous steps that \"S\" is the most Embarked port. Let's fill the null values in Embarked with \"S\" port.","8a5e0342":"## 5. Actions based on assumptions","604cf7f2":"Let's see the number of null values now.","6e713031":"#### Age columns seems to have null values.\n\nWe will look at the distribution of Age column to see if it's skewed or symmetrical. This will help us to determine what values to replaec wit NaN values.","04d22eed":"## 9. Build Model and Make Prediction\n\nWe are all set to build our model. I will be using different models and will select the one with the best accuracy.\n\n\n\n#### Import all the sklearn models to test","07e95912":"## 8. Create New Features\n\nSometimes in a dataset individual features might be of no use. But, if you create a new features using them, then you might be able to get some more insights into the problem. But, who knows. So, Let's try it out.\n\nAfter reading on kaggle discussions, I came to know that we can combine \"SibSp\" and \"Parch\" and make a new feature named \"FamilySize\". Since, People who have had family might have risked their life to search for their family.\n\nAlso, We can make a new feature for people who were alone on the ship. We can name it as \"Single\" or \"IsAlone\" or whatever you like.","8b8be60d":"# Using Machine Learning to Predict Titanic Survivors\n\nThis is my first submission for a kaggle competition. I will try to keep things simple and will explain things on the go. In  this notebook I will build a machine learning model using sklearn to predict the outcomes of each passenger aboard titanic. I will build this model step by step. This kernel will help people who are getting started with data visualization, analysis and machine learning.\n\n\n#### If you like my work. Please, leave an upvote, the upvote will me to help to contribute more  and more to the community.\n\n#### Please leave your valuable  suggestions in the comments section.\n\n#### Follow me for even better kernels than this one.\n\n\n\n## What type of problem is this one?\n\nSince we have to classify passengers as either survived, or not survived. Hence, This is a supervised classification machine learning problem.\n\n\n## Content \n\nThe file 'train.csv' contains 12 columns and - rows. Each row containes details of individual passenger onboard. The columns  are: \n\n1. PassengerId - type should be integers\n2. Survived - Survived or Not\n3. PclassClass - of Travel\n4. Name - Name of Passenger\n5. Sex - Gender\n6. Age - Age of passenger\n7. SibSp - Number of Sibling\/Spouse abord\n8. Parch - Number of Parent\/Child abord\n9. Ticket\n10. Fare\n11. Cabin\n12. EmbarkedThe port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown\n\n\n## Version\n\nVersion 1.0\n\n\n\n\n## Index of Content\n\n1. Importing packages\n2. Importing dataset\n3. Analysing dataset\n4. Assumptions based on data analysis done so far\n5. Actions based on assumptions\n6. Visualizing by plotting data\n7. Feature Engineering\n8. Creating New Features\n9. Build Model and Make Predictions\n10. Compare Model Performances\n","d9b3d33c":"The data is not fit to feed into machine learning model. We have to clean it. ","2488d9f2":"We can change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. ","6edf8b91":"Let's have a look at our dataset.","43d97e7a":"\n\n\n## 1. Importing Modules","e31d65cd":"Let's replace title with common name and the unique ones as unique.\n\nLet's also combine the dataset into an array so that we can perform operations on both dataset using a loop.","83d1c83e":"So, The first class people were more likely to survive. ","9ee5562c":"Clearly, Women were the top survivors.\n\nAlso, Gender is a good feature to use for our machine learning model. But, we have to engineer it before feeding into our model.","034ef125":"Points to be taken from these categorical features:\n\n-  No person with same name.\n-  Sex variable have two possible values with 65% male (top=male, freq= 577\/count=891)\n-  Cabin values have several duplicates as many passengers shared cabin.\n-  S port is used most among the three possible values.\n-  Ticket feature has 681 unique values i.e.: 22% duplicate ratio","755e8ed9":"## 4. Assumptions based on data analysis done so far:\n\n#### Based on our data analysis so far. We can state that:\n\n-  Ticket Feature contains 22% duplicates and can be dropped. As there may not be a correlation b\/w Ticket and Survival\n-  Cabin feature can be dropped as it is highly incomplete and contains null values\n-  We can drop PassengerId as it does not contribute to survival\n-  We can create a new feature called Title for Name feature\n-  We can create a new feature called FamilySize based on Parch and SibSp to get total count of family members on board\n-  We should complete Age feature as it is directly correlated to survival\n-  Women might have been more likely to have survived\n-  Children below some certain age were also more likely to have survived\n-  The upper class passengers were more likely to have survived","5aa952a0":"## 10. Compare Model Performances\n\nWe have done predictions with many models. Now, we should see which model performed the best.","9a46cf62":"As it is evident from the graph. The distribution is slightly skewed right. That's why we will fill the null values with median for better accuracy.\n\nI know you have questions here. Ask me in the comments section. :)","924b45b0":"Nice!\n\nWe can now drop \"Name\", \"SibSp\" and \"Parch\" column.","7cd3f62c":"As evident from the these distributions that younger people were more likely to survive than those of older people. Also, There were more females who survived.\n\nLet's drap a pairplot to see possible relatins between all the features.","165bd1f1":"So, Class also plays an important role in surival of the passengers. 1st Class were more likely to survive than other classes passengers.","fea7ccc8":"## 7. Feature Engineering\n\nCategorical features needs to be represented as numerical values before we feed it into the machine learning model. \"Sex\" and \"Embarked\" columns needs to be engineered.","0231082b":"Points to be taken from these numerical features:\n\n-  Total samples are 891 i.e. 40% of the actual number of passengers on board the Titanic(2224)\n-  Survived is a categorical feature with 0 or 1 values.\n-  Most expensive ticket is $512","9034db46":"## 6. Visualizing by Plotting Data\n\nVisualizing the data is important to see the trends and general associations of Variables. We can make different kinds of graphs for the features we want to work with.","0008ba53":"Let's define features in Training\/Test set","a769576f":"Let's do the same with Embarked column. Since, We already took care of the NaN values then we just have to convert the categorical port feature to numerical port feature.","5d84cfd6":"## 3. Analysing the dataset\n\nWe will use pandas for this.","ee53b5c1":"## 2. Importing dataset\n\nWe will use Python Pandas package to import the dataset and play with it.","8edb6dba":"After seeing these head of the dataset. We can comment on categorical features, numerical features, data types of features, typos in features and null or empty features.\n\n#### Categorical features: \nA categorical variable (sometimes called a nominal variable) is one that has two or more categories, but there is no intrinsic ordering to the categories. This is further classified as nominal, ordinal, ratio, or interval based.\n-  Categorical: Survived, Sex and Embarked\n-  Ordinal: Pclass\n\n#### Numerical Features:\nAs the name suggest, these values are numerical in nature and changes from sample to sample. This is further classified as discrete, continuous, or timeseries based.\n-  Continuous: Age, Fare\n-  Discrete: SibSp, Parch\n\n#### Data types of features:\n\n-  Seven features are integer or floats. Six in case of test dataset\n-  Five features are strings (object)\n-  MIXED DATA TYPES: Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric\n\n#### Featurs with error and typos:\n-  Name feature may contain errors or typos ass there are several ways to write a name\n\n#### Null or Empty Features:\n-  Age, Cabin and Embarked containes a lot of null values for the training dataset","a094cd19":"Everything looks out numeric except the name column. Let's engineer the name column also.\n\nSince,name column is not used. We can extract the title from the names and then encode them in to numeric values. Then we can use name column to predict the outcome.","eae56e66":"Yes!! We got rid of missing values. Now let's see our cleaned data.","fbe78ca8":"It's is clear that both Decision Tree and Random Forest score the same. We choose to use the Random Forest as they tend not to overfit as decision tree."}}