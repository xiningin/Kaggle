{"cell_type":{"3892903e":"code","1024b256":"code","d53decf5":"code","1e85e4db":"code","c191c8e0":"code","e8f43529":"code","66a56479":"code","52ed734b":"code","add49fea":"code","ed280754":"code","3da79aaf":"code","c3e6d597":"code","5e97df1d":"code","b17ea3d0":"code","078233f0":"code","f87a02c5":"code","6fe7f751":"code","b258c35a":"code","d69c3077":"code","26cc94e1":"code","0d75783b":"code","6d8afe2a":"code","d2290403":"code","83cabb60":"code","e4ceaf3e":"code","1a614532":"code","8591d40a":"code","0b99fcf3":"code","e5f32c83":"code","69daf119":"code","bf698cea":"code","26fa5537":"code","b09ad954":"code","5e68b448":"code","59e74896":"code","e2def431":"code","2b2451ba":"code","f37ddcde":"code","990b10db":"code","2e27c2df":"code","dd20517b":"code","79d42820":"code","62476f11":"code","1e165132":"code","473f8a7c":"code","80860445":"code","17906adf":"code","d08cad72":"code","a51e5da1":"code","d5b4c1b2":"code","65d1ebe6":"code","0321438d":"code","17bbca66":"code","1ca4d793":"code","4ca1fe38":"code","96737197":"code","413c56e5":"code","f6ecdb5d":"code","48245831":"code","c12ecebe":"code","00a329d6":"code","7a3e56c4":"code","1cd841bc":"code","c620bf86":"code","37894e90":"code","ffa37466":"code","a98cc68f":"code","ade90925":"code","7944d68a":"code","45603dc8":"code","81b35d15":"code","a1d181da":"code","1251b817":"code","6a616e22":"code","49759743":"code","69b6a17d":"code","15984fce":"code","0e451904":"code","088d7a53":"code","851d5047":"code","7a8c2bf6":"code","fd26ce59":"code","116cbdb0":"code","29907c1a":"code","301f100c":"code","a95d3f7e":"code","7797da28":"code","96041f1e":"code","e4b55d4e":"code","83a15cf6":"code","05204b7b":"code","9a3a7285":"code","44e26218":"code","879a22f8":"code","3fddc1fd":"code","dae23f03":"code","6056b72c":"code","7d3aa977":"code","6810daf1":"code","92ecbd48":"code","3905de6d":"code","1bc8a03b":"code","3a2ca4bb":"code","4dbe39d8":"code","b48b25e1":"code","1ff0871b":"code","aaf2f366":"code","77e8ae7d":"code","d9a9f0d8":"code","26fa0ecb":"code","29a6166c":"code","70a1c127":"code","0835c865":"code","b6e74a85":"code","b1ff1747":"code","b61da98a":"code","8d8bd31a":"markdown","8d178584":"markdown","a9142763":"markdown","533571e9":"markdown","c1379bf6":"markdown","f4c2bc72":"markdown","923484ec":"markdown","1bb631f1":"markdown","7b732b53":"markdown","97e8c280":"markdown","593ce776":"markdown","18ff62b1":"markdown","206c9245":"markdown","4aa0bfdd":"markdown","648511bc":"markdown","97f66f99":"markdown","0d57bc2c":"markdown","646838af":"markdown","8d150b7e":"markdown","bea9e595":"markdown","d0dd51c4":"markdown","707e74fb":"markdown","81f1fb9d":"markdown","a740c36c":"markdown","80a24e70":"markdown"},"source":{"3892903e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt","1024b256":"data = 'https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-03-churn-prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv'","d53decf5":"!wget $data -O data-week-3.csv ","1e85e4db":"df = pd.read_csv('data-week-3.csv')\ndf.head()","c191c8e0":"df.columns = df.columns.str.lower().str.replace(' ', '_')\n\ncategorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower().str.replace(' ', '_')","e8f43529":"df.dtypes","66a56479":"# This allows us to look at the dataset columns instead. \ndf.head().T","52ed734b":"tc = pd.to_numeric(df.totalcharges, errors='coerce')","add49fea":"df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')","ed280754":"df.totalcharges = df.totalcharges.fillna(0)","3da79aaf":"df.churn.head()","c3e6d597":"# Converting no\/yes into 0\/1 aka True or False but numerically \ndf.churn = (df.churn == 'yes').astype(int)","5e97df1d":"# Recall from Notebook 2 that we did splits manually. With train_test_split, we can do it easily :) \nfrom sklearn.model_selection import train_test_split","b17ea3d0":"# Recall that df_full_train consists of your training and validation dataset \ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n\n# Further splitting of training and val data \ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)","078233f0":"len(df_train), len(df_val), len(df_test)","f87a02c5":"# Resetting index of the 3 datasets \ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","6fe7f751":"# Setting the y values.\ny_train = df_train.churn.values\ny_val = df_val.churn.values\ny_test = df_test.churn.values\n\ndel df_train['churn']\ndel df_val['churn']\ndel df_test['churn']","b258c35a":"df_full_train = df_full_train.reset_index(drop=True)","d69c3077":"df_full_train.isnull().sum()","26cc94e1":"df_full_train.churn.value_counts()","0d75783b":"# The normalize parameter is used so that we're able to see the percentage instead of the value counts\ndf_full_train.churn.value_counts(normalize=True)","6d8afe2a":"df_full_train.churn.mean()","d2290403":"numerical = ['tenure', 'monthlycharges', 'totalcharges']","83cabb60":"categorical = [\n    'gender',\n    'seniorcitizen',\n    'partner',\n    'dependents',\n    'phoneservice',\n    'multiplelines',\n    'internetservice',\n    'onlinesecurity',\n    'onlinebackup',\n    'deviceprotection',\n    'techsupport',\n    'streamingtv',\n    'streamingmovies',\n    'contract',\n    'paperlessbilling',\n    'paymentmethod',\n]","e4ceaf3e":"# The nunique() functions shows how many unique values in a specfic category or specific categories.\ndf_full_train[categorical].nunique()","1a614532":"df_full_train.head()","8591d40a":"# We can look at churn rate within different groups. For example, we can use gender\nchurn_female = df_full_train[df_full_train.gender == 'female'].churn.mean()\nchurn_female","0b99fcf3":"churn_male = df_full_train[df_full_train.gender == 'male'].churn.mean()\nchurn_male","e5f32c83":"global_churn = df_full_train.churn.mean()\nglobal_churn","69daf119":"# Not much difference for female, as we can see that there is only a 0.6% difference. \nglobal_churn - churn_female","bf698cea":"# Same for male \nglobal_churn - churn_male","26fa5537":"# We can also use the 'partner' group \ndf_full_train.partner.value_counts()","b09ad954":"churn_partner = df_full_train[df_full_train.partner == 'yes'].churn.mean()\nchurn_partner","5e68b448":"# Oh wow, the difference is huge! 6% of the dataset.\nglobal_churn - churn_partner","59e74896":"churn_no_partner = df_full_train[df_full_train.partner == 'no'].churn.mean()\nchurn_no_partner","e2def431":"# 6%, still quite a big difference in percentage. \nglobal_churn - churn_no_partner","2b2451ba":"churn_no_partner \/ global_churn","f37ddcde":"churn_partner \/ global_churn","990b10db":"from IPython.display import display","2e27c2df":"for c in categorical:\n    # Printing category via for loop aka point 1\n    print(c)\n    \n    # We create a new dataframe, including the churn rate and the churn count\n    df_group = df_full_train.groupby(c).churn.agg(['mean', 'count'])\n    \n    # Then, we create a new column 'diff' and measure the churn rate difference\n    df_group['diff'] = df_group['mean'] - global_churn\n    \n    #Lastly, we create another new column 'risk' and measure the risk ratio. \n    df_group['risk'] = df_group['mean'] \/ global_churn\n    \n    #Formatting things\n    display(df_group)\n    print()\n    print()","dd20517b":"from sklearn.metrics import mutual_info_score","79d42820":"# The value can tell us that there is some relation between the 'churn' variable and the 'contract' variable\nmutual_info_score(df_full_train.churn, df_full_train.contract)","62476f11":"# However, this values shows us that there is little to no relation at all. \nmutual_info_score(df_full_train.gender, df_full_train.churn)","1e165132":"# The order of these factors do not matter.\nmutual_info_score(df_full_train.contract, df_full_train.churn)","473f8a7c":"mutual_info_score(df_full_train.partner, df_full_train.churn)","80860445":"# MI scores between 'churn' column and other 'groups'\ndef mutual_info_churn_score(series):\n    return mutual_info_score(series, df_full_train.churn)","17906adf":"# .apply() allows us to apply a function to the dataframe\/dataset itself\n# in this case, we apply the previous function above to get the MI scores. \nmi = df_full_train[categorical].apply(mutual_info_churn_score)\n\nmi.sort_values(ascending=False)","d08cad72":"# We are using tenure for this case\ndf_full_train.tenure.max()","a51e5da1":"# These are the correlations between the numerical variables and the churn variable.\ndf_full_train[numerical].corrwith(df_full_train.churn).abs()","d5b4c1b2":"# We will be exploring tenure first.\ndf_full_train[df_full_train.tenure <= 2].churn.mean()","65d1ebe6":"df_full_train[(df_full_train.tenure > 2) & (df_full_train.tenure <= 12)].churn.mean()","0321438d":"df_full_train[df_full_train.tenure > 12].churn.mean()","17bbca66":"df_full_train[df_full_train.monthlycharges <= 20].churn.mean()","1ca4d793":"df_full_train[(df_full_train.monthlycharges > 20) & (df_full_train.monthlycharges <= 50)].churn.mean()","4ca1fe38":"df_full_train[df_full_train.monthlycharges > 50].churn.mean()","96737197":"from sklearn.feature_extraction import DictVectorizer","413c56e5":"# Here's some code to make more sense of the process first. \n# We have categorical variables like male & female in gender, month-to-month, one_year & two_year in contract.\ndf_train[['gender','contract']].iloc[:10]","f6ecdb5d":"# We are using a dictionary vectorizer to make vectors out of a dictionary.\n# However, the dictionary has to be saved such that each input consists of the gender and contract.\n# Basically not like the code below.\ndf_train[['gender','contract']].iloc[:10].to_dict()","48245831":"# This is how we would have to save it, with the orient 'records'.\ntest = df_train[['gender','contract']].iloc[:100].to_dict(orient = 'records')\n\n# Feel free to delete the # to see the dictionary in its proper form.\n# test","c12ecebe":"# Fitting and transforming the data\ndv = DictVectorizer(sparse=False)\ntest = dv.fit_transform(test)","00a329d6":"# If you want to take a look at the array after it has been fit and transformed\n# test","7a3e56c4":"# The features are in order, aka month-to-month is the first column, one_year is the second column, etc.\ndv.get_feature_names()","1cd841bc":"# Let's add in a numerical column to see what happens when we vectorize it\ntest_2 = df_train[['gender','contract', 'tenure']].iloc[:100].to_dict(orient = 'records')","c620bf86":"test_2 = dv.fit_transform(test_2)\ntest\n\n# We can tell from the output that nothing really happens to it.\n# Therefore, we can conclude that nothing will happen to numerical variables once we vectorise it! ","37894e90":"train_dict = df_train[categorical + numerical].to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\nval_dict = df_val[categorical + numerical].to_dict(orient='records')\nX_val = dv.transform(val_dict)","ffa37466":"test_3 = df_train[['gender','contract']].iloc[:100]\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown = 'ignore')\ntest_3 = enc.fit_transform(test_3)","a98cc68f":"test_3_display = pd.DataFrame(test_3.toarray())\ntest_3_display","ade90925":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","7944d68a":"# linspace() gives 51 evenly spaced out numbers from the range (-7, 7).\nz = np.linspace(-7, 7, 51)","45603dc8":"# Very extreme values usually lie towards 0 or 1, depending on the positivity\/negativity of the number\n# You can tell from -10 onwards whereby the value, when rounded to 5 decimal places, leads to 0.\n# Same goes for 10 onwards, whereby the value leads to 1.\n\nvalues = [-1000, -100, -50, -10, -7.5, -5, -3, -1, 0, 1, 3, 5, 7.5, 10, 50, 100, 1000]\n\nfor value in values:\n    print(round(sigmoid(value),5))","81b35d15":"plt.plot(z, sigmoid(z))","a1d181da":"def linear_regression(xi):\n    result = w0\n    \n    for j in range(len(w)):\n        result = result + xi[j] * w[j]\n        \n    return result","1251b817":"# Logistic Regressions works just like linear regression WITH the sigmoid function applied to the scores.\ndef logistic_regression(xi):\n    score = w0\n    \n    for j in range(len(w)):\n        score = score + xi[j] * w[j]\n        \n    result = sigmoid(score)\n    return result","6a616e22":"from sklearn.linear_model import LogisticRegression","49759743":"model = LogisticRegression(solver='lbfgs')\n\n# solver='lbfgs' is the default solver in newer version of sklearn\n# for older versions, you need to specify it explicitly\nmodel.fit(X_train, y_train)","69b6a17d":"# Bias \nmodel.intercept_[0]","15984fce":"# Weights\nmodel.coef_[0].round(3)","0e451904":"# Hard predictions\nmodel.predict(X_train) ","088d7a53":"# Soft predictions\n# predict_proba will give us the probability of churn being 0, and churn being 1.\n# The probabilities in each element of the array add up to 1, like [0.98945691, 0.01054309] for example\ny_pred = model.predict_proba(X_val)\ny_pred","851d5047":"# We only want the probability of churn being 1. \ny_pred[:, 1]","7a8c2bf6":"y_pred = model.predict_proba(X_val)[:, 1]","fd26ce59":"# Ultimately, we have decided that 0.5 will be the threshold whereby we consider churn to be 1. \nchurn_decision = (y_pred >= 0.5)\nchurn_decision","116cbdb0":"# Calculating the accuracy of the model itself. 0.803, not bad! \n(y_val == churn_decision).mean()","29907c1a":"# Creation of prediction dataframe\n\ndf_pred = pd.DataFrame()\n\n# Probability of the customer churning\ndf_pred['probability'] = y_pred\n\n# Convert into 0 or 1, basically y_pred in a typical situation\ndf_pred['prediction'] = churn_decision.astype(int)\n\n# y_val data being converted into a column \ndf_pred['actual'] = y_val","301f100c":"df_pred['correct'] = df_pred.prediction == df_pred.actual","a95d3f7e":"df_pred.correct.mean()","7797da28":"churn_decision.astype(int)","96041f1e":"a = [1, 2, 3, 4]\nb = 'abcd'","e4b55d4e":"# Demonstrating how dict(zip()) works\ndict(zip(a, b))","83a15cf6":"dict(zip(dv.get_feature_names(), model.coef_[0].round(3)))","05204b7b":"# We're going to be working with a smaller model instead, with fewer features \nsmall = ['contract', 'tenure', 'monthlycharges']","9a3a7285":"df_train[small].iloc[:10].to_dict(orient='records')","44e26218":"dicts_train_small = df_train[small].to_dict(orient='records')\ndicts_val_small = df_val[small].to_dict(orient='records')","879a22f8":"# Fitting to DictVectorizer\ndv_small = DictVectorizer(sparse=False)\ndv_small.fit(dicts_train_small)","3fddc1fd":"# Feature names\ndv_small.get_feature_names()","dae23f03":"X_train_small = dv_small.transform(dicts_train_small)","6056b72c":"model_small = LogisticRegression(solver='lbfgs')\nmodel_small.fit(X_train_small, y_train)","7d3aa977":"# w0 \nw0 = model_small.intercept_[0]\nw0","6810daf1":"# w1 to w5 \nw = model_small.coef_[0]\nw.round(3)","92ecbd48":"dict(zip(dv_small.get_feature_names(), w.round(3)))","3905de6d":"# Remember, logistic regression is just like linear regression just with the added Sigmoid function.\n\n-2.47 + (-0.949) + 30 * 0.027 + 24 * (-0.036)","1bc8a03b":"sigmoid(_)","3a2ca4bb":"# Preparation of the data to be vectorized \ndicts_full_train = df_full_train[categorical + numerical].to_dict(orient='records')","4dbe39d8":"dv = DictVectorizer(sparse=False)\n\n# Splitting of the data to X and y \nX_full_train = dv.fit_transform(dicts_full_train)","b48b25e1":"\ny_full_train = df_full_train.churn.values","1ff0871b":"# Fitting into model\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(X_full_train, y_full_train)","aaf2f366":"# Preparation of test data to be vectorized \ndicts_test = df_test[categorical + numerical].to_dict(orient='records')","77e8ae7d":"X_test = dv.transform(dicts_test)","d9a9f0d8":"# Predictions from X_test\ny_pred = model.predict_proba(X_test)[:, 1]","26fa0ecb":"churn_decision = (y_pred >= 0.5)","29a6166c":"(churn_decision == y_test).mean()","70a1c127":"y_test","0835c865":"# Getting the info of the customer itself, so that we can calculate the probability.\ncustomer = dicts_test[-1]\ncustomer","b6e74a85":"X_small = dv.transform([customer])","b1ff1747":"model.predict_proba(X_small)[0, 1]","b61da98a":"y_test[-1]","8d8bd31a":"## 3.5 Feature importance: Churn rate and risk ratio\n\nFeature importance analysis (part of EDA) - identifying which features affect our target variable\n\n* Churn rate\n* Risk ratio\n* Mutual information - later","8d178584":"## 3.12 Using the model","a9142763":"#### Risk ratio\nWe can also look at the risk ratio itself, whereby a ratio of more than 1 would mean more likely to churn and a ratio of less than 1 would mean less likely to churn. \n\nA risk ratio of about 1 would mean that there is the same risk as the global churn rate.","533571e9":"## 3.10 Training logistic regression with Scikit-Learn\n\n* Train a model with Scikit-Learn\n* Apply it to the validation dataset\n* Calculate the accuracy","c1379bf6":"We can tell now that with the increase of monthly charges, there will be an increase in the percentage of churn. This demonstrates a positive correlation between the two variables. ","f4c2bc72":"## 3.14 Explore more\n\nMore things\n\n* Try to exclude least useful features\n\n\nUse scikit-learn in project of last week\n\n* Re-implement train\/val\/test split using scikit-learn in the project from the last week\n* Also, instead of our own linear regression, use `LinearRegression` (not regularized) and `RidgeRegression` (regularized). Find the best regularization parameter for Ridge\n\nOther projects\n\n* Lead scoring - https:\/\/www.kaggle.com\/ashydv\/leads-dataset\n* Default prediction - https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\n\n","923484ec":"There are some things that are obviously off here.\n1. Senior Citizen should be an onject, but is represented by int64 instead. Seems like it's a 0 and 1 count.\n2. Churn is an object - however, it's the thing that we're predicting. We should look further into it and convert churn into 0 or 1.","1bb631f1":"## 3.9 Logistic regression\n\n* Binary classification\n* Linear vs logistic regression","7b732b53":"# 3. Machine Learning for Classification\n\nWe'll use logistic regression to predict churn\n\n\n## 3.1 Churn prediction project\n\n* Dataset: https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\n* https:\/\/raw.githubusercontent.com\/alexeygrigorev\/mlbookcamp-code\/master\/chapter-03-churn-prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv\n","97e8c280":"#### Churn rate","593ce776":"## 3.4 EDA\n\n* Check missing values\n* Look at the target variable (churn)\n* Look at numerical and categorical variables","18ff62b1":"Recall df.dtypes gives us int64\/float64, we're going to split it to numerical and categorical variables. \n\nHowever, there are categorical variables hiding as numerical variables, and we have to sort them accordingly.\n","206c9245":"As we can tell, with the increasing of tenure, there will be an decrease in the percentage of churn. This demonstrates a negative correlation between the two variables. ","4aa0bfdd":"## 3.2 Data preparation\n\n* Download the data, read it with pandas\n* Look at the data\n* Make column names and values look uniform\n* Check if all the columns read correctly\n* Check if the churn variable needs any preparation","648511bc":"## 3.8 One-hot encoding\n\n* Use Scikit-Learn to encode categorical features","97f66f99":"## 3.11 Model interpretation\n\n* Look at the coefficients\n* Train a smaller model with fewer features","0d57bc2c":"## 3.6 Feature importance: Mutual information\n\nMutual information - concept from information theory, it tells us how much \nwe can learn about one variable if we know the value of another\n\n* https:\/\/en.wikipedia.org\/wiki\/Mutual_information","646838af":"To be specific, it measures the mutual dependence between two variables. \nTwo variables are independent from each other if one variable does not affect another varaible, and is determined if P(A i B) = P(A) * P(B). \nIf that does not hold true, the two variables are dependent. ","8d150b7e":"Correlations are another way of showing the relationship between two variables. The correlation coefficient has a range of -1 <= r <= 1.\n\n- For a negative correlation, it means that if one variable increases, the other variable will decrease.\n- For a positive correlation, it means that if one variable increases, the other variable will increase along with it.\n\nHowever, the magnitude is more important than the positivity of the correlation. The bigger the magnitude, the stronger the relationship between two variables. ","bea9e595":"## 3.3 Setting up the validation framework\n\n* Perform the train\/validation\/test split with Scikit-Learn","d0dd51c4":"Since we're using the formula (global churn rate) - (group churn rate),\n\n<0 would mean that there's a bigger likelihood to churn while >0 would mean that there's a smaller likelihood to churn as the having a positive churn rate would mean that the global churn rate is more than that of the group churn rate and vice versa. \n\nOf course, what really matters more is the magnitude\/percentage of the churn rate before we look at the positivity\/negativity of the churn rate.","707e74fb":"## 3.7 Feature importance: Correlation\n\nHow about numerical columns?\n\n* Correlation coefficient - https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient","81f1fb9d":"This is SQL code. To explain the logic here:\n\nWe select a few columns from the data itself: \n1. Group (in this case Gender), \n2. the churn rate (aka mean of the churn), \n3. The churn rate difference & \n4. The risk ratio.\n\nThis is grouped by the certain group itself so as to see the difference and the risk ratio for each group, therefore the GROUP BY code at the end. \n```\nSELECT\n    gender,\n    AVG(churn),\n    AVG(churn) - global_churn AS diff,\n    AVG(churn) \/ global_churn AS risk\nFROM\n    data\nGROUP BY\n    gender;\n```\n\nNow that we understand the logic, we have to convert this into python code.","a740c36c":"The mean is calculated by: \n\n# 1\/n * summation of values\n\nSince values are either 0 or 1, essentially we're counting the number of ones\/n, which is the churn rate.","80a24e70":"## 3.13 Summary\n\n* Feature importance - risk, mutual information, correlation\n* One-hot encoding can be implemented with `DictVectorizer`\n* Logistic regression - linear model like linear regression\n* Output of log reg - probability\n* Interpretation of weights is similar to linear regression"}}