{"cell_type":{"1b155daf":"code","0e911641":"code","41d9f1c4":"code","0af628c5":"code","cee2d3d5":"code","fbfc164c":"code","b73e04ab":"code","3cd90408":"code","cac4e955":"code","0c02614e":"code","f3cb1942":"code","b2596bf6":"code","79943509":"code","febd9ac4":"code","60ca8b44":"code","a8254e18":"code","e3f7ef23":"code","a4e28686":"code","29463b9d":"code","c088fb43":"code","fef1dbf4":"code","7c53b639":"code","06904686":"code","afaa85cc":"code","0fc40a61":"code","bea19bfe":"code","54ffbf2e":"code","680b346e":"code","465fc3ff":"code","f34ae132":"code","ce3dd6ea":"code","7ecf031e":"code","554592f2":"code","72a35c0f":"code","1cca4872":"code","f0674bad":"code","abf949fb":"code","db46ae98":"code","c43d8291":"code","8ea923ae":"code","c837b4e4":"code","572b95b4":"code","96af7e86":"code","b677a2d7":"code","e30adca3":"code","61343188":"code","482c3b1e":"code","95c78713":"code","c2154daa":"code","17e02bea":"code","d5e42d30":"code","39aabb4e":"code","55636e01":"code","9b3a863e":"code","a377d090":"code","1b2d4092":"code","feb9672e":"code","9f48d478":"code","c48aeeb0":"code","1f52e5d0":"code","1a9daf08":"code","65e2c887":"code","8afc1918":"code","e43b2d46":"code","46da9c81":"code","d237db43":"code","77d7ae20":"code","f37d960f":"code","4fae57d4":"code","a47c9ffd":"code","88b36e3e":"code","28ec5480":"code","1fff082c":"code","109df79b":"code","3acb2daa":"code","430c01cd":"code","a315bbd5":"code","55784625":"code","bf29c427":"code","6aacf44b":"code","3d4069bf":"code","8ceee4f8":"code","94d00897":"code","3862cfea":"code","0fe63495":"code","69f7dc45":"code","7b6b3b2a":"code","ac5caf5e":"markdown","aa21e4d7":"markdown","24fa435f":"markdown","54a9fcf7":"markdown","e9496641":"markdown","dd7a5754":"markdown","796a186b":"markdown","1466f30d":"markdown","efd110ef":"markdown","e735ae94":"markdown","683add87":"markdown","990e9402":"markdown","4bdab479":"markdown","501e2ba4":"markdown","ad43304f":"markdown","7e515c71":"markdown","667418ee":"markdown","098bf90e":"markdown","a5a8c757":"markdown","4ab15922":"markdown","8380956e":"markdown","ed258230":"markdown","6aca6c2b":"markdown","351640da":"markdown","cbd1e9b0":"markdown","749ef875":"markdown","7469b75a":"markdown","5ff1d157":"markdown","1079c3d2":"markdown","905f5f3a":"markdown","d543c9a4":"markdown","e38e0945":"markdown"},"source":{"1b155daf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0e911641":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","41d9f1c4":"from fastai import *\nfrom fastai.vision import *","0af628c5":"path = untar_data(URLs.IMAGENETTE_160)","cee2d3d5":"import PIL,os,mimetypes #PIL python Image Liberary \nPath.ls = lambda x: list(x.iterdir())","fbfc164c":"\npath.ls()","b73e04ab":"(path\/'val').ls() #note we have one directory for each category ","3cd90408":"path_tench = path\/'val'\/'n01440764' #look at one category ","cac4e955":"img_fn = path_tench.ls()[0] #grap one file name \nimg_fn","0c02614e":"img = PIL.Image.open(img_fn) #look at the one file name \nimg","f3cb1942":"plt.imshow(img)","b2596bf6":"import numpy\nimga = numpy.array(img) #turning image into an array so we can see that properties it has \n","79943509":"imga.shape","febd9ac4":"imga[:10,:10,0] #print the image in numbers and note it is of dtype=unit8 which means it contains bits so they are numbers of integers not float ","60ca8b44":"#export\nimage_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image\/')) #image \"udvidelser\" that the computer already knows about \n#we use mimetypes for this. ","a8254e18":"' '.join(image_extensions) #so here is the images the mimetypes knows exsist and note we want to use this to get only images when we train","e3f7ef23":"\ndef setify(o): return o if isinstance(o,set) else set(listify(o)) #so now we can loop though each file in the directory and see which files there are \n#so the fastets way to check is something is in a list we first put it in a 'set' therefor we create a function called 'setify'\n#it simply check if it a set (isinstance(o,set)) else it will it first turnes it into a list and then a set (set(listify(o)))\n","a4e28686":"#just testing it is working \n# test_eq(setify('aa'), {'aa'})\n# test_eq(setify(['aa',1]), {'aa',1})\n# test_eq(setify(None), set())\n# test_eq(setify(1), {1})\n# test_eq(setify({1}), {1})","29463b9d":"#go thorugh a singel directory and grap the images in that \ndef _get_files(p, fs, extensions=None): #get files #p =path and fs=list of files\n    p = Path(p) #just makes sure u can use Path \n    res = [p\/f for f in fs if not f.startswith('.') #so we go throught the list of files(fs) and makes sure it doesnt start ith '.' because if it does ot is a unit or mac hidden file\n           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)] #check if we asked for extensions and if the ektension is in the list of extensions\n    return res","c088fb43":"#so now we can grap the image files \nt = [o.name for o in os.scandir(path_tench)] #python has function called 'scandir' which will take a the path ('path_tench') and list all the files in that path \nt = _get_files(path, t, extensions=image_extensions) #so here is how we are gonna call _get_files.\nt[:3]#and we are just gonna show the first 3 files \n","fef1dbf4":"#we can rewrite the code above and put it together and we get this\ndef get_files(path, extensions=None, recurse=False, include=None):\n    path = Path(path)\n    extensions = setify(extensions)\n    extensions = {e.lower() for e in extensions}\n    if recurse: #if recurse sat to True this code will be exsicuted\n        res = []\n        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n            if include is not None and i==0: d[:] = [o for o in d if o in include]\n            else:                            d[:] = [o for o in d if not o.startswith('.')] #change the list of directories if you want to\n            res += _get_files(p, f, extensions)\n        return res\n    else: #other wise if recurse is False this code wil exsicute\n        f = [o.name for o in os.scandir(path) if o.is_file()]\n        return _get_files(path, f, extensions)\n    \n    #one thing to note is that scandir is super fast ","7c53b639":"get_files(path_tench, image_extensions)[:3]#use the function above and see we can grap tre files in this way also  ","06904686":"get_files(path, image_extensions, recurse=True)[:3] #here we use recurse because got a few levels of directories strutures before we get to the pictures","afaa85cc":"all_fns = get_files(path, image_extensions, recurse=True) #so now we want to get all the file names \nlen(all_fns)#and we can see we have 13394","0fc40a61":"\n%timeit -n 10 get_files(path, image_extensions, recurse=True) #and we can see it takes about 70 ms which is fast","bea19bfe":"#export just a list that contains a lot of usefull features not inparticular needed\nclass ListContainer():\n    def __init__(self, items): self.items = listify(items)\n    def __getitem__(self, idx): #being clled when we are useing 'firkant parantesterne' for correct operation usage \n        if isinstance(idx, (int,slice)): return self.items[idx] #bla bla bla ...\n        if isinstance(idx[0],bool): #bla bla bla...\n            assert len(idx)==len(self) # bool mask\n            return [o for m,o in zip(idx,self.items) if m]\n        return [self.items[i] for i in idx]\n    def __len__(self): return len(self.items) #return lenght \n    def __iter__(self): return iter(self.items) #iteration\n    def __setitem__(self, i, o): self.items[i] = o \n    def __delitem__(self, i): del(self.items[i])\n    def __repr__(self): #printing\n        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n        if len(self)>10: res = res[:-1]+ '...]'\n        return res","54ffbf2e":"def compose(x, funcs, *args, order_key='_order', **kwargs):\n    key = lambda o: getattr(o, order_key, 0) #check to see if the _order is used and then will sort them in line below (sorted)\n    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n    return x\n\nclass ItemList(ListContainer): #ListContainer comes from last lesson and it written in the hidden cell above \n    #so besic what ItemList does is making sure we can get any kind of data \n    def __init__(self, items, path='.', tfms=None):\n        #items in this case is the filenames and the path (path='.') they came from and optinaly we can use tfms(transformed items)\n        super().__init__(items)\n        self.path,self.tfms = Path(path),tfms\n\n    def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n    \n    #below 'new' function create a new itemlist of the type of items we pass in\n    def new(self, items, cls=None):\n        if cls is None: cls=self.__class__ #so if cls(class) is not defined set cls = to whatever class the object is \n            #this chould be any class like ImageList or another class that havent been defined yet. (in this case it is ItemList)\n        return cls(items, self.path, tfms=self.tfms) #and let pass it in the items that we asked for and pass in our path and our transform\n    #so all in all 'new' is gonna create a new ItemList with the same type, with the same path and the same transform but with new items (this is uses longer done in code)\n    \n    def  get(self, i): return i #overwrite get to return the item itself which in this chase whould be the file name \n    def _get(self, i): return compose(self.get(i), self.tfms) #it will call the get method (defined at buttom) and call open the image and then it wil compose the transforms\n    #compose it a concept that where you go through a list of functions (sorted(listify(funcs)) and call the function (x = f(x,) --> means replace x with the result of given function when looping through each function  that \n    #note a deep neural network is just composes function where each layer is a function and we compose them all together.\n    #so all in all what is does is it will modify the image with the transform function (tfms).\n    \n    def __getitem__(self, idx): #when you index into you itemlist \n        res = super().__getitem__(idx) #we will pass this back up to ListContainer and this will either return one item or a list of items \n        if isinstance(res,list): return [self._get(o) for o in res] #if it is a list of items we will call self._get(o) on all of them (for o in res)\n        return self._get(res) #if it is a singel item we will just call \n\nclass ImageList(ItemList):\n    @classmethod\n    def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):\n        if extensions is None: extensions = image_extensions\n        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n    \n    def get(self, fn): return PIL.Image.open(fn) #when you get something from the ImageList it should open the image ","680b346e":"#So this is our fist trans form \nclass Transform(): _order=0 #_order means it is the first thing it should do \n\nclass MakeRGB(Transform):\n    def __call__(self, item): return item.convert('RGB') #... so what will do is call pillow. convert and 'RGB', which means if something is not RGB it will turn it into RGB \n    #remembter __call__ wil treat the class function as if it only was a function \n\n#instead of the class above we can just make it into a function and we will get the same result     \ndef make_rgb(item): return item.convert('RGB')","465fc3ff":"\nil = ImageList.from_files(path, tfms=make_rgb) #so now lets use it, here will only use the function make_rgb ","f34ae132":"il #view the itemlist and remember that itemlist inherit from ListContainer which had a __repr__ so we get all the nice printing ","ce3dd6ea":"img = il[0]; img#index into it ","7ecf031e":"il[:1] #we can also use splice since we wrote the function for it in ListContainer ","554592f2":"fn = il.items[0]; fn","72a35c0f":"fn.parent.parent.name #this is the grandparrent file since it goes through 2 dictories to get to the train folder ","1cca4872":"#So now lets make a function that grap the grandparent file \ndef grandparent_splitter(fn, valid_name='valid', train_name='train'):\n    gp = fn.parent.parent.name #grap the grandparent name  \n    return True if gp==valid_name else False if gp==train_name else None\n\ndef split_by_func(items, f):\n    mask = [f(o) for o in items] #create a mask where you pass it same function (f)\n    # `None` values will be filtered out\n    f = [o for o,m in zip(items,mask) if m==False] #grap all the thing when it is true (grandparent_splitter) here is f=train\n    t = [o for o,m in zip(items,mask) if m==True ]#grap all the thing when it is false (grandparent_splitter) here is t=valid\n    return f,t #and it will return them ","f0674bad":"splitter = partial(grandparent_splitter, valid_name='val') #so here are the grandparant name for valid called 'val' in imagenette. \n#and so we find all the validation tings ","abf949fb":"%time train,valid = split_by_func(il, splitter) #and so we split the data up in training and validation sets ","db46ae98":"len(train),len(valid) #and now we see we got 500 valdation things and the rest is for training ","c43d8291":"#so now lets use it \nclass SplitData():\n    def __init__(self, train, valid): self.train,self.valid = train,valid #store the train and valid varibles \n        \n    def __getattr__(self,k): return getattr(self.train,k) #dounder get attribute, so if we pass it a attribute it doenst know about it will take it from the training dataset \n    #This is needed if we want to pickle SplitData and be able to load it back without recursion errors\n    def __setstate__(self,data:Any): self.__dict__.update(data) \n    \n    @classmethod\n    def split_by_func(cls, il, f):\n        lists = map(il.new, split_by_func(il.items, f)) #so it will call the split_by_func function we defined above \n        #note we are using il.new where new is a function defined in ItemList above. Note that we now have create a training set \n        # and a validation set with the same path and the sam transform and the same type \n        return cls(*lists)\n\n    def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n' #we give it a representation when we print it ","8ea923ae":"sd = SplitData.split_by_func(il, splitter); sd #so now when we call it, we can see we got our training set and or validation set ","c837b4e4":"from collections import OrderedDict\n\ndef uniqueify(x, sort=False): #so to convert labels to numbers (int) we need to know all the posible labels so therefor we just need to find all the uniique things in a list (x=list)\n    #so the 2 lines below is how to get the unique thing from a list \n    res = list(OrderedDict.fromkeys(x).keys())\n    if sort: res.sort() #if we set it to True return the list to the function called 'sort' from above else\n    return res#just return the list ","572b95b4":"#so now let create a processor \nclass Processor(): \n    def process(self, items): return items #and a processer sinply is just something at can process some items \n\n    #a category processer is just a processer that is the thing that create a list of all our posible categories\nclass CategoryProcessor(Processor):\n    def __init__(self): self.vocab=None\n    \n    def process(self, items): #could also use __call__ \n        #The vocab is defined on the first use. \n        if self.vocab is None: #check if there is a vocab yet and if there is not, this must be the training set \n            self.vocab = uniqueify(items) #so we will create a vocab and it is just the unique values of all the items \n            self.otoi  = {v:k for k,v in enumerate(self.vocab)} #and now we want something that goes from object to int\n            #so this is reversed mapping so we enumerate the vocab and create a dicornary with the revered mapping(v:k for k,v)\n        return [self.proc1(o) for o in items] #so now since we have a vocab we can go through ever item and process them one at the time (proc1)\n    def proc1(self, item):  return self.otoi[item]# and process one (proc1) simply means look ind the revered mappong \n   \n    #we can also deprocess which takes a lot of indexses (inxs)\n    def deprocess(self, idxs):\n        assert self.vocab is not None #make sure we have vocab otherwice we cant do anything \n        return [self.deproc1(idx) for idx in idxs] # and then we just deprocess one (deproc1) for each index\n    def deproc1(self, idx): return self.vocab[idx] #and deprocess one(deproc1) just looks it up in the vocab (vocab[idx])\n    \n#so now we can combine it all in together \nclass ProgessedItemList(ListContainer):\n    def __init__(self, inputs, processor):\n        self.processor=processor #contains a processer \n        items=processor.process(inputs) #and the items in it was whatever it waas given after it was processed(inputs)\n        super().__init__(items)\n        \n    def obj(self, idx): #so object(obj) and that is just the thing that is going to...\n        res=sef[idx]\n        if isinstance(res(tuple,list,Generator)): return self.processor.deprocess(res) #... deprocess the items again\n        return self.processor.deproc1(idx)\n\n    #so this is all the stuff we need to label things ","96af7e86":"#we fund that for the splitting we needed the grandparant but for the labeling we need the paratens \ndef parent_labeler(fn): return fn.parent.name #so this is just a parant labeler \n\ndef _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.items], path=ds.path) #so this is just a function that label things it just \n#call 'f(o)' (function) for each thing in the item list \n\n#This is a slightly different from what was seen during the lesson,\n#   we'll discuss the changes in lesson 11\nclass LabeledData():\n    def process(self, il, proc): return il.new(compose(il.items, proc))\n\n    def __init__(self, x, y, proc_x=None, proc_y=None): #pass it its independed varible(x) and a depended varible(y)\n        self.x,self.y = self.process(x, proc_x),self.process(y, proc_y)\n        self.proc_x,self.proc_y = proc_x,proc_y\n        \n    def __repr__(self): return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n' #make it print out nicely \n    def __getitem__(self,idx): return self.x[idx],self.y[idx] #a indexser to grap the x and to grap the y \n    def __len__(self): return len(self.x) #we need a lenthg \n    \n    def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x)\n    def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y)\n    \n    def obj(self, items, idx, procs):\n        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)\n        item = items[idx]\n        for proc in reversed(listify(procs)):\n            item = proc.deproc1(item) if isint else proc.deprocess(item)\n        return item\n\n    @classmethod\n    def label_by_func(cls, il, f, proc_x=None, proc_y=None): #does the labeling  \n        return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)\n\ndef label_by_func(sd, f, proc_x=None, proc_y=None):\n    train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y) #note that in the validation set will use the trainings set vovab\n    valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)\n    return SplitData(train,valid)","b677a2d7":"ll = label_by_func(sd, parent_labeler)","e30adca3":"assert ll.train.proc_y is ll.valid.proc_y","61343188":"ll.train.y","482c3b1e":"ll.train.y.items[0], ll.train.y_obj(0), ll.train.y_obj(slice(2)) #to get the name of a categorieor just some of them ","95c78713":"ll","c2154daa":"ll.train[0]","17e02bea":"ll.train[0][0] ","d5e42d30":"ll.train[0][0].resize((128,128)) #for all to be in the same batch they all need the same size so we just use resize ","39aabb4e":"#so below is a transform that resizes things \nclass ResizeFixed(Transform):\n    _order=10 #and it has to be after all the ohter transforms \n    def __init__(self,size): #tages a size\n        if isinstance(size,int): size=(size,size) #if you passed in a int it will turn it into a tuble \n        self.size = size\n        \n    def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR) #and when you call it, it wil do resize and it will do linear resizing \n\n    #now they all have the same size we can turn them all into tensors \ndef to_byte_tensor(item):\n    res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))\n    w,h = item.size\n    return res.view(h,w,-1).permute(2,0,1)\nto_byte_tensor._order=20 #it hass to be done after the resizing so it gets a lesser order. note we can add orders to functions aswell \n\n#since the above turns it into a byte tensor and we need a float tensor we can do like:\ndef to_float_tensor(item): return item.float().div_(255.) #we divide it since we dont want it between 0 and 255 but between 0 and 1, so we divide it by 255\nto_float_tensor._order=30 #it gets a higher order since the above function needs to run first ","55636e01":"tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] #so this is the list of transofrms we have \n\nil = ImageList.from_files(path, tfms=tfms) #we can pass that to out imagelist \nsd = SplitData.split_by_func(il, splitter) #we can split it \nll = label_by_func(sd, parent_labeler) # we can label it ","9b3a863e":"\n#export\ndef show_image(im, figsize=(3,3)):\n    plt.figure(figsize=figsize)\n    plt.axis('off')\n    plt.imshow(im.permute(1,2,0))","a377d090":"x,y = ll.train[0]\nx.shape","1b2d4092":"show_image(x)","feb9672e":"bs=64","9f48d478":"def get_dls(train_ds, valid_ds, bs, **kwargs):\n    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=bs*2, **kwargs)) #since we dont have to do the backward pass, so we have twice as much space\n            # therefor we can multipy the batch size with 2.","c48aeeb0":"train_dl,valid_dl = get_dls(ll.train,ll.valid,bs, num_workers=4) #we will use the get dataloader from before(get_dls)\n#and pass in the ll.train and ll.valid directly from our labeled list ","1f52e5d0":"x,y = next(iter(train_dl)) #lets grap a minibatch ","1a9daf08":"x.shape #and here we can se the minibacth ","65e2c887":"show_image(x[0])\nll.train.y","8afc1918":"y","e43b2d46":"class DataBunch():\n    def __init__(self, train_dl, valid_dl, c_in=None, c_out=None, c=None):\n        self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out #c_in is the numbers it need in input and c_out is the correct numbers for are databunch\n\n    @property\n    def train_ds(self): return self.train_dl.dataset\n\n    @property\n    def valid_ds(self): return self.valid_dl.dataset","46da9c81":"def databunchify(sd, bs, c_in=None, c_out=None,c=None, **kwargs):\n    dls = get_dls(sd.train, sd.valid, bs, **kwargs)\n    return DataBunch(*dls, c_in=c_in, c_out=c_out)\n\nSplitData.to_databunch = databunchify","d237db43":"path = untar_data(URLs.IMAGENETTE_160) #grap the path\ntfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] #grap the transform \n\nil = ImageList.from_files(path, tfms=tfms) #grap the itemlist \nsd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val')) #slit the data \nll = label_by_func(sd, parent_labeler) #label it \ndata = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) #create databunch with 3 chanels in and 10 chanels out and 4 processers(num_worker)","77d7ae20":"#export\nclass AvgStatsCallback(Callback):\n    def __init__(self, metrics):\n        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n        \n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n        \n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad(): stats.accumulate(self.run)\n    \n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)\n        \nclass Recorder(Callback):\n    def begin_fit(self):\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n\n    def after_batch(self):\n        if not self.in_train: return\n        for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])\n        self.losses.append(self.loss.detach().cpu())        \n\n    def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])\n    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n        \n    def plot(self, skip_last=0, pgid=-1):\n        losses = [o.item() for o in self.losses]\n        lrs    = self.lrs[pgid]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(lrs[:n], losses[:n])\n\nclass ParamScheduler(Callback):\n    _order=1\n    def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs\n        \n    def begin_fit(self):\n        if not isinstance(self.sched_funcs, (list,tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        assert len(self.opt.param_groups)==len(self.sched_funcs)\n        for pg,f in zip(self.opt.param_groups,self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs\/self.epochs)\n            \n    def begin_batch(self): \n        if self.in_train: self.set_param()","f37d960f":"class CudaCallback(Callback):\n    def begin_fit(self): self.model.cuda() #note we dont have to move it to a device we just use .cuda\n    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()","4fae57d4":"cbfs = [partial(AvgStatsCallback,accuracy),\n        CudaCallback] #callback function from last time ","a47c9ffd":"m,s = x.mean((0,2,3)),x.std((0,2,3))\nm,s #normalize ","88b36e3e":"#create function that normalize things that have 3 chanels \ndef normalize_chan(x, mean, std):\n    return (x-mean[...,None,None]) \/ std[...,None,None]\n\n_m = tensor([0.47, 0.45,  0.42]) #mean of the imagenette \n_s = tensor([0.27, 0.27, 0.29])#std of the imagenette \nnorm_imagenette = partial(normalize_chan, mean=_m, std=_s)","28ec5480":"#previus when we wrote the transforming of mnist it will only work with it but here we will make a more generel transformation of the data \nclass BatchTransformXCallback(Callback): #transform the indepened verible (X-training data) for a batch \n    _order=2\n    def __init__(self, tfm): self.tfm = tfm #you pass it som etransformation function (tfm) which its stores away \n    def begin_batch(self): self.run.xb = self.tfm(self.xb) #begin batch just replaces the current batch (xb) with the result of the transformation (tfm)\n\ndef view_tfm(*size): #view_tfm takes and transform the size\n    def _inner(x): return x.view(*((-1,)+size))\n    return _inner","1fff082c":"cbfs.append(partial(BatchTransformXCallback, norm_imagenette)) #add the above to a callback ","109df79b":"nfs = [64,64,128,256] #create a conv net of these layers ","3acb2daa":"import math\ndef prev_pow_2(x): return 2**math.floor(math.log2(x))\n\ndef get_cnn_layers(data, nfs, layer, **kwargs):\n    def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs)\n    l1 = data.c_in #knows how big the first layer has to be (c_in) \n    l2 = prev_pow_2(l1*3*3) #secound layer takes the 3 by 3 kernel and mulitipy it by next biggest power of two to that number \n    layers =  [f(l1  , l2  , stride=1), #tird layer takes previus output \n               f(l2  , l2*2, stride=2), #and multiply first by 2\n               f(l2*2, l2*4, stride=2)] # and then by 4 \n    nfs = [l2*4] + nfs #and the next layers is whatever we asked for (nfs is defined above)\n    layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]\n    layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), \n               nn.Linear(nfs[-1], data.c_out)] #note also the c_out is predefined and is how many classes we have in our data \n    return layers\n\ndef get_cnn_model(data, nfs, layer, **kwargs):\n    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))\n\ndef get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs):\n    model = get_cnn_model(data, nfs, layer, **kwargs)\n    init_cnn(model)\n    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)","430c01cd":"#dont think to much about the below code \ndef combine_scheds(pcts, scheds):\n    assert sum(pcts) == 1.\n    pcts = tensor([0] + listify(pcts))\n    assert torch.all(pcts >= 0)\n    pcts = torch.cumsum(pcts, 0)\n    def _inner(pos):\n        idx = (pos >= pcts).nonzero().max()\n        actual_pos = (pos-pcts[idx]) \/ (pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos)\n    return _inner","a315bbd5":"#a decorator is a function that returns a function \ndef annealer(f):\n    def _inner(start, end): return partial(f, start, end)\n    return _inner\n\n@annealer #the annealer decorator does the same as the partial \ndef sched_lin(start, end, pos): return start + pos*(end-start)","55784625":"\n#export\n@annealer\ndef sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) \/ 2\n@annealer\ndef sched_no(start, end, pos):  return start #no schedular return always start\n@annealer\ndef sched_exp(start, end, pos): return start * (end\/start) ** pos\n\ndef cos_1cycle_anneal(start, high, end):\n    return [sched_cos(start, high), sched_cos(high, end)]\n\n#This monkey-patch is there to be able to plot tensors\ntorch.Tensor.ndim = property(lambda x: len(x.shape))","bf29c427":"sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05)) #one cycle scheduling ","6aacf44b":"#export\nclass Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x): return self.func(x) #the only thing it does is\n\ndef flatten(x):      return x.view(x.shape[0], -1)","3d4069bf":"#we are gonna try a few things to fix the above problem \ndef get_cnn_layers(data, nfs, layer, **kwargs): #note we use **kwargs since we can get extra arguments from GeneralRelu class\n    nfs = [1] + nfs\n    return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) #note we use **kwargs since we can get extra arguments from GeneralRelu class\n            for i in range(len(nfs)-1)] + [\n        nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)]\n\ndef conv_layer(ni, nf, ks=3, stride=2, **kwargs): #note we use **kwargs since we can get extra arguments from GeneralRelu class\n    return nn.Sequential(\n        nn.Conv2d(ni, nf, ks, padding=ks\/\/2, stride=stride), GeneralRelu(**kwargs)) #note we use **kwargs since we can get extra arguments from GeneralRelu class\n\n#better Relu \nclass GeneralRelu(nn.Module):\n    def __init__(self, leak=None, sub=None, maxv=None): #so now we can use subret from the relu(which we found was good about 0.5)\n        #and we can handle leaking (leak). And maybe we want a limit so we can use maximum value (maxv)\n        super().__init__()\n        self.leak,self.sub,self.maxv = leak,sub,maxv\n\n    def forward(self, x): \n        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) #so if you pass leak(from GeneralRelu) it will\n        #use leaky_relu otherwise we will use normal relu (F.relu(x))\n        if self.sub is not None: x.sub_(self.sub) #if you want to subrat something: go do that\n        if self.maxv is not None: x.clamp_max_(self.maxv) #if you want to use maksimum value: go do that\n        return x\n\ndef init_cnn(m, uniform=False): #uniform boolien \n    f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ #so some people say uniform is better then normal, so it is an optinal see buttom of this part when used\n    for l in m:\n        if isinstance(l, nn.Sequential):\n            f(l[0].weight, a=0.1)\n            l[0].bias.data.zero_()\n\ndef get_cnn_model(data, nfs, layer, **kwargs):\n    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))","8ceee4f8":"from torch.nn import init","94d00897":"#lets put the model and the layers intot he function below \ndef get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): \n    if opt_func is None: opt_func = optim.SGD #grap optmisation function\n    opt = opt_func(model.parameters(), lr=lr) #grap the optimazer\n    learn = Learner(model, opt, loss_func, data) #grap our learner \n    return learn, Runner(cb_funcs=listify(cbs)) ","3862cfea":"learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[\n    partial(ParamScheduler, 'lr', sched)\n])","0fe63495":"\n#export\ndef model_summary(run, learn, data, find_all=False):\n    xb,yb = get_batch(data.valid_dl, run)\n    device = next(learn.model.parameters()).device#Model may not be on the GPU yet\n    xb,yb = xb.to(device),yb.to(device)\n    mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()\n    f = lambda hook,mod,inp,out: print(f\"{mod}\\n{out.shape}\\n\")\n    with Hooks(mods, f) as hooks: learn.model(xb)","69f7dc45":"model_summary(run, learn, data)","7b6b3b2a":"\n%time run.fit(5, learn)","ac5caf5e":"Since our filenames are path object, we can find the directory of the file with .parent. We need to go back two folders before since the last folders are the class names.","aa21e4d7":"This gives us the full summary on how to grab our data and put it in a DataBunch:","24fa435f":"First, let's define the processor. We also define a ProcessedItemList with an obj method that can get the unprocessed items: for instance a processed label will be an index between 0 and the number of classes - 1, the corresponding obj will be the name of the class. The first one is needed by the model for the training, but the second one is better for displaying the objects.","54a9fcf7":"Note that if we pass it a black-white picture pillow (PIL) opens the image it give you back by default a rank 2 tenser, so just the x and the y axsis and there no chanel axsis \nthen you can stack them into a minibach since they ar enot the same shape if the other imges is colored ....","e9496641":"Just in case there are other files in the directory (models, texts...) we want to keep only the images. Let's not write it out by hand, but instead use what's already on our computer (the MIME types database).","dd7a5754":"## Split validation set\nHere, we need to split the files between those in the folder train and those in the folder val.","796a186b":"\nHere we label according to the folders of the images, so simply fn.parent.name. We label the training set first with a newly created CategoryProcessor so that it computes its inner vocab on that set. Then we label the validation set using the same processor, which means it uses the same vocab. The end result is another SplitData object.","1466f30d":"We build our model using Bag of Tricks for Image Classification with Convolutional Neural Networks, in particular: we don't use a big conv 7x7 at first but three 3x3 convs, and don't go directly from 3 channels to 64 but progressively add those.","efd110ef":"\nThe first transform resizes to a given size, then we convert the image to a by tensor before converting it to float and dividing by 255. We will investigate data augmentation transforms at length in notebook 10.","e735ae94":"We need the recurse argument when we start from path since the pictures are two level below in directories.","683add87":"# Modeling\n## DataBunch\nNow we are ready to put our datasets together in a DataBunch.","990e9402":"## Transform to tensor","4bdab479":"\nNow that we can split our data, let's create the class that will contain it. It just needs two ItemList to be initialized, and we create a shortcut to all the unknown attributes by trying to grab them in the train ItemList.","501e2ba4":"# Model","ad43304f":"Previously we were reading in to RAM the whole MNIST dataset at once, loading it as a pickle file. We can't do that for datasets larger than our RAM capacity, so instead we leave the images on disk and just grab the ones we need for each mini-batch as we use them.\n\nLet's use the imagenette dataset and build the data blocks we need along the way.","7e515c71":"We can also index with a range or a list of integers:","667418ee":"Transforms aren't only used for data augmentation. To allow total flexibility, ImageList returns the raw PIL image. The first thing is to convert it to 'RGB' (or something else).\n\nTransforms only need to be functions that take an element of the ItemList and transform it. If they need state, they can be defined as a class. Also, having them as a class allows to define an _order attribute (default 0) that is used to sort the transforms.","098bf90e":"\n# Prepare for modeling\nWhat we need to do:\n\nGet files\n* Split validation set\n* random%, folder name, csv, ...\n* Label:\n* folder name, file name\/re, csv, ...\n* Transform per image (optional)\n* Transform to tensor\n* DataLoader\n* Transform per batch (optional)\n* DataBunch\n* Add test set (optional)\n\n\nGet files\nWe use the ListContainer class from notebook 06 to store our objects in an ItemList. The get method will need to be subclassed to explain how to access an element (open an image for instance), then the private _get method can allow us to apply any additional transform to it.\n\nnew will be used in conjunction with \n\n    __getitem__ \n(that works for one index or a list of indices) to create training and validation set from a single stream when we split the data.","a5a8c757":"\n## Labeling\nLabeling has to be done after splitting, because it uses training set information to apply to the validation set, using a Processor.\n\nA Processor is a transformation that is applied to all the inputs once at initialization, with some state computed on the training set that is then applied without modification on the validation set (and maybe the test set or at inference time on a single item). For instance, it could be processing texts to tokenize, then numericalize them. In that case we want the validation set to be numericalized with exactly the same vocabulary as the training set.\n\nAnother example is in tabular data, where we fill missing values with (for instance) the median computed on the training set. That statistic is stored in the inner state of the Processor and applied on the validation set.\n\nIn our case, we want to convert label strings to numbers in a consistent and reproducible way. So we create a list of possible labels in the training set, and then convert our labels to numbers based on this vocab.","4ab15922":"We will normalize with the statistics from a batch.","8380956e":"Then we define a function that goes directly from the SplitData to a DataBunch.","ed258230":"![image.png](attachment:image.png)\nnote that the chanel(c_in) is the depht and the kernel size is 3 by 3 ","6aca6c2b":"To be able to put all our images in a batch, we need them to have all the same size. We can do this easily in PIL.","351640da":"## Get images","cbd1e9b0":"Though we cant train on the data above since it is pillows and not tensors so we have to change that ","749ef875":"\nHere is a little convenience function to show an image from the corresponding tensor.","7469b75a":"We can still see the images in a batch and get the corresponding classes.","5ff1d157":"We change a little bit our DataBunch to add a few attributes: c_in (for channel in) and c_out (for channel out) instead of just c. This will help when we need to build our model.","1079c3d2":"# Data block API foundations","905f5f3a":"# Image ItemList","d543c9a4":"Imagenet is 100 times bigger than imagenette, so we need this to be fast.","e38e0945":"\nNow let's walk through the directories and grab all the images. The first private function grabs all the images inside a given directory and the second one walks (potentially recursively) through all the folder in path."}}