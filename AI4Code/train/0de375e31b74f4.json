{"cell_type":{"76862aa8":"code","973eb3c5":"code","b546c598":"code","35445617":"code","f91c241d":"code","62b8113a":"code","a4836b8a":"code","a5b9108c":"code","4ed23526":"code","e5a79e71":"code","7abfd704":"code","fa06fc07":"code","476e197e":"code","c4b782b8":"code","3371d8aa":"code","d7174dd7":"code","d5e8eff3":"code","c5b14ff0":"code","5dbac46d":"code","db34f985":"code","fbb4ed3e":"code","a47a3a57":"code","b6dfc64d":"code","1bc23b19":"code","8a4b7546":"code","7ad5e147":"code","ee6474bc":"code","ceca8e9f":"code","5c7d71c1":"markdown","877047a9":"markdown","eeca784f":"markdown","2b1a18d5":"markdown","0da6f7e3":"markdown","e8ed6942":"markdown","76b441e3":"markdown","2b372032":"markdown","fa188208":"markdown","6d8f8929":"markdown","83557829":"markdown","f2660e6c":"markdown","50510770":"markdown","6af19f0d":"markdown","76b82b2e":"markdown","65bdbf9a":"markdown","c72fe92f":"markdown","092065e9":"markdown","36d71a15":"markdown","6c27b789":"markdown","f898f965":"markdown","05405e7a":"markdown","46ce5680":"markdown","33adc7cc":"markdown","ac31fd44":"markdown","d66f8a45":"markdown","0395fb92":"markdown","b9c74454":"markdown","f6872db4":"markdown","bf67bc8f":"markdown"},"source":{"76862aa8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap as shap\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","973eb3c5":"# Use Jupyter Widges Package - https:\/\/ipywidgets.readthedocs.io\/en\/stable\/index.html \nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Layout\nimport ipywidgets as widgets\nstyle = {'description_width': 'initial'}\n\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 300)","b546c598":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n## Load the data \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/train.csv\", sep=',')\ntest = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/test.csv\", sep=',')","35445617":"# Check the number of rows and columns\nprint(\"Number of train samples are\",train.shape)\nprint(\"Number of test samples are\",test.shape) ","f91c241d":"# Have a glance at data (shortcut tip: (un)comment: control + \/ )\ntrain.head() \n\n# Check features in your dataset\n# train.columns","62b8113a":"data_dict = pd.read_csv(\"\/kaggle\/input\/addfile\/dataDictionary.csv\", sep=',')\n# data_dict","a4836b8a":"search_column_name = 'variable'\n\n### Print the selected data rows ###\ndef f(x):\n    return data_dict[data_dict[search_column_name].isin(list(x))]\n\n### Multiple selection widgets ###\nwidget_variable=widgets.SelectMultiple(\n    options=data_dict[search_column_name].unique(),\n    layout=Layout(width='25%', height='150px'),\n    description=search_column_name, \n    style = style\n)\ninteract(f, x=widget_variable);","a5b9108c":"# They are auto-recognized by Pandas when you read the csv file into dataframe.\n# Question: Are they all correct?\ntrain.dtypes","4ed23526":"categorical_features = ['State_Factor', 'building_class', 'facility_type']\n# visualize_features = ['State_Factor', 'building_class', 'facility_type', 'Year_Factor']\nnumerical_features=train.select_dtypes('number').columns","e5a79e71":"plot_cat_dataframe = train\n\n### hist plot categorical features ###\ndef count_plot(var, plot_cat_dataframe):\n    plt.figure(figsize = (10,8))\n    ax = sns.countplot(y = var, data = plot_cat_dataframe)\n    plt.title(var, size = 15)\n\ndef inter_cat_plot(x):\n    return count_plot(x, plot_cat_dataframe)\n\n### Multiple selection widgets ###\nwidget_cat_plot=widgets.Dropdown(\n    options=categorical_features,\n    value='State_Factor',\n    description=\"Categorical Variable:\", \n    style = style\n)\ninteract(inter_cat_plot, x=widget_cat_plot);","7abfd704":"plot_num_data =train[numerical_features]\n\n# plot_num_data = train[train['State_Factor']=='State_1']\n\n# data_state_1 = train[train['State_Factor']=='State_2']\n# plot_num_data = data_state_1[data_state_1['building_class']=='Commercial']\n\n\n### Trend line plot ###\ndef line_plot(var, plot_num_data):\n    plt.figure(figsize = (20,4))\n    plot_num_data[var].plot(figsize=(20,4));\n\ndef inter_cat_plot(x):\n    return line_plot(x, plot_num_data)\n\n### Multiple selection widgets ###\nwidget_cat_plot=widgets.Dropdown(\n    options=plot_num_data.select_dtypes('number').columns,\n    value=\"january_avg_temp\",\n    description=\"Numerical Variable:\", \n    style = style\n)\ninteract(inter_cat_plot, x=widget_cat_plot);","fa06fc07":"### Distribution plot ###\ndef dist_plot(feature_list, train, test):\n    for each_feature in feature_list:\n        plt.figure(figsize = (20, 4))\n\n        sns.kdeplot(train[each_feature].to_numpy(), color = '#5499C7') # blue\n        sns.kdeplot(test[each_feature].to_numpy(), color = '#D35400') # red\n\n        plt.title(each_feature, fontsize=15)\n        plt.show()\n    \n#     del values_train , values_test\n    \ndef inter_dist_plot(x):\n    return dist_plot(x, train, test)\n\n### Multiple selection widgets ###\nwidget_dist_plot=widgets.SelectMultiple(\n    options=train.select_dtypes('number').columns,\n    value=[\"floor_area\"],\n    layout=Layout(width='50%', height='200px'),\n    description=\"Numerical Variable:\", \n    style = style\n)\ninteract(inter_dist_plot, x=widget_dist_plot);","476e197e":"month_avg_temp = ['january_avg_temp','february_avg_temp','march_avg_temp','april_avg_temp',\n'may_avg_temp', 'june_avg_temp', 'july_avg_temp', 'august_avg_temp',\n'september_avg_temp', 'october_avg_temp', 'november_avg_temp','december_avg_temp']\n\n# Calculate correlation matrix \n# Check parameters: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.corr.html \ncorr = train[month_avg_temp].corr() # method='pearson', 'kendall' , 'spearman'\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9,6))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-0.8,vmax=0.8, square=True, linewidths=.5)","c4b782b8":"# ### Note: Not all the labels are printed. \n# corr = train[numerical_features].corr() # TRY DIFFERENT METHOD: method='pearson', 'kendall' , 'spearman'\n\n# ### Generate a mask for the upper triangle\n# mask = np.triu(np.ones_like(corr, dtype=bool))\n\n# ### Set up the matplotlib figure\n# f, ax = plt.subplots(figsize=(9,6)) # TRY TO CHANGE THE SIZE\n\n# ### Generate a custom diverging colormap\n# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# ### Draw the heatmap with the mask and correct aspect ratio\n# sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-0.8,vmax=0.8, square=True, linewidths=.5) # TRY TO CHANGE vmin, vmax","3371d8aa":"# plt.figure(figsize = (25,11))\n# sns.heatmap(train.isna().values, xticklabels=train.columns)\n# plt.title(\"Missing values in training Data\", size=20)","d7174dd7":"### check if there is any missing value in the dataset ###\ndef check_missing(df, col):\n    missing  = 0\n    misVariables = []\n    CheckNull = df.isnull().sum()\n    for var in range(0, len(CheckNull)):\n        misVariables.append([col[var], CheckNull[var], round(CheckNull[var]\/len(df),3)])\n        missing = missing + 1\n\n    if missing == 0:\n        print('Dataset is complete with no blanks.')\n    else:\n        df_misVariables = pd.DataFrame.from_records(misVariables)\n        df_misVariables.columns = ['Variable', 'Missing', 'Percentage']\n        s = df_misVariables.sort_values(by=['Percentage'], ascending=False)\n        display(s)\n    return df_misVariables","d5e8eff3":"ranked_df_missing_value = check_missing(train, train.columns) ","c5b14ff0":"# ranked_df_missing_value = check_missing(test, test.columns)","5dbac46d":"### Take all variables which has less than 5% missing values ###\nincluded_col = list(ranked_df_missing_value[ranked_df_missing_value['Percentage']<0.05]['Variable'])\n\ntrain_partial = train[included_col]\n\nincluded_col.remove('site_eui')\ntest_partial = test[included_col]\n\n### Remove the rows with missing values ###\n### Please note how many rows you have excluded ###\ntrain_partial = train_partial.dropna().reset_index(drop=True)\n\nprint(\"Original training dataset (rows):\", len(train))\nprint(\"After removing missing (rows):\", len(train_partial))","db34f985":"target = train_partial[\"site_eui\"]\ntrain_partial = train_partial.drop([\"site_eui\",\"id\"],axis =1)\ntest_partial = test_partial.drop([\"id\"],axis =1)","fbb4ed3e":"#encoding\n\nlabel_encoder = LabelEncoder()\nfor col in categorical_features:\n    train_partial[col] = label_encoder.fit_transform(train_partial[col])\n    test_partial[col] = label_encoder.fit_transform(test_partial[col])","a47a3a57":"#scaling\n\nscaler = StandardScaler()\ntrain_partial = scaler.fit_transform(train_partial)\ntest_partial = scaler.transform(test_partial)","b6dfc64d":"# #### If you want to inspect outliers or use some type of flag features if the sample is an outlier\n# from sklearn.neighbors import LocalOutlierFactor\n# from sklearn.ensemble import IsolationForest\n# #iso = LocalOutlierFactor(n_neighbors=35, contamination=0.01)\n# iso = IsolationForest(contamination=0.01)\n\n# outliers = iso.fit_predict(train)\n# ### select all rows that are not outliers\n\n# ### train = train[outliers!=-1]\n# ### target = target[outliers!=-1]","1bc23b19":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_partial, target, test_size = 0.2, random_state = 2022)","8a4b7546":"import xgboost\nxgboost_model = xgboost.XGBRegressor(n_estimators=200, learning_rate=0.02, gamma=0, subsample=0.75,\n                           colsample_bytree=0.4, max_depth=5)\n\nxgboost_model.fit(X_train,y_train)\ny_pred = xgboost_model.predict(X_test)\n# regression evalution metrics\nRMSE = math.sqrt(np.square(np.subtract(y_pred,y_test)).mean())","7ad5e147":"RMSE","ee6474bc":"# model evaluation\nres = xgboost_model.predict(test_partial)\nsub = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/sample_solution.csv\")\nsub[\"site_eui\"] = res\nsub.to_csv(\"submission.csv\", index = False)","ceca8e9f":"sub","5c7d71c1":"# WiDS Maastricht Datathon 2022 Pre-training Day-1 Notebook #\nThis jupyter notebook is prepared for [WiDS Maastricht Datathon 2022](https:\/\/www.maastrichtuniversity.nl\/events\/wids-datathon-maastricht-2022) training session on 4-5th Feb 2022. The objective of this pre-training notebook is to provide background knowledge and guidence for participants to start the data challenge. \n\n- The dataset contains approximately **100k** observations of building energy usage records, collected over **7 years** and a number of states within the United States. \n- The dataset consists of **building characteristics, weather data** for the location of the building, and energy usage for the building and the given year. \n- The **goal** is to predict energy usage for each building given the characteristics of the building and the weather data for the location of the building.\n\n\nThis notebook is delivered by [Chang Sun](https:\/\/www.linkedin.com\/in\/chang-sun-maastricht\/), [Nicolas Perez](http:\/\/www.linkedin.com\/in\/nicolas-perez-zambrano\/), [Yenisel Plasencia Cala\u00f1a](https:\/\/www.linkedin.com\/in\/yenisel-plasencia-cala%C3%B1a-phd-10144190\/), [Carlos Utrilla Guerrero](https:\/\/nl.linkedin.com\/in\/carlos-utrilla-guerrero-97ba7b31), [Parveen Kumar](https:\/\/nl.linkedin.com\/in\/parveensenza).","877047a9":"## Why climate change matters?\n**Climate Change costs lives and money.**\n- The average temperature in Europe has risen sharply over the past 40 years.\n- Climate change causes extreme weather.\n- People are dying because of extreme weather\n- Climate change leads to economic losses\n\n*Source: European Council https:\/\/www.consilium.europa.eu\/en\/infographics\/climate-costs\/*\n\n## Limit climate change and its effects\nImmediate, rapid and large-scale **reductions in greenhouse gas (GHG) emissions** and reaching net-zero CO2 emissions have the potential to limit climate change and its effects.\n\nMitigation of GHG emissions requires changes to electricity systems, transportation, **buildings**, industry, and land use.\n","eeca784f":"**Building energy prediction**\n- prevent power shortages in modern cities\n- reduce social costs caused by unnecessary energy supply\n- support stable and efficient power grid operation\n- make impactful mitigation strategies \n","2b1a18d5":"> **Question:** from which step should we exclude the target features? ","0da6f7e3":"## Types of features\n- Categorical or numerical?\n- how to deal with different types of features?\n- why they matter? \n    - for example: gender, country code, level of education","e8ed6942":"### Look at categorical variables","76b441e3":"> Question: What is the name of the target feature??","2b372032":"## Check missing values in the dataset","fa188208":"### Check the data type of each variable. ","6d8f8929":"> **Question**: Will the test data has the same missing value situation? ","83557829":"> Thinking: CHECK Features \n> - **Year_Factor**: int64?\n> - **id**: int64?\n> - **year_built**: float64?","f2660e6c":"# Feature exploration \n## What do these features mean in your dataset?\n(Please note people would call them features or variables or attributes or columns. All are collect ;)\n\n**We Added an additional file - dataDictionary.csv**\nThis file is created from the Data description on WIDS Kaggle page https:\/\/www.kaggle.com\/c\/widsdatathon2022\/data \n\nIt is NOT necessary to have this file for your analysis. I added it here to help you have an easy way to explore the features in the dataset.","50510770":"# Load train and test data files","6af19f0d":"> Question: Why training datasets has 64 features, but test dataset has 63??","76b82b2e":"### Correlation matrix\nCorrelation map to see how features are correlated with each other and with target","65bdbf9a":"# Remove target feature + id feature","c72fe92f":"### Distribution plot of numerical variables","092065e9":"# Import needed packages","36d71a15":"## What does the previous number mean?\nIn this case, we're dealing with a regression problem. Meaning we're trying to predict a number given the features.\n\nWhen trying to evaluate how close all of our predictions were to reality. There are a couple of ways of doing this, one of the most common is through the Root-mean-square error, the method that we're using. What matters in general is that a lower number is better, as that means there were less errors.\n\nThe following wikipedia article contains more details about how it works:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation","6c27b789":"## Encoding [Challenge2 Dive into details on Day 2]##\nWhy we need to encode features? There are many ways to encode the features... Figure out the differences between them, and how to choose the optimal one for this dataset, and why!","f898f965":"## Anomaly Detection [Challenge Dive into details on Day 2] ##\n- **Outlier detection**: The **training data contains outliers** which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.\n- **Novelty detection**: The training data is not polluted by outliers and we are interested in detecting whether a **new** observation is an outlier. In this context an outlier is also called a novelty.\n\nSource:https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html","05405e7a":"## Feature Scaling: standardization, normalization [Challenge1 Dive into details on Day 2] ##\n- Why we need to normalize variables\n- What methods can we use: \n    - https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#normalization\n    - https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html\n    - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n    - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html\n    - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html\n- What are the differences using different normalization method","46ce5680":"### Group categorical and numercial features","33adc7cc":"https:\/\/www.kaggle.com\/changsun1025\/wids-2022-datathon-maastricht-day-1\/edit","ac31fd44":"## Glance at data\nAlways check the number of rows and columns, how data looks like, what variables\/features\/attributes\/columns are in the dataset.\n\nWith the .shape method, you get this information. The first number is the amount of rows, while the second one is the amount of features.","d66f8a45":"# Model training and testing [Optimization will be presented in DAY 2]","0395fb92":"### Look at numerical variables","b9c74454":"## Handling missing values [Challenge1 Dive into details on Day 2] ##\n- Why missing values affect our results?\n- How to handle missing values?\n- How to handle missing values in training and test datasets? in the same or different way?","f6872db4":"**TO DO:** Try to plot correlation matrix for all features or other features that you think are interesting to see the \ncorrelations","bf67bc8f":"**Please note there are only 30 features\/variables in the dictionary**"}}