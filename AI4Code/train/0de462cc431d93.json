{"cell_type":{"1a256146":"code","62235205":"code","d19ebd7a":"code","15d0677f":"code","07c6eb85":"code","7a920c49":"code","fe26c2b7":"code","4c2238fe":"code","213568e0":"code","51044d03":"code","4f5b8949":"code","429feec6":"code","698fbe45":"code","0e13a4c7":"code","1d41d6d3":"code","93a26b56":"code","1f907e43":"code","40e6cb09":"code","c8d09048":"code","b36ce558":"code","5700eabf":"code","7f34763c":"code","aa765770":"code","b32e5ec7":"code","c4fad408":"code","0523feb8":"code","b8bbeaaf":"markdown"},"source":{"1a256146":"!pip install pingouin","62235205":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import iqr\nimport math\nfrom pingouin import multivariate_normality\nfrom scipy.stats import mode\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler \nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier","d19ebd7a":"data = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata_model, data_submission = train_test_split(\n    data,\n    test_size = 0.25,\n    random_state = 42,\n    stratify = data.loc[:, 'Attrition'])","15d0677f":"data_model.head().T","07c6eb85":"data_model.info()","7a920c49":"convert_num_to_cat = ['EmployeeNumber']\n\n# 1. Numeric -> Categorical\ndef convert_to_categorical(x):\n    return x.astype(object)\ndata_model['Attrition'] = data_model['Attrition'].apply(lambda x: 0 if x == 'No' else 1)\ndata_submission['Attrition'] = data_submission['Attrition'].apply(lambda x: 0 if x == 'No' else 1)\n\nfor col in convert_num_to_cat:\n    data_model[col] = data_model.loc[:, [col]].apply(convert_to_categorical)","fe26c2b7":"data_model.info()","4c2238fe":"y = ['Attrition']\ndata_x = data_model.loc[:, [col for col in data_model.columns if col not in y]]\ndata_y = data_model.loc[:, ['Attrition']]\nsubmission_x = data_submission.loc[:, [col for col in data_submission.columns if col not in y]]\nsubmission_y = data_submission.loc[:, ['Attrition']]\ntrain_x, test_x, train_y, test_y = train_test_split(\n    data_x,\n    data_y,\n    test_size = 0.25,\n    random_state = 42,\n    stratify = data_y.loc[:, 'Attrition'])\nall_cols = list(data_x.columns)\nnumeric_cols = [col for col in all_cols if data_x[col].dtypes != object]\ncat_cols = [col for col in all_cols if data_x[col].dtypes == object]\nlen(numeric_cols + cat_cols) == len(all_cols)","213568e0":"# Checking if the stratified split has happened or not\nprint(data_y.loc[:, 'Attrition'].sum() \/ data_y.loc[:, 'Attrition'].count())\nprint(train_y.loc[:, 'Attrition'].sum() \/ train_y.loc[:, 'Attrition'].count())\nprint(test_y.loc[:, 'Attrition'].sum() \/ test_y.loc[:, 'Attrition'].count())\nprint(submission_y.loc[:, 'Attrition'].sum() \/ submission_y.loc[:, 'Attrition'].count())","51044d03":"# Exploratory Data Analysis: Col-wise\n\ndef derive_outlier_perc(ser, iqr_width = 1.5):\n    q1, q3 = ser.quantile(q = 0.25,\n      interpolation = 'linear'), ser.quantile(q = 0.75,\n      interpolation = 'linear')\n    width = iqr_width * (q3 - q1)\n    down_, up_ = q1 - width, q3 + width\n    mask = ser.apply(lambda x: 0 if down_ <= x <= up_ else 1)\n    return mask.sum() \/ mask.shape[0]\ndef derive_negative_perc(ser, threshold = 0):\n    mask = ser.apply(lambda x: 1 if x < threshold else 0)\n    return mask.sum() \/ mask.shape[0]\ndef derive_positive_perc(ser, threshold = 0):\n    mask = ser.apply(lambda x: 1 if x >= threshold else 0)\n    return mask.sum() \/ mask.shape[0]\n    \nunivariate_numeric = train_x.loc[:, numeric_cols].describe().T\nunivariate_categorical = train_x.loc[:, cat_cols].describe().T\nunivariate_numeric['perc_missing'] = 1 - univariate_numeric['count'] \/ train_x.shape[0]\nunivariate_numeric['skew'] = [train_x[col].skew() for col in univariate_numeric.index]\nunivariate_numeric['kurt'] = [train_x[col].kurt() for col in univariate_numeric.index]\nunivariate_numeric['perc_outliers'] = [derive_outlier_perc(train_x[col]) for col in univariate_numeric.index]\nunivariate_numeric['perc_negatives'] = [derive_negative_perc(train_x[col]) for col in univariate_numeric.index]\nunivariate_numeric['perc_positives'] = [derive_positive_perc(train_x[col]) for col in univariate_numeric.index]\nunivariate_categorical['perc_missing'] = 1 - univariate_categorical['count'] \/ train_x.shape[0]\nunivariate_categorical['mode_freq'] = univariate_categorical['freq'] \/ univariate_categorical['count']","4f5b8949":"# Preparing data for bivariate and row-wise analysis\n\nmissing_cols = []\nirrelevant_cols = ['EmployeeNumber', 'EmployeeCount']\nconstant_cols = ['Over18']\ncols_to_remove = missing_cols + irrelevant_cols + constant_cols\ntrain_x_copy = train_x.loc[:, [col for col in train_x.columns if col not in cols_to_remove]]\ntrain_x_nm_mask_full = ~pd.isna(train_x_copy)\ntrain_x_nm_mask = (train_x_nm_mask_full.any(axis = 1))\nidentifier = train_x_copy.loc[train_x_nm_mask, :].index\nall_cols = list(train_x_copy.columns)\nnumeric_cols = [col for col in all_cols if train_x_copy[col].dtypes != object]\ncat_cols = [col for col in all_cols if train_x_copy[col].dtypes == object]\nlen(numeric_cols + cat_cols) == len(all_cols)\nnumeric_data, cat_data = train_x_copy.loc[train_x_nm_mask, numeric_cols], train_x_copy.loc[train_x_nm_mask, cat_cols]\nohe = OneHotEncoder(sparse = False)\nohe_data_array = ohe.fit_transform(cat_data.values)\ncat_ohe = pd.DataFrame(ohe_data_array, columns = ohe.get_feature_names(), index = identifier)\ntrain_x_analysis = pd.concat(\n    objs = (numeric_data, cat_ohe),\n    axis = 1,\n    join = 'outer',\n    ignore_index = False,\n    copy = True)\ntrain_x_analysis.index = identifier","429feec6":"# Exploratory Data Analysis: Bivariate Analysis\n\n# X ~ y\n_, pvals = chi2(X = train_x_analysis, y = train_y)\nbivariate_x_y = pd.DataFrame(pvals, index = train_x_analysis.columns, columns = ['chi2_pvals'])\nbivariate_x_y.sort_values(by = 'chi2_pvals', ascending = False, inplace = True)\n\n# X ~ X\nbivariate_x_x = train_x_analysis.corr()","698fbe45":"# Exploratory Data Analysis: Row-wise analysis\n\n# Missing\nthreshold_perc_n_missing = 0.50\nthreshold_n_missing = math.ceil(train_x_nm_mask_full.shape[1] * threshold_perc_n_missing)\ntrain_x_nm_count = train_x_nm_mask_full.sum(axis = 1)\nmissing_ = train_x_nm_count < threshold_n_missing\n\n# Grouping\ncluster_data = pd.concat(\n    objs = (train_x_analysis, train_y),\n    axis = 1,\n    join = 'outer',\n    ignore_index = False,\n    copy = True)\ncluster_data_standardized = pd.DataFrame(\n    StandardScaler().fit_transform(X = cluster_data.values), \n    index = cluster_data.index, \n    columns = cluster_data.columns)\ndef return_inertia(n_clusters):\n    kmeans = KMeans(\n        n_clusters = n_clusters,\n        init = 'k-means++',\n        n_init = 10,\n        max_iter = 300,\n        verbose = 0,\n        random_state = 42,\n        copy_x = True,\n        algorithm = 'auto')\n    kmeans.fit(X = cluster_data.values)\n    return kmeans.inertia_\ninertias = list(map(return_inertia, np.arange(10, 15)))\nop_n_cluster = np.argmin(np.array(inertias)) + 1\nkmeans = KMeans(\n    n_clusters = op_n_cluster,\n    init = 'k-means++',\n    n_init = 10,\n    max_iter = 300,\n    verbose = 0,\n    random_state = 42,\n    copy_x = True,\n    algorithm = 'auto')\nkmeans.fit(X = cluster_data.values)\ngroup = pd.DataFrame(kmeans.labels_, index = identifier) #This excludes the rows where there are missing values\n\n# Outlier Detection\ndb = DBSCAN(eps = 7.5) #Chosen this to get the desired no of outliers\ncluster_pred = pd.Series(db.fit_predict(X = cluster_data_standardized), index = cluster_data_standardized.index)\noutlier = cluster_pred.apply(lambda x: True if x < 0 else False)\noutlier.sum() \/ outlier.shape[0]","0e13a4c7":"# Exploratory Data Analysis: Overall\n\n# Testing for normality\nresult = multivariate_normality(X = cluster_data_standardized.values, alpha = 0.05)\npval = result.pval","1d41d6d3":"# Preparing the data for modeling: Excluding columns\ndataset = dict(\n    train_x_model = train_x,\n    test_x_model = test_x,\n    train_y_model = train_y,\n    test_y_model = test_y,\n    submission_x_model = submission_x,\n    submission_y_model = submission_y)\nexclude_datanames = ['train_y_model', 'test_y_model', 'submission_y_model']\nfor data_name in dataset:\n    if data_name in exclude_datanames:\n        continue\n    data = dataset[data_name].copy(deep = False)\n    data = data.loc[:, [col for col in data.columns if col not in cols_to_remove]]\n    dataset.update({data_name: data})","93a26b56":"# Preparing the data for modeling: Excluding rows\nexclude_datanames = ['test_x_model', 'submission_x_model', 'test_y_model', 'submission_y_model']\nfor data_name in dataset:\n    if data_name in exclude_datanames:\n        continue\n    data = dataset[data_name].copy(deep = False)\n    data = data.loc[~outlier.values, :]\n    dataset.update({data_name: data})","1f907e43":"# Preparing the data for modeling: Imputing missing values: None as there are no missing values","40e6cb09":"# Preparing the data for modeling: OneHotEncode the categorical features\n\nnumeric_cols = [col for col in dataset['train_x_model'].columns if dataset['train_x_model'][col].dtypes != object]\ncat_cols = [col for col in dataset['train_x_model'].columns if dataset['train_x_model'][col].dtypes == object]\nlen(numeric_cols + cat_cols) == len(all_cols)\ndef ohe_data(data):\n    identifier = data.index\n    numeric_data, cat_data = data.loc[:, numeric_cols], data.loc[:, cat_cols]\n    ohe = OneHotEncoder(sparse = False)\n    ohe_data_array = ohe.fit_transform(cat_data.values)\n    cat_ohe = pd.DataFrame(ohe_data_array, columns = ohe.get_feature_names(), index = identifier)\n    c_data = pd.concat(\n        objs = (numeric_data, cat_ohe),\n        axis = 1,\n        join = 'outer',\n        ignore_index = False,\n        copy = True)\n    return c_data\nexclude_datanames = ['train_y_model', 'test_y_model', 'submission_y_model']\nfor data_name in dataset:\n    if data_name in exclude_datanames:\n        continue\n    data = dataset[data_name].copy(deep = False)\n    data_ = ohe_data(data = data)\n    dataset.update({data_name: data_})","c8d09048":"# Preparing the data for modeling: Reducing dimensionality: Manual feature selection: None","b36ce558":"# Preparing the data for modeling: Reducing dimensionality: Algorithmic feature selection\n\n# Determine the n_components\npca = PCA(n_components = 2)\npca.fit(X = dataset['train_x_model'].values)\npca.explained_variance_ratio_\nopt_n_components = 2\n\n# Reduce the data using the opt_n_components\nopt_pca = PCA(n_components = opt_n_components)\ndef reduce_data(data):\n    data = opt_pca.fit_transform(data.values)\n    return data\nexclude_datanames = ['train_y_model', 'test_y_model', 'submission_y_model']\nfor data_name in dataset:\n    if data_name in exclude_datanames:\n        continue\n    data = dataset[data_name].copy(deep = False)\n    data_ = reduce_data(data = data)\n    dataset.update({data_name: data_})","5700eabf":"# Select a performance measure\n\neval_metric = accuracy_score","7f34763c":"# Selecting the standalone models:\n# 1.a. Unsupervised approach: Nearest Neighbor approach\n# 1.b. Supervised approach: Logistic Regression, Ridge Classifier, SGD Classifier with default parameters\n\nmodels = []\nmodels.append(NearestNeighbors(n_neighbors = 5))\nmodels.append(LogisticRegression())\nmodels.append(RidgeClassifier())\nmodels.append(SGDClassifier())\n\n# Prediction functions\n# NN Model\ndef predict_and_score_from_nn_model(train_x, test_x, train_y, test_y, model, return_pred = False):\n    for data in [train_x, test_x, train_y, test_y]:\n        data = data.values if isinstance(data, pd.DataFrame) else data\n    model.fit(X = train_x, y = None)\n    _, indices = model.kneighbors(X = test_x)\n    y_pred = []\n    for neighbors in indices:\n        pred_point = mode(train_y.values[neighbors, -1])[0][0]\n        y_pred.append(pred_point)\n    if return_pred:\n        return y_pred\n    return eval_metric(y_true = test_y.values, y_pred = y_pred)\ndef predict_and_score_from_other_models(train_x, test_x, train_y, test_y, model, return_pred = False):\n    model = LogisticRegression()\n    model.fit(X = train_x, y = train_y.values)\n    y_pred = model.predict(X = test_x)\n    if return_pred:\n        return y_pred\n    return eval_metric(y_true = test_y.values, y_pred = y_pred)\npredict_and_score = []\npredict_and_score.append(predict_and_score_from_nn_model)\npredict_and_score += [predict_and_score_from_other_models] * 3","aa765770":"# Selecting the ensemble models: None","b32e5ec7":"# Fitting strategy: Non-iterative fitting strategy\n\nscores = []\nfor model_, pred_score in zip(models, predict_and_score):\n    scores.append(pred_score(\n        train_x = dataset['train_x_model'],\n        test_x = dataset['test_x_model'],\n        train_y = dataset['train_y_model'],\n        test_y = dataset['test_y_model'],\n        model = model_))","c4fad408":"# Selecting the best model and submitting the predictions\n\nbest_index = 0\nsubmission = predict_and_score[best_index](\n    train_x = dataset['train_x_model'],\n    test_x = dataset['submission_x_model'],\n    train_y = dataset['train_y_model'],\n    test_y = None,\n    model = models[best_index],\n    return_pred = True)","0523feb8":"submission","b8bbeaaf":"Following algorithm undertaken to solve the problem\n\nFormatting the columns: X\n1. Num -> Cat: EmployeeNumber\n2. Cat -> Num: None\n3. Cat -> Date: None\n4. Num -> Date: None\n5. Others: Attrition ()\n\nFormatting the columns: y\n1. Num -> Cat: EmployeeNumber\n2. Cat -> Num: None\n3. Cat -> Date: None\n4. Num -> Date: None\n5. Others: Encoded Attrition: 1 if Yes else 0\n\nIncluding additional features:\n1. Manual: None\n2. Kernel-based: None\n\nExploratory Data Analysis: Col-wise: Univariate\n\n1. Numeric Columns\na. Mean, Median, Percentiles: Done. Used standard formula\nb. Higher-order moments: Done. Used standard formula\nc. Outlier: Done. Used IQR to find % of outliers and outliers mask\nd. perc_negatives, perc_positives\ne. perc_missing\n\n2. Categorical Columns\na. Count, Unique, Mode, Freq of Mode,\nb. Missing_perc\nc. Mode_freq_perc\n\nPreparing data for bivariate exploratory analysis and row-wise exploratory analysis:\na. Cols removed\nb. Rows removed\nc. Cat -> Num\n\nExploratory Data Analysis: Col-wise: Bivariate\n\n1. Numeric ~ y. Used sklearn.feature_selection.chi2 since all features are non-negative\n2. Categorical ~ y. Same as above\n3. Numeric ~ Numeric. Used correlation\n4. Numeric ~ Categorical. Same as above\n5. Categorical ~ Categorical. Same as above\n\nExploratory Data Analysis: Row-wise:\n\n1. Missing rows: Where 50% of cols is missing\n2. Clustering: Used KMeans with default parameters to determine the clusters\n3. Outlier Detection: Unsupervised Outlier Detection using DBScan Clustering\n4. Exploratory Data Analysis: Overall\na. Linearity (For regression problem)\nb. Linearly separable (For classification problem)\n\nPreparing the data for modeling: Dimensionality Reduction: Excluding rows and irrelevant columns\n\n1. Excluding irrelevant features: features having high missing values or relatively constant or perfectly correlated features\n2. Excluding rows: outlier rows\n3. Preparing the data for modeling: Imputing missing values: None\na. Basis Mean: None\nb. Basis Median: None\nc. Basis Mode: None\nd. Custom: None\n\nPreparing the data for modeling: Reducing dimensionality\n\n1. Manual Feature Selection: None\n2. Algorithmic: Used PCA\n\nSelecting a performance measure: Chose accuracy_score\n\nSelecting candidate models and parameters: \n1. Standalone models: \n1.a. Unsupervised approach: Nearest Neighbor approach\n1.b. Supervised approach: Logistic Regression, Ridge Classifier, SGD Classifier with default parameters\n2. Ensemble models: None\n\n\nSelecting fitting strategy and fit the models: Non-Iterative fitting strategy\n\nEvaluation and selecting the best model: SGD classifier\n\nSubmitting the results"}}