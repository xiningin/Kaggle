{"cell_type":{"b2184428":"code","92ee7593":"code","49a41458":"code","23121be5":"code","dc25feef":"code","b563868e":"code","08caa02e":"code","ef42958e":"code","0f22c86f":"code","96c0a3e6":"code","4a7d18e4":"code","59834195":"code","f4e03dcc":"code","b04bdb18":"code","5ad8e253":"code","03d9f5aa":"code","c121e5be":"code","f583cf2e":"code","787244da":"code","f98142d1":"code","539078b1":"code","42a399b4":"code","1a7f469a":"code","481883a1":"code","63193b54":"code","8b459fe9":"code","fc0f28ef":"code","e0f0f053":"code","09daa1af":"code","2b98102a":"code","0270861b":"code","9f51a04a":"markdown","e215cfe9":"markdown","dfaae726":"markdown","c50612ac":"markdown","5d1752a6":"markdown","0605b84a":"markdown","672061fb":"markdown","b1216198":"markdown","5c95870c":"markdown","2a0a7836":"markdown","abdb6399":"markdown","fa559b1c":"markdown","da2f5138":"markdown","a0c8718c":"markdown"},"source":{"b2184428":"import numpy as np\nimport pandas as pd\n\nfrom pandas_profiling import ProfileReport\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nplt.style.use(\"fivethirtyeight\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n%matplotlib inline","92ee7593":"data = pd.read_csv(\"..\/input\/top-1000-kaggle-datasets\/kaggle_-1000.csv\")","49a41458":"data.head()","23121be5":"data.drop(\"Unnamed: 0\",axis = 1, inplace = True)","dc25feef":"#preparing the column before main preprocessing. data[\"size\"][785] = ' 2 Files (CSV)' originally.\n# Source: https:\/\/www.kaggle.com\/fivethirtyeight\/fivethirtyeight-comic-characters-dataset?select=dc-wikia-data.csv\ndata[\"size\"][785] = \" 3.52 MB\"","b563868e":"# Trying to equalize the units.\ndef size_column_regulator(data):\n    if data[-2:] == \"kB\":\n        data = (float(data[1:-3]))\/1024\n    elif data[-2:] == \"MB\":\n        data = float(data[1:-3])\n    elif data[-2:] == \"GB\":\n        data = (float(data[1:-3]))*1024 \n    elif data[-2:] == \" B\":\n        data = (float(data[1:-3]))\/(1024**2)   \n    return data\n\n\ndata.insert(6,\"size_in_MB\",data[\"size\"].apply(lambda x: size_column_regulator(x)), True)\ndata.drop(\"size\", axis = 1, inplace = True)\n\n# Replace the BigQuery elements with the maximum file size in dataset.\n\n# The following limits apply to BigQuery datasets:\n# Maximum number of datasets: There is no limit on the number of datasets that a project can have.\n# [https:\/\/cloud.google.com\/bigquery\/quotas]\n\nmax_value_for_imputation = pd.to_numeric(data[data.size_in_MB.str.isnumeric() != False][\"size_in_MB\"]).max()\ndata[\"size_in_MB\"] = data[\"size_in_MB\"].replace(\" BigQuery\", max_value_for_imputation)\ndata[\"size_in_MB\"] = pd.to_numeric(data[\"size_in_MB\"])","08caa02e":"data.info()","ef42958e":"# To detect how many file(s) each dataset has.\n\ndef file_number_regulator(data):\n    \n    for i in data.split():\n        if i.isdigit() == True:\n            data = int(i)\n        elif i == 'BigQuery':\n            data = 0\n        else:\n            pass\n    \n    return data\n\ndata.insert(5, \"number_of_files\", data[\"files\"].apply(lambda x: file_number_regulator(x)), True)\nmaximum_number_of_files = data.number_of_files.max()\ndata.number_of_files = data.number_of_files.replace(0, maximum_number_of_files)\ndata.head()","0f22c86f":"# To detect which type of file(s) each dataset contains.\n# Some data have placed wrong. (data.files[238] = 16 MB for instance).\n# I ignored them.\n\ndef file_type_regulator(data):\n    CSV = []\n    JSON = []\n    OTHER = []\n    SQLITE = []\n    BIGQUERY = []\n    \n    ident_list = data.files.str.lower().str.contains(\"csv\")\n    for i in ident_list:\n        if i == True:\n            CSV.append(1)\n        else:\n            CSV.append(0)\n    \n    ident_list = data.files.str.lower().str.contains(\"json\")\n    for i in ident_list:\n        if i == True:\n            JSON.append(1)\n        else:\n            JSON.append(0)\n            \n    ident_list = data.files.str.lower().str.contains(\"other\")\n    for i in ident_list:\n        if i == True:\n            OTHER.append(1)\n        else:\n            OTHER.append(0)\n            \n    ident_list = data.files.str.lower().str.contains(\"sqlite\")\n    for i in ident_list:\n        if i == True:\n            SQLITE.append(1)\n        else:\n            SQLITE.append(0)\n            \n    ident_list = data.files.str.lower().str.contains(\"bigquery\")\n    for i in ident_list:\n        if i == True:\n            BIGQUERY.append(1)\n        else:\n            BIGQUERY.append(0)\n    data.insert(5, \"CSV\", CSV)\n    data.insert(5, \"JSON\", JSON)\n    data.insert(5, \"OTHER\", OTHER)\n    data.insert(5, \"SQLITE\", SQLITE)\n    data.insert(5, \"BIGQUERY\", BIGQUERY)\n    \n    return data","96c0a3e6":"data = file_type_regulator(data)\ndata.drop(\"files\", axis = 1, inplace = True)\ndata.head()","4a7d18e4":"data[\"last_updated\"] = data[\"last_updated\"].str.replace(\"a year\", \"1 year\")\ndata[\"last_updated\"] = data[\"last_updated\"].str.replace(\"a month\", \"1 month\")\ndata[\"last_updated\"] = data[\"last_updated\"].str.replace(\"a day\", \"1 day\")","59834195":"def update_column_regulator(data):\n    num = 0\n    multipicate = 0\n    for i in data.split():\n        if i.isdigit() == True:\n            num += int(i)\n            \n        if (i == \"days\") or (i == \"day\"):\n            multipicate += 1\n        if (i == \"months\") or (i ==\"month\"):\n            multipicate += 30\n        if (i == \"years\") or (i == \"year\"):\n            multipicate += 360\n        else:\n            continue\n    \n    data = num*multipicate\n    \n    return data\n            ","f4e03dcc":"data.insert(2, \"Last_Updated_Days\", data.last_updated.apply(lambda x: update_column_regulator(x)), True)\ndata.drop(\"last_updated\", axis = 1, inplace = True)\ndata.head()","b04bdb18":"data.drop_duplicates(inplace = True)","5ad8e253":"print(f\"Number of duplicate rows: {len(data[data.duplicated()==True])}\")","03d9f5aa":"data.info()","c121e5be":"ProfileReport(data)","f583cf2e":"# Creating a new dataframe to visualize which kaggle users take the most upvotes.\nupvotes = data.groupby(\"uploaded_by\")[\"upvotes\"].sum().reset_index().sort_values(\"upvotes\", ascending = False).reset_index().drop(\"index\", axis = 1)\n\nplt.figure(figsize = (10,5))\nsns.barplot(x = \"uploaded_by\",\n           y = \"upvotes\",\n           data = upvotes[:20], palette = \"Blues_r\")\nplt.xticks(rotation = 90)\nplt.title(\"Top 20 Upvoted Users in Kaggle Datasets\")\nplt.xlabel(\"\")\nplt.show()","787244da":"data.head(3)","f98142d1":"# Creating a new frame consisting only the most upvoted kaggle users\nnew_frame1 = data[data.uploaded_by == upvotes.uploaded_by[0]]\nnew_frame2 = data[data.uploaded_by == upvotes.uploaded_by[1]]\nnew_frame3 = data[data.uploaded_by == upvotes.uploaded_by[2]]\nnew_frame4 = data[data.uploaded_by == upvotes.uploaded_by[3]]\nnew_frame5 = data[data.uploaded_by == upvotes.uploaded_by[4]]\n\nnew_frame = pd.concat([new_frame1,new_frame2,new_frame3,new_frame4,new_frame5])\nnew_frame.head()","539078b1":"new_frame_for_file_types = new_frame.groupby(\"uploaded_by\")[\"BIGQUERY\",\"SQLITE\",\"OTHER\",\"JSON\",\"CSV\"].sum().reset_index()\n\nfig = px.bar(data_frame = new_frame_for_file_types,\n    x = \"uploaded_by\",\n    y = [\"BIGQUERY\",\"SQLITE\",\"OTHER\",\"JSON\",\"CSV\"])\n\nfig.update_layout(title = \"File Distribution of Most Upvoted Kaggle Users\")\nfig.show()","42a399b4":"new_frame_for_usability = new_frame.groupby(\"uploaded_by\")[\"usability\"].mean().reset_index().sort_values(\"usability\", ascending = False).reset_index().drop(\"index\", axis = 1)\n\nfig = px.bar(x = \"uploaded_by\",\n            y = \"usability\",\n            data_frame = new_frame_for_usability,\n            color = \"usability\",\n             color_continuous_scale = \"Peach\")\nfig.show()","1a7f469a":"plt.figure(figsize = (10,5))\nsns.countplot(new_frame.badge)\nplt.xlabel([])\nplt.ylabel(\"Number of Badges\")\nplt.title(\"Total Number of the Badges for Each Type\")\nplt.show()","481883a1":"fig = px.pie(data_frame = new_frame,\n    names = \"badge\",\n    color_discrete_sequence = [\"#A4160B\", \"#ffd966\"],\n             hole = 0.4\n             \n    )\nfig.update_layout(title = \"The Ratio of The Badges Belonging to The Most Upvoted Kaggle Users\")\nfig.show()","63193b54":"fig, ax = plt.subplots(nrows = 1, ncols = 5, figsize = (20,5))\n\nax = ax.flatten()\n\nfor ind, axis in enumerate (ax):\n    \n    ax = sns.countplot(ax = axis ,\n                       data = new_frame,\n                       x = new_frame[new_frame.uploaded_by == upvotes.uploaded_by[ind]].badge)\n                      \n                      \n    ax.set_xlabel(f\"User: {upvotes.uploaded_by[ind]}\")\n    ax.set_ylabel(\"\")\nfig.tight_layout()\nplt.suptitle(\"Users and Badges\",va = 'baseline',fontsize = 15)\n\nplt.show()","8b459fe9":"fig, ax = plt.subplots(nrows = 1, ncols = 5, figsize = (20,5))\n\nax = ax.flatten()\n\nfor ind, axis in enumerate (ax):\n    \n    axis.pie(x = new_frame[new_frame.uploaded_by == upvotes.uploaded_by[ind]].badge.value_counts(),\n                 autopct = \"%.1f\", \n                 labels = new_frame[new_frame.uploaded_by == upvotes.uploaded_by[ind]].badge.value_counts().index,\n             colors = [\"#e57c5e\", \"#ffd966\"])\n    axis.set_title(f\"User: {upvotes.uploaded_by[ind]}\")\n                                   \n                    \n    \n    \nfig.tight_layout()\nplt.suptitle(\"Users and Badges (Pie Charts)\",va = 'baseline',fontsize = 15)\n\nplt.show()","fc0f28ef":"data.head()","e0f0f053":"y = data.pop(\"badge\")\ny = pd.get_dummies(y,drop_first = True)\n\nX = data.iloc[:,2:]","09daa1af":"ss = StandardScaler()\nX = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\nX.head()","2b98102a":"model = Sequential([\n                    Dense(100, activation = \"swish\", input_shape = (X.shape[1],)),\n                    Dropout(0.5),\n                    Dense(50, activation = \"swish\"),\n                    Dropout(0.5),\n                    Dense(25,activation = \"swish\"),\n                    Dropout(0.5),\n                    Dense(3, activation = \"softmax\")\n])\n\nmodel.summary()","0270861b":"model.compile(optimizer = \"adam\",\n              loss = \"categorical_crossentropy\",\n              metrics = AUC())\n\nhistory = model.fit(X,y, validation_split = 0.2,\n                     callbacks = [EarlyStopping(patience = 30),ReduceLROnPlateau(factor = 0.05, patience = 10, mode = \"max\")],\n                     epochs = 100)","9f51a04a":"### 2.2.1. Which Kaggle Users Got the Most Upvotes? ","e215cfe9":"### 2.2.4.1. Numbers and Ratios of the Badges (Globally)","dfaae726":"## Pandas Profiling","c50612ac":"Thanks...","5d1752a6":"# Content of This Notebook\n\n1. [Analyzing and Preprocessing the Data](#1)\n1. [EDA](#2)\n1. [Designing a Prediction Algorithm](#3)","0605b84a":"### 2.2.4.2. Numbers and Ratios of the Badges (Individually)","672061fb":"<a id='2'><\/a>\n# 2. EDA","b1216198":"## 2.2. Couple Additions to EDA\n\nPandas profiling is a great tool as I have told before. However, it is not enough for a complete EDA. So in this part, we'll fill the the missing parts of the data visualization. \n\nSince we can observe too many facts from this graph, we'll continue the EDA mainly on 5 top most upvoted kaggle users.  \n\nThe things that we will adress in this part are,\n\n1. Which kaggle users got the most upvotes?\n1. Which file types have they uploaded the most?\n1. What are their average usability points?\n1. Numbers and ratios of badges (we'll observe both globally and individually)\n","5c95870c":"Pandas Profiling is a great tool for analyzing data deeply numerically and graphically. Most of the time, we use pd.describe() and pd.info() to analyze the data and then continue to this process with exploratory data analysis (EDA) by using several modules like matplotlib, seaborn and plotly. Even though they are useful, for the sake of convenience or to detect the problems in the dataset that we missed (I missed duplicate rows for instance while I was preparing this notebook), we can use Pandas Profiling. ","2a0a7836":"### 2.2.3 What are Their Average Usability Points?","abdb6399":"<a id='3'><\/a>\n# 3. Designing a Prediction Algorithm","fa559b1c":"<a id='1'><\/a>\n# 1. Analyzing and Preprocessing the Data","da2f5138":"### 2.2.2. Which File Types Have They Uploaded the Most?","a0c8718c":"# Introduction\n\nIn this notebook, we will,\n\n1. Analyze and preprocess the data.\n1. EDA using pandas profiling\n1. Complete the missing parts of the EDA that pandas profiling was inferior to explain.\n1. Design an ANN algorithm to predict the type of badge a dataset has achieved based on other parameters.\n\nI hope you'll enjoy!\n"}}