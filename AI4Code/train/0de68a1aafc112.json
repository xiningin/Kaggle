{"cell_type":{"fdc30a36":"code","131b2096":"code","e33ac181":"code","f4fe653b":"code","7674ea47":"markdown","6dea83cc":"markdown","276bf56c":"markdown","3cf11521":"markdown","c0feaf9f":"markdown","463d1383":"markdown","e9f1a540":"markdown","7ec1ae24":"markdown","9e6458d7":"markdown"},"source":{"fdc30a36":"import numpy as np\n\nsentence1 = np.array([[0.31,0.14,0.93],[0.14,0.88,0.98]]) # \"Popcorn Popped.\"\nsentence2 = np.array([[0.85,0.2,0.14],[0.46,0.61,0.49]]) # \"Tea Steeped.\"","131b2096":"# Average and Variace for First Sentence:\naverage1 = sentence1.mean()\nvariance1 = sentence1.var()\n\n# Average and Variance for Second Sentence:\naverage2 = sentence2.mean()\nvariance2 = sentence2.var()","e33ac181":"# Sentence 1 Normalization:\nsentence1_norm = (sentence1 - average1)\/(np.sqrt(variance1))\nprint(f\"Sentence1:\\n{sentence1}\\n\\n Sentence1 (Normalized):\\n{sentence1_norm}\\n\")\n\n# Sentence 2 Normalization:\nsentence2_norm = (sentence2 - average2)\/(np.sqrt(variance2))\nprint(f\"Sentence2:\\n{sentence2}\\n\\n Sentence2 (Normalized):\\n{sentence2_norm}\")\n","f4fe653b":"import torch\n\ntorch1 = torch.from_numpy(sentence1) # \"Popcorn Popped.\"\ntorch2 = torch.from_numpy(sentence2) # \"Tea Steeped.\"\n\nlayer_norm = torch.nn.LayerNorm(torch1.size())\n\n# Sentence 1 Normalization:\ntorch1_norm = layer_norm(torch1.float())\nprint(f\"Sentence1:\\n{torch1}\\n\\n Sentence1 (Normalized):\\n{torch1_norm}\\n\")\n\n# Sentence 2 Normalization:\ntorch2_norm = layer_norm(torch2.float())\nprint(f\"Sentence2:\\n{torch2}\\n\\n Sentence2 (Normalized):\\n{torch2_norm}\")","7674ea47":"# How does Layer Normalization Work?\n\nLayer normalization was initially intended to be used in Recurrent neural networks because the result of batch normalization is depending on the mini-batch size and it is not clear how to apply it to RNNs. But it actually became a thing after \"Attention is all you need\" and introduction of Transformer architecture.\nThe developers of Transformer architecture chose it as their preferred method of normalization throughout the model because it performs exceptionally well, especially in NLP tasks.\n\n**But what exactly is layer normalization, and why we should normalize our data? Let\u2019s begin with the later question.**","6dea83cc":"## Layer Normalization\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https:\/\/i.ibb.co\/tpDnBBg\/Picture1.png\">\n<\/div>\n\nIn the layer norm, we take the average and variance from all of the features of a single sentence. \nLet\u2019s see what it means using the same two sentences: \n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https:\/\/i.ibb.co\/4YSxCqr\/Picture1.png\">\n<\/div>\n\nHere we don\u2019t care about the fact that these two sentences are from the same batch. In order to obtain the average and variance, we simply use all of the features in every sentence:\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https:\/\/i.ibb.co\/2ytx9Jh\/Picture1.png\">\n<\/div>\n\nAnd again, after normalization, we\u2019ll have matrices with average of 0 and variance of 1:\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https:\/\/i.ibb.co\/42b38Q8\/Picture1.png\">\n<\/div>","276bf56c":"Now we Calculte Average and Variance for each of these Sentences. ","3cf11521":"## Batch Normalization\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https:\/\/i.ibb.co\/60m5Dm3\/Picture1.png\">\n<\/div>\n\nIn batch norm, we take all sentences in a batch, and for each feature in these sentences, we can find an average and a variance, which will be used to normalize the data in that feature. \n\nFor example, Imagine that we have a batch of 2 senteces: \u201c`Popcorn popped.`\u201d and \u201c`Tea steeped.`\u201d you can see that each sentence is displayed by a matrix in which each row represents a word:\n\n<div align=\"center\">\n  <img width=\"320\" height=\"360\" src=\"https:\/\/i.ibb.co\/4YSxCqr\/Picture1.png\">\n<\/div>\n\nIn batch norm, we take one feature and calculate the average and variance of it. \n\n<div align=\"center\">\n  <img width=\"320\" src=\"https:\/\/i.ibb.co\/TPr8bM2\/Picture1.png\">\n<\/div>\n\nAnd then normalize the data so that the average is near zero and variance is about one. Here is the formula:\n\n$$x_{norm}=\\frac{x-avg(x)}{\\sqrt{var(x)}}$$\n\n<div align=\"center\">\n  <img width=\"320\" src=\"https:\/\/i.ibb.co\/gDKgrLk\/Picture1.png\">\n<\/div>\n\nOf course, we should repeat this for other features as well.","c0feaf9f":"Now we normalize by applying following equation on our matrices:\n$$x_{norm}=\\frac{x-avg(x)}{\\sqrt{var(x)}}$$","463d1383":"You could also Simply use the [LayerNorm](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LayerNorm.html) Implementation from Pytorch Library:","e9f1a540":"## Layer Norm in code\n\nNow we want to implement what I just described in code. I create two numpy arrays, `sentence1` and `sentence2`, which are the same dummy matrices that we used in the illustrations:","7ec1ae24":"#### \ud83d\udca1 If you liked this post, you'd probably like [my Video on Transformers](https:\/\/youtu.be\/XowwKOAWYoQ) as well. in that video, I try to present a comprehensive study on Ashish Vaswani and his coauthors' renowned paper, \u201cAttention is All You Need\u201d.","9e6458d7":"## Benefits of Normalization\n\n<div align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https:\/\/i.ibb.co\/0p0r6VW\/Capture.png\">\n<\/div>\n\nNormalization is good for your model. It reduces training time, unbiases model to higher value features and doesn\u2019t allow weights to explode all over the place and restricts them to a certain range. \nAll in all, It is undesirable to train a model with\u00a0gradient descent\u00a0with non-normalized features.\n\nThere are more then one way to perform normalization, two of which are presented here:\n\n<div align=\"center\">\n  <img width=\"460\" height=\"300\" src=\"https:\/\/i.ibb.co\/BfgTjGQ\/Picture1.png\">\n<\/div>\n\nthe main difference between these normalization methods is the way we calculate average and variance in order to normalize our data.\nYou are probably familiar with the one on the right, the **batch norm**. \n"}}