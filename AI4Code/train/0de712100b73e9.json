{"cell_type":{"76a67ed9":"code","75b939c7":"code","d1690706":"code","8c2f41d4":"code","2b272e00":"code","c1cca384":"code","40ea0fd3":"code","f99590c2":"code","a8b03432":"code","6e9062dd":"code","c7962e7c":"code","3a07b164":"code","389679ab":"code","f43b9ccb":"code","47013fd7":"code","008dccc7":"code","fbf18cf4":"code","cc9cde45":"code","c375d5f6":"code","060e6326":"code","d5704bcc":"code","7a6661e9":"code","3a4ff00f":"code","a8c900bc":"code","69e3c12e":"code","7a538f1c":"markdown","00e7333d":"markdown","953c300c":"markdown","825a8427":"markdown","0d431d71":"markdown","125c30d9":"markdown","2d95c70c":"markdown","f5435178":"markdown","a660818f":"markdown","47042486":"markdown","17acd132":"markdown","322e0919":"markdown","5a7f3eb0":"markdown","7b21912d":"markdown","81de5911":"markdown","1a8e788b":"markdown","c0b8992c":"markdown","74588cc8":"markdown","5a7a7a8f":"markdown","20631eea":"markdown","b8232a13":"markdown"},"source":{"76a67ed9":"#The AutoML we are using here is pycaret, this is the step to install pycaret.\n!pip install pycaret","75b939c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Data visualisation \nimport seaborn as sns #Data visualisation \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d1690706":"# Getting the dataset to \"dataset\" variable\ndataset = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","8c2f41d4":"# Showing first 5 rows.\ndataset.head()","2b272e00":"dataset.shape","c1cca384":"countNoDisease = len(dataset[dataset.target == 0])\ncountHaveDisease = len(dataset[dataset.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(dataset.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(dataset.target))*100)))","40ea0fd3":"sns.countplot(x=\"target\", data=dataset, palette=\"bwr\")\nplt.show()","f99590c2":"sns.countplot(x='sex', data=dataset, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","a8b03432":"countFemale = len(dataset[dataset.sex == 0])\ncountMale = len(dataset[dataset.sex == 1])\nprint(\"Percentage of Female Patients: {:.2f}%\".format((countFemale \/ (len(dataset.sex))*100)))\nprint(\"Percentage of Male Patients: {:.2f}%\".format((countMale \/ (len(dataset.sex))*100)))","6e9062dd":"pd.crosstab(dataset.age,dataset.target).plot(kind=\"bar\",figsize=(30,15))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","c7962e7c":"pd.crosstab(dataset.sex,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","3a07b164":"plt.scatter(x=dataset.age[dataset.target==1], y=dataset.thalach[(dataset.target==1)], c=\"red\")\nplt.scatter(x=dataset.age[dataset.target==0], y=dataset.thalach[(dataset.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","389679ab":"pd.crosstab(dataset.slope,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=['#DAF7A6','#FF5733' ])\nplt.title('Heart Disease Frequency for Slope')\nplt.xlabel('The Slope of The Peak Exercise ST Segment ')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency')\nplt.show()","f43b9ccb":"pd.crosstab(dataset.fbs,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=['#FFC300','#581845' ])\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","47013fd7":"pd.crosstab(dataset.cp,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease Frequency According To Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","008dccc7":"data = dataset.sample(frac=0.95, random_state=786).reset_index(drop=True)\ndata_unseen = dataset.drop(data.index).reset_index(drop=True)\n\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions ' + str(data_unseen.shape))","fbf18cf4":"# Imporing pycaret classification method\nfrom pycaret.classification import *\n\n# This is the first step of model selection\n# Here the data is our dataset, target is the labeled column(dependent variable), section is is random number for future identification.\nexp = setup(data = data, target = 'target', session_id=1,\n                  normalize = True, \n                  transformation = True, \n                  ignore_low_variance = True,\n                  remove_multicollinearity = True, multicollinearity_threshold = 0.95 )\n\n# After this we will get a list of our columns and its type, just conferm they are the same. Then hit enter.","cc9cde45":"#This comand is used to compare different models with our dataset.\n#The acuuracy,F1 etc of each model is listed in a table.\n#Choose which model you want\ncompare_models()","c375d5f6":"# With this command we are creating a Naives Byes model\n# The code for Naives Byes is \" nb \"\n# fold is the number of fold you want\nlda_model = create_model('lda', fold = 10)","060e6326":"tuned_lda = tune_model('lda', optimize='F1')","d5704bcc":"plot_model(tuned_lda, plot = 'auc')","7a6661e9":"plot_model(tuned_lda, plot = 'confusion_matrix')","3a4ff00f":"predict_model(tuned_lda);","a8c900bc":"new_prediction = predict_model(tuned_lda, data=data_unseen)","69e3c12e":"new_prediction","7a538f1c":"# 3. Getting the data","00e7333d":"# 5. Visualisation ","953c300c":"# 9. Creating the model","825a8427":"Logistic Regression \u2018lr\u2019\n\nK Nearest Neighbour \u2018knn\u2019\n\nNaives Bayes \u2018nb\u2019\n\nDecision Tree \u2018dt\u2019\n\nSVM (Linear) \u2018svm\u2019\n\nSVM (RBF) \u2018rbfsvm\u2019\n\nGaussian Process \u2018gpc\u2019\n\nMulti Level Perceptron \u2018mlp\u2019\n\nRidge Classifier \u2018ridge\u2019\n\nRandom Forest \u2018rf\u2019\n\nQuadratic Disc. Analysis \u2018qda\u2019\n\nAdaBoost \u2018ada\u2019\n\nGradient Boosting Classifier \u2018gbc\u2019\n\nLinear Disc. Analysis \u2018lda\u2019\n\nExtra Trees Classifier \u2018et\u2019\n\nExtreme Gradient Boosting \u2018xgboost\u2019\n\nLight Gradient Boosting \u2018lightgbm\u2019\n\nCat Boost Classifier \u2018catboost\u2019","0d431d71":"![Heart%20Disease%20Symptoms1.jpg](attachment:Heart%20Disease%20Symptoms1.jpg)","125c30d9":"# 14. Checking with the unseen data\n\nInitially we separated a part of the dataset as unseen data set for checking the final deployed model. Below we are checking this. The result is a data frame with Label and score. Where label is the predicted label(which species) and score is how many percentage the machine think this data is that species.","2d95c70c":"In the above table we can see 6 models is showing accuracy of 100%(i.e. 1). For this notebook we use Naive Bayes.\n\nCodes for different models are given below.","f5435178":"# 4. Attribute Information:\n\nIt's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\nage: The person's age in years\n\nsex: The person's sex (1 = male, 0 = female)\n\ncp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\ntrestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\nchol: The person's cholesterol measurement in mg\/dl\n\nfbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\nrestecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nthalach: The person's maximum heart rate achieved\n\nexang: Exercise induced angina (1 = yes; 0 = no)\n\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\nslope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\nca: The number of major vessels (0-3)\n\nthal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\ntarget: Heart disease (0 = no, 1 = yes)","a660818f":"# Heart Disease visualisation & Prediction(for those who are lazy) with AutoML(pycaret)","47042486":"# 1. What you will get from this notebook\n\nThis is bit different from the usual Kaggle works you will see, where most of the others are building the model using the row or can be say as the traditional method. As in this method a large amount of time is wasted in data pre-processing, feature selection, model selection, hyperparameter tuning etc. But now days there are many AutoML which can be easily pip installed and can be used. Many time-consuming works can be easily done with a couple of codes. Most of the times the model accuracy will be greater than the model made by the traditional method.\n\n**This is for those who need a highly accurate, perfect model, with less headache.**\n\n**This is not for those who are trying to study the models deeply.**","17acd132":"# 7. Imporing pycaret classification method","322e0919":"# 11. Ploating the ROC Curves","5a7f3eb0":"This table shows the accuracy and other reading for all 10 folds.\n\nWith this command we are tuning the hyperparameters. For this dataset we are already having 100% accuracy.So without tunnig will be ok.\n\nBut for other difficult dataset tunning the hyperparameters will be very usefull to increase the accuracy and other features\n\nFor unbalanced datasets we mainly look F1 score, as our dataset is balanced we can use the accuracy.","7b21912d":"# 15. Summary\n\nAs we see above, we got a hight accuracy model with 100% accuracy, with no overfitting. This is a very easy dataset that\u2019s why we get very high accuracy. As explained, this is a very simple method to get a very high accurate model. All the works are done by the pycaret.\n\nI am not an expert in this field, if anyone find any errors or suggestion please feel free to command below.\n\nMail ID : jerryjohn1995@gmail.com\n\n**If this notebook finds interesting and useful please upvote.**","81de5911":"# 13. Predicting the accuracy using the test dataset\n\n## We get a accuracy of 0.8506 and F1 score of 0.8632\n","1a8e788b":"# 6. Preparing the data for model selection\n\nIn this step we are splitting the dataset into two. The first part contains 95% of the data that is used for training and testing. The remaining 5% is stored and is used to try out with the final model we deployed (This data is named as unseen data).","c0b8992c":"# 2. Installing necessary packages","74588cc8":"## Checking the data is balanced or not\n\n### As we can see 45% & 54% is a balanced data.","5a7a7a8f":"# 12. Confusion Matrix\n\nHere we can see that every value is Predicted accurately. All are in true positive.","20631eea":"# 8. Comparing the models","b8232a13":"# 10. Tuning the hyper parameters"}}