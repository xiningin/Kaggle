{"cell_type":{"7ede9c0c":"code","6545d0a8":"code","cc9b4dae":"code","97f07a9e":"code","35334dd3":"code","a62df62c":"code","b931aee0":"code","bdcb3134":"code","7f013b0e":"code","ee3def76":"code","0a164204":"code","68043eea":"markdown","c25dcb71":"markdown"},"source":{"7ede9c0c":"######################################################\n# Imports\n\nimport numpy as np\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom IPython.core.pylabtools import figsize\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('ggplot')\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', 60)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV","6545d0a8":"######################################################\n# Datasets\n\ndata_train = pd.read_csv('..\/input\/dataset_treino.csv')\ndata_test = pd.read_csv('..\/input\/dataset_teste.csv')\n\ndata_test.columns = [\n'Order','Property Id', 'Property Name', 'Parent Property Id','Parent Property Name', \n'BBL - 10 digits','NYC Borough, Block and Lot (BBL) self-reported',\n'NYC Building Identification Number (BIN)', 'Address 1 (self-reported)','Address 2', \n'Postal Code', 'Street Number', 'Street Name', 'Borough','DOF Gross Floor Area', \n'Primary Property Type - Self Selected','List of All Property Use Types at Property',\n'Largest Property Use Type','Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n'2nd Largest Property Use Type','2nd Largest Property Use - Gross Floor Area (ft\u00b2)',\n'3rd Largest Property Use Type','3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)', \n'Year Built','Number of Buildings - Self-reported', 'Occupancy','Metered Areas (Energy)', \n'Metered Areas  (Water)','Site EUI (kBtu\/ft\u00b2)', 'Weather Normalized Site EUI (kBtu\/ft\u00b2)',\n'Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)',\n'Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)','Weather Normalized Source EUI (kBtu\/ft\u00b2)', \n'Fuel Oil #1 Use (kBtu)','Fuel Oil #2 Use (kBtu)', 'Fuel Oil #4 Use (kBtu)',\n'Fuel Oil #5 & 6 Use (kBtu)', 'Diesel #2 Use (kBtu)','District Steam Use (kBtu)', \n'Natural Gas Use (kBtu)','Weather Normalized Site Natural Gas Use (therms)',\n'Electricity Use - Grid Purchase (kBtu)','Weather Normalized Site Electricity (kWh)',\n'Total GHG Emissions (Metric Tons CO2e)','Direct GHG Emissions (Metric Tons CO2e)',\n'Indirect GHG Emissions (Metric Tons CO2e)','Property GFA - Self-Reported (ft\u00b2)',\n'Water Use (All Water Sources) (kgal)','Water Intensity (All Water Sources) (gal\/ft\u00b2)',\n'Source EUI (kBtu\/ft\u00b2)', 'Release Date', 'Water Required?','DOF Benchmarking Submission Status',\n'Latitude', 'Longitude','Community Board', 'Council District', 'Census Tract', 'NTA']\n\n\n############################################################################\n## Uni\u00e3o dataset\n\nscore_train = data_train['ENERGY STAR Score']\ndata_train = data_train.drop('ENERGY STAR Score', 1)\ndata_train['score'] = score_train.copy()\ndata_test['score'] = -1\ndata_full = pd.concat([data_train, data_test], sort=False).reset_index(drop=True)\n\nprint('shapes: ', data_full[data_full['score'] != -1].shape, data_full[data_full['score'] == -1].shape, data_full.shape)\n","cc9b4dae":"data_full.head(5)","97f07a9e":"############################################################################\n# Replace Not Available para NaN\n\ndata_full = data_full.replace({'Not Available': np.nan})\n\n\n############################################################################\n# Verificando dados nulos\n\npd.DataFrame(data_full.isnull().sum().sort_values(ascending=False))","35334dd3":"############################################################################\n# Vamos elimitar as colunas com mais de 50% de nulos - os 11 primeiros\n\ncols = pd.DataFrame(data_full.isnull().sum().sort_values(ascending=False)).index[0:11]\ndata_full = data_full.drop(columns = list(cols))\n\n\n\n############################################################################\n## Regras de neg\u00f3cio\n## \n## https:\/\/portfoliomanager.zendesk.com\/hc\/en-us\/articles\/211027418-How-does-EPA-determine-what-variables-to-include-in-the-ENERGY-STAR-score-calculation-\n##\n## Colunas de Water n\u00e3o tem validade para score\n## Colunas de Fuel n\u00e3o tem validade para score\n## Ano de constru\u00e7\u00e3o nao tem validade para score\n## Dados normalizados s\u00e3o mais completos que os puros\n## BBL - 10-digit property borough, block and lot identifier, originally entered into the \"Property\n##    Notes\" field in Portfolio Manager and then verified and corrected, as necessary, by the\n##    Department of Finance (DOF). The first number represents the borough, where 1 is\n##    Manhattan, 2 is the Bronx, 3 is Brooklyn, 4 is Queens, and 5 is Staten Island. The following\n##    five numbers represent the tax block. If a property has a tax block that is less than 5 digits,\n##    then zeros are added before the block number so there are five digits in total. The last four\n##    digits are the tax lot number.\n## \n## C\u00e1lculo do Score: \n## We calculate your Actual Source EUI\n## We calculate your \u201cPredicted\u201d Source EUI for your Office\n## We assume that the predicted Source EUI for your Fitness Center is identical to the predicted Source EUI of your Offic\n## The ratio of your actual source EUI to your predicted source EUI is called the efficiency ratio\n\n############################################################################\n## Considera\u00e7\u00f5es por experimenta\u00e7\u00e3o\n##\n## Utilizar o bairro retirando do BBL - 10-digit, pois a coluna Borough tem mais dados nulos\n## Priorizar colunas de Bairro, Energia, SourceEUI e Tamanho m\u00e1ximo da propriedade\n## Fazer o c\u00e1lculo do rate do score, mesmo n\u00e3o tendo a f\u00f3rmula completa\n## Retirada de colunas com alto grau de correla\u00e7\u00e3o - considerando as regras de negocio e experimenta\u00e7\u00e3o\n\n","a62df62c":"############################################################################\n## Dataframe v2\n\ndata_full_v2 = pd.DataFrame()\ndata_full_v2['Property Id'] = data_full['Property Id']\n\ndata_full_v2['Borough'] = data_full['BBL - 10 digits'].str[0:1]\ndata_full_v2.loc[(data_full_v2['Borough'] == '1'), 'Borough_Named'] = 'Manhattan'\ndata_full_v2.loc[(data_full_v2['Borough'] == '2'), 'Borough_Named'] = 'Bronx'\ndata_full_v2.loc[(data_full_v2['Borough'] == '3'), 'Borough_Named'] = 'Brooklyn'\ndata_full_v2.loc[(data_full_v2['Borough'] == '4'), 'Borough_Named'] = 'Queens'\ndata_full_v2.loc[(data_full_v2['Borough'] == '5'), 'Borough_Named'] = 'Staten'\n\ndata_full_v2['Tax_Block'] = data_full['BBL - 10 digits'].str[1:6]\ndata_full_v2['DOF_Gross_Floor_Area'] = data_full['DOF Gross Floor Area']\ndata_full_v2['Largest_Property_Use_Type'] = data_full['Largest Property Use Type']\ndata_full_v2['Number_Buildings'] = data_full['Number of Buildings - Self-reported']\ndata_full_v2['Weather_Normalized_Site_Electricity_Intensity'] = data_full['Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)']\ndata_full_v2['Source_EUI'] = data_full['Source EUI (kBtu\/ft\u00b2)']\ndata_full_v2['score'] = data_full['score']\n\n\n\n############################################################################\n## Eliminar uma inconsist\u00eancia nos dados\n\ndata_full_v2['Borough'].replace(u'\\u200b', np.nan, inplace=True)\n\n\n\n############################################################################\n## Ajuste dos valores nulos com a mediana das colunas\ndata_full_v2.fillna(data_full_v2.median(), inplace=True)\n\n\n\n############################################################################\n## Convers\u00e3o de tipos\n\ndata_full_v2['Borough'] = data_full_v2['Borough'].astype(int)\ndata_full_v2['Tax_Block'] = data_full_v2['Tax_Block'].astype(int)\ndata_full_v2['Weather_Normalized_Site_Electricity_Intensity'] = data_full_v2['Weather_Normalized_Site_Electricity_Intensity'].astype(float)\n\n\n\n############################################################################\n## Efficience_Ratio \u00e9 dado pelo SourceEUI dividido pelo SourceEUI estimado\n## Existem outras vari\u00e1veis que n\u00e3o consegui acesso, mas j\u00e1 melhora um pouco a performance\n\ndata_mean = data_full_v2.groupby(['Largest_Property_Use_Type'], as_index=False)['Source_EUI'].mean()\n\n\ndata_mean.columns = ['Largest_Property_Use_Type', 'Largest_Property_Use_Type_Source_EUI_Mean']\n\ndata_full_v2 = pd.merge(data_full_v2,\n        data_mean[['Largest_Property_Use_Type', 'Largest_Property_Use_Type_Source_EUI_Mean']],\n                    on='Largest_Property_Use_Type', how='left')\n\ndata_full_v2['Efficience_Ratio'] = (data_full_v2['Source_EUI'] \/ \n                                    data_full_v2['Largest_Property_Use_Type_Source_EUI_Mean']) \n\n\n\n############################################################################\n## Ajuste das colunas Borough_Named e Largest_Property_Use_Type para One Hot Encoding\n\ndata_dummies = pd.get_dummies(data_full_v2['Largest_Property_Use_Type'])\ndata_full_v2 = pd.concat([data_full_v2, data_dummies], sort=False, axis=1)\n\n\ndata_dummies = pd.get_dummies(data_full_v2['Borough_Named'])\ndata_full_v2 = pd.concat([data_full_v2, data_dummies], sort=False, axis=1)\n\n\n\n############################################################################\n## Drop das colunas desnecess\u00e1rias\n\ndata_full_v2 = data_full_v2.drop('Borough', 1)\ndata_full_v2 = data_full_v2.drop('Borough_Named', 1)\ndata_full_v2 = data_full_v2.drop('Source_EUI', 1)\ndata_full_v2 = data_full_v2.drop('Largest_Property_Use_Type', 1)\ndata_full_v2 = data_full_v2.drop('Largest_Property_Use_Type_Source_EUI_Mean', 1)\n\n\ndata_full_v2.head(5)","b931aee0":"############################################################################\n## Correla\u00e7\u00e3o ap\u00f3s ajustes das colunas\n\ncorr = data_full_v2.corr()\n_ , ax = plt.subplots( figsize =( 30 , 30 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 10 })","bdcb3134":"############################################################################\n## Separa\u00e7\u00e3o do train, test e target\n\nfeatures_train = data_full_v2[data_full_v2['score'] != -1]\nfeatures_test = data_full_v2[data_full_v2['score'] == -1]\nProperty_Id_test = features_test[['Property Id']]\ntargets_train = pd.DataFrame(features_train['score'])\n\n\n############################################################################\n## Drop das colunas desnecess\u00e1rias\n\nfeatures_train = features_train.drop('Property Id', 1)\nfeatures_test = features_test.drop('Property Id', 1)\nfeatures_test = features_test.drop('score', 1)\nfeatures_train = features_train.drop('score', 1)\n\n\nX = features_train.values\ny = np.array(targets_train).reshape((-1, ))\n\nX_test = features_test.values","7f013b0e":"############################################################################\n## Tuning par\u00e2metros\n## Obs: Se demorar para rodar pular para pr\u00f3ximo passo\n\nloss = ['ls', 'lad', 'huber']\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nmin_samples_leaf = [1, 2, 4, 6, 8]\nmin_samples_split = [2, 4, 6, 10]\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n\nhyperparameter_grid = {'loss': loss,\n                       'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features\n                      }\n\nmodel = GradientBoostingRegressor(random_state = 42)\n\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv = 8, \n                               n_iter = 25, \n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = -1, \n                               verbose = 1, \n                               return_train_score = True,\n                               random_state=42)\n\nrandom_cv.fit(X, y)\n\nprint (random_cv.best_score_)\nprint (random_cv.best_params_)","ee3def76":"############################################################################\n## Modelo Final\n\nmodel = GradientBoostingRegressor(\n    n_estimators = 500\n    ,min_samples_split = 6\n    ,min_samples_leaf = 6\n    ,max_features = None\n    ,max_depth = 5\n    ,loss = 'lad'\n    ,random_state=42)\n\nmodel.fit(X, y)\n","0a164204":"############################################################################\n## Resultado\n\npred = np.round(model.predict(X_test)).astype(int)\npred[pred > 100] = 100\npred[pred < 1] = 1\n\n\nsubmission = pd.DataFrame({'Property Id': Property_Id_test['Property Id'], 'score': pred })\nsubmission.to_csv('submission.csv',index=False)","68043eea":"## Competi\u00e7\u00e3o DSA - Kaggle - Fevereiro 2019\n\n## Prevendo Energy Star Score\n\nDesenvolvido por: Allyson de Lima Medeiros   \nData: 04-03-2019    \nhttps:\/\/www.linkedin.com\/in\/allysonlm\/    \n","c25dcb71":"Fim !!"}}