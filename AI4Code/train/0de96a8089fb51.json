{"cell_type":{"f9ae2310":"code","0d696c55":"code","3fdd543f":"code","bad05045":"code","abfcc911":"code","e506eeb3":"code","3eeebbc1":"code","09f186bb":"code","d476d6ae":"code","5a0e41bf":"code","364c3164":"code","742a8b0e":"code","15edbf42":"code","318c0f36":"code","2dfda2b5":"code","ce98b5d1":"code","cb62c483":"code","e6af2e79":"code","c2825e7c":"code","8ae32505":"code","e4a22754":"code","fd06e70e":"code","9bd9baf1":"code","00cc2384":"code","d869c2c1":"code","f05ae6d0":"code","21575e7a":"code","6fc9edee":"code","dce8df96":"code","b4fd4e42":"code","129a04cd":"code","31f035f7":"code","bb1f3173":"code","e613ee0e":"code","61e1315c":"code","8eab401e":"code","6d778bb5":"code","d3dfa521":"code","a657d51a":"code","f625c4c1":"markdown","5707d407":"markdown","181a829e":"markdown","c5932f54":"markdown","8682fedd":"markdown","c8f9dde6":"markdown","925a2b41":"markdown","17423f61":"markdown","3d7802a5":"markdown","fd8baec4":"markdown","6b6fd1dc":"markdown","5e5b5e0b":"markdown","7aca15b8":"markdown","e48e77e1":"markdown","5a752467":"markdown","d8d1e772":"markdown","7c13654c":"markdown","1037f593":"markdown","ad7afed1":"markdown","2980ab26":"markdown","85cd27ec":"markdown","7833b310":"markdown","45dcbd48":"markdown","e8046ec4":"markdown","9c9eb4ae":"markdown","92e3a8c0":"markdown","987d8863":"markdown","f1f3347e":"markdown","2936d034":"markdown","c28df52f":"markdown","e1e25704":"markdown","090d1432":"markdown","d29f29b4":"markdown","4ea79484":"markdown","2e9cf40d":"markdown","3688a595":"markdown","3590d8ee":"markdown","d4a40bbc":"markdown","c40d5e2b":"markdown","95bf975f":"markdown","d49f9d31":"markdown","24c281a9":"markdown","6998d162":"markdown"},"source":{"f9ae2310":"# The information are from here:\n# https:\/\/www.kaggle.com\/sk1812\/titanic-ml-model\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# ML algorithms from scikit;\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\n\n# Load library\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import linear_model\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Display the files in folder\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0d696c55":"# Get train\/test data\n# Notice that train and test have same columns EXCEPT survial;\ntitanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic.head(10)","3fdd543f":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_test.head(10)","bad05045":"# Describe the data;\ntitanic.info()\ntitanic.describe()","abfcc911":"# Function to check the missing percent of a DatFrame;\ndef check_missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","e506eeb3":"# Check the data missing rate of titanic and titanic_test;\ncheck_missing_data(titanic)\ncheck_missing_data(titanic_test)","3eeebbc1":"drop_column = ['Ticket', 'PassengerId']\ntitanic.drop(drop_column, axis= 1, inplace = True)\ntitanic_test.drop(drop_column,axis = 1,inplace = True)","09f186bb":"# Fill cabin with 0 if NaN; otherwise 1. Do this for both titanic and titanic_test;\ntitanic['Cabin']=np.where(titanic['Cabin'].isnull(),0,1)\ntitanic_test['Cabin']=np.where(titanic_test['Cabin'].isnull(),0,1)","d476d6ae":"# Fill 'Age' and 'Fare' missing data;\nall_data = [titanic, titanic_test]\n\n# def missing_data(x):\nfor data in all_data:\n    #complete missing age with median\n    data['Age'].fillna(data['Age'].mean(), inplace = True)\n\n    #complete missing Fare with median\n    data['Fare'].fillna(data['Fare'].mean(), inplace = True)","5a0e41bf":"# Fill 'Embarked' with most frequent info;\ntitanic[\"Embarked\"].fillna(\"S\",inplace = True)","364c3164":"# Chekck again if there is missing data;\ncheck_missing_data(titanic)\ncheck_missing_data(titanic_test)","742a8b0e":"titanic.head()","15edbf42":"# Function of drawing graph;\ndef draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 5,height ,ha= \"center\")","318c0f36":"# Draw survided vs. non-survived of trainign data.\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph= sns.countplot(x='Survived', hue=\"Survived\", data=titanic)\ndraw(graph)","2dfda2b5":"# Cabin and survived;\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Cabin\", hue =\"Survived\", data = titanic)\ndraw(graph)","ce98b5d1":"titanic.head()","cb62c483":"drop_column = ['Name']\ntitanic.drop(drop_column, axis= 1, inplace = True)\ntitanic_test.drop(drop_column,axis = 1,inplace = True)","e6af2e79":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n    \ntitanic['Sex'].value_counts()","c2825e7c":"# Handle 'Embarked': \n\nencoder = OneHotEncoder(sparse=False)\n\n# Use train data to determine the encoding rule;\nencoder.fit(titanic[['Embarked']])\n\n# Apply encoding to trian and test data;\nencode_train = encoder.transform(titanic[['Embarked']])\nencode_test = encoder.transform(titanic_test[['Embarked']])\n\n# Convert encoder to dataframe and merge to trian data; drop old column of 'Embarked';\nencode_train_df = pd.DataFrame(encode_train,columns=['Embarked_1','Embarked_2', 'Embarked_3'])\ntitanic = pd.concat([titanic,encode_train_df], axis=1)\ntitanic.drop('Embarked', axis=1, inplace=True)\n\n# Do the same for test data;\nencode_test_df = pd.DataFrame(encode_test,columns=['Embarked_1','Embarked_2', 'Embarked_3'])\ntitanic_test = pd.concat([titanic_test,encode_test_df], axis=1)\ntitanic_test.drop('Embarked', axis=1, inplace=True)","8ae32505":"# Correlation\ncorr=titanic.corr()#['Survived']\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (12,8))\nsns.heatmap(corr, \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu',\n            linewidths=.9, \n            linecolor='white',\n            vmax = 0.3,\n            fmt='.2f',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Matrix\", y = 1,fontsize = 20, pad = 20);","e4a22754":"all_data = [titanic, titanic_test]","fd06e70e":"# Use bin to group ages to bins;\nfor dataset in all_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 15, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 26), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 28), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 35), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 35) & (dataset['Age'] <= 45), 'Age'] = 5\n    dataset.loc[ dataset['Age'] > 45, 'Age'] = 6\ntitanic['Age'].value_counts()","9bd9baf1":"# Combine SibSp and Parch as new feature; \n# Combne train test first;\nall_data=[titanic,titanic_test]\n\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1","00cc2384":"titanic.head()","d869c2c1":"titanic_test.head()","f05ae6d0":"# Re-organize the data; keep the columns with useful features;\ninput_cols = ['Pclass','Sex','Age','SibSp', 'Parch', 'Fare', 'Cabin','Family', 'Embarked_1', \n              'Embarked_2', 'Embarked_3']\noutput_cols = [\"Survived\"]\n\nX_train = titanic[input_cols] # feature of train data\nY_train = titanic[output_cols] # label\n\nX_test = titanic_test[input_cols] # feature of test data; no label\n","21575e7a":"# Method 1\n# Rank the importance of feature;\nclf = LassoCV(alphas=[.01]).fit(X_train, Y_train.values.ravel())\nimportance = np.abs(clf.coef_)\n\nprint(importance)","6fc9edee":"# Method 2\n# Use Lasso to select features;\nclf = linear_model.Lasso(alpha=0.01).fit(X_train, Y_train.values.ravel())\n\nmodel = SelectFromModel(clf, prefit=True)\n\n# Visualize which features are selected\nmodel.get_support()\n","dce8df96":"# Re-organize the data; keep the columns with useful features;\ninput_cols = ['Pclass','Sex','Age','SibSp', 'Parch', 'Fare', 'Cabin','Family', 'Embarked_1', \n              'Embarked_2', 'Embarked_3']\noutput_cols = [\"Survived\"]\n\nX_train = titanic[input_cols] # feature of train data\nY_train = titanic[output_cols] # label\n\nX_test = titanic_test[input_cols] # feature of test data; no label\n","b4fd4e42":"# Init a model;\nmodel = XGBClassifier(\n    max_depth=6,\n    eta = 0.1,\n    colsample_bytree = 0.5,\n    subsample = 0.8,\n    eval_metric='rmse',\n    importance_type='weight')\n\n# Use data to fit the model; \nmodel.fit(X_train,Y_train.values.ravel())","129a04cd":"# Use the model to pred the label of test data;\npreds = model.predict(X_test)","31f035f7":"model.score(X_train,Y_train)","bb1f3173":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,5))","e613ee0e":"# survived ratio of test prediction\npreds.sum() \/ len(preds)","61e1315c":"# survived ratio of train data\nlen(titanic[titanic['Survived']==1]) \/ len(titanic)","8eab401e":"# ~~~~~~~~~~~~~~~~~ Example of Submission File ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Use results of rendom forest\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": preds   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission_xgboost.csv', index=False)\nsubmission = pd.read_csv('submission_xgboost.csv')\nsubmission.head(20)","6d778bb5":"# Check if submit file exists\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d3dfa521":"model = XGBClassifier(\n    max_depth=6,\n    eta = 0.1,\n    colsample_bytree = 0.5,\n    subsample = 0.8,\n    eval_metric='rmse',\n    importance_type='weight')","a657d51a":"kfold = KFold(n_splits=10)\n\nresults = cross_val_score(model, X_train, Y_train, cv=kfold)\n\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","f625c4c1":"# **Stage 1: Construct A Baseline Model**","5707d407":"Column 'Cabin' has missing data. What about filling non-empty cabin with 1, and empty cabin with 0? Let's do this way first. We may find better way to handle 'Cabin'.","181a829e":"Let's try XGBoost model with simple k-fold cross validation.","c5932f54":"Now the data is clean: no missing data, all numeric type. Let's check redudancy and duplication. One way of doing this is to check correlation matrix. As we can see, there are no features with high correlation (either positively or negatively). We believe there is no redundancy in features.","8682fedd":"Load train and test data. Display first few rows of data to get some sense. Notice the difference of data structure between train and test.\n\nThink about it: is this a classification or regression problem?","c8f9dde6":"The feature 'Name' is a bit complicated, we drop it for now, and will manage it in stage 2.","925a2b41":"Before selecting the model, think about it: is this classification or regression problem? Here we start setting up the input parameters of model.","17423f61":"# **Feature Engineering**","3d7802a5":"# **Tune the Model Parameters**","fd8baec4":"Feature sets of train\/tes data are ready. Here we construct a baseline model using xgboost. As a baseline we billd the model from train data, and use the model to predict test data.\n\nLater we will see other techniques such as cross validation etc.","6b6fd1dc":"# **Cross Validation**","5e5b5e0b":"The ways of improving model include:\n* Split train data into train\/validation data\n* Construct new features to dig into hidden information\n* Tune model hyper-parameters","7aca15b8":"Fill missing values in 'Age' and 'Fare' with mean of non-empty values.","e48e77e1":"# **Stage 2: Improve The Model**","5a752467":"After checking the columns of train\/test data, we see three columns have categorical information: 'Name', 'Sex', 'Embarked'.","d8d1e772":"Choose features that we belive are useful (just for demo). Organize the data using selected features.","7c13654c":"Try create new features. Let's combine the features 'SibSp' and 'Parch'. We believe that will provide new information.","1037f593":"Here we use 'seaborn' library to demonstrate the plots. The goal of plot is to get some feeling of data.\n\nThe function below is used to adjust the text and size of graph.","ad7afed1":"# **Modeling**","2980ab26":"Plot the feature importance of this specific model.","85cd27ec":"# **Write Result for Submit**","7833b310":"So far all the missing data are filled. There are still categorical\/text info. Let's check which columns have categorical information before encoding them. The encoding is part of 'Feature Engineering', which will be discussed later.","45dcbd48":"Check the data: describe the data information,get statistical information of each column.","e8046ec4":"Since we don't have labels for test data, here we use train data to demo the accuracy of model.","9c9eb4ae":"Similarly, plots of other featues can be done. We leave that for exercise.","92e3a8c0":"Load packages. Display the files in folders.","987d8863":"So far, we have encoded category features, and created several new features. Let's check.","f1f3347e":"Now all features are numeric. Features are uncorrelated. Can we find new information from the information provided?\n\nWe check the feature 'Age'. We wil group it in bins. Think about it: why group the age data?","2936d034":"A baseline model is done. How do we get some sense of prediction results? Is that reasonable. Usually we have benchmark model to compare. Here we roughtly look at survived ratio of train and test prediction.","c28df52f":"# **Data Loading**","e1e25704":"There are many feature engineering methods. Mainly used are:\n\n* Encode categorical features;\n* Construct new features;\n* Remove redundant features;","090d1432":"Check and handling missing data. Below is a function to check the missing data rate of a dataframe. Use this function to check the missing rate of train and test data.","d29f29b4":"Encode feature 'Sex': map it to 0 or 1. ","4ea79484":"# **Titanic: ML Framework**","2e9cf40d":"# **EDA**\n\nEDA has multiple goals:\n* Check missing data \/ fill missing values\n* Check outlier \/ remove outlier\n* Check redundancy \/ duplications\n* Encoding\n\nIn brief, the goal is to have clean data ready for feature engineering.","3688a595":"# **Data Visualization**","3590d8ee":"This is a review of ML modeling framework using Titanic data. The goal is to review the steps of ML modeling and demonstrate the use of techniques we have introduced in the class.\n\nThe materials are organized in two stages: \n\n* Stage 1: construct a baseline model\n* Stage 2: improve the model","d4a40bbc":"# **Feature Selection \/ Importance**","c40d5e2b":"The ML Modeling Process includes several steps:\n* Data loading\n* EDA (Exploratory Data Analysis)\n* Visualization\n* Feature engineering\n* Modeling\n* Submit","95bf975f":"Think about it: is there any column has irrelevant information, i.e., has no impact on 'Survived'? If so, delete it.\n\nAssuming: I believe two columns 'Ticket', 'PassengerID' are irrelevant. Drop them.","d49f9d31":"# **Construct New Features**","24c281a9":"As demo, we plot survied and non-survied, and Cabin vs. Survived.","6998d162":"Encode feature 'Embarked', use one-hot encoding. Therefore new features will be created. And old feature will be dropped."}}