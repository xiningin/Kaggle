{"cell_type":{"305fefd3":"code","1d735aa6":"code","8572d885":"code","9cbd5d13":"code","d76b5867":"code","0804c2fe":"code","76a91b82":"code","c406bd4a":"code","3b2b5f5b":"code","d405cfd8":"code","b850d288":"code","92602229":"code","81830c6c":"code","83933338":"code","f6c23717":"code","bfdce239":"code","2dc97cbd":"code","24e38907":"code","30346ddc":"code","7e5a65a6":"code","8242600f":"code","28b6da77":"code","3d789fb6":"code","9b896fff":"code","6065962e":"code","1bc42eff":"code","d5397f1b":"code","561482d2":"code","6f25d69a":"code","73a567aa":"code","4637b18f":"code","13ad7f59":"code","38851ec8":"code","ba11fdbc":"code","6dfaa0c9":"code","a602e70f":"code","78753714":"code","2fe3b763":"code","4beb2bbe":"code","ffa9761c":"code","3c41680f":"code","fa8299d1":"code","a5c8d603":"code","da4c874d":"code","0a75aaa6":"code","111a8832":"code","d1f53520":"code","5505444e":"code","679f03b7":"code","ab0bc296":"code","1fe199a7":"code","70881982":"code","212f8983":"code","cfedfc79":"code","8a17ff46":"code","82852cea":"code","75ba8530":"code","08da9491":"code","326994e2":"code","538b58d7":"code","85ba9ab7":"code","9264cc15":"code","439f245a":"code","be5d15ac":"code","407e5aa6":"code","430c37a7":"code","e64aad38":"code","89cf0c5a":"code","4e9a433a":"code","c1ca664c":"code","3af569f6":"code","5d7c694a":"code","fcade535":"code","948fffd5":"code","72f6eabb":"code","7a2d91ca":"code","1f6cfa37":"code","81cc6b74":"code","e868b5f6":"code","bd541441":"code","1951391b":"code","2171bcfa":"code","facd92b5":"code","c55b87d8":"code","d521f4ef":"code","ddc68d41":"code","623d426b":"code","661091b1":"markdown","2d8fa0a0":"markdown","9864e2d5":"markdown","126cea8b":"markdown","06fc7b15":"markdown","1ef6c190":"markdown","36f37112":"markdown","f09494da":"markdown","494336c4":"markdown","1eb4fc93":"markdown","d51123ce":"markdown","5dfa8cd5":"markdown","087aaed7":"markdown","5f24bc85":"markdown","587da7ae":"markdown","d80adc87":"markdown","7118d013":"markdown","de0e1636":"markdown","47faf62e":"markdown","dd4de265":"markdown","0368f623":"markdown","89b24af4":"markdown","d58ff0db":"markdown","6e57596a":"markdown","15395b89":"markdown","005a3c83":"markdown","0e7128a3":"markdown","0566a3b5":"markdown","33f61259":"markdown","5b915a03":"markdown","d443a017":"markdown","decef375":"markdown","2688f3e0":"markdown","5bf52d38":"markdown","6fffd7ce":"markdown","9174bc0b":"markdown","ce899fb1":"markdown","fd8ceaae":"markdown","80053c86":"markdown","f7c1b869":"markdown","5fb9bd2a":"markdown","11b96120":"markdown","5febfd29":"markdown","eb0295ab":"markdown","58f55ce6":"markdown","17c4493b":"markdown","8b58ab74":"markdown","0b07869a":"markdown","c60109fc":"markdown","f298767b":"markdown","a709d0cf":"markdown","08be269c":"markdown","5bb62b48":"markdown","b28ee6ee":"markdown","191a825a":"markdown","d09d49ba":"markdown","cad23692":"markdown","046927da":"markdown","39927812":"markdown","669ac6bd":"markdown","9e0f4706":"markdown","8059baf4":"markdown","8051c05f":"markdown","6c96a140":"markdown","93ccefe0":"markdown","0ffe5a0a":"markdown","290792da":"markdown","b868498c":"markdown","dfd0b3ee":"markdown","f9922a94":"markdown","4282fda6":"markdown","783d2e4f":"markdown"},"source":{"305fefd3":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom math import sqrt\n#sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing, svm\nfrom sklearn.feature_selection import SelectFromModel\nsns.set_style('whitegrid')\n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1d735aa6":"# display all\nfrom IPython.display import display\npd.options.display.max_columns = None\npd.options.display.max_rows = None","8572d885":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')\n\n# combine train and test data \ndf_all = pd.concat([train_data.drop('SalePrice', axis=1), test_data], sort=True)  #df without the target\n\n\n\n","9cbd5d13":"df_all.head()","d76b5867":"df_all.shape","0804c2fe":"df_all.info()","76a91b82":"numeric_features = df_all.select_dtypes(include=['int64','float64'])\ncategorical_features = df_all.select_dtypes(include=['object'])","c406bd4a":"print('Numeric Features:',len(list(numeric_features.columns)))\nprint('Categorical Features:',len(list(categorical_features.columns)))","3b2b5f5b":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\nsns.heatmap(train_data.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Trian data')\n\nsns.heatmap(test_data.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","d405cfd8":"missing = df_all.isnull().sum().sort_values(ascending=False)\npercentage=(df_all.isnull().sum()\/df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([missing,percentage],axis=1,keys=['Missing','%']) \nmissing_data[missing_data != 0].dropna()","b850d288":"# numeric_features = df_all.loc[:, df_all.dtypes != np.object]\n# imputer = KNNImputer(n_neighbors=60)\n# df_all.loc[:, df_all.dtypes != np.object] = imputer.fit_transform(numeric_features)","92602229":"imp = SimpleImputer(missing_values=np.nan, strategy='median')\ndf_all.loc[:, df_all.dtypes != np.object] = imp.fit_transform(numeric_features)\ndf_all.head()","81830c6c":"edit_values = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType',\n              'GarageQual','PoolQC','Fence','MiscFeature','MasVnrType', 'GarageCond', 'GarageFinish']\n\nfor col in edit_values:\n    df_all[col].fillna('NA',inplace=True)","83933338":"df_all.Exterior1st.fillna(value='VinylSd', inplace=True)\n\ndf_all.Exterior2nd.fillna(value='VinylSd', inplace=True)\n\ndf_all.KitchenQual.fillna(value='TA', inplace=True)\n\ndf_all.SaleType.fillna(value='WD', inplace=True)\n\ndf_all.Utilities.fillna(value='AllPub', inplace=True)\n\ndf_all.Electrical.fillna(value='SBrkr', inplace=True)\n\ndf_all.Functional.fillna(value='Typ', inplace=True)\n\ndf_all.MSZoning.fillna(value='RL', inplace=True)","f6c23717":"# Missung values \nprint('Missing values:' ,df_all.isnull().sum().sum())","bfdce239":"df_all['GarageYrBlt']= df_all['GarageYrBlt'].astype(int)","2dc97cbd":"# adding the target to our df\ndf_all = pd.concat([df_all, train_data['SalePrice']], axis=1)\nnormal_sp = df_all['SalePrice'].dropna().map(lambda i: np.log(i) if i > 0 else 0)\nprint(df_all['SalePrice'].skew())\nprint(normal_sp.skew())\n\nfig, ax = plt.subplots(ncols=2, figsize=(12,6))\ndf_all.hist('SalePrice', ax=ax[0])\nnormal_sp.hist(ax=ax[1])\nplt.show","24e38907":"fig, ax = plt.subplots(figsize=(14, 8))\nk = 10 #number of variables for heatmap\ncorrmat = df_all.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_all[cols].dropna().values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values, ax=ax, cmap=\"YlGnBu\")\nax.set_ylim(0 ,10)\nplt.show()","30346ddc":"# correlation with the target\ncorr_matrix = df_all.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","7e5a65a6":"fig, axes = plt.subplots(ncols=4, nrows=4, \n                         figsize=(5 * 5, 5 * 5), sharey=True)\naxes = np.ravel(axes)\ncols = ['OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n        'BsmtCond','GarageQual','GarageCond', 'MSSubClass','MSZoning',\n        'Neighborhood','BldgType','HouseStyle','Heating','Electrical','SaleType']\nfor i, c in zip(np.arange(len(axes)), cols):\n    ax = sns.boxplot(x=c, y='SalePrice', data=df_all, ax=axes[i], palette=\"Set2\")\n    ax.set_title(c)\n    ax.set_xlabel(\"\")","8242600f":"Q1 = df_all.quantile(0.25)\nQ3 = df_all.quantile(0.75)\nIQR = Q3 - Q1\noutliars = (df_all < (Q1 - 5 * IQR)) | (df_all > (Q3 + 5 * IQR))\n#removing bad columns and outliars\nno_outliars_df = df_all.drop(['EnclosedPorch', 'KitchenAbvGr'], axis=True)\nrm_rows = ['LotArea', 'MasVnrArea', 'PoolArea', 'OpenPorchSF', 'LotFrontage', 'TotalBsmtSF','1stFlrSF',\n           'GrLivArea', 'BsmtFinSF1', 'WoodDeckSF']\ndf_all.drop(['EnclosedPorch', 'KitchenAbvGr'], axis=True, inplace=True)\nfor row in rm_rows:\n    no_outliars_df.drop(no_outliars_df[row][outliars[row]].index, inplace=True)","28b6da77":"object_features =df_all.loc[:, df_all.dtypes == np.object]   #object features\ndf_all= pd.get_dummies(df_all, columns=object_features.columns.values, drop_first=True)\nno_outliars_df = pd.get_dummies(no_outliars_df, columns=object_features.columns.values, drop_first=True)","3d789fb6":"# Traing Data with Outliers\nnewtraining=df_all.loc[  : 1460]\n# Testing Data with Outliers\nnewtesting=df_all.loc[1461 : ].drop('SalePrice', axis=1)\n# newtraining['SalePrice'] = np.log(newtraining['SalePrice'])\n# lab_enc = preprocessing.LabelEncoder()\n# newtraining['SalePrice'] = la","9b896fff":"# Traing Data without Outliers\nno_outliars_training = no_outliars_df.loc[  : 1460]\n# Testing Data without Outliers\nno_outliars_test = no_outliars_df.loc[1461 : ].drop('SalePrice', axis=1)","6065962e":"y = newtraining['SalePrice']\nX = newtraining.drop('SalePrice', axis=1)","1bc42eff":"ss = StandardScaler()\nXs =ss.fit_transform(X)","d5397f1b":"X_train, X_test, y_train, y_test = train_test_split(\n    Xs, y, test_size=.3, random_state=1)","561482d2":"yo = no_outliars_training['SalePrice']\nXo = no_outliars_training.drop('SalePrice', axis=1)","6f25d69a":"sso = StandardScaler()\nXso =sso.fit_transform(Xo)","73a567aa":"Xo_train, Xo_test, yo_train, yo_test = train_test_split(\n    Xso, yo, test_size=30, random_state=1)","4637b18f":"lr = LinearRegression()\nmodel= lr.fit(X_train, y_train)\nprint('Train Score:',model.score(X_train, y_train))\nprint('Test Score :',model.score(X_test, y_test))","13ad7f59":"lr_predictions = model.predict(newtesting) ","38851ec8":"lro = LinearRegression()\nmodel_o= lro.fit(Xo_train, yo_train)\nprint('Train Score:', model_o.score(Xo_train, yo_train))\nprint('Test Score :',model_o.score(Xo_test, yo_test))","ba11fdbc":"lro_predictions = model_o.predict(Xso) ","6dfaa0c9":"lasso = Lasso(alpha=.0002)\nlasso.fit(X_train, y_train)\nprint('Train Score:',lasso.score(X_train, y_train))\nprint('Test Score: ', lasso.score(X_test, y_test))","a602e70f":"lasso_predictions = lasso.predict(newtesting)","78753714":"# sqrt(mean_squared_error(submission['SalePrice'],lasso_predictions))","2fe3b763":"lasso_o = Lasso(alpha=.2)\nlasso_o.fit(Xo_train, yo_train)\nprint('Train Score:',lasso_o.score(Xo_train, yo_train))\nprint('Test Score:',lasso_o.score(Xo_test, yo_test))","4beb2bbe":"lasso_cv = LassoCV(alphas=np.logspace(-4, 4, 1), cv=5,random_state=1)\nlasso_cv.fit(X_train, y_train)\nprint('Train Score :',lasso_cv.score(X_train, y_train))\nprint('Test Score:',lasso_cv.score(X_test, y_test))","ffa9761c":"lasso_cv_predictions = lasso_cv.predict(newtesting) ","3c41680f":"lasso_cv_o = LassoCV(cv=10,random_state=1)\nlasso_cv_o.fit(Xo_train, yo_train)\nprint('Train Score:',lasso_cv_o.score(Xo_train, yo_train))\nprint('Test Score:',lasso_cv_o.score(Xo_test, yo_test))","fa8299d1":"ridge = Ridge(alpha=1) \nridge.fit(X_train, y_train)\nprint('Train Score:',ridge.score(X_train, y_train))\nprint('Test Score:',ridge.score(X_test, y_test))","a5c8d603":"ridge_predictions = ridge.predict(newtesting) ","da4c874d":"ridge_o = Ridge(alpha=.01) \nridge_o.fit(Xo_train, yo_train)\nprint('Train Score:',ridge_o.score(Xo_train, yo_train))\nprint('Test Score:',ridge_o.score(Xo_test, yo_test))","0a75aaa6":"ridgecv = RidgeCV(alphas=np.logspace(-4, 4, 1))\nridgecv.fit(X_train, y_train)\nprint('Train Score:',ridgecv.score(X_train, y_train))\nprint('Test Score:',ridgecv.score(X_test, y_test))","111a8832":"ridgeCV_predictions = ridgecv.predict(newtesting) ","d1f53520":"ridgecv_o = RidgeCV(alphas=np.logspace(-4, 4, 1)) \nridgecv_o.fit(Xo_train, yo_train)\nprint('Train Score:',ridgecv_o.score(Xo_train, yo_train))\nprint('Test Score:',ridgecv_o.score(Xo_test, yo_test))","5505444e":"elastic=ElasticNet(.00001)\nelastic = elastic.fit(X_train, y_train)\nprint('Train Score:',elastic.score(X_train, y_train))\nprint('Test Score:',elastic.score(X_test, y_test))","679f03b7":"elastic_predictions = elastic.predict(newtesting) ","ab0bc296":"elastic_o=ElasticNet(.00001)\nelastic_o = elastic_o.fit(Xo_train, yo_train)\nprint('Train Score:',elastic_o.score(Xo_train, yo_train))\nprint('Test Score:',elastic_o.score(Xo_test, yo_test))","1fe199a7":"elastic_cv=ElasticNetCV(alphas=np.logspace(-10, 6, 1))\nelastic_cv = elastic_cv.fit(X_train, y_train)\nprint('Train Score:',elastic_cv.score(X_train, y_train))\nprint('Test score:',elastic_cv.score(X_test, y_test))","70881982":"elastic_cv_o=ElasticNetCV(alphas=np.logspace(-4, 4, 1))\nelastic_cv_o = elastic_cv_o.fit(Xo_train, yo_train)\nprint('Train Score :',elastic_cv_o.score(Xo_train, yo_train))\nprint('Test score  :',elastic_cv_o.score(Xo_test, yo_test))","212f8983":"tree = DecisionTreeClassifier(max_depth = 28)\ntree.fit(X, y)\nprint('Score : ',tree.score(X, y))","cfedfc79":"tree_predictions = tree.predict(newtesting)","8a17ff46":"# sqrt(mean_squared_error(submission['SalePrice'], tree_predictions))","82852cea":"tree_o = DecisionTreeClassifier(max_depth = 29)\ntree_o.fit(Xo, yo)\nprint('Score : ',tree_o.score(Xo, yo))","75ba8530":"randomF = RandomForestRegressor(max_depth=50)\nrandomF.fit(X, y)\nprint('Train score :',randomF.score(X, y))","08da9491":"# from sklearn.model_selection import KFold\n# cv=KFold(n_splits=5, shuffle=True, random_state=1)","326994e2":"# cross_val_score(randomF, X, y, cv=cv)","538b58d7":"# cross_val_score(randomF, X, y, cv=cv).mean()","85ba9ab7":"randomF_predictions = randomF.predict(newtesting) ","9264cc15":"# sqrt(mean_squared_error(submission_tree['SalePrice'], randomF_predictions))","439f245a":"# param_grid = {\n#     'n_estimators': [i for i in range(50,1000,10)],\n#     'max_features': [86],\n#     'max_depth' :[i for i in range(1,100,1)]\n# }","be5d15ac":"# rand = RandomForestRegressor(n_jobs=-1)\n\n# gs = GridSearchCV(rand, \n#                   param_grid, \n#                   cv=5)","407e5aa6":"# gs.fit(X, y)","430c37a7":"# gs.best_params_","e64aad38":"# gs.best_score_","89cf0c5a":"randomF_o = RandomForestRegressor(max_depth = 50)\nrandomF_o.fit(Xo, yo)\nprint('train score : ',randomF_o.score(Xo, yo))","4e9a433a":"neigh = KNeighborsRegressor(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint('Train score : ',neigh.score(X_train, y_train))\nprint('Test score  : ',neigh.score(X_test, y_test))","c1ca664c":"neigh_predictions = neigh.predict(newtesting)","3af569f6":"# sqrt(mean_squared_error(submission_tree['SalePrice'], neigh_predictions))","5d7c694a":"neigh_o = KNeighborsRegressor(n_neighbors=3)\nneigh_o.fit(Xo_train, yo_train)\nprint('Train score : ',neigh_o.score(Xo_train, yo_train))\nprint('Test score  : ',neigh_o.score(Xo_test, yo_test))","fcade535":"svm_l = svm.SVC(kernel='linear')\nsvm_l.fit(X, y)\nsvm_l.score(X, y)","948fffd5":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_l, X, y, cv=cv, n_jobs=-1).mean()","72f6eabb":"svm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(X, y)\nsvm_rbf.score(X, y)\ncross_val_score(svm_rbf, X, y, cv=5, n_jobs=-1).mean()","7a2d91ca":"svm_p = svm.SVC(kernel='poly')\nsvm_p.fit(X, y)\nsvm_p.score(X, y)\ncross_val_score(svm_p, X, y, cv=5, n_jobs=-1).mean()","1f6cfa37":"svm_rbf = svm.SVC(kernel='rbf', gamma=0.001)\ncross_val_score(svm_rbf, X, y, cv=5).mean()","81cc6b74":"list_of_Scores = list()","e868b5f6":"# LinearRegression\nresults = {'Model':'LinearRegression',\n           'Train Score': model.score(X_train, y_train),\n           'Test Score':model.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n# Lasso\nresults = {'Model':'Lasso',\n           'Train Score':lasso.score(X_train, y_train),\n           'Test Score': lasso.score(X_test, y_test),\n           'Kaggle Score':0.59683}\nlist_of_Scores.append(results)\n# LassoCV\nresults = {'Model':'LassoCV',\n           'Train Score': lasso_cv.score(X_train, y_train),\n           'Test Score':lasso_cv.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# Ridg\nresults = {'Model':'Ridg',\n           'Train Score': ridge.score(X_train, y_train),\n           'Test Score':ridge.score(X_test, y_test),\n           'Kaggle Score':0.36706}\nlist_of_Scores.append(results)\n\n# RidgCV\nresults = {'Model':'RidgCV',\n           'Train Score': ridgecv.score(X_train, y_train),\n           'Test Score':ridgecv.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# ElasticNet\nresults = {'Model':'ElasticNet',\n           'Train Score': elastic.score(X_train, y_train),\n           'Test Score':elastic.score(X_test, y_test),\n           'Kaggle Score':6.63994}\nlist_of_Scores.append(results)\n\n# ElasticNetCV\nresults = {'Model':'ElasticNetCV',\n           'Train Score':elastic_cv.score(X_train, y_train),\n           'Test Score':elastic_cv.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# DecisionTreeRegressor\nresults = {'Model':'DecisionTreeRegressor',\n           'Train Score':tree.score(X, y),\n           'Test Score':None,\n           'Kaggle Score':0.25525}\nlist_of_Scores.append(results)\n\n# RandomForest\nresults = {'Model':'RandomForest',\n           'Train Score':randomF.score(X, y),\n           'Test Score':None,\n           'Kaggle Score':0.14824}\nlist_of_Scores.append(results) \n\n# KNeighborsRegressor\nresults = {'Model':'KNeighborsRegressor',\n           'Train Score': neigh.score(X_train, y_train),\n           'Test Score':neigh.score(X_test, y_test),\n           'Kaggle Score':None}\nlist_of_Scores.append(results)\n\n# SVM\nresults = {'Model':'SVM',\n           'Train Score': svm_l.score(X, y),\n           'Test Score':None,\n           'Kaggle Score':None}\nlist_of_Scores.append(results)","bd541441":"df_results = pd.DataFrame(list_of_Scores)","1951391b":"df_results","2171bcfa":"list_of_Scores_o = list()","facd92b5":"# LinearRegression\nresults_o = {'Model':'LinearRegression',\n           'Train Score': lro.score(Xo_train, yo_train),\n           'Test Score':lro.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n\n# Lasso\nresults_o = {'Model':'Lasso',\n           'Train Score': lasso_o.score(Xo_train, yo_train),\n           'Test Score':lasso_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# LassoCv\nresults_o = {'Model':'LassoCv',\n           'Train Score': lasso_cv_o.score(Xo_train, yo_train),\n           'Test Score':lasso_cv_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# Ridg\nresults_o = {'Model':'Ridg',\n           'Train Score':ridge_o.score(Xo_train, yo_train),\n           'Test Score':ridge_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# RidgCV\nresults_o = {'Model':'RidgCV',\n           'Train Score':ridgecv_o.score(Xo_train, yo_train),\n           'Test Score':ridgecv_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# ElasticNet\nresults_o = {'Model':'ElasticNet',\n           'Train Score':elastic_o.score(Xo_train, yo_train),\n           'Test Score':elastic_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# ElasticNetCV\nresults_o = {'Model':'ElasticNetCV',\n           'Train Score':elastic_cv_o.score(Xo_train, yo_train),\n           'Test Score':elastic_cv_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)\n\n# DecisionTreeRegressor\nresults_o = {'Model':'DecisionTreeRegressor',\n           'Train Score':tree_o.score(Xo, yo),\n           'Test Score':None}\nlist_of_Scores_o.append(results_o)\n\n# RandomForest\nresults_o = {'Model':'RandomForest',\n           'Train Score':randomF_o.score(Xo, yo),\n           'Test Score':None}\nlist_of_Scores_o.append(results_o)\n\n# KNeighborsRegressor\nresults_o = {'Model':'KNeighborsRegressor',\n           'Train Score':neigh_o.score(Xo_train, yo_train),\n           'Test Score':neigh_o.score(Xo_test, yo_test)}\nlist_of_Scores_o.append(results_o)","c55b87d8":"df_results_o = pd.DataFrame(list_of_Scores_o)","d521f4ef":"df_results_o","ddc68d41":"submission_randomF = submission.copy()\nsubmission_randomF['SalePrice'] = randomF_predictions\nsubmission_randomF['SalePrice'].head()","623d426b":"# submission_randomF.to_csv('sample_submissionRandom.csv')","661091b1":"- #### Build the Model Without Outliers\n","2d8fa0a0":"### Cleaning Data ","9864e2d5":"- #### Build the Model Without Outliers","126cea8b":"  # <center>  House Prices <\/center>\n  ","06fc7b15":"- #### Build the Model Without Outliers","1ef6c190":"![image.png](attachment:image.png)","36f37112":"### Distribution of SalePrice","f09494da":"- #### How can your model be used for inference? Why do you believe your model will generalize to new data? \nAccording to the random forest important featrues, we inference those featrues can play a major part in prediction. As we got these result: train score : 0.9821625329876957 test score : 0.9821625329876957 And when we tested the Corss Validation of it, the results were: \"[0.85885306, 0.80347532, 0.81402519, 0.77931117, 0.87881093]\" With an average of: \"0.8508149158677224\" ~ 0.85 , this is a good ratio, as it means that the model can generalize any new data that can enter the model at 85 percent of accuracy, as this result indicates that the model is right fitt because a low viariance of it .","494336c4":"##### Out of 81 features, 19 features have missing values.","1eb4fc93":"- #### Build the Model With Outliers","d51123ce":" ### 11 - Build SVM  Model \n","5dfa8cd5":"### Evaluate Models","087aaed7":"   ### 1 - Build Linear Regression Model","5f24bc85":"- #### Build the Model With Outliers","587da7ae":"### Group Number: 8\n### Group members:\n- Abdulrahman ALQannas\n- Doaa Alsenani\n- Ghida Qahtan\n- Moayad Magadmi\n----","d80adc87":"- #### Build the Model Without Outliers","7118d013":"- #### Build the Model With Outliers","de0e1636":"- #### Build the Model Without Outliers","47faf62e":" ### 5 - Build RidgeCV Model","dd4de265":"- #### Build the Model With Outliers","0368f623":"-  #### This dataset includes 36 numeric features: \n\n|Classification|Dataset|Description|\n|-------|---|---|\n|Years\/Months|df_all|These numeric features represent time of built, sold, and the age of the property.| \n|Area|df_all|These features show the square footage| \n|Amount of Rooms and Amenities|df_all|These features represent the number of rooms, bathrooms,kitchens...ect.| \n|Condition and Quality|df_all|Theses features show the condition and quality of land that are determined by surveyors| \n","89b24af4":"\n ### 6 - Build ElasticNet Model \n","d58ff0db":"- #### Build the Model With Outliers\n","6e57596a":"\n- #### Creating a list to Store Score Values for Each Model\n     -  This list will display the train and test scores without outliers","15395b89":"- #### Creating a list to Store Score Values for Each Model\n     -  This list will display the train and test scores without outliers\n","005a3c83":"If you have Sklearn 0.22.2 you can use this code in this cell","0e7128a3":"* #### This table provides all the scores that we got from each model.\n","0566a3b5":"- #### Build the Model Without Outliers","33f61259":"### Modeling","5b915a03":"  - ####   Converting NaN Numeric Values with KNNImputer","d443a017":"   ### 2 - Build Lasso Model","decef375":"- #### Build the Model With Outliers","2688f3e0":"If you don't have sklearn 0.22.2 comment the cell above and use this cell","5bf52d38":"- #### Build the Model With Outliers","6fffd7ce":"### Kaggle Score","9174bc0b":"\n ### 8 - Build Decision Tree Model ","ce899fb1":"- ##### Splitting and Standardizing Train Data to Obtain Test Scores Without Removing the Outliers","fd8ceaae":"- #### This table provides all the scores that we got from each model.\n","80053c86":"### These datasets include 79 explanatory variables:\n\n*   Train data have SalePrice (dependent variable) and other predictor variables.\n*   Test data include the same  variables  that in train data, but  without SalePrice (dependent variable) because this data will be submitted to kaggle.\n\n ","f7c1b869":"## Categoriesing features","5fb9bd2a":"- #### Build the Model Without Outliers","11b96120":"\n ### 4 - Build Ridge Model","5febfd29":"\n ### 9 - Build Random Forest Model ","eb0295ab":"## Importing packages","58f55ce6":"- ##### Splitting and Standardizing Train Data to Obtain Test Scores With Removing the Outliers\n","17c4493b":"- #### Build the Model Without Outliers","8b58ab74":"- ### Data Types","0b07869a":"### Submission  RandomForest","c60109fc":"### Outliars","f298767b":"- #### Build the Model Without Outliers","a709d0cf":"Let's take a look at the most important variable <b>SalePrice<\/b> It seems there is a long tail to the right which means high sale prices, which will make the mean to be much higher than the median.<br><br>","08be269c":"###  Results","5bb62b48":"- ####   Filling Missing Data","b28ee6ee":"### Check Missing Values","191a825a":"- ### Change the Data Type","d09d49ba":" ### 3 - Build LassoCV Model","cad23692":"  - #### Build the Model With Outliers","046927da":"- #### Build the Model Without Outliers\n","39927812":"These datasets include information about residential homes that were sold from 2006 to 2010 in Ames, Iowa. Our purpose will be to predict the final price of each home.","669ac6bd":"## Introduction\n","9e0f4706":"- #### Build the Model Without Outliers","8059baf4":"GridSearch","8051c05f":"- #### Consider your evaluation metrics score\nOur evaluation metrics scores are data cleaning and cross-validation","6c96a140":"\n ### 7 - Build ElasticNetCV Model ","93ccefe0":" ### 10 - Build KNeighborsRegressor Model ","0ffe5a0a":" As we see in the correlation matrix, the features that related to quality and the size affect the sale price, which might affect our results. In addition, OverallQual feature has a significant impact on sale price more than other features.\n","290792da":"\n### Separating the Dataset After Cleaning\n\n\n","b868498c":"## Exploring the Data","dfd0b3ee":"Now let's take a look at the most important variables, which will have strong linear releationship with <b>SalePrice<\/b> variable.<br><br>","f9922a94":"## Loading the House Price Dataset","4282fda6":"- #### Build the Model With Outliers","783d2e4f":"- ### SalePrice Correlation Matrix "}}