{"cell_type":{"4db9ef86":"code","7742a24f":"code","f611055d":"code","2a9c209e":"code","e346840c":"code","d7065ea2":"code","6a6f8ba5":"code","2d5096e8":"code","bab450ff":"code","5660eb73":"code","5825f4ac":"code","e6c3b40e":"code","bbc33dda":"code","84ea3209":"code","478d052e":"code","c1f5b8d5":"code","0ed1e4e4":"code","07fd728e":"code","ae192f31":"code","b0a1c7b8":"code","f0bfa9d6":"code","d51020f3":"code","77bc9bc7":"code","7f03c99d":"code","64d7a058":"code","5fabf324":"code","962695f8":"code","62ff471e":"code","f2d824d9":"code","a9d26990":"code","46d05958":"code","f74a9570":"code","8e62391b":"code","134a0f93":"code","4b837277":"code","4b05a15a":"code","af6eb9f7":"code","b1c5c39d":"code","7463ae8c":"code","6b8ece6d":"code","295a4e50":"code","6c60220e":"code","2259aa86":"code","5d11e9ee":"code","6a83e867":"code","286641ef":"code","169406d3":"code","2ce396e8":"code","05787fa6":"code","f277546d":"code","864806d1":"code","e622da90":"code","7afdf100":"code","23e2df55":"code","c2e5c185":"code","8bf52979":"code","7710a2b2":"code","223ed4ac":"code","8cc4d3e2":"code","ca49d96f":"code","16e742da":"code","6b081ec7":"code","7af7a541":"code","7a841267":"code","14db22c7":"code","d7729c24":"code","f785f3e4":"code","3ca557c1":"code","9369ea81":"code","b9d34387":"code","2ee8e155":"code","c71911c8":"code","54a7564b":"code","ac12f6c3":"code","2f265ddb":"code","ba7bcaee":"code","1de92cce":"code","8ae8d362":"code","14cdc53c":"code","c01f1bb1":"code","fd84a057":"code","81d0ca9d":"code","b1b6bccb":"code","b8fc2f07":"code","b5a12281":"code","0751558b":"code","e5eef082":"code","694d9e7d":"code","2bc4b361":"code","a6f4cc48":"code","ef4ad671":"code","9482208b":"code","32b3687b":"code","656e64f5":"code","9a96b678":"code","707dac81":"code","bc7532e2":"code","7fe25011":"code","b4a21bb9":"code","75785b65":"code","3d696264":"markdown","e9bf4758":"markdown","54bebd1a":"markdown","714ab2d8":"markdown","473cf2e9":"markdown","ad9a1bc7":"markdown","f8aa91bf":"markdown","36e10c99":"markdown","a608808d":"markdown"},"source":{"4db9ef86":"import pandas as pd\nimport numpy as np\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7742a24f":"# reading .csv file\ndata=pd.read_csv('..\/input\/predicting-employee-status\/employee_data (1).csv')","f611055d":"data.head()#getting top 5 rows of our dataframe","2a9c209e":"data.dtypes","e346840c":"data.describe(include='all')#getting all summary statistics of our data","d7065ea2":"data.isnull().sum()#finding number of null values in individual column","6a6f8ba5":"data.status.value_counts()#getting count of different classes in a column ","2d5096e8":"sns.countplot(x='status',data=data)\nplt.title('status distribution')","bab450ff":"sns.countplot(x='status',data=data,hue='department')\nplt.title('status distribution vs department')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","5660eb73":"data.dtypes","5825f4ac":"#creating a function which splits a dataframe into train,validation and test\ndef split_x(df):\n    train,test=train_test_split(df,test_size=0.3,random_state=1)\n    val,test=train_test_split(test,test_size=0.5,random_state=1)\n    return train,val,test","e6c3b40e":"#splitting data in train,validation and test\ntrain,val,test=split_x(data)\nprint('train shape:',train.shape,'\\n','val shape:',val.shape,'\\n','test shape:',test.shape,'\\n')","bbc33dda":"train.describe(include='all')","84ea3209":"val.describe(include='all')","478d052e":"#getting percentage of missing values in each column\nround(train.isnull().sum()\/len(train)*100,2)","c1f5b8d5":"print('percent of null values in department:',round(train.department.isnull().sum()\/len(train)*100,2),'%')","0ed1e4e4":"train.department.isnull().sum()","07fd728e":"train.department.value_counts()","ae192f31":"sns.countplot(x='department',data=train,order=train.department.value_counts().index)\nplt.title('Department wise distribution')\nplt.xticks(rotation=45)","b0a1c7b8":"#after complete examination of data creating a single function to fill all na's in a dataframe\ndef fill_all_na(df):\n    #creating a new department 'No_dept'(not sure about what to impute it with)...later will create a categorical column\n    df.department.fillna(value='No_dept',axis=0,inplace=True)\n    df.filed_complaint.fillna(value=0,axis=0,inplace=True)#filling na's with 0...beacuse most probably people will not file a complaint.\n    df.last_evaluation.fillna(value=0.716819,axis=0,inplace=True)#filling with the mean\n    df.recently_promoted.fillna(value=0,axis=0,inplace=True)#filling na's with 0\n    df.satisfaction.fillna(value=0.622162,axis=0,inplace=True)\n    df.tenure.fillna(value=3.50218,axis=0,inplace=True)\n    return df","f0bfa9d6":"train=fill_all_na(train)\nval=fill_all_na(val)\ntest=fill_all_na(test)\ntrain.describe(include='all')","d51020f3":"round(train.isnull().sum()\/len(train)*100,2)","77bc9bc7":"train.dtypes","7f03c99d":"#creating a function to convert few variables in categorical type.\ndef into_category(df):\n    cols=['department','filed_complaint','recently_promoted','salary','status']\n    df[cols]=df[cols].astype('category')\n    return df","64d7a058":"train=into_category(train)\nval=into_category(val)\ntest=into_category(test)","5fabf324":"train.dtypes","962695f8":"#creating a new columnn 'No_dept' which gives a value 1 if a row in department column has a string 'No_dept'\n#else 0....it is to give \ndef new_col(df):\n    df['no_dept']=[1 if x=='No_dept' else 0 for x in df['department']]\n    df['no_dept']=df['no_dept'].astype('category')\n    return df","62ff471e":"train=new_col(train)\nval=new_col(val)\ntest=new_col(test)\n","f2d824d9":"train.dtypes","a9d26990":"train.describe(include='all')","46d05958":"sns.boxplot(x='status',data=train,y='satisfaction')\nplt.title('status vs satisfaction')\n#observations\n#people who leave are usually less satisfied of their job(role)","f74a9570":"sns.boxplot(x='status',y='avg_monthly_hrs',data=train)  \nplt.title('status vs avg_monthly_hrs')\n#apart of not being satisfied they work for more hours than other employees.","8e62391b":"sns.countplot(x='status',data=train,hue='filed_complaint')\nplt.title('status vs count vs file_complaint')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","134a0f93":"sns.boxplot(x='status',data=train,y='last_evaluation')\nplt.title('status vs last_evaluation')\n#people who left the company usually scored more than those in the company.","4b837277":"sns.countplot(x='n_projects',data=train,hue='status')\nplt.title('#projects vs count vs status')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","4b05a15a":"sns.boxplot(x='n_projects',data=train,y='last_evaluation',hue='status')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('#projects vs last_evaluation vs status')","af6eb9f7":"sns.boxplot(x='recently_promoted',data=train,y='last_evaluation',hue='status')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('recently promoted vs last_evaluation vs status')","b1c5c39d":"sns.boxplot(x='salary',data=train,y='last_evaluation',hue='status',order=['low','medium','high'])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('salary vs last_evaluation vs status')","7463ae8c":"sns.boxplot(x='tenure',data=train,y='satisfaction',hue='status',order=[0,1,2,3,4,5,6,7,8,9,10])\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n","6b8ece6d":"#function to split data into predictors and targets\ndef splitting_y(df):\n  df_x=df.loc[:,df.columns!='status']\n  df_y=df['status']\n  return df_x,df_y","295a4e50":"train_x,train_y=splitting_y(train)\nval_x,val_y=splitting_y(val)\ntest_x,test_y=splitting_y(test)","6c60220e":"train_x.describe(include='all')","2259aa86":"train_x.dtypes","5d11e9ee":"#creating list of numerical and categorical columns\nnum_cols=['avg_monthly_hrs','last_evaluation','n_projects','satisfaction','tenure']\ncat_cols=['department','filed_complaint','recently_promoted','salary','no_dept']","6a83e867":"#creating numerical and categorical dataframes for train,validation and test\ntrain_num_x=train_x.loc[:,num_cols]\ntrain_cat_x=train_x.loc[:,cat_cols]\nval_num_x=val_x.loc[:,num_cols]\nval_cat_x=val_x.loc[:,cat_cols]\ntest_num_x=test_x.loc[:,num_cols]\ntest_cat_x=test_x.loc[:,cat_cols]","286641ef":"#standardizing the numerical columns using standardscaler()\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_num_x[train_num_x.columns])\ntrain_num_x[train_num_x.columns] = scaler.transform(train_num_x[train_num_x.columns])\nval_num_x[val_num_x.columns] = scaler.transform(val_num_x[val_num_x.columns])\ntest_num_x[test_num_x.columns] = scaler.transform(test_num_x[test_num_x.columns])","169406d3":"#creating dummy variables of categorical variables\ntrain_dummy_x=pd.get_dummies(train_cat_x,drop_first=True)\nval_dummy_x=pd.get_dummies(val_cat_x,drop_first=True)\ntest_dummy_x=pd.get_dummies(test_cat_x,drop_first=True)","2ce396e8":"#combining standardized numerical dataframe and dummyfied categorical dataframe.\nfull_train_x=pd.concat([train_num_x,train_dummy_x],axis=1)\nfull_val_x=pd.concat([val_num_x,val_dummy_x],axis=1)\nfull_test_x=pd.concat([test_num_x,test_dummy_x],axis=1)","05787fa6":"full_train_x.describe(include='all')","f277546d":"full_val_x.isnull().sum()\n#no null values are present","864806d1":"#importing required packages for model building\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","e622da90":"knn=KNeighborsClassifier()\nparam_grid_knn_1={\n    'n_neighbors':np.arange(3,15,1),\n    'weights':['uniform','distance'],\n    'algorithm':['auto','brute'],\n}\nknn_random=RandomizedSearchCV(estimator=knn,param_distributions=param_grid_knn_1,n_iter=1000,n_jobs=-1,cv=5,verbose=1)","7afdf100":"knn_random.fit(full_train_x,train_y)#training knn algorithm","23e2df55":"knn_random.best_params_#getting best performing hyperparameters","c2e5c185":"#using grid search to search for best parameters around the output of random search parameters\nparam_grid_knn2={\n    'n_neighbors':np.arange(2,10,1),\n    'weights':['uniform','distance'],\n    'algorithm':['auto','brute'],\n}","8bf52979":"knn_grid=GridSearchCV(estimator=knn,param_grid=param_grid_knn2,n_jobs=-1,cv=5,verbose=1)","7710a2b2":"#training knn model with grid search\nknn_grid.fit(full_train_x,train_y)","223ed4ac":"knn_grid.best_params_#best parameters of grid search are similar to random search","8cc4d3e2":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","ca49d96f":"knn2=KNeighborsClassifier(algorithm= 'auto', n_neighbors= 2, weights= 'uniform')","16e742da":"#learning curves for knn\nplot_learning_curve(estimator=knn2,title='knn_learning_curves',X=full_train_x,y=train_y,ylim=(0.75,1.05),cv=5)","6b081ec7":"#learning seems to be good we will train knn with best parameters\nknn2.fit(full_train_x,train_y)","7af7a541":"#predicting the validation labels \nknn_prediction_val=knn2.predict(full_val_x)","7a841267":"#classification report to get all important metrics required for classification\nfrom sklearn.metrics import classification_report\nprint('knn classification report\\n')\nprint(classification_report(val_y,knn_prediction_val))","14db22c7":"from sklearn import tree","d7729c24":"#creating a random search for some hyper parameters given in param_grid_1\ndt=tree.DecisionTreeClassifier()\nparam_grid_1={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(4,20,1),\n    'min_samples_split':np.arange(0.001,0.1,0.01),\n    'max_features':['log2','sqrt','auto'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05)\n}\nr_search=RandomizedSearchCV(dt,param_distributions=param_grid_1,n_iter=1000,verbose=1)\nr_search.fit(full_train_x,train_y)","f785f3e4":"#getting best performing hyper parameters from random search \nr_search.best_params_","3ca557c1":"#creating another parameter grid for grid search by taking values around the best performing random search\n#hyper parameters\nparam_grid_2={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(12,18,1),\n    'min_samples_split':np.arange(0.001,0.01,0.01),\n    'max_features':['log2','sqrt'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.05,0.01)\n}","9369ea81":"dt=tree.DecisionTreeClassifier()\ngrid_search=GridSearchCV(estimator=dt,param_grid = param_grid_2,cv=5,verbose=1,n_jobs=-1)\ngrid_search.fit(full_train_x,train_y)\ngrid_search.best_params_#getting best parameters of grid search","b9d34387":"#learning curve for decision tree\ndt=tree.DecisionTreeClassifier(criterion= 'gini',max_depth= 16,max_features= 'log2',min_samples_split= 0.001,min_weight_fraction_leaf= 0.001)\nplot_learning_curve(estimator=dt,title='dt_learning_curves',X=full_train_x,y=train_y,ylim=(0.75,1.05),cv=5)","2ee8e155":"#learning curve seems ok....fitting dt with best parameters\ndt=tree.DecisionTreeClassifier(criterion= 'gini',max_depth= 16,max_features= 'log2',min_samples_split= 0.001,min_weight_fraction_leaf= 0.001)\ndt.fit(full_train_x,train_y)","c71911c8":"#predicting validation labels\ndt_prediction_val=dt.predict(full_val_x)","54a7564b":"#getting classification report for decision tree\nprint('Decision tree classification report\\n\\n')\nprint(classification_report(val_y,dt_prediction_val))","ac12f6c3":"#importing random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier()\nrf.get_params","2f265ddb":"#creating parameter grid for random search\ngrid_forest_1={'criterion':['gini','entropy'],\n      'n_estimators':np.arange(5,200,1),\n      'max_depth':np.arange(5,20,1),\n      'min_samples_split':np.arange(0.001,0.1,0.01),\n      'max_features':['log2','sqrt','auto'],    \n      'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05)\n}","ba7bcaee":"#getting best parameters form random search\nrf_random=RandomizedSearchCV(estimator=rf,param_distributions=grid_forest_1,n_iter=500,n_jobs=-1,cv=5,verbose=1)","1de92cce":"rf_random.fit(full_train_x,train_y)","8ae8d362":"rf_random.best_params_","14cdc53c":"grid_forest_2={'criterion':['entropy'],\n      'n_estimators':np.arange(115,135,2),\n      'max_depth':(17,18,19,20,21),\n      'min_samples_split':np.arange(0.001,0.01,0.005),\n      'max_features':['log2'],    \n      'min_weight_fraction_leaf':np.arange(0.0001,0.1,0.005)\n}","c01f1bb1":"rf=RandomForestClassifier()\ngrid_search_rf=GridSearchCV(estimator=rf,param_grid = grid_forest_2,cv=3,n_jobs=-1,verbose=1)","fd84a057":"grid_search_rf.fit(full_train_x,train_y)\ngrid_search_rf.best_params_","81d0ca9d":"rf=RandomForestClassifier(criterion='entropy',max_depth= 17,max_features='log2',min_samples_split= 0.001,min_weight_fraction_leaf= 0.001,n_estimators= 129)\nrf.fit(full_train_x,train_y)","b1b6bccb":"plot_learning_curve(estimator=rf,title='RF_learning_curves',X=full_train_x,y=train_y,ylim=(0.75,1.05),cv=5)","b8fc2f07":"rf_predictions_val_y=rf.predict(full_val_x)","b5a12281":"print(classification_report(val_y,rf_predictions_val_y))","0751558b":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.get_params","e5eef082":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV","694d9e7d":"param_grid_gbc_1={\n    'n_estimators':np.arange(50,200,10),\n    'learning_rate':np.arange(0.01,0.3,0.01),\n    'subsample':np.arange(0.5,1.0,0.1),\n    'max_depth':np.arange(2,7,1),\n    'max_features':['sqrt','log2'],\n    'verbose':[1]\n}","2bc4b361":"random_gbc=RandomizedSearchCV(estimator=gbc,param_distributions=param_grid_gbc_1,n_jobs=-1,n_iter=1000,verbose=1)","a6f4cc48":"random_gbc.fit(full_train_x,train_y)\nrandom_gbc.best_params_","ef4ad671":"#searching for better parameters in random search with grid search\nparam_grid_gbc_2={'learning_rate':np.arange(0.1,0.2,0.05),\n 'max_depth': (5,6,7,8),\n 'max_features': ['log2'],\n 'n_estimators': np.arange(158,162,1),\n 'subsample': np.arange(0.8,0.9,0.02),\n}","9482208b":"grid_search_gbc=GridSearchCV(estimator=gbc,param_grid=param_grid_gbc_2,n_jobs=-1,cv=3,verbose=1)","32b3687b":"grid_search_gbc.fit(full_train_x,train_y)\ngrid_search_gbc.best_params_","656e64f5":"gradient_boosting=GradientBoostingClassifier(learning_rate= 0.15,max_depth= 8,max_features= 'log2',n_estimators= 161,subsample= 0.8)\n","9a96b678":"#plotting learning curves\nplot_learning_curve(estimator=gradient_boosting,title='GB_learning_curves',X=full_train_x,y=train_y,ylim=(0.75,1.05),cv=5)","707dac81":"gradient_boosting.fit(full_train_x,train_y)\ngb_prediction_val=gradient_boosting.predict(full_val_x)","bc7532e2":"print('gradient boosting classification report\\n\\n')\nprint(classification_report(val_y,gb_prediction_val))","7fe25011":"score_df=pd.DataFrame({' ':['Employeed','Left'],'KNN-f1_score':[0.97,0.90],'Decision Tree-f1_score':[0.9,0.90],'Random_forest-f1_score':[0.98,0.93],'Gradient_boosting-f1_score':[0.99,0.96]})","b4a21bb9":"score_df","75785b65":"#since we got same f1 scores on validation...choose random forest or gradient boosting for predictions","3d696264":"## modelling","e9bf4758":"## DT-Decision Tree","54bebd1a":"## Gradient Boosting Classifier","714ab2d8":"## Problem Statement: To predict if a person is going to leave the company or going to stay in it.\n### Data Description:\n1. 'status' \u2013 Current employment status (Employed \/ Left)\n2. 'department' \u2013 Department employees belong(ed) to\n3. 'salary' \u2013 Salary level relative to rest of their department\n4. 'tenure' \u2013 Number of years at the company\n5. 'recently_promoted' \u2013 Was the employee promoted in the last 3 years?\n6. 'n_projects' \u2013 Number of projects employee is staffed on\n7. 'avg_monthly_hrs' \u2013 Average number of hours worked per month\n8. 'satisfaction' \u2013 Score for employee\u2019s satisfaction with the company (higher is better)\n9. 'last_evaluation' \u2013 Score for most recent evaluation of employee (higher is better)\n10. 'filed_complaint' \u2013 Has the employee filed a formal complaint in the last 3 years?\n\n","473cf2e9":"# Employee attrition prediction","ad9a1bc7":"### Importing required packages","f8aa91bf":"## function to create learning curves to see bias and variance of models","36e10c99":"## RF-Random Forest","a608808d":"## KNN-(Kneighbour classification)"}}