{"cell_type":{"a732ef64":"code","68f198c5":"code","b5f3d49d":"code","3aacdb42":"code","defe299a":"code","3b325703":"code","af6c9418":"code","ccdbc9fa":"code","fb8b30cf":"code","119b32ec":"code","e34570df":"code","479e040c":"code","f10df2f3":"code","b7eebf8b":"markdown"},"source":{"a732ef64":"# General\nimport numpy as np \nimport pandas as pd\n\n\n#For Keras \nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, Input, GlobalMaxPool2D\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# For visualizing results\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import class_weight\n","68f198c5":"# Defining the path to the images.\nimg_path = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"\n\n# Reading the datasets csv files:\nsample_submission = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\")\ntest              = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")\ntrain             = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\")\n\n#Adding the full image filname to easily read it from ImageDataGenerator\ntrain[\"imaged_id_fileName\"] = train.image_id+\".jpg\"\ntest[\"imaged_id_fileName\"]  = test.image_id+\".jpg\"\n\n#Show the strucutre of the training structure:\ntrain.head()","b5f3d49d":"# Data augmentation using ImageDataGenereator. \n#Applying moderate amount of zoom in\/out and brightness variation. Full rotation and flips are applied since there is no obvious orientation that the pictures of the leafs are taken.\n\n#Keeping approximate aspect ratio of the images:\nimg_height = 100\nimg_width = 133\n\n#Defining the batch size that will be used in training:\nbatch_size = 32\n\n#Labels inferred from the dataset: \nlabels = [\"healthy\",\"multiple_diseases\",\"rust\",\"scab\"]\n\n#Define the ImageDataGenerator using a training\/validation split of 80\/20% \ntrain_dataGenerator = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0,\n    zoom_range=(1, 1.3),\n    rotation_range = 360,\n    brightness_range = (0.7, 1.3),                                                   \n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0.2)\n\ntrain_generator = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size,class_mode='raw', subset='training') \n\nvalidation_generator = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw', subset='validation')\n\n#This validator generator will be used to plot confusion matrix where we need the shuffle to be off.\nvalidation_generator2 = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw',shuffle=False,\n    sort = False, subset='validation') \n\n# Later we want to use the full dataset for training since we have quite a limited number of images. Below we define the generator for that case:\ntrain_dataGenerator_full = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0,\n    zoom_range=(1, 1.3),\n    rotation_range = 360,\n    brightness_range = (0.7, 1.3),                                                   \n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0) \n\ntrain_generator_full = train_dataGenerator_full.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw', subset='training') \n\n# Finally we also define the ImageDataGenerator for the unlabled test data:\ntest_dataGenerator = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_dataGenerator.flow_from_dataframe(\n    dataframe=test,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, shuffle = False, sort = False,\n    target_size=(img_height, img_width), batch_size=1, class_mode=None)","3aacdb42":"# Calculating the prior probability of the different labes from the training dataset\nclassProb =np.zeros(len(labels))\nidx = 0\nfor k in labels:\n    print(f\"{k} contains {train[k].sum()} samples\")\n    classProb[idx] = train[k].sum()\n    idx+=1\n\n# Visualizing the results in a pie-chart:\nprint() #Empty line before figure\ncolor = ['#58508d','#bc5090','#ff6361', '#ffa600'] \nplt.figure(figsize=(15,7))\nplt.pie(classProb, shadow=True, explode=[0,0.5, 0, 0],labels=labels,\n        autopct='%1.2f%%', colors=color, startangle=-90,\n        textprops={'fontsize': 14})\n\nclass_weight_vect =np.square(1 \/ (classProb\/classProb.sum()) )# Calculate the weight per classbased on the prior probability dervied from the training data.\nclass_weight_vect=class_weight_vect\/np.min(class_weight_vect)           ","defe299a":"# Visualize the data augmentation \n# Plot function taken inspiration from here:\n# https:\/\/github.com\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb\n\ndef plotImages(imgs):\n    col=5\n    row=2\n    fig, axes = plt.subplots(row, col, figsize=(25,25))  \n    axes = axes.flatten()\n    for k in range(10):\n        axes[k].imshow(imgs[k])\n    fig.subplots_adjust(hspace=-0.75, wspace=0.2) \n    plt.show()\n\n    \n#Apply augmentation to the same picture 10 times and plot the outcome:   \nplotImageAugmentation = [validation_generator2[1][0][0] for i in range(10)] #Using validation_generator2 for consitency since shuffle is turned off.\nplotImages(plotImageAugmentation)","3b325703":"# Define the convolutional neural network:\nmodel = Sequential()\nmodel.add(Conv2D(35, kernel_size=(3, 3), activation='relu', kernel_initializer='glorot_uniform', \n                 bias_initializer='zeros',  input_shape=(img_height, img_width, 3), padding='same'))\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', \n                 activation='relu', padding='same'))\nmodel.add(Dropout(0.1))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(MaxPool2D(pool_size=(5, 5)))\n\nmodel.add(Conv2D(50, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Conv2D(50, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(GlobalMaxPool2D())\n\nmodel.add(Dropout(0.1))\nmodel.add(Dense(4, activation='softmax'))\n\noptimizerAdam = Adam(lr=0.00125, amsgrad=True)\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizerAdam, metrics=[\"accuracy\"])","af6c9418":"#Print a summary of the model:\nmodel.summary()","ccdbc9fa":"#Train the CNN:\nnb_epochs = 100\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples \/\/ batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ batch_size,\n    epochs = nb_epochs,\n    class_weight=class_weight_vect)","fb8b30cf":"# Display the training performance \nfs = 17\nfig = plt.figure(figsize=(9,5))\nfig.patch.set_facecolor('xkcd:white')\nplt.plot(history.history['accuracy'], color=color[0])\nplt.plot(history.history['val_accuracy'], color=color[3])\nplt.ylabel('Accuracy',fontsize=fs)\nplt.xlabel('Epoch #',fontsize=fs)\nplt.legend(['training', 'validation'],fontsize=fs)\nplt.grid('both', linestyle='--')\nplt.xticks(fontsize = fs)\nplt.yticks(fontsize = fs)\nplt.show()\n\n# summarize history for loss\nfig = plt.figure(figsize=(9,5))\nfig.patch.set_facecolor('xkcd:white')\nplt.plot(history.history['loss'], color=color[0])\nplt.plot(history.history['val_loss'], color=color[3])\nplt.ylabel('Loss',fontsize=fs)\nplt.xlabel('Epoch #',fontsize=fs)\nplt.legend(['training', 'validation'],fontsize=fs)\nplt.grid('both', linestyle='--')\nplt.xticks(fontsize = fs)\nplt.yticks(fontsize = fs)\nplt.show()","119b32ec":"# Plot the classification performance in a confusion matrix\nY_pred = model.predict(validation_generator2)\nY_pred_labels = np.argmax(Y_pred, axis=1)\ny_true = np.argmax(validation_generator.labels, axis=1 )\n\n\nlabels_num = [0,1,2,3]\ncm = confusion_matrix( y_true, Y_pred_labels, normalize='true')\nsn.set(font_scale=1.4) # for label size\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, cmap=\"YlGnBu\", xticklabels = labels, yticklabels = labels)\nplt.show()\n\n# Print the classification report:\nprint(classification_report(y_true, Y_pred_labels))","e34570df":"# Since the labeled dataset is limited and we seen that overfitting is not a major issue, we proceed to train the model over all the images to hopefully incrase the accuracy for the unlabled data\nnb_epochs = 50\nhistory = model.fit_generator(\n    train_generator_full,\n    steps_per_epoch = train_generator_full.samples \/\/ batch_size,\n    epochs = nb_epochs,\n    class_weight=class_weight_vect)","479e040c":"# Finally we apply the model to predict the unlabed test data:\ntest_predictions = model.predict_generator(test_generator)","f10df2f3":"# Download the final predictions on the test data as a csv-file that can be uploaded to Kaggle.\npredictions = pd.DataFrame()\npredictions['image_id'] = test.image_id\npredictions['healthy'] = test_predictions[:, 0]\npredictions['multiple_diseases'] = test_predictions[:, 1]\npredictions['rust'] = test_predictions[:, 2]\npredictions['scab'] = test_predictions[:, 3]\npredictions.to_csv('submission.csv', index=False)\npredictions.head(10)\n\n# Uncomment to donwload csv-file:\n# from google.colab import files\n# files.download(\"submission.csv\")","b7eebf8b":"# This notebook implements classification using CNNs in Keras for the Plant Pathology 2020 Challenge \n\nThe main purpouos of this notebook is to strengthen my pratical knowledge of implementing different methods of deep learning. However, I decided share this notebook in hope that others can benefit from it. \n\n**Purpouse of this notebook:**\n* Learn how to implelemt data augmentation in Keras\n* Practice implementing CNNs in Keras\n* Learn how to use training with class weights for an imbalanced data set\n\n** This notebook does NOT attempt:**\n* Achieving top score on the dataset \n* Optimize learning speed\n\n**Comments:**\n* Achives approxmiately a score of 0.94 on the public test data.\n* Design to run with GPU accelerator on Kaggle. However, at the time of commit the GPU accleration is not working properly on Kaggle ( https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/136058#833053 )\n    * Some failed commits due to running out of memory on CPU \n* Even with heavy class weighting, the CNN does not perform well for the class with low number of observations (5%), i.e. \"multiple_diseases\".\n \n "}}