{"cell_type":{"71f1e541":"code","f52a0093":"code","2b80e28b":"code","95d1e055":"code","c9b6783b":"code","3cabda1a":"code","afa6e08b":"code","456d9b0b":"code","94644649":"code","4ded7912":"code","9e410994":"code","832aa78f":"code","30738390":"code","f649cc0b":"code","e1f3a438":"code","89550ced":"code","b94c700e":"code","e10cccb7":"code","b77cffad":"code","b4ca1d6c":"markdown","7010ce9a":"markdown","33890af6":"markdown","3e472594":"markdown","a6034f5a":"markdown","8ebc10d0":"markdown","dc423d8b":"markdown","3a4d02bb":"markdown","e3ba6415":"markdown","f49361f9":"markdown","599a2cc5":"markdown","1c21ccc8":"markdown","26ad6bec":"markdown","46373066":"markdown","8356e1da":"markdown","9ac52886":"markdown","4d8c6bba":"markdown"},"source":{"71f1e541":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport umap\n\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n","f52a0093":"class params:\n    IMG_SIZE = (28, 28)\n    RANDOM_SEED = 790\n    NUM_LABELS = 10","2b80e28b":"def plot_entry_row(ax, row, label=None):\n    mat = row.reshape(params.IMG_SIZE)\n    ax.imshow(mat)\n    ax.set_xlabel(f'Label : {label}') if label is not None else 0\n    \n    \ndef plot_history(h):\n    h = h.history\n    _, axs = plt.subplots(1, 2, figsize=(15, 5))\n    axs = np.ravel(axs)\n    nb_epochs = len(h['acc']) + 1\n    \n    ax = axs[0]\n    ax.plot(range(1, nb_epochs), h['acc'], '--', label='Train')\n    ax.plot(range(1, nb_epochs), h['val_acc'], label='Val')\n    ax.legend()\n    \n    ax = axs[1]\n    ax.plot(range(1, nb_epochs), h['loss'], '--', label='Train')\n    ax.plot(range(1, nb_epochs), h['val_loss'], label='Val')\n    ax.legend()\n    \n    \ndef plot_embeddings(emb_train, train_labels, emb_test):\n    reducer = umap.UMAP()\n    twodim_train = reducer.fit_transform(emb_train)\n    twodim_test = reducer.transform(emb_test)\n    \n    _, ax = plt.subplots(1, 1, figsize=(10, 10))\n    ax.scatter(twodim_test[:,0], twodim_test[:,1])\n    \n    for i in range(100):\n        pos_x, pos_y = twodim_train[i,0], twodim_train[i,1]\n        ax.text(pos_x, pos_y, f'{train_labels[i]}')\n        \n        \ndef get_confusion_matrix(y_pred, y_true, normalize=True):\n    num_labels = params.NUM_LABELS\n    conf = np.zeros((num_labels, num_labels))\n    for yp, yt in zip(y_pred, y_true):\n        conf[yt][yp] += 1\n    if normalize:\n        conf = conf \/ conf.sum(axis=1)\n    return conf\n    \n\ndef plot_confusion_matrix(y_pred, y_true):\n    mat = get_confusion_matrix(y_pred, y_true)\n    _, ax = plt.subplots(1, 1, figsize=(10, 8))\n    sns.heatmap(mat, annot=True, ax=ax, cmap='Blues')\n    ax.set_ylabel('True')\n    ax.set_xlabel('Prediction')\n    ","95d1e055":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(f'(Rows, Cols): {df_train.shape}')","c9b6783b":"X = df_train[[f'pixel{i}' for i in range(784)]].values.astype(float)\nX = X.reshape((X.shape[0], 28, 28, 1))\ny = tf.keras.utils.to_categorical(df_train.label.values)","3cabda1a":"# Visualize some entries\n_, axs = plt.subplots(1, 10, figsize=(20, 5))\nfor i, ax in enumerate(np.ravel(axs)):\n    plot_entry_row(ax, X[i], label=np.argmax(y[i]))","afa6e08b":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05, random_state=params.RANDOM_SEED, shuffle=True)\nprint(\"X_train :\", X_train.shape)\nprint(\"X_val\", X_val.shape)","456d9b0b":"# Non-categorical labels\ntrain_labels = np.argmax(y_train, axis=1)\nval_labels = np.argmax(y_val, axis=1)","94644649":"flat_X_train = X_train.reshape(X_train.shape[0], 28*28)\nflat_X_val = X_val.reshape(X_val.shape[0], 28*28)\n\nplot_embeddings(flat_X_train[:2000], train_labels, flat_X_val)","4ded7912":"\ninp = tf.keras.Input(shape=(28, 28, 1))\n\nx = layers.Conv2D(128, (5, 5), activation='relu', padding='same')(inp)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.MaxPooling2D((2, 2))(x)\nx = layers.Dropout(0.2)(x)\n\nx = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.MaxPooling2D((2, 2))(x)\nx = layers.Dropout(0.2)(x)\n\nx = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.MaxPooling2D((2, 2))(x)\nx = layers.Dropout(0.2)(x)\n\nx = layers.Flatten()(x)\n\nx = layers.Dense(128, activation='relu')(x)\n\nencoder = tf.keras.Model(inputs=inp, outputs=x)\n\ninp = tf.keras.Input(shape=(28, 28, 1))\nx = encoder(inp)\nout = layers.Dense(10, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inp, outputs=out)\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n             loss='categorical_crossentropy',\n             metrics=['acc']\n             )\n\nmodel.summary()","9e410994":"earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nh = model.fit(X_train, y_train, epochs=20, batch_size=64,\n              validation_data=(X_val, y_val), \n              callbacks=[earlystop]\n             )","832aa78f":"plot_history(h)","30738390":"model.evaluate(X_val, y_val)","f649cc0b":"emb_train = encoder(X_train[:2000])\nemb_val = encoder(X_val)\nplot_embeddings(emb_train, np.argmax(y_train, axis=1), emb_val)","e1f3a438":"# Errors\nprobas = model.predict(X_val)\npreds = np.argmax(probas, axis=1)\n\ni_errors = np.where(preds != val_labels)[0]\nerrors = X_val[i_errors]\ny_label_error_val = val_labels[i_errors]\npred_errors = preds[i_errors]\n\n_, axs = plt.subplots(1, 20, figsize=(40, 3))\nfor i, ax in enumerate(np.ravel(axs)):\n    if i >= len(errors):\n        break\n    plot_entry_row(ax, errors[i], label=y_label_error_val[i])\n    ax.set_title(f'Pred : {pred_errors[i]}')","89550ced":"plot_confusion_matrix(preds, val_labels)","b94c700e":"df_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nX_test = df_test[[f'pixel{i}' for i in range(784)]].values.astype(float)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)","e10cccb7":"probas = model.predict(X_test)\npreds = np.argmax(probas, axis=1)\n\ndf = pd.DataFrame({\n    'ImageId': range(1, len(X_test)+1),\n    'Label': preds\n})","b77cffad":"df.to_csv('sample.csv', index=False)","b4ca1d6c":"## Modelling","7010ce9a":"The image embeddings can be easily grouped to form clusters that share the same label. This is way better than before embedding. \n* Dots = validation examples\n* Numbers = Train examples","33890af6":"## Split","3e472594":"## Visualizing errors","a6034f5a":"We can clusters but they do not separate labels very well. Let's see if it is possible to do better.","8ebc10d0":"## Import & params","dc423d8b":"## Reading data","3a4d02bb":"Here, I will use a simple CNN model. It is composed of 3 blocks of convolutional layers. Each block is made of 2 convolutional layers.\n\n* Loss : Crossentropy\n* Otpimizer : Adam (lr=10e-4)\n\nI define an **encoder** variable to see how images are embedded to a smaller representation.","e3ba6415":"## Submit predictions","f49361f9":"## Visualizing","599a2cc5":"Since the dataset is relatively huge, I use a small portion (5%) of the initial set as a validation set. It is enough to get a good reprentation of the test set. ","1c21ccc8":"## TL;DR\n\n* **Accuracy on leaderboard** : 0.9928\n* **Model** : CNN\n\n<img src=\"https:\/\/i.ibb.co\/sQz51mf\/ab.png\" style='width: 1000px;'>\n\n**Room for improvements :**\n* Data augmentation :  transform inital set to produce more examples\n* Filter out weird examples for the train set\n* Add data from another source (not sure about adding data from tensorflow's MNIST dataset, might be data leakage if both sets are from the same source)\n* Adding helpful preprocessing steps","26ad6bec":"Images are reshaped into a 28x28 shape. \nNothing fancy here.","46373066":"Let's plot the projection of data to see if we can isolate some clusters using UMAP.\n\n* Dots = validation examples\n* Numbers = Train examples","8356e1da":"## Preprocessing","9ac52886":"## Utilities\n\nFunctions that are going to be used later on.","4d8c6bba":"# CNN classifier for MNIST dataset (> 0.99 acc in test)"}}