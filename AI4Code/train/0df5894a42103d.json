{"cell_type":{"1d5d5fbd":"code","c02f56a9":"code","b01843f0":"code","23ef7c52":"code","2834098e":"code","8b717a7f":"code","b0477524":"code","640c8ebf":"code","d472da72":"code","abe3db16":"code","cb99cb9f":"code","03663d4f":"code","190cd480":"code","b5cb6702":"code","0cabe4d8":"code","3fa30867":"markdown","5ff4c3f7":"markdown","7b2d1c0d":"markdown","5abc9170":"markdown","a8fc8b4a":"markdown","e1a02550":"markdown","46355ac7":"markdown","1a061915":"markdown"},"source":{"1d5d5fbd":"# from data_processing import modify_dataset\nimport sklearn.model_selection as cv\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn.model_selection  as ms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport missingno as msno\nimport seaborn as sns\nfrom collections import namedtuple\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c02f56a9":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b01843f0":"# train missing \nmsno.matrix(train)\n","23ef7c52":"\n# test missing \nmsno.matrix(test)","2834098e":"# extract train x and y\ntrain_x = train.drop(columns=[\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\", \"Survived\"])\n\nmedian_age = np.median(train.loc[train[\"Age\"].isna() == False, \"Age\"])\n\ntrain_x[\"Embarked\"] = train_x[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\":2})\nmedian_em = np.median(train_x.loc[train_x[\"Embarked\"].isnull()== False, \"Embarked\"])\nmedian_fare = np.median(train_x[\"Fare\"])\n\ntrain_x.loc[train_x[\"Embarked\"].isnull(),\"Embarked\" ] = median_em\ntrain_x.loc[train_x[\"Age\"].isna(), \"Age\"] = median_age\ntrain_x[\"Sex\"] = train_x[\"Sex\"].map({\"male\": 0, \"female\": 1})\n\ntest_x = test.drop(columns=[\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"])\n\ntest_x[\"Embarked\"] = test_x[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\ntest_x.loc[test_x[\"Age\"].isna(), \"Age\"] = median_age\ntest_x.loc[test_x[\"Embarked\"].isnull(), \"Embarked\"] = median_em\ntest_x[\"Sex\"] = test_x[\"Sex\"].map({\"male\": 0, \"female\": 1})\ntest_x.loc[test_x[\"Fare\"].isna(), \"Fare\"] = median_fare\n","8b717a7f":"train_x = train_x.convert_dtypes()\ntest_x = test_x.convert_dtypes()\ntrain_y = train[\"Survived\"]","b0477524":"fig, ax = plt.subplots(figsize=(8, 7))\nax = sns.heatmap(train_x.corr(), ax=ax, annot=True, vmax=1, vmin=-1, cmap=\"RdBu_r\")\nax.set_title(\"Correlation Plot Of Train Data\")\n# fig.savefig(\"corr_plot\", dpi = 150, bbox_inches = \"tight\")\nplt.show()\n","640c8ebf":"# using the typical 70 -30 split\nXtrain, Xtest, Ytrain, Ytest = ms.train_test_split(train_x, train_y, test_size=.3, random_state=0)\n","d472da72":"def prec_results(model, Xtrain, Ytrain, Xtest, Ytest):\n    fig, ax = plt.subplots(figsize=(5, 5))\n    lg_cm = ConfusionMatrix(model, classes=[\"Deaths\", \"Survive\"], label_encoder={0: \"Deaths\", 1: \"Survive\"})\n    lg_cm.fit(Xtrain, Ytrain)\n    lg_cm.score(Xtest, Ytest)\n    lg_cm.show()\n\n    lg_cr = ClassificationReport(model, classes=[\"Deaths\", \"Survive\"], label_encoder={0: \"Deaths\", 1: \"Survive\"})\n    lg_cr.fit(Xtrain, Ytrain)\n    lg_cr.score(Xtest, Ytest)\n    lg_cr.show()\n\n    lg_roc = ROCAUC(model)\n    lg_roc.fit(Xtrain, Ytrain)\n    lg_roc.score(Xtest, Ytest)\n    lg_roc.show()","abe3db16":"\n# collection to hold my final precison data\nplt_lst = []\nplot_data = namedtuple(\"plot_data\",  \"x y model d\")\n","cb99cb9f":"\n# using tunning parameters\n\npen_val = [\"l1\", \"l2\"]\nc_val = 2. ** np.arange(-40, 10, step=2)\nsol = [\"liblinear\"]\nmax = np.arange(10000, 15000, 1000)\ngrid_s = {\"C\": c_val, \"penalty\": pen_val, 'solver': sol, 'max_iter': max}\n\ncv_logr = GridSearchCV(estimator=LogisticRegression(), param_grid=grid_s, cv=ms.KFold(n_splits=10))\ncv_logr.fit(Xtrain, Ytrain)\n\n# using best parameters\nmodel = LogisticRegression(**cv_logr.best_params_)\nmodel.fit(Xtrain, Ytrain)\ntune_predict = model.predict_proba(Xtest)\nprec_results(model, Xtrain, Ytrain, Xtest, Ytest)\n\nfpr, tpr, threshold = roc_curve(Ytest, tune_predict[:, 1])\nfor i, j in enumerate(tpr):\n    plt_lst.append(plot_data(y=tpr[i], x=fpr[i], model=\"logistic\", d = \"T1\"))\n","03663d4f":"nei = np.arange(1, 10)\n\ngrid_s = {'n_neighbors': nei}\ncv_rf = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=grid_s, cv=ms.KFold(n_splits=10))\ncv_rf.fit(Xtrain, Ytrain)\n# print(cv_rf.best_params_)\n#\nknn_model = KNeighborsClassifier(**cv_rf.best_params_)\nknn_model.fit(Xtrain, Ytrain)\npred = knn_model.predict_proba(Xtest)\n\nprec_results(knn_model, Xtrain, Ytrain, Xtest, Ytest)\n\nfpr, tpr, threshold = roc_curve(Ytest, pred[:, 1])\nfor i, j in enumerate(tpr):\n    plt_lst.append(plot_data(y=tpr[i], x=fpr[i], model=\"knn\", d = \"T1\"))","190cd480":"ranom_model = RandomForestClassifier(criterion='entropy', bootstrap=False)\nmdepth = np.arange(2, 4)\ncri = ['entropy']\nmss = np.arange(2, 4)\nmsl = np.arange(3, 6)\nboot = [False]\nne = np.arange(200, 500, 100)\n\ngrid_s = {'max_depth': mdepth, 'min_samples_split': mss, 'min_samples_leaf': msl, 'n_estimators': ne,\n            'criterion': cri, 'bootstrap': boot}\ncv_rf = GridSearchCV(estimator=ranom_model, param_grid=grid_s, cv=ms.KFold(n_splits=10))\ncv_rf.fit(Xtrain, Ytrain)\n\nranom_model = RandomForestClassifier(**cv_rf.best_params_)\nranom_model.fit(Xtrain, Ytrain)\npred = ranom_model.predict_proba(Xtest)\nprec_results(ranom_model, Xtrain, Ytrain, Xtest, Ytest)\n\nfpr, tpr, threshold = roc_curve(Ytest, pred[:, 1])\n\nfor i, j in enumerate(tpr):\n    plt_lst.append(plot_data(y=tpr[i], x=fpr[i], model=\"random_forest\", d = \"T1\"))","b5cb6702":"h = np.linspace(0,1, 40)\nfor i in h:\n    plt_lst.append(plot_data(y=i, x=i, model=\"\", d = \"T2\"))\n\ndf = pd.DataFrame(plt_lst)\n# print(df.head(5))\nplt.title('ROC Plot Of All Three Models')\nax = sns.lineplot(data=df, x=\"x\", y=\"y\", hue=\"model\",style = \"d\")\nax.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\nax.legend(title = \"Models\", loc='lower right', labels=['logistic', 'knn' , 'random forest', 'random classifier'])\nplt.show()","0cabe4d8":"# adding the out_data\nout_pred = ranom_model.predict(test_x)\ntest[\"Survived\"] = out_pred\nprint(test.head(10))\n\nfinal = test.loc[:,[\"PassengerId\", \"Survived\"]]\nfinal.to_csv(\"submission.csv\", index=False)","3fa30867":"Using the function below to create my roc, confusion and classification report for all 3 candidate models.","5ff4c3f7":"Second candidate model is a Knn Classifer model. ","7b2d1c0d":"First candidate model is a Spare logistic regression model. ","5abc9170":"Second candidate model is Random Forest model. ","a8fc8b4a":"Also all categorical variables like Age, sex and Embarked have been given numerical values. Finally all values have been made the same type.","e1a02550":"The correlation matrix shows some correlation between the predictor variables. Because the existence of muti-collinearity I will employ a spare model for my logistic model. Also, I will use the KNN classifier and ensemble method such as the Random Forest classifier.  \n","46355ac7":"In the training and the testing dataset we have missing data in Fare, Age and Cabin. I am going to remove Cabin, along with Ticket, Name, PassengerId and Survived. My approach for missing data is to use the median for age, fare and Embarked and impute the data base on these values.","1a061915":"Random Forest wins."}}