{"cell_type":{"b6b94265":"code","daeb94c8":"code","6146f711":"code","1344da20":"code","d117cbf1":"code","132368c6":"code","a6630c34":"code","4a346ae3":"code","5bddbe66":"code","4750275c":"code","6105cae9":"code","e3a4152e":"code","a8f75d9f":"code","3ed29c72":"code","5b4640e2":"code","576515fc":"code","53aa0b98":"code","3f2c2cee":"code","7b6445a2":"code","377425f5":"code","085126db":"code","5e02c733":"code","66523a95":"code","600a5116":"code","b4ecfadd":"code","3c0f5bc8":"code","257ee8e0":"code","52abd584":"code","443e8415":"code","a8c4bcf5":"markdown","61e4fde3":"markdown","76e4233f":"markdown","edbbdc88":"markdown","bb0badd5":"markdown"},"source":{"b6b94265":"import re\nimport pandas as pd\n","daeb94c8":"messages=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding=\"ISO-8859-1\")","6146f711":"messages.head()","1344da20":"messages.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)","d117cbf1":"messages.head()","132368c6":"messages.info()","a6630c34":"message = messages['v2']\nlabel = messages['v1']\n\nmessages = pd.DataFrame({'text':message,'label':label})\nmessages","4a346ae3":"import nltk\nimport numpy as np","5bddbe66":"nltk.download('stopwords')","4750275c":"nltk.download('all')","6105cae9":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix , accuracy_score , classification_report\nfrom nltk.stem import WordNetLemmatizer ","e3a4152e":"ps=PorterStemmer()","a8f75d9f":"corpus=[]","3ed29c72":"def process(sentence):\n    text = re.sub('[^\\w]+',' ',sentence).split()\n    words = [x.lower() for x in text if x not in stopwords.words('english')]\n    \n    lemma = WordNetLemmatizer()\n    word = [lemma.lemmatize(word,'v') for word in words ]\n    word = ' '.join(word)\n    return word","5b4640e2":"messages","576515fc":"messages['text'] = messages['text'].apply(process)","53aa0b98":"messages","3f2c2cee":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=5000)\nX=cv.fit_transform(messages['text']).toarray()","7b6445a2":"X.shape","377425f5":"print(X)","085126db":"y = pd.get_dummies(messages['label'])\ny = y.iloc[:,1].values\ny","5e02c733":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","66523a95":"from sklearn.naive_bayes import MultinomialNB\nspam_detection_model=MultinomialNB().fit(X_train,y_train)","600a5116":"y_pred=spam_detection_model.predict(X_test)","b4ecfadd":"from sklearn.metrics import confusion_matrix , accuracy_score , classification_report","3c0f5bc8":"import seaborn as sns","257ee8e0":"sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)","52abd584":"print(f'Accuarcy is : {accuracy_score(y_test,y_pred)}')","443e8415":"print(classification_report(y_test,y_pred))","a8c4bcf5":"Creating Bag of Words Model","61e4fde3":"End of file","76e4233f":"Train test split","edbbdc88":"Training Using the Naive Bayes Theorom","bb0badd5":"Data Cleaning and Preprocessing"}}