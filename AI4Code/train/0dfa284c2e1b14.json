{"cell_type":{"a181a724":"code","58ee9fb9":"code","0b4a2d17":"code","07336f9b":"code","d2021259":"code","55b48e89":"code","e09f8781":"code","37cdf6c4":"code","c6a70d0b":"code","9950a58a":"code","f9e56bd2":"code","815841a0":"code","e6842b76":"code","9ec8647c":"code","e174cc30":"code","7b4a7c43":"code","6b23add4":"code","521d2127":"code","4e7ce72c":"code","d61ba226":"code","fb630a7a":"code","b3ab1504":"code","53b19823":"code","dd76ba45":"code","ef90a79c":"code","58e03aac":"code","a07e6fac":"code","049d4442":"code","06b77e80":"code","e34be3f0":"code","e7cfe98d":"code","a4ecf484":"code","249cc559":"code","9229f2f9":"code","679a72da":"code","a81eece0":"code","f2cc47ac":"code","49d04005":"code","83010f94":"code","cbece33d":"code","2c3f3211":"code","80ae2a32":"code","391ac145":"code","78226ae8":"code","51a3ac63":"code","bf2eebed":"code","91ea5d4e":"code","e92b291d":"code","8fba00ae":"code","77e561d5":"code","573e69c4":"code","c81e16a5":"code","0806b7e5":"code","fcfffd0c":"code","f4309ec1":"code","86116920":"code","6b9cb5d1":"code","fcfbe297":"code","4130d9d9":"code","deb0fc62":"code","0fbde5c9":"code","5e0fe295":"code","6ef946db":"code","e99b17c2":"code","10e5f14f":"code","7bb00a81":"code","0e0de2fc":"code","808d896c":"code","200e5a29":"code","b318ec08":"code","bfadc6a1":"code","add91f7f":"code","3061433f":"code","6a478968":"code","4e772b71":"code","34b3974f":"code","82ad21ae":"code","5a14d719":"code","b5f4612b":"code","c6c76e43":"code","79aa47e3":"code","0bf3ad76":"code","016b27c8":"code","798f94a2":"code","a917963f":"code","e027f8d1":"code","85e1a4da":"code","b04cdeac":"code","e6733407":"code","a5b76a85":"code","2768f1c2":"code","49506138":"code","f6083079":"code","d2aa69c1":"code","2417443f":"code","a87efd57":"code","087584cd":"code","7ae5cc6f":"code","2d33b176":"code","68362ea0":"code","f63eb353":"code","d7885ba5":"code","08260dab":"code","d0944dc1":"code","64c3bfa5":"code","068a6a5c":"code","0c4bb70c":"code","ff379ba2":"code","79f0103d":"code","5886db77":"code","6403c4ab":"code","d75cb6e9":"code","e5b6bb8c":"code","29aa1fba":"code","235ef288":"code","b58d177c":"code","76946f02":"code","5dd99438":"code","edbd1ab1":"code","2de881bd":"code","4cf42463":"code","d4102322":"code","148476ea":"code","c021b558":"code","81adadad":"code","de3bd23b":"code","e6c09a92":"code","db25b84c":"code","3ebebaef":"code","caf91f59":"code","509b6b33":"code","828bd422":"code","4301309a":"code","bfe1b383":"code","59ae6c8c":"code","7ea56153":"code","63fb53ad":"code","719dd2e2":"markdown","b97d81f8":"markdown","511adbdd":"markdown","5cba944d":"markdown","968d2e95":"markdown","4bcb8a09":"markdown","1c1eed08":"markdown","a96f47e3":"markdown","745f16d4":"markdown","55da11db":"markdown","d1577ed3":"markdown","378704f2":"markdown","85a2ada4":"markdown","0e126957":"markdown","fea22c65":"markdown","2eea8235":"markdown","4a74ed68":"markdown","4c8a5471":"markdown","7a1bfcfd":"markdown","8fa6dd52":"markdown"},"source":{"a181a724":"import torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport math\nimport cv2\nfrom scipy import signal","58ee9fb9":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b4a2d17":"image1 = np.array(Image.open('..\/input\/100-bird-species\/train\/AFRICAN FIREFINCH\/001.jpg').convert('L'))  #convert image to black and white\nplt.imshow(image1, cmap='gray')","07336f9b":"np.array(image1).shape","d2021259":"image1_1 = np.array(Image.open('..\/input\/100-bird-species\/train\/AFRICAN FIREFINCH\/002.jpg').convert('L'))  #convert image to black and white\nimage1_1 = cv2.Canny(image1_1,224,224)\nplt.imshow(image1_1, cmap='gray')","55b48e89":"kernel = np.array([[ 0, 1, 0],\n                   [ 1,-4, 1],\n                   [ 0, 1, 0],]) \n\ngrad = signal.convolve2d(image1_1, kernel, mode='same', boundary='symm')\nprint('GRADIENT MAGNITUDE - Feature map')\n\nfig, aux = plt.subplots(figsize=(10, 8))\naux.imshow(np.absolute(grad), cmap='gray');","e09f8781":"df=pd.DataFrame(np.array(image1))\ndf.style.set_properties().background_gradient(\"Greys\")","37cdf6c4":"df=pd.DataFrame(np.array(image1_1))\ndf.style.set_properties().background_gradient(\"Greys\")","c6a70d0b":"PATH=Path('..\/input\/100-bird-species')","9950a58a":"list(PATH.iterdir())","f9e56bd2":"Path.ls = lambda x: list(x.iterdir())","815841a0":"(PATH\/\"train\").ls()","e6842b76":"(PATH\/\"train\/AFRICAN FIREFINCH\").ls()","9ec8647c":"Image.open((PATH\/\"train\/AFRICAN FIREFINCH\").ls()[0])","e174cc30":"img1=torch.tensor(np.array(Image.open('..\/input\/100-bird-species\/train\/AFRICAN FIREFINCH\/001.jpg').convert('L')), dtype = torch.float32)\n","7b4a7c43":"plt.figure(figsize=(7,7))\nplt.imshow(img1);","6b23add4":"albatross=[torch.tensor(np.array(Image.open(img).convert('L')), dtype = torch.float32) for img in (PATH\/\"train\/ALBATROSS\").ls()]\nantbird=[torch.tensor(np.array(Image.open(img).convert('L')), dtype = torch.float32) for img in (PATH\/\"train\/ANTBIRD\").ls()]","521d2127":"albatross_1=[torch.tensor(cv2.Canny(np.array(Image.open(img).convert('L')),224,224), dtype = torch.float32) for img in (PATH\/\"train\/ALBATROSS\").ls()]\nantbird_1=[torch.tensor(cv2.Canny(np.array(Image.open(img).convert('L')),224,224), dtype = torch.float32) for img in (PATH\/\"train\/ANTBIRD\").ls()]","4e7ce72c":"plt.imshow(albatross[1])","d61ba226":"plt.imshow(albatross_1[1])","fb630a7a":"plt.imshow(antbird[0])","b3ab1504":"plt.imshow(antbird_1[0])","53b19823":"albatross_stacked = torch.stack(albatross)\/255","dd76ba45":"albatross_stacked.shape","ef90a79c":"albatross_1_stacked =torch.stack(albatross_1)\/255","58e03aac":"albatross_1_stacked.shape","a07e6fac":"antbird_stacked = torch.stack(antbird)\/255","049d4442":"antbird_stacked.shape","06b77e80":"antbird_1_stacked = torch.stack(antbird_1)\/255","e34be3f0":"antbird_1_stacked.shape","e7cfe98d":"valid_albatross=[torch.tensor(np.array(Image.open(img).convert('L')), dtype = torch.float32) for img in (PATH\/\"valid\/ALBATROSS\").ls()]\nvalid_antbird=[torch.tensor(np.array(Image.open(img).convert('L')), dtype = torch.float32) for img in (PATH\/\"valid\/ANTBIRD\").ls()]","a4ecf484":"avr_albatross= albatross_stacked.mean(0)","249cc559":"avr_antbird= antbird_stacked.mean(0)","9229f2f9":"plt.imshow(avr_albatross, cmap='gray')","679a72da":"plt.imshow(avr_antbird, cmap='gray');","a81eece0":"avr_1_albatross= albatross_1_stacked.mean(0)","f2cc47ac":"avr_1_antbird= antbird_1_stacked.mean(0)","49d04005":"plt.imshow(avr_1_albatross, cmap='gray')","83010f94":"plt.imshow(avr_1_antbird, cmap='gray');","cbece33d":"sample_albatross= albatross_stacked[1]","2c3f3211":"sample_albatross_1 = albatross_1_stacked[1]","80ae2a32":"plt.imshow(sample_albatross, cmap = \"gray\");","391ac145":"dist_to_albatross = ((sample_albatross - avr_albatross)**2).mean().sqrt()","78226ae8":"dist_to_antbird = ((sample_albatross - avr_antbird)**2).mean().sqrt()","51a3ac63":"dist_to_albatross_1 = ((sample_albatross - avr_1_albatross)**2).mean().sqrt()","bf2eebed":"dist_to_antbird_1 = ((sample_albatross - avr_1_antbird)**2).mean().sqrt()","91ea5d4e":"print(dist_to_albatross.item(),dist_to_albatross_1.item())","e92b291d":"print(dist_to_antbird.item(),dist_to_antbird_1.item())","8fba00ae":"dist_to_albatross_1_1 = ((sample_albatross_1 - avr_albatross)**2).mean().sqrt()","77e561d5":"dist_to_albatross_1_2 = ((sample_albatross_1 - avr_1_albatross)**2).mean().sqrt()","573e69c4":"print(dist_to_albatross_1_1.item(),dist_to_albatross_1_2.item())","c81e16a5":"dist_to_antbird_1_1 = ((sample_albatross_1 - avr_antbird)**2).mean().sqrt()","0806b7e5":"dist_to_antbird_1_2 = ((sample_albatross_1 - avr_1_antbird)**2).mean().sqrt()","fcfffd0c":"print(dist_to_antbird_1_1.item(),dist_to_antbird_1_2.item())","f4309ec1":"def distance(a, b):\n    return ((a - b)**2).mean((-1,-2)).sqrt()","86116920":"distance(sample_albatross, avr_albatross)","6b9cb5d1":"valid_dist_albatross= distance(albatross_stacked,avr_albatross)\nvalid_dist_albatross, valid_dist_albatross.shape","fcfbe297":"def is_albatross(x):\n    return distance(x, avr_albatross) < distance(x, avr_antbird)","4130d9d9":"distance(sample_albatross,avr_albatross)","deb0fc62":"distance(sample_albatross,avr_antbird)","0fbde5c9":"is_albatross(sample_albatross)","5e0fe295":"type(sample_albatross)","6ef946db":"for i in range(len(valid_albatross)):\n    print(is_albatross(valid_albatross[i]))","e99b17c2":"for i in range(len(valid_antbird)):\n    print(is_albatross(valid_antbird[i]))","10e5f14f":"distance(valid_antbird[4],avr_albatross)","7bb00a81":"distance(valid_antbird[4],avr_antbird)","0e0de2fc":"arr1=[]\nfor i in range(len(valid_albatross)):\n    arr1.append(is_albatross(valid_albatross[i].mean()*0.01\/2))  \narr1    ","808d896c":"arr2=[]\nfor i in range(len(valid_antbird)):\n    arr2.append(is_albatross(valid_antbird[i].mean()*0.01\/2))\narr2   ","200e5a29":"avr_albatross.mean()","b318ec08":"avr_antbird.mean()","bfadc6a1":"valid_antbird[1].mean()*0.01\/2","add91f7f":"print(valid_albatross[0].mean())\nprint((valid_albatross[0].mean() * 0.01 \/ 2))","3061433f":"def accuracy_1(a,b):\n    i=0\n    true_num=0\n    false_num=0\n    first_loop=0\n    second_loop=0\n    \n    a= np.array(a)\n    for i in range(len(a)):\n        if a[i] == True:\n            true_num = true_num + 1\n    first_loop= true_num \/ len(a)\n    \n    \n    b= np.array(b)\n    for i in range(len(b)):\n        if b[i]==False:\n            false_num = false_num + 1\n    second_loop= false_num \/ len(b)\n    \n    \n    res = (first_loop+second_loop)\/2\n    return res\n    ","6a478968":"baseline1_accuracy = accuracy_1(arr1,arr2)\nbaseline1_accuracy","4e772b71":"def is_albatross_1(x):\n    return distance(x, avr_1_albatross) < distance(x, avr_1_antbird)","34b3974f":"avr_1_albatross.mean()","82ad21ae":"avr_1_antbird.mean()","5a14d719":"valid_albatross[i].mean()*0.01\/2","b5f4612b":"arr11=[]\nfor i in range(len(valid_albatross)):\n    arr11.append(is_albatross(valid_albatross[i]*0.01\/2))  \narr11  ","c6c76e43":"arr12=[]\nfor i in range(len(valid_albatross)):\n    arr12.append(is_albatross(valid_antbird[i]*0.01\/2))  \narr12  ","79aa47e3":"baseline2_accuracy = accuracy_1(arr11,arr12)\nbaseline2_accuracy","0bf3ad76":"x = torch.tensor(2.).requires_grad_()","016b27c8":"x","798f94a2":"def f(x):\n    return x**2","a917963f":"grad = f(x)\ngrad","e027f8d1":"x.grad","85e1a4da":"labels = {1:\"Albatross\", 0:\"Antbird\"}","b04cdeac":"train_x = torch.cat([albatross_stacked, antbird_stacked]).view(-1, 224*224)","e6733407":"train_y = torch.tensor([1] * len(albatross) + [0] * len(antbird))","a5b76a85":"train_x","2768f1c2":"train_y","49506138":"train_x.shape, train_y.shape","f6083079":"train_y.unsqueeze_(-1)","d2aa69c1":"train_y.shape","2417443f":"valid_albatross_stacked = torch.stack(valid_albatross)","a87efd57":"valid_antbird_stacked = torch.stack(valid_antbird)","087584cd":"valid_albatross_stacked.shape","7ae5cc6f":"train_x.shape","2d33b176":"valid_x = torch.cat([valid_albatross_stacked, valid_antbird_stacked]).view(-1, 224*224)","68362ea0":"valid_x.shape","f63eb353":"valid_y = torch.tensor([1] * len(valid_albatross_stacked) + [0] * len(valid_antbird_stacked))  \n\n# 1 = Albatross \n# 0 = Antbird","d7885ba5":"valid_y.shape","08260dab":"valid_y.unsqueeze(0)","d0944dc1":"valid_y.unsqueeze(0).shape","64c3bfa5":"valid_y.unsqueeze(1)","068a6a5c":"valid_y.unsqueeze(1).shape","0c4bb70c":"valid_y = valid_y.unsqueeze(1)","ff379ba2":"ds_train = list(zip(train_x, train_y))\nds_valid = list(zip(valid_x, valid_y))","79f0103d":"ds_train[0]","5886db77":"plt.imshow(ds_train[0][0].view(224,224), cmap=\"gray\");","6403c4ab":"torch.randn(5)","d75cb6e9":"torch.randn(5,2)","e5b6bb8c":"def init(size):\n    return torch.randn(size, dtype=torch.float32).requires_grad_()","29aa1fba":"w = init((224*224,1))","235ef288":"w.shape","b58d177c":"b = init(1)","76946f02":"train_x[0].shape","5dd99438":"w.shape","edbd1ab1":"(train_x[0] * w.T).sum() + b","2de881bd":"def linear_layer(xb):\n    return xb @ w + b","4cf42463":"preds = linear_layer(train_x)","d4102322":"preds.shape","148476ea":"def accuracy(preds, actuals):\n    return ((preds > 0.0).float() == actuals).float().mean().item()","c021b558":"accuracy(preds, train_y)","81adadad":"w[0] = w[0] * 1.0001","de3bd23b":"preds = linear_layer(train_x)","e6c09a92":"accuracy(preds, train_y)","db25b84c":"def loss(preds, targets):\n    return torch.where(targets==1, 1-preds, preds).mean()","3ebebaef":"def sigmoid(x):\n    return 1\/(1+torch.exp(-x))","caf91f59":"def loss_func(preds, targets):\n    preds = preds.sigmoid()\n    return torch.where(targets==1, 1-preds, preds).mean()","509b6b33":"def f(x):\n    for i in range(x):\n        yield i","828bd422":"t1=f(5)","4301309a":"next(t1)","bfe1b383":"w = init((224*224,1))","59ae6c8c":"b = init(1)","7ea56153":"class DataLoader():\n    def __init__(self, ds, bs): \n        self.ds, self.bs = ds, bs\n    def __iter__(self):\n        n = len(self.ds)\n        l = torch.randperm(n)\n\n        \n        for i in range(0, n, self.bs): \n            idxs_l = l[i:i+self.bs]\n            yield self.ds[idxs_l]","63fb53ad":"train_dl = DataLoader(ds_train, bs = 20)","719dd2e2":"# Different Model - Assigning Weights\n","b97d81f8":"Looking at the images in our lists","511adbdd":"# Creating images list for Albatros and Antbird","5cba944d":"# Creating list of images from folders","968d2e95":"Which one is more close","4bcb8a09":"-----------------","1c1eed08":"*Baseline Accuracy ","a96f47e3":"Valid","745f16d4":"# **To see it as an array of number:**","55da11db":"# Mean","d1577ed3":"# Making prediction with our untrained model ","378704f2":"# Writing Training Loop","85a2ada4":"*I understood that there are so many noise in the pictures. First of all i have to do edge detection for it. Probably it will be better for my model. I will try both of them *","0e126957":"---------------------------------------------------------------------","fea22c65":"# Find Generic Function ","2eea8235":"**Function that maps any values to (0,1)**","4a74ed68":"# Initializing Parameters","4c8a5471":"# Stack","7a1bfcfd":"# Creating Dataset","8fa6dd52":"# Creating training and validation set"}}