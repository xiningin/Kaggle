{"cell_type":{"18c248cb":"code","a62f75e8":"code","060128f9":"code","ec692304":"code","1654da56":"code","56bae88b":"code","2e2369d0":"code","2b951b62":"code","b9f794ac":"code","8798a7db":"code","f45fe5ed":"code","dcc53ebf":"code","84bffef6":"code","4cf53e32":"code","294cf47f":"code","98143568":"code","37f7dfc1":"code","0907c439":"code","94e53432":"code","3fc48460":"code","b673a445":"code","ba7f1713":"code","e58f02ca":"code","19d20f73":"code","931db5da":"code","f569ce82":"code","abcc221a":"code","7c707052":"code","e78c31ef":"code","e857c214":"code","07290572":"code","0f5b9702":"code","435d7117":"code","992a94cd":"code","080f2436":"code","2b755424":"code","a703eda4":"code","4f9ea72e":"code","be2eb6a9":"code","547914dc":"code","b745fa66":"code","84d6c525":"code","d740b245":"code","879a9b31":"code","795fbae3":"code","c3a61318":"code","3a4f6c89":"code","5bdcd252":"code","6db35581":"code","b1323705":"code","5e695bf5":"code","5acdcb44":"code","12191149":"code","c9baa405":"code","a3447482":"code","fd4fd8d3":"code","3baf7428":"code","2e56c1da":"code","d4cab3ad":"code","e8a80615":"markdown"},"source":{"18c248cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport gensim\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.callbacks import EarlyStopping\n\nfrom keras.optimizers import Adam\n\nimport keras.models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU, BatchNormalization, GlobalMaxPooling1D, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a62f75e8":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","060128f9":"!ls ..\/input","ec692304":"df = pd.read_csv('..\/input\/imdb-reviews-dataset\/imdb_reviews_dataset.csv')","1654da56":"model_weights = '..\/input\/model-imdb\/model.h5'","56bae88b":"df['type_review'] = df['id'].apply(lambda x: x.split('_')[0])","2e2369d0":"df['type_review'].unique()","2b951b62":"df_labeled = df[df['type_review'] != 'unsup'].copy() ","b9f794ac":"df_labeled['sentiment'] = df_labeled['type_review'].apply(lambda x: 1 if x == 'pos' else 0)","8798a7db":"df_labeled['len_text'] = df_labeled['text'].apply(lambda x: len(x.split()))\ndf_labeled","f45fe5ed":"len_text_info = df_labeled['len_text'].describe()\nlen_text_info","dcc53ebf":"# set max len for padding\nmax_length = int(len_text_info['mean'] + 2 * len_text_info['std'])\nprint(max_length) # = 200","84bffef6":"VALIDATION_SPLIT = 0.2\n\nindices = np.arange(df_labeled.shape[0])\nnp.random.shuffle(indices)\nreview = df_labeled.iloc[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * review.shape[0])\n\nX_train = review.iloc[:-num_validation_samples]['text'].values\ny_train = review.iloc[:-num_validation_samples]['sentiment'].values\nX_test = review.iloc[-num_validation_samples:]['text'].values\ny_test = review.iloc[-num_validation_samples:]['sentiment'].values","4cf53e32":"print(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))","294cf47f":"def split_long_texts(text_tokens, labels):\n    _text_tokens = []\n    _labels = []\n    for text, label in zip(text_tokens, labels):\n        text_size = len(text) \/\/ max_length\n        text_res = len(text) % max_length\n\n        for i in range(text_size):\n            _text_tokens.append(text[i: i + max_length])\n            _labels.append(label)\n\n        if text_res > 0.5 * max_length or text_size == 0:\n            _text_tokens.append(text[text_size * max_length:])\n            _labels.append(label)\n    return _text_tokens, np.array(_labels)","98143568":"tokenizer_obj = Tokenizer()\ntotal_reviews = np.hstack((X_train, X_test))\ntokenizer_obj.fit_on_texts(total_reviews)\n\nvocab_size = len(tokenizer_obj.word_index) + 1\n\nX_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\nX_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n\nX_train_tokens, y_train = split_long_texts(X_train_tokens, y_train)\nX_test_tokens, y_test = split_long_texts(X_test_tokens, y_test)\n\nX_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')","37f7dfc1":"X_train_pad.shape","0907c439":"vocab_size","94e53432":"#vocab_size = 124253","3fc48460":"max_length","b673a445":"EMBEDDING_DIM = 2\n\nprint('Build model...')\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n#model.add(GRU(units=1, return_sequences=True)) #, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(LSTM(2, dropout=0.1, recurrent_dropout=0.1))\n#Global Maxpooling\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(lr=0.0005)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","ba7f1713":"model.summary()","e58f02ca":"print('Train...')\n\nmy_callbacks = [\n    EarlyStopping(patience=10),\n]\n\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=5, validation_data=(X_test_pad, y_test), verbose=2, callbacks=my_callbacks)","19d20f73":"model.save('model.h5')","931db5da":"model.save_weights('model_weights.h5')","f569ce82":"model_weights","abcc221a":"os.listdir('..\/input\/learning-word-vectors-for-sentiment-analysis\/')","7c707052":"model.load_weights(model_weights)","e78c31ef":"import pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer_obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)","e857c214":"model = keras.models.load_model('..\/working\/model.h5')","07290572":"test_sample_1 = 'This movie is fantasric! I realy like it because it is so good!'\ntest_sample_2 = 'Good movie!'\ntest_sample_3 = 'Bad movie!'\ntest_samples = [test_sample_1, test_sample_2, test_sample_3]\n\ntest_samples_tokens = tokenizer.texts_to_sequences(test_samples)\ntest_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n\nmodel.predict(x=test_samples_tokens_pad)","0f5b9702":"from scipy.stats import norm\n\n# https:\/\/www.quora.com\/Do-the-user-ratings-on-IMDB-follow-a-bell-curve-If-so-what-is-the-mean-and-standard-deviation\nmean = 6.2\nstd = 1.4 \n\nnorm.pdf(10, loc=mean, scale=std)","435d7117":"rate_dict = {i: 0 for i in range(1, 11)}\nprev_prob = 0\nfor i, rate in enumerate([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10], 1):\n    curr_prob = norm.cdf(rate, loc=mean, scale=std)\n    rate_dict[i] = [prev_prob, curr_prob]\n    prev_prob = curr_prob\nrate_dict[10] = [rate_dict[10][0], 1]\nprint(rate_dict)","992a94cd":"def get_score(p, rate_dict):\n    for key, interval in rate_dict.items():\n        if interval[0] < p < interval[1]:\n            return key","080f2436":"for p in model.predict(x=test_samples_tokens_pad).reshape(-1).tolist():\n    print(p, get_score(p, rate_dict))","2b755424":"prediction = model.predict(x=X_test_pad)","a703eda4":"roc_auc_score(y_test, prediction.reshape(-1))","4f9ea72e":"%%time\n\nreview_lines = list()\nlines = df['text'].values.tolist()\n\nstop_words = set(stopwords.words('english'))\n\nfor line in lines:\n    tokens = word_tokenize(line)\n    tokens = [w.lower() for w in tokens]\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [word for word in stripped if word.isalpha()]\n    words = [w for w in words if not w in stop_words]\n    review_lines.append(words)","be2eb6a9":"len(review_lines)","547914dc":"%%time\nEMBEDDING_DIM = 256\n\nmodel = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=5)\nwords = list(model.wv.vocab)\nprint('Vocabulary size: %d' % len(words))","b745fa66":"## Vocabulary size: 28115","84d6c525":"model.wv.most_similar('horrible')","d740b245":"filename = 'imdb_embedding_word2vec.txt'\nmodel.wv.save_word2vec_format(filename, binary=False)","879a9b31":"embedding_index = {}\nwith open(os.path.join('.\/imdb_embedding_word2vec.txt')) as fin:\n    for line in fin:\n        values = line.split()\n        if len(values) == 2:\n            print('Num words - ', values[0])\n            print('EMBEDDING_DIM =', values[1])\n            continue\n        word = values[0]\n        coefs = np.asarray(values[1:])\n        embedding_index[word] = coefs","795fbae3":"len(embedding_index.keys())","c3a61318":"tokenizer_obj = Tokenizer()\ntotal_reviews = df_labeled['text'].values\ntokenizer_obj.fit_on_texts(total_reviews)\nsequences = tokenizer_obj.texts_to_sequences(total_reviews)\n\nword_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nreview_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\nsentiment = df_labeled['sentiment'].values\nprint(review_pad.shape)\nprint(sentiment.shape)","3a4f6c89":"word_index","5bdcd252":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nwords_n = 0\nfinde_n = 0\nfor word, i in word_index.items():\n    words_n += 1\n    if i > num_words:\n        continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        finde_n += 1\n        embedding_matrix[i] = embedding_vector","6db35581":"rev_model = Sequential()\nembedding_layer = Embedding(\n    num_words,\n    EMBEDDING_DIM,\n    embeddings_initializer=Constant(embedding_matrix),\n    input_length=max_length,\n    trainable=False,\n)\n\nrev_model.add(embedding_layer)\nrev_model.add(BatchNormalization())\nrev_model.add(Dropout(0.2))\n#rev_model.add(LSTM(64)) #, return_sequences=True)) # , dropout=0.2, recurrent_dropout=0.2))\nrev_model.add(GRU(units=32))#, dropout=0.1, recurrent_dropout=0.1))\nrev_model.add(BatchNormalization())\nrev_model.add(Dropout(0.2))\nrev_model.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(lr=0.0005)\nrev_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b1323705":"rev_model.summary()","5e695bf5":"VALIDATION_SPLIT = 0.2\n\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\nreview_pad = review_pad[indices]\nsentiment = sentiment[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n\nX_train_pad = review_pad[:-num_validation_samples]\ny_train = sentiment[:-num_validation_samples]\nX_test_pad = review_pad[-num_validation_samples:]\ny_test = sentiment[-num_validation_samples:]","5acdcb44":"print(X_train_pad.shape)\nprint(y_train.shape)\nprint(X_test_pad.shape)\nprint(y_test.shape)","12191149":"print('Train...')\n\nmy_callbacks = [\n    EarlyStopping(patience=10),\n]\nrev_model.fit(X_train_pad, y_train, batch_size=128, epochs=100, validation_data=(X_test_pad, y_test), verbose=2, callbacks=my_callbacks)","c9baa405":"rev_model.evaluate(X_test_pad, y_test)","a3447482":"rev_model.evaluate(X_train_pad, y_train)","fd4fd8d3":"rev_model.save('rev_model.h5')","3baf7428":"rev_model.save_weights('rev_model_weights.h5')","2e56c1da":"prediction = rev_model.predict(x=X_test_pad)","d4cab3ad":"roc_auc_score(y_test, prediction.reshape(-1))","e8a80615":"# word2vec"}}