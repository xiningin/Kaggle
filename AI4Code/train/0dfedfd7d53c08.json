{"cell_type":{"d6705715":"code","710159b9":"code","6670426a":"code","97ce2e62":"code","7f0f39f6":"code","f5d0b908":"code","71d1bbf5":"code","962653cf":"code","28599fcf":"code","54925589":"code","ac8aa373":"code","0fae6e1c":"code","e7a66aa1":"code","18dbb5c1":"code","cfa77779":"code","e5f50fb6":"code","4d38ef17":"code","780f1440":"code","964598bd":"code","37f97a14":"code","3f60ef34":"code","e85a749e":"markdown","826183e0":"markdown","a88d6e68":"markdown","86b13762":"markdown","e6b2fec2":"markdown","4012d0f2":"markdown","d096216a":"markdown","a348c0ba":"markdown","dd284902":"markdown","315eef97":"markdown","0e0bc9c4":"markdown","6dab1c6a":"markdown","5242376f":"markdown","9af92053":"markdown","924cd5dd":"markdown","6ec7665b":"markdown"},"source":{"d6705715":"#import all essential libraries\nimport sys \nimport numpy as np # linear algebra\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \nfrom sklearn.metrics import mean_squared_error,r2_score\n## Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","710159b9":" data = pd.read_csv('..\/input\/household_power_consumption.txt', sep=';', \n                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='dt')","6670426a":"data.head()","97ce2e62":"data.describe()","7f0f39f6":"## finding all columns that have nan:\ndata.columns[data.isnull().any()]","f5d0b908":"#To fill in missing values, first check for linearity in each column using autocorelation plots\ndef get_acf(data,lags): \n    frame = []\n    for i in range(lags+1):\n        frame.append(data.apply(lambda col: col.autocorr(i), axis=0))\n    return pd.DataFrame(frame).plot.line()\nget_acf(data,10)","71d1bbf5":"#fill in missing values using linear interpolation with limit to 10 using autocorelation plot\nclean_data = data.interpolate(method = 'linear', axis = 0, limit = 10)\n","962653cf":"#Its imperative from autocorealtion plots that we dont need each minute data, perhaps we can decide roll up the aggregate\ni = 1\n# plot each column\nplt.figure(figsize=(20, 15))\nfor counter in range(1,len(clean_data.columns)):\n    plt.subplot(len(clean_data.columns), 1, i)\n    plt.plot(clean_data.resample('D').mean().values[:, counter], color = 'blue')\n    plt.title(clean_data.columns[counter], y=0.8, loc='right')\n    i = i+1\nplt.show()","28599fcf":"#check corelation matrices for minute, hour and day\n#minute\ncorr_min = clean_data.corr()\n#hour\ncorr_hour = pd.DataFrame(clean_data.resample('H').mean().values).corr()\n#day\ncorr_day = pd.DataFrame(clean_data.resample('D').mean().values).corr()\ncmap=sns.diverging_palette(5, 250, as_cmap=True)\n\ndef magnify():\n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])\n]\n#Plot Minute\ncorr_min.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n    .set_caption(\"Hover to magify\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","54925589":"corr_hour.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n    .set_caption(\"Hover to magify\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","ac8aa373":"corr_day.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n    .set_caption(\"Hover to magify\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","0fae6e1c":"#drop highly corelated columns \n\nclean_data = clean_data.drop(columns = ['Global_intensity'])","e7a66aa1":"#Credit: Adopted from https:\/\/machinelearningmastery.com\/convert-time-series-supervised-learning-problem-python\/\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","18dbb5c1":"#framing the problem\n#resample over day\n\n## resampling of data over hour\nresampled_data = clean_data.resample('H').mean() \nresampled_data.shape","cfa77779":"#normalised_data=(resampled_data-resampled_data.min())\/(resampled_data.max()-resampled_data.min())\n#print(normalised_data.head())","e5f50fb6":"# Feature engineering for LSTM\nreframed_data = series_to_supervised(resampled_data, 1, 1)\nprint(reframed_data.head())","4d38ef17":"# drop columns we don't want\nreframed_data.drop(reframed_data.columns[[7,8,9,10,11]], axis=1, inplace=True)\nprint(reframed_data.columns)","780f1440":"# split into train and test sets\nvalues = reframed_data.values\ntrain_index = 500*24 #The logic is to have 500 days worth of training data. this could also be a hyperparameter that can be tuned.\ntrain = values[:train_index, :]\ntest = values[train_index:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","964598bd":"model = Sequential()\nmodel.add(LSTM(100, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(50))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=10, batch_size=20, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","37f97a14":"# make a prediction\nyhat = model.predict(test_X, verbose=0)\nrmse = np.sqrt(mean_squared_error(test_y, yhat))\nprint('Test RMSE: %.3f' % rmse)","3f60ef34":"## time steps, every step is one hour (you can easily convert the time step to the actual time index)\n## for a demonstration purpose, I only compare the predictions in 200 hours. \n\naa=[x for x in range(200)]\nplt.plot(aa, test_y[:200], marker='.', label=\"actual\")\nplt.plot(aa, yhat[:200], 'r', label=\"prediction\")\nplt.ylabel('Global_active_power', size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=13)\nplt.show()","e85a749e":"Read the txt file. Raw Data has the datetime field split into two different columns so we will merge it into single column  ","826183e0":"By Hour","a88d6e68":"This problem is mapped as many to one with stacked LSTMs with dropouts to control fitting. Loss is measured in MSE and optimised using adam optimizer.\nHyperparameter tuning: Technically I should be doing a grid\/bayesian\/random search on parameters like number of nuerons, dropouts, etc and get the parameters with least MSE. Beyond scope of this notebook.","86b13762":"Seems like acfs start dropping after 10 lags for majority of signals. So lets do linear interpolation but with a limit of 10 ","e6b2fec2":"Now comes the problem formulation.  Lets assume the task is to predict global active power at current hour given previous readings. We have to convert this to a supervised problem. \nNumber of lags can be a hyperparameter for the model. it specifies number of time steps to go back for feature engineering. Its easier to understand by playing with different lags in below function.","4012d0f2":"**1. Importing libraries and reading data**","d096216a":"Plot Actual vs Predicted","a348c0ba":"Now, the data is aggregated every minute. Do we need that? Maybe not, we can do corelation plots of different aggregations to see how data change. But first lets do quick charting per day.","dd284902":"**2. Data Preprocessing **\n\nDo we have any missing values? Do we need all the columns?","315eef97":"By Day","0e0bc9c4":"Grouping by day, minute, hour is really a business problem. If the business needs a strategy for short term forecasting then by hour seems a good option. Lets assume this is the case. \nCorelation plot shows a very high corelation between global active power and global intensity for all 3 aggregations. It is safe to drop itl ","6dab1c6a":"Looks reasonable? But how do the corelation changes when we roll up the aggregation? Lets check it out","5242376f":"Its important to clean the data and treat missing values. But filling in  with mean or zeros can skew the model a lot espeically for fields with high variance. \nSo First step before filling in the missing values is to check linearity and variation in data. This problem is more tricky than it sounds as ideally for any time series, \nwe would like to decompose it to see the trend and seasonality. But this tutorial is limited to providing a general workflow for LSTM based forecasting.\n\nBelow, we get a plot of how autocorelation of different signals change with lag to better model the imputation","9af92053":"Now before moving forward, I would like to point out that you could potentially normalise the data in the data preprocessign phase and later decode it, i have left it out intentionally here as it made no difference in results.","924cd5dd":"This is a tutorial on **Time series forecasting using multi-layer LSTM.** \nData: Household electric power consumption \nThe description of data can be found here: http:\/\/archive.ics.uci.edu\/ml\/datasets\/Individual+household+electric+power+consumption   \nTask: Given some parameters in time steps (t-1, t-2 ...t-x), can we predict Global_active_power at the current time t.\n***More to come***","6ec7665b":"**3. Problem formulation and training LSTM**"}}