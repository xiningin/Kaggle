{"cell_type":{"54de9ebf":"code","3d2a3d07":"code","ab5a02bc":"code","3b1dc628":"code","4ba76ad4":"code","5a9019f8":"code","c7c7591c":"code","c986c2db":"code","b75455ae":"code","1ff43cd5":"code","d2e3454f":"code","57125685":"code","4a0dd3ba":"code","32be2b58":"code","53eedfc2":"code","bfd2122f":"code","03e36c34":"code","fc7abf6e":"code","51de2ed9":"code","376cdcda":"code","358001f5":"code","7c3f81f1":"code","79db40bf":"code","7d0085d1":"code","6b6fb89e":"code","8af4df6c":"code","ea2577bf":"code","01511f45":"code","035d1f9c":"code","ede194b6":"code","2754363a":"code","3527a04d":"code","cfe51154":"code","f000effa":"code","23b8912b":"code","86e926bf":"code","cee9d596":"code","191a8909":"code","61374340":"code","6db17ab3":"markdown","64ec4c36":"markdown","b8367964":"markdown","4ccb4365":"markdown","df742a32":"markdown","0b6a9ece":"markdown","90a19fcc":"markdown","38d2a5fd":"markdown","dc8a62d3":"markdown","ce8041e4":"markdown","ac57c51f":"markdown","ad421f72":"markdown","9198a673":"markdown","a752fb3d":"markdown","f4ca7c6e":"markdown","a3f795c4":"markdown"},"source":{"54de9ebf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3d2a3d07":"# following https:\/\/www.datacamp.com\/community\/tutorials\/kaggle-machine-learning-eda\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\n\n%matplotlib inline\nsns.set()\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(df_train.info())\nprint(df_train.shape)\ndf_train.head()","ab5a02bc":"# Distribution of Survival \ndf_train['Survived'].value_counts()\n","3b1dc628":"df_test.head()\n\nfrom random import * \ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nsur_list =[]\nfor i in range(418):\n    sur_list.append(randint(0,1))\n\ndf_test['Survived'] = sur_list\n\nprint(df_test['Survived'].value_counts())\n\ndf_test[['PassengerId','Survived']].to_csv('random_survivors.csv', index =False)","4ba76ad4":"# Distribution of Survival \ndf_train['Survived'].value_counts()\n\n# 0- (deceased): 549, 1 - Alive : 342\n\ndf_test.head()\n# test doesn't contain the survived class.\n\n# sns barplot of survival - target variable\n\nsns.countplot(x = 'Survived', data = df_train)\n\n# In training set, more people died than those that survived.\n\n# Base model that predicts that everyone died.\n\ndf_test['Survived'] = 0\n\n# save PassengerId and Survived columns of dataset to a csv and submit to kaggle.\n\ndf_test[['PassengerId','Survived']].to_csv('no_survivors_baseline.csv', index = False)\n\n# submitting this to Kaggle gives an accuracy of 62.7%, clearly dataset is not balanced but good for comparison.\n\n# barplot of the feature sex\n\nsns.countplot(x = 'Sex', data = df_train)\n\n#build bar plots of the Titanic dataset feature 'Survived' split (faceted) over the feature 'Sex'.\n\nsns.catplot(x ='Survived', col = 'Sex', kind = 'count', data = df_train)\n# Most female passengers survived","5a9019f8":"# Insight 1 : Women were more likely to survive than men.\n\n# check how many women and men survived\n\nprint(df_train.groupby(['Sex']).Survived.sum())\n\n# Check number of people that survived per class.\nprint(df_train.groupby(['Pclass']).Survived.sum())\n# Check the percentage of people that survived from a particular class\n\nprint(df_train.groupby(['Pclass']).Survived.sum()\/df_train.groupby(['Pclass']).Survived.count())\n\nprint('Proportion of total women and men that survived.')\n\nprint(df_train[df_train.Sex == 'female'].Survived.sum()\/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()\/df_train[df_train.Sex == 'male'].Survived.count())\n\n# Insight 2 : 74% of the women survived as opposed to 18% men.\n\n# 2nd Model where all women survive and all men die\n\nprint('2nd Model where all women survive and all men die')\ndf_test['Survived'] = df_test.Sex == 'female'\n# .apply allows users to pass a function and apply it to entire pandas series.\n# Boolean to int conversion\ndf_test['Survived'] = df_test.Survived.apply(lambda x: int(x))  # converts True - 1 , False - 0\ndf_test.head()\ndf_test[['PassengerId','Survived']].to_csv('only_women_survivors_baseline.csv', index = False)\n\n# Submitting this gives accuracy of 76.6%. Decent improvement based just on feature engineering.","c7c7591c":"## Another model idea, all Pclass = 1 survived and all women survived.\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#df_test['Survived'] = df_test.Sex =='female'\n#df_test['Survived'] = df_test.Pclass ==1\n# This wouldn't work as the second condition will overwrite the first \n\n# correct method \n\ndf_test['Survived'] = (df_test.Sex=='female') | (df_test.Pclass ==1 )\n\n# convert boolean to integer \n\ndf_test['Survived'] = df_test.Survived.apply(lambda x: int(x))\n#df_test.loc[df_test['Pclass']==1]\n\n#df_test['Survived'].value_counts()\n\ndf_test[['PassengerId','Survived']].to_csv('only_women_and_1st_class_survive.csv', index = False)\ndf_test.head(20)","c986c2db":"# Bar plots of the dataset feature 'Survived' split over the feature Pclass\n\nsns.catplot(x = 'Survived', col ='Pclass', kind = 'count', data = df_train)\n\n# Insight 2 : Passengers in class 1 were more likely to survive, and passengers from class \n# 3 were very unlikely to survive.\n","b75455ae":"sns.catplot(x = 'Survived', col ='Embarked', kind = 'count', data = df_train)\n\n# Insight 3 : Passengers that embarked in Southampton were less likely to survive, seems like thats \n# not a cause but a correlation, people from there were poorer\/lower  calss ticket\/ generally male? ","1ff43cd5":"# Plotting fare that people paid for the Titanic ticket.\nsns.distplot(df_train.Fare, kde =False)\n\n# Most people paid around 50 units of currency or less\nplt.figure(figsize=(15,15))\n\n# using pandas to plot Fare for each value of survived on plot\ndf_train.groupby('Survived').Fare.hist(alpha =0.6)\n\n# Insight 3 : People that paid more generally had a higher rate of survival.","d2e3454f":"# Distribution of ages of people on the Titanic\n\n# sns.distplot(df_train.Age, kde =False)\n\n# has null values, need to be dropped before plotted\n\ndf_train.dropna(inplace =True)\nsns.distplot(df_train.Age, kde =False)","57125685":"# Plotting Strip and Swarm plot of Fare and Survived on the x-axis\nplt.figure(figsize=(15,15))\nsns.stripplot(x='Survived', y = 'Fare', data = df_train, alpha =0.3, jitter =True)","4a0dd3ba":"plt.figure(figsize=(15,15))\n\nsns.swarmplot(x = 'Survived', y ='Fare', data =df_train)\n\n# Insight : Fare is correlated with Survival chances. ","32be2b58":"# see stats of fare as a function of survival\n\ndf_train.groupby('Survived').Fare.describe()\n\n# The people that survived paid a mean fare of 85 units, 20 higher than those who died.","53eedfc2":"# seaborn plot of Age against Fare colored by Survival\nplt.figure(figsize=(15,15))\nsns.lmplot(x = 'Age', y ='Fare', hue = 'Survived', data =df_train, fit_reg=False, scatter_kws={'alpha':0.5})\n\n# Insight : You generally had to be on the younger side or richer side to have survived \n# the Titanic disaster.","bfd2122f":"# Make a pairplot of df_train colored by Survival \n# Gives plot with every other numeric feature ?\n# plt.figure(figsize = (15,15))\nsns.pairplot(df_train, hue = 'Survived')","03e36c34":"## ## ## Some Other EDA Tutorial ## ## ##\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store target variable of training data in a safe place\nsurvived_train = df_train['Survived']\n\n# Concatenate training and test sets after dropping survived from df_train, otherwise number of column mismatch\ndata = pd.concat([df_train.drop(['Survived'], axis = 1), df_test])\n\ndata.info()\ndata.head()\n","fc7abf6e":"# Missing numerical values in the two columns Age, Fare\n\n# Method 1 : Impute missing values using Median of the data\n\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\ndata['Embarked'] = data.Embarked.fillna('S')\ndata.info()","51de2ed9":"import re\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression  # Not Used \nfrom sklearn.model_selection import GridSearchCV\n\n# Inline figures and set visual style\nimport seaborn as sns \n%matplotlib inline \nsns.set()\n# From EDA whether a passenger 'Survived' or not was dependednt on features 'Fare', but also 'Age' and 'Sex'\n\n# Category Sex needs to be encoded to numbers.\n# use pandas get_dummies function to do so.\n\ndata = pd.get_dummies(data, columns = ['Sex'], drop_first = True)\n","376cdcda":"data.head()\n# get dummies created two new columns sex_male and sex_female, drop_first dropped a column as they contain \n# the same info. So, Sex_male = 1 for male, = 0 for female\n\n# select relevant columns \ndata = data[['Sex_male','Fare','Age', 'Pclass', 'SibSp']]\n\ndata.info()\ndata.head()\n","358001f5":"# Split dataset into Train and test \n\ndata_train = data.iloc[0:891]\n# number of samples have to match number of labels saved earlier \ndata_test = data.iloc[891:]\n\n# Scikit learn requires data as arrays.\n\nx = data_train.values\ntest = data_test.values\ny = survived_train.values\n\n# Train or fit Decision tree on data\nclf = tree.DecisionTreeClassifier(max_depth = 6)\nclf.fit(x,y)\n\n# getting and saving predictions \ny_pred = clf.predict(test)\n\n# saving dataframe with results\ndf_test['Survived'] = y_pred\n\n# Saves \ndf_test[['PassengerId', 'Survived']].to_csv('decision_tree.csv', index=False)\n\n#tree.plot_tree(clf.fit(x,y)) \n\nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None,\n                                feature_names = data_train.columns,\n                                class_names = ['Survived','Dead'],\n                               filled=True, rounded=True,  \n                                special_characters=True) \ngraph = graphviz.Source(dot_data) \ngraph","7c3f81f1":"print(data_train.head())","79db40bf":"# Choosing the correct hyperparameter.\n# max_depth in this case, too small a value is underfitting data. Too large is overfitting.\n# Related to bias variance tradeoff.\n\n# split dataset into train and test\n\nX_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size =0.33, random_state = 42, stratify = y)\n\n# Iterating over max_depth from 1 to n and plotting the accuracy of the model on the test set\n\n# storing train and test accuracies \n\ndep = np.arange(1,9)\n\ntrain_accuracy = np.empty(len(dep))    # Members not zero ? floats ?\ntest_accuracy = np.empty(len(dep))\n\n# Loop over different values of k\n\n#enumerate([1,2,3]) - methods adds a counter to an iterable and returns in form of an enumerate object.\n\nfor i,k in enumerate(dep):\n    # Define decision tree\n    clf = tree.DecisionTreeClassifier(max_depth = k)\n    # Fit the classifier to training data.\n    clf.fit(X_train,Y_train)\n    \n    # Compute accuracy on train set\n    \n    train_accuracy[i] = clf.score(X_train,Y_train)\n    \n    # Compute accuracy on test set\n    \n    test_accuracy[i]  = clf.score(X_test,Y_test)\n    \nprint(train_accuracy)\nprint(test_accuracy)\n\n\n# Plotting this train and test accuracy.\nplt.figure(figsize=(18,12))\nplt.title('clf: Varying depth of DecisionTreeClassifier to check overfitting')\nplt.plot(dep, train_accuracy, label = 'Training Accuracy')\nplt.plot(dep, test_accuracy, label = 'Testing Accuracy')\nplt.legend()\nplt.xlabel('Depth of Tree')\nplt.ylabel('Accuracy')\nplt.savefig('DecisionTreeAccuracyOverfit.svg')\nplt.show()","7d0085d1":"# Insight:As can be seen in the plot, the training accuracy almost reaches perfect but the testing accuracy\n# actually goes down after a few epochs. The decision tree classifier is overfitting to the data.\n\n# So select depth of tree to be something sensible, like >10","6b6fb89e":"# Read data again \n\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint(data_train.info())\nprint(data_test.info())","8af4df6c":"# It is a good idea to create a function that cleans the data\n# Fare and Age have missing values \n\ndef clean_data(data):\n    data['Fare'] =data.Fare.fillna(data.Fare.median())\n    data['Age'] = data.Age.fillna(data.Age.median())\n    data['Embarked'] = data.Embarked.fillna('S')\n    \n    data.loc[data['Sex'] == 'male', 'Sex'] = 0\n    data.loc[data['Sex'] == 'female', 'Sex'] = 1\n    \n    data.loc[data['Embarked']=='S', 'Embarked'] = 0\n    data.loc[data['Embarked']=='C', 'Embarked'] = 1\n    data.loc[data['Embarked']=='Q', 'Embarked'] = 2\n\n# Function to save csv of predictions \n# Check if this works (Spoiler: it doesn't)\n#def save_csv(data):\n#    data[['PassengerId', 'Survived']].to_csv('ANN_same_features_as_DT_solution.csv', index=False)","ea2577bf":"clean_data(data_train)\nclean_data(data_test)\ndata_train.info()\ndata_test.info()\n\ndata_train.head()","01511f45":"print(data_train.isnull().sum())\nprint(data_test.isnull().sum())\n# Cabin missing lots of values \n# So drop cabin \n\ndata_train.drop(['Cabin'], axis = 1, inplace = True)\ndata_test.drop(['Cabin'], axis = 1, inplace = True)","035d1f9c":"sns.heatmap(data_train.corr(), annot =True, linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()\n\n# We can see that Fare and Sex are strongly correlated with Survival.","ede194b6":"# The neural network requires numpy like arrays\ntarget = data_train['Survived'].values\n\n# here order matters, needs to match with test dataframe feature ordering, when testing with ANN\nfeatures = data_train[['Pclass','Sex','Age','SibSp','Fare']].values","2754363a":"import keras ","3527a04d":"from keras.models import Sequential\nfrom keras.layers import Dense \n\nmodel = Sequential()\n\n# Input and then first Dense\/FC layer with relu and 4 neurons\n# kernel_initializer=\"uniform\", selects the distribution from which weights of layer are initialized\nmodel.add(Dense(activation = 'relu', input_dim = 5, units = 4))\n\n# second FC layer with relu and 4 neurons \nmodel.add(Dense(activation = 'relu', units = 4))\n\n# Final classification layer with sigmoid activation for output - survived\/not survived\n\nmodel.add(Dense(activation = 'sigmoid', units = 1))","cfe51154":"# specifying learning parameters \nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","f000effa":"# Training Model \n\nmodel.fit(features, target, batch_size = 10, epochs = 100)","23b8912b":"#features = data_train[['Pclass','Sex','Age','SibSp','Fare']].values\ndata_test.drop(['PassengerId','Name','Parch', 'Ticket', 'Embarked'], axis =1, inplace =True)\ndata_test.head()","86e926bf":"# predictions on test set using ANN\ny_pred = model.predict(data_test)","cee9d596":"print(y_pred)\n# Output in round of float","191a8909":"# Needs to be rounded \n#print(y_pred.round())\ny_pred = y_pred.round()\ny_pred = y_pred.astype(int)\nprint(y_pred)","61374340":"data_test_orig = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test['Survived'] = y_pred\ndf_test[['PassengerId', 'Survived']].to_csv(\"ANN_same_features_as_DT_solution.csv\", index=False)\n#write_prediction(y_pred, \"ANN_same_features_as_DT_solution.csv\")","6db17ab3":"POSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nBUT NOT IF A FEATURE IS CORRELATED WITH THE TARGET VARIABLE (CONFIRM?)\n\nSelecting Feature from training set to feed to the neural networks","64ec4c36":"> # ML Model on this Dataset \n\n# Part 2: Data wrangling<br>\nfill nan, convert categorical to numerical,\ncreate train and test data for ML algorithms\n","b8367964":"### Data wrangling<br>\nfill nan, convert categorical to numerical,\ncreate train and test data for ML algorithms\n\n### Data Imputation \n\n#### Converting categorical sex to numerical data.\nCan also use pandas.get_dummies() <br>\nOR<br>\nFrom pandas.loc documentation:\n<br>.loc([[ ]]) returns dataframe <br>\n<b>Setting values<\/b><br>\nSet value for all items matching the list of labels\n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n","4ccb4365":"# Part 3: Scikit-learn basic ML algorithms\n# 4th Model : Building a Decision Tree Classifier\nGives 78.468% Accuracy.\n","df742a32":"## 2nd Model: where only women survive\n\ngives accuracy of 76.6%. Decent improvement based just on feature engineering.","0b6a9ece":"### Missing Values in Age and Fare \n### Data Imputation Needed","90a19fcc":"## Base Model: where survival is random\nGives 48.3% accuracy.","38d2a5fd":"# Part 1: Exploratory Data Analysis (EDA)\nunderstand the data by EDA and derive simple models with Pandas as baseline using only <b> Feature engineering.<\/b><br>\n\nData Defination\n\nVariable Definition Key\n\nsurvival Survival 0 = No, 1 = Yes\n\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex Male or Female\n\nAge Age in years\n\nsibsp # of siblings \/ spouses aboard the Titanic\n\nparch # of parents \/ children aboard the Titanic\n\nticket Ticket number\n\nfare Passenger fare\n\ncabin Cabin number\n\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them","dc8a62d3":"## Insight 1 : Women were more likely to survive than men.\n\n","ce8041e4":"### Applying Feature Engineering\n\nFeature engineering is the art of converting raw data into useful features.","ac57c51f":"## 3rd Model : All women and 1st class Passengers survived.\n\nGives 70.8% accuracy. ","ad421f72":"## Insight 0: Proportion of people that died is way higher.","9198a673":"# Part 4: Deep Learning Model with Keras\n\n# 5th Model : Simple 3 Layer ANN with same features as provided to Decision Tree\n1. Gives 75.598% Accuracy.<br>\nParameters: Network 3 layer- 4*4 Neurons, Batch size - 10, Epochs -100\n\n2. Gives 76.555% Accuracy.<br>\nParameters: Network 3 layer- 512*512 Neurons, Batch size - 10, Epochs -100\n\n2. Gives 73.205% Accuracy.<br>\nParameters: Network 3 layer- 512*512 Neurons, Batch size - 32, Epochs -1000 \nUsing GPU. Definitely overfits - 90% train accuracy.\n\n\n(Lower than Decision Tree and even worse than plain feature engineering where only women survive)\nusing : https:\/\/www.kaggle.com\/saife245\/titanic-deep-learning-model-with-80-accuracy","a752fb3d":"## Insight 2 : Passengers in class 1 were more likely to survive, and passengers from class 3 were very unlikely to survive.\n\n","f4ca7c6e":"### Using Keras to create simple ANN","a3f795c4":"## Base Model: Predicts everyone died.\n\nGives 62.7% accuracy"}}