{"cell_type":{"4c3145c8":"code","8c8182a4":"code","87e3f812":"code","1a1fd8b4":"code","20ff4849":"code","f1761fd9":"code","fc5177e0":"code","7c86e532":"code","08dcd3b2":"code","e67dd198":"code","8f3a9a51":"code","17f88894":"code","2f9aa469":"code","35170f35":"code","47f8bdc7":"code","ad4fbab7":"code","d6c4e8d4":"code","867bdf2d":"code","8aae004a":"code","08a5e655":"code","0de905a6":"code","cdc093b2":"code","dde21f45":"code","48d8bedf":"code","8f53ecde":"code","1827d963":"code","22e70ad3":"code","6815b7a4":"code","e67baa9a":"code","09808bb6":"code","582535cb":"code","f36cbf8d":"code","5316967c":"code","a2fc41b6":"code","14f4d834":"code","e4cd736c":"code","2420d24b":"code","cadf8038":"code","1f05895e":"code","11be2896":"code","189d3560":"code","91ba2ac6":"code","6f931990":"code","76fe9923":"code","8da505a9":"markdown","9e7a5493":"markdown","d73e04dc":"markdown","c38f9513":"markdown","6b30c129":"markdown"},"source":{"4c3145c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c8182a4":"# Importing the libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\nimport nltk\nfrom collections import Counter\n\nfrom plotly import graph_objs as go\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nimport nltk\nimport gensim\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers","87e3f812":"df = pd.read_csv('\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv')\ndf.head()","1a1fd8b4":"df['text'] = df['Title'] + df['Body']\n\ndf.drop(['Id', 'Title', 'Body', 'CreationDate', 'Tags'], axis=1, inplace=True)\ndf.head()","20ff4849":"sns.countplot(df['Y'])","f1761fd9":"df.info()","fc5177e0":"# Data Cleaning\nstop = set(stopwords.words('english'))\n\ndef cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase\n\ncleaned_title = []\n\nfor sentance in tqdm(df['text'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())\n    \ndf['text'] = cleaned_title\ndf.head()","7c86e532":"df.head()","08dcd3b2":"# Creating some basic EDA plots","e67dd198":"# WordCloud for HighQuality Posts\n\nplt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Y == 'HQ'].text))\nplt.imshow(wc , interpolation = 'bilinear')","8f3a9a51":"# WordCloud for LowQuality Posts Closed\n\nplt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Y == 'LQ_CLOSE'].text))\nplt.imshow(wc , interpolation = 'bilinear')","17f88894":"# WordCloud for LowQuality Posts Open\n\nplt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Y == 'LQ_EDIT'].text))\nplt.imshow(wc , interpolation = 'bilinear')","2f9aa469":"# Continuing with some n-gram analysis\n\ndef basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","35170f35":"# Bi-grams for HQ posts\n\nHQ_words = basic_clean(''.join(str(df[df.Y == 'HQ']['text'].tolist())))\nbigram_HQ=(pd.Series(nltk.ngrams(HQ_words, 2)).value_counts())[:20]\nbigram_HQ=pd.DataFrame(bigram_HQ)\nbigram_HQ['idx']=bigram_HQ.index\nbigram_HQ['idx'] = bigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","47f8bdc7":"plot_data = [\n    go.Bar(\n        x=bigram_HQ['idx'],\n        y=bigram_HQ[0],\n        marker = dict(\n            color = 'Blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 bi-grams from High Quality Posts',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","ad4fbab7":"# Bi-grams for LQ-CLOSED posts\n\nLQC_words = basic_clean(''.join(str(df[df.Y == 'LQ_CLOSE']['text'].tolist())))\nbigram_LQC=(pd.Series(nltk.ngrams(LQC_words, 2)).value_counts())[:20]\nbigram_LQC=pd.DataFrame(bigram_LQC)\nbigram_LQC['idx']=bigram_LQC.index\nbigram_LQC['idx'] = bigram_LQC.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","d6c4e8d4":"plot_data = [\n    go.Bar(\n        x=bigram_LQC['idx'],\n        y=bigram_LQC[0],\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 bi-grams from Low Quality Posts Closed',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","867bdf2d":"# Bi-grams for LQ-OPEN posts\n\nLQE_words = basic_clean(''.join(str(df[df.Y == 'LQ_EDIT']['text'].tolist())))\nbigram_LQE=(pd.Series(nltk.ngrams(LQE_words, 2)).value_counts())[:20]\nbigram_LQE=pd.DataFrame(bigram_LQE)\nbigram_LQE['idx']=bigram_LQE.index\nbigram_LQE['idx'] = bigram_LQE.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","8aae004a":"plot_data = [\n    go.Bar(\n        x=bigram_LQE['idx'],\n        y=bigram_LQE[0],\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 bi-grams from Low Quality Posts Open',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","08a5e655":"# Word2Vec","0de905a6":"# Model Building\n# Step 1 - Tokenization\nX = []\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in df['text'].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)\nprint ('Tokenization done...')   \n# Model Building and Training\nw2v_model = gensim.models.Word2Vec(sentences=X, size=150, window=5, min_count=2)\nprint ('Word2Vec model created')","cdc093b2":"# Making some naive observations\n\nw2v_model.wv.most_similar(positive = 'python')","dde21f45":"w2v_model.wv.most_similar(positive = 'java')","48d8bedf":"w2v_model.wv.most_similar(positive = 'bug')","8f53ecde":"w2v_model.wv.most_similar(positive = 'stack')","1827d963":"w2v_model.wv.similarity('java', 'kotlin')","22e70ad3":"w2v_model.wv.similarity('java', 'python')","6815b7a4":"w2v_model.wv.doesnt_match(['java', 'python', 'scala', 'kotlin'])","e67baa9a":"w2v_model.wv.doesnt_match(['java', 'python', 'pandas', 'numpy'])","09808bb6":"# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'class'. \ndf['Y']= label_encoder.fit_transform(df['Y']) ","582535cb":"X = df['text']\ny = df['Y']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","f36cbf8d":"tokenizer = text.Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\n\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=300)\n\ntokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=300)","5316967c":"print(len(tokenizer.word_index))\nvocab_size = 10000 + 1","a2fc41b6":"EMBEDDING_FILE = '..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt'","14f4d834":"embeddings_index = dict()\nf = open(EMBEDDING_FILE)\nfor line in f:\n\tvalues = line.split()\n\tword = values[0]\n\tcoefs = asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))\n# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","e4cd736c":"embedding_matrix = zeros((vocab_size, 200))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","2420d24b":"embedding_matrix.shape","cadf8038":"# Training the Model. We will use a GRU model.\n\nbatch_size = 256\nepochs = 10\nembed_size = 200\nmaxlen = 300\nmax_features = 10001\n\n#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n#LSTM\nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.4 , dropout = 0.4))\n#GRU\nmodel.add(GRU(units=256 , return_sequences = False, dropout = 0.4))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","1f05895e":"model.summary()","11be2896":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)","189d3560":"history = model.fit(X_train, y_train, batch_size = batch_size , \n                    validation_data = (X_test, y_test) , epochs = 5, \n                    callbacks = [learning_rate_reduction])","91ba2ac6":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","6f931990":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","76fe9923":"# We can see that the accuracy rises steadily for training while the growth is damped in case of testing\n# The loss values for both training and testing are decreasing steadily.\n# If we train for 15-20 epochs we can have a good convergent model\n# We can also use different layers like :\n# 1. Stacked GRU's\n# 2. Bidirectional LSTM\n# 3. Stacked LSTM's\n# 4. Stacked Bidirectional LSTM's","8da505a9":"## Loading the GLoVe embeddings pretrained","9e7a5493":"Data looks pretty much balanced.","d73e04dc":"## String Tokenization","c38f9513":"We first concatenate both the 'Title' and 'Body' as a simple 'text' column. We shall remove the tags, Id, CreationDate","6b30c129":"### Model Training"}}