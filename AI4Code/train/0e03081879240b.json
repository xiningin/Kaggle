{"cell_type":{"515bcca5":"code","cfbac282":"code","ade9bff5":"code","b9c8e265":"code","92952c45":"code","0dc234fe":"code","497a2ca8":"code","d68933e0":"code","dac9b519":"code","2c161c62":"code","46a06aa5":"code","2a71cb06":"code","029b994c":"code","dc32eb46":"code","000226a3":"code","5e7669b7":"code","79ef37d5":"code","afc51e4f":"code","187fb4c3":"code","d0003f9e":"code","c76c51ad":"code","f2f1dc05":"code","f077d631":"code","343c7011":"code","af62dc4f":"markdown","ea4f0c29":"markdown"},"source":{"515bcca5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfbac282":"from IPython.display import Image, display\nimport numpy as np\nfrom os.path import join\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","ade9bff5":"df_link = pd.read_csv('..\/input\/dataset\/datasets.csv')\ndf_link.head()","b9c8e265":"df = df_link.copy()\ndf = df.dropna(axis=0, subset=['Source'])\ndf.head()","92952c45":"import requests\nfrom io import BytesIO\nfrom PIL import Image\n\nfor i in range(100):\n  r = requests.get(df['Source'][i])\n  print(\"Status:\", r.status_code)\n  print(r.url)","0dc234fe":"from skimage import io\n\nimage = io.imread('..\/input\/data-image\/BurgosPuertaDeLaCoroneria\/DPP_0245.JPG')\nplt.imshow(image)","497a2ca8":"list = os.listdir('..\/input\/data-image\/BurgosPuertaDeLaCoroneria')\nfor i in range(len(list)):\n  print(list[i])","d68933e0":"import matplotlib.image as mpimg\n\ndef process(filename):\n    image = mpimg.imread('..\/input\/data-image\/BurgosPuertaDeLaCoroneria\/'+filename)\n    plt.figure()\n    plt.imshow(image)\n\nfor file in list:\n    process(file)","dac9b519":"import matplotlib.pyplot as plt\nimport cv2\nfrom skimage import data, color\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\nimg = cv2.imread('..\/input\/data-image\/BurgosPuertaDeLaCoroneria\/DPP_0247.JPG')\nimage = color.rgb2gray(img)\n\nimage_rescaled = rescale(image, 0.25, anti_aliasing=False)\nimage_resized = resize(image, (image.shape[0] \/\/ 4, image.shape[1] \/\/ 4),\n                       anti_aliasing=True)\nimage_downscaled = downscale_local_mean(image, (4, 3))\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\nax = axes.ravel()\n\nax[0].imshow(image, cmap='gray')\nax[0].set_title(\"Original image\")\n\nax[1].imshow(image_rescaled, cmap='gray')\nax[1].set_title(\"Rescaled image (aliasing)\")\n\nax[2].imshow(image_resized, cmap='gray')\nax[2].set_title(\"Resized image (no aliasing)\")\n\nax[3].imshow(image_downscaled, cmap='gray')\nax[3].set_title(\"Downscaled image (no aliasing)\")\n\nax[0].set_xlim(0, 512)\nax[0].set_ylim(512, 0)\nplt.tight_layout()\nplt.show()","2c161c62":"!pip install imutils","46a06aa5":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\nimport imutils\ncv2.ocl.setUseOpenCL(False)","2a71cb06":"# select the image id (valid values 1,2,3, or 4)\nfeature_extractor = 'orb' # one of 'sift', 'surf', 'brisk', 'orb'\nfeature_matching = 'bf'","029b994c":"# read images and transform them to grayscale\n# Make sure that the train image is the image that will be transformed\ntrainImg = imageio.imread('..\/input\/data-image\/BurgosPuertaDeLaCoroneria\/DPP_0247.JPG')\ntrainImg_gray = cv2.cvtColor(trainImg, cv2.COLOR_RGB2GRAY)\n\nqueryImg = imageio.imread('..\/input\/data-image\/BurgosPuertaDeLaCoroneria\/DPP_0246.JPG')\n# Opencv defines the color channel in the order BGR. \n# Transform it to RGB to be compatible to matplotlib\nqueryImg_gray = cv2.cvtColor(queryImg, cv2.COLOR_RGB2GRAY)\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))\nax1.imshow(queryImg, cmap=\"gray\")\nax1.set_xlabel(\"Query image\", fontsize=14)\n\nax2.imshow(trainImg, cmap=\"gray\")\nax2.set_xlabel(\"Train image (Image to be transformed)\", fontsize=14)\n\nplt.show()","dc32eb46":"def detectAndDescribe(image, method=None):\n    \"\"\"\n    Compute key points and feature descriptors using an specific method\n    \"\"\"\n    \n    assert method is not None, \"You need to define a feature detection method. Values are: 'sift', 'surf'\"\n    \n    # detect and extract features from the image\n    if method == 'sift':\n        descriptor = cv2.xfeatures2d.SIFT_create()\n    elif method == 'surf':\n        descriptor = cv2.xfeatures2d.SURF_create()\n    elif method == 'brisk':\n        descriptor = cv2.BRISK_create()\n    elif method == 'orb':\n        descriptor = cv2.ORB_create()\n        \n    # get keypoints and descriptors\n    (kps, features) = descriptor.detectAndCompute(image, None)\n    \n    return (kps, features)","000226a3":"kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)\nkpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)","5e7669b7":"# display the keypoints and features detected on both images\nfig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)\nax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))\nax1.set_xlabel(\"\", fontsize=14)\nax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))\nax2.set_xlabel(\"(b)\", fontsize=14)\n\nplt.show()","79ef37d5":"def createMatcher(method,crossCheck):\n    \"Create and return a Matcher Object\"\n    \n    if method == 'sift' or method == 'surf':\n        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=crossCheck)\n    elif method == 'orb' or method == 'brisk':\n        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)\n    return bf","afc51e4f":"def matchKeyPointsBF(featuresA, featuresB, method):\n    bf = createMatcher(method, crossCheck=True)\n        \n    # Match descriptors.\n    best_matches = bf.match(featuresA,featuresB)\n    \n    # Sort the features in order of distance.\n    # The points with small distance (more similarity) are ordered first in the vector\n    rawMatches = sorted(best_matches, key = lambda x:x.distance)\n    print(\"Raw matches (Brute force):\", len(rawMatches))\n    return rawMatches","187fb4c3":"def matchKeyPointsKNN(featuresA, featuresB, ratio, method):\n    bf = createMatcher(method, crossCheck=False)\n    # compute the raw matches and initialize the list of actual matches\n    rawMatches = bf.knnMatch(featuresA, featuresB, 2)\n    print(\"Raw matches (knn):\", len(rawMatches))\n    matches = []\n\n    # loop over the raw matches\n    for m,n in rawMatches:\n        # ensure the distance is within a certain ratio of each\n        # other (i.e. Lowe's ratio test)\n        if m.distance < n.distance * ratio:\n            matches.append(m)\n    return matches","d0003f9e":"print(\"Using: {} feature matcher\".format(feature_matching))\n\nfig = plt.figure(figsize=(20,8))\n\nif feature_matching == 'bf':\n    matches = matchKeyPointsBF(featuresA, featuresB, method=feature_extractor)\n    img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,matches[:100],\n                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\nelif feature_matching == 'knn':\n    matches = matchKeyPointsKNN(featuresA, featuresB, ratio=0.75, method=feature_extractor)\n    img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,np.random.choice(matches,100),\n                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n    \n\nplt.imshow(img3)\nplt.show()","c76c51ad":"def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n    # convert the keypoints to numpy arrays\n    kpsA = np.float32([kp.pt for kp in kpsA])\n    kpsB = np.float32([kp.pt for kp in kpsB])\n    \n    if len(matches) > 4:\n\n        # construct the two sets of points\n        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n        \n        # estimate the homography between the sets of points\n        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n            reprojThresh)\n\n        return (matches, H, status)\n    else:\n        return None","f2f1dc05":"M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\nif M is None:\n    print(\"Error!\")\n(matches, H, status) = M\nprint(H)","f077d631":"# Apply panorama correction\nwidth = trainImg.shape[1] + queryImg.shape[1]\nheight = trainImg.shape[0] + queryImg.shape[0]\n\nresult = cv2.warpPerspective(trainImg, H, (width, height))\nresult[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg\n\nplt.figure(figsize=(20,10))\nplt.imshow(result)\n\nplt.axis('off')\nplt.show()","343c7011":"# transform the panorama image to grayscale and threshold it \ngray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n\n# Finds contours from the binary image\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\n\n# get the maximum contour area\nc = max(cnts, key=cv2.contourArea)\n\n# get a bbox from the contour area\n(x, y, w, h) = cv2.boundingRect(c)\n\n# crop the image to the bbox coordinates\nresult = result[y:y + h, x:x + w]\n\n# show the cropped image\nplt.figure(figsize=(20,10))\nplt.imshow(result)","af62dc4f":"## Image Stitching\n\n","ea4f0c29":"## Image Resizing and Scaling"}}