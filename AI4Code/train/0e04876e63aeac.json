{"cell_type":{"d784cbf3":"code","ac39e7d2":"code","4a451eda":"code","317c0b7c":"code","05575a29":"code","3b8b6b1c":"code","22340666":"code","0fd43f99":"code","f83099db":"code","fe7aa308":"code","d042933e":"code","ad255a7d":"code","091026e9":"code","e81e8b21":"code","bd22b295":"code","35cc6331":"code","ca0e3419":"code","66955e41":"code","77f58e67":"code","dbaad009":"code","8033c260":"code","33324067":"code","e95763d6":"code","ab20d859":"code","23f9853d":"code","a8605f74":"code","d4317355":"code","795424bf":"code","b9e34696":"code","c867f1df":"code","732724cb":"code","b43caf8f":"code","5609e83f":"code","04cdd1ff":"code","26cd381b":"code","8cf46875":"code","a66a3e19":"code","2af7202d":"code","63397dd9":"code","91001f6c":"code","cd349cdc":"code","88756378":"code","ac6ab33d":"code","457f2385":"code","f8ceb0f0":"markdown","9678b6c7":"markdown","6d0b80c1":"markdown","b3a56a12":"markdown","2f19d83d":"markdown","8c12f7ef":"markdown","b7e2a75d":"markdown","c48157b5":"markdown","e33dd7a4":"markdown","31024878":"markdown","9661134c":"markdown","8d5f7061":"markdown","c20bcf79":"markdown","000272c4":"markdown","6161272a":"markdown","6d8ca69a":"markdown","096ab67d":"markdown","d06a3c54":"markdown","3216ded9":"markdown","b10ec8f2":"markdown","eb965373":"markdown","a469a8d9":"markdown","c4366d56":"markdown","82d944b1":"markdown","90722d28":"markdown","5a8f813f":"markdown","5479580f":"markdown","50936e29":"markdown","ff76a067":"markdown","8a339d23":"markdown","673dcd50":"markdown"},"source":{"d784cbf3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import ElasticNet, Lasso,BayesianRidge, LassoLarsIC, Ridge, LinearRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.metrics import make_scorer\nimport xgboost as xg\nimport time\nimport folium\nfrom folium.plugins import HeatMap\nimport plotly.express as px\n\nplt.style.use('fivethirtyeight')\n%matplotlib inline\npd.set_option('display.max_columns', 32)","ac39e7d2":"#read dataset\ndf = pd.read_csv(\"..\/input\/student-csv\/Student.csv\")\ndf.head()","4a451eda":"# check type of dataset and count how many non-null values\ndf.info()","317c0b7c":"df = df.rename(columns={\"Medu\":\"mother's education\", \"Fedu\":\"father's education\", \"Mjob\":\"mother's job\", \"Fjob\": \"father's job\", \n                        \"famsup\":\"family's support\", \"Dalc\":\"workday alcohol\", \"Walc\":\"weekend alcohol\", \"higher\": \"study higher\", \n                       \"Pstatus\":\"parent's status\"})","05575a29":"print(\"Name of all the columns: \", list(df.columns))","3b8b6b1c":"fig, ax = plt.subplots(figsize=(25,20))\nax = msno.bar(df)\nax.set_title(\"Missing values in bar chart\")\nax.set_xlabel(\"Categories\", fontsize=22)\nax.set_ylabel(\"Percent\", fontsize=22)","22340666":"fig, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nax = sns.distplot(df['G3'], color=\"b\");\nax.xaxis.grid(False)\nax.set_ylabel(\"Frequency\")\nax.set_xlabel(\"Final Grades\")\nax.set(title=\"Final Grades\")\n","0fd43f99":"# Skew and kurt\nprint(f\"Skewness: {df['G3'].skew()}\")\nprint(f\"Kurtosis: {df['G3'].kurt()}\")","f83099db":"# Get list of all categoric and numeric columns\ncolumns_object = list(df.select_dtypes(['object']))\ncolumns_numeric = list(df.select_dtypes(['int64']))\n\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(df[columns_numeric], 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(columns_numeric), 3, i)\n    sns.scatterplot(x=feature, y='G3', hue='G3', palette='Blues', data=df)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('G3', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","fe7aa308":"fig, ax = plt.subplots(figsize=(15,10))\nax = sns.countplot(x = \"sex\", data=df)\nax.set_title(\"Distribution of gender\")\n","d042933e":"fig, ax = plt.subplots(figsize=(15,10))\nax = sns.kdeplot(x = \"age\", data=df, color=\"blue\", shade=True, Label=\"Age\")\nax.set_title(\"Distribution of age\")\nax.set_xlabel('Age')\nax.legend()","ad255a7d":"# Grade distribution by address\nfig, ax = plt.subplots(figsize=(15,10))\nsns.kdeplot(df.loc[df['romantic'] == 'yes', 'G3'], label='Have rela', shade = True)\nsns.kdeplot(df.loc[df['romantic'] == 'no', 'G3'], label='Do not in rela', shade = True)\nax.set_title('Students has relationship vs grades ?', fontsize = 20)\nax.set_xlabel('Grade', fontsize = 20);\nax.set_ylabel('Density', fontsize = 20)\nax.legend()","091026e9":"fig, ax = plt.subplots(figsize=(15,10))\nsns.kdeplot(df.loc[df['address'] == 'U', 'G3'], label='Urban', shade = True)\nsns.kdeplot(df.loc[df['address'] == 'R', 'G3'], label='Rural', shade = True)\nax.set_title('Do urban students score higher than rural students?', fontsize = 20)\nax.set_xlabel('Grade', fontsize = 20);\nax.set_ylabel('Density', fontsize = 20)","e81e8b21":"fig, ax = plt.subplots(1,2, figsize=(15,10))\norder_by = df.groupby(\"father's education\")['G3'].mean().sort_values(ascending = False).index\nax[0] = sns.boxplot(x = df[\"father's education\"], y = df['G3'], order = order_by, ax=ax[0])\nax[0].set_title(\"Father's education vs final grades\")\n\norder_by1 = df.groupby(\"mother's education\")['G3'].mean().sort_values(ascending = False).index\nax[1] = sns.boxplot(x = df[\"mother's education\"], y = df['G3'], order = order_by1, ax=ax[1])\nax[1].set_title(\"Mother's education vs final grades\")\n\nplt.show()","bd22b295":"fig, ax = plt.subplots(figsize=(15,10))\nsns.kdeplot(df.loc[df['sex'] == 'M', 'G3'], label='Male', shade = True)\nsns.kdeplot(df.loc[df['sex'] == 'F', 'G3'], label='Female', shade = True)\nax.set_title('Gender vs grades ?', fontsize = 20)\nax.set_xlabel('Grades', fontsize = 20);\nax.set_ylabel('Density', fontsize = 20)\nax.legend()","35cc6331":"fig, ax = plt.subplots(figsize=(15,10))\nax = sns.boxplot(x='studytime', y='G3',data=df,palette='gist_heat')\nax.set_title('Study Time vs Final Grade',fontsize = 20)\nax.set_xlabel('Study Time', fontsize = 20);\nax.set_ylabel('G3', fontsize = 20)","ca0e3419":"result = df.groupby('goout', as_index=False)['G3'].mean()\nfig, ax = plt.subplots(figsize=(15,10))\nax = sns.barplot(x=result['goout'], y=result['G3'],data=result,palette='gist_heat')\nax.set_title('Frequency of going out vs Final Grades',fontsize = 20)\nax.set_xlabel('Frequency of going out', fontsize = 20);\nax.set_ylabel('G3', fontsize = 20)","66955e41":"fig, ax = plt.subplots(figsize=(15,10))\nax = sns.boxplot(x='study higher', y='G3',data=df,palette='gist_heat')\nax.set_title('Study higher vs Final Grade',fontsize = 20)\nax.set_xlabel('Study higher', fontsize = 20);\nax.set_ylabel('G3', fontsize = 20)","77f58e67":"df['total_alcohol'] = df['workday alcohol']+df['weekend alcohol']\nfig, ax = plt.subplots(figsize=(15,10))\nax = sns.scatterplot(x=df['total_alcohol'], y=df['G3'],data=df,palette='gist_heat')\nax.set_title('Frequency of drinking in  vs Final Grades',fontsize = 20)\nax.set_xlabel('Frequency of drinking', fontsize = 20);\nax.set_ylabel('G3', fontsize = 20)","dbaad009":"fig, ax = plt.subplots(figsize=(15,10))\nax = sns.histplot(x=df['absences'], data=df,palette='gist_heat')\nax.set_title('Number of school absences',fontsize = 20)\nax.set_xlabel('NUmber of absences', fontsize = 20);\nax.set_ylabel('number of students', fontsize = 20)","8033c260":"# df.drop(['workday alcohol', 'weekend alcohol'], axis=1, inplace=True)\ndf.drop(['total_alcohol'], axis=1, inplace=True)\nle = LabelEncoder()\ndf[columns_object] = df[columns_object].apply(lambda col: le.fit_transform(col))  \ndf.head()","33324067":"df_correlated = abs(df.corr())['G3'].sort_values(ascending=False)\ndf_correlated","e95763d6":"# Most correlated features\nmat = df.corr()\nf, ax = plt.subplots(figsize=(15, 10))\ntop_corr_features = mat.index[abs(mat[\"G3\"])>0.12]\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nax = sns.heatmap(df[top_corr_features].corr(), cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","ab20d859":"corr_dict=mat['G3'].sort_values(ascending=False).to_dict()\nimportant_columns=[]\nfor key,value in corr_dict.items():\n    if (abs(value) >0.12 and  abs(value)<0.8):\n        important_columns.append(key)\nimportant_columns.append('G3')\nimportant_columns","23f9853d":"df_final = df[important_columns]\ndf_final.head()","a8605f74":"Y = df_final['G3']\nX = df_final.drop(\"G3\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)","d4317355":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","795424bf":"# root mean squared_loss\ndef rmse(predict, actual):\n    predict = np.array(predict)\n    actual = np.array(actual)\n\n    distance = predict - actual\n\n    square_distance = distance ** 2\n\n    mean_square_distance = square_distance.mean()\n\n    score = np.sqrt(mean_square_distance)\n\n    return score\n\nrmse_score = make_scorer(rmse, greater_is_better = False)","b9e34696":"# define some list here, we will put them in dataframe later\nmodel_name_list = ['Ridge Regression', 'Lasso', 'ElasticNet Regression', 'RandomForestRegressor', 'GradientBoostRegressor', \"XGBoostRegressor\", \"Blends 6 model\", \"Stacking Regressor\", \"Final Ensemble Regressor\"]","c867f1df":"def linear_regression():\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    print(f\"Root mean squared error: {(rmse):.4f}\")\n    \nlinear_regression()","732724cb":"def regression_model(model, input_x, input_y):\n    # prepare a range of alpha values to test\n    alphas = np.arange(0.0005, 20, 0.001)\n    param_grid = {'alpha':alphas}\n    cv = KFold(10, shuffle=True, random_state = 42)\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n    \n    grid_search_regression = GridSearchCV(estimator=model,  \n                         param_grid=(param_grid),\n                         scoring=rmse_score,\n                         cv = cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_regression.fit(input_x, input_y)\n    best_parameters_regression = grid_search_regression.best_params_  \n    best_score_regression = grid_search_regression.best_score_ \n    print(\"Best parameter:\", best_parameters_regression)\n    print(f\"Best rsme score {(best_score_regression):.4f}\")\n    pred = grid_search_regression.best_estimator_.predict(X_test)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    print(f\"Root mean squared error in test set: {(rmse):.4f}\")\n\nmodels = [Ridge(random_state=42), Lasso(random_state=42), ElasticNet(random_state=42)]\nfor model in models:\n    regression_model(model, X_train, y_train)","b43caf8f":"def tree_regression_model(model, input_x, input_y):\n    # prepare a range of hyper parameters to tune in\n    params = {'n_estimators': list(range(2, 40, 1)), 'max_features': ['auto', 'sqrt', 'log2'], \n         'min_samples_leaf': list(range(2, 40, 1))}\n    cv = KFold(10, shuffle=True, random_state = 42)\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_tree = GridSearchCV(estimator=model,  \n                         param_grid=params,\n                         scoring=rmse_score,\n                         cv = cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_tree.fit(input_x, input_y)\n    best_parameters_tree = grid_search_tree.best_params_  \n    best_score_tree = grid_search_tree.best_score_ \n    print(\"Best parameter:\", best_parameters_tree)\n    print(f\"Best rsme score: {(best_score_tree):.4f}\")\n    pred = grid_search_tree.best_estimator_.predict(X_test)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    print(f\"Root mean squared error: {(rmse):.4f}\")\n\nmodels = [RandomForestRegressor(random_state=42), GradientBoostingRegressor(random_state=42)]\nfor model in models:\n    tree_regression_model(model, X_train, y_train)","5609e83f":"def xgboost_regression_model(input_x, input_y):\n    # prepare a range of hyper parameters to tune in\n    params ={\"learning_rate\": [0.1, 0.2, 0.3, 0.4, 0.5], \n             \"max_depth\": list(range(2, 6, 1))}\n    cv = KFold(10, shuffle=True, random_state = 42)\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n    model = xg.XGBRegressor(objective=\"reg:squarederror\", random_state = 42)\n    grid_search_tree = GridSearchCV(estimator=model,  \n                         param_grid=params,\n                         scoring=rmse_score,\n                         cv = cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_tree.fit(input_x, input_y)\n    best_parameters_tree = grid_search_tree.best_params_  \n    best_score_tree = grid_search_tree.best_score_ \n    print(\"Best parameter:\", best_parameters_tree)\n    print(f\"Best rsme score: {(best_score_tree):.4f}\")\n    pred = grid_search_tree.best_estimator_.predict(X_test)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    print(f\"Root mean squared error: {(rmse):.4f}\")\n\nxgboost_regression_model(X_train, y_train)","04cdd1ff":"preds_all = []\nrmse_all = []\nmodels = [Ridge(alpha=19.9995, random_state=42), Lasso(alpha=0.0885, random_state=42), ElasticNet(alpha=0.885, random_state=42), \n          RandomForestRegressor(n_estimators = 7, min_samples_leaf = 14, max_features = 'sqrt', random_state=42), \n          GradientBoostingRegressor(n_estimators = 39, min_samples_leaf = 2, max_features = 'log2', random_state=42), \n          xg.XGBRegressor(objective=\"reg:squarederror\", learning_rate = 0.1, max_depth=2, random_state = 42)]\ndef predict(model):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    preds_all.append(pred)\n    rmse = mean_squared_error(y_test, pred, squared=False)\n    rmse_all.append(rmse)\n\nfor model in models:\n    predict(model)\n","26cd381b":"print(rmse_all)","8cf46875":"a = np.arange(0.05, 1.00, 0.05)\nrmse_ensemble = 3.4942\nfor i in a:\n    for j in a:\n        for k in a:\n            for m in a:\n                for n in a:\n                    for p in a:\n                        if (i+j+k+m+n+p)==1:\n                            preds = preds_all[0]*i + preds_all[1]*j+ preds_all[2]*k + preds_all[3]*m + preds_all[4]*n + preds_all[5]*p\n                            rmse_final = mean_squared_error(y_test, preds, squared=False)\n                            if(rmse_final < rmse_ensemble):\n                                rmse_ensemble = rmse_final\n                                print(i, j, k, m, n, p)\n                                print(rmse_ensemble)","a66a3e19":"pred_ensemble = preds_all[0]*0.05+ preds_all[1]*0.05 + preds_all[2]*0.1 + preds_all[3]*0.05 +preds_all[4]*0.7 + preds_all[5]*0.05\nrmse_ensemble = mean_squared_error(y_test, pred_ensemble, squared=False)\nprint(f\"Root mean squared error: {(rmse_ensemble):.6f}\")\npreds_all.append(rmse_ensemble)\nrmse_all.append(rmse_ensemble)","2af7202d":"ridge = Ridge(alpha=19.995, random_state=42)\nlasso = Lasso(alpha=0.0885, random_state=42)\nelasticnet = ElasticNet(alpha=0.885, random_state=42)\nrf = RandomForestRegressor(n_estimators = 7, min_samples_leaf = 14, max_features = 'sqrt', random_state=42)\ngboost = GradientBoostingRegressor(n_estimators = 39, min_samples_leaf = 2, max_features = 'log2', random_state=42)\nxgb = xg.XGBRegressor(objective=\"reg:squarederror\", learning_rate = 0.1, max_depth=2, random_state = 42)\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, rf, gboost, xgb),\n                                meta_regressor= xgb, use_features_in_secondary=True,\n                                cv = KFold(12, random_state=42),\n                                random_state=42)\nstack_gen.fit(X_train, y_train)\npred_stack = stack_gen.predict(X_test)\nrmse_stack = mean_squared_error(y_test, pred_stack, squared=False)\nprint(f\"Root mean squared error in test set: {(rmse_stack):.4f}\")","63397dd9":"preds_all.append(pred_stack)\nrmse_all.append(rmse_stack)","91001f6c":"rmse_baseline = 3.475\na = np.arange(0.01, 1, 0.01)\nfor i in a:\n    for j in a:\n        for k in a:\n            for m in a:\n                if (i+j+k+m)==1:\n                    pred_final = pred_stack* i + preds_all[-2] *j + preds_all[-4]* k + preds_all[0]* m\n                    rmse_final = mean_squared_error(y_test, pred_final, squared=False)\n                    if(rmse_final < rmse_baseline):\n                        rmse_baseline = rmse_final\n                        print(i, j, k, m)\nprint(rmse_baseline)","cd349cdc":"final_predict = preds_all[-1]*0.5+ preds_all[-2]*0.02 + preds_all[-4]*0.35 + preds_all[0]*0.13\nrmse_final = mean_squared_error(y_test, final_predict, squared=False)\nprint(f\"Root mean squared error: {(rmse_final):.6f}\")\npreds_all.append(final_predict)\nrmse_all.append(rmse_final)","88756378":"print(model_name_list)\nprint(rmse_all)","ac6ab33d":"results = pd.DataFrame({'algorithm': model_name_list, 'rmse': rmse_all})\nresults","457f2385":"fig, ax = plt.subplots(figsize=(25,20))\nax = sns.barplot(x=results['algorithm'], y=results['rmse'], data=results, palette='gist_heat')\nax.set_title('Root mean squared error ',fontsize = 15)\nax.set_xlabel('Algorithms', fontsize = 15);\nax.set_ylabel('RMSE', fontsize = 15)","f8ceb0f0":"### Label encoder is only for numerical values. Therefore, this correlation will miss a lot of object features. So we will perform one hot encoding with all the object features","9678b6c7":"# Try Stacking CV Regressor","6d0b80c1":"# Do study more affects the grade?","b3a56a12":"## Distribution of ages","2f19d83d":"# Number of students who are absences","8c12f7ef":"## Count number of male and female students","b7e2a75d":"# Does father and mother's education affect the students' grades?","c48157b5":"## It seems that people who goes out less will have a higher mark although there is no difference between group 5(most frequently go out) and 1(least frequently go out)","e33dd7a4":"## Grade distribution vs address","31024878":"# Final prediction","9661134c":"## Try Final Blend with stackcv, blend 6 models, gradientboost, and ridge ","8d5f7061":"# Check correlation and draw them using heatmap","c20bcf79":"### Out of nowhere, student's who parents do not have education(0) perform nearly the same as those whose parents have higher education(4)","000272c4":"# Study higher vs final grades","6161272a":"### G3 is normalized, we do not have to modify anything","6d8ca69a":"### It is clearly that students who will pursue further study will have higher grades that who do not","096ab67d":"# Does alcohol makes students get less score? (We will consider another plot)","d06a3c54":"## Checking missing values","3216ded9":"### 3.49411, breaking the 3.50 threshold, that's good","b10ec8f2":"### G2 and G1 are strongly correlated with G3, we will excluding them in order to avoid overfitting","eb965373":"## From this heatmap, we can see that, number of past class failures, parent's education, studytime, alcohol consumption, goout, traveltime and ages are the 7 most correlated factors with G3. We will not consider G1 and G3 since they are strongly correlated with G2, including them may lead to overfitting. ","a469a8d9":"## We use standardscaler here","c4366d56":"# Does male students outperform female students?","82d944b1":"### Final result: root mean squared error 3.4749, we will try as if we can lower the score","90722d28":"# Blending models to get a better result","5a8f813f":"# Do students have relationship perform better than students do not have?","5479580f":"### Most students have less than 10 days of absences","50936e29":"# Trying Linear Regression model","ff76a067":"# G3 the variables we will try to predict","8a339d23":"### It seams that people with study time belongs to group 3(5-10 hours) or group 4(>10 hours) will perform better in the final exams","673dcd50":"# Trying Tree Regression Model"}}