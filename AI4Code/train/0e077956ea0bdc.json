{"cell_type":{"5beaf5bc":"code","8d976dd1":"code","b9b48c8e":"code","bd8fe437":"code","59d6637d":"code","3185d3a2":"code","d4e77567":"code","eb70531f":"code","81b0258a":"code","faba134b":"code","fe4d5f64":"code","eedf7908":"code","5a9f53ac":"code","d73e39dd":"code","c130a656":"code","2c81cbff":"code","327e5cc8":"code","c8924f0e":"code","59b5333f":"code","1fae07e1":"code","f465353a":"code","267693f0":"code","a3053e5e":"code","21d2a488":"code","c9826dad":"code","7da97d80":"code","ba27a532":"code","254e4203":"markdown","d495829e":"markdown"},"source":{"5beaf5bc":"# downloading a tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","8d976dd1":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_colwidth', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk, re, string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ntf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\nimport tokenization","b9b48c8e":"# Load the data\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","bd8fe437":"# train and text shapes\ntrain_df.shape, test_df.shape","59d6637d":"# Check the duplicated tweets\ndup_train = train_df['text'].duplicated().sum()\nprint(f'there are {dup_train} tweets duplicated in train_df.')","3185d3a2":"# drop duplictes\ntrain_df = train_df.drop_duplicates(subset=['text'], keep='first')","d4e77567":"# new shape for train data\ntrain_df.shape, test_df.shape","eb70531f":"train_df.head(5)","81b0258a":"# check the distribution of the disaster and no-disaster tweets\ncount = train_df['target'].value_counts()\nsns.barplot(count.index, count)\ncount","faba134b":"# First 15 disaster tweets\nfor x in range(15):\n    ex = train_df[train_df['target'] == 0]['text'][0:15].tolist()\n    print(ex[x])","fe4d5f64":"# First 15 non-disaster tweets\nfor x in range(15):\n    ex = train_df[train_df['target'] == 1]['text'][0:15].tolist()\n    print(ex[x])","eedf7908":"# Cleaning the data and removing the stopwords\ndef Data_Cleaning(text):\n    text = text.lower()\n    text = re.sub(\"won\\'t\", \"will not\", text)\n    text = re.sub(\"can\\'t\", \"can not\", text)\n    text = re.sub(\"don\\'t\", \"do not\", text)\n    \n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ', text)\n    text = re.sub(r'&amp?;',' ', text)\n    text = re.sub(r'&lt;',' ', text)\n    text = re.sub(r'&gt;',' ', text)\n    \n    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', ' ', text)\n    text = re.sub(r'UTC', ' ', text)\n    text = re.sub(r'\\d{2}km', ' ', text)\n    text = re.sub(r\"\\b\\d+\\b\", \" \", text) # removing the numbers\n\n    text = re.sub(r\"#\",\"\",text) \n    text = re.sub(r\"(?:\\@)\\w+\", ' ', text)\n    text = re.sub(r'\\n', ' ', text)\n    \n    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n    text = re.sub(' +', ' ', text) # remove multiple spaces\n    \n    text = [word for word in word_tokenize(text) if not word in stopwords.words('english')]\n    text = ' '.join(text)\n\n    return text","5a9f53ac":"# apply the cleaning function to the dataset and creating a new column of the cleaned data\ntrain_df['cleaned'] = train_df['text'].apply(lambda x: Data_Cleaning(x))\ntest_df['cleaned'] = test_df['text'].apply(lambda x: Data_Cleaning(x))","d73e39dd":"train_df.tail(10)","c130a656":"test_df.head()","2c81cbff":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    masks = []\n    segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        masks.append(pad_masks)\n        segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(masks), np.array(segments)","327e5cc8":"# tokenizer from tokenization script\nF_tokenizer = tokenization.FullTokenizer","c8924f0e":"bert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1', trainable=True)","59b5333f":"to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nvocabulary = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\ntokenizer = F_tokenizer(vocabulary, to_lower_case)","1fae07e1":"def build_model(bert_layer, max_len=512):\n    \n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    segment_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"segment_ids\")\n\n    pooled_sequence, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    output = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs = output)\n    model.compile(Adam(lr=1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","f465353a":"df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\nmax_length = len(max(df.cleaned, key=len))\nmax_length","267693f0":"train_input = bert_encode(train_df.cleaned.values, tokenizer, max_len=140)\ntest_input = bert_encode(test_df.cleaned.values, tokenizer, max_len=140)\ntrain_labels = train_df['target'].values","a3053e5e":"model = build_model(bert_layer, max_len=140)\nmodel.summary()","21d2a488":"checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', save_best_only = True)\ntrain_history = model.fit(train_input, train_labels, validation_split = 0.25, epochs = 5, callbacks = [checkpoint], batch_size = 16)","c9826dad":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","7da97d80":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","ba27a532":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","254e4203":"it seems that we have 110 duplicated tweets based on text column","d495829e":"It seems like the maximum lentgh of the cleaned tweets is 138 therefore we are going to use max len of 140"}}