{"cell_type":{"7d9cdcb0":"code","a8249982":"code","bc7f415e":"code","c1c6a85d":"code","c34c5062":"code","e1ae5dbb":"code","64005f07":"code","af1e2ff1":"code","b4eeaf97":"code","6cc89923":"code","de755b80":"code","0d66ec5c":"code","af747f6e":"code","d68eaec4":"code","82c124a0":"code","47625ab3":"code","9e218d1c":"code","4de1f3c5":"code","976914e5":"code","1473570d":"code","f21fba71":"code","22be0ec7":"code","ce5a7729":"markdown","dc6e6f5b":"markdown","528ff5e1":"markdown","e7facf14":"markdown","0b2aaec9":"markdown","b831dc14":"markdown","f72be806":"markdown","934dd58f":"markdown","1b9c795d":"markdown","f37ca0a6":"markdown","98ab084c":"markdown","42be7a55":"markdown","2de6fdec":"markdown","de3171f2":"markdown","da708cb5":"markdown","47ba2691":"markdown","79385d2f":"markdown","6519a900":"markdown","0dbfaee7":"markdown","1656060b":"markdown"},"source":{"7d9cdcb0":"# IMPORTING LIBRARIES\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a8249982":"# IMPORTING DATASET\n\ntrain_df = pd.read_csv(r'..\/input\/mental-health-in-tech-survey\/survey.csv')\n\ntrain_df.head()","bc7f415e":"train_df.describe()","c1c6a85d":"train_df.info()","c34c5062":"train_df = train_df.drop(['comments'], axis= 1)\ntrain_df = train_df.drop(['state'], axis= 1)\ntrain_df = train_df.drop(['Timestamp'], axis= 1)\n\nprint(train_df.isnull().sum().max()) #just checking if there's no missing data missing...\ntrain_df.head(5)","e1ae5dbb":"#  Assign default values for each data type\n\ndefaultInt = 0\ndefaultString = 'NaN'\ndefaultFloat = 0.0\n\n# Create lists by data tpe\n\nintFeatures = ['Age']\n\nstringFeatures = ['Gender', 'Country', 'self_employed', 'family_history', 'treatment', 'work_interfere',\n                 'no_employees', 'remote_work', 'tech_company', 'anonymity', 'leave', 'mental_health_consequence',\n                 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview',\n                 'mental_vs_physical', 'obs_consequence', 'benefits', 'care_options', 'wellness_program',\n                 'seek_help']\n\nfloatFeatures = []\n\n# Clean the NaN's\nfor feature in train_df:\n    if feature in intFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultInt)\n    elif feature in stringFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultString)\n    elif feature in floatFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultFloat)\n    else:\n        print('Error: Feature %s not recognized.' % feature)\n        \ntrain_df.head()","64005f07":"#clean 'Gender'\n#lower case all columm's elements\ngender = train_df['Gender'].str.lower()\n\n#Select unique elements\ngender = train_df['Gender'].unique()\n\n#Made gender groups\nmale_str = [\"male\", \"m\", \"male-ish\", \"maile\", \"mal\", \"male (cis)\", \"make\", \"male \", \"man\",\"msle\", \"mail\", \"malr\",\"cis man\", \"Cis Male\", \"cis male\"]\n\ntrans_str = [\"trans-female\", \"something kinda male?\", \"queer\/she\/they\", \"non-binary\",\"nah\", \"all\", \"enby\", \"fluid\", \"genderqueer\", \"androgyne\", \"agender\", \"male leaning androgynous\", \"guy (-ish) ^_^\", \"trans woman\", \"neuter\", \"female (trans)\", \"queer\", \"ostensibly male, unsure what that really means\"]           \n\nfemale_str = [\"cis female\", \"f\", \"female\", \"woman\",  \"femake\", \"female \",\"cis-female\/femme\", \"female (cis)\", \"femail\"]\n\nfor (row, col) in train_df.iterrows():\n\n    if str.lower(col.Gender) in male_str:\n        train_df['Gender'].replace(to_replace=col.Gender, value='male', inplace=True)\n\n    if str.lower(col.Gender) in female_str:\n        train_df['Gender'].replace(to_replace=col.Gender, value='female', inplace=True)\n\n    if str.lower(col.Gender) in trans_str:\n        train_df['Gender'].replace(to_replace=col.Gender, value='trans', inplace=True)\n\n\nstk_list = ['A little about you', 'p']\ntrain_df = train_df[~train_df['Gender'].isin(stk_list)]\n\nprint(train_df['Gender'].unique())","af1e2ff1":"#complete missing age with mean\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\n\n# Fill with media() values < 18 and > 120\ns = pd.Series(train_df['Age'])\ns[s<18] = train_df['Age'].median()\ntrain_df['Age'] = s\ns = pd.Series(train_df['Age'])\ns[s>120] = train_df['Age'].median()\ntrain_df['Age'] = s\n\n#Ranges of Age\ntrain_df['age_range'] = pd.cut(train_df['Age'], [0,20,30,65,100], labels=[\"0-20\", \"21-30\", \"31-65\", \"66-100\"], include_lowest=True)\n#There are only 0.014% of self employed so let's change NaN to NOT self_employed\n\n#Replace \"NaN\" string from defaultString\ntrain_df['self_employed'] = train_df['self_employed'].replace([defaultString], 'No')\n\nprint(train_df['self_employed'].unique())","b4eeaf97":"#There are only 0.20% of self work_interfere so let's change NaN to \"Don't know\n#Replace \"NaN\" string from defaultString\n\ntrain_df['work_interfere'] = train_df['work_interfere'].replace([defaultString], 'Don\\'t know' )\nprint(train_df['work_interfere'].unique())","6cc89923":"from sklearn import preprocessing\n\nlabelDict = {}\nfor feature in train_df:\n    le = preprocessing.LabelEncoder()\n    le.fit(train_df[feature])\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    train_df[feature] = le.transform(train_df[feature])\n    # Get labels\n    labelKey = 'label_' + feature\n    labelValue = [*le_name_mapping]\n    labelDict[labelKey] =labelValue\n    \nfor key, value in labelDict.items():     \n    print(key, value)\n\n#Get rid of 'Country'\ntrain_df = train_df.drop(['Country'], axis= 1)\ntrain_df.head()","de755b80":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\nprint(missing_data)","0d66ec5c":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\nplt.show()\n\n#treatment correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'treatment')['treatment'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","af747f6e":"# Distribiution and density by Age\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df[\"Age\"], bins=24)\nplt.title(\"Distribuition and density by Age\")\nplt.xlabel(\"Age\")","d68eaec4":"# Separate by treatment or not\n\ng = sns.FacetGrid(train_df, col='treatment', size=5)\ng = g.map(sns.distplot, \"Age\")","82c124a0":"# Scaling Age\nfrom sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n\nscaler = MinMaxScaler()\ntrain_df['Age'] = scaler.fit_transform(train_df[['Age']])\ntrain_df.head()","47625ab3":"from sklearn.model_selection import train_test_split\n\n# define X and y\nfeature_cols = ['Age', 'Gender', 'family_history', 'benefits', 'care_options', 'anonymity', 'leave', 'work_interfere']\nX = train_df[feature_cols]\ny = train_df.treatment\n\n# split X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\n# Create dictionaries for final graph\n# Use: methodDict['Stacking'] = accuracy_score\nmethodDict = {}\nrmseDict = ()","9e218d1c":"from sklearn.ensemble import ExtraTreesClassifier","4de1f3c5":"# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\nlabels = []\nfor f in range(X.shape[1]):\n    labels.append(feature_cols[f])      \n    \n# Plot the feature importances of the forest\nplt.figure(figsize=(12,8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), labels, rotation='vertical')\nplt.xlim([-1, X.shape[1]])\nplt.show()","976914e5":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n    \ny_pred_class = logreg.predict(X_test)    \n\nprint(\"Training Accuracy: \",logreg.score(X_train,y_train))\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred_class,normalize=True))","1473570d":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=27, weights='uniform')\nknn.fit(X_train, y_train)\n\ny_pred_class = knn.predict(X_test)\n    \nprint(\"Training Accuracy: \",knn.score(X_train,y_train))\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred_class,normalize=True))","f21fba71":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth=3, min_samples_split=8, max_features=6, criterion='entropy', min_samples_leaf=7)\ntree.fit(X_train, y_train)\n\ny_pred_class = tree.predict(X_test)\n\nprint(\"Training Accuracy: \",tree.score(X_train,y_train))\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred_class,normalize=True))    ","22be0ec7":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(max_depth = None, min_samples_leaf=8, min_samples_split=2, n_estimators = 20, random_state = 1)\nforest.fit(X_train, y_train)\n    \ny_pred_class = forest.predict(X_test)\n\nprint(\"Training Accuracy: \",forest.score(X_train,y_train))\nprint(\"Test Accuracy: \",accuracy_score(y_test,y_pred_class,normalize=True))","ce5a7729":"##  Covariance Matrix. Variability comparison between categories of variables","dc6e6f5b":"# *This concludes our Research Project for Mental Issues and prediction and their treatment.*","528ff5e1":"## Data Cleaning","e7facf14":"### *1) Logistic Regression*","0b2aaec9":"## Importing Models","b831dc14":"# Topic: Mental Issues in Tech World","f72be806":"### *2) K - Nearest Neighbour*","934dd58f":"## Encoding Data","1b9c795d":"## Splitting Dataset","f37ca0a6":"## Evaluating Models","98ab084c":"## Scaling and fitting","42be7a55":"### Checking For Missing Data","2de6fdec":"## Data Visualization","de3171f2":"Features Scaling We're going to scale age, because is extremely different from the othere ones.","da708cb5":"### *3) Decision Tree*","47ba2691":"# Thank You !!","79385d2f":"Some charts to see data relationship","6519a900":"# *Pianalytix ML Research Project*\n\n## Team 7.1\n\n## Members:\n## *Yash Dalsaniya*\n## *Ramansh Sanghal*\n## *Divyansh Tiwari*\n## *Sujith Kethireddy*","0dbfaee7":"## Clearly, Random forest model has highest Test Accuracy and it can be used for our predictions.","1656060b":"### *4) Random Forest*"}}