{"cell_type":{"46d886a8":"code","784f4f06":"code","7ad8ae62":"code","c6efadbe":"code","0bcae895":"code","dd42ffbc":"code","380dfce9":"code","d316c274":"code","1a1b42cd":"code","9b384729":"code","39e4bc74":"code","0dd76de7":"code","21f44780":"code","e9a02fe6":"code","76595854":"code","388d9be0":"code","222cb31c":"code","4c22c416":"code","ad06a25b":"code","1a4d8fc1":"code","fd4f9bd9":"code","433d415a":"markdown","ec4d7493":"markdown","7e68415a":"markdown","9a38e358":"markdown","e2648494":"markdown","d3b07139":"markdown","e0e71d49":"markdown"},"source":{"46d886a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport pickle\nimport random\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","784f4f06":"# The pickle module implements binary protocols for serializing and de-serializing a Python object structure.\n\nwith open(\"\/kaggle\/input\/traffic-sign-classification\/train.p\", mode='rb') as training_data:\n    train = pickle.load(training_data)\nwith open(\"\/kaggle\/input\/traffic-sign-classification\/valid.p\", mode='rb') as validation_data:\n    valid = pickle.load(validation_data)\nwith open(\"\/kaggle\/input\/traffic-sign-classification\/test.p\", mode='rb') as testing_data:\n    test = pickle.load(testing_data)","7ad8ae62":"X_train, y_train = train['features'], train['labels']\nX_validation, y_validation = valid['features'], valid['labels']\nX_test,y_test= test['features'],test['labels']","c6efadbe":"X_train.shape","0bcae895":"y_train.shape","dd42ffbc":"i=np.random.randint(1,len(X_train))\nplt.imshow(X_train[i])\ny_train[i]","380dfce9":"# more images in a grid format\n# the dimensions of the plot grid \nW_grid = 10\nL_grid = 10\n\n# fig, axes = plt.subplots(L_grid, W_grid)\n# subplot return the figure object and axes object\n# we can use the axes object to plot specific figures at various locations\n\nfig, axes = plt.subplots(L_grid, W_grid, figsize = (10,10))\n\naxes = axes.ravel() # flaten the 5 x 5 matrix into 25 array\n\nn_training = len(X_train) # get the length of the training dataset\n\n# Select a random number from 0 to n_training\n#  evenly spaces variables\nfor i in np.arange(0,W_grid * L_grid):\n\n# Select a random number\n    index=np.random.randint(0,n_training)\n    \n\n# read and display an image with the selected index\n\n    axes[i].imshow(X_train[index])\n    axes[i].set_title(y_train[index],fontsize=15)\n    axes[i].axis('off')\nplt.subplots_adjust(hspace = 0.4)\n    \n    \n\n        ","d316c274":"# Shuffle the dataset \nfrom sklearn.utils import shuffle\nX_train,y_train = shuffle(X_train,y_train)","1a1b42cd":"X_train_gray = np.sum(X_train\/3, axis=3, keepdims=True)\nX_test_gray  = np.sum(X_test\/3, axis=3, keepdims=True)\nX_validation_gray  = np.sum(X_validation\/3, axis=3, keepdims=True)","9b384729":"X_train_gray_norm = (X_train_gray - 128)\/128 \nX_test_gray_norm = (X_test_gray - 128)\/128\nX_validation_gray_norm = (X_validation_gray - 128)\/128\n","39e4bc74":"i = random.randint(1, len(X_train_gray))\nplt.imshow(X_train_gray[i].squeeze(), cmap = 'gray')\nplt.figure()\nplt.imshow(X_train[i])\nplt.figure()\nplt.imshow(X_train_gray_norm[i].squeeze(), cmap = 'gray')","0dd76de7":"from tensorflow.keras import datasets, layers, models\nCNN =models.Sequential()\n\nCNN.add(layers.Conv2D(6, (5, 5), activation='relu', input_shape = (32,32,1)))\nCNN.add(layers.AveragePooling2D())\n\nCNN.add(layers.Conv2D(16, (5, 5), activation='relu'))\nCNN.add(layers.AveragePooling2D())\n\n\nCNN.add(layers.Dropout(0.2))\n\nCNN.add(layers.Flatten())\n\nCNN.add(layers.Dense(120, activation = 'relu'))\nCNN.add(layers.Dense(84, activation = 'relu'))\nCNN.add(layers.Dense(43, activation = 'softmax'))\nCNN.summary()","21f44780":"CNN.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","e9a02fe6":"history = CNN.fit(X_train_gray_norm,\n                  y_train,\n                  batch_size = 500,\n                  epochs = 50,\n                  verbose = 1,\n                  validation_data = (X_validation_gray_norm,y_validation))","76595854":"score = CNN.evaluate(X_test_gray_norm, y_test)\nprint('Test Accuracy: {}'.format(score[1]))","388d9be0":"history.history.keys()","222cb31c":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","4c22c416":"epochs = range(len(accuracy))\nplt.plot(epochs,loss,'ro',label = 'Traning loss')\nplt.plot(epochs,val_loss,'r',label = 'Validation loss')\nplt.title('Traning and Validation loss')","ad06a25b":"epochs = range(len(accuracy))\nplt.plot(epochs,accuracy,'ro',label = 'Traning accuarcy')\nplt.plot(epochs,val_accuracy,'r',label = 'Validation accuracy')\nplt.title('Traning and Validation accuracy')","1a4d8fc1":"predicted_classes = CNN.predict_classes(X_test_gray_norm)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, predicted_classes)\nplt.figure(figsize = (25, 25))\nsns.heatmap(cm, annot = True)","fd4f9bd9":"L = 5\nW = 5\n\nfig, axes = plt.subplots(L, W, figsize = (12, 12))\naxes = axes.ravel()\n\nfor i in np.arange(0, L*W):\n    axes[i].imshow(X_test[i])\n    axes[i].set_title('Prediction = {}\\n True = {}'.format(predicted_classes[i], y_true[i]))\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace = 1)    ","433d415a":"# ASSESS TRAINED CNN MODEL PERFORMANCE ","ec4d7493":"wee 34799 images with 32pixcel ,32 pixcel ,3","7e68415a":" # BUILD DEEP CONVOLUTIONAL NEURAL NETWORK MODEL","9a38e358":"# APPENDIX","e2648494":"# COMPILE AND TRAIN DEEP CNN MODEL","d3b07139":"- In this case study, we want to classify images of traffic signs using deep Convolutional Neural Networks (CNNs).\n- The dataset consists of 43 different classes of images. \n- Classes are as listed below: \n  - 0 = Speed limit (20km\/h) \n  - 1 = Speed limit (30km\/h)\n  - 2 = Speed limit (50km\/h) \n  - 3 = Speed limit (60km\/h)\n  - 4 = Speed limit (70km\/h) \n  - 5 = Speed limit (80km\/h)\n  - 6 = End of speed limit (80km\/h)\n  - 7 = Speed limit (100km\/h)\n  - 8 = Speed limit (120km\/h)\n  - 9 = No passing\n  - 10 = No passing for vehicles over 3.5 metric tons\n  - 11 = Right-of-way at the next intersection\n  - 12 = Priority road\n  - 13 = Yield\n  - 14 = Stop\n  - 15 = No vehicles\n  - 16 = Vehicles over 3.5 metric tons prohibited\n  - 17 = No entry\n  - 18 = General caution\n  - 19 = Dangerous curve to the left\n  - 20 = Dangerous curve to the right\n  - 21 = Double curve\n  - 22 = Bumpy road\n  - 23 = Slippery road\n  - 24 = Road narrows on the right\n  - 25 = Road work\n  - 26 = Traffic signals\n  - 27 = Pedestrians\n  - 28 = Children crossing \n  - 29 = Bicycles crossing\n  - 30 = Beware of ice\/snow\n  - 31 = Wild animals crossing\n  - 32 = End of all speed and passing limits\n  - 33 = Turn right ahead\n  - 34 = Turn left ahead\n  - 35 = Ahead only\n  - 36 = Go straight or right\n  - 37 = Go straight or left\n  - 38 = Keep right\n  - 39 = Keep left\n  - 40 = Roundabout mandatory\n  - 41 = End of no passing\n  - 42 = End of no passing by vehicles over 3.5 metric tons\n","e0e71d49":"# DATA PEPARATION "}}