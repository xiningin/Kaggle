{"cell_type":{"6e65cc9b":"code","d0aa2111":"code","1046d58c":"code","ef234b3a":"code","aa849127":"code","0ef5aa27":"code","1c145d99":"code","c9bd3897":"code","1668623c":"code","869049a5":"code","58198ce0":"code","827a8cac":"code","7e82b07d":"code","1d5a922c":"code","addd710f":"code","98360a5e":"code","817b8543":"code","0d7f9be9":"code","36d35e70":"code","bc42cd59":"code","60b10ae8":"code","cafbc3f8":"code","8e07a128":"code","ddacca71":"code","ce1f5727":"code","aa1b606b":"code","7ba13d0e":"code","ee0fa40e":"code","0dd4095c":"code","c9b59a42":"code","850d9ca0":"code","3f06856d":"code","9fd119c1":"code","9f25aff3":"code","f356a830":"code","1929546d":"code","0fe0dced":"code","9d9effe7":"code","eba4051a":"code","aef80c0c":"code","c9dfaa64":"code","0b491999":"code","80cec8d6":"code","5dfaba9d":"code","2c7f2fec":"code","8679186d":"code","6c0b4d9e":"code","f3c89993":"code","fa1bebb5":"code","e4186d09":"code","338ab9b4":"code","40b42c1e":"code","c0d8529e":"code","922fe1ec":"code","49012020":"code","f734d09a":"code","3d36c9bd":"code","32c4b663":"code","2157be18":"code","6db0e034":"code","57feb178":"code","864f0023":"code","ca29d13c":"code","35d36f85":"code","fc026897":"code","58bd40e6":"code","9424dcd7":"code","df38b5b7":"code","ad56d929":"code","dbb44fdd":"code","2df92170":"code","b15541fa":"code","b370e943":"markdown","2aff8cb3":"markdown","1ffbbc6f":"markdown","a98aa50c":"markdown","b031a50b":"markdown","4e657c6c":"markdown","07da86ea":"markdown","a99e350a":"markdown","8fc86bc2":"markdown","2493a41b":"markdown","45185a88":"markdown","7c6fa510":"markdown","b0a29e40":"markdown","42380a41":"markdown","92c28b33":"markdown","c3aa4677":"markdown"},"source":{"6e65cc9b":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import tree\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf","d0aa2111":"#age - \u0432\u043e\u0437\u0440\u0430\u0441\u0442\n#sex (1 = \u043c\u0443\u0436; 0 = \u0436\u0435\u043d)\n#cp - \u0442\u0438\u043f \u0433\u0440\u0443\u0434\u043d\u043e\u0439 \u0431\u043e\u043b\u0438\n#trestbps - \u043a\u0440\u043e\u0432\u044f\u043d\u043e\u0435 \u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432 \u043f\u043e\u043a\u043e\u0435(\u0432 \u043c\u043c \u0440\u0442.\u0441\u0442. \u043f\u0440\u0438 \u043f\u043e\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0438 \u0432 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440)\n#chol - \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0445\u043e\u043b\u0435\u0441\u0442\u0435\u0440\u0438\u043d\u0430 \u0432 \u043c\u0433\/\u0434\u043b\n#fbs  - (\u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0441\u0430\u0445\u0430\u0440\u0430 \u0432 \u043a\u0440\u043e\u0432\u0438 \u043d\u0430\u0442\u043e\u0449\u0430\u043a > 120 \u043c\u0433\/\u0434\u043b) (1 = true; 0 = false)\n#restecg - \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043a\u0430\u0440\u0434\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0432 \u043f\u043e\u043a\u043e\u0435\n#thalach - \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u0435\u0440\u0434\u0435\u0447\u043d\u044b\u0445 \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0439\n#exang -  \u0441\u0442\u0435\u043d\u043e\u043a\u0430\u0440\u0434\u0438\u044f(1 = yes; 0 = no)\n#oldpeak - \u0414\u0435\u043f\u0440\u0435\u0441\u0441\u0438\u044f ST, \u0432\u044b\u0437\u0432\u0430\u043d\u043d\u0430\u044f \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0443\u043f\u0440\u0430\u0436\u043d\u0435\u043d\u0438\u044f\u043c\u0438 \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u044e \u043f\u043e\u043a\u043e\u044f\n#slope - \u043d\u0430\u043a\u043b\u043e\u043d \u043f\u0438\u043a\u0430 \u0443\u043f\u0440\u0430\u0436\u043d\u0435\u043d\u0438\u0439 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430 ST\n#ca = \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0440\u0443\u043f\u043d\u044b\u0445 \u0441\u043e\u0441\u0443\u0434\u043e\u0432 (0-3), \u043e\u043a\u0440\u0430\u0448\u0435\u043d\u043d\u044b\u0445 \u043f\u043e \u0446\u0432\u0435\u0442\u0443\n#thal - 3 = \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e; 6 = \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0434\u0435\u0444\u0435\u043a\u0442; 7 = \u043e\u0431\u0440\u0430\u0442\u0438\u043c\u044b\u0439 \u0434\u0435\u0444\u0435\u043a\u0442\n#target 1 or 0","1046d58c":"df_0 = df.loc[df['target'] == 0]\ndf_1 = df.loc[df['target'] == 1]","ef234b3a":"df_01_mean = pd.DataFrame(df_0.mean()).T\ndf_01_mean.loc[1] = pd.DataFrame(df_1.mean()).T.loc[0] \ndf_01_mean #\u0441\u0440\u0435\u0434\u043d\u0435\u0435","aa849127":"df_01_range = pd.DataFrame(df_0.max() - df_0.min()).T \ndf_01_range.loc[1] = pd.DataFrame(df_1.max() - df_1.min()).T.loc[0] \ndf_01_range #\u0440\u0430\u0437\u043c\u0430\u0445","0ef5aa27":"df_01_std = pd.DataFrame(df_0.std()).T\ndf_01_std.loc[1] = pd.DataFrame(df_1.std()).T.loc[0] \ndf_01_std #\u0441\u0442\u0430\u043d\u0434 \u043e\u0442\u043a\u043b\u043e\u043d","1c145d99":"df_01_disp = pd.DataFrame(df_0.std()**2).T\ndf_01_disp.loc[1] = pd.DataFrame(df_1.std()**2).T.loc[0] \ndf_01_disp\n#\u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f","c9bd3897":"ej_0 = (abs(df_0 - df_0.mean()) > 3 * df_0.std()).sum() #\u043a\u043e\u043b-\u0432\u043e \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u043f\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u0443 \u0442\u0440\u0435\u0445 \u0441\u0438\u0433\u043c\nej_1 = (abs(df_1 - df_1.mean()) > 3 * df_1.std()).sum()\nej_01 = pd.DataFrame(ej_0).T\nej_01.loc[1] = pd.DataFrame(ej_1).T.loc[0]\nej_01\n#\u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0438\u043c\u0435\u044e\u0442 \u0435\u0434\u0438\u043d\u0438\u0447\u043d\u044b\u0439 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u043e\u0436\u043d\u043e \u0438\u0445 \u043d\u0435 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c","1668623c":"df.isnull().sum() #NULL - \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043d\u0435\u0442","869049a5":"df.boxplot(column=['age', 'trestbps', 'chol', 'thalach'], by='target', figsize=(30, 30))","58198ce0":"df.boxplot(column='oldpeak', by='target', figsize=(10, 7))","827a8cac":"df.boxplot(column='age', by='target', figsize=(10, 7))","7e82b07d":"df.boxplot(column='trestbps', by='target', figsize=(10, 7))","1d5a922c":"df.boxplot(column='chol', by='target', figsize=(10, 7))","addd710f":"df.boxplot(column='thalach', by='target', figsize=(10, 7))","98360a5e":"from scipy import stats\n[stats.shapiro(df['trestbps']), stats.shapiro(df['oldpeak']), stats.shapiro(df['chol']), stats.shapiro(df['thalach'])] \n#\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0435\u0442(p-value < 0.05), \u043d\u043e \u0442-\u0442\u0435\u0441\u0442 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c, \u0442. \u043a. \u043e\u0431\u044a\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0439\n","817b8543":"df['trestbps'].hist(density = \"True\")\nplt.xlabel('Trestbps')\ndf['trestbps'].plot.kde()","0d7f9be9":"df['oldpeak'].hist(density = \"True\")\nplt.xlabel('Oldpeak')\ndf['oldpeak'].plot.kde()","36d35e70":"df['chol'].hist(density = \"True\")\nplt.xlabel('Chol')\ndf['chol'].plot.kde()","bc42cd59":"df['thalach'].hist(density = \"True\")\nplt.xlabel('Thalach')\ndf['thalach'].plot.kde()","60b10ae8":"stats.mannwhitneyu(df_0['trestbps'],df_1['trestbps']) #\u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f","cafbc3f8":"stats.mannwhitneyu(df_0['chol'],df_1['chol']) #\u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f","8e07a128":"stats.mannwhitneyu(df_0['thalach'],df_1['thalach']) #\u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f","ddacca71":"stats.mannwhitneyu(df_0['oldpeak'],df_1['oldpeak']) #\u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f","ce1f5727":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(\"Classification Report:\", end='')\n        print(f\"\\tPrecision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tRecall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n        print(f\"\\t\\t\\tF1 score: {f1_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","aa1b606b":"# \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_X = scaler.fit_transform(X)","7ba13d0e":"Y = df[\"target\"] \nX = df.drop(\"target\", axis=1, inplace=False)","ee0fa40e":"X_train, X_test, y_train, y_test = train_test_split(scaled_X, Y, test_size=0.33, random_state=42)","0dd4095c":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train, y_train)","c9b59a42":"print_score(log_reg, X_train, y_train, X_test, y_test, train=True)\nprint_score(log_reg, X_train, y_train, X_test, y_test, train=False)","850d9ca0":"test_score = accuracy_score(y_test, log_reg.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, log_reg.predict(X_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","3f06856d":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)\n\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=False)","9fd119c1":"test_score = accuracy_score(y_test, knn_classifier.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn_classifier.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","9f25aff3":"from sklearn.svm import SVC\n\n\nsvm_model = SVC(kernel='rbf', gamma=0.1, C=1.0)\nsvm_model.fit(X_train, y_train)","f356a830":"print_score(svm_model, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_model, X_train, y_train, X_test, y_test, train=False)","1929546d":"test_score = accuracy_score(y_test, svm_model.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm_model.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","0fe0dced":"from sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=42)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","9d9effe7":"test_score = accuracy_score(y_test, tree.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","eba4051a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrand_forest = RandomForestClassifier(n_estimators=1000, random_state=42)\nrand_forest.fit(X_train, y_train)\n\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","aef80c0c":"test_score = accuracy_score(y_test, rand_forest.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rand_forest.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","c9dfaa64":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nprint_score(xgb, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb, X_train, y_train, X_test, y_test, train=False)","0b491999":"test_score = accuracy_score(y_test, xgb.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, xgb.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","80cec8d6":"from sklearn.model_selection import GridSearchCV\n\nparams = {\"C\": np.logspace(-4, 4, 20),\n          \"solver\": [\"liblinear\"]}\n\nlog_reg = LogisticRegression()\n\ngrid_search_cv = GridSearchCV(log_reg, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5, iid=True)\n#grid_search_cv.fit(X_train, y_train)","5dfaba9d":"#grid_search_cv.best_estimator_","2c7f2fec":"log_reg = LogisticRegression(C=4.281332398719396, \n                             solver='liblinear')\n\nlog_reg.fit(X_train, y_train)\n\nprint_score(log_reg, X_train, y_train, X_test, y_test, train=True)\nprint_score(log_reg, X_train, y_train, X_test, y_test, train=False)","8679186d":"test_score = accuracy_score(y_test, log_reg.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, log_reg.predict(X_train)) * 100\n\ntuning_results_df = pd.DataFrame(data=[[\"Tuned Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df","6c0b4d9e":"train_score = []\ntest_score = []\nneighbors = range(1, 21)\n\nfor k in neighbors:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    train_score.append(accuracy_score(y_train, model.predict(X_train)))\n    test_score.append(accuracy_score(y_test, model.predict(X_test)))","f3c89993":"plt.figure(figsize=(12, 8))\n\nplt.plot(neighbors, train_score, label=\"Train score\")\nplt.plot(neighbors, test_score, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_score)*100:.2f}%\")","fa1bebb5":"knn_classifier = KNeighborsClassifier(n_neighbors=19)\nknn_classifier.fit(X_train, y_train)\n\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_classifier, X_train, y_train, X_test, y_test, train=False)","e4186d09":"test_score = accuracy_score(y_test, knn_classifier.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn_classifier.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","338ab9b4":"svm_model = SVC(kernel='rbf', gamma=0.1, C=1.0)\n\nparams = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20), \n          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1), \n          \"kernel\":('linear', 'poly', 'rbf')}\n\nsvm_grid = GridSearchCV(svm_model, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")\n#svm_grid.fit(X_train, y_train)","40b42c1e":"#svm_grid.best_estimator_","c0d8529e":"svm_model = SVC(C=0.1, gamma=0.001, kernel='linear')\nsvm_model.fit(X_train, y_train)\n\nprint_score(svm_model, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_model, X_train, y_train, X_test, y_test, train=False)","922fe1ec":"test_score = accuracy_score(y_test, svm_model.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm_model.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","49012020":"params = {\"criterion\":(\"gini\", \"entropy\"), \n          \"splitter\":(\"best\", \"random\"), \n          \"max_depth\":(list(range(1, 20))), \n          \"min_samples_split\":[2, 3, 4], \n          \"min_samples_leaf\":list(range(1, 20))\n          }\n\ntree = DecisionTreeClassifier(random_state=42)\ngrid_search_cv = GridSearchCV(tree, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3, iid=True)\n#grid_search_cv.fit(X_train, y_train)","f734d09a":"#grid_search_cv.best_estimator_","3d36c9bd":"tree = DecisionTreeClassifier(criterion='gini', \n                              max_depth=6,\n                              min_samples_leaf=4, \n                              min_samples_split=2, \n                              splitter='random')\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)","32c4b663":"test_score = accuracy_score(y_test, tree.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","2157be18":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrand_forest = RandomForestClassifier(random_state=42)\n\nrf_random = RandomizedSearchCV(estimator=rand_forest, param_distributions=random_grid, n_iter=100, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\n\n\n#rf_random.fit(X_train, y_train)","6db0e034":"#rf_random.best_estimator_","57feb178":"rand_forest = RandomForestClassifier(bootstrap=True,\n                                     max_depth=10, \n                                     max_features='sqrt', \n                                     min_samples_leaf=4, \n                                     min_samples_split=2,\n                                     n_estimators=1600)\nrand_forest.fit(X_train, y_train)","864f0023":"print_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\nprint_score(rand_forest, X_train, y_train, X_test, y_test, train=False)","ca29d13c":"test_score = accuracy_score(y_test, rand_forest.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rand_forest.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","35d36f85":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree']\nbase_score = [0.25, 0.5, 0.75, 0.99]\nlearning_rate = [0.05, 0.1, 0.15, 0.20]\nmin_child_weight = [1, 2, 3, 4]\n\nhyperparameter_grid = {'n_estimators': n_estimators, 'max_depth': max_depth,\n                       'learning_rate' : learning_rate, 'min_child_weight' : min_child_weight, \n                       'booster' : booster, 'base_score' : base_score\n                      }\n\nxgb_model = XGBClassifier()\n\nxgb_cv = RandomizedSearchCV(estimator=xgb_model, param_distributions=hyperparameter_grid,\n                               cv=5, n_iter=650, scoring = 'accuracy',n_jobs =-1, iid=True,\n                               verbose=1, return_train_score = True, random_state=42)\n\n\n#xgb_cv.fit(X_train, y_train)","fc026897":"#xgb_cv.best_estimator_","58bd40e6":"xgb_best = XGBClassifier(base_score=0.5, \n                         booster='gbtree',\n                         learning_rate=0.05, \n                         max_depth=3,\n                         min_child_weight=2, \n                         n_estimators=500)\nxgb_best.fit(X_train, y_train)","9424dcd7":"print_score(xgb_best, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_best, X_train, y_train, X_test, y_test, train=False)","df38b5b7":"test_score = accuracy_score(y_test, xgb_best.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_best.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df\n","ad56d929":"rand_forest.feature_importances_","dbb44fdd":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","2df92170":"feature_imp(X, rand_forest).plot(kind='barh', figsize=(12,7), legend=False)","b15541fa":"feature_imp(X, xgb_best).plot(kind='barh', figsize=(12,7), legend=False)","b370e943":"# 6. XGBoost Classifer","2aff8cb3":"# \u0421\u0442\u0440\u043e\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c","1ffbbc6f":"# 3. Support Vector Machine Hyperparameter Tuning","a98aa50c":"# 5. Random Forest","b031a50b":"# 2. K-nearest neighbors Hyperparameter Tuning","4e657c6c":"# Features Importance According to Random Forest and XGBoost","07da86ea":"# 2. K-nearest neighbors","a99e350a":"# 4. Decision Tree Classifier","8fc86bc2":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u043b\u0435\u0443\u0434\u044e\u0449\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438:\n\n1. Logistic Regression\n2. K-Nearest Neighbours Classifier\n3. Support Vector machine\n4. Decision Tree Classifier\n5. Random Forest Classifier\n6. XGBoost Classifier","2493a41b":"# 1. Logistic Regression Hyperparameter Tuning","45185a88":"# 3. Support Vector machine","7c6fa510":"# 4. Decision Tree Classifier Hyperparameter Tuning","b0a29e40":"# 1. Logistic Regression","42380a41":"# \u041d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u043c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b","92c28b33":"# 6. XGBoost Classifier Hyperparameter Tuning","c3aa4677":"# 5. Random Forest Classifier Hyperparameter Tuning"}}