{"cell_type":{"220aec9b":"code","52e3042a":"code","0a67a57d":"code","cb630c0c":"code","0c547366":"code","d943f7dd":"code","c1d5388b":"code","8e352d19":"code","471d6bac":"code","7d7d0cbb":"code","f05968a0":"code","d47b40e9":"code","f9315d23":"code","e28bc5fc":"code","388ff952":"code","80280af3":"markdown","d5fb0ce9":"markdown","02eb46bb":"markdown","e671e880":"markdown","011c4742":"markdown","781eb1f0":"markdown","e5835c30":"markdown","12d574db":"markdown","7d1e61da":"markdown"},"source":{"220aec9b":"import os\nimport cv2\nimport ast\nimport json\nimport subprocess\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom pprint import pprint\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Video","52e3042a":"# Root of input\nINPUT_PATH = '..\/input\/tensorflow-great-barrier-reef'\nHEIGHT = 720 # image height\nWIDTH  = 1280 # image width","0a67a57d":"df_train = pd.read_csv(INPUT_PATH + '\/train.csv')\ndisplay(df_train)\nprint(df_train.info())","cb630c0c":"for video_id in df_train['video_id'].unique():\n    print(f'video_id: {video_id}')\n    print(f'w   annotations:  {sum(df_train[df_train[\"video_id\"]==video_id][\"annotations\"] == \"[]\")}')\n    print(f'w\/o annotations:  {sum(df_train[df_train[\"video_id\"]==video_id][\"annotations\"] != \"[]\")}\\n')","0c547366":"# \u5c06'annotations'\u7684\u7c7b\u578b\u4ecestr\u66f4\u6539\u4e3alist\ndf_train['annotations'] = df_train['annotations'].apply(ast.literal_eval) # str -> list\n# \u6dfb\u52a0\u5217\u7684\u56fe\u50cf\u8def\u5f84\u548c\u6570\u91cf\u7684\u76d2\u5b50\ndf_train['image_path'] = INPUT_PATH + '\/train_images\/video_' + df_train['video_id'].astype(str) + '\/' + df_train['video_frame'].astype(str) + \".jpg\"\ndf_train['num_bboxes'] = df_train['annotations'].apply(lambda x: len(x))\ndisplay(df_train)","d943f7dd":"max_num_bboxes = max(df_train['num_bboxes'])\nindexes = df_train[df_train['num_bboxes']==max_num_bboxes].index.values\nprint(f'Maximum number of bboxes in an image: {max_num_bboxes}')\ndisplay(df_train.iloc[indexes])","c1d5388b":"# indexes[0] \u548c indexes[1] \u662f\u8fde\u7eed\u7684\u5e27\nindexes = [indexes[0], indexes[2]]","8e352d19":"def get_bboxes(annotations):\n    \"\"\"\n    annotations: list of annotations\n    return: bboxes as [x_min, y_min, x_max, y_max]\n    \"\"\"\n    if len(annotations)==0:\n        return []\n    boxes = pd.DataFrame(annotations, columns=['x', 'y', 'width', 'height']).astype(np.int32).values\n    # [x_min, y_min, w, h] -> [x_min, y_min, x_max, y_max]\n    boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n    boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n    return boxes   \n\ndef plot_img_and_bbox(img_path, anntations):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    fig, ax = plt.subplots(1, 1, figsize=(16,10))\n    if len(annotations)>0:\n        bboxes = get_bboxes(annotations)\n        for i, box in enumerate(bboxes):\n            # pur bbox on image\n            cv2.rectangle(img,\n                          (box[0], box[1]),\n                          (box[2], box[3]),\n                          color = (255, 0, 0),\n                          thickness = 2)\n            # numbering\n            ax.text(box[0], box[1]-5, i+1, color='red')\n\n    ax.set_axis_off()\n    ax.imshow(img)\n\n\ndef zoom_bbox(img_path, annotations):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    bboxes = get_bboxes(annotations)\n    \n    col = 6\n    row = np.ceil(len(bboxes)\/\/6).astype(int)\n    fig, ax = plt.subplots(row, col, figsize=(16,9))\n    cnt = 0\n    for i in range(row):\n        if cnt >= len(bboxes):\n            break\n        for j in range(col):\n            bbox = bboxes[cnt]\n            sliced_img = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n            ax[i,j].imshow(sliced_img)\n            ax[i,j].set_title(cnt+1, color='red')\n            ax[i,j].set_axis_off()\n            cnt += 1\n    plt.show() ","471d6bac":"samples = df_train.iloc[indexes].copy()\nfor idx, row in samples.iterrows():\n    img_path    = row['image_path']\n    annotations = row['annotations']\n    print('image_id:', row['image_id'])\n    # plot image with bboxes\n    plot_img_and_bbox(img_path, annotations)\n    # plot zoom of bboxes\n    zoom_bbox(img_path, annotations)","7d7d0cbb":"def get_img_with_annotations(img_path, annotations):\n    img = cv2.imread(img_path)\n    video_id = img_path.split('\/')[-2].split('_')[-1]\n    frame_id = img_path.split('\/')[-1].split('.')[0]\n    img_id = video_id + '-' + frame_id\n    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if len(annotations)>0:\n        bboxes = get_bboxes(annotations)\n        for i, box in enumerate(bboxes):\n            # put bbox\n            cv2.rectangle(img,\n                          (box[0], box[1]),\n                          (box[2], box[3]),\n                          color = (0, 0, 255),\n                          thickness = 2)\n    # put image_id, #bbox\n    cv2.putText(img,\n                f'image_id: {img_id}, #bbox: {len(annotations)}',\n                org = (30, 50), \n                color = (0, 0, 255), \n                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=1.0,\n                thickness=3)\n    \n    return img\n\ndef make_video(df, video_id, start_frame, end_frame, fps=15, width=WIDTH, height=HEIGHT):\n    '''\n    df          : DataFrame\n    video_id    : 0, 1, or 2\n    start_frame : video_frame at start of video\n    num_frame   : video_frame at end of video\n    return      : path to video\n    '''\n    video_path = f'video_{video_id}_{start_frame}_to_{end_frame}.mp4' # video after encode\n    tmp_path = 'tmp_' + video_path # video before encode (removed after encode)\n    video = cv2.VideoWriter(tmp_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n    \n    df = df[df['video_id']==video_id].reset_index(drop=True)\n    start_idx = df[df['video_frame']==start_frame].index[0]\n    end_idx   = df[df['video_frame']==end_frame].index[0]\n    df = df.iloc[start_idx:end_idx]\n    for idx, row in tqdm(df.iterrows(), total=len(df)):\n        image_path  = row['image_path']\n        annotations = row['annotations']\n        frame = get_img_with_annotations(image_path, annotations)\n        video.write(frame)\n    \n    video.release()\n    \n    if os.path.exists(video_path):\n        os.remove(video_path)\n    \n    # encode by ffmpeg command \n    subprocess.run(\n        ['ffmpeg', \n         '-i', tmp_path, \n         '-loglevel', 'quiet', \n         '-crf', '18', \n         '-preset', 'veryfast', \n         '-vcodec', 'libx264', \n         video_path]\n    )\n    os.remove(tmp_path)\n    \n    return video_path","f05968a0":"video_paths = []\nfor idx in indexes:\n    video_id    = df_train.loc[idx, 'video_id']\n    start_frame = df_train.loc[idx, 'video_frame'] - 100 # peek before 100 frames\n    end_frame   = df_train.loc[idx, 'video_frame'] + 200 # peek after 200 frames\n    print(f'video_id: {video_id}, video_frame: {start_frame} to {end_frame}')\n    print('Create video ...')\n    video_path = make_video(df_train,\n                            video_id=video_id,\n                            start_frame=start_frame,\n                            end_frame=end_frame)\n    video_paths.append(video_path)","d47b40e9":"Video(video_paths[0], width=WIDTH*0.7, height=HEIGHT*0.7)","f9315d23":"img_ids = ['1-9071', '1-9072']\nfig, ax = plt.subplots(1, 2, figsize=(20,10))\nfor i, img_id in enumerate(img_ids):\n    img_path    = df_train[df_train['image_id'].str.contains(img_id)]['image_path'].values[0]\n    annotations = df_train[df_train['image_id'].str.contains(img_id)]['annotations'].values[0]\n    img = get_img_with_annotations(img_path, annotations)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[i].imshow(img)\n    ax[i].set_axis_off()\nplt.show()","e28bc5fc":"Video(video_paths[1], width=WIDTH*0.7, height=HEIGHT*0.7)","388ff952":"img_ids = ['2-5715', '2-5721']\nfig, ax = plt.subplots(1, 2, figsize=(20,10))\nfor i, img_id in enumerate(img_ids):\n    img_path    = df_train[df_train['image_id'].str.contains(img_id)]['image_path'].values[0]\n    annotations = df_train[df_train['image_id'].str.contains(img_id)]['annotations'].values[0]\n    img = get_img_with_annotations(img_path, annotations)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[i].imshow(img)\n    ax[i].set_axis_off()\nplt.show()","80280af3":"\u7c7b\u4f3c\u5730\uff0c\u5728id=2-5715\u548c5721\u4e4b\u95f4\uff0cbbox\u7684\u6570\u91cf\u4ece5\u4e2a\u66f4\u6539\u4e3a8\u4e2a","d5fb0ce9":"### \u7b2c\u4e8c\u4e2a\u89c6\u9891","02eb46bb":"### \u53c2\u6570\u8bbe\u7f6e","e671e880":"### \u7b2c\u4e00\u4e2a\u89c6\u9891","011c4742":"<span style=\"font-size: 120%;\">The change from id=1-9071 to 9072 (around at 3 sec in this video) is small but the number of bboxes jumps up from 4 to 7 as shown below, so some starfishes are not annotated in id=1-9071. <\/span>","781eb1f0":"# \u5236\u4f5c\u89c6\u9891\n\u751f\u6210300\u5e27\u89c6\u9891\u56f4\u7ed5\u56fe\u50cf\u4e0e\u6700\u5927\u6570\u91cf\u7684bboxes\u3002\n","e5835c30":"# \u8f93\u5165\u6570\u636e","12d574db":"# \u5bfc\u5165\u7b2c\u4e09\u65b9\u5e93","7d1e61da":"# \u8303\u4f8b\u56fe\u7247"}}