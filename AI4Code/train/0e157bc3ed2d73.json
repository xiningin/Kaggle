{"cell_type":{"cf621a76":"code","57175801":"code","6157d145":"code","1ed97bfd":"code","7beda1d0":"code","1823ef9f":"code","fcefcd87":"code","8992a795":"code","33df9e7c":"code","ff0eceb1":"code","573af829":"code","08bdad2b":"code","597fbc03":"code","890d519b":"code","cc7e66e3":"code","96123c42":"code","8c52c2fa":"code","7409a1af":"code","0e058a91":"code","f6a4056a":"code","38f9d342":"code","06ed01d1":"code","ec841fb9":"code","9839dd8c":"code","185c119d":"code","c9de706b":"code","776e7ad6":"markdown","0bc426dd":"markdown","286bebde":"markdown","92c1e1e8":"markdown","5c00a3e9":"markdown","f1588625":"markdown","e13da52d":"markdown","9923ab70":"markdown","b2b7daef":"markdown","540c13a3":"markdown","fca80b44":"markdown","bcc9727b":"markdown","81715b1e":"markdown","7ed9dd41":"markdown","697d0846":"markdown","cbc85502":"markdown","9975bece":"markdown","214e6bdc":"markdown","f77f0d4a":"markdown","9ad33bed":"markdown","841048f4":"markdown","50be4036":"markdown","71215fb7":"markdown","6b28796a":"markdown","e9d71130":"markdown","4d21e59b":"markdown","7024c9c9":"markdown","d73cb433":"markdown","64a8b3ab":"markdown","ef0cc319":"markdown"},"source":{"cf621a76":"# !pip install tensorflow==2.2.0","57175801":"import pandas as pd\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n","6157d145":"!pip list","1ed97bfd":"directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/annotations\"\nOC_Net = cv2.dnn.readNetFromCaffe('..\/input\/caffe-face-detector-opencv-pretrained-model\/architecture.txt','..\/input\/caffe-face-detector-opencv-pretrained-model\/weights.caffemodel')\ndf = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/submission.csv\")\nimage_directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\"\nimage_test_directory = \"...\/input\/pictest1\/\"\n# 1. Importing face-mask-detection-dataset dataset, The Annotations field contains the data of all the faces present in a particular image. \ndf.head(10)","7beda1d0":"data = []\nimg_size = 124\nmask_describe = ['face_with_mask',\"face_no_mask\"]\nlabels={'mask':0,'without mask':1}\ntype_mask = []\nfileste= \".json\"\n\ndef get_bounding(j,i,data,labels_item):\n    x,y,w,h = j[\"BoundingBox\"]\n    img = cv2.imread(os.path.join(image_directory,i),1)\n    img = img[y:h,x:w]\n    img = cv2.resize(img,(img_size,img_size))\n    data.append([img,labels_item])\n    \ndef get_json_file(PathName):\n    with open(PathName,'r') as f:\n        return json.load(f)\n\nfor i in df[\"name\"].unique():\n    f = i+fileste\n    for j in get_json_file(os.path.join(directory,f)).get(\"Annotations\"):\n        if j[\"classname\"] in mask_describe[0]:\n            get_bounding(j,i,data,labels['mask'])\n            type_mask.append(labels['mask'])\n            \n        if j[\"classname\"] in mask_describe[1]:\n            get_bounding(j,i,data,labels['without mask'])\n            type_mask.append(labels['without mask'])\n\nrandom.shuffle(data)  ","1823ef9f":"# show mask and nomask\nsns.countplot(type_mask)","fcefcd87":"features = []\nlabels = []\nfor f,l in data:\n    features.append(f)\n    labels.append(l)","8992a795":"features[0].shape","33df9e7c":"features = np.array(features)\/255.0\nfeatures = features.reshape(-1,img_size,img_size,3)\nlabels = np.array(labels)\n\nprint(np.unique(labels))\nprint(features.shape)","ff0eceb1":"model=Sequential()\nmodel.add(Conv2D(32,(3,3),input_shape=(img_size,img_size,3),activation='relu',strides=2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64,(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(128,(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","573af829":"from tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\n\nfrom keras.applications.mobilenet_v2 import MobileNetV2\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\n\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.vgg19 import preprocess_input\n\n\nbase_model_vgg16 = VGG16(\n    weights = 'imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(124, 124, 3),\n    include_top=False) \n\nfor layer in base_model_vgg16.layers:\n    layer.trainable = False\n    \nmodel_vgg16 = Sequential()\nmodel_vgg16.add(base_model_vgg16)\nmodel_vgg16.add(Flatten())\nmodel_vgg16.add(Dense(1,activation='sigmoid'))\nmodel_vgg16.summary()\n\nbase_model_vgg19 = VGG19(\n   weights = 'imagenet',  # Load weights pre-trained on ImageNet.\n   input_shape=(124, 124, 3),\n   include_top=False)\n\nfor layer in base_model_vgg19.layers:\n    layer.trainable = False\n    \nmodel_vgg19 = Sequential()\nmodel_vgg19.add(base_model_vgg19)\nmodel_vgg19.add(Flatten())\nmodel_vgg19.add(Dense(1,activation='sigmoid'))\nmodel_vgg19.summary()\n\n\n\n# base_model_resnet50 = ResNet50(\n#     weights = 'imagenet',  # Load weights pre-trained on ImageNet.\n#     input_shape=(124, 124, 3),\n#     include_top=False) \n\n\n# for layer in base_model_resnet50.layers:\n#     layer.trainable = False\n    \n# model_resnet50 = Sequential()\n# model_resnet50.add(base_model_resnet50)\n# model_resnet50.add(Flatten())\n# model_resnet50.add(Dense(1,activation='sigmoid'))\n# model_resnet50.summary()\n\n\n\nbase_model_mobile = MobileNetV2(weights='imagenet',include_top=False,input_shape=(124,124,3))\n\nfor layer in base_model_mobile.layers:\n    layer.trainable = False\n    \nmodel_mobile = Sequential()\nmodel_mobile.add(base_model_mobile)\nmodel_mobile.add(Flatten())\nmodel_mobile.add(Dense(1,activation='sigmoid'))\nmodel_mobile.summary()","08bdad2b":"model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\ntrain_f,val_f,train_l,val_l=train_test_split(features, labels,train_size=0.8,random_state=0)\ndatagen = ImageDataGenerator(\n        rotation_range= 17,    \n        width_shift_range=0.1,\n        height_shift_range=0.1,  \n        #zca_epsilon = 1e-05, \n        horizontal_flip=True, \n        #rescale=1.\/255,\n        shear_range=0.2,\n        #zoom_range=0.1,\n        #brightness_range = [0.1, 10], \n)\ndatagen.fit(train_f)","597fbc03":"history_list = []\nhistory = model.fit_generator(datagen.flow(train_f, train_l, batch_size=32),\n                    steps_per_epoch=train_f.shape[0]\/\/32,\n                    epochs=50,\n                    verbose=1,\n                    validation_data=(val_f, val_l))\nhistory_list.append(history)","890d519b":"model_vgg19.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n\nhistory_vgg19 = model_vgg19.fit_generator(datagen.flow(train_f, train_l, batch_size=32),\n                   steps_per_epoch=train_f.shape[0]\/\/32,\n                   epochs=50,\n                   verbose=1,\n                   validation_data=(val_f, val_l))\nhistory_list.append(history_vgg19)","cc7e66e3":"model_vgg16.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n\nhistory_vgg16 = model_vgg16.fit_generator(datagen.flow(train_f, train_l, batch_size=32),\n                   steps_per_epoch=train_f.shape[0]\/\/32,\n                   epochs=50,\n                   verbose=1,\n                   validation_data=(val_f, val_l))\n\nhistory_list.append(history_vgg16)","96123c42":"# model_resnet50.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n\n# history_resnet50 = model_resnet50.fit_generator(datagen.flow(train_f, train_l, batch_size=32),\n#                    steps_per_epoch=train_f.shape[0]\/\/32,\n#                    epochs=50,\n#                    verbose=1,\n#                    validation_data=(val_f, val_l))\n\n# history_list.append(history_resnet50)","8c52c2fa":"model_mobile.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n\nhistory_mobile = model_mobile.fit_generator(datagen.flow(train_f, train_l, batch_size=32),\n                   steps_per_epoch=train_f.shape[0]\/\/32,\n                   epochs=50,\n                   verbose=1,\n                   validation_data=(val_f, val_l))\nhistory_list.append(history_mobile)","7409a1af":"# from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n\n# model.evaluate(val_f, val_l)\n# prediction = model.predict_classes(val_f)\n# # classes_x=np.argmax(predict_x,axis=1)\n# print(classification_report(val_l, prediction))\n# print(confusion_matrix(val_l, prediction))\n\n# model_vgg19.evaluate(val_f, val_l)\n# prediction_vgg19 = model_vgg19.predict_classes(val_f)\n# print(classification_report(val_l, prediction_vgg19))\n# print(confusion_matrix(val_l, prediction_vgg19))\n\n# model_vgg16.evaluate(val_f, val_l)\n# prediction_vgg16 = model_vgg16.predict_classes(val_f)\n# print(classification_report(val_l, prediction_vgg16))\n# print(confusion_matrix(val_l, prediction_vgg16))\n\n\n\n# # model_resnet50.evaluate(val_f, val_l)\n# # prediction_resnet50 = model_resnet50.predict_classes(val_f)\n# # print(classification_report(val_l, prediction_resnet50))\n# # print(confusion_matrix(val_l, prediction_resnet50))\n\n\n\n# model_mobile.evaluate(val_f, val_l)\n# prediction_mobile = model_mobile.predict_classes(val_f)\n# print(classification_report(val_l, prediction_mobile))\n# print(confusion_matrix(val_l, prediction_mobile))\n\n","0e058a91":"model.evaluate(val_f, val_l)\n","f6a4056a":"model.save('model-net.h5')\n\n# model_resnet50.save('model_resnet50-net.h5')\nmodel_vgg19.save('vgg19-mask-net.h5')\nmodel_vgg16.save('vgg16-mask-net.h5')\nmodel_mobile.save('model_mobile-net.h5')","38f9d342":"import numpy\nimport pandas \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#1 \nfig,ax = plt.subplots(figsize=(22,28))\n\nprint(ax)\n#2 \nax1 = fig.add_subplot(5,2,1) \nax2 = fig.add_subplot(5,2,2)\nax3 = fig.add_subplot(5,2,3)\nax4 = fig.add_subplot(5,2,4)\n\nax5 = fig.add_subplot(5,2,5)\nax6 = fig.add_subplot(5,2,6)\nax7 = fig.add_subplot(5,2,7)\nax8 = fig.add_subplot(5,2,8)\n\n\n\n#3\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nax1.set_title('Custom Net Training accuracy vs Validation accuracy')\nax1.set_ylabel('accuracy')\nax1.set_xlabel('Epoch')\nax1.legend(['Train', 'Validation'], loc='upper left')\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nax2.set_title('Training Loss vs Validation Loss')\nax2.set_ylabel('Loss')\nax2.set_xlabel('Epoch')\nax2.legend(['Train', 'Validation'], loc='upper left')\n\nax3.plot(history_vgg19.history['accuracy'])\nax3.plot(history_vgg19.history['val_accuracy'])\nax3.set_title('vgg19 Training accuracy vs Validation accuracy')\nax3.set_ylabel('accuracy')\nax3.set_xlabel('Epoch')\nax3.legend(['Train', 'Validation'], loc='upper left')\n\nax4.plot(history_vgg19.history['loss'])\nax4.plot(history_vgg19.history['val_loss'])\nax4.set_title('vgg19 Training Loss vs Validation Loss')\nax4.set_ylabel('Loss')\nax4.set_xlabel('Epoch')\nax4.legend(['Train', 'Validation'], loc='upper left')\n\n\nax5.plot(history_vgg16.history['accuracy'])\nax5.plot(history_vgg16.history['val_accuracy'])\nax5.set_title('vgg16 Training accuracy vs Validation accuracy')\nax5.set_ylabel('accuracy')\nax5.set_xlabel('Epoch')\nax5.legend(['Train', 'Validation'], loc='upper left')\n\nax6.plot(history_vgg16.history['loss'])\nax6.plot(history_vgg16.history['val_loss'])\nax6.set_title('vgg16 Training Loss vs Validation Loss')\nax6.set_ylabel('Loss')\nax6.set_xlabel('Epoch')\nax6.legend(['Train', 'Validation'], loc='upper left')\n\nax7.plot(history_mobile.history['accuracy'])\nax7.plot(history_mobile.history['val_accuracy'])\nax7.set_title('mobileNet Training accuracy vs Validation accuracy')\nax7.set_ylabel('accuracy')\nax7.set_xlabel('Epoch')\nax7.legend(['Train', 'Validation'], loc='upper left')\n\nax8.plot(history_mobile.history['loss'])\nax8.plot(history_mobile.history['val_loss'])\nax8.set_title('mobileNet Training Loss vs Validation Loss')\nax8.set_ylabel('Loss')\nax8.set_xlabel('Epoch')\nax8.legend(['Train', 'Validation'], loc='upper left')","06ed01d1":"test_img = [\"0078.png\",\"0002.png\",\"0091.jpg\",\"1797.jpg\"]\nimage_test_directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\/\"\n\n#image_test_directory = \"..\/input\/maskedfaceteest\"\n\n#test = [\"me-masked-test.png\",\"me-test.png\"]","ec841fb9":"def extract_face(face):\n\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    return face_array\n\ndef get_label(frame,img_size):\n    im = cv2.resize(frame,(img_size,img_size))\n    im = np.array(im)\/255.0\n    im = im.reshape(1,img_size,img_size,3)\n    result = model.predict(im)\n    label_Y=1 if result>0.3 else 0\n    return label_Y,result","9839dd8c":"# adjust image darker and light \nadjust_light,rows,cols = 2.2,3,2\nfigure= plt.figure(figsize = (14,14))\naxes = []\nassign = {'0':'Mask','1':\"No Mask\"}\nface_list = [] # save no mask\n\nfor j,im in enumerate(test_img):\n    image =  cv2.imread(os.path.join(image_test_directory,im),1)\n    back_image = image\n    image = cv2.LUT(image.astype(np.uint8), np.array([((i \/ 255.0) ** (1.0 \/ adjust_light)) * 255 for i in np.arange(0, 256)]).astype(np.uint8))\n    blob = cv2.dnn.blobFromImage(image=cv2.resize(image, (300,300)), scalefactor=1.0, size=(300, 300), mean=(104.0, 177.0, 123.0))\n    \n    OC_Net.setInput(blob)\n    detections = OC_Net.forward()\n    length = detections.shape[2]\n    (hight, width) = image.shape[:2]\n    for i in range(0, length):\n        # get the face box\n        boxs = detections[0, 0, i, 3:7] * np.array([width, hight, width, hight])\n        (startX, startY, endX, endY) = boxs.astype(\"int\")\n        frame = image[startY:endY, startX:endX]\n        \n        # according to the confidence_threshold, to get the lable\n        confidence_threshold = detections[0, 0, i, 2]\n        if confidence_threshold > 0.2:\n            label_Y,result = get_label(frame,img_size)\n            print(label_Y,result)\n           \n            # add the Text\n            cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n            cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n            \n            ## save nomask people mesage\n            if(label_Y==1):\n                print(\"1\")\n#                     plt.imshow(back_image)\n                face_list.append(im)\n                print(im)\n\n    axes.append(figure.add_subplot(rows, cols, j+1))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n","185c119d":"print(face_list)\n","c9de706b":"\nfor im in face_list:\n    image =  cv2.imread(os.path.join(image_test_directory,im),1)\n    plt.imshow(image)\n    plt.show()\n   ","776e7ad6":"### summary\nAccording to the variation of Accuracy with Epoch on the left side of CMNET Picture, from the first graph, we can see that the curve of Train slowly tends to 0.98 as Epoch increases, while Validation is similar to Train, but with some fluctuations, it tends to about 0.97. From the right picture, we can verify that the curve has the same trend as the Train curve, but with some fluctuations, and finally tends to about 0.1 and 0.15.\n\nSecond Line correspond to the variation of Accuracy with Epoch and Loss with Epoch for the Vgg19 and Vgg16 models. the overall trends of Vgg19 and Vgg16 are the same as CMNET, but the fluctuations in their plots are more obvious. And their accuracies eventually converge to 0.95 (Vgg19 Train), 0.94 (Vgg19 Validation), 0.96 (Vgg16 Train), and 0.96 (Vgg19 Validation), respectively, and in the loss plots Vgg19 is 0.1 (Train), 0.15 ( Validation). In the loss plot Vgg16 is 0.09 (Train), 0.12 (Validation), respectively.\n\n\nFinal line shows the plot of MobileNet, whose trend of Accuracy is consistent with the trend of the other plots, and finally tends to 0.94 (Train). The trend of Loss, Train's curve is relatively flat, always tending to 0.25, but the corresponding Validation curve has been fluctuating, and the curve fluctuates a lot, with a gradually increasing trend, always fluctuating around 1.5.The Loss curve of Validation becomes larger slowly and the Loss curve of Train decreases slowly, indicating a possible overfitting.\n\n\nThe CMNET network has a simple structure and the number of layers is much less than VGG19, VGG16 and MobileNetV2, but CMNET has the highest accuracy rate. In general, as the number of layers deepens, the expressiveness and abstraction ability of the network will also improve, and the deepening of the neural network may bring some negative problems, as described below.\n- Gradient disappearance and gradient explosion problems: often due to deep network layers and inappropriate activation function selection (e.g., sigmoid function). Gradient explosion also occurs because the network level is too deep, or the initialization value of the weights is too large.\n- Overfitting problem: As the number of layers deepens and the parameters become more, the fitting ability of the neural network becomes very strong, which means that the expressed function will be more complex, and if the function is too complex for simple problems such as dichotomous problems, it is very easy to cause overfitting.\n- Degradation problem: For CNN, each layer will produce a similar effect of lossy compression after passing the convolution kernel, and if convolution is done many times, it will inevitably lead to a high degree of abstraction and loss of information, which eventually leads to a larger training error and eventually leads to degradation.\nSince MobileNetV2 introduces the residual structure, MobileNetV2 does not have a degradation problem. At the same time, there is no gradient disappearance or gradient explosion, which leads to the failure of normal training, so it is not a gradient disappearance or gradient explosion problem either. So for simple classification problem, CMNET (simple network structure) is compared with complex network structure, but CMNET works better, and the reason may be overfitting. \n","0bc426dd":"\n## 1. Importing Libraries\nImporting face-mask-detection-dataset dataset, The Annotations field contains the data of all the faces present in a particular image. Pass the json file, get the mask labels and non-mask labels ( face_with_mask and face_no_mask), and obtains the boundary data. According to the statistical label data, the following figure can be obtained. The blue color indicates the data with mask, and the orange color indicates the data without mask","286bebde":"According to the above analysis, whether or not wearing a mask, there are still some cases of misjudgment, such as the above image only one face, but still identify two boxes, from this image analysis, this may be related to the impact of shooting light, resulting in inaccurate detection. Or there are some blurred faces that are not recognized, or whether they are wearing masks or not. So this detection model has high requirements on the angle of the image, light and other factors.\nSee the paper for detailed analysis and description.","92c1e1e8":"According to the statistical label data, the following figure  can be obtained. The blue color indicates the data with mask, and the orange color indicates the data without mask","5c00a3e9":"<a id=\"section-eight\"><\/a>\n## 8. Conclusion\n","f1588625":"### Looking at the data\nClick to view the data in the dataset on the right to see some annotation files underneath the Annotations directory. Look closely at the fields such as BoundingBox and classnames, where a series of processes need to be performed on the data below.","e13da52d":"\nBy using tags (mask and non-mask), it is possible to extract the corresponding BOX. in addition it is necessary to extract the face and store this data","9923ab70":"### check the shape","b2b7daef":"<a id=\"section-three\"><\/a>\n","540c13a3":"<a id=\"section-three\"><\/a>\n## 3. CMNET Model Architecture","fca80b44":"**model.add(Conv2D(32,(3,3),input_shape=(img_size,img_size,3),activation='relu',strides=2))**\n* input_shape allows the input image to have a consistent format, the 3 in the program refers to the three channels of R\/B\/G\n* The activation function is set to ReLU\n\n**model.add(MaxPooling2D(pool_size=(2,2)))**\n* Use max pooling, the effect is to reduce irritability and reduce computing resources, set to 2 * 2 size\n\n**model.add(Flatten())** \n* Flatten flatten the feature maps into a vector\n","bcc9727b":"<a id=\"section-seven\"><\/a>\n## 7.using OpenCV to test model\nThe Caffe Face Detector (OpenCV Pre-trained Model) is used here to get the position of the face and further draw the face frame. With the above CMNET model, we can get whether the face is wearing a mask or not, and then complete the effect in the figure below. The position of the face can be framed and whether the face is wearing a mask or not can be clearly marked.","81715b1e":"### Processes and parameters Description\n- adjust_light\uff1aUsed to encode and decode the brightness or tristimulus values in video or still image systems. In simple terms, it is used to inject some light into the image. If adjust_light <1, the image will move to the darker end of the spectrum, when adjust_light> 1, there will be more light in the image.(adjust_light=2.2)\n- blobFromImage:BlobFromImage is mainly used to pre-process the image. When performing scalefactor, size, mean, and swapRB operations at the same time, first switch channels according to swapRB, then scale according to scalefactor, then reduce according to mean, and finally perform resize operation according to size\n- confidence_threshold: Detections higher than this score will be considered as human faces(threshold>0.2)\n- The detected values are passed to the frame and a score is set, and the corresponding faces with and without masks are obtained.(threshold>0.3)\n\n","7ed9dd41":"* Training set: The data set used to train the parameters of the model. Classfier directly adjusts itself according to the training set to obtain better classification results\n\n* Validation set: used to test the state and convergence of the model during the training process. Validation sets are usually used to adjust hyperparameters, and determine which set of hyperparameters has the best performance based on the performance on several sets of model validation sets.\nAt the same time, the validation set can also be used to monitor whether the model has overfitted during the training process. Generally speaking, after the performance of the validation set is stable, if you continue training, the performance of the training set will continue to rise, but the validation set will not rise but fall. , So that overfitting generally occurs. So the validation set is also used to determine when to stop training\n\n* Test set: The test set is used to evaluate the generalization ability of the model, that is, the model used the verification set to determine the hyperparameters, the training set was used to adjust the parameters, and finally a data set that had never been seen before was used to determine whether the model was Work.","697d0846":" If you are not wearing a mask, you need to take out the facial image, and then do further processing","cbc85502":"From the image above, it can be seen that the number with masks is greater than the number without, indicating that this is a data category imbalance.","9975bece":"### Parameter Description\n#### Dropout\nIt can more effectively alleviate the occurrence of over-fitting, and achieve the effect of regularization to a certain extent.\n\n#### Max Pooling\nThe meaning is to extract several eigenvalues from a certain Filter, and only obtain the largest Pooling layer as the retained value, and discard all other eigenvalues. The largest value means that only the strongest of these features is retained, and other weak such features are discarded.\n\n#### Batch Normalization\nBatch Normalization has been proven in actual engineering to alleviate the difficulty of neural network training:\n\n(1) BN makes the distribution of input data in each layer of the network relatively stable, accelerating the speed of model learning\n\n(2) BN makes the model less sensitive to the parameters in the network, simplifies the tuning process, and makes the network learning more stable\n\n(3) BN allows the network to use saturation activation functions (such as sigmoid, tanh, etc.) to alleviate the problem of gradient disappearance\n\n(4) BN has a certain regularization effect\n\n\n","214e6bdc":"According to the project context, we focus more on people without masks, which is the feature with a concern value of 1. That is, the accuracy rate is how accurate the prediction is for that event that we are concerned about. So in that context, we compare the precision of several algorithms. Based on the analysis in Table 4 above, it can be concluded that the highest precision for 1 label is the CMNET and MobileNet models, both with 0.99, followed by the VGG19 and VGG16 models with 0.91 and 0.84, respectively. Regarding recall, the number of correct predictions out of all data with a true value of 1. That is, what is the percentage of our successful predictions for the real occurrence of that event we are concerned about. According to Table 4, CMNET model is better than MobileNet,0.75 and 0.51, respectively. Regarding accuracy, CMNET model is better than MobileNet, 0.93 and 0.87 ,respectively. In summary, the CMNET model performs best.","f77f0d4a":"### define the function\nget the type of image","9ad33bed":"- [Masked Face Detection](#section-one)\n- [Importing Libraries](#section-two)\n- [Data Preprocessing](#section-three)\n- [CMNET Model Architecture](#section-four)\n- [Training Process ](#section-five)\n- [Training and Validation Visualizations and Evaluation Model](#section-six)\n- [face detection-OpenCV](#section-seven)\n- [Conclusion](#section-eight)","841048f4":"<a id=\"section-two\"><\/a>\n## 2. Data Preprocessing ","50be4036":"<a id=\"section-six\"><\/a>\n## 6. Training and Validation Visualizations and Evaluation Model","71215fb7":"### Explanation of the different layers\n- Convolution layer: is convolved using 32 (32 types) filters with 3x3 vertical and horizontal pixels with a 0 fill of 2. The activation function used is relu.\n- BatchNormalization layer: is to pull the increasingly biased distribution back to the standard human not by normalization, so that the input value of the activation function falls in the region where the activation function is more sensitive to the input, thus making the gradient larger, speeding up the learning convergence and avoiding the problem of gradient disappearance.\n- Pooling layer: Sampling the input, reducing the dimension of the input, reducing the parameters (simplifying the computation), enhancing the generalization ability of the model, and reducing the possibility of overfitting.\n- Dropout: Enhance the generalization ability of the model to prevent overfitting. And can reduce the complexity of the model.\n- Flatten layer: Turn multidimensional inputs into one dimension for the transition from convolutional layers to fully connected layers (Dense).\n- Fully connected layer (Dense): connects all the features and sends the output values to the classifier. The activation function of the last Dense uses the sigmoid function.","6b28796a":"And we use data augmentation technique to generate a batch of images and give the model training in the form of a generator, and at the same time do data augmentation on the images, setting the following parameters.\n- rotation_range= 17, the number of degrees of random rotation range;\n- width_shift_range=0.1, horizontal shift percentage;\n- height_shift_range=0.1, the percentage of vertical shift;\n- horizontal_flip=True, horizontal flip;\n- shear_range=0.2, shear shift angle;\n","e9d71130":"<a id=\"section-four\"><\/a>\n## 4. Compare model\n- VGG16\n- VGG19\n- MobileNetV2","4d21e59b":"<a id=\"section-one\"><\/a>\n# Masked Face Detection\nThis section introduces the experimental procedure of Masked Face Detection and the analysis of experimental results.","7024c9c9":"### get the features list and labels list","d73cb433":"### Evaluation Model\n- TN: the number of negative cases (N) predicted by the algorithm, and indeed the number of negative cases (N), i.e., the algorithm predicted correctly (True).\n- FP: the algorithm predicts the number of positive cases (P), which is actually the number of negative cases (N), i.e., the algorithm predicts wrongly (False).\n- FN: the number of negative cases (N) predicted by the algorithm, which is actually the number of positive cases (P), i.e., the algorithm predicts wrongly (False).\n- TP: the number of positive cases (P) predicted by the algorithm, which is also the number of positive cases (P), i.e., the algorithm predicts correctly (True).\n\n- The accuracy rate is defined as the percentage of correct predictions in the total sample, (TP+TN)\/(TP+TN+FP+FN)\n- Precision is the probability of the actual positive sample among all the samples predicted to be positive, which means how much we can predict correctly among the positive samples, TP\/(TP+FP)\n- Recall, also known as the full rate, is for the original sample, and means the probability that a positive sample is predicted among the actual positive samples, TP\/(TP+FN)","64a8b3ab":"### save model","ef0cc319":"<a id=\"section-five\"><\/a>\n## 5. Training Process "}}