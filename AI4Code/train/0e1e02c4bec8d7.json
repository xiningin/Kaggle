{"cell_type":{"f9a09b2c":"code","4e5c7b20":"code","5e8c8816":"code","3667115c":"code","e0769fd6":"code","6b2847dc":"code","4ca217a5":"code","c861f10f":"code","5115bcab":"code","6df50a59":"code","33039aa6":"code","26563c8c":"code","e9add311":"code","16dca307":"code","82ed4712":"code","deabc6da":"code","e0c505cd":"code","a839fd0a":"code","24c3fdde":"code","159ecb07":"code","d25a46e0":"code","c3d522c5":"code","7bdcba64":"code","73ba9c5a":"code","830b5c68":"code","b1468c7f":"code","3e9b51d7":"code","6397c03b":"code","8d031c17":"code","04b9702e":"code","a36f5c2d":"code","ba02a0fb":"code","41d10b9c":"code","5ea3fb38":"code","e1fce11f":"code","70539901":"code","73a132a1":"code","dfa0e4eb":"code","aa1eb1fc":"code","82febe32":"code","7109b812":"code","4df7a1b9":"code","9bd2f0c5":"code","09023783":"code","d9f16019":"code","094fd925":"code","a4a0a42f":"code","2e8af1ba":"code","2550e290":"markdown","5f53ec0b":"markdown","4eb65b53":"markdown","c89febfc":"markdown","e0f285eb":"markdown","280e638b":"markdown","aa114312":"markdown","20a32004":"markdown","bb3c2a08":"markdown","01807b68":"markdown","074fbebd":"markdown","09393782":"markdown","d0d38262":"markdown","a3687a8b":"markdown","85c3723b":"markdown","a3f5a6fd":"markdown","cbf2aac3":"markdown","049f1d43":"markdown","a29b496d":"markdown","637620f7":"markdown","55cd0d8f":"markdown","dfe31947":"markdown","25e08d99":"markdown","3673a536":"markdown","698786b6":"markdown","35687eca":"markdown","9cbb0728":"markdown","24d13bbf":"markdown","aaf85844":"markdown"},"source":{"f9a09b2c":"!pip install fast-bert","4e5c7b20":"import pandas as pd\npd.options.mode.chained_assignment = None\nimport logging\nimport os\nimport pickle\nfrom time import time\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\n\nimport plotly.graph_objs as go\n!pip install chart_studio\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\n#plotly.offline.init_notebook_mode(connected=True)\nimport re\nimport emoji\n\nfrom nltk.tokenize import word_tokenize\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nfrom collections import defaultdict\n\nfrom wordcloud import WordCloud,STOPWORDS","5e8c8816":"import torch\nfrom fast_bert.data_cls import BertDataBunch\nfrom fast_bert.learner_cls import BertLearner\nfrom fast_bert.data_lm import BertLMDataBunch\nfrom fast_bert.learner_lm import BertLMLearner\n\nfrom fast_bert.metrics import fbeta, roc_auc, accuracy\nfrom fast_bert.prediction import BertClassificationPredictor\nfrom pathlib import Path\nlogger = logging.getLogger()\ndevice_cuda = torch.device(\"cuda\")","3667115c":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', sep=\",\")","e0769fd6":"test_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', sep=\",\")","6b2847dc":"print(\"Total rows in train data: \", train_data.shape[0])\nprint(\"Total columns in train data: \", train_data.shape[1])\nprint(\"-\"*30)\nprint(\"Total rows in test data: \", test_data.shape[0])\nprint(\"Total columns in test data: \", test_data.shape[1])","4ca217a5":"train_data.describe()","c861f10f":"test_data.describe()","5115bcab":"train_data.head()","6df50a59":"print(train_data.isnull().sum())\nprint(\"-\"*30)\nprint(test_data.isnull().sum())","33039aa6":"train_data.groupby(train_data.target).count().text","26563c8c":"sns.countplot(train_data.target,data = train_data)\nplt.show()","e9add311":"word_len_dis = train_data[train_data['target']==1]['text'].str.split().map(lambda x : len(x))\n\nword_len_non_dis = train_data[train_data['target']==0]['text'].str.split().map(lambda x : len(x))\n\ndata = []\ndata.append(word_len_dis)\ndata.append(word_len_non_dis)\n\ntPlot, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(10,5))\naxes = np.array(axes)\n\ni=0\ntitles = [\"Disaster Tweets\", \"Non-Disaster Tweets\"]\nfor ax in axes.reshape(-1):\n    ax.title.set_text(titles[i])\n    ax.hist(data[i])\n    i += 1\n    \ntitle=\"Length of words in Tweets\"\ntPlot.text(0.5, 1.01, title, ha='center', fontsize = 14)\nplt.tight_layout()","16dca307":"def avgwordlen(strlist):\n    sum=[]\n    for i in strlist:\n        sum.append(len(i))\n    return sum\n\navgword_len_dis = train_data[train_data['target']==1]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\navgword_len_non_dis = train_data[train_data['target']==0]['text'].str.split().apply(avgwordlen).map(lambda x: np.mean(x))\n\ngroup_labels = ['Disaster', 'Non-Disaster']\ncolors = ['rgb(0, 0, 100)', 'rgb(0, 200, 200)']\n\nfig = ff.create_distplot([avgword_len_dis, avgword_len_non_dis], group_labels, bin_size=.2, colors=colors,)\n\nfig.update_layout(title_text=\"Average word length in tweets\",title_x=0.5,xaxis_title=\"Text\",yaxis_title=\"Density\").show()\n","82ed4712":"def create_corpus(target):\n    corpus = []\n    for i in train_data[train_data['target']==target]['text'].str.split():\n        for x in i:\n            corpus.append(x)\n    return corpus","deabc6da":"from string import punctuation\nvalues_list = []\ndef analyze_punctuations(data,func,targetlist):\n  \n  for label in range(0,len(targetlist)):\n    corpus = func(targetlist[label])\n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in punctuation:\n            dic[word] += 1 \n    x_items, y_values = zip(*dic.items())\n    values_list.append(x_items)\n    values_list.append(y_values)\n\n#analyzing punctuations for 0 and 1 target labels\nanalyze_punctuations(data,create_corpus,[0,1])\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Disaster Tweets\", \"Non-Disaster Tweets\"))\n  \nfig.add_trace(\n      go.Bar(x=values_list[0],y=values_list[1],\n             marker=dict(color= 'rgba(196, 94, 255,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=1\n).add_trace(\n      go.Bar(x=values_list[2],y=values_list[3],\n             marker=dict(color= 'rgba(255, 163, 102,0.8)'),\n             marker_line=dict(color='black'),marker_line_width=1.2),\n      row=1, col=2\n).update_layout(title_text=\"Top Punctuations in the text\",title_x=0.5,showlegend=False).show()","e0c505cd":"def wordcloud(data,title):\n    words = ' '.join(data['text'].astype('str').tolist())\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(stopwords = stopwords,width= 512, height = 512).generate(words)\n    plt.figure(figsize=(10,8),frameon=True)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.title(title,fontsize=20)\n    plt.show()\n    \ndata_disaster = train_data[train_data['target'] == 1]\ndata_non_disaster = train_data[train_data['target'] == 0]","a839fd0a":"wordcloud(data_disaster,\"Disaster Tweets\")","24c3fdde":"wordcloud(data_non_disaster,\"Non-Disaster Tweets\")","159ecb07":"train_data = train_data.drop(['keyword','location'], axis = 1)\ntest_data = test_data.drop(['keyword','location'], axis = 1)","d25a46e0":"train_data.head()","c3d522c5":"def cleanTweet(txt):\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    txt = re.sub(r'\\n','',txt)\n    # to remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    txt = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+','',txt)\n    txt = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\",\"\",txt)\n    txt = re.sub(r\"<.*?>\",\"\",txt)\n    txt = re.sub(r\"[\u00e0&\u00e9\u00e8\u00fb\u00f2\u00e5\u00ea\u00f9\u00fb_]\",\" \",txt)\n    txt = re.sub(r'[^\\x00-\\x7F]+', '', txt)\n    return txt  \n\ndef make_Lower(text):\n    return str.lower(text)","7bdcba64":"train_data.text = train_data.text.apply(cleanTweet)\ntrain_data.text = train_data.text.apply(make_Lower)\n\ntest_data.text = test_data.text.apply(cleanTweet)\ntest_data.text = test_data.text.apply(make_Lower)","73ba9c5a":"abbreviations = {\n\"$\" : \"dollar \",\n\"\u20ac\" : \"euro \",\n\"4ao\" : \"for adults only\",\n\"a.m\" : \"before midday\",\n\"a3\" : \"anytime anywhere anyplace\",\n\"aamof\" : \"as a matter of fact\",\n\"abt\" : \"about\",\n\"acct\" : \"account\",\n\"adih\" : \"another day in hell\",\n\"af\" : \"as freak\",\n\"afaic\" : \"as far as i am concerned\",\n\"afaict\" : \"as far as i can tell\",\n\"afaik\" : \"as far as i know\",\n\"afair\" : \"as far as i remember\",\n\"afk\" : \"away from keyboard\",\n\"aight\" : \"alright\",\n\"app\" : \"application\",\n\"approx\" : \"approximately\",\n\"apps\" : \"applications\",\n\"asap\" : \"as soon as possible\",\n\"asl\" : \"age sex location\",\n\"atk\" : \"at the keyboard\",\n\"ave.\" : \"avenue\",\n\"awol\" : \"away without leaving\",\n\"aymm\" : \"are you my mother\",\n\"ayor\" : \"at your own risk \",\n\"b.c\" : \"before christ\",\n\"b@u\" : \"back at you\",\n\"b&b\" : \"bed and breakfast\",\n\"b+b\" : \"bed and breakfast\",\n\"b2b\" : \"business to business\",\n\"b2c\" : \"business to customer\",\n\"b4\" : \"before\",\n\"b4n\" : \"bye for now\",\n\"bae\" : \"before anyone else\",\n\"bak\" : \"back at keyboard\",\n\"bb\" : \"baby\",\n\"bbbg\" : \"bye bye be good\",\n\"bbc\" : \"british broadcasting corporation\",\n\"bbias\" : \"be back in a second\",\n\"bbl\" : \"be back later\",\n\"bbs\" : \"be back soon\",\n\"bcos\" : \"because\",\n\"bcoz\" : \"because\",\n\"be4\" : \"before\",\n\"bfn\" : \"bye for now\",\n\"blvd\" : \"boulevard\",\n\"bout\" : \"about\",\n\"brb\" : \"be right back\",\n\"brb\" : \"be right back\",\n\"bro\" : \"brother\",\n\"bros\" : \"brothers\",\n\"brt\" : \"be right there\",\n\"bsaaw\" : \"big smile and a wink\",\n\"bt\" : \"bad trip\",\n\"btw\" : \"by the way\",\n\"bwl\" : \"bursting with laughter\",\n\"c\/o\" : \"care of\",\n\"cet\" : \"central european time\",\n\"cf\" : \"compare\",\n\"cia\" : \"central intelligence agency\",\n\"cld\" : \"could\",\n\"coz\" : \"because\",\n\"csl\" : \"can not stop laughing\",\n\"cu\" : \"see you\",\n\"cul8r\" : \"see you later\",\n\"cv\" : \"curriculum vitae\",\n\"cwot\" : \"complete waste of time\",\n\"cya\" : \"see you\",\n\"cyt\" : \"see you tomorrow\",\n\"dae\" : \"does anyone else\",\n\"dbmib\" : \"do not bother me i am busy\",\n\"df\" : \"the freak \",\n\"dgaf\" : \"don't give a freak\",\n\"dis\" : \"this\",\n\"diy\" : \"do it yourself\",\n\"dm\" : \"direct message\",\n\"dm\" : \"direct message\",\n\"dnt\" : \"don't \",\n\"dw\" : \"don't worry\",\n\"dwh\" : \"during work hours\",\n\"e123\" : \"easy as one two three\",\n\"eet\" : \"eastern european time\",\n\"eg\" : \"example\",\n\"embm\" : \"early morning business meeting\",\n\"encl\" : \"enclosed\",\n\"enf\" : \"enough\",\n\"eta\" : \"estimated time of arrival\",\n\"etc\" : \"and so on\",\n\"ez\" : \"easy\",\n\"faq\" : \"frequently asked questions\",\n\"fawc\" : \"for anyone who cares\",\n\"fb\" : \"facebook\",\n\"fbm\" : \"fine by me\",\n\"fc\" : \"fingers crossed\",\n\"ffs\" : \"for freaks sake\",\n\"fig\" : \"figure\",\n\"fimh\" : \"forever in my heart \",\n\"fomo\" : \"fear of missing out\",\n\"ft\" : \"featuring\",\n\"ft.\" : \"feet\",\n\"ftl\" : \"for the loss\",\n\"ftw\" : \"for the win\",\n\"fu\" : \"freak you\",\n\"fwiw\" : \"for what it is worth\",\n\"fwm\" : \"fine with me\",\n\"fyi\" : \"for your information\",\n\"g.o.a.t\" : \"greatest of all time\",\n\"g9\" : \"genius\",\n\"gahoy\" : \"get a hold of yourself\",\n\"gal\" : \"get a life\",\n\"gcse\" : \"general certificate of secondary education\",\n\"gfn\" : \"gone for now\",\n\"gg\" : \"good game\",\n\"gl\" : \"good luck\",\n\"glhf\" : \"good luck have fun\",\n\"gm\" : \"good morning\",\n\"gmt\" : \"greenwich mean time\",\n\"gmta\" : \"great minds think alike\",\n\"gn\" : \"good night\",\n\"goat\" : \"greatest of all time\",\n\"goi\" : \"get over it\",\n\"gps\" : \"global positioning system\",\n\"gr8\" : \"great\",\n\"gratz\" : \"congratulations\",\n\"grl\" : \"girl\",\n\"grw\" : \"get ready with me\",\n\"gtg\" : \"going to go\",\n\"gyal\" : \"girl\",\n\"h&c\" : \"hot and cold\",\n\"h8\" : \"hate\",\n\"hbd\" : \"happy birthday\",\n\"hbu\" : \"how about you\",\n\"hp\" : \"horsepower\",\n\"hr\" : \"hour\",\n\"hrh\" : \"his royal highness\",\n\"hru\" : \"how are you\",\n\"ht\" : \"height\",\n\"hw\" : \"homework\",\n\"i.e\" : \"that is\",\n\"ibrb\" : \"i will be right back\",\n\"ic\" : \"i see\",\n\"icq\" : \"i seek you\",\n\"icymi\" : \"in case you missed it\",\n\"idc\" : \"i do not care\",\n\"idgadf\" : \"i do not give a damn fuck\",\n\"idgaf\" : \"i do not give a fuck\",\n\"idk\" : \"i do not know\",\n\"idts\" : \"i don't think so\",\n\"ie\" : \"that is\",\n\"ifyp\" : \"i feel your pain\",\n\"ig\" : \"instagram\",\n\"iirc\" : \"if i remember correctly\",\n\"ik\" : \"i know\",\n\"ilu\" : \"i love you\",\n\"ily\" : \"i love you\",\n\"ilysm\" : \"i love you so much\",\n\"imho\" : \"in my humble opinion\",\n\"imo\" : \"in my opinion\",\n\"imu\" : \"i miss you\",\n\"imy\" : \"i miss you\",\n\"iow\" : \"in other words\",\n\"irl\" : \"in real life\",\n\"j4f\" : \"just for fun\",\n\"jic\" : \"just in case\",\n\"jk\" : \"just kidding\",\n\"jk\" : \"just kidding\",\n\"jsyk\" : \"just so you know\",\n\"k\" : \"okay\",\n\"l2g\" : \"like to go\",\n\"l8r\" : \"later\",\n\"lb\" : \"pound\",\n\"lbs\" : \"pounds\",\n\"ldr\" : \"long distance relationship\",\n\"lmao\" : \"laugh my ass off\",\n\"lmfao\" : \"laugh my fucking ass off\",\n\"lmk\" : \"let me know\",\n\"lol\" : \"laughing\",\n\"ltd\" : \"limited\",\n\"ltns\" : \"long time no see\",\n\"ly\" : \"love you\",\n\"m8\" : \"mate\",\n\"mf\" : \"motherfucker\",\n\"mfs\" : \"motherfuckers\",\n\"mfw\" : \"my face when\",\n\"mofo\" : \"motherfucker\",\n\"mph\" : \"miles per hour\",\n\"mr\" : \"mister\",\n\"mrw\" : \"my reaction when\",\n\"ms\" : \"miss\",\n\"mte\" : \"my thoughts exactly\",\n\"nagi\" : \"not a good idea\",\n\"nbc\" : \"national broadcasting company\",\n\"nbd\" : \"no big deal\",\n\"nbd\" : \"not big deal\",\n\"nfs\" : \"not for sale\",\n\"ngl\" : \"not going to lie\",\n\"nhs\" : \"national health service\",\n\"nm\" : \"nothing much\",\n\"np\" : \"no problem\",\n\"nrn\" : \"no reply necessary\",\n\"nsfl\" : \"not safe for life\",\n\"nsfw\" : \"not safe for work\",\n\"nth\" : \"nice to have\",\n\"nvm\" : \"never mind\",\n\"nvr\" : \"never\",\n\"nw\" : \"no way\",\n\"nyc\" : \"new york city\",\n\"oc\" : \"original content\",\n\"ofc\" : \"ofcourse\",\n\"og\" : \"original\",\n\"ohp\" : \"overhead projector\",\n\"oic\" : \"oh i see\",\n\"omdb\" : \"over my dead body\",\n\"omfg\" : \"oh my freaking god\",\n\"omg\" : \"oh my god\",\n\"omw\" : \"on my way\",\n\"ootd\" : \"outfit of the day\",\n\"otb\" : \"off to bed\",\n\"otw\" : \"off to work\",\n\"p.a\" : \"per annum\",\n\"p.m\" : \"after midday\",\n\"pm\" : \"private message\",\n\"poc\" : \"people of color\",\n\"pov\" : \"point of view\",\n\"pp\" : \"pages\",\n\"ppl\" : \"people\",\n\"prob\" : \"probably\",\n\"prw\" : \"parents are watching\",\n\"ps\" : \"postscript\",\n\"pt\" : \"point\",\n\"ptb\" : \"please text back\",\n\"pto\" : \"please turn over\",\n\"qpsa\" : \"what happens #que pasa\",\n\"qt\" : \"cutie\",\n\"r\" : \"are\",\n\"ratchet\" : \"rude\",\n\"rbtl\" : \"read between the lines\",\n\"rlrt\" : \"real life retweet \",\n\"rly\" : \"really\",\n\"rofl\" : \"laughing\",\n\"roflol\" : \"rolling on the floor laughing out loud\",\n\"rotflmao\" : \"rolling on the floor laughing my ass off\",\n\"rt\" : \"retweet\",\n\"ruok\" : \"are you ok\",\n\"sfw\" : \"safe for work\",\n\"sh\" : \"same here\",\n\"sis\" : \"sister\",\n\"sk8\" : \"skate\",\n\"smh\" : \"shake my head\",\n\"sq\" : \"square\",\n\"srsly\" : \"seriously \",\n\"sry\" : \"sorry\",\n\"ssdd\" : \"same stuff different day\",\n\"sup\" : \"what's up\",\n\"tbh\" : \"to be honest\",\n\"tbs\" : \"tablespooful\",\n\"tbsp\" : \"tablespooful\",\n\"tfw\" : \"that feeling when\",\n\"thks\" : \"thank you\",\n\"thnk\" : \"thank you\",\n\"tho\" : \"though\",\n\"thx\" : \"thanks\",\n\"tia\" : \"thanks in advance\",\n\"til\" : \"today i learned\",\n\"tl;dr\" : \"too long i did not read\",\n\"tldr\" : \"too long i did not read\",\n\"tmb\" : \"tweet me back\",\n\"tntl\" : \"trying not to laugh\",\n\"tryna\" : \"trying to be \",\n\"ttly\" : \"totally\",\n\"ttyl\" : \"talk to you later\",\n\"ty\" : \"thank you\",\n\"u\" : \"you\",\n\"u2\" : \"you too\",\n\"u4e\" : \"yours for ever\",\n\"ur\" : \"you are\",\n\"utc\" : \"coordinated universal time\",\n\"w\" : \"with\",\n\"w\/\" : \"with\",\n\"w\/o\" : \"without\",\n\"w8\" : \"wait\",\n\"wassup\" : \"what is up\",\n\"wb\" : \"welcome back\",\n\"wdyk\" : \"what do you know\",\n\"wfh\" : \"work from home\",\n\"whatevs\" : \"whatever\",\n\"wru\" : \"where are you\",\n\"wtf\" : \"what the freak\",\n\"wtg\" : \"way to go\",\n\"wtpa\" : \"where the party at\",\n\"wuf\" : \"where are you from\",\n\"wuzup\" : \"what is up\",\n\"wyd\" : \"what are you doing\",\n\"wywh\" : \"wish you were here\",\n\"XD\" : \"laugh\",\n\"xo\" : \"hugs and kisses\",\n\"xoxo\" : \"hugs and kisses\",\n\"y\" : \"why\",\n\"yd\" : \"yard\",\n\"ygtr\" : \"you got that right\",\n\"ynk\" : \"you never know\",\n\"yolo\" : \"you only live once\",\n\"zzz\" : \"sleeping bored and tired\"\n}","830b5c68":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","b1468c7f":"def convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens=[convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","3e9b51d7":"train_data.text = train_data.text.apply(convert_abbrev_in_text)\ntest_data.text = test_data.text.apply(convert_abbrev_in_text)","6397c03b":"train_data.to_csv('train_preprocessed.csv', index=False) \ntest_data.to_csv('test_preprocessed.csv', index=False) ","8d031c17":"DATA_PATH = Path('data\/')\nLABEL_PATH = Path('data\/')\nMODEL_PATH=Path('models\/')\nLOG_PATH=Path('logs\/')\nMODEL_PATH.mkdir(exist_ok=True)\nDATA_PATH.mkdir(exist_ok=True)\nLOG_PATH.mkdir(exist_ok=True)","04b9702e":"val_set = train_data.sample(frac=0.2, replace=False, random_state=42)\ntrain_set = train_data.drop(index = val_set.index)\nprint('Number of tweets in val_set:',len(val_set))\nprint('Number of tweets in train_set:', len(train_set))\nval_set.to_csv(str(DATA_PATH) + '\/val_set.csv')\ntrain_set.to_csv(str(DATA_PATH) + '\/train_set.csv')","a36f5c2d":"labels = train_data.columns[2:].to_list()\nwith open(str(LABEL_PATH) + '\/labels.txt', 'w') as f:\n    for i in labels:\n        f.write(i + \"\\n\")\nwith open(str(LABEL_PATH) + '\/labels.csv', 'w') as f:\n    for i in labels:\n        f.write(i + \"\\n\")        ","ba02a0fb":"all_texts = train_data['text'].to_list()\nprint('Number of tweets:', len(all_texts))","41d10b9c":"print('Create LM databunch object')\nt0 = time()\ndatabunch_lm = BertLMDataBunch.from_raw_corpus(\n                    data_dir=Path(DATA_PATH),\n                    text_list=all_texts,\n                    tokenizer='bert-base-uncased',\n                    batch_size_per_gpu=16,\n                    max_seq_length=280,\n                    multi_gpu=False,\n                    model_type='bert',\n                    logger=logger)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')\n","5ea3fb38":"print('Create LM learner object')\nt0 = time()\nmetrics = []\nmetrics.append({'name': 'accuracy', 'function': accuracy})\nlm_learner = BertLMLearner.from_pretrained_model(\n                            dataBunch=databunch_lm,\n                            pretrained_path='bert-base-uncased',\n                            output_dir=Path(MODEL_PATH),\n                            metrics=metrics,\n                            device=device_cuda,\n                            logger=logger,\n                            multi_gpu=False,\n                            logging_steps=50,\n                            fp16_opt_level=\"O2\")\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","e1fce11f":"print('Fit LM learner object')\n\nlm_learner.fit(epochs=5,\n            lr=1e-4,\n            validate=True,\n            schedule_type=\"warmup_cosine\",\n            optimizer_type=\"adamw\")","70539901":"print('Validate LM learner object')\nt0 = time()\nlm_learner.validate()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","73a132a1":"print('Save LM learner object')\nt0 = time()\nlm_learner.save_model()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","dfa0e4eb":"labels_col = ['target']","aa1eb1fc":"print('Create databunch object')\nt0 = time()\ndatabunch = BertDataBunch(Path(DATA_PATH), Path(LABEL_PATH),\n                          tokenizer='bert-base-uncased',\n                          train_file='train_set.csv',\n                          val_file='val_set.csv',\n                          label_file='labels.txt',\n                          text_col='text',\n                          label_col=labels_col,\n                          batch_size_per_gpu=16,\n                          max_seq_length=280,\n                          multi_gpu=False,\n                          multi_label=True,\n                          model_type='bert')\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","82febe32":"metrics = []\nmetrics.append({'name': 'accuracy', 'function': accuracy})","7109b812":"OUTPUT_DIR = Path(str(MODEL_PATH) + '\/finetuned_model')\nWGTS_PATH = Path(str(MODEL_PATH) + '\/model_out\/pytorch_model.bin')\nFINETUNED_MODEL_PATH=Path('models\/finetuned_model')\nFINETUNED_MODEL_PATH.mkdir(exist_ok=True)\nFINETUNED_MODEL_TENSORBOARD_PATH = Path(str(MODEL_PATH) + '\/finetuned_model\/tensorboard')\nOUTPUT_DIR.mkdir(exist_ok=True)\nFINETUNED_MODEL_TENSORBOARD_PATH.mkdir(exist_ok=True)","4df7a1b9":"print('Create learner object')\nt0 = time()\ncl_learner = BertLearner.from_pretrained_model(\n                        databunch,\n                        pretrained_path=str(MODEL_PATH) + '\/model_out',\n                        metrics=metrics,\n                        device=device_cuda,\n                        logger=logger,\n                        output_dir=Path(OUTPUT_DIR),\n                        finetuned_wgts_path=Path(WGTS_PATH),\n                        warmup_steps=50,\n                        multi_gpu=False,\n                        multi_label=True,\n                        is_fp16=True,\n                        logging_steps=50)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","9bd2f0c5":"print('Fit classification learner object')\nt0 = time()\ncl_learner.fit(epochs=5,\n            lr=9e-5,\n            validate=True,\n            schedule_type=\"warmup_cosine\",\n            optimizer_type=\"adamw\")\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","09023783":"print('Validate classification learner object')\nt0 = time()\ncl_learner.validate()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","d9f16019":"print('Save classification learner object')\nt0 = time()\ncl_learner.save_model()\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')","094fd925":"predictor = BertClassificationPredictor(\n                model_path=str(MODEL_PATH) + '\/finetuned_model\/model_out',\n                label_path=str(LABEL_PATH),\n                multi_label=True,\n                model_type='bert',\n                do_lower_case=False)","a4a0a42f":"t0 = time()\ntweet = test_data['text'][0]\nt = predictor.predict(tweet)\nt1 = time()\ntotal_time = t1 - t0\nprint('total time for operation: ' + str(total_time) + 's')\nfor pred in t:\n    print(tweet + ' - ' + str(pred[1]))","2e8af1ba":"predictions = []\nfor idx, row in test_data.iterrows():\n    t = predictor.predict(row.text)\n    for pred in t:\n        predictions.append([row.id, round(pred[1])])\n\ndf = pd.DataFrame(predictions, columns=['id','target'])        \ndf.to_csv('Submission.csv', index=False)        ","2550e290":"Disaster tweets number of words is slightly higher than for non disaster tweets, which suggests that contents is richer for details compared to standard comm that is generally short and quick.","5f53ec0b":"### Create Learner for classification\nPrepare metrics output with accuracy","4eb65b53":"## Predictions","c89febfc":"If you want to get your model accuracy from the metrics you defined, you can download the tensorboard folder and use tensorboard --logdir <myfolder> which will run a localhost entrypoint to the board","e0f285eb":"Both wordclouds are explicite !","280e638b":"The dataset is quite balanced between the 2 classes which is ok for our classification purpose.","aa114312":"# Data Exploration & Cleaning\nI found a very interesting kernel for DEA which I get inspired from : \nhttps:\/\/www.kaggle.com\/mohitnirgulkar\/disaster-tweets-classification-using-ml\/notebook\nThanks to him, and please upvote him !","20a32004":"#### Tweet length","bb3c2a08":"# Introduction\nWe intend to produce a model based on BERT to infer capacity of predicting if a tweet can be categorized as an Alert disaster tweet or not.\nSome kernels have already implemented such models :\n\n* egortrushin\/nlp-with-disaster-tweets-roberta-using-pytorch         NLP with Disaster Tweets: RoBERTa using PyTorch     Egor Trushin\n\n* ashishsingh226\/disaster-tweet-classification-using-distil-bert     Disaster Tweet Classification using Distil Bert     AshishSingh226\n\nIn a perspective to contribute to the community, we propose another implementation for BERT with fast-bert api on PyTorch.\n\nThe objective is to try achieve a high accuracy scoring by :\n\n* implementing most appropriate pretrained model\n* fine tuning hyperparameters\n* fine tune optimizer","01807b68":"### Cleaning & Preprocessing\nIn the perspective of using BERT we don't want to preprocess and clean the dataset too much as we want to stress the model to see how it manages natural language as of. Particularity of BERT is to work on the context understanding so classical cleaning\/preprocessing should not be applied.\n\nThis is why we only drop keyword and location columns as it is not used in the scope of this project, and remove useless punctuation\/emoji, useless characters and url links.","074fbebd":"Model learning metrics are saved under a tensorboard format in a dedicated folder.\n\nWe create this tensorboard out dir as it is not created straightly by the function.","09393782":"We save these preprocessed datasets as they will be model input from disk resources","d0d38262":"# Imports","a3687a8b":"### General prediction","85c3723b":"### Prepare train and validation sets","a3f5a6fd":"# Datasets loading","cbf2aac3":"We want to be as close to the original BERT model as possible. So we need to revert abbreviations to standard word to be closer to standard conversation context as the model was trained over.\n\nMain abbreviations dictionary is built-on manually aggregating standard abbreviations for tweeter found on several project on kaggle (ex: https:\/\/www.kaggle.com\/gogylogy\/twitterslang)","049f1d43":"More punctuation is used for disaster tweets which suggests people reinforce the text to show panic or seriousness of the situation.","a29b496d":"## Phase 1 - Fine-tune the model\n### Create LMDataBunch\n\nIn this phase we first fine tune BERT model to our corpus.\n\nWe first need to instanciate data bunches for batching learning phase.\n\nNote here we use 'bert-base-uncased' tokenizer defined on pretrained BERT model. ","637620f7":"### Create LMLearner\nWe instanciate a learner for fine tuning the model based on pretrained bert model.\n\nHere we choose simple 'bert' model, and accuracy as metric of learning process.","55cd0d8f":"### Define the optimal learning rate\nBased on integrated Bert Adam optimizer we try to fit the model with the most suitable learning rate for our fine-tuning. \n\nThis step is important as it is fine tuning on the classification objective, we don't want to retrain the whole model but be as accurate as possible.\n\nWe then choose very low lr which constrains the learning process to be slow but accurate.","dfe31947":"## Phase 2 - Train fine-tuned model for our downstream classification objective\n### Create databunch for our classification","25e08d99":"### Fitting the learner\nWe fit the LM model with 5 epochs which is enough as here we don't train the model from scratch but fine-tune a pretrained model that knows much (large bert uncased). If we increase number of epochs to a too large value, it will only result to messing the fine tuned weights coming with pretrained model.\n\nWe choose AdamW with warmup_cosine that variates learning rate from its original value to 0 with cosine periodicity.","3673a536":"# Model\n### Define main paths","698786b6":"### DEA","35687eca":"### Unit test","9cbb0728":"#### Main words in both tweets categories","24d13bbf":"We need to remove superfluous data like url, emoji, unicode character, ...","aaf85844":"Quite normalized unimodal distribution with a peak of 20 words. "}}