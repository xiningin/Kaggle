{"cell_type":{"5af64086":"code","f8e93d7c":"code","83b9e878":"code","6b8f231d":"code","98112883":"code","455947ed":"code","66f92b00":"code","bc0897d7":"code","edfd138f":"code","090b641f":"code","ffea61f3":"code","8eaa003e":"code","be96f73a":"code","6eb48a37":"code","e8a18aa2":"code","04c1385b":"code","82cfef50":"code","cc7b89e8":"code","c6af365a":"code","4124f568":"code","6047d167":"code","3bf478cf":"code","76ce8768":"code","930fd436":"code","9c668e35":"markdown","a356e1b8":"markdown","0f25006e":"markdown","1d93bd0c":"markdown","5ae3d7cc":"markdown","2279c248":"markdown","cf52ed61":"markdown","b7c1dcdf":"markdown","77ee5dde":"markdown","0e7ffce9":"markdown","7f363b13":"markdown","00ed2409":"markdown","fabfc179":"markdown"},"source":{"5af64086":"!pip install bert-for-tf2","f8e93d7c":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score\n\nimport tensorflow as tf\nimport transformers\nimport bert \nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer \nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint","83b9e878":"df_train = pd.read_csv('..\/input\/imdb-dataset\/imdb_train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('..\/input\/imdb-dataset\/imdb_val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('..\/input\/imdb-dataset\/imdb_test.csv', usecols = ['review','sentiment'])","6b8f231d":"print(df_train.info())\ndf_train","98112883":"print(df_val.info())\ndf_val","455947ed":"print(df_test.info())\ndf_test","66f92b00":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndef remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", data)\n    data = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", data)\n    data = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", data)\n    data = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"don\u00e5\u00abt\", \"do not\", data)  \n    return data\n","bc0897d7":"# Data Cleaning\ndf_train['review'] = df_train['review'].apply(lambda z: remove_punctuations(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_html(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_url(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_emoji(z))\n\ndf_val['review'] = df_val['review'].apply(lambda z: remove_punctuations(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_html(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_url(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_emoji(z))\n\ndf_test['review'] = df_test['review'].apply(lambda z: remove_punctuations(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_html(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_url(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_emoji(z))\n\ndf_train['review'] = df_train['review'].apply(lambda z: remove_abb(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_abb(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_abb(z))\n\n","edfd138f":"print(df_train.shape)\nprint(df_train.head(5))\nprint(df_val.shape)\nprint(df_val.head(5))\nprint(df_test.shape)\nprint(df_test.head(5))","090b641f":"## Encodeing \nclass IntentDetectionData:\n    DATA_COLUMN,  LABEL_COLUMN  = \"review\",\"sentiment\"\n\n    def __init__(self, train, val, test, tokenizer: FullTokenizer, classes, max_seq_len):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        ((self.train_x, self.train_y), (self.val_x, self.val_y), (self.test_x, self.test_y)) = map(self._prepare, [train, val, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.val_x, self.test_x = map(self._pad, [self.train_x, self.val_x, self.test_x])\n\n    def _prepare(self, df):\n        x, y = [], []\n    \n        for non, row in tqdm(df.iterrows()):\n            text, label =\\\n                row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"] ## Tokens beigning and ending specified by separation of tokens.\n\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens) ## Convert Tokens to IDs\n\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)] ## -2 as ignoring tokens provided by bert\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids)) ## padding by zeros\n            x.append(np.array(input_ids))\n        \n        return np.array(x)","ffea61f3":"def BiLSTM_V0(bert_output):\n    net = Bidirectional(LSTM(units=32, return_sequences=True,))(bert_output)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","8eaa003e":"def CNN_V0(bert_output):\n    net = Conv1D(128, 7, activation='relu',padding='same')(bert_output)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","be96f73a":"def CNN_LSTM_V0(bert_output):\n    net = Dropout(0.3)(bert_output)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs\n\ndef CNN_LSTM_V1(bert_output):\n\n    # channel 1\n    net = Conv1D(filters=128, kernel_size=3*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    a = LSTM(128)(net)\n\n    # channel 2\n    net = Conv1D(filters=128, kernel_size=5*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    b = LSTM(128)(net)\n\n    # channel 3\n    net = Conv1D(filters=128, kernel_size=7*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    c = LSTM(128)(net)\n\n    # channel 4\n    net = Conv1D(filters=128, kernel_size=9*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    d = LSTM(128)(net)\n\n    merged = concatenate([a,b,c,d])\n    dense = Dense(100, activation='relu')(merged)\n    drop = Dropout(0.2)(dense)\n    outputs = Dense(1, activation='sigmoid')(merged)\n    return outputs\n","6eb48a37":"def BiLSTM_CNN_V0(bert_output):\n    net = Bidirectional(LSTM(128, return_sequences=True))(bert_output)\n    net = Conv1D(128, 7, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","e8a18aa2":"def create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file):\n\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read()) ## Reading bert config\n        bert_params = map_stock_config_to_params(bc) ## Mapping parameters \n        bert_params.adapter_size = None # Adapter size helps tune Bert model faster\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'CNN':{0: CNN_V0},\n                    'BiLSTM':{0: BiLSTM_V0},\n                    'CNN+LSTM':{0: CNN_LSTM_V0, 1: CNN_LSTM_V1},\n                    'BiLSTM+CNN':{0: BiLSTM_CNN_V0},}\n    \n    ## Specifying input\n    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n        \n    outputs = choose_model[model_name][model_ver](bert_output)\n\n    model = keras.Model(input_ids, outputs)\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_checkpnt_file) ##Loading the weights from bert chckpoint file\n        \n    return model","04c1385b":"# Load BERT model\n# https:\/\/github.com\/google-research\/bert\/blob\/master\/README.md\n\nbert_model_name = \"uncased_L-12_H-768_A-12\"\n# uncased_L-4_H-512_A-8\n# uncased_L-12_H-768_A-12\n\n!wget  https:\/\/storage.googleapis.com\/bert_models\/2020_02_20\/{bert_model_name}.zip\n!unzip {bert_model_name}.zip","82cfef50":"import os\nbert_model_path = \".\/\"\nbert_checkpnt_file = os.path.join(bert_model_path, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_model_path, \"bert_config.json\")\nbert_vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\nprint(bert_checkpnt_file)\nprint(bert_config_file)\nprint(bert_vocab_file)","cc7b89e8":"# Tokenize\ntokenizer = FullTokenizer(vocab_file=bert_vocab_file)\nclasses = [0, 1]\nmax_seq_len = 384\ndata = IntentDetectionData(df_train, df_val, df_test, tokenizer, classes, max_seq_len)\nprint(data.max_seq_len)","c6af365a":"# Metrics\ndef get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('F1_SCORE: ', round(f1_score(y_test, y_pred_proba >= 0.5, average = \"macro\"), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","4124f568":"model_name = \"BiLSTM+CNN\"\nmodel_ver = 0\nLR = 2e-5\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=LR)\nmetrics = tf.metrics.BinaryAccuracy()\n\nmodel = create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()\n\n# Plot architecture model\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=96) #to_file='model.jpeg'\n","6047d167":"# Save model\nmodel_ckpt_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_binary_accuracy', mode='max',verbose=1, save_best_only=True, save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with {bert_model_name}_{model_name}_V{model_ver}_{max_seq_len}\\n\")\ntrain_history = model.fit(data.train_x, data.train_y, validation_data=(data.val_x,data.val_y), epochs=3, batch_size=16, verbose=1, callbacks=callbacks_list)","3bf478cf":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","76ce8768":"# Save architecture model\nconfig = model.to_json()\nmodel_config_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.json\"\nwith open(model_config_path, \"w\") as outfile:\n    json.dump(config, outfile)","930fd436":"model.load_weights(model_ckpt_path)\ny_pred_proba = model.predict(data.test_x)\nget_metrics(data.test_y, y_pred_proba)","9c668e35":"### Choose Model","a356e1b8":"## Build and Train","0f25006e":"#### BERT + BiLSTM + CNN","1d93bd0c":"### CNN","5ae3d7cc":"### Data pretrain","2279c248":"### BiLSTM + CNN ","cf52ed61":"# Set up","b7c1dcdf":"### CNN + LSTM","77ee5dde":"## Load Data and Cleaning","0e7ffce9":"## Model","7f363b13":"Refer Cleaning Data \n[https:\/\/www.kaggle.com\/colearninglounge\/nlp-data-preprocessing-and-cleaning](https:\/\/www.kaggle.com\/colearninglounge\/nlp-data-preprocessing-and-cleaning)","00ed2409":"### BiLSTM","fabfc179":"### BERT model"}}