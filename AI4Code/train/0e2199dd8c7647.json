{"cell_type":{"2d6a96ac":"code","90e90b7e":"code","de842976":"code","ea5f4037":"code","9b6c0115":"code","bb0c20ee":"code","f8a73b6d":"code","fad2bb36":"code","3ed3365f":"code","c30228e4":"code","77e90cdb":"code","87ab9ef3":"code","9ad68ac0":"code","7a7beeab":"code","43b29b04":"code","a9c861da":"code","95fb0900":"code","ab10876b":"markdown","9df7091d":"markdown","6cab5ccc":"markdown","d77cece1":"markdown","f30a876f":"markdown","1da56361":"markdown","4a2c1d05":"markdown","dd5f0f7c":"markdown"},"source":{"2d6a96ac":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport time\nimport pickle\nfrom contextlib import contextmanager\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_format = 'retina'","90e90b7e":"# nice way to report running times\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')","de842976":"PATH_TO_DATA = Path('..\/input\/hierarchical-text-classification\/')","ea5f4037":"train_df = pd.read_csv(PATH_TO_DATA \/ 'train_40k.csv').fillna(' ')\nval_df = pd.read_csv(PATH_TO_DATA \/ 'val_10k.csv').fillna(' ')","9b6c0115":"train_df.head()","bb0c20ee":"train_df.loc[0, 'Text']","f8a73b6d":"train_df.loc[0, 'Cat1'], train_df.loc[0, 'Cat2']","fad2bb36":"train_df['Cat1'].value_counts()","3ed3365f":"train_df['Cat1_Cat2'] = train_df['Cat1'] + '\/' + train_df['Cat2']\nval_df['Cat1_Cat2'] = val_df['Cat1'] + '\/' + val_df['Cat2']","c30228e4":"# put a limit on maximal number of features and minimal word frequency\ntf_idf = TfidfVectorizer(max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1e2, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=0, \n                           multi_class='multinomial',\n                           fit_intercept=True)\n# sklearn's pipeline\nbase_model = Pipeline([('tf_idf', tf_idf), \n                       ('logit', logit)])","77e90cdb":"class TfIdfLogitPipelineHierarchical(BaseEstimator):\n    \n    def __init__(self, \n                 base_model, \n                 model_store_path,\n                 class_separator = '\/',\n                 min_size_to_train=50\n                ):\n        \"\"\"\n\n        :param base_model: Sklearn model to train, one instance for level 1,\n                           and several instances for level 2 \n        :param model_store_path: where to store models as pickle files\n        :param class_separator: separator between level 1 and level 2 class names\n        :param min_size_to_train: do not train a model with less data\n        \"\"\"\n        self.base_model = base_model\n        self.model_store_path = Path(model_store_path)\n        self.class_separator = class_separator\n        self.min_size_to_train = min_size_to_train\n        \n        self.model_store_path.mkdir(exist_ok=True)\n        \n    def fit(self, X, y):\n        \n        lev1_classes = [label.split(self.class_separator)[0]\n                        for label in y]\n        \n        with timer('Training level 1 model'):\n            self.base_model.fit(X, lev1_classes)\n            \n            \n            with open(self.model_store_path \/ 'level1_model.pkl', 'wb') as f:\n                pickle.dump(self.base_model, f)\n        \n        \n        for lev1_class in np.unique(lev1_classes):\n            \n            with timer(f'Training level 2 model for class: {lev1_class}'):\n                curr_X = X.loc[y.str.startswith(lev1_class)]\n                curr_y = y.loc[y.str.startswith(lev1_class)].apply(lambda s: s.split(self.class_separator)[1])\n                \n                if len(curr_X) < self.min_size_to_train:\n                    print(f\"Skipped class {lev1_class.replace(' ', '_')} due to a too small dataset size: {len(curr_X)}\")\n                    continue\n                    \n                self.base_model.fit(curr_X, curr_y)\n                \n                model_name = f\"level2_model_{lev1_class.replace(' ', '_')}.pkl\"\n                \n                with open(self.model_store_path \/ model_name, 'wb') as f:\n                    pickle.dump(self.base_model, f)\n    \n    def predict(self, X):\n        \n        model_name =  'level1_model.pkl'\n        with open(self.model_store_path \/ model_name, 'rb') as f:\n            level1_model = pickle.load(f)\n        \n        level1_preds = level1_model.predict(X)\n            \n        level2_preds = np.zeros_like(level1_preds)\n            \n        for lev1_class in np.unique(level1_preds):\n            \n            idx = level1_preds == lev1_class\n            curr_X = X.iloc[idx]\n            \n            model_name = f\"level2_model_{lev1_class.replace(' ', '_')}.pkl\"\n            \n            if Path(self.model_store_path \/ model_name).exists():\n            \n                with open(self.model_store_path \/ model_name, 'rb') as f:\n                    level2_model = pickle.load(f)\n\n                curr_level2_preds = level2_model.predict(curr_X)\n                level2_preds[idx] = curr_level2_preds\n            \n            else:\n                level2_preds[idx] = lev1_class\n                \n        return level1_preds, level2_preds    ","87ab9ef3":"model = TfIdfLogitPipelineHierarchical(\n    base_model=base_model,\n    model_store_path='models'\n)","9ad68ac0":"model.fit(train_df['Title'], train_df['Cat1_Cat2'])","7a7beeab":"level1_pred, level2_pred = model.predict(val_df['Title'])","43b29b04":"f1_score(y_true=val_df['Cat1'], y_pred=level1_pred, average='micro').round(3),\\\nf1_score(y_true=val_df['Cat1'], y_pred=level1_pred, average='weighted').round(3)","a9c861da":"f1_score(y_true=val_df['Cat2'], y_pred=level2_pred, average='micro').round(3),\\\nf1_score(y_true=val_df['Cat2'], y_pred=level2_pred, average='weighted').round(3)","95fb0900":"print(classification_report(\n    y_true=val_df['Cat1'], \n    y_pred=level1_pred)\n)","ab10876b":"Here we train one level 1 model and several level 2 models (as many as there are level 1 classes). At prediction time, we first pick up level 1 model, make a level 1 prediction, and then pick up the corresponding level 2 model based on level 1 prediction. \n\n**Results:**\n\n**Results:**\n\nF1 micro (=accuracy):\n- Level 1: **0.945**\n- Level 2: **0.809**\n\nA bit worse than in case of [\"flat\" classification](https:\/\/www.kaggle.com\/kashnitsky\/flat-hierarchical-tf-idf-logreg-baseline) where we trained a single model for all level 2 classes.","9df7091d":"We can further analyze confusion matrices for each level 1, but not much insight there.","6cab5ccc":"We are training our model only with review titles, a couple of experiments show that it works better than with review text. ","d77cece1":"Distribution of Cat1 classes","f30a876f":"Example of a review","1da56361":"## <center> Classifying amazon product reviews with logistic regression\n## <center> Simple hierarchical approach","4a2c1d05":"## Evaluation","dd5f0f7c":"## Defining the model"}}