{"cell_type":{"e6920f47":"code","27a91ff1":"code","c71467a6":"code","1c8acd24":"code","f18f7b01":"code","69252802":"code","3ab7d20a":"code","9f7e9d18":"code","2af4f528":"code","2f6ca2e4":"markdown","5b69088b":"markdown"},"source":{"e6920f47":"#import required libraries\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom numpy import mean, std\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom optuna import Trial\nfrom optuna.samplers import TPESampler","27a91ff1":"#bring in data\nraw_training = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\nraw_test = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\n\n#pull out features\nX = raw_training.iloc[:, 1:76]\nX = np.ascontiguousarray(X)\n\n#pull out responses\ny = raw_training.iloc[:, 76]\ny = LabelEncoder().fit_transform(y)","c71467a6":"# #define objective function\n# def objective(trial: Trial,X,y) -> float:\n    \n#     #create a data split for validation\n#     train_X,test_X,train_y,test_y = train_test_split(X, y, test_size = 0.30, random_state = 101)\n    \n#     #create a parameter space\n#     #Note: I started very general with wide intervals and honed in with repeated studies.\n#     #The history dataframe in the next cell is your friend.\n#     #Read the table to pare down your intervals after each study.\n    \n#     param = {                \n#                  'learning_rate':trial.suggest_uniform('learning_rate', 0.03, 0.06),\n#                  'gamma':trial.suggest_uniform('gamma', .2, .5),\n#                  'reg_alpha':trial.suggest_int('reg_alpha', 1, 5),\n#                  'reg_lambda':trial.suggest_int('reg_lambda', 1, 7),\n#                  'n_estimators':trial.suggest_int('n_estimators', 300, 500),\n#                  'colsample_bynode':trial.suggest_uniform('colsample_bynode', .2, .4),\n#                  'colsample_bylevel':trial.suggest_uniform('colsample_bylevel', .65, .75),\n#                  'subsample':trial.suggest_uniform('subsample', .55, .75),               \n#                  'min_child_weight':trial.suggest_int('min_child_weight', 100, 200),\n#                  'colsample_bytree':trial.suggest_uniform('colsample_bytree',0.2, .4)\n#             }\n    \n#     #set up the baseline model parameters.\n#     #be careful about how high you let max_depth go\n#     #this can lead to long training times and overfitting\n    \n#     model = XGBClassifier(objective='multi:softprob', eval_metric = \"mlogloss\", num_class = 9,\n#                           tree_method = 'gpu_hist', max_depth = 13, use_label_encoder=False, **param)\n    \n#     #fit to training sample\n#     model.fit(train_X, train_y)\n    \n#     #return the cv score\n#     return -cross_val_score(model, test_X, test_y, cv = 3, scoring = 'neg_log_loss').mean()","1c8acd24":"# #run an Optuna study\n# study = optuna.create_study(direction='minimize', sampler=TPESampler())\n# study.optimize(lambda trial : objective(trial, X, y), n_trials = 25)\n\n# #print our best outcome\n# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","f18f7b01":"# #take a look at the trial runs to pare down parameter intervals\n# hist = study.trials_dataframe()\n# hist = hist.sort_values(by = ['value'])\n# hist","69252802":"#here are the parameters from the study\nparms = {'learning_rate': 0.03817329673009776, 'gamma': 0.3993428240049768, 'reg_alpha': 3,\n         'reg_lambda': 1, 'n_estimators': 334, 'colsample_bynode': 0.2695766080178446,\n         'colsample_bylevel': 0.6832712495239914, 'subsample': 0.6999062848890633,\n         'min_child_weight': 100, 'colsample_bytree': 0.34663755614898173}\n\n#create the xgb model we will feed to the probabiltiy calibrator\nXGBmod = XGBClassifier(objective='multi:softprob',\n                         eval_metric = \"mlogloss\",\n                         num_class = 9,\n                         tree_method = 'gpu_hist',\n                         max_depth = 13,\n                         use_label_encoder=False, **parms)","3ab7d20a":"#take a look at the model perfomance                     \nXGBscores = -cross_val_score(XGBmod, X, y, cv = 5, scoring = 'neg_log_loss')\nprint(\"Mean log loss: \", XGBscores.mean())\nprint(\"Sigma of log loss: \", XGBscores.std())\n\n#make final fit of model\nXGBmod.fit(X, y)","9f7e9d18":"#grab test features\nX_test = raw_test.iloc[:,1:76]\nX_test = np.ascontiguousarray(X_test)\n\n#predict probabilities on test data\ntest_pred = XGBmod.predict_proba(X_test)\ntest_pred = pd.DataFrame(test_pred)\ntest_pred","2af4f528":"#prepare output and save\noutput = pd.DataFrame(raw_test.iloc[:,0])\n\noutput = output.merge(test_pred, left_index = True, right_index = True)\n\noutput.columns = [\"id\", \"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\",\n                  \"Class_5\", \"Class_6\", \"Class_7\", \"Class_8\", \"Class_9\"]\n\noutput.to_csv('submission.csv', index=False)","2f6ca2e4":"This notebook works by honing in on an XGBClassifier paramterization using Optuna.\n\nKaggle\/your own GPU is reccomended here since we are training hundreds of XGB models.","5b69088b":"Thanks to Adesh Bansode for starter code on optuna\n\nhttps:\/\/medium.com\/subex-ai-labs\/efficient-hyperparameter-optimization-for-xgboost-model-using-optuna-3ee9a02566b1"}}