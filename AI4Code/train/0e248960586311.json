{"cell_type":{"4ca98386":"code","ac3f34bf":"code","0fc62be6":"code","ec793bff":"code","789bcf25":"code","d51447e8":"code","1b76d368":"code","2b7db5dd":"code","66fcc785":"code","253c24ef":"code","bcbe9969":"code","bde64b61":"code","06ae8133":"code","83767c19":"code","2d855524":"code","8403d86d":"code","1089c4db":"code","d50cc1dd":"code","94e676d1":"markdown","efbe50c4":"markdown","8e613bbc":"markdown","9030ef5a":"markdown","23b62f49":"markdown","e7ad8474":"markdown","92357a65":"markdown","1f4d1e60":"markdown","ecf395b8":"markdown","cc800fc3":"markdown","2087754d":"markdown","7944fc12":"markdown","da4039c4":"markdown","7e55995a":"markdown","8bdafe71":"markdown","de9fcc64":"markdown","4b3581e4":"markdown","3d5dd2f0":"markdown"},"source":{"4ca98386":"## Importing necessary packages ##\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import transforms\nfrom torchvision.utils import make_grid\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL","ac3f34bf":"## Defining the transformation ##\n\naug = transforms.Compose([\n    transforms.Resize((28 , 28)),\n    transforms.ToTensor()\n])","0fc62be6":"## Downloading the dataset ##\n\nmnist_data = ImageFolder(root = '..\/input\/mnistasjpg\/trainingSet\/trainingSet' , \n                         transform = aug)\n\nprint('----Data Loaded Successfully----')\nprint('----Number of Images =' , len(mnist_data) , '----')\nprint('----Image dimension =' , mnist_data[45][0].shape , '----')","ec793bff":"## Visualizing a sample data ##\n\nrand_idx = int(np.random.randint(low = 0 , high = len(mnist_data) , size = 1))\n\nimg , label = mnist_data[rand_idx]\n\n\nprint('Label :' , label)\nplt.imshow(img.permute(1 , 2, 0))\nplt.show()","789bcf25":"## Setting our dataloader ##\n\nmnist_dataloader = DataLoader(dataset = mnist_data,\n                      batch_size = 32 , \n                      shuffle = True)\n\nprint('----Data Loader Implemented Successfully----')\nprint('----Number of Batches =' , len(mnist_dataloader) , '----')","d51447e8":"## Visualizing a batch of data ##\n\nfor img , _ in mnist_dataloader:\n    fig , ax = plt.subplots(figsize = (8 , 4))\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    \n    plt.imshow(make_grid(img).permute(1 , 2 , 0))\n    \n    break","1b76d368":"## Transfering the data into GPU ##\n\ndef get_device():\n    \n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    return torch.device('cpu')\n\n## Setting the device ##\n\ndevice = get_device()\n\n\ndef transfer_data(data , device):\n    \n    if isinstance(data , (list , tuple)):\n        return [transfer_data(each_data , device) for each_data in data]\n    \n    return data.to(device)\n\n## GPU DataLoader ##\n\nclass GpuDL:\n    \n    def __init__(self , data , device):\n        \n        self.data = data\n        self.device = device\n        \n    def __iter__(self):\n        \n        for each_data in self.data:\n            \n            yield transfer_data(each_data , self.device)\n    \n    def __len__(self):\n        \n        return len(self.data)","2b7db5dd":"## Setting our GPU Datalader ##\n\nmnist_dl = GpuDL(mnist_dataloader , device)\n\n## Visualizing ##\n\nfor img , _ in mnist_dl:\n    fig , ax = plt.subplots(figsize = (8 , 4))\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    \n    plt.imshow(make_grid(img.to('cpu')).permute(1 , 2 , 0))\n    \n    break","66fcc785":"## Making our model ##\n\n## Making Extractor ##\n\nclass CNN_Extractor(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.extractor = nn.Sequential(\n            nn.Conv2d(in_channels = 3 , out_channels = 64 , kernel_size = 3 , stride = 1 , padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2 , stride = 2), ## 14 * 14 ##\n            nn.Conv2d(in_channels = 64 , out_channels = 128 , kernel_size = 3 , stride = 1 , padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2 , stride = 2), ## 7 * 7 ##\n            nn.Conv2d(in_channels = 128 , out_channels = 256 , kernel_size = 3 , stride = 1 , padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2 , stride = 2), ## 3 * 3 ##\n            nn.Conv2d(in_channels = 256 , out_channels = 512 , kernel_size = 3 , stride = 1 , padding = 1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2 , stride = 2), ## 1 * 1 ##\n            nn.Flatten()\n        )\n        \n    def forward(self , x):\n        \n        out = self.extractor(x)\n        \n        return out\n    \n\n## Making the classifier ##\n\nclass Main_Classier(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(in_features = 512 , out_features = 10)\n        )\n        \n    def forward(self , x):\n        \n        out = self.classifier(x)\n        \n        return out\n    \n## Making the Confidence model ##\n\nclass ConfidNet(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.confid = nn.Sequential(\n            nn.Linear(in_features = 512 , out_features = 256),\n            nn.ReLU(),\n            nn.Linear(in_features = 256 , out_features = 128),\n            nn.ReLU(),\n            nn.Linear(in_features = 128 , out_features = 64),\n            nn.ReLU(),\n            nn.Linear(in_features = 64 , out_features = 32),\n            nn.ReLU(),\n            nn.Linear(in_features = 32 , out_features = 1)\n        )\n        \n    def forward(self , x):\n        \n        out = self.confid(x)\n        \n        return out","253c24ef":"## End to End Classifier ##\n\nclass MNIST_Classifier(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.feature_extractor = CNN_Extractor()\n        \n        self.classifier = Main_Classier()\n        \n    def forward(self , x):\n        \n        features = self.feature_extractor(x)\n        \n        out = self.classifier(features)\n        \n        return features , out","bcbe9969":"## Setting the model objects ##\n\nclass_model = MNIST_Classifier().to(device)\n\nconf_model = ConfidNet().to(device)","bde64b61":"## Classifier loss ##\n\nclass_loss = nn.CrossEntropyLoss()\n\n## Confidence Loss ##\n\nconf_loss = nn.MSELoss()","06ae8133":"## Classifier Optimizer ##\n\nclassifier_optimizer = torch.optim.Adam(class_model.parameters() , lr = 1e-3)\n\n## ConfidNet Optimizer ##\n\nconf_optimizer = torch.optim.Adam(conf_model.parameters() , lr = 1e-3)","83767c19":"## Accuracy utility function ##\n\ndef accuracy(pred , target):\n    num = (torch.sum(pred == target)).item()\n    den = pred.numel()\n    return num \/ den","2d855524":"## Training the model ##\n\nacc_list = list()\nloss_list = list()\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    \n    minibatch_acc = list()\n    minibatch_loss = list()\n    \n    for img , label in mnist_dl:\n        \n        \n        \n        classifier_optimizer.zero_grad()\n        \n        conf_optimizer.zero_grad()\n        \n        features , pred = class_model(img)\n        \n        c_loss = class_loss(pred , label)\n        \n        output = torch.argmax(pred , dim = 1)\n        \n        c_acc = accuracy(output , label)\n        \n        minibatch_acc.append(c_acc)\n        \n        minibatch_loss.append(c_loss.item())\n        \n        confidence = conf_model(features.detach())\n        \n        tcp = torch.gather(pred, dim=1, index=label.unsqueeze(1))\n        \n        cf_loss = conf_loss(confidence , tcp)\n        \n        c_loss.backward(retain_graph = True)\n        \n        cf_loss.backward()\n        \n        classifier_optimizer.step()\n        \n        conf_optimizer.step()\n    \n    epoch_acc = sum(minibatch_acc) \/ len(minibatch_acc)\n    acc_list.append(epoch_acc)\n    \n    epoch_loss = sum(minibatch_loss) \/ len(minibatch_loss)\n    loss_list.append(epoch_loss)\n    \n    print('Epoch : {} \/ {} --> Classification Accuracy = {:.2f} , Classification Loss = {:.2f} , Confidence Loss : {:.2f}'.format(epoch + 1 , \n                                                                                                                                  num_epochs , \n                                                                                                                                  epoch_acc ,\n                                                                                                                                  epoch_loss ,\n                                                                                                                                  cf_loss.item()))","8403d86d":"## Saving the model ##\n\ntorch.save(class_model , 'class_model.pth')\ntorch.save(conf_model , 'conf_model.pth')","1089c4db":"def prediction():\n    rand_idx = int(np.random.randint(low = 0 , high = len(mnist_data) , size = 1))\n    feature , pred = class_model(mnist_data[rand_idx][0].unsqueeze(0).to(device))\n    output = torch.argmax(pred , dim = 1)\n    confidence = nn.Sigmoid()(conf_model(feature))\n    print('Label is :' , output.item())\n    print('Confidence is :' , confidence.item())\n    plt.imshow(mnist_data[rand_idx][0].permute(1 , 2 , 0))","d50cc1dd":"prediction()","94e676d1":"Now save the models.","efbe50c4":"We make only resize transform. It is kind of unnecessary (Since they are already 28 * 28) but we do it just for the sake of it.","8e613bbc":"Now we need to set our dataloader.","9030ef5a":"Now we need to set our accuracy.","23b62f49":"Now like we always do, lets visualize a sample data.","e7ad8474":"Now lets set our Optimizers.","92357a65":"Now we will do something different here. We are going to make the classifier model separately, since it needs to stay fixed.","1f4d1e60":"Now lets visualize a batch of data.","ecf395b8":"Well, the ConfidNet model is not that special. It is just like any other model, but it shifts itself by having two outpoints.\n\nTo make ConfidNet what we need to do is make a CNN extractor. A smaller model will also work.\n\nThis extracted feature map then diverges into two parts:\n1. Goes out to make a classification as a normal classifier.\n2. The second one predicts a confidence value via a simple MLP.\n\nSo lets go!","cc800fc3":"With that done and dusted, lets set our loss functions.","2087754d":"In this notebook we are going to implement the Confidence Network based on the paper : Addressing Failure Prediction\nby Learning Model Confidence.","7944fc12":"Now lets train the models.","da4039c4":"## ConfidNet Implementation on MNIST Dataset","7e55995a":"Now with the dataset out of way.\n\nLet's make our masterpiece-- our ConfidNet model.","8bdafe71":"Now lets set the model objects.","de9fcc64":"Now lets build our prediction function.","4b3581e4":"So, lets get our hands dirty by importing the necessary packages.","3d5dd2f0":"Now moving on, we will fetch our dataset. But we are going to bring in after applying some transformation."}}