{"cell_type":{"3c2f72eb":"code","3bf365ba":"code","86cf44d5":"code","2a9c53cd":"code","0c78c2f0":"code","e7fa9487":"code","9b330a5a":"code","d01e2a2a":"code","470f00e4":"code","af178eae":"code","e566405f":"code","a704efba":"code","234531d0":"code","2ae41e9e":"code","802f808c":"code","e8817e04":"code","ec30cd27":"code","b49599ce":"code","71b399c5":"code","30e094af":"code","0b39f4f7":"code","1be67504":"code","36ef0c3b":"code","646cdfe9":"code","bf4b8e02":"code","f3818545":"code","42193883":"code","203c3790":"markdown","7e5b733f":"markdown","deb53986":"markdown","6c906907":"markdown","23bb0afc":"markdown","49b056bd":"markdown","86d5458d":"markdown","c01dcc5a":"markdown","07570e53":"markdown","9b11b64d":"markdown","22de1f83":"markdown","950cff04":"markdown","3d3e45e2":"markdown","f1464b22":"markdown"},"source":{"3c2f72eb":"import os\nimport time as t\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\n\nimport tensorflow.keras as k\nfrom keras.losses import categorical_crossentropy\nfrom keras.utils import np_utils\n\nprint('Keras version {}'.format(k.__version__))\n\nTI = t.time()","3bf365ba":"# Useful functions and shortcuts\ndef sp(int):\n    # Returns a blank string of length int\n    return ' ' * int\n\n\ndef timer(ti, tf, rnd = 2):\n    # Returns esapsed time in sec, min\n    dif = tf - ti\n    sc = round(dif, rnd)\n    print('{} sec'.format(sc))\n    return sc\n\n\ndef norm_pic(pic):\n    # Returns normalized array as type float32\n    return (pic - pic.mean()) \/ (pic.max() - pic.min()).astype('float32')\n\n\ndef batcher(num_pics):\n    # Returns dict of batch sizes for CNN input\n    lib = {'batch_size': []}\n    for i in range(10, num_pics):\n        batch_size = i + 1\n        num = num_pics % batch_size\n        if num == 0 and batch_size <= 150:\n            iters = int(num_pics \/ batch_size)\n            lib['batch_size'].append((batch_size, iters))\n    return lib\n\ndef reshaper(xtrain, xtest, ytrain, ytest):\n    xtr = np.asarray(xtrain).reshape(xtrain.shape[0], 28, 28, 1)\n    xtst = np.asarray(xtest).reshape(xtest.shape[0], 28, 28, 1)\n    ytr = np_utils.to_categorical(ytrain, 10)\n    ytst = np_utils.to_categorical(ytest, 10)\n    return (xtr, xtst, ytr, ytst)\n\n# Set random state for all classifiers\nrnd_st = 42","86cf44d5":"# File path\npath = '..\/input\/overheadmnist\/overhead\/'\npath_tr = path + 'training\/'\n\n# Save files as dataframes\ntrain = pd.read_csv(path + 'train.csv')\nlabels = pd.read_csv(path + 'labels.csv')\ntr_labels = labels[labels['dataset'] == 'train'].drop('dataset', axis = 1)[['image', 'class', 'label']]\nts_labels = labels[labels['dataset'] == 'test'].drop('dataset', axis = 1)[['image', 'class', 'label']]\nclasses = pd.read_csv(path + 'classes.csv')\n\n# Create master DF to export \nmaster_tr = tr_labels.join(train.drop('label', axis = 1))\nmaster_ts = ts_labels.join(train.drop('label', axis = 1))\n\n# Save master DFs for future notebooks\nmaster_tr.to_csv('master_tr.csv')\nmaster_ts.to_csv('master_ts.csv')\n\n# Reference lists\nclss_lst = classes['class'].values\n\n# Store useful values\ntot_pics = len(train)\nnum_classes = len(classes)\nresults_dict = {}\n\n_ = timer(TI, t.time())","2a9c53cd":"# Normalize arrays\nX = norm_pic(train.drop('label', axis = 1))\n\n# Create categorical labels\ny = train['label']\n\n# Split the trainig data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .2, \n                                                  stratify = y, random_state = rnd_st)","0c78c2f0":"from sklearn.neural_network import MLPClassifier\n\ny_dum = pd.get_dummies(y_train)\ny_val_dum = pd.get_dummies(y_val)\ny_dum.columns = clss_lst\ny_val_dum.columns = clss_lst\n\n# Instantiate the classifier\nmlp = MLPClassifier(random_state = rnd_st)\n\nst = t.time()\n# Fit the classifier to the training data\nmlp.fit(X_train, y_dum)\n\n# Makre predictions on evaluation data\nmlp_pred = mlp.predict_proba(X_val)\n\n_ = timer(st, t.time())","e7fa9487":"# Make predictions\ny_pred = mlp.predict(X_val)\n\n# Evaluate performance\nmlp_rpt = metrics.classification_report(y_val_dum, y_pred, output_dict = True,\n                                       target_names = clss_lst, zero_division = 1)\nacc = metrics.accuracy_score(y_val_dum, y_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_val_dum, y_pred)))\n\n# Save results\nmlp_results = pd.DataFrame(mlp_rpt).T\n\nresults_dict['Multi-Layer Perceptron'] = {'accuracy' : acc, \n                       'classification report' : mlp_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, mlp_results))","9b330a5a":"# Reshape input files \nX_tr, X_vl, y_tr, y_vl = reshaper(X_train, X_val, y_train, y_val)","d01e2a2a":"# Instantiate classifier\ncnn = k.Sequential()\n\n# Add a layers & compile\ncnn.add(k.layers.Conv2D(filters = 32, kernel_size = (3, 3)))\ncnn.add(k.layers.Flatten())\ncnn.add(k.layers.Dense(100))\ncnn.add(k.layers.Dense(10, activation = 'softmax'))\n\ncnn.compile(loss = 'categorical_crossentropy', \n            metrics = ['accuracy', k.metrics.CategoricalAccuracy()])\n\n# Fit and evaluate\nst = t.time()\n\ncnn.fit(X_tr, y_tr)\n\n# Make predictions on evaluation data\ncnn_pred = np.round(cnn.predict(X_vl))\n\n_ = timer(st, t.time())","470f00e4":"# Evaluate performance\ncnn_rpt = metrics.classification_report(y_vl, cnn_pred, output_dict = True, \n                                       target_names = clss_lst, zero_division = 1)\nacc = metrics.accuracy_score(y_vl, cnn_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, cnn_pred)))\n\n# Save results\ncnn_results = pd.DataFrame(cnn_rpt).T\n\nresults_dict['Convolutional Neural Network'] = {'accuracy' : acc, \n                       'classification report' : cnn_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, cnn_results))","af178eae":"from sklearn.ensemble import RandomForestClassifier\n\n# Instantiate \nfst = RandomForestClassifier(random_state = rnd_st)\n\nst = t.time()\n# Fit\nfst.fit(X_train, y_tr)\n\n# Predict\nfst_pred = fst.predict(X_val)\n\n_ = timer(st, t.time())","e566405f":"# Evaluate performance\nfst_rpt = metrics.classification_report(y_vl, fst_pred, output_dict = True, \n                                       target_names = clss_lst, zero_division = 1)\nacc = metrics.accuracy_score(y_vl, fst_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, fst_pred)))\n\n# Save results\nfst_results = pd.DataFrame(fst_rpt).T\n\nresults_dict['Random Forest'] = {'accuracy' : acc, \n                       'classification report' : fst_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, fst_results))","a704efba":"from sklearn.cluster import KMeans\n\nkmn = KMeans(n_clusters = 10, random_state = rnd_st)\n\nst = t.time()\nkmn.fit(X_train, y_train)\n\nkmn_pred = np.asarray(pd.get_dummies(kmn.predict(X_val)))\n\n_ = timer(st, t.time())","234531d0":"# Evaluate performance\nkmn_rpt = metrics.classification_report(y_vl, kmn_pred, output_dict = True, zero_division = 1)\nacc = metrics.accuracy_score(y_vl, kmn_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, kmn_pred)))\n\n# Save results\nkmn_results = pd.DataFrame(kmn_rpt).T\n\nresults_dict['K-Means'] = {'accuracy' : acc, \n                       'classification report' : kmn_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, kmn_results))","2ae41e9e":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\nst = t.time()\nknn.fit(X_train, y_dum)\n\nknn_pred = knn.predict(X_val)\n\n_ = timer(st, t.time())","802f808c":"# Evaluate performance\nknn_rpt = metrics.classification_report(y_vl, knn_pred, output_dict = True, zero_division = 1, \n                                       target_names = clss_lst)\nacc = metrics.accuracy_score(y_vl, knn_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, knn_pred)))\n\n# Save results\nknn_results = pd.DataFrame(knn_rpt).T\n\nresults_dict['K-Nearest Neighbors'] = {'accuracy' : acc, \n                       'classification report' : knn_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, knn_results))","e8817e04":"from sklearn.svm import SVC\n\nsvm = SVC()\n\nst = t.time()\nsvm.fit(X_train, y_train)\n\nsvm_pred = np.asarray(pd.get_dummies(svm.predict(X_val)))\n\n_ = timer(st, t.time())","ec30cd27":"# Evaluate performance\nsvm_rpt = metrics.classification_report(y_vl, svm_pred, output_dict = True, zero_division = 1, \n                                       target_names = clss_lst)\nacc = metrics.accuracy_score(y_vl, svm_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, svm_pred)))\n\n# Save results\nsvm_results = pd.DataFrame(svm_rpt).T\n\nresults_dict['Support Vector Machine'] = {'accuracy' : acc, \n                       'classification report' : svm_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, svm_results))","b49599ce":"from sklearn.linear_model import LogisticRegression\n\nlgc = LogisticRegression()\n\nst = t.time()\nlgc.fit(X_train, y_train)\n\nlgc_pred = np.asarray(pd.get_dummies(lgc.predict(X_val)))\n\n_ = timer(st, t.time())","71b399c5":"# Evaluate performance\nlgc_rpt = metrics.classification_report(y_vl, lgc_pred, output_dict = True, zero_division = 1, \n                                       target_names = clss_lst)\nacc = metrics.accuracy_score(y_vl, lgc_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, lgc_pred)))\n\n# Save results\nlgc_results = pd.DataFrame(lgc_rpt).T\n\nresults_dict['Logistic Regression'] = {'accuracy' : acc, \n                       'classification report' : lgc_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, lgc_results))","30e094af":"from sklearn.ensemble import AdaBoostClassifier\n\nadb = AdaBoostClassifier()\n\nadb.fit(X_train, y_train)\n\nst = t.time()\nadb_pred = np.asarray(pd.get_dummies(adb.predict(X_val)))\n\n_ = timer(st, t.time())","0b39f4f7":"# Evaluate performance\nadb_rpt = metrics.classification_report(y_vl, adb_pred, output_dict = True, \n                                        zero_division = 1, target_names = clss_lst)\nacc = metrics.accuracy_score(y_vl, adb_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_vl, adb_pred)))\n\n# Save results\nadb_results = pd.DataFrame(adb_rpt).T\n\nresults_dict['AdaBoost'] = {'accuracy' : acc, \n                       'classification report' : adb_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, adb_results))","1be67504":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\n\nst = t.time()\nsgd.fit(X_train, y_train)\n\nsgd_pred = sgd.predict(X_val)\n\n_ = timer(st, t.time())","36ef0c3b":"# Evaluate performance\nsgd_rpt = metrics.classification_report(y_val, sgd_pred, output_dict = True, zero_division = 1,\n                                        target_names = clss_lst)\nacc = metrics.accuracy_score(y_val, sgd_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_val, sgd_pred)))\n\n# Save results\nsgd_results = pd.DataFrame(sgd_rpt).T\n\nresults_dict['Stochastic Gradient Descent'] = {'accuracy' : acc, \n                       'classification report' : sgd_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, sgd_results))","646cdfe9":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\nst = t.time()\ngnb.fit(X_train, y_train)\n\ngnb_pred = gnb.predict(X_val)\n\n_ = timer(st, t.time())","bf4b8e02":"# Evaluate performance\ngnb_rpt = metrics.classification_report(y_val, gnb_pred, output_dict = True, zero_division = 1,\n                                        target_names = clss_lst)\nacc = metrics.accuracy_score(y_val, gnb_pred)\nconf = dict(zip(clss_lst, metrics.multilabel_confusion_matrix(y_val, gnb_pred)))\n\n# Save results\ngnb_results = pd.DataFrame(gnb_rpt).T\n\nresults_dict['Naive Bayes'] = {'accuracy' : acc, \n                       'classification report' : gnb_rpt, \n                       'confusion matrix' : conf}\n\nprint('accuracy: {}\\n\\n{}'.format(acc, gnb_results))","f3818545":"import json\n\nresultsDF = pd.DataFrame(results_dict)\nresultsDF.to_json('baselines.json')\n\n# Calculate total elapsed run time\nTF = t.time()\ntimer(TI, TF)","42193883":"# Accuracy comparison\nresultsDF.T['accuracy']","203c3790":"# Summary\nA Support Vector Machine model shows the best 'out-of-the-box' performance. Gaussian Naive Bayes and Multi-Layer Perceptron models achieve > 50% accuracy. All other models fall below this threshold, averaging 42%. The least accurate model is K-Means at 9.6%. Two models, MLP and LR, both failed to converge. \n\nFinal scores and confusion matrices are exported in json format as 'baselines.json'.\n## Next Steps\n* Complete EDA\n* Image Processing\n* Hyperparameter optimization","7e5b733f":"# 3) Model Creation & Training\n## Multi-Layer Perceptron (Neural Network)","deb53986":"## Random Forest","6c906907":"## AdaBoost","23bb0afc":"### Create training and validation sets","49b056bd":"## K-Nearest Neighbors","86d5458d":"## K-Means","c01dcc5a":"## Stochastic Gradient Descent","07570e53":"## Naive Bayes","9b11b64d":"# 2) Prepare Overhead-MNIST","22de1f83":"# Purpose:\nInstantiate, train, and evaluate multiple classification models, establishing baseline performance metrics for each.\n## Implementation:\nMachine learning libraries used include scikit-learn, scikit-image, and Tensorflow's Keras API. Training and evaluation will occur with normalized arrays using the untuned, \"out-of-the-box\" classifiers seen below.\n* Multi-layer Perceptron\n* Convolutional Neural Network\n* Random Forest\n* K-Means\n* K-Nearest Neighbors\n* Support Vector Machine\n* Logistic Regression\n* AdaBoost\n* Stochastic Gradient Descent\n* Naive Bayes\n\n### Import Required Libraries","950cff04":"## Logistic Regression","3d3e45e2":"## Support Vector Machine","f1464b22":"## Convolutional Neural Network"}}