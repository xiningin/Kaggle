{"cell_type":{"547b6e0a":"code","7f6f1dc3":"code","8318810c":"code","c08de4d1":"code","93807425":"code","5663ba01":"code","af998467":"code","6cd91936":"code","9f777ffb":"code","ce4d81f5":"code","71d1698e":"code","d4947b15":"code","3d90571e":"code","19fd9be6":"code","56dc50cd":"code","5916cfed":"code","0f2745e9":"code","26484860":"code","60b52b5f":"code","fbe2b762":"code","b2395324":"code","76d25f41":"code","e9a310bc":"code","091890e2":"code","ee298873":"code","abcea9ff":"code","a95fbc7e":"code","66ddb6a0":"code","52b0bab4":"code","441fdda2":"code","405704d3":"code","ec9fb58c":"code","bf05a2cb":"code","b7daae1e":"code","5c253061":"code","4426326b":"code","11f457dd":"code","f7ec9b49":"code","6cb504da":"code","0c1f0ce4":"code","295bb76f":"code","847f2f63":"code","49c5ee26":"code","be43538c":"code","6d995848":"code","371698f7":"code","99a076a1":"code","597ddce5":"code","790645b8":"code","c99477c7":"code","3bb37f9b":"code","54daec25":"code","720f2a36":"code","40a58282":"code","c79af755":"code","7bbae049":"code","1b0b792a":"code","2b31f5ad":"code","c643a176":"code","d4ec82db":"code","0852d3ad":"code","691ebdd5":"code","ada39093":"code","c2ae4132":"code","1df3ddea":"code","d5f77d31":"code","00001c88":"code","b9192160":"code","c984912e":"code","d5f0d104":"code","2e893265":"code","2128958e":"code","b593cf82":"markdown","ca3efbb6":"markdown","98741397":"markdown","f74ae3d1":"markdown","e1f89a01":"markdown","767fd01c":"markdown","62989132":"markdown","ccc436bb":"markdown","fdf503f8":"markdown","cbe0c189":"markdown","c9fe8c66":"markdown","7a3c186d":"markdown","03ccec57":"markdown","e6519eb7":"markdown","3098740f":"markdown","2dacf430":"markdown","f2af1ff6":"markdown","b4df1875":"markdown","b9a0ed80":"markdown","282460b7":"markdown","7b4bf675":"markdown","47af5a8f":"markdown","b00f2d87":"markdown","f704c8ea":"markdown","726db71c":"markdown","e74ab740":"markdown","1c4953ed":"markdown","31b00b1e":"markdown","432946e4":"markdown","7805009b":"markdown","4b945a0f":"markdown","d353c79b":"markdown","6d660f99":"markdown","cd10d4e7":"markdown","82090155":"markdown","c46bdefc":"markdown","4c8a6f06":"markdown","28df86e9":"markdown","d97d7452":"markdown","92a4b5cc":"markdown","b18fbeec":"markdown","b3894b17":"markdown","377e6c9f":"markdown","20e6644b":"markdown","404cd551":"markdown","7c671610":"markdown","aeba7566":"markdown","a2968a11":"markdown","61c0be6c":"markdown","42930d20":"markdown","57748357":"markdown","181dabc0":"markdown","1ed962a9":"markdown","529d1811":"markdown","212154c5":"markdown","0f75e75e":"markdown","9fe84c06":"markdown","e378c04a":"markdown","bc76523c":"markdown","9aa281f8":"markdown","707af917":"markdown","43d3718a":"markdown","1f615c8c":"markdown","052480d0":"markdown","e000908b":"markdown","7e3169d8":"markdown"},"source":{"547b6e0a":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\n\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n\n# display all the outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","7f6f1dc3":"# utility functions\ndef get_col_stats(df):\n    temp_nulls = df.isnull().sum()\n    temp = pd.DataFrame({\"nulls\": temp_nulls, \"null_percent\": temp_nulls*100\/df.shape[0]})\n    \n    uniqs = []\n    for col in df.columns:\n        uniqs.append(df[col].unique().shape[0])\n    temp[\"uniqs\"] = uniqs\n    return temp\n\ndef get_categorical_stats(df, col):\n    counts = df[col].value_counts(dropna=False)\n    temp = pd.DataFrame({\n        \"counts\": counts,\n        \"count_per\": counts * 100 \/ df.shape[0],\n    })\n    return temp\n\ndef numeric_to_cat(df, cols):\n    for col in cols:\n        df[col] = df[col].astype(\"category\")\n    return df\n","8318810c":"DATA_ROOT = Path(\"..\/input\/\")\n\ncard_f = DATA_ROOT \/ \"train.csv\"\nmerchant_f = DATA_ROOT \/ \"merchants.csv\"\nhist_f = DATA_ROOT \/ \"historical_transactions.csv\"\nnew_hist_f = DATA_ROOT \/ \"new_merchant_transactions.csv\"\n\ntest_f = DATA_ROOT \/ \"test.csv\"","c08de4d1":"card_df = pd.read_csv(card_f)\ncard_df.shape\ncard_df.head()","93807425":"test_df = pd.read_csv(test_f)\ntest_df.shape\ntest_df.head()","5663ba01":"card_df.card_id.unique().shape\ncard_df.card_id.unique().shape[0] == card_df.shape[0]","af998467":"test_df.card_id.unique().shape\ntest_df.card_id.unique().shape[0] == test_df.shape[0]","6cd91936":"len(set(test_df.card_id).intersection(set(card_df.card_id)))","9f777ffb":"print(\"Training data:\")\ncard_df.isna().sum()\nprint(\"Testing data:\")\ntest_df.isna().sum()","ce4d81f5":"test_df[test_df.first_active_month.isnull()]","71d1698e":"ax = card_df.target.plot.hist(bins=20, figsize=(10, 5))\n_ = ax.set_title(\"target histogram\")\nplt.show()\n\nfig, axs = plt.subplots(1,2, figsize=(20, 5))\n_ = card_df.target[card_df.target > 10].plot.hist(ax=axs[0])\n_ = axs[0].set_title(\"target histogram for values greater than 10\")\n_ = card_df.target[card_df.target < -10].plot.hist(ax=axs[1])\n_ = axs[1].set_title(\"target histogram for values less than -10\")\nplt.show()\n\ncard_df.target.describe()\n","d4947b15":"card_df[\"target_sign\"] = card_df.target.apply(lambda x: 0 if x <= 0 else 1)\ncard_df.target_sign.value_counts()","3d90571e":"print(\"feature_1\")\npd.DataFrame({\"counts\": card_df.feature_1.value_counts(), \"counts_per\": card_df.feature_1.value_counts()*100\/card_df.shape[0]})\nprint(\"feature_2\")\npd.DataFrame({\"counts\": card_df.feature_2.value_counts(), \"counts_per\": card_df.feature_2.value_counts()*100\/card_df.shape[0]})\nprint(\"feature_3\")\npd.DataFrame({\"counts\": card_df.feature_3.value_counts(), \"counts_per\": card_df.feature_3.value_counts()*100\/card_df.shape[0]})","19fd9be6":"temp = card_df.first_active_month.value_counts().sort_index()\nax = temp.plot(figsize=(10, 5))\n_ = ax.set_xticklabels(range(2010, 2019))\n_ = ax.set_title(\"Distribution across years\")","56dc50cd":"card_df[\"yr\"] = card_df.first_active_month.str.split(\"-\").str[0]\ncard_df[\"month\"] = card_df.first_active_month.str.split(\"-\").str[1]\ncard_df.head()","5916cfed":"temp = get_categorical_stats(card_df, \"yr\")\ntemp\n\nax = temp.counts.sort_index().plot()\n_ = ax.set_xticklabels(range(2010, 2019))","0f2745e9":"temp = get_categorical_stats(card_df, \"month\")\ntemp\n\nax = temp.counts.sort_index().plot()\n_ = ax.set_xticklabels(range(-1, 13, 2))","26484860":"card_df[\"card_id_dec\"] = card_df.card_id.str.split(\"_\").str[2].apply(lambda x: int(x, 16))\n\ncard_df.card_id.str.split(\"_\").str[0].unique()\ncard_df.card_id.str.split(\"_\").str[1].unique()\ncard_df.card_id_dec.describe()","60b52b5f":"card_df[[\"card_id_dec\", \"first_active_month\"]].sort_values(\"card_id_dec\")","fbe2b762":"_ = card_df[[\"feature_1\", \"target\"]].plot.scatter(x=\"feature_1\", y=\"target\")\n_ = card_df[[\"feature_2\", \"target\"]].plot.scatter(x=\"feature_2\", y=\"target\")\n_ = card_df[[\"feature_3\", \"target\"]].plot.scatter(x=\"feature_3\", y=\"target\")","b2395324":"card_df.groupby([\"yr\", \"feature_1\"])[\"month\"].count()","76d25f41":"card_df.groupby([\"yr\", \"feature_2\"])[\"month\"].count()","e9a310bc":"merc_df = pd.read_csv(merchant_f)\n\nminus_1_to_nan_cols = [\"city_id\", \"state_id\", \"merchant_group_id\",\n                      \"merchant_category_id\", \"subsector_id\"]\nfor col in minus_1_to_nan_cols:\n    merc_df[col] = merc_df[col].replace(-1, pd.np.nan)\n\nnum_to_cat_cols = [\"category_2\", \"city_id\", \"state_id\",\n                  \"merchant_group_id\", \"merchant_category_id\",\n                   \"subsector_id\"]\nmerc_df = numeric_to_cat(merc_df, num_to_cat_cols)\n\nmerc_df.shape\nmerc_df.head()","091890e2":"merc_df.merchant_id.unique().shape\nmerc_df.merchant_id.unique().shape[0] == merc_df.shape[0]","ee298873":"temp = merc_df.merchant_id.value_counts()\ntemp[temp > 1]","abcea9ff":"merc_df[merc_df.merchant_id == \"M_ID_d123532c72\"]","a95fbc7e":"temp_nulls = merc_df.isnull().sum()\ntemp = pd.DataFrame({\"nulls\": temp_nulls, \"null_percent\": temp_nulls*100\/merc_df.shape[0]})\ntemp","66ddb6a0":"merc_df[[\"numerical_1\", \"numerical_2\"]].describe()","52b0bab4":"fig, ax = plt.subplots(1, 2)\n_ = merc_df[[\"numerical_1\"]].boxplot(ax=ax[0])\n_ = merc_df[[\"numerical_2\"]].boxplot(ax=ax[1])\n\nfig, ax = plt.subplots(1, 2)\n_ = merc_df.loc[merc_df.numerical_1 < 0.1, [\"numerical_1\"]].boxplot(ax=ax[0])\n_ = merc_df.loc[merc_df.numerical_2 < 0.01, [\"numerical_2\"]].boxplot(ax=ax[1])\nplt.tight_layout()","441fdda2":"_ = merc_df[[\"numerical_1\", \"numerical_2\"]].plot.scatter(x=\"numerical_1\", y=\"numerical_2\")","405704d3":"print(\"category_1\")\nget_categorical_stats(merc_df, \"category_1\")\nprint(\"category_2\")\nget_categorical_stats(merc_df, \"category_2\")\nprint(\"category_4\")\nget_categorical_stats(merc_df, \"category_4\")","ec9fb58c":"merc_df.groupby([\"category_4\", \"category_2\"])[\"category_1\"].count()","bf05a2cb":"print(\"Ratio of category_4 with value N to value Y based on category_2 freqeuncy counts\")\ntemp1 = merc_df.loc[merc_df.category_4 == \"N\", \"category_2\"].value_counts(dropna=False)\ntemp2 = merc_df.loc[merc_df.category_4 == \"Y\", \"category_2\"].value_counts(dropna=False)\ntemp1\/temp2","b7daae1e":"merc_df.groupby([\"category_1\", \"category_2\"])[\"category_4\"].count()","5c253061":"merc_df.loc[merc_df.category_1 == \"Y\", \"category_2\"].value_counts(dropna=False)","4426326b":"merc_df.groupby([\"category_1\", \"category_4\"])[\"category_2\"].count()","11f457dd":"print(\"city_id\")\nget_categorical_stats(merc_df, \"city_id\")","f7ec9b49":"print(\"state_id\")\nget_categorical_stats(merc_df, \"state_id\")","6cb504da":"merc_df[merc_df.state_id.isna()].category_2.value_counts(dropna=False)","0c1f0ce4":"get_col_stats(merc_df[[\"merchant_id\", \"merchant_group_id\", \"merchant_category_id\", \"subsector_id\"]])","295bb76f":"(\n    merc_df.merchant_id.astype(str)\\\n    + merc_df.merchant_group_id.astype(str)\n).unique().shape\nmerc_df.shape","847f2f63":"print(\"most_recent_sales_range\")\nget_categorical_stats(merc_df, \"most_recent_sales_range\")\nprint(\"most_recent_purchases_range\")\nget_categorical_stats(merc_df, \"most_recent_purchases_range\")","49c5ee26":"merc_df.groupby([\"most_recent_sales_range\", \"most_recent_purchases_range\"])\\\n    .merchant_id\\\n    .count()","be43538c":"fig, axs = plt.subplots(2, 3, figsize=(10, 5))\n\ni = 0\nj = 0\nfor val in merc_df.most_recent_purchases_range.unique():\n    ax = axs[i, j]\n    _ = merc_df\\\n        .loc[merc_df.most_recent_purchases_range == val, \"most_recent_sales_range\"]\\\n        .value_counts()\\\n        .plot.bar(ax=ax)\n    _ = ax.set_title(f\"most_recent_purchases_range = {val}\")\n    if i==0 and j==2:\n        i = 1\n        j = 0\n    else:\n        j += 1\n\nplt.tight_layout()","6d995848":"merc_df[[\"avg_sales_lag3\", \"avg_purchases_lag3\", \"active_months_lag3\"]].describe()","371698f7":"get_categorical_stats(merc_df, \"active_months_lag3\")","99a076a1":"fig, axs = plt.subplots(1, 2)\n_ = merc_df[[\"avg_sales_lag3\"]].boxplot(ax=axs[0])\n_ = merc_df[[\"avg_purchases_lag3\"]].boxplot(ax=axs[1])\nplt.tight_layout()","597ddce5":"fig, axs = plt.subplots(1, 2)\n_ = merc_df.loc[merc_df.avg_sales_lag3 < 10, [\"avg_sales_lag3\"]].boxplot(ax=axs[0])\n_ = merc_df.loc[merc_df.avg_purchases_lag3 < 10, [\"avg_purchases_lag3\"]].boxplot(ax=axs[1])\nplt.tight_layout()","790645b8":"temp = merc_df.loc[(merc_df.avg_sales_lag3 < 10) & (merc_df.avg_sales_lag3 > -10), [\"avg_sales_lag3\", \"avg_purchases_lag3\"]]\ntemp.plot.scatter(x=\"avg_sales_lag3\", y=\"avg_purchases_lag3\")","c99477c7":"merc_df[[\"avg_sales_lag6\", \"avg_purchases_lag6\", \"active_months_lag6\"]].describe()","3bb37f9b":"get_categorical_stats(merc_df, \"active_months_lag6\")","54daec25":"fig, axs = plt.subplots(1, 2)\n_ = merc_df[[\"avg_sales_lag6\"]].boxplot(ax=axs[0])\n_ = merc_df[[\"avg_purchases_lag6\"]].boxplot(ax=axs[1])\nplt.tight_layout()","720f2a36":"fig, axs = plt.subplots(1, 2)\n_ = merc_df.loc[merc_df.avg_sales_lag6 < 10, [\"avg_sales_lag6\"]].boxplot(ax=axs[0])\n_ = merc_df.loc[merc_df.avg_purchases_lag6 < 10, [\"avg_purchases_lag6\"]].boxplot(ax=axs[1])\nplt.tight_layout()","40a58282":"temp = merc_df.loc[(merc_df.avg_sales_lag6 < 10) & (merc_df.avg_sales_lag6 > -10), [\"avg_sales_lag6\", \"avg_purchases_lag6\"]]\ntemp.plot.scatter(x=\"avg_sales_lag6\", y=\"avg_purchases_lag6\")","c79af755":"merc_df[[\"avg_sales_lag12\", \"avg_purchases_lag12\", \"active_months_lag12\"]].describe()","7bbae049":"get_categorical_stats(merc_df, \"active_months_lag12\")","1b0b792a":"fig, axs = plt.subplots(1, 2)\n_ = merc_df[[\"avg_sales_lag12\"]].boxplot(ax=axs[0])\n_ = merc_df[[\"avg_purchases_lag12\"]].boxplot(ax=axs[1])\nplt.tight_layout()","2b31f5ad":"fig, axs = plt.subplots(1, 2)\n_ = merc_df.loc[merc_df.avg_sales_lag12 < 10, [\"avg_sales_lag12\"]].boxplot(ax=axs[0])\n_ = merc_df.loc[merc_df.avg_purchases_lag12 < 10, [\"avg_purchases_lag12\"]].boxplot(ax=axs[1])\nplt.tight_layout()","c643a176":"temp = merc_df.loc[(merc_df.avg_sales_lag12 < 10) & (merc_df.avg_sales_lag12 > -10), [\"avg_sales_lag12\", \"avg_purchases_lag12\"]]\ntemp.plot.scatter(x=\"avg_sales_lag12\", y=\"avg_purchases_lag12\")","d4ec82db":"fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfilt = (merc_df.avg_sales_lag3 < 10) & (merc_df.avg_sales_lag3 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_sales_lag3\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_sales_lag3\", ax=axs[0])\n\nfilt = (merc_df.avg_sales_lag6 < 10) & (merc_df.avg_sales_lag6 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_sales_lag6\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_sales_lag6\", ax=axs[1])\n\nfilt = (merc_df.avg_sales_lag12 < 10) & (merc_df.avg_sales_lag12 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_sales_lag12\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_sales_lag12\", ax=axs[2])","0852d3ad":"fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfilt = (merc_df.avg_purchases_lag3 < 10) & (merc_df.avg_purchases_lag3 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_purchases_lag3\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_purchases_lag3\", ax=axs[0])\n\nfilt = (merc_df.avg_purchases_lag6 < 10) & (merc_df.avg_purchases_lag6 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_purchases_lag6\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_purchases_lag6\", ax=axs[1])\n\nfilt = (merc_df.avg_purchases_lag12 < 10) & (merc_df.avg_purchases_lag12 > -10)\ntemp = merc_df.loc[filt, [\"numerical_1\", \"avg_purchases_lag12\"]]\n_ = temp.plot.scatter(x=\"numerical_1\", y=\"avg_purchases_lag12\", ax=axs[2])","691ebdd5":"merc_df.head()","ada39093":"lag_cols = [\n    \"avg_sales_lag3\", \"avg_purchases_lag3\", \"active_months_lag3\",\n    \"avg_sales_lag6\", \"avg_purchases_lag6\", \"active_months_lag6\",\n    \"avg_sales_lag12\", \"avg_purchases_lag12\", \"active_months_lag12\"\n]\ncorr = merc_df[lag_cols].corr()\ncorr.style.background_gradient()","c2ae4132":"filt = (merc_df.state_id.isna()) & (merc_df.category_2.isna())\ntemp = merc_df.loc[filt]\ntemp.head()","1df3ddea":"hist_df = pd.read_csv(hist_f)\nhist_df.shape\nhist_df.head()","d5f77d31":"len(set(hist_df.card_id) - set(card_df.card_id) - set(test_df.card_id))","00001c88":"set(hist_df.merchant_id) - set(merc_df.merchant_id)","b9192160":"temp_nulls = hist_df.isnull().sum()\ntemp = pd.DataFrame({\"nulls\": temp_nulls, \"null_percent\": temp_nulls\/hist_df.shape[0]})\ntemp","c984912e":"new_hist_df = pd.read_csv(new_hist_f)\nnew_hist_df.shape\nnew_hist_df.head()","d5f0d104":"len(set(new_hist_df.card_id) - set(card_df.card_id) - set(test_df.card_id))","2e893265":"set(new_hist_df.merchant_id) - set(merc_df.merchant_id)","2128958e":"temp_nulls = new_hist_df.isnull().sum()\ntemp = pd.DataFrame({\"nulls\": temp_nulls, \"null_percent\": temp_nulls\/new_hist_df.shape[0]})\ntemp","b593cf82":"Observations:\n- Quantity of active months within last 3 months, is 3 in 99.5% of data meaning, they were active during all the 3 months","ca3efbb6":"Observations:\n\n- Negative and positive target values are almost in the same proportion\n\n### 2. Anonymised Features\n\nfeature_1, feature_2, feature_3","98741397":"Most of the columns have same values, the ```_lag``` columns are different. We can try deduping the rows by taking an average of those values.","f74ae3d1":"### 4. Merchant ID cols\n\nmerchant_id, merchant_group_id, merchant_category_id, subsector_id","e1f89a01":"Observations:\n- Big tail after IQR (many outliers) for both the columns","767fd01c":"There's no overlapping cards in the training and testing datasets.","62989132":"New merchants file also has nulls in the ```merchant_id```. Other than that every ```merchant_id``` is present in the merchants file.","ccc436bb":"Observations:\n- both ```avg_purchases_lag12``` and ```avg_sales_lag12``` have a few outliers in the extremes ","fdf503f8":"Except the id cols, we'll not go into other columns. Among ids, only ```merchant_id``` column has 178,159 nulls, which is 0.0061% of the total data. We'll need to handle these when we are creating the final data","cbe0c189":"## Sanity Check\n\n- Unique ```merchant_id```s\n- Nulls","c9fe8c66":"Observations:\n- ```avg_purchases_lag``` columns show similar patterns according when plotted corresponding to ```numerical_1```.\n","7a3c186d":"There are no new ```card_id```s which are not there in training or test set.","03ccec57":"Observations:\n\n- last 6 months (July to December) has relatively more data than first 6 months (January to June)\n\n### 4. card_id","e6519eb7":"Great!! No nulls in the id columns.\n\n## Exploration\n\n### 1. Anonymised measure\nnumerical_1, numerical_1","3098740f":"Observations:\n- There are 24 unique states; not sure if this is US (US has 50 states)\n- 3.5% nulls\n\n```state_id``` and ```category_2``` has same number of nulls - 11,887. Let's see if they occur together","2dacf430":"Observations:\n- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10\n- it seems to be between 0 and 10\n- Need to handle the outliers\n- there is not apparent pattern between the columns though","f2af1ff6":"Observations:\n- category_1 is a binary column (N\/Y)\n- category_4 is a binary column (N\/Y)\n- category_1 and category_4 have very high number of N\n- category_2 has 5 categories and also nulls (11,887)\n- category_2 seems like a ratings column with 1 being the most frequent rating followed by 5 and 3.","b4df1875":"### 7. lag_6 columns\n\n- ```avg_sales_lag6```: Monthly average of revenue in last 6 months divided by revenue in last active month\n- ```avg_purchases_lag6```: Monthly average of transactions in last 6 months divided by transactions in last active month\n- ```active_months_lag6```: Quantity of active months within last 6 months\n","b9a0ed80":"Observations:\n- All the merchants under category_1 as Y have all nulls in category_2\n- Remove category_1 column when using in training as it's not adding much information\n","282460b7":"Observations:\n\n- tried to see if there was any pattern in the ids.\n- converted hex values to decimal, but when we see it according to the ```first_active_month``` then there is no apparent order in the ids\n\n\n### 5. Anonimised features vs target","7b4bf675":"### 12. Deduping merchant data based on ```merchant_id```\n\nwork in progress","47af5a8f":"Since ```merchant_id``` is not uniquely identifying each row in the table, I tried to find some compound key. Unfortunately, no such key is there. Data will need to be deduped on the ```merchant_id``` as we discussed during the sanity check part.","b00f2d87":"Observations:\n\n- feature_1, feature_2, feature_3, all are categorical variables\n- feature_1 has 5 unique values\n- feature_2 has 3 unique values\n- feature_3 is a binary column\n\n### 3. first_active_month","f704c8ea":"### 9. numerical_1 vs other columns","726db71c":"Observations:\n\n- Most of the data lies in the years ranging from 2016 to 2018\n","e74ab740":"Observations:\n- ```avg_sales_lag``` columns show similar patterns according when plotted corresponding to ```numerical_1```.\n","1c4953ed":"### 13. ```category_2``` vs lag columns","31b00b1e":"Observations:\n- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10\n- it seems to be between 0 and 10\n- Need to handle the outliers\n- there is not apparent pattern between the columns though","432946e4":"Observations:\n- Quantity of active months within last 6 months, is 3 in 97.8% of data meaning, they were active during all the 6 months","7805009b":"Observations:\n- Quantity of active months within last 12 months, is 3 in 91.1% of data meaning, they were active during all the 12 months","4b945a0f":"### 2. Anon Categories\ncategory_1, category_2, category_4","d353c79b":"### 11. looking at the slice of data where ```state_id``` and ```category_2``` are nulls\n","6d660f99":"Observations:\n- No nulls in both the columns\n- ```numerical_1``` values ranges from -0.057 to 183.735\n- ```numerical_2``` values ranges from -0.008 to 182.097\n- 75th quantile is at -0.047 for both ```numerical_1``` and ```numerical_2```\n- max value for both is very close","cd10d4e7":"# New Historical Data","82090155":"No extra ```card_id``` in the new merchants file.","c46bdefc":"Few ```merchant_id```s have more than one rows. Lets investigate some of them.","4c8a6f06":"Observations:\n- both ```avg_purchases_lag6``` and ```avg_sales_lag6``` have a few outliers in the extremes ","28df86e9":"### 8. lag_12 columns\n\n- ```avg_sales_lag12```: Monthly average of revenue in last 12 months divided by revenue in last active month\n- ```avg_purchases_lag12```: Monthly average of transactions in last 12 months divided by transactions in last active month\n- ```active_months_lag12```: Quantity of active months within last 12 months\n\nwork in progress","d97d7452":"Observations:\n\n- Value 3 (most frequent category in the whole feature_1 column) is at all time high across all the years except 2018\n- Value 2 is another value which is 2nd highest after 2015\n\n### 7. Year vs feature_2","92a4b5cc":"Observations:\n- Both the columns have 5 unique values\n- No nulls are present\n- most_recent_sales_range give the range of revenue (monetary units) in last active month  \n- most_recent_purchases_range gives the range of quantity of transactions in last active month\n- the above plots show for each transactions bucket, the distribution across the revenue bucket\n- The lowest transactions bucket (E), highest is the lowest revenue bucket (E) showing that most of the merchants are small in size and most of the transactions happening with them are low\n- similarly, A corresponds to A, B to B, C to C and D to D for sales and revenue in the highest values\n- Can I say these columns are correlated?","b18fbeec":"## Sanity Check\n\n- All ```card_id```s in training and testing set\n- All ```merchant_id```s in merchant file\n- Nulls\n","b3894b17":"### 10. Correlation in lag columns\n\n","377e6c9f":"Observations:\n\n- Values range from -33.2 to 17.9\n- -33 seems like an outlier as can be seen in the 3rd plot\n- other values less than -10 also seem like outliers due to very less in number\n- All values above 10 are also looking like outliers\n","20e6644b":"## Sanity Check\n\n- Unique ```card_id```'s in training data\n- Unique ```card_id```'s in testing data\n- Overlap of cards in training and testing data\n- Nulls","404cd551":"### 3. Location ID cols\n\ncity_id, state_id","7c671610":"There are no duplicate ```card_id```'s in the test set.","aeba7566":"Observations:\n- Ratio of merchants with N to Y is 3.4 for category_2 as 1\n- Ratio of merchants with N to Y is 4.2 for category_2 as 5\n- All others revolve around 1.5","a2968a11":"# Card Data\n\n\n","61c0be6c":"Observations:\n\n- all anonymised features are similar in value ranges for the target column\n- -33 target value is quite distinct across all the variables\n- Maybe, -33 is a default value of loyalty score.\n\n\n### 6. Year vs feature_1\n","42930d20":"Yup, nulls in both the cols - ```state_id``` and ```category_2``` - occur together. We'll investigate this slice after we are done with the remaining columns in the merchant table.","57748357":"Observations:\n- A clean linear relationship b\/w numerical_1 and numerical_2\n- will remove one column when being used in the model\n- also handle the outliers (what will be the threshold)","181dabc0":"Elo Merchant Category Recommendation\n=====\n\n# Objective\n\nDevelop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. We have to predict a loyalty score for each ```card_id``` represented in ```test.csv```.\n\n# Evaluation Metric\n\nEvaluation metric is RMSE - $\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$, \nwhere $\\hat{y}$ is the predicted loyalty score for each ```card_id```, and $y$ is the actual loyalty score assigned to a ```card_id```.\n\n# Data\n\n- **train.csv**: Training set having the information of cards. It also has the loyalty score column.\n- **test.csv**: Testing set having the information on cards\n- **historical_transactions.csv**: Up to 3 months' worth of historical transactions for each card_id\n- **merchants.csv**: Additional information about all merchants \/ ```merchant_id```s in the dataset.\n- **new_merchant_transactions.csv**: two months' worth of data for each ```card_id``` containing ALL purchases that ```card_id``` made at ```merchant_id```s that were not visited in the historical data.\n","1ed962a9":"# Merchant Data","529d1811":"Training dataset has no nulls but testing dataset has one row where the ```first_active_month``` column is Null. Impute testing data specifically?  \nWe are done with our sanity check now. Let's explore the table.\n\n## Exploration\n\n### 1. Target Variable\n\nTarget column is a numerical loyalty score.\n\n- Negative means the card holder is not loyal?\n- And, positive means the opposite?","212154c5":"Observations:\n- both ```avg_purchases_lag3``` and ```avg_sales_lag3``` have a few outliers in the extremes ","0f75e75e":"### 5. Most recent sales and purchases\n\nmost_recent_sales_range, most_recent_purchases_range\n\nThses categorical variables have the following values: A > B > C > D > E\n\n\nmost_recent_sales_range: Range of revenue (monetary units) in last active month  \nmost_recent_purchases_range: Range of quantity of transactions in last active month","9fe84c06":"Except null values in ```merchant_id``` column from historical data, every ```merchant_id``` is present in merchant data. Lets explore nulls","e378c04a":"## Sanity Check\n\n- All ```card_id```s in training and testing set\n- All ```merchant_id```s in merchant file\n- Nulls\n","bc76523c":"### 6. lag_3 columns\n\n- ```avg_sales_lag3```: Monthly average of revenue in last 3 months divided by revenue in last active month\n- ```avg_purchases_lag3```: Monthly average of transactions in last 3 months divided by transactions in last active month\n- ```active_months_lag3```: Quantity of active months within last 3 months\n","9aa281f8":"There are no duplicate ```card_id```'s in the train set.","707af917":"Observations:\n- 31.4% data in city_id is null\n- There are 216 distinct cities merchants belong to","43d3718a":"Following the same pattern as the historical data, we will only discuss id columns here. Col ```merchant_id``` has 26,216 nulls which is 0.013% of the data.","1f615c8c":"Observations:\n- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10\n- it seems to be between 0 and 10\n- Need to handle the outliers\n- there is not apparent pattern between the columns though","052480d0":"# Historical Data","e000908b":"Observations:\n\n- 1 and 3 are present for all the years\n- 2 started showing up with good numbers from the year 2015","7e3169d8":"Observations:\n\n- Years range from 2011 to 2018\n- 64% data is from 2017, followed by 2016 (25%) and 2015 (7%)\n- Very less data from 2017 (may be, test data will have data from 2018?)"}}