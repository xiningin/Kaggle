{"cell_type":{"eee9b997":"code","27780e8d":"code","83cc8681":"code","62a88573":"code","15b9b1ca":"code","7c5f96f4":"code","64afb68d":"code","a5a07c4a":"code","5e4dcd20":"code","4537c80d":"code","ef0b85f5":"code","dec4502e":"code","e442ab3f":"code","15791039":"code","d3014801":"code","4380243c":"code","ac9b36d3":"code","375179c8":"code","e21c2cd5":"code","64ba5467":"code","bbe34e4e":"code","1a74d0c5":"code","d0155400":"code","ada6be63":"code","f7599555":"code","e207f735":"code","32e0f4e1":"code","9952553c":"code","af5c7648":"code","28857995":"code","132a1994":"code","7c02578e":"code","cb80a161":"code","d35abea1":"code","b252ba80":"code","0a0adbd7":"code","03300bc8":"code","e84453bd":"code","68ed1d2e":"code","06eb1861":"code","e1afe605":"code","7123efd8":"code","a24aa2ad":"code","7f332fbb":"code","b967c27f":"code","9d1d2077":"code","beccf90b":"code","fa2f84c4":"code","645d627a":"markdown","c65f7f5a":"markdown","2b832f67":"markdown","e9628f2c":"markdown","d3c25413":"markdown","3d2a5d32":"markdown","835effda":"markdown","48003d65":"markdown","5fe4e643":"markdown","e6440a03":"markdown","1a943f2e":"markdown","55da66e8":"markdown","a879bf9c":"markdown","34273135":"markdown","c17bd73a":"markdown","60ffb60e":"markdown","c8a0560c":"markdown","e27a4f34":"markdown","2bfc5687":"markdown","b28ef680":"markdown","28664bae":"markdown","ee60cd23":"markdown","c6dfb689":"markdown","98562387":"markdown"},"source":{"eee9b997":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","27780e8d":"mtcars=pd.read_csv('..\/input\/linear-regression-eda-python\/mtcars.csv')\nmtcars.head()","83cc8681":"mtcar1=mtcars\nmtcar1.head()","62a88573":"mtcar1.info()","15b9b1ca":"mtcar1.nunique()","7c5f96f4":"mtcar1.cyl.unique()","64afb68d":"mtcar1.vs.unique()","a5a07c4a":"mtcar1.am.unique()","5e4dcd20":"mtcar1.gear.unique()","4537c80d":"mtcar1.carb.unique()","ef0b85f5":"mtcar1.describe()","dec4502e":"plt.boxplot(mtcar1.mpg)\nplt.show()","e442ab3f":"plt.boxplot(mtcar1.disp)\nplt.show()","15791039":"plt.boxplot(mtcar1.hp)\nplt.show()","d3014801":"plt.boxplot(mtcar1.drat)\nplt.show()","4380243c":"plt.boxplot(mtcar1.wt)\nplt.show()","ac9b36d3":"plt.boxplot(mtcar1.qsec)\nplt.show()","375179c8":"f,ax=plt.subplots(1,2,figsize=(10,5))\nmtcar1.vs.value_counts().plot(kind='bar',ax=ax[0])\nmtcar1.am.value_counts().plot(kind='bar',ax=ax[1])\nplt.show()","e21c2cd5":"f,ax=plt.subplots(1,2,figsize=(10,5))\nmtcar1.cyl.value_counts().plot(kind='bar',ax=ax[0])\nmtcar1.gear.value_counts().plot(kind='bar',ax=ax[1])\nplt.show()","64ba5467":"sns.countplot(mtcar1.gear,hue=mtcar1.cyl)\nplt.show()","bbe34e4e":"mtcar2=mtcar1.drop(['cyl','vs','am','gear','carb'],axis=1)\nmtcar2.head()","1a74d0c5":"mtcar1.groupby(['cyl'])['carb'].value_counts()[8].plot(kind='bar')\nplt.show()","d0155400":"mtcar1.groupby(['vs','am'])['gear'].value_counts()[1][1].plot(kind='bar')\nplt.show()","ada6be63":"mtcar1.groupby(['vs','am'])['gear'].value_counts()[0][1].plot(kind='bar')\nplt.show()","f7599555":"mtcar1.groupby(['vs','am'])['carb'].value_counts()[1][1].plot(kind='bar')\nplt.show()","e207f735":"mtcar1.groupby(['vs','am'])['carb'].value_counts()[1][0].plot(kind='bar')\nplt.show()","32e0f4e1":"sns.pairplot(mtcar2)\nplt.show()","9952553c":"X=mtcar1.drop(['mpg','model'],axis=1)\nY=mtcar1.mpg\nimport statsmodels.api as sm\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(Y,X_constant).fit()\nmodel.summary()\n","af5c7648":"from sklearn.linear_model import LinearRegression\nlin_reg1=LinearRegression()\nlin_reg1.fit(X,Y)\nlin_reg1.score(X,Y)","28857995":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(model.resid, alpha=0.05) # ACF is auto correlation function\nacf.show()","132a1994":"from scipy.stats import jarque_bera\nname=['ch-stat','p-value']\nvalues=jarque_bera(model.resid)\nfrom statsmodels.compat import lzip\njb=lzip(name,values)\nprint(jb)","7c02578e":"sns.distplot(model.resid)\nplt.show()","cb80a161":"mean_res=model.resid.mean()\nprint('Mean of residuals is %.6f'%mean_res)","d35abea1":"y_pre=model.predict(X_constant)\nf,ax=plt.subplots(1,2,figsize=(10,8))\nsns.regplot(Y,y_pre,ax=ax[0])\nsns.regplot(model.resid,y_pre,ax=ax[1])\nplt.show()","b252ba80":"test = sm.stats.diagnostic.linear_rainbow(res=model)\nprint(test)","0a0adbd7":"name = ['F statistic', 'p-value']\nimport statsmodels.stats.api as sms\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlzip(name, test)","03300bc8":"sns.set_style('whitegrid')\nsns.residplot(y_pre,model.resid,lowess=True,color = 'g')\nplt.xlabel('Predicted')\nplt.ylabel('Residual')\nplt.title('Residual vs Predicted')\nplt.show()","e84453bd":"sns.heatmap(mtcar1.corr(),annot=True)\nplt.show()","68ed1d2e":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncol=X_constant.shape[1]\nvif=[variance_inflation_factor( X_constant.values,i) for i in range(col)]\nvif_pd=pd.DataFrame({'vif':vif[1:]},index=X.columns).T\nvif_pd","06eb1861":"X1=mtcar1.drop(['cyl','disp','hp','wt','qsec','gear','model','mpg','carb'],axis=1)\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X1,Y,test_size=0.3,random_state=1)\nX1.head()","e1afe605":"X_constant1 = sm.add_constant(x_train)\nmodel1 = sm.OLS(y_train,X_constant1).fit()\nmodel1.summary()\n","7123efd8":"from sklearn import metrics\nx_cont_train = sm.add_constant(x_train)\nx_cont_test = sm.add_constant(x_test)\ny_tr_pred=model1.predict(x_cont_train)\ny_tst_pred=model1.predict(x_cont_test)\nprint('R2 for train:',metrics.r2_score(y_train,y_tr_pred))\nprint('R2 for test:',metrics.r2_score(y_test,y_tst_pred))","a24aa2ad":"lin_reg2=LinearRegression()\nlin_reg2.fit(x_train,y_train)\nprint('R2 for train:',lin_reg2.score(x_train,y_train))\nprint('R2 for test:',lin_reg2.score(x_test,y_test))","7f332fbb":"q1=mtcar1.mpg.quantile(0.25)\nq3=mtcar1.mpg.quantile(0.75)\niqr=q3-q1\nll=q1-1.5*iqr\nul=q3+1.5*iqr\nprint(mtcar1.shape)\nmtcar3 = mtcar1[~((mtcar1['mpg']<ll) | (mtcar1['mpg']>ul))]\nprint(mtcar3.shape)\n","b967c27f":"X_wo=mtcar3.drop(['mpg','model'],axis=1)\nY_wo=mtcar3['mpg'].values\nX_const_wo=sm.add_constant(X_wo)\nmodel_wo=sm.OLS(Y_wo,X_const_wo).fit()\nmodel_wo.summary()","9d1d2077":"name = ['F statistic', 'p-value']\nimport statsmodels.stats.api as sms\ntest_wo = sms.het_goldfeldquandt(model_wo.resid, model_wo.model.exog)\nlzip(name, test_wo)\n","beccf90b":"vif1=[variance_inflation_factor( X_const_wo.values,i) for i in range(X_const_wo.shape[1])]\nvif_pd1=pd.DataFrame({'vif':vif1[1:]},index=X_wo.columns).T\nvif_pd1\n","fa2f84c4":"lin_reg_wo=LinearRegression()\nX_wo1=X_wo[['drat','vs','am']]\n\nx_train1,x_test1,y_train1,y_test1=train_test_split(X_wo1,Y_wo,test_size=0.3,random_state=2)\nlin_reg_wo.fit(x_train1,y_train1)\n\nprint(lin_reg_wo.score(x_train1,y_train1))\nprint(lin_reg_wo.score(x_test1,y_test1))","645d627a":"* Durbin-Watson value Its value ranges from 0-4. If the value of Durbin- Watson is between 0-2, it's known as Positive Autocorrelation between residuals.\n* If the value ranges from 2-4, it is known as Negative autocorrelation.\n* If the value is exactly 2, it means No Autocorrelation.\n* It is 1.86. Hence we can say that there is very small\/neglible Positive Autocorrelation between residuals","c65f7f5a":"### -----------------------------------------------------------------------END-------------------------------------------------------------------","2b832f67":"* mpg has strong correlation with disp,hp and wt. All are negatively correlated to mpg\n* mpg also has moderate correlation with drat\n* disp has strong negative correlation with drat\n* hp has strong negative correlation with qsec\n* wt has strong negative correlation with drat\n* multicollinearity is present in our dataset","e9628f2c":"A data frame with 32 observations on 11 (numeric) variables.\n\n* [, 1]\tmpg\tMiles\/(US) gallon\n* [, 2]\tcyl\tNumber of cylinders\n* [, 3]\tdisp\tDisplacement (cu.in.)\n* [, 4]\thp\tGross horsepower\n* [, 5]\tdrat\tRear axle ratio\n* [, 6]\twt\tWeight (1000 lbs)\n* [, 7]\tqsec\t1\/4 mile time\n* [, 8]\tvs\tEngine (0 = V-shaped, 1 = straight)\n* [, 9]\tam\tTransmission (0 = automatic, 1 = manual)\n* [,10]\tgear\tNumber of forward gears\n* [,11]\tcarb\tNumber of carburetors","d3c25413":"* Null Hypothesis - Error terms are normally distributed.\n* Alternate Hypothesis - Error terms are NOT normally distributed.","3d2a5d32":"This test is based on the hytpothesis testing where null and alternate hypothesis are:\n* H0 = constant variance among residuals. (Homoscedacity)\n* Ha = Heteroscedacity.\n\nThe residuals should be homoscedacious.\n","835effda":"* The goldfeld test show that it is still heteroscedasticity","48003d65":"### Removing Outliers","5fe4e643":"To detect nonlinearity one can inspect plots of observed vs. predicted values or residuals vs. predicted values. \nThe desired outcome is that points are symmetrically distributed around a diagonal line in the former plot or \naround horizontal line in the latter one. \n\n","e6440a03":"### Assumption 5- multicollinearity","1a943f2e":"### Assumption 1: No autocorrelation","55da66e8":"#### Rainbow Test","a879bf9c":"* The difference between train score and test score is more. Here we have problem of overfit","34273135":"### Assumption 3 - Linearity of Residuals","c17bd73a":"* The critical chi square value at the 5% level of significance is 5.99. If the computed value is below this value the null hypothesis is not rejected.\n\n* In this case the computed value of the JB statistic 1.74 is lesser than 5.99. Thus we fail to reject the null hypothesis and conclude that the error terms are normally distributed.","60ffb60e":"### Assumption 4 -\tHomoscedasticity test","c8a0560c":"* The Null hypothesis is that the regression is correctly modelled as linear.\n* The alternative for which the power might be large are convex, check\n","e27a4f34":"### Observations","2bfc5687":"* After removing outliers and selecting non collinear variables, we can see that out R^2 is close for training and test data set","b28ef680":"The higher the value of Jarque Bera test , the lesser the residuals are normally distributed.\nWe generally prefer a lower value of jarque bera test.","28664bae":"* There are no missing values\n* Total 32 observations are present\n* There are 5 categorical variables\n* mpg,hp,wt and qsec has outliers","ee60cd23":"### Assumption 2- Normality of Residuals","c6dfb689":"* In both cases good linearity of residuals can be seen\n* Also mean of residuals is 0.\n* Also p value for null hypothesis is more than 0.05. So we fail to reject null hypothesis. So we conclude regression is correctly modelled as linear\n* We conclude that residuals are linear","98562387":"* p value is less than 0.05. So we reject null hypothesis\n* Also plot for Residual vs Predicted show that points are scattered unequally to some extent\n* We conclude there is some heteroscedasticity"}}