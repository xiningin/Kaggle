{"cell_type":{"1e9cf218":"code","801595dc":"code","82a15c49":"code","e94510fe":"code","67b498b4":"code","02326266":"code","2690c3d8":"code","44d53ea9":"code","8e294048":"code","bedff4bd":"code","4160c5f7":"code","34ced252":"code","e1a4bb0b":"code","188402e9":"code","a8735ac5":"code","312c04ff":"code","f057be7f":"code","45875fc6":"code","4b66854f":"code","8d69a0b1":"code","2b9ae66f":"code","1d1d21e3":"code","aa475e64":"code","a3cb463f":"code","6499e425":"code","98ae5de9":"code","108e1aa1":"code","84c16863":"code","a9914500":"markdown","de96c498":"markdown","101973fd":"markdown","7ccc49df":"markdown","c1272cfb":"markdown","2012b7a0":"markdown","54ed4c06":"markdown","9119e91c":"markdown","4fee7d50":"markdown","2f3a59d5":"markdown","87ca7870":"markdown","186b8014":"markdown","15c67b65":"markdown","2b171c9c":"markdown","1fbe8058":"markdown","5dab7612":"markdown","803f6371":"markdown","e5f7cdba":"markdown","950f73bc":"markdown","1c8acfcd":"markdown","f4fc88a6":"markdown","6e9f13f3":"markdown","bbb836eb":"markdown","55e82926":"markdown","1c411a58":"markdown","985e9eb5":"markdown","f30a8ed2":"markdown"},"source":{"1e9cf218":"import pandas as pd\npd.set_option('display.max_columns', 999)\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","801595dc":"print('Loading trian set...')\ntrain = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv')\nprint('Loading test set...')\ntest = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv')\nprint('We have {} rows and {} columns in our train set'.format(train.shape[0], train.shape[1]))\nprint('We have {} rows and {} columns in our test set'.format(test.shape[0], test.shape[1]))","82a15c49":"train.head()","e94510fe":"test.head()","67b498b4":"def missing_values(train):\n    df = pd.DataFrame(train.isnull().sum()).reset_index()\n    df.columns = ['Feature', 'Frequency']\n    df['Percentage'] = (df['Frequency']\/train.shape[0])*100\n    df['Percentage'] = df['Percentage'].astype(str) + '%'\n    df.sort_values('Percentage', inplace = True, ascending = False)\n    return df\n\nmissing_values(train).head()","02326266":"missing_values(test)","2690c3d8":"for i in ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', \n          'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']:\n    plt.figure(figsize = (12, 8))\n    plt.scatter(train.index, train[i])\n    plt.title('{} distribution'.format(i))","44d53ea9":"def tv_ratio(train, column):\n    df = train[train[column]==0]\n    ratio = df.shape[0] \/ train.shape[0]\n    return ratio\n\ntarget_variables = ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', \n                    'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']\n\nfor i in target_variables:\n    print('{} have a 0 ratio of: '.format(i), tv_ratio(train, i))","8e294048":"def plot_dist(train, test, column, type = 'kde', together = True):\n    if type == 'kde':\n        if together == False:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n            sns.kdeplot(train[column], ax = ax1, color = 'blue', shade=True)\n            ax1.set_title('{} distribution of the train set'.format(column))\n            sns.kdeplot(test[column], ax = ax2, color = 'red', shade=True)\n            ax2.set_title('{} distribution of the test set'.format(column))\n            plt.show()\n        else:\n            fig , ax = plt.subplots(1, 1, figsize = (12,8))\n            sns.kdeplot(train[column], ax = ax, color = 'blue', shade=True, label = 'Train {}'.format(column))\n            sns.kdeplot(test[column], ax = ax, color = 'red', shade=True, label = 'Test {}'.format(column))\n            ax.set_title('{} Distribution'.format(column))\n            plt.show()\n    else:\n        if together == False:\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n            sns.distplot(train[column], ax = ax1, color = 'blue', kde = False)\n            ax1.set_title('{} distribution of the train set'.format(column))\n            sns.distplot(test[column], ax = ax2, color = 'red', kde = False)\n            ax2.set_title('{} distribution of the test set'.format(column))\n            plt.show()\n        else:\n            fig , ax = plt.subplots(1, 1, figsize = (12,8))\n            sns.distplot(train[column], ax = ax, color = 'blue', kde = False)\n            sns.distplot(test[column], ax = ax, color = 'red', kde = False)\n            plt.show()\n    \nplot_dist(train, test, 'Latitude', type = 'kde', together = True)\nplot_dist(train, test, 'Latitude', type = 'other', together = False)","bedff4bd":"plot_dist(train, test, 'Longitude', type = 'kde', together = True)\nplot_dist(train, test, 'Longitude', type = 'other', together = False)","4160c5f7":"def scatter_plot(data, column1, column2, city = 'All'):\n    if city == 'All':\n        plt.figure(figsize = (12, 8))\n        sns.scatterplot(data[column1], data[column2])\n        plt.title('{} vs {} scatter plot'.format(column1, column2))\n        plt.show()\n    elif city == 'Atlanta':\n        data1 = data[data['City']=='Atlanta']\n        plt.figure(figsize = (12, 8))\n        sns.scatterplot(data1[column1], data1[column2])\n        plt.title('{} vs {} scatter plot for Atlanta city'.format(column1, column2))\n        plt.show()\n    elif city == 'Boston':\n        data1 = data[data['City']=='Boston']\n        plt.figure(figsize = (12, 8))\n        sns.scatterplot(data1[column1], data1[column2])\n        plt.title('{} vs {} scatter plot for Boston city'.format(column1, column2))\n        plt.show()\n    elif city == 'Chicago':\n        data1 = data[data['City']=='Chicago']\n        plt.figure(figsize = (12, 8))\n        sns.scatterplot(data1[column1], data1[column2])\n        plt.title('{} vs {} scatter plot for Chicago'.format(column1, column2))\n        plt.show()\n    elif city == 'Philadelphia':\n        data1 = data[data['City']=='Philadelphia']\n        plt.figure(figsize = (12, 8))\n        sns.scatterplot(data1[column1], data1[column2])\n        plt.title('{} vs {} scatter plot for Philadelphia'.format(column1, column2))\n        plt.show()\n\n        \nscatter_plot(train, 'Latitude', 'Longitude')","34ced252":"scatter_plot(train, 'Latitude', 'Longitude', city = 'Atlanta')\nscatter_plot(train, 'Latitude', 'Longitude', city = 'Boston')\nscatter_plot(train, 'Latitude', 'Longitude', city = 'Chicago')\nscatter_plot(train, 'Latitude', 'Longitude', city = 'Philadelphia')","e1a4bb0b":"def get_correlation(train, column):\n    df = train[[column, 'TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80',\n               'DistanceToFirstStop_p20', 'DistanceToFirstStop_p40', 'DistanceToFirstStop_p80']]\n    correlation = df.corr()\n    plt.figure(figsize = (12, 8))\n    sns.heatmap(correlation, annot = True)\n    return df\n    \ndf = get_correlation(train, 'Latitude')\ndf = get_correlation(train, 'Longitude')","188402e9":"train.head()","a8735ac5":"def get_frec(df, column):\n    df1 = pd.DataFrame(df[column].value_counts(normalize = True)).reset_index()\n    df1.columns = [column, 'Percentage']\n    df1.sort_values(column, inplace = True, ascending = True)\n    return df1\n\n\ndef plot_frec(train, test, column):\n    df = get_frec(train, column)\n    df1 = get_frec(test, column)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12,8))\n    sns.barplot(df[column], df['Percentage'], ax = ax1, color = 'blue')\n    ax1.set_title('{} percentages for the train set'.format(column))\n    sns.barplot(df1[column], df1['Percentage'], ax = ax2, color = 'red')\n    ax2.set_title('{} percentages for the test set'.format(column))\n    \nplot_frec(train, test, 'Month')","312c04ff":"def get_target_mean(train, column, target_variables):\n    df = train.groupby([column])[target_variables].agg(['mean']).reset_index()\n    df.columns = [column] + [x + '_mean' for x in target_variables]\n    return df\n\nget_target_mean(train, 'Month', target_variables)","f057be7f":"plot_frec(train, test, 'Hour')\nget_target_mean(train, 'Hour', target_variables)","45875fc6":"plot_frec(train, test, 'Weekend')\nget_target_mean(train, 'Weekend', target_variables)","4b66854f":"# city\nplot_frec(train, test, 'City')\nget_target_mean(train, 'City', target_variables)","8d69a0b1":"# EntryStreetName\ndef n_unique_cat(train, test, col):\n    n_u_train = train[col].nunique()\n    n_u_test = test[col].nunique()\n    df = pd.DataFrame({'n_u_train_{}'.format(col): [n_u_train], 'n_u_test_{}'.format(col): [n_u_train]})\n    return df\nn_unique_cat(train, test, 'EntryStreetName')","2b9ae66f":"# ExitStreetName\nn_unique_cat(train, test, 'ExitStreetName')","1d1d21e3":"# EntryHeading\nplot_frec(train, test, 'EntryHeading')\nget_target_mean(train, 'EntryHeading', target_variables)","aa475e64":"# ExitHeading\nplot_frec(train, test, 'ExitHeading')\nget_target_mean(train, 'ExitHeading', target_variables)","a3cb463f":"# IntersectionId\nn_unique_cat(train, test, 'IntersectionId')","6499e425":"# Road Encoding\n\nroad_encoding = {'Street': 0, 'St': 0, 'Avenue': 1, 'Ave': 1, 'Boulevard': 2, 'Road': 3,\n                'Drive': 4, 'Lane': 5, 'Tunnel': 6, 'Highway': 7, 'Way': 8, 'Parkway': 9,\n                'Parking': 10, 'Oval': 11, 'Square': 12, 'Place': 13, 'Bridge': 14}\n\ndef encode(x):\n    if pd.isna(x):\n        return 0\n    for road in road_encoding.keys():\n        if road in x:\n            return road_encoding[road]\n    return 0\n\nfor par in [train, test]:\n    par['EntryType'] = par['EntryStreetName'].apply(encode)\n    par['ExitType'] = par['ExitStreetName'].apply(encode)\n    par['EntryType_1'] = pd.Series(par['EntryStreetName'].str.split().str.get(0))\n    par['ExitType_1'] = pd.Series(par['ExitStreetName'].str.split().str.get(0))\n    par['EntryType_2'] = pd.Series(par['EntryStreetName'].str.split().str.get(1))\n    par['ExitType_2'] = pd.Series(par['ExitStreetName'].str.split().str.get(1))\n    par.loc[par['EntryType_1'].isin(par['EntryType_1'].value_counts()[par['EntryType_1'].value_counts()<=500].index), 'EntryType_1'] = 'Other'\n    par.loc[par['ExitType_1'].isin(par['ExitType_1'].value_counts()[par['ExitType_1'].value_counts()<=500].index), 'ExitType_1'] = 'Other'\n    par.loc[par['EntryType_2'].isin(par['EntryType_2'].value_counts()[par['EntryType_2'].value_counts()<=500].index), 'EntryType_2'] = 'Other'\n    par.loc[par['ExitType_2'].isin(par['ExitType_2'].value_counts()[par['ExitType_2'].value_counts()<=500].index), 'ExitType_2'] = 'Other'\n    \n    \n    \n    \n\n# The cardinal directions can be expressed using the equation: \u03b8\/\u03c0\n# Where  \u03b8  is the angle between the direction we want to encode and the north compass direction, measured clockwise.\ndirections = {'N': 0, 'NE': 1\/4, 'E': 1\/2, 'SE': 3\/4, 'S': 1, 'SW': 5\/4, 'W': 3\/2, 'NW': 7\/4}\ntrain['EntryHeading'] = train['EntryHeading'].map(directions)\ntrain['ExitHeading'] = train['ExitHeading'].map(directions)\ntest['EntryHeading'] = test['EntryHeading'].map(directions)\ntest['ExitHeading'] = test['ExitHeading'].map(directions)\n\n# EntryStreetName == ExitStreetName ?\n# EntryHeading == ExitHeading ?\nfor par in [train, test]:\n    par[\"same_street_exact\"] = (par[\"EntryStreetName\"] ==  par[\"ExitStreetName\"]).astype(int)\n    par[\"same_heading_exact\"] = (par[\"EntryHeading\"] ==  par[\"ExitHeading\"]).astype(int)\n    \n# We have some intersection id that are in more than one city, it is a good idea to feature cross them\nfor par in [train, test]:\n    par['Intersection'] = par['IntersectionId'].astype(str) + '_' + par['City'].astype(str)\n    \n# Add temperature (\u00b0F) of each city by month\nmonthly_av = {'Atlanta1': 43, 'Atlanta5': 69, 'Atlanta6': 76, 'Atlanta7': 79, 'Atlanta8': 78, 'Atlanta9': 73,\n              'Atlanta10': 62, 'Atlanta11': 53, 'Atlanta12': 45, 'Boston1': 30, 'Boston5': 59, 'Boston6': 68,\n              'Boston7': 74, 'Boston8': 73, 'Boston9': 66, 'Boston10': 55,'Boston11': 45, 'Boston12': 35,\n              'Chicago1': 27, 'Chicago5': 60, 'Chicago6': 70, 'Chicago7': 76, 'Chicago8': 76, 'Chicago9': 68,\n              'Chicago10': 56,  'Chicago11': 45, 'Chicago12': 32, 'Philadelphia1': 35, 'Philadelphia5': 66,\n              'Philadelphia6': 76, 'Philadelphia7': 81, 'Philadelphia8': 79, 'Philadelphia9': 72, 'Philadelphia10': 60,\n              'Philadelphia11': 49, 'Philadelphia12': 40}\n\nfor par in [train, test]:\n    # Concatenating the city and month into one variable\n    par['city_month'] = par[\"City\"].astype(str) + par[\"Month\"].astype(str)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly temperature\n    par[\"average_temp\"] = par['city_month'].map(monthly_av)\n    \n# Add climate data\nmonthly_rainfall = {'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12, 'Atlanta8': 3.67, 'Atlanta9': 4.09, \n                    'Atlanta10': 3.11, 'Atlanta11': 4.10, 'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22, \n                    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,'Boston11': 3.98, 'Boston12': 3.73, \n                    'Chicago1': 1.75, 'Chicago5': 3.38, 'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27, \n                    'Chicago10': 2.71,  'Chicago11': 3.01, 'Chicago12': 2.43, 'Philadelphia1': 3.52, 'Philadelphia5': 3.88, \n                    'Philadelphia6': 3.29, 'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9':3.88 , \n                    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31}\n\nmonthly_snowfall = {'Atlanta1': 0.6, 'Atlanta5': 0, 'Atlanta6': 0, 'Atlanta7': 0, 'Atlanta8': 0, 'Atlanta9': 0, \n                    'Atlanta10': 0, 'Atlanta11': 0, 'Atlanta12': 0.2, 'Boston1': 12.9, 'Boston5': 0, 'Boston6': 0, \n                    'Boston7': 0, 'Boston8': 0, 'Boston9': 0, 'Boston10': 0,'Boston11': 1.3, 'Boston12': 9.0, \n                    'Chicago1': 11.5, 'Chicago5': 0, 'Chicago6': 0, 'Chicago7': 0, 'Chicago8': 0, 'Chicago9': 0, \n                    'Chicago10': 0,  'Chicago11': 1.3, 'Chicago12': 8.7, 'Philadelphia1': 6.5, 'Philadelphia5': 0, \n                    'Philadelphia6': 0, 'Philadelphia7': 0, 'Philadelphia8': 0, 'Philadelphia9':0 , 'Philadelphia10': 0, \n                    'Philadelphia11': 0.3, 'Philadelphia12': 3.4}\n\nmonthly_daylight = {'Atlanta1': 10, 'Atlanta5': 14, 'Atlanta6': 14, 'Atlanta7': 14, 'Atlanta8': 13, 'Atlanta9': 12, \n                    'Atlanta10': 11, 'Atlanta11': 10, 'Atlanta12': 10, 'Boston1': 9, 'Boston5': 15, 'Boston6': 15, \n                    'Boston7': 15, 'Boston8': 14, 'Boston9': 12, 'Boston10': 11,'Boston11': 10, 'Boston12': 9, \n                    'Chicago1': 10, 'Chicago5': 15, 'Chicago6': 15, 'Chicago7': 15, 'Chicago8': 14, 'Chicago9': 12, \n                    'Chicago10': 11,  'Chicago11': 10, 'Chicago12': 9, 'Philadelphia1': 10, 'Philadelphia5': 14, \n                    'Philadelphia6': 15, 'Philadelphia7': 15, 'Philadelphia8': 14, 'Philadelphia9':12 , 'Philadelphia10': 11, \n                    'Philadelphia11': 10, 'Philadelphia12': 9}\n\nmonthly_sunshine = {'Atlanta1': 5.3, 'Atlanta5': 9.3, 'Atlanta6': 9.5, 'Atlanta7': 8.8, 'Atlanta8': 8.3, 'Atlanta9': 7.6, \n                    'Atlanta10': 7.7, 'Atlanta11': 6.2, 'Atlanta12': 5.3, 'Boston1': 5.3, 'Boston5': 8.6, 'Boston6': 9.6, \n                    'Boston7': 9.7, 'Boston8': 8.9, 'Boston9': 7.9, 'Boston10': 6.7,'Boston11': 4.8, 'Boston12': 4.6, \n                    'Chicago1': 4.4, 'Chicago5': 9.1, 'Chicago6': 10.4, 'Chicago7': 10.3, 'Chicago8': 9.1, 'Chicago9': 7.6, \n                    'Chicago10': 6.2,  'Chicago11': 3.6, 'Chicago12': 3.4, 'Philadelphia1': 5.0, 'Philadelphia5': 7.9, \n                    'Philadelphia6': 9.0, 'Philadelphia7': 8.9, 'Philadelphia8': 8.4, 'Philadelphia9':7.9 , \n                    'Philadelphia10': 6.6,  'Philadelphia11': 5.2, 'Philadelphia12': 4.4}\n\n\nfor par in [train, test]:\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n    par[\"average_rainfall\"] = par['city_month'].map(monthly_rainfall)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly snowfall\n    par['average_snowfall'] = par['city_month'].map(monthly_snowfall)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly daylight\n    par[\"average_daylight\"] = par['city_month'].map(monthly_daylight)\n    # Creating a new column by mapping the city_month variable to it's corresponding average monthly sunsine\n    par[\"average_sunshine\"] = par['city_month'].map(monthly_sunshine)\n    \n# drop city month\ntrain.drop('city_month', axis=1, inplace=True)\ntest.drop('city_month', axis=1, inplace=True)\n\n# Add feature is day\ntrain['is_day'] = train['Hour'].apply(lambda x: 1 if 5 < x < 20 else 0)\ntest['is_day'] = test['Hour'].apply(lambda x: 1 if 5 < x < 20 else 0)\n\n# fill NaN categories\ntrain.fillna(-999, inplace = True)\ntest.fillna(-999, inplace = True)\n\n\n# distance from the center of the city\ndef add_distance(df):\n    \n    df_center = pd.DataFrame({\"Atlanta\":[33.753746, -84.386330],\n                             \"Boston\":[42.361145, -71.057083],\n                             \"Chicago\":[41.881832, -87.623177],\n                             \"Philadelphia\":[39.952583, -75.165222]})\n    \n    df[\"CenterDistance\"] = df.apply(lambda row: math.sqrt((df_center[row.City][0] - row.Latitude) ** 2 +\n                                                          (df_center[row.City][1] - row.Longitude) ** 2) , axis=1)\n\nadd_distance(train)\nadd_distance(test)\n\n\n# frequency encode\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# COMBINE FEATURES\ndef encode_CB(col1, col2 , df1 = train, df2 = test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    print(nm,', ',end='')\n    \n# group aggregations nunique\ndef encode_AG2(main_columns, agg_col, train_df = train, test_df = test):\n    for main_column in main_columns:  \n        for col in agg_col:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')\n\ndef encode_AG(main_columns, agg_col, aggregations=['mean'], train_df = train, test_df = test, fillna=True, usena=False):\n    # aggregation of main agg_cols\n    for main_column in main_columns:  \n        for col in agg_col:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# Frequency encode \nencode_FE(train, test, ['Hour', 'Month', 'EntryType', 'ExitType', 'EntryType_1', 'EntryType_2', 'ExitType_1', 'ExitType_2', 'Intersection', 'City'])\n                \n# Agreggations of main columns\nencode_AG(['Longitude', 'Latitude', 'CenterDistance', 'EntryHeading', 'ExitHeading'], ['Hour', 'Weekend', 'Month', 'Intersection'], ['mean', 'std'])\n\n# bucketize lat and lon\ntemp_df = pd.concat([train[['Latitude', 'Longitude']], test[['Latitude', 'Longitude']]]).reset_index(drop = True)\ntemp_df['Latitude_B'] = pd.cut(temp_df['Latitude'], 30)\ntemp_df['Longitude_B'] = pd.cut(temp_df['Longitude'], 30)\n\n# feature cross lat and lon\ntemp_df['Latitude_B_Longitude_B'] = temp_df['Latitude_B'].astype(str) + '_' + temp_df['Longitude_B'].astype(str)\ntrain['Latitude_B'] = temp_df.loc[:(train.shape[0]), 'Latitude_B']\ntest['Latitude_B'] = temp_df.loc[(train.shape[0]):, 'Latitude_B']\ntrain['Longitude_B'] = temp_df.loc[:(train.shape[0]), 'Longitude_B']\ntest['Longitude_B'] = temp_df.loc[(train.shape[0]):, 'Longitude_B']\ntrain['Latitude_B_Longitude_B'] = temp_df.loc[:(train.shape[0]), 'Latitude_B_Longitude_B']\ntest['Latitude_B_Longitude_B'] = temp_df.loc[(train.shape[0]):, 'Latitude_B_Longitude_B']\n\n# feature crosses hour with month\nencode_CB('Hour', 'Month')\n\n# group aggregations nunique \nencode_AG2(['Intersection', 'Latitude_B_Longitude_B'], ['Hour', 'Month'])\n\n# label encode\nfor i,f in enumerate(train.columns):\n    if (np.str(train[f].dtype)=='category')|(train[f].dtype=='object'): \n        df_comb = pd.concat([train[f],test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        train[f] = df_comb[:len(train)].astype('int16')\n        test[f] = df_comb[len(train):].astype('int16')","98ae5de9":"# drop useless features\nfor par in [train, test]:\n    par.drop(['RowId', 'Path', 'EntryStreetName', 'ExitStreetName'], axis = 1, inplace = True)\n    \n# drop target variables from the train set\npreds = train.iloc[:,8:23]\n# get target_variables\ntarget1 = preds['TotalTimeStopped_p20']\ntarget2 = preds['TotalTimeStopped_p50']\ntarget3 = preds['TotalTimeStopped_p80']\ntarget4 = preds['DistanceToFirstStop_p20']\ntarget5 = preds['DistanceToFirstStop_p50']\ntarget6 = preds['DistanceToFirstStop_p80']\ntrain.drop(preds.columns.tolist(), axis=1, inplace =True)","108e1aa1":"# train lgb\nparam = {'num_leaves': 230, \n         'feature_fraction': 0.8115011063299449,\n         'bagging_fraction': 0.9557214979912946,\n         'max_depth': 19,\n         'lambda_l1': 1.1159237398459447,\n         'lambda_l2': 0.7092738973066476,\n         'min_split_gain': 0.007200100317150616,\n         'min_child_weight': 19.751392371168137,\n         'learning_rate': 0.05,\n         'objective': 'regression',\n         'boosting_type': 'gbdt',\n         'verbose': 1,'metric': 'rmse',\n         'seed': 7}\n\ndef run_lgb(train, test):\n    # get prediction dictonary were we are going to store predictions\n    all_preds = {0 : [], 1 : [], 2 : [], 3 : [], 4 : [], 5 : []}\n    # get a list with all the target variables\n    all_target = [target1, target2, target3, target4, target5, target6]\n    nfold = 5\n    kf = KFold(n_splits=nfold, random_state=228, shuffle=True)\n    for i in range(len(all_preds)):\n        print('Training and predicting for target {}'.format(i+1))\n        oof = np.zeros(len(train))\n        all_preds[i] = np.zeros(len(test))\n        n = 1\n        for train_index, valid_index in kf.split(all_target[i]):\n            print(\"fold {}\".format(n))\n            xg_train = lgb.Dataset(train.iloc[train_index],\n                                   label=all_target[i][train_index]\n                                   )\n            xg_valid = lgb.Dataset(train.iloc[valid_index],\n                                   label=all_target[i][valid_index]\n                                   )   \n\n            clf = lgb.train(param, xg_train, 10000, valid_sets=[xg_train, xg_valid], \n                            verbose_eval=100, early_stopping_rounds=200)\n            oof[valid_index] = clf.predict(train.iloc[valid_index], num_iteration=clf.best_iteration) \n\n            all_preds[i] += clf.predict(test, num_iteration=clf.best_iteration) \/ nfold\n            n = n + 1\n\n        print(\"\\n\\nCV RMSE: {:<0.4f}\".format(np.sqrt(mean_squared_error(all_target[i], oof))))\n    return all_preds\n\nall_preds = run_lgb(train, test)","84c16863":"submission = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv')\ndata2 = pd.DataFrame(all_preds).stack()\ndata2 = pd.DataFrame(data2)\nsubmission['Target'] = data2[0].values\nsubmission.to_csv('lgbm_baseline.csv', index=False)","a9914500":"Same for weekends","de96c498":"Also predictive","101973fd":"The model is definetly overfitting (check train rmse vs val rmse). A much better model can be achieved with time and proper iterations.","7ccc49df":"Same unique names in train, and test for previous features.","c1272cfb":"Many of this features were extracted in the public kernel mention above. I diden't make a forward feature engineering phase or feature elimination phase. This features are just some ideas, maybee some of them are not good and performance decrease. ","2012b7a0":"# Target Variables\n\nIn this problem we are asked to predict 6 continous variables. Here is an extract of kaggle Data tab.\n\nFor each grouping in the test set, you need to make predictions for three different quantiles of two different metrics covering how long it took the group of vehicles to drive through the intersection. Specifically, the 20th, 50th, and 80th percentiles for the total time stopped at an intersection and the distance between the intersection and the first place a vehicle stopped while waiting. You can think of your goal as summarizing the distribution of wait times and stop distances at each intersection.","54ed4c06":"# Objective\n\n* The objective of this notebook is to make a exploratory data analysis and build a baseline model with lgbm\n* Thanks to https:\/\/www.kaggle.com\/dcaichara\/feature-engineering-and-lightgbm, were i got some of the ideas for feature engineering.","9119e91c":"We have 2 features in the train set and test set that have missing values. \n\nLet's check each feature.","4fee7d50":"A lot of 0. Let's calculate what is the percentage of 0 in each of our target variables","2f3a59d5":"Same for hour.","87ca7870":"# Time Features\n\nLet's analyze our time features","186b8014":"It seems their are no outliers in our training data. Now let's check the correlation between the latitude and longitude for each target variable","15c67b65":"# Categorical Features","2b171c9c":"This feature dont have the same frequencies (train set vs test set)","1fbe8058":"# Feature Engineering and LGBM Model","5dab7612":"# Target Variables","803f6371":"We are asked to predict TotalTimeStopped_p20, TotalTimeStopped_p50, TotalTimeStopped_p80, DistanceToFirstStop_p20, DistanceToFirstStop_p50 and DistanceToFirstStop_p80\n\nWe also have a feature called TimeFromFirstStop_px in the training set that can be usefull\n\nOther percentiles for the features mention recently can be found in the training set. Maybee it is a good idea to predict all the percentiles and use it in a smart way to improve our results.","e5f7cdba":"# Reading Files\n\n* We are going to read the files with pandas and plot with seaborn to make the exploratory data analysis\n\n* In the second part we are going to use tensorflow 2.0 apis to make the model.","950f73bc":"Previous table show that month have a lot of predictive power. Let's see Hour and Weekend.","1c8acfcd":"We have four points were the data concetrates. This are the four cities (Atlanta, Boston, Chicago, Philadelphia)\n\nLet's plot each city to visualize better lat vs lon","f4fc88a6":"When a time percentile is 0, it's also 0 for the distance. This make sense because if you never stop, the distance between the intersaction and the fist stop is obviously 0.\n\nMaybee we can make a post model processing where we check this (for predictions).\n\nIn other words:\n\n91% of the observations are intersections where 20% of the users diden't stop.\n\n68% of the observations are intersections where 50% of the users diden't stop.\n\n34% of the observations are intersections where 80% of the users diden't stop.","6e9f13f3":"We have a similar distribution, with minor differences. Let's check longitude","bbb836eb":"# Latitude and Longitude","55e82926":"Latitude and Longitude have a low correlation with target variables. On the other hand target variables have a positive correlation with each other. For example TotalTimeStopped_p20\t have 0.71 correlation with TotalTimeStopped_p50. This is mainly because when p50 is 0, p20 is also 0 (68% of the observations are p50 = 0)","1c411a58":"# Missing Values","985e9eb5":"Similar distribution were test have more peaks, train is more flat. Let's check lat vs lon","f30a8ed2":"# Features"}}