{"cell_type":{"95cebffc":"code","e619df1d":"code","e1d7fc32":"code","26c9e2d7":"code","78ec7e6e":"code","ce205546":"code","b9632f46":"code","019eb340":"code","824c672d":"code","764dc2de":"code","36496f3f":"code","85515adb":"code","552378fd":"code","c0621077":"code","354000dd":"code","4e6e572a":"code","6ec4f231":"code","03025191":"code","e3742186":"code","2feee070":"code","cae2f548":"markdown","819ea67f":"markdown","b49bd420":"markdown","47db0804":"markdown","ec64b8b4":"markdown","4426f471":"markdown","04a2be7a":"markdown","280fbcec":"markdown","c6a3d1ce":"markdown","2ccde6e7":"markdown","b48dda7f":"markdown","ca464570":"markdown","0fc8284c":"markdown","ff92c81f":"markdown","aa29836c":"markdown","fc870a9c":"markdown","2fb65993":"markdown","faae2a92":"markdown"},"source":{"95cebffc":"# importing library to handle files\nimport os\nfrom os import path\n\n# importing library to handle folders\nimport shutil\n\n# importing library to display status bars\nfrom tqdm.notebook import tqdm\n\n# importing library to handle warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing libraries to handle images\nimport cv2\n\n# importing library to handle file checksums\nimport hashlib\n\n# importing library to handle data structures\nimport pandas as pd\n\n# importing library to handle arrays\nimport numpy as np\n\n# importing library to handle randomness\nimport random\n\n# importing library to display\nimport matplotlib.pyplot as plt\n\n# importing library for deep learning\nimport tensorflow as tf\n\n# importing library for preprocessing\nfrom sklearn.model_selection import train_test_split","e619df1d":"# initializing lists to store file paths\npaths = []\n\n# filtering original dataset with a smaller number of classes\nan_list = ['black_bear', 'cougar', 'gray_wolf', 'bobcat']\n\n# iteration through directories and preprocessing filepaths and filenames\nfor dirname, _, filenames in tqdm(os.walk('\/kaggle\/input')):\n    for filename in filenames:\n        \n        fileloc = os.path.join(dirname, filename)\n        \n        if filename!='wildlife.h5':\n            if fileloc.split(os.path.sep)[-2] in an_list:\n                paths.append(fileloc)","e1d7fc32":"# defining name for output directory\nod = 'output_images'\n\n# defining function to create output directory\ndef dir_tree(o_d):\n    if path.isdir(o_d) == False:\n        os.mkdir(o_d)\n        print(\"Output image directory created\")\n    else:\n        print(\"Output image directory already exists!\")\n        shutil.rmtree(o_d)\n        os.mkdir(o_d)\n        print(\"Fresh directory created under same name after clean-up!\")","26c9e2d7":"# defining a function to plot class distributions\ndef label_dist(class_dis):\n    list_counts = np.unique(class_dis, return_counts=True)\n    \n    fig, ax = plt.subplots(figsize = (15,6))    \n\n    ax.barh(np.arange(len(list_counts[0])), list_counts[1], \n            height = 0.3, align = 'center')\n    \n    ax.set_yticks(np.arange(len(list_counts[0])))\n    ax.set_yticklabels(list_counts[0])\n    ax.set_xlabel('Label Count')\n    ax.set_title('Class Distribution')\n\n    plt.show()","78ec7e6e":"# output directory\ndir_tree(od)\n\n# creating lists to store data\nlabel = []\nimg_data = []\nout_p = []\n\n# initializing resizing dimensions\nIMAGE_DIMS = (224, 224, 3)\n\n# exception count\ne_count = 0\n\n# iterating through image paths\nfor enum, imagePath in tqdm(enumerate(paths)):\n    \n    try:\n        counter = 0\n        img=cv2.imread(imagePath)\n        \n        img=cv2.resize(img, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n            \n    except Exception as e:\n        counter = 1\n        e_count = e_count + 1\n    \n    if counter==0:\n        img_cat = imagePath.split(os.path.sep)[-2]\n        label.append(img_cat)\n        \n        img_data.append(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        \n        mod_name_temp = \"im\" + str(enum).zfill(4) + \".jpg\"\n        cv2.imwrite(os.path.join(od, mod_name_temp), img)\n        out_p.append(os.path.join(od, mod_name_temp))\n        \nprint(\"Number of images that could not be processed:\", e_count)","ce205546":"# initial label distributions\nprint(\"Label distributions after preprocessing:\")\n\nlabel_dist(label)","b9632f46":"# defining a dictionary to store hash values for processed files\nhash_keys = dict()\n\n# list to store replicates\/multiple instances of files\nreplicates = []\n\n# iterating through files\nfor enum, filename in enumerate(out_p):\n\n    if os.path.isfile(filename):\n        with open(filename, 'rb') as f:\n            filehash = hashlib.md5(f.read()).hexdigest()\n        if filehash not in hash_keys:\n            hash_keys[filehash] = enum\n        else:\n            replicates.append((enum, hash_keys[filehash]))\n\nprint(\"Number of exact replicates found:\", len(replicates))","019eb340":"# visualizing replicates\nif len(replicates) > 0:\n\n    # number of replicates to display\n    num_repl = 8\n\n    # randomly sampling replicates to be displayed\n    sample_repl = random.sample(replicates, len(replicates))[0: num_repl]\n\n    # flattening replicate indices that have been sampled\n    sample_index = [sample_repl[i][j] for j in range(0, 2) for i in range(0, num_repl)] \n\n    # getting images for respective indices\n    sample_images = [img_data[i] for i in sample_index]\n\n    # getting sample image labels\n    sample_labels = [label[i] for i in sample_index]\n\n    # figure creation\n    fig, ax = plt.subplots(2, num_repl, figsize=(15, 6))\n    k = 0\n\n    # creating subplots\n    for i in range(0, 2):\n        for j in range(0, num_repl):\n            x_title = 'Index: ' + str(sample_index[k])\n            ax[i][j].set_title(x_title, fontsize = 8)\n            ax[i][j].imshow(sample_images[k])\n            \n            ax[i][j].set_xlabel(sample_labels[k], fontsize = 8)\n            ax[i][j].set_yticklabels([])\n            ax[i][j].set_xticklabels([])\n            k = k+1\n    \n    plt.show()\n    \nelse:\n    print(\"No replicates to visualize\")","824c672d":"# getting indices of replicates to be removed\ndel_indices = [index[0] for index in replicates]\n\n# removing replicates\nfor index in tqdm(sorted(del_indices, reverse=True)):\n    del out_p[index]\n    del img_data[index]\n    del label[index]","764dc2de":"# replotting class distributions after image clean-up\nprint(\"Label distributions after clean-up:\")\n\nlabel_dist(label)","36496f3f":"# dataframes for training, validation and test datasets\nmain_df = pd.DataFrame({'Path': out_p, 'Label': label}).sample(frac = 1, random_state = 10)\n\n# splitting to create relatively small datasets to be used for model fitting\noX_train, X_test, oy_train, y_test = train_test_split(main_df['Path'], main_df['Label'], \n                                                      test_size = 0.8,\n                                                      stratify = main_df['Label'])\n\n# splitting into training and validation datasets\nX_train, X_val, y_train, y_val = train_test_split(oX_train, oy_train, \n                                                  test_size = 0.2,\n                                                  stratify = oy_train)\n# train dataframe\ntrain_df = pd.DataFrame({'Path': X_train, 'Label': y_train})\n\n# validation dataframe\nval_df = pd.DataFrame({'Path': X_val, 'Label': y_val})\n\n# test dataframe\ntest_df = pd.DataFrame({'Path': X_test, 'Label': y_test})","85515adb":"# loading preprocessing function\nprep_func = tf.keras.applications.mobilenet.preprocess_input \n        \n# importing pretrained model\nmnet_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape = IMAGE_DIMS,\n                                                           include_top = False, weights = 'imagenet')\n         \n# freezing layers in pretrained model\nfor i, layer in enumerate(mnet_model.layers):\n    layer.trainable = False\n\n# training generator without any augmentation\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func)  \n\n# validation\/testing generator without any augmentation\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function = prep_func)\n\n# batch size for training\ntrain_bs = 16\n\n# loading training data in batches\ntrain_generator = train_datagen.flow_from_dataframe(dataframe=train_df, x_col=\"Path\",\n                                                    y_col=\"Label\", target_size=(IMAGE_DIMS[1], \n                                                                                IMAGE_DIMS[0]),\n                                                    batch_size=train_bs, \n                                                    class_mode='sparse')\n\n# batch size for validation\nval_bs = 8\n\n# loading validation data in batches\nval_generator = val_datagen.flow_from_dataframe(dataframe=val_df, x_col=\"Path\",\n                                                y_col=\"Label\", target_size=(IMAGE_DIMS[1], \n                                                                            IMAGE_DIMS[0]),\n                                                batch_size=val_bs, \n                                                class_mode='sparse')","552378fd":"# visualization of images from generator\ntrain_sample_images = [next(train_generator) for i in range(18)]\ntrain_sample_labels = [gen[1][0] for gen in train_sample_images]\n\n# inverting encoded labels to display beside corresponding images\ninv_labels = {v: k for k, v in (train_generator.class_indices).items()}\nkeys = list(inv_labels.keys())\n\ncoded_label = []\n\n# storing labels to be displayed\nfor train_sample_label in train_sample_labels:\n    for key in keys:\n        if train_sample_label == key:\n            coded_label.append(inv_labels.get(key))\n\nfig, ax = plt.subplots(3,6, figsize=(15, 10))\n\nk = 0\n\n# creating subplots of images\nfor i in range(3):\n    for j in range(6):\n        ax[i][j].set_title(coded_label[k], fontsize = 8)\n        ax[i][j].imshow(np.array(train_sample_images[k][0][0]))\n        \n        ax[i][j].set_yticklabels([])\n        ax[i][j].set_xticklabels([])\n        k = k + 1\n\nplt.show()","c0621077":"# defining a sequential model to learn \nmodel = mnet_model.layers[-3].output\n\n# adding pretrained model\nmodel = tf.keras.layers.GlobalAveragePooling2D()(model)\n\nmodel = tf.keras.layers.Dense(len(np.unique(y_train)), activation=tf.nn.softmax)(model)\n\n# define a new model \nclf_model = tf.keras.Model(mnet_model.input, model)\n\nclf_model.summary()","354000dd":"# compiling the model\nclf_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n# training and validation steps for model fitting\ntrain_steps = np.ceil(X_train.shape[0]\/train_bs)\nval_steps = np.ceil(X_val.shape[0]\/val_bs)\n\n# creating a callback to stop training\nclass es_myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        \n        # setting an accuracy threshold\n        AC_TH = 0.90\n        \n        # checking if threshold has been reached\n        if(logs.get('val_accuracy') > AC_TH and logs.get('accuracy') > AC_TH):\n            print(\"\\nReached set accuracy threshold on training and validation!!\")\n            self.model.stop_training = True\n\n# instantiating a callback object\nearlystop = es_myCallback()\n\n# list of callbacks\ncallbacks_list = [earlystop]\n\n# training\nhistory = clf_model.fit_generator(train_generator, steps_per_epoch=train_steps,\n                                  validation_data = val_generator, epochs = 10,\n                                  validation_steps = val_steps, \n                                  callbacks = callbacks_list, verbose = 1)","4e6e572a":"# class indices for validation generator\nprint(\"Class mappings of validation generator:\", val_generator.class_indices)\n\n# filtering test dataframe for new class\ncam_df = test_df[test_df.Label == 'bobcat']\n\n# getting images and labels for test class\ncam_paths = cam_df['Path']\ncam_labels = cam_df['Label'].apply(lambda x: 1)","6ec4f231":"# batch size for testing\ntest_bs = 32\n\n# steps for testing\ntest_steps = np.ceil(X_test.shape[0]\/test_bs)\n\n# loading validation data in batches\ntest_generator = val_datagen.flow_from_dataframe(dataframe=cam_df, x_col=\"Path\",\n                                                 target_size=(IMAGE_DIMS[1], \n                                                              IMAGE_DIMS[0]),\n                                                 batch_size=test_bs, \n                                                 class_mode=None, shuffle = False)\n\n# getting label probabilities from test set\ncam_probs = clf_model.predict_generator(test_generator, verbose = 1)\n\n# getting labels for target class from test set\ncam_preds = np.argmax(cam_probs, axis=-1)","03025191":"# getting missed detections for target class from test set\nmissdet_target = cam_labels.shape[0] - np.unique(cam_preds, \n                                                 return_counts=True)[1][1]\n\nprint(\"Number of missed detections:\", missdet_target)\n                                                                                   \n# getting recall value for target set from test set\nrecall_target = round(np.unique(cam_preds, \n                                return_counts=True)[1][1]\/cam_labels.shape[0],2)\n\nprint(\"Recall value for target class:\", recall_target)","e3742186":"# dataframe of predictions for target class\ncam_corr_preds = pd.DataFrame({\"Predicted_Class\": cam_preds})\n\n# randomly sampling correct predictions for target class\ncam_corr_preds = cam_corr_preds[cam_corr_preds['Predicted_Class']==1].sample(n = missdet_target)\n\n# getting indices of rows with correct predictions\ncorr_indices = list(cam_corr_preds.index)","2feee070":"# getting class weights for last layer in model\nclass_weights = clf_model.layers[-1].get_weights()[0]\n\n# commening process of getting class weights for last convolutional layer by first defining it\nfinal_conv_layer = clf_model.layers[-3]\n\n# defining a backend function to get outputs for various layers in the model\nget_output = tf.keras.backend.function([clf_model.layers[0].input], \n                                       [final_conv_layer.output])\n\n# target indices\ncam_len = np.arange(cam_df.shape[0])\n\n# iterating through files and labels for target class\nfor cam_path, cam_label, cam_prob, cam_pred, ind  in zip(cam_paths, cam_labels, \n                                                         cam_probs, cam_preds, cam_len): \n    \n    # loading and reading images\n    image_loaded = cv2.imread(cam_path)\n    image_loaded = cv2.cvtColor(image_loaded, cv2.COLOR_BGR2RGB)\n    image_loaded = np.asarray(image_loaded)\n    \n    # preprocessing images to make predictions using the model\n    prep_loaded = prep_func(image_loaded)\n    prep_loaded = np.expand_dims(prep_loaded, axis=0)\n    \n    # getting outputs for each target file\n    [conv_outputs] = get_output(prep_loaded)\n    conv_outputs = conv_outputs[0, :, :, :]\n    \n    # initializing a matrix to store class activation map\n    cam = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n    \n    # iterating through weights and adding them to activation map\n    for index, weight in enumerate(class_weights[:, cam_label]):\n        cam += weight * conv_outputs[:, :, index]\n    \n    # normalizing activation map\n    cam = np.maximum(cam, 0)\n    cam \/= np.max(cam)\n    \n    # postprocessing heatmap\n    heatmap = cv2.resize(cam, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n    heatmap = heatmap * 255\n    heatmap = np.clip(heatmap, 0, 255).astype(np.uint8)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_RGB2BGR)\n    \n    # superimposing heatmap and image\n    img = heatmap * 0.5 + image_loaded\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    \n    # displaying plots only for those predictions that do not match actual label\n    if cam_label != cam_pred or ind in corr_indices:\n        \n        print(\"Test Set Target Index:\", ind)\n        \n        if cam_label != cam_pred:\n            print(\"Misclassified Image\")\n            \n        elif ind in corr_indices:\n            print(\"Correctly Classified Image\")\n        \n        fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n        \n        # plotting original image\n        ax[0].imshow(image_loaded)\n        ax[0].set_title('Original Image Plot', \n                        fontsize = 12)\n        ax[0].set_xlabel(f'True Class: {inv_labels.get(cam_label)}', \n                         fontsize = 12)\n        ax[0].set_yticklabels([])\n        ax[0].set_xticklabels([])\n        \n        # plotting activation map\n        ax[1].imshow(heatmap)\n        ax[1].set_title('Activation Map Plot', \n                        fontsize = 12)\n        ax[1].set_xlabel(f'Predicted Class: {inv_labels.get(cam_pred)}', \n                         fontsize = 12)\n        ax[1].set_yticklabels([])\n        ax[1].set_xticklabels([])\n        \n        # plotting superimposed image\n        ax[2].imshow(img)\n        ax[2].set_title('Sumperimposed Image Plot', \n                        fontsize = 12)\n        ax[2].set_xlabel(f'Prediction Probability: {round(np.max(cam_prob)*100,1)} %', \n                         fontsize = 12)\n        ax[2].set_yticklabels([])\n        ax[2].set_xticklabels([])\n\n        plt.show()","cae2f548":"# Removing replicates","819ea67f":"# Importing libraries","b49bd420":"# Model training","47db0804":"This notebook implements a gradient methodology to visualize class activation maps for smaller training datasets produced as a result of clean-up through the removal of multiple replicated instances to get an insight into what neural networks see during learning. \n\nI would like to thank Jacob for putting together the original code which can be found [here](https:\/\/github.com\/jacobgil\/keras-grad-cam), Vincente for putting together an informative post on his blog which can be found [here](https:\/\/vincentblog.xyz\/posts\/class-activation-maps) (which I forgot to mention when I first started working on this) and Nain for putting together an awesome kernel on class activations which can be found [here](https:\/\/www.kaggle.com\/aakashnain\/what-does-a-cnn-see).","ec64b8b4":"# Creating generators ","4426f471":"# Model architecture","04a2be7a":"# Accessing files","280fbcec":"# Preprocessing files","c6a3d1ce":"# Visualizing distributions","2ccde6e7":"# Introduction","b48dda7f":"# Detecting replicates","ca464570":"# Visualizing replicates","0fc8284c":"# Test predictions","ff92c81f":"# Splitting dataset","aa29836c":"The original dataset having 20 classes was filtered to include three classes that have previously been a part of the ImageNet Large Scale Visual Recognition Competition Synsets along with a fourth class that wasn't.\n\nThe fourth class also has a few instances that were very different from the intended download collection and ended up being a part of the dataset due to the nature of the image search query but weren't removed which will result in a higher error during both training and prediction.","fc870a9c":"# Visualizing generators","2fb65993":"The following snippet of code can be used for debugging neural networks and to understand what the model sees for those instances that have been misclassified for the test set of the target class and a few randomly picked instances that have been correctly identified.","faae2a92":"# Visualizing activations"}}