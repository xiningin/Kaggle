{"cell_type":{"5332bebd":"code","8cb04b57":"code","fac6adcc":"code","0a8dd632":"code","bc52ced8":"code","8f5c94bf":"code","ed36a515":"code","78e5a38e":"code","04e55917":"code","4ee1dab8":"code","20913265":"code","ac7654c0":"markdown","29b0ad65":"markdown","af565a52":"markdown","7a4b543d":"markdown","76ee0c46":"markdown","980eae48":"markdown","d4c24268":"markdown","5c19a95d":"markdown","82ca4781":"markdown","e74ab348":"markdown"},"source":{"5332bebd":"import pandas as pd\n\ndata = pd.read_csv(\"..\/input\/cricket-on-reddit\/reddit_cricket.csv\")\n\nprint(data.shape)\ndata.head","8cb04b57":"import emoji\nimport re\n\ndata['emojiless'] = data['body'].apply(lambda x: emoji.get_emoji_regexp().sub(u'', str(x)))\n# data['emojiless']","fac6adcc":"from nltk.tokenize import RegexpTokenizer  \ntokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+http\\S+')\ndata['token'] = data['emojiless'].apply(lambda x: tokenizer.tokenize(x))\n# print(data['token'])\ndata['lower_token'] = data['token'].apply(lambda x: [word.lower() for word in x])\n# data['lower_token']","0a8dd632":"from nltk.corpus import stopwords\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nall_stopwords = nlp.Defaults.stop_words\n\ndata['token_without_sw'] = data['lower_token'].apply(lambda x: [word for word in x if not word in all_stopwords and len(word)>2 and not word in [\"www\", \"http\", \"https\", \"reddit\", \"cricket\", \"nan\"]])\n# data['token_without_sw']","bc52ced8":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndata['lemmatizer_tokens'] = data['token_without_sw'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n# print(data['lemmatizer_tokens'])","8f5c94bf":"print(f'Original number of words: ', sum([len(str(x)) for x in data['body']]))\nprint(f'Emojiless number of words: ', sum([len(str(x)) for x in data['emojiless']]))\nprint(f'Tokenized number of words: ', sum([len(str(x)) for x in data['lower_token']]))\nprint(f'Stopwordless number of words: ', sum([len(str(x)) for x in data['token_without_sw']]))\nprint(f'Lemmatized number of words: ', sum([len(str(x)) for x in data['lemmatizer_tokens']]))\n","ed36a515":"from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nimport numpy as np\n\nsia = SIA()\nresults = []\n\ndata['pool_score'] = data['lemmatizer_tokens'].apply(lambda x: [sia.polarity_scores(w).get('compound') for w in x]) \n# data['pool_score']\ndata['label'] = 0\ndata.loc[data['pool_score'].apply(lambda x: np.mean(x) > 0.1), 'label'] = 1\ndata.loc[data['pool_score'].apply(lambda x: np.mean(x) < 0.1), 'label'] = -1\n# data['label']","78e5a38e":"print(data.label.value_counts())","04e55917":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,8))\n\ncounts = data.label.value_counts(normalize=True) * 100\n\nsns.barplot(x=counts.index, y=counts, ax=ax)\n\nax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\nax.set_ylabel('Percentage')\n\nplt.show()","4ee1dab8":"from nltk import FreqDist \n\nall_words = \" \".join(x for x in data['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\npositive_words = \" \".join(x for x in data[data['label'] == 1]['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\nnegative_words = \" \".join(x for x in data[data['label'] == -1]['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\n\npos_freq = FreqDist(positive_words).most_common(5)\nneg_freq = FreqDist(negative_words).most_common(5)\nprint('pos_freq\\n', pos_freq)\nprint('neg_freq\\n', neg_freq)","20913265":"from wordcloud import WordCloud\n\nwordcloud_total = WordCloud(background_color=\"white\").generate(\" , \".join(all_words))\nwordcloud_posit = WordCloud(background_color=\"white\").generate(\" , \".join(positive_words))\nwordcloud_negat = WordCloud(background_color=\"white\").generate(\" , \".join(negative_words))\n\nplt.imshow(wordcloud_total, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\nplt.imshow(wordcloud_posit, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\nplt.imshow(wordcloud_negat, interpolation='bilinear')\nplt.axis('off')\nplt.show()","ac7654c0":"Using a quick value count, we can see that the sentiment for most of the words is nagative; they are also more positive than neutral in sentiment.","29b0ad65":"# Apply A Sentiment Analyzer (VADER)","af565a52":"Once we obtain our cleaned output, we would calculate each tokenized word's polarity scores using the **VADER** (Valence Aware Dictionary for Sentiment Reasoning) model.\n\nThe polarity scores measure the positivity and negativity for each word. We are mostly interested in the **compound score**, which is normalized to be between -1 (most extreme negative sentiment) and +1 (most extreme positive sentiment). This provides a single unidimensional measure of sentiment for a given word.","7a4b543d":"# Preprocess The Comments\nWhen dealing with raw or uncleaned data, we would first have to preprocess or \u201cclean\u201d it before modelling or in our case, to apply a sentiment analysis.\n\nIn our instance, we would include the following steps for our preprocessing:\n* Removing Emojis\n* Tokenizing, removing links etc.\n* Removing stopwords\n* Normalizing words via lemmatizing","76ee0c46":"For visualization purposes, we can see that the number of words pre and post-processing has reduced dramatically!","980eae48":"# Simple Visualizations\n\nLastly, before ending this post, we can perform some simple visualizations and explore the word frequency of positive and negative words.\n\nFrequency distribution of the 20 most common positive words:","d4c24268":"We can also visualize them via WordClouds.","5c19a95d":"The last pre-processing step is one of **Lemmatizing** or **Stemming**.\n\nBoth processes are used to trim words down to their root words. However, stemming might return a root word that is not an actual word whereas, lemmatizing returns a root word that is an actual language word.","82ca4781":"# Representation Of Sentiment Results","e74ab348":"# Reddit Sentiment Analysis"}}