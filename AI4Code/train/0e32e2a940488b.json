{"cell_type":{"2f00d39f":"code","e38a7680":"code","e8109b49":"code","4231e2d5":"code","bd649d05":"code","b638c536":"code","40a17276":"code","2625c6ed":"code","e5977a98":"code","86d8f359":"code","382ab060":"code","25388cad":"markdown","b0e1ae24":"markdown","87950cb0":"markdown"},"source":{"2f00d39f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')","e38a7680":"filename =\"\/kaggle\/input\/salary.csv\"\nsalary_data= pd.read_csv(filename)\nsalary_data.head()","e8109b49":"x=np.array(salary_data['YearsExperience'])\ny=np.array(salary_data['Salary'])\nl=len(x)","4231e2d5":"from sklearn.model_selection import train_test_split","bd649d05":"x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=6)\nlx=len(x_train)","b638c536":"m=0.1\nc=0.5\nalpha=0.01\nm_values=[]\nc_values=[]\nfor i in range(4000):\n    slope=0\n    intercept=0\n    for j in range(l):\n        intercept=intercept+((m*x[j]+c)-y[j])\n        slope=slope+((m*x[j]+c)-y[j])*x[j]\n    c=c-((alpha*intercept)\/l)\n    m=m-((alpha*slope)\/l)\n    c_values.append(c)\n    m_values.append(m)","40a17276":"print(f\"slope is {m}\")\nprint(f\"intercept is {c}\")","2625c6ed":"plt.plot(m_values[:],c_values[:],marker='o',\n         color='blue',markerfacecolor='red',\n         markersize=10,linestyle='dashed')\nplt.xlabel(\"slope\")\nplt.ylabel(\"intercept\")\nplt.title(\"convex function or bowl shape\")\nplt.show()","e5977a98":"y_pred=[]\nfor i in x_test:\n    v=m*i+c\n    y_pred.append(v)\nprint(y_pred)","86d8f359":"plt.plot(x_test,y_pred,marker='o',\n         color='blue',markerfacecolor='red',\n         markersize=10,linestyle='dashed')\nplt.scatter(x,y,marker='o',color='red')\nplt.xlabel(\"age\")\nplt.ylabel(\"slaary\")\nplt.title(\"linear regression with one variable\")\nplt.show()","382ab060":"def cost_function():\n    t=0\n    for i in range(len(y_pred)):\n        t=t+(y_pred[i]-y_test[i])**2\n    error=t\/(2*len(y_pred))\n    print(f\"error is {error}\")\ncost_function()","25388cad":"> **Linear Regression using Gradient Descent**","b0e1ae24":"cost function\n\n$ \\displaystyle J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 $","87950cb0":"Gradient Descent formula\n\n$ \\displaystyle h_{\\theta}(x)=\\theta_0+\\theta_1x \\mbox{ and } J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2 $\n\n$ \\displaystyle\\theta_j:=\\theta_j-\\alpha \\frac{\\partial}{\\partial\\theta_j}\\left(\\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\theta_0+\\theta_1x^{(i)}-y^{(i)}\\right)^2 \\right) $\n\n$ \\displaystyle \\frac{\\partial}{\\partial\\theta_0}\\left(\\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\theta_0+\\theta_1x^{(i)}-y^{(i)}\\right)^2 \\right) = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\theta_0+\\theta_1x^{(i)}-y^{(i)}\\right) = \\frac{1}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right) $\n\n$ \\displaystyle \\frac{\\partial}{\\partial\\theta_1}\\left(\\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\theta_0+\\theta_1x^{(i)}-y^{(i)}\\right)^2 \\right) = \\frac{1}{m}\\sum_{i=1}^{m} \\left(\\theta_0+\\theta_1x^{(i)}-y^{(i)}\\right).x^{(i)} = \\frac{1}{m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right).x^{(i)} $\n\n$ \\displaystyle\\mbox{ repeat until convergence }\\left\\{\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}f(\\theta_0,\\theta_1)\\mbox{ for j=0,1} \\right\\} $"}}