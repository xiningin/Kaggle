{"cell_type":{"0b737923":"code","cbb05e86":"code","9040e7c0":"code","7d9a4480":"code","addecbb3":"code","20f43546":"code","7e195dbd":"code","35112aca":"code","419ff913":"code","83f614cf":"code","01637771":"code","c3a3cc4f":"code","75b15139":"code","2babcd03":"code","86c45569":"code","5464eeb2":"code","75dd5ae6":"code","36ebc61d":"code","b972ea1f":"code","34f15c9a":"code","6968d843":"code","f668766c":"code","e4e333c5":"markdown","b24e6457":"markdown","1f98226b":"markdown"},"source":{"0b737923":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cbb05e86":"import pandas as pd\ndata = pd.read_csv('..\/input\/usinlppracticum\/imdb_train.csv',delimiter = \",\",encoding=\"latin-1\")\ndata.head() ","9040e7c0":"data_test = pd.read_csv(\"..\/input\/usinlppracticum\/imdb_test.csv\", delimiter=\",\",header=0,encoding=\"latin-1\")\ndata_test.head()","7d9a4480":"data_mas = pd.read_csv('..\/input\/imdb-review-dataset\/imdb_master.csv',encoding=\"latin-1\")\ndata_mas.head()","addecbb3":"data_mas = data_mas.drop(['Unnamed: 0','type','file'],axis=1)\ndata_mas.columns = [\"review\",\"sentiment\"]\ndata_mas.head()","20f43546":"data_mas = data_mas[data_mas.sentiment != 'unsup']\ndata_mas['sentiment'] = data_mas['sentiment'].map({'pos': 1, 'neg': 0})\ndata_mas.head()","7e195dbd":"data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0}) \ndata.head()","35112aca":"data = pd.concat([data, data_mas]).reset_index(drop=True)\ndata.head()","419ff913":"import string\nalphabet = string.ascii_letters+string.punctuation\ndata.review.str.strip(alphabet).astype(bool).any()","83f614cf":"data.review = data.review.str.replace('<br \/>', ' ')\ndata_test.review = data_test.review.str.replace('<br \/>', ' ')\ndata.head(17)","01637771":"data.review = data.review.str.replace(r\"[^a-zA-Z\\s]+\", \"\") \ndata.head()\n\ndata_test.review = data_test.review.str.replace(r\"[^a-zA-Z\\s]+\", \"\") \ndata_test.head()","c3a3cc4f":"data['review'] = data['review'].str.lower()\ndata.head()\n\ndata_test['review'] = data_test['review'].str.lower()\ndata_test.head()","75b15139":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text): \n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndata['c_review'] = data.review.apply(lambda x: clean_text(x))\ndata.head()\n","2babcd03":"import keras.preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Activation, GRU,Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","86c45569":"max_features = 8800\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(data['c_review'])\nlist_tokenized_train = tokenizer.texts_to_sequences(data['c_review'])\n\nmaxlen = 130\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\ny = data['sentiment']","5464eeb2":"embed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(Dropout(0.3))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20,activation = \"relu\")) \nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation = \"sigmoid\"))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nmodel.fit(X_t, y,batch_size=500,epochs = 10, validation_split=0.1 )","75dd5ae6":"data_test.head(20)","36ebc61d":"data_test['review']=data_test.review.apply(lambda x: clean_text(x)) \ndata_test.head()","b972ea1f":"list_sentences_test = data_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nprediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\n\n","34f15c9a":"pred = pd.DataFrame(y_pred.flatten())\ndata_pred= pd.merge(data_test, pred, left_index=True, right_index=True)","6968d843":"data_pred.columns = ['id','review','sentiment']\ndata_pred['sentiment'] = data_pred['sentiment'].map({True: 'positive', False: 'negative'})\ndata_pred_s = data_pred[['id','sentiment']]\ndata_pred_s.head()","f668766c":"data_pred_s.to_csv (r'submissions_v4.csv', index = None, header=True)","e4e333c5":"Remove special characters to clean data.","b24e6457":"First we will remove html tags","1f98226b":"**Data PreProcessing:**\nCheck for any special character in the review column"}}