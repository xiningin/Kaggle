{"cell_type":{"2147c13a":"code","7bcd3d04":"code","1e9209ca":"code","97881903":"code","863a67d9":"code","f5f4a8e8":"code","1d864ffe":"code","dc2dc01c":"code","18c69c70":"code","cee78e88":"code","3a41478a":"code","8fbb5c0c":"code","db07f4d4":"code","d72e3d40":"code","c07da273":"code","3072025b":"code","61a9f59f":"code","6cbf168e":"code","0c1e469f":"code","6d251e27":"code","edc6b1d2":"code","62acd404":"code","16e58353":"code","a8f72538":"code","d946ef3b":"code","2b9e8e14":"code","49fa8de9":"code","77c95c25":"code","9a38d41b":"code","b08a0925":"code","3639ad4d":"code","cda5790c":"code","2cacd2e8":"code","343a5bbf":"code","303d7f3c":"code","5e460deb":"code","1dd7008f":"code","a5670e74":"code","b482345a":"code","97b12ec5":"markdown","c46653e7":"markdown","bab2c4bb":"markdown","8b01049e":"markdown","bbb1881a":"markdown","3a8f5638":"markdown","4798a674":"markdown","f63ff9b9":"markdown","ae33e24e":"markdown","254cf3cb":"markdown","6acdd5a1":"markdown","e1cc590d":"markdown","6cb3f849":"markdown","159c03c2":"markdown","3a89a289":"markdown","9a54cebf":"markdown","02f6f79f":"markdown","22b8cc24":"markdown","115ebdc2":"markdown","a8bfc922":"markdown","dd39fd40":"markdown","f1a84c82":"markdown","c8de30a1":"markdown","69f92869":"markdown","f256e4f9":"markdown","4ce7da7d":"markdown","a58ec538":"markdown","48a54a5f":"markdown","580a9ae5":"markdown","20b3ec2a":"markdown","f6c6f226":"markdown","4393cf8c":"markdown","186482fa":"markdown","ee514a6a":"markdown","e799d4c8":"markdown","18cedca0":"markdown","bbaa2684":"markdown"},"source":{"2147c13a":"import numpy as np \nimport os \nimport pandas as pd \nfrom math import sqrt\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nimport tensorflow_probability as tfp\nimport gc\nimport time","7bcd3d04":"import geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\nfrom geopandas.tools import sjoin\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns; sns.set()\nfrom IPython.display import Image\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm\nimport folium","1e9209ca":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","97881903":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","863a67d9":"# BXL_timeseries_kaggle.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\nnew_table = pd.read_csv('..\/input\/obu-data-preprocessing\/Flow_BEL_street_30min.csv')\nnRow, nCol = new_table.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f5f4a8e8":"df_belgium = gpd.read_file('\/kaggle\/input\/belgium-obu\/Belgium_streets.json')\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\n\nm","1d864ffe":"mean_value = 10\n\ntable_index = new_table.iloc[:,1:]\nALL_STREETS = list(table_index.columns.values)\n\nmean_flow =[]\nnew_street=[]\n\n\nfor street in ALL_STREETS:\n    \n    single_street=table_index[street]\n    mean = np.mean(single_street)\n    mean_flow.append(mean)\n    new_street.append(street)\n    \ndf_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\nprint('')\nprint(df_mean_flow.head())\nprint('')\n\nSTREETS = df_mean_flow[(df_mean_flow['mean_flow']>= mean_value)] \nSTREETS = STREETS.sort_values(by=['street_index'])\nSTREETS = list(STREETS.street_index)\n\nprint('considering a average traffic flow of ' + str(mean_value)+' per street')\nprint('')\nprint('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))\n","dc2dc01c":"new_table['Datetime'] = pd.to_datetime(new_table['datetime'])\n\nDATAFRAME = new_table\nDATAFRAME = DATAFRAME.drop(['datetime'],axis=1) \nDATAFRAME = DATAFRAME[DATAFRAME.columns.intersection(STREETS)]\n\n# Time-based Covariates\n\nDATAFRAME['minutes'] = new_table['Datetime'].dt.minute\nDATAFRAME['hour'] = new_table['Datetime'].dt.hour\n\nDATAFRAME['hour_x']=np.sin(DATAFRAME.hour*(2.*np.pi\/23))\nDATAFRAME['hour_y']=np.cos(DATAFRAME.hour*(2.*np.pi\/23))\n\nDATAFRAME['day'] = new_table['Datetime'].dt.day\nDATAFRAME['DayOfWeek'] = new_table['Datetime'].dt.dayofweek\n\nDATAFRAME['WorkingDays'] = DATAFRAME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATAFRAME = DATAFRAME.drop(['minutes','hour','day'],axis=1)\n\n# temporal features = 4\nfeat_time = 4\n\nDATAFRAME.head()","18c69c70":"STREETS = [int(float(s)) for s in STREETS]\n\ndf_belgium = df_belgium[df_belgium.index.isin(STREETS)]\ndf_belgium['Trucks_Flow'] =  DATAFRAME.iloc[2182,:-4].astype(float).values\n\nnbh_count_colormap = linear.YlOrRd_09.scale(0,200)\n\ncolormap_dept = cm.StepColormap(\n    colors=['#00ae53', '#86dc76', '#daf8aa',\n            '#ffe6a4', '#ff9a61', '#ee0028'],\n    vmin = 0,\n    vmax = 200,\n    index=[0, 20, 50, 80, 110, 150, 180])\n\npolygons = df_belgium\nm = folium.Map([50.85045, 4.34878], zoom_start= 9, tiles='cartodbpositron')\n\nstyle_function = lambda x: {\n    'fillColor': colormap_dept(x['properties']['Trucks_Flow']),\n    'color': colormap_dept(x['properties']['Trucks_Flow']),\n    'weight': 1.5,\n    'fillOpacity': 1\n}\nfolium.GeoJson(polygons,\n    style_function=style_function).add_to(m)\n\n\ncolormap_dept.caption = 'Traffic Flow (N#Trucks\/30min) at (not real) 12:00 a.m.'\ncolormap_dept.add_to(m)\n\nm","cee78e88":"Image(\"\/kaggle\/input\/image-lstm\/MATRIX.jpg\")","3a41478a":"val_step = 168*2 + 168*2 # 1 WEEK\ntest_step = 168*2 # 1 WEEK\n\n# ATTENTION: anything you learn and is not known in advance, must be learnt only from training data!\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_aux = MinMaxScaler(feature_range=(0, 1))\n\n# TRAINING --- (scaler\/scaler_aux).fit_transform()\n# TESTING --- (scaler\/scaler_aux).transform()\n\n# TRAINING SET\nTRAIN = DATAFRAME[: -val_step]\ntrain_feat = scaler.fit_transform(TRAIN.values[:,:-feat_time])\n\n# VALIDATION SET\nVAL = DATAFRAME[-val_step : -test_step]\nvalid_feat = scaler.transform(VAL.values[:,:-feat_time])\n\n# TESTING SET\nTEST = DATAFRAME[-test_step:]\ntest_feat = scaler.transform(TEST.values[:,:-feat_time])\n\n\n# AUX are known in advance\nAUX = scaler_aux.fit_transform(DATAFRAME.values[:,-feat_time:])\ntrain_aux = AUX[: -val_step]\nvalid_aux = AUX[-val_step: -test_step]\ntest_aux = AUX[-test_step:]\n\n\n# concate final results\ntrain_feat = np.hstack([train_feat, train_aux])\nvalid_feat = np.hstack([valid_feat, valid_aux])\ntest_feat = np.hstack([test_feat, test_aux])\n","8fbb5c0c":"def inverse_transform(forecasts, scaler):\n    # invert scaling\n    inv_pred = scaler.inverse_transform(forecasts)\n    return inv_pred","db07f4d4":"nRow, nCol = DATAFRAME.shape\n\nplt.figure(figsize=(10,5))\n\nlength = list(range(DATAFRAME.shape[0]))\n\nplt.plot(length[:TRAIN.shape[0]], np.sum(TRAIN.iloc[:,:-feat_time], axis=1))\nplt.plot(length[TRAIN.shape[0]:TRAIN.shape[0]+VAL.shape[0]],np.sum(VAL.iloc[:,:-feat_time], axis=1))\nplt.plot(length[TRAIN.shape[0]+VAL.shape[0]:TRAIN.shape[0]+VAL.shape[0]+TEST.shape[0]],np.sum(TEST.iloc[:,:-feat_time], axis=1))\nplt.legend(['training','validation','testing'], loc='upper left')\nplt.show()\n\nprint(f'Consider {nRow} instances (rows) and {nCol} streets segments (columns)')\nprint('')\nprint('TRAIN SIZE: '+ str(TRAIN.shape))\nprint('')\nprint('VAL SIZE: '+ str(VAL.shape))\nprint('')\nprint('TEST SIZE: '+ str(TEST.shape))","d72e3d40":"Image(\"\/kaggle\/input\/image-lstm\/DATAPREP.png\")","c07da273":"def prep_data(dataframe, INPUT, OUTPUT, AUX, BATCH):\n    \n    TOTAL = INPUT + OUTPUT\n    \n    dataset_feat = tf.data.Dataset.from_tensor_slices(dataframe)    \n    aux = tf.data.Dataset.from_tensor_slices(dataframe[:,-AUX:])    \n    dataset_labels = tf.data.Dataset.from_tensor_slices(dataframe)\n\n    # features - past observations\n    feat = dataset_feat.window(INPUT,  shift=1,  stride=1,  drop_remainder=True) \n    feat = feat.flat_map(lambda window: window.batch(INPUT))\n    \n    # aux - temporal features\n    aux = aux.window(OUTPUT,  shift=1,  stride=1,  drop_remainder=True ).skip(INPUT)\n    aux = aux.flat_map(lambda window: window.batch(OUTPUT))\n    \n    # labels - future observations\n    label = dataset_labels.window(OUTPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    label = label.flat_map(lambda window: window.batch(OUTPUT))\n    \n    dataset = tf.data.Dataset.zip(((feat, aux), label))\n    \n    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset\n","3072025b":"n_total_features = len(DATAFRAME.columns) \n\nsize_input = 12\nsize_forecast = 12\nsize_total = size_input + size_forecast\nsize_aux = feat_time\n\nbatch_size = 32\nbatch_train = batch_size\nbatch_valid = batch_size\nbatch_test = 1\n\nwindowed_train = prep_data(train_feat, size_input, size_forecast, size_aux, batch_train)\nwindowed_valid = prep_data(valid_feat, size_input, size_forecast, size_aux, batch_valid)\nwindowed_test = prep_data(test_feat, size_input, size_forecast, size_aux, batch_test)\n\nlatent_dim = 25\nEPOCHS = 250 #250 # in the paper 250","61a9f59f":"# timestamp for predictions\ntimestamp = new_table['Datetime']\ntimestamp_pred = timestamp[-test_step+size_input-1:]","6cbf168e":"Image(\"\/kaggle\/input\/image-lstm\/ECDEC.jpg\")","0c1e469f":"# the input for encoder\npast_inputs = tf.keras.Input(shape=(size_input, n_total_features), name = 'enc_inputs')\npast_inputs","6d251e27":"class Encoder(tf.keras.Model):\n    \n    def __init__(self, enc_units, sz_input, sz_tot,  batch_sz):\n        \n        super(Encoder, self).__init__()\n        \n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        \n        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n                                         return_sequences=False, #turn to False for one layer\n                                         return_state=True,\n                                          recurrent_initializer='glorot_uniform',\n                                          kernel_regularizer=regularizers.l2(0.001),\n                                          name='Encoder')\n\n                \n    def __call__(self, x):\n\n        output, state_h, state_c = self.lstm(x) # just one layer on the paper \n        \n        return output, state_h, state_c\n","edc6b1d2":"encoder = Encoder(latent_dim, size_input, n_total_features,  batch_size)\nout_enc, h, c = encoder(past_inputs)\nout_enc","62acd404":"# the future input for decoder\nfuture_inputs = tf.keras.Input(shape=(size_forecast, size_aux), name='aux_inputs')\nfuture_inputs","16e58353":"class Decoder(tf.keras.Model):\n    \n    def __init__(self, dec_units, sz_forecast, sz_aux, tot_feat,  batch_sz):\n        \n        super(Decoder, self).__init__()\n        \n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.feat = tot_feat\n        self.out = sz_forecast\n        self.sz_aux = sz_aux\n\n        \n        self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences = True,\n                                        recurrent_initializer='glorot_uniform',\n                                         kernel_regularizer=regularizers.l2(0.001),\n                                         name ='Decoder') # \n        \n        self.dense1 = tf.keras.layers.Dense(self.dec_units, kernel_regularizer=regularizers.l2(0.001))\n        \n        self.drop = tf.keras.layers.Dropout(0.1)\n        \n        self.fc = tf.keras.layers.Dense(self.feat, kernel_regularizer=regularizers.l2(0.001)) #\n        \n\n    def __call__(self, x, h, c,  training=False):\n    \n        output_dec = self.lstm(x, initial_state= [h, c]) # just one layer on the paper \n        \n        if training:\n            \n            output_dec = self.drop(output_dec, training =training)\n\n        out = self.fc(output_dec)\n\n        return out\n        \n\n        ","a8f72538":"decoder = Decoder(latent_dim, 12, size_aux, n_total_features,  batch_size)\nout_dec = decoder(future_inputs, out_enc, c)\nout_dec ","d946ef3b":"opt = tf.keras.optimizers.Adam()","2b9e8e14":"loss_fct = tf.keras.losses.MeanAbsoluteError(name='loss_function')\nvalid_loss_fct = tf.keras.losses.MeanAbsoluteError(name='valid_loss_function')","49fa8de9":"@tf.function\ndef batch_loss(inp, aux, targ, loss_funct, opt=None):\n    loss = 0\n    with tf.GradientTape() as tape:\n        context_vector, state_h, state_c = encoder(inp)\n        predictions = decoder(aux, state_h, state_c, training=True)\n        loss = loss_funct(targ, predictions)\n    if opt is not None:\n        variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        opt.apply_gradients(zip(gradients, variables))\n    return loss","77c95c25":"def monitor_loss(val, epoch, delta, cnt):\n    if ((epoch > 0) and (val[epoch-1] - val[epoch])) > delta:\n        cnt = 0    \n    else:\n        cnt += 1 \n        \n    return cnt","9a38d41b":"print('')\nprint('Training & Validation')\nprint('')\n\n# early stopping\npatience = 20   \nmin_delta = 0.0001   \npatience_cnt = 0 \n\n# Keep results for plotting\ntrain_loss_results = []\nvalid_loss_results = []\nsteps_per_epoch = len(TRAIN) \/\/ batch_size\n\nstart = time.time()\n\nfor epoch in range(EPOCHS):\n    \n    ## training\n    step = 0\n    epoch_loss_avg = tf.keras.metrics.Mean()\n    \n    for (batch, (inp_tot, targ)) in enumerate(windowed_train.take(steps_per_epoch)):\n        \n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        batch_loss_results = batch_loss(inp, aux, targ, loss_fct, opt)\n        \n        # training progress\n        epoch_loss_avg.update_state(batch_loss_results)\n\n    # collect training loss values\n    train_loss_results.append(epoch_loss_avg.result())\n    \n    ## validation\n    step = 0\n    epoch_valid_loss_avg = tf.keras.metrics.Mean()\n    \n    for (batch, (inp_tot, targ)) in enumerate(windowed_valid.take(steps_per_epoch)):\n        \n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        batch_loss_results = batch_loss(inp, aux, targ, valid_loss_fct, None)\n        \n        # training progress\n        epoch_valid_loss_avg.update_state(batch_loss_results)\n\n    # collect training loss values\n    valid_loss_results.append(epoch_valid_loss_avg.result())\n    \n    if epoch % 10 == 0:\n        \n        print(\"Epoch {}: Loss MAE: {:.5f} --- Val Loss MAE: {:.5f}\".format(epoch,\n                                                                       epoch_loss_avg.result(),\n                                                                       epoch_valid_loss_avg.result()))\n        \n      # ----- EARLY STOPPING -------\n    \n    patience_cnt = monitor_loss(valid_loss_results, epoch, min_delta, patience_cnt)\n\n    if patience_cnt > patience:\n        \n        print(\"early stopping...\") \n        break  \n        \nprint('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","b08a0925":"fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\n\nfig.suptitle('Training Metrics')\naxes.set_ylabel(\"Loss (MAE) - training and validation\", fontsize=14)\naxes.plot(train_loss_results)\naxes.plot(valid_loss_results)\naxes.set_xlabel(\"Epoch\", fontsize=14)\nplt.show()","3639ad4d":"def evaluate_forecasts(targets, forecasts, n_seq):\n    \n    list_rmse = []\n    list_mae = []\n    \n    for i in range(n_seq):\n        true = np.vstack([target[i] for target in targets])\n        predicted = np.vstack([forecast[i] for forecast in forecasts])\n        \n        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n        mae = np.absolute(true - predicted).mean(axis=0)\n        \n        list_rmse.append(rmse)\n        list_mae.append(mae)\n        \n    list_rmse = np.vstack(list_rmse)\n    list_mae = np.vstack(list_mae)\n    \n    return list_rmse, list_mae","cda5790c":"def update_model(step, train_feat, test_feat,  size_input, size_forecast, size_aux, batch_size, loss_fct, opt, epoch_loss_avg, train_loss_results):\n\n        # new instance\n        new_instance = test_feat[step,:].reshape(1,-1)\n        # new sample of data\n        train_feat = np.vstack([train_feat[-23:,:], new_instance])\n        # prepare the data\n        windowed_new = prep_data(train_feat, size_input, size_forecast, size_aux, batch_size) \n\n        update_steps_per_epoch = len(train_feat)\/\/batch_size\n        \n        # number of epochs for update\n        UPDATE = 2\n        \n        for epoch in range(UPDATE):\n            \n            for (batch, (inp_tot_new, targ_new)) in enumerate(windowed_new.take(update_steps_per_epoch)):\n                \n                inp_new = inp_tot_new[0]\n                aux_new = inp_tot_new[1]\n                targ_new = tf.cast(targ_new, tf.float32)\n                \n                batch_loss_results = batch_loss(inp_new, aux_new, targ_new, loss_fct, opt)\n                \n                # Track progress\n                epoch_loss_avg.update_state(batch_loss_results)\n                \n            # End epoch\n            train_loss_results.append(epoch_loss_avg.result())\n            \n            if epoch % UPDATE == 0:\n                print(\"UPDATE - Epoch {}: Loss MAE: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n        \n        return train_feat, epoch_loss_avg,  train_loss_results\n        ","2cacd2e8":"forecasts = []\ntargets = []\n\nrmse_list = []\nmae_list = []\n\nprint('Starting')\nprint('')\n\nfor (step, (inp_tot, targ)) in enumerate(windowed_test):\n           \n\n        inp = inp_tot[0]\n        aux = inp_tot[1]\n        \n        targ = tf.cast(targ, tf.float32)\n        \n        out_enc, state_h, state_c  = encoder(inp)\n        \n        pred = decoder(aux, state_h, state_c, training=False)\n        \n        truth = inverse_transform(targ[0][:,:-size_aux],  scaler)\n        pred = inverse_transform(pred[0][:,:-size_aux],  scaler)\n        \n        forecasts.append(pred)\n        targets.append(truth)\n        \n        rmse, mae = evaluate_forecasts(targets, forecasts, size_forecast)\n           \n        rmse_list.append(rmse)\n        mae_list.append(mae)\n        \n        t = timestamp_pred.iloc[step+1:step+13]\n        \n       \n        plt.fill_between(t,  np.mean(pred, axis=1) + np.std(pred, axis=1), \n                   (np.mean(pred, axis=1) - np.std(pred, axis=1)),\n                   color = 'green', label = 'pred mean +- std', alpha=0.13,\n                   linewidth = 2)\n\n        plt.fill_between(t,  np.mean(truth, axis=1) + np.std(truth, axis=1), \n                   (np.mean(truth, axis=1) - np.std(truth, axis=1)),\n                   color = 'orange', label = 'targ mean +- std', alpha=0.13,\n                   linewidth = 2)\n\n        plt.plot(t, np.mean(pred, axis=1), color = 'green', lw=2, label='Mean Prediction') \n        plt.plot(t, np.mean(truth, axis=1), color = 'orange', lw=2, label='Mean Truth')\n\n        plt.title('$Mean\\ Traffic\\ Flow\\ in\\ Belgium$ at: '+str(timestamp_pred.iloc[step]))\n        plt.ylabel('$Traffic\\ Flow$')\n        plt.xlabel('$Forecasting\\ Horizon$')\n        plt.xticks(rotation=45)\n        plt.legend(loc='upper left')\n        plt.show()\n        \n        \n        print('* Time step '+str(step))\n        print('* Timestamp '+str(timestamp_pred.iloc[step]))\n        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n        print('* After prediction UPDATE model with new streets observations')\n        \n        MODEL_UPDATE = True\n        if MODEL_UPDATE == True:\n            \n            train_feat, epoch_loss_avg,  train_loss_results  = update_model(step, train_feat, test_feat,  size_input, \n                 size_forecast, size_aux, batch_size, loss_fct, opt, epoch_loss_avg, train_loss_results)\n\n        \n        # ---> comment this to have full prediction\n        if step == 10:\n            break\n            \n        print('')     ","343a5bbf":"RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\nRMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n\nfor i in range(len(RMSE_MEAN)):\n    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n    print('')","303d7f3c":"MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\nMAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n\nfor i in range(len(MAE_MEAN)):\n    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n    print('')","5e460deb":"t = timestamp_pred.iloc[step+1:step+13]\n        \n       \nplt.fill_between(t,  np.mean(pred, axis=1) + np.std(pred, axis=1), \n           (np.mean(pred, axis=1) - np.std(pred, axis=1)),\n           color = 'green', label = 'pred mean +- std', alpha=0.13,\n           linewidth = 2)\n\nplt.fill_between(t,  np.mean(truth, axis=1) + np.std(truth, axis=1), \n           (np.mean(truth, axis=1) - np.std(truth, axis=1)),\n           color = 'orange', label = 'targ mean +- std', alpha=0.13,\n           linewidth = 2)\n\nplt.plot(t, np.mean(pred, axis=1), color = 'green', lw=2, label='Mean Prediction') \nplt.plot(t, np.mean(truth, axis=1), color = 'orange', lw=2, label='Mean Truth')\n\nplt.title('$Mean\\ Traffic\\ Flow\\ in\\ Belgium$ at: '+str(timestamp_pred.iloc[step]))\nplt.ylabel('$Traffic\\ Flow$')\nplt.xlabel('$Forecasting\\ Horizon$')\nplt.xticks(rotation=45)\nplt.legend(loc='upper left')\nplt.show()","1dd7008f":"!pip install pydeck","a5670e74":"import pydeck as pdk\n\ndf_belgium['traffic_flow'] = pred[0]\nprint('Prediction for time: ' +str(timestamp_pred.iloc[step+1]))\n\ntooltip = {\"text\": \"Traffic Flow : {traffic_flow}\"}\n\nINITIAL_VIEW_STATE = pdk.ViewState(latitude=50.85045, longitude=4.34878, zoom=8, max_zoom=30, pitch=10, bearing=0)\ngeojson = pdk.Layer(\n\"GeoJsonLayer\",\ndf_belgium,\nstroked=False,\nfilled=True,\nextruded=True,\nwireframe=True,\nget_elevation = \"traffic_flow*5\",\nget_fill_color='[255, (1-traffic_flow\/250)*255, 0]',\nget_line_color='[255, 255, 255]')\n\nr = pdk.Deck(map_style=pdk.map_styles.LIGHT, layers=geojson, initial_view_state=INITIAL_VIEW_STATE,tooltip={\"text\": \"Traffic Flow: {traffic_flow}\"},)\nr.to_html(\"geojson_layer.html\", notebook_display=True)\n\n\n","b482345a":"import pickle\n\n# Saving the objects:\nwith open('save_predictions_results.pkl', 'wb') as f: \n    pickle.dump([rmse_list, mae_list], f)","97b12ec5":"# Train Model","c46653e7":"### Decoder Output","bab2c4bb":"# SPLITTING Training\/Testing","8b01049e":"### Add time-based Covariates","bbb1881a":"### Optimizer","3a8f5638":"# Check Files","4798a674":"# Seed","f63ff9b9":"# Data Preparation\n\n## *{Batch_Sz, Input_Sq, Feature_Sz}*","ae33e24e":"# Example Visualizations Results for Time step t","254cf3cb":"### early stopping","6acdd5a1":"# Paper - A tutorial on traffic forecasting with deep learning: [Tutorial Deep Learning](https:\/\/www.researchgate.net\/profile\/Giovanni-Buroni-2\/publication\/348930068_A_Tutorial_on_Network-Wide_Multi-Horizon_Traffic_Forecasting_with_Deep_Learning\/links\/6017c45a92851c2d4d0aa267\/A-Tutorial-on-Network-Wide-Multi-Horizon-Traffic-Forecasting-with-Deep-Learning.pdf)\n","e1cc590d":"### Input Decoder","6cb3f849":"# Visualize Street Network","159c03c2":"### pydeck","3a89a289":"# LSTM encoder decoder model - Multivariate Multiple-step ahead Prediction Model","9a54cebf":"## General Import","02f6f79f":"# Direct Approach","22b8cc24":"# Select Streets based on Average Traffic Flow","115ebdc2":"# Parameters","a8bfc922":"## Visualize testing set","dd39fd40":"# Performance Metrics for each forecasting horizon (here for first 5 predictions steps).\n\nTo get metrics for the full test set, comment the above\n\n```\nif step == 5:\n            break\n```","f1a84c82":"# Test and Update Model","c8de30a1":"### Input Encoder","69f92869":"#### visualization for prediction at (t+1)","f256e4f9":"# Plot Training Progress","4ce7da7d":"# MAE Metric","a58ec538":"# LSTM Encoder Decoder Architecture","48a54a5f":"#### Matplotlib","580a9ae5":"# Import OBU File","20b3ec2a":"# Save Results Pickle","f6c6f226":"# Visualize Traffic Flow at Particular Time","4393cf8c":"### Batch Loss","186482fa":"### Training and Validation","ee514a6a":"### Encoder","e799d4c8":"# RMSE Metric","18cedca0":"### Mean Absolute Loss function ","bbaa2684":"### Decoder"}}