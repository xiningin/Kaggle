{"cell_type":{"9b241242":"code","71ed5f7d":"code","cdad06c3":"code","22762b51":"code","ee122425":"code","41d39e99":"code","78388429":"code","44d0c986":"code","8014d99f":"code","bc9f727c":"code","06be81ae":"code","9615c5a1":"code","e294b421":"code","bc18a305":"code","27a50a75":"code","6aaacb70":"code","ba85dd1b":"code","b61b4682":"code","f5c53614":"code","07ed3501":"code","4e2b3ac7":"code","c1dd6096":"code","5df93f63":"code","179ca62f":"code","2c4e64f4":"code","8abb1431":"code","fa7b03b3":"code","64a35dce":"code","11c104ec":"code","64f6696d":"code","0c90442a":"code","4a532fb4":"code","1e01723d":"code","b79ea0c2":"code","2b3b347c":"code","961c18d7":"code","576d2852":"code","0e430813":"code","f69397c1":"code","c13aa844":"code","c1f8474f":"code","6fe2f932":"code","0b385217":"code","6c997a01":"code","5e88bd67":"code","eaa08306":"code","b7091b3f":"code","c9d3f285":"code","02ddc8ee":"code","c9ac8adf":"code","3b3be409":"code","c07b9ac0":"code","eb8b3fce":"code","03412623":"code","bad74c71":"code","720d7c16":"code","97aa2f0e":"code","f0474974":"code","2a788c4a":"code","9fc400db":"code","3386c42d":"code","f9f1ed68":"code","9fe25b70":"code","3a2cac58":"code","dce39db0":"code","01afb5fc":"code","926f1fad":"code","2fbf3cca":"code","077086a6":"code","6df6d90c":"code","a26817eb":"code","494b809a":"code","e11e56c4":"code","fb14ff51":"code","306412e2":"code","fd049b0f":"code","17045e0a":"code","71b497d1":"code","8df2166c":"code","a635bd76":"code","009af308":"code","22a8aa1a":"code","ca15dcdc":"code","8e428572":"code","03e91c49":"code","284f5ff5":"code","f23d90ba":"code","41e5eebe":"code","2c8dd4d7":"code","78b74329":"code","7bb873dd":"code","64c86117":"code","141087af":"code","fde1f5dd":"code","d0af9c42":"code","17496219":"code","a64c84f7":"markdown","2e0a7e75":"markdown","92e57012":"markdown","15beffe2":"markdown","739cf4d0":"markdown","3b3109dd":"markdown","83580bfe":"markdown","8f9c637e":"markdown","673578d0":"markdown","41a4e368":"markdown","a20f895c":"markdown","8f966eb0":"markdown","23cace71":"markdown","a541f5ac":"markdown","025ecab4":"markdown","fa96d150":"markdown","afe1a854":"markdown","00efaaf0":"markdown","f562b20e":"markdown","77c1055b":"markdown","e03c6a0e":"markdown","d081e46f":"markdown","9957bfcc":"markdown","3b24357e":"markdown","ddeba84a":"markdown","7329abe9":"markdown","690b14bc":"markdown","69a71f0f":"markdown","d62b1284":"markdown","611c4031":"markdown","15f0401d":"markdown","d4c49b17":"markdown","8bc8fbdd":"markdown","e28e93c0":"markdown","de7a3769":"markdown","a2cae301":"markdown","c373dec8":"markdown","da500c4c":"markdown","9411aeac":"markdown","b97d19cf":"markdown","2d662c02":"markdown","0208f956":"markdown","2b2ea4ae":"markdown","6b6b028c":"markdown","6f0bb0c9":"markdown","da3e30a1":"markdown","bccf4ad0":"markdown","de13ab53":"markdown","e178922c":"markdown","838311cb":"markdown","32d2d934":"markdown","d4cc93f5":"markdown","d2bfc984":"markdown","822bcab5":"markdown","190ded4f":"markdown","381e573a":"markdown","adbe361c":"markdown","4ac329f0":"markdown","1419ffde":"markdown","63859873":"markdown","1d4c35df":"markdown","735c77ad":"markdown","6d58686d":"markdown","59a5890a":"markdown","cfc7ad20":"markdown","61919b97":"markdown","c1087b34":"markdown","c9c8b3ba":"markdown","60d98037":"markdown"},"source":{"9b241242":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n\n# to display all the columns of the dataframe in the notebook\npd.pandas.set_option('display.max_columns', None)","71ed5f7d":"import os\nprint(os.listdir(\"..\/input\"))","cdad06c3":"# load dataset\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# rows and columns of the data\nprint(data.shape)\n\n# visualise the dataset\ndata.head()","22762b51":"# make a list of the variables that contain missing values\nvars_with_na = [var for var in data.columns if data[var].isnull().sum() > 0]\n\n# determine percentage of missing values\ndata[vars_with_na].isnull().mean()","ee122425":"def analyse_na_value(df, var):\n\n    df = df.copy()\n\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    df[var] = np.where(df[var].isnull(), 1, 0)\n\n    # let's compare the median SalePrice in the observations where data is missing\n    # vs the observations where a value is available\n\n    df.groupby(var)['SalePrice'].median().plot.bar()\n\n    plt.title(var)\n    plt.show()\n\n\n# let's run the function on each variable with missing data\nfor var in vars_with_na:\n    analyse_na_value(data, var)","41d39e99":"# make list of numerical variables\nnum_vars = [var for var in data.columns if data[var].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(num_vars))\n\n# visualise the numerical variables\ndata[num_vars].head()","78388429":"print('Number of House Id labels: ', len(data.Id.unique()))\nprint('Number of Houses in the Dataset: ', len(data))","44d0c986":"# list of variables that contain year information\n\nyear_vars = [var for var in num_vars if 'Yr' in var or 'Year' in var]\n\nyear_vars","8014d99f":"# let's explore the values of these temporal variables\n\nfor var in year_vars:\n    print(var, data[var].unique())\n    print()","bc9f727c":"data.groupby('YrSold')['SalePrice'].median().plot()\nplt.ylabel('Median House Price')\nplt.title('Change in House price with the years')","06be81ae":"# let's explore the relationship between the year variables\n# and the house price in a bit of more detail:\n\ndef analyse_year_vars(df, var):\n    df = df.copy()\n    \n    # capture difference between year variable and year\n    # in which the house was sold\n    df[var] = df['YrSold'] - df[var]\n    \n    plt.scatter(df[var], df['SalePrice'])\n    plt.ylabel('SalePrice')\n    plt.xlabel(var)\n    plt.show()\n    \n    \nfor var in year_vars:\n    if var !='YrSold':\n        analyse_year_vars(data, var)\n    ","9615c5a1":"#  let's male a list of discrete variables\ndiscrete_vars = [var for var in num_vars if len(\n    data[var].unique()) < 20 and var not in year_vars+['Id']]\n\n\nprint('Number of discrete variables: ', len(discrete_vars))","e294b421":"# let's visualise the discrete variables\n\ndata[discrete_vars].head()","bc18a305":"def analyse_discrete(df, var):\n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('Median SalePrice')\n    plt.show()\n    \nfor var in discrete_vars:\n    analyse_discrete(data, var)","27a50a75":"# make list of continuous variables\ncont_vars = [\n    var for var in num_vars if var not in discrete_vars+year_vars+['Id']]\n\nprint('Number of continuous variables: ', len(cont_vars))","6aaacb70":"# let's visualise the continuous variables\n\ndata[cont_vars].head()","ba85dd1b":"# Let's go ahead and analyse the distributions of these variables\n\n\ndef analyse_continuous(df, var):\n    df = df.copy()\n    df[var].hist(bins=30)\n    plt.ylabel('Number of houses')\n    plt.xlabel(var)\n    plt.title(var)\n    plt.show()\n\n\nfor var in cont_vars:\n    analyse_continuous(data, var)","b61b4682":"# Let's go ahead and analyse the distributions of these variables\n# after applying a logarithmic transformation\n\n\ndef analyse_transformed_continuous(df, var):\n    df = df.copy()\n\n    # log does not take 0 or negative values, so let's be\n    # careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log(df[var])\n        df[var].hist(bins=30)\n        plt.ylabel('Number of houses')\n        plt.xlabel(var)\n        plt.title(var)\n        plt.show()\n\n\nfor var in cont_vars:\n    analyse_transformed_continuous(data, var)","f5c53614":"# let's explore the relationship between the house price and\n# the transformed variables with more detail:\n\n\ndef transform_analyse_continuous(df, var):\n    df = df.copy()\n\n    # log does not take negative values, so let's be careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        # log transform the variable\n        df[var] = np.log(df[var])\n        \n        # log transform the target (remember it was also skewed)\n        df['SalePrice'] = np.log(df['SalePrice'])\n        \n        # plot\n        plt.scatter(df[var], df['SalePrice'])\n        plt.ylabel('SalePrice')\n        plt.xlabel(var)\n        plt.show()\n\n\nfor var in cont_vars:\n    if var != 'SalePrice':\n        transform_analyse_continuous(data, var)","07ed3501":"# let's make boxplots to visualise outliers in the continuous variables\n\n\ndef find_outliers(df, var):\n    df = df.copy()\n\n    # log does not take negative values, so let's be\n    # careful and skip those variables\n    if any(data[var] <= 0):\n        pass\n    else:\n        df[var] = np.log(df[var])\n        df.boxplot(column=var)\n        plt.title(var)\n        plt.ylabel(var)\n        plt.show()\n\n\nfor var in cont_vars:\n    find_outliers(data, var)","4e2b3ac7":"# capture categorical variables in a list\ncat_vars = [var for var in data.columns if data[var].dtypes == 'O']\n\nprint('Number of categorical variables: ', len(cat_vars))","c1dd6096":"# let's visualise the values of the categorical variables\ndata[cat_vars].head()","5df93f63":"data[cat_vars].nunique()","179ca62f":"def analyse_rare_labels(df, var, rare_perc):\n    df = df.copy()\n\n    # determine the % of observations per category\n    tmp = df.groupby(var)['SalePrice'].count() \/ len(df)\n\n    # return categories that are rare\n    return tmp[tmp < rare_perc]\n\n# print categories that are present in less than\n# 1 % of the observations\n\n\nfor var in cat_vars:\n    print(analyse_rare_labels(data, var, 0.01))\n    print()","2c4e64f4":"for var in cat_vars:\n    # we can re-use the function to determine median\n    # sale price, that we created for discrete variables\n\n    analyse_discrete(data, var)","8abb1431":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","fa7b03b3":"# load dataset\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(data.shape)\ndata.head()","64a35dce":"# Let's separate into train and test set\n# Remember to set the seed (random_state for this sklearn function)\n\nX_train, X_test, y_train, y_test = train_test_split(data,\n                                                    data['SalePrice'],\n                                                    test_size=0.1,\n                                                    # we are setting the seed here:\n                                                    random_state=0)  \n\nX_train.shape, X_test.shape","11c104ec":"# make a list of the categorical variables that contain missing values\n\nvars_with_na = [\n    var for var in data.columns\n    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes == 'O'\n]\n\n# print percentage of missing values per variable\nX_train[vars_with_na].isnull().mean()","64f6696d":"# replace missing values with new label: \"Missing\"\n\nX_train[vars_with_na] = X_train[vars_with_na].fillna('Missing')\nX_test[vars_with_na] = X_test[vars_with_na].fillna('Missing')","0c90442a":"# check that we have no missing information in the engineered variables\nX_train[vars_with_na].isnull().sum()","4a532fb4":"# check that test set does not contain null values in the engineered variables\n[var for var in vars_with_na if X_test[var].isnull().sum() > 0]","1e01723d":"# make a list with the numerical variables that contain missing values\nvars_with_na = [\n    var for var in data.columns\n    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes != 'O'\n]\n\n# print percentage of missing values per variable\nX_train[vars_with_na].isnull().mean()","b79ea0c2":"# replace engineer missing values as we described above\n\nfor var in vars_with_na:\n\n    # calculate the mode using the train set\n    mode_val = X_train[var].mode()[0]\n\n    # add binary missing indicator (in train and test)\n    X_train[var+'_na'] = np.where(X_train[var].isnull(), 1, 0)\n    X_test[var+'_na'] = np.where(X_test[var].isnull(), 1, 0)\n\n    # replace missing values by the mode\n    # (in train and test)\n    X_train[var] = X_train[var].fillna(mode_val)\n    X_test[var] = X_test[var].fillna(mode_val)\n\n# check that we have no more missing values in the engineered variables\nX_train[vars_with_na].isnull().sum()","2b3b347c":"# check that test set does not contain null values in the engineered variables\n\n[vr for var in vars_with_na if X_test[var].isnull().sum() > 0]","961c18d7":"# check the binary missing indicator variables\n\nX_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()","576d2852":"def elapsed_years(df, var):\n    # capture difference between the year variable\n    # and the year in which the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df","0e430813":"for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    X_train = elapsed_years(X_train, var)\n    X_test = elapsed_years(X_test, var)","f69397c1":"for var in ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']:\n    X_train[var] = np.log(X_train[var])\n    X_test[var] = np.log(X_test[var])","c13aa844":"# check that test set does not contain null values in the engineered variables\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF',\n                 'GrLivArea', 'SalePrice'] if X_test[var].isnull().sum() > 0]","c1f8474f":"# same for train set\n[var for var in ['LotFrontage', 'LotArea', '1stFlrSF',\n                 'GrLivArea', 'SalePrice'] if X_train[var].isnull().sum() > 0]","6fe2f932":"# let's capture the categorical variables in a list\n\ncat_vars = [var for var in X_train.columns if X_train[var].dtype == 'O']","0b385217":"def find_frequent_labels(df, var, rare_perc):\n    \n    # function finds the labels that are shared by more than\n    # a certain % of the houses in the dataset\n\n    df = df.copy()\n\n    tmp = df.groupby(var)['SalePrice'].count() \/ len(df)\n\n    return tmp[tmp > rare_perc].index\n\n\nfor var in cat_vars:\n    \n    # find the frequent categories\n    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n    \n    # replace rare categories by the string \"Rare\"\n    X_train[var] = np.where(X_train[var].isin(\n        frequent_ls), X_train[var], 'Rare')\n    \n    X_test[var] = np.where(X_test[var].isin(\n        frequent_ls), X_test[var], 'Rare')","6c997a01":"# this function will assign discrete values to the strings of the variables,\n# so that the smaller value corresponds to the category that shows the smaller\n# mean house sale price\n\n\ndef replace_categories(train, test, var, target):\n\n    # order the categories in a variable from that with the lowest\n    # house sale price, to that with the highest\n    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n\n    # create a dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n\n    # use the dictionary to replace the categorical strings by integers\n    train[var] = train[var].map(ordinal_label)\n    test[var] = test[var].map(ordinal_label)","5e88bd67":"for var in cat_vars:\n    replace_categories(X_train, X_test, var, 'SalePrice')","eaa08306":"# check absence of na in the train set\n[var for var in X_train.columns if X_train[var].isnull().sum() > 0]","b7091b3f":"# check absence of na in the test set\n[var for var in X_test.columns if X_test[var].isnull().sum() > 0]","c9d3f285":"# let me show you what I mean by monotonic relationship\n# between labels and target\n\ndef analyse_vars(df, var):\n    \n    # function plots median house sale price per encoded\n    # category\n    \n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar()\n    plt.title(var)\n    plt.ylabel('SalePrice')\n    plt.show()\n    \nfor var in cat_vars:\n    analyse_vars(X_train, var)","02ddc8ee":"# capture all variables in a list\n# except the target and the ID\n\ntrain_vars = [var for var in X_train.columns if var not in ['Id', 'SalePrice']]\n\n# count number of variables\nlen(train_vars)","c9ac8adf":"# create scaler\nscaler = MinMaxScaler()\n\n#  fit  the scaler to the train set\nscaler.fit(X_train[train_vars]) \n\n# transform the train and test set\nX_train[train_vars] = scaler.transform(X_train[train_vars])\n\nX_test[train_vars] = scaler.transform(X_test[train_vars])","3b3be409":"X_train.head()","c07b9ac0":"# let's now save the train and test sets for the next notebook!\n\nX_train.to_csv('xtrain.csv', index=False)\nX_test.to_csv('xtest.csv', index=False)","eb8b3fce":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n\n# to build the models\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","03412623":"# load the train and test set with the engineered variables\n\n# we built and saved these datasets in the previous lecture.\n# If you haven't done so, go ahead and check the previous notebook\n# to find out how to create these datasets\n\nX_train = pd.read_csv('xtrain.csv')\nX_test = pd.read_csv('xtest.csv')\n\nX_train.head()","bad74c71":"# capture the target (remember that the target is log transformed)\ny_train = X_train['SalePrice']\ny_test = X_test['SalePrice']\n\n# drop unnecessary variables from our training and testing sets\nX_train.drop(['Id', 'SalePrice'], axis=1, inplace=True)\nX_test.drop(['Id', 'SalePrice'], axis=1, inplace=True)","720d7c16":"# We will do the model fitting and feature selection\n# altogether in a few lines of code\n\n# first, we specify the Lasso Regression model, and we\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then we use the selectFromModel object from sklearn, which\n# will select automatically the features which coefficients are non-zero\n\n# remember to set the seed, the random state in this function\nsel_ = SelectFromModel(Lasso(alpha=0.005, random_state=0))\n\n# train Lasso model and select features\nsel_.fit(X_train, y_train)","97aa2f0e":"# let's visualise those features that were selected.\n# (selected features marked with True)\n\nsel_.get_support()","f0474974":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feats = X_train.columns[(sel_.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feats)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(sel_.estimator_.coef_ == 0)))","2a788c4a":"# print the selected features\nselected_feats","9fc400db":"# this is an alternative way of identifying the selected features\n# based on the non-zero regularisation coefficients:\n\nselected_feats = X_train.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\n\nselected_feats","3386c42d":"pd.Series(selected_feats).to_csv('selected_features.csv', index=False)","f9f1ed68":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n\n# to build the model\nfrom sklearn.linear_model import Lasso\n\n# to evaluate the model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","9fe25b70":"# load the train and test set with the engineered variables\n\n# we built and saved these datasets in a previous notebook.\n# If you haven't done so, go ahead and check the previous notebooks (step 2)\n# to find out how to create these datasets\n\nX_train = pd.read_csv('xtrain.csv')\nX_test = pd.read_csv('xtest.csv')\n\nX_train.head()","3a2cac58":"# capture the target (remember that is log transformed)\n\ny_train = X_train['SalePrice']\ny_test = X_test['SalePrice']","dce39db0":"# load the pre-selected features\n# ==============================\n\n# we selected the features in the previous notebook (step 3)\n\n# if you haven't done so, go ahead and visit the previous notebook\n# to find out how to select the features\n\nfeatures = pd.read_csv('selected_features.csv')\nfeatures = features['0'].to_list() \n\n# We will add one additional feature to the ones we selected in the\n# previous notebook: LotFrontage\n\n# why?\n#=====\n\n# because it needs key feature engineering steps that we want to\n# discuss further during the deployment part of the course. \n\nfeatures = features + ['LotFrontage'] \n\n# display final feature set\nfeatures","01afb5fc":"# reduce the train and test set to the selected features\n\nX_train = X_train[features]\nX_test = X_test[features]","926f1fad":"# set up the model\n# remember to set the random_state \/ seed\n\nlin_model = Lasso(alpha=0.005, random_state=0)\n\n# train the model\n\nlin_model.fit(X_train, y_train)","2fbf3cca":"# evaluate the model:\n# ====================\n\n# remember that we log transformed the output (SalePrice)\n# in our feature engineering notebook (step 2).\n\n# In order to get the true performance of the Lasso\n# we need to transform both the target and the predictions\n# back to the original house prices values.\n\n# We will evaluate performance using the mean squared error and\n# the root of the mean squared error and r2\n\n# make predictions for train set\npred = lin_model.predict(X_train)\n\n# determine mse and rmse\nprint('train mse: {}'.format(int(\n    mean_squared_error(np.exp(y_train), np.exp(pred)))))\nprint('train rmse: {}'.format(int(\n    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\nprint('train r2: {}'.format(\n    r2_score(np.exp(y_train), np.exp(pred))))\nprint()\n\n# make predictions for test set\npred = lin_model.predict(X_test)\n\n# determine mse and rmse\nprint('test mse: {}'.format(int(\n    mean_squared_error(np.exp(y_test), np.exp(pred)))))\nprint('test rmse: {}'.format(int(\n    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\nprint('test r2: {}'.format(\n    r2_score(np.exp(y_test), np.exp(pred))))\nprint()\n\nprint('Average house price: ', int(np.exp(y_train).median()))","077086a6":"# let's evaluate our predictions respect to the real sale price\nplt.scatter(y_test, lin_model.predict(X_test))\nplt.xlabel('True House Price')\nplt.ylabel('Predicted House Price')\nplt.title('Evaluation of Lasso Predictions')","6df6d90c":"# let's evaluate the distribution of the errors: \n# they should be fairly normally distributed\n\nerrors = y_test - lin_model.predict(X_test)\nerrors.hist(bins=30)","a26817eb":"# Finally, just for fun, let's look at the feature importance\n\nimportance = pd.Series(np.abs(lin_model.coef_.ravel()))\nimportance.index = features\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(18,6))\nplt.ylabel('Lasso Coefficients')\nplt.title('Feature Importance')","494b809a":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to build the models\nfrom sklearn.linear_model import Lasso\n\n# to evaluate the models\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\n# to persist the model and the scaler\nimport joblib\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","e11e56c4":"# load dataset\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(data.shape)\ndata.head()","fb14ff51":"X_train, X_test, y_train, y_test = train_test_split(\n    data,\n    data['SalePrice'],\n    test_size=0.1,\n    # we are setting the seed here\n    random_state=0)\n\nX_train.shape, X_test.shape","306412e2":"X_train.head()","fd049b0f":"# load selected features\nfeatures = pd.read_csv('selected_features.csv')\n\n# Added the extra feature, LotFrontage\nfeatures = features['0'].to_list() + ['LotFrontage']\n\nprint('Number of features: ', len(features))","17045e0a":"# make a list of the categorical variables that contain missing values\n\nvars_with_na = [\n    var for var in features\n    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes == 'O'\n]\n\n# display categorical variables that we will engineer:\nvars_with_na","71b497d1":"# I bring forward the code used in the feature engineering notebook:\n# (step 2)\n\nX_train[vars_with_na] = X_train[vars_with_na].fillna('Missing')\nX_test[vars_with_na] = X_test[vars_with_na].fillna('Missing')\n\n# check that we have no missing information in the engineered variables\nX_train[vars_with_na].isnull().sum()","8df2166c":"# make a list of the numerical variables that contain missing values:\n\nvars_with_na = [\n    var for var in features\n    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes != 'O'\n]\n\n# display numerical variables with NA\nvars_with_na","a635bd76":"# I bring forward the code used in the feature engineering notebook\n# with minor adjustments (step 2):\n\nvar = 'LotFrontage'\n\n# calculate the mode\nmode_val = X_train[var].mode()[0]\nprint('mode of LotFrontage: {}'.format(mode_val))\n\n# replace missing values by the mode\n# (in train and test)\nX_train[var] = X_train[var].fillna(mode_val)\nX_test[var] = X_test[var].fillna(mode_val)","009af308":"# create the temporal var \"elapsed years\"\n\n# I bring this bit of code forward from the notebook on feature\n# engineering (step 2)\n\ndef elapsed_years(df, var):\n    # capture difference between year variable\n    # and year in which the house was sold\n    \n    df[var] = df['YrSold'] - df[var]\n    \n    return df","22a8aa1a":"X_train = elapsed_years(X_train, 'YearRemodAdd')\nX_test = elapsed_years(X_test, 'YearRemodAdd')","ca15dcdc":"# we apply the logarithmic function to the variables that\n# were selected (and the target):\n\nfor var in ['LotFrontage', '1stFlrSF', 'GrLivArea', 'SalePrice']:\n    X_train[var] = np.log(X_train[var])\n    X_test[var] = np.log(X_test[var])","8e428572":"# let's capture the categorical variables first\n\ncat_vars = [var for var in features if X_train[var].dtype == 'O']\n\ncat_vars","03e91c49":"# bringing thise from the notebook on feature engineering (step 2):\n\ndef find_frequent_labels(df, var, rare_perc):\n    \n    # function finds the labels that are shared by more than\n    # a certain % of the houses in the dataset\n\n    df = df.copy()\n\n    tmp = df.groupby(var)['SalePrice'].count() \/ len(df)\n\n    return tmp[tmp > rare_perc].index\n\n\nfor var in cat_vars:\n    \n    # find the frequent categories\n    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n    print(var)\n    print(frequent_ls)\n    print()\n    \n    # replace rare categories by the string \"Rare\"\n    X_train[var] = np.where(X_train[var].isin(\n        frequent_ls), X_train[var], 'Rare')\n    \n    X_test[var] = np.where(X_test[var].isin(\n        frequent_ls), X_test[var], 'Rare')","284f5ff5":"# this function will assign discrete values to the strings of the variables,\n# so that the smaller value corresponds to the category that shows the smaller\n# mean house sale price\n\n\ndef replace_categories(train, test, var, target):\n\n    # order the categories in a variable from that with the lowest\n    # house sale price, to that with the highest\n    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n\n    # create a dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n\n    # use the dictionary to replace the categorical strings by integers\n    train[var] = train[var].map(ordinal_label)\n    test[var] = test[var].map(ordinal_label)\n    \n    print(var)\n    print(ordinal_label)\n    print()","f23d90ba":"for var in cat_vars:\n    replace_categories(X_train, X_test, var, 'SalePrice')","41e5eebe":"# check absence of na\n[var for var in features if X_train[var].isnull().sum() > 0]","2c8dd4d7":"# check absence of na\n[var for var in features if X_test[var].isnull().sum() > 0]","78b74329":"# capture the target\ny_train = X_train['SalePrice']\ny_test = X_test['SalePrice']","7bb873dd":"# set up scaler\nscaler = MinMaxScaler()\n\n# train scaler\nscaler.fit(X_train[features])","64c86117":"# explore maximum values of variables\nscaler.data_max_","141087af":"# explore minimum values of variables\nscaler.data_min_","fde1f5dd":"# transform the train and test set, and add on the Id and SalePrice variables\nX_train = scaler.transform(X_train[features])\nX_test = scaler.transform(X_test[features])","d0af9c42":"# set up the model\n# remember to set the random_state \/ seed\n\nlin_model = Lasso(alpha=0.005, random_state=0)\n\n# train the model\nlin_model.fit(X_train, y_train)\n\n# we persist the model for future use\njoblib.dump(lin_model, 'lasso_regression.pkl')","17496219":"# evaluate the model:\n# ====================\n\n# remember that we log transformed the output (SalePrice)\n# in our feature engineering notebook (step 2).\n\n# In order to get the true performance of the Lasso\n# we need to transform both the target and the predictions\n# back to the original house prices values.\n\n# We will evaluate performance using the mean squared error and\n# the root of the mean squared error and r2\n\n# make predictions for train set\npred = lin_model.predict(X_train)\n\n# determine mse and rmse\nprint('train mse: {}'.format(int(\n    mean_squared_error(np.exp(y_train), np.exp(pred)))))\nprint('train rmse: {}'.format(int(\n    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\nprint('train r2: {}'.format(\n    r2_score(np.exp(y_train), np.exp(pred))))\nprint()\n\n# make predictions for test set\npred = lin_model.predict(X_test)\n\n# determine mse and rmse\nprint('test mse: {}'.format(int(\n    mean_squared_error(np.exp(y_test), np.exp(pred)))))\nprint('test rmse: {}'.format(int(\n    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\nprint('test r2: {}'.format(\n    r2_score(np.exp(y_test), np.exp(pred))))\nprint()\n\nprint('Average house price: ', int(np.exp(y_train).median()))","a64c84f7":"Our dataset contains a few variables with missing values. We need to account for this in our following notebook \/ video, where we will engineer the variables for use in Machine Learning Models.","2e0a7e75":"### Numerical variables\n\nTo engineer missing values in numerical variables, we will:\n\n- add a binary missing value indicator variable\n- and then replace the missing values in the original variable with the mode\n","92e57012":"## Separate dataset into train and test\n\nBefore beginning to engineer our features, it is important to separate our data intro training and testing set. When we engineer features, some techniques learn parameters from data. It is important to learn this parameters only from the train set. This is to avoid over-fitting. \n\n**Separating the data into train and test involves randomness, therefore, we need to set the seed.**","15beffe2":"#### Outliers\n\nExtreme values may affect the performance of a linear model. Let's find out if we have any in our variables.","739cf4d0":"That is all for this notebook. And that is all for this section too.\n\n**In the next section, we will show you how to productionise this code for model deployment**.","3b3109dd":"### Missing values\n\nLet's go ahead and find out which variables of the dataset contain missing values.","83580bfe":"## House Prices dataset: Feature Selection\n\nIn the following cells, we will select a group of variables, the most predictive ones, to build our machine learning model. \n\n### Why do we select variables?\n\n- For production: Fewer variables mean smaller client input requirements (e.g. customers filling out a form on a website or mobile app), and hence less code for error handling. This reduces the chances of introducing bugs.\n\n- For model performance: Fewer variables mean simpler, more interpretable, better generalizing models\n\n\n**We will select variables using the Lasso regression: Lasso has the property of setting the coefficient of non-informative variables to zero. This way we can identify those variables and remove them from our final model.**\n\n\n### Setting the seed\n\nIt is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.\n\nLet's go ahead and load the dataset.","8f9c637e":"### Encoding of categorical variables\n\nNext, we need to transform the strings of the categorical variables into numbers. We will do it so that we capture the monotonic relationship between the label and the target.\n\nTo learn more about how to encode categorical variables visit our course [Feature Engineering for Machine Learning](https:\/\/www.udemy.com\/feature-engineering-for-machine-learning\/?couponCode=UDEMY2018) in Udemy.","673578d0":"## House Prices dataset: Feature Engineering\n\nIn the following cells, we will engineer \/ pre-process the variables of the House Price Dataset from Kaggle. We will engineer the variables so that we tackle:\n\n1. Missing values\n2. Temporal variables\n3. Non-Gaussian distributed variables\n4. Categorical variables: remove rare labels\n5. Categorical variables: convert strings to numbers\n5. Standarise the values of the variables to the same range\n\n### Setting the seed\n\nIt is important to note that we are engineering variables and pre-processing data with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.\n\nLet's go ahead and load the dataset.","41a4e368":"## Categorical variables\n\n### Group rare labels","a20f895c":"## Machine Learning Model Building Pipeline: Machine Learning Model Build\n\nIn the following videos, we will take you through a practical example of each one of the steps in the Machine Learning model building pipeline, which we described in the previous lectures. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n\n**This is the notebook for step 4: Building the Final Machine Learning Model**\n\nWe will use the house price dataset available on [Kaggle.com](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). See below for more details.\n\n===================================================================================================\n\n## Predicting Sale Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses. \n\n### Why is this important? \n\nPredicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or under-estimated.\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).\n\n### How do I download the dataset?\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Rename the file as 'houseprice.csv' and save it to a directory of your choice.\n\n**Note the following:**\n-  You need to be logged in to Kaggle in order to download the datasets.\n-  You need to accept the terms and conditions of the competition to download the dataset\n-  If you save the file to the same directory where you saved this jupyter notebook, then you can run the code as it is written here.\n\n====================================================================================================","8f966eb0":"## Categorical variables\n\n### Removing rare labels\n\nFirst, we will group those categories within variables that are present in less than 1% of the observations. That is, all values of categorical variables that are shared by less than 1% of houses, well be replaced by the string \"Rare\".\n\nTo learn more about how to handle categorical variables visit our course [Feature Engineering for Machine Learning](https:\/\/www.udemy.com\/feature-engineering-for-machine-learning\/?couponCode=UDEMY2018) in Udemy.","23cace71":"There has been a drop in the value of the houses. That is unusual, in real life, house prices typically go up as years go by.\n\n\nLet's go ahead and explore whether there is a relationship between the year variables and SalePrice. For this, we will capture the elapsed years between the Year variables and the year in which the house was sold:","a541f5ac":"As expected, the values are years.\n\nWe can explore the evolution of the sale price with the years in which the house was sold:","025ecab4":"The house price dataset contains 1460 rows, i.e., houses, and 81 columns, i.e., variables. \n\n**We will analyse the dataset to identify:**\n\n1. Missing values\n2. Numerical variables\n3. Distribution of the numerical variables\n4. Outliers\n5. Categorical variables\n6. Cardinality of the categorical variables\n7. Potential relationship between the variables and the target: SalePrice","fa96d150":"## House Prices dataset: Data Analysis\n\nIn the following cells, we will analyse the variables of the House Price Dataset from Kaggle. We will take you through the different aspects of the analysis of the variables, and introduce you to the meaning of each of the variables in the dataset as well. If you want to know more about this dataset, visit [Kaggle.com](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data).\n\nLet's go ahead and load the dataset.","afe1a854":"## Temporal variables\n\n### Capture elapsed time\n\nWe learned in the previous Jupyter notebook, that there are 4 variables that refer to the years in which the house or the garage were built or remodeled. We will capture the time elapsed between those variables and the year in which the house was sold:","00efaaf0":"We get a better spread of the values for most variables when we use the logarithmic transformation. This engineering step will most likely add performance value to our final model.","f562b20e":"**Disclaimer:**\n\nThe data exploration shown in this notebook by no means wants to be an exhaustive data exploration. There is certainly more to be done to understand the nature of this data and the relationship of these variables with the target, SalePrice.\n\nHowever, we hope that through this notebook we gave you both a flavour of what data analysis looks like, and set the bases for the coming steps in the machine learning model building pipeline. Through data exploration, we decide which feature engineering techniques we will apply to our variables.","77c1055b":"The majority of the continuous variables seem to contain outliers. Outliers tend to affect the performance of linear model. So it is worth spending some time understanding if removing outliers will add performance value to our  final machine learning model.\n\nThe purpose of this course is however to teach you how to put your models in production. Therefore, we will not spend more time looking at how best to remove outliers, and we will rather deploy a simpler model.\n\nHowever, if you want to learn more about the value of removing outliers, visit our course [Feature Engineering for Machine Learning](https:\/\/www.udemy.com\/feature-engineering-for-machine-learning\/?couponCode=UDEMY2018).\n\nThe same is true for variable transformation. There are multiple ways to improve the spread of the variable over a wider range of values. You can learn more about it in our course [Feature Engineering for Machine Learning](https:\/\/www.udemy.com\/feature-engineering-for-machine-learning\/?couponCode=UDEMY2018).","e03c6a0e":"## Machine Learning Model Building Pipeline: Data Analysis\n\nIn the following videos, we will take you through a practical example of each one of the steps in the Machine Learning model building pipeline, which we described in the previous lectures. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n\n**This is the notebook for step 1: Data Analysis**\n\nWe will use the house price dataset available on [Kaggle.com](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). See below for more details.\n\n===================================================================================================\n\n## Predicting Sale Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses. \n\n### Why is this important? \n\nPredicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or under-estimated.\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).\n\n### How do I download the dataset?\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Rename the file as 'houseprice.csv' and save it to a directory of your choice.\n\n**Note the following:**\n-  You need to be logged in to Kaggle in order to download the datasets.\n-  You need to accept the terms and conditions of the competition to download the dataset\n-  If you save the file to the same directory where you saved this jupyter notebook, then you can run the code as it is written here.\n\n====================================================================================================","d081e46f":"## Load data\n\nWe need the training data to train our model in the production environment. ","9957bfcc":"### Numerical variable transformation","3b24357e":"## Temporal variables\n\nOne of our temporal variables was selected to be used in the final model: 'YearRemodAdd'\n\nSo we need to deploy the bit of code that creates it.","ddeba84a":"The average Sale Price in houses where the information is missing, differs from the average Sale Price in houses where information exists. \n\nWe will capture this information when we engineer the variables in our next lecture \/ video.","7329abe9":"Clearly, the categories give information on the SalePrice, as different categories show different median sale prices.\n\nIn the next video, we will transform these strings \/ labels into numbers, so that we capture this information and transform it into a monotonic relationship between the category and the house price.","690b14bc":"We see that there is a tendency to a decrease in price, with older features. In other words, the longer the time between the house was built or remodeled and sale date, the lower the sale Price. \n\nWhich makes sense, cause this means that the house will have an older look, and potentially needs repairs.","69a71f0f":"The distribution of the errors follows quite closely a gaussian distribution. That suggests that our model is doing a good job as well.","d62b1284":"From the above view of the dataset, we notice the variable Id, which is an indicator of the house. We will not use this variable to make our predictions, as there is one different value of the variable per each row, i.e., each house in the dataset. See below:","611c4031":"### Numerical variables\n\nLet's go ahead and find out what numerical variables we have in the dataset","15f0401d":"### Encoding of categorical variables\n","d4c49b17":"We can see that our model is doing a pretty good job at estimating house prices.","8bc8fbdd":"## Train the Linear Regression: Lasso","e28e93c0":"### Feature Selection\n\nLet's go ahead and select a subset of the most predictive features. There is an element of randomness in the Lasso regression, so remember to set the seed.","de7a3769":"That is all for this notebook. We hope you enjoyed it and see you in the next one!","a2cae301":"#### Discrete variables\n\nLet's go ahead and find which variables are discrete, i.e., show a finite number of values","c373dec8":"The variables are not normally distributed, including the target variable 'SalePrice'. \n\nSometimes, transforming the variables to improve the value spread, improves the model performance. Thus, we will transform our variables in the next lecture \/ video, during our feature engineering step.\n\nLet's evaluate if a logarithmic transformation of the variables returns values that follow a normal distribution:","da500c4c":"#### Continuous variables\n\nLet's go ahead and find the distribution of the continuous variables. We will consider continuous variables to all those that are not temporal or discrete variables in our dataset.","9411aeac":"### Regularised linear regression: Lasso\n\nRemember to set the seed.","b97d19cf":"## Separate dataset into train and test","2d662c02":"## House Prices dataset: Model building\n\nIn the following cells, we will finally build our machine learning model, utilising the engineered data and the pre-selected features. \n\n\n### Setting the seed\n\nIt is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.\n\nLet's go ahead and load the dataset.","0208f956":"## Feature Scaling\n\nFor use in linear models, features need to be either scaled or normalised. In the next section, I will scale features to the minimum and maximum values:","2b2ea4ae":"That is all for this lecture \/ notebook. I hope you enjoyed it, and see you in the next one!","6b6b028c":"There tend to be a relationship between the variables values and the SalePrice, but this relationship is not always monotonic. \n\nFor example, for OverallQual, there is a monotonic relationship: the higher the quality, the higher the SalePrice.  \n\nHowever, for OverallCond, the relationship is not monotonic. Clearly, some Condition grades, like 5, correlate with higher sale prices, but higher values do not necessarily do so. We need to be careful on how we engineer these variables to extract maximum value for a linear model.\n\nThere are ways to re-arrange the order of the discrete values of a variable, to create a monotonic relationship between the variable and the target. However, for the purpose of this course, we will not do that, to keep feature engineering simple. If you want to learn more about how to engineer features, visit our course [Feature Engineering for Machine Learning](https:\/\/www.udemy.com\/feature-engineering-for-machine-learning\/?couponCode=UDEMY2018) in Udemy.","6f0bb0c9":"## Selected features","da3e30a1":"## Numerical variable transformation\n\nIn the previous Jupyter notebook, we observed that the numerical variables are not normally distributed.\n\nWe will log transform the positive numerical variables in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models. ","bccf4ad0":"#### Temporal variables\n\nWe have 4 year variables in the dataset:\n\n- YearBuilt: year in which the house was built\n- YearRemodAdd: year in which the house was remodeled\n- GarageYrBlt: year in which a garage was built\n- YrSold: year in which the house was sold\n\nWe generally don't use date variables in their raw format. Instead, we extract information from them. For example, we can capture the difference in years between the year the house was built and the year the house was sold.","de13ab53":"#### Relationship between values being missing and Sale Price\n\nLet's evaluate the price of the house in those observations where the information is missing, for each variable.","e178922c":"Some of the categorical variables show multiple labels that are present in less than 1% of the houses. We will engineer these variables in our next video. Labels that are under-represented in the dataset tend to cause over-fitting of machine learning models. That is why we want to remove them.\n\nFinally, we want to explore the relationship between the categories of the different variables and the house sale price:","838311cb":"### Identify the selected variables","32d2d934":"## Missing values\n\n### Categorical variables\nFor categorical variables, we will replace missing values with the string \"missing\".","d4cc93f5":"### Feature Scaling\n\nFor use in linear models, features need to be either scaled or normalised. In the next section, I will scale features between the min and max values:","d2bfc984":"### Feature importance","822bcab5":"From the previous plots, we observe some monotonic associations between SalePrice and the variables to which we applied the log transformation, for example 'GrLivArea'.","190ded4f":"## Machine Learning Model Building Pipeline: Feature Engineering\n\nIn the following videos, we will take you through a practical example of each one of the steps in the Machine Learning model building pipeline, which we described in the previous lectures. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n\n**This is the notebook for step 2: Feature Engineering**\n\nWe will use the house price dataset available on [Kaggle.com](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). See below for more details.\n\n===================================================================================================\n\n## Predicting Sale Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses. \n\n### Why is this important? \n\nPredicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or under-estimated.\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).\n\n### How do I download the dataset?\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Rename the file as 'houseprice.csv' and save it to a directory of your choice.\n\n**Note the following:**\n-  You need to be logged in to Kaggle in order to download the datasets.\n-  You need to accept the terms and conditions of the competition to download the dataset\n-  If you save the file to the same directory where you saved this jupyter notebook, then you can run the code as it is written here.\n\n====================================================================================================","381e573a":"That is all for this notebook. In the next video, we will go ahead and build the final model using the selected features. See you then!","adbe361c":"## Machine Learning Model Building Pipeline: Feature Selection\n\nIn the following videos, we will take you through a practical example of each one of the steps in the Machine Learning model building pipeline, which we described in the previous lectures. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n\n**This is the notebook for step 3: Feature Selection**\n\n\nWe will use the house price dataset available on [Kaggle.com](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). See below for more details.\n\n===================================================================================================\n\n## Predicting Sale Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses. \n\n### Why is this important? \n\nPredicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or under-estimated.\n\n### What is the objective of the machine learning model?\n\nWe aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance using the mean squared error (mse) and the root squared of the mean squared error (rmse).\n\n### How do I download the dataset?\n\nTo download the House Price dataset go this website:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nScroll down to the bottom of the page, and click on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Rename the file as 'houseprice.csv' and save it to a directory of your choice.\n\n**Note the following:**\n-  You need to be logged in to Kaggle in order to download the datasets.\n-  You need to accept the terms and conditions of the competition to download the dataset\n-  If you save the file to the same directory where you saved this jupyter notebook, then you can run the code as it is written here.\n\n====================================================================================================","4ac329f0":"These discrete variables tend to be qualifications or grading scales, or refer to the number of rooms, or units.\n\nLet's go ahead and analyse their contribution to the house price.","1419ffde":"## Engineer missing values\n\n### Categorical variables\n\nFor categorical variables, we will replace missing values with the string \"missing\".","63859873":"### Setting the seed\n\nIt is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.\n\nThis is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.\n\nLet's go ahead and load the dataset.","1d4c35df":"Note that we have much less categorical variables with missing values than in our original dataset. But we still use categorical variables with NA for the final model, so we need to include this piece of feature engineering logic in the deployment pipeline. ","735c77ad":"### Categorical variables\n\nLet's go ahead and analyse the categorical variables present in the dataset.","6d58686d":"And that is all! Now we have our entire pipeline ready for deployment. \n\nIn the next video, we will summarise which steps from the pipeline we will deploy to production.","59a5890a":"That concludes the feature engineering section for this dataset.\n\n**Remember: the aim of this course is to show you how to put models in production. We deliberately kept the feature engineering pipeline, yet included many of the traditional engineering steps, to give you a full flavour of building and deploying a machine learning model pipeline** as we will see in the coming sections of the course.","cfc7ad20":"#### Number of labels: cardinality\n\nLet's evaluate how many different categories are present in each of the variables.","61919b97":"The monotonic relationship is particularly clear for the variables MSZoning, Neighborhood, and ExterQual. Note how, the higher the integer that now represents the category, the higher the mean house sale price.\n\n(remember that the target is log-transformed, that is why the differences seem so small).","c1087b34":"All the categorical variables show low cardinality, this means that they have only few different labels. That is good as we won't need to tackle cardinality during our feature engineering lecture.\n\n#### Rare labels:\n\nLet's go ahead and investigate now if there are labels that are present only in a small number of houses:","c9c8b3ba":"## Machine Learning Pipeline: Wrapping up for Deployment\n\n\nIn the previous notebooks, we worked through the typical Machine Learning pipeline steps to build a regression model that allows us to predict house prices. Briefly, we transformed variables in the dataset to make them suitable for use in a Regression model, then we selected the most predictive variables and finally we trained our model.\n\nNow, we want to deploy our model. We want to create an API, which we can call with new data, with new characteristics about houses, to get an estimate of the SalePrice. In order to do so, we need to write code in a very specific way. We will show you how to write production code in the next sections.\n\nHere, we will summarise the key pieces of code, that we need to take forward for this particular project, to put our model in production.\n\nLet's go ahead and get started.","60d98037":"### Numerical variables\n\nTo engineer missing values in numerical variables, we will:\n\n- add a binary missing value indicator variable\n- and then replace the missing values in the original variable with the mode\n"}}