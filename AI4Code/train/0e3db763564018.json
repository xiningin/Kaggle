{"cell_type":{"c2473a5b":"code","4d092763":"code","12428b05":"code","742424cc":"code","ab2f036e":"code","2e33d365":"code","18dc6a7e":"code","aa0bada4":"code","5e379171":"code","31619d50":"code","4b6ebfb2":"code","23f7f033":"code","3e13e04f":"code","40b075e3":"code","5b7c5bc4":"code","204c8f06":"code","8e4de904":"code","040e86dc":"code","ee2b5ad5":"code","3ae21e1b":"code","c2fd02c4":"code","d68c0200":"code","02fcde3d":"code","b73ad324":"code","40ea9bd4":"code","56d21e60":"code","2b01c8ab":"code","9531e427":"code","afd940c2":"code","a0855e1e":"code","c940768f":"markdown","a284e0d3":"markdown","8bd96cc7":"markdown","19514bbe":"markdown","c4dbe7c0":"markdown","195288cc":"markdown","7d95f475":"markdown","b76e2da6":"markdown","67f54fcf":"markdown","2a1a6635":"markdown","c2ff05ae":"markdown","2173834d":"markdown","2c17a489":"markdown","37aad35a":"markdown","dd2143c3":"markdown","31aac7b4":"markdown","d74b480d":"markdown","91f67c32":"markdown","4b7569fa":"markdown","fac49268":"markdown","a82162a8":"markdown","bf2d3cb4":"markdown","78fcb4ef":"markdown","8de4e82b":"markdown","9e975207":"markdown","499a4b6e":"markdown","02bff5da":"markdown","b935f4c0":"markdown","718cb76b":"markdown","01e56ab7":"markdown","4d7f6e7e":"markdown","8caf9fc5":"markdown"},"source":{"c2473a5b":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4d092763":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#take a look at the training data\ntrain.describe(include=\"all\")","12428b05":"# save PassengerId for a future output \nids = test['PassengerId']","742424cc":"#get a list of the features within the dataset\nprint(train.columns)","ab2f036e":"#see a sample of the dataset to get an idea of the variables\ntrain.sample(5)","2e33d365":"# categorial features\nCat_Features = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']","18dc6a7e":"#see a summary of the training dataset\ntrain.describe(include = \"all\")","aa0bada4":"#check for any other unusable values\nprint(pd.isnull(train).sum())","5e379171":"fig, saxis = plt.subplots(1, len(Cat_Features),figsize=(len(Cat_Features) * 6,6))\nfor ind, x in enumerate(Cat_Features):\n    print('Survival Correlation by:', x)\n    print(train[[x, \"Survived\"]].groupby(x, as_index=False).mean()) \n    print('-'*10, '\\n')\n    #draw a bar plot of survival by sex\n    sns.barplot(x, y=\"Survived\", data=train, ax = saxis[ind])\n    ","31619d50":"#sort the ages into logical categories\nbins = [0, 2, 12, 17, 60, np.inf]\nlabels = ['baby', 'child', 'teenager', 'adult', 'elderly']\nage_groups = pd.cut(train.Age, bins, labels = labels)\ntrain['AgeGroup'] = age_groups\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","4b6ebfb2":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n#calculate percentages of CabinBool vs. survived\n\nprint('Survival Correlation by: Cabin')\nprint(train[[\"CabinBool\", \"Survived\"]].groupby(\"CabinBool\", as_index=False).mean()) \n\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()","23f7f033":"test.describe(include=\"all\")","3e13e04f":"all_data = pd.concat([train.drop(columns='Survived'), test], ignore_index=True)\nprint(all_data.shape)","40b075e3":"#we'll start off by dropping some features, from which we can't extract a lot of useful information\n\nfor feature in ['PassengerId','Cabin', 'Ticket']:\n    all_data.drop(feature, axis = 1, inplace=True)\nall_data.head()    ","5b7c5bc4":"#complete embarked with mode\nall_data['Embarked'].fillna(all_data['Embarked'].mode()[0], inplace = True)\n\n#complete missing fare with median\nall_data['Fare'].fillna(all_data['Fare'].median(), inplace = True)\n","204c8f06":"#extract a title for each Name \nall_data['Title'] = all_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#pd.crosstab(all_data['Title'], all_data['Sex'])\nall_data['Title'].value_counts()","8e4de904":"frequent_titles = all_data['Title'].value_counts()[:5].index.tolist()\nfrequent_titles","040e86dc":"# keep only the most frequent titles\nall_data['Title'] = all_data['Title'].apply(lambda x: x if x in frequent_titles else 'Other')\n# all_data.head()","ee2b5ad5":"# fill missing age with median age group for each title\nmedian_ages = {}\n# calculate median age for different titles\nfor title in frequent_titles:\n    median_ages[title] = all_data.loc[all_data['Title'] == title]['Age'].median()\nmedian_ages['Other'] =  all_data['Age'].median()\nall_data.loc[all_data['Age'].isnull(), 'Age'] = all_data[all_data['Age'].isnull()]['Title'].map(median_ages)","3ae21e1b":"#sort the ages into logical categories including now missing values\nbins = [0, 2, 12, 17, 60, np.inf]\nlabels = ['baby', 'child', 'teenager', 'adult', 'elderly']\nage_groups = pd.cut(train.Age, bins, labels = labels)\ntrain['AgeGroup'] = age_groups\n","c2fd02c4":"num_bins = 5\nfeature = 'Fare'\nlabels = [feature + '_' + str(i + 1) for i in range(num_bins)]\nall_data['FareBin'] = pd.qcut(all_data['Fare'], num_bins, labels)","d68c0200":"all_data.head()","02fcde3d":"used_features = ['Age', 'Fare','Name']\nfor feature in used_features:\n    all_data.drop(feature, axis = 1, inplace=True)","b73ad324":"all_data.head()","40ea9bd4":"all_data = pd.get_dummies(all_data)\nall_data.head()","56d21e60":"target = train['Survived']\ntrain = all_data.iloc[: len(train)]\ntest = all_data.iloc[len(train):]\ntrain.shape","2b01c8ab":"# This part is essentially borrowed from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n# import sklearn\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\n\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier() \n    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, random_state = 0) # run model 10x with 70\/30 split\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Train Accuracy Mean', 'MLA Validation Accuracy Mean',  'MLA Validation Accuracy STD']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# list of fitted estimators that we are going to use later for voting\nestimators = []\n\n#loop through MLA and save performance to table\nfor row_index, alg in enumerate(MLA):\n\n    #set name and parameters \n    alg_name =  alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = alg_name\n    print(alg.__class__.__name__) \n        \n    #score model with cross validation: \n    #http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n  \n    cv_results = model_selection.cross_validate(alg, train, target, cv=cv_split, return_estimator=True, return_train_score=True)\n    \n    # add all estimators obtained through cross validation\n    estimators += list(cv_results['estimator'])\n    \n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()     \n    MLA_compare.loc[row_index, 'MLA Validation Accuracy Mean'] = cv_results['test_score'].mean()  \n    MLA_compare.loc[row_index, 'MLA Validation Accuracy STD'] = cv_results['test_score'].std()\n           \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Validation Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n","9531e427":"# # Voting Classifier\n# from sklearn.ensemble import VotingClassifier\n\n# estimators_with_names = []\n# _ = [estimators_with_names.append((alg.__class__.__name__, alg)) for alg in  MLA]\n# voting = VotingClassifier(estimators_with_names, voting='hard')\n\n# cv_results = model_selection.cross_validate(voting, train, target, cv=cv_split)\n# print(cv_results['test_score'].mean())\n\n# voting.fit(train, target)\n# predictions = voting.predict(test)  # submission score: 0.7799043062200957","afd940c2":"from sklearn.metrics import accuracy_score\n\ndef hard_voting(estimators, X):\n        # get values\n        Y = np.zeros([X.shape[0], len(estimators)], dtype=int)\n        for i, estimator in enumerate(estimators):\n            Y[:, i] = estimator.predict(X)\n        # apply voting \n        y = np.zeros(X.shape[0], dtype=int)\n        for i in range(X.shape[0]):\n            y[i] = np.argmax(np.bincount(Y[i,:]))\n        return y\n    \ny_predict = hard_voting(estimators, train)\naccuracy_score(target, y_predict)    \n","a0855e1e":"predictions = hard_voting(estimators, test) \n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission_voting_cross_validation.csv', index=False)","c940768f":"### Splitting back Train and Test data","a284e0d3":"People with a recorded Cabin number are, in fact, more likely to survive. (66.6% vs 29.9%)","8bd96cc7":"### Features to drop immediately ","19514bbe":"### Age Feature","c4dbe7c0":"## 1) Import Necessary Libraries\nFirst, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","195288cc":"### Some Predictions:\n* Sex: Females are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: People of higher socioeconomic class are more likely to survive.","7d95f475":"# Titanic Survival Predictions: simple voting based on cross-validation\n\n\n**Update: You might want to look at the imroved version of this notebok:**\n\n[Titanic: simple ensemble voting (Top 3%)](https:\/\/www.kaggle.com\/alexanderossipov\/titanic-simple-ensemble-voting-top-3)\n\n\nIf you like me have recently started your journey\u00a0on Kaggle and you have already done a nice [tutorial](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) by Alexis Cook, then you may wonder how you can improve\u00a0your score.\u00a0\n\nYou will try some simple and not so simple tricks, but you will realise it's not so easy to get a score higher\u00a0than your original 0.77...!\n\nYou might start to look at some great and popular (but a bit intimidating for beginners ;)) notebooks such as\u00a0\n\n[**A Data Science Framework: To Achieve 99% Accuracy**](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\u00a0\u00a0\n\n\nand to your disappointment you will see that the magical number 0.77... is still there.\n\nIn this notebook I tried to keep things simple. My original intention was just to learn more about ensemble approaches and I used first the standard scikit-learn Voting Classifier applied to the models I borrowed from the above notebook. Of course, in this way your score will be again 0.77... ;).\n\nThen a new idea came to my mind: **why not use all estimators that we get during the cross-validation for voting?** For k-fold cross-validation it will increase your ensemble by a factor of k! It turns out that in this way I was finally able to get something higher than 0.77...!\n\nThe nice thing is that you get this result without any hyperparameter tuning or advanced feature engineering. This should be the next steps, which hopefully will improve the result further. \n\n\n### Contents:\n1. Import Necessary Libraries\n2. Read In and Explore the Data\n3. Data Analysis and Visualisation\n4. Cleaning Data\n5. Models\n6. Creating Submission File\n\nFor the first 4 steps I used [**Titanic Survival Predictions (Beginner)**](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner) as a template trying to improve some code there. \n\n\nYour feedback is very welcome! ","b76e2da6":"#### Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps. \n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","67f54fcf":"#### Age Feature","2a1a6635":"We can see that except for the abovementioned missing values, no NaN values exist.","c2ff05ae":"## 6) Creating Submission File\nIt's time to create a submission.csv file to upload to the Kaggle competition!","2173834d":"### Features to drop after transformation\nWe can drop now some original features that we have used already to create new features from them.****","2c17a489":"### Filling missing features using other features","37aad35a":"### Testing Different Models","dd2143c3":"### Creating frequency bins for continuous variables\nUse qcut for continuous variable bins","31aac7b4":"### Cabin Feature\nThe idea here is that people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive. Thanks for the tips, [@salvus82](https:\/\/www.kaggle.com\/salvus82) and [Daniel Ellis](https:\/\/www.kaggle.com\/dellis83)!","d74b480d":"### Looking at the Test Data\nLet's see how our test data looks!","91f67c32":"## 3) Data Analysis and Visualisation\nIt's time to analize our data so we can see whether our predictions were accurate! ","4b7569fa":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages. ","fac49268":"The survival probability deacreases with age. ","a82162a8":"## 4) Cleaning Data\nTime to clean our data to account for missing values and unnecessary information!","bf2d3cb4":"Using the standard VotingClassifier we get the submission score: 0.7799043062200957","78fcb4ef":"### One-hot encoding for categorial features","8de4e82b":"Simpe implementation of hard voting using a list of fitted estimators","9e975207":"* We have a total of 418 passengers.\n* 1 value from the Fare feature is missing.\n* Around 20.5% of the Age feature is missing, we will need to fill that in.","499a4b6e":"### Combining Training and Test data for cleaning and transformation","02bff5da":"## Sources:\n* [**Titanic Survival Predictions (Beginner)**](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)\n* [**A Data Science Framework: To Achieve 99% Accuracy**](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\u00a0\n\nYour feedback is very welcome! ","b935f4c0":"### Filling simple missing features","718cb76b":"## 2) Read in and Explore the Data \nIt's time to read in our training and testing data using `pd.read_csv`, and take a first look at the training data using the `describe()` function.","01e56ab7":"## 5) Models","4d7f6e7e":"> ### Create Voting Classifier from all estimators","8caf9fc5":"As predicted, \n* females have a much higher chance of survival than males. The Sex feature is essential in our predictions\n* people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)\n\nIn general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)\n\nPeople with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children."}}