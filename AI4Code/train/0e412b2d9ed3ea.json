{"cell_type":{"96510edf":"code","054d64b8":"code","1a86c275":"code","d84682ef":"code","aa1fadb2":"code","ab23d385":"code","f1d538e1":"code","353b8984":"code","4dc8deda":"code","015508bc":"code","ce3eb129":"code","9204832c":"code","40a1d8d3":"code","0aadb535":"code","1dd58c1b":"code","bcb5f9f3":"code","499f8600":"markdown"},"source":{"96510edf":"import os\nimport shutil\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","054d64b8":"Path = \"\/kaggle\/input\/world-cuisines\/Dishes\/\"\ncuisines = ['American', 'Chinese', 'European', 'Indian', 'Japanese', 'Korean']","1a86c275":"images = []\nlabel = []\nfor dish in cuisines:\n    food = os.listdir(Path+dish)\n    for i in food:\n        images.append(dish+'_'+i)\n        label.append(dish)","d84682ef":"df = pd.DataFrame(list(zip(images, label)), columns=['Image', 'Cuisine'])\ndf.Cuisine.unique()","aa1fadb2":"df.Cuisine.hist()","ab23d385":"# Augment data\nbatch_size = 64\ntrain_input_shape = (224, 224, 3)\nn_classes = 6\n\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255.,\n                                   rotation_range=15,\n                                   #width_shift_range=0.5,\n                                   #height_shift_range=0.5,\n                                   shear_range=0.1,\n                                   zoom_range=0.5,\n                                   #horizontal_flip=True,\n                                   #vertical_flip=True,\n                                  )\n\ntrain_generator = train_datagen.flow_from_directory(directory=Path,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=df.Cuisine.unique().tolist()\n                                                   )\n\nvalid_generator = train_datagen.flow_from_directory(directory=Path,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=df.Cuisine.unique().tolist()\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","f1d538e1":"base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \nX = base_model.output\nX = Flatten()(X)\n\nX = Dense(512, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(128, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(16, kernel_initializer='he_uniform')(X)\nX = Dropout(0.4)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\noutput = Dense(n_classes, activation='softmax')(X)\n\nmodel = Model(inputs=base_model.input, outputs=output)","353b8984":"optimizer = Adam(lr=0.00001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])","4dc8deda":"n_epoch = 50\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","015508bc":"# Train the model - all layers\nhistory1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=4\n                             )","ce3eb129":"# Freeze core ResNet layers and train again \nfor layer in model.layers[-6:]:\n   layer.trainable = False\n\nfor layer in model.layers:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.00001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 50\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=4                             \n                              )","9204832c":"# Merge history1 and history2\nhistory = {}\nhistory['loss'] = history1.history['loss'] + history2.history['loss']\nhistory['acc'] = history1.history['accuracy'] + history2.history['accuracy']\nhistory['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\nhistory['val_acc'] = history1.history['val_accuracy'] + history2.history['val_accuracy']\nhistory['lr'] = history1.history['lr'] + history2.history['lr']","40a1d8d3":"# Plot the training graph\nimport matplotlib.pyplot as plt\ndef plot_training(history):\n    acc = history['acc']\n    val_acc = history['val_acc']\n    loss = history['loss']\n    val_loss = history['val_loss']\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n    \n    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='best')\n\n    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='best')\n    \n    plt.show()\n    \nplot_training(history)","0aadb535":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator)\nprint(\"Prediction accuracy on train data =\", score[1])","1dd58c1b":"# Prediction accuracy on CV data\nscore = model.evaluate_generator(valid_generator)\nprint(\"Prediction accuracy on CV data =\", score[1])","bcb5f9f3":"from sklearn.metrics import *\nimport seaborn as sns\n\ntick_labels = df.Cuisine.unique().tolist()\n\ndef showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n    # Loop on each generator batch and predict\n    y_pred, y_true = [], []\n    for i in range(STEP_SIZE_VALID):\n        (X,y) = next(valid_generator)\n        y_pred.append(model.predict(X))\n        y_true.append(y)\n    \n    # Create a flat list for y_true and y_pred\n    y_pred = [subresult for result in y_pred for subresult in result]\n    y_true = [subresult for result in y_true for subresult in result]\n    \n    # Update Truth vector based on argmax\n    y_true = np.argmax(y_true, axis=1)\n    y_true = np.asarray(y_true).ravel()\n    \n    # Update Prediction vector based on argmax\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = np.asarray(y_pred).ravel()\n    \n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(10,10))\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n    conf_matrix = conf_matrix\/np.sum(conf_matrix, axis=1)\n    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n                ax=ax)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=df.Cuisine.unique().tolist()))\n\nshowClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)","499f8600":"# Classification of Food using Resnet50.\n### ToDo - Improve overfitting and validation accuracy."}}