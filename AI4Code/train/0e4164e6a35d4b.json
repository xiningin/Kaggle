{"cell_type":{"b3ec2727":"code","002d523a":"code","0695cdc1":"code","87f08111":"code","16767a9b":"code","41b90c13":"code","5627c594":"code","00401142":"code","3a927d26":"code","c0f650ec":"code","84204d1f":"code","48ae25c7":"code","57799309":"code","23a23135":"code","567e3835":"code","bb02af2c":"code","a6f05daa":"code","bef6356c":"code","e5066dc0":"code","c45736e0":"code","4bc24849":"code","fe0b4a9f":"code","b68b695f":"code","f2f995aa":"code","bf90a177":"code","550e5370":"code","3d8435fb":"code","31468188":"code","41748189":"code","f5d91dc7":"code","f03da4c2":"code","e92ffca5":"code","7425803a":"code","5602e2fd":"code","bc0506f0":"code","56d3213c":"code","6a270f43":"code","974430d9":"code","10b07a18":"code","7ed236c0":"code","cee75f28":"code","123f0b35":"code","b9e5b526":"code","65242977":"code","8353dfd0":"code","196a301d":"code","9ad60b55":"code","a333b523":"code","8ce4cf70":"code","8d118049":"code","22422b21":"code","da8aaf63":"code","49fdad96":"markdown","9ef129e2":"markdown","29fbc686":"markdown","f0853414":"markdown","697595ca":"markdown","d993c70c":"markdown","0935beb1":"markdown","7416057e":"markdown","e58c024b":"markdown","60c76d4a":"markdown","389caa8f":"markdown","a6b5fa5e":"markdown","169cf600":"markdown","a97de5ad":"markdown","70156646":"markdown","347c72a9":"markdown","45ab45df":"markdown","230ec82c":"markdown","80f09bee":"markdown","a8b16d68":"markdown","cda77828":"markdown","df00f721":"markdown","9abe727c":"markdown","a6828823":"markdown","00d60479":"markdown","cfd83bf8":"markdown","e65efda8":"markdown","87290c3b":"markdown","8e275401":"markdown","c74690c3":"markdown","b8d198da":"markdown","27333522":"markdown","3cd823e3":"markdown"},"source":{"b3ec2727":"import warnings\nwarnings.filterwarnings('ignore')","002d523a":"import pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split, learning_curve, ShuffleSplit, TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.sparse import hstack\nimport eli5\n","0695cdc1":"times = ['time%s' % i for i in range(1, 11)]\ntrain_df = pd.read_csv('..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/train_sessions.csv',\n                       index_col='session_id', parse_dates=times)\ntest_df = pd.read_csv('..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/test_sessions.csv',\n                      index_col='session_id', parse_dates=times)","87f08111":"train_df = train_df.sort_values(by='time1')\ntrain_df.head()","16767a9b":"# Here we make a new DataFrame and store sites in one feature\nsites = ['site%s' % i for i in range(1, 11)]\ntrain_df[sites] = train_df[sites].fillna(0).astype(np.uint16).astype(str)\nmodel_df = train_df[sites].apply(lambda x: \" \".join(x), axis=1)","41b90c13":"# Reset index, delete 0 from the string and change type to np.str_\nmodel_df = model_df.reset_index(name='sites').set_index('session_id')\nmodel_df = model_df.replace({'sites':r'( 0)*'},{'sites':''}, regex=True)\nmodel_df['sites'] = model_df['sites'].apply(lambda x: np.str_(x))","5627c594":"# Here we add new features: start, end and the length of the session\nmodel_df['target'] = train_df['target']\nmodel_df['start'] = train_df[times].min(axis=1)\nmodel_df['end'] = train_df[times].max(axis=1)\nmodel_df['session_len'] = (model_df['end'] - model_df['start']) \/ np.timedelta64(1, 's')","00401142":"model_df.head()","3a927d26":"model_df.groupby('target')['sites'].count()","c0f650ec":"sns.boxplot(x='target', y='session_len', data = model_df)","84204d1f":"vec1 = CountVectorizer(ngram_range=(1,3), max_features=100000)","48ae25c7":"def freq_sites(vectorizer, data):\n    X = vectorizer.fit_transform(data)\n    freqs = zip(vectorizer.get_feature_names(), np.asarray(X.sum(axis=0)).ravel())\n    return sorted(freqs, key = lambda x: x[1], reverse=True)[:10]","57799309":"# First column for Alice, second - not.\nl = [freq_sites(vec1, model_df[model_df['target']==1]['sites']),\n     freq_sites(vec1, model_df[model_df['target']==0]['sites'])]\nlist(map(list, zip(*l)))","23a23135":"model_df['day_of_week'] = model_df['start'].apply(lambda x: x.dayofweek).astype(int)\nmodel_df['hour'] = model_df['start'].apply(lambda x: x.hour).astype(int)\nmodel_df['month'] = model_df['start'].apply(lambda x: x.month).astype(int)","567e3835":"model_df['day'] = model_df['start'].apply(lambda x: x.day).astype(int)","bb02af2c":"sns.countplot(x='day_of_week', hue='target', data=model_df[model_df['target']==0])","a6f05daa":"sns.countplot(x='day_of_week', hue='target', data=model_df[model_df['target']==1])","bef6356c":"model_df['active_days'] = model_df['day_of_week'].apply(lambda x: x in [0,1,3,4]).astype(int)","e5066dc0":"sns.countplot(x='hour', data=model_df[model_df['target']==0])","c45736e0":"sns.countplot(x='hour', data=model_df[model_df['target']==1])","4bc24849":"model_df['active_hours']= model_df['hour'].apply(lambda x: x in [12,13,16,17,18]).astype(int)","fe0b4a9f":"sns.countplot(x='day', data=model_df[model_df['target']==0])","b68b695f":"sns.countplot(x='day', data=model_df[model_df['target']==1])","f2f995aa":"sns.countplot(x='month', data=model_df[model_df['target']==0])","bf90a177":"sns.countplot(x='month', data=model_df[model_df['target']==1])","550e5370":"model_df['active_month']= model_df['month'].apply(lambda x: x in [1,2,3,4,9,11,12]).astype(int)","3d8435fb":"log_reg = LogisticRegression(random_state=17, solver='liblinear')\nscaler = StandardScaler()\nvec2=TfidfVectorizer(ngram_range=(1,3), max_features=100000)","31468188":"cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\ntime_split = TimeSeriesSplit(n_splits=10)","41748189":"# Function for printing metrics for our predicted result\ndef print_report(model,train_x, train_y, test_x, test_y):\n    pred_y = model.predict(test_x)\n    report = metrics.classification_report(test_y, pred_y)\n    print(report)\n    cv_scores = cross_val_score(model, train_x, train_y, cv=time_split, scoring='roc_auc', n_jobs=4)\n    print(\"cv_scores (train)\",cv_scores)\n    print(\"cv_scores (train) mean: {:0.3f}\".format(cv_scores.mean()))\n    print(\"roc_auc_score (test):  {:0.3f}\".format(roc_auc_score(test_y, pred_y)))","f5d91dc7":"# This function helps us not to repeat the same lines of code many times.\ndef model_cycle(model, train_x, test_x, train_y, test_y):\n    model.fit(train_x,train_y)\n    print_report(model, train_x, train_y, test_x , test_y)\n    return model","f03da4c2":"def data_preparing(train_df, test_df, vec, feature_names):\n    train_vec = vec.fit_transform(train_df['sites'])\n    test_vec = vec.transform(test_df['sites'])\n    if feature_names!=[]:\n        scaled_features_train = scaler.fit_transform(train_df[feature_names])\n        joined_train = hstack([train_vec, scaled_features_train])\n        scaled_features_test = scaler.transform(test_df[feature_names])\n        joined_test = hstack([test_vec, scaled_features_test])\n        return (joined_train, joined_test)\n    return (train_vec, test_vec)    ","e92ffca5":"def plot_learning_curves(model, train_x, train_y,  cv, n_jobs=4):\n    #train_vec = vectorizer.fit_transform(train_x)\n    train_sizes, train_scores, valid_scores = learning_curve(model, train_x, train_y,\n                                                             train_sizes=np.linspace(.1, 1.0, 5), scoring='roc_auc', cv=cv, n_jobs=n_jobs)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    valid_scores_mean = np.mean(valid_scores, axis=1)\n    valid_scores_std = np.std(valid_scores, axis=1)\n\n    # Plot learning curve \n    plt.grid()\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    plt.ylim(0.9,1)\n    plt.legend(loc=\"best\")\n    plt.xlabel(\"training_examples\")\n    plt.ylabel(\"Score\")\n    plt.show()    ","7425803a":"# Here we split our data for the train and test sets\ntrain_x, test_x, train_y, test_y = train_test_split(model_df, model_df['target'], test_size=0.33, random_state=17)","5602e2fd":"train_temp, test_temp = data_preparing(train_x, test_x, vec1, [])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y, cv,4)","bc0506f0":"eli5.show_weights(model, feature_names=vec1.get_feature_names())","56d3213c":"train_temp, test_temp = data_preparing(train_x, test_x, vec2, [])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y,  cv,4)","6a270f43":"eli5.show_weights(model, feature_names=vec2.get_feature_names())","974430d9":"train_temp, test_temp = data_preparing(train_x, test_x, vec1, ['hour', 'session_len', 'day_of_week','day','month'])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y,  cv,4)","10b07a18":"eli5.show_weights(model, feature_names=vec1.get_feature_names()+['hour', 'session_len', 'day_of_week','day','month'])","7ed236c0":"train_temp, test_temp = data_preparing(train_x, test_x, vec2, ['hour', 'session_len', 'day_of_week', 'day','month'])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y,  cv,4)","cee75f28":"eli5.show_weights(model, feature_names=vec2.get_feature_names()+['hour', 'session_len', 'day_of_week','day','month'])","123f0b35":"train_temp, test_temp = data_preparing(train_x, test_x, vec1, ['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month'])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y,  cv,4)","b9e5b526":"eli5.show_weights(model, feature_names=vec1.get_feature_names()+['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month'])","65242977":"train_temp, test_temp = data_preparing(train_x, test_x, vec2, ['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month'])\nmodel = model_cycle(log_reg, train_temp, test_temp, train_y, test_y )\nplot_learning_curves(log_reg, train_temp, train_y,  cv,4)","8353dfd0":"eli5.show_weights(model, feature_names=vec2.get_feature_names()+['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month'])","196a301d":"from sklearn.preprocessing import PolynomialFeatures","9ad60b55":" poly = PolynomialFeatures(2, include_bias=False)","a333b523":"train_temp, test_temp = data_preparing(train_x, test_x, vec1, [])\npoly_feat = poly.fit_transform(train_x[['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month']])\npoly_scaled = scaler.fit_transform(poly_feat)\ntrain_poly = hstack([train_temp, poly_scaled])\npoly_feat = poly.fit_transform(test_x[['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month']])\npoly_scaled = scaler.fit_transform(poly_feat)\ntest_poly = hstack([test_temp, poly_scaled])","8ce4cf70":"model = model_cycle(log_reg, train_poly, test_poly, train_y, test_y )\nplot_learning_curves(log_reg, train_poly, train_y,  cv,4)","8d118049":"train_temp, test_temp = data_preparing(train_x, test_x, vec2, [])\npoly_feat = poly.fit_transform(train_x[['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month']])\npoly_scaled = scaler.fit_transform(poly_feat)\ntrain_poly = hstack([train_temp, poly_scaled])\npoly_feat = poly.fit_transform(test_x[['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month']])\npoly_scaled = scaler.fit_transform(poly_feat)\ntest_poly = hstack([test_temp, poly_scaled])","22422b21":"model = model_cycle(log_reg, train_poly, test_poly, train_y, test_y )\nplot_learning_curves(log_reg, train_poly, train_y,  cv,4)","da8aaf63":"\"\"\"\ntest_df[sites] = test_df[sites].fillna(0).astype(np.uint16).astype(str)\ntest_sites_df = test_df[sites].apply(lambda x: \" \".join(x), axis=1)\ntest_sites_df = test_sites_df.reset_index(name='sites').set_index('session_id')\ntest_sites_df = test_sites_df.replace({'sites':r'( 0)*'},{'sites':''}, regex=True)\ntest_model_df = pd.DataFrame(index=test_df.index)\n#\ntest_model_df['start'] = test_df[times].min(axis=1)\ntest_model_df['end'] = test_df[times].max(axis=1)\n#\ntest_model_df['session_len'] = (test_model_df['end'] - test_model_df['start']) \/ np.timedelta64(1, 's')\ntest_model_df['sites'] = test_sites_df['sites']\ntest_model_df['sites'] = test_model_df['sites'].apply(lambda x: np.str_(x))\n#\ntest_model_df['day_of_week'] = test_model_df['start'].apply(lambda x: x.dayofweek).astype(int)\ntest_model_df['hour']= test_model_df['start'].apply(lambda x: x.hour).astype(int)\ntest_model_df['active_hours']= test_model_df['hour'].apply(lambda x: x in [12,13,16,17,18]).astype(int)\ntest_model_df['active_days'] = test_model_df['day_of_week'].apply(lambda x: x in [0,1,3,4]).astype(int)\ntest_model_df['month']= test_model_df['start'].apply(lambda x: x.month).astype(int)\ntest_model_df['active_month']= test_model_df['month'].apply(lambda x: x in [1,2,3,4,9,11,12]).astype(int)\ntest_model_df['day'] = test_model_df['start'].apply(lambda x: x.day).astype(int)\n#\ntest_vec = vec1.transform(test_model_df['sites'])\npoly_feat = poly.transform(test_model_df[['hour', 'session_len', 'day_of_week','day','month','active_hours','active_days','active_month']])\npoly_scaled = scaler.transform(poly_feat)\nfinal_test = hstack([test_vec, poly_scaled])\n\"\"\"","49fdad96":"What do we have in the end of the work? We shoul use as many tools as possible to make the right decision in model selecting. The next steps to do:\n* 1) Select the model. I'd rather take the firt one, beacuse of it's higher recall, but the second one looks like it still has reserve for improvement. If you want to add more features - better to select the second.\n* 2) Tuning of the parameters: regularization C for the logistic regression and degree for polynom. This part is missed here, because calculations take a long time.\n* 3) Transform the test set in the same way, that train if you want to make the submission to the competition (I have this part for you commented)","9ef129e2":"And here are the result for first model. The calculation takes some time, please, be patient","29fbc686":"## EDA part","f0853414":"And now we visualize them","697595ca":"## Model performance and learning curves","d993c70c":"Here we have active months, let's add them to the features","0935beb1":"More improvement: variance becomes lower and test results and recall goes up: were are able to detect almost half of the Alice's sessions.","7416057e":"So we can see, that polynomial features also improve our results: 0.67 vs 0.616 on test and lower bias. But still we have have here relatively low recall, that concerns me.","e58c024b":"That's it, thanks for reading!","60c76d4a":"Here we initialize our model, scaler, TfIdf vectorizer, also write useful functions to not repeat the same code again","389caa8f":"We can that result improves significanlty, 0.676 vs 0.629 on test and the variance is smaler,recall is also goes up, so we're on the right way. Now we check our second vectorizer","a6b5fa5e":"We can see, that all sessions have different length, there are codded names for sites and the time when user navigated to each site. We will transform our data in the following way:\n* 1) All sites we will have in one text feature, so we will be able to treat them as text feature\n* 2) We will save times when session begins and ends\n* 3) We will create a session_len feature","169cf600":"Here our model also shows better scores 0.558 vs 0.529, but it's bias and variance remains the same. We can see, that it's learning curve looks different and shows us the potential for improvement. Now we add our special features, that describe Alise's pattern","a97de5ad":"## Data loading and transformation","70156646":"This notebook doesn't beat the second baseline for the in-class competition. The idea here is to try use lerning curves for model selecting and to see how adding new features improves model's performance . This notebook consists of:\n* 1) Data loading and transforming\n* 2) EDA and feature creation \n* 3) LogisticRegression and it's performance inspection\n* 4) Adding polynomial features\n* 5) Conclusion","347c72a9":"Here we can see the improvement too: both bias and variance goes down, but in average it shows worse results than the first one. And the main difference between model remains too: first gas lower bias and higher variance and vice-versa. Now we add polynomial features and look at the result  ","45ab45df":"Let's explore our dataset and try to find any patterns in the Alice's behavior","230ec82c":"Here are the results: you can see the improvement again: 0.771 vs 0.739 on test, higher recall 0.54 vs 0.48 and lower variance. Now let's look at the second model.","80f09bee":"We will apply polynomial features on all non-sites features","a8b16d68":"We can see here, that in average Alice spend less time for session, so the session_len can help us to detect Alice. Next we check sites and find the most popular","cda77828":"# Building Lerning curves for Logit","df00f721":"Here we can see that Alice has some kind of pattern: she has almost no sessions on 2,5,6 days. Maybe we should use it as feature","9abe727c":"Here our first attempt to train model. We use only sites here, no additional information","a6828823":"Here we can see, that the dataset is unbalanced ","00d60479":"The degree of polynom should be selected within the cross-validation, but here we will use degree=2 just to show the idea.","cfd83bf8":"Here we can see low bias on training set and high variance between train and validation scores. We also have the test score, that differs significantly. What also concerns here - low recall of Alice's sessions, which means that our model misses most sessions of Alice. Next we will try TfidfVectorizer for converting sites to features and compare results.","e65efda8":"Here the results are even worse, and we have the classic problem here, when comparing different models: first one has lower bias and higher variance and the second - vice-versa. Let's add more features and look at the results. We will add only the part of features, that we have created, just mearures without attempts to detect pattern ","87290c3b":"## Conclusion","8e275401":"No special info here, but let's keep this feature","c74690c3":"## LogisticRegression and useful functions","b8d198da":"We can see here a significant difference in favorite sites for  Alice and not-Alice, so this feature will be really useful. We also can use TdIdf vectorizer to decrease the impact of common sites and then compare the results. Let's add more features: hour, day_of_week and month","27333522":"## Polynomial features","3cd823e3":"Well here again there is a pattern. Alice's sessions are from 9 till 18 hours, and the most active hours are 12,13,16,17,18.  We definetelly should take it as feature"}}