{"cell_type":{"7758920a":"code","673eb087":"code","d63887b7":"code","ea4ced3b":"code","5430edd8":"code","19c91b83":"code","e3d10162":"code","f7a86948":"code","a7b77dca":"code","e99a9cfa":"code","ed83b25b":"code","132e2ef7":"code","755c80e6":"code","0fd896b6":"code","f71f8344":"code","2cd31d70":"code","ba382c07":"code","01fcdf78":"code","88e4d0ab":"code","03d55881":"code","d9987759":"code","f9af3da3":"code","778167b7":"code","201e71c9":"code","b6145f46":"code","ce55f54a":"code","f4cebf73":"code","9081b53d":"code","c109bec9":"code","ed79b4dd":"code","81276c5e":"code","eda17ca2":"code","b5bcb658":"code","5113bd3a":"code","3e8ad492":"code","acf825e0":"code","de43ae83":"code","71bfca22":"code","1251e650":"code","033989c3":"code","c517f986":"code","aecf6438":"code","84724921":"code","66c27878":"code","62f91d1d":"code","03c6c672":"code","41782083":"code","b7c0df57":"code","be6f0104":"code","568cecc6":"code","29007d56":"code","bb48cf56":"code","3db53822":"code","1c8c8e7b":"code","b52b0741":"code","ce98b658":"code","e026e971":"code","4bb75133":"code","f3552e28":"code","0b121e14":"code","8f39e4ed":"code","04cece7c":"code","83a29d63":"code","3cbba65b":"code","c873079c":"code","6e5f6a96":"code","dba9dcbe":"markdown","7cb25e2a":"markdown","d6dbcde8":"markdown","ae351e83":"markdown","4d39c21f":"markdown","8d60cfc3":"markdown","6e11a446":"markdown","b2b200d0":"markdown","40d2eba9":"markdown","9c4529b7":"markdown","68dbb657":"markdown","bd57a6f5":"markdown","8bd8a278":"markdown","a5f72de0":"markdown","5ce2a11e":"markdown","570974f8":"markdown","d2b030e5":"markdown","c6d089ef":"markdown","4fef5e4e":"markdown","dbef6013":"markdown","ed08e508":"markdown","36ea5d2f":"markdown","460f568b":"markdown","2f86e71c":"markdown","967a99d4":"markdown","ce529490":"markdown","3f2ba650":"markdown","6e763947":"markdown","cbfa82ce":"markdown","0ce6f6f2":"markdown","966f9165":"markdown","7c9beff3":"markdown","8c3077f0":"markdown","94fd1fd9":"markdown","214c1b35":"markdown","4b0a5eb9":"markdown","c9e6458d":"markdown","ddc72776":"markdown","7d19954f":"markdown","8e57fc49":"markdown","54ead8ae":"markdown","c40b05bd":"markdown","41315d1e":"markdown","8b72aeaf":"markdown","483f6e04":"markdown","57f45624":"markdown","6920ee0c":"markdown","fe4e0c78":"markdown","7df177f1":"markdown","dd816328":"markdown","26d86d24":"markdown","f30912af":"markdown"},"source":{"7758920a":"!pip install contractions\n!pip install keyBERT\n!pip install transformers\nfrom keybert import KeyBERT\nmodel = KeyBERT('distilbert-base-nli-mean-tokens')\nfrom transformers import pipeline\nclassifier = pipeline('sentiment-analysis')","673eb087":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport contractions\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwords=set(nltk.corpus.words.words())\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings(\"ignore\")\noutput_notebook()","d63887b7":"def check_null(data):\n    for i in data.columns:\n        print(i,\":\",(data[str(i)].isnull().sum()\/data.shape[0])*100)","ea4ced3b":"def to_delete_cols(data):\n    to_delete = []\n    for i in data.columns:\n        if((data[str(i)].isnull().sum()\/data.shape[0])*100 > 95):\n            to_delete.append(i)\n    return to_delete","5430edd8":"def clean_txt(input_txt, pattern = \"@[\\w]*\"):\n    #removing hashtags,emojis,stopwords\n    input_txt=re.sub(r'#[\\w]*','',input_txt)\n    input_txt=input_txt.encode(\"ascii\",\"ignore\")\n    input_txt=input_txt.decode()\n    \n    ##removing @user\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n   \n    #removing stopwords    \n    input_txt = ' '.join([i for i in input_txt.split() if not i in words])\n    #contractions\n    input_txt=contractions.fix(input_txt)\n    #removing punctuation,numbers and whitespace   \n    res=re.sub(r'[^\\w\\s]', '', input_txt.lower())\n    res=re.sub('\\s+',' ',res)\n    ##removing links\n    res=re.sub(r'https[\\w]*', '', res, flags=re.MULTILINE)\n    #removing acronyms\n    res=''.join(i for i in res if not i.isdigit())\n    res=' '.join([i for i in res.split() if len(i)>2])\n    lem = WordNetLemmatizer()\n    res = lem.lemmatize(res)\n    return res","19c91b83":"def get_keywords(x):\n    list_keywords = []\n    keywords = model.extract_keywords(x, keyphrase_ngram_range=(1, 1))\n    for i in keywords:\n        list_keywords.append(i[0])\n    return list_keywords","e3d10162":"def get_keywords_ratings(df):\n    keywords = []\n    for _,row in df.iterrows():\n        for i in row['Keywords']:\n            keywords.append(i)\n    keywords = list(set(keywords))\n    keywords_percentage = dict.fromkeys(set(keywords),0)\n    for _,row in df.iterrows():\n        for i in row['Keywords']:\n            keywords_percentage[i]+=1\n    \n    return pd.DataFrame.from_dict(keywords_percentage,orient = 'index',columns = ['Frequency']).reset_index().rename(columns = {'index':'Words'}).sort_values('Frequency',ascending = False).reset_index()","f7a86948":"def get_tokens(x):\n    return x.split()","a7b77dca":"def vectorize(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    emb = tfidf_vectorizer.fit_transform(data)\n    return emb,tfidf_vectorizer","e99a9cfa":"def get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)\n\n","ed83b25b":"def get_top_n_words(n, keys, document_term_matrix, count_vectorizer,n_topics = 8):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","132e2ef7":"def get_hashtags(data):\n    hashtags = \"\"\n    for _,row in data.iterrows():\n        a=\"\"\n        for i in row['hashtags']:\n            if(i == '[' or i == \"'\" or i == ']'):\n                continue\n            else:\n                a+=i\n        if(a.split(',') != ['']):\n            hashtags+=a\n            hashtags+=','\n    return hashtags.split(',')","755c80e6":"def get_frequency_hashtags(data):\n    hashtags = get_hashtags(data)\n    freq_hashtags = {'Hashtag':[],'Frequency':[]}\n    for i in set(hashtags):\n        if (i !=''):\n            freq_hashtags['Hashtag'].append(i)\n            freq_hashtags['Frequency'].append(hashtags.count(i))\n    return pd.DataFrame.from_dict(freq_hashtags).sort_values('Frequency',ascending = False)","0fd896b6":"def get_sentiment(x):\n    if(classifier(x)[0]['label'] == 'POSITIVE'):\n        return 1\n    return 0","f71f8344":"def wc(data,bgcolor,title):\n    plt.figure(figsize = (15,15))\n    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.title(title , fontsize = 20)\n    plt.imshow(wc)\n    plt.axis('off')","2cd31d70":"def get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(8):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","ba382c07":"data = pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2010.csv')\nfor i in range(2011,2019):\n    print(i)\n    new_data = pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets{year}.csv'.format(year = str(i)))\n    data = data.append(new_data)","01fcdf78":"data = data.append(pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2019Part1.csv'))\ndata = data.append(pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2019Part2.csv'))\ndata = data.append(pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2020Part1.csv'))\ndata = data.append(pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2020Part2.csv'))\ndata = data.append(pd.read_csv('..\/input\/kaggle-tweets-2010-2021\/KaggleTweets2021.csv'))\ndata.tail()","88e4d0ab":"data.head()","03d55881":"check_null(data)","d9987759":"data.drop(columns = to_delete_cols(data) , axis = 1 , inplace = True)\ndata.head()","f9af3da3":"check_null(data)","778167b7":"data.info()","201e71c9":"data.drop(columns = ['timezone','photos','video','reply_to','username','thumbnail','name','date','time'],axis = 1,inplace = True)\ndata['created_at'] = pd.to_datetime(data['created_at'])\ndata.head()","b6145f46":"def clean_txt(input_txt, pattern = \"@[\\w]*\"):\n    #removing hashtags,emojis,stopwords\n    input_txt=re.sub(r'#[\\w]*','',input_txt)\n    input_txt=input_txt.encode(\"ascii\",\"ignore\")\n    input_txt=input_txt.decode()\n    \n    ##removing @user\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n   \n    #removing stopwords    \n    input_txt = ' '.join([i for i in input_txt.split() if not i in words])\n    #contractions\n    input_txt=contractions.fix(input_txt)\n    #removing punctuation,numbers and whitespace   \n    res=re.sub(r'[^\\w\\s]', '', input_txt.lower())\n    res=re.sub('\\s+',' ',res)\n    ##removing links\n    res=re.sub(r'https[\\w]*', '', res, flags=re.MULTILINE)\n    #removing acronyms\n    res=''.join(i for i in res if not i.isdigit())\n    res=' '.join([i for i in res.split() if len(i)>2])\n    lem = WordNetLemmatizer()\n    res = lem.lemmatize(res)\n    return res","ce55f54a":"language = {'en' : 'English' , 'ja' : 'Japanese' , 'ht' : 'ht' , 'es' : 'Spanish' , 'zh' : 'Chinese' , 'it' : 'Italian' , 'und' : 'und' , 'fr' : 'French' , 'de' : 'German'\n           , 'ru' : 'Russian', 'pt' : 'Portuguese' , 'ko' : 'Korean' , 'nl' : 'Dutch' , 'ro' : 'Romanian' , 'uk' : 'Ukrainian' , 'in' : 'in' , 'cy' : 'cy' , 'tl' : 'tl' , 'no' : 'Norwegian', 'hu':'hu', 'sl':'sl',\n       'da' : 'Danish', 'el' : 'Greek', 'sv':'Swedish', 'lt': 'lt', 'et':'et', 'tr' : 'Turkish', 'lv' : 'lv', 'cs' : 'Czech', 'pl' : 'Polish', 'fi' : 'Finnish', 'ar' : 'Arabic',\n       'is':'is', 'th' : 'Thai', 'fa' : 'Persian', 'bg' : 'bg',  'hi' : 'Hindi', 'iw' : 'iw', 'vi' : 'Vietnamese', 'eu' : 'eu', 'lo' : 'lo', 'ta':'ta', 'te':'te',\n       'ur': 'Urdu', 'bn' : 'Bengali', 'ne' : 'ne', 'mr' : 'mr'}\ndata['language'] = data['language'].map(language)","f4cebf73":"data['language'] = data['language'].replace(np.nan , 'LCode Not Found')","9081b53d":"data['language'].unique()","c109bec9":"data_2010 = data[data['created_at'].dt.year == 2010]\ndata_2010.head()","ed79b4dd":"data_2010['language'].value_counts().index[:5]","81276c5e":"language_series = pd.DataFrame({ 'Languages': data_2010['language'].value_counts().index , 'Total Tweets' : data_2010['language'].value_counts().tolist()})\nlanguage_series","eda17ca2":"fig,ax = plt.subplots(ncols = 2, figsize = (20,4) , dpi = 100)\nplt.tight_layout()\ncolors = ['#3d5a80', '#98c1d9' , '#e0fbfc' ,'#ee6c4d' , '#293241' , '#ffd92f','#e5c494']\nlanguage_series['Total Tweets'][:5].plot(kind = 'pie', labels = [' ',' ',' ',' ',' '], colors = colors[:5] , ax = ax[0])\nsns.barplot(y = language_series['Languages'][1:5] , x = language_series['Total Tweets'][1:5] ,ax = ax[1], palette = colors[:4])\nax[0].set_ylabel('')\nax[0].legend(labels = language_series['Languages'])\nax[0].set_title('Most Frequently Used Languages' ,  fontsize = 10)\nax[1].set_title('Subtracting English From the Equation',fontsize = 10)","b5bcb658":"plt.figure(figsize=(15,15))\nsns.barplot(y = [str(i) for i in data_2010['user_id'].unique()][:20] , x = data_2010['user_id'].value_counts().values[:20] , color = '#293241')\nplt.ylabel('User Ids' , fontsize = 15)\nplt.xlabel('Number of Tweets', fontsize = 15)\nplt.title('Top 20 Tweeters' , fontsize = 20)","5113bd3a":"sns.set_style(\"darkgrid\")\nplt.figure(figsize = (13,10))\ncolors = ['#ee6c4d' for i in range(10)]\ncolors[3] = '#293241'\ncolors[5] = '#293241'\ncolors[7] = '#293241'\ncolors[8] = '#293241'\nsns.countplot(data_2010['created_at'].dt.month , palette = colors)\nplt.xlabel('Months',fontsize = 13)\nplt.ylabel('')\nplt.title('Count of Tweets Monthly',fontsize = 18)","3e8ad492":"plt.figure(figsize = (15,10))\ncolors = ['#ee6c4d' for i in range(24)]\ncolors[23] = '#293241'\nsns.countplot(data_2010['created_at'].dt.hour , palette = colors)\nplt.xlabel('Hours',fontsize = 15)\nplt.ylabel('')\nplt.title('Count of Tweets Hourly',fontsize = 20)","acf825e0":"data_2010 = data_2010[data_2010['language'] == 'English']\ndata_2010['Clean Tweet'] = data_2010['tweet'].apply(clean_txt)\ndata_2010['Keywords'] = data_2010['Clean Tweet'].apply(get_keywords)\ndata_2010['Sentiment-Tweet'] = data_2010['tweet'].apply(get_sentiment)\ndata_2010.head()","de43ae83":"keywords_2010 = get_keywords_ratings(data_2010).drop(columns = ['index'], axis=0)[:30]\nplt.figure(figsize = (20,20))\nsns.barplot(y = keywords_2010['Words'][1:] , x = keywords_2010['Frequency'][1:] , color = '#293241')\nplt.tick_params(axis = 'y',labelsize = 12)\nplt.xlabel('Frequency', fontsize = 15)\nplt.ylabel('Keywords' , fontsize = 15)\nplt.title('Top 30 Keywords from Tweets of 2010' , fontsize = 20)","71bfca22":"list_corpus,tfidf = vectorize(data_2010[\"Clean Tweet\"].tolist())\n\ncolormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:8]","1251e650":"n_topics = 8\nlda_model = LatentDirichletAllocation(n_components=8, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(list_corpus)\n\nlda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)\n\ntop_n_words_lda = get_top_n_words(10, lda_keys,list_corpus, tfidf)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","033989c3":"top_3_words = get_top_n_words(3, lda_keys, list_corpus, tfidf)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lda_categories, lda_counts , color = '#293241')\nax.tick_params(axis = 'x' , labelsize = 12)\nax.set_xticks(lda_categories)\nax.set_xticklabels(labels)\nax.set_title('LDA topic counts' , fontsize = 20)\nax.set_ylabel('Number of headlines', fontsize = 15)\nplt.xticks(rotation = 45)\n\n","c517f986":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","aecf6438":"top_3_words_lda = get_top_n_words(3, lda_keys, list_corpus, tfidf)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","84724921":"frequency_hashtags = get_frequency_hashtags(data_2010)[:20]\nplt.figure(figsize = (20,20))\nsns.barplot(y = frequency_hashtags['Hashtag'] , x = frequency_hashtags['Frequency'] , color = '#293241')\nplt.tick_params(axis = 'y',labelsize = 12)\nplt.ylabel('Hashtags' , fontsize = 15)\nplt.xlabel('Frequency', fontsize = 15)\nplt.title('Top 20 Hashtags from Tweets of 2010' , fontsize = 18)","66c27878":"plt.figure(figsize = (10,10))\ncolors_senti = ['#ee6c4d' , '#3d5a80']\nsns.countplot(data_2010['Sentiment-Tweet'], palette = colors_senti)\nplt.xlabel('Sentiment',fontsize = 10)\nplt.ylabel('')\nplt.title('Assigned Sentiment Distribution',fontsize = 15)","62f91d1d":"positive_tweets = data_2010[data_2010['Sentiment-Tweet'] == 1]['Clean Tweet'].tolist()\npositive_tweets = list(set(positive_tweets))\nnegative_tweets = data_2010[data_2010['Sentiment-Tweet'] == 0]['Clean Tweet'].tolist()\nnegative_tweets = list(set(negative_tweets))\ncommon_words = list(set(positive_tweets)&set(negative_tweets))\npositive_tweets = list(set(positive_tweets).difference(set(common_words)))\nnegative_tweets = list(set(negative_tweets).difference(set(common_words)))","03c6c672":"wc(positive_tweets,'black','Positive Ones')","41782083":"wc(negative_tweets,'black','Negative Ones')","b7c0df57":"data_2021 = data[data['created_at'].dt.year == 2021]\ndata_2021.to_csv('data_2021.csv')\ndata_2021.head()","be6f0104":"data_2021.info()","568cecc6":"language_series = pd.DataFrame({ 'Languages': data_2021['language'].value_counts().index , 'Total Tweets' : data_2021['language'].value_counts().tolist()})\nlanguage_series.head()","29007d56":"fig,ax = plt.subplots(ncols = 3, figsize = (28,5) , dpi = 100)\ncolors = ['#780000', '#c1121f' , '#fdf0d5' ,'#003049' , '#669bbc']\nlanguage_series['Total Tweets'][:5].plot(kind = 'pie', labels = [' ',' ',' ',' ',' '], colors = colors , ax = ax[0])\nsns.barplot(y = language_series['Languages'][:2] , x = language_series['Total Tweets'][:2] ,ax = ax[1], palette = [colors[1],colors[3]])\nsns.barplot(y = language_series['Languages'][2:6] , x = language_series['Total Tweets'][2:6] ,ax = ax[2], palette = colors[:4])\nax[0].set_ylabel('')\nax[0].legend(labels = language_series['Languages'])\nax[0].set_title('Most Frequently Used Languages' ,  fontsize = 15)\nax[1].set_title('Comparision between Japanese and English' , fontsize = 15)\nax[2].set_title('Subtracting Japanese and English From the Equation',fontsize = 15)","bb48cf56":"plt.figure(figsize=(20,20))\nsns.barplot(y = [str(i) for i in data_2021['user_id'].unique()][:20] , x = data_2021['user_id'].value_counts().values[:20] , color = colors[1])\nplt.ylabel('User Ids',fontsize = 15)\nplt.xlabel('Number of Tweets',fontsize = 15)\nplt.title('Top 20 Tweeters' , fontsize = 20)","3db53822":"plt.figure(figsize = (8,8))\ncolors_months = [colors[1] for i in range(4)]\ncolors_months[0] = colors[3]\nsns.countplot(data_2021['created_at'].dt.month , palette = colors_months)\nplt.xlabel('Months',fontsize = 10)\nplt.ylabel('')\nplt.title('Count of Tweets Monthly',fontsize = 15)","1c8c8e7b":"fig,ax = plt.subplots(ncols = 2 , figsize = (20,10))\npale = [colors[1] for i in range(24)]\nfor i in range(8,17):\n    pale[i] = colors[4]\npale[14] = colors[3]\npale[13] = colors[3]\nsns.countplot(data_2021['created_at'].dt.hour , palette = pale , ax = ax[0])\nsns.distplot(data_2021['created_at'].dt.hour , ax = ax[1])\nax[0].set_xlabel('Hours',fontsize = 10)\nax[0].set_ylabel('')\nax[0].set_title('Count of Tweets Hourly',fontsize = 15)\nax[1].set_title('Tweet Distribution Hourly' , fontsize = 15)\nax[1].set_xlabel('Hours' , fontsize = 10)","b52b0741":"data_2021 = data_2021[data_2021['language'] == 'English']\ndata_2021['Clean Tweet'] = data_2021['tweet'].apply(clean_txt)\ndata_2021['Keywords'] = data_2021['Clean Tweet'].apply(get_keywords)\ndata_2021['Sentiment-Tweet'] = data_2021['tweet'].apply(get_sentiment)\ndata_2021.head()","ce98b658":"keywords_2021 = get_keywords_ratings(data_2021).drop(columns = ['index'], axis=0)[:30]\nplt.figure(figsize = (20,20))\nsns.barplot(y = keywords_2021['Words'][1:] , x = keywords_2021['Frequency'][1:] , color = colors[1])\nplt.tick_params(axis = 'y',labelsize = 12)\nplt.title('Top 30 Keywords from Tweets of 2010' , fontsize = 15)\n","e026e971":"list_corpus,tfidf = vectorize(data_2021[\"Clean Tweet\"].tolist())","4bb75133":"n_topics = 8\nlda_model = LatentDirichletAllocation(n_components=8, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(list_corpus)\n\nlda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)\n\ntop_n_words_lda = get_top_n_words(10, lda_keys,list_corpus, tfidf)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","f3552e28":"top_3_words = get_top_n_words(3, lda_keys, list_corpus, tfidf)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lda_categories, lda_counts , color = colors[1])\nax.set_xticks(lda_categories)\nax.set_xticklabels(labels)\nax.set_title('LDA topic counts')\nax.set_ylabel('Number of headlines')\nplt.xticks(rotation = 45)","0b121e14":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","8f39e4ed":"top_3_words_lda = get_top_n_words(3, lda_keys, list_corpus, tfidf)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","04cece7c":"frequency_hashtags = get_frequency_hashtags(data_2021)[:20]\nplt.figure(figsize = (20,20))\nsns.barplot(y = frequency_hashtags['Hashtag'] , x = frequency_hashtags['Frequency'] , color = colors[1])\nplt.tick_params(axis = 'y',labelsize = 12)\nplt.title('Top 20 Hashtags from Tweets of 2021' , fontsize = 15)","83a29d63":"plt.figure(figsize = (10,10))\npale = [colors[1] , colors[4]]\nsns.countplot(data_2021['Sentiment-Tweet'], palette = pale)\nplt.xlabel('Sentiment',fontsize = 10)\nplt.ylabel('')\nplt.title('Assigned Sentiment Distribution',fontsize = 15)","3cbba65b":"positive_tweets = data_2021[data_2021['Sentiment-Tweet'] == 1]['Clean Tweet'].tolist()\npositive_tweets = list(set(positive_tweets))\nnegative_tweets = data_2021[data_2021['Sentiment-Tweet'] == 0]['Clean Tweet'].tolist()\nnegative_tweets = list(set(negative_tweets))\ncommon_words = list(set(positive_tweets)&set(negative_tweets))\npositive_tweets = list(set(positive_tweets).difference(set(common_words)))\nnegative_tweets = list(set(negative_tweets).difference(set(common_words)))","c873079c":"wc(positive_tweets,'black','Positive Ones')","6e5f6a96":"wc(negative_tweets,'black','Negative Ones')","dba9dcbe":"# **Top 30 Keywords From Tweets of 2010**","7cb25e2a":"Clearly, I am not alone in scrolling through social media at the wrong time. From this we can see that for most of the office hours that is 9am-5pm, the number of tweets are less as compared to the late hours at night. Especially 11 pm. We'll see if this trend is continued for the next years.","d6dbcde8":"It looks like people have become extremely addicted to social media by now. The peak tweeting hours have however shifted b\/w 8 am to 4 pm with 2 pm recording the most tweets in the first 4 months of this year.","ae351e83":"# **Most Frequently Used Languages In Tweets 2021**","4d39c21f":"# **Decoding Languages**","8d60cfc3":"# **Topic Modelling using LDA 2010**","6e11a446":"<h1 id = 'sec6' style = 'background-color:black;color:white;font-family:Ubuntu'><center> Conclusion of 2021<\/center> <\/h1>","b2b200d0":"We can see that there are extremely few tweets in other languages , so just for now, in the rest of the notebook I  have provided the analysis on the basis of the tweets in english.","40d2eba9":"<h1 id = 'sec4' style = 'background-color:black;color:white;font-family:Ubuntu'><center> Conclusion of 2010 <\/center> <\/h1>","9c4529b7":"Maximum tweets , so far , seem to have come from the month of January.","68dbb657":"<h1 style = 'background-color:black;color:white;font-family:Ubuntu'><center> About the Dataset And This Notebook <\/center> <\/h1>","bd57a6f5":"# **Most Frequently Used Languages In Tweets 2010**","8bd8a278":"<div style =\"display: block;margin-left: auto;margin-right: auto;width: 50%;\">\n    <img src = \"https:\/\/miro.medium.com\/max\/1024\/1*bQYZ7fp_wcjWFwpBbcRS-Q.png\"\/>\n <\/div>","a5f72de0":"# **Keywords of Tweets of Year 2021**","5ce2a11e":"# **Total Tweets Segregated Hourwise 2010**","570974f8":"# **Sentiment Analysis of Tweets 2021**","d2b030e5":"# **Top 8 Topics Found Using LDA**","c6d089ef":"The peak months were clearly Novemeber, October, August and June.","4fef5e4e":"<h1 style = 'background-color:black;color:white;font-family:Ubuntu'><center>Index<\/center> <\/h1>\n\n1. [Libraries and Methods](#sec1)\n2. [Time to Clean:Preprocessing and Cleaning Data](#sec2)\n3. [Year in Review : 2010](#sec3)\n4. [Conclusion of Year 2010](#sec4)\n5. [Year in Review : 2021](#sec5)\n6. [Conclusion of Year 2021](#sec6)\n7. [What has really changed?](#sec7)","dbef6013":"I looked up twitter's language codes to decode the given representations. I was unable to find certain codes and have left them as such.","ed08e508":"# **Topic Modelling 2021**","36ea5d2f":"# **Positive Word Cloud 2021**","460f568b":"# **Methods**","2f86e71c":"# **TSNE Representation Of Top 8 Topics Using LDA 2010**","967a99d4":"\n1. **Most Frequently Used Languages** \n    * Japanese and English dominate the Languages Region. \n    * This could imply that Kaggle has a huge fan base in Japan.\n\n2. **Top 20 Tweeters** \n    * I used User Ids instead of names for the sake of privacy. From the Graph , we can clearly see which user id was the most active on tweeting about Kaggle.\n\n3. **Peak Months** \n    * The peak month so far has been January.\n4. **Most Frequent Hours for incoming Tweets**\n    * Well, the number of incoming Tweets regarding Kaggle have increased as compared to the numbers in 2010. \n    * Another thing which has changed is that most number of tweets are found in the 8am to 4pm period.\n    \n5. **Top 30 Keywords** \n    * This list is topped by the words learning, python , competitions and grandmaster.\n    * It certainly shows a vast difference as compared to the starting days of Kaggle in 2010. The only thing in common with that year and 2021 are the tweets regarding competition.    \n\n6. **Topic Modelling**\n    *  Through Topic Modelling few important topics of discussions were identified most of which were talks about competitions but no particular competitions came up so far.\n7. **Top 20 Hashtags**\n   * The top most hashtag was of course of the word kaggle . It was followed closely by other words such as data science, machine learning and 66daysofData.\n   * There are many challenges which can be seen in the list. This makes one feel that the tweets related to Kaggle in 2021 are all about learning.\n   ","ce529490":"<h1 id = 'sec7' style = 'background-color:black;color:white;font-family:Ubuntu'><center> So, what really has changed?<\/center> <\/h1>","3f2ba650":"# **Top 20 Hashtags from Tweets of 2021**","6e763947":"<h1 style = 'background-color:black;color:white;font-family:Ubuntu'><center> References <\/center> <\/h1>\n\n[Topic Modelling ](https:\/\/www.kaggle.com\/rcushen\/topic-modelling-with-lsa-and-lda)","cbfa82ce":"# **Word Cloud For Positive Tweets 2010**","0ce6f6f2":"The majority of texts are in Japanese. However , I am yet to train the MBERT embeddings for the task of Sentiment Analysis. Will include the results , after having done that.","966f9165":"<h1 style = 'background-color:black;color:white;font-family:Ubuntu'><center>\u2728 Kaggle in Tweets :What has changed in these 11 years? \u2728 <\/center> <\/h1>","7c9beff3":"# **User Ids Of Top 20 Tweet Accounts 2010**","8c3077f0":"<h1 id = 'sec3' style = 'background-color:black;color:white;font-family:Ubuntu'><center>Year in Review : 2010<\/center> <\/h1>","94fd1fd9":"<h1 style = 'background-color:black;color:white;font-family:Ubuntu'><center> That's all folks ! <\/center> <\/h1>\n\nThanks for checking out my notebook. Liked my work , leave an upvote ! Found something and want to discuss it , leave a comment and i'll get back to you ASAP \ud83d\ude0a\n","214c1b35":"\n\n\ud83c\udf1f Kaggle has been a temple for Data Scientists , Deep Learning Researchers and aspirants like me. This notebook is aimed at drawing comparision between the presence of kaggle on twitter in the year 2010 and that in the current year 2021 ,via Tweet Analysis. Before you scroll through the notebook , let me tell you a bit about the Data quickly.\n\n```\n 1.The dataset contains tweets regarding \"Kaggle\" from verified twitter accounts. \n 2.It has a record of nearly 11 years , i.e, from 2010 to 2021.\n 3.These tweets have been scraped using Twint.\n\n```\n\ud83c\udf1f In this notebook , I have done analysis with English as the base language , for now. Will include updated analysis results in a revised version.\n\n\ud83c\udf1f All right , that's all you need to do. Let's start ! ","4b0a5eb9":"# **Top 20 Tweeters 2021**","c9e6458d":"# **Top Hashtags of The Year 2010**","ddc72776":"# **Total Tweets Segregated Hourwise 2021**","7d19954f":"# **Word Clouds for Negative Tweets 2010**","8e57fc49":"# **Installing Necessary Libraries and Importing Them**","54ead8ae":"# **Loading Files**","c40b05bd":"# **Sentiment Analysis Of Tweets 2010**","41315d1e":"# **Monthly Count of Total Tweets 2010**","8b72aeaf":"> In 2010 , when kaggle had just started , most of the people who tweeted about it were using english as a medium to share their experience. The incoming tweets were observed to increase when a new competition was launched and also , during late night hours. Most people used to tweet about an ongoing competition or perhaps something related to statistics as the word 'stats' came across a lot. If ,an overall image were to be formed for this year, it would probably have the names of the famous competitions in this year and a lot of congratulatory messages(which can be seen in the word cloud of 2010). This year can be called the year of competitions according to me.\n\n> In 2021, the Japanese language was the language which was used more than often to share one's experience w.r.t kaggle. It was followed by English very closely. One cannot draw a conclusion or hypothesize any theory on the increase of tweets in a particular month as the months so far have been mostly neck to neck. The total number of incoming tweets increased by a huge margin and most of them seem to be coming in the time period of 8am to 4pm. Most of the people seem to be talking about learning , competitions and perhaps congratulating the newly annointed Grandmasters. The hastags also confirm this story as they seem to be dominated by challenges such as 100daysofml , which help people learn new things in the field of ML. Also, the topic of discussions have shifted from statistics to machine learning and AI.\n\n **So,one can say that 2010 was all really about the competitions held on kaggle and statistics, whereas 2021 has become all about learning new concepts,the titles of masters and grandmasters , competitions, Machine Learning and Artificial Intelligence.**","483f6e04":"<h1 id='sec5' style = 'background-color:black;color:white;font-family:Ubuntu'><center> Year in Review : 2021<\/center> <\/h1>","57f45624":"We will be deleting these values.","6920ee0c":"# **Monthly Count of Total Tweets 2021**","fe4e0c78":"\n\n1. **Most Frequently Used Languages** \n    *  English is the most frequently used language as it appeared in more than 96% of Data, followed by an unknown language code 'und' which had only 5 tweets to its name.\n\n2. **Top 20 Tweeters** \n    * I used User Ids instead of names for the sake of privacy. From the Graph , we can clearly see which user id was the most active on tweeting about Kaggle.\n\n3. **Peak Months** \n    * The most tweets were recorded in the month of Novemeber , followed by October , August and June. Below I have discussed possible reasons for trends in these particular months:\n        * November : The winners of the INFORMS Data Mining Contest were honoured in Austin, Texas in November in a huge event.\n        * October : Results for INFORMS Data Mining Contest and HIV Progression Prediction Competition were announced. \n        * June : A Dataset was relased by JPMorgan and other leading Investment Banks on Kaggle , challenging Data Scientists to predict the winner for the 2010 Fifa World Cup.It was between 3th June - 6th June. The Fifa World Cup started on the 11th of June. \n        * I tried looking for explanation for the other months but couldn't find any , so far. Will Update once I find something.\n\n4. **Most Frequent Hours for incoming Tweets**\n    * The Twitter Data clearly shows that even a decade back , people were more active on twitter during Night Time. \n    *  The late night hours between 10 pm to 2 am have more high frequencies of tweets than any other period observed. \n    *  The period b\/w 12pm to 8 pm registered the least amount of tweets. This could be because of the office hours.\n    \n5. **Top 30 Keywords** \n    * This list is topped by the word competition and is followed by words like data mining , bioinformatics , hiv and goldbloom.\n    * The word data mining is in the top because of the INFORMS Data Mining Event , which I covered in the Peak Months.\n    * Anthony Goldbloom is the CFO and CEO of Kaggle and Kaggle started in the year 2010.\n    * Bioinformatics and HIV Progression were competitions which were launched by Kaggle.\n\n6. **Topic Modelling**\n    *  Through Topic Modelling few important topics of discussions were identified most of which were talks about competitions. They are as follows:\n          * Melbourne : A competition for predicting the outcome of grants for University of Melbourne was launched.\n          * HIV : I already discussed this.\n          * Rest topics found were either a link to the Kaggle Website or a thanks statement or perhaps appreciation of any competition.     \n          \n7. **Top 30 Hashtags**\n   * The top most hashtag was of course of the word kaggle . It was followed closely by other words such as chess , world cup , gov2au , kdnuggets , etc.\n   * gov2au : Another Data Science Competition related to Travel Analysis was launched by the Government of Australia. People were tasked with predicting the Travel Time on Sydney's M4 freeway.","7df177f1":"# **Preprocessing and Cleaning The Data**","dd816328":"<h1 id = 'sec2' style = 'background-color:black;color:white;font-family:Ubuntu'><center>Time to Clean : Preprocessing and Preparing Data For Analysis<\/center> <\/h1>","26d86d24":"<h1 id = 'sec1' style = 'background-color:black;color:white;font-family:Ubuntu'><center>Libraries and Methods<\/center> <\/h1>  ","f30912af":"# **Negative Word Cloud 2021**"}}