{"cell_type":{"26cca5b9":"code","4c4e8400":"code","863ea2b8":"code","b36dee1e":"code","5418028f":"code","0be8debe":"code","83f489a3":"code","1c2f3f95":"code","54efa412":"code","a1d602fa":"code","1f6c652d":"code","6aa6c421":"code","87c1ef6d":"code","094c8a7b":"code","7d0d6f2f":"code","3367322b":"code","1ac3eeaa":"code","c25a7ed7":"code","075cfcb5":"code","afa6c215":"code","f59744b1":"code","dd810ada":"code","493af281":"code","24b96783":"markdown","cd3e1971":"markdown","d3c3091f":"markdown","dd5dee6c":"markdown","b684bee8":"markdown","db5187b4":"markdown","e136f3fb":"markdown","8ba11b65":"markdown","55055867":"markdown","e00a5535":"markdown","141a1bce":"markdown","2622bfd4":"markdown","bd158fee":"markdown"},"source":{"26cca5b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c4e8400":"import cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","863ea2b8":"train = pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')","b36dee1e":"train.head()","5418028f":"plt.figure(figsize=(10,5))\nsns.countplot(train['label'])","0be8debe":"y_train = train['label']\ny_test  = test['label']\ndel train['label']\ndel test['label']","83f489a3":"lB = LabelBinarizer()\ny_train = lB.fit_transform(y_train)\ny_test  = lB.fit_transform(y_test)","1c2f3f95":"X_train = train.values\nX_test  = test.values","54efa412":"X_train = X_train \/ 255\nX_test  = X_test \/ 255","a1d602fa":"X_train = X_train.reshape(-1, 28, 28, 1)\nX_test  = X_test.reshape(-1, 28, 28, 1)","1f6c652d":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","6aa6c421":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)","87c1ef6d":"X_train.shape, X_test.shape, X_valid.shape, y_train.shape, y_test.shape, y_valid.shape","094c8a7b":"f, ax = plt.subplots(2,5)\nf.set_size_inches(10,10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(X_train[k].reshape(28,28), cmap='gray')\n        k += 1\n    plt.tight_layout() ","7d0d6f2f":"dataGen = ImageDataGenerator(rotation_range=10,\n                             zoom_range=0.1,\n                             width_shift_range=0.1,\n                             height_shift_range=0.1)\n\ndataGen.fit(X_train)","3367322b":"model = Sequential()\nmodel.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 512 , activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units = 24 , activation = 'softmax'))\n\n\nmodel.summary()","1ac3eeaa":"model.compile(optimizer = 'adam',\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])","c25a7ed7":"history = model.fit(dataGen.flow(X_train,y_train, batch_size = 128) ,epochs = 5 , \n                    validation_data = (X_valid, y_valid))","075cfcb5":"print(\"Test accuracy of the model:- \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","afa6c215":"plt.figure(1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training','validation'])\nplt.title('Loss')\nplt.xlabel('epoch')\nplt.figure(2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training','validation'])\nplt.title('Accuracy')\nplt.xlabel('epoch')\nplt.show()","f59744b1":"className = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G',\n             7:'H', 8:'I', 9:'K', 10:'L', 11:'M', 12:'N',\n             13:'O', 14:'P', 15:'Q', 16:'R', 17:'S', 18:'T', 19:'U',\n             20:'V', 21:'W', 22:'X', 23:'Y'}","dd810ada":"predictions = model.predict_classes(X_test)","493af281":"plt.figure(figsize=(10,10))\nfor i in range(15):\n    plt.subplot(3,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n    plt.title(className[np.argmax(y_test[i])])\n    plt.xlabel(className[predictions[i]])\nplt.show()","24b96783":"## Spliting data for Validation\n* It is always good to have validation data so that our model wont see test data.","cd3e1971":"## Visualizing few prediction with therir actual label","d3c3091f":" # ** ############# Data Augmentation ################**\n* with data augmentation we can save us from overfitting.","dd5dee6c":"## Plotting Loss\/Accuracy Graph","b684bee8":"## Reshaping \nNeural network entertains values in 3D like(pixel, pixle, color_channel).","db5187b4":"## Model Creation","e136f3fb":"## Visualizing training dataset","8ba11b65":"## Normalizing\nAs we know our machine knows values 0 & 1 so we have to convert our values b\/w 0 & 1.\n","55055867":"* Number of classes are pretty equal.\n* Now we can easily Seprate dependent variable from independent variables.","e00a5535":"## Model Testing","141a1bce":"## Encoding\n* Labels to OneHot.\n* Images to values.","2622bfd4":"## Visualizing Few images from training set\n","bd158fee":"## Importing Libraries"}}