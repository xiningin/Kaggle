{"cell_type":{"e8fff680":"code","545f0e21":"code","09ff176c":"code","16a1ceb9":"code","098aaab7":"code","3ec80292":"code","fa007bbc":"code","58723df4":"code","e4880d58":"code","31955c3a":"code","a7dde4ac":"code","2d92beb6":"code","7aefd6ba":"code","c9b00b5b":"code","79ccc5aa":"code","8101d39c":"code","901686d5":"code","4ccf59bb":"code","45835c23":"code","ba67d000":"code","0274e7dd":"code","af02030e":"code","5fd11479":"code","3f50d6ec":"code","196afe8f":"code","7407f815":"code","c2966754":"code","c2437741":"code","0dba0fd6":"code","2d0819c2":"markdown","e340c2a5":"markdown","39bcc827":"markdown","f3fa1782":"markdown","074a5e86":"markdown","2187c416":"markdown","a6035839":"markdown","fb4bcecb":"markdown","6f8acea7":"markdown","4dbd93ca":"markdown","43b8f9a4":"markdown","b35d1349":"markdown","d0a5f802":"markdown","d426dc7b":"markdown","ef48693b":"markdown","468b692e":"markdown","b5536aff":"markdown","2308bd3f":"markdown","dc41ec3d":"markdown","6c7c2e08":"markdown","817288d4":"markdown","1a191a37":"markdown","9faf4269":"markdown","c89ee695":"markdown"},"source":{"e8fff680":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns","545f0e21":"from sklearn.datasets import load_breast_cancer","09ff176c":"cancer=load_breast_cancer()","16a1ceb9":"cancer.keys()","098aaab7":"df1=pd.DataFrame(cancer['data'], columns=cancer.feature_names)","3ec80292":"df2=pd.DataFrame(cancer.target, columns=['Type of Cancer'])","fa007bbc":"df=pd.concat((df1,df2),axis=1)","58723df4":"df.head(5)","e4880d58":"#Now in our Dataset we have 30 input features\nprint('Number of Input Features Originally:',len(cancer.feature_names))","31955c3a":"from sklearn.preprocessing import StandardScaler","a7dde4ac":"scaler=StandardScaler()","2d92beb6":"inp_feat=df.iloc[:,:-1]","7aefd6ba":"scaled_data=scaler.fit_transform(inp_feat)","c9b00b5b":"scaled_df=pd.DataFrame(scaled_data,columns=cancer.feature_names)","79ccc5aa":"scaled_df.head(5)","8101d39c":"from sklearn.decomposition import PCA","901686d5":"pca=PCA()\nPrin_comp=pca.fit_transform(scaled_df)\nfig=px.line(x=range(1,31),y=np.cumsum(pca.explained_variance_ratio_))\n\n\nfig.update_layout(\n    title=\"Scree Plot: % Variance(Data) covered Vs Number of Features\",\n    xaxis_title=\"Number of Features\",\n    yaxis_title=\"Variance Covered (in %)\",\n    legend_title=\"Type\",\n    font=dict(family=\"Arial\",size=15),\n    showlegend=False,\n    grid_rows=True,\n    \n)\n\nfig.show()","4ccf59bb":"fig,axes=plt.subplots(figsize=[17,11])\nsns.heatmap(scaled_df.corr(), cmap='YlGnBu');","45835c23":"pca=PCA(n_components=10)\nnew_data=pca.fit_transform(scaled_df)\nPCA_df=pd.DataFrame(new_data,columns=['PC_1','PC_2','PC_3','PC_4','PC_5','PC_6','PC_7','PC_8','PC_9','PC_10'])\nPCA_df=pd.concat((PCA_df,df2),axis=1)\nPCA_df.head(5)","ba67d000":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score,plot_confusion_matrix\n","0274e7dd":"LR=LogisticRegression()","af02030e":"X=df.iloc[:,:-1]\ny=df.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8,random_state=42)","5fd11479":"model=LR.fit(X_train,y_train)\ny_pred=model.predict(X_test)","3f50d6ec":"print('Accuracy Score:',accuracy_score(y_test,y_pred))\nprint('\\nClassification Report:')\nprint('\\n',classification_report(y_test,y_pred))","196afe8f":"plot_confusion_matrix(LR,X,y, colorbar=False);","7407f815":"X=PCA_df.iloc[:,:-1]\ny=PCA_df.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8,random_state=42)","c2966754":"LR1=LogisticRegression()\nLR1_model=LR1.fit(X_train,y_train)\ny_pred=LR1_model.predict(X_test)\n","c2437741":"print('Accuracy Score:',accuracy_score(y_test,y_pred))\nprint('\\nClassification Report:')\nprint('\\n',classification_report(y_test,y_pred))","0dba0fd6":"plot_confusion_matrix(LR1,X,y, colorbar=False);","2d0819c2":"### Orginal Dataframe: Train - Test Split","e340c2a5":"## Step #3: Computing the Covariance Matrix","39bcc827":"In Target Column - Type of Cancer:\n\n0 Denotes WDBC-Malignant Cancer   \n1 Denotes WDBC-Benign Cancer","f3fa1782":"\n- Used for Dimensionality Reduction\n- Reduces the number of features, so that model can be trained quickly and accurately.\n- Helps find highly correlated features\n- Can be used in model overfitting situations (Accuracy Obtained on Training Data > Accuracy obtained on Testing Data)\n- PCA is not an algorithm, rather a unsupervised ML technique that can be used along with different Algorithms for reducing the number of features","074a5e86":"# How to select which should be my PC1 and which should be my second PC2 and so on like that?  ","2187c416":"- Original df\n- PCA df\n\nAnd we would check the accuracy we are able to obtain through these 2 datasets","a6035839":"### PCA DataFrame: Train - Test Split","fb4bcecb":"From the diagram above, it can be seen that 10 principal components explain almost 95.4% of the variance in data and 7 principal components explain around 91% of the variance in data.\n\nSo, instead of giving all the columns as input, we would only feed these 10 principal components of the dataset to the machine learning algorithm and we\u2019d obtain a similarly accurate results.\n","6f8acea7":"1) We can now observe the difference by ourselves. We are rather able to get better accuracy of our \npredictions with lesser number of features or only using the most useful principal components that we\nare able to derive through PCA.\n\n2) In our Orginal dataframe we are able to obtain around 96% accuracy while in our PCA datadrame we are \nable to achive around 98% accuracy","4dbd93ca":"### We can observe that features are very highly correlated and hence it would be better to train our model by after removing the highly correlated feature and that's what exactly PCA does.","43b8f9a4":"# INSIGHTS AND SUMMARY:","b35d1349":"## <h> Step #1 : Standardization of Data <\/h>","d0a5f802":"## Step #2: Computing the optimum no.of PC using Scree Plot, to be given as input for PCA","d426dc7b":"# We would now use the Step by Step Eigen approach to perform PCA;","ef48693b":"## Step #4 Now we would go ahead and apply these components for our PCA","468b692e":"In Target Column - Type of Cancer:\n\n0 Denotes WDBC-Malignant Cancer   \n1 Denotes WDBC-Benign Cancer","b5536aff":" - To determine this we would use the concept of <b>Explained Variance Ratio<\/b>\n - Explained Variance Ratio: It essentially represents the amount of Variance, each PC is able to cover\n - For example, suppose: the square of distances of all the points from the origin that lie on PC1 is 100 and the square of distances of all the points from the origin that lie on PC2 is 10.\n\nEVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{100}{110}=0.91 $\n\nEVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{10}{110}=0.09 $\n\n\nThus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n\n- Thus we can say that PC1 itself covers 91% of data and hence we can just go ahead and train our model with PC1 to get decently accurate predictions.\n","2308bd3f":"- They are the newly <b>Derived Features<\/b> from the existing features of Dataset, covering maximum variance (i.e covering the maximum knowledge from most of the original features)\n- There can be multiple principle components that we can derive. We can determine that number using Scree Plot\n- PC1 would cover maxm. variance, PC2 would cover variance lesser than PC1 and so on like this.\n- Each Principal Component would be perpendicular to each other.","dc41ec3d":"### We now have 2 Dataframes:","6c7c2e08":"# Optimum Number of Principal Components Required:","817288d4":"# PCA (Principal Component Analysis):","1a191a37":"# Practical Implementation:","9faf4269":"# What is Principal Component?","c89ee695":"Scree plots are the graphs that convey how much variance is explained by corresponding Principal components. \n<img src='https:\/\/github.com\/imamanmehrotra\/Images_for_projects\/blob\/main\/scree.PNG?raw=true' width=\"500\">\n\nAs shown in the given diagram, around 75 principal components explain approximately 90 % of the variance. Hence, 75 can be a good choice based on the usecase to choose the most optimum number of Principal Components out of 400 original features"}}