{"cell_type":{"4fd8967b":"code","61b2ac71":"code","692b23c4":"code","ffb9d9a8":"code","a1d61d34":"code","3a9fc574":"code","3fc86158":"code","876a05e6":"code","1b63aad5":"code","07582660":"code","a8c04adb":"code","56c66df5":"code","c3aca366":"code","113fecb8":"code","2dbda7e3":"code","db740a3f":"code","4969da91":"code","e6916f27":"code","cd34bf7c":"code","092c300b":"code","6f799222":"code","ad0d387d":"code","3c4b1ed7":"code","5bf49eef":"code","9eb75684":"code","d071a49e":"code","6c9b9f2e":"code","c8c76786":"code","540870ed":"code","7927f447":"code","b3bc3a32":"code","b0aae176":"code","e694987b":"code","dbbae92f":"code","9713fcaf":"code","19c33dce":"code","02dda5ad":"code","19e7f85f":"code","53c12b8b":"code","a6d8a87f":"code","b189f98a":"code","5b42d038":"code","3bf78066":"code","18a06e0b":"code","cf12040c":"code","854b0f5e":"code","b713ee48":"code","c900a8d3":"code","fdd86230":"code","d8201f77":"code","d78a5472":"code","10a49034":"code","56c15d3e":"code","dcbae61a":"code","93d9e825":"code","05c40ae1":"code","bcb142cf":"markdown","dd7ec317":"markdown"},"source":{"4fd8967b":"# Explanatory Data Analysis\n# Remove outliers\n# Remove NaN\n# Add Dummy Variables\n# Imputer\n# Standardization\/Scaling\n# Pipeline\n# Grid Search\n# Test\n# Upload","61b2ac71":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","692b23c4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","ffb9d9a8":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ndf_validation = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","a1d61d34":"df.head()","3a9fc574":"df.describe()","3fc86158":"df.dtypes","876a05e6":"df.info()","1b63aad5":"df.isnull().sum()","07582660":"df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a8c04adb":"df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","56c66df5":"df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c3aca366":"df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","113fecb8":"fig = plt.figure(figsize=(18,6))      ## To get a figure with proper structure\n\ndf.Survived.value_counts().plot(kind=\"bar\",alpha=0.5)  ## Setting alpha as per transparency","2dbda7e3":"df[df['Embarked'].isnull()]","db740a3f":"sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=df);","4969da91":"df[\"Embarked\"].fillna('C')","e6916f27":"df.Cabin.value_counts()","cd34bf7c":"df.Cabin.str[0].value_counts()","092c300b":"sns.catplot(x=\"Sex\", y=\"Survived\", data=df, kind='bar');","6f799222":"\nsns.swarmplot('Survived', 'Age', data=df)","ad0d387d":"df.Embarked.value_counts(normalize=True).plot(kind=\"bar\", alpha = 0.5)","3c4b1ed7":"g = sns.FacetGrid(df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","5bf49eef":"grid = sns.FacetGrid(df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","9eb75684":"sns.boxplot('Pclass', 'Fare', data=df)","d071a49e":"# Convert data types\ndef convertDataTypes(df):\n    df.Pclass = df.Pclass.astype('object')\n\n    df['Fare_binned'] = pd.cut(x = df.Fare, bins = 10, labels = range(10))\n    df['Age'] = pd.cut(x = df['Age'], bins=5, labels=range(5))\n    \n    df[\"FamilySize\"] = df['SibSp'] + df['Parch']\n    df[\"LargeFamily\"] = df['FamilySize'] > 2\n    df[\"Alone\"] = df['FamilySize'] == 0\n    df['HasBoardedSiblingsOrSpouses'] = df['SibSp'] >= 1\n    df = df.drop('SibSp', axis=1)\n    df['HasBoardedParentsChildren'] = df['Parch'] >= 1\n    df = df.drop('Parch', axis=1)\n    \n    df['Deck'] = df['Cabin'].str[0]\n    df['Deck'] = df['Deck'].fillna(\"Unknown\")\n    #df = pd.concat([df.drop('Deck', axis=1), pd.get_dummies(df['Deck'], prefix='Deck')], axis=1)\n    #df = df.drop('Cabin', axis=1)\n    \n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    \n    return df","6c9b9f2e":"df = convertDataTypes(df)\ndf_validation = convertDataTypes(df_validation)\n\ndf.Survived = df.Survived.astype('int')","c8c76786":"df.FamilySize.hist()","540870ed":"df.Sex.value_counts()","7927f447":"df","b3bc3a32":"df.isnull().sum()","b0aae176":"df.index","e694987b":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize=(20, 16))\ncorr = df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax, annot=True)","dbbae92f":"df.corr()","9713fcaf":"# Removed Fare, added title\nfeatures = ['Sex', 'Pclass', 'Fare_binned', \"Embarked\", \"Deck\", \"Title\", \"Alone\",\"HasBoardedParentsChildren\", \"HasBoardedSiblingsOrSpouses\"]\n\ntarget = 'Survived'","19c33dce":"X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.25, random_state=5)\nX_train.head()","02dda5ad":"X_test","19e7f85f":"X_train.dtypes","53c12b8b":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\nnumeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X_train.select_dtypes(include=['object']).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('classifier', RandomForestClassifier())\n                          ])\n","a6d8a87f":"pipeline.fit(X_train, y_train)","b189f98a":"preds = pipeline.predict(X_test)\nnp.mean(preds == y_test)","5b42d038":"from sklearn.metrics import accuracy_score,recall_score,confusion_matrix\n\nprint(len(X_test))\nprint(accuracy_score(y_test,preds))\nprint(recall_score(y_test,preds))\nprint(confusion_matrix(y_test,preds))","3bf78066":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    xgb.XGBClassifier(),\n    LogisticRegression()\n]\n\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('classifier', classifier)])\n    pipe.fit(X_train, y_train)   \n    print(classifier)\n    print(\"model score: %.3f\" % pipe.score(X_test, y_test))","18a06e0b":"'''\n# Plot coeficients\nlogreg = LogisticRegression()\npipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', logreg)])\npipe.fit(X_train, y_train)   \nprint(\"model score: %.3f\" % pipe.score(X_test, y_test))\n    \nY_pred = pipe.predict(X_test)\nacc_log = round(pipe.score(X_train, y_train) * 100, 2)\nprint(acc_log)\n\ncoeff_df = pd.DataFrame(df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)\n'''","cf12040c":"from sklearn.model_selection import RandomizedSearchCV\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestClassifier(random_state=5))])\n\n\n#Random Forest\nparam_grid = { \"classifier__criterion\" : [\"gini\", \"entropy\"], \n              \"classifier__min_samples_leaf\" : [5, 10, 25, 50], \n              \"classifier__min_samples_split\" : [2, 4, 10, 12, 16, 18], \n              \"classifier__n_estimators\": [25,50,100,200]}\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# ADA\n#param_grid = {'classifier__n_estimators':[50,100,250,500,1000,2000],'classifier__learning_rate':[.001,0.01,.1,1]}\n\n# Gradient Boosting\n'''\nparam_grid = {\n    \"classifier__loss\":[\"deviance\"],\n    \"classifier__learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"classifier__min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"classifier__min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n    \"classifier__max_depth\":[3,5,8],\n    \"classifier__max_features\":[\"log2\",\"sqrt\"],\n    \"classifier__criterion\": [\"friedman_mse\",  \"mae\"],\n    \"classifier__subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"classifier__n_estimators\":[10]\n}\n# XGBoost\nparam_grid = {\n        'classifier__min_child_weight': [1, 5, 10],\n        'classifier__gamma': [0.5, 1, 1.5, 2, 5],\n        'classifier__subsample': [0.6, 0.8, 1.0],\n        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n        'classifier__max_depth': [3, 4, 5]\n}\n'''\n\n# create the GridSearchCV object\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', refit=True, verbose=1)\n#grid_search = RandomizedSearchCV(estimator = pipeline, param_distributions = param_grid, n_iter = 500, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n\n# fine-tune the hyperparameters\ngrid_search.fit(X_train, y_train)","854b0f5e":"# get the best model\nfinal_model = grid_search.best_estimator_\nprint(grid_search.best_params_)","b713ee48":"final_model","c900a8d3":"feature_importances = final_model.named_steps['classifier'].feature_importances_","fdd86230":"final_model.named_steps['preprocessor'].transformers_[1][1]\\\n   .named_steps['onehot'].get_feature_names()","d8201f77":"# predict using the test dataset\npreds = final_model.predict(X_test)","d78a5472":"print(accuracy_score(y_test,preds))\nprint(recall_score(y_test,preds))\nprint(confusion_matrix(y_test,preds))","10a49034":"df_validation","56c15d3e":"final_preds = final_model.predict(df_validation[features])\n#len(df_validation)\n#len(final_preds)","dcbae61a":"submission = pd.DataFrame({'PassengerId':df_validation.index,'Survived':final_preds})\n\n#Visualize the first 5 rows\nsubmission.Survived = submission.Survived.astype('int')","93d9e825":"submission.dtypes","05c40ae1":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","bcb142cf":"> # Final run","dd7ec317":"## Check different models"}}