{"cell_type":{"cf1449e7":"code","ed71d1cc":"code","f3f6608a":"code","5d75d3dc":"code","12d67db6":"code","22e17e2c":"code","879550b5":"code","c48f05fa":"code","0c2ad70c":"code","ec3b6589":"code","0f7f91b1":"code","ea53a532":"code","08b20919":"code","10f39c48":"code","79bca5f8":"code","41479c82":"code","47de9e39":"code","f0f934de":"code","0c5b31f3":"markdown","85915efb":"markdown","155078cf":"markdown","65eab93d":"markdown","7d564849":"markdown"},"source":{"cf1449e7":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null","ed71d1cc":"import numpy as np  \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport janestreet\nimport tensorflow as tf\nimport tensorflow.keras.backend as k\nimport datatable as dt\nimport gc\nimport random\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport tensorflow_addons as tfad","f3f6608a":"seed=1111\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)","5d75d3dc":"full_train=dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\n#full_train=pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv',low_memory=False,nrows=1.5*10**6)\n#full_train=full_train.sort_values(['date', 'ts_id'], ascending=[True, True])\nfull_train.info()","12d67db6":"# contain_null_indices=full_train.columns[pd.isnull(full_train).any()]\n# for item in contain_null_indices:\n#     if item.split('_')[0]=='feature':\n#         pass\n#     else:\n#         print(item)\n# #nothing prints means all null are in feature named columns...","22e17e2c":"# feature_info=pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\n# print('null_info : ',feature_info.columns[pd.isnull(feature_info).any()]) # no nulls in feature_info\n\n# mean_value_dict={}            #lets assume these are the values to be filled in the nan portion\n# for row in feature_info.values:\n#     mean_value_dict[row[0]]=np.mean(row[1:])\n    \n# full_train.fillna(mean_value_dict,inplace=True)","879550b5":"full_train=full_train.query('date>85').reset_index(drop=True)\nfull_train=full_train[full_train['weight']!=0.0]\ntrain_mean_dict=dict(full_train.drop(['resp','resp_1','resp_2','resp_3','resp_4','date','ts_id'],axis=1).mean())\nfull_train=full_train.fillna(train_mean_dict)","c48f05fa":"# scaler=MinMaxScaler()\n# x=scaler.fit_transform(x)\n\n# from sklearn.ensemble import RandomForestClassifier\n# clf=RandomForestClassifier(n_estimators=100)\n# clf.fit(x[:100000],y[:100000,0])\n\n# features=full_train.drop(['resp','resp_1','resp_2','resp_3','resp_4','date','ts_id'],axis=1).columns\n# feature_importance = {features[i]: clf.feature_importances_[i] for i in range(len(features))}\n# imp=dict(sorted(feature_importance.items(), key=lambda item: item[1]))\n\n# plt.figure(figsize=(10,30))\n# plt.barh(range(len(imp)),imp.values())\n# plt.ylabel('feature importances')\n# plt.xlabel('features available')\n# _=plt.yticks(range(len(imp)),imp.keys())\n# plt.grid(True)\n# plt.savefig('feature_importances.png')\n","0c2ad70c":"features=['feature_0','feature_97','feature_103','feature_85','feature_91','feature_94', 'feature_106', 'feature_100', 'feature_88',\n       'feature_76', 'feature_99', 'feature_93', 'feature_79',\n       'feature_82', 'feature_73', 'feature_109', 'feature_105',\n       'feature_87', 'feature_115', 'feature_112', 'feature_118',\n       'feature_15', 'feature_9', 'feature_29', 'feature_10',\n       'feature_25', 'feature_1', 'feature_81', 'feature_2', 'feature_75',\n       'feature_35', 'feature_26', 'feature_52', 'feature_23',\n       'feature_17', 'feature_18', 'feature_19', 'feature_16',\n       'feature_20', 'feature_13', 'feature_33', 'feature_69',\n       'feature_14', 'feature_36', 'feature_27', 'feature_117',\n       'feature_22', 'feature_21', 'feature_111', 'feature_24',\n       'feature_30', 'feature_71', 'feature_48', 'feature_80',\n       'feature_34', 'feature_49', 'feature_46', 'feature_11',\n       'feature_12', 'feature_50', 'feature_104', 'feature_28',\n       'feature_47', 'feature_31', 'feature_8', 'feature_32',\n       'feature_92', 'feature_86', 'feature_7', 'feature_98',\n       'feature_122', 'feature_74', 'feature_128', 'feature_114',\n       'feature_102', 'feature_116', 'feature_53', 'feature_96',\n       'feature_110','feature_108', 'feature_70', 'feature_51',\n       'feature_72', 'feature_90', 'feature_78', 'feature_84',\n       'feature_126', 'feature_129', 'feature_123', 'feature_54',\n       'feature_124', 'feature_65', 'feature_127', 'feature_66',\n       'feature_68', 'feature_56', 'feature_120', 'feature_59',\n       'feature_67', 'feature_64', 'feature_125', 'feature_119',\n       'feature_113', 'feature_101', 'feature_107', 'feature_95',\n       'feature_58', 'feature_121', 'feature_89', 'feature_57',\n       'feature_39', 'feature_40', 'feature_55', 'feature_38',\n       'feature_37', 'feature_62', 'feature_4', 'feature_77',\n       'feature_63', 'feature_60', 'feature_61', 'feature_3',\n       'feature_42', 'feature_83', 'feature_41', 'feature_45',\n       'feature_6', 'feature_44', 'feature_5', 'feature_43']\n#weight only removed","ec3b6589":"x=full_train.loc[:,features]\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3','resp_4']\ny= np.stack([(full_train[col] > 0).astype('int') for col in resp_cols]).T","0f7f91b1":"# losses=[tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n#        tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n#        tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n#        tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n#        tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2)]\n# loss_weights=[1.0,0.3,0.3,0.3,0.3]","ea53a532":"# class SquareRootScheduler:\n#     def __init__(self, lr=0.1):\n#         self.lr = lr\n\n#     def __call__(self, num_update):\n#         return self.lr * pow(num_update + 1.0, -0.5)","08b20919":"inputs = tf.keras.layers.Input(shape=x.shape[1:])\n#encoded = tf.keras.layers.BatchNormalization()(inputs)\n#encoded = tf.keras.layers.GaussianNoise(0.1)(encoded)\n#encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n#decoded = tf.keras.layers.Dropout(0.2)(encoded)\n#decoded = tf.keras.layers.Dense(x.shape[1],name='decoded')(decoded)\n\n############################################################################\n\n#inp_norm=tf.keras.layers.BatchNormalization()(decoded)\ninp_norm=tf.keras.layers.BatchNormalization()(inputs)\ninp_drop=tf.keras.layers.Dropout(0.2)(inp_norm)\n\ndense1=tf.keras.layers.Dense(160)(inp_drop)\nnorm1=tf.keras.layers.BatchNormalization()(dense1)\nact1=tf.keras.layers.Activation(tf.keras.activations.swish)(norm1)\ndrop1=tf.keras.layers.Dropout(0.2)(act1)\n\ndense2=tf.keras.layers.Dense(160)(drop1)\nnorm2=tf.keras.layers.BatchNormalization()(dense2)\nact2=tf.keras.layers.Activation(tf.keras.activations.swish)(norm2)\ndrop2=tf.keras.layers.Dropout(0.2)(act2)\n\ndense3=tf.keras.layers.Dense(160)(drop2)\nnorm3=tf.keras.layers.BatchNormalization()(dense3)\nact3=tf.keras.layers.Activation(tf.keras.activations.swish)(norm3)\ndrop3=tf.keras.layers.Dropout(0.2)(act3)\n\nout=tf.keras.layers.Dense(5, activation='sigmoid',name='out')(drop3)\n#model=tf.keras.models.Model(inputs=[inputs],outputs=[decoded,out],name='my_baseline_model')\nmodel=tf.keras.models.Model(inputs=[inputs],outputs=[out],name='my_baseline_model')\n\n#model.compile(loss={'decoded':'mse','out':'binary_crossentropy'},optimizer=tf.keras.optimizers.Adam(lr=0.001,decay=0.0001))\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),optimizer=tfad.optimizers.RectifiedAdam(learning_rate=1e-3))\n\nmodel.summary()","10f39c48":"# class print_lr_callback(tf.keras.callbacks.Callback):\n#     def on_epoch_begin(self,epoch,logs=None):\n#         print('lr now : ',self.model.optimizer.lr.numpy())","79bca5f8":"#history=model.fit(x,(x,y),epochs=100,batch_size=10000,verbose=1)\nhistory=model.fit(x,y,epochs=200,batch_size=10000,verbose=1)","41479c82":"model.save('model.h5')","47de9e39":"#now for the predictions\njanestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","f0f934de":"filling_dict=train_mean_dict\n\nmodel.call = tf.function(model.call, experimental_relax_shapes=True)\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n        \n    if test_df['weight'].item()>0:\n        test_df=test_df.loc[:,features]\n        for key in features:\n            if np.isnan(np.array(test_df[key])[0]):\n                test_df[key]=filling_dict[key] \n                \n        #preds=model(test_df,training=False)[1].numpy()\n        preds=model(test_df,training=False).numpy()\n        sample_prediction_df.action=(np.mean(preds) > 0.503).astype(int)\n    else:\n        sample_prediction_df.action=0\n    \n\n    env.predict(sample_prediction_df)","0c5b31f3":"![image.png](attachment:image.png)\nthis shows that:   \n1. feature_0 is atually useless.  \n2. i'll drop all the featurs below 0.004  \nwe will work on only the rest of the features ..","85915efb":"# making model","155078cf":"# making feature importances and x,y","65eab93d":"# fillna using feature.csv avg values","7d564849":"# filling using mean values from train.csv"}}