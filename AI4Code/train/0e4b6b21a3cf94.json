{"cell_type":{"32604ab2":"code","c7e76ced":"code","532c30b2":"code","19f3d2f6":"code","70a3a91c":"code","42aed250":"code","353f736d":"code","46fa3a4b":"code","eaba40d7":"code","c07b1bde":"code","07b681ea":"code","8909cc38":"code","e65eba4e":"code","c622a7e5":"code","8cfe2245":"code","d92715fa":"code","81c438b2":"code","442c4f92":"code","2d3439a6":"code","8523f39a":"code","a8006787":"code","47ab66e8":"markdown","cccfbe81":"markdown","0d33c8fc":"markdown","16489ae3":"markdown"},"source":{"32604ab2":"import re\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport numpy as np\nimport pandas as pd\nimport random as rn\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt  \nfrom urllib.parse import urlparse","c7e76ced":"import plotly as ply","532c30b2":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\nmispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n                \"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"havent\" : \"have not\",\n                \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\",\n                \"isn't\" : \"is not\",\"it's\" : \"it is\",\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\",\n                \"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n                \"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n                \"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n                \"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n                \"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\",\n                \"wouldn't\" : \"would not\", \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\",\n                \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns):\n    for col in tqdm(columns):\n        df[col] = df[col].apply(lambda x: re.sub(' +', ' ', x)).values\n        df[col] = df[col].apply(lambda x: re.sub('\\n', '', x)).values\n        df[col] = df[col].apply(lambda x: clean_numbers(x)).values\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x)).values\n        df[col] = df[col].apply(lambda x: clean_text(x.lower())).values\n        df[col] = df[col].apply(lambda x: x.lower()).values\n        df[col] = df[col].apply(lambda x: re.sub(' +', ' ', x)).values\n\n    return df","19f3d2f6":"from sklearn.preprocessing import MinMaxScaler\ndef preprocess_data(train):\n\n  y = train[train.columns[11:]] # storing the target labels in 'y'\n\n  # I'll be cleaning and adding the domain name from the website's url.\n  find = re.compile(r\"^[^.]*\")\n  train['clean_url'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\n  # creating train and test data\n  X = train[['question_title', 'question_body', 'answer', 'host', 'category']]\n  text_features = ['question_title', 'question_body', 'answer']\n\n  # Cleaning data for contracted words, numbers and punctuations.\n  X = clean_data(X, text_features)\n\n  return X","70a3a91c":"X = pd.read_csv('..\/input\/google-quest-challenge\/train.csv').iloc[:, 11:] \nunique_labels = np.unique(X.values)\ndenominator = 60\nq = np.arange(0, 101, 100 \/ denominator)\nexp_labels = np.percentile(unique_labels, q) # Generating the 60 bins.\n\ndef optimize_ranks(preds, unique_labels=exp_labels): \n  new_preds = np.zeros(preds.shape)\n  for i in range(preds.shape[1]):\n    interpolate_bins = np.digitize(preds[:, i], bins=unique_labels, right=False)-1\n    if len(np.unique(interpolate_bins)) == 1:\n      new_preds[:, i] = preds[:, i]\n    else:\n      # new_preds[:, i] = unique_labels[interpolate_bins]\n      new_preds[:, i] = interpolate_bins\n  \n  return new_preds","42aed250":"y_true = pd.read_csv('..\/input\/google-quest-challenge\/train.csv').iloc[:, 11:]\ny_pred = pd.read_csv('..\/input\/google-quest-qna-bert-pred\/pred_train.csv').iloc[:, 1:]","353f736d":"y_true = optimize_ranks(y_true.values)\ny_pred = optimize_ranks(y_pred.values)","46fa3a4b":"# Generating the MSE-score for each data point in train data.\nfrom sklearn.metrics import mean_squared_error\ntrain_score = [mean_squared_error(i,j) for i,j in zip(y_pred, y_true)]","eaba40d7":"# sorting the losses from minimum to maximum imdex wise.\ntrain_score_args = np.argsort(train_score)","c07b1bde":"train = pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\nX_train = preprocess_data(train)","07b681ea":"# function for generating wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nsns.set()\n\ndef generate_wordcloud(indexes, data, color='black'):\n  comment_words = '' \n  stopwords = set(STOPWORDS)\n\n  title_words = data['question_title'].iloc[i]\n  body_words = data['question_body'].iloc[i]\n  answer_words = data['answer'].iloc[i]\n\n  title_cloud = WordCloud(width = 400, height = 200, background_color = color,\n                        stopwords = stopwords, min_font_size = 10).generate(title_words)\n\n  body_cloud = WordCloud(width = 400, height = 200, background_color = color,\n                        stopwords = stopwords, min_font_size = 10).generate(body_words)\n\n  answer_cloud = WordCloud(width = 400, height = 200, background_color = color,\n                        stopwords = stopwords, min_font_size = 10).generate(answer_words)\n  \n  return title_cloud, body_cloud, answer_cloud","8909cc38":"# I've picked the top 5 datapoints from train data with lowest loss and plotted the wordcloud of their question_title, question_body and answer.\nprint('Top 5 data points from train data that give the \"lowest\" loss.')\nfor i, idx in enumerate(train_score_args[:5]):\n  title, body, answer = generate_wordcloud(idx, X_train)\n  plt.figure(figsize=(20,12))\n  plt.subplot(131)\n  plt.imshow(title)\n  if i==0: plt.title('question_title')\n  plt.ylabel(f'loss: {train_score[idx]}')\n  plt.subplot(132)\n  plt.imshow(body)\n  if i==0: plt.title('question_body')\n  plt.subplot(133)\n  plt.imshow(answer)\n  if i==0: plt.title('answer')\n  plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\n  plt.show()","e65eba4e":"# I've picked the top 5 datapoints from train data with 'highest' loss and plotted the wordcloud of their question_title, question_body and answer.\nprint('Top 5 data points from Train data that give the \"highest\" loss.')\nfor i, idx in enumerate(train_score_args[-5:]):\n  title, body, answer = generate_wordcloud(idx, X_train, color='white')\n  plt.figure(figsize=(20,12))\n  plt.subplot(131)\n  plt.imshow(title)\n  if i==0: plt.title('question_title')\n  plt.ylabel(f'loss: {train_score[idx]}')\n  plt.subplot(132)\n  plt.imshow(body)\n  if i==0: plt.title('question_body')\n  plt.subplot(133)\n  plt.imshow(answer)\n  if i==0: plt.title('answer')\n  plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\n  plt.show()","c622a7e5":"# I've picked the top 30 datapoints from train and cv data with 'lowest' loss and plotted the word counts of their question_title, question_body and answer.\nprint(\"word counts of the question_title, question_body and answer of top 30 train and cv data with 'lowest' loss.\")\ni = 30\ntitle_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[:i]]['question_title'].values]\nbody_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[:i]]['question_body'].values]\nanswer_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[:i]]['answer'].values]\n\nplt.figure(figsize=(20,4))\nplt.subplot(131)\nplt.plot(title_train_len)\nplt.title('question_title (train data)')\nplt.ylabel('number of words')\nplt.xlabel('datapoint (loss: high --> low)')\nplt.subplot(132)\nplt.plot(body_train_len)\nplt.title('question_body (train data)')\nplt.xlabel('datapoint (loss: low --> high)')\nplt.subplot(133)\nplt.plot(answer_train_len)\nplt.title('answer (train data)')\nplt.xlabel('datapoint (loss: high --> low)')\n# plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\nplt.show()","8cfe2245":"# I've picked the top 30 datapoints from train and cv data with 'highest' loss and plotted the word counts of their question_title, question_body and answer.\nprint(\"word counts of the question_title, question_body and answer of top 30 train and cv data with 'highest' loss.\")\ni = -30\ntitle_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[i:]]['question_title'].values]\nbody_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[i:]]['question_body'].values]\nanswer_train_len = [len(l.split(' ')) for l in X_train.iloc[train_score_args[i:]]['answer'].values]\n\nplt.figure(figsize=(20,4))\nplt.subplot(131)\nplt.plot(title_train_len)\nplt.title('question_title (train data)')\nplt.ylabel('number of words')\nplt.xlabel('datapoint (loss: high --> low)')\nplt.subplot(132)\nplt.plot(body_train_len)\nplt.title('question_body (train data)')\nplt.xlabel('datapoint (loss: high --> low)')\nplt.subplot(133)\nplt.plot(answer_train_len)\nplt.title('answer (train data)')\nplt.xlabel('datapoint (loss: high --> low)')\n# plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\nplt.show()","d92715fa":"# I've picked the top 100 datapoints from train data with 'highest' loss and collected the values of domain names.\ntop_url = X_train['host'].iloc[train_score_args[:100]].value_counts()\nbottom_url = X_train['host'].iloc[train_score_args[-100:]].value_counts()","81c438b2":"# Top 10 frequently occuring domain names that lead to minimum loss\ntop_url[1:10].plot.bar(figsize=(12,8))\nplt.title('top 10 url domain that produce the minimum loss')\nplt.ylabel('frequency')\nplt.show()","442c4f92":"# Top 10 frequently occuring domain names that lead to maximum loss\nbottom_url[1:10].plot.bar(figsize=(12,8))\nplt.title('top 10 url domain that produce the maximum loss')\nplt.ylabel('frequency')\nplt.show()","2d3439a6":"# finding the unique domain names that contribute to low and high losses\nbest_url = ' '.join(list(set(top_url.keys()) - set(bottom_url.keys()))) # set of urls that contribute solely to low loss\nworst_url = ' '.join(list(set(bottom_url.keys()) - set(top_url.keys()))) # set of urls that contribute solely to high loss","8523f39a":"best_url_cloud = WordCloud(width = 400, height = 200, background_color ='orange',\n                           stopwords = STOPWORDS, min_font_size = 10).generate(best_url)\n\nworst_url_cloud = WordCloud(width = 400, height = 200, background_color ='cyan',\n                            stopwords = STOPWORDS, min_font_size = 10).generate(worst_url)\n\nplt.figure(figsize=(20,12))\nplt.subplot(121)\nplt.imshow(best_url_cloud)\nplt.title('url domain with well predicted labels (low loss)')\nplt.subplot(122)\nplt.imshow(worst_url_cloud)\nplt.title('url domain with bad predicted labels (high loss)')\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\nplt.show()","a8006787":"# for train data\nplt.figure(figsize=(20,20))\nplt.subplot(121)\nX_train['category'].iloc[train_score_args[:100]].value_counts().plot.pie(autopct='%1.1f%%', explode=(0,0.02,0.04,0.06,0.08), shadow=True)\nplt.ylabel('')\nplt.title('categories of best fitted data points with minimum loss (on train data)')\nplt.subplot(122)\nX_train['category'].iloc[train_score_args[-100:]].value_counts().plot.pie(autopct='%1.1f%%', explode=(0,0.02,0.04,0.06,0.08), shadow=True)\nplt.ylabel('')\nplt.title('categories of worst fitted data points with maximum loss (on train data)')\nplt.show()","47ab66e8":"### Next analysis is on the word counts in question_title, question_body and answer.","cccfbe81":"## Overview:\n#### In this section, we will go through an analysis of train data to figure out what parts of the data is the model doing well on and what parts of the data it's not.\n#### The main idea behind this step is to know the capability of the trained model and it works like a charm if applied properly for fine-tuning the model and data.\n#### But we won't get into the fine-tuning part in this section, we will just be performing some basic EDA on the train data using the predicted target values for the train data.\n#### I'll be covering the data feature by feature. Here are the top features we'll be performing analysis on-\n- question_title, question_body, and answer.\n- Word lengths of question_title, question_body, and answer.\n- Host\n- Category\n\n#### First, we will have to divide the data into a spectrum of good data and bad data. Good data will be the data points on which the model achieves a good score and bad data will be the data points on which the model achieves a bad score.\u00a0\n#### Now for scoring, we will be comparing the actual target values of the train data with the model's predicted target values on train data. I used mean squared error (MSE) as a metric for scoring since it focuses on how close the actual and target values are. Remember the more the MSE-score is, the bad the data point will be.","0d33c8fc":"## Challenge description:\n#### In this competition, we're challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. The raters received minimal guidance and training and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task.\n#### Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.\u00b6","16489ae3":"### We'll go through various analysis first starting with word cloud of the question_title, question_body and answer"}}