{"cell_type":{"fc8a8030":"code","1b93ce07":"code","25bfaf10":"code","9a531b2b":"code","67c46e31":"code","77976356":"code","d35cc274":"code","f3cc91ec":"code","cde057e8":"code","683cc2ab":"code","2b6d5d8e":"code","d26afb10":"code","8b6979e1":"markdown","3eaa08ac":"markdown","2423b47c":"markdown","b741d338":"markdown","11930e53":"markdown","ce3bf4eb":"markdown"},"source":{"fc8a8030":"import sklearn\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport json\nimport glob\nimport random\nimport cv2\nimport tqdm\nfrom tqdm import tqdm\nimport re\nimport ast","1b93ce07":"def get_boxes(row):\n    \"\"\"Return the bboxes for a given row as a 3D matrix \"\"\"\n    #if len(row['annotations']) == 0:\n    #    row['annotations'] = [{'x': -1, 'y': -1, 'width': -1, 'height': -1}]\n    return pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values","25bfaf10":"train_df = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')","9a531b2b":"train_df","67c46e31":"output_json_dict = {\n    \"images\": [],\n    \"videos\": [],\n    \"annotations\": [],\n    \"categories\": []\n}\ncategory_dict = {\"id\": 1, \"name\": \"starfish\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\nvideo_0_dict = {\"name\": \"video_0\", \"id\": 0}\noutput_json_dict[\"videos\"].append(video_0_dict)\nvideo_1_dict = {\"name\": \"video_1\", \"id\": 1}\noutput_json_dict[\"videos\"].append(video_1_dict)\n# video_2_dict = {\"name\": \"video_2\", \"id\": 2}\n# output_json_dict[\"videos\"].append(video_2_dict)","77976356":"annot_id = 0\nimg_id = 0\nvideo_0_frames = 0\nvideo_1_frames = 0","d35cc274":"for f in tqdm(train_df.itertuples(), total=len(train_df)):\n    if f[1] != 2:\n        img_path = '..\/input\/tensorflow-great-barrier-reef\/train_images\/video_' + str(f[1]) + '\/' + f[5].split('-')[1] + '.jpg'\n        img = cv2.imread(img_path)\n        height, width, channels = img.shape\n#         frame_id = f[3]\n        video_id = f[1]\n        if video_id == 0:\n            frame_id = video_0_frames\n        else:\n            frame_id = video_1_frames\n\n        img_info = {\n            \"id\": f[0],\n            \"width\": width,\n            \"height\": height,\n            \"file_name\": img_path,\n            \"frame_id\": frame_id,\n            \"video_id\": video_id,\n        }\n        output_json_dict[\"images\"].append(img_info)\n        if f[6] != '[]':\n    #         print(train_df.iloc[f[0]]['annotations'])\n            bbox_list = ast.literal_eval(f[6])\n            for bbox in bbox_list:\n    #             bbox = ast.literal_eval(bbox)\n                if bbox['height'] + bbox['y'] > 720:\n                    bbox['height'] = 720 - bbox['y']\n                annot = {\n                    \"category_id\": 1,\n                    \"bbox\": [bbox['x'], bbox['y'], bbox['width'], bbox['height']],\n                    \"id\": annot_id,\n                    \"image_id\": f[0],\n                    \"area\": bbox['width'] * bbox['height'],\n                    \"segmentation\": [],\n                    \"iscrowd\": 0,\n                    \"video_id\": video_id,\n                }\n                output_json_dict[\"annotations\"].append(annot)\n                annot_id += 1\n        img_id += 1\n        if video_id == 0:\n            video_0_frames += 1\n        else:\n            video_1_frames += 1","f3cc91ec":"with open('train_dataset.json', 'w') as f:\n    output_json = json.dumps(output_json_dict)\n    f.write(output_json)","cde057e8":"output_json_dict = {\n    \"images\": [],\n    \"videos\": [],\n    \"annotations\": [],\n    \"categories\": []\n}\ncategory_dict = {\"id\": 1, \"name\": \"starfish\", \"supercategory\": \"none\"}\noutput_json_dict[\"categories\"].append(category_dict)\n# video_0_dict = {\"name\": \"video_0\", \"id\": 0}\n# output_json_dict[\"videos\"].append(video_0_dict)\n# video_1_dict = {\"name\": \"video_1\", \"id\": 1}\n# output_json_dict[\"videos\"].append(video_1_dict)\nvideo_2_dict = {\"name\": \"video_2\", \"id\": 2}\noutput_json_dict[\"videos\"].append(video_2_dict)","683cc2ab":"annot_id = 0\nimg_id = 0\nvideo_2_frames = 0","2b6d5d8e":"for f in tqdm(train_df.itertuples(), total=len(train_df)):\n    if f[1] == 2:\n        img_path = '..\/input\/tensorflow-great-barrier-reef\/train_images\/video_' + str(f[1]) + '\/' + f[5].split('-')[1] + '.jpg'\n        img = cv2.imread(img_path)\n        height, width, channels = img.shape\n        frame_id = video_2_frames\n        video_id = f[1]\n\n        img_info = {\n            \"id\": img_id,\n            \"width\": width,\n            \"height\": height,\n            \"file_name\": img_path,\n            \"frame_id\": frame_id,\n            \"video_id\": video_id\n        }\n        output_json_dict[\"images\"].append(img_info)\n        if f[6] != '[]':\n    #         print(train_df.iloc[f[0]]['annotations'])\n            bbox_list = ast.literal_eval(f[6])\n            for bbox in bbox_list:\n    #             bbox = ast.literal_eval(bbox)\n                if bbox['height'] + bbox['y'] > 720:\n                    bbox['height'] = 720 - bbox['y']\n                annot = {\n                    \"category_id\": 1,\n                    \"bbox\": [bbox['x'], bbox['y'], bbox['width'], bbox['height']],\n                    \"id\": annot_id,\n                    \"image_id\": img_id,\n                    \"area\": bbox['width'] * bbox['height'],\n                    \"segmentation\": [],\n                    \"iscrowd\": 0,\n                    \"video_id\": video_id,\n                }\n                output_json_dict[\"annotations\"].append(annot)\n                annot_id += 1\n        img_id += 1\n        video_2_frames += 1","d26afb10":"with open('val_dataset.json', 'w') as f:\n    output_json = json.dumps(output_json_dict)\n    f.write(output_json)","8b6979e1":"# **Import Libraries**","3eaa08ac":"# **Helper Functions**","2423b47c":"https:\/\/www.kaggle.com\/julian3833\/reef-starter-torch-fasterrcnn-train-lb-0-361\n\nhttps:\/\/www.kaggle.com\/rhythmcam\/ast-basic-string-expression\n\nhttps:\/\/www.kaggle.com\/vexxingbanana\/sartorius-coco-dataset-notebook\n\nhttps:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/discussion\/293723","b741d338":"# **Create CocoVid Json File**","11930e53":"# This Coco Json File Format is useful for training models with [MMTracking](http:\/\/github.com\/open-mmlab\/mmtracking)","ce3bf4eb":"# **References**"}}