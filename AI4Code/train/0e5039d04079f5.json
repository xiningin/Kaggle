{"cell_type":{"3b7128b5":"code","f39d1093":"code","db941bf2":"code","c598bbda":"code","9d6e30cd":"code","10791d65":"code","e563b072":"code","9497c8c7":"code","34b92d94":"code","33c667d9":"code","486a9872":"code","73f1b0e7":"code","f3863e3f":"code","17dadc2b":"code","dc7efc7a":"code","cf58dba5":"code","1b1d2d7a":"code","e16bc1ac":"code","403db9cd":"code","8649dd3a":"code","fdcd63fb":"code","3fb7dc8d":"code","ef2871d1":"code","44ea02b1":"code","61719a43":"code","fa3e0c92":"code","0bb4d3d8":"code","1cf312e4":"code","eba4cefc":"code","a53cd2c8":"code","b20f0f25":"code","62001ea7":"code","88beac56":"code","bc1ae57b":"code","25651b10":"code","35f32e7b":"code","53dbfff5":"code","dda31f09":"code","d2dab1bc":"code","0b081c3d":"code","855022be":"code","e90ad9dc":"code","f36e66e8":"markdown","b64516de":"markdown","202c4b46":"markdown","45d577ca":"markdown","86374b84":"markdown","5deac626":"markdown"},"source":{"3b7128b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f39d1093":"import matplotlib.pyplot as plt\nimport seaborn as sns","db941bf2":"data = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines = True)","c598bbda":"data.head()","9d6e30cd":"df = data.drop('article_link',axis  = 1)","10791d65":"df.head()","e563b072":"df.shape","9497c8c7":"df.info()","34b92d94":"df.isnull().sum()","33c667d9":"sns.countplot(df['is_sarcastic'].value_counts())","486a9872":"import nltk\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.corpus import stopwords\nimport spacy","73f1b0e7":"nlp = spacy.load('en_core_web_sm')\ndef preprocess(text):\n    doc = nlp(text)\n    lemmas = [token.lemma_ for token in doc]\n    a_lemmas = [lemma.lower() for lemma in lemmas if lemma.isalpha() and lemma not in stopwords.words('english')]\n    lemmatized_text = ' '.join(a_lemmas)\n    return lemmatized_text","f3863e3f":"cleaned_text = []\nfor text in df.headline:\n    cleaned_text.append(preprocess(text))\ndf['clean_text'] = cleaned_text","17dadc2b":"df.head()","dc7efc7a":"from wordcloud import WordCloud\nplt.figure(figsize = (20,10))\nwc = WordCloud(width = 1500,height = 1000,max_words = 1000).generate(' '.join(word for word in df.clean_text))\nplt.axis(\"off\")\nplt.title('Wordcloud')\nplt.imshow(wc , interpolation = 'bilinear')","cf58dba5":"X = df.clean_text\ny = df.is_sarcastic","1b1d2d7a":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\nfrom sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer","e16bc1ac":"def print_metrices(pred,true):\n    print(confusion_matrix(true,pred))\n    print(classification_report(true,pred,))\n    print(\"Accuracy : \",accuracy_score(pred,true))\n    print(\"Precison : \",precision_score(pred,true, average = 'weighted'))\n    print(\"Recall : \",recall_score(pred,true,  average = 'weighted'))\n    print(\"F1 : \",f1_score(pred,true,  average = 'weighted'))","403db9cd":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 42)","8649dd3a":"tfidf = TfidfVectorizer(ngram_range = (1,3))\nX_tfidf_train = tfidf.fit_transform(X_train.tolist())\nX_tfidf_test = tfidf.transform(X_test.tolist())","fdcd63fb":"X_train.head()","3fb7dc8d":"lr = LogisticRegression(class_weight = 'balanced')\nlr.fit(X_tfidf_train,y_train)","ef2871d1":"y_pred_lr = lr.predict(X_tfidf_test)\nprint_metrices(y_pred_lr,y_test)","44ea02b1":"clf_nb = MultinomialNB()\nclf_nb.fit(X_tfidf_train,y_train)\ny_pred_nb = clf_nb.predict(X_tfidf_test)\nprint_metrices(y_pred_nb,y_test)","61719a43":"from sklearn.svm import LinearSVC\nsvc =  LinearSVC(C= 10,random_state=42,class_weight='balanced')\nsvc.fit(X_tfidf_train,y_train)\ny_pred_svc = svc.predict(X_tfidf_test)\nprint_metrices(y_pred_svc,y_test)","fa3e0c92":"clf_dt = DecisionTreeClassifier(criterion='gini', splitter = 'best', max_depth=6, random_state=42)\nclf_dt.fit(X_tfidf_train, y_train)\ny_pred_dt = clf_dt.predict(X_tfidf_test)\nprint_metrices(y_pred_dt,y_test)","0bb4d3d8":"from sklearn import tree\nplt.figure(figsize = (30,14))\ntree.plot_tree(clf_dt)\nplt.show()","1cf312e4":"clf_lr = LogisticRegression(class_weight='balanced')\nclf_dt = DecisionTreeClassifier(class_weight='balanced')\nclf_rf =   RandomForestClassifier(class_weight='balanced')\nclf_svc = SVC(class_weight='balanced')\n\n\nvoting_clf = VotingClassifier(estimators=[('SVC', clf_svc), ('DecisionTree', clf_dt), ('LogReg', clf_lr),('RandromForest', clf_rf)], voting='hard')\nvoting_clf.fit(X_tfidf_train, y_train)\ny_pred_ensemble = voting_clf.predict(X_tfidf_test)","eba4cefc":"print_metrices(y_pred_ensemble,y_test)","a53cd2c8":"acc_table = {\n    'Logistic Regression' : accuracy_score(y_pred_lr,y_test),\n    'LinearSVC' : accuracy_score(y_pred_svc,y_test),\n    'Decision Tree' : accuracy_score(y_pred_dt,y_test),\n    'Naive Bayes' : accuracy_score(y_pred_nb,y_test),\n    'Ensemble ': accuracy_score(y_pred_ensemble,y_test),\n}","b20f0f25":"acc_df = pd.DataFrame(acc_table.items(),columns = ['Model','Accuracy'])","62001ea7":"# fig, ax = plt.subplots()\nplt.figure(figsize = (16,10))\nsns.barplot(x=acc_df['Model'], y=acc_df['Accuracy'], data=acc_df)","88beac56":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","bc1ae57b":"max_words = 1000\nmax_len = 100\ntokenizer = Tokenizer(num_words = max_words,oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_padded_sequences = pad_sequences(train_sequences,maxlen = max_len,padding = 'post')\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_padded_sequences = pad_sequences(test_sequences,maxlen = max_len,padding = 'post')","25651b10":"print(train_sequences[0])\nprint(train_padded_sequences[0])\n","35f32e7b":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,GlobalAveragePooling1D\nfrom keras.models import Model\nfrom keras.models import Sequential","53dbfff5":"import numpy as np\ntraining_padded = np.array(train_padded_sequences)\ntraining_labels = np.array(y_train)\ntesting_padded = np.array(test_padded_sequences)\ntesting_labels = np.array(y_test)","dda31f09":"vocab_size = 10000\nembedding_dim = 16","d2dab1bc":"Model = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_len),\n    GlobalAveragePooling1D(),\n    Dense(24, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nModel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","0b081c3d":"Model.summary()","855022be":"num_epochs = 30\nhistory = Model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)","e90ad9dc":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","f36e66e8":"**LinearSVC**","b64516de":"**RNN**","202c4b46":"**Ensemble Approach**","45d577ca":"**Naive Bayes**","86374b84":"**Logistic Regression**","5deac626":"**Decision Tree**"}}