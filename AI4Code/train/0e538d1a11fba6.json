{"cell_type":{"2ca6b030":"code","bacc7086":"code","72c817af":"code","a6ee776a":"code","ced33f03":"code","94205672":"code","bb63bf03":"code","830d94d7":"code","53b281d0":"code","a24ea1c9":"code","eb511010":"code","298f15fc":"code","07b2fb0e":"code","d9572a05":"code","d0a157b2":"code","b9b34fb8":"code","4640cb19":"code","802b0c62":"code","a2f360bc":"code","048a8c58":"code","14bf48de":"code","3a03ee33":"code","fc5ef44f":"code","1da4e677":"code","05aec903":"code","6794d54b":"code","75c21bff":"code","a8fdc168":"markdown","4d13ae96":"markdown","ac2305ed":"markdown","91b0e31f":"markdown","acca4d9e":"markdown","d6fa9d87":"markdown","737a3555":"markdown","942653d1":"markdown","b43a642b":"markdown","fa694a82":"markdown","a0e2452e":"markdown","a188ab17":"markdown","719ab9c7":"markdown","5a478289":"markdown","36bca918":"markdown","2e17bf46":"markdown","71f53a75":"markdown","ec140adb":"markdown","157e64a5":"markdown"},"source":{"2ca6b030":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","bacc7086":"!wget https:\/\/raw.githubusercontent.com\/alexeygrigorev\/datasets\/master\/AB_NYC_2019.csv","72c817af":"columns = [\n    'neighbourhood_group', 'room_type', 'latitude', 'longitude',\n    'minimum_nights', 'number_of_reviews','reviews_per_month',\n    'calculated_host_listings_count', 'availability_365',\n    'price'\n]\n\ndf = pd.read_csv('AB_NYC_2019.csv', usecols=columns)\ndf.reviews_per_month = df.reviews_per_month.fillna(0)","a6ee776a":"df.head()","ced33f03":"df.shape\n","94205672":"df['price'] = np.log1p(df['price'])\ndf.price","bb63bf03":"from sklearn.model_selection import train_test_split\ndf_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)\n\nlen(df_train), len(df_val), len(df_test)\n","830d94d7":"df_train","53b281d0":"y_train = df_train['price'].values\ny_val = df_val['price'].values\n#y_test = df_test['price'].values\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\n#df_test = df_test.reset_index(drop=True)\n\ndel df_train['price']\ndel df_val['price']\n#del df_test['price']\ndf_train\n","a24ea1c9":"df_train.isnull().sum() #why 0? while there is a lot of 0 values","eb511010":"from sklearn.feature_extraction import DictVectorizer\n\ndict_train = df_train.to_dict(orient='records')\ndict_val = df_val.to_dict(orient='records')\ndict_train[0]","298f15fc":"dv = DictVectorizer(sparse=False)\n\nX_train = dv.fit_transform(dict_train)\nX_val = dv.transform(dict_val)","07b2fb0e":"from sklearn.tree import DecisionTreeRegressor, export_text\nfrom sklearn.metrics import mean_squared_error","d9572a05":"dt = DecisionTreeRegressor(max_depth=1)\ndt.fit(X_train, y_train)\n#y_pred = dt.mean_squared_error(X_train)[:,1]\n#mean_squared_error(y_train, y_pred)","d0a157b2":"score = dt.score(X_train, y_train)\nprint(f\"Score: {score}\\nRMSE: {np.sqrt(score)}\")","b9b34fb8":"print(export_text(dt, feature_names=dv.get_feature_names()))","4640cb19":"'''\nimportances = list(zip(dv.feature_names_, dt.feature_importances_))\ndf_importance = pd.DataFrame(importances, columns=['feature', 'gain'])\ndf_importance = df_importance.sort_values(by='gain', ascending=False)\ndf_importance\natau pake grafik\/bar\n'''","802b0c62":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=10, random_state=1)\nrf.fit(X_train, y_train)","a2f360bc":"score_rf = rf.score(X_train, y_train)\nprint(f\"Score: {score_rf}n\\RMSE: {np.sqrt(score_rf)}\")","048a8c58":"pred = rf.predict(X_val)\nrmse = mean_squared_error(y_val, pred, squared=False)\n#rmse = np.sqrt(mse)\nprint(f\"RMSE: {rmse}\")","14bf48de":"for n in range(10, 200, 10):\n    rf1 = RandomForestRegressor(n_estimators=n, random_state=1)\n    rf1.fit(X_train, y_train)\n    score_train1 = rf1.score(X_train, y_train)\n    pred_train1 = rf1.predict(X_train)\n    rmse_train1 = mean_squared_error(y_train, pred_train1, squared=False)\n    \n    score_val1 = rf1.score(X_val, y_val)\n    pred_val1 = rf1.predict(X_val)\n    rmse_val1 = mean_squared_error(y_val, pred_val1, squared=False)\n    #rmse1 = np.sqrt(mse1)\n    \n    print(f\"{n}\\nScore train:\\t\\t{score_rf1}\\nRMSE train:\\t\\t{rmse_train1}\\nRMSE val:\\t\\t{rmse1}\")\n    ","3a03ee33":"scores2 = []\nmodels2 = []\n\nfor m in [10, 15, 20, 25]:\n    pre_score = 0\n    for n in range(10, 200,10):\n        rf2 = RandomForestRegressor(n_estimators=n, max_depth=m, random_state=1)\n        rf2.fit(X_train, y_train)\n        \n        score_train2 = rf2.score(X_train, y_train)\n        pred_train2 = rf2.predict(X_train)\n        rmse_train2 = mean_squared_error(y_train, pred_train2, squared=False)\n        \n        score_val2 = rf2.score(X_val, y_val)\n        pred_val2 = rf2.predict(X_val)\n        rmse_val2 = mean_squared_error(y_val, pred_val2, squared=False)\n        \n        scores2.append([n, m, rmse_train2, rmse_val2])\n        print(n, m, \" ==> (train)-(val)\" ,rmse_train2, rmse_val2)\n        models2.append(rf2)\n\n        # early stopping\n        if -1e-5 <= round(rmse_val2 - pre_score, 5) <= 1e-5 :\n            print(\"BREAK\")\n            break\n        else:\n            pre_score = rmse_val2","fc5ef44f":"rf3 = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=1, n_jobs=-1)\nrf3.fit(X_train, y_train)\nscore_train3 = rf3.score(X_train, y_train)\npred_train3 = rf3.predict(X_train)\nrmse_train3 = mean_squared_error(y_train, pred_train3, squared=False)\n\nprint(f\"\\nScore train:\\t\\t{score_train3}\\nRMSE train:\\t\\t{rmse_train3}\")\n","1da4e677":"\nfeat_importances = pd.Series(rf3.feature_importances_, index=dv.get_feature_names())\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.title(\"Top 15 important features\")\n","05aec903":"import xgboost as xgb\n\n#create DMatrix \nfeatures = dv.get_feature_names()\ndm_train = xgb.DMatrix(X_train, label=y_train, feature_names=features)\ndm_val = xgb.DMatrix(X_val, label=y_val, feature_names=features)","6794d54b":"params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    'seed': 1,\n    'verbosity': 1,\n}","75c21bff":"xgb_scores=[]\nfor eta in [0.01, 0.1, 0.3]:\n    xgb1 = xgb.train({**params, 'eta':eta}, dm_train, evals=[(dm_train, 'train'), (dm_val, 'val')]) #early_stopping_rounds=10\n    #score_xgb1 = xgb1.score(X_train, y_train)\n    pred_txgb1 = xgb1.predict(dm_train)\n    rmse_txgb1 = mean_squared_error(y_train, pred_txgb1, squared=False)\n    pred_vxgb1 = xgb1.predict(dm_val)\n    rmse_vxgb1 = mean_squared_error(y_val, pred_vxgb1, squared=False)\n    xgb_scores.append([eta, rmse_txgb1, rmse_vxgb1])\n    print(f\"{eta}\\n ==>(train)-(val)\\n{np.mean(rmse_txgb1)}\\n{np.mean(rmse_vxgb1)}\")","a8fdc168":"## Question 1\n\nLet's train a decision tree regressor to predict the price variable. \n\n* Train a model with `max_depth=1`","4d13ae96":"Which feature is used for splitting the data?\n\n `room_type`\n* `neighbourhood_group`\n* `number_of_reviews`\n* `reviews_per_month`","ac2305ed":"Now, use `DictVectorizer` to turn train and validation into matrices:","91b0e31f":"## Question 6","acca4d9e":"## 6.10 Homework\n\nThe goal of this homework is to create a tree-based regression model for prediction apartment prices (column `'price'`).\n\nIn this homework we'll again use the New York City Airbnb Open Data dataset - the same one we used in homework 2 and 3.\n\nYou can take it from [Kaggle](https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv)\nor download from [here](https:\/\/raw.githubusercontent.com\/alexeygrigorev\/datasets\/master\/AB_NYC_2019.csv)\nif you don't want to sign up to Kaggle.\n\nLet's load the data:","d6fa9d87":"Now let's train an XGBoost model! For this question, we'll tune the `eta` parameter\n\n* Install XGBoost\n* Create DMatrix for train and validation\n* Create a watchlist\n* Train a model with these parameters for 100 rounds:\n\n```\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    \n    'seed': 1,\n    'verbosity': 1,\n}\n```","737a3555":"What's the RMSE of this model on validation?\n\n* 0.059\n* 0.259\n* 0.459*\n* 0.659","942653d1":"## Question 2\n\nTrain a random forest model with these parameters:\n\n* `n_estimators=10`\n* `random_state=1`\n* `n_jobs=-1`  (optional - to make training faster)","b43a642b":"## Question 5\n\nWe can extract feature importance information from tree-based models. \n\nAt each step of the decision tree learning algorith, it finds the best split. \nWhen doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. \nThis gain is quite useful in understanding what are the imporatant features \nfor tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the `feature_importances_` field. \n\nFor this homework question, we'll find the most important feature:\n\n* Train the model with these parametes:\n    * `n_estimators=10`,\n    * `max_depth=20`,\n    * `random_state=1`,\n    * `n_jobs=-1` (optional)\n* Get the feature importance information from this model","fa694a82":"## Question 4\n\nLet's select the best `max_depth`:\n\n* Try different values of `max_depth`: `[10, 15, 20, 25]`\n* For each of these values, try different values of `n_estimators` from 10 till 200 (with step 10)\n* Fix the random seed: `random_state=1`","a0e2452e":"Ref:\n* https:\/\/github.com\/alexeygrigorev\/mlbookcamp-code\/blob\/master\/course-zoomcamp\/06-trees\/homework.md\n* https:\/\/github.com\/RCSnyder\/machine-learning-zoom-camp\/blob\/main\/src\/Chapters\/06-trees\/homework_6.ipynb\n* https:\/\/github.com\/JonathanLoscalzo\/ml-zoomcamp-2021\/blob\/main\/notebooks\/homework-06.ipynb\n","a188ab17":"## Question 3\n\nNow let's experiment with the `n_estimators` parameter\n\n* Try different values of this parameter from 10 to 200 with step 10\n* Set `random_state` to `1`\n* Evaluate the model on the validation dataset","719ab9c7":"After which value of `n_estimators` does RMSE stop improving?\n\n- 10\n- 50\n- 70\n- 120*","5a478289":"* Apply the log tranform to `price`\n* Do train\/validation\/test split with 60%\/20%\/20% distribution. \n* Use the `train_test_split` function and set the `random_state` parameter to 1","36bca918":"What's the best `max_depth`:\n\n* 10\n* 15*\n* 20\n* 25\n\nBonus question (not graded):\n\nWill the answer be different if we change the seed for the model?","2e17bf46":"## Submit the results\n\n\nSubmit your results here: https:\/\/forms.gle\/wQgFkYE6CtdDed4w8\n\nIt's possible that your answers won't match exactly. If it's the case, select the closest one.\n\n\n## Deadline\n\n\nThe deadline for submitting is 20 October 2021, 17:00 CET (Wednesday). After that, the form will be closed.\n\n","71f53a75":"What's the most important feature? \n\n* `neighbourhood_group=Manhattan`\n* `room_type=Entire home\/apt`*\n* `longitude`\n* `latitude`","ec140adb":"Now change `eta` first to `0.1` and then to `0.01`","157e64a5":"Which eta leads to the best RMSE score on the validation dataset?\n\n* 0.3*\n* 0.1\n* 0.01"}}