{"cell_type":{"0fa551ed":"code","6d342fb0":"code","ac9d5f8f":"code","db97b16e":"code","c6f03941":"code","4e6a9c0c":"code","e2ab02a1":"code","0595f946":"code","6d0fddf9":"code","60ca32bd":"code","1e87b921":"code","e2a006cf":"code","32ad264f":"code","e399c34a":"code","32b71822":"code","fb15f403":"code","43ed78c4":"code","b2a5ab74":"code","9b4849f3":"code","5a79c41d":"code","b14d097f":"code","6d0c233b":"code","1fa3751d":"code","2c6d435d":"code","fc876688":"code","532e7346":"code","ae00ce36":"code","7eefe7ea":"code","6bd88ec7":"code","180c4755":"code","3137ef54":"code","ad07ca9d":"code","4fdc0526":"code","2c407445":"code","efd7d313":"code","7416c330":"code","860aaecc":"code","c4f9266d":"code","33a0e568":"code","8b55fb01":"code","7e0b4bbf":"code","9f4456fd":"code","cf2d5701":"code","1e340f33":"code","9e7a978d":"code","28e61441":"code","8e397b75":"code","ee22dc10":"code","db86b584":"code","b67c99fc":"code","c5b75007":"code","6faac05c":"code","7d9fbe40":"code","94ba3992":"code","30c52072":"code","d69280ea":"code","20682471":"code","e35f7801":"code","26120e7f":"code","1b42d50c":"code","59721197":"code","548556f1":"code","94ca4b06":"code","890309e2":"code","06c9f43d":"code","086693ef":"code","65cd0f66":"code","97e6907a":"code","9ea3bf8f":"code","161d4739":"code","31520ea9":"code","62070822":"code","b2d0b576":"code","92df83e7":"code","73ea0fbc":"code","ba3e5242":"code","7e785122":"code","f88c8139":"code","c845e1ee":"code","9d38bd8c":"code","c6197ee7":"code","73297d80":"code","cb9efe31":"code","9863a1da":"code","ed943ffd":"code","909c3878":"code","5ee4ce53":"code","222793fe":"code","becb4a26":"code","b665ebf9":"code","f6a7f92e":"code","d71a39be":"code","27266ba9":"code","fa376e5e":"code","f628429c":"code","5e7a42cd":"code","16c9164a":"code","13ea05e4":"code","445882e3":"code","6e0fcf6a":"code","97f150e0":"code","5612a2ea":"code","8983cd9f":"code","c90ac3af":"code","7dd29cbf":"code","1ce7b088":"code","29d310f1":"code","4bd13303":"code","3ac27b1c":"code","205e4c29":"code","bf48bc6c":"code","38a4a8db":"code","cc7bceb3":"code","0322ec41":"code","f9b96567":"code","688f1714":"code","bbd265b7":"code","506b4567":"code","b47fc144":"code","ca5ac6e1":"code","350caba0":"code","9b2f7e54":"code","fd7c48fe":"code","16efc319":"code","99e0d4cf":"code","f0994321":"code","19cce9d7":"code","c76dbea9":"code","9e75ec00":"code","7faa2778":"code","a0d54d67":"code","12ef88d3":"code","f4b7fc67":"code","7ace8700":"code","a8f9ae9b":"code","140ff407":"markdown","2fe8e67d":"markdown","f5d188a5":"markdown","36cca4df":"markdown","ed1584bd":"markdown","533501a2":"markdown","0c8e20f4":"markdown","730371e8":"markdown"},"source":{"0fa551ed":"!pip install tensorflow-gpu==2.0.0-beta1","6d342fb0":"import tensorflow as tf\nprint(tf.__version__)","ac9d5f8f":"##########################  start making sequences data ###############################","db97b16e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n","c6f03941":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","4e6a9c0c":"def trend(time, slope=0):\n    return slope * time","e2ab02a1":"# Create the series\ntime=np.arange(4*365+1)\nbaseline=10\nseries=trend(time,0.1)\nplot_series(time, series)\nplt.show()\n","0595f946":"def seasonal_pattern(season_time):\n    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n    return np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 \/ np.exp(3 * season_time))\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"Repeats the same pattern at each period\"\"\"\n    season_time = ((time + phase) % period) \/ period\n    return amplitude * seasonal_pattern(season_time)","6d0fddf9":"# Create the series\nbaseline=10\namplitude=40\nseries = seasonality(time, period=365, amplitude=amplitude)\nplot_series(time, series)\nplt.show()","60ca32bd":"def noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","1e87b921":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)\nplt.show()","e2a006cf":"noise_level = 40\nnoisy_series = series + noise(time, noise_level)\nplot_series(time, noisy_series)","32ad264f":"def autocorrelation(time, amplitude):\n    rho1 = 0.5\n    rho2 = -0.1\n    ar = np.random.randn(len(time) + 50)\n    ar[:50] = 100\n    for step in range(50, len(time) + 50):\n        ar[step] += rho1 * ar[step - 50]\n        ar[step] += rho2 * ar[step - 33]\n    return ar[50:] * amplitude","e399c34a":"series = autocorrelation(time, 10)\nplot_series(time[:200], series[:200])","32b71822":"\ndef autocorrelation(time, amplitude):\n    rho = 0.8\n    ar = np.random.randn(len(time) + 1)\n    for step in range(1, len(time) + 1):\n        ar[step] += rho * ar[step - 1]\n    return ar[1:] * amplitude","fb15f403":"series = autocorrelation(time, 10)\nplot_series(time[:200], series[:200])","43ed78c4":"###################################################################  make final time data  ###########################","b2a5ab74":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras","9b4849f3":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\ndef trend(time, slope=0):\n    return slope * time\n\ndef seasonal_pattern(season_time):\n    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n    return np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 \/ np.exp(3 * season_time))\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"Repeats the same pattern at each period\"\"\"\n    season_time = ((time + phase) % period) \/ period\n    return amplitude * seasonal_pattern(season_time)\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","5a79c41d":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)","b14d097f":"plt.figure(figsize=(10, 6))\nplot_series(time, series)\nplt.show()","6d0c233b":"# make time_train as first 1000 day\n# make time_vaild as 1000 to 1400 day","1fa3751d":"split_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n","2c6d435d":"# train\nplt.figure(figsize=(10, 6))\nplot_series(time_train, x_train)\nplt.show()","fc876688":"# valid\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplt.show()","532e7346":"#Naive Forecast\nnaive_forecast = series[split_time - 1:-1]","ae00ce36":"plot_series(time_valid, x_valid, start=0, end=450)\nplot_series(time_valid, naive_forecast, start=1, end=451)","7eefe7ea":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid, start=0, end=150)\nplot_series(time_valid, naive_forecast, start=1, end=151)","6bd88ec7":"#naive_forecast\nprint('mean_squared_error:')\nprint(keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\nprint('mean_absolute_error:')\nprint(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())","180c4755":"def moving_average_forecast(series, window_size):\n  \"\"\"Forecasts the mean of the last few values.\n     If window_size=1, then this is equivalent to naive forecast\"\"\"\n  forecast = []\n  for time in range(len(series) - window_size):\n    forecast.append(series[time:time + window_size].mean())\n  return np.array(forecast)","3137ef54":"moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, moving_avg)","ad07ca9d":"# moving_avg\nprint('mean_squared_error:')\nprint(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\nprint('mean_absolute_error:')\nprint(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","4fdc0526":"diff_series = (series[365:] - series[:-365])\ndiff_time = time[365:]\n\nplt.figure(figsize=(10, 6))\nplot_series(diff_time, diff_series)\nplt.show()","2c407445":"diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, diff_series[split_time - 365:])\nplot_series(time_valid, diff_moving_avg)\nplt.show()","efd7d313":"diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_past)\nplt.show()","7416c330":"\n# diff_moving_avg\nprint('mean_squared_error:')\nprint(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\nprint('mean_absolute_error:')\nprint(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())","860aaecc":"diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_smooth_past)\nplt.show()","c4f9266d":"# diff_moving_avg_plus_smooth_past\n\nprint('mean_squared_error:')\nprint(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\nprint('mean_absolute_error:')\nprint(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())","33a0e568":"# 0 to 9\ndataset = tf.data.Dataset.range(10)\nfor val in dataset:\n   print(val.numpy())","8b55fb01":"# 5 number window\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1)\nfor window_dataset in dataset:\n  for val in window_dataset:\n    print(val.numpy(), end=\" \")\n  print()","7e0b4bbf":"# drop remainder\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\nfor window_dataset in dataset:\n  for val in window_dataset:\n    print(val.numpy(), end=\" \")\n  print()","9f4456fd":"# make it numpy\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\nfor window in dataset:\n  print(window.numpy())","cf2d5701":"# make X Y\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\nfor x,y in dataset:\n  print(x.numpy(), y.numpy())","1e340f33":"# shuffle\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\nfor x,y in dataset:\n  print(x.numpy(), y.numpy())","9e7a978d":"\n# 2 line to 1 batch \ndataset = tf.data.Dataset.range(10)\ndataset = dataset.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\ndataset = dataset.batch(2).prefetch(1)\nfor x,y in dataset:\n  print(\"x = \", x.numpy())\n  print(\"y = \", y.numpy())","28e61441":"#######################","8e397b75":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras","ee22dc10":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\ndef trend(time, slope=0):\n    return slope * time\n\ndef seasonal_pattern(season_time):\n    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n    return np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 \/ np.exp(3 * season_time))\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"Repeats the same pattern at each period\"\"\"\n    season_time = ((time + phase) % period) \/ period\n    return amplitude * seasonal_pattern(season_time)\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","db86b584":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)","b67c99fc":"# create sf data time series function\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n  dataset = tf.data.Dataset.from_tensor_slices(series)\n  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n  dataset = dataset.batch(batch_size).prefetch(1)\n  return dataset","c5b75007":"split_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","6faac05c":"# create sf data time series\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(dataset)","7d9fbe40":"# define model\nl_0 = tf.keras.layers.Dense(1, input_shape=[window_size])\nmodel = tf.keras.models.Sequential([l_0])","94ba3992":"# compile model\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))","30c52072":"# train model\nmodel.fit(dataset,epochs=100,verbose=0)","d69280ea":"# past 20 input patamer and 1 b\nprint(\"Layer weights {}\".format(l_0.get_weights()))","20682471":"print(series[1:21])","e35f7801":"# perdiction\nmodel.predict(series[1:21][np.newaxis])","26120e7f":"# perdiction\nforecast = []\n\nfor time in range(len(series) - window_size):\n  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","1b42d50c":"# linear regression\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())","59721197":"# DNN\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n    tf.keras.layers.Dense(10, activation=\"relu\"), \n    tf.keras.layers.Dense(1)\n])\n","548556f1":"model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))","94ca4b06":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)","890309e2":"history=model.fit(dataset,epochs=100,verbose=1,callbacks=[lr_schedule])","06c9f43d":"lrs=1e-8*(10**(np.arange(100)\/20))\nplt.semilogx(lrs,history.history[\"loss\"])\nplt.axis([1e-8,1e-3,0,300])","086693ef":"# DNN\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, input_shape=[window_size], activation=\"relu\"), \n    tf.keras.layers.Dense(10, activation=\"relu\"), \n    tf.keras.layers.Dense(1)\n])","65cd0f66":"model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=7e-6, momentum=0.9))","97e6907a":"history=model.fit(dataset,epochs=500,verbose=1)","9ea3bf8f":"epochs=range(500)\nloss=history.history['loss']\nplt.plot  ( epochs, loss )","161d4739":"\n\nloss=history.history['loss']\nloss2=loss[10:]\nepochs2=epochs[10:]\nplt.plot  ( epochs2,     loss2 )","31520ea9":"forecast = []\nfor time in range(len(series) - window_size):\n  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","62070822":"# DNN result\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())","b2d0b576":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(tf.__version__)","92df83e7":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\ndef trend(time, slope=0):\n    return slope * time\n\ndef seasonal_pattern(season_time):\n    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n    return np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 \/ np.exp(3 * season_time))\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"Repeats the same pattern at each period\"\"\"\n    season_time = ((time + phase) % period) \/ period\n    return amplitude * seasonal_pattern(season_time)\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","73ea0fbc":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nsplit_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","ba3e5242":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n  dataset = tf.data.Dataset.from_tensor_slices(series)\n  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n  dataset = dataset.batch(batch_size).prefetch(1)\n  return dataset","7e785122":"tf.keras.backend.clear_session()","f88c8139":"tf.random.set_seed(51)\nnp.random.seed(51)\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)","c845e1ee":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[None]),\n  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n  tf.keras.layers.SimpleRNN(40),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])","9d38bd8c":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))","c6197ee7":"optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)","73297d80":"model.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])","cb9efe31":"history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","9863a1da":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 30])","ed943ffd":"#################### train with new learning rate","909c3878":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\n","5ee4ce53":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[None]),\n  tf.keras.layers.SimpleRNN(40, return_sequences=True),\n  tf.keras.layers.SimpleRNN(40),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])","222793fe":"optimizer = tf.keras.optimizers.SGD(lr=5e-5, momentum=0.9)","becb4a26":"model.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])","b665ebf9":"history = model.fit(dataset,epochs=400)","f6a7f92e":"forecast=[]\nfor time in range(len(series) - window_size):\n  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","d71a39be":"# RNN result\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())","27266ba9":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae=history.history['mae']\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()","fa376e5e":"# LSTM\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ntf.keras.backend.clear_session()","f628429c":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)","5e7a42cd":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[None]),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])","16c9164a":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))","13ea05e4":"optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)","445882e3":"model.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])","6e0fcf6a":"history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])","97f150e0":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 30])","5612a2ea":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ntf.keras.backend.clear_session()\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[None]),\n   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n\nmodel.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=500,verbose=1)","8983cd9f":"# LSTM\nprint(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())","c90ac3af":"forecast = []\nresults = []\nfor time in range(len(series) - window_size):\n  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)","7dd29cbf":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae=history.history['mae']\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()","1ce7b088":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(tf.__version__)","29d310f1":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n\ndef trend(time, slope=0):\n    return slope * time\n\ndef seasonal_pattern(season_time):\n    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n    return np.where(season_time < 0.4,\n                    np.cos(season_time * 2 * np.pi),\n                    1 \/ np.exp(3 * season_time))\n\ndef seasonality(time, period, amplitude=1, phase=0):\n    \"\"\"Repeats the same pattern at each period\"\"\"\n    season_time = ((time + phase) % period) \/ period\n    return amplitude * seasonal_pattern(season_time)\n\ndef noise(time, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(len(time)) * noise_level","4bd13303":"time = np.arange(4 * 365 + 1, dtype=\"float32\")\nbaseline = 10\nseries = trend(time, 0.1)  \nbaseline = 10\namplitude = 40\nslope = 0.05\nnoise_level = 5\n\n# Create the series\nseries = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n# Update with noise\nseries += noise(time, noise_level, seed=42)\n\nsplit_time = 1000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 20\nbatch_size = 32\nshuffle_buffer_size = 1000","3ac27b1c":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","205e4c29":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","bf48bc6c":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\nwindow_size = 30\ntrain_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)","38a4a8db":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])","cc7bceb3":"history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","0322ec41":"# get learning rate 1e-5\nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 30])","f9b96567":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nbatch_size = 16\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=500)","688f1714":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","bbd265b7":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","506b4567":"print(tf.keras.metrics.mean_squared_error(x_valid, rnn_forecast).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy())","b47fc144":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nmae=history.history['mae']\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n#------------------------------------------------\n# Plot MAE and Loss\n#------------------------------------------------\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, loss, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()\n\nepochs_zoom = epochs[200:]\nmae_zoom = mae[200:]\nloss_zoom = loss[200:]\n\n#------------------------------------------------\n# Plot Zoomed MAE and Loss\n#------------------------------------------------\nplt.plot(epochs_zoom, mae_zoom, 'r')\nplt.plot(epochs_zoom, loss_zoom, 'b')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"Loss\"])\n\nplt.figure()","ca5ac6e1":"############### Sunspots data ######################","350caba0":"import numpy as np\nimport matplotlib.pyplot as plt\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","9b2f7e54":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/Sunspots.csv \\\n    -O sunspots.csv","fd7c48fe":"!ls","16efc319":"import csv\ntime_step = []\nsunspots = []\n\nwith open('sunspots.csv') as csvfile:\n  reader = csv.reader(csvfile, delimiter=',')\n  next(reader)\n  for row in reader:\n    sunspots.append(float(row[2]))\n    time_step.append(int(row[0]))\n\nseries = np.array(sunspots)\ntime = np.array(time_step)\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","99e0d4cf":"series = np.array(sunspots)\ntime = np.array(time_step)\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","f0994321":"split_time = 3000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 30\nbatch_size = 32\nshuffle_buffer_size = 1000","19cce9d7":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\n\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","c76dbea9":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","9e75ec00":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 60])","7faa2778":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set,epochs=500)","a0d54d67":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","12ef88d3":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","f4b7fc67":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","7ace8700":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\n\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","a8f9ae9b":"######################### end #############################","140ff407":"# Course 4 : Sequences, Time Series and Prediction ","2fe8e67d":"## Class 1 Sequences and Prediction","f5d188a5":"## Class 4 Real-world time series data","36cca4df":"## Class 2 Deep Neural Networks for Time Series","ed1584bd":"# TensorFlow in Practice Specialization\ncoursera:  https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice<br>\n\nhttps:\/\/www.coursera.org\/account\/accomplishments\/specialization\/certificate\/7HWVLBEQS62E<br>\n\nCERTIFICATE:https:\/\/www.coursera.org\/account\/accomplishments\/certificate\/YBVT2FPU6JZN<br>\n\nCourse 1 : Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning <br>\ncoursera: https:\/\/www.coursera.org\/learn\/introduction-tensorflow\n\nCourse 2 : Convolutional Neural Networks in TensorFlow <br>\ncoursera: https:\/\/www.coursera.org\/learn\/convolutional-neural-networks-tensorflow \n\nCourse 3 : Natural Language Processing in TensorFlow <br>\ncoursera: https:\/\/www.coursera.org\/learn\/natural-language-processing-tensorflow\n\nCourse 4 : Sequences, Time Series and Prediction <br>\ncoursera: https:\/\/www.coursera.org\/learn\/tensorflow-sequences-time-series-and-prediction\n\n","533501a2":"## Class 3 Recurrent Neural Networks for Time Series","0c8e20f4":" #####################           end of making sequences data      ##########################################","730371e8":"https:\/\/www.coursera.org\/learn\/tensorflow-sequences-time-series-and-prediction\n\nhttps:\/\/www.kaggle.com\/tduan007\/04-time-series-in-tensorflow-by-coursera\n\nhttps:\/\/github.com\/lmoroney\/dlaicourse\/tree\/master\/TensorFlow%20In%20Practice\/Course%204%20-%20S%2BP\n"}}