{"cell_type":{"4732be1b":"code","922fc3c4":"code","fac44dce":"code","a960457e":"code","a0033a01":"code","9c1a3638":"code","37c5d7da":"code","020b1085":"code","15cb071a":"code","f7f76e08":"code","c4c3e82d":"code","405e85f7":"code","2ac4cb90":"code","62007998":"code","4712f3b6":"code","e7eb0555":"code","3187ab85":"code","b647e5e9":"code","4e885403":"code","64895d3a":"code","3935eb4e":"code","d83ddfe7":"code","ef68936f":"code","64010651":"code","7548f081":"code","4482740f":"code","6282ee9a":"code","5196e3b9":"code","801d78ed":"code","d136e382":"code","8e130067":"code","7eee67de":"code","3aaf0ce0":"code","7006ec8d":"code","4db1a5c4":"code","46df8a90":"code","20ba44be":"code","17847a4d":"code","fc22fbf6":"code","fcc6a707":"code","1af9a2a6":"code","8b5879a8":"code","4c6a9c5f":"code","9582b1c1":"code","54112fcd":"code","8c312bd6":"code","64e26d3b":"code","49f8fa89":"code","94f514b5":"code","e6bd8494":"code","b9c742b2":"code","c7f146ae":"code","43196dca":"code","d2de44b5":"code","21b6de34":"code","ca3387d0":"code","d7e24345":"code","3b867dc1":"code","5aff2ff3":"code","3631db70":"code","cb848b86":"code","39a9a5ca":"code","724225d8":"code","4ab31f85":"code","f4f1ddb9":"code","c3a15de7":"code","6659e07a":"code","6fe737e8":"code","a42f4b07":"code","971a42df":"code","cac5cc5d":"code","513022c7":"code","c61ca646":"code","601712f6":"code","5266a0b1":"code","6aca2de9":"code","b3c3c31a":"code","62a7bd90":"code","8e7bb951":"markdown","5c1a2fb5":"markdown","59f2903a":"markdown","d8c98608":"markdown","3ad11b33":"markdown","df4c9132":"markdown","41d1a7ed":"markdown","5ce9f2fe":"markdown","638ce643":"markdown","961a4c86":"markdown","496683c9":"markdown","930eed46":"markdown","646808a5":"markdown","daf46068":"markdown","b6558d55":"markdown","5d1dca23":"markdown","04eedb72":"markdown","e8c3bc3c":"markdown","179da563":"markdown","6c3b2b11":"markdown","a56c6f67":"markdown","d8d0d806":"markdown","46ba6c00":"markdown","6e290b7c":"markdown","639b22e6":"markdown","f205384c":"markdown","d9c10a78":"markdown","e9672a16":"markdown","739fac97":"markdown","1e627221":"markdown","5f7b0127":"markdown","bba729f7":"markdown","a1570818":"markdown","f0090687":"markdown","7f34209c":"markdown","ed8434bc":"markdown","ffc944e1":"markdown"},"source":{"4732be1b":"# Data analysis tools\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization Tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data Pre-Processing Libraries\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\n\n# For Train-Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Libraries for various Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Metrics Tools\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\n\n#For Receiver Operating Characteristic (ROC)\nfrom sklearn.metrics import roc_curve ,roc_auc_score, auc","922fc3c4":"df = pd.read_csv('..\/input\/heart-disease-prediction\/Heart Disease.csv')","fac44dce":"df.head(10)","a960457e":"df.info()","a0033a01":"df.isnull().sum()","9c1a3638":"len(df[df.duplicated()])","37c5d7da":"# Listing all columns in dataset\ndf.columns","020b1085":"# Identifying the number of unique values in each column\nfor label in df.columns:\n    print(label,':',len(df[label].unique()))","15cb071a":"sns.countplot(df[\"target\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"frequency\")\nplt.title(\"Checking imbalance\")","f7f76e08":"sns.distplot(df.skew(),hist=False)\nplt.show()","c4c3e82d":"# Printing interquartile range (IQR) for each column\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","405e85f7":"# Boxplot visualization for columns with high IQR\n\nplt.boxplot([df[\"thalach\"]])\nplt.xticks([1],[\"thalach\"])\nplt.show()\nplt.boxplot([df[\"trestbps\"]])\nplt.xticks([1],[\"trestbps\"])\nplt.show()\nplt.boxplot([df[\"chol\"]])\nplt.xticks([1],[\"chol\"])\nplt.show()","2ac4cb90":"# Identifying the Ideal min and maximum value\n\nprint(df['trestbps'].quantile(0.10))\nprint(df['trestbps'].quantile(0.90))\n\nprint(df['chol'].quantile(0.10))\nprint(df['chol'].quantile(0.90))","62007998":"# Capping and Flooring of Outliers\n\ndf[\"trestbps\"] = np.where(df[\"trestbps\"] <110.0, 110.0,df['trestbps'])\ndf[\"trestbps\"] = np.where(df[\"trestbps\"] >152.0, 152.0,df['trestbps'])\n\ndf[\"chol\"] = np.where(df[\"chol\"] <188.0, 188.0,df['chol'])\ndf[\"chol\"] = np.where(df[\"chol\"] >308.8, 308.8,df['chol'])\n\ndf.head()","4712f3b6":"# Boxplot visualization after treating outliers\n\nplt.boxplot([df[\"thalach\"]])\nplt.xticks([1],[\"thalach\"])\nplt.show()\nplt.boxplot([df[\"trestbps\"]])\nplt.xticks([1],[\"trestbps\"])\nplt.show()\nplt.boxplot([df[\"chol\"]])\nplt.xticks([1],[\"chol\"])\nplt.show()","e7eb0555":"# Correlation \ndf.corr()[\"target\"]","3187ab85":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr(), cmap = \"RdYlGn\", annot = True)","b647e5e9":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8),title=\"Correlation with target\",color=\"red\")","4e885403":"fig, ax = plt.subplots(4,2,figsize = (15,15))\n\n# sex vs target\nsex = pd.crosstab(df['sex'],df['target'])\nsex.div(sex.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[0,0])\n\n# cp vs target\ncp = pd.crosstab(df['cp'],df['target'])\ncp.div(cp.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True, ax=ax[0,1])\n\n# ca vs target\nca= pd.crosstab(df['ca'],df['target'])\nca.div(ca.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[1,0])\n\n# thal vs target\nthal= pd.crosstab(df['thal'],df['target'])\nthal.div(thal.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[1,1])\n\n# restecg vs target\nrestecg= pd.crosstab(df['restecg'],df['target'])\nrestecg.div(restecg.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[2,0])\n\n# exang vs target\nexang= pd.crosstab(df['exang'],df['target'])\nexang.div(exang.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[2,1])\n\n# slope vs target\nslope= pd.crosstab(df['slope'],df['target'])\nslope.div(slope.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[3,0])\n\n# target vs target\ntarget= pd.crosstab(df['target'],df['target'])\ntarget.div(target.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True,ax=ax[3,1])","64895d3a":"# Age vs. Max Heart Rate for Heart Disease\n\nplt.figure(figsize=(10, 10))\n\n# Scatter with postivie examples\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c=\"Red\")\n\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c=\"Green\")\n\n# Add some helpful info\nplt.title(\"Age vs. Max Heart Rate for Heart Disease\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);\n","3935eb4e":"scaler=StandardScaler()","d83ddfe7":"X=df.drop([\"target\"],axis=1)\ny=df[\"target\"]\n\nX =scaler.fit_transform(X)","ef68936f":"# Train-Test Split\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","64010651":"#Fitting the model\n\nlogistic_Regression = LogisticRegression(random_state=1,solver = \"liblinear\",C=10)\nlogistic_Regression.fit(x_train,y_train)","7548f081":"# Applying the model to the x_test\n\ny_pred = logistic_Regression.predict(x_test)\ny_pred","4482740f":"# Finding Accuracy\n\nlog = accuracy_score(y_pred,y_test)*100","6282ee9a":"# Confusion Matrix\n\ncmlr=confusion_matrix(y_pred,y_test)\nprint(cmlr)","5196e3b9":"# Classification Report that computes various\n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_pred,y_test))","801d78ed":"# Plotting the ROC Curve\n\nprob_lr=logistic_Regression.predict_proba(x_test)\nauc_lr = roc_auc_score(y_test,prob_lr[:,1])\nfprlr,tprlr,_ = roc_curve(y_test,prob_lr[:,1])\nroc_auc=auc(fprlr,tprlr)\nplt.plot(fprlr,tprlr,label = \"AUC = %.2f\" % auc_lr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Logistic Regression\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","d136e382":"#Fitting the model\n\nknn = KNeighborsClassifier(weights= 'distance', p= 1, n_neighbors= 25, algorithm= 'brute')\nknn.fit(x_train,y_train)","8e130067":"# Applying the model to the x_test\n\npred_knn = knn.predict(x_test)\npred_knn","7eee67de":"# Finding Accuracy\n\nKNN = accuracy_score(pred_knn,y_test)*100","3aaf0ce0":"error=[]\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    pred1=knn.predict(x_test)\n    error.append(np.mean(pred1!=y_test))\nprint(error)","7006ec8d":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error,color='blue',linestyle='dashed',marker = 'o',markerfacecolor='red',markersize=10)\nplt.title('Error rate vs K value')\nplt.xlabel('k')\nplt.ylabel('error rate')","4db1a5c4":"# Confusion Matrix\n\ncm_knn=confusion_matrix(pred_knn,y_test)\nprint(cm_knn)","46df8a90":"# Classification Report that computes various\n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_knn,y_test))","20ba44be":"# Plotting the ROC Curve\n\nprob_knn= knn.predict_proba(x_test)\nauc_knn = roc_auc_score(y_test,prob_knn[:,1])\nfprknn,tprknn,_= roc_curve(y_test,prob_knn[:,1])\nroc_auc_knn=auc(fprknn,tprknn)\nplt.plot(fprknn,tprknn,label = \"AUC = %.2f\" % auc_knn)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for KNN\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","17847a4d":"#Fitting the model\n\ngnb=GaussianNB()\ngnb.fit(x_train,y_train)","fc22fbf6":"# Applying the model to the x_test\n\npred_gnb = gnb.predict(x_test)\npred_gnb","fcc6a707":"# Finding Accuracy\n\nGNB = accuracy_score(pred_gnb,y_test)*100","1af9a2a6":"# Confusion Matrix\n\ncm_gnb=confusion_matrix(pred_gnb,y_test)\nprint(cm_gnb)","8b5879a8":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_gnb,y_test))","4c6a9c5f":"# Plotting the ROC Curve\n\nprob_gnb= gnb.predict_proba(x_test)\nauc_gnb = roc_auc_score(y_test,prob_gnb[:,1])\nfprgnb,tprgnb,_= roc_curve(y_test,prob_gnb[:,1])\nroc_auc_gnb=auc(fprgnb,tprgnb)\nplt.plot(fprgnb,tprgnb,label = \"AUC = %.2f\" % auc_gnb)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Naive-Bayes\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","9582b1c1":"#Fitting the model\n\nsvc = SVC(probability=True,gamma=0.9,kernel='linear', degree=4,C=72.04,random_state=0)\nsvc.fit(x_train,y_train)\n\n# Applying the model to the x_test\npred_svc = svc.predict(x_test)\npred_svc","54112fcd":"# Finding Accuracy\n\nSVC = accuracy_score(pred_svc,y_test)*100","8c312bd6":"# Confusion Matrix\n\ncm_svc=confusion_matrix(pred_svc,y_test)\nprint(cm_svc)","64e26d3b":"# Classification Report that computes various \n#metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_svc,y_test))","49f8fa89":"# Plotting the ROC Curve\n\nprob_svc= svc.predict_proba(x_test)\nauc_svc = roc_auc_score(y_test,prob_svc[:,1])\nfprsvc,tprsvc,_= roc_curve(y_test,prob_svc[:,1])\nroc_auc_svc=auc(fprsvc,tprsvc)\nplt.plot(fprsvc,tprsvc,label = \"AUC = %.2f\" % auc_svc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for SVM\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","94f514b5":"#Fitting the model\n\ndtree_en = DecisionTreeClassifier(criterion='entropy',min_samples_leaf=21, min_samples_split=10, random_state=5)\nclf = dtree_en.fit(x_train,y_train)","e6bd8494":"# Applying the model to the x_test\n\npred_dt = clf.predict(x_test)\npred_dt","b9c742b2":"# Finding Accuracy\n\nDTREE = accuracy_score(pred_dt,y_test)*100","c7f146ae":"# Confusion Matrix\n\ncm_dt=confusion_matrix(y_test,pred_dt)\nprint(cm_dt)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,pred_dt))","43196dca":"# Plotting the ROC Curve\n\nprob_dt= dtree_en.predict_proba(x_test)\nauc_dt = roc_auc_score(y_test,prob_dt[:,1])\nfprdt,tprdt,_= roc_curve(y_test,prob_dt[:,1])\nroc_auc_dt=auc(fprdt,tprdt)\nplt.plot(fprdt,tprdt,label = \"AUC = %.2f\" % auc_dt)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Decision Tree\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","d2de44b5":"#Fitting the model\n\nGBC=GradientBoostingClassifier(n_estimators=70, loss='deviance', learning_rate=0.8, subsample=1.0, \n                               criterion='mae', min_samples_split=4, min_samples_leaf=13,max_depth=20, random_state=2)\nGBC.fit(x_train,y_train)","21b6de34":"# Applying the model to the x_test\n\nY_predict=GBC.predict(x_test)\nY_predict","ca3387d0":"# Finding Accuracy\n\ngbc = accuracy_score(y_test,Y_predict)*100","d7e24345":"# Confusion Matrix\n\ncm_gbc=confusion_matrix(y_test,Y_predict)\nprint(cm_gbc)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,Y_predict))","3b867dc1":"# Plotting the ROC Curve\n\nprob_GBC= GBC.predict_proba(x_test)\nauc_GBC = roc_auc_score(y_test,prob_GBC[:,1])\nfprGBC,tprGBC,_= roc_curve(y_test,prob_GBC[:,1])\nroc_auc_GBC=auc(fprGBC,tprGBC)\nplt.plot(fprGBC,tprGBC,label = \"AUC = %.2f\" % auc_GBC)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Gradient Boosting\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","5aff2ff3":"#Fitting the model\n\nrfc = RandomForestClassifier(criterion='gini',n_estimators=70,bootstrap=True,max_depth=13,max_features='auto', min_samples_leaf=5, min_samples_split=10,random_state=1)\nrfc.fit(x_train, y_train)","3631db70":"# Applying the model to the x_test\n\npred_rf= rfc.predict(x_test)\npred_rf","cb848b86":"# Finding Accuracy\n\nRFC = accuracy_score(y_test,pred_rf)*100","39a9a5ca":"# Confusion Matrix\n\ncm_rf=confusion_matrix(pred_rf,y_test)\nprint(cm_rf)","724225d8":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_rf,y_test))","4ab31f85":"# Plotting the ROC Curve\n\nprob_rfc= rfc.predict_proba(x_test)\nauc_rfc = roc_auc_score(y_test,prob_rfc[:,1])\nfprrfc,tprrfc,_= roc_curve(y_test,prob_rfc[:,1])\nroc_auc_rfc=auc(fprrfc,tprrfc)\nplt.plot(fprrfc,tprrfc,label = \"AUC = %.2f\" % auc_rfc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Random Forest\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","f4f1ddb9":"#Fitting the model. Base model is chosen to be Decision Tree\n\nmodel = dtree_en\nadaboost = AdaBoostClassifier(n_estimators=450, base_estimator=model,random_state=2,learning_rate=2.0)\nadaboost.fit(x_train,y_train)","c3a15de7":"# Applying the model to the x_test\n\npred = adaboost.predict(x_test)\npred","6659e07a":"# Finding Accuracy\n\nada = accuracy_score(y_test,pred)*100","6fe737e8":"# Confusion Matrix\n\ncm_ada=confusion_matrix(pred,y_test)\nprint(cm_ada)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred,y_test))","a42f4b07":"# Plotting the ROC Curve\n\nprob_adaboost= adaboost.predict_proba(x_test)\nauc_adaboost = roc_auc_score(y_test,prob_adaboost[:,1])\nfpradaboost,tpradaboost,_= roc_curve(y_test,prob_adaboost[:,1])\nroc_auc_adaboost=auc(fpradaboost,tpradaboost)\nplt.plot(fpradaboost,tpradaboost,label = \"AUC = %.2f\" % auc_adaboost)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for AdaBoost (Entropy-Decision Tree)\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","971a42df":"#Fitting the model\n\nxgb =  XGBClassifier(learning_rate =0.6,n_estimators=100,max_depth=10,min_child_weight=2,booster='gblinear',\n                     subsample=0.8,colsample_bytree=0.8,nthread=5,scale_pos_weight=1,seed=27)\nxgb.fit(x_train, y_train)","cac5cc5d":"# Applying the model to the x_test\n\n\npredxg = xgb.predict(x_test)\n\n# Finding Accuracy\nxg = accuracy_score(y_test,predxg)*100\n","513022c7":"# Confusion Matrix\n\ncm_xg=confusion_matrix(predxg,y_test)\nprint(cm_xg)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(predxg,y_test))","c61ca646":"# Plotting the ROC Curve\n\nprob_xgb= xgb.predict_proba(x_test)\nauc_xgb = roc_auc_score(y_test,prob_xgb[:,1])\nfprxgb,tprxgb,_= roc_curve(y_test,prob_xgb[:,1])\nroc_auc_xgb=auc(fprxgb,tprxgb)\nplt.plot(fprxgb,tprxgb,label = \"AUC = %.2f\" % auc_xgb)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for XGBoost\")\nplt.plot([0,1],[0,1],\"--\")\nplt.legend()\nplt.show()","601712f6":"# Accuracy values for all the models\nprint(\"1)  Logistic Regression    :\",round(log, 2))\nprint(\"2)  KNN                    :\",round(KNN, 2))\nprint(\"3)  Naive-Bayes            :\",round(GNB, 2))\nprint(\"4)  SVM                    :\",round(SVC, 2))\nprint(\"5)  Decision Tree          :\",round(DTREE, 2))\nprint(\"6)  Gradient Boosting      :\",round(gbc, 2))\nprint(\"7)  Random Forest          :\",round(RFC, 2))\nprint(\"8)  AdaBoost               :\",round(ada, 2))\nprint(\"9)  XGBoost                :\",round(xg, 2))","5266a0b1":"# Area Under the Curve(AUC) of all the models\nprint('Area under the curve for Logistic Regression :',round(roc_auc, 2))\nprint('Area under the curve for KNN                 :',round(roc_auc_knn, 2))\nprint('Area under the curve for Naive-Bayes         :',round(roc_auc_gnb, 2))\nprint('Area under the curve for SVM                 :',round(roc_auc_svc, 2))\nprint('Area under the curve for Decision Tree       :',round(roc_auc_dt, 2))\nprint('Area under the curve for Gradient Boosting   :',round(roc_auc_GBC, 2))\nprint('Area under the curve for Random Forest       :',round(roc_auc_rfc, 2))\nprint('Area under the curve for AdaBoost            :',round(roc_auc_adaboost, 2))\nprint('Area under the curve for XGBoost             :',round(roc_auc_xgb, 2))","6aca2de9":"#ROC Curve for all models\nplt.figure(figsize = (20,10))\nplt.plot(fprlr,tprlr,label = \"Logistic Regression\")\nplt.plot(fprknn,tprknn,label = \"KNN\")\nplt.plot(fprgnb,tprgnb,label = \"Naive-Bayes\")\nplt.plot(fprsvc,tprsvc,label = \"SVM\")\nplt.plot(fprdt,tprdt,label = \"Decision Tree\")\nplt.plot(fprGBC,tprGBC,label = \"Gradient Boosting\",color='black')\nplt.plot(fprrfc,tprrfc,label = \"Random Forest\",color='yellow')\nplt.plot(fpradaboost,tpradaboost,label = \" AdaBoost\")\nplt.plot(fprxgb,tprxgb,label = \"XGBoost\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True)","b3c3c31a":"# f1_score of all models\nprint(\"1)  Logistic Regression    :\",round(f1_score(y_pred,y_test), 2))\nprint(\"2)  KNN                    :\",round(f1_score(pred_knn,y_test), 2))\nprint(\"3)  Naive-Bayes            :\",round(f1_score(pred_gnb,y_test), 2))\nprint(\"4)  SVM                    :\",round(f1_score(pred_svc,y_test), 2))\nprint(\"5)  Decision Tree          :\",round(f1_score(pred_dt,y_test), 2))\nprint(\"6)  Gradient Boosting      :\",round(f1_score(Y_predict,y_test), 2))\nprint(\"7)  Random Forest          :\",round(f1_score(pred_rf,y_test), 2))\nprint(\"8)  AdaBoost               :\",round(f1_score(pred,y_test), 2))\nprint(\"9)  XGBoost                :\",round(f1_score(predxg,y_test), 2))","62a7bd90":"#Accessing the False Positives of all models from their confusion Matrix\nprint(\"1)  Logistic Regression    :\",cmlr[0][1])\nprint(\"2)  KNN                    :\",cm_knn[0][1])\nprint(\"3)  Naive-Bayes            :\",cm_gnb[0][1])\nprint(\"4)  SVM                    :\",cm_svc[0][1])\nprint(\"5)  Decision Tree          :\",cm_dt[0][1])\nprint(\"6)  Gradient Boosting      :\",cm_gbc[0][1])\nprint(\"7)  Random Forest          :\",cm_rf[0][1])\nprint(\"8)  AdaBoost               :\",cm_ada[0][1])\nprint(\"9)  XGBoost                :\",cm_xg[0][1])","8e7bb951":"# Step 2: Loading the Dataset","5c1a2fb5":"<b> The dataset has only 1 duplicate value. Hence, we dont need to treat this as well.<\/b>","59f2903a":"1)  Random Forest          : 85.25 %\n\n2)  XGBoost                : 85.25 %\n\n3)  Logistic Regression    : 85.25%\n\n4)  KNN                    : 85.25 %\n\n5)  Gradient Boosting      : 83.61 %\n\n6)  Naive-Bayes            : 83.61 %\n\n7)  SVM                    : 81.97 %\n\n8)  Decision Tree          : 81.97 %\n\n9)  AdaBoost               : 78.69 %\n\n\nHere, <b>Random Forest Classifier, XGBoost, Logistic Regression and KNN have the highest accuracy rate.<\/b> But, We need to choose one best model.<\/b>\n\n<b>In this case, Accuracy metric cannot be taken as the best indicator of model performance. Therefore, we need to consider other metrics before deciding the best model.<\/b>","d8c98608":"# 8) AdaBoost (Entropy-Decision Tree)","3ad11b33":"# 9) XGBoost ","df4c9132":"# 4) SVM","41d1a7ed":"# Step 4: Data Pre-Processing\n\n   # a) Treating Missing Values","5ce9f2fe":"# Step 6: Defining the Tatget and Predictor Variables and Standard Scaling\n\n<b> If a feature\u2019s variance is more than the variance of other features, that particular feature might dominate other features in the dataset. This could affect the accuracy of predictions. Hence, we need to scale all the features to a standard centred scale. For this purpose, we use StandardScaler() method.<\/b>","638ce643":"# 3) ROC Curve","961a4c86":"<b>fbs is the lowest correlated variable with the target variable. All other variables have a significant correlation with the target variable.<\/b>\n","496683c9":"The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values. AUC calculates the area under the ROC curve, and therefore it is between 0 and 1.<b> For any classifier, the higher the AUC of a model the better it is.<\/b> The AUC values of all the models are listed below:\n\n1)  Random Forest          : 0.94\n\n2)  Gradient Boosting      : 0.94\n\n3)  KNN                    : 0.92\n\n4)  AdaBoost               : 0.89\n\n5)  SVM                    : 0.91\n\n6)  Naive-Bayes            : 0.91\n\n7)  XGBoost                : 0.90\n\n8)  Logistic Regression    : 0.90\n\n9)  Decision Tree          : 0.85\n\n\nHere, <b>Random Forest<\/b> and <b>Gradient Boosting<\/b> have the highest AUC value. ","930eed46":"<b>Here we can see that there are outliers in the trestbps and chol columns in the dataset. There are two options here:\n    \n    \n1) We can drop the entire column from the dataset\n\n2) We can treat the outliers\n\nThe best option is to treat the outliers rather than removing the entire column. This way we will not lose more data.\n\nI am using Capping method in order to treat the outliers<\/b>","646808a5":"# c) Checking for Imbalance\n\n<b>From the graph, it is clear that the class distribution is almost balanced. Hence, we wont have any issues with imbalance.<\/b>","daf46068":"# 1) Accuracy","b6558d55":"# 4) F1-Score","5d1dca23":"# 2) Area Under Curve (AUC)","04eedb72":"# Step 7: Fitting the dataset to various models\n\n<b>We will fit the dataset to various models and find out the best fit model among these.\n\nVarious models used in this notebook are:\n    \n\n1)  Logistic Regression\n\n2)  KNN                \n\n3)  Naive-Bayes       \n\n4)  SVM                   \n\n5)  Decision Tree         \n\n6)  Gradient Boosting     \n\n7)  Random Forest         \n\n8)  AdaBoost             \n\n9)  XGBoost    \n\n<\/b>           \n\n# 1) Logistic Regression","e8c3bc3c":"# 7) Random Forest","179da563":"# Independent Variables vs Target Variable","6c3b2b11":"# Step 9: Finalizing the Best Model\n\nAfter all the comparison using 5 different metrics:\n\n# Both <b><u>Random Forest Classifier<\/u><\/b> and <b><u>K-Nearest Neighbors (KNN)<\/u><\/b> prove to be best models for predicting if a person has a heart disease.\n\n\n","a56c6f67":"Dataset is very less skewed.\n# e) Identifying Outliers with Interquartile Range (IQR) and Boxplot Visualization\n\n<b>Outliers are observations that are significantly different from other data points. Outliers can adversely affect the training process of a machine learning algorithm, resulting in very low accuracy.\n\nOutliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training time, less accurate models and ultimately poorer results.\n\nThe interquartile range (IQR) is a measure of statistical dispersion and is calculated as the difference between the 75th and 25th percentiles. It is represented by the formula IQR = Q3 \u2212 Q1.<\/b>","d8d0d806":"# 2) KNN","46ba6c00":"# 5) Decision Tree","6e290b7c":"# Step 3: Understanding the Structure of the Dataset","639b22e6":"# Heart Disease Prediction\n\n# Lasya Ippagunta\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\nAttribute Information:\n\n1. age\n2. sex (Male\/Female)\n3. CP - Chest pain type (4 values)\n4. trestbps - Resting blood pressure\n5. Chol - serum cholestoral in mg\/dl\n6. FBS - Fasting blood sugar > 120 mg\/dl\n7. RestECG - Resting Electrocardiographic results (values 0,1,2)\n8. thalach - Maximum heart rate achieved\n9. exang - Exercise induced angina\n10. oldpeak - ST depression induced by exercise relative to rest\n11. Slope - The slope of the peak exercise ST segment\n12. CA - Number of major vessels (0-3) colored by flourosopy\n13. thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n\nWe are supposed to use these attributes to predict if a person has a heart disease or not.","f205384c":"# Step 5: Data Visualization\n\n# Correlation between each predictor and the target variable. \n<b>This can be done using the corr() method and we can visualize using seabors plotting method heatmap().<\/b>","d9c10a78":"# 5)Type I Error","e9672a16":"The <b>Receiver Operating Characteristic (ROC)<\/b> curve is plot which shows the performance of a binary classifier as function of its cut-off threshold. ROC curve is one of the most effective evaluation metrics because it visualizes the accuracy of predictions for a whole range of cutoff values. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for all possible threshold values. <b>A model is said to be the best model when the ROC is close to the upper left corner.<\/b>\n\nLooking at the ROC curve plot above, the <b>yellow curve (Random Forest) and Black curve (Gradient Boosting), are the curves that is closest to the upper left corner. Hence, based on the ROC plot, Random Forest or Gradient Boosting are the best fit models.<\/b>","739fac97":"# Average vs Maximum Heart Rate for Heart Diseases","1e627221":"<b>Precision<\/b>           - It is the number of True Positive divided by the number of positive results predicted by the classifier.\n\n<b>Recall\/ Sensitivity<\/b> - It is the number of True Positives divided by the number of all relevant samples\n\n<b>F1 Score<\/b>            - F1 Score is the Harmonic Mean between precision and recall.\n\nF1 Score tells how precise the classifier is (how many values it classifies correctly).\n\n<b>The greater the F1 Score, the better is the performance of our model.<\/b>\n\nf1_Scores for all the models are:\n\n1)  KNN                    : 0.88\n\n2)  XGBoost                : 0.87\n\n3)  Random Forest          : 0.87\n\n4)  Logistic Regression    : 0.87\n\n5)  Gradient Boosting      : 0.85\n\n6)  Naive-Bayes            : 0.86\n\n7)  SVM                    : 0.85\n\n8)  Decision Tree          : 0.83\n\n9)  AdaBoost               : 0.81\n\n\nHere, <b>KNN<\/b> has the highest f1_score. Hence, based on the f1_score, KNN is the best fit model.","5f7b0127":"# 6) Gradient Boosting","bba729f7":"# d) Checking Skewness of data\n\n<b>Skewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. If there is too much skewness in the data, then the statistical model don\u2019t work properly. This is because, in skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model\u2019s performance. Hence, we need to check for outliers.<\/b>","a1570818":"False Positives(Type I Error) occurs when we incorrectly reject a true hypothesis.<b>Lower the value of False Positives, better is the model<\/b>. This is because, while predicting, <b>if we predict that a person has a heart disease, but later he\/she does not actually have any heart disease, then this kind of wrong prediction could further increase the risk factor to an alarming range.<\/b>\n\nThe False Positives(Type I Error) for all the models can be accessed from the confusion matrix. The values for various models are:\n\n1)  AdaBoost               : 6\n\n2)  Random Forest          : 5\n\n3)  Naive-Bayes            : 3\n\n4)  Gradient Boosting      : 5\n\n5)  Decision Tree          : 4\n\n6)  SVM                    : 4\n\n7)  XGBoost                : 3\n\n8)  Logistic Regression    : 3\n\n9)  KNN                    : 2\n\n\n<b>KNN algorithm has the least number of False Positives(Type I Error). Hence, based on the False Positives(Type I Error), KNN is the best fit model.<\/b>","f0090687":"# 3) Naive-Bayes","7f34209c":"# Step 1: Importing Libraries","ed8434bc":"<b> No Missing Values in the dataset. Hence no treatment for missing values required<\/b>\n\n\n# b) Finding and removing all the duplicated values","ffc944e1":"# Step 8: Choosing the Best model\n\nThere are various ways to evaluate a classification model. Some of them are:\n \n1) Accuracy\n    \n2) AUC\n    \n3) ROC\n    \n4) f1 Score\n    \n5) Type I Error\n\nI am evaluating with all these metrics in order to find the best fit model\n\n# Confusion Matrix\n\nA confusion matrix is an N X N matrix, where N is the number of classes being predicted. Confusion Matrix gives us a matrix as output and describes the complete performance of the model.\n\nThe correct predictions falls on the diagonal line of the matrix.\n\n4 important terms in Confusion Matrix:\n\n<b>True Positives<\/b>  : We predict YES and the actual output is also YES.\n\n<b>True Negatives<\/b>  : We predict NO and the actual output is NO.\n\n<b>False Positives(Type I Error)<\/b> : We predict YES but the actual output is NO.\n\n<b>False Negatives(Type II error)<\/b> : We predict NO but the actual output is YES.\n\n<b>The Confusion matrix in itself is not a performance measure, but almost all of the performance metrics are based on Confusion Matrix.\n"}}