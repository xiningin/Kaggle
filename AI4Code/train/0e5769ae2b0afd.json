{"cell_type":{"4530daaa":"code","b21f99a5":"code","0896992f":"code","08c2d3c7":"code","ba133627":"code","098bbf96":"code","7cb011f8":"code","c0ba1b0c":"code","1c133fd8":"code","599d72ff":"code","ed068729":"code","a2bfa93f":"code","5132d379":"code","327cdf76":"code","517ce63d":"code","4d025725":"code","09711ec3":"code","8e48c962":"code","200c6d16":"code","303a2468":"code","e44caba2":"code","96cfd3ac":"code","d097624b":"code","b40ee8f9":"code","8dc9926f":"code","6258483a":"code","fd4452e9":"code","0e92590e":"code","16f6949d":"code","fc76638c":"code","53794574":"code","3baa3b07":"code","9afbea2b":"code","c38552d0":"code","1e4f3902":"code","6c88587c":"code","3676f6d2":"code","8fd8c268":"code","1e85ad2f":"code","d0872203":"code","650c0378":"code","3ce3968f":"code","d88e991a":"code","ba3e53f8":"code","c3515a05":"code","b0d5af2b":"code","d4f1ddb1":"code","c1e81008":"code","4b66750a":"code","86276385":"markdown","fbaf2cf9":"markdown","c0dd55fe":"markdown","bd67c61a":"markdown","bec3611f":"markdown","3d447ada":"markdown","6c52013d":"markdown","3c71b8f6":"markdown","e9280471":"markdown","acee0588":"markdown","9bcbfba5":"markdown","9e76bb8f":"markdown","cc035c55":"markdown","0d7758cf":"markdown","33406b8d":"markdown","b613cd26":"markdown","95de3bf8":"markdown","a8552abf":"markdown","0b87fff0":"markdown","690d543d":"markdown"},"source":{"4530daaa":"\n#!pip install facenet-pytorch\n!pip install \/kaggle\/input\/pytorch\/facenet_pytorch-2.2.8-py3-none-any.whl\nfrom facenet_pytorch.models.inception_resnet_v1 import get_torch_home\ntorch_home = get_torch_home()\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p $torch_home\/checkpoints\/\n!pwd\n!cp \/kaggle\/input\/pytorchf\/20180402-114759-vggface2-logits.pth\/20180402-114759-vggface2-logits.pth $torch_home\/checkpoints\/vggface2_DG3kwML46X.pt\n!cp \/kaggle\/input\/pytorchf\/20180402-114759-vggface2-features.pth\/20180402-114759-vggface2-features.pth $torch_home\/checkpoints\/vggface2_G5aNV2VSMn.pt","b21f99a5":"#Import Packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom skimage.color import rgb2gray\nimport cv2\nimport cv2 as cv\nimport os\nimport matplotlib.pylab as plt\ntrain_dir = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/'\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import ndimage\n# See github.com\/timesler\/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\nimport os\nimport sys\nimport random\nimport math\nimport numpy as np\nimport skimage.io\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob as glob\nfilenames = glob.glob('\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/*.mp4')\nimport torch  \nimport time\n# See github.com\/timesler\/facenet-pytorch:\nfrom facenet_pytorch import MTCNN, InceptionResnetV1 , extract_face\nfrom tqdm.notebook import tqdm\n\n","0896992f":"#Load Data\nDATA_FOLDER = '\/kaggle\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nTEST_FOLDER = 'test_videos'\n\nprint(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\nprint(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")","08c2d3c7":"train_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\next_dict = []\nfor file in train_list:\n    file_ext = file.split('.')[1]\n    if (file_ext not in ext_dict):\n        ext_dict.append(file_ext)\nprint(f\"Extensions: {ext_dict}\")    ","ba133627":"for file_ext in ext_dict:\n    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")","098bbf96":"test_list = list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))\next_dict = []\nfor file in test_list:\n    file_ext = file.split('.')[1]\n    if (file_ext not in ext_dict):\n        ext_dict.append(file_ext)\nprint(f\"Extensions: {ext_dict}\")\nfor file_ext in ext_dict:\n    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")","7cb011f8":"json_file = [file for file in train_list if  file.endswith('json')][0]\nprint(f\"JSON file: {json_file}\")","c0ba1b0c":"def get_meta_from_json(path):\n    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n    df = df.T\n    return df\n\nmeta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\nmeta_train_df.head()","1c133fd8":"meta_train_df.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()","599d72ff":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","ed068729":"missing_data(meta_train_df)","a2bfa93f":"missing_data(meta_train_df.loc[meta_train_df.label=='REAL'])","5132d379":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","327cdf76":"unique_values(meta_train_df)","517ce63d":"def most_frequent_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    items = []\n    vals = []\n    for col in data.columns:\n        itm = data[col].value_counts().index[0]\n        val = data[col].value_counts().values[0]\n        items.append(itm)\n        vals.append(val)\n    tt['Most frequent item'] = items\n    tt['Frequence'] = vals\n    tt['Percent from total'] = np.round(vals \/ total * 100, 3)\n    return(np.transpose(tt))","4d025725":"most_frequent_values(meta_train_df)","09711ec3":"def plot_count(feature, title, df, size=1):\n    '''\n    Plot count of classes \/ feature\n    param: feature - the feature to analyze\n    param: title - title to add to the graph\n    param: df - dataframe from which we plot feature's classes distribution \n    param: size - default 1.\n    '''\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()    ","8e48c962":"plot_count('split', 'split (train)', meta_train_df)","200c6d16":"plot_count('label', 'label (train)', meta_train_df)","303a2468":"meta = np.array(list(meta_train_df.index))\nstorage = np.array([file for file in train_list if  file.endswith('mp4')])\nprint(f\"Metadata: {meta.shape[0]}, Folder: {storage.shape[0]}\")\nprint(f\"Files in metadata and not in folder: {np.setdiff1d(meta,storage,assume_unique=False).shape[0]}\")\nprint(f\"Files in folder and not in metadata: {np.setdiff1d(storage,meta,assume_unique=False).shape[0]}\")","e44caba2":"fake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(3).index)\nfake_train_sample_video","96cfd3ac":"def display_image_from_video(video_path):\n    '''\n    input: video_path - path for video\n    process:\n    1. perform a video capture from the video\n    2. read the image\n    3. display the image\n    '''\n    capture_image = cv2.VideoCapture(video_path) \n    ret, frame = capture_image.read()\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111)\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    ax.imshow(frame)","d097624b":"for video_file in fake_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","b40ee8f9":"real_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='REAL'].sample(3).index)\nreal_train_sample_video","8dc9926f":"for video_file in real_train_sample_video:\n    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))","6258483a":"meta_train_df['original'].value_counts()[0:5]","fd4452e9":"def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n    '''\n    input: video_path_list - path for video\n    process:\n    0. for each video in the video path list\n        1. perform a video capture from the video\n        2. read the image\n        3. display the image\n    '''\n    plt.figure()\n    fig, ax = plt.subplots(2,3,figsize=(16,8))\n    # we only show images extracted from the first 6 videos\n    for i, video_file in enumerate(video_path_list[0:6]):\n        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n        capture_image = cv2.VideoCapture(video_path) \n        ret, frame = capture_image.read()\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        ax[i\/\/3, i%3].imshow(frame)\n        ax[i\/\/3, i%3].set_title(f\"Video: {video_file}\")\n        ax[i\/\/3, i%3].axis('on')","0e92590e":"same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='meawmsgiti.mp4'].index)\ndisplay_image_from_video_list(same_original_fake_train_sample_video)","16f6949d":"same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='qeumxirsme.mp4'].index)\ndisplay_image_from_video_list(same_original_fake_train_sample_video)","fc76638c":"same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='atvmxvwyns.mp4'].index)\ndisplay_image_from_video_list(same_original_fake_train_sample_video)","53794574":"same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='qeumxirsme.mp4'].index)\ndisplay_image_from_video_list(same_original_fake_train_sample_video)","3baa3b07":"test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])","9afbea2b":"test_videos.head()","c38552d0":"display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[0].video))","1e4f3902":"display_image_from_video_list(test_videos.sample(6).video, TEST_FOLDER)","6c88587c":"fake_videos = list(meta_train_df.loc[meta_train_df.label=='FAKE'].index)","3676f6d2":"from IPython.display import HTML\nfrom base64 import b64encode\n\ndef play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n    '''\n    Display video\n    param: video_file - the name of the video file to display\n    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n    '''\n    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n    data_url = \"data:video\/mp4;base64,\" + b64encode(video_url).decode()\n    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video\/mp4\"><\/video>\"\"\" % data_url)","8fd8c268":"play_video(fake_videos[0])","1e85ad2f":"play_video(fake_videos[1])","d0872203":"play_video(fake_videos[2])","650c0378":"play_video(fake_videos[220])","3ce3968f":"import cv2 as cv\nimport os\nimport matplotlib.pylab as plt\ntrain_dir = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/'\nfig, ax = plt.subplots(1,1, figsize=(15, 15))\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\n# video_file = train_video_files[30]\nvideo_file = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/afoovlsmtx.mp4'\ncap = cv.VideoCapture(video_file)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \nax.imshow(image)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.title.set_text(f\"FRAME 0: {video_file.split('\/')[-1]}\")\nplt.grid(False)","d88e991a":"\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')","ba3e53f8":"# Load face detector\nmtcnn = MTCNN(margin=14, keep_all=True, factor=0.5, device=device).eval()\n\n# Load facial recognition model\nresnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()","c3515a05":"class DetectionPipeline:\n    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n    \n    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n        \"\"\"Constructor for DetectionPipeline class.\n        \n        Keyword Arguments:\n            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n                throughout the video. If not specified (i.e., None), all frames will be loaded.\n                (default: {None})\n            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n            resize {float} -- Fraction by which to resize frames from original prior to face\n                detection. A value less than 1 results in downsampling and a value greater than\n                1 result in upsampling. (default: {None})\n        \"\"\"\n        self.detector = detector\n        self.n_frames = n_frames\n        self.batch_size = batch_size\n        self.resize = resize\n    \n    def __call__(self, filename):\n        \"\"\"Load frames from an MP4 video and detect faces.\n\n        Arguments:\n            filename {str} -- Path to video.\n        \"\"\"\n        # Create video reader and find length\n        v_cap = cv2.VideoCapture(filename)\n        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Pick 'n_frames' evenly spaced frames to sample\n        if self.n_frames is None:\n            sample = np.arange(0, v_len)\n        else:\n            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n\n        # Loop through frames\n        faces = []\n        frames = []\n        for j in range(v_len):\n            success = v_cap.grab()\n            if j in sample:\n                # Load frame\n                success, frame = v_cap.retrieve()\n                if not success:\n                    continue\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = Image.fromarray(frame)\n                \n                # Resize frame to desired size\n                if self.resize is not None:\n                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n                frames.append(frame)\n\n                # When batch is full, detect faces and reset frame list\n                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n                    faces.extend(self.detector(frames))\n                    frames = []\n\n        v_cap.release()\n\n        return faces    \n\n\ndef process_faces(faces, resnet):\n    # Filter out frames without faces\n    faces = [f for f in faces if f is not None]\n    faces = torch.cat(faces).to(device)\n\n    # Generate facial feature vectors using a pretrained model\n    embeddings = resnet(faces)\n\n    # Calculate centroid for video and distance of each face's feature vector from centroid\n    centroid = embeddings.mean(dim=0)\n    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n    \n    return x","b0d5af2b":"   # Define face detection pipeline\ndetection_pipeline = DetectionPipeline(detector=mtcnn, batch_size=60, resize=0.25)\n\n# Get all test videos\nfilenames = glob.glob('\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/*.mp4')\n\nX = []\nstart = time.time()\nn_processed = 0\nwith torch.no_grad():\n    for i, filename in tqdm(enumerate(filenames), total=len(filenames)):\n        try:\n            # Load frames and find faces\n            faces = detection_pipeline(filename)\n            \n            # Calculate embeddings\n            X.append(process_faces(faces, resnet))\n\n        except KeyboardInterrupt:\n            print('\\nStopped.')\n            break\n\n        except Exception as e:\n            print(e)\n            X.append(None)\n        \n        n_processed += len(faces)\n        print(f'Frames per second (load+detect+embed): {n_processed \/ (time.time() - start):6.3}\\r', end='')","d4f1ddb1":"bias = -0.4\nweight = 0.068235746\n\nsubmission = []\nfor filename, x_i in zip(filenames, X):\n    if x_i is not None and len(x_i) == 10:\n        prob = 1 \/ (1 + np.exp(-(bias + (weight * x_i).sum())))\n    else:\n        prob = 0.5\n    submission.append([os.path.basename(filename), prob])","c1e81008":"submission = pd.DataFrame(submission, columns=['filename', 'label'])\nsubmission.sort_values('filename').to_csv('submission.csv', index=False)","4b66750a":"plt.hist(submission.label, 20)\nplt.show()\nsubmission","86276385":"**Submission**","fbaf2cf9":"## Check files Types\nVerifying the file extensions as mp4 or somewhat different","c0dd55fe":"*Install Dependency*","bd67c61a":"## Play video files  ","bec3611f":"## Few real videos","3d447ada":"## Missing video (or meta) data","6c52013d":"**Create MTCNN and Inception Resnet models**","3c71b8f6":"## Missing data\nMissing data exploration","e9280471":"## Unique values\n*Finding the unique values*","acee0588":"## Face Detection","9bcbfba5":"> **Process test Files**","9e76bb8f":"## Most frequent originals","cc035c55":"## Few fake videos","0d7758cf":"1. ## Videos with similar original count","33406b8d":"*Predict classes*","b613cd26":"*Plotting the graph*","95de3bf8":"## Test video files\n","a8552abf":"*training the real data*","0b87fff0":"*Analysis for Metadata Json file*","690d543d":"**Plotting the graphs**"}}