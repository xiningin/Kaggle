{"cell_type":{"136a5064":"code","85c4f057":"code","2c596644":"code","14476fd2":"code","9959097a":"code","7f7a889a":"code","95c233b6":"code","0e597254":"code","3a73510f":"code","c4a993fb":"code","a2fbe3cb":"code","42c75245":"code","bf43f16d":"code","d26c8abf":"code","dd713ef4":"code","56c3fa45":"code","95c12b78":"code","d18b8fdf":"code","e1f70c05":"code","af08487d":"code","db96ad65":"code","f2cecd79":"code","857ba80e":"code","211ddae5":"code","04e8986f":"code","5573e6b8":"code","e9e7528c":"code","e5c57b1e":"code","6ffd33a8":"code","3e14ddc3":"code","eed555f1":"code","6ea66c51":"code","067658c9":"code","620b25d6":"code","96488fc9":"code","31253611":"code","97ef54e8":"markdown","4b8d3487":"markdown","af4fdd73":"markdown","c736cda3":"markdown","abc5b477":"markdown","2c1a1ab3":"markdown","c804933b":"markdown","26d15614":"markdown","87380d47":"markdown","80841063":"markdown","4dcad226":"markdown","75cedef9":"markdown","f3a108b9":"markdown","2ea7a5fc":"markdown","db19ccf9":"markdown","795f4bc3":"markdown","95ab378c":"markdown","5cd730e0":"markdown","cef47cf7":"markdown"},"source":{"136a5064":"pip install -U lightautoml","85c4f057":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nimport time\nimport random\nimport torch\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.dataset.roles import CategoryRole, DatetimeRole\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c596644":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv\", low_memory=False)#, nrows=10000)\ntrain[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv\", low_memory=False)\ntest[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","14476fd2":"test.info(memory_usage=\"deep\")","9959097a":"train.head(10)","7f7a889a":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\ntarget_names = [\"Carbon monoxide\", \"Benzene\", \"Nitrogen oxides\"]","95c233b6":"def add_new_plot_features(df):\n    \"\"\"\n    Adds new features to a given dataset for plotting\n    \"\"\"\n    df[\"month\"] = df[\"date_time\"].dt.month\n    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n    df[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"quarter\"] = df[\"date_time\"].dt.quarter\n    df[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\n#     df[\"is_winter\"] = df[\"month\"].isin([1, 2, 12])\n#     df[\"is_sprint\"] = df[\"month\"].isin([3, 4, 5])\n#     df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8])\n#     df[\"is_autumn\"] = df[\"month\"].isin([9, 10, 11])\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    return df\n\ndef add_new_ml_features(df, i=3): # i=3 is for heatmap plot\n    \"\"\"\n    Adds new features to a given dataset for training\n    \"\"\"\n    # Features to be added to every target dataset\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"maximum_hours\"] =  df[\"hour\"].isin([8, 9, 17, 18, 19, 20]).astype(\"int\")\n    # Marking weekends because they usually have lower target values\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) \/ df[\"relative_humidity\"]\n    \n    # A list of features to generate shifted and lagged values\n    shift_features = [[\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\", \"target_benzene_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_5\"],\n                      # Features for heatmap plot\n                      [\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]]\n    \n    # Amounts of hour shifts and lags\n    shifts = [1, 2, 3, 4, 5, 6, 12, 24]\n#     shifts = [1, 2, 3, 6, 12, 24]\n    \n    for feature in shift_features[i]:\n        for shift in shifts:\n            df[feature+\"-\"+str(shift)+\"abs_shift\"] = df[feature] - df[feature].shift(periods=shift, fill_value=0)\n            df[feature+\"+\"+str(shift)+\"abs_shift\"] = df[feature] - df[feature].shift(periods=-shift, fill_value=0)\n#             df[feature+\"-\"+str(shift)+\"prc_shift\"] = (df[feature] \/ df[feature].shift(periods=shift, fill_value=0)) - 1\n#             df[feature+\"+\"+str(shift)+\"prc_shift\"] = (df[feature] \/ df[feature].shift(periods=-shift, fill_value=0)) - 1\n    \n#     # Dropping the least important features as per previous runs\n#     to_drop = [ [\"sensor_2+2abs_shift\", \"sensor_1-2abs_shift\", \"deg_C-4abs_shift\", \"sensor_2-3abs_shift\", \"sensor_1+3abs_shift\",\n#                  \"deg_C-5abs_shift\", \"sensor_1+2abs_shift\", \"sensor_1-4abs_shift\", \"sensor_2-4abs_shift\", \"sensor_1-3abs_shift\"],\n#                 [\"sensor_5-12abs_shift\", \"sensor_3-5abs_shift\", \"sensor_5-3abs_shift\", \"sensor_5-4abs_shift\", \"sensor_5-5abs_shift\",\n#                  \"sensor_4-5abs_shift\", \"absolute_humidity-3abs_shift\", \"sensor_5+6abs_shift\", \"sensor_5+3abs_shift\", \"sensor_1-5abs_shift\"],\n#                 [\"sensor_3+2abs_shift\", \"sensor_1+4abs_shift\", \"sensor_1+3abs_shift\", \"sensor_2+3abs_shift\", \"maximum_hours\",\n#                  \"SMC+5abs_shift\", \"sensor_3+1abs_shift\", \"sensor_3+3abs_shift\", \"sensor_5+5abs_shift\", \"sensor_1+2abs_shift\"]\n#                 ]\n#     if i <= 2:\n#         df.drop(to_drop[i], axis=1, inplace=True)\n#     # Replacing infinity values as a result of devision by zero at the end of a dataset\n#     df.replace(to_replace=np.inf, value=0, inplace=True)\n    \n#     return df.drop([\"hour\", \"week_of_year\"], axis=1)\n    return df.drop([\"hour\"], axis=1)","0e597254":"train_copy = train.copy()\ntest_copy = test.copy()\ntrain = add_new_plot_features(train)\ntest = add_new_plot_features(test)","3a73510f":"# Plot dataframe\ndf = pd.concat([train[\"date_time\"], test[\"date_time\"]], axis=0).reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(16, 1.5))\nbar1 =  ax.barh(0, 7111+2247, color=\"salmon\", height=0.2)\nbar2 =  ax.barh(0, 7111, color=\"teal\", height=0.2)\nax.set_title(\"Train and test datasets size comparison\", fontsize=20, pad=5)\nax.bar_label(bar1, [\"Test dataset\"], label_type=\"edge\", padding=-170,\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.bar_label(bar2, [\"Train dataset\"], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.set_xticks([0, 7111, 7111+2247])\nax.set_xticklabels([\"2010-03-10\", \"2011-01-01\", \"2011-04-04\"])\nax.set_yticks([])\nplt.show();","c4a993fb":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    axs[i].plot(train[\"date_time\"], train[targets[i]], color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","a2fbe3cb":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\ndf[\"day\"] = df[\"date_time\"].dt.dayofyear\ndf[\"weekday\"] = df[\"date_time\"].dt.dayofweek\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\n# An array of number of days of year (i.e. from 1 to 365) which are mondays to mark week starts\nmondays = df.loc[df[\"weekday\"] == 0][\"day\"].value_counts(sort=False).index\n# An array of number of weeks of year to be used as label ticks\nweeks = df[\"date_time\"].dt.isocalendar().week.unique()[1:]\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\nfor i in [0, 1, 2]:\n    axs[i].plot(df.groupby(\"day\")[targets[i]].mean().index,\n                df.groupby(\"day\")[targets[i]].mean().values, color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) mean levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Week starts\", fontsize=14, labelpad=5)\n    axs[i].set_xticks(mondays)\n    axs[i].set_xticklabels(weeks)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","42c75245":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[0], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[0]], color=\"steelblue\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","bf43f16d":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[1], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[1]], color=\"palevioletred\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","d26c8abf":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[2], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[2]], color=\"goldenrod\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","dd713ef4":"fig, axs = plt.subplots(figsize=(15, 6), ncols=3, nrows=1, sharey=False)\n\nfig.suptitle(\"Target values distribution\", fontsize=20)\n\ncolors = [\"mediumorchid\", \"lightseagreen\", \"cornflowerblue\"]\n\nfor i in [0, 1, 2]:\n    axs[i].hist(train[targets[i]], bins=60, edgecolor=\"black\", color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=5)\n    axs[i].set_ylabel(\"Amount of values\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(f\"{target_names[i]} level\", fontsize=13, labelpad=5)\n    axs[i].grid(axis=\"y\")\n\nplt.show();","56c3fa45":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.35\nx = train.groupby(\"hour\")[\"target_carbon_monoxide\"].mean().index\n\nfor i in np.arange(3):\n    bars1 = axs[i].bar(x-width\/2, train.groupby(\"hour\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"cornflowerblue\")\n    bars2 = axs[i].bar(x+width\/2, train.groupby(\"hour\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"palevioletred\")\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(\"Day hours\", fontsize=13, labelpad=5)\n    axs[i].set_xticks(x)\n    axs[i].grid(axis=\"y\")\n    axs[i].legend(fontsize=13)","95c12b78":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 19), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.25)\nwidth=0.35\nx = df.groupby(\"day_of_week\")[\"target_carbon_monoxide\"].mean().index + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width\/2, df.groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x+width\/2, df.groupby(\"day_of_week\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=13)\n\nx = df.groupby(\"month\")[\"target_carbon_monoxide\"].mean().index\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width\/2, df.groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x+width\/2, df.groupby(\"month\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=13)","d18b8fdf":"# Day hours which will be used for plotting data\nhours = [0, 5, 8, 14, 19]\n# Dataframe copy excluding the last row which is the only one representing January\ndf = train.loc[train[\"hour\"].isin(hours)].drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week at given hours\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.15\nx = np.sort(df[\"day_of_week\"].unique()) + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 0].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 0].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 0].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=10)\n\nx = df[\"month\"].unique()\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 1].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 1].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 1].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=10)","e1f70c05":"# Lists of feature names to be used for plots below\nall_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\", \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nall_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\", \"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]\n\nweather_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\"]\nweather_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\"]\n\nsensor_features = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nsensor_feature_names = [\"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]","af08487d":"fig, axs = plt.subplots(figsize=(16, 30), ncols=1, nrows=8, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\nfor i in np.arange(8):\n    legend_lines = [Line2D([0], [0], color=colors[i], lw=10),\n                    Line2D([0], [0], color=\"black\", lw=10)]\n    axs[i].plot(train[\"date_time\"], train[all_features[i]], color=colors[i], label=\"Train data\")\n    axs[i].plot(test[\"date_time\"], test[all_features[i]], color=\"black\", label=\"Test data\")\n    axs[i].set_title(f\"{all_feature_names[i]} levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{all_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].legend(legend_lines, [\"Train data\", \"Test data\"], fontsize=12, loc=1)\n    axs[i].grid(axis=\"both\")","db96ad65":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    # New year days start from 7110th row\n    data = df.iloc[:7110].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in [0, 1, 2]:\n    # New year weeks start from 7159th row. \n    # Because of Jan 1st and 2nd from the test dataset are counted as 52nd week of 2010,\n    # the colored plotline contains some test data. \n    data = df.iloc[:7159].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean weekly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","f2cecd79":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 30), ncols=2, nrows=5, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\", \"goldenrod\", \"indianred\"]\n\nfor i in np.arange(5):\n    data = df.iloc[:7110].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in np.arange(5):\n    data = df.iloc[:7159].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","857ba80e":"# Plot dataframe\ndf = train_copy.copy()\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(12,12))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", linewidths=1,\n                 annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Original dataset correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"bold\")\nplt.setp(ax.get_yticklabels(), weight=\"bold\")\nplt.show();","211ddae5":"# Plot dataframe\ndf = add_new_ml_features(train_copy.copy())\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\": 7})\nax.set_title(\"Original and engineered features correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\")\nplt.show();","04e8986f":"# Checking the most correlated features \nfor col in df.columns:\n    for index in df[col].index:\n        if df[col][index] != 1:\n            if (df[col][index] >= 0.93) | (df[col][index] <=-0.93):\n                print(f\"Correlation of {index} and {col} is {df[col][index]}\")","5573e6b8":"def prepare_dataset(train_copy, test_copy, i):\n    \n    X = add_new_ml_features(train_copy.copy(), i)\n\n    # Dropping the last row which is 2011-01-01 00:00:00\n    if X.index[-1] == 7110:\n        X.drop([7110], axis=0, inplace=True)\n\n    # Resetting dataframe index\n    X.reset_index(drop=True, inplace=True)\n\n    # The months will be used for folds split\n    months = X[\"date_time\"].dt.month\n\n    # Adding 72 last train set rows to the head of test set in order to get shifting feature values\n    X_test_temp = pd.concat([train_copy.iloc[-25:-1].drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1), test_copy], axis=0)\n    X_test_temp.reset_index(inplace=True, drop=True)\n    X_test = add_new_ml_features(X_test_temp.copy(), i)\n    # Deleting added train set rows\n    X_test.drop(X_test.loc[:23].index, axis=0, inplace=True)\n    X_test.reset_index(inplace=True, drop=True)\n\n    y = np.log1p(X[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]])\n    X.drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1, inplace=True)\n    \n    X['date_time'] = X['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n    X_test['date_time'] = X_test['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n    \n#     print(X.shape, y.shape, X_test.shape)\n#     display(X.head())\n#     display(y.head())\n    \n    return X, X_test, y","e9e7528c":"%%time\n\n# LightAutoML parameters\nN_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 2.9 * 3600 # Time in seconds for automl run\n\n# Fixing parameters for better repeatability \nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)\n\n# Initializing and filling predictions dataframe before datetime conversion\npreds = pd.DataFrame()\npreds[\"date_time\"] = test_copy[\"date_time\"].copy()\n\ntrain_preds_df = pd.DataFrame(index=np.arange(7110))\ntest_preds_df = pd.DataFrame(index=test.index)\n\nfeature_importances = []\nfor i, target in enumerate(targets):\n    \n    ROLES = {CategoryRole(force_input=True, ordinal=True): [\"working_hours\", \"maximum_hours\", \"is_weekend\"],\n#              DatetimeRole(base_date=False, base_feats=True, seasonality=(\"d\", \"wd\", \"hour\")): \"date_time\",\n             \"target\": target}    \n    X, X_test, y = prepare_dataset(pd.concat([train_copy, train_preds_df], axis=1), pd.concat([test_copy, test_preds_df], axis=1), i)\n    display(X)\n    \n    model = TabularUtilizedAutoML(task = Task(\"reg\", loss=\"rmsle\", metric=\"rmsle\"),\n                                  verbose=1,\n                                  timeout = TIMEOUT,\n                                  cpu_limit = N_THREADS,\n                                  reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n#                                   general_params = {'use_algos': [['lgb_tuned', 'cb_tuned'], ['lgb', 'cb_tuned']]}\n                                  general_params = {'use_algos': [['lgb_tuned', 'cb_tuned', 'lgb', 'cb'], ['cb']]}\n#                                   general_params = {'use_algos': [['cb']]}\n                                 )\n\n    oof_preds = model.fit_predict(pd.concat([X, y[target]], axis=1), roles = ROLES)\n\n    print(f\"{target} oof_score score is {np.sqrt(mean_squared_log_error(np.expm1(y[target].values), np.expm1(oof_preds.data)))}\")\n    preds[target] = np.expm1(model.predict(X_test).data)\n    feature_importances.append(model.get_feature_scores('fast', silent=False))\n    \n    train_preds_df[target+\"_preds\"] = oof_preds.data\n    test_preds_df[target+\"_preds\"] = np.log1p(preds[target])","e5c57b1e":"# Creating feature list from feature importance dataframes in case there are diffrent dataset used for each target\nfeature_list = set()\nfor i in np.arange(len(feature_importances)):\n    feature_list = set.union(feature_list, set(feature_importances[i][\"Feature\"]))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","6ffd33a8":"# Creating a sorted dataframe with all feature importances\nfi_df = pd.DataFrame(columns=[\"Feature\", \"Target1_imp\", \"Target2_imp\", \"Target3_imp\"])\nfi_df[\"Feature\"] = list(feature_list)\nfi_df.sort_values(\"Feature\", inplace=True)\nfi_df.reset_index(drop=True, inplace=True)\nfor i, fi in enumerate(feature_importances):\n    for feature in fi[\"Feature\"]:\n        fi_df.loc[fi_df[\"Feature\"]==feature, \"Target\"+str(i+1)+\"_imp\"] = fi.loc[fi[\"Feature\"]==feature, \"Importance\"].values \/ fi[\"Importance\"].sum()\n\nfi_df.fillna(0, inplace=True)\nfi_df[\"Overall_importance\"] = fi_df[\"Target1_imp\"] + fi_df[\"Target2_imp\"] + fi_df[\"Target3_imp\"]\nfi_df.sort_values(\"Overall_importance\", ascending=False, inplace=True)\nfi_df.reset_index(drop=True, inplace=True)\n\n# Displaying original feature importance dataframes to quickly check target specific feature performance\ndisplay(feature_importances[0].T)\nprint(\"Importance sum\", feature_importances[0][\"Importance\"].sum())\ndisplay(feature_importances[1].T)\nprint(\"Importance sum\", feature_importances[1][\"Importance\"].sum())\ndisplay(feature_importances[2].T)\nprint(\"Importance sum\", feature_importances[2][\"Importance\"].sum())","3e14ddc3":"fi_df.T","eed555f1":"# Displaying 10 least important features for each target\ndisplay(feature_importances[0].tail(10).T)\ndisplay(feature_importances[1].tail(10).T)\ndisplay(feature_importances[2].tail(10).T)","6ea66c51":"df= fi_df\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 80))\nbars1 = ax.barh(x-height, df[\"Target1_imp\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Target2_imp\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Target3_imp\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\nax.legend(fontsize=13, loc=\"upper right\", bbox_to_anchor=(0, 0, 1, 0.92))\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","067658c9":"preds.to_csv('submission.csv', index=False)\npreds.head()","620b25d6":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 744, 1), train.loc[train[\"month\"]==12, target], label=\"Train, 12th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","96488fc9":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 720, 1), train.loc[train[\"month\"]==11, target], label=\"Train, 11th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","31253611":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(y.columns):\n    axs[i].plot(np.arange(0, 598, 1), train.loc[:597, target], label=\"Train, from 10.3 to 4.4\")\n    axs[i].plot(np.arange(0, 596, 1), preds.loc[1651: , target],\n                label=\"Test, from 10.3 to 4.4\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","97ef54e8":"Let's check how each target value chenges depending on the time of day, day of week, and month.","4b8d3487":"There are some near zer flat areas at 4th, 6th, 8th, 12th month plots. Need to figure out what is so special about these days. It also may be a garbage data which sould be deleted before machine learning.","af4fdd73":"## **Data import**","c736cda3":"Let's check feature correlation.","abc5b477":"# **Predictions submission and comparison**","2c1a1ab3":"# **Model training**","c804933b":"Let's compare predictions with the closest months from the train datasets.","26d15614":"The datasets have timestamps. Let's compare which dates are in each dataset.","87380d47":"The datasets also have three target columns that the model have to predict. Let's see how each target is changing in time.","80841063":"Let's check each target value distribution.","4dcad226":"# **EDA**","75cedef9":"# **Feature importances**","f3a108b9":"Let's compare our train and test feature data.","2ea7a5fc":"## Feature plots","db19ccf9":"The datetime conversion shown below was found in this [notebook](https:\/\/www.kaggle.com\/jarupula\/eda-rf-model-tps-july-21). It gives a significant score boost.","795f4bc3":"As you can see, the predictions are the closest to the training set in the overlapping months (from March 10 to April 4). ","95ab378c":"As you can see, all target values usually go down at the end of each week (i.e. during weekends). \n\nLet's check targets distribution along each month.","5cd730e0":"Let's see mean target values per day of year.","cef47cf7":"The idea of SMC feature below was taken from this [notebook](https:\/\/www.kaggle.com\/junhyeok99\/automl-pycaret).\n\nTaking into account temperature changes was suggested by [@lukaszborecki](https:\/\/www.kaggle.com\/lukaszborecki) [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/discussion\/250931#1380107)."}}