{"cell_type":{"5f242a49":"code","d3847aae":"code","085123e2":"code","8638441c":"code","d0bc65ee":"code","a27282bf":"code","16d6fb22":"code","6085d767":"code","e5630898":"code","529f7182":"code","09fac516":"code","a7948f79":"code","9c538489":"code","f3ff33bf":"code","6ea7e74c":"code","a2d1acf5":"code","979e2647":"code","e6728995":"code","fd2e3be6":"code","3aded483":"code","b47b8c5a":"code","17593e1e":"code","e0f7e777":"code","d8c6f26a":"code","be6781f5":"code","92a99308":"code","fbb59ebc":"code","b1e51e36":"code","b31d2092":"code","2bb8943b":"code","be2dfdfc":"code","9dd53a3b":"code","8e911ac5":"code","51f8cbc4":"code","bc8c4250":"code","fcd8bf0f":"code","c01328db":"code","cf760e64":"code","4b12bb25":"code","3f3006d4":"code","819f035b":"code","dbc088be":"code","8e4eda91":"code","845aaa98":"code","f1ff5e93":"markdown","f8b50e4c":"markdown","37605288":"markdown","d205b01d":"markdown","4aa52fe6":"markdown","88a8b3e2":"markdown","a22e56f7":"markdown","0ba95fad":"markdown","6b36144a":"markdown","2b271439":"markdown","76124576":"markdown","afcdadc9":"markdown","0608c0f1":"markdown","6a51f06f":"markdown","f2c1dc2c":"markdown","bd241c99":"markdown","593af564":"markdown","53a225de":"markdown","4b1d4553":"markdown","92c46552":"markdown","ac4b8503":"markdown","264641c2":"markdown","c27ddcf5":"markdown","0922b962":"markdown","501ddc86":"markdown","7c67d584":"markdown","b0f6a213":"markdown","a4f7702f":"markdown"},"source":{"5f242a49":"import pandas as pd\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, validation_curve, ShuffleSplit, learning_curve\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Lasso,LassoCV,LassoLarsCV, LogisticRegression, Ridge, ElasticNet\nfrom sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error,r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom  sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc  , mean_squared_error   ###\u8ba1\u7b97roc\u548cauc\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import MultipleLocator\nfrom matplotlib.colors import ListedColormap\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly import plot\nimport plotly.offline as py\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve","d3847aae":"df = pd.read_csv(\"..\/input\/heart-failure-clinical-records\/heart_failure_clinical_records_dataset.csv\")\ndf.head(3)","085123e2":"df.describe()","8638441c":"survi = df[df[\"DEATH_EVENT\"]==0]\nnot_survi = df[df[\"DEATH_EVENT\"]==1]\n\nlabels = ['Survived','Not Survived']\nvalues = [len(survi),len(not_survi)]\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.2)])\nfig.update_layout(\n    title_text=\"Analysis on Survival\")\nfig.show()","d0bc65ee":"numerical_features = [\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"]\nplt.figure(figsize=(18, 27))\n\nfor i, col in enumerate(numerical_features):\n    plt.subplot(6, 4, i*2+2) \n    sns.boxplot(y = col, data = df, x=\"DEATH_EVENT\")   ","a27282bf":"categorical_features = [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"]\nplt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(categorical_features):\n    plt.subplot(2, 3, i+1)\n    plt.title(col)\n    plt.subplots_adjust(hspace =.5, wspace=.5)\n    sns.countplot(data=df, x=col, hue=\"DEATH_EVENT\", alpha=0.8, edgecolor=\"k\", linewidth=1)","16d6fb22":"pd.crosstab(index=df['anaemia'], columns=df['DEATH_EVENT'], margins=True, normalize='columns').round(2)*100","6085d767":"pd.crosstab(index=df['diabetes'], columns=df['DEATH_EVENT'], margins=True, normalize='columns').round(2)*100","e5630898":"pd.crosstab(index=df['high_blood_pressure'], columns=df['DEATH_EVENT'], margins=True, normalize='columns').round(2)*100","529f7182":"pd.crosstab(index=df['sex'], columns=df['DEATH_EVENT'], margins=True, normalize='columns').round(2)*100","09fac516":"pd.crosstab(index=df['smoking'], columns=df['DEATH_EVENT'], margins=True, normalize='columns').round(2)*100","a7948f79":"all_features = categorical_features.copy()\nall_features.extend(numerical_features)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(df[all_features].corr(), vmin=-1, cmap='coolwarm', annot=True, fmt='.2f');","9c538489":"pre_df = df;\ndf_Y = pre_df[\"DEATH_EVENT\"];\ndf_X = pre_df.drop([\"DEATH_EVENT\"], axis = 1);\ndf_X_normed = (df_X - df_X.mean()) \/ df_X.std()\nSEED=12\npca = PCA(random_state=SEED)\ndf_X_pca = pca.fit_transform(df_X_normed)\ntot = sum(pca.explained_variance_) # total explained variance of all principal components\nvar_exp = [(i \/ tot) * 100 for i in sorted(pca.explained_variance_, reverse=True)] # individual explained variance\ncum_var_exp = np.cumsum(var_exp) # cumulative explained variance\ntrace_cum_var_exp = go.Bar(\n    x=list(range(1, len(cum_var_exp) + 1)), \n    y=var_exp,\n    name=\"individual explained variance\",\n)\ntrace_ind_var_exp = go.Scatter(\n    x=list(range(1, len(cum_var_exp) + 1)),\n    y=cum_var_exp,\n    mode='lines+markers',\n    name=\"cumulative explained variance\",\n    line=dict(\n        shape='hv',\n    ))\ndata = [trace_cum_var_exp, trace_ind_var_exp]\nlayout = go.Layout(\n    title='Individual and Cumulative Explained Variance',\n    autosize=True,\n    yaxis=dict(\n        title='percentage of explained variance',\n    ),\n    xaxis=dict(\n        title=\"principal components\",\n        dtick=1,\n    ),\n    legend=dict(\n        x=0,\n        y=1,\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.init_notebook_mode();\npy.iplot(fig);","f3ff33bf":"n_components = 8\ndf_X_reduced = np.dot(df_X_normed.values, pca.components_[:n_components,:].T)\ndf_X_reduced = pd.DataFrame(df_X_reduced, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])","6ea7e74c":"X_train, X_test, y_train, y_test = train_test_split(df_X_normed, df_Y, test_size=0.25, random_state=SEED)\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(df_X_reduced, df_Y, test_size=0.25, random_state=SEED)","a2d1acf5":"def do_gridsearch_with_cv(clf, params, X_train, y_train, cv):\n\n    pipeline = Pipeline([('clf', clf)])\n        \n    gs = GridSearchCV(pipeline, params, cv=kf, n_jobs=-1, scoring='f1', return_train_score=True)\n    gs.fit(X_train, y_train)\n    return gs\n\ndef score_on_test_set(clfs, datasets):\n    scores = []\n    for c, (X_test, y_test) in zip(clfs, datasets):\n        scores.append(c.score(X_test, y_test))\n    return scores","979e2647":"kf = StratifiedKFold(shuffle=True, n_splits=5, random_state=SEED)\nclf_rf = RandomForestClassifier(random_state=SEED)\nRANDOM_FOREST_PARAMS = {\n    'clf__max_depth': [25, 50, 75],\n    'clf__max_features': [\"sqrt\"], \n    'clf__criterion': ['gini', 'entropy'],\n    'clf__n_estimators': [100, 300, 500, 1000]\n}\ngs_full = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf)\ngs_pca = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf)\ngss_raw = [gs_full, gs_pca]","e6728995":"test_results_raw = score_on_test_set(gss_raw, [(X_test, y_test), (X_test_pca, y_test_pca)])","fd2e3be6":"dataset_strings = [\"full dataset\", \"dataset with first 8 principal components\"]\nresult_strings = dict()\nfor ds, res in zip(dataset_strings, test_results_raw):\n    string = \"%.3f\" % res + \"     \" + ds\n    result_strings[string] = res\nresult_strings = sorted(result_strings.items(), key=lambda kv: kv[1], reverse=True)\nprint(\"F1 score  dataset and method\")\nfor k, _ in result_strings:\n    print(k)","3aded483":"\ndef plot_decision_boundaries(X, y, model_class, **model_params):\n    \"\"\"Function to plot the decision boundaries of a classification model.\n    This uses just the first two columns of the data for fitting\n    the model as we need to find the predicted value for every point in\n    scatter plot.\n    One possible improvement could be to use all columns fot fitting\n    and using the first 2 columns and median of all other columns\n    for predicting.\n    Adopted from:\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_voting_decision_regions.html\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_digits.html\n    \"\"\"\n    plt.figure()\n    plt.title(str(model_class) + \"decision_boundaries\")\n    reduced_data = X[:, :2]\n    model = model_class(**model_params)\n    model.fit(reduced_data, y)\n    # Step size of the mesh. Decrease to increase the quality of the VQ.\n    h = .02  # point in the mesh [x_min, m_max]x[y_min, y_max].\n    # Plot the decision boundary. For that, we will assign a color to each\n    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Obtain labels for each point in mesh using the model.\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8, cmap=plt.cm.Spectral)\n    return plt\n","b47b8c5a":"logisticRegression = LogisticRegression()\nlogisticRegression.fit(X_train, y_train)\ny_predict = logisticRegression.predict(X_test)\nprint(\"logisticRegression accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"logisticRegression precision score \", precision_score(y_test,y_predict) )\nprint(\"logisticRegression recall score \", recall_score(y_test,y_predict) )\nprint(\"logisticRegression f1 score \", f1_score(y_test,y_predict) )\nprint('logisticRegression roc auc score\uff1a',roc_auc_score(y_test, y_predict))\nprint(\"\\n\")","17593e1e":"fpr, tpr, threshold= roc_curve(np.array(y_test), logisticRegression.predict_proba(X_test)[:, :1])\nroc_auc = auc(fpr, tpr)  ###\u8ba1\u7b97auc\u7684\u503c\ndisplay = metrics.plot_roc_curve(logisticRegression, X_test, y_test)\nprint('type(display):',type(display))\nplt.title('knn_classifier Receiver operating characteristic example')\nplt.show()","e0f7e777":"plot_decision_boundaries(np.array(X_test.iloc[:, 7:9]), np.array(y_test),model_class = LogisticRegression)","d8c6f26a":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\ncv_method = StratifiedKFold(n_splits=3)\nparam={'n_neighbors':(1,3,5,7),'metric':('euclidean','manhattan','chebyshev','minkowski'),'p' :(1,2)}\nclf_knn = GridSearchCV(neigh, param,cv=cv_method,scoring=\"accuracy\")\nclf_knn.fit(X_train,y_train)","be6781f5":"clf_knn.best_params_","92a99308":"knn = KNeighborsClassifier(n_neighbors= 5, p= 1,metric= 'manhattan')\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)\ny_predict = knn.predict(X_test)\nprint(\"knn_classifier accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"knn_classifier precision score \", precision_score(y_test,y_predict) )\nprint(\"knn_classifier recall score \", recall_score(y_test,y_predict) )\nprint(\"knn_classifier f1 score \", f1_score(y_test,y_predict) )\nprint('knn_classifier roc auc score\uff1a',roc_auc_score(y_test, y_predict))","fbb59ebc":"fpr, tpr, threshold= roc_curve(np.array(y_test), knn.predict_proba(X_test)[:, :1])\nroc_auc = auc(fpr, tpr)  ###\u8ba1\u7b97auc\u7684\u503c\ndisplay = metrics.plot_roc_curve(knn, X_test, y_test)\nprint('type(display):',type(display))\nplt.title('knn_classifier Receiver operating characteristic example')\nplt.show()","b1e51e36":"plot_decision_boundaries(np.array(X_test.iloc[:, 7:9]), np.array(y_test),model_class = KNeighborsClassifier)","b31d2092":"cv_method = StratifiedKFold(n_splits=3)\nparam={'C': [0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf','sigmoid','linear']}\nclf_svc = GridSearchCV(SVC(), param,cv=cv_method,scoring=\"accuracy\")\nclf_svc.fit(X_train,y_train)","2bb8943b":"clf_svc.best_params_","be2dfdfc":"svc=SVC(C= 10, gamma= 1, kernel='linear', probability=True)\nsvc.fit(X_train, y_train)\nsvc.score(X_test, y_test)\ny_predict = svc.predict(X_test)\nprint(\"svc accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"svc precision score \", precision_score(y_test,y_predict) )\nprint(\"svc recall score \", recall_score(y_test,y_predict) )\nprint(\"svc f1 score \", f1_score(y_test,y_predict) )\nprint('svc roc auc score\uff1a',roc_auc_score(y_test, y_predict))","9dd53a3b":"plot_decision_boundaries(np.array(X_test.iloc[:, 7:9]), np.array(y_test),model_class = SVC)","8e911ac5":"fpr, tpr, threshold= roc_curve(np.array(y_test), svc.predict_proba(X_test)[:, :1])\nroc_auc = auc(fpr, tpr)  ###\u8ba1\u7b97auc\u7684\u503c\ndisplay = metrics.plot_roc_curve(svc, X_test, y_test)\nprint('type(display):',type(display))\nplt.title('SVC Receiver operating characteristic example')\nplt.show()","51f8cbc4":"decision_tree = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=3)\ndecision_tree.fit(X_train, y_train)     #Fit the train dataset\ny_predict = decision_tree.predict(X_test)               #Predict the result by utilizing the decide_tree model\nprint(\"decision_tree accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"decision_tree precision score \", precision_score(y_test,y_predict) )\nprint(\"decision_tree recall score \", recall_score(y_test,y_predict) )\nprint(\"decision_tree f1 score \", f1_score(y_test,y_predict) )\nprint('decision_tree roc auc score\uff1a',roc_auc_score(y_test, y_predict))\nprint(\"\\n\")","bc8c4250":"dct=DecisionTreeClassifier()\ncv_method = StratifiedKFold(n_splits=3)\nparam={'criterion':('gini', 'entropy'),'min_samples_split': (2, 6, 20),'min_samples_leaf': (1, 4, 16)}\nclf_dct = GridSearchCV(dct, param,cv=cv_method,scoring=\"accuracy\")\nclf_dct.fit(X_train,y_train)","fcd8bf0f":"clf_dct.best_params_","c01328db":"decision_tree = DecisionTreeClassifier(criterion= 'gini',min_samples_leaf= 16,min_samples_split=2)\ndecision_tree.fit(X_train, y_train)\ndecision_tree.score(X_test, y_test)\ny_predict = decision_tree.predict(X_test)\nprint(\"decision_tree accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"decision_tree precision score \", precision_score(y_test,y_predict) )\nprint(\"decision_tree recall score \", recall_score(y_test,y_predict) )\nprint(\"decision_tree f1 score \", f1_score(y_test,y_predict) )\nprint('decision_tree roc auc score\uff1a',roc_auc_score(y_test, y_predict))","cf760e64":"plot_decision_boundaries(np.array(X_test.iloc[:, 7:9]), np.array(y_test),model_class = DecisionTreeClassifier)","4b12bb25":"fpr, tpr, threshold= roc_curve(np.array(y_test), decision_tree.predict_proba(X_test)[:, :1])\nroc_auc = auc(fpr, tpr)  ###\u8ba1\u7b97auc\u7684\u503c\ndisplay = metrics.plot_roc_curve(decision_tree, X_test, y_test)\nprint('type(display):',type(display))\nplt.title('decision_tree Receiver operating characteristic example')\nplt.show()","3f3006d4":"rdf=RandomForestClassifier()\ncv_method = StratifiedKFold(n_splits=3)\nparam={'criterion':('gini', 'entropy'),'min_samples_split': (2, 6, 20),'min_samples_leaf': (1, 4, 16),'n_estimators' :(100,150, 200, 250)}\nclf_rdf = GridSearchCV(rdf, param,cv=cv_method,scoring=\"accuracy\")\nclf_rdf.fit(X_train,y_train)","819f035b":"clf_rdf.best_params_","dbc088be":"random_forest = RandomForestClassifier(criterion= 'gini',min_samples_leaf= 4,min_samples_split=6,n_estimators= 100)\nrandom_forest.fit(X_train, y_train)\nrandom_forest.score(X_test, y_test)\ny_predict = random_forest.predict(X_test)\nprint(\"random_forest accuracy score \", accuracy_score(y_test,y_predict) )\nprint(\"random_forest precision score \", precision_score(y_test,y_predict) )\nprint(\"random_forest recall score \", recall_score(y_test,y_predict) )\nprint(\"random_forest f1 score \", f1_score(y_test,y_predict) )\nprint('random_forest roc auc score\uff1a',roc_auc_score(y_test, y_predict))","8e4eda91":"plot_decision_boundaries(np.array(X_test.iloc[:, 7:9]), np.array(y_test),model_class = RandomForestClassifier)","845aaa98":"\nfpr, tpr, threshold= roc_curve(np.array(y_test), random_forest.predict_proba(X_test)[:, :1])\nroc_auc = auc(fpr, tpr)  ###\u8ba1\u7b97auc\u7684\u503c\ndisplay = metrics.plot_roc_curve(random_forest, X_test, y_test)\nprint('type(display):',type(display))\nplt.title('random_forest Receiver operating characteristic example')\nplt.show()","f1ff5e93":"These numbers are hard to interpret in this format, so we create some graphs which visualize them in a better way. First, we look at the distribution of the our target variable:\nWe have more samples for patient survived than not survived, about 2:1.","f8b50e4c":"Concluding, we suggest use the Random_Forest model to identify the patients in the rise of death.","37605288":"# Decision Treee","d205b01d":"There are no strong correlations between features, exception of sex and smoking that seems to be slightly positively correlated.","4aa52fe6":"For the first try of our parameters and LogisticRegression model, we can see the first one is the forward slashes which distinguish the dataset, it can be explained by the imbalance of our data there is a large majority of anaemia. The second nteresting fact is that we can still see only two different colours it means that the method estimates that there are 2 different classifiers at least the last point is that most of the data elements are not well classified .","88a8b3e2":"# Support-Vector Machine\nSupport vector machine is a powerful model used for both classification and regression.<br>\nIt consists in trying to fit an hyperplane that best divides the dataset into the two classes by maximizing the margin (the distance between the hyperplane and the closest points).\n#### Hard margin\nThe simplest implementation is the hard margin SVM in which data needs to be linearly separable to allow the algorithm to converge.<br>\nConsidering the hyperplane described by the vector **w** such that:<br>\n\n\\begin{align}\n{L=\\{v:\\langle w,v\\rangle+ b= 0\\}}\\space , \\space \u2016w\u2016=1\n\\end{align}\n\nthe distance of a point fom the hyperplane L can be evaluated in that way:<br>\n\n\\begin{align}\n{d(x,L) =| \\langle w,x \\rangle+ b |}\n\\end{align}\n\nwhile the distance between two points of two different classes on the margin is:\n\n\\begin{align}\n\\frac{<x_{+} - x_{-}, W>}{\\left \\| W \\right \\|} = \\frac{(<x_{+}, W> + \\space b) - (<x_{-}, W> + \\space b)}{\\left \\| W \\right \\|} = \\frac{2}{\\left \\| W \\right \\|}\n\\end{align}\n\nSo we need to find w and b such that that distance is **maximized** (considering the whole training set) and at the same time all the points are **classified correctly**.<br>\nEquivalently:<br>\n\n\\begin{align}\n{min_{w,b} \\frac{1}{2}\u2016w\u2016^2 \\space\\space s.t. \\space\\space \\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)>1}\n\\end{align}\n\n\n#### Soft margin\nThe main drawback is that, in the real world, the vast majority of the problems are not linearly separable, and an algorithm like this one would not converge.\nFor this reason we can add a term on the constraints to relax them.<br>\nIn this implementation, a \n<span style=\"color:red\">relaxation<\/span>\nis added to the constraint.<br>\n\n\\begin{align}\n{\nmin_{w,b} \\Big(\\frac{1}{2} \u2016W\u2016^2 + {\\color{red} {C  \\sum_{i=1}^{m} {\\xi_i}}}\\Big)\n\\space \\space\ns.t.\n\\space \\space\n\\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)\u2265 1 \\color{red}{\u2212\\xi_i\\space\\space and \\space\\space \\xi_i\u22650}\n} \n\\end{align}\n\n\n\nBut in some cases in which there are a lot of features, mapping every time from the original to the new space could be costly (second order features in 1000 dimensions are around $5\\cdot10^5$ numbers)<br>\nThe solution is using kernel functions.<br>\n\n#### Kernel trick\nTo solve the svm optimization problem, computing inner products it's needed.<br>\nIn fact, according to **representer theorem**, $w$ can be written as:\n\n${w = \\sum_{i} \\alpha_i\\psi(x_i)}\\space$\n\nso it follows that: <br>\n\n$\\space \u2016w\u2016^2 = \\langle \\sum_j{\\alpha_j\\psi(x_j)}, \\sum_j{\\alpha_j\\psi(x_j)}\\rangle = \\sum_{i,j=1}^{m}\\alpha_i\\alpha_j\\langle\\psi(x_i),\\psi(x_j)\\rangle $\n\n","a22e56f7":"We can see in these boxplot there are some outliers in the values of the features which is correctly recorded and belongs to the data set. In this case, these outliers may indicate some special properties, so i think it should be retained.\n\nAnd we choose the median as a reference, we can see the patients who died following a heart failure seem to have a lower Ejection Fraction and Serum Sodium. They also seem to have slightly higher levels of Serum Creatinine and Creatine Phosphokinase. \n\nNext we reported the categorical features","0ba95fad":"We can see from the graph that the first two components explain the most effective factors, followed by the next 7 components which dscend sequentially from 10.56% to 4.25%. I choose to reduce our dataset by using the first 8 components which explain about 80% of the total variance.","6b36144a":"Through the visualization form, it is easy to see the correlation between the features.\n\nAccording to the below figure, we can easily see the \"Impact factor\" between the features. The darker the color of the area where the horizontal and vertical coordinates cross, the deeper the relationship between them. The number marked on the block also shows this.\n\nHere also remove time.","2b271439":"# Introduce\nIn this report, a dataset on medical clinical records of heart failure is analysed. The dataset contains 299 patients who had heart failure.Each row represents a patient and the columns contain patient\u2019s attributes which are described in the following:\n* age: age of the patient (years)\n* anaemia: decrease of red blood cells or hemoglobin (boolean)\n* high blood pressure: if the patient has hypertension (boolean)\n* creatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg\/L)\n* diabetes: if the patient has diabetes (boolean)\n* ejection fraction: percentage of blood leaving the heart at each contraction (percentage)\n* platelets: platelets in the blood (kiloplatelets\/mL)\n* sex: woman or man (binary)\n* serum creatinine: level of serum creatinine in the blood (mg\/dL)\n* serum sodium: level of serum sodium in the blood (mEq\/L)\n* smoking: if the patient smokes or not (boolean)\n* time: follow-up period (days)\n* [target] death event: if the patient deceased during the follow-up period (boolean)\n\n\nLabel DEATH_EVENT that indicates whether or not the patient has died. Predicting DEATH_EVENT can help the patients, so it can effectively prevent death caused by heart failure by paying attention to certain indicators. \n","76124576":"Now will follow a series of different models used to perform classification of the DEATH_EVENT\n* Logistic Regression\n* K-Nearest Neighbors\n* Support-Vector Machine\n* Decision Treee\n* Random Forest\n\n\nAll models are evaluated considering the following metrics\n\n* ${accuracy = \\frac {TP+TN}{TP+TN+FP+FN}\\quad }$\n\n\n* ${precision = \\frac {TP}{TP+FP}\\quad}$\n\n\n* ${recall = \\frac {TP}{TP+FN}\\quad}$ \n\n\n* ${F_1 = 2 \\times \\frac{precision \\times recall}{precision+recall}}$\n\n\n\nMoreover, **ROC curve** is also evaluated.<br>\nReceiver operating characteristic is a plot that shows the True Positive and False positive rates applying different thresholds on the prediction (that needs to be a number between 0 and 1).<br>\nThen, model selection can be also performed according to the Area Under the Curve (**AUC**) that is the area under the roc curve. (the bigger the better)","afcdadc9":"<a id='Table of Contents'><\/a>\n### Table of Contents\n\n* [1. Introduction](#Introduction)\n\n* [2. Data exploration and visualization](#Data_exploration_and_visualization)\n\n* [3. Features selection](#Features_selection)\n\n* [4. Classi\ufb01cation](#Classi\ufb01cation)\n    * [4.1  Logistic Regression](#Logistic_Regression)\n    * [4.2  K-Nearest Neighbors](#K-Nearest_Neighbors)\n    * [4.3  Support-Vector Machine](#Support-Vector_Machine)\n    * [4.4  Decision Tree](#Decision_Tree)\n    * [4.5  Random Forest](#Random_Forest)\n    \n    \n* [5. Conclusion](#Conclusion)\n","0608c0f1":"# K-Nearest Neighbors\n\nKNN model tries to classify new points according to the class of the nearest neighbors.<br>\nNearest neighbors are evaluated according to a **distance metric** function and for each new point, only a fixed number of neighbors are taken into account.<br>\nThis model is quite simple but it doesn't scale well.<br>\nHere the selected k  = 5.","6a51f06f":"# Heart Failure clinical records \n\n**By Haihang Huang** <br>\n\n\n","f2c1dc2c":"Next, we plot the numerical features (omitting time because is not used in the prediction).\nI choose boxplot to visualize these feature because it can clearly identify the outliers in the data batch and it is not affected by outliers, can accurately and stably depict the discrete distribution of data.","bd241c99":"# Conclusion\nlogisticRegression accuracy score  0.8<br>\nlogisticRegression precision score  0.7619047619047619<br>\nlogisticRegression recall score  0.6153846153846154<br>\nlogisticRegression f1 score  0.6808510638297872<br>\nlogisticRegression roc auc score\uff1a 0.7566718995290423<br>\n\nknn_classifier accuracy score  0.7466666666666667<br>\nknn_classifier precision score  0.6842105263157895<br>\nknn_classifier recall score  0.5<br>\nknn_classifier f1 score  0.5777777777777778<br>\nknn_classifier roc auc score\uff1a 0.6887755102040817<br>\n\nLinear_svc accuracy score  0.8<br>\nLinear_svc precision score  0.7619047619047619<br>\nLinear_svc recall score  0.6153846153846154<br>\nLinear_svc f1 score  0.6808510638297872<br>\nLinear_svc roc auc score\uff1a 0.7566718995290423<br>\n\ndecision_tree accuracy score  0.8666666666666667<br>\ndecision_tree precision score  0.9<br>\ndecision_tree recall score  0.6923076923076923<br>\ndecision_tree f1 score  0.7826086956521738<br>\ndecision_tree roc auc score\uff1a 0.82574568288854<br>\n\nrandom_forest accuracy score  0.9066666666666666<br>\nrandom_forest precision score  0.9130434782608695<br>\nrandom_forest recall score  0.8076923076923077<br>\nrandom_forest f1 score  0.8571428571428572<br>\nrandom_forest roc auc score\uff1a 0.8834379905808477<br>","593af564":"# Classi\ufb01cation","53a225de":"# Data exploration and visualization","4b1d4553":"Train the Random Forest on the full dataset and the dataset processed by PCA. We use the GridSearchCV to find the best hyperparameter combination.","92c46552":"# Features selection\n\nIn this chapter i wan to use PCA to apply features extraction, and compared with full dataset to decide which one is used for the following prediction part. The Random Forest classifier is used as baseline model.The following different versions of the dataset are investigated:\n\n* full dataset\n* dataset reduced by considering the first ten principal components after applying PCA\n\n\nTo account for the class imbalance of our target variable, we use the f1-score as our main evaluation metric.","ac4b8503":"Since our interest is in predicting survival, we normalize with respect to death.","264641c2":"# Principal component analysis\n\n\n\n","c27ddcf5":"# Random Forest","0922b962":"From above crosstable, we can see:\n* 48% of the patients who died were anemic while 41% of the patients who survived were anemic as wel\n* 42% of the patients who died and 42% who survived were diabeti\n* 41% of those who died had high blood pressure, while 33% of those who survived had high blood pressure as well.\n* 65% of the Male and 35% of the Female heart patients died.\n* 31% of the dead were smokers while 33% of the survivors were smokers.\n\n\nBased on these statistics,The greatest difference is in the case of high blood pressure, which could perhaps have a greater influence on the survival of heart patients.","501ddc86":"The results show that the knnclassifier 0.75 is a very lowest precision value, which means that when it predicts a person in the risk of death, it is just in 75% of the cases correct.\nMoreover, in accuracy the Decision Tree and Randomforest performs about 0.9 better than the others. But in the AUC measure the Decision Tree outperforms with 0.82, the Randomforest with 0.88, but the Decision Tree achieves lower results for the recall value than Randomforest, which makes its f1-score the second worst of all classifiers.\nThe SVM  and logisticRegression they are the same with 0.76 the third highest precision score. This also impacts the f1-score for which they are also achieves the second smallest with an value of 0.68.\n","7c67d584":"* Age:    We can see that the average age of the patients is 60 years with most of the patients (<75%) below 70 years and above 40 years.\n* Ejection fraction:    In a healthy adult, this fraction is 55% and heart failure with reduced ejection fraction implies a value < 40%. In our dataset,75% of the patients have this value < 45% which is expected *\n* Platelets:A normal person has a platelet count of 150,000\u2013400,000 kiloplatelets\/mL of blood. In our dataset, 75% of the patients have a platelet count well within this range.\n* creatinine_phosphokinase:Total CPK normal values: 10 to 120 micrograms per liter. In our dataset, the average value (550 mcg\/L) and median (250 mcg\/L) are higher than normal. *","b0f6a213":"<a id='Logistic_regression'><\/a>\n# Logistic Regression\n\n\n\n\nLogistic regression is a generalized linear model in which the link function is not the identity (as in the case of linear regression) but is the **logit**.<br>\n\n\\begin{align}\n{logit(p) = ln(\\frac {p}{1-p})}\n\\end{align}\n\nThe **link function** is a function connecting the expected value of the response and the linear combination of predictors.<br>\nPractically:<br>\n\n\\begin{align}\n{logit(p(X)) = X\\cdot\\beta}\n\\end{align}\n\n\\begin{align}\n{p(X) = logit^{-1}(X\\cdot\\beta)}\n\\end{align}\n\n\\begin{align}\n{p(X) = S(X\\cdot\\beta)}\n\\end{align}\n\nwhere $p$ is the expected value of the prediction that in this case (binary) can be modeled as a bernoulli.<br>\n\nThe logit \"stretches\" the interval (0, 1) into the whole real line.\nThe inverse of the logit is called **sigmoid**:\n\n\\begin{align}\n{\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}\n\\end{align}\nso:\n\\begin{align}\n{p(X) = \\frac{e^{X \\beta}}{1+e^{X \\beta}}}\n\\end{align}\n\n\nIn this way every prediction is bounded between 0 and 1, assuming a probabilistic meaning.<br>\nFor this reason, logistic regression is very suitable for binary classification.","a4f7702f":"We can see the PCA is not very good, so i decide to use the full dataset."}}