{"cell_type":{"ca8a36cd":"code","9ae42d32":"code","28b62fe8":"code","c77e91ed":"code","c5f34bdb":"code","c05b5b46":"code","56291b1c":"code","2676b52b":"code","65710988":"code","155597b5":"code","3b7ab7e2":"code","40c09a8d":"code","7c9bba67":"code","4d858a20":"code","5ed0bb20":"code","ffdd927c":"code","4faabb79":"code","2aec3d05":"code","88be91dd":"code","60487238":"code","0ef9ab0c":"code","e038130f":"code","512aafb2":"code","80c8936a":"code","5eb40d28":"code","1fad977a":"code","39cf5333":"code","2cfdda05":"code","142a7e0e":"code","4b8c808a":"code","61b6db20":"markdown","7023918d":"markdown"},"source":{"ca8a36cd":"import numpy as np\nimport pandas as pd\nimport os\nos.environ['KERAS_BACKEND']='tensorflow' \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom keras import Sequential\nfrom keras.layers import (GRU,LSTM,\n                          Embedding, \n                          Dense, \n                          Dropout, \n                          Bidirectional)\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport re\nfrom string import punctuation","9ae42d32":"train_df = pd.read_csv(\"..\/input\/fake-news\/train.csv\", index_col = 'id')\n\nprint('Shape of dataset ',train_df.shape)\nprint(train_df.columns)\nprint('No. of unique classes',len(set(train_df['label'])))\ntrain_df.head()","28b62fe8":"train_df.isnull().sum()","c77e91ed":"train_df = train_df.dropna()","c5f34bdb":"print('Shape of dataset ',train_df.shape)","c05b5b46":"stop_words = stopwords.words('english')\nstem = PorterStemmer()","56291b1c":"def cleaning(text): \n    text = re.sub('(@[A-Za-z0-9]+)', ' ', text)\n    text = text.lower().split()\n    text = [stem.stem(word) for word in text if word not in stop_words]\n    text = ' '.join(text)\n    text = re.sub(r\"\\d+\",' ', text)\n    text = ''.join(p for p in text if p not in punctuation)\n    return text","2676b52b":"train_df['clean'] = train_df['text'].apply(cleaning)","65710988":"train_df['clean'].head(10)","155597b5":"texts = train_df['clean']\ntargets = np.asarray(train_df['label'])","3b7ab7e2":"MAX_NB_WORDS = 20000\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index  #count unique tokens\nprint('Number of Unique Tokens',len(word_index))","40c09a8d":"MAX_SEQUENCE_LENGTH = 1000\ntext_data = pad_sequences(sequences,maxlen = MAX_SEQUENCE_LENGTH,\n                          padding = 'post',\n                          truncating = 'post')","7c9bba67":"EMBEDDING_DIM = 100\n\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\ncp=ModelCheckpoint('model_Rnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)","4d858a20":"model.compile(optimizer='adam',\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy'])","5ed0bb20":"VALIDATION_SPLIT = 0.2\nEPOCHS = 5\n\nX_train, X_test, y_train, y_test = train_test_split(text_data, targets, test_size=0.25, random_state=7, shuffle=True)\nhistory = model.fit(X_train,\n                    y_train, \n                    batch_size = 128, \n                    validation_split = VALIDATION_SPLIT,\n                    epochs = EPOCHS,\n                    callbacks=[cp])","ffdd927c":"loss, accuracy = model.evaluate(X_train, y_train, verbose=True)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nprint(\"Training Loss: {:.4f}\".format(loss))\n\nloss, accuracy = model.evaluate(X_test, y_test, verbose=True)\nprint(\"Testing Accuracy: {:.4f}\".format(accuracy))\nprint(\"Testing Loss: {:.4f}\".format(loss))","4faabb79":"from sklearn.metrics import classification_report\n\ny_pred=model.predict(X_test, batch_size=200, verbose=1)\nreport = classification_report(y_test, y_pred.round())\nprint(report)","2aec3d05":"test_df = pd.read_csv(\"..\/input\/fake-news\/test.csv\")","88be91dd":"print('Shape of dataset ',test_df.shape)\nprint(test_df.columns)\ntest_df.head()","60487238":"test_df.isnull().sum()","0ef9ab0c":"test_df.fillna(method = 'bfill', inplace = True)","e038130f":"test_df['clean'] = test_df['text'].apply(cleaning)","512aafb2":"text_test = test_df['clean']\ntest_id = test_df['id']","80c8936a":"test_sequences = tokenizer.texts_to_sequences(text_test)\ntest_data = pad_sequences(test_sequences,\n                          maxlen = MAX_SEQUENCE_LENGTH,\n                          padding = 'post',\n                          truncating = 'post') ","5eb40d28":"preds = model.predict_classes(test_data)\npreds","1fad977a":"predictions =[]\nfor i in preds:\n    predictions.append(i[0])","39cf5333":"len(predictions)","2cfdda05":"submission = pd.DataFrame({'id':test_id, 'label':predictions})\nsubmission.shape","142a7e0e":"submission.head(5)","4b8c808a":"submission.to_csv('submit.csv',index=False)","61b6db20":"## Testing","7023918d":"# Preparing the text data"}}