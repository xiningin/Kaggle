{"cell_type":{"0ad35494":"code","85b594ab":"code","3a1dbdb0":"code","993080f7":"code","b1b99c9b":"code","474d3e5b":"code","f5ed8141":"code","fa0c1770":"code","afce4917":"code","50104b0f":"code","94bb1e48":"markdown"},"source":{"0ad35494":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85b594ab":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n","3a1dbdb0":"test = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')\ntrain = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\n","993080f7":"\ndef get_offset(a,b):\n  if (a)>0 and b<0:\n    if b<0-a:\n      offset=b\n    else:\n      offset=0-a\n  elif a>0 and b>0:\n    if b>a:\n      offset=0-b\n    else:\n      offset=0-a\n  elif a<0 and b>0:\n    if b<0-a:\n      offset=b\n    else:\n      offset=0-a\n  elif a<0 and b<0:\n    offset=a+b\n  return offset\n\n\n\ndef get_samples(input_batch,single_batch_size,roll_period):\n  test_drift=input_batch;roll_period=np.int(roll_period)\n  result = seasonal_decompose(input_batch, model='additive',freq=1)\n  trend_rolling=result.trend.rolling(roll_period).mean()\n  trend_rolling=trend_rolling.fillna(trend_rolling.min())\n  c=trend_rolling.values\n  data_without_drift=pd.Series(c-np.array(input_batch))\n  c=pd.Series(c)\n  xx=data_without_drift.rolling(roll_period).mean().min()\n  a=xx-c.min();b=input_batch.mean()\n  return a,b,c,data_without_drift\n\n##################\ndef remove_redundant(sub_index,batch_index,relax_rate=5):\n    sub2=[];\n    #print(\"sub_index:\",sub_index)\n    #print(\"sub2 :\",sub2)\n    for i in range(0,len(sub_index)-1):\n        if sub_index[i+1]-sub_index[i]>relax_rate*num_record_per_sec:\n            sub2.append(sub_index[i])\n    sub_index=sub2;sub2=[]\n    for i in range(0,len(sub_index)):\n        c=0\n        for j in range(len(batch_index)):\n            if abs(sub_index[i]-batch_index[j])<relax_rate*num_record_per_sec:\n                c+=1\n        if c==0:\n            sub2.append(sub_index[i])\n    sub2.extend(batch_index)\n    sub2 = list(dict.fromkeys(sub2))\n    sub2=np.sort(sub2)\n    return list(sub2)\n","b1b99c9b":"def get_subindex2sample(df,istrain,batch_index,batch_index1,order=2,relax_rate=5):\n  sub_index=[];count=0;\n  while(order!=0):\n    #print(order)\n    for i in range(len(batch_index)-1):\n      train_series=pd.Series(df.iloc[batch_index[i]:batch_index[i+1]].signal);idx=[]\n      a,b,c,data_without_drift_init=get_samples(train_series,batch_index[i+1]-batch_index[i],np.round((num_record_per_sec\/single_batch_size)*(batch_index[i+1]-batch_index[i])))\n      offset=get_offset(a,b)\n\n      data_without_drift_init=offset-data_without_drift_init\n      f=data_without_drift_init.rolling(num_record_per_sec).mean()\n      f=f.fillna(f.mean())\n      g=np.array(c)\n      idx = np.argwhere(np.diff(np.sign(f - g))).flatten()\n      if count==0:\n        idx=np.insert(idx,len(idx),single_batch_size);idx=np.insert(idx,0,0)\n      idx=[val+batch_index[i] for val in idx]\n      sub_index.extend(idx)\n    sub_index=remove_redundant(sub_index,batch_index1,relax_rate)\n    #sub_index=np.sort(sub_index)\n    batch_index=sub_index\n    order-=1;count+=1\n  if sub_index[-1]!=df.shape[0]:\n    sub_index[-1]=df.shape[0] \n  if sub_index[0]!=0:\n    sub_index[0]=0 \n  return sub_index\n","474d3e5b":"#vary relax_rate to get better results\nsingle_batch_size=500000;num_record_per_sec=10000;relax_rate=5\n\nfor df in [train,test]:\n  num_of_batch=np.int(df.shape[0]\/single_batch_size)\n  count=pd.Series()\n  batch_index=[val*single_batch_size for val in range(num_of_batch+1)]\n  sampling_idx= get_subindex2sample(df,True,batch_index,batch_index,order=2,relax_rate=relax_rate)\n  print(sampling_idx[0])\n  sampling_idx=np.sort(sampling_idx)\n  for i in range(len(sampling_idx)-1):\n    train_series=pd.Series(df.iloc[sampling_idx[i]:sampling_idx[i+1]].signal)\n    a,b,c,data_without_drift_init=get_samples(train_series,sampling_idx[i+1]-sampling_idx[i],np.round((num_record_per_sec\/single_batch_size)*(sampling_idx[i+1]-sampling_idx[i])))\n    offset=get_offset(a,b)\n    data_without_drift_init=offset-data_without_drift_init\n    count=count.append(data_without_drift_init)\n  count=list(count)\n  print(len(count))\n  df['new_sig']=count\n","f5ed8141":"plt.figure(figsize=(30,8))\n\ntrain.signal.plot(color='r')\ntrain.new_sig.plot()\n","fa0c1770":"plt.figure(figsize=(30,8))\n\ntest.signal.plot(color='r')\ntest.new_sig.plot()\n","afce4917":"train[train.signal>0]","50104b0f":"test[test.signal<0]","94bb1e48":"This notebook is attempted to remove the drift from the signal\n\n1) First sampling index is generated\n2) The samples are then processed for drift removal from the sampling index\n\n\n#vary relax_rate to get different results(relax_rate=5) is optimal value \nThe points where the drift occurs are noted down and those points are the sampling index.\n\n\n\nPlease do upvote the kernel if you enjoy the approach!!!"}}