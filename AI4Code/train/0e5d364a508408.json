{"cell_type":{"5ff57b47":"code","387323be":"code","c4ebec0d":"code","1b12db94":"markdown","3ddd878a":"markdown","76b3f71e":"markdown","415328c8":"markdown","114b5cb4":"markdown"},"source":{"5ff57b47":"import numpy as np # linear algebra\nimport itertools   # compact iteration\n\nclass MLP:\n    \"\"\"Multilayer Perceptron class\"\"\"\n    \n    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2):\n        \"\"\"Constructor for the MLP class. Takes the number of inputs, a\n        variable number of hidden layers and number of outputs\n        \n        Args:\n            num_inputs (int):     number of inputs\n            hidden_layers (list): list of ints for the hidden layer\n            num_outputs (int):    number of outputs\n        \"\"\"\n        # Initialize the numbers of nodes\n        self.inputs = num_inputs\n        self.hidden_layers = hidden_layers\n        self.outputs = num_outputs\n        \n        # Generic representation of layers\n        layers = [num_inputs] + hidden_layers + [num_outputs]\n        \n        # Random wights (each layer has a weight matrix)\n        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n        \n        # Activations (each layer has an activation array)\n        self.activations = [np.zeros((num_nodes, 1)) for num_nodes in layers]\n        \n        # Derivatives (each layer has a derivative matrix)\n        self.derivatives = [np.zeros((layers[i], layers[i+1])) for i in range(len(layers)-1)]\n        \n        \n    def forward_propagate(self, inputs):\n        \"\"\"Computes forward propagation of the network based on signal inputs\n        \n        Args:\n            inputs (ndarray): input signals\n        Returns:\n            activations (ndarray): output values\n        \"\"\"\n        # Define activations caclulation for 1 whole layer\n        get_activations = lambda a, w: self._sigmoid(w.T @ a)\n        \n        # Caculate activations for each layer iteratively (and transform inputs from row to column vector)\n        self.activations = list(itertools.accumulate([np.atleast_2d(inputs).T] + self.weights, get_activations))\n        \n        # Return output layer activation\n        return self.activations[-1]\n    \n    \n    def back_propagate(self, error):\n        \"\"\"Computes backward propagation of the network based on error\n        \n        Args:\n            error (ndarray): output errors\n        Returns:\n            error (ndarray): input layer error\n        \"\"\"\n        for i in reversed(range(len(self.derivatives))):\n            delta = error * self._sigmoid_derivative(self.activations[i+1]).T\n            self.derivatives[i] = self.activations[i] @ delta\n            error = delta @ self.weights[i].T\n            \n        return error\n    \n    def gradient_descent(self, alpha):\n        \"\"\"Applies gradient descent to update the network's weights\n        \n        Args:\n            alpha (float): learning rate\/step\n        \"\"\"\n        for i, d in enumerate(self.derivatives):\n            self.weights[i] += alpha * d\n    \n    \n    def train(self, inputs, targets, epochs, alpha):\n        \"\"\"Trains the neural network's weights by applying gradient descent\n        \n        Args:\n            inputs (float):  training data - x values\n            targets (float): training labels - y values\n            epochs (int):    number of times to go through all the weights\n            alpha (float):   learning rate\/step\n        \"\"\"\n        # Repeat a given number of times\/epochs\n        for e in range(epochs):\n            \n            # Overall error of all training samples\n            sum_error = 0\n            \n            # Loop through all the given training samples\n            for x, y in zip(inputs, targets):\n                # Forward propagation\n                output = self.forward_propagate(x)\n                \n                # Backpropagation based on error\n                self.back_propagate(y - output)\n                \n                # Apply gradient descent\n                self.gradient_descent(alpha)\n                \n                # Accumulate the error\n                sum_error += self._mse(y, output)\n            \n            # Report normalized error\n            print(f\"Epoch {e}: error = {sum_error\/len(inputs)}\")\n        \n    def _mse(self, target, output):\n        \"\"\"Mean Square Error function\n        \n        Args:\n            target (ndarray): target values\n            output (ndarray): actual values\n            \n        Returns:\n            (float): average squared difference between labels and network outputs\n        \"\"\"\n        return np.average((target - output)**2)\n    \n    def _sigmoid(self, x):\n        \"\"\"Sigmoid activation function\n        \n        Args:\n            x (float): value to be processed\n        Returns:\n            (float): output\n        \"\"\"\n        return 1.0 \/ (1 + np.exp(-x))\n    \n    \n    def _sigmoid_derivative(self, x):\n        \"\"\"Sigmoid derivative function\n        \n        Args:\n            x (float): sigmoid function value\n        Returns:\n            (float): sigmoid derivative value\n        \"\"\"\n        return x * (1.0 - x)","387323be":"# Create a model\nmlp = MLP(2, [5], 1)\n\n# Create dummy inuts and outpus\ninputs = np.random.rand(1000, 2)\/2\ntargets = np.sum(inputs, axis=1)\n\n# Perform training with 50 epochs\nmlp.train(inputs, targets, 50, 0.1)\n\n# Create a dummy new test input-output pair\nx_new = np.array([0.3, 0.1])\ny_new = np.array([0.4])\n\n# Perform forward propagation to get the trained model's guess\noutput = mlp.forward_propagate(x_new)\n\n# Print the output to see the result\nprint(f\"Network's guess: {output} | True label: {y_new}\")","c4ebec0d":"import tensorflow as tf                              # ML algorithm library\nfrom sklearn.model_selection import train_test_split # data splitter\n\n# Helper method to generate the train-test data\ndef generate_dataset(num_samples, test_size):\n    # Create dummy inuts and outpus\n    x = np.random.rand(num_samples, 2)\/2\n    y = np.sum(x, axis=1)\n    \n    # Return training and test inputs and labels\n    return train_test_split(x, y, test_size=test_size)\n\n# Generate the dataset\n(x_train, x_test, y_train, y_test) = generate_dataset(5000, 0.3)\n\n# Build the model with dense (fully connected) layers\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(5, input_dim=2, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\nmodel.compile(optimizer=optimizer, loss=\"MSE\")\n\n# Train\nmodel.fit(x_train, y_train, epochs=100)\n\n# Evaluate\nprint(\"\\nModel evaluation:\")\nmodel.evaluate(x_test, y_test, verbose=1)\n\n# Create some dummy new values and make a prediction\nx_new = np.array([[0.1, 0.2], [0.2, 0.2]])\npredictions = model.predict(x_new)\n\n# Check the predictions\nprint(\"\\nPredictions:\")\nfor x, y in zip(x_new, predictions):\n    print(f\"{x[0]} + {x[1]} = {y[0]}\")","1b12db94":"# Multilayer Perceptron Class\n## \\_\\_init\\_\\_\n1. We create layers by providing a number of nodes each layer should have (input layer, every hidden layer and the output layer)\n2. We calculate a weight matrix for each layer (except last):\n    * We take every node in $i^{th}$ layer and assign as many weights (random values) to it as there are nodes in $i^{th}+1$ layer\n    * This gives us a matrix where rows correspond to $i^{th}$ layer nodes and columns correspond to $i^{th} + 1$ layer nodes\n3. We initialize a vertical array of activations for every layer:\n    * We take every node in $i^{th}$ layer and map it to an activation value initially set to $0$\n    * This gives us a column vector where each row value corresponds to that layer's node\n4. We initialize a derivative matrix for each layer (except last):\n    * We take every node in $i^{th}$ layer and assign as many derivative values (initially set to $0$) to it as there are nodes in $i^{th}+1$ layer\n    * This gives us a matrix where rows correspond to $i^{th}$ layer nodes and columns correspond to $i^{th} + 1$ layer nodes\n\n## forward_propagate\nWe know that activations for $l^{th}+1$ layer are calculated by taking a sigmoid of the dot product between $l^{th}$ layer's weight matrix and $l^{th}$ layer's activations:\n\n$$\\mathbf{a}^{(l+1)}=g(W^{T(l)}\\mathbf{a}^{(l)})$$\n\n* $\\mathbf{a}^{(l)}$ - vector of each node's activation in $l^{th}$ layer\n* $W^{(l)}$ - matrix of weights in $l^{th}$ layer (we take transpose because we want rows to correspond to $l^{th}+1$ layer)\n* $g(\\mathbf{z})$ - activation function (sigmoid)\n\nWe calculate activations for _all_ nodes in every layer:\n1. We first layer's weights (transpose matrix) and dot it with inputs as first layer activations (column vector) \n2. We pass the result through sigmoid function to get activations (column vector) of the next layer\n3. If the next layer is not the output layer, we repeat the process on this next layer\n\n## backward_propagate\nWe use backpropagation to determine the contribution to the model's inaccuracy of all the weights. We use _sum of squares_ as our _overall_ cost function$^{1}$.\n\n### Backpropagation from $l^{th}+1$ (last) to $l^{th}$ layer$^2$\nWe know that the gradient of the cost function with respect the weights in $l^{th}$ layer (except last) is calculated as follows:\n\n$$\\frac{\\partial C}{\\partial W^{(l)}}=\\frac{\\partial C}{\\partial \\mathbf{a}^{(l+1)}}\\frac{\\partial \\mathbf{a}^{(l+1)}}{\\partial \\mathbf{z}^{(l+1)}}\\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial W^{(l)}}$$\n\n* $C_k=\\sum_k(y_k-a_k)^2$\n* $\\mathbf{a}^{(l+1)}=g(\\mathbf{z}^{(l+1)})$\n* $\\mathbf{z}^{(l+1)}=W^{T(l)}\\mathbf{a}^{(l)}$\n\nFinding the partial derivatives and viewing the error and sigmoid derivative multiplication as some change $\\delta$, we attain a compact formula:\n\n$$D_{W}^{(l)}=\\mathbf{a}^{(l)}\\mathbf{\\delta}^{T(l+1)},\\ where$$\n\n$$\\mathbf{\\delta}^{T(l+1)}=((\\mathbf{y}^{(l+1)}-\\mathbf{a}^{(l+1)}) \\times g'(\\mathbf{z}^{(l+1)}))^T$$\n\n### Backpropagation from $l^{th}$ to $l^{th}-1$ layer$^3$\n\nWe know that the gradient of the cost function with respect the weights in $l^{th}-1$ layer is calculated as follows:\n\n$$\\frac{\\partial C}{\\partial W^{(l-1)}}=\\frac{\\partial C}{\\partial \\mathbf{a}^{(l+1)}}\\frac{\\partial \\mathbf{a}^{(l+1)}}{\\partial \\mathbf{z}^{(l+1)}}\\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{a}^{(l)}}\\frac{\\partial \\mathbf{a}^{(l)}}{\\partial \\mathbf{z}^{(l)}}\\frac{\\partial \\mathbf{z}^{(l)}}{\\partial W^{(l-1)}}$$\n\nThis time there are more terms in the chain rule but the first two are the same. For this reason, the change $\\delta$ this time depends on more terms:\n\n$$D_{W}^{(l-1)}=\\mathbf{a}^{(l-1)} \\cdot \\mathbf{\\delta}^{T(l)},\\ where$$\n\n$$\\mathbf{\\delta}^{T(l)}=(\\mathbf{\\delta}^{T(l+1)} \\cdot W^{T(l)}) \\times g'(\\mathbf{z}^{(l)})^T$$\n\n### Algorithm\nWe start from the last - 1 layer (becasue last layer doesn't have weights) and go one by one backwards to the first layer:\n1. We calculate upper layer's `delta` usd for derivative calculation\n    * We multiply upper layer's `errors` (first time provided as a parameter, other times claculated in previous iteration) by upper layer's `sigmoid derivatives` (based on upper layer's activations (or outputs) known by the model)\n2. We calculate `derivative matrix` which we store in a list representing every layer\n    * We dot current layer's `activations` (known by model) with upper layer's `delta`\n3. We update `error` for the next layer backwards\n    * We dot the upper layer's `delta` with current layer's `weight matrix`\n\n<hr><\/hr>\n\n* <sub><sup>1 - see Appendix A - reasons behind the cost function for more details about the cost function<\/sup><\/sub>\n* <sub><sup>2 - see Appendix B - differentiation of the _sum of squares_ for more details on how to derive the formulas<\/sup><\/sub>\n* <sub><sup>3 - see Appendix C - differentiation of the _sum of squares_ continued for more details on how to derive the formulas<\/sup><\/sub>\n    \n## gradient_descent\nGradient descent simply updates our weights by adjusting them towards the direction at which the error will be minimized. For every weight we add some value controlled by the learning rate to update that weight. In matrix form it looks as follows:\n\n$$W^{(l)}_{new}=W^{(l)}_{old}+\\alpha D_{W}^{(l)}$$\n\n## train\nTraining happens by performing gradient descent on all the training samples a specified number of times (`epochs`):\n1. Find activations for one input\n2. Perform back-propagation based on the acquired activations and desired target\n3. Use the caclulated derivatives to perform gradient descent\n4. Repeat for all inputs and check the network's accuracy after one such epoch\n\n## \\_mse\nMean Square Error allows to check model's accuracy by averaging the vector of squared errors, i.e., the squared difference between model's outputs and labels:\n$$MSE^{(l+1)}=\\frac{1}{K}\\sum_k(a_k^{(l+1)}-y_k^{(l+1)})^2$$\n\n## \\_sigmoid\nSigmoid is the activation function we use to squash the node's input (regression value) to a value between 0 and 1. This normalizes the behavior of nodes:\n\n$$g(z)=\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n\n## \\_sigmoid_derivative\nSigmoid derivative with respect to what's inside it is part of the chain rule equation when we calculate the total derivative:\n\n$$g'(z)=\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$$","3ddd878a":"# Applying MLP\n## Model creation\n1. Firstly we initialize a model with 2 nodes in the input layer, 5 nodes in one hidden layer, and one node in the output layer\n2. Secondly we create 1000 pairs of inputs, all of which are $0 \\le input \\le 0.5$ and 1000 labels which are just the sums of each pair of inputs\n3. Finally we train the model for 50 epochs with learning rate 0.1\n\n## Model test\n1. We create a new input pair and a supposed output\n2. We use the model to guess the answer\n3. We check how much the guessed answer differs from the supposed one","76b3f71e":"# Appendix\n## Appendix A - reasons behind the cost function\n### Overall error\nIt may feel intuitive to take an error at some output node and differentiate it with respect to any weight we want to find its gradient. But we actually need an _overall error_. \n\nSay we want to differentiate an error at $k^{th}$ node in $l^{th}+1$ layer with respect to a weight $w_{ji}$ in $l^{th}-1$ layer. This means there exists a $j^{th}$ node in $l^{th}$ layer which has another weight $w_{kj}$ connected to the $k^{th}$ node in $l^{th}+1$ layer. But $j^{th}$ node has multiple weights connected to $l^{th}+1$ layer nodes, therefore, its error depends on the overall error of all the nodes in $l^{th}+1$ layer. For this reason, any weight in $l^{th}-1$ layer that's connected to $j^{th}$ node in the $l^{th}$ layer is influenced by the overall error of the $l^{th}+1$ layer, so we cannot simply differentiate with respect to just a single node in $l^{th}+1$ layer.\n\nThis means we firstly need to find the total error at $j^{th}$ node and then we can differentiate it with respect to any weight in $l^{th}-1$ layer that's connected to it. But this means we first need find that total error at $j^{th}$ node which is calculated by differentiating the overall error of the $l^{th}+1$ layer with respect to every weight that's connected to it from the $j^{th}$ node. So this actually means that differentiating the total error at $j^{th}$ node in $l^{th}$ layer with respect to a weight in $l^{th}-1$ layer that's connected to that node is the same as differentiating the overall error in $l^{th}+1$ layer with respect to such node.\n\n### Sum of squares\nOne of the reasons why _sum of squares_ is used is because it is continuously differentiable. It also emphasizes and allows to punish more those weights which induce bigger errors. _Sum of squares_ actually rises from an assumption that the regression is performed on variables with normally distributed error.\n\n## Appendix B - differentiation of the _sum of squares_\n\nThe _sum of squares_ can be represented through a vector notation (dot product):\n\n$$\\sum_k(y_k^{(l+1)}-a_k^{(l+1)})^2=\\begin{bmatrix}1 & \\cdots & 1 \\end{bmatrix}\\cdot \\begin{bmatrix} (y_0^{(l+1)}-a_0^{(l+1)})^2 \\\\ \\vdots \\\\ (y_k^{(l+1)} - a_k^{(l+1)})^2 \\end{bmatrix}$$\n\nWhich allows to introduce vectors as denominators in the chain rule applied to the differentiation of the _sum of squares_:\n\n$$\\frac{\\partial C}{\\partial W^{(l)}}=\\frac{\\partial C}{\\partial \\mathbf{a}^{(l+1)}}\\frac{\\partial \\mathbf{a}^{(l+1)}}{\\partial \\mathbf{z}^{(l+1)}}\\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial W^{(l)}}$$\n\nBut differentiating with respect to vectors can lead to difficultly interpretable tensors and uncommon element-wise operations, such as dot product. Therefore we will stick to differentiating with respect to a single element _that is being summed_ (except for the weight matrix which has multiple elements). Therefore it is simpler to comprehend the following differentiation:\n\n$$\\frac{\\partial C}{\\partial W^{(l)}}=\\frac{\\partial C}{\\partial a_k^{(l+1)}}\\frac{\\partial a_k^{(l+1)}}{\\partial z_k^{(l+1)}}\\frac{\\partial z_k^{(l+1)}}{\\partial W^{(l)}}$$\n\nNote:\n* We choose **$\\times$** to represent element-wise matrix multiplication\n* We choose **$\\cdot$** to represent the dot product of 2 matrices\n\n### First term\n\nFirstly, we calculate the derivative of the overall cost with respect to each $k^{th}$ activation in the $l^{th}+1$ layer (note: as $k$ varies, the denominator also varies, i.e., $a_k$ changes so we need to take the sum out):\n\n$$\\frac{\\partial C}{\\partial a_k^{(l+1)}}=\\frac{\\partial \\sum_k(y_k^{(l+1)}-a_k^{(l+1)})^2}{\\partial a_k^{(l+1)}}=-2 \\sum_k (y_k^{(l+1)}-a_k^{(l+1)})$$\n\n### Second term\nNow, we calculate the derivatives of every $k^{th}$ activation in the $l^{th}+1$ with respect to their node input value:\n\n$$\\frac{\\partial a_k^{(l+1)}}{\\partial z_k^{(l+1)}}=\\frac{\\partial g(z_k^{(l+1)})}{\\partial z_k^{(l+1)}}=g'(z_k^{(l+1)})=g(z_k^{(l+1)})(1-g(z_k^{(l+1)}))$$\n\n### Third term\nFinally, we calculate the derivative of every $k^{th}$ node input value in the $l^{th}+1$ layer with respect to all the weights ($J\\times K$ matrix) in the $l^{th}$ layer (note: as $j$ varies, none of the denominators of weights changes so sum does not need to be taken out when differentiating with respect to some weight):\n\n$$\\frac{\\partial z_k^{(l+1)}}{\\partial W^{(l)}}=\\frac{\\partial \\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial \\begin{pmatrix}\n    w_{00}^{(l)} & \\cdots & w_{K0}^{(l)} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    w_{0J}^{(l)} & \\cdots & w_{KJ}^{(l)}\n\\end{pmatrix}}=\\begin{pmatrix}\n    \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{00}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{k0}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{K0}^{(l)}} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{0j}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{kj}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{Kj}^{(l)}} \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{0J}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{kJ}^{(l)}} & \\cdots & \\frac{\\sum_j w_{kj}^{(l)}a_j^{(l)}}{\\partial w_{KJ}^{(l)}}\n\\end{pmatrix}=\\begin{pmatrix}\n    0 & \\cdots & a_0^{(l)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & a_j^{(l)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & a_J^{(l)} & \\cdots & 0\n\\end{pmatrix}$$\n\n### Total derivative\nNow we can multiply every term of the derivative, denote the first part as some change $\\delta$, get rid of $-2$ (because it is included in learning rate) and compactify it:\n\n$$\\sum_k \\underbrace{(y_k^{(l+1)}-a_k^{(l+1)})g(z_k^{(l+1)})(1-g(z_k^{(l+1)}))}_{\\delta_k^{(l+1)}}\\begin{pmatrix}\n    0 & \\cdots & a_0^{(l)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & a_j^{(l)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & \\underbrace{a_J^{(l)}}_{k^{th}\\ column} & \\cdots & 0\n\\end{pmatrix}=\\begin{pmatrix}\n    y_0^{(l+1)}-a_0^{(l+1)} & \\cdots & y_K^{(l+1)}-a_K^{(l+1)} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    y_0^{(l+1)}-a_0^{(l+1)} & \\cdots & y_K^{(l+1)}-a_K^{(l+1)}\n\\end{pmatrix}\n\\begin{pmatrix}\n    g(z_0^{(l+1)})(1-g(z_0^{(l+1)})) & \\cdots & g(z_K^{(l+1)})(1-g(z_K^{(l+1)})) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    g(z_0^{(l+1)})(1-g(z_0^{(l+1)})) & \\cdots & g(z_K^{(l+1)})(1-g(z_K^{(l+1)}))\n\\end{pmatrix}\n\\begin{pmatrix}\n    a_0^{(l)} & \\cdots & a_0^{(l)} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    a_J^{(l)} & \\cdots & a_J^{(l)}\n\\end{pmatrix}$$\n\nWe note that some elements in matrices repeat themselves therefore we can represent the equation more neatly by introducing the dot product (also, we use a vetor $\\delta$ to visually keep the calculations performed on the activation vector compressed):\n\n$$D_{W^{(l)}}=\\begin{bmatrix} a_{0}^{(l)} \\\\ \\vdots \\\\ a_{J}^{(l)} \\end{bmatrix}\\begin{bmatrix}\\delta_0^{(l+1)} & \\cdots & \\delta_K^{(l+1)} \\end{bmatrix},\\ where$$\n\n$$\\begin{bmatrix}\\delta_0^{(l+1)} & \\cdots & \\delta_K^{(l+1)} \\end{bmatrix}=\\begin{bmatrix}y_0^{(l+1)}-a_0^{(l+1)} & \\cdots & y_K^{(l+1)}-a_K^{(l+1)}\\end{bmatrix} \\times \\begin{bmatrix}g'(z_0^{(l+1)}) & \\cdots & g'(z_K^{(l+1)})\\end{bmatrix}$$\n\n## Appendix C - differentiation of the _sum of squares_ continued\n\nGoing back 1 layer further, the derivatives of each weight at that layer can be caclulcated as follows:\n\n$$\\frac{\\partial C}{\\partial W^{(l)}}=\\frac{\\partial C}{\\partial \\mathbf{a}^{(l+1)}}\\frac{\\partial \\mathbf{a}^{(l+1)}}{\\partial \\mathbf{z}^{(l+1)}}\\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{a}^{(l)}}\\frac{\\partial \\mathbf{a}^{(l)}}{\\partial \\mathbf{z}^{(l)}}\\frac{\\partial \\mathbf{z}^{(l)}}{\\partial W^{(l-1)}}$$\n\nBut again we simplify this to differentiating with respect to an element that is being summed:\n\n$$\\frac{\\partial C}{\\partial W^{(l)}}=\\frac{\\partial C}{\\partial a_k^{(l+1)}}\\frac{\\partial a_k^{(l+1)}}{\\partial z_k^{(l+1)}}\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}}\\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}\\frac{\\partial z_j^{(l)}}{\\partial W^{(l-1)}}$$\n\n### Third term\nOnly the 3rd term is not familiar to us but it can be differentiated easily (note again that as $j$ changes, the respect with which we're differentiating also changes so we need to take out the sum):\n\n$$\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}}= \\frac{\\partial \\sum_jw_{kj}^{(l)}a_j^{(l)}}{\\partial a_j^{(l)}} = \\sum_jw_{kj}^{(l)}$$\n\n### Total derivative\nFollowing the similarity of the upper layer derivatives, we can write down the total derivative by multiplying each term and compactify it by replacing sums with matrix multiplication:\n\n$$\\sum_k\\sum_j \\underbrace{\\underbrace{(y_k^{(l+1)}-a_k^{(l+1)})g'(z_k^{(l+1)})}_{\\delta_k^{(l+1)}}w_{kj}^{(l)}g'(z_j^{(l)})}_{\\delta_j^{(l)}}\\begin{pmatrix}\n    0 & \\cdots & a_0^{(l-1)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & a_i^{(l-1)} & \\cdots & 0 \\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n    0 & \\cdots & \\underbrace{a_I^{(l-1)}}_{j^{th}\\ column} & \\cdots & 0\n\\end{pmatrix}=\\sum_k\\begin{pmatrix}\n    \\delta_k^{(l+1)} & \\cdots & \\delta_k^{(l+1)} \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    \\delta_k^{(l+1)} & \\cdots & \\delta_k^{(l+1)}\n\\end{pmatrix}\\begin{pmatrix}w_{k0}^{(l)} & \\cdots & w_{kJ}^{(l)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{k0}^{(l)} & \\cdots & w_{kJ}^{(l)}\\end{pmatrix}\\begin{pmatrix}g'(z_0^{(l)}) & \\cdots & g'(z_J^{(l)}) \\\\ \\vdots & \\ddots & \\vdots \\\\ g'(z_0^{(l)}) & \\cdots & g'(z_J^{(l)})\\end{pmatrix}\\begin{pmatrix}a_0^{(l-1)} & \\cdots & a_0^{(l-1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_I^{(l-1)} & \\cdots & a_I^{(l-1)}\\end{pmatrix}=\n\\begin{pmatrix}\\delta_{0}^{(l+1)} & \\cdots & \\delta_{0}^{(l+1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\delta_{K}^{(l+1)} & \\cdots & \\delta_{K}^{(l+1)}\\end{pmatrix}\\begin{pmatrix}w_{00}^{(l)} & \\cdots & w_{0J}^{(l)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{K0}^{(l)} & \\cdots & w_{KJ}^{(l)}\\end{pmatrix}\\begin{pmatrix}g'(z_0^{(l)}) & \\cdots & g'(z_J^{(l)}) \\\\ \\vdots & \\ddots & \\vdots \\\\ g'(z_0^{(l)}) & \\cdots & g'(z_J^{(l)})\\end{pmatrix}\\begin{pmatrix}a_0^{(l-1)} & \\cdots & a_0^{(l-1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_I^{(l-1)} & \\cdots & a_I^{(l-1)}\\end{pmatrix}$$\n\nSimilarly to the total derivative in the upper layer, some terms in matrices repeat themselves therefore we can further compactify the notation with dot product:\n\n$$D_{W^{(l-1)}}=\\begin{bmatrix} a_{0}^{(l-1)} \\\\ \\vdots \\\\ a_{I}^{(l-1)} \\end{bmatrix}\\begin{bmatrix}\\delta_0^{(l)} & \\cdots & \\delta_J^{(l)} \\end{bmatrix},\\ where$$\n\n$$\\begin{bmatrix}\\delta_0^{(l)} & \\cdots & \\delta_J^{(l)} \\end{bmatrix}=\\begin{bmatrix}\\delta_{0}^{(l+1)} & \\cdots & \\delta_{K}^{(l+1)}\\end{bmatrix} \\cdot \\begin{pmatrix}w_{00}^{(l)} & \\cdots & w_{0J}^{(l)} \\\\ \\vdots & \\ddots & \\vdots \\\\ w_{K0}^{(l)} & \\cdots & w_{KJ}^{(l)}\\end{pmatrix} \\times \\begin{bmatrix}g'(z_0^{(l)}) & \\cdots & g'(z_J^{(l)})\\end{bmatrix}$$","415328c8":"# Implementing MLP with Tensorflow\n## Data Generation\nWe create a helper method which generates a given number of samples and splits those samples to training and test data based on the specified proportion. In this example, we simply create some pairs of float numbers between `0` and `0.5` as our `x` data and the sums of each pair as our `y` data. \n\n## Model Creation\n1. We create a sequential model with 3 fully connected layers and specify activations as sigmoids\n2. We choose our optimizer as SGD (Stochastic Gradient Descent) which defaults to vanilla Gradient Descent\n3. We compile the created model with our chosen optimizer and MSE as our loss function\n4. We train the model for 100 epochs on randomly chosen samples from each batch forming the overall training data (the default `batch_size` is `32`)\n\n## Model Evaluation\n1. We evaluate how well the model performed on unseen test data\n2. We create a dummy new value and make a prediction on it\n3. We check how close the network's guess is to the true value","114b5cb4":"# Theory and algorithms behind a simple NN\n## Overview\nIn this notebook, I create and test a simple _Multilayer Perceptron Class_, explain the algorithms it uses and the theory behind them with all the necessary calculations and mathematical notations. Also, the same class implementation is done using Tensorflow to show how easy it is to create a model using a big library.\n\n> Note: to understand this, one should be familiar with the general way the neural networks work and should know the core concepts of linear algebra, such as differentiation and dot product.\n\n## Motivation\nPrimary purpose of this notebook is to help myself to undertand the details of a simple _neural network_ before diving into data science libraries which create such networks in just 3 lines of code. Perhaps this can also be useful for those who want to refresh or get a clear mathematical understanding of the algorithms a _Multilayer Perceptorn_ uses.\n\n## Resources\nThe main sources of reference are these videos:\n* https:\/\/www.youtube.com\/watch?v=ScL18goxsSg\n* https:\/\/www.youtube.com\/watch?v=Z97XGNUUx9o\n* https:\/\/www.youtube.com\/watch?v=JdXxaZcQer8\n\nHowever this notebook has a lot of influences from various internet sites, such as https:\/\/datascience.stackexchange.com\/, my own comprension and analysis."}}