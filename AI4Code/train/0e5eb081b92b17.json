{"cell_type":{"e3df8618":"code","43adf1cf":"code","f6c3ae5f":"code","1bce30e9":"code","6c65deb0":"code","54f88542":"code","c4215177":"markdown","e52297e1":"markdown","27049d02":"markdown","1ce545bd":"markdown","0141773b":"markdown","17afbf4f":"markdown","ac4a5837":"markdown","6c2d667d":"markdown"},"source":{"e3df8618":"import os\nimport json\n\nfrom kaggle_secrets import UserSecretsClient","43adf1cf":"secrets = UserSecretsClient()\n\nos.environ['KAGGLE_USERNAME'] = secrets.get_secret(\"KAGGLE_USERNAME\")\nos.environ['KAGGLE_KEY'] = secrets.get_secret(\"KAGGLE_KEY\")","f6c3ae5f":"os.makedirs('\/kaggle\/dataset\/', exist_ok=True)\n\n\n# Change below\nmeta = dict(\n    id=\"xhlulu\/my-dataset\",\n    title=\"My brand new dataset\",\n    isPrivate=True,\n    licenses=[dict(name=\"other\")]\n)\n\nwith open('\/kaggle\/dataset\/dataset-metadata.json', 'w') as f:\n    json.dump(meta, f)","1bce30e9":"import pandas as pd\nimport numpy as np\n\nos.makedirs('\/kaggle\/dataset\/my-directory', exist_ok=True)\n\ndf = pd.DataFrame(\n    np.random.randint(0, 10, (10, 3)),\n    columns=['apple', 'pear', 'orange']\n)\n\ndf.to_csv('\/kaggle\/dataset\/some_table.csv')\ndf[['apple', 'pear']].to_csv('\/kaggle\/dataset\/my-directory\/some_other_table.csv', index=False)","6c65deb0":"!kaggle datasets create -p \"\/kaggle\/dataset\" --dir-mode zip","54f88542":"!kaggle datasets version -p \"\/kaggle\/dataset\" -m \"Updated via notebook\" --dir-mode zip","c4215177":"We are done! For more information, read the [docs on the Kaggle API](https:\/\/github.com\/Kaggle\/kaggle-api).","e52297e1":"## Creating the metadata file\n\nNow, let's create the folder that will hold our dataset and create the metadata file:","27049d02":"## Setting up environment variables\n\nAt this point, you will need to set up the environment variables in order to use the `kaggle` CLI. We will retrieve them from the user secrets that we just created.","1ce545bd":"## Uploading the dataset to Kaggle\n\nNow, it's time to upload the dataset we created inside `\/kaggle\/dataset`. If it's the first time you will need to run this:","0141773b":"## Preliminary steps\n\nBefore starting, you will first need to get an API token. To do that, navigate to \"Account\" -> \"API\":\n\n![](https:\/\/i.imgur.com\/ZKxDCvB.png)\n\nThen, click on \"Create New API Token\":\n\n![](https:\/\/i.imgur.com\/46OOLkv.png)\n\nIt will download a JSON file. Open the JSON file; you will have a *username* and a *key*. Now, click on \"Add-ons\" -> \"Secrets\":\n\n![](https:\/\/i.imgur.com\/sSIRm8X.png)\n\nClick on \"Add a new secret\" and set the value of `KAGGLE_KEY` to the key you just got, and `KAGGLE_USERNAME` to the username you got.\n\n![](https:\/\/i.imgur.com\/juigYPK.png)\n![](https:\/\/i.imgur.com\/g0UIB2l.png)","17afbf4f":"Above, we are manually creating the metadata file. Alternatively, you can call `!kaggle init -p \/kaggle\/dataset` but you will still need to edit the file using the `json` python library.\n\n## Dataset generation\n\nNow, it's time to create your very large dataset. We will generate some dummy files for demo purposes:","ac4a5837":"This notebook will walk you through how to create very large datasets (>20GB) directly from inside a notebook. This is a little bit more complex than generating the dataset from the output, but it's more flexible and not capped at 20GB; in fact, you can create multiple datasets as well.","6c2d667d":"If you have already created the dataset before and just want to push a new version, run this:"}}