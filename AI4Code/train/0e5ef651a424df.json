{"cell_type":{"92d10fe5":"code","d8fe06bf":"code","b39fac01":"code","072b39f9":"code","6619d889":"code","7f6ecf4d":"code","bd1c5b4e":"code","788b07fd":"code","f7ba2cdb":"code","2e265eab":"code","c87c071f":"code","747f9769":"code","c375f8f8":"code","5caffe7d":"code","208a5208":"code","ed20f561":"code","4fcfad3f":"code","c6aa02f2":"code","e4aff7f1":"code","a4e02499":"code","44f46828":"code","bd17e577":"code","6cb2b967":"code","c27fbe28":"code","9cf1fe5b":"code","ce867dc8":"code","0ddfcad7":"code","7b0d60ca":"code","bfba4035":"markdown"},"source":{"92d10fe5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8fe06bf":"from collections import Counter\nimport spacy\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier","b39fac01":"df = pd.read_csv('\/kaggle\/input\/newyork-room-rentalads\/room-rental-ads.csv')","072b39f9":"df.head()","6619d889":"df.shape","7f6ecf4d":"df.describe()","bd1c5b4e":"df.isnull().sum()","788b07fd":"df.dropna(how = 'any', inplace = True)","f7ba2cdb":"df['Vague\/Not'] = df['Vague\/Not'].astype('int64')","2e265eab":"df.rename(columns = {\"Vague\/Not\":\"Target\"},inplace = True)","c87c071f":"df.dtypes","747f9769":"df.columns","c375f8f8":"df.Target = df.Target.astype('category')","5caffe7d":"len(df[df.duplicated()])","208a5208":"df.drop_duplicates(inplace = True, subset = ['Description'])","ed20f561":"df.shape","4fcfad3f":"nlp = spacy.load('en')\ndef normalize (msg):\n    msg = re.sub('[^A-Za-z]+', ' ', msg) #remove special character and intergers\n    doc = nlp(msg)\n    res=[]\n    for token in doc:\n        if(token.is_stop or token.is_punct or token.is_currency or token.is_space or len(token.text) <= 2): #Remove Stopwords, Punctuations, Currency and Spaces\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    return res","c6aa02f2":"df['Description'] = df['Description'].apply(normalize)","e4aff7f1":"df['Description']","a4e02499":"words_collection = Counter([item for sublist in df['Description'] for item in sublist])\nfreqword = pd.DataFrame(words_collection.most_common(30))\nfreqword.columns = ['repeated_word','count']","44f46828":"fig, ax = plt.subplots(figsize=(30,25))\nsns.barplot(x = 'repeated_word', y = 'count', data = freqword, ax = ax)\nplt.show()","bd17e577":"df['Description'] = df['Description'].apply(lambda a:' '.join(a))","6cb2b967":"c = TfidfVectorizer(ngram_range = (1,2)) \nmat = pd.DataFrame(c.fit_transform(df[\"Description\"]).toarray(), columns = c.get_feature_names())\nmat","c27fbe28":"X = mat\ny = df[\"Target\"]","9cf1fe5b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)","ce867dc8":"classifier = DecisionTreeClassifier()\nclassifier.fit(X_train, y_train)","0ddfcad7":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","7b0d60ca":"'''\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'min_samples_split':[1, 2, 3, 4]}, {'criterion':['entropy'], 'splitter':['random'], 'min_samples_split':[1, 2, 3, 4]}]\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)\n'''","bfba4035":"# Grid Search CV"}}