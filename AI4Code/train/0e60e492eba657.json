{"cell_type":{"9bdc2a75":"code","6b7ba702":"code","a9d0f33c":"code","3a964a8c":"code","e656b463":"code","21b6bb8c":"code","1245bd4c":"code","a4162c9d":"code","8aa9c304":"code","37a0d457":"code","52dcc616":"code","295719e8":"code","aac4b9e2":"code","aa468c76":"code","6f5dbb99":"code","e14389b3":"code","25590e6a":"code","24cebdca":"markdown","27e4334c":"markdown","c3d6b2c7":"markdown","18b254c4":"markdown","8b52ad64":"markdown","9ba962de":"markdown","c57235fd":"markdown","349a57fe":"markdown","083c374e":"markdown","9df407e5":"markdown","7e519804":"markdown","6d36a75e":"markdown","0d36208c":"markdown","b6cf279a":"markdown","e819c10b":"markdown","c42de7bc":"markdown","c8383ac6":"markdown","501c0a83":"markdown","6d3e4f6a":"markdown","a17c7c06":"markdown"},"source":{"9bdc2a75":"# Kh\u1edfi t\u1ea1o m\u00f4i tr\u01b0\u1eddng\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport os\nimport time\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom unidecode import unidecode\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport string\n\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Input, Embedding, Dropout, Dense, CuDNNLSTM, BatchNormalization,SpatialDropout1D \nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n\nimport zipfile\nfrom gensim.models.keyedvectors import KeyedVectors","6b7ba702":"# Load d\u1eef li\u1ec7u t\u1eeb file csv\ntrain_df = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\")","a9d0f33c":"print(\"K\u00edch th\u01b0\u1edbc b\u1ea3ng file 'train':\", train_df.shape)\nprint(\"M\u1eabu d\u1eef li\u1ec7u file 'train':\")\ntrain_df.head()","3a964a8c":"print(\"K\u00edch th\u01b0\u1edbc b\u1ea3ng file 'test':\", test_df.shape)\nprint(\"M\u1eabu d\u1eef li\u1ec7u file 'test':\")\ntest_df.head()","e656b463":"# L\u1ea5y s\u1ed1 c\u00e2u h\u1ecfi, s\u1ed1 c\u00e2u sincere\/insincere\nsincere_qt = train_df[train_df.target == 0]\ninsincere_qt = train_df[train_df.target == 1]\n\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi: \", train_df.shape[0])\nprint(\"Sincere: \", sincere_qt.shape[0])\nprint(\"Insincere: \", insincere_qt.shape[0])\n\n# Bi\u1ec3u di\u1ec5n qua \u0111\u1ed3 th\u1ecb h\u00ecnh tr\u00f2n\nlabel = 'Sincere', 'Insincere'\nsize = [(sincere_qt.shape[0] \/ train_df.shape[0]) * 100, (insincere_qt.shape[0] \/ train_df.shape[0]) * 100]\nplt.pie(size, labels = label, colors=[\"c\", \"y\"], autopct=\"%.2f%%\")\nplt.show()\n","21b6bb8c":"# Ki\u1ec3m tra t\u1ec7p train\nprint('S\u1ed1 t\u1eeb trung b\u00ecnh m\u1ed7i c\u00e2u h\u1ecfi: {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 t\u1eeb t\u1ed1i \u0111a trong c\u00e2u h\u1ecfi: {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('S\u1ed1 k\u00fd t\u1ef1 trung b\u00ecnh m\u1ed7i c\u00e2u h\u1ecfi: {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\n\n# Ph\u00e2n b\u1ed1 \u0111\u1ed9 d\u00e0i `question_text` theo s\u1ed1 t\u1eeb\nplt.figure(figsize=(16,5))\nplt.hist(train_df['question_text'].apply(lambda x: len(x.split())), bins=60)\nplt.yscale('log')\nplt.xlabel('S\u1ed1 t\u1eeb\/c\u00e2u')\nplt.title('Ph\u00e2n b\u1ed1 \u0111\u1ed9 d\u00e0i `question_text` theo s\u1ed1 t\u1eeb')\nplt.xticks(range(0,140,5))\nplt.show()","1245bd4c":"from collections import defaultdict\n\n# Thi\u1ebft l\u1eadp stop-words\nstop_words = set(stopwords.words('english'))\n\n# T\u1ea1o dictionary \u0111\u1ec3 \u0111\u1ebfm t\u1eeb\nsin_freq_dict = defaultdict(int)\nins_freq_dict = defaultdict(int)\n\n# H\u00e0m t\u00e1ch c\u00e2u th\u00e0nh t\u1eeb, lo\u1ea1i b\u1ecf stop-words \ndef tokens(text):\n    tokens = [token for token in text.lower().split(\" \") if token != \"\" if token not in stop_words]\n    return tokens\n\n# \u0110\u1ebfm t\u1eeb\nfor qt in sincere_qt[\"question_text\"]:\n    for word in tokens(qt):\n        sin_freq_dict[word] += 1\nsfd_sorted = pd.DataFrame(sorted(sin_freq_dict.items(), key=lambda x: x[1], reverse=True)[::-1])\nsfd_sorted.columns = [\"word\", \"frequency\"]\n\nfor qt in insincere_qt[\"question_text\"]:\n    for word in tokens(qt):\n        ins_freq_dict[word] += 1\nifd_sorted = pd.DataFrame(sorted(ins_freq_dict.items(), key=lambda x: x[1], reverse=True)[::-1])\nifd_sorted.columns = [\"word\", \"frequency\"]\n\n# Bi\u1ec3u di\u1ec5n b\u1eb1ng \u0111\u1ed3 th\u1ecb\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n\nsfd_sorted.tail(20).plot(x= 'word', kind = \"barh\", title=\"Top 20 t\u1eeb trong c\u00e2u h\u1ecfi label 'sincere'\", ax=ax1)\nifd_sorted.tail(20).plot(x= 'word', kind = \"barh\", title=\"Top 20 t\u1eeb trong c\u00e2u h\u1ecfi label 'insincere'\", ax=ax2)","a4162c9d":"# T\u00e1ch file `train` th\u00e0nh 2 ph\u1ea7n `train` v\u00e0 `validate` \u0111\u1ec3 hu\u1ea5n luy\u1ec7n\ntrain_df, val_df = train_test_split(train_df, test_size=0.25, random_state=40)\n\n# T\u00e1ch \u0111\u1ea7u v\u00e0o 'question_text' t\u1eeb file 'train'\ntrain_X = train_df[\"question_text\"]\nval_X = val_df[\"question_text\"]\ntest_X = test_df[\"question_text\"]\n\n# T\u00e1ch \u0111\u1ea7u ra 'target' t\u1eeb file 'train'\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\n\n# Load model r\u00fat g\u1ecdn t\u1eeb lo\u1ea1i\nlemmatizer = WordNetLemmatizer();\n\n# H\u00e0m x\u1eed l\u00fd c\u00e1c chu\u1ed7i 'question text' th\u00e0nh c\u00e1c m\u1ea3ng ch\u1ee9a t\u1eeb \ndef clean_qt(question_text):\n    q_text = str(unidecode(question_text.lower()).encode(\"ascii\"),\"ascii\")           # Chuy\u1ec3n v\u1ec1 unicode v\u00e0 ch\u1eef th\u01b0\u1eddng\n    q_text = q_text.translate(str.maketrans('', '', string.punctuation + string.digits)) # Lo\u1ea1i b\u1ecf k\u00fd t\u1ef1 v\u00e0 ch\u1eef s\u1ed1\n    q_tokens = text_to_word_sequence(q_text)                                         # T\u00e1ch t\u1eebng c\u00e2u th\u00e0nh m\u1ea3ng ch\u1ee9a t\u1eeb\n    q_tokens_simplify = [lemmatizer.lemmatize(token) for token in q_tokens]          # R\u00fat g\u1ecdn t\u1eeb lo\u1ea1i\n    q_tokens_min = [token for token in q_tokens_simplify if token not in stop_words] # Lo\u1ea1i b\u1ecf stop-word\n    return q_tokens_min\n    \ntrain_X = train_X.apply(clean_qt)\nval_X = val_X.apply(clean_qt)\ntest_X = test_X.apply(clean_qt)","8aa9c304":"# Test k\u1ebft qu\u1ea3\nprint(train_df[\"question_text\"].head(5), \"\\n\", train_X[0:5])","37a0d457":"# Thi\u1ebft l\u1eadp c\u00e1c h\u1eafng s\u1ed1\nEMBED_SIZE = 300 # K\u00edch th\u01b0\u1edbc hay s\u1ed1 chi\u1ec1u c\u1ee7a vector t\u1eeb\nMAX_FEATURES  = 100000 # S\u1ed1 l\u01b0\u1ee3ng t\u1eeb \u0111\u1eb7c tr\u01b0ng t\u1ed1i \u0111a, hay s\u1ed1 l\u01b0\u1ee3ng vector t\u00f4i \u0111a \nMAX_LEN = 70 # S\u1ed1 l\u01b0\u1ee3ng t\u1eeb t\u1ed1i \u0111a trong m\u1ed9t c\u00e2u\n\n# L\u1eadp t\u1eeb \u0111i\u1ec3n m\u00e3 h\u00f3a t\u1eeb th\u00e0nh c\u00e1c ID\ntokenizer = Tokenizer(filters='', num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(train_X)\n\n# Chuy\u1ec3n c\u00e1c chu\u1ed7i t\u1eeb th\u00e0nh chu\u1ed7i ID\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n# C\u00e2n b\u1eb1ng \u0111\u1ecd d\u00e0i chu\u1ed7i b\u1eb1ng v\u1edbi MAX_LEN  \ntrain_X = pad_sequences(train_X, maxlen=MAX_LEN)\nval_X = pad_sequences(val_X, maxlen=MAX_LEN)\ntest_X = pad_sequences(test_X, maxlen=MAX_LEN)","52dcc616":"# Load pre-trained weights t\u1eeb t\u1ec7p n\u00e9n ch\u1ee9a c\u00e1c t\u1eeb v\u00e0 vector t\u01b0\u01a1ng \u1ee9ng  \narchive = zipfile.ZipFile('..\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r')\n\n# H\u00e0m chuy\u1ec3n weights pretrained cho vector t\u1eeb th\u00e0nh weights cho model \ndef loadEmbeddingMatrix(typeToLoad): # typetoLoad = 'glove' | 'word2vec' \n    # GloVe\n    embeddings_index = dict()\n    if (typeToLoad == \"glove\"):\n        # Load file\n        EMB_FILE =  archive.open('glove.840B.300d\/glove.840B.300d.txt')\n        # Chuy\u1ec3n weights th\u00e0nh t\u1eeb \u0111i\u1ec3n qua vi\u1ec7c \u0111\u1ecdc t\u1eebng d\u00f2ng trong file\n        for embd in EMB_FILE:\n            # Ng\u1eaft d\u00f2ng th\u00e0nh m\u1ea3ng\n            word2vec = embd.decode().split(' ')\n            # V\u1ecb tr\u00ed \u0111\u1ea7u ti\u00ean trong m\u1ea3ng l\u00e0 t\u1eeb\n            word = word2vec[0]\n            # T\u1ea1o m\u1ea3ng ch\u1ee9a c\u00e1c v\u1ecb tr\u00ed c\u00f2n l\u1ea1i th\u00e0nh vector t\u01b0\u01a1ng \u1ee9ng v\u1edbi t\u1eeb\n            embeddings_index[word] = np.asarray(word2vec[1:EMBED_SIZE+1], dtype='float32')\n        EMB_FILE.close()\n        gc.collect()\n        print('GloVe - Loaded %s vector t\u1eeb.' % len(embeddings_index))\n        \n    # Google Word2vec \n    elif (typeToLoad == \"word2vec\"):\n        # Load file \n        EMB_FILE =  archive.open('GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin')\n        # Chuy\u1ec3n th\u00e0nh t\u1eeb \u0111i\u1ec3n b\u1eb1ng module \u0111\u01b0\u1ee3c implement s\u1eb5n  \n        emb_google_word2vec = KeyedVectors.load_word2vec_format(EMB_FILE, binary=True, limit=1000000)\n        for word in emb_google_word2vec.index_to_key:\n                embeddings_index[word] = emb_google_word2vec.get_vector(word)\n        EMB_FILE.close()\n        gc.collect()\n        print('Word2Vec - Loaded %s vector t\u1eeb.' % len(embeddings_index))\n    \n    # T\u1ea1o ma tr\u1eadn k\u00edch th\u01b0\u1edbc (s\u1ed1 t\u1eeb x k\u00edch th\u01b0\u1edbc vector) hay (MAX_FEATURES x EMBED_SIZE)\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    nb_words = min(MAX_FEATURES, len(tokenizer.word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n    \n    # Nh\u1eadp t\u1eebng vector t\u01b0\u01a1ng \u1ee9ng v\u1edbi word ID\n    embeddedCount = 0\n    for word, i in tokenizer.word_index.items():\n        if i >= MAX_FEATURES: continue\n        # Load vector \n        embedding_vector = embeddings_index.get(word)\n        # L\u01b0u v\u00e0o ma tr\u1eadn\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            embeddedCount+=1\n    print('Nh\u1eadp v\u00e0o %d vector' % embeddedCount)\n    \n    del(embeddings_index)\n    gc.collect()\n    \n    return embedding_matrix\n\nemb_glove_matrix = loadEmbeddingMatrix(\"glove\")\nemb_google_matrix = loadEmbeddingMatrix(\"word2vec\")\n# G\u1ed9p 2 ma tr\u1eadn \u0111\u1ec3 t\u0103ng hi\u1ec7u qu\u1ea3\nembd_total_matrix = np.mean([emb_glove_matrix, emb_google_matrix], axis = 0)","295719e8":"model = Sequential()\n\nmodel.add(Input(shape=(MAX_LEN,)))\n# L\u1edbp Embedding\nmodel.add(Embedding(MAX_FEATURES, EMBED_SIZE, weights=[embd_total_matrix]))\nmodel.add(SpatialDropout1D(0.5)) \n\n# L\u1edbp Bi-directional LTSM\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\n\n# M\u1ea1ng MLP\nmodel.add(Dense(16, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n# Model ho\u00e0n ch\u1ec9nh\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = 'adam', \n              metrics = ['accuracy'])\nprint(model.summary())","aac4b9e2":"# Th\u1ef1c hi\u1ec7n train\n# T\u1ea1o chekpoint\nfile_path = \"model_b128_val0.2_e10.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = False, mode = \"min\")\n\n# Hu\u1ea5n luy\u1ec7n v\u00e0 ghi l\u1ea1i th\u1ed1ng k\u00ea\nhistory = model.fit(train_X, train_y, batch_size=128, epochs=10, validation_data=(val_X, val_y)) #, callbacks=[check_point])","aa468c76":"# \u0110\u1ed3 th\u00ec model loss\nepochRange = np.arange(1,11,1)\nplt.plot(epochRange, history.history['loss'])\nplt.plot(epochRange, history.history['val_loss'])\nplt.xticks(np.arange(1,11,1))\nplt.yticks(np.arange(0,0.5,0.05))\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Training', 'Validation'], loc='upper left')\nplt.show()","6f5dbb99":"pred_train_y = model.predict([train_X], batch_size=128, verbose=1)\nthreshs = np.arange(0.1, 0.5, 0.01, dtype=float)\nf1 = np.zeros_like(threshs, dtype=float)\ni = 0\nfor thresh in threshs:\n    f1[i]=(metrics.f1_score(train_y, (pred_train_y>thresh).astype(int)))\n    i+=1\nplt.plot(threshs, f1)","e14389b3":"f1_max = np.amax(f1)\nindex = np.where(f1==f1_max)\nthresh_f1_max = threshs[index]\n\nprint('F1-score t\u1ed1i \u0111a l\u00e0 %.2f t\u1ea1i ng\u01b0\u1ee1ng %.2f' % (f1_max, thresh_f1_max))","25590e6a":"pred_test_y = model.predict([test_X], batch_size=256, verbose=1)\npred_test_y = np.where(pred_test_y>thresh_f1_max,1,0)                                \nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","24cebdca":"## 2.2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n\nC\u00f3 3 tr\u01b0\u1eddng c\u1ea7n ph\u1ea3i x\u1eed l\u00fd:  \n* **Tr\u01b0\u1eddng `qid`**: l\u00e0 ID \u0111\u1eb7c tr\u01b0ng cho t\u1eebng c\u00e2u h\u1ecfi, kh\u00f4ng t\u1ed3n t\u1ea1i 2 c\u00e2u h\u1ecfi n\u00e0o tr\u00f9ng ID.  \n* **Tr\u01b0\u1eddng `question_text`**: l\u00e0 c\u00e1c c\u00e2u h\u1ecfi b\u1eb1ng ti\u1ebfng Anh, c\u1ea7n \u0111\u01b0\u1ee3c x\u1eed l\u00fd tr\u01b0\u1edbc khi \u0111\u01b0a v\u00e0o model.  \n* **Tr\u01b0\u1eddng `target`**: l\u00e0 label k\u1ebft qu\u1ea3 \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1, 0\/1 t\u01b0\u01a1ng \u1ee9ng v\u1edbi *sincere\/insincere*.\n\n#### **Ki\u1ec3m tra ph\u00e2n b\u1ed1 t\u1eadp d\u1eef li\u1ec7u theo `target`:** ","27e4334c":"#### Model loss qua m\u1ed7i epoch:","c3d6b2c7":"#### \u0110\u00e1nh gi\u00e1 F1_score\n\nB\u00ecnh th\u01b0\u1eddng \u1edf c\u00e1c m\u00f4 h\u00ecnh ph\u00e2n l\u1edbp nh\u1ecb ph\u00e2n k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n lu\u00f4n thu\u1ed9c \u0111o\u1ea1n (0,1) v\u1edbi ng\u01b0\u1ee1ng (threshold) l\u00e0 0.5, t\u1ee9c l\u00e0 n\u1ebfu k\u1ebft qu\u1ea3 d\u01b0\u1edbi ng\u01b0\u1ee1ng 0.5 th\u00ec ph\u00e2n l\u1edbp l\u00e0 0, ng\u01b0\u1ee3c l\u1ea1i th\u00ec l\u00e0 1.\n\nNh\u01b0ng trong m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p, gi\u00e1 tr\u1ecb ng\u01b0\u1ee1ng 0.5 n\u00e0y c\u00f3 th\u1ec3 kh\u00f4ng ph\u1ea3i l\u00e0 t\u1ed1t nh\u1ea5t. Trong b\u00e0i to\u00e1n n\u00e0y c\u1ea7n ch\u1ecdn threshold sao cho F1_score l\u00e0 cao nh\u1ea5t, l\u00e0m t\u0103ng t\u1ec9 l\u1ec7 d\u1ef1 \u0111o\u00e1n ","18b254c4":"# **2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u** ","8b52ad64":"## 3.2. Hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1\nHu\u1ea5n luy\u1ec7n v\u1edbi h\u00e0m loss l\u00e0 **binary cross entropy**, h\u00e0m t\u1ed1i \u01b0u l\u00e0 **Adam**.","9ba962de":"#### **Ki\u1ec3m tra c\u00e1c t\u1eeb trong `question_text`**: ","c57235fd":"> # ***Quora Insincere Questions Classification***\n\n#### B\u00c1O C\u00c1O \u0110\u1ec0 T\u00c0I CU\u1ed0I K\u1ef2  \n   **H\u1ecd v\u00e0 t\u00ean**: L\u00ea Ti\u1ebfn Ph\u00e1t  \n   **MSSV**: 18020993  \n   **L\u1edbp h\u1ecdc ph\u1ea7n**: INT_3405_1  \n   **Gi\u1ea3ng vi\u00ean**: Tr\u1ea7n Qu\u1ed1c Long  ","349a57fe":"   * **Vector h\u00f3a t\u1eeb**: l\u00e0 vi\u1ec7c thay th\u1ec3 t\u1eeb b\u1eb1ng m\u1ed9t vector s\u1ed1 th\u1ef1c nhi\u1ec1u chi\u1ec1u, n\u00f3i c\u00e1ch kh\u00e1c l\u00e0 m\u00e3 h\u00f3a bi\u1ec3u di\u1ec5n t\u1eeb l\u00ean kh\u00f4ng gian \u0111a chi\u1ec1u, v\u1edbi m\u1ee5c \u0111\u00edch gi\u00fap m\u00e1y t\u00ednh hi\u1ec3u \u0111\u01b0\u1ee3c \u00fd ngh\u0129a c\u1ee7a t\u1eeb theo c\u00e1ch c\u00e1c t\u1eeb \u0111\u1ed3ng ngh\u0129a, g\u1ea7n ngh\u0129a s\u1ebd c\u00f3 bi\u1ec3u di\u1ec5n vector g\u1ea7n gi\u1ed1ng nhau. \u0110\u00f4ng th\u1eddi vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c model pretrained gi\u00fap t\u0103ng \u0111\u1ed9 chu\u1ea9n x\u00e1c c\u0169ng nh\u01b0 gi\u1ea3m th\u1eddi gian x\u1eed l\u00fd.  \n  \n\u1ede \u0111\u00e2y ta d\u00f9ng pretrained weights cung c\u1ea5p b\u1edfi Google ([Word2Vec](https:\/\/code.google.com\/archive\/p\/word2vec)) v\u00e0 Stanford NLP ([GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/)), k\u1ebft h\u1ee3p t\u1eeb \u0111i\u1ec3n word - ID t\u1ea1o \u1edf ph\u00eda tr\u00ean c\u00f3 th\u1ec3 t\u1ea1o ra ma tr\u1eadn li\u00ean h\u1ec7 gi\u1eefa ID - vector, \u0111\u01b0a v\u00e0o model","083c374e":"M\u1ed9t c\u00e2u h\u1ecfi cho l\u00e0 *insincere* khi c\u00f3 c\u00e1c y\u1ebfu t\u1ed1:  \n* T\u00f4ng gi\u1ecdng kh\u00e1c bi\u1ec7t: \n * Ph\u00f3ng \u0111\u1ea1i, khoa tr\u01b0\u01a1ng v\u1ec1 m\u1ed9t g\u00f3c nh\u00ecn hay ph\u00e1t bi\u1ec3u h\u01b0\u1edbng t\u1edbi m\u1ed9t nh\u00f3m ng\u01b0\u1eddi nh\u1ea5t \u0111\u1ecbnh  \n* C\u00f3 t\u00ednh mi\u1ec7t th\u1ecb hay k\u00edch \u0111\u1ed9ng:  \n * Ph\u00e2n bi\u1ec7t \u0111\u1ed1i x\u1eed hay mi\u1ec7t th\u1ecb v\u1edbi m\u1ed9t nh\u00f3m ng\u01b0\u1eddi m\u1ed9t c\u00e1 nh\u00e2n ho\u1eb7c m\u1ed9t \u0111\u00e1m ng\u01b0\u1eddi  \n * Mi\u1ec7t th\u1ecb m\u1ed9t t\u00ednh c\u00e1ch hay t\u00ednh ch\u1ea5t kh\u00e1c th\u01b0\u1eddng  \n* Kh\u00f4ng \u0111\u00fang s\u1ef1 th\u1eadt: th\u00f4ng tin sai \/ gi\u1ea3 \u0111\u1ecbnh v\u00f4 l\u00fd  \n* Ch\u1ee9a quan h\u1ec7 t\u00ecnh d\u1ee5c \u0111\u1ec3 g\u00e2y s\u1ed1c  \n\n## 2.1. C\u00e1c t\u1eadp d\u1eef li\u1ec7u","9df407e5":"# **1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**\n\n[Quora](https:\/\/www.quora.com) l\u00e0 m\u1ed9t di\u1ec5n \u0111\u00e0n m\u1edf v\u1edbi m\u1ee5c \u0111\u00edch trao \u0111\u1ed5i ki\u1ebfn th\u1ee9c gi\u1eefa ng\u01b0\u1eddi v\u1edbi ng\u01b0\u1eddi. Tr\u00ean di\u1ec5n \u0111\u00e0n m\u1ecdi ng\u01b0\u1eddi s\u1ebd \u0111\u01b0a ra nh\u1eefng th\u1eafc m\u1eafc v\u00e0 nh\u1eadn l\u1ea1i nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi mang t\u00ednh chuy\u00ean s\u00e2u v\u00e0 ch\u1ea5t l\u01b0\u1ee3ng. V\u1ea5n \u0111\u1ec1 c\u1ea7n gi\u1ea3i quy\u1ebft l\u00fac n\u00e0y l\u00e0 t\u00ecm v\u00e0 lo\u1ea1i b\u1ecf nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu th\u00e0nh th\u1eadt hay thi\u1ebfu t\u1ebf nh\u1ecb (insincere) mang t\u00ednh ph\u00e1t bi\u1ec3u th\u1ec3 hi\u1ec7n h\u01a1n l\u00e0 c\u1ea7n t\u00ecm ki\u1ebfm c\u00e2u tr\u1ea3 l\u1eddi. \n\n* **Input:** C\u00e2u h\u1ecfi b\u1eb1ng ti\u1ebfng Anh\n* **Output:** X\u00e1c nh\u1eadn TRUE\/FALSE r\u1eb1ng c\u00e2u h\u1ecfi c\u00f3 insincere hay kh\u00f4ng, b\u1eb1ng c\u00e1ch \u0111\u00e1nh label 0 cho *sincere* v\u00e0 1 cho *insincere* ","7e519804":"Ph\u00e2n b\u1ed1 trong t\u1eadp `train` cho th\u1ea5y t\u1ec9 l\u1ec7 th\u1ea5p c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh nh\u00e3n *insincere* hay t\u1eadp d\u1eef li\u1ec7u r\u1ea5t m\u1ea5t c\u00e2n b\u1eb1ng, d\u1eabn \u0111\u1ebfn nh\u1eefng v\u1ea5n \u0111\u1ec1 nh\u01b0 m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng k\u1ebfm hi\u1ec7u qu\u1ea3 v\u1edbi nh\u1eefng tr\u01b0\u1eddng h\u1ee3p `target` = 1, hay metric accuracy kh\u00f4ng th\u1ec3 hi\u1ec7n r\u00f5 ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh.\n\n=> C\u1ea7n s\u1eed d\u1ee5ng metric thay th\u1ebf nh\u01b0 *F1_score* ","6d36a75e":"Ngo\u00e0i ra c\u00f2n ch\u1ee9a t\u1ec7p n\u00e9n `embeddings.zip` ch\u1ee9a danh s\u00e1ch c\u00e1c t\u1eeb \u0111\u00e3 vector h\u00f3a c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c d\u00f9ng trong model  ","0d36208c":"## 2.4. Bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n  \n\n\u0110\u1ec3 vi\u1ec7c hu\u1ea5n luy\u1ec7n \u0111\u1ea1t hi\u1ec7u qu\u1ea3, ta c\u1ea7n ph\u1ea3i \u0111\u01b0a d\u1eef li\u1ec7u t\u1eeb c\u00e1c t\u1eeb kh\u00f3a th\u00e0nh d\u1ea1ng m\u00e0 m\u00e1y t\u00ednh c\u00f3 th\u1ec3 hi\u1ec3u.  \nC\u00e1c qu\u00e1 tr\u00ecnh bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u b\u00e1o g\u1ed3m:\n  * **M\u00e3 h\u00f3a t\u1eeb**: g\u1ed3m vi\u1ec7c thay th\u1ebf t\u1eeb b\u1eb1ng m\u1ed9t ID l\u00e0 s\u1ed1 t\u1ef1 nhi\u00ean v\u00e0 ghi l\u1ea1i t\u1eeb \u0111i\u1ec3n \u0111\u1ed1i chi\u1ebfu gi\u1eefa t\u1eeb v\u00e0 ID, m\u1ea3ng t\u1eeb s\u1ebd thay b\u1eb1ng chu\u1ed7i s\u1ed1. Vi\u1ec7c n\u00e0y c\u00f3 t\u00e1c d\u1ee5ng th\u1ed1ng k\u00ea s\u1ed1 t\u1eeb \u0111\u1eb7c tr\u01b0ng (c\u00f3 bao nhi\u00eau t\u1eeb, t\u1eeb n\u00e0o xu\u1ea5t hi\u1ec7n nhi\u1ec1u, ...) v\u00e0 gi\u1ea3m b\u1ed9 nh\u1edb khi c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u th\u00ec s\u1ebd bi\u1ec3u di\u1ec5n b\u1edfi s\u1ed1 nh\u1ecf\n  * **C\u00e2n b\u1eb1ng chu\u1ed7i**: l\u00e0 vi\u1ec7c k\u00e9o d\u00e0i chu\u1ed7i l\u00ean \u0111\u1ed9 d\u00e0i MAX_LEN l\u00e0 70 v\u1edbi nh\u1eefng chu\u1ed7i ng\u1eafn h\u01a1n v\u00e0 b\u1ed5 sung v\u00e0o chu\u1ed7i l\u00e0 nh\u1eefng ID 0, ho\u1eb7c c\u1eaft ng\u1eafn chu\u1ed7i d\u00e0i h\u01a1n v\u1ec1 70 (Do nh\u01b0 th\u1ed1ng k\u00ea ph\u00eda tr\u00ean, \u0111a ph\u1ea7n \u0111\u1ed9 d\u00e0i c\u00e2u khi ch\u01b0a t\u1ed1i \u01b0u th\u01b0\u1eddng d\u00e0i d\u01b0\u1edbi 70 t\u1eeb). C\u00e2n b\u1eb1ng chu\u1ed7i gi\u00fap hu\u1ea5n luy\u1ec7n t\u1ed1t h\u01a1n do \u0111\u1ed9 d\u00e0i input l\u00e0 nh\u01b0 nhau\n","b6cf279a":"* C\u00e1c c\u00e2u h\u1ecfi trong t\u1ec7p `train` c\u00f3 \u0111\u1ed9 d\u00e0i trung b\u00ecnh kh\u00f4ng qu\u00e1 l\u1edbn, ph\u1ea7n l\u1edbn d\u01b0\u1edbi 70 t\u1eeb, nh\u01b0ng c\u0169ng c\u00f3 m\u1ed9t s\u1ed1 c\u00e2u kh\u00e1 d\u00e0i. \n\n* T\u1ea7n xu\u1ea5t c\u00e1c t\u1eeb trong t\u1ec7p `train` t\u01b0\u01a1ng \u1ee9ng v\u1edbi `target`:  \n * C\u1ea7n lo\u1ea1i b\u1ecf \u0111i c\u00e1c t\u1eeb l\u1eb7p l\u1ea1i nhi\u1ec1u nh\u01b0ng \u00edt t\u00e1c \u0111\u1ed9ng \u0111\u1ebfn \u0111\u00e1nh gi\u00e1 *sincere\/insincere* (g\u1ecdi l\u00e0 stop-word, \u0111\u01b0\u1ee3c th\u00eam v\u00e0o b\u1eb1ng b\u1ed9 c\u00f4ng c\u1ee5 ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean NLTK), g\u00e2y c\u1ea3n tr\u1edf v\u00e0 l\u00e0m gi\u1ea3m t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd. ","e819c10b":"## 4. \u00c1p d\u1ee5ng ","c42de7bc":"## 2.3. T\u1ed1i \u01b0u d\u1eef li\u1ec7u\n\n**C\u00e1c b\u01b0\u1edbc ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u `question_text` bao g\u1ed3m:**\n* Chuy\u1ec3n v\u1ec1 unicode v\u00e0 ch\u1eef th\u01b0\u1eddng \u0111\u1ec3 lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n* Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 (punctuation), ch\u1eef s\u1ed1 do kh\u00f4ng nhi\u1ec1u t\u00e1c d\u1ee5ng trong vi\u1ec7c hu\u1ea5n luy\u1ec7n model\n* T\u00e1ch r\u1eddi t\u1eebng c\u00e2u th\u00e0nh c\u00e1c m\u1ea3ng ch\u1ee9a t\u1eeb (tokens)\n* R\u00fat g\u1ecdn t\u1eeb lo\u1ea1i (\u0111\u01b0a c\u00e1c t\u1eeb c\u00f9ng d\u1ea1ng v\u1ec1 th\u00e0nh 1 t\u1eeb, nh\u01b0 c\u00e1c \u0111\u1ed9ng t\u1eeb t\u1eeb c\u00e1c d\u1ea1ng v\u1ec1 nguy\u00ean th\u1ec3, hay c\u00e1c t\u1eeb c\u00f9ng ng\u1eef ph\u00e1p s\u1ebd \u0111\u01b0a v\u1ec1 t\u1eeb lo\u1ea1i g\u1ed1c - [Stemming \/ Lemmatization](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html))\n* Lo\u1ea1i b\u1ecf stop-words (c\u00e1c t\u1eeb l\u1eb7p l\u1ea1i nhi\u1ec1u nh\u01b0ng \u00edt t\u00e1c \u0111\u1ed9ng \u0111\u1ebfn \u0111\u00e1nh gi\u00e1, g\u00e2y c\u1ea3n tr\u1edf v\u00e0 l\u00e0m gi\u1ea3m t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd)  \n##### => Thu g\u1ecdn d\u1eef li\u1ec7u `question_text` tr\u1edf th\u00e0nh c\u00e1c chu\u1ed7i g\u1ed3m c\u00e1c t\u1eeb kh\u00f3a mang g\u1ea7n nh\u01b0 to\u00e0n b\u1ed9 \u00fd ngh\u0129a c\u1ee7a c\u00e2u   ","c8383ac6":"# **3. Hu\u1ea5n luy\u1ec7n**\n\n## 3.1. Ki\u1ebfn tr\u00fac c\u1ee7a m\u00f4 h\u00ecnh\n\n  M\u00f4 h\u00ecnh m\u1ea1ng **RNN** (Recurrent Neural Network) \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u \u1edf c\u00e1c b\u00e0i to\u00e1n x\u1eed l\u00fd ng\u00f4n ng\u1eef, do vi\u1ec7c x\u1eed l\u00fd th\u00f4ng tin d\u01b0\u1edbi d\u1ea1ng chu\u1ed7i hi\u1ec7u qu\u1ea3 v\u00ec ngo\u00e0i \u0111\u1ea7u v\u00e0o th\u00f4ng th\u01b0\u1eddng c\u1ee7a m\u1ed9t m\u1ea1ng neural, tr\u1ea1ng th\u00e1i \u1ea9n b\u01b0\u1edbc tr\u01b0\u1edbc c\u0169ng \u0111\u01b0\u1ee3c th\u00eam v\u00e0o th\u00e0nh \u0111\u1ea7u v\u00e0o b\u01b0\u1edbc sau. Trong b\u00e0i n\u00e0y s\u1ebd s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean RNN l\u00e0 m\u1ea1ng **LTSM 2 chi\u1ec1u** (Bi-directional LSTM).  \n![image.png](attachment:20ea8955-fff7-47dc-9dfb-035038729aca.png)\n  M\u1ea1ng LTSM sinh ra \u0111\u1ec3 kh\u1eafc ph\u1ee5c vi\u1ec7c m\u1ea1ng RNN kh\u00f4ng h\u1ecdc \u0111\u01b0\u1ee3c t\u1eeb chu\u1ed7i qu\u00e1 d\u00e0i do tri\u1ec7t ti\u00eau \u0111\u1ea1o h\u00e0m b\u1eb1ng c\u00e1ch th\u00eam m\u1ed9t bi\u1ebfn tr\u1ea1ng th\u00e1i \u00f4 c\u00f9ng v\u1edbi tr\u1ea1ng th\u00e1i \u1ea9n l\u00e0m \u0111\u1ea7u v\u00e0o b\u01b0\u1edbc sau, nh\u1eefng th\u00f4ng tin c\u1ea7n l\u01b0u tr\u1eef s\u1ebd \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt qua tr\u1ea1ng th\u00e1i \u00f4. Khi d\u00f9ng m\u1ea1ng LTSM 2 chi\u1ec1u, th\u00f4ng tin l\u01b0u tr\u1eef \u0111\u01b0\u1ee3c kh\u00f4ng ch\u1ec9 \u1edf c\u00e1c b\u01b0\u1edbc qu\u00e1 kh\u1ee9 m\u00e0 c\u00f2n c\u00f3 c\u1ea3 c\u00e1c b\u01b0\u1edbc t\u01b0\u01a1ng lai, l\u00e0m t\u0103ng t\u00ednh hi\u1ec7u qu\u1ea3 v\u00ec d\u1ef1 \u0111o\u00e1n c\u1ea7n c\u1ea3 ng\u1eef c\u1ea3nh ph\u00eda tr\u01b0\u1edbc v\u00e0 sau.  \n  \n  C\u1ea5u tr\u00fac m\u1ea1ng bao g\u1ed3m:\n* L\u1edbp Embedding l\u00e0 b\u1ed9 pre-trained embedding \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n ph\u00eda tr\u00ean. \u0110\u1ea7u ra c\u00f3 s\u1eed d\u1ee5ng Dropout \u0111\u1ec3 tr\u00e1nh overfit.  \n* L\u1edbp LSTM v\u1edbi 2 t\u1ea7ng v\u00e0 m\u1ed7i t\u1ea7ng 128 units.   \n* M\u1ea1ng MLP v\u1edbi h\u00e0m k\u00edch ho\u1ea1t l\u00e0 ReLU, c\u00f3 s\u1eed d\u1ee5ng Batchnorm v\u00e0 Dropout, cu\u1ed1i c\u00f9ng l\u00e0 1 l\u1edbp h\u00e0m sigmoid.  ","501c0a83":"* C\u00e1c t\u1eeb trong c\u00e1c c\u00e2u c\u00f3 nh\u00e3n `sincere` th\u01b0\u1eddng l\u00e0 c\u00e1c t\u1eeb chung chung v\u00e0 th\u00f4ng d\u1ee5ng, trong khi \u0111\u00f3 c\u00e1c t\u1eeb trong c\u00e2u c\u00f3 nh\u00e3n `insincere` th\u01b0\u1eddng c\u00f3 xu h\u01b0\u1edbng nh\u1eafm v\u00e0o c\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng, l\u0129nh v\u1ef1c ri\u00eang nh\u01b0 ch\u1ee7ng t\u1ed9c, t\u00f4n gi\u00e1o hay gi\u1edbi t\u00ednh","6d3e4f6a":"* T\u1ec7p `test`: G\u1ed3m 375806 d\u00f2ng v\u00e0 2 c\u1ed9t (`qid`, `question_text`)","a17c7c06":"D\u1ee9 li\u1ec7u \u0111\u1ea7u v\u00e0o g\u1ed3m 2 t\u1ec7p `train.csv` v\u00e0 `test.csv`.  \n\n* T\u1ec7p `train`: G\u1ed3m 1306122 d\u00f2ng v\u00e0 3 c\u1ed9t (`qid`, `question_text`, `target`)"}}