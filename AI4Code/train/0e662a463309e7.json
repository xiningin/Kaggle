{"cell_type":{"334f2f0c":"code","6a908473":"code","ff9a9eb5":"code","35a9e0da":"code","46a5798e":"code","728f60f6":"code","81ee387b":"code","0683983d":"code","1f9f9749":"code","857431af":"code","d51eeb3c":"code","f830d5e1":"markdown","6969c277":"markdown","d806db8b":"markdown","1a2e08db":"markdown","56474a48":"markdown","01044c21":"markdown"},"source":{"334f2f0c":"import os \nimport pandas as pd\nimport numpy as np\nimport nltk\n\nfile_path = \"..\/input\/movies-similarity\/movies.csv\"\n\ndata = pd.read_csv(file_path)\ndata","6a908473":"data['plot'] = data['wiki_plot'].astype(str) + \"\\n\" + data['imdb_plot'].astype(str)\ndata","ff9a9eb5":"nltk.download('punkt')","35a9e0da":"# Tokenize a paragraph into sentences and store in sent_tokenized\nsent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\n                        Today (May 19, 2016) is his only daughter's wedding. \n                        Vito Corleone is the Godfather.\n                        \"\"\")]\n\n# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\nwords_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n\n# Remove tokens that do not contain any letters from words_tokenized\nimport re\n\nfiltered = [word for word in words_tokenized if re.search(r'[a-zA-Z]', word)]\n\n# Display filtered words to observe words after tokenization\nfiltered","46a5798e":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\nprint(\"Without Stemming:\",filtered)\n\nstemmed_words = [stemmer.stem(word) for word in filtered]\n\nprint(\"After Stemming: \",stemmed_words)","728f60f6":"import re\ndef tokenize_and_stem(text):\n\n  tokens = [y for x in nltk.sent_tokenize(text) for y in nltk.word_tokenize(x)]\n\n  filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]',token)]\n\n  stems = [stemmer.stem(token) for token in filtered_tokens]\n\n  return stems\n\nwords_stemmed = tokenize_and_stem(data['plot'][1])\nprint(words_stemmed)","81ee387b":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(max_df=0.8,max_features=200000,min_df=0.2,stop_words='english',use_idf=True,tokenizer=tokenize_and_stem,ngram_range=(1,3))","0683983d":"tfidf_matrix = tfidf.fit_transform( data['plot'])\n\nprint(tfidf_matrix.shape)","1f9f9749":"from sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=5)\nkm.fit(tfidf_matrix)\n\ndata['clusters'] = km.labels_.tolist()\n\ndata['clusters'].value_counts()","857431af":"from sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_scores = cosine_similarity(tfidf_matrix)","d51eeb3c":"import matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmergings = linkage(similarity_scores,method='complete')\n\ndendrogram = dendrogram(mergings,labels=[x for x in data['title']],leaf_rotation=90,leaf_font_size=16)\n\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\n# Show the plotted dendrogram\nplt.show()","f830d5e1":"KMeans CLustering","6969c277":"Stemming","d806db8b":"Tokenization","1a2e08db":"Linkage Dendrogram","56474a48":"Tfidf Vectorizer","01044c21":"Cosine Similarity"}}