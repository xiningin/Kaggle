{"cell_type":{"fb7c5021":"code","38432257":"code","13a6137a":"code","97e83c06":"code","53cc03da":"code","5d792b61":"code","e7f689e8":"code","d8229131":"code","8fa4d1d2":"code","f900ae0f":"code","75619906":"code","a82ded32":"code","108fe5d1":"code","47acd3c2":"code","5949ddd2":"code","5b280372":"code","a37557bd":"code","5629718b":"code","8a682421":"code","dccc7313":"code","1d04edf3":"code","653a307c":"code","dfebe253":"code","f46317d9":"code","e0465704":"code","a85bca6a":"code","022e17ea":"code","d2b4141b":"code","effd2036":"code","40060ac8":"code","0059cc6d":"code","78462052":"code","dd8281c3":"markdown","31502463":"markdown","55211a8a":"markdown","dbfe61d9":"markdown","6902a35c":"markdown","098958fa":"markdown","dd1217ae":"markdown","2f055839":"markdown","eddb59d8":"markdown","a078eeda":"markdown","69974214":"markdown","037d6c09":"markdown"},"source":{"fb7c5021":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline ","38432257":"data = pd.read_csv('..\/input\/pokemon\/Pokemon.csv')","13a6137a":"data.head(10)","97e83c06":"data.shape","53cc03da":"# Removing irrelevant features(# and Name) and features with Nan values(Type 2)\ndata = data.drop(['#','Type 2','Name'],axis='columns')","5d792b61":"data.head(10)","e7f689e8":"data.Legendary.value_counts()","d8229131":"legendaryPokemon = data.loc[data['Legendary']==True]\nlegendaryPokemon = legendaryPokemon.append(legendaryPokemon.append(legendaryPokemon))\nbal_data = data.append(legendaryPokemon.append(legendaryPokemon.append(legendaryPokemon)))","8fa4d1d2":"# mapping true and false to 1 and 0 respectively\nbal_data['Legendary'] = bal_data.Legendary.map({False:0,True:1})","f900ae0f":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\ncol_trans = make_column_transformer(\n            (OneHotEncoder(),['Type 1','Generation']),\n            (StandardScaler(),['Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed']),\n            remainder = 'passthrough')","75619906":"df = bal_data","a82ded32":"from sklearn.model_selection import train_test_split\nX = df.drop(['Legendary'], axis = 1)\ny = df['Legendary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","108fe5d1":"col_trans.fit_transform(X_train)","47acd3c2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nlogreg = LogisticRegression(solver='lbfgs')\npipe = make_pipeline(col_trans,logreg)","5949ddd2":"from sklearn.model_selection import cross_val_score\nprint('Accuracy score on Train data: {}'.format(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()*100))","5b280372":"pipe = make_pipeline(col_trans,logreg)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nfrom sklearn import metrics\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","a37557bd":"from sklearn.neighbors import KNeighborsClassifier\nknn_scores = []\nfor k in range(1,31):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    pipe = make_pipeline(col_trans,knn_classifier)\n    knn_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","5629718b":"plt.figure(figsize=(12,12))\nplt.plot([k for k in range(1, 31)], knn_scores, color = 'red')\nfor i in range(1,31):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1]*100,2)))\nplt.xticks([i for i in range(1, 31)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","8a682421":"print('Accuracy score on Train data: {}'.format(knn_scores[1]*100))","dccc7313":"knn_classifier = KNeighborsClassifier(n_neighbors = 2)\npipe = make_pipeline(col_trans,knn_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test Data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","1d04edf3":"from sklearn.svm import SVC\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    pipe = make_pipeline(col_trans,svc_classifier)\n    svc_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","653a307c":"from matplotlib.cm import rainbow\nimport numpy as np\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.figure(figsize=(10,7))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","dfebe253":"print('Accuracy score on Train data: {}'.format(svc_scores[0]*100))","f46317d9":"svc_classifier = SVC(kernel = 'linear')\npipe = make_pipeline(col_trans,svc_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","e0465704":"from sklearn.tree import DecisionTreeClassifier\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    pipe = make_pipeline(col_trans,dt_classifier)\n    dt_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","a85bca6a":"plt.figure(figsize=(10,10))\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","022e17ea":"print('Accuracy score on Train data: {}'.format(dt_scores[5]*100))","d2b4141b":"dt_classifier = DecisionTreeClassifier(max_features = 6, random_state = 0)\npipe = make_pipeline(col_trans,dt_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy  score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","effd2036":"from sklearn.ensemble import RandomForestClassifier\nrf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    pipe = make_pipeline(col_trans,rf_classifier)\n    rf_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","40060ac8":"plt.figure(figsize=(10,7))\ncolors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],5))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","0059cc6d":"print('Accuracy score on Train data: {}'.format(rf_scores[0]*100))","78462052":"rf_classifier = RandomForestClassifier(n_estimators = 10, random_state = 0)\npipe = make_pipeline(col_trans,rf_classifier)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nprint('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","dd8281c3":"### 5. Random Forest Classifier","31502463":"So, what I did was I performed oversampling of True values (multiple times) in Legendary column and appended it to our original dataset","55211a8a":"## Classification Models","dbfe61d9":"### 1. Logistic Regression","6902a35c":"It is clear that it is imbalanced dataset. Our model will fail to predict True values and memorizes on False values. So we need to make this dataset as balanced dataset.","098958fa":"### 2. K Nearest Neighbors Classifier","dd1217ae":"### 3. Support Vector Classifier (SVC)","2f055839":"## Data Cleaning","eddb59d8":"## Data Preprocessing","a078eeda":"## Importing Libraries","69974214":"## Train Test Split","037d6c09":"### 4. Decision Tree Classifier"}}