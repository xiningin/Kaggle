{"cell_type":{"a6c3d773":"code","8a37fee0":"code","4911d1d9":"code","a5c1b6d9":"code","1597f689":"code","ab077cde":"code","bd394e65":"code","1c825f67":"code","dfc4e8e0":"code","6eba1bde":"code","b4a48bdd":"code","b7a24977":"code","316d5b1d":"code","76934446":"code","43975922":"code","204212e4":"code","d48fb60f":"code","af9eff7e":"code","c796ba68":"code","1e15d582":"code","848a0263":"code","fa0d3d2a":"code","9f246e9e":"code","3b9499b9":"code","2b509170":"code","e2f11910":"code","d4c8935c":"code","f8402f0c":"code","1b04c65c":"code","db2990e4":"code","3033bea0":"code","57e4a40d":"code","b041f188":"code","9aece16e":"code","77d2bafb":"code","c4c2231d":"markdown","75362ee0":"markdown","46e5c13f":"markdown","e897d549":"markdown","06f6dbc8":"markdown","c5bc4f06":"markdown","fcc4fb37":"markdown","6c9e9211":"markdown","910a2c99":"markdown","01b3f4d9":"markdown","4d44ec19":"markdown","c64cc6ad":"markdown","e3eeb98f":"markdown","f3f632f7":"markdown","d854fff3":"markdown","245f5cf8":"markdown","da981228":"markdown","48e414e6":"markdown"},"source":{"a6c3d773":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a37fee0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import roc_curve, auc, f1_score, plot_confusion_matrix, plot_roc_curve, classification_report, accuracy_score","4911d1d9":"df = pd.read_csv('..\/input\/diabetes-dataset\/diabetes.csv')","a5c1b6d9":"df.head(3)","1597f689":"df.isna().sum().sum()","ab077cde":"df.duplicated().sum()","bd394e65":"df.info()","1c825f67":"df.describe()","dfc4e8e0":"features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']","6eba1bde":"plt.figure(figsize=(16,8))\nfor i in enumerate(features):\n    plt.subplot(2,4,i[0]+1)\n    sns.boxplot(df[i[1]], color='fuchsia')","b4a48bdd":"def countoutlier(df, col):\n    iqr = df[col].quantile(0.75) - df[col].quantile(0.25) #Inter-quartile range\n    ul = (1.5 * iqr) + df[col].quantile(0.75) #upper limit\n    ll = df[col].quantile(0.25) - (1.5 * iqr) #lower limit\n    print(f'There are {(df[col] > ul).sum()} values greater than ul in {col} column')\n    print(f'There are {(df[col] < ll).sum()} values less than ll in {col} column')\n    print('')","b7a24977":"for i in df.columns:\n    countoutlier(df, i)","316d5b1d":"def pushoutlier(df, col):\n    iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n    ul = (1.5 * iqr) + df[col].quantile(0.75)\n    ll = df[col].quantile(0.25) - (1.5 * iqr)\n    df[col][df[col] >  ul] = ul\n    df[col][df[col] < ll] = ll","76934446":"for i in df.columns:\n    pushoutlier(df, i)","43975922":"for i in df.columns:\n    countoutlier(df, i)","204212e4":"features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']","d48fb60f":"plt.figure(figsize=(16,8))\nfor i in enumerate(features):\n    plt.subplot(2,4,i[0]+1)\n    sns.distplot(df[i[1]], hist=True, color='purple',kde_kws={'color':'black'})","af9eff7e":"plt.figure(figsize=(14,8))\nfor i in df.columns[:-1]:\n    sns.displot(df, x=i, hue=\"Outcome\", aspect=2.5,palette='tab10',rug=True)\n    plt.show()","c796ba68":"plt.figure(figsize=(14,8))\nsns.heatmap(df.corr(), annot=True, cmap='plasma')","1e15d582":"plt.figure(figsize=(14,7))\nsns.countplot(df.Outcome, palette='mako', dodge=False, saturation=1.0)","848a0263":"x = df.drop('Outcome',axis=1)\ny = df.Outcome","fa0d3d2a":"xtrain2, xtest2, ytrain2, ytest2 = train_test_split(x, y, random_state=42, test_size =0.3)","9f246e9e":"from imblearn.over_sampling import SMOTE ","3b9499b9":"smote = SMOTE()\nxtrainsmote, ytrainsmote = smote.fit_resample(xtrain2, ytrain2)","2b509170":"plt.figure(figsize=(14,7))\nsns.countplot(ytrainsmote, palette='turbo')\nplt.title('Count of values in target')\nplt.show()","e2f11910":"classifier=[['Logistic Regression :',LogisticRegression(solver='liblinear')],\n       ['Decision Tree Regression :',DecisionTreeClassifier()],\n       ['Random Forest Regression :',RandomForestClassifier()],\n       ['Gradient Boosting Regression :', GradientBoostingClassifier()],\n       ['Ada Boosting Regression :',AdaBoostClassifier()],\n       ['SVC : ', SVC()],\n       ['KNN :', KNeighborsClassifier()],\n       ['Naive Bayes :', GaussianNB()],\n       ['XGboost :' ,XGBClassifier()]]\n\n\nresult = pd.DataFrame(columns = ['Name'])\n\nfor name,model in classifier:\n    model = model\n    model.fit(xtrain2,ytrain2)\n    predictions = model.predict(xtest2)\n    accuracy_scores = accuracy_score(ytest2, predictions)\n    clf_pred.append(accuracy_scores)\n    f1_scores = f1_score(ytest2, predictions)\n    result = result.append({'Name' : name, 'Accuracy Score' : accuracy_scores, 'F1_scores': f1_scores}, \n                ignore_index = True)\n    \nresult.sort_values(by=['Accuracy Score'],inplace=True, ascending=False)\nresult.style.hide_index()","d4c8935c":"from sklearn.model_selection import GridSearchCV","f8402f0c":"param_grid = [    \n    {\n    'C' : np.logspace(-4, 4, 20),\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\n\nlogModel = LogisticRegression(solver='liblinear')\nclf = GridSearchCV(logModel, param_grid = param_grid, cv = 10,verbose=True, n_jobs=-1)","1b04c65c":"best_clf = clf.fit(xtrainsmote, ytrainsmote)","db2990e4":"best_clf.best_estimator_","3033bea0":"finallr = LogisticRegression(C=1.623776739188721, max_iter=1000, solver='liblinear')","57e4a40d":"lrfinalmod = finallr.fit(xtrainsmote, ytrainsmote)\npreds = lrfinalmod.predict(xtest2)","b041f188":"from sklearn.metrics import plot_confusion_matrix, plot_roc_curve","9aece16e":"plot_confusion_matrix(lrfinalmod, xtest2, ytest2, cmap='rainbow')","77d2bafb":"plot_roc_curve(lrfinalmod,xtest2, ytest2)","c4c2231d":"# FINAL METRICS OF THE MODEL","75362ee0":"# SMOTE","46e5c13f":"***NOW LETS LOOK AT THE DISTRIBUTION OF EACH OF THE VARIABLE***","e897d549":"***WE CAN SEE THAT MANY COLUMNS HAVE OUTLIERS, THE DATASET IS SMALL SO LETS NOT DROP THE OUTLIERS, INSTEAD WE WILL PUSH THEM TO MINIMUM VALUE AND MAXIMUM VALUE ACCORDINGLY***","06f6dbc8":"***THE TARGET COLUMN HAS MORE NUMBER OF 1 GROUP COMPARED TO OTHER, THE MODEL WOULD BE BAISED TOWARDS THE MAJORITY GROUP, TO AVOID THIS LET'S USE SMOTE TO UP-SAMPLE THE MINORITY GROUP***","c5bc4f06":"***NOW LET'S BUILD THE FINAL LOGISTIC REGRESSION MODEL WITH THE BEST HYPERPARAMETERS***","fcc4fb37":"# LOADING THE DATASET","6c9e9211":"***Let's look at the dataset***","910a2c99":"***IMPORTING ESSENTIAL LIBRARIES***","01b3f4d9":"# BASIC EDA","4d44ec19":"***NOW THAT OUR TARGET HAS EQUAL NUMBER OF COUNT IN EACH GROUP, THIS WILL AVOID BIASING OF THE MODEL***","c64cc6ad":"***MOST OF THE COLUMNS ARE LINEARLY DISTRIBUTED, LOGISTIC REGRESSOR MAY PREFORM WELL HERE***","e3eeb98f":"***Seems like we might have some outliers in several columns, we need to treat them***","f3f632f7":"***THE OUTLIERS HAVE BEEN SUCCESSFULLY HANDELED***","d854fff3":"***THE COLUMNS ARE NOT MUCH CORELATED TO EACH OTHER***","245f5cf8":"# HYPERPARAMETER TUNING OF LOGISTIC REGRESSOR","da981228":"***WE CAN SEE ABOVE THAT ALMOST ALL THE COLUMNS HAVE OUTLIERS IN THEM, BUT WE DONT KNOW HOW MANY OUTLIERS ARE THERE IN EACH OF THE COLUMNS***","48e414e6":"***AS EXPECTED THAT THE LOGISTIC REGRESSION MODEL PREFORMS BETTER THAN THE REST OF THE MODELS***\n<BR>\n***NOW LET'S BUILD LOGISTIC REGRESSOR MODEL WITH HYPERPARAMETER TUNING***"}}