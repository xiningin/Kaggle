{"cell_type":{"fdf6cc76":"code","9fdb1dcf":"code","5c0d89b2":"code","9b8945c2":"code","5ec74df1":"code","b54796cd":"code","7200a97a":"code","0b907749":"code","e0cc5207":"code","af13ed81":"code","1de637ee":"code","76b77bbf":"code","36ff2b46":"code","114a9cc4":"code","d4822fa2":"code","b89caca5":"code","8b38cfee":"code","2c9eb61e":"code","cc585341":"code","7e1ba730":"code","1f990703":"code","50741c67":"code","c6eb0da0":"code","9237acfe":"code","187119b0":"code","c621ff75":"code","806d20fe":"code","ec867d19":"code","67ad2620":"markdown","4553daf4":"markdown"},"source":{"fdf6cc76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fdb1dcf":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","5c0d89b2":"import torch","9b8945c2":"## Setting device for PyTorch to GPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"There are %d GPU(s) avaiable\" % torch.cuda.device_count())\n    print(\"We will user the GPU: \",torch.cuda.get_device_name(0))\n\nelse:\n    print(\"No GPU available using the CPU instead\")\n    device =  torch.device(\"cpu\")","5ec74df1":"from transformers import BertTokenizer, BertForSequenceClassification\nimport matplotlib.pyplot as plt\n%matplotlib inline","b54796cd":"df = pd.read_csv(dirname + \"\/train.csv\")","7200a97a":"train_df = df.iloc[0: 9697]\nval_df = df.iloc[9697:]","0b907749":"labels, frequencies = np.unique(df['language'].values, return_counts=True)\n\nplt.figure(figsize=(10, 10))\nplt.pie(frequencies, labels=labels, autopct='%1.1f%%')\nplt.show()","e0cc5207":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)","af13ed81":"model.config.id2label = {\"0\": \"entailment\", \"1\": \"neutral\", \"2\": \"contradiction\"}\nmodel.config.label2id = {\"entailment\": \"0\", \"neutral\": \"1\", \"contradiction\": \"2\"}","1de637ee":"from torch.utils.data import Dataset, DataLoader","76b77bbf":"class MNLIDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, transform=None, is_test_dataset=False):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.is_test_dataset = is_test_dataset\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        tokenized_data = self.tokenizer(item.premise, item.hypothesis, padding=\"max_length\", truncation=True, max_length=512,\n                                       return_tensors='pt')\n        if not self.is_test_dataset:\n            tokenized_data['labels'] = torch.tensor(item.label)\n        return tokenized_data","36ff2b46":"from torch.utils.data import DataLoader\ntrain_dataset = MNLIDataset(train_df, tokenizer)\nval_dataset = MNLIDataset(val_df, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, drop_last=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, drop_last=True)","114a9cc4":"model.to(device)","d4822fa2":"from transformers import AdamW\nfrom transformers import get_scheduler","b89caca5":"optimizer = AdamW(model.parameters(), lr=1e-5)\nnum_epochs = 5\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)","8b38cfee":"from sklearn.metrics import accuracy_score","2c9eb61e":"def batch_accuracy(logits, labels):\n    return accuracy_score(np.argmax(logits, axis=1).flatten(), labels.flatten())","cc585341":"from tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n    batches_accuracy = 0    \n    for batch_dict in train_dataloader:\n        batch_dict = {k: v.squeeze().to(device) for k, v in batch_dict.items()}\n        outputs = model(**batch_dict)\n\n        loss = outputs.loss\n        logits = outputs.logits.cpu().detach().numpy()\n\n        labels = batch_dict['labels'].cpu().detach().squeeze()\n        batches_accuracy += batch_accuracy(logits, labels)\n\n        loss.backward()        \n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n    \n    num_batches = len(train_dataloader)\n    epoch_avg_acc = batches_accuracy \/ num_batches\n    print(f\"Average train accuracy for epoch {epoch}: {epoch_avg_acc}\")","7e1ba730":"## To clear the GPU memory occupied by PyTorch\ntorch.cuda.empty_cache()","1f990703":"model.eval()\n\nbatch_val_acc = 0\n    \nfor batch_dict in tqdm(val_dataloader):\n    batch_dict = {k: v.squeeze().to(device) for k, v in batch_dict.items()}\n    outputs = model(**batch_dict)\n\n    loss = outputs.loss\n    logits = outputs.logits.cpu().detach().numpy()\n\n    labels = batch_dict['labels'].cpu().detach().squeeze()\n    batch_val_acc += batch_accuracy(logits, labels)\n\nnum_batches = len(val_dataloader)\nval_accuracy = batch_val_acc \/ num_batches","50741c67":"print(f\"Validation accuracy: {val_accuracy}\")","c6eb0da0":"test_df = pd.read_csv(dirname + \"\/test.csv\")","9237acfe":"test_dataset = MNLIDataset(test_df, tokenizer, is_test_dataset=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)","187119b0":"model.eval()\npredictions = []\nfor batch_dict in tqdm(test_dataloader):\n    batch_dict = {k: v.squeeze(axis=1).to(device) for k, v in batch_dict.items()}\n    outputs = model(**batch_dict)\n\n    loss = outputs.loss\n    logits = outputs.logits.cpu().detach().numpy()\n    pred = np.argmax(logits) \n    predictions.append(pred)","c621ff75":"submission = test_df.id.copy().to_frame()\nsubmission['prediction'] = predictions","806d20fe":"submission.head()","ec867d19":"submission.to_csv('submission.csv', index=False)","67ad2620":"### Bert takes in input three variables\n- input_ids: ids of the tokens - tensor of integer values\n- attention_mask: attention mask\n- token_type_ids: To encode 2 sequences as different","4553daf4":"### Submitting Results"}}