{"cell_type":{"95937823":"code","8c776500":"code","d78f473b":"code","6bf8bfd8":"code","ca65c942":"code","74b7ed03":"code","8f0cb547":"code","777ee63d":"code","d28ced8e":"code","1a5a91d6":"code","5827cf93":"code","989249b3":"code","8efee828":"code","7bab6550":"code","8bb5beff":"code","c894f835":"code","609c21b7":"code","e3a613d1":"code","f94ccdd1":"code","34a3b657":"code","14681b2d":"code","1ff7dea1":"code","9d6127bb":"code","c7fbfb8a":"code","cf4b8b36":"code","9a53f7c1":"code","58a6dcce":"code","cd4cae1d":"code","5347510d":"code","affcd379":"code","3c75f778":"markdown","3b1c0fb4":"markdown","01aa0ed1":"markdown","652a0ca0":"markdown","e34f95f1":"markdown","c90b4593":"markdown","21efc06e":"markdown","adc6e41e":"markdown","cb4f1c04":"markdown","63502fd6":"markdown","fcefbd9e":"markdown","be7011d3":"markdown","4a2f96f8":"markdown","dc020585":"markdown","e71cfee2":"markdown","e23e91c7":"markdown","8b5db17c":"markdown","85ad410b":"markdown","b0990dab":"markdown","0f9eedc8":"markdown","6254ad2c":"markdown","33b6bbf4":"markdown","28600123":"markdown","84f18f1a":"markdown","03a1cb51":"markdown","6a1d9f89":"markdown","86ae8a5d":"markdown","4660104f":"markdown","efe33355":"markdown","8bf6be90":"markdown","2a12526e":"markdown"},"source":{"95937823":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport itertools\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #To generate plots\nfrom haversine import haversine, Unit # To calculate the distance between origin and destination states\nimport glob # to extract file locations\nimport seaborn as sns #To generate plots\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nimport sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport scipy\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c776500":"df = pd.read_csv('\/kaggle\/input\/projectdata\/ffdata_cleaned.csv')\ndf.columns","d78f473b":"df.columns[13:37]","6bf8bfd8":"df.Distance.unique()","ca65c942":"df = df[df['Distance'] != 0]","74b7ed03":"missing_values = df.isna().sum()\npercent_missing = ((missing_values\/df.index.size)*100)\npercent_missing","8f0cb547":"object_cols = df.select_dtypes(include='object').columns \nobject_cols","777ee63d":"for i in range(len(object_cols)):\n    df[object_cols[i]] = df[object_cols[i]].str.replace(r'[\\D+]','')\n    df[object_cols[i]].replace('',np.nan, inplace = True)\n    df[object_cols[i]] = df[object_cols[i]].fillna(-1).astype('float64')\n    ","d28ced8e":"df.replace(-1,np.nan, inplace =True)","1a5a91d6":"df.drop(['rice_dms_destst','rice_dms_origst'], axis =1, inplace = True)\ndf= df.drop([\"fr_inmode\",\"fr_outmode\"],axis = 1)\ndf = df.reset_index(drop=True)","5827cf93":"corr = df[df.columns.values[7:]].corr()\nf,ax = plt.subplots(figsize=(18,15))\n#plot a heatmap\nsns.heatmap(corr, annot = True,cmap='Blues')\nplt.title(\"Pearson correlation Matrix\",fontsize=20)\nplt.show()","989249b3":"hist = df.hist(figsize=(20,18))","8efee828":"df.describe()","7bab6550":"fig, ax = plt.subplots()\nax.hist(df[df[\"value\"]<= 1000][\"value\"])\nax.set_xlabel(\"Value in million $\")\nax.set_ylabel(\"frequency\")","8bb5beff":"df = df[df['value'] <= 1000]","c894f835":"df.describe()","609c21b7":"df.info()","e3a613d1":"df = df.fillna(0)","f94ccdd1":"df = df[df['value'] > 0]","34a3b657":"def onehotfeatures(col):\n    global df\n    one_hot = pd.get_dummies(df[col])\n    df.drop(col, axis =1, inplace = True)\n    one_hot.columns = [i for i in range(len(one_hot.columns))]\n    df = pd.concat([df,pd.DataFrame(one_hot)], axis =1)\n    return df","14681b2d":"onehotfeatures('fr_orig')\nonehotfeatures('fr_dest')\nonehotfeatures('dms_origst')\nonehotfeatures('dms_destst')\nonehotfeatures('dms_mode')\nonehotfeatures('trade_type')\nonehotfeatures('Year')","1ff7dea1":"label_cols = [\"value\"]\nfeature_cols = np.array(df.drop(label_cols,axis=1))\nlabel_cols = np.array(df[label_cols]).ravel()\n\ntrain_features, n_features, train_labels, n_labels = train_test_split(feature_cols, label_cols, test_size = 0.4, random_state = 42)\ntest_features, validation_features, test_labels, validation_labels = train_test_split(n_features, n_labels, test_size = 0.5, random_state = 42)","9d6127bb":"model = tf.keras.Sequential()\nnorm = tf.keras.layers.experimental.preprocessing.Normalization()\nnorm.adapt(train_features)\nmodel.add(norm)\n\nmodel.add(tf.keras.layers.Dense(30, activation='relu'))\nmodel.add(tf.keras.layers.Dense(12, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss='mean_squared_error',\n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nmodel.fit(train_features, train_labels, batch_size = 100, validation_split = 0.2 , epochs=25, verbose = 2)","c7fbfb8a":"test_predictions = model.predict(test_features)\nfig, ax = plt.subplots()\nax.set_title(\"Model Prediction scatter plot\")\nax.set_xlabel(\"Test value in million $\")\nax.set_ylabel(\"predicted value in million $\")\nplt.scatter(test_labels, test_predictions)","cf4b8b36":"test_errors = abs(test_predictions - test_labels)\nmse = sklearn.metrics.mean_squared_error(test_labels, test_predictions)\nr2_score = sklearn.metrics.r2_score(test_labels, test_predictions)\nprint('Absolute Error is ', round(np.mean(test_errors), 2))\nprint(\"MSE is \", round(mse, 2))\nprint(\"R2 score is \", round(r2_score, 2))","9a53f7c1":"# list of tuples: the first element is a string, the second is an object\nestimators = [('RidgeCV', RidgeCV()),('RandomForest', RandomForestRegressor())]\n\nfor estimator in estimators:\n    scores = cross_val_score(estimator=estimator[1],\n                            X=train_features,\n                            y=train_labels,\n                            scoring='r2',\n                            cv=3,\n                            n_jobs=-1)\n    #print('CV accuracy scores: %s' % scores)\n    print(estimator[0], 'CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","58a6dcce":"#RidgeCV\ngs = GridSearchCV(\n                estimator=RidgeCV(),\n                param_grid={'fit_intercept':[True, False],\n                            'normalize':[True, False]},\n                scoring='r2',\n                cv=3,\n                n_jobs=-1)\n\ngs = gs.fit(train_features, train_labels)\nprint('RidgeCV:')\nprint('Training accuracy: %.3f' % gs.best_score_)\nprint(gs.best_params_)\nest = gs.best_estimator_\nest.fit(train_features, train_labels)\nprint('Best alpha: ', est.alpha_)\nprint('Validation accuracy: %.3f' % est.score(validation_features, validation_labels))","cd4cae1d":"seed = 42\n#RandomForest\ngs = GridSearchCV(\n                estimator=RandomForestRegressor(random_state=seed),\n                 param_grid={'max_depth':[3, 10, 20],\n                             'n_estimators':[10, 30, 50]},\n\n        \n                scoring='r2',\n                n_jobs=-1)\n\ngs = gs.fit(train_features, train_labels)\nprint('Random Forest:')\nprint('BR: ', gs.best_score_)\nprint('BR: ', gs.best_params_)\nest = gs.best_estimator_\nest.fit(train_features, train_labels)\nprint('Validation accuracy: %.3f' % est.score(validation_features, validation_labels))","5347510d":"import pickle\n# Save to file in the current working directory\npkl_filename = \"pickle_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(est, file)\n\nprint('Validation accuracy: %.3f' % est.score(test_features, test_labels))","affcd379":"predictions = est.predict(test_features)\nfig, ax = plt.subplots()\nax.set_title(\"Model Prediction scatter plot\")\nax.set_xlabel(\"Test value in million $\")\nax.set_ylabel(\"predicted value in million $\")\nplt.scatter(test_labels, test_predictions)","3c75f778":"As we can see, there are some null values in crop production feature columns. We fill those values with 0.","3b1c0fb4":"### Loading the processed data","01aa0ed1":"## Feature description\n\nThere are 39 features in total. All of them are briefly described below.\n","652a0ca0":"First of all, we import all the necessary packages and functions.","e34f95f1":"Now, we compare our NN model with one linear model and one ensemble model, RidgeCV and RandomForestRegressor respectively.","c90b4593":"## Preprocessing","21efc06e":"We first select out those variables with \"object\" data types.","adc6e41e":"For our project analysis, we our group is using different model approaches. In this analysis though, we will just be using neural network to predict the food flow between US states. We will be using two hidden layers with 30 and 12 neurons respectively with ReLu activation function. We will be using Adam optimizer function with a learning rate of 0.001, \"mean squared error\" loss function and RMSE as the metric. Batch size of 100 is considered with a validation split of 0.2 and number of epochs equals to 20.","cb4f1c04":"As we see, Rice for both domestic origin and destination have very high missing value percentages. So we will drop both of them. Also, foreign inmode and outmode also has a lot of missing values so will get rid of them too.","63502fd6":"After creating the categorical features, we need make the data into model readable format (into numpy arrays) and to split the data into training and test datasets.","fcefbd9e":"We see that most of the crop production values are object variables. We need to correct their data type  float values. In these following steps we do exactly that.","be7011d3":"Before we move towards building the neural network we need to have correct data type for different features and we also need to categorize data according to their type (i.e. categorical or numerical). Because there are some null values in many features, we need to remove them carefully so that we do not lose import data. First, we remove the columns with missing percentage greater than 30%.","4a2f96f8":"**Overview:**\n\nGoal: To predict the food flows in the US.","dc020585":"Since one of the \"Distance\" unique values is 0, we need to remove those values.","e71cfee2":"In the exploratory data analysis, we manipulated the data to find appropriate features for the model. Those features were saved in a separate file and that file is now loaded here.","e23e91c7":"**Ridge regression in short:**\nAn L2 penalized model where we add the squared sum of the weights (scaled by lambda) to the least-square cost function.\n\n**RandomForestRegressor in short:**\nAn ensemble of different regression trees, where each leaf contains a distribution for the continuous output variable\/s. ","8b5db17c":"Next, we see the correlation matrix and histogram of all variables to find collinearity and any anolmaly which might affect our results.","85ad410b":"### Notice: the whole project is different from what we plan at first. The train and test data in kaggle competition use 2013-2017 data but acturally only 2017 data is real survey data. Others are model output. So now we use 1997, 2002, 2007, 2012 and 2017 Freight Analysis Framework (FAF) survey data. Also previously, we plan to distinguish the difference among export, import and domestic trade data. But now we only estimate the food (7 categories) flow among states and We don\u2019t care if these commodities are locally produced or imported.****","b0990dab":"As we can clearly see, the standard deviation for the dataset is very high and also the quantile values are quite low. We need to remove the major outliers from the dataset.","0f9eedc8":"In our analysis, features like 'fr_orig', 'dms_origst', 'dms_destst', 'fr_dest', 'dms_mode', 'trade_type' and 'Year' will behave as categorical variables. To make sure act like one, we need to convert them into onehot encoding format. To achieve this, we create a function that takes the string value of the column and return the whole datframe with a new onehot encoded column for the column fed to the function.","6254ad2c":"Before we move forward, we would like to make sure that there are no self loops in the food flow network, i.e. the origin and destination of the domestic flows are not the same.","33b6bbf4":"\"dms\" means domestic, \"fr\" foreign, \"orig\" means origin and \"dest\" means destination\n\n\"dms_origst\":the origination state of domestic trade\n\n\"dms_destst\":the destination state of domestic trade\n\n\"dms_mode\" :Domestic: Mode Imports: Mode from Zone of entry to destination Exports: Mode from origin to zone of exit\n\n\"sctg2\":commodity type\n\n\"value\" : trade value in $\n\n\"year\": trade year\n\n\"distance\": distance between the dms_origst and dms_destst\n\n\"gdp\",\"income\",xxx commodity production are match for \"origst\" and \"destst\" states.\n\nAll other columns of the dataset are crop variables with relevant descriptions as suffixes and prefixes.","28600123":"### Tune hyperparameters using GridSearchCV\n\nThe models that we have chosen perform ok, but we can improve their performance by tuning the hyperparameters, i.e., those parameters that are not learned from the training data.\n\nIn order to do so, we will use an operation technique called grid search that will help us finding the optimal combination of hyperparameter values. Grid search is a brute-force exhaustive search paradigm.","84f18f1a":"## Feature engineering","03a1cb51":"Now we look at the model prediction power of this neural network.","6a1d9f89":"## Conclusion\n\nWe ran NN model and two different models to find out that the Random Forest model gives the best results. ","86ae8a5d":"# Predicting food flows in the US using Neural Networks","4660104f":"Most of the crop production values are right-tailed which means that we need to normalize the data before we feed it to our model. Also, we notice that the \"value\" feature is highly concentrated at one bin. Let's check the mean and standard deviation of the feature. ","efe33355":"The grid search method find the max_depth and max_tree number is the optimal condition and the model doesn't overfit.","8bf6be90":"Finally we remove the values for which the \"value\" has non-zero entries.","2a12526e":"Here we call the function for the required features."}}