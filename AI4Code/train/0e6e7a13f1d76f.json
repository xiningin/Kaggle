{"cell_type":{"56c5e65e":"code","4cb3359f":"code","25ce81ec":"code","75b745ee":"code","07bac475":"code","d207157b":"code","c474ba2b":"code","f4dceeb0":"code","be5d1d87":"code","c5097555":"code","bfb1ccbd":"code","5dc63ed0":"code","757b2c91":"code","efc87b0b":"code","44243400":"code","1330e98f":"code","c4f12a03":"code","317b0932":"code","1ce18896":"code","56c3f520":"code","7e4948cf":"code","6f5f9ffe":"code","b72be4e6":"code","8c23cb0c":"code","7d230995":"code","f6c82871":"code","54c1311c":"code","7bef4134":"code","d2c22e1d":"code","f120305f":"code","6898da95":"code","26c7e7e5":"code","705827a1":"code","5dfef6df":"code","e1c42da1":"markdown","da0c02b9":"markdown","884195e1":"markdown","476d9893":"markdown","556f87b9":"markdown","63510a93":"markdown","6e3ed8e2":"markdown","3f5cfdb0":"markdown","6dd176cb":"markdown","e259d643":"markdown","f7ad3c9b":"markdown","48d18d63":"markdown","3cab32a8":"markdown","7c8226ab":"markdown","b87b70b0":"markdown","b809c21c":"markdown","e36c4d6f":"markdown","c7fd8f83":"markdown","0cca8272":"markdown","f4a2f1d7":"markdown","439171f7":"markdown","e3544800":"markdown","51857eaa":"markdown","288baac8":"markdown"},"source":{"56c5e65e":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score,accuracy_score\nfrom sklearn.metrics import make_scorer,f1_score,precision_score,recall_score\nfrom sklearn.metrics import mean_squared_error,classification_report\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\n#import tensorflow as tf \n#from tensorflow.keras import layers\n#from tensorflow.keras.callbacks import EarlyStopping\n#from tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold,TimeSeriesSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n# Feature Selection \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBClassifier,XGBRegressor\nfrom sklearn import set_config\nfrom itertools import combinations\n# Cluster :\nfrom sklearn.cluster import MiniBatchKMeans\n#from yellowbrick.cluster import KElbowVisualizer\n#import smong \nimport category_encoders as ce\nimport warnings\nimport optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\nfrom typing import List, Optional, Union\nset_config(display='diagram')\nwarnings.filterwarnings('ignore')","4cb3359f":"%%time\n# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/frauddetection\/transactions_train.csv\")\n# Preview the data\ntrain.head(3)","25ce81ec":"# Convert Dtypes :\ntrain[train.select_dtypes(['int64','int16','float32','float64','int8']).columns] = train[train.select_dtypes(['int64','int16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','category']).columns] = train.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))","75b745ee":"# Pour le train test\ntarget= \"isFraud\"\nX = train.drop(target, axis='columns').iloc[0:500000,:]# axis=1\ny = train[target].iloc[0:500000]\ndel train ","07bac475":"# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','int16','float32','float64','int8']).columns","d207157b":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','int16','float32','float64','int8']).columns","c474ba2b":"all_columns = (num_columns.append(cat_columns))\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","f4dceeb0":"if set(all_columns) == set(X.columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('in all_columns but not in  train  :', set(all_columns) - set(X.columns))\n    print('in X.columns   but not all_columns :', set(X.columns) - set(all_columns))","be5d1d87":"class ColumnsSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, positions):\n        self.positions = positions\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        #return np.array(X)[:, self.positions]\n        return X.loc[:, self.positions] \n########################################################################\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n    # https:\/\/towardsdatascience.com\/how-to-write-powerful-code-others-admire-with-custom-sklearn-transformers-34bc9087fdd\n    def __init__(self):\n        self._estimator = PowerTransformer()\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1\n        self._estimator.fit(X_copy)\n\n        return self\n\n    def transform(self, X):\n        X_copy = np.copy(X) + 1\n\n        return self._estimator.transform(X_copy)\n\n    def inverse_transform(self, X):\n        X_reversed = self._estimator.inverse_transform(np.copy(X))\n\n        return X_reversed - 1  \n\nclass TemporalVariableTransformer(BaseEstimator, TransformerMixin):\n    # Temporal elapsed time transformer\n\n    def __init__(self, variables, reference_variable):\n        \n        if not isinstance(variables, list):\n            raise ValueError('variables should be a list')\n        \n        self.variables = variables\n        self.reference_variable = reference_variable\n\n    def fit(self, X, y=None):\n        # we need this step to fit the sklearn pipeline\n        return self\n\n    def transform(self, X):\n\n       # so that we do not over-write the original dataframe\n        X = X.copy()\n        \n        for feature in self.variables:\n            X[feature] = X[self.reference_variable] - X[feature]\n\n        return X\nclass CustomImputer(BaseEstimator, TransformerMixin) : \n    def __init__(self, variable, by) : \n            #self.something enables you to include the passed parameters\n            #as object attributes and use it in other methods of the class\n            self.variable = variable\n            self.by = by\n\n    def fit(self, X, y=None) : \n        self.map = X.groupby(self.by)[variable].mean()\n        #self.map become an attribute that is, the map of values to\n        #impute in function of index (corresponding table, like a dict)\n        return self\n\ndef transform(self, X, y=None) : \n    X[variable] = X[variable].fillna(value = X[by].map(self.map))\n    #Change the variable column. If the value is missing, value should \n    #be replaced by the mapping of column \"by\" according to the map you\n    #created in fit method (self.map)\n    return X\n\n    # categorical missing value imputer\nclass Mapper(BaseEstimator, TransformerMixin):\n\n    def __init__(self, variables, mappings):\n\n        if not isinstance(variables, list):\n            raise ValueError('variables should be a list')\n\n        self.variables = variables\n        self.mappings = mappings\n\n    def fit(self, X, y=None):\n        # we need the fit statement to accomodate the sklearn pipeline\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        for feature in self.variables:\n            X[feature] = X[feature].map(self.mappings)\n\n        return X  \n    \n##########################################################################\nclass CountFrequencyEncoder(BaseEstimator, TransformerMixin):\n    #temp = df['card1'].value_counts().to_dict()\n    #df['card1_counts'] = df['card1'].map(temp)\n    def __init__(\n        self,\n        encoding_method: str = \"count\",\n        variables: Union[None, int, str, List[Union[str, int]]] = None,\n        keep_variable=True,\n                  ) -> None:\n\n        self.encoding_method = encoding_method\n        self.variables = variables\n        self.keep_variable=keep_variable\n\n    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):\n        \"\"\"\n        Learn the counts or frequencies which will be used to replace the categories.\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n            The training dataset. Can be the entire dataframe, not just the\n            variables to be transformed.\n        y: pandas Series, default = None\n            y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        self.encoder_dict_ = {}\n\n        # learn encoding maps\n        for var in self.variables:\n            if self.encoding_method == \"count\":\n                self.encoder_dict_[var] = X[var].value_counts().to_dict()\n\n            elif self.encoding_method == \"frequency\":\n                n_obs = float(len(X))\n                self.encoder_dict_[var] = (X[var].value_counts() \/ n_obs).to_dict()\n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        # replace categories by the learned parameters\n        X = X.copy()\n        for feature in self.encoder_dict_.keys():\n            if self.keep_variable:\n                X[feature+'_fq_enc'] = X[feature].map(self.encoder_dict_[feature])\n            else:\n                X[feature] = X[feature].map(self.encoder_dict_[feature])\n        return X[self.variables].to_numpy()\n#################################################   \nclass FeaturesEngineerGroup(BaseEstimator, TransformerMixin):\n    def __init__(self,groupping_method =\"mean\",\n                   variables=  \"amount\",\n                   groupby_variables = \"nameOrig\"                         \n                 ) :\n        self.groupping_method = groupping_method\n        self.variables=variables\n        self.groupby_variables=groupby_variables\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n        The training dataset. Can be the entire dataframe, not just the\n        variables to be transformed.\n        y: pandas Series, default = None\n        y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        self.group_amount_dict_ = {}\n        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n        #df = pd.merge(df,temp,on='card1',how='left')\n        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n        # learn mean\/medain \n        #for groupby in self.groupby_variables:\n         #   for var in self.variables:\n        if self.groupping_method == \"mean\":\n            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['mean']).to_dict()\n        elif self.groupping_method == \"median\":\n            self.group_amount_dict_[self.variables] =X.fillna(np.nan).groupby([self.groupby_variables])[self.variables].agg(['median']).to_dict()\n        else:\n            print('error , chose mean or median')\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        #for col in self.variables:\n         #   for agg_type in self.groupping_method:\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_[ self.variables][self.groupping_method])\n        return X[new_col_name].to_numpy().reshape(-1,1)    \n    \n################################################   \nclass FeaturesEngineerGroup2(BaseEstimator, TransformerMixin):\n    def __init__(self,groupping_method =\"mean\",\n                   variables=  \"amount\",\n                   groupby_variables = \"nameOrig\"                         \n                 ) :\n        self.groupping_method = groupping_method\n        self.variables=variables\n        self.groupby_variables=groupby_variables\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        Learn the mean or median of  amount of each client which will be used to create new feature for each unqiue client in order to undersatant thier behavior .\n        Parameters\n        ----------\n        X: pandas dataframe of shape = [n_samples, n_features]\n        The training dataset. Can be the entire dataframe, not just the\n        variables to be transformed.\n        y: pandas Series, default = None\n        y is not needed in this encoder. You can pass y or None.\n        \"\"\"\n        X = X.copy()\n        self.group_amount_dict_ = {}\n        #df.groupby('card1')['TransactionAmt'].agg(['mean']).to_dict()\n        #temp = df.groupby('card1')['TransactionAmt'].agg(['mean']).rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n        #df = pd.merge(df,temp,on='card1',how='left')\n        #target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n        #df_test = df_test.join(target_mean, on=['id1', 'id2'])\n        #lifeExp_per_continent = gapminder.groupby('continent').lifeExp.mean()\n        # learn mean\/medain \n        #for groupby in self.groupby_variables:\n         #   for var in self.variables:\n\n        print('we have {} unique clients'.format(X[self.groupby_variables].nunique()))\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method    \n        X[new_col_name] = X.groupby([self.groupby_variables])[[self.variables]].transform(self.groupping_method)\n        X = X.drop_duplicates(['nameOrig'])\n    \n        self.group_amount_dict_ = dict(zip(X[self.groupby_variables], X[new_col_name]))\n        del X\n        #print('we have {} unique mean amount : one for each client'.format(len(self.group_amount_dict_)))\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        #for col in self.variables:\n         #   for agg_type in self.groupping_method:\n        new_col_name =  self.variables+'_Transaction_'+ self.groupping_method\n        X[new_col_name] = X[self.groupby_variables].map(self.group_amount_dict_)\n        return X[new_col_name].to_numpy().reshape(-1,1)   \n    \n############################################  \nclass FeaturesEngineerCumCount(BaseEstimator, TransformerMixin):\n    def __init__(self,group_one =\"step\",\n                   group_two=  \"nameOrig\"                       \n                 ) :\n        self.group_one =group_one\n        self.group_two=group_two\n        \n    def fit(self, X, y=None):\n        \"\"\"\n        \"\"\"\n        return self\n    \n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.copy()\n        new_col_name =  self.group_two+'_Transaction_count'\n        X[new_col_name] = X.groupby([self.group_one, self.group_two])[[self.group_two]].transform('count')\n        return X[new_col_name].to_numpy().reshape(-1,1)","c5097555":"# complete pipe :\n# select the float\/cat columns\n#cat_feautres = X.select_dtypes(include=['object','category']).columns\n#num_features = X.select_dtypes(exclude=['object','category']).columns\n#Define vcat pipeline\nfeatures_cum_count=['step','nameOrig']\nfeatures_groupby_amount=['amount','nameOrig']\nfeatures_frequency_orig_dest=['nameOrig','nameDest']\nfeatures_cum_count_pipe = Pipeline([\n                     ('transformer_Encoder', FeaturesEngineerCumCount())\n                    ])\nfeatures_groupby_pipe = Pipeline([\n                     ('transformer_group_amount_mean', FeaturesEngineerGroup2()),\n                     ('transformer_group_scaler', PowerTransformer())\n                    ])\nfeatures_frequency_pipe = Pipeline([\n                     ('Encoder', CountFrequencyEncoder(variables=['nameOrig','nameDest'],encoding_method =\"frequency\", keep_variable=False))\n                    ])\ntype_pipe= Pipeline([\n                     ('transformer_Encoder', ce.cat_boost.CatBoostEncoder())\n                    ])\nnum_features0=[  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']\n#Define vnum pipeline\nnum_pipe = Pipeline([\n                     ('scaler', PowerTransformer()),\n                    ])\n#Featureunion fitting training data\npreprocessor = FeatureUnion(transformer_list=[('cum_count', features_cum_count_pipe),\n                                              ('mean_amount', features_groupby_pipe),\n                                              ('frequency_dest_orig', features_frequency_pipe),\n                                              ('trans_type', type_pipe),\n                                              ('num', num_pipe)])\ndata_preparing= ColumnTransformer([\n    ('cum_count', features_cum_count_pipe, features_cum_count ),\n    ('mean_amount', features_groupby_pipe, features_groupby_amount ),\n    ('frequency_dest_orig', features_frequency_pipe, features_frequency_orig_dest ),\n    ('trans_type', type_pipe, ['type'] ),\n    ('num', num_pipe, num_features0)\n], remainder='drop')\ndata_preparing","bfb1ccbd":"final_columns=['cum_count','mean_amount','frequency_Orig','frequency_Dest','trans_type',  'amount',  'oldbalanceOrig', 'newbalanceOrig' ,'oldbalanceDest', 'newbalanceDest']","5dc63ed0":"param_lgbm1 ={'metric': 'auc',\n             # \"device_type\" : \"gpu\",\n              'boosting_type': 'gbdt',\n              'lambda_l1': 1.363777251241345e-08,\n              'lambda_l2': 4.62760154881154e-05,\n              'num_leaves': 220,\n              'n_estimators': 11544,\n              'reg_alpha': 0.2364285339007138,\n              'reg_lambda': 0.2640697497357361,\n              'colsample_bytree': 1.0,\n              'subsample': 0.7,\n              'learning_rate': 0.014,\n              'tree_method': \"gpu_hist\",\n              'max_depth': 100, \n              'min_child_samples': 128,\n              'min_data_per_groups': 36} \n\nmodel_lgbm1 = LGBMClassifier(\n                        **param_lgbm1,\n                       random_state = 0,\n                      )\n\npipeline_model_lgbm1 = Pipeline([\n        ('pre', data_preparing),\n        ('estimator', model_lgbm1)\n    ])\npipeline_model_lgbm1 ","757b2c91":"%%time\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1,shuffle=False,random_state=7)\ndata_preparing.fit(X_train, y_train)\nx_pre = data_preparing.transform(X_train)\nx_test_pre =data_preparing.transform(X_test)\n# pipeline_model_lgbm.fit(X,y)","efc87b0b":"x_pre.shape","44243400":"df_test_pre = pd.DataFrame(data=x_test_pre, columns=final_columns)\ndf_test_pre.head()","1330e98f":"df_test_pre.describe()","c4f12a03":"%%time\nmodel_lgbm1.fit(x_pre,y_train)","317b0932":"f, ax = plt.subplots(figsize=(6, 15))\nfeature_importances = (model_lgbm1.feature_importances_ \/ sum(model_lgbm1.feature_importances_)) * 100\nresults = pd.DataFrame({'Features': final_columns,\n                        'Importances': feature_importances})\nresults.sort_values(by='Importances', inplace=True)\n\nax = plt.barh(results['Features'], results['Importances'])\nplt.xlabel('Importance percentages')\nplt.show()\n#########################################","1ce18896":"df_feature_importance = (\n    pd.DataFrame({\n        'feature': final_columns,\n        'importance': model_lgbm1.feature_importances_,\n    })\n    .sort_values('importance', ascending=False)\n)\ndf_feature_importance","56c3f520":"import shap\n# print the JS visualization code to the notebook\nshap.initjs()","7e4948cf":"explainer = shap.TreeExplainer(model_lgbm1)\n","6f5f9ffe":"shap_values = explainer.shap_values(df_test_pre)","b72be4e6":"explainer.expected_value","8c23cb0c":"# visualize the first prediction's explanation\n#shap.plots.waterfall( shap_values[1][0,:])","7d230995":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], df_test_pre.iloc[0,:])","f6c82871":"# Get the index of elements with value 1\nresult = np.where(y_test == 1)\nfraudvalue=result[0][0]","54c1311c":"df_test_pre.iloc[fraudvalue,:].to_numpy()","7bef4134":"print(\"Test data (actual observation): {}\".format(y_test.iloc[fraudvalue]))\npreds=model_lgbm1.predict(df_test_pre.iloc[fraudvalue,:].to_numpy().reshape(1, -1))\nprint(\"Model's prediction: {}\".format(preds))","d2c22e1d":"shap.force_plot(explainer.expected_value[1], shap_values[1][fraudvalue,:], df_test_pre.iloc[fraudvalue,:])","f120305f":"shap.decision_plot(explainer.expected_value[1], shap_values[1][fraudvalue,:], df_test_pre.iloc[fraudvalue,:],\n                   link='logit', highlight=0)","6898da95":"shap.summary_plot(shap_values, df_test_pre, plot_type='bar')","26c7e7e5":"# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], df_test_pre\n                  #,use_log_scale=True\n                 )","705827a1":"shap.initjs()\nshap.dependence_plot(ind='frequency_Dest', \n                     interaction_index='frequency_Dest',\n                     shap_values=shap_values[1], \n                     features=df_test_pre)","5dfef6df":"shap.initjs()\n\nshap.dependence_plot(ind='frequency_Dest', interaction_index='amount',\n\nshap_values=shap_values[1], features=df_test_pre)","e1c42da1":"# check that we have all column","da0c02b9":"\n## Step 2: Load the data\nComplete guid to read data : \nNext, we'll load the training and test data.","884195e1":"\n<a id=2><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n\n### Explore the data\/Analysis \n\nWe will analyse the following:\n\n    The target variable\n    \n    Variable types (categorical and numerical)\n    \n    Numerical variables\n        Discrete\n        Continuous\n        Distributions\n        Transformations\n\n    Categorical variables\n        Cardinality\n        Rare Labels\n        Special mappings\n\n    Null Data\n\n    Text data \n    \n    wich columns will we use\n    \n    IS there outliers that can destory our algo\n    \n    IS there diffrent range of data\n    \n    Curse of dimm...\n    \nThis Step is done here : [https:\/\/www.kaggle.com\/bannourchaker\/frauddetection-part1-eda\/edit](http:\/\/)","476d9893":"# Train on all data ","556f87b9":"references: \n\nhttps:\/\/stackoverflow.com\/questions\/66018154\/how-to-understand-shapley-value-for-binary-classification-problem\n\nhttps:\/\/stackoverflow.com\/questions\/65110798\/feature-importance-in-a-binary-classification-and-extracting-shap-values-for-one\n\n\nhttps:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\n\nlime : \nhttps:\/\/towardsai.net\/p\/machine-learning\/lime%e2%80%8a-%e2%80%8aexplaining-any-machine-learning-prediction?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR33ZJo5U0li9AHnjFxNHLI8u0ZJg9Y29pfC-3bn6AYfrDzEH6Hv2V94-ck\n\n\nThis plot is made of many dots. Each dot has three characteristics:\n\n    Vertical location shows what feature it is depicting\n    Color shows whether that feature was high or low for that row of the dataset\n    Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n    \nFor example, the point in the upper  right was for a Transactions  that have  big amount orig, increase  the prediction by +10,.\n\nSome things you should be able to easily pick out:\n\n    The model ignored the cum_count and Frenquency of orig  features.\n    Low values of new balance origi caused higher predictions of fraud , and high  values caused low predictions\n\nIf you look for long enough, there's a lot of information in this graph\n\nThis provides a great overview of the model, but we might want to delve into a single feature. That's where SHAP dependence contribution plots come into play.\n\nDependence plots are another way of interpreting models from a global perspective. It can be plotted for one or more features at the same time. It is basically a scatter plot of features and their shapley values for every observation.","63510a93":"<a id=0><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n\n<center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n    \n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    \n    Tasks:\n    \n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n   Tasks:\n    \n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    \n    Tasks:\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=1><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n\nThere may be two types of questions:\n\n**A.Technical Questions:**\n  \nCan ML be a solution to the problem?\n\n    \n                Do we have THE data?\n                Do we have all necessary related data?\n                Is there enough amount of data to develop algorithm?\n                Is data collected in the right way?\n                Is data saved in the right format?\n                Is the access to information guaranteed?\n\nCan we satisfy all the Business Questions by means of ML?\n\n**B.Business Questions:**\n    \nWhat are the organization's business goals?\n    \n                To reduce cost and increase revenue? \n                To increase efficiencies?\n                To avoid risks? To improve quality?\n    \nIs it worth to develop ML?\n    \n                In short term? In long term?\n                What are the success metrics?\n                Can we handle the risk if the project is unsuccessful?\n    \nDo we have the resources?\n    \n                Do we have enough time to develop ML?\n                Do we have a right talented team?\n\n\n    \nWE are provided a synthetic dataset for a mobile payments application. In this dataset, you are provided the sender and recipient of a transaction as well as whether transactions are tagged as fraud or not fraud. Your task is to build a fraud detection API that can be called to predict whether or not a transaction is fraudulent.\nYou can download the dataset here:https:\/\/www.kaggle.com\/bannourchaker\/frauddetection\n    \nYou are expected to build a REST API that predicts whether a given transaction is fraudulent or not. You are also to assume that the previous API calls are to be stored in order to engineer\nfeatures relevant to finding fraud. The API calls will include the time step of the transaction, so you can assume that a transaction happens sequentially within the same time step.\nFor example, if I make the following transactions in the same time step:  \n    \n![image.png](attachment:c6d681b2-1201-43b2-a5cf-587bd46f7839.png)\n    \nThe first transaction is unlikely to be fraudulent, since anon is initiating a normal transfer.\nHowever, multiple successive transfers of the same amount in the same hour is potentially fraudulent, since anon\u2019s account might have been taken over by a fraudster. On the first API call,your model is unlikely to classify the transaction as fraudulent. However, on the fifth call, it\u2019s likely that it will be tagged as fraudulent.\nThe REST API only has 1 endpoint \/is-fraud that takes in a POST request:\n    \nThe body is expected to receive the following fields(which are also the fields that can be found in your dataset:\nThe following is a sample body when making a POST request to your\n    \n    \n            {\n        \"step\":1,\n        \"type\":\"PAYMENT\",\n        \"amount\":9839.64,\n        \"nameOrig\":\"C1231006815\",\n        \"oldbalanceOrig\":170136.0,\n        \"newbalanceOrig\":160296.36,\n        \"nameDest\":\"M1979787155\",\n        \"oldbalanceDest\":0.0,\n        \"newbalanceDest\":0.0\n        }\n    \n    \nYour API is expected to return a JSON object with a boolean field isFraud. You may find a\nsample response below:\n    \n    {\"isFraud\": true}\n    \n**summary:**\nwe are expecting the following:\n    \n- 1. Deployed REST API:\n    \n    a. As mentioned above, we would need an API that takes in a POST request for the\n    \/is-fraud url and returns a prediction on whether or not a transaction is\n    fraudulent.\n    \n    b. Your REST API should be public for us to call the API and evaluate the accuracy\n    of your model\n    \n    c. Given the nature of the data, your REST API will likely need to take into account\n    previous transactions, so make sure it is able to take note of transactions from\n    your training dataset as well as previous API calls.\n\n- 2. Model\n    \n    a. We are expecting a machine learning model that can correctly classify whether or\n    not a transaction is fraudulent.\n\n**What is the objective of the machine learning model?**\n\nWe aim to predict  the real transactions fraud  and the fraud estimated by our model. We will evaluate model performance with the:\n\n   - F beta score\n    \n   - ROC AUC score\n    \n   - PR AUC score | Average precision\n    \n    \n## Step 1: Import helpful libraries","6e3ed8e2":"<a id=\"Introduction\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Introduction<\/center><\/h3>\n    \nDue to rapid growth in field of cashless or digital \ntransactions, credit cards are widely used in all \naround the world. Credit cards providers are \nissuing thousands of cards to their customers.\n Providers have to ensure all the credit card users \nshould be genuine and real. Any mistake in issuing \na card can be reason of financial crises. \nDue to rapid growth in cashless transaction,\n the chances of number of fraudulent transactions can also increasing.\n A Fraud transaction can be identified by analyzing various\n behaviors of credit card customers from previous \ntransaction history datasets. If any deviation\n is noticed in spending behavior from available patterns, \nit is possibly of fraudulent transaction. \nData mining and machine learning techniques are widely used in credit card \nfraud detection. In this article we are presenting review \nof various data mining and machine learning methods\n which are widely used for credit card fraud detections and  complete this project end to end from Data Understanding to deploy Model via API .  \n    \n    \n ","3f5cfdb0":"Here we use the Tree SHAP implementation integrated into Light GBM to explain the entire train set \n\n\nIf you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in shap.TreeExplainer(my_model). But the SHAP package has explainers for every type of model.\n\n    shap.DeepExplainer works with Deep Learning models.\n    shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n","6dd176cb":"**Num Features**\n\n","e259d643":"# What should we do for each colmun\n\n**Separate features by dtype**\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n\n**Cat Features**\n\n\n\n","f7ad3c9b":"Now that we examined the model from a local perspective, we can move onto the global explanations. Importance of our features are listed as follows.\n\n**Summary Plots**\n\nPermutation importance is great because it created simple numeric measures to see which features mattered to a model. This helped us make comparisons between features easily, and you can present the resulting graphs to non-technical audiences.\n\nBut it doesn't tell you how each features matter. If a feature has medium permutation importance, that could mean it has\n\n    a large effect for a few predictions, but no effect in general, or\n    a medium effect for all predictions.\n\nSHAP summary plots give us a birds-eye view of feature importance and what is driving it. We'll walk through an example plot for the soccer data:","48d18d63":"## Define the model features and target\n\n### Extract X and y ","3cab32a8":"<a id=6><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Deploy<\/center><\/h3>\n\nThe deployment of machine learning models is the process for making models available in production environments, where they can provide predictions to other software systems.\n\n\u25cfOne of the last stages in the Machine Learning Lifecycle.\n\n\u25cfPotentially the most challenging stage.\n\n\u25cfChallenges of traditional software\n\noReliability\noReusability\noMaintainability\noFlexibility\n\n\u25cfAdditional challenges specific to Machine Learning\n\noReproducibility\n\nNeeds coordination of data scientists, IT teams, software developers and business professionals:\n\noEnsure model works reliably\noEnsure model delivers the intended result.\n\n\u25cfPotential discrepancy between programming language in which the model is developed and the production system language.\n\noRe-coding the model extends the project timeline and risks lack of reproducibility\n\nWhy is Model Deployment important?\n\n\u25cfTo start using a Machine Learning Model, it needs to be effectively deployed into production, so that they can provide predictions to other software systems.\n\n\u25cfTo maximize the value of the Machine Learning Model, we need to be able to reliably extract the predictions and share them with other systems.\n\n![image.png](attachment:2de583ad-bc7d-4a96-b0d5-6d89549d96c0.png)\n\n**Research Environment**\n\n\u25cfThe Research Environment is a setting with tools, programs and software suitable for data analysis and the development of machine learning models.\n\n\u25cfHere, we develop the Machine Learning Models and identify their value.\nIts done by a data scientist : i prefer work on jupyter for this phase .\n\n**Production Environment**\n\n\u25cfThe Production Environment is a real-time setting with running programs and hardware setups that allow the organization\u2019s daily operations.\n\n\u25cfIt\u2019s the place where the machine learning models is actually available for business use.\n\n\u25cfIt allows organisations to show clients a \u201clive\u201d service.\nThis job is done by solid sofware+ml engineer+ devops team\n\n![image.png](attachment:691b6fb5-b6cc-499a-ac75-73439be05e2b.png)\n\nwe have 4 ways to deploy models .\nML System Architectures:\n1. Model embedded in application\n![image.png](attachment:b8994531-30eb-4890-b7eb-848eff7843c3.png)\n2. Served via a dedicated service\n![image.png](attachment:6938938c-101f-4bb1-acd5-2786d3618285.png)\n3. Model published as data(streaming)\n![image.png](attachment:0d257ba6-39f9-46e2-b535-bf181fbdfea1.png)\n4. Batch prediction (offline process)\n![image.png](attachment:74857300-8d62-4489-a1a5-1d9e47b531b2.png)\n\nI developed  a baseline how to deploy model using Fastapi+docker on herokou :\n\nhttps:\/\/github.com\/DeepSparkChaker\/FraudDetection_Fastapi\n\n![image.png](attachment:c1aff2f2-ba80-4012-9c04-94348ff3b99b.png)\nComplete deployment of our model after saving best models with best params : \n","7c8226ab":"Two-way PDP (Partial Dependence Plot) shows the relationship between two features based on their shapley values.","b87b70b0":"<a id=7><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Summary<\/center><\/h3> \n\nWe had developed end-to-end machine learning using the CRISP_DM methodology. Work still in progress. Always keep in mind that the data science \/ ML project must be done as a team and iteratively in order to properly exploit our data and add value to our business. Also keep in mind that AI helps you make the decision by using the added value extracted from the data but not the accountability. So we have to keep in mind to always use a composite AI in order to make the final decision.\nDon't forgot to upvote if you find it useful .\n\nhttps:\/\/www.kaggle.com\/bannourchaker\/frauddetection-part3-modeling2-selectbestmodel?scriptVersionId=81276811\n\nfor complete deployement baseline see : \n\nhttps:\/\/github.com\/DeepSparkChaker\/FraudDetection_Fastapi\n\nReferences :\n\nhttps:\/\/developer.nvidia.com\/blog\/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution\/\n\npython guidline : \n\nhttps:\/\/gist.github.com\/sloria\/7001839\n\nfeatures  selections :\n\nhttps:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n\nhttps:\/\/pub.towardsai.net\/feature-selection-and-removing-in-machine-learning-dd3726f5865c\n\nhttps:\/\/www.kaggle.com\/bannourchaker\/1-featuresengineer-selectionpart1?scriptVersionId=72906910\n\nCripspdm :\nhttps:\/\/www.kaggle.com\/bannourchaker\/4-featureengineer-featuresselectionpart4?scriptVersionId=73374083\n\nQuanrile transformer : \n\nhttps:\/\/machinelearningmastery.com\/quantile-transforms-for-machine-learning\/\n\nBest link for all : \n\nhttps:\/\/neptune.ai\/blog\/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\n\ncomplete guide Stacking :\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/08\/ensemble-stacking-for-machine-learning-and-deep-learning\/\n\nhttps:\/\/neptune.ai\/blog\/ensemble-learning-guide\n\nhttps:\/\/www.kaggle.com\/prashant111\/adaboost-classifier-tutorial\n\n\nMissing : \n\nhttps:\/\/www.kaggle.com\/dansbecker\/handling-missing-values\n\nBinning : \n\nhttps:\/\/heartbeat.fritz.ai\/hands-on-with-feature-engineering-techniques-variable-discretization-7deb6a5c6e27\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/getting-started-with-feature-engineering\/\n\nCat :\n\nhttps:\/\/innovation.alteryx.com\/encode-smarter\/\n\nhttps:\/\/github.com\/alteryx\/categorical_encoding\/blob\/main\/guides\/notebooks\/categorical-encoding-guide.ipynb\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/\n\nhttps:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\n\nChoice of kmeans : \n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/k-mean-getting-the-optimal-number-of-clusters\/\n\nImputation : \n\nhttps:\/\/machinelearningmastery.com\/knn-imputation-for-missing-values-in-machine-learning\/\n\nhttps:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/\n\nChoice of  roc vs precssion_recall : \n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nHow to tune for he futur work : \n\nhttps:\/\/www.kaggle.com\/hamidrezabakhtaki\/xgboost-catboost-lighgbm-optuna-final-submission\n\nhttps:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding\n\n\n\nDeploy:\n\nhttps:\/\/towardsdatascience.com\/from-jupyter-notebook-to-deployment-a-straightforward-example-1838c203a437\n\n https:\/\/github.com\/DeepSparkChaker\/Titanic_Deep_Spark\/blob\/main\/app.py\nhttps:\/\/github.com\/Kunal-Varma\/Deployment-of-ML-model-using-FASTAPI\/tree\/2cc0319abbec469010a5139f460004f2a75a7482\nhttps:\/\/realpython.com\/fastapi-python-web-apis\/\n https:\/\/github.com\/tiangolo\/fastapi\/issues\/3373\n https:\/\/www.freecodecamp.org\/news\/data-science-and-machine-learning-project-house-prices\/\nhttps:\/\/github.com\/tiangolo\/fastapi\/issues\/1616\nhttps:\/\/stackoverflow.com\/questions\/68244582\/display-dataframe-as-fastapi-output\nhttps:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers\nhttps:\/\/github.com\/renanmouraf\/data-science-house-prices    \nhttps:\/\/towardsdatascience.com\/data-science-quick-tips-012-creating-a-machine-learning-inference-api-with-fastapi-bb6bcd0e6b01\nhttps:\/\/towardsdatascience.com\/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857\nhttps:\/\/analyticsindiamag.com\/complete-hands-on-guide-to-fastapi-with-machine-learning-deployment\/\n\nhttps:\/\/github.com\/shaz13\/katana\/blob\/develop\/Dockerfile\n\n\nhttps:\/\/github.com\/TripathiAshutosh\/FastAPI\/blob\/main\/main.py\n\nBest practices : \n    \nhttps:\/\/theaisummer.com\/best-practices-deep-learning-code\/    \nhttps:\/\/github.com\/The-AI-Summer\/Deep-Learning-In-Production\/tree\/master\/2.%20Writing%20Deep%20Learning%20code:%20Best%20Practises\n\n Docker :\n \n https:\/\/towardsdatascience.com\/docker-in-pieces-353525ec39b0?fbclid=IwAR102sks2L0vRTde2qz1g4I4NhqXxnoqfV4IFzmZke4DvGcuiuYhj25eVSY\n \nhttps:\/\/github.com\/dkhundley\/ds-quick-tips\/blob\/master\/012_dockerizing_fastapi\/Dockerfile\n\n\n Deploy + scaling :\nhttps:\/\/towardsdatascience.com\/deploying-ml-models-in-production-with-fastapi-and-celery-7063e539a5db\nhttps:\/\/github.com\/jonathanreadshaw\/ServingMLFastCelery\n\nhttps:\/\/github.com\/trainindata\/deploying-machine-learning-models\/blob\/aaeb3e65d0a58ad583289aaa39b089f11d06a4eb\/section-04-research-and-development\/07-feature-engineering-pipeline.ipynb\n\nMl OPS : \nhttps:\/\/www.linkedin.com\/posts\/vipulppatel_getting-started-with-mlops-21-page-tutorial-activity-6863895411837415424-dWMh\/?fbclid=IwAR3Y4clbzujS_s2FFWg3tTYMKaGhh3vo25NUyoVdKHAJ7zynmCTNtzlHQ4M\n\nhttps:\/\/towardsai.net\/p\/machine-learning\/mlops-demystified?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR3MimsSXCFq3GqiLKoaQqXbeb3bkSwKhSkfQSKT_c1gsHDMGSBAv63s7Po\nhttps:\/\/www.youtube.com\/watch?v=9I8X-3HIErc\n\nhttps:\/\/pub.towardsai.net\/deployment-ml-ops-guide-series-2-69d4a13b0dcf\n\nPublish to medium : \n\nhttps:\/\/towardsai.net\/p\/data-science\/how-to-publish-a-jupyter-notebook-as-a-medium-blogpost?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR2-an7kknO3bsI5xjRdjL3jiwuPy7MBN5lVBc6fzx15mGY2iLS5KndCYWc\n\n","b809c21c":"The shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome , and the second array is the list of SHAP values for the positive outcome (Transactions is Fraud). We typically think about predictions in terms of the prediction of a positive outcome, so we'll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\n\n\nShap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).\n\nFor example, consider an ultra-simple model:\ny=4\u2217x1+2\u2217x2\n\nIf x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for x1\n\nwould be 8 (from 4 times 2).\n\nThese are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:\n\nIt's cumbersome to review raw arrays, but the shap package has a nice way to visualize the results.\n\nhttps:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values\n\n**Visualize a single prediction**\n\nNote that we use the \u201cdisplay values\u201d data frame so we get nice strings instead of category codes.","e36c4d6f":"<a id=3><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Preparation<\/center><\/h3>\n\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling.\nThis Step is Done Here :\n[https:\/\/www.kaggle.com\/bannourchaker\/frauddetection-part2-preparation\/edit](http:\/\/)\n\n","c7fd8f83":"# Explain Our model:\nIt's an important phase to try to simplify things to majority of people and exlain your model.\n\n","0cca8272":"Let\u2019s take a look at the decision plot for this transactions . Decision plots help us to see how model finalized its decision. It is very similar to force plots but easier to read. We can order features according to their importance for the model. We can even show more than one observation and highlight the specific ones among them (for example miscancellous predictions can be highlighted among all).","f4a2f1d7":"# Convert Dtypes :","439171f7":"<a id=4><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Modeling<\/center><\/h3>\n\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\nwe have select the preprocess pipe  and we the best model and tune it .\nNow it's time to get our complete evaluation  before we go to production .\n","e3544800":"we can see that this model give more importance to the behavior of client his mean amount and also if he will pay new destination .\n\n**Explainable AI (XAI)** helps build trust and confidence in machine learning models by making them more transparent. XAI is a set of tools and frameworks that can be used to understand and interpret how a machine learning model makes decisions. One useful XAI tool is the SHAP library in Python. This tool allows us to quantify feature\u2019s contribution towards a single prediction as well as predictions at an overall level. The library also comes with aesthetically pleasing easy-to-use visualisations.\n\nLet's see shap value : \n\n**SHAP** is an approach based on a game theory to explain the output of machine learning models. It provides a means to estimate and demonstrate how each feature\u2019s contribution influence the model. SHAP values are calculated for each feature, for each value present, and approximate the contribution towards the output given by that data point. It\u2019s worth noting, that the same values of a feature can contribute different amounts towards an output, depending on the other feature values for that row.\n\nThis method approximates the individual contribution of each feature, for each row of data. It approximates the contribution of that feature by estimating the model output without using it versus all the models that do include it. As this is applied to each row of data in our training set, we can use this to observe both granular (row-level) and higher-level behaviours of certain features.\nShap value helps us quantify feature\u2019s contribution towards a prediction. Shap value closer to zero means the feature contributes little to the prediction whereas shap value away from zero indicates the feature contributes more. \n\nOnce we\u2019ve fitted our model, and defined our SHAP method, we just need to use the below line to generate our first plot.\n\nreference : \nhttps:\/\/towardsdatascience.com\/explaining-scikit-learn-models-with-shap-61daff21b12a\n","51857eaa":"# Advanced Pipe :\nThis pipe include features engineer+ some advanced preprocessing steps for each columns.","288baac8":"The red features in the force plot drives our prediction to be 1: transactions will be fraud  . The bigger the arrow gets, the more effect that feature has on the prediction. Most contributing features are as follows:frequency_dest , oldbalance .\n\nThe blue features indicate features reducing the probability of the transactions will be fraud . like transactions type mean amount ...\n\n**How do you interpret this?**\n\nWe predicted -24,91, whereas the base_value is  -24.6381. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect. Feature values decreasing the prediction are in blue. The biggest impact comes from FranquencyDest and oldorigbalance . Though the trans type and old balncedest value has a meaningful effect decreasing the prediction.\n\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output."}}