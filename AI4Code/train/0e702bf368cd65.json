{"cell_type":{"b60146d1":"code","bb931d25":"code","f4173356":"code","7fa12cb7":"code","a58a0c53":"code","a2294fa0":"code","9b97f254":"code","93100ade":"code","f92408b6":"code","8ba74a94":"code","df44abcd":"code","1c5aefa2":"code","bce8cfeb":"code","638e62d6":"code","0774e52b":"code","9d66ec98":"code","d4881437":"code","d456206f":"code","b93a5500":"code","f768f4e6":"code","da4f7b72":"code","35d5b266":"code","6ac120aa":"code","f5fa1079":"code","2fc4c247":"code","ac5e0c85":"markdown","154c8031":"markdown","d3c775d8":"markdown","2f04ae13":"markdown","ee2f71aa":"markdown","4e4a4b68":"markdown","f5a30718":"markdown","f4cb04da":"markdown","41bfcc96":"markdown","52246d9f":"markdown","e4f67119":"markdown","d48e3c28":"markdown","c3f167b3":"markdown","0de88959":"markdown","b008a1b2":"markdown","76ab106c":"markdown","7aeceb8a":"markdown","c78dcd09":"markdown","7748af83":"markdown"},"source":{"b60146d1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)","bb931d25":"import pandas as pd\n\nspam_collection = pd.read_csv(file_path, sep='\\t', header=None, names=['Label', 'SMS']) #data is separated with 'tabs' so we include 'sep' parameter","f4173356":"print(spam_collection.info(),'\\n')\nprint(spam_collection.describe(),'\\n')\nprint(spam_collection.head())","7fa12cb7":"#spam\/ham percentage\n\nspam_share = spam_collection['Label'].value_counts(normalize=True)\nprint(spam_share * 100)","a58a0c53":"randomized = spam_collection.sample(frac=1, random_state=1) #using 'random_state' parameter to set a random seed and to be able to reproduce\/control the randomization","a2294fa0":"#finding the split-point\n\nsplit_point = int(len(randomized) * .2)\n\ntest = randomized[:split_point].reset_index(drop=True)\ntrain = randomized[split_point:].reset_index(drop=True)","9b97f254":"#checking the spit results\n\nprint('Test:',test.shape,'\\n',test['Label'].value_counts(normalize=True) * 100,'\\n\\nTrain:',train.shape,'\\n',train['Label'].value_counts(normalize=True) * 100)","93100ade":"train['SMS'] = train['SMS'].str.replace('\\W',' ').str.lower()\n\n#check\nprint(train.head(0))","f92408b6":"#splitting messages\n\ntrain['SMS'] = train['SMS'].str.split()","8ba74a94":"#getting all unique words in one list\n\nvocabulary = []\n\nfor row in train['SMS']:\n    for word in row:\n        if not word in vocabulary:\n            vocabulary.append(word)\n\n#voc = list(set(vocabulary))","df44abcd":"#check what we have here\n\nprint(len(vocabulary))","1c5aefa2":"#making dict with unique words and length of SMS column and then filling the 'table'\n\nword_counts_per_sms = {unique_word: [0] * len(train['SMS']) for unique_word in vocabulary}\n\nfor index, sms in enumerate(train['SMS']):\n    for word in sms:\n        word_counts_per_sms[word][index] += 1","bce8cfeb":"#transforming dict into dataframe and adding columns from old training dataframe\n\ntransformed_train = pd.DataFrame(word_counts_per_sms)\ntransformed_train = pd.concat([train, transformed_train], axis=1)","638e62d6":"#quick check\n\ntransformed_train.head()","0774e52b":"#isolating spam and ham messages first\n\nham = transformed_train[transformed_train['Label'] == 'ham']\nspam = transformed_train[transformed_train['Label'] == 'spam']","9d66ec98":"#calculating probabilites\n\np = transformed_train['Label'].value_counts()\np_spam = p['spam']\np_ham = p['ham']","d4881437":"#number of words in all 'spam' messages and in all 'ham'' messages\n\ntransformed_train['n_words'] = transformed_train['SMS'].apply(len) \n\nn_spam = transformed_train[transformed_train['Label'] == 'spam']['n_words'].sum()\nn_ham = transformed_train[transformed_train['Label'] == 'ham']['n_words'].sum()\n\nn_vocabulary = len(vocabulary)","d456206f":"#laplace smoothing\n\nlaplace = 1","b93a5500":"#counting word probabilities for 'spam' and 'ham' messages\n\ndict_spam = {x : 0 for x in vocabulary}\ndict_ham = {x : 0 for x in vocabulary}\n\nfor word in dict_spam:\n    dict_spam[word] = (laplace + spam[word].sum()) \/ (n_spam + laplace * n_vocabulary)\n    \nfor word in dict_ham:\n    dict_ham[word] = (laplace + ham[word].sum()) \/ (n_ham + laplace * n_vocabulary)    ","f768f4e6":"import re\n\ndef classify(message:str):\n    '''\n    labels message with \"Spam\" or \"Ham\"\n    '''\n    message = re.sub('\\W', ' ', message)\n    message = message.lower()\n    message = message.split()\n\n    p_spam_given_message = p_spam\n    p_ham_given_message = p_ham\n    for word in message:\n        if word in dict_spam:\n            p_spam_given_message *= dict_spam[word]\n        if word in dict_ham:\n            p_ham_given_message *= dict_ham[word]\n    \n    print('P(Spam|message):', p_spam_given_message)\n    print('P(Ham |message):', p_ham_given_message)\n\n    if p_ham_given_message > p_spam_given_message:\n        print('Label: Ham')\n    elif p_ham_given_message < p_spam_given_message:\n        print('Label: Spam')\n    else:\n        print('Equal proabilities, have a human classify this!')","da4f7b72":"classify('This is the secret code to wish you luck, Tom')","35d5b266":"classify(\"Sounds good, Tom, i'll be in a winning tram in an hour\")","6ac120aa":"def classify_test(message:str):\n    '''\n    labels message with \"Spam\" or \"Ham\"\n    '''\n    message = re.sub('\\W', ' ', message)\n    message = message.lower()\n    message = message.split()\n\n    p_spam_given_message = p_spam\n    p_ham_given_message = p_ham\n    for word in message:\n        if word in dict_spam:\n            p_spam_given_message *= dict_spam[word]\n        if word in dict_ham:\n            p_ham_given_message *= dict_ham[word]\n    \n    if p_ham_given_message > p_spam_given_message:\n        return 'ham'\n    elif p_spam_given_message > p_ham_given_message:\n        return 'spam'\n    else:\n        return 'needs human classification'    ","f5fa1079":"#implementing into test dataframe\n\ntest['predicted'] = test['SMS'].apply(classify_test)\ntest.head()","2fc4c247":"#calculating accuracy\n\ncorrect = test['Label'] == test['predicted']\n\naccuracy = sum(correct) \/ len(test)\n\nprint('Correct:', correct.sum())\nprint('Incorrect:', len(test) - sum(correct))\nprint('Accuracy: {:.2%}'.format(accuracy)) ","ac5e0c85":"We are done now with cleaning and ready to start defining what spam is.","154c8031":"We can see that dataset consists of 2 columns and 5572 that are cleared (there are 0 null values in both columns).\n\nLabel column has 2 unique values that are 'spam' and 'ham' (non-spam). 'Ham' is widely presented in dataset: 4825 values out of 5572 possible.\n\nSMS column is formed by mostly unique (5169 out of 5572) values that represent messages.\n\nLet's count exact spam percentage (we'll need it lately):","d3c775d8":"Now let's do some testing.","2f04ae13":"# Cleaning data\n\nTo calculate all the probabilities required by the algorithm, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need.\nWe will clean SMS column. Delete the punctuation at first and turn messages into lowercase.","ee2f71aa":"## Getting familiar with dataset\n\nLet's open dataset as pandas dataframe and investigate it a bit.","4e4a4b68":"## Creating the vocabulary\n\nTo proceed we are going to create the vocabulary containing all the unique words in training dataset.","f5a30718":"# Test\/training dataset split\n\nOnce our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n\nA training set, that we'll use to \"train\" the algorithm how to classify messages.\nAnd a test set, that will be used to test how good the spam filter is with classifying new messages.\n\nWe're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n\nThe training set will have 4,458 messages (about 80% of the dataset).\nThe test set will have 1,114 messages (about 20% of the dataset).\n\n\nFor this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80% \u2014 so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).","f4cb04da":"# Classifying a new message\n\nNow that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n\n* Takes in as input a new message (w1, w2, ..., wn)\n* Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n* Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n    * If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n    * If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n    * If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help.","41bfcc96":"## Measuring the Spam Filter's Accuracy\n\n\nAlgorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human). \nNote that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm.\n\nFirst off, we'll change the classify() function that we wrote previously to return the labels instead of printing them. Below, note that we now have return statements instead of print() functions.","52246d9f":"Dataset split was successful as the proportions of spam-ham in new dataframe objexts remained close to inital dataset. Now we'll continue with training dataframe.","e4f67119":"## Calculating parameters \n\nNow that we have the constant terms calculated above, we can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$. Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n\nTo calculate them we will use two last formulas of the given above","d48e3c28":"## Transforming the dataset\n\nUsing the vocabulary of unique words we'are going to transform our training dataset by adding columns for each unique word in the dataset (7753 columns added). The values for each row are going to be the count of the words that are in corresponding SMS message.","c3f167b3":"## Calculating Constants First\n\n\nWe're now done with cleaning the training set, and we can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n![image.png](attachment:image.png)\n\n\n\n","0de88959":"Some probabilities will end up equal 0, because there are words belonging to only one category or even to none. A solution would be [Laplace smoothing](https:\/\/en.wikipedia.org\/wiki\/Additive_smoothing) , which is a technique for smoothing categorical data. A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Consequently, no probability will be zero. this is a way of regularizing Naive Bayes, and when the pseudo-count is zero, it is called Laplace smoothing:\n\n![image.png](attachment:image.png)\n\nWe'll set Laplace smoothing $\\alpha = 1$ \n\nSome of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:\n\n* P(Spam) and P(Ham)\n* NSpam, NHam, NVocabulary","b008a1b2":"We've got 7753 unique words in all messages of our training set.","76ab106c":"\nThe accuracy is close to 98.83%, which is really good. Our spam filter checked through 1,114 messages that it hasn't seen in training, and classified 1,101 correctly. \nThe project goal is accomplished.\n","7aeceb8a":"Seems like our function works. But to tell it for sure, we need to check this accuracy. To do this we can apply the function to our testing dataset.\n","c78dcd09":"To read dataset here on Kaggle let's save the inner path into variable and then read it into dataframe using pandas. Also let's name columns according to dataset documentation and do some exploration after loading it.","7748af83":"# Spam filter using multnomial Naive Bayes theorem\n\nThe goal of this project is to make a working sms spam filter using multinomal Bayes theorem. Let's consider it 'working' when classifying spam will exceed 80% accuracy. \n\nTo train the algorithm, we'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by Tiago A. Almeida and Jos\u00e9 Mar\u00eda G\u00f3mez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/sms+spam+collection).\n"}}