{"cell_type":{"45676d9d":"code","7178d51d":"code","93bba005":"code","9a6f0b6e":"code","5211b94a":"code","5eb587d7":"code","ea8cc150":"code","4bf0d13d":"code","f3c45f86":"code","3a53bcf5":"code","5df7ac70":"code","b49dd1be":"code","12b2aa5d":"code","a49aa4b7":"code","21179bad":"code","82f8164e":"code","03749159":"code","b6b7de99":"code","f683c7e7":"code","2f1506f1":"code","1794771f":"code","da20468f":"code","7feed436":"code","b13a78fa":"code","5be13065":"code","80ba3b9c":"code","32e20095":"code","71d8fa44":"code","ce936241":"code","169a04e0":"code","4781dc65":"code","a56efdb3":"code","d187a8a6":"code","6dee37bd":"code","730b77be":"code","5279570b":"code","d1fcfe2f":"code","123de7c7":"code","9e2d721a":"code","b54e32b5":"code","4ff4cd3c":"code","eb456000":"code","632841d9":"code","63c37928":"code","d938525f":"code","c961c343":"code","f99d8d82":"code","26cf566f":"code","eb3ee57e":"code","d53b10a9":"code","75cd0c99":"code","1e3bfedc":"code","0eaf7310":"code","c4ec5c7b":"code","3068ac70":"code","d5484b94":"code","c58169eb":"code","56a609de":"code","a8ddc612":"code","ac1de0ee":"code","4c9d4827":"code","c60671f4":"code","ee7094ed":"markdown","7239c13b":"markdown","0749b2e5":"markdown","1558d34a":"markdown","53354440":"markdown","f386a1a7":"markdown","5750f611":"markdown","92df136f":"markdown","cd309c88":"markdown","4f68f102":"markdown","ccda6efb":"markdown","1533448c":"markdown","5dd67560":"markdown","60c15159":"markdown","1468253c":"markdown","96ca9b19":"markdown","4e9f6077":"markdown","fae6e03e":"markdown","01935a16":"markdown","9571a0a2":"markdown","5eeb89ed":"markdown","eaf7af0c":"markdown","c11af6bd":"markdown","cf392d04":"markdown","7e6639ea":"markdown","68900b67":"markdown","96586089":"markdown","e6a1ced3":"markdown","b7167c18":"markdown","a5066d2c":"markdown","d19c17ea":"markdown","600d3cec":"markdown","6b859e02":"markdown","23d26746":"markdown","cff17024":"markdown","f2ded3dd":"markdown"},"source":{"45676d9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7178d51d":"# Data analysis and wrangling\nimport pandas as pd    # File read & write operation\nimport numpy as np     # Linear algebra\n\n# Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as stats\nimport statsmodels.formula.api as smf\nfrom pandas.plotting import scatter_matrix\n\n# Machine Learning \nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n","93bba005":"train_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")  # For reading the data\ntest_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")","9a6f0b6e":"print(train_data.head())","5211b94a":"print('Shape of train_data:',train_data.shape)\nprint(train_data.columns.values)       # Checking the column names","5eb587d7":"print(test_data.head())","ea8cc150":"print('Shape of test_data:',test_data.shape)\nprint(test_data.columns.values)       # Checking the column names","4bf0d13d":"combine=[train_data,test_data]\n# Finding the shape of data i.e. number of rows & columns\nprint('length of combined_data:',len(combine))","f3c45f86":"# Trained data information\nprint(train_data.info())","3a53bcf5":"# Test data information\nprint(test_data.info())","5df7ac70":"# Missing values in trained data\nprint(train_data.isnull().sum())","b49dd1be":"# Another way of checking missing values is by using heatmap\nsns.heatmap(train_data.isnull(),yticklabels=False)","12b2aa5d":"def missing_percent(df):\n    \"\"\"This function will calculate the percentage of missing values in each column. The input will be dataframe and \n    the output will show as two columns\"\"\"\n    total=df.isnull().sum().sort_values(ascending=False)\n    percentage=round(total*100\/len(df),2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","a49aa4b7":"missing_percent(train_data)","21179bad":"sns.set_style(\"whitegrid\")\nsns.boxplot(x='Pclass',y='Age',data=train_data,)","82f8164e":"def impute_age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else: \n            return 24\n    else:\n        return Age","03749159":"train_data['Age']=train_data[['Age','Pclass']].apply(impute_age,axis=1)\nsns.heatmap(train_data.isnull(),yticklabels=False)","b6b7de99":"# Missing values in test data\nprint(test_data.isnull().sum())","f683c7e7":"missing_percent(test_data)","2f1506f1":"sns.set_style(\"whitegrid\")\nsns.boxplot(x='Pclass',y='Age',data=test_data)","1794771f":"def impute_age_test(cols1):\n    Age=cols1[0]\n    Pclass=cols1[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 42\n        elif Pclass==2:\n            return 27\n        else: \n            return 25\n    else:\n        return Age","da20468f":"test_data['Age']=test_data[['Age','Pclass']].apply(impute_age_test,axis=1)\nsns.heatmap(test_data.isnull(),yticklabels=False)","7feed436":"train_data.drop('Cabin',axis=1,inplace=True)\ntest_data.drop('Cabin',axis=1,inplace=True)","b13a78fa":"sns.heatmap(train_data.isnull(),yticklabels=False)","5be13065":"train_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)","80ba3b9c":"print(train_data.isnull().sum())\nprint('shape of train data: ',train_data.shape)","32e20095":"test_data['Fare'].fillna(test_data['Fare'].mode()[0],inplace=True)","71d8fa44":"print(test_data.isnull().sum())\nprint('shape of test data: ',test_data.shape)","ce936241":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train_data)","169a04e0":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train_data)","4781dc65":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train_data)","a56efdb3":"train_data[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","d187a8a6":"train_data[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)","6dee37bd":"sns.catplot(x='Sex',y='Survived',hue='Pclass', kind='bar',data=train_data)","730b77be":"train_data[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)","5279570b":"train_data[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)","d1fcfe2f":"sns.distplot(train_data['Age'],kde=True,bins=100,color='darkred')","123de7c7":"sns.countplot(x='SibSp',data=train_data)","9e2d721a":"train_data['Fare'].hist(bins=40)","b54e32b5":"print(train_data.describe())","4ff4cd3c":"print(test_data.describe())","eb456000":"train_data.head()","632841d9":"test_data.head()","63c37928":"pd.get_dummies(train_data['Embarked'],drop_first=True).head()\npd.get_dummies(test_data['Embarked'],drop_first=True).head()","d938525f":"sex_1=pd.get_dummies(train_data['Sex'],drop_first=True)\nembark_1=pd.get_dummies(train_data['Embarked'],drop_first=True)\nsex_2=pd.get_dummies(test_data['Sex'],drop_first=True)\nembark_2=pd.get_dummies(test_data['Embarked'],drop_first=True)","c961c343":"train_data.drop(['Name','Sex','Ticket','Embarked'],axis=1,inplace=True)\ntest_data.drop(['Name','Sex','Ticket','Embarked'],axis=1,inplace=True)","f99d8d82":"train_data=pd.concat([train_data,sex_1,embark_1],axis=1)\ntest_data=pd.concat([test_data,sex_2,embark_2],axis=1)","26cf566f":"train_data.head()","eb3ee57e":"test_data.head()","d53b10a9":"# Correlation \npd.DataFrame(abs(train_data.corr()['Survived']).sort_values(ascending=False))","75cd0c99":"print(train_data.shape)\nprint(test_data.shape)","1e3bfedc":"X_train=train_data.drop('Survived',axis=1)  # Removing the dependent feature from the train data\ny_train=train_data['Survived']\nX_test=test_data  \n# we have to predict y_test (i.e. y_pred)","0eaf7310":"print('Shape of X_train',X_train.shape)\nprint('Shape of y_train',y_train.shape)\nprint('Shape of X_test',X_test.shape)","c4ec5c7b":"# Logistic Regression\n\nlogreg=LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred_lr=logreg.predict(X_test)\naccuracy_logistic=logreg.score(X_train, y_train)\nprint('Accuracy score by using logistic regression is:',round(accuracy_logistic,2))","3068ac70":"# Decision Tree\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred_dt = dtc.predict(X_test)\naccuracy_decision=dtc.score(X_train, y_train)\nprint('Accuracy score by using Decision tree is:',round(accuracy_decision,2))","d5484b94":"# Random Forest\n\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\naccuracy_randomforest=rf.score(X_train, y_train)\nprint('Accuracy score by using Random forest is:',round(accuracy_randomforest,2))","c58169eb":"# Gaussian naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred_gn = gaussian.predict(X_test)\naccuracy_gaussian=gaussian.score(X_train, y_train)\nprint('Accuracy score by using Gaussian is:',round(accuracy_gaussian,2))","56a609de":"# Support Vector Machines\n\nsvc=SVC()\nsvc.fit(X_train,y_train)\ny_pred_sv=svc.predict(X_test)\naccuracy_svc=svc.score(X_train, y_train)\nprint('Accuracy score by using SVM is:',round(accuracy_svc,2))","a8ddc612":"# K-Nearest Neighbor\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred_kn = knn.predict(X_test)\naccuracy_knn=knn.score(X_train, y_train)\nprint('Accuracy score by using KNN is:',round(accuracy_knn,2))","ac1de0ee":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','Support Vector Machines', 'KNN'],'Score': [accuracy_logistic, accuracy_decision, accuracy_randomforest,accuracy_gaussian, accuracy_svc,accuracy_knn]})\nmodels.sort_values(by='Score', ascending=False)","4c9d4827":"submission=pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':y_pred_rf})\nsubmission.to_csv('C:\/\/Users\/\/santh\/\/Kaggle Projects\/titanic_submission1.csv',index=False)","c60671f4":"submission.head()","ee7094ed":"Out of all the independent features, Sex is having highest positive coefficient which means Sex is playing major role to predict the dependent feature (Survived).","7239c13b":"Now we are not observing any missing values in 'Age' column in training data set.","0749b2e5":"Now we will work on 'Embarked' column missing values in the train data.","1558d34a":"This count plot shows \n- around 550 people did not survive and around 340 people survived out of 891 (i.e. Train data)","53354440":"Below function will replace the missing values in 'Age' column based on its class(1 or 2 or 3) with the above mean values.","f386a1a7":"As we discussed earlier, morethan 77% of the data is missing in 'Cabin' column from both Train & Test data sets. So, we are going to remove this feature from the data sets.","5750f611":"Now we will work on 'Fare' column missing value in the test data.","92df136f":"#### Type of features\n- a) Categorical\n     - Nominal (Values serves as labels, which do not have any meaningful order)\n         - Cabin\n         - Embarked (C,Q,S)\n         - Sex (Male, Female)\n     - Ordinal (Values have meaningful order)\n         - Pclass (1,2,3)\n- b) Numeric\n    - Discrete\n         - Passender ID\n         - SibSp\n         - Parch\n         - Survived\n    - Continous\n         - Age\n         - Fare\n- c) Alphanumeric or text features\n    - Name\n    - Ticket","cd309c88":"Above box plots of the test data will helped me, to identify what is the mean age in each of the 'Pclass'.\n- Mean age in Pclass 1 is 42.\n- Mean age in Pclass 2 is 27.\n- Mean age in Pclass 3 is 25.","4f68f102":"There are missing values in below features: <br>\n- Cabin    : 327 (78.2% of test data)\n- Age      : 86  (20.6% of test data)\n- Fare     : 1   (0.2% of test data)\n\nAlmost 78% of the cabin column having missing data. So, we are going to remove this column from the test data set. \nWe are going to fill the missing values in 'Age' column and in the 'Fare' column we have only one missing values. So, I'm going replace this value with the most common occurance value(i.e. with mode) of the fare column.","ccda6efb":"Our problem will come under classification (i.e. have to predict whether passenger survived or not). I'm considering few of the classification algorithms for predicting the results.","1533448c":"This count plot shows\n- More number of Pclass 3 (Third class) passengers did not survive(around 370 passengers died).\n- More number of Pclass 1 (First class) passengers survived(around 140 passengers saved)","5dd67560":"There are missing values in below features: <br>\n- Cabin    : 687  (77% of train data)      \n- Age      : 177  (19.8% of train data)\n- Embarked : 2    (0.2% of train data)\n\nAlmost 77% of the cabin column having missing data. So, we are going to remove this column from the training data set. \nWe are going to fill the missing values in 'Age' column and in the 'Embarked' column we are going to replace with most common occurance (i.e.mode).","60c15159":"### 4. Data preprocessing\n#### 4a) Dealing with missing values","1468253c":"First we will try to fill the missing values in train data set, then we will repeat the same steps in test data set.","96ca9b19":"Hello Kagglers,\nThis is my first Kaggle project, I'm super excited to share my knowledge with the kaggle community and also looking for your valuable suggestions on my notebook. \nI prepared this notebook in such a way that every beginner will understand the code very easily. Lets go through the code.","4e9f6077":"### 8. Modeling and Prediction","fae6e03e":"Now we are not observing any missing values in 'Age' column in test data set.","01935a16":"### 2. Import necessary libraries and datasets","9571a0a2":"- 60% of passengers survived who is having 3 number of parents\/children aboard.\n- No passenger survived with morethan 4 number of parents\/children aboard.","5eeb89ed":"- 74.2% of females survived.\n- 18.8% of males survived.","eaf7af0c":"### Workflow \n1. Problem statement\n2. Import necessary libraries and datasets\n3. Explore the data\n4. Data preprocessing\n   - 4a) Dealing with missing values\n5. Identify the patterns & relations\n6. Statistical overview\n7. Feature Engineering\n   - 7a) Converting categorical data into numerical data\n8. Modelling and Prediction\n9. Submit the results","c11af6bd":"This count plot shows \n- among all the males 470 did not survived and around 110 survived. \n- Out of all female around 85 did not survive and around 235 survived out of 891 (i.e. Train data).","cf392d04":"- 53.6% of passengers survived who is having 1 Sibling\/Spouse.\n- No passenger survived with greater than 5 number of Sibling\/Spouse.","7e6639ea":"- 62.9% of Pclass 1 (First class) passengers survived.\n- 47.3% of Pclass 2 (Second class) passengers survived.\n- 24.2% of Pclass 3 (Third class) passengers survived.","68900b67":"Train data we have 12 columns and in test data we have only 11 columns (In test data 'Survived' column is not there, We need to predict this feature with our Machine Learning model).","96586089":"The above score will tell us how confident our model is on the training data. Decision tree, Random forest is giving very good score. So, I'm going to use Random Forest algorithm prediction results on test data for submitting the results.","e6a1ced3":"### 5. Identify the patterns and relations","b7167c18":"Do not worry about the above lines of code, which will be automatically generated by kaggle when you click on \"New notebook\".","a5066d2c":"- Around 38% of male & 95% of female survived in Pclass 1 (First class).\n- Around 16% of male & 90% of female survived in Pclass 2 (Second class).\n- Around 14% of male & 50% of female survived in Pclass 3 (Third class).","d19c17ea":"### 1.Problem Statement\nEvery one of us know that the Titanic ship hit an iceburg and sank in the early morning on 15th April 1912, resulting 1503 people deaths out of 2208. By watching the Titanic movie, we undertstood that, ship crew members tried to save women,childern first by using lifeboats then they gave priority to the rich people. This truth we can check after solving the project by using different machine learning algorithms. Our problem statement was to predict whether a particular passenger is survived or not.  ","600d3cec":"### 3. Explore the data","6b859e02":"### 9. Submit the results","23d26746":"Above box plot of the training data will helps us, to identify what is the mean age in each of the 'Pclass'.\n- Mean age in Pclass 1 is 37.\n- Mean age in Pclass 2 is 29.\n- Mean age in Pclass 3 is 24.","cff17024":"### 7. Feature Engineering\n#### 7a) Converting Categorical Features","f2ded3dd":"### 6. Statistical Overview"}}