{"cell_type":{"c12a940b":"code","b0878d90":"code","80bb8847":"code","46d8f135":"code","9805c680":"code","f8be4243":"code","40bb114c":"code","44f32814":"code","7f34049b":"code","8ad399f8":"code","0782ede4":"code","ce14a325":"code","defcbb72":"code","04086500":"code","ba8fe771":"code","43420b9a":"code","bccf4f66":"code","69a08520":"code","a98db8b3":"markdown","4f75163d":"markdown","e8a1ba82":"markdown","c5c922f6":"markdown","4b210ae2":"markdown","d4af565c":"markdown","6122e869":"markdown","b38c7fd3":"markdown","02f0363f":"markdown","02c264b0":"markdown","a0dde9a1":"markdown","c81f6bbc":"markdown"},"source":{"c12a940b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint('Files in folder: ')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nstroke_data = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\nprint('Data shape: ', stroke_data.shape)\nstroke_data.drop(columns='id',inplace=True)\nstroke_data.head()","b0878d90":"#checking the number of missing values and unique values in target column\nprint('Number of missing values: {}'.format(stroke_data.stroke.isnull().sum()))\nstroke_data.stroke.value_counts()","80bb8847":"stroke_data.stroke.plot.hist();","46d8f135":"\ndef missing_values_table(df):\n        \n        mis_val = df.isnull().sum()                                   # Total missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)           # Percentage of missing values\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) # Make a table with the results\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})    # Rename the columns\n        \n        \n        mis_val_table_ren_columns = mis_val_table_ren_columns[        # Sort the table by percentage of missing descending\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n\n        return mis_val_table_ren_columns\n    \nmissing_values = missing_values_table(stroke_data)\nmissing_values.head(20)","9805c680":"stroke_data['missing_bmi'] = np.where(stroke_data.bmi.isna(),1,0)\nbmi_median = stroke_data.bmi.median()\nstroke_data['bmi'] = np.where(stroke_data.bmi.isna(),bmi_median,stroke_data.bmi)\nprint('avg:       {},\\nstroke:    {} +- {}, \\nno_stroke: {} +- {}'.format(round(stroke_data.bmi.mean(),1),\n                                                             round(stroke_data[stroke_data.stroke==1].bmi.mean(),1),\n                                                             round(stroke_data[stroke_data.stroke==1].bmi.std(),1),\n                                                             round(stroke_data[stroke_data.stroke==0].bmi.mean(),1),\n                                                             round(stroke_data[stroke_data.stroke==1].bmi.std(),1),\n               )\n     )","f8be4243":"stroke_data.dtypes.value_counts()","40bb114c":"stroke_data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n","44f32814":"cat_value_columns = stroke_data.select_dtypes('object').columns.tolist()\nfor column in cat_value_columns:\n    print('{} unique values in column  {:15s}: {}'.format(\n        stroke_data[column].nunique(),\n        column,\n        stroke_data[column].unique())\n         )","7f34049b":"stroke_data.gender.value_counts()","8ad399f8":"#numpy where functions returns elements chosen depending on condition np.where(condition, value if true, value if false)\n\nstroke_data['ever_married']=np.where(stroke_data['ever_married']=='Yes',1,0)      #Yes->1, No->0\n\nstroke_data['gender']=np.where(stroke_data['gender']=='Female',1,0)               #Female->1, Male and other->0\n\nstroke_data['Residence_type']=np.where(stroke_data['ever_married']=='Urban',1,0)  #urban->1, Rural->0","0782ede4":"stroke_data = pd.get_dummies(stroke_data)\nstroke_data.head()\n","ce14a325":"#changing continous values to 0,1 range\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(X_train)\nX_train=scaler.transform(X_train)\nscaler.fit(X_test)\nX_test=scaler.transform(X_test)\n","defcbb72":"from sklearn.model_selection import train_test_split\nX, y = stroke_data.drop('stroke', axis=1), stroke_data['stroke']\nprint(X.shape,y.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nstroke_data.head()","04086500":"from sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nlog_reg = LogisticRegression(C = 0.0001)\nlog_reg.fit(X_train, y_train)\n\nlog_reg_pred = log_reg.predict_proba(X_test)[:, 1]\nrounded_log_reg =  [np.round(x) for x in log_reg_pred]\nprint('Roc auc score for simple logistic regression is {:.1%}'.format(roc_auc_score(y_test, log_reg_pred)))\nprint(confusion_matrix(y_test, rounded_log_reg))","ba8fe771":"importance = abs(log_reg.coef_)\nimportance = 100*importance\/(importance.sum())\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=X.columns,y=abs(importance[0])))\nfig.update_layout(title='The Importance Of Features On Our Prediction',xaxis_title='Model',yaxis_title='Weight Percentage')\nfig.show()","43420b9a":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier()  # hyperparameter tuning needs to be done later \nrandom_forest.fit(X_train,y_train)\n\nrd_forest_pred = random_forest.predict_proba(X_test)[:, 1]\nrounded_rd_forest =  [np.round(x) for x in rd_forest_pred]\nprint('Roc auc score for random forest {:.1%}'.format(roc_auc_score(y_test, rd_forest_pred)))\nprint(confusion_matrix(y_test, rounded_rd_forest))","bccf4f66":"from xgboost import XGBClassifier\nxgb_cls = XGBClassifier(scale_pos_weight=1\/y_test.mean(),#scale_pos_weight class ratio negativee to positive\n                       early_stopping_rounds=2,\n                       colsample_bytree= 0.1,\n                       max_depth = 4) \nxgb_cls.fit(X_train, y_train)\n \nXGB_pred = xgb_cls.predict_proba(X_test)[:, 1]\nrounded_XGB_predt =  [np.round(x) for x in XGB_pred]\nprint('Roc auc score for XGB {:.1%}'.format(roc_auc_score(y_test, XGB_pred)))\nprint(confusion_matrix(y_test, rounded_XGB_predt))\nprint(xgb_cls.get_params())","69a08520":"#print('Roc auc score for log_reg_pred    {:.1%}'.format(roc_auc_score(y_test, log_reg_pred)))\nprint('Roc auc score for rd_forest_predt {:.1%}'.format(roc_auc_score(y_test, rd_forest_pred)))\nprint('Roc auc score for XGB_pred        {:.1%}'.format(roc_auc_score(y_test, XGB_pred)))\nX, y = stroke_data.drop('stroke', axis=1), stroke_data['stroke']\nprint(':::: Results on original data ::::')\n#print('Roc auc score for log_reg_pred    {:.1%}'.format(roc_auc_score(y_train, log_reg.predict_proba(X_train)[:, 1])))\nprint('Roc auc score for rd_forest_predt {:.1%}'.format(roc_auc_score(y_train, random_forest.predict_proba(X_train)[:, 1])))\nprint('Roc auc score for XGB_pred        {:.1%}'.format(roc_auc_score(y_train, xgb_cls.predict_proba(X_train)[:, 1])))","a98db8b3":"Checking for values of gender","4f75163d":"We can see that the random forest and xgb models are overfited\n","e8a1ba82":"## Random Forest model ","c5c922f6":"## Train-Test split\n\nIn order to evaluate the ML model we have to split data to Training dataset ( used to fit the ML model) and Test dataset (to evaluate the ML model).\nThe goal is to evaluate the performance of the ML model on data not used to train the model, which is what we expect to use modell in practice.\nCommon split percentages range from 80:20 to 50:50, bigger percentage being the train set.\nAdditionally data needs to have defined input values (X) and  output values (Y)\n\n[More on Train-test split](https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/)","4b210ae2":"We will change yes and no in 'ever_married' to 1 and 0, 1 being yes and 0 being no.\nIn 'Residence_type' column we will rename to 'Urban_residance' and change value to 1 and 0, 1 being urban and 0 being rural.    \nColumn gender we will change Female to 1 and the rest to 0.\nColumns work_type and smoking status we will one hot encode.","d4af565c":"## INTRODUCTION","6122e869":"We will create column 'missing_bmi' with values 1 and 0, and replace missing values with median value.  \nSome models can handle missing values without the need for imputation.   \nThere is also an option to drop column bmi (or droping columns with big percentage of missing values in general), but it is not the best practice, and it is not possible to know will it be helpfull to the model. \n\n","b38c7fd3":"Now we check for number of unique values of categorical features ","02f0363f":"## Target analysis \nTarget is the value we are tryin to predict, in our case 'stroke' column","02c264b0":"## Column data types\n'int64' and 'float64' are numeric values which can be discrete or continuous. 'object' columns are known as categorical features.\n","a0dde9a1":"From this we can see that this is imbalanced class problem.\n\n### Examine missing values \n\nChecking for number and % of missing values ","c81f6bbc":"We will change continuoous variables to range [ 1,0 ]"}}