{"cell_type":{"40b900e4":"code","2fb47987":"code","e9aea73a":"code","7e2ba5cf":"code","33068c0c":"code","5f68f22d":"code","d7737152":"code","545aed1f":"code","004daacf":"code","29e570b2":"code","409b5d65":"code","7971b2e4":"code","3a486e05":"code","7017b5cf":"code","0ca218bc":"code","3c998040":"code","7b8971a9":"code","1cbed19c":"code","24944fb4":"code","2a7cb2c7":"code","b812e31e":"code","8f3cfc0f":"code","8bdfe6b1":"markdown","7b683c4b":"markdown","72fb5e0d":"markdown","3e5083b2":"markdown","031a502e":"markdown","5ea9ac02":"markdown","d365ea90":"markdown","59dc05aa":"markdown","35c6fbd0":"markdown","60a83466":"markdown","f14aca5e":"markdown","da2af441":"markdown"},"source":{"40b900e4":"#Preliminaries\n\nfrom __future__ import absolute_import, division, print_function  # Python 2\/3 compatibility\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport seaborn as sns\n\n%matplotlib inline","2fb47987":"## Import Keras objects for Deep Learning\n\nfrom keras.models  import Sequential, K\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.optimizers import Adam, SGD, RMSprop","e9aea73a":"## Load in the data set (Internet Access needed)\n\ndiabetes_df = pd.read_csv('..\/input\/diabetes.csv')","7e2ba5cf":"# Take a peek at the data -- if there are lots of \"NaN\" we may have internet connectivity issues\nprint(diabetes_df.shape)\ndiabetes_df.sample(5)","33068c0c":"X = diabetes_df.iloc[:, :-1].values\ny = diabetes_df[\"Outcome\"].values","5f68f22d":"# Split the data to Train, and Test (75%, 25%)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)","d7737152":"np.mean(y), np.mean(1-y)","545aed1f":"## Train the RF Model\nrf_model = RandomForestClassifier(n_estimators=200)\nrf_model.fit(X_train, y_train)","004daacf":"# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\ny_pred_class_rf = rf_model.predict(X_test)\ny_pred_prob_rf = rf_model.predict_proba(X_test)\n\n\nprint('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\nprint('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))","29e570b2":"def plot_roc(y_test, y_pred, model_name):\n    fpr, tpr, thr = roc_curve(y_test, y_pred)\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(fpr, tpr, 'k-')\n    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n    ax.grid(True)\n    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n\n\nplot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')","409b5d65":"## First let's normalize the data\n## This aids the training of neural nets by providing numerical stability\n## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n\n\nnormalizer = StandardScaler()\nX_train_norm = normalizer.fit_transform(X_train)\nX_test_norm = normalizer.transform(X_test)","7971b2e4":"# Define the Model \n# Input size is 8-dimensional\n# 1 hidden layer, 12 hidden nodes, sigmoid activation\n# Final layer has just one node with a sigmoid activation (standard for binary classification)\n\nmodel_1 = Sequential([\n    Dense(12, input_shape=(8,), activation=\"relu\"),\n    Dense(1, activation=\"sigmoid\")\n])","3a486e05":"#  This is a nice tool to view the model we have created and count the parameters\n\nmodel_1.summary()","7017b5cf":"# Fit(Train) the Model\n\n# Compile the model with Optimizer, Loss Function and Metrics\n# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n\nmodel_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\nrun_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n# the fit function returns the run history. \n# It is very convenient, as it contains information about the model fit, iterations etc.","0ca218bc":"## Like we did for the Random Forest, we generate two kinds of predictions\n#  One is a hard decision, the other is a probabilitistic score.\n\ny_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\ny_pred_prob_nn_1 = model_1.predict(X_test_norm)","3c998040":"# Let's check out the outputs to get a feel for how keras apis work.\ny_pred_class_nn_1[:10]","7b8971a9":"y_pred_prob_nn_1[:10]","1cbed19c":"# Print model performance and plot the roc curve\nprint('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\nprint('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n\nplot_roc(y_test, y_pred_prob_nn_1, 'NN')","24944fb4":"run_hist_1.history.keys()","2a7cb2c7":"fig, ax = plt.subplots()\nax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\nax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\nax.legend()","b812e31e":"## Note that when we call \"fit\" again, it picks up where it left off\nrun_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)","8f3cfc0f":"n = len(run_hist_1.history[\"loss\"])\nm = len(run_hist_1b.history['loss'])\nfig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\nax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n\nax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\nax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n\nax.legend()","8bdfe6b1":"In this kernel we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n\n##UCI Pima Diabetes Dataset\n\n* Kaggle Repositiory (https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)\n\n\n### Attributes: (all numeric-valued)\n   1. Number of times pregnant\n   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n   3. Diastolic blood pressure (mm Hg)\n   4. Triceps skin fold thickness (mm)\n   5. 2-Hour serum insulin (mu U\/ml)\n   6. Body mass index (weight in kg\/(height in m)^2)\n   7. Diabetes pedigree function\n   8. Age (years)\n   9. Class variable (0 or 1)","7b683c4b":"There may be some variation in exact numbers due to randomness, but we should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC.","72fb5e0d":"Let's plot the training loss and the validation loss over the different epochs and see how it looks.","3e5083b2":"Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs.","031a502e":"Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n## Baseline performance using Random Forest\nTo begin, and get a baseline for classifier performance:\n1. We train a Random Forest model with 200 trees on the training data.\n2. We calculate the accuracy and roc_auc_score of the predictions.","5ea9ac02":"## Neural Networks to predict Diabetes","d365ea90":"Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training. ","59dc05aa":"\n\n\nLet's fit our model for 200 epochs.","35c6fbd0":"## Building a Single Hidden Layer Neural Network\n\nWe will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes.","60a83466":"The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome.","f14aca5e":"Let's look at the `run_hist_1` object that was created, specifically its `history` attribute.","da2af441":"Hi, I would like to share this kernel with everyone for educational purpose as I am, myself, Machine Learning Enthusiast.\nAny comments will be highly appreciated."}}