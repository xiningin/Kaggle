{"cell_type":{"fc2ccc1a":"code","e880abc6":"code","cbfdc9be":"code","96acebec":"code","78c3256f":"code","81ff4165":"code","715f977a":"code","c24f47b4":"code","bb9de381":"code","6dad0f85":"code","907661bd":"code","1bc81f4f":"code","efa3af8e":"code","850eaad1":"code","58b5f813":"code","5e7b9d63":"code","304ce76d":"code","fe5c8a3e":"code","411dde54":"code","04059173":"code","b3821219":"code","d56a206d":"code","8528c592":"code","351a4412":"code","d31b4e4c":"code","930c29ab":"code","a8fbb3d2":"code","2fa5ee9a":"code","b7abd0ad":"code","beb6690d":"code","dc6938d5":"code","276f50bd":"code","7a8ab935":"code","17fdec05":"code","a4658f08":"code","aa835e92":"code","f00762a1":"code","eb49218e":"code","1ef2d807":"code","33ddfef8":"code","ee7af268":"code","3473d5c5":"code","9d0acdf4":"code","f22d9f4e":"markdown","5a9054c8":"markdown","fcb6bcea":"markdown","3343e065":"markdown","3804d476":"markdown","9b97e339":"markdown","1ce2f1d4":"markdown","ad8fd724":"markdown","0357e63d":"markdown","ac38e7e2":"markdown","0f262c23":"markdown"},"source":{"fc2ccc1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom collections import defaultdict\nimport os\n\ntrainFileDict = defaultdict(list)\nvalFileDict = defaultdict(list)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/chicken-breeds\/Chicken Breeds\/training\/'):\n    baseDir = os.path.basename(dirname) \n    for filename in filenames:\n        trainFileDict[baseDir].append(os.path.join(dirname,filename))\n        \nfor dirname, _, filenames in os.walk('\/kaggle\/input\/chicken-breeds\/Chicken Breeds\/validation\/'):\n    baseDir = os.path.basename(dirname) \n    for filename in filenames:\n        valFileDict[baseDir].append(os.path.join(dirname,filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e880abc6":"import cv2 as cv\n\ntrainImgDict = {className:[cv.imread(imgPath) for imgPath in imgList] for className, imgList in trainFileDict.items()}\nvalImgDict = {className:[cv.imread(imgPath) for imgPath in imgList] for className, imgList in valFileDict.items()}","cbfdc9be":"import matplotlib.pyplot as plt\n\nprint(trainImgDict.keys())\n\n#(imgDict:dict<string:classname, images:image[]>) => (className:string, image:Image)[]\ndef dict2ImgClassPair(imgDict):\n    classImgPairs = []\n    for className, imageList in imgDict.items():\n        for img in imageList:\n            classImgPairs.append((className,img))\n    return classImgPairs\n\nallTrainImages = dict2ImgClassPair(trainImgDict)\nallValImages =  dict2ImgClassPair(valImgDict)\n\nimageDimensions = [img.shape for _, img in allTrainImages]\n\nheightList, widthList, channelList = zip(*imageDimensions)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,2))\n\nax1.hist(heightList)\nax1.set_title(\"Height Distribution\")\nax1.set_xlabel(\"Height(Pixels)\")\n\nax2.hist(widthList)\nax2.set_title(\"Width Distribution\")\nax2.set_xlabel(\"Width (Pixels)\")\n\nax3.hist(channelList)\nax3.set_title(\"Channel Distribution\")\nax3.set_xlabel(\"No. of Colour Channels\")\n\nplt.tight_layout()\nplt.show()\n\nplt.title(\"Chicken without Coffee Or Cheetos\", fontsize=15)\nplt.imshow(cv.cvtColor(trainImgDict['Wyandotte'][0], cv.COLOR_BGR2RGB))","96acebec":"allTrainImagesHLS = [(classname, cv.cvtColor(bgrImg, cv.COLOR_BGR2HLS)) for classname, bgrImg in allTrainImages]\nallValImagesHLS = [(classname, cv.cvtColor(bgrImg, cv.COLOR_BGR2HLS)) for classname, bgrImg in allValImages]\n\nallTrainClassNames = [classname for classname, _ in allTrainImagesHLS]","78c3256f":"trainImageHLSDict = defaultdict(list)\nfor classname, hlsImg in allTrainImagesHLS:\n    trainImageHLSDict[classname].append(hlsImg)\n\nLH_RGB_map = np.zeros((256, 181, 3), dtype=np.uint8)\nfor l in range(0, 256):\n    for h in range(0,181):\n        LH_RGB_map[l,h,:] = np.array([h,l, 255])\n\nLH_RGB_map = cv.cvtColor(LH_RGB_map, cv.COLOR_HLS2RGB)\nplt.figure(figsize=(6,4))\nplt.title(\"RGB Representation of Hue vs. Lightness\\n(Full Saturation)\")\nplt.xlabel(\"Hue\")\nplt.ylabel(\"Lightness\")\nplt.imshow(LH_RGB_map, origin='lower', interpolation='nearest', aspect='auto')\nplt.show()\n    \nhistDict = {className: \n                np.transpose(cv.calcHist(imgList, [0, 2], None, [181, 256], [0, 181, 0, 256])) for \n                className, imgList in trainImageHLSDict.items()}\n\ndef plotHistDict(histogramDict):\n    for className, classHist in histogramDict.items():\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n        fig.suptitle(className).set_size(15)\n        \n        ax1.pcolormesh(classHist)\n        ax1.set_title(\"Raw Histogram\\n(Hue & Lightness)\")\n        ax1.set_xlabel(\"Hue\")\n        ax1.set_ylabel(\"Lightness\")\n        \n        classHistNorm = np.power(classHist \/ np.amax(classHist), 0.25)\n        colorScaled = np.multiply(np.repeat(classHistNorm[:, :, np.newaxis], 3, axis=2), LH_RGB_map).astype(np.uint8)\n        \n        ax2.set_title(\"Color View\")\n        ax2.imshow(colorScaled, origin='lower', interpolation='nearest', aspect='auto')\n        plt.tight_layout\n        plt.show()\n        \nplotHistDict(histDict)","81ff4165":"contour = np.array([[160, 45],[220, 70],[25, 90],[25, 35]])\n\nLH_RGB_map_filtered = np.zeros((256, 181, 3), dtype=np.uint8)\nfor l in range(0, 256):\n    for h in range(0,181):\n        HSLColour = np.array([h,l, 255]) if (cv.pointPolygonTest(contour, (l,h), False) < 0) else np.array([0,0, 255])\n        LH_RGB_map_filtered[l,h,:] = HSLColour\n\nLH_RGB_map_filtered = cv.cvtColor(LH_RGB_map_filtered, cv.COLOR_HLS2RGB)\nplt.figure(figsize=(6,4))\nplt.title(\"Green Colour Mask\")\nplt.imshow(LH_RGB_map_filtered, origin='lower', interpolation='nearest', aspect='auto')\nplt.show()","715f977a":"def removeGreenFromHistogram(histogram):\n    histCopy = np.copy(histogram)\n    nRows, nCols = histogram.shape\n    for lightness in range(nRows):\n        for hue in range(nCols):\n            isGreen = (cv.pointPolygonTest(contour, (lightness,hue), False) > 0)\n            if isGreen:\n                histCopy[lightness,hue] = 0\n    return histCopy\n\nfilteredHistDict = {className: \n                      removeGreenFromHistogram(hist) \n                      for className, hist in histDict.items()}\n\nplotHistDict(filteredHistDict)","c24f47b4":"import math\n#color classifier -> convert hist. to prob. map -> for each image, count no. of non-green pixels, then get avg pixel probability\n#laplacianSmoothingFactor = probability of any given colour occuring in an image once.\ndef histogram2ProbabilityMap(histogram, nImages, laplacianSmoothingFactor):\n    perImageProbability = histogram \/ nImages\n    #Models colours that don't appear as instead having a miniscule chance of appearing\n    nPixels = histogram.size\n    smoothedPerImageProbabilty = (perImageProbability + laplacianSmoothingFactor)  \/ ((1 + laplacianSmoothingFactor) * nPixels)\n    return smoothedPerImageProbabilty\n \ncolourProbDict = {className: \n                      histogram2ProbabilityMap(hist, len(trainImageHLSDict[className]), 0.05) \n                      for className, hist in filteredHistDict.items()}\n\n#Essentially a NB classifier\ndef calculateCumProb(npHLSImg, colourProbDict):\n    resultDict = {}\n    hlsImg = npHLSImg.tolist()\n    for className, npHLProbabilityMap in colourProbDict.items():\n        hlProbabilityMap = npHLProbabilityMap.tolist()\n        row, col, _ = npHLSImg.shape\n        pixelProbs = [[0] * col for _ in range(row)]\n        for r in range(row):\n            for c in range(col):\n                [h, l, _] = hlsImg[r][c]\n                #print(r,c, l,h)\n                pixelProbs[r][c] = hlProbabilityMap[l][h]\n        resultDict[className] = np.sum(np.log(pixelProbs))\n    return resultDict\n\ndef getHighestScoringClass(probabilityResultDict):\n    pairWithMaxProb = max(probabilityResultDict.items(), key=lambda kvPair:kvPair[1])\n    return pairWithMaxProb[0]\n\nprint(allValImagesHLS[0][0])\ncalculateCumProb(allValImagesHLS[0][1], colourProbDict)","bb9de381":"import multiprocessing\n\ndef classifyImage(hlsImg):\n    return getHighestScoringClass(calculateCumProb(hlsImg,colourProbDict))\n\npool = multiprocessing.Pool()\ncolourBasedPredictions= pool.map(classifyImage, [hlsImg for  _, hlsImg in allValImagesHLS])\npool.close()\n\ngroundTruth = [className for className, _ in allValImagesHLS]","6dad0f85":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\n\ndef showClassificationReport(groundTruth, predictions):\n    print(\"Accuracy={:.2f}%\".format(accuracy_score(groundTruth, predictions) * 100))\n    print(classification_report(groundTruth, predictions))\n    \n    labels = [className for className, _  in colourProbDict.items()]\n    cm = confusion_matrix(groundTruth, predictions, labels=labels)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    fig, ax = plt.subplots(figsize=(10,10))\n    disp.plot(ax=ax)\n\nshowClassificationReport(groundTruth, colourBasedPredictions)","907661bd":"H_RGB_map = np.zeros((1, 181, 3), dtype=np.uint8)\nfor h in range(0,181):\n        H_RGB_map[0, h,:] = np.array([h,122, 255], dtype=np.uint8)\n\nH_RGB_map = cv.cvtColor(H_RGB_map, cv.COLOR_HLS2RGB)\nplt.figure(figsize = (12,1))\nplt.title(\"Hue\\n(50% Lightness, 100% Saturation)\")\nplt.xlabel(\"Hue(Degrees * 0.5)\")\nplt.imshow(H_RGB_map, origin='lower', interpolation='nearest', aspect='auto')\nplt.show()\n    \nhueHistDict = {className: \n                np.transpose(cv.calcHist(imgList, [0], None, [181], [0, 181])).reshape((-1,)) for \n                className, imgList in trainImageHLSDict.items()}\n\ndef plotHueHistDict(hueDict):\n    def color2Rgb(npRgbColour):\n        colorNorm = npRgbColour * (1\/255) #to 0-1\n        return (colorNorm[0], colorNorm[1], colorNorm[2])\n    \n    colourList = [color2Rgb(H_RGB_map[0, hue, :]) for hue in range(0, 181)]\n    for className, classHueHist in hueDict.items():\n        fig, (ax1) = plt.subplots(1, 1, figsize=(12,4))\n        \n        ax1.bar(np.arange(0, 181), classHueHist, color=colourList)\n        ax1.set_title(\"{0}\\nRaw Histogram\\n(Hue)\".format(className))\n        ax1.set_xlabel(\"Hue\")\n        ax1.set_ylabel(\"Counts\")\n        \n        plt.tight_layout\n        plt.show()\n        \nplotHueHistDict(hueHistDict)","1bc81f4f":"greenHueMask = np.interp(np.arange(0, 181), [0, 30, 40, 70, 80,180],[1,1,0,0,1,1])\n\ndef removeGreenAndNormalizeHist(histogram, hueSmoothingFactor):\n    greenRemoved = (histogram * greenHueMask)\n    smoothed = (greenRemoved \/ np.sum(greenRemoved)) + (hueSmoothingFactor \/ 180)\n    normalized = smoothed \/ np.sum(smoothed)\n    return normalized\n        \nhueHistDictNormalized = {\n    className: \n        removeGreenAndNormalizeHist(bins, 1) for \n        className, bins in hueHistDict.items()}\n\nplotHueHistDict(hueHistDictNormalized)","efa3af8e":"def calculateCumProb1D(npHLSImg, colourProbDict):\n    resultDict = {}\n    for className, npHLProbabilityMap in colourProbDict.items():\n        hist = np.transpose(cv.calcHist(npHLSImg, [0], None, [181], [0, 181])).reshape((-1,))\n        cumProb = np.sum(hist * npHLProbabilityMap)\n        resultDict[className] = cumProb\n    return resultDict\n\npredictions = calculateCumProb1D(allValImagesHLS[0][1], hueHistDictNormalized)\n\nprint(predictions)\nprint(getHighestScoringClass(predictions))","850eaad1":"def classifyImage1D(hlsImg):\n    return getHighestScoringClass(calculateCumProb1D(hlsImg,hueHistDictNormalized))\n\npool = multiprocessing.Pool()\ncolourBasedPredictions1D = pool.map(classifyImage1D, [hlsImg for  _, hlsImg in allValImagesHLS])\npool.close()\n\ngroundTruth1D = [className for className, _ in allValImagesHLS]\n\nshowClassificationReport(groundTruth1D, colourBasedPredictions1D)","58b5f813":"from sklearn.svm import LinearSVC\n\ndef getHueHistograms(npHLSImg):\n    hist = np.transpose(cv.calcHist(npHLSImg, [0], None, [181], [0, 181])).reshape((-1,))\n    histWithoutGreen = hist #* greenHueMask\n    return histWithoutGreen\n\ndef getInputAndLabels(npHLSImgList):\n    pairList = [(className, getHueHistograms(npHLSImg)) for className, npHLSImg in npHLSImgList]\n    return zip(*pairList)\n\ntrainLabels, trainData = getInputAndLabels(allTrainImagesHLS)\n\nvalLabels, valData =  getInputAndLabels(allValImagesHLS)","5e7b9d63":"hueLinSVC = LinearSVC(random_state=0, tol=1e-5, C=1, dual=False)\nhueLinSVC.fit(trainData, trainLabels)\n\npredictions = hueLinSVC.predict(valData)\n\nshowClassificationReport(valLabels, predictions)","304ce76d":"from skimage.feature import local_binary_pattern\n\ndef getLocalBinPatern(bgrImage, nPoints, radius):\n    grayscale = cv.cvtColor(bgrImage, cv.COLOR_BGR2GRAY)\n    lbp = local_binary_pattern(grayscale, nPoints, radius, method='ror')\n    return lbp\n\nnPoints = 8\nradius = 3\n\nlbp =  getLocalBinPatern(allTrainImages[0][1], nPoints, radius)\nplt.title(\"Visual Representation of Local Binary Pattern values\")\nplt.imshow(lbp)","fe5c8a3e":"allImgLBP = [(classname, getLocalBinPatern(bgrImg, nPoints, radius)) for classname, bgrImg in allTrainImages]\nallValLBP = [(classname, getLocalBinPatern(bgrImg, nPoints, radius)) for classname, bgrImg in allValImages]\n\nimgLBPDict = defaultdict(list)\nfor classname, lbp in allImgLBP:\n    imgLBPDict[classname].append(lbp.astype(np.uint8))","411dde54":"def getMeanHist(imgArr):\n    hist = cv.calcHist(imgArr,[0],None,[256],[0,256]).reshape((-1,)) \/ len(imgArr)\n    return hist\n\ndef plotMeanHistogramsDict(meanHistograms, caption):\n    for classname, hist in meanHistograms.items():\n        plt.figure(figsize=(10,5))\n        plt.title(\"{0}\\n({1})\".format(caption, classname)).set_size(14)\n        nBins = hist.size\n        plt.bar(np.arange(0, nBins), hist)\n        plt.show()\n\nimgMeanLBPHist = {classname: getMeanHist(imgArr) for classname, imgArr in imgLBPDict.items()}\nplotMeanHistogramsDict(imgMeanLBPHist, \"Mean Local Binary Patterns [Grayscale Image]\")","04059173":"classLabels, lbpList2d = zip(*allImgLBP)\nlbpList1d = [img2d.reshape((-1,)) for img2d in lbpList2d]\n\nlinSVC = LinearSVC(random_state=0, tol=1e-5, C=1, dual=False)\nlinSVC.fit(lbpList1d, classLabels)","b3821219":"valClassLabels, vallLBPList2d = zip(*allValLBP)\nvallLBPList2d = [img2d.reshape((-1,)) for img2d in vallLBPList2d]\npredictions = linSVC.predict(vallLBPList2d)\n\nshowClassificationReport(valClassLabels, predictions)","d56a206d":"def extractHueOnly(hlsImage):\n    rows,cols,channels = hlsImage.shape\n    hueChannel = hlsImage[:,:,0].reshape((rows,cols))\n    return hueChannel\n    \ntrainHueOnly = [(classname, extractHueOnly(hlsImg)) for classname, hlsImg in allTrainImagesHLS ]\nvalHueOnly = [(classname, extractHueOnly(hlsImg)) for classname, hlsImg in allValImagesHLS ]","8528c592":"def getHueBasedLocalBinPattern(hueChannel, nPoints, radius):\n    lbp = local_binary_pattern(np.floor_divide(hueChannel, 5), nPoints, radius, method='ror')\n    return lbp\n\nnPoints = 8\nradius = 3\n\nlbp = getHueBasedLocalBinPattern(trainHueOnly[0][1], nPoints, radius)\nplt.title(\"Visual Representation of Local Binary Pattern values (Hue Channel)\")\nplt.imshow(lbp)","351a4412":"allHueLBP = [(classname, getHueBasedLocalBinPattern(hueImg, nPoints, radius)) for classname, hueImg in trainHueOnly]\nallValHueLBP = [(classname, getHueBasedLocalBinPattern(hueImg, nPoints, radius)) for classname, hueImg in valHueOnly]\n             \nhueLBPDict = defaultdict(list)\n             \nfor classname, lbp in allHueLBP:\n    hueLBPDict[classname].append(lbp.astype(np.uint8))\n\nhueMeanLBPHist = {classname: getMeanHist(imgArr) for classname, imgArr in hueLBPDict.items()}    \nplotMeanHistogramsDict(hueMeanLBPHist, \"Mean Local Binary Patterns (Hue Channel)\")","d31b4e4c":"from sklearn.preprocessing import StandardScaler\n\ndef getLabelFeaturePair(pairList):\n    hueClassLabels, hueLbpList2d = zip(*pairList)\n    hueLbpList1d = [img2d.reshape((-1,)) for img2d in hueLbpList2d]\n    return (hueClassLabels, hueLbpList1d)\n\nhueClassLabels, hueLbpList1d = getLabelFeaturePair(allHueLBP)\n\nhueSVC = LinearSVC(random_state=0, tol=1e-5, C=1, dual=False)\nhueSVC.fit(hueLbpList1d, hueClassLabels)\n\nhueValClassLabels, hueValLbpList1d = getLabelFeaturePair(allValHueLBP)\npredictions = linSVC.predict(hueValLbpList1d)\n\ntruthPredPairs = (hueValClassLabels, predictions)\n\nshowClassificationReport(*truthPredPairs)","930c29ab":"plt.title(\"Original Image (Hue Channel Only)\")\nplt.imshow(trainHueOnly[0][1])","a8fbb3d2":"from skimage.feature import blob_doh\n\ndef genHueDist(hueImage, refHue):\n    diff = hueImage.astype(np.int16) - refHue\n    dist = np.abs(np.mod((diff + 90), 180) - 90)\n    hueDistImg = 90 - dist\n    return hueDistImg.astype(np.uint8)\n\ndef threshold(image, threshold):\n    aboveThreshold = image > threshold\n    return image * aboveThreshold\n\nhueDistances = genHueDist(trainHueOnly[0][1], 179)\nplt.title(\"Hue Channel\")\nplt.imshow(hueDistances)\nplt.show()\n\nhueDistancesThreshold = threshold(hueDistances, 82)\nplt.imshow(hueDistancesThreshold)\n\nblobs = blob_doh(hueDistancesThreshold * 2, min_sigma=5, max_sigma=25)\n\nfor y, x, radius in  blobs:\n    blobCircle = plt.Circle((x, y), radius, color='r', fill=False)\n    plt.gca().add_patch(blobCircle)\n\nplt.title(\"Hue Channel [Threshold of 82]\\nDetected Blobs indicated by red circles\")\nplt.show()","2fa5ee9a":"def getBlobRadii(hueImage, referenceHue, nBins, minMaxRadii=(7,21), imgMultiplier=1, thresholdVal=70):\n    hueDistances = genHueDist(hueImage, referenceHue)\n    hueDistancesThreshold = threshold(hueDistances, thresholdVal) #70 out of 90\n    minRadius,maxRadius = minMaxRadii\n    blobs = blob_doh(hueDistancesThreshold * imgMultiplier, min_sigma=minRadius, max_sigma=maxRadius, num_sigma=nBins)\n    radii = [t[2] for t in blobs]\n    return np.histogram(radii, nBins, range=(minRadius, maxRadius))[0] #max image resolution = 256, hence max radii = 128\n\nprint(getBlobRadii(trainHueOnly[0][1], 0, 10, imgMultiplier=2))\n\ndef procGetBlobs(blobParams):\n    hueImage, refHues = blobParams\n    histograms = []\n    for hue in refHues:\n        #histograms.append(getBlobRadii(hueImage, hue, 5))\n        histograms.append(getBlobRadii(hueImage, hue, 3,  imgMultiplier=2))\n    return np.concatenate((*histograms,))","b7abd0ad":"hueBins = list(range(0, 180, 15))\nprint(hueBins)\n\npool = multiprocessing.Pool()\n\ntrainInputList = [(hueImage, hueBins) for _, hueImage in trainHueOnly]\nvalInputList = [(hueImage, hueBins) for _, hueImage in valHueOnly]\ntrainHueBlobDistList = pool.map(procGetBlobs, trainInputList)\nvalHueBlobDistList = pool.map(procGetBlobs, valInputList)\n    \npool.close()","beb6690d":"hueDistDict = defaultdict(list)\nfor classname, dist in zip([classname for classname, _ in trainHueOnly], trainHueBlobDistList):\n    hueDistDict[classname].append(dist)\n\nmeanHueDistDict = {classname: np.mean(histograms, axis=0) for classname, histograms in hueDistDict.items()}\n\nplotMeanHistogramsDict(meanHueDistDict, \"Mean Blob Count per Hue Group\")","dc6938d5":"from sklearn.preprocessing import StandardScaler\n\nblobScaler = StandardScaler()\n\nclassLabels, hueBlobSizes = getLabelFeaturePair(zip([classname for classname, _ in trainHueOnly], trainHueBlobDistList))\nblobSVC = LinearSVC(random_state=0, tol=1e-5, C=0.01, dual=False)\nscaledBlobSizes = blobScaler.fit_transform(hueBlobSizes)\n\nblobSVC.fit(scaledBlobSizes, classLabels)\n\nvalClassLabels, valHueBlobSizes = getLabelFeaturePair(zip([classname for classname, _ in valHueOnly], valHueBlobDistList))\nscaledValBlobSizes = blobScaler.transform(valHueBlobSizes)\npredictions = blobSVC.predict(scaledValBlobSizes)\n\ntruthPredPairs = (valClassLabels, predictions)\n\nshowClassificationReport(*truthPredPairs)","276f50bd":"#Lightness\ndef extractLightnessOnly(hlsImage):\n    rows,cols,channels = hlsImage.shape\n    lightnessChannel = hlsImage[:,:,1].reshape((rows,cols))\n    return lightnessChannel\n\ntrainLightnessOnly = [(classname, extractLightnessOnly(hlsImg)) for classname, hlsImg in allTrainImagesHLS ]\nvalLightnessOnly = [(classname, extractLightnessOnly(hlsImg)) for classname, hlsImg in allValImagesHLS ]","7a8ab935":"#unlike hue, lightness does not wrap around\ndef genLightnessDist(imgLightness, refLightness):\n    return 255 - np.abs(imgLightness.astype(np.int16) - refLightness).astype(np.uint8)\n\ndef getBlobRadiiBasedOnLightness(lightnessImage, refLightness, nBins):\n    lightnessDistances = genLightnessDist(lightnessImage, refLightness)\n    lightnessDistancesThreshold = threshold(lightnessDistances, 225) \n    minRadius,maxRadius = (7, 21)\n    blobs = blob_doh(lightnessDistancesThreshold, min_sigma=minRadius, max_sigma=maxRadius, num_sigma=nBins)\n    radii = [t[2] for t in blobs]\n    return np.histogram(radii, nBins, range=(minRadius, maxRadius))[0] #max image resolution = 256, hence max radii = 128\n\ndef procGetLightnessBlobs(blobParams):\n    lightnessImage, refLightness = blobParams\n    histograms = []\n    for lightness in refLightness:\n        histograms.append(getBlobRadiiBasedOnLightness(lightnessImage, lightness, 3))\n    return np.concatenate((*histograms,))\n\nlightnessImg = threshold(genLightnessDist(trainLightnessOnly[0][1], 100), 225)\n\nplt.title(\"Lighness Channel\")\nplt.imshow(trainLightnessOnly[0][1])\nplt.show()\n\nblobs = blob_doh(lightnessImg, min_sigma=5, max_sigma=25)\nfor y, x, radius in blobs:\n    blobCircle = plt.Circle((x, y), radius, color='r', fill=False)\n    plt.gca().add_patch(blobCircle)\n    \nplt.title(\"Lightness Channel, Threshold=225\\nDetected Blobs indicated in red\")\nplt.imshow(lightnessImg)\nplt.show()","17fdec05":"lightnessBins = list(range(0, 255, 25))[:-1]#[0, 12, 20, 30, 100, 115, 150]\nprint(lightnessBins)\n\npool = multiprocessing.Pool()\n\ntrainInputListL = [(lightnessImg, lightnessBins) for _, lightnessImg in trainLightnessOnly]\nvalInputListL = [(lightnessImg, lightnessBins) for _, lightnessImg in valLightnessOnly]\ntrainLightnessBlobDistList = pool.map(procGetLightnessBlobs, trainInputListL)\nvalLightnessBlobDistList = pool.map(procGetLightnessBlobs, valInputListL)\n    \npool.close()","a4658f08":"lightnessDistDict = defaultdict(list)\nfor classname, dist in zip([classname for classname, _ in trainLightnessOnly], trainLightnessBlobDistList):\n    lightnessDistDict[classname].append(dist)\n\nmeanLightnessDistDict = {classname: np.mean(histograms, axis=0) for classname, histograms in lightnessDistDict.items()}\n\nplotMeanHistogramsDict(meanLightnessDistDict, \"Mean Blob Counts per Lightness Group\")","aa835e92":"blobScalerL = StandardScaler()\n\nclassLabelsL, blobSizesL = getLabelFeaturePair(zip([classname for classname, _ in trainHueOnly], trainLightnessBlobDistList))\nblobSVCL = LinearSVC(random_state=0, tol=1e-5, C=0.0001, dual=False)\nscaledBlobSizesL = blobScalerL.fit_transform(blobSizesL)\n\nblobSVCL.fit(scaledBlobSizesL, classLabelsL)\n\nvalClassLabelsL, valBlobSizesL = getLabelFeaturePair(zip([classname for classname, _ in valHueOnly], valLightnessBlobDistList))\nscaledValBlobSizesL = blobScalerL.transform(valBlobSizesL)\npredictionsL = blobSVCL.predict(scaledValBlobSizesL)\n\ntruthPredPairs = (valClassLabelsL, predictionsL)\n\nshowClassificationReport(*truthPredPairs)","f00762a1":"hueAndLightnessTrain = [np.concatenate(arrays) for arrays in zip(hueBlobSizes, blobSizesL)]\nhueAndLightnessVal = [np.concatenate(arrays) for arrays in zip(valHueBlobSizes, valBlobSizesL)]\n\nhlScaler = StandardScaler()\n\nhueAndLightnessTrainScaled = hlScaler.fit_transform(hueAndLightnessTrain)\n\nhlSVC = LinearSVC(random_state=0, tol=1e-5,C=0.002, dual=False)\nhlSVC.fit(hueAndLightnessTrainScaled, classLabelsL)\n\nhueAndLightnessValScaled = hlScaler.transform(hueAndLightnessVal)\n\npredictionsHL = hlSVC.predict(hueAndLightnessValScaled)\n\ntruthPredPairs = (valClassLabelsL, predictionsHL)\nshowClassificationReport(*truthPredPairs)","eb49218e":"from sklearn.preprocessing import OneHotEncoder\n\nlabelEncoder = OneHotEncoder(handle_unknown='ignore')\n\ndef getImageVecAndOneHotEncoding(classImagePairList, bFitEncoder):\n    classLabelsList, bgrImgList = zip(*classImagePairList) \n    classLabelsListList = [[elem] for elem in classLabelsList]\n    if(bFitEncoder):\n        labelEncoder.fit(classLabelsListList)\n        \n    onehotLabels = labelEncoder.transform(classLabelsListList)\n    bgrImgVec = np.array(bgrImgList) \/ 255.0\n    return bgrImgVec, onehotLabels.toarray()\n    \ntrainVec, trainLabels = getImageVecAndOneHotEncoding(allTrainImages, True)\nvalVec, valLabels = getImageVecAndOneHotEncoding(allValImages, False)","1ef2d807":"print(labelEncoder.categories_)\nprint(trainLabels[0])\nprint(trainVec.shape)","33ddfef8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nkeras.backend.clear_session()\n\nmodel = keras.Sequential(\n    [\n        layers.InputLayer(input_shape=(224, 224, 3)),\n        \n        layers.Conv2D(128, 5, strides=(2,2), activation='relu'),\n        layers.MaxPooling2D(2),\n                \n        layers.Conv2D(196, 5,  activation='relu'),\n        layers.MaxPooling2D(2),\n        \n        layers.Conv2D(384, 3, activation='relu'),   \n        layers.Conv2D(384, 1, activation='relu'),\n        \n        layers.Flatten(),\n        \n        layers.Dense(1024, activation=\"relu\"),\n        layers.Dropout(0.5),\n        layers.Dense(128, activation=\"relu\"),\n        layers.Dense(5, activation=\"sigmoid\")\n    ]\n)\n\nopt = keras.optimizers.Adam(learning_rate=0.0005)\n\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics = ['acc'])\nmodel.summary()","ee7af268":"history = model.fit(\n    x=trainVec,\n    y=trainLabels,\n    validation_data=(valVec, valLabels),\n    epochs=20)","3473d5c5":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Accuracy across Epochs')\nplt.ylabel('Accuracy')\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\nplt.show()","9d0acdf4":"predictions = model.predict(valVec)\npredictionsOneHot = np.array([np.equal(arr, np.amax(arr)).astype(np.float) for arr in predictions])\npredictedClasses = labelEncoder.inverse_transform(predictionsOneHot)\npredictedClasses1D = [subArr[0] for subArr in predictedClasses]\n\ntruthPredPairs = (valClassLabels, predictedClasses1D)\nshowClassificationReport(*truthPredPairs)","f22d9f4e":"# Approach 5: Local Binary Pattern Histograms (Hue Channel) with SVM\n\nLBPs are used again, but on the Hue Channel of the HLS images. This yields a particularly poor accuracy of 25%, which upon closer inspection, isn't surprising given how blocky the Hue Channel looks (Shown below). ","5a9054c8":"# Approach 8: Blob Size Count Histograms for Selected Hues & Lightness Values\n\nCombining the feature vectors of the previous 2 approaches yields the best classification accuracy so far (64%). ","fcb6bcea":"# Approach 3: Hue & Lightness Histograms with SVM\n\nA more sophisticated classifier is used this time - a Support Vector Machine. Seeing as the ommision of the Lightness channel caused the NB Classifier accuracy to deteriorate, Lightness information was retained as an input feature for the SVM. Conversion of the 2D Histograms to a 1D vector (required by the SVM) is done by simply flattening the 2D Hue & Lightness histogram array.\n\nUse of a more advanced classifer significantly boosts the accuracy given the same input features (37% vs 20%)","3343e065":"# Approach 1: Hue & Lightness Histograms with Naive Bayes\n\nImages are first converted to HLS (Hue, Lightness Saturation) colourspace. The Hue channel gives a scalar representation of the raw colour information \\[sans details like how dark\/bright (Lightness) it is or how intense\/rich the colour is (saturation)\\]. This is in contrast to RGB colorspace where each channel only contains information about the intensity of a single primary colour.\n\nThe mean probability of each Hue\/Lightness combination occurring is computed using the training dataset. This is then used as the basis for a colour based Naive-Bayes classifier.\nIt should be noted that majority of the hues representing Green are omitted in the probability calculation. This is to avoid the presence of grass in the background from affecting the prediction outcome.","3804d476":"# Approach 4: Local Binary Pattern Histograms (Grayscale Image) with SVM\n\nAside from raw colours, the difference between adjacent pixels can be used as input features as well (i.e. applying the Laplacian operator to one of the axis). Local Binary Patterns (BLPs) are one possible representation of how intense\/bright a pixel is relative to a circle of pixels surrounding it at a given radius. The target pixel is compared with its neighbours within the circle and assigned a binary value of 0\/1 based on whether it is brighter\/darker than its neighbour. If the number of neighbours is a power of two (i.e. 8), the binary values can be packed conveniently into a single byte\/integer, essentially resulting in a unique numerical value for each combination of bright\/dark neighbours. \n\nLBPs are able to crudely encode intensity gradient information along the directions of the chosen neighbours, and thus may be used as a classification feature for images.\nFeeding the LBPs of the Grayscale images into an SVM yields an accuracy of 36%.","9b97e339":"# Introduction\n\nKeeping in theme with the lifestyle associated with rearing lifestock, I've decided to dedicate this notebook to learning what it feels like to be a total technological luddite who refuses to accept the technological progress achieved within the past decade.\n\nIn other words, majority of this notebook will focus on classifying chickens without a neural network.\n\nVarious features were tested, with Blob-size counting within the Hue & Lightness channels proving to be the most accurate (~64%) among the non Deep Learning approaches.\n\n<table style=\"text-align:center\">\n    <thead>\n        <tr>\n            <th>No.<\/th>\n            <th>Feature<\/th>\n            <th>Classifier<\/th>\n            <th>Validation Accuracy(%)<\/th>\n        <\/tr>  \n    <\/thead>\n    <tr>\n        <td>1<\/td>\n        <td>Hue & Lightness Histograms<\/td>\n        <td>Naive Bayes<\/td>\n        <td>20%<\/td>\n    <\/tr>\n        <td>2<\/td>\n        <td>Hue Histograms<\/td>\n        <td>Naive Bayes<\/td>\n        <td>13%<\/td>\n    <tr>\n        <td>3<\/td>\n        <td>Hue & Lightness Histograms<\/td>\n        <td>SVM<\/td>\n        <td>37%<\/td>\n    <\/tr>\n    <tr>\n        <td>4<\/td>\n        <td>Local Binary Pattern Histograms (Grayscale Image)<\/td>\n        <td>SVM<\/td>\n        <td>36%<\/td>\n    <\/tr>\n    <tr>\n        <td>5<\/td>\n        <td>Local Binary Pattern Histograms (Hue Channel)<\/td>\n        <td>SVM<\/td>\n        <td>25%<\/td>\n    <\/tr>\n    <tr>\n        <td>6<\/td>\n        <td>Blob Size Count Histograms for Selected Hues<\/td>\n        <td>SVM<\/td>\n        <td>49%<\/td>\n    <\/tr>\n    <tr>\n        <td>7<\/td>\n        <td>Blob Size Count Histograms for Selected Lightness Values<\/td>\n        <td>SVM<\/td>\n        <td>54%<\/td>\n    <\/tr>\n    <tr>\n        <td>8<\/td>\n        <td>Blob Size Count Histograms for Selected Hues & Lightness Values<\/td>\n        <td>SVM<\/td>\n        <td>64%<\/td>\n    <\/tr>\n    <tr>\n        <td>9<\/td>\n        <td>Raw RGB Image<\/td>\n        <td>Inscrutable Black Box conjured up<br\/>using gradient descent optimization and<br\/>copious amounts of copypasta-ed code<br\/><br\/>AKA, a CNN<\/td>\n        <td>>70%<\/td>\n    <\/tr>\n<\/table>\n\nAs a benchmark for comparison, a minimal effort neural network (zero data augmentation & no regularization) was trained, which easily surpasses this score.","1ce2f1d4":"# Approach 6: Blob Size Count Histograms for Selected Hues with SVM\n\nSo far, only histograms have been used. While this is able to encode information about which specific Hues\/Lightnesses are present, it completely omits any notion of size. \n\nTo remedy this, blobs of specific Hues and their radii will be counted and used as an input instead. The radii will also be binned into a histogram to minimize the number of possibilities.\nSkImage has a built-in Blob detector, which defines Blobs are bright regions against a dark background.\n\nTo prepare the hue channel for Blob detection, the distance of the Hue of a pixel from a reference Hue is computed and then fed through a threshold filter to further emphasize the contrast between \"Bright\" (closer to the reference hue) and \"Dark\" (further from the reference hue) regions.\n\nThe histogram of blob radii is computed for each reference Hue and concatenated into a single vector for use with an SVM.\n\nThis approach allows the assoicated size information of each colour patch to be fed into the SVM, essentially giving it another dimension as an input feature, and ultimately results in a significant boost in accuracy (49%).","ad8fd724":"# Approach 9: Minimal Effort CNN\n\nZero effort in every sense.\n* No data augmentation \n* No attempt at feature engineering\n* This CNN model is literally just a minor edit from a previous notebook involving rat brain cells\n* Heck, I even copied the validation graphing code off some guy on StackOverflow\n\nYet, it still outperforms all the non-deep learning  approaches \u00af\\_(\u30c4)_\/\u00af","0357e63d":"# Approach 2: Hue Histograms with Naive Bayes\n\nNaive Bayes is used again, but with the Lightness channel omitted. As with the previous approach, the green hues are removed. Ommision of the Lightness channel results in a prediction accuracy that is worse than random guessing (~13%).","ac38e7e2":"# Approach 7: Blob Size Count Histograms for Selected Lightness Values with SVM\n\nThe blob-counting approach is repeated, but using the Lightness channel instead of the Hue Channel. \n\nNote that the distance computation within the Lightness channel differs in the sense that it does not wrap around each extreme. This is so as unlike the rotational representation of Hues within the Color wheel, it makes no physical sense for Bright regions to suddenly wrap back to pure darknesses.\n\nOperating within the lightness space results in a slight boost in accuracy (54%). ","0f262c23":"# Loading the dataset\n\nImages are separated into \"training\" and \"validation\" folders, which are further subdivided into folders for each available breed \\[American Gamefowl, Wyandotte, Speckled Sussex, Sapphire Gem & a class for chicks (breed not specified)\\].\nAll images use the RGB colourspace and are exactly 224 by 224 pixels large."}}