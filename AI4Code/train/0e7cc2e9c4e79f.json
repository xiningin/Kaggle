{"cell_type":{"0190ecfb":"code","11a77e96":"code","e3fe5ccb":"code","fcac3a30":"code","46ab4bf0":"code","1d41e856":"code","2275cd90":"code","ae27abfc":"code","61125401":"code","7e5a6741":"code","73999649":"code","407db58a":"code","5b03c4c6":"code","b777a07c":"code","e1d98d6a":"code","d1d9a6f2":"code","1c7fe56e":"code","fdf126a9":"code","7540b548":"code","4aa7bd2d":"code","fb41c885":"code","811c47cd":"code","0b30fc11":"code","b2b5e716":"code","b893de2e":"code","c561b040":"code","e9217d3b":"code","db361b2e":"code","0e3a0a50":"code","0219459c":"code","b32c33f6":"code","b1b8876f":"code","d33b24fd":"code","f5542950":"code","3ac8c800":"code","6edd0ffe":"code","eb7db420":"code","e3bf82d7":"code","63dc18de":"code","dbb9c327":"code","06ae7e0b":"code","d069916c":"code","affa3eb3":"code","a659cf9c":"code","d13a6cd2":"code","7c99cba6":"code","49f0a685":"code","be6db97b":"code","73d5db05":"code","f1812e42":"code","ae0c0047":"code","916e3739":"code","c28d9528":"code","15dedbac":"code","f4d403a0":"code","b6671eef":"code","f9253081":"code","91e619ec":"code","cd4cbf2f":"code","416d58dd":"code","6bccb736":"code","4e3ea6ea":"markdown","885873a8":"markdown","45bb8b1b":"markdown","ef2b8953":"markdown","d116a7aa":"markdown","7499f450":"markdown"},"source":{"0190ecfb":"import pandas as pd\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer,  text_to_word_sequence\nfrom keras.engine.topology import Layer\nfrom keras import initializers as initializers, regularizers, constraints\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed, Dropout, CuDNNLSTM\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.models import Model\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\nimport sys\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nfrom nltk import tokenize,word_tokenize\nimport gc\nfrom tqdm import tqdm\ntqdm.pandas()\ngc.collect()\nimport seaborn as sns","11a77e96":"def dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatibl|e with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context\/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https:\/\/www.cs.cmu.edu\/~diyiy\/docs\/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification\/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a \/= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","e3fe5ccb":"max_features= 200000\nmax_senten_len = 40\nmax_senten_num = 3\nembed_size = 300\nVALIDATION_SPLIT = 0.1","fcac3a30":"from sklearn.utils import shuffle","46ab4bf0":"df = pd.read_csv('..\/input\/train.csv')","1d41e856":"test_df = pd.read_csv(\"..\/input\/test.csv\")","2275cd90":"df.head()","ae27abfc":"len(df.target.unique())","61125401":"df.head()","7e5a6741":"df.columns = ['qid', 'text', 'category']\ntest_df.columns = ['qid', 'text']","73999649":"df.head()","407db58a":"df = df[['text', 'category']]","5b03c4c6":"df.info()","b777a07c":"df['text'] = df['text'].apply(lambda x: x.lower())\ntest_df['text'] = test_df['text'].apply(lambda x: x.lower())","e1d98d6a":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}","d1d9a6f2":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","1c7fe56e":"df['text'] = df['text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_contractions(x, contraction_mapping))","fdf126a9":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","7540b548":"punct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', '!':' '}","4aa7bd2d":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","fb41c885":"df['text'] = df['text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","811c47cd":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}","0b30fc11":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","b2b5e716":"df['text'] = df['text'].apply(lambda x: correct_spelling(x, mispell_dict))\ntest_df['text'] = test_df['text'].apply(lambda x: correct_spelling(x, mispell_dict))","b893de2e":"labels = df['category']\ntext = df['text']","c561b040":"indices = np.arange(text.shape[0])\nnp.random.shuffle(indices)\ntext = text[indices]\nlabels = labels.iloc[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * df.shape[0])\n\ntrain_text = text[:-nb_validation_samples].reset_index().drop('index', axis=1)\ny_train = labels[:-nb_validation_samples].reset_index().drop('index', axis=1)\nval_text = text[-nb_validation_samples:].reset_index().drop('index', axis=1)\ny_val = labels[-nb_validation_samples:].reset_index().drop('index', axis=1)","e9217d3b":"test = test_df['text']","db361b2e":"cates = df.groupby('category')\nprint(\"total categories:\", cates.ngroups)\nprint(cates.size())","0e3a0a50":"paras = []\nlabels = []\ntexts = []","0219459c":"sent_lens = []\nsent_nums = []\nfor idx in range(train_text.shape[0]):\n    text = train_text.text[idx]\n    texts.append(text)\n    sentences = tokenize.sent_tokenize(text)\n    sent_nums.append(len(sentences))\n    for sent in sentences:\n        sent_lens.append(len(text_to_word_sequence(sent)))\n    paras.append(sentences)","b32c33f6":"f, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(sent_lens, ax=ax)\nplt.show()","b1b8876f":"sns.distplot(sent_nums)\nplt.show()","d33b24fd":"val_paras = []\nval_labels = []","f5542950":"for idx in range(val_text.shape[0]):\n    text = val_text.text[idx]\n    sentences = tokenize.sent_tokenize(text)\n    val_paras.append(sentences)","3ac8c800":"test_paras = []\ntest_labels = []","6edd0ffe":"for idx in range(test.shape[0]):\n    text = test[idx]\n    sentences = tokenize.sent_tokenize(text)\n    test_paras.append(sentences)","eb7db420":"tokenizer = Tokenizer(num_words=max_features, oov_token=True)\ntokenizer.fit_on_texts(texts)","e3bf82d7":"x_train = np.zeros((len(texts), max_senten_num, max_senten_len), dtype='int32')\nfor i, sentences in enumerate(paras):\n        tokenized_sent = tokenizer.texts_to_sequences(sentences)\n        padded_seq = pad_sequences(tokenized_sent, maxlen=max_senten_len, padding='post', truncating='post')\n        for j, seq in enumerate(padded_seq):\n            if(j < max_senten_num):\n                x_train[i,j,:] = seq\n            else:\n                break","63dc18de":"x_train.shape","dbb9c327":"x_val = np.zeros((val_text.shape[0], max_senten_num, max_senten_len), dtype='int32')\nfor i, sentences in enumerate(val_paras):\n        tokenized_sent = tokenizer.texts_to_sequences(sentences)\n        padded_seq = pad_sequences(tokenized_sent, maxlen=max_senten_len, padding='post', truncating='post')\n        for j, seq in enumerate(padded_seq):\n            if(j < max_senten_num):\n                x_val[i,j,:] = seq\n            else:\n                break","06ae7e0b":"test_data = np.zeros((test.shape[0], max_senten_num, max_senten_len), dtype='int32')\nfor i, sentences in enumerate(test_paras):\n        tokenized_sent = tokenizer.texts_to_sequences(sentences)\n        padded_seq = pad_sequences(tokenized_sent, maxlen=max_senten_len, padding='post', truncating='post')\n        for j, seq in enumerate(padded_seq):\n            if(j < max_senten_num):\n                test_data[i,j,:] = seq\n            else:\n                break","d069916c":"print(test_data.shape, x_val.shape)","affa3eb3":"word_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))","a659cf9c":"print('Shape of training tensor:', x_train.shape)\nprint('Shape of validation tensor:', x_val.shape)\nprint('Shape of test tensor:', test_data.shape)","d13a6cd2":"import os","7c99cba6":"gc.collect()\nword_index = tokenizer.word_index\nmax_features = len(word_index)+1\ndef load_glove(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word.lower(), np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word.lower(), np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    return embedding_matrix","49f0a685":"Embedding_funs = [load_glove,  load_fasttext, load_para]","be6db97b":"import itertools","73d5db05":"REG_PARAM = 1e-2\nl2_reg = regularizers.l2(REG_PARAM)","f1812e42":"def f1(y_true, y_pred):\n    '''\n    metric from here \n    https:\/\/stackoverflow.com\/questions\/43547402\/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","ae0c0047":"# https:\/\/www.kaggle.com\/ryanzhang\/tfidf-naivebayes-logreg-baseline\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","916e3739":"def get_model(embedding_matrix):\n    embedding_layer = Embedding(nb_words, embed_size, weights=[embedding_matrix])\n    word_input = Input(shape=(max_senten_len,), dtype='float32')\n    word_sequences = embedding_layer(word_input)\n    word_lstm = Bidirectional(CuDNNLSTM(max_senten_len, return_sequences=True, recurrent_regularizer=l2_reg))(word_sequences)\n    word_dense = TimeDistributed(Dense(64))(word_lstm)\n    word_att = AttentionWithContext()(word_dense)\n    wordEncoder = Model(word_input, word_att)\n\n    sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')\n    sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n    sent_lstm = Bidirectional(CuDNNLSTM(max_senten_num, return_sequences=True, recurrent_regularizer=l2_reg))(sent_encoder)\n    sent_dense = TimeDistributed(Dense(32))(sent_lstm)\n    sent_att = AttentionWithContext()(sent_dense)\n    preds = Dense(1, activation='sigmoid', kernel_regularizer=l2_reg)(sent_att)\n    model = Model(sent_input, preds)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[f1])\n    return model","c28d9528":"# https:\/\/www.kaggle.com\/strideradu\/word2vec-and-gensim-go-go-go\ndef train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n    h = model.fit(train_X, train_y, batch_size=512, epochs=epochs, validation_data=(val_X, val_y), callbacks = callback, verbose=1)\n    model.load_weights(filepath)\n    pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n    pred_test_y = model.predict([test_data], batch_size=1024, verbose=0)\n    print(metrics.f1_score(val_y, (pred_val_y > 0.33).astype(int)))\n    print('=' * 60)\n    return pred_val_y, pred_test_y","15dedbac":"filepath=\"weights_best.h5\"\nvalidation_results = np.zeros((len(Embedding_funs), x_val.shape[0]))\ntest_results = np.zeros((len(Embedding_funs), test_data.shape[0]))\nfor indx, fun in enumerate(Embedding_funs):\n    checkpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n    callbacks = [checkpoint]\n    embedding_matrix = fun(word_index)\n    nb_words = embedding_matrix.shape[0]\n    model = get_model(embedding_matrix)\n    pred_val_y , pred_test_y= train_pred(model, x_train, y_train, x_val, y_val, epochs = 3, callback = callbacks)\n    validation_results[indx] = pred_val_y.reshape(-1)   \n    test_results[indx] = pred_test_y.reshape(-1)\n    os.remove(filepath)\n    del model\n    gc.collect()","f4d403a0":"def check_all_validations(validation_results, val_y, total=3):\n    all_combs_f1 = {}\n    all_combs_thres = {}\n    for i in range(total):\n        combinations = list(itertools.combinations(range(total), i+1))\n        for indexes in combinations:\n            val_res = np.mean(validation_results[list(indexes)], axis=0)\n            search_result = threshold_search(val_y, val_res)\n            all_combs_f1[indexes] = search_result['f1']\n            all_combs_thres[indexes] = search_result['threshold']\n    return all_combs_f1, all_combs_thres","b6671eef":"all_combinations_f1, all_combinations_thresh = check_all_validations(validation_results, y_val)","f9253081":"for i in all_combinations_f1:\n    print(i, ':', all_combinations_f1[i], 'with threshold equals to', all_combinations_thresh[i])","91e619ec":"import operator\nall_comb_sorted = sorted(all_combinations_f1.items(), key=operator.itemgetter(1), reverse=True)","cd4cbf2f":"pred_test_y = np.mean(test_results[list(all_comb_sorted[0][0])], axis=0)","416d58dd":"best_thresh = all_combinations_thresh[all_comb_sorted[0][0]]","6bccb736":"pred_test_y = (pred_test_y>best_thresh).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","4e3ea6ea":"### Data","885873a8":"### Embeddings","45bb8b1b":"### Config","ef2b8953":"## Model","d116a7aa":"## Preprocessing","7499f450":"### Attention Layer"}}