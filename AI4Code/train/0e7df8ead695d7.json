{"cell_type":{"f19c7bfa":"code","33e6946b":"code","c872750d":"code","032f8d5e":"code","eb54d176":"code","3840356a":"code","9cd66f90":"code","025dd2a9":"code","46168a68":"code","b558128a":"code","21aaad89":"code","8f43b8d3":"code","cb6f3c95":"code","98b89d9f":"code","91a009ea":"code","f7331891":"code","a4062cd8":"code","ad401d8a":"code","ea762c60":"code","d0c5292c":"code","05baa72c":"code","3cc0bfcf":"code","e41438f9":"code","cd886bf7":"code","05384bc0":"code","7a540097":"code","5466a22a":"code","3754b428":"code","ff4d0234":"code","32c26aa2":"code","a1e164c8":"code","aa759650":"code","07989c8a":"code","11875a29":"code","3298f0c4":"code","23c46c87":"code","b7576812":"code","b7342343":"code","a8c8e460":"code","75ce61c9":"code","73c411c8":"code","28310627":"code","548c43ec":"code","e7980af2":"code","e90f6669":"code","3be93c95":"code","3ff17bfc":"code","18faeaec":"code","3bca540b":"code","00603fd0":"code","1ed347fb":"code","a7c534ec":"code","7e1e6349":"code","28122189":"code","21f202c3":"code","5e74b0af":"code","bbd05939":"code","5b5338f4":"code","054fc0a5":"code","1e7b6c8a":"code","89ca0460":"code","65affa1e":"code","0be343fb":"code","c5bd93a1":"code","86ccd3bc":"code","041a832c":"code","0668d835":"code","b7729a0b":"code","77d66c58":"code","bb220f91":"code","809c3d3a":"code","2fc29a8a":"code","14551587":"code","d8204515":"code","0518f57d":"code","43140afb":"code","61aec5f1":"code","eeb6cc1a":"code","620d906c":"code","e33e6395":"code","3fc06cf4":"code","dc279d9d":"code","a2c7fd42":"code","fb1889df":"code","a47edb3e":"code","0fa5b29c":"code","c8ec7877":"code","75f7d7f5":"code","35f00337":"code","a77a0742":"code","6a1e5c45":"code","6c393882":"code","69a48861":"code","a26d3a67":"code","0885b4ec":"code","d771d0d7":"markdown","25a7afcd":"markdown","b03a2c85":"markdown","1c702d14":"markdown","b53ebf10":"markdown","59d756f0":"markdown","f83929fe":"markdown","7801b95c":"markdown","454cb092":"markdown","9d55946f":"markdown","fb3f91b1":"markdown","e40de010":"markdown","e4044ceb":"markdown","273132fe":"markdown","b56aa784":"markdown","e4f624ef":"markdown","cf3f91e1":"markdown","995755dc":"markdown","6e7be4e9":"markdown","f6b83725":"markdown","f08f7b21":"markdown","10adc4ca":"markdown","55932845":"markdown","80882b04":"markdown","702909ab":"markdown","53fc9687":"markdown","0b652b72":"markdown","6d5c23e8":"markdown","f2e03705":"markdown","2528e788":"markdown","0b149744":"markdown","f040bb53":"markdown","1e8871b1":"markdown","10fd3101":"markdown","11240c2c":"markdown","5e172785":"markdown","c655c787":"markdown","30973b96":"markdown","5392f1c4":"markdown","725b7717":"markdown","f6625a6b":"markdown","24fc086a":"markdown","5c508372":"markdown","62e6a95a":"markdown","48b6774d":"markdown","ef710f90":"markdown","01d3e4ac":"markdown","ab06f1d7":"markdown","af22dc31":"markdown","b052a72a":"markdown","25c0f440":"markdown","d9c58c01":"markdown","6b8baa46":"markdown","036c61a2":"markdown","d13cdff9":"markdown","8824c4ae":"markdown","95d7a640":"markdown","39955975":"markdown","ec0cb412":"markdown","bf5e45fb":"markdown","7eb21a00":"markdown","f52e5fa5":"markdown"},"source":{"f19c7bfa":"# Import Libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import cufflinks as cf\nimport plotly\nimport datetime\nimport math\nimport matplotlib\nimport sklearn\nfrom IPython.display import HTML\n#from IPython.display import YouTubeVideo\n\nimport pickle\nimport os\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.model_selection import train_test_split\n\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"SkLearn version : SkLearn {sklearn.__version__}\")\n# print(f\"Cufflinks version : cufflinks {cf.__version__}\")\nprint(f\"Plotly version : plotly {plotly.__version__}\")\n\n# Magic Functions for In-Notebook Display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='colorblind')\n\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100","33e6946b":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv', encoding='latin_1')\ndf.columns = df.columns.str.lower()\nprint(df.shape)\ndf.head()","c872750d":"print(df['class'].value_counts())\nprint('\\n')\nprint(df['class'].value_counts(normalize=True))","032f8d5e":"df[\"class\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",fontsize=20)\nplt.legend([\"Genuine\",\"Fraud\"])\nplt.show()","eb54d176":"# Dealing with missing data\ndf.isnull().sum().max()","3840356a":"fig, axs = plt.subplots(ncols=2,figsize=(16,4))\nsns.distplot(df[df['class'] == 1]['amount'], bins=100, ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.distplot(df[df['class'] == 0]['amount'], bins=100, ax=axs[1])\naxs[1].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","9cd66f90":"print(\"Fraud Transaction distribution : \\n\",df[(df['class'] == 1)]['amount'].value_counts().head())\nprint(\"\\n\")\nprint(\"Maximum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].max())\nprint(\"Minimum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].min())","025dd2a9":"print(\"Genuine Transaction distribution : \\n\",df[(df['class'] == 0)]['amount'].value_counts().head())\nprint(\"\\n\")\nprint(\"Maximum amount of Genuine transaction - \",df[(df['class'] == 0)]['amount'].max())\nprint(\"Minimum amount of Genuine transaction - \",df[(df['class'] == 0)]['amount'].min())","46168a68":"plt.figure(figsize=(8,6))\nplt.title('Distribution of Transaction Time', fontsize=14)\nsns.distplot(df['time'], bins=100)\nplt.show()","b558128a":"fig, axs = plt.subplots(ncols=2, figsize=(16,4))\n\nsns.distplot(df[(df['class'] == 1)]['time'], bins=100, color='red', ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.distplot(df[(df['class'] == 0)]['time'], bins=100, color='green', ax=axs[1])\naxs[1].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","21aaad89":"df[['time','amount','class']].corr()['class'].sort_values(ascending=False).head(10)","8f43b8d3":"plt.title('Pearson Correlation Matrix')\nsns.heatmap(df[['time', 'amount','class']].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"winter\",\n            linecolor='w',annot=True);","cb6f3c95":"df.shape","98b89d9f":"df['class'].value_counts(normalize=True)","91a009ea":"df.hist(figsize = (25,25))\nplt.show()","f7331891":"df.reset_index(inplace = True , drop = True)","a4062cd8":"# Scale amount by log\ndf['amount_log'] = np.log(df.amount + 0.01)","ad401d8a":"from sklearn.preprocessing import StandardScaler # importing a class from a module of a library\n\nss = StandardScaler() # object of the class StandardScaler ()\ndf['amount_scaled'] = ss.fit_transform(df['amount'].values.reshape(-1,1))","ea762c60":"from sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler() # object of the class StandardScaler ()\ndf['amount_minmax'] = mm.fit_transform(df['amount'].values.reshape(-1,1))","d0c5292c":"#Feature engineering to a better visualization of the values\n\n# Let's explore the Amount by Class and see the distribuition of Amount transactions\nfig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n\nsns.boxplot(x =\"class\",y=\"amount\",data=df, ax = axs[0])\naxs[0].set_title(\"Class vs Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_log\",data=df, ax = axs[1])\naxs[1].set_title(\"Class vs Log Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_scaled\",data=df, ax = axs[2])\naxs[2].set_title(\"Class vs Scaled Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_minmax\",data=df, ax = axs[3])\naxs[3].set_title(\"Class vs Min Max Amount\")\n\n# fig.suptitle('Amount by Class', fontsize=20)\nplt.show()","05baa72c":"CreditCardFraudDataCleaned = df\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final Data set after all the transformations as a file\nwith open('CreditCardFraudDataCleaned.pkl', 'wb') as fileWriteStream:\n    pickle.dump(CreditCardFraudDataCleaned, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file is saved at Location:',os.getcwd())","3cc0bfcf":"# Reading a Pickle file\nwith open('CreditCardFraudDataCleaned.pkl', 'rb') as fileReadStream:\n    CreditCardFraudDataFromPickle = pickle.load(fileReadStream)\n    # Don't forget to close the filestream!\n    fileReadStream.close()\n    \n# Checking the data read from pickle file. It is exactly same as the DiamondPricesData\ndf = CreditCardFraudDataFromPickle\ndf.head()","e41438f9":"df.shape","cd886bf7":"# Separate Target Variable and Predictor Variables\n# Here I am keeping the log amount and dropping the amount and scaled amount columns.\nX = df.drop(['time','class','amount'],axis=1)\ny = df['class']","05384bc0":"X","7a540097":"# Split the data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=101)","5466a22a":"# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","3754b428":"from sklearn.linear_model import LogisticRegression # Importing Classifier Step","ff4d0234":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0) \n\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train) ","32c26aa2":"y_pred = logreg.predict(X_test)","a1e164c8":"from sklearn import metrics","aa759650":"# https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall\nprint(metrics.classification_report(y_test, y_pred))","07989c8a":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred , y_test))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))\n# print('Confusion Matrix : \\n', cnf_matrix)\nprint(\"\\n\")","11875a29":"cnf_matrix = metrics.confusion_matrix(y_test,y_pred)\ncnf_matrix","3298f0c4":"conf_mx = metrics.confusion_matrix(y_test,y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint ('recall = ', round(recall,3), 'precision = ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\nprint('F1 = ', round(F1,3))","23c46c87":"def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,\n                          normalize=False):\n    import itertools\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n        \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","b7576812":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","b7342343":"from sklearn.metrics import precision_recall_curve, average_precision_score, auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred)\nprint('f1=%.3f' % (f1))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","a8c8e460":"metrics.roc_auc_score(y_test , y_pred) ","75ce61c9":"y_pred_proba = logreg.predict_proba(X_test)\ny_pred_proba","73c411c8":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n\nauc = metrics.roc_auc_score(y_test, y_pred)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","28310627":"# Import imbalace technique algorithims\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler","548c43ec":"from collections import Counter # counter takes values returns value_counts dictionary\nfrom sklearn.datasets import make_classification","e7980af2":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","e90f6669":"# Undersampling only on train\n\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nrus = RandomUnderSampler(random_state=random_state)\nX_res, y_res = rus.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\n\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","3be93c95":"# Undersampling with Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)","3ff17bfc":"conf_mx = metrics.confusion_matrix(y_test,y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint ('recall = ', round(recall,3), 'precision = ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\nprint('F1 = ', round(F1,3))","18faeaec":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","3bca540b":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred , y_test))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))","00603fd0":"# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred)\nprint('f1=%.3f' % (f1))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","1ed347fb":"from imblearn.over_sampling import RandomOverSampler","a7c534ec":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","7e1e6349":"# Oversampling only on train\n\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nros = RandomOverSampler(random_state=random_state)\nX_res, y_res = ros.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\n\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","28122189":"# Oversampling with Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)","21f202c3":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_test , y_pred))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))","5e74b0af":"conf_mx = metrics.confusion_matrix(y_test,y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint ('recall = ', round(recall,3), 'precision = ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\nprint('F1 = ', round(F1,3))","bbd05939":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","5b5338f4":"from imblearn.over_sampling import SMOTE, ADASYN","054fc0a5":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","1e7b6c8a":"# Oversampling only on train\n\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\n\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","89ca0460":"\n# SMOTE Sampling with Logistic Regression\nlogreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)","65affa1e":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_test , y_pred))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))","0be343fb":"conf_mx = metrics.confusion_matrix(y_test,y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint ('recall = ', round(recall,3), 'precision = ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\nprint('F1 = ', round(F1,3))","c5bd93a1":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","86ccd3bc":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","041a832c":"# Oversampling only on train\n\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nadasyn = ADASYN(random_state=42)\nX_res, y_res = adasyn.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\n\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","0668d835":"\n#  ADASYN Sampling with Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)","b7729a0b":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred , y_test))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))","77d66c58":"conf_mx = metrics.confusion_matrix(y_test,y_pred)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint ('recall = ', round(recall,3), 'precision = ', round(precision,3))\n\nF1 = 2 * recall * precision \/ (recall + precision)\nprint('F1 = ', round(F1,3))","bb220f91":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","809c3d3a":"from sklearn.decomposition import PCA","2fc29a8a":"X_reduced_pca_im = PCA(n_components=2, random_state=42).fit_transform(X)","14551587":"# Generate and plot a synthetic imbalanced classification dataset\nplt.figure(figsize=(12,8))\n\nplt.scatter(X_reduced_pca_im[:,0], X_reduced_pca_im[:,1], c=(y == 0), label='No Fraud', cmap='coolwarm', linewidths=1)\nplt.scatter(X_reduced_pca_im[:,0], X_reduced_pca_im[:,1], c=(y == 1), label='Fraud', cmap='coolwarm', linewidths=1)\n\nplt.title(\"Scatter Plot of Imbalanced Dataset\")\nplt.legend()\nplt.show()","d8204515":"# Split BEFORE any under \/ oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)\nprint('-'*50)\n# Oversampling only on train\n#print('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nrus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X_train, y_train)\n\nX_train = X_res\ny_train = y_res\n\n# Slit into train and test datasets\nX_train_under, X_test_under, y_train_under, y_test_under = X_train, X_test, y_train, y_test\n\nprint(\"X_train_under - \",X_train_under.shape)\nprint(\"y_train_under - \",y_train_under.shape)\nprint(\"X_test_under - \",X_test_under.shape)\nprint(\"y_test_under - \",y_test_under.shape)","0518f57d":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)\nprint('-'*50)\n# Oversampling only on train\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nros = RandomOverSampler(random_state=random_state)\nX_res, y_res = ros.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\nX_train_over, X_test_over, y_train_over, y_test_over = X_train, X_test, y_train, y_test\n\nprint(\"X_train_over - \",X_train_over.shape)\nprint(\"y_train_over - \",y_train_over.shape)\nprint(\"X_test_over - \",X_test_over.shape)\nprint(\"y_test_over - \",y_test_over.shape)","43140afb":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)\nprint('-'*50)\n# Oversampling only on train\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nsmote = SMOTE(random_state=42)\nX_res, y_res  = smote.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = X_train, X_test, y_train, y_test\n\nprint(\"X_train_smote - \",X_train_smote.shape)\nprint(\"y_train_smote - \",y_train_smote.shape)\nprint(\"X_test_smote - \",X_test_smote.shape)\nprint(\"y_test_smote - \",y_test_smote.shape)","61aec5f1":"# Split BEFORE any oversampling - prevent data leakage\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)\nprint('-'*50)\n# Oversampling only on train\nprint('Original dataset shape %s' % Counter(y_train))\nrandom_state = 42\n\nadasyn = ADASYN(random_state=42)\nX_res, y_res = adasyn.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))\n\nX_train = X_res\ny_train = y_res\nX_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = X_train, X_test, y_train, y_test\n\nprint(\"X_train_adasyn - \",X_train_adasyn.shape)\nprint(\"y_train_adasyn - \",y_train_adasyn.shape)\nprint(\"X_test_adasyn - \",X_test_adasyn.shape)\nprint(\"y_test_adasyn - \",y_test_adasyn.shape)","eeb6cc1a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","620d906c":"names_lst = []\n\n# Empty list to capture performance matrix for train set\naucs_train_lst = []\naccuracy_train_lst = []\nprecision_train_lst = []\nrecall_train_lst = []\nf1_train_lst = []\n\n# Empty list to capture performance matrix for test set\naucs_test_lst = []\naccuracy_test_lst = []\nprecision_test_lst = []\nrecall_test_lst = []\nf1_test_lst = []\n\n# Function for model building and performance measure\n\ndef build_measure_model(models):\n    #plt.figure(figsize=(12,6))\n\n    for name, model, X_train, X_test, y_train, y_test in models:\n        \n        names_lst.append(name)\n\n        # split data in train test set\n        X_train, X_test, y_train, y_test = X_train, X_test, y_train, y_test\n        \n        # Build model\n        model.fit(X_train, y_train)\n        \n        # Predict\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        # calculate accuracy\n        Accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n        accuracy_train_lst.append(Accuracy_train)\n        \n        Accuracy_test = metrics.accuracy_score(y_test, y_test_pred)\n        accuracy_test_lst.append(Accuracy_test)\n\n        # calculate auc\n        Aucs_train = metrics.roc_auc_score(y_train, y_train_pred)\n        aucs_train_lst.append(Aucs_train)\n        \n        Aucs_test = metrics.roc_auc_score(y_test , y_test_pred)\n        aucs_test_lst.append(Aucs_test)\n\n        # calculate precision\n        PrecisionScore_train = metrics.precision_score(y_train , y_train_pred)\n        precision_train_lst.append(PrecisionScore_train)\n        \n        PrecisionScore_test = metrics.precision_score(y_test , y_test_pred)\n        precision_test_lst.append(PrecisionScore_test)\n\n        # calculate recall\n        RecallScore_train = metrics.recall_score(y_train , y_train_pred)\n        recall_train_lst.append(RecallScore_train)\n        \n        RecallScore_test = metrics.recall_score(y_test , y_test_pred)\n        recall_test_lst.append(RecallScore_test)\n\n        # calculate f1 score\n        F1Score_train = metrics.f1_score(y_train , y_train_pred)\n        f1_train_lst.append(F1Score_train)\n        \n        F1Score_test = metrics.f1_score(y_test , y_test_pred)\n        f1_test_lst.append(F1Score_test)\n\n        #print('F1 Score of '+ name +' model : {0:0.5f}'.format(F1Score_test))\n\n        # draw confusion matrix\n        cnf_matrix = metrics.confusion_matrix(y_test , y_test_pred)\n\n        print(\"Model Name :\", name)\n        \n        #print('Train Accuracy :{0:0.5f}'.format(Accuracy_train)) \n        print('Test Accuracy :{0:0.5f}'.format(Accuracy_test))\n        \n        #print('Train AUC : {0:0.5f}'.format(Aucs_train))\n        print('Test AUC : {0:0.5f}'.format(Aucs_test))\n        \n        #print('Train Precision : {0:0.5f}'.format(PrecisionScore_train))\n        print('Test Precision : {0:0.5f}'.format(PrecisionScore_test))\n        \n        #print('Train Recall : {0:0.5f}'.format(RecallScore_train))\n        print('Test Recall : {0:0.5f}'.format(RecallScore_test))\n        \n        #print('Train F1 : {0:0.5f}'.format(F1Score_train))\n        print('Test F1 : {0:0.5f}'.format(F1Score_test))\n        \n        print('Confusion Matrix : \\n', cnf_matrix)\n        print(\"\\n\")\n","e33e6395":"#------------------ Logistic Regression (LR) ------------------#\nLRmodels = []\n\n#LRmodels.append(('LR imbalance', LogisticRegression(solver='liblinear', multi_class='ovr'),X,y))\nLRmodels.append(('LR Undersampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_under, X_test_under, y_train_under, y_test_under))\nLRmodels.append(('LR Oversampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_over, X_test_over, y_train_over, y_test_over))\nLRmodels.append(('LR SMOTE', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_smote, X_test_smote, y_train_smote, y_test_smote))\nLRmodels.append(('LR ADASYN', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(LRmodels)","3fc06cf4":"#-----------------Decision Tree (DT)------------------#\nDTmodels = []\n\ndt = DecisionTreeClassifier()\n\n#DTmodels.append(('DT imbalance', dt,X,y))\nDTmodels.append(('DT Undersampling', dt, X_train_under, X_test_under, y_train_under, y_test_under))\nDTmodels.append(('DT Oversampling', dt,X_train_over, X_test_over, y_train_over, y_test_over))\nDTmodels.append(('DT SMOTE', dt,X_train_smote, X_test_smote, y_train_smote, y_test_smote))\nDTmodels.append(('DT ADASYN', dt ,X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(DTmodels)","dc279d9d":"#-----------------Random Forest (RF) ------------------#\nRFmodels = []\n\n#RFmodels.append(('RF imbalance', RandomForestClassifier(),X,y))\nRFmodels.append(('RF Undersampling', RandomForestClassifier(),X_train_under, X_test_under, y_train_under, y_test_under))\nRFmodels.append(('RF Oversampling', RandomForestClassifier(),X_train_over, X_test_over, y_train_over, y_test_over))\nRFmodels.append(('RF SMOTE', RandomForestClassifier(),X_train_smote, X_test_smote, y_train_smote, y_test_smote))\nRFmodels.append(('RF ADASYN', RandomForestClassifier(),X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(RFmodels)","a2c7fd42":"# Display comparison of the models performance\n\ndata = {'Model':names_lst,\n       #'Accuracy_Train':accuracy_train_lst,\n       #'Accuracy_Test':accuracy_test_lst,\n       #'AUC_Train':aucs_train_lst,\n       #'AUC_Test':aucs_test_lst,\n       #'PrecisionScore_Train':precision_train_lst,\n       'PrecisionScore_Test':precision_test_lst,\n       #'RecallScore_Train':recall_train_lst,\n       'RecallScore_Test':recall_test_lst,\n       #'F1Score_Train':f1_train_lst,\n       'F1Score_Test':f1_test_lst}\n\nprint(\"Performance measures of various classifiers: \\n\")\nperformance_df = pd.DataFrame(data) \nperformance_df.sort_values(['F1Score_Test'],ascending=False)","fb1889df":"import tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils","a47edb3e":"# F1 function as there is none in Keras metrics\n# https:\/\/medium.com\/@aakashgoel12\/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n\nimport keras.backend as K\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","0fa5b29c":"# NN model\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(2048, activation='relu', input_shape=(31,)))\n#model.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(30,)))\n#model.add(layers.BatchNormalization())\n#model.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n#model.add(layers.Dropout(0.5))\n#model.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n#model.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(optimizer=optimizers.Adam(lr=1e-4), \n              loss='binary_crossentropy', \n              metrics=[get_f1])\n              #metrics=['binary_accuracy'])\n\nprint(model.summary())","c8ec7877":"# Pick one of the four: under, over, smote or adasyn\n\nX_train, y_train, X_test, y_test = X_train_over, y_train_over, X_test_over, y_test_over\n\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","75f7d7f5":"# FIT \/ TRAIN model\n\nNumEpochs = 10\nBatchSize = 16\n\nhistory = model.fit(X_train, y_train, epochs=NumEpochs, batch_size=BatchSize, validation_split = 0.2)","35f00337":"results = model.evaluate(X_test, y_test)\nprint(\"_\"*100)\nprint(\"Test Loss and Metric (acc or f1)\")\nprint(\"results \", results)\nhistory_dict = history.history\nhistory_dict.keys()","a77a0742":"# Learning curves for F1\n\nf1 = history.history['get_f1'] \nval_f1 = history.history['val_get_f1'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss'] \nepochs = range(1, len(f1) + 1) \nplt.plot(epochs, f1, 'bo', label='Training f1') \nplt.plot(epochs, val_f1, 'b', label='Validation f1') \nplt.title('Training and validation F1') \nplt.legend() \nplt.figure() \nplt.plot(epochs, loss, 'bo', label='Training loss') \nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss') \nplt.legend() \nplt.show()","6a1e5c45":"# Final Fit \/ Predict\n# NOTE final_predictions is a list of probabilities\n\nfinal_predictions = model.predict(X_test)\nfinal_predictions.shape","6c393882":"# Modify the raw final_predictions - prediction probs into 0 and 1\n\nPreds = final_predictions.copy()\n#print(len(Preds))\n#print(Preds)\nPreds[ np.where( Preds >= 0.5 ) ] = 1\nPreds[ np.where( Preds < 0.5 ) ] = 0","69a48861":"# Confusion matrix\n\nfrom sklearn import metrics\nconf_mx = metrics.confusion_matrix(y_test, Preds)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint (recall, precision)","a26d3a67":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['Genuine', 'Fraud'],\n                      title        = \"Confusion Matrix on test\")","0885b4ec":"print ('precision ',precision_score(y_test, Preds))\nprint ('recall ',recall_score(y_test, Preds) )\nprint ('accuracy ',accuracy_score(y_test, Preds))\nprint ('F1 score ',f1_score(y_test, Preds))","d771d0d7":"### Logistic Regression (LR)","25a7afcd":"**Highlights**\n\nThis graph shows that most of the fraud transaction amount is less than 500 dollars. This also shows that the fraud transaction is very high for an amount near to 0, let's find that amount.","b03a2c85":"# **Credit Card Fraud Detection**\n**Anonymized credit card transactions labeled as fraudulent or genuine**\n\n<img src=\"https:\/\/i.imgur.com\/lBuWqxx.png\" \/>","1c702d14":"\nplt.clf()\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history_dict['loss']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# VALIDATION ACCURACY curves\n\nplt.clf()\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\nepochs = range(1, (len(history_dict['binary_accuracy']) + 1))\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","b53ebf10":"## <a id='logregovrsamp'>1.3.Logistic Regression with Random Oversampling technique<\/a>","59d756f0":"**Highlights**\n\nBy seeing the graph, we can see there are two peaks in the graph and even there are some local peaks. We can think of these as the time of the day like the peak is the day time when most people do the transactions and the depth is the night time when most people just sleeps. We already know that data contains a credit card transaction for only two days, so there are two peaks for day time and one depth for one night time.","f83929fe":"\n### F1 on test: 0.94 with data leakage vs 0.12 without ","7801b95c":"# <a id='splitdata'>Splitting data into Training and Testing samples<\/a>\n\nWe don't use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as training data and the rest 30% is used as testing data.","454cb092":"**Highlights**\n\nIt looks like that no features are highly correlated with any other features.","9d55946f":"# <a id='classimbalance'>Class Imbalance<\/a>\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.  If one applies classifiers on the dataset, they are likely to predict everything as the majority class. This was often regarded as a problem in learning from highly imbalanced datasets.\n\n<img src='https:\/\/i.imgur.com\/uqh1peJ.gif' \/>\n\n\nLet's Fix the class Imbalance and apply some sampling techniques.\n\n\nRef : https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/","fb3f91b1":"# AUC is not a good metric for imbalanced datasets\n\n* Imagine that in the graphs above, one of the groups, the blue right one for example is extremely small...but you can always read about the reasoning of the above statement https:\/\/stats.stackexchange.com\/questions\/262616\/roc-vs-precision-recall-curves-on-imbalanced-dataset","e40de010":"### Optimizing on accuracy:\n\n* NN - under ... F1 = 0.17\n* NN - smote ... F1 = 0.76\n\n\n\n### Optimizing on F1:\n\n* NN - over ... F1 = 0.82\n* NN - smote ... F1 = 0.81\n* NN - adasyn ... F1 = 0.79\n\n","e4044ceb":"### F1 on test: 0.94 with data leakage vs 0.12 without ","273132fe":"# <a id='introduction'>Introduction<\/a>\n\n\nThe datasets contain transactions that have 492 frauds out of 284,807 transactions. So the dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. When we try to build the prediction model with this kind\u00a0of unbalanced dataset, then the model will be more inclined towards to detect new unseen transaction as genuine as our dataset contains about 99% genuine data.\n\nAs our dataset is highly imbalanced, so we shouldn't use accuracy score as a metric because it will be usually high and misleading, instead use we should focus on f1-score, precision\/recall score or confusion matrix.","b56aa784":"##------------------ K-Nearest Neighbors (KNN) ------------------#\n\nKNNmodels = []\n\nKNNmodels.append(('KNN Undersampling', KNeighborsClassifier(),X_train_under, X_test_under, y_train_under, y_test_under))\nKNNmodels.append(('KNN Oversampling', KNeighborsClassifier(),X_train_over, X_test_over, y_train_over, y_test_over))\nKNNmodels.append(('KNN SMOTE', KNeighborsClassifier(),X_train_smote, X_test_smote, y_train_smote, y_test_smote))\nKNNmodels.append(('KNN ADASYN', KNeighborsClassifier(),X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn))\n\n\nbuild_measure_model(KNNmodels)","e4f624ef":"## <a id='usdata'>1. Undersampled Data<\/a>","cf3f91e1":"## <a id='null'>Finding null values<\/a>","995755dc":"### <a id='modevelmatrix'>Confusion Matrix<\/a>\n\nEvery problem is different and derives a different set of values for a particular business use case , thus every model must be evaluated differently.\n\n**Let's get to know the terminology and Structure first**\n\nA confusion matrix is defined into four parts : __{ TRUE , FALSE } (Actual) ,{POSITIVE , NEGATIVE} (Predicted)__\nPositive and Negative is what you predict , True and False is what you are told\n\nWhich brings us to 4 relations : True Positive , True Negative , False Positive , False Negative <br>\n__P__ redicted - __R__ ows and __A__ ctual as __C__ olumns <br>\n\n<img src = 'https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/final_cnf.png?raw=true'>\n\n\n### Accuracy , Precision and Recall\n\n##### __Accuracy__ : The most used and classic classification metric : Suited for binary classification problems.\n\n$$  \\text{Accuracy} = \\frac{( TP + TN ) }{ (TP + TN + FP + FN )}$$\n\nBasically Rightly predicted results amongst all the results , used when the classes are balanced\n\n##### __Precision__ : What proportion of predicted positives are truly positive ? Used when we need to predict the positive thoroughly, sure about it !\n\n$$ \\text{Precision} = \\frac{( TP )}{( TP + FP )} $$\n\n##### __Sensitivity or Recall__ : What proportion of actual positives is correctly classified ? choice when we want to capture as many positives as possible\n\n$$ \\text{Recall} = \\frac{(TP)}{( TP + FN )} $$\n\n##### __F1 Score__ : Harmonic mean of Precision and Recall. It basically maintains a balance between the precision and recall for your classifier\n\n$$ F1 = \\frac{2 * (\\text{ precision } * \\text{ recall })}{(\\text{ precision } + \\text{ recall } )} $$\n\n<img src='https:\/\/i.imgur.com\/IYuqqic.gif' \/>\n\n**Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.**\n\n\n**In reference of our case**:\n\nRecall (True Positive Rate): % of all fraudulent transactions cases captured.\n\nPrecision: Out of all items labeled as fraud, what percentage of them is actually fraud?\n\nAccuracy: How correct the model is (misleading for fraud\/imbalanced data)\n\nF1 score: combination of recall and precision into one metric. F1 score is the weighted average of precision and recall, taking BOTH false positives and false negatives into account. Usually much more useful than accuracy, especially with uneven classes.","6e7be4e9":"## <a id='scalenorm'>3. Scale  amount by Normalization<\/a>\n\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n$$ x_{norm} = \\frac{x_i - x_{min}}{x_{max}-x_{min}} $$","f6b83725":"# <a id='logreg'>1. Logistic Regression<\/a>","f08f7b21":"## Lets check the data again after cleaning","10adc4ca":"## <a id='logregim'>1.1 Logistic Regression with imbalanced data<\/a>","55932845":"## Reset the index","80882b04":"### <a id='roccurve'>Receiver Operating Characteristics (ROC)<\/a>\n\nThe ROC is a performance measurement for classification problems at various thresholds. It is essentially a probability curve, and the higher the Area Under the Curve (AUC) score the better the model is at predicting fraudulent\/non-fraudulent transactions.\n\nIt is an evaluation metric that helps identify the strength of the model to **distinguish between two outcomes**. It defines if a model can create a clear boundary between the postive and the negative class. \n\n<div style='width:100%;'>\n   <div style='width:30%; float:left;'> <img  src ='https:\/\/i.imgur.com\/fzBGUDy.jpg' \/> <\/div>\n   <div style=''> <img  src ='https:\/\/i.imgur.com\/hZQiNCn.png' \/> <\/div>\n<\/div>\n\n\nLet's talk about some definitions first: \n\n##### __Sensitivity__ or __Recall__\n\nThe sensitivity of a model is defined by the proportion of actual positives that are classified as Positives , i.e = TP \/ ( TP + FN )\n\n$$ \\text{Recall or Sensitivity} = \\frac{(TP)}{( TP + FN )} $$\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/sens.png?raw=true\">\n\n##### __Specificity__\n\nThe specificity of a model is defined by the proportion of actual negatives that are classified as Negatives , i.e = TN \/ ( TN + FP )\n\n$$ \\text{Specificity} = \\frac{(TN)}{( TN + FP )} $$\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/spec.png?raw=true\">\n\nAs we can see that both are independent of each other and lie in teo different quadrants , we can understand that they are inversely related to each other. Thus as Sensitivity goes up , Specificity goes down and vice versa.\n\n### ROC CURVE\n\nIt is a plot between Sesitivity and ( 1 - Specificity ) , which intuitively is a plot between True Positive Rate and False Positive Rate. \nIt depicts if a model can clearly identify each class or not\n\nHigher the area under the curve , better the model and it's ability to seperate the positive and negative class.\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/tpfpfntn.jpeg?raw=true\">\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/auc.png?raw=true\">\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/auc2.png?raw=true\">\n\n<img src='https:\/\/i.imgur.com\/GRuZpez.gif'>","702909ab":"### <a id='modevel'>Model Evolution<\/a>","53fc9687":"### Predict from Test set","0b652b72":"# <a id='pickle'>Saving preprossed data as serialized files<\/a>\n* To deploy the predictive models built we save them along with the required data files as serialized file objects\n* We save cleaned and processed input data, tuned predictive models as files so that they can later be re-used\/shared","6d5c23e8":"# <a id='scaleamount'>Scale Amount Feature<\/a>\n\n* It is a good idea to scale the data so that the column(feature) with lesser significance might not end up dominating the objective function due to its larger range. like a column like age has a range between 0 to 80, but a column like a salary has ranged from thousands to lakhs, hence, salary column will dominate to predict the outcome even if it may not be important.\n* In addition, features having different unit should also be scaled thus providing each feature equal initial weightage. Like Age in years and Sales in Dollars must be brought down to a common scale before feeding it to the ML algorithm\n* This will result in a better prediction model.\n\n\n\n**PCA Transformation**: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) except for time and amount.\n\n**Scaling**: Keep in mind that in order to implement a PCA transformation features need to be previously scaled.","f2e03705":"## <a id='logregadasyn'>1.5 Logistic Regression with ADASYN data<\/a>","2528e788":"### Distribution of time w.r.t. transactions types","0b149744":"## <a id='adasyndata'>4. ADASYN Data<\/a>","f040bb53":"## <a id='logregsomote'>1.4 Logistic Regression with SMOTE data<\/a>","1e8871b1":"### Load preprocessed data","10fd3101":"### Random Forest (RF)","11240c2c":"## Import imbalace technique algorithms","5e172785":"## <a id='scalestand'>2. Scale  amount by Standardization<\/a>\n\nStandardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n$$ z = \\frac{x_i - \\mu}{\\sigma} $$","c655c787":"## <a id='corr'>Correlation Among Explanatory Variables<\/a>\n\nHaving **too many features** in a model is not always a good thing because it might cause overfitting and worse results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice.**\n\nAnother important thing is **correlation. If there is a very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting.** However, this does not mean that you must remove one of the highly correlated features. \n\nLet's find out top 10 features which are highly correlated with a price.","30973b96":"# Modified the original nb so the test is separated BEFORE any oversampling - to prevent data leakage\n\nIdentical samples in the train AND test = data leakage....\n\nOversampling is performed ONLY on train","5392f1c4":"**Hightlights**\n\n* We can see a slight difference in the log amount of our two Classes. \n* The IQR of fraudulent transactions are higher than normal transactions, but normal transactions have the highest values.\n* **By seeing the above three graphs, I think scaling the amount by log will best suit for our model.**","725b7717":"## \u00a0<a id='distimbds'>Distribution of balaced dataset<\/a>\n\nFinally, we can create a scatter plot of the dataset and colour the examples for each class a different colour to clearly see the spatial nature of the class imbalance.\n\nA scatter plot of the dataset is created showing the large mass of points that belong to the minority class (red) and a small number of points spread out for the minority class (blue). We can see some measure of overlap between the two classes.","f6625a6b":"## <a id='logregundsamp'>1.2.Logistic Regression with Random Undersampling technique<\/a>","24fc086a":"## Import the Dataset","5c508372":"##------------------ Support Vector Machines (SVM) ------------------#\n\nSVMmodels = []\n\nSVMmodels.append(('SVM imbalance', SVC(gamma='auto'),X,y))\nSVMmodels.append(('SVM Undersampling', SVC(gamma='auto'),X_under,y_under))\nSVMmodels.append(('SVM Oversampling', SVC(gamma='auto'),X_over,y_over))\nSVMmodels.append(('SVM SMOTE', SVC(gamma='auto'),X_smote,y_smote))\nSVMmodels.append(('SVM ADASYN', SVC(gamma='auto'),X_adasyn,y_adasyn))\n\nbuild_measure_model(SVMmodels)","62e6a95a":"### F1 on test: 0.87 with data leakage vs 0.03 without ","48b6774d":"# <a id='feateng'>Feature Engineering<\/a> ","ef710f90":"## <a id='osdata'>2. Oversampled Data<\/a>","01d3e4ac":"# <a id='dataset'>Load Data<\/a>","ab06f1d7":"### Decision Tree (DT)","af22dc31":"## <a id='scalelog'>1. Scale amount by Log<\/a>\n\n**Scaling using the log**: There are two main reasons to use logarithmic scales in charts and graphs. \n* The first is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. \n* The second is to show per cent change or multiplicative factors.\u00a0","b052a72a":"# <a id='modelwith'>Building different models with different balanced datasets<\/a>\nLet's now try different models , first by creating multiple datasets for undersampled , oversampled and SMOTE sampled","25c0f440":"## <a id='undovrsamp'>Under Sampling and Over Sampling<\/a>\n\nOversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. \n\n* Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n\n* Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model.\n\n<img src = 'https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/under_over_sampling.jpg?raw=true'>\n\n## <a id='smote'>Synthetic Minority OverSampling Technique (SMOTE)<\/a>\n\nIn this technique, instead of simply duplicating data from the minority class, we synthesize new data from the minority class. This is a type of data augmentation for tabular data can be very effective. This approach to synthesizing new data is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short. \n\n<img src='https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/smote.png?raw=true'>\n\n## <a id='adasyn'>Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN)<\/a>\n\nADASYN (Adaptive Synthetic) is an algorithm that generates synthetic data, and its greatest advantages are not copying the same minority data, and generating more data for \u201charder to learn\u201d examples.\n\nRef : https:\/\/medium.com\/@ruinian\/an-introduction-to-adasyn-with-code-1383a5ece7aa","d9c58c01":"# <a id='modelbuild'>Model Building<\/a>\n\n##### We are aware that our dataset is highly imbalanced, however, we check the performance of imbalance dataset first and later we implement some techniques to balance the dataset and again check the performance of balanced dataset. Finally, we will compare each regression models performance.","6b8baa46":"#### Let's Discuss Next Steps - \n\n1  __Classification Models__\n\n- Logistic Regression\n- Decision Trees\n- Random Forest\n- Naive Bayes Classifier \n\n2  __Class Imbalance Solutions__\n\n- Under Sampling\n- Over Sampling\n- SMOTE\n- ADASYN\n\n3  __Metrics__\n\n- Accuracy Score\n- Confusion Matrix\n- Precision Score\n- Recall Score\n- ROC_AUC\n- F1 Score","036c61a2":"### Based on the excellent nb from\nhttps:\/\/www.kaggle.com\/dktalaicha\/credit-card-fraud-detection-using-smote-adasyn\n\n* The data is very skewed - there are only 0.17% fraudulent transactions in the 280k samples - accuracy is not a good metric: any \"model\" predicting ALL are normal transactions will have a 99.83% accuracy.\n* We need to use Recall, Precision and their prodigy (harmonic mean) - the F1 score. \n\n### The main modifications I've done to the original:\n* Instead of oversampling and then split into train\/test = ***data leakage*** ... I've first split into train\/test and then oversampled ONLY on train\n* Focus on F1 (as ROC AUC is not a good metric for imbalanced datasets)\n* See my previous nb comparing 6 anomaly detection algorithms at https:\/\/www.kaggle.com\/drscarlat\/compare-6-unsupervised-anomaly-detection-models ... that one achieved **F1 = 0.8** on test.\n\n* The original compared between Logistic Regression, Decision Trees, Random Forest, etc. I've added a NN to the list. \n* F1 score on Test using **undersampling, oversampling, SMOTE & ADASYN**:\n\n### The best performance is RF SMOTE ... F1 = 0.87 on test**\n\nModel\tF1Score_Test\n* RF SMOTE\t0.867384\n* RF ADASYN\t0.855072\n* RF Oversampling\t0.853933\n* DT Oversampling\t0.747405\n* DT ADASYN\t0.508393\n* DT SMOTE\t0.480519\n* RF Undersampling\t0.151637\n* Oversampling\t0.121874\n* LR SMOTE\t0.115830\n* LR Undersampling\t0.102721\n...\n### Best NN = 0.82\n* NN - over ... F1 = 0.82\n* NN - smote ... F1 = 0.81\n* NN - adasyn ... F1 = 0.79\n","d13cdff9":"## <a id='timedist'>Distribution of Time<\/a>","8824c4ae":"**Highlights**\n\n* There are 113 fraud transactions for just one dollor and 27 fraud transaction for $99.99. And higest fraud transaction amount was 2125.87 and lowest was just 0.00.\n* There are 27 fraud transaction for zero amount. Zero Authorization is an account verification method for credit cards that is used to verify a cardholders information without charging the consumer. Instead, an amount of zero is charged on the card to store the credit card information in the form of a token and to determine whether the card is legitimate or not. After creating the token, is then possible to charge the consumer with a new transaction with either Tokenization or Recurring Payments\n\nRef : https:\/\/docs.multisafepay.com\/tools\/zero-authorization\/what-is-zero-authorization\/","95d7a640":"## <a id='unique'>Count unique values of label<\/a>","39955975":"# <a id='spatial'>Spatial nature of class imbalance<\/a>\n\nI will reduce 29 columns to 2 columns with the help of **Principal Component Analysis** so that I can look at them in a plot! (because to plot graph we need two dimensions)","ec0cb412":"## <a id='smotedata'>3. SMOTE Data<\/a>","bf5e45fb":"### NN","7eb21a00":"# <a id='modelbaseline'>Baseline for models<\/a>\n\nWe will train four types of classifiers and decide which classifier will be more effective in detecting **fraud transactions**.","f52e5fa5":"### Function to create the datasets for under, oversampling and capture their performance"}}