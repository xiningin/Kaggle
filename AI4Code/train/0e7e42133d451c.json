{"cell_type":{"5d493a74":"code","3f83adb0":"code","7b64e053":"code","104b7507":"code","536d6fbc":"code","dae73a55":"code","3050fb4c":"code","746fc035":"code","861e0ec7":"code","abaaf89a":"code","db4c1fa3":"code","17d27f50":"code","f0cc8963":"code","f1b371dd":"code","7971c284":"code","cea163a0":"code","d80e2c8d":"code","a7bf76de":"markdown","6c3aadf6":"markdown","eab6c1b6":"markdown","f3814efe":"markdown","ea7c4a26":"markdown","2514a0e4":"markdown","7b2d2243":"markdown","d3f102e8":"markdown","f59303d7":"markdown","c27927ef":"markdown","8ce6feda":"markdown","7120acbd":"markdown","5dd46bf6":"markdown","5e20be86":"markdown","0437de8e":"markdown","b1372aef":"markdown","ab0402f4":"markdown","08314d42":"markdown","799fe552":"markdown","ceaefd91":"markdown","85bff07f":"markdown","d8c4d2d0":"markdown","7e3fe592":"markdown","8915ce34":"markdown","a332319d":"markdown"},"source":{"5d493a74":"import numpy as np\nimport pandas as pd\n\ndf_train = pd.read_csv('..\/input\/train.csv')\n\ntarget_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","3f83adb0":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Remove 'id' and 'target' columns\nlabels = df_train.columns[2:]\n\nX = df_train[labels]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","7b64e053":"model = XGBClassifier()\nmodel.fit(X_train[['ps_calc_01']], y_train)\ny_pred = model.predict(X_test[['ps_calc_01']])\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","104b7507":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","536d6fbc":"# Class count\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\n# Divide by class\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]","dae73a55":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())\n\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)');","3050fb4c":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');","746fc035":"import imblearn","861e0ec7":"from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)');","abaaf89a":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","db4c1fa3":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')","17d27f50":"from collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler \n\n\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_sample(X, y)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')","f0cc8963":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')","f1b371dd":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks()\nX_tl, y_tl = tl.fit_sample(X, y)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')","7971c284":"from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids()\nX_cc, y_cc = cc.fit_sample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')","cea163a0":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","d80e2c8d":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek()\nX_smt, y_smt = smt.fit_sample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')","a7bf76de":"<h2 id=\"t10\" style=\"margin-bottom: 18px\">Over-sampling: SMOTE<\/h2>\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","6c3aadf6":"<h2 id=\"t9\" style=\"margin-bottom: 18px\">Under-sampling: Cluster Centroids<\/h2>\n\nThis technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n\nIn this example we will pass the <code>{0: 10}<\/code> dict for the parameter <code>ratio<\/code>, to preserve 10 elements from the majority class (0), and all minority class (1) .","eab6c1b6":"<h2 id=\"t7\" style=\"margin-bottom: 18px\">Python imbalanced-learn module<\/h2>\n\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n\nLet's apply some of these resampling techniques, using the Python library [imbalanced-learn](http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/). It is compatible with scikit-learn and is part of scikit-learn-contrib projects.","f3814efe":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample<\/code> method to get random samples each class:","ea7c4a26":"Now let's run the same code, but using only one feature (which should drastically reduce the accuracy of the classifier):","2514a0e4":"<h2 id=\"t3\" style=\"margin-bottom: 18px\">Confusion matrix<\/h2>\n\nAn interesting way to evaluate the results is by means of a confusion matrix, which shows the correct and incorrect predictions for each class. In the first row, the first column indicates how many classes 0 were predicted correctly, and the second column, how many classes 0 were predicted as 1. In the second row, we note that all class 1 entries were erroneously predicted as class 0.\n\nTherefore, the higher the diagonal values of the confusion matrix the better, indicating many correct predictions.","7b2d2243":"<h2 id=\"t4\" style=\"margin-bottom: 18px\">Resampling<\/h2>\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","d3f102e8":"<h2 id=\"t8\" style=\"margin-bottom: 18px\">Under-sampling: Tomek links<\/h2>\n\nTomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.","f59303d7":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/tomek.png?v=2)","c27927ef":"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):","8ce6feda":"<h2 id=\"t12\" style=\"margin-bottom: 18px\">Recommended reading<\/h2>\n\nThe imbalanced-learn documentation:<br>\nhttp:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/index.html\n\nThe imbalanced-learn GitHub:<br>\nhttps:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn\n\nComparison of the combination of over- and under-sampling algorithms:<br>\nhttp:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/auto_examples\/combine\/plot_comparison_combine.html\n\nChawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002):<br>\nhttps:\/\/www.jair.org\/media\/953\/live-953-2037-jair.pdf","7120acbd":"<h2 style=\"margin-bottom: 18px\">Index<\/h2>\n\n* Imbalanced datasets\n* The metric trap\n* Confusion matrix\n* Resampling\n* Random under-sampling\n* Random over-sampling\n* Python imbalanced-learn module\n* Random under-sampling and over-sampling with imbalanced-learn\n* Under-sampling: Tomek links\n* Under-sampling: Cluster Centroids\n* Over-sampling: SMOTE\n* Over-sampling followed by under-sampling\n* Recommended reading","5dd46bf6":"As we can see, the high accuracy rate was just an illusion. In this way, the choice of the metric used in unbalanced datasets is extremely important. In this competition, the evaluation metric is the Normalized Gini Coefficient, a more robust metric for imbalanced datasets, that ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score.","5e20be86":"<h2 id=\"t1\" style=\"margin-bottom: 18px\">Imbalanced datasets<\/h2>\n\nIn this kernel we will know some techniques to handle highly unbalanced datasets, with a focus on resampling. The Porto Seguro's Safe Driver Prediction competition, used in this kernel, is a classic problem of unbalanced classes, since insurance claims can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.\n\nLet's see how unbalanced the dataset is:","0437de8e":"For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification<\/code> method:","b1372aef":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","ab0402f4":"We'll use <code>ratio='minority'<\/code> to resample the minority class.","08314d42":"<h2 id=\"#t72\">Random under-sampling and over-sampling with imbalanced-learn<\/h2>","799fe552":"<h2 id=\"t11\" style=\"margin-bottom: 18px\">Over-sampling followed by under-sampling<\/h2>\n\nNow, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:","ceaefd91":" ![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png)","85bff07f":"We will also create a 2-dimensional plot function, <code>plot_2d_space<\/code>, to see the data distribution:","d8c4d2d0":"<h2 id=\"t6\">Random over-sampling<\/h2>","7e3fe592":"In the code below, we'll use <code>ratio='majority'<\/code> to resample the majority class.","8915ce34":"<h2 id=\"t2\" style=\"margin-bottom: 18px\">The metric trap<\/h2>\n\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like <code>accuracy_score<\/code> can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n\nLet's do this experiment, using simple cross-validation and no feature engineering:","a332319d":"<h2 id=\"t5\">Random under-sampling<\/h2>"}}