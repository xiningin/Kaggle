{"cell_type":{"c11ab7ea":"code","76808bee":"code","d1a52405":"code","057deb19":"code","9491a9cc":"code","2a674ebb":"code","5d3e8633":"code","d2d831ac":"code","db568798":"code","1ff6f9db":"code","35e493c1":"code","5f62c404":"code","591fbc54":"code","a8c3a175":"code","e623569e":"code","099328f3":"code","c3be306a":"code","1f93ed7f":"code","d8ccab03":"code","995456e8":"code","2060b160":"code","7e6d1cde":"code","f6801ef4":"code","14e389cb":"code","9a5382a6":"code","db2b3c2d":"code","3a34c292":"code","5ebe7bea":"code","c6d47194":"code","9e95b3b2":"code","cff1a326":"code","7fd069de":"code","43db2825":"code","7550cc9b":"code","6ac38a5b":"code","fd467873":"code","c9700e93":"code","7825e23a":"code","fe9b7406":"code","d0cb3aed":"code","7d42f5b1":"code","6d287367":"code","f7e8165e":"code","34a4a928":"code","043a7ca5":"code","752f7b61":"code","bcaf299d":"markdown","835f24ff":"markdown","7d289b81":"markdown","a16f1c4e":"markdown","f50949a7":"markdown","1f35e051":"markdown","8d02fc9b":"markdown","0f52bc15":"markdown","9beeca80":"markdown","66453230":"markdown","5eb7ed07":"markdown","81da310b":"markdown","e28ba61a":"markdown","b2131e3b":"markdown","5adc2aac":"markdown","50fd3fe2":"markdown","cb86242a":"markdown","df35b4d9":"markdown","19bf5efe":"markdown","b5ac1c39":"markdown","28fb644b":"markdown","92713f20":"markdown","fba0ab53":"markdown","24bb59bb":"markdown","8c0e8e80":"markdown","7c815efc":"markdown","b81a4bf8":"markdown","91e400be":"markdown","8f95db60":"markdown"},"source":{"c11ab7ea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.3f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","76808bee":"file_path = '\/kaggle\/input\/insurance\/insurance.csv'\ndf = pd.read_csv(file_path)\n\nprint(\"Test DataSet = {} rows and {} columns\\n\".format(\n    df.shape[0], df.shape[1]))\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative  = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"Qualitative Variables: (Numerics)\", \"\\n=>\", qualitative,\n      \"\\n\\nQuantitative Variable: (Strings)\\n=>\", quantitative)\n\ndf.head()","d1a52405":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","057deb19":"def eda_numerical_feat(series, title=\"\", number_format=\"\", with_label=True):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 4), sharex=False)\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1, rug=True)\n    sns.boxplot(series, ax=ax2)\n    ax1.set_title(\"distplot\")\n    ax2.set_title(\"boxplot\")\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=10, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=10, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","9491a9cc":"def plot_model_score_regression(models_name_list, model_score_list, title=''):\n    fig = plt.figure(figsize=(15, 6))\n    ax = sns.pointplot( x = models_name_list, y = model_score_list, \n        markers=['o'], linestyles=['-'])\n    for i, score in enumerate(model_score_list):\n        ax.text(i, score + 0.002, '{:.4f}'.format(score),\n                horizontalalignment='left', size='large', \n                color='black', weight='semibold')\n    plt.ylabel('Score', size=20, labelpad=12)\n    plt.xlabel('Model', size=20, labelpad=12)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.xticks(rotation=70)\n    plt.title(title, size=20)\n    plt.show()","2a674ebb":"df.isnull().sum()","5d3e8633":"df.columns","d2d831ac":"eda_numerical_feat(df['charges'], \"'charges' Distribution\", \"{:,.0f}\")","db568798":"eda_numerical_feat(df['bmi'], \"'bmi' Distribution\", \"{:.2f}\")","1ff6f9db":"eda_numerical_feat(df['age'], \"'Age' Distribution\")","35e493c1":"eda_categ_feat_desc_plot(df['children'], '\"children\" Distribution')","5f62c404":"eda_categ_feat_desc_plot(df['sex'], '\"Sex\" Distribution')","591fbc54":"eda_categ_feat_desc_plot(df['smoker'], '\"smoker\" Distribution')","a8c3a175":"eda_categ_feat_desc_plot(df['region'], '\"region\" Distribution')","e623569e":"fig, ((ax1, ax2), (ax3,ax4), (ax5, ax6)) = plt.subplots(figsize = (16,14), ncols=2, nrows=3, sharex=False, sharey=False)\n\n# sex\nsns.boxplot(x=\"sex\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[ df['sex'] == 'male']['charges'], ax=ax2, hist=False, label=\"male\")\nsns.distplot(df[ df['sex'] == 'female']['charges'], ax=ax2, hist=False, label=\"female\")\n# region\nsns.boxplot(x=\"region\", y=\"charges\", data=df, ax=ax3)\nsns.kdeplot(df[ df['region'] == 'southwest']['charges'], ax=ax4, label=\"southwest\")\nsns.kdeplot(df[ df['region'] == 'southeast']['charges'], ax=ax4, label=\"southeast\")\nsns.kdeplot(df[ df['region'] == 'northwest']['charges'], ax=ax4, label=\"northwest\")\nsns.kdeplot(df[ df['region'] == 'northeast']['charges'], ax=ax4, label=\"northeast\")\n# children\nsns.boxplot(x=\"children\", y=\"charges\", data=df, ax=ax5)\nsns.distplot(df[ df['children'] == 0]['charges'], ax=ax6, hist=False, label=\"0\")\nsns.distplot(df[ df['children'] == 1]['charges'], ax=ax6, hist=False, label=\"1\")\nsns.distplot(df[ df['children'] == 2]['charges'], ax=ax6, hist=False, label=\"2\")\nsns.distplot(df[ df['children'] == 3]['charges'], ax=ax6, hist=False, label=\"3\")\nsns.distplot(df[ df['children'] == 4]['charges'], ax=ax6, hist=False, label=\"4\")\nsns.distplot(df[ df['children'] == 5]['charges'], ax=ax6, hist=False, label=\"5\")\n\n# Config Titles\nfig.suptitle('Categorical Features with \"charge\"', fontsize=20)\nfont_size = 16\nax1.set_title('charges by sex')\nax2.set_title('charges by sex')\nax3.set_title('charges by children')\nax4.set_title('charges by children')\nax5.set_title('charges by region')\nax6.set_title('charges by region')\n\nplt.legend();\nplt.show()","099328f3":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('charge x smoke', fontsize=18)\n\nsns.boxplot(x=\"smoker\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[(df.smoker == 'yes')][\"charges\"],color='c',ax=ax2, label='smoke')\nsns.distplot(df[(df.smoker == 'no')]['charges'],color='b',ax=ax2, label='not smoke')\n\nax1.set_title('charges by smoke or not', fontsize=font_size)\nax2.set_title('Distribution of charges for smokers or  not', fontsize=font_size)\nplt.show()","c3be306a":"fig, (ax1, ax2) = plt.subplots(figsize = (16,5), ncols=2, sharex=False, sharey=False)\n\nfont_size = 14\nfig.suptitle('charge x sex', fontsize=18)\n\nsns.boxplot(x=\"sex\", y=\"charges\", data=df, ax=ax1)\nsns.distplot(df[(df.sex == 'male')][\"charges\"],color='c',ax=ax2, hist=False, label='male')\nsns.distplot(df[(df.sex == 'female')]['charges'],color='b',ax=ax2, hist=False, label='female')\n\nax1.set_title('charges by sex', fontsize=font_size)\nax2.set_title('Distribution of charges for male\/female', fontsize=font_size)\n\nplt.show()","1f93ed7f":"fig, (ax1, ax2) = plt.subplots(figsize = (16,11), nrows=2)\n\nsns.scatterplot(x=\"age\", y=\"charges\", data=df, ax=ax1)\nsns.boxplot(x=\"age\", y=\"charges\", data=df, palette=\"Set3\", ax=ax2)\n\n# config scatterplot x_axis\nax1.set_xticks(range(18,65)) # show age axis\nax1.set_xlim(17.5,64.5) # remove right\/left margin\n\n# Config Titles\nfig.suptitle('charge by age', fontsize=20)\nplt.show()","d8ccab03":"# Feature Engineering: Create 'weight_condition' to see better see bmi importance\n\ndf[\"weight_condition\"] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[col[\"bmi\"] < 18.5, \"weight_condition\"] = \"Underweight\"\n    col.loc[(col[\"bmi\"] >= 18.5) & (col[\"bmi\"] < 24.986), \"weight_condition\"] = \"Normal Weight\"\n    col.loc[(col[\"bmi\"] >= 25) & (col[\"bmi\"] < 29.926), \"weight_condition\"] = \"Overweight\"\n    col.loc[col[\"bmi\"] >= 30, \"weight_condition\"] = \"Obese\"","995456e8":"ax = sns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"weight_condition\", data=df)\nax.set_title(\"charges by bmi'\")\nplt.show()","2060b160":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (17,11), ncols=2, nrows=3, sharex=False, sharey=False)\n\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"smoker\", data=df, ax=ax1)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"sex\", data=df, ax=ax2)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"weight_condition\", data=df, ax=ax3)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"bmi\", data=df, size=\"bmi\", ax=ax4)\n\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"children\", data=df, ax=ax5)\nsns.scatterplot(x=\"age\", y=\"charges\", hue=\"region\", data=df, ax=ax6)\n\n# Config Titles\nfig.suptitle('charge x age with others features', fontsize=18)\nax1.set_title(\"charges by age and smoke\")\nax2.set_title(\"charges by age and sex\")\nax3.set_title(\"charges by age and weight_condition\")\nax4.set_title(\"charges by age and bmi\")\nax5.set_title(\"charges by age and children\")\nax6.set_title(\"charges by age and region\")\nplt.show()","7e6d1cde":"# Feature Engineering: Create 'weight_condition' to see better 'age' importance\n\ndf['age_cat'] = np.nan\nlst = [df]\n\nfor col in lst:\n    col.loc[(col['age'] >= 18) & (col['age'] <= 30), 'age_cat'] = 'Young Adult'\n    col.loc[(col['age'] >  30) & (col['age'] <= 50), 'age_cat'] = 'Adult'\n    col.loc[(col['age'] >  50) & (col['age'] <= 60), 'age_cat'] = 'Senior'\n    col.loc[ col['age'] >  60, 'age_cat'] = 'Elder'\n    \ndf.head()","f6801ef4":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(figsize = (17,11), ncols=2, nrows=3, sharex=False, sharey=False)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=df, ax=ax1)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"sex\", data=df, ax=ax2)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"weight_condition\", data=df, ax=ax3)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"age_cat\", data=df, ax=ax4)\n\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"children\", data=df, ax=ax5)\nsns.scatterplot(x=\"bmi\", y=\"charges\", hue=\"region\", data=df, ax=ax6)\n\n# Config Titles\nfig.suptitle('charge by bmi with others features', fontsize=18)\nax1.set_title(\"charges by bmi and smoke\")\nax2.set_title(\"charges by bmi and sex\")\nax3.set_title(\"charges by bmi and weight_condition\")\nax4.set_title(\"charges by bmi and age_cat\")\nax5.set_title(\"charges by bmi and children\")\nax6.set_title(\"charges by bmi and region\")\n\nplt.show()","14e389cb":"# Before\ndf.head()","9a5382a6":"from sklearn.preprocessing import LabelEncoder\n\ndf = df.drop(['weight_condition','age_cat'], axis=1)\n\n# sex\nle = LabelEncoder()\nle.fit(df.sex.drop_duplicates()) \ndf.sex = le.transform(df.sex)\n\n# smoker or not\nle.fit(df.smoker.drop_duplicates()) \ndf.smoker = le.transform(df.smoker)\n\n# region\nle.fit(df.region.drop_duplicates()) \ndf.region = le.transform(df.region)\n\ndf.head() # after pre-processing","db2b3c2d":"corr_matrix = df.corr()\nf, ax1 = plt.subplots(figsize=(18, 6), sharex=False)\n\nax1.set_title('Top Corr to {}'.format('\"charges\"'))\ncols_top = corr_matrix.sort_values(by=\"charges\", ascending=False)['charges'].index\n\ncm = np.corrcoef(df[cols_top].values.T)\nmask = np.zeros_like(cm)\nmask[np.triu_indices_from(mask)] = True\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                 annot_kws={'size': 14}, yticklabels=cols_top.values,\n                 xticklabels=cols_top.values, mask=mask, ax=ax1)","3a34c292":"# https:\/\/www.kaggle.com\/hamelg\/python-for-data-26-anova\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef anova_analysis(y_target, x_cat_feats, datf):\n    for x_feat in x_cat_feats:\n        model = ols('{} ~ {}'.format(y_target, x_feat),\n                    data = datf).fit()\n        anova_result = sm.stats.anova_lm(model, typ=2)\n        print(anova_result,'\\n')    \n        \n\n# If PR(>F) is less than 0.05 (alpha = cofiant level) means that the categorical feauture influence 'charges'\nanova_analysis('charges', ['smoker', 'region'], df)","5ebe7bea":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# # Normal Split\n# x = df.drop(['charges'], axis = 1)\n# y = df['charges']\n# x_train,x_test,y_train,y_test = train_test_split(x,y, random_state = 42)\n\n####### OBS: IS better use Polinomal Transform than only split ############\n\n# # Polinomial REgression: Feature Transform : \n#   create x^0, x^1, x^2 .... to linear models to be polinomial\n\nX = df.drop(['charges','region'], axis = 1)\nY = df.charges\n\nquad = PolynomialFeatures(degree = 2)\nx_quad = quad.fit_transform(X)\n\nx_train,x_test,y_train,y_test = train_test_split(x_quad, Y, random_state = 0)","c6d47194":"from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler, scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import ElasticNet, LassoCV, BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, ElasticNetCV, LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.svm import SVR\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","9e95b3b2":"# Setup cross validation folds\n\nkf = KFold(n_splits=4, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=x_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","cff1a326":"# Create ML Models\n\n# Light Gradient Boosting Regressor\nlightgb_model = LGBMRegressor(objective='regression',  num_leaves=6, learning_rate=0.01,  n_estimators=7000,\n                       max_bin=200,  bagging_fraction=0.8, bagging_freq=4,  bagging_seed=8,\n                       feature_fraction=0.2, feature_fraction_seed=8, min_sum_hessian_in_leaf = 11,\n                       verbose=-1, random_state=42)\n\n# XGBoost Regressor\nxgboost_model = XGBRegressor(learning_rate=0.01, n_estimators=6000, max_depth=4, min_child_weight=0,\n                       gamma=0.6, subsample=0.7, colsample_bytree=0.7, objective='reg:squarederror',\n                       nthread=-1, scale_pos_weight=1, seed=42, reg_alpha=0.00006, random_state=42)\n\n# Linear Regressor\nlinear_model = LinearRegression()\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, \n                1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge_model = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Lasso Regressor\nlasso_alphas2 = [5e-05, 0.0001, 0.0008, 0.01, 0.1, 1]\nlasso_model = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=lasso_alphas2,\n                              random_state=42, cv=kf))\n\n# Elastic Net Regressor\nelastic_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nelastic_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet_model = make_pipeline(RobustScaler(),  \n                           ElasticNetCV(max_iter=1e7, alphas=elastic_alphas,\n                                        cv=kf, l1_ratio=elastic_l1ratio))\n\n# Kernel Ridge\nkeridge_model = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# Support Vector Regressor\nsvm_model = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngboost_model = GradientBoostingRegressor(n_estimators=6000, learning_rate=0.01, max_depth=4, max_features='sqrt', \n                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=42)  \n\n# Random Forest Regressor\nrandomforest_model = RandomForestRegressor(n_estimators=1200, max_depth=15, min_samples_split=5, min_samples_leaf=5,\n                          max_features=None, oob_score=True, random_state=42)\n\n# Neural Net\nneuralnet_model = MLPRegressor()\n\n# Extra Tree Regressor\nextratree_model = ExtraTreesRegressor()","7fd069de":"# SVM and NeuralNet was completally terrible\n\nregressor_models = {\n    'Linear': linear_model,\n    'Ridge': ridge_model,\n    'Lasso': lasso_model,\n    'KernelRidge': keridge_model,\n    'ElasticNet': elasticnet_model,\n#     'SVM': svm_model,\n    'RandomForest': randomforest_model,\n    'ExtraTree': extratree_model,\n#     'NeuralNet': neuralnet_model,\n    'GBoost': gboost_model,\n    'LightGB': lightgb_model,\n    'XGBoost': xgboost_model,\n}","43db2825":"## Cross Validation\n\ncv_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:17}'.format(model_name), end='')\n    t0 = time.time()\n    score = cv_rmse(model)\n    m, s = score.mean(), score.std()\n    cv_scores[model_name] = [m,s]\n    print('| MSE in CV | mean: {:11,.3f}, | std: {:9,.3f}  | took: {:9,.3f} s |'.format(m,s, time.time() - t0))\n    \nprint('\\nTime total to CrossValidation: took {:9,.3f} s'.format(time.time() - t_start)) # 200s\n\n# Show Sorted DataFrame\ndf_cv = pd.DataFrame(data = cv_scores.values(), columns=['rmse_cv', 'std_cv'], index=cv_scores.keys())\ndf_cv = df_cv.sort_values(by='rmse_cv').reset_index().rename({'index': 'model'}, axis=1)\ndf_cv","7550cc9b":"# Def Stack Model: Stack up some the models above, optimized using one ml model\nstack_regressors = (regressor_models['Lasso'],\n                    regressor_models['LightGB'],\n                    regressor_models['Ridge'],\n                    regressor_models['RandomForest'])\n\nstack_model = StackingCVRegressor(regressors = stack_regressors,\n                                meta_regressor = regressor_models['ElasticNet'],\n                                use_features_in_secondary=True)\n\nregressor_models['Stack'] = stack_model","6ac38a5b":"train_scores = {}\nt_start = time.time()\n\nfor model_name, model in regressor_models.items():\n    print('{:17}'.format(model_name), end='')\n    t0 = time.time()\n    if(model_name == 'Stack'):\n        model  = model.fit(np.array(x_train), np.array(y_train))\n        y_pred = model.predict(np.array(x_train))\n    else:\n        model  = model.fit( x_train, y_train )\n        y_pred = model.predict(x_train)\n    r2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\n    train_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    text_print = '| Train | r2: {:6,.3f}, | mse: {:15,.3f}  | took: {:9,.3f} s |'\n    print(text_print.format(r2, mse, time.time() - t0))\n    regressor_models[model_name] = model\n    \nprint('\\nTime total to Fit Models: took {:9,.3f} s'.format(time.time() - t_start)) # 200s","fd467873":"regressor_models.keys()","c9700e93":"# Blend Model is use a porcentage of some models mixing\nclass BlendModel:\n    \n    @classmethod\n    def predict(self, X):\n        return ((0.10 * regressor_models['Lasso'].predict(X)) + \\\n            (0.10 * regressor_models['GBoost'].predict(X)) + \\\n            (0.15 * regressor_models['XGBoost'].predict(X)) + \\\n            (0.10 * regressor_models['LightGB'].predict(X)) + \\\n            (0.20 * regressor_models['RandomForest'].predict(X)) + \\\n            (0.35 * regressor_models['Stack'].predict(np.array(X))))\n\nregressor_models['BlendModel'] = BlendModel()\ny_pred = BlendModel.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores['BlendModel'] = [r2, mse, np.sqrt(mse)]\nprint('RMSE score on train data to Blend Model:\\n\\t=>', np.sqrt(mse))","7825e23a":"from mlens.ensemble import SuperLearner\n\n# create a list of base-models\ndef get_models_to_super_leaner():\n    models = list()\n    models.append(regressor_models['Linear'])\n    models.append(regressor_models['Ridge'])\n    models.append(regressor_models['Lasso'])\n    models.append(regressor_models['KernelRidge'])\n    models.append(regressor_models['ElasticNet'])\n    models.append(regressor_models['RandomForest'])\n    models.append(regressor_models['GBoost'])\n    return models\n\n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=rmse, folds=5, shuffle=True, sample_size=len(X))\n    # add base models\n    models = get_models_to_super_leaner()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LinearRegression())\n    return ensemble\n\n# key to regressros models\nmodel_name = 'SuperLeaner'\n\n# create the super learner\nensemble = get_super_learner(x_train)\n# fit the super learner\nt0 = time.time()\nensemble.fit(x_train, np.array(y_train)) # took 350s = 6min\n# pred and evaluate in train dataset\ny_pred = ensemble.predict(x_train)\nr2, mse = r2_score(y_train, y_pred), mean_squared_error(y_train, y_pred)\ntrain_scores[model_name] = [r2, mse, np.sqrt(mse)]\n# show results\ntext_print = '| Super Leaner in Train | r2: {:6,.3f}, | mse: {:9,.3f}  | took: {:9,.3f} s |\\n'\nprint(text_print.format(r2, mse, time.time() - t0))\n# set in dict regressors\nregressor_models[model_name] = ensemble\n# summarize base learners\nprint(ensemble.data)\n# evaluate meta model","fe9b7406":"# Show train_scores dataframe\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_train_scores","d0cb3aed":"test_scores = {}\n\n# predcit x_test to y_test and compare\nfor model_name, model in regressor_models.items():\n    if(model_name == 'Stack'):\n        y_pred = model.predict(np.array(x_test))\n    else:\n        y_pred = model.predict(x_test)\n    r2, mse = r2_score(y_test, y_pred), mean_squared_error(y_test, y_pred)\n    test_scores[model_name] = [r2, mse, np.sqrt(mse)]\n    \n# Sort DF test scores\ndf_test_scores = pd.DataFrame(data = test_scores.values(), columns=['r2_test', 'mse_test', 'rmse_test'], index=test_scores.keys())\ndf_test_scores = df_test_scores.sort_values(by='r2_test', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\ndf_test_scores","7d42f5b1":"# Include Blend in Train Scores\ndf_train_scores = pd.DataFrame(data = train_scores.values(),index=train_scores.keys(), columns=['r2_train', 'mse_train', 'rmse_train'])\ndf_train_scores = df_train_scores.sort_values(by='r2_train', ascending=False).reset_index().rename({'index': 'model'}, axis=1)\n\n# df_test_scores\ndf_cv2 = df_cv.merge(df_train_scores, how='right'  ,left_on='model', right_on='model')\ndf_final_scores = df_cv2.merge(df_test_scores, how='right' ,left_on='model', right_on='model')\n\nprint(list(df_final_scores.columns))\ndf_final_scores.sort_values(by='mse_test')","6d287367":"plot_model_score_regression(list(test_scores.keys()), [r2 for r2, mse, rmse in test_scores.values()], 'Evaluate Models in Test: R2')","f7e8165e":"# To one of best models: GBoost\n\nplt.figure(figsize = (12,4))\nfeat_importances = pd.Series(regressor_models['GBoost'].feature_importances_)#, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","34a4a928":"from yellowbrick.model_selection import FeatureImportances, RFECV\n\n# FeatureImportances and RFECV to a good model, if put GBoost(the best) take a long time\n\nfig, (ax3,ax4) = plt.subplots(figsize = (15,5), ncols=2, sharex=False, sharey=False)\n\nthe_model = 'Linear'\nt_start = time.time()\n\nviz3 = FeatureImportances(regressor_models[the_model], ax=ax3, relative=False)\nviz3.fit(x_train, y_train)\nviz3.finalize()\n\nviz4 = RFECV(regressor_models[the_model], ax=ax4)\nviz4.fit(x_train, y_train)\nviz4.finalize()\n\nprint('Time total to RFECV to {} : took {:9,.3f} s'.format(the_model, time.time() - t_start))\n\nplt.show()","043a7ca5":"from yellowbrick.regressor import ResidualsPlot, PredictionError\nfrom yellowbrick.model_selection import FeatureImportances, RFECV\n\n# Can't use 'SuperLeaner' than, use the second place: GBoost\n\nfig, (ax1, ax2) = plt.subplots(figsize = (15,5), ncols=2)\n\nviz1 = ResidualsPlot(regressor_models['GBoost'], ax=ax1)\nviz1.score(x_test, y_test)\nviz1.finalize()\n\nviz2 = PredictionError(regressor_models['GBoost'], ax=ax2)\nviz2.score(x_test, y_test)  \nviz2.finalize()\n\nplt.show()","752f7b61":"from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n\ny_pred = regressor_models['SuperLeaner'].predict(x_test)\nprint('The best Regressor Model to Test DataSet:')\nprint('MAE : {:14,.3f}'.format(mean_absolute_error(y_pred, y_test)))\nprint('MSE : {:14,.3f}'.format(mean_squared_error(y_pred, y_test)))\nprint('RMSE: {:14,.3f}'.format(np.sqrt(mean_squared_error(y_pred, y_test))))\nprint('MSLE: {:14,.3f}'.format(mean_squared_log_error(y_pred, y_test)))\nprint('R2  : {:14,.3f}'.format(r2_score(y_pred, y_test)))","bcaf299d":"## Develop Models <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n<span style='font-size: 15pt'>Prepare ML Models and training<\/span>","835f24ff":"The best model for the test data was the SuperLeaner, after it GBoost. Both had excellent scores on the training data, even though they were not so good on the training data, as they probably did not have overfitting.\n\nUnfortunately, there is no (or do not know how) to do cross validation for SuperLeaner. Since your score on the training and test data is similar to that of GBoost we can assume your score for cross validation, it would be similar to that of GBoost.\n\nIt happened that in the training data some models were overfitted, such as ExtraTree, XGBoost and LightGB. You can see this by observing that they were almost perfect in the training data but failed like any other model in the test data.\n\nEven though some models cannot do corss validation (Blend, Stack, SuperLeaner) when ordering by 'mse_test' we realize that the models that can make cv are in the same position if you compare 'mse_test' with 'rmse_cv'. So Blend and Stack were also good models, too.\n\n<!-- \nO melhor modelo para os dados de teste foi o SuperLeaner, depois dele GBoost. Ambos tiveram \u00f3timos scores nos dados de treino, mesmo n\u00e2o sendo t\u00e3o boons nos dados de treino, pois provavelmente n\u00e3o tiveram overfitting.\n\nInfelismente n\u00e3o tem (ou n\u00e3o sei fazer) como fazer cross validation para o SuperLeaner. Como a sua pontua\u00e7\u00e2o nos dados de treino e teste s\u00e3o parecidas com a do GBoost podemos supor sua pontua\u00e7\u00e2o de cross validation, seria parecida com a do GBoost.\n\nOcorreu que nos dados de treinamento alguns modelo tiveram overfitting, como ExtraTree, XGBoost e LightGB. \u00c9 poss\u00edvel notar isso observando que foram quase perfeitos nos dados de treino mas falharam como qualquer outro modelo nos dados de teste.\n\nMesmo que alguns modelos n\u00e3o possam fazer corss validation (Blend, Stack, SuperLeaner) ao ordenar por 'mse_test' percebemos que os modelos que podem fazer cv ficam  na mesma coloca\u00e7\u00e2o se comparar 'mse_test' com 'rmse_cv'. Dessa forma Blend e Stack tamb\u00e9m foram bons modelos tamb\u00e9m.\n-->","7d289b81":"### charges by bmi","a16f1c4e":"## EDA","f50949a7":"<span style='font-size: 15pt'>ANOVA: to categorical features<\/span>","1f35e051":"### Cross Validation <a id='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","8d02fc9b":"## Fit Models <a id='index12'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","0f52bc15":"### Each feature individually <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","9beeca80":"What makes medical costs more expensive are: smoking, age and bmi","66453230":"## Problem Description\n\n[Kaggle Link DataSet](https:\/\/www.kaggle.com\/mirichoi0218\/insurance)\n\n**Context**\n\nDataSet with the cost of treatment of different patients of US. Of course, there are several factors that influence the price of treatment but in this dataset has: age, bmi, sex, number of children \/ dependents, region of US, has children or not and finally the cost of treatment.\n\n**File Description**\n\n`insurance.csv`: DataSet with 1,338 rows and 7 columns\n\n## DataSet Description\n\n| Column   | Description                                                                                                                                                                                                                          | Values                                                     |\n|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n| age      | age of primary beneficiary                                                                                                                                                                                                           | int :: [18, 64]                                            |\n| sex      | insurance contractor gender, female, male                                                                                                                                                                                            | string :: ['female','male']                                |\n| bmi      | Body mass index, providing an understanding of<br>body, weights that are relatively high or low<br>relative to height, objective index of body <br>weight (kg \/ m ^ 2) using the ratio of height<br> to weight, ideally 18.5 to 24.9 | number :: [15.960, 53.130]                                 |\n| children | Number of children covered by health insurance.<br>Number of dependents                                                                                                                                                              | number :: [0,5]                                            |\n| smoker   | Smoking or Not                                                                                                                                                                                                                       | string :: ['yes','no']                                     |\n| region   | the beneficiary's residential area in the US                                                                                                                                                                                         | string :: [northeast, southeast, <br>southwest, northwest] |\n| charges  | Individual medical costs billed by health insurance                                                                                                                                                                                  | number :: [1,121.878 , 63,770.428]                         |\n\n\n\n## The Goal\n\nBased on the features predict the cost for a patient\n","5eb7ed07":"## Split in Train and Test <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","81da310b":"<img src=\"https:\/\/www.researchgate.net\/profile\/Selcuk_Nas\/publication\/320067348\/figure\/tbl2\/AS:614180059090945@1523443345088\/Classification-of-body-mass-according-to-body-mass-index-BMI.png\" width=\"40%\"\/>","e28ba61a":"## Best Models <a id='index14'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","b2131e3b":"The better model was SuperLeaner with the results above.\n\n---\n\nThis Kernel is still under development. I would highly appreciate your feedback for improvement and, of course, if you like it, please upvote it!\n\n\nPlease Upvote (It motivates me)","5adc2aac":"## Pre-Processing <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","50fd3fe2":"## Conclusion <a id='index25'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThe better model was:\nSuperLeaner with MSE\n\n\n","cb86242a":"## Import Libs and DataSet <a id='index01'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","df35b4d9":"<h1 align=\"center\"> Medical Cost: EDA and Regression <\/h1>\n\n<img src=\"https:\/\/qtxasset.com\/styles\/breakpoint_xl_880px_w\/s3\/2017-04\/healthcare_costs.jpg?kc3g1eGqOgDg6.ABpPzctQ1kmDcH_A0L&itok=EyhX40L5\" width=\"30%\" \/>\n\nCreated: 2020-09-01\n\nLast updated: 2020-09-01\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>\n\nIn Progress\n\n**Some References**\n+ https:\/\/www.kaggle.com\/hely333\/eda-regression\n+ https:\/\/www.kaggle.com\/janiobachmann\/patient-charges-clustering-and-regression\n+ https:\/\/www.kaggle.com\/mariapushkareva\/medical-insurance-cost-with-linear-regression","19bf5efe":"### Conclusions Of EDA <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n==> chart: 'charge' x 'smoke'\n\nAs in 'charge x smoke' the fact that smoking is very important, the distribution of cartoons is clearly different between a smoker and a non-smoker. Most smokers have much higher charges than non-smokers\n\n==> chart: charge by age\n\nThe higher the age the higher the price\n\n==> chart: cartoons by bmi\n\nThe larger the bmi, the greater the tendency to have large values, although only bmi does not explain large costs\n\n==> chart: charge x age with others features\n\nIn 'cartoons by age and smoke' you can clearly perceive 3 classes.\n+ class 1, lower costs, are non-smokers\n+ class 2, average expenses, smokers and non-smokers\n+ class 3, higher expenses, smokers\nAfter looking at 'cartoons by age and weight_condition' we have that in the vast majority this third class is of obese people (high bmi)\n\n==> chart: 'cartoons by bmi with others features'\n\nIn 'cartoons by bmi and age_cat' he shows us the missing piece, together with 'cartoons by bmi and smoke'.\n\nOnly by looking at the data of the mental activity, in which a decision tree is smoke, age and bmi.\n\n1. If you smoke you will have more expenses than non-smokers (a good part of the population), expenses over 15,000\n  - The BMI is analyzed, if not obese, it is in a group between 18,000 and 30,000, if obese over 35,000\n  - For each of these two groups, the older you are, the more expensive it becomes\n\n2. If you don't smoke expenses below 15,000\n  - For non-fulmenates the second criterion would be age, the older the higher the expenditure\n  - In this, bmi does not influence much. Despite this, some with normal weight or above (NormalWeight, Overweight or Obese) can fall in the cost of being as expensive as a smoker, mainly obese\n\n==>  Other features\n\ngender, children and region have very little influence, this will also be seen in the correlations\n\n<!-- \n\nchart: 'charge' x 'smoke'\n\nComo em 'charge x smoke' o fato de fumar \u00e9 bem importnate, a distribui\u00e7\u00e2o de charges \u00e9 claramente diferente entre um fumante e um n\u00e3o fumante. A maior parte dos fumantes tem encargos muito maiores que os n\u00e2o fumantes\n\nchart: charge by age\n\nQuanto maior a idade maior o pre\u00e7o\n\nchart: charges by bmi\n\nQuanto maior o bmi maior \u00e9 a tendencia de se ter grandes valores, apesar disso s\u00f3 o bmi n\u00e2o explicar grandes custos\n\nchart: charge x age with others features\n\nEm 'charges by age and smoke' podemo perceber nitidamente 3 classes. \n+ classe 1, menor gastos, s\u00e3o os n\u00e3o fumantes\n+ classe 2, gastos medianos, fumantes e n\u00e3o fumantes\n+ classe 3, maiores gastos, fumantes\nDepois olhando para 'charges by age and weight_condition' temos que em grande maioria essa terceira classe \u00e9 das pessoas obesas (alto bmi)\n\nchart: 'charges by bmi with others features'\n\nEm 'charges by bmi and age_cat' nos mostra a pe\u00e7a que falta, junto com 'charges by bmi and smoke'.\n\nSomente olhando os dados dapra fazer mental,emtne uma \u00e1rvore de decisao s\u00e3o fumo, idade e bmi.\n\n+ Se fuma tera mais gastos que os n\u00e3o fulmantes (boa parte da popula\u00e7\u00e3o), gastos acima de 15,000\n  - Analisa-se o BMI, se n\u00e3o for obseo, fica num grupo entre 18,000 e 30,000, se obeso acima de 35,000\n  - Para cada um desses dois grupos, quanto maior a idade, mais caro fica\n\n+ Se n\u00e3o fuma gastos abaixo de 15,000\n  - Para os nao fulmenates o segundo crit\u00e9rio seria a idade, quanto mais velho maior o gasto\n  - Nisso o bmi n\u00e3o influencia muito. Apesar disso alguns com peso normal ou acima (NormalWeight, Overweight or Obese) podem cair no custo de serem t\u00e3o caro quanto fumante, principlamente obesos\n\nsexo, children e region influenciam bem pouco, isso tamb\u00e9m ser\u00e1 visto na parte de correla\u00e7\u00f5es\n\n\n-->","b5ac1c39":"### Analyze feature crossover <a id='index05'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","28fb644b":"## Table Of Content (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA](#index03)\n  - [Each feature individually](#index03)\n  - [Each Feauture with 'charges'](#index04)\n  - [Analyze feature crossover](#index05)\n  - [Conclusions of EDA](#index06)\n+ [Pre-Processing](#index07)\n+ [Correlation](#index08)\n+ [Split in Train and Test](#index09)\n+ [Develop Models](#index10)\n  - [Cross Validation](#index11)\n  - [Fit Models](#index12)\n  - [Test Models](#index13)\n  - [Bests Models](#index14)\n+ [Feature Importance](#index15)\n+ [Hyperparameter Tuning Best Model](#index16)\n+ [Evaluate Best Model to Regression](#index20)\n+ [Conclusion](#index25)\n","92713f20":"So smoking is really an important factor","fba0ab53":"## Correlation <a id='index08'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\n<span style='font-size: 15pt'>Numerical correlations with heatmap<\/span>","24bb59bb":"## Missing data <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nHas no missing data","8c0e8e80":"### Each feauture with 'charges' <a id='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","7c815efc":"## Test Models <a id='index13'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","b81a4bf8":"### Evaluate Best Model to Regression <a id='index20'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","91e400be":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","8f95db60":"## Feature Importance <a id='index15'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>"}}