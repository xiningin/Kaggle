{"cell_type":{"2b5dc1db":"code","ec21c503":"code","1cceb40e":"code","572e08e0":"code","20666138":"code","c1cef1e6":"code","9a3054a0":"code","120f8d97":"code","1ce9ca6a":"code","a160a7c9":"code","e84bd615":"code","e4a38e19":"code","0a79e6c1":"code","168e8304":"code","d3c9fa64":"code","238f6eda":"code","3c7e3957":"code","b711f1a1":"code","106670fb":"code","72cf9211":"code","40c2eb1c":"code","bf189686":"code","5f8190da":"code","e827a2d8":"code","bd24ad23":"code","3d803e01":"code","96f5113d":"code","e79afd45":"code","863f1f55":"code","8b396212":"code","ffb863c8":"code","0f7c5728":"code","d78b8cc8":"code","b5bb1fce":"code","d3048686":"code","82a7989c":"code","78a2f3d4":"code","6fde39c3":"code","08c8d315":"code","9a66d4a7":"code","f9bb94ef":"code","16fd32a7":"code","b528c54e":"code","6b63eb33":"code","bb219517":"code","440c3b22":"markdown","041b77cf":"markdown","bbb31f10":"markdown","b3663bff":"markdown","8b32237b":"markdown","207c4855":"markdown","c662f22f":"markdown","a58546bf":"markdown","b09b25ca":"markdown","72ae688e":"markdown","fe1da307":"markdown","41e3f36d":"markdown","06c2fedd":"markdown","b2f6fbae":"markdown","657c2196":"markdown","8dffbf10":"markdown","ebb5625f":"markdown","241f8268":"markdown","eef5d04d":"markdown","8ad0a486":"markdown","396a131a":"markdown"},"source":{"2b5dc1db":"import numpy as np\nimport pandas as pd\nimport datetime as dt\nimport lightgbm as lgb\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Time series analysis\nimport statsmodels.api as sm\nfrom pandas.plotting import autocorrelation_plot\n\n# Data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV\n\n# Validataion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom IPython.display import Image","ec21c503":"cal = pd.read_csv('..\/input\/walmart-refdata\/calendar.csv') #calendar data\nsal_t = pd.read_csv('..\/input\/walmart-refdata\/sales_train_validation.csv') #sales_train data \nsell_p = pd.read_csv('..\/input\/walmart-refdata\/sell_prices.csv') #sell_price data\nsample = pd.read_csv('..\/input\/walmart-refdata\/sample_submission.csv')","1cceb40e":"#checking the sample data\nsample.head()","572e08e0":"#checking the training data\nsal_t.head()","20666138":"#shecking the sell_prices data\nsell_p.head()","c1cef1e6":"#calander data\ncal.head()","9a3054a0":"#changing the data type of date in cal\ncal['Date_dt']=pd.to_datetime(cal['date'])","120f8d97":"#data size\nprint(\"Train data: {}\".format(sal_t.shape))\nprint(\"Calender data: {}\".format(cal.shape))\nprint(\"Sell Price data: {}\".format(sell_p.shape))\nprint(\"Sample data: {}\".format(sample.shape))","1ce9ca6a":"Image(\"..\/input\/imagesm5\/M5.PNG\")","a160a7c9":"#data copy\ntrain = sal_t.copy()\ncalendar = cal.copy()\nprice = sell_p.copy()","e84bd615":"# Number of events per year\ncalendar[['year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']].groupby(\"year\").count()","e4a38e19":"# Number of flags per year by store\ncalendar[['year', 'snap_CA', 'snap_TX', 'snap_WI']].groupby(\"year\").sum()","0a79e6c1":"print(\"*\"*30, \"store_id\", \"*\"*30)\nprint(\"store_id unique value counts:{}\".format(len(price[\"store_id\"].unique())))\nprint(price[\"store_id\"].unique())\n\nprint(\"*\"*30, \"item_id\", \"*\"*30)\nprint(\"item_id unique value counts:{}\".format(len(price[\"item_id\"].unique())))\nprint(price[\"item_id\"].unique())","168e8304":"print(\"Whole data avarage:{}\".format(price[\"sell_price\"].mean()))\nprint(\"Whole data standard deviation:{}\".format(price[\"sell_price\"].std()))\n\n# Distribution visualization\nplt.figure(figsize=(10,6))\nsns.distplot(price[\"sell_price\"])\nplt.title(\"Price data distribution of whole data\")\nplt.ylabel(\"Frequency\");","d3c9fa64":"# price average\npd.DataFrame(data=price.groupby(\"store_id\").sell_price.mean().round(3)).T","238f6eda":"# price standard deviation\npd.DataFrame(data=price.groupby(\"store_id\").sell_price.std().round(3)).T","3c7e3957":"# box plot\nstore_ca = price[(price[\"store_id\"]=='CA_1') | (price[\"store_id\"]=='CA_2') | (price[\"store_id\"]=='CA_3') | (price[\"store_id\"]=='CA_4')]\nstore_tx = price[(price[\"store_id\"]=='TX_1') | (price[\"store_id\"]=='TX_2') | (price[\"store_id\"]=='TX_3')]\nstore_wi = price[(price[\"store_id\"]=='WI_1') | (price[\"store_id\"]=='WI_2') | (price[\"store_id\"]=='WI_3')]\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 6))\nstore_df = [store_ca, store_tx, store_wi]\n\nfor i in range(len(store_df)):\n    sns.boxplot(x=\"store_id\", y=\"sell_price\", data=store_df[i], ax=ax[i])\n    ax[i].set_ylabel(\"Price\")","b711f1a1":"print('*'*30, 'Item_Id','*'*30 )\nprint(\"Item Id unique value count {}\".format(len(train['item_id'].unique())))\nprint(train['item_id'].unique())\n\nprint(\"*\"*30, \"dept_id\", \"*\"*30)\nprint(\"dept_id unique value counts:{}\".format(len(train[\"dept_id\"].unique())))\nprint(train[\"store_id\"].unique())\n\nprint(\"*\"*30, \"cat_id\", \"*\"*30)\nprint(\"cat_id unique value counts:{}\".format(len(train[\"cat_id\"].unique())))\nprint(train[\"cat_id\"].unique())\n\nprint(\"*\"*30, \"state_id\", \"*\"*30)\nprint(\"state_id unique value counts:{}\".format(len(train[\"state_id\"].unique())))\nprint(train[\"state_id\"].unique())","106670fb":"# sample\nsample_item = train.loc[:,\"d_1\":].T\nsample_item = pd.merge(sample_item, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"Date_dt\") #merge with calender dataset\n\n# Visualization\nfig, ax = plt.subplots(4,1, figsize=(15,20))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\", \"gray\"]\nsample_col = [0, 100, 1000, 10000]\n\nfor i in range(len(sample_col)):\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]].rolling(3).mean(), color=\"white\", linewidth=1)\n    ax[i].plot(sample_item.index, sample_item[sample_col[i]].rolling(28).mean(), color=color[i], linewidth=2, linestyle='--')\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(sample_col[i]), \"Rolling 3 days\", \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(sample_col[i]))","72cf9211":"# Create dataframe by grouping\nstate_group = train.groupby(\"state_id\").sum().T\nstate_group = pd.merge(state_group, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"Date_dt\")\n\n# Visualization\nfig, ax = plt.subplots(3,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\"]\nstate_col = train[\"state_id\"].unique()\n\nfor i in range(len(state_col)):\n    ax[i].plot(state_group.index, state_group[state_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(state_group.index, state_group[state_col[i]].rolling(28).mean(), color=color[i], linewidth=2)\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(state_col[i]), \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(state_col[i]))","40c2eb1c":"# Sales volume per year\nstate_group.groupby(\"year\")['CA', 'TX', 'WI'].sum().plot()\nplt.title(\"Sales volume per year\")\nplt.ylabel(\"Sales volume\");","bf189686":"# Xmas\nXmas_date = [pd.datetime(2011,12,25), pd.datetime(2012,12,25), pd.datetime(2013,12,25), pd.datetime(2014,12,25), pd.datetime(2015,12,25)]\n\n# Drop Xmas date\nstate_group.drop(Xmas_date, inplace=True)","5f8190da":"# Define time series analysis function\ndef plot_ts_decomp(data, col, lag, color):\n    print(\"Analised Data:{}\".format(col.upper()))\n    # Stats model\n    res = sm.tsa.seasonal_decompose(data[col], period=lag)\n    data[\"trend\"] = res.trend\n    data[\"seaso\"] = res.seasonal\n    data[\"resid\"] = res.resid\n    \n    # Visualization\n    fig = plt.figure(figsize=(20,15))\n    grid = plt.GridSpec(4,2, hspace=0.4, wspace=0.2)\n    ax1 = fig.add_subplot(grid[0,0])\n    ax2 = fig.add_subplot(grid[1,0])\n    ax3 = fig.add_subplot(grid[2,0])\n    ax4 = fig.add_subplot(grid[3,0])\n    ax5 = fig.add_subplot(grid[:-2,1])\n    ax6 = fig.add_subplot(grid[2:,1])\n    \n    # raw price data\n    ax1.plot(data.index, data[col], label=\"price of {}\".format(col), color=color, linewidth=0.5)\n    ax1.plot(data.index, data[col].rolling(lag\/\/12).mean(), label=\"Rolling {}\".format(lag\/\/12), color=color, linewidth=2)\n    ax1.set_xlabel(\"date\")\n    ax1.set_ylabel(\"price\")\n    ax1.set_title(\"raw data\")\n    ax1.legend()\n    # trend\n    ax2.plot(data.index, data[\"trend\"], label=\"trend of {}\".format(col), color=color, linewidth=3)\n    ax2.set_xlabel(\"date\")\n    ax2.set_ylabel(\"trend\")\n    ax2.set_title(\"trend\")\n    ax2.legend()\n    # seasonaly\n    ax3.plot(data.index, data[\"seaso\"], label=\"seasonaly of {}\".format(col), color=color, linewidth=0.5)\n    ax3.set_xlabel(\"date\")\n    ax3.set_ylabel(\"seasonaly\")\n    ax3.set_title(\"seasonaly\")\n    ax3.legend()\n    # residual\n    ax4.plot(data.index, data[\"resid\"], label=\"residual error of {}\".format(col), color=color, linewidth=0.5)\n    ax4.set_xlabel(\"date\")\n    ax4.set_ylabel(\"residual error\")\n    ax4.set_title(\"residual\")\n    ax4.legend()\n    # distribution\n    sns.distplot(data[col], ax=ax5)\n    ax5.set_xlabel(\"Price\")\n    ax5.set_ylabel(\"Frequency\")\n    ax5.set_title(\"distribution\")\n    # auto correlation\n    autocorrelation_plot(data[col], ax=ax6, linewidth=0.5)\n    ax6.set_title(\"autocorrelation\")","e827a2d8":"# CA of state\nplot_ts_decomp(state_group, \"CA\", 365, \"aqua\");","bd24ad23":"# TX of state\nplot_ts_decomp(state_group, \"TX\", 365, \"aqua\");","3d803e01":"# WI of state\nplot_ts_decomp(state_group, \"WI\", 365, \"aqua\");","96f5113d":"# Create dataframe by grouping\ncate_group = train.groupby(\"cat_id\").sum().T\ncate_group = pd.merge(cate_group, calendar, left_index=True, right_on=\"d\", how=\"left\").set_index(\"Date_dt\")\n\n# Drop Xmas date\ncate_group.drop(Xmas_date, inplace=True)\n\n# Visualization\nfig, ax = plt.subplots(3,1, figsize=(15,15))\nplt.subplots_adjust(hspace=0.4)\n\ncolor=[\"magenta\", \"cyan\", \"lightgreen\"]\ncate_col = train[\"cat_id\"].unique()\n\nfor i in range(len(cate_col)):\n    ax[i].plot(cate_group.index, cate_group[cate_col[i]], color=color[i], linewidth=0.5)\n    # Rolling\n    ax[i].plot(cate_group.index, cate_group[cate_col[i]].rolling(28).mean(), color=color[i], linewidth=2)\n    ax[i].set_xlabel(\"datetime\")\n    ax[i].set_ylabel(\"Sales volume\")\n    ax[i].legend([\"{}\".format(cate_col[i]), \"Rolling 28 days\"])\n    ax[i].set_title(\"{}\".format(cate_col[i]))","e79afd45":"# Sales volume per year\ncate_group.groupby(\"year\")['FOODS', 'HOBBIES', 'HOUSEHOLD'].sum().plot()\nplt.title(\"Sales volume per year\")\nplt.ylabel(\"Sales volume\");","863f1f55":"# HOBBIES of cat\nplot_ts_decomp(cate_group, \"HOBBIES\", 365, \"aqua\");","8b396212":"# HOUSEHOLD of cat\nplot_ts_decomp(cate_group, \"HOUSEHOLD\", 365, \"aqua\");","ffb863c8":"# FOODS of cat\nplot_ts_decomp(cate_group, \"FOODS\", 365, \"aqua\");","0f7c5728":"Image(\"..\/input\/modelplan\/p2.jpg\")","d78b8cc8":"# Define function\ndef lag_featrues(df):\n    out_df = df[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n    ###############################################################################\n    # day lag 29~57 day and last year's day lag 1~28 day \n    day_lag = df.iloc[:,-28:]\n    day_year_lag = df.iloc[:,-393:-365]\n    day_lag.columns = [str(\"lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    day_year_lag.columns = [str(\"lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    # Rolling mean(3) and (7) and (28) and (84) 29~57 day and last year's day lag 1~28 day \n    rolling_3 = df.iloc[:,-730:].T.rolling(3).mean().T.iloc[:,-28:]\n    rolling_3.columns = [str(\"rolling3_lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    rolling_3_year = df.iloc[:,-730:].T.rolling(3).mean().T.iloc[:,-393:-365]\n    rolling_3_year.columns = [str(\"rolling3_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_7 = df.iloc[:,-730:].T.rolling(7).mean().T.iloc[:,-28:]\n    rolling_7.columns = [str(\"rolling7_lag_{}_day\".format(i)) for i in range(29,57)] # Rename columns\n    rolling_7_year = df.iloc[:,-730:].T.rolling(7).mean().T.iloc[:,-393:-365]\n    rolling_7_year.columns = [str(\"rolling7_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_28 = df.iloc[:,-730:].T.rolling(28).mean().T.iloc[:,-28:]\n    rolling_28.columns = [str(\"rolling28_lag_{}_day\".format(i)) for i in range(29,57)]\n    rolling_28_year = df.iloc[:,-730:].T.rolling(28).mean().T.iloc[:,-393:-365]\n    rolling_28_year.columns = [str(\"rolling28_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    rolling_84 = df.iloc[:,-730:].T.rolling(84).mean().T.iloc[:,-28:]\n    rolling_84.columns = [str(\"rolling84_lag_{}_day\".format(i)) for i in range(29,57)]\n    rolling_84_year = df.iloc[:,-730:].T.rolling(84).mean().T.iloc[:,-393:-365]\n    rolling_84_year.columns = [str(\"rolling84_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    \n    # monthly lag 1~18 month\n    month_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly = df.iloc[:,-28*i:].T.sum().T\n            month_lag[\"monthly_lag_{}_month\".format(i)] = monthly\n        else:\n            monthly = df.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_lag[\"monthly_lag_{}_month\".format(i)] = monthly\n            \n    # combine day lag and monthly lag\n    out_df = pd.concat([out_df, day_lag], axis=1)\n    out_df = pd.concat([out_df, day_year_lag], axis=1)\n    out_df = pd.concat([out_df, rolling_3], axis=1)\n    out_df = pd.concat([out_df, rolling_3_year], axis=1)\n    out_df = pd.concat([out_df, rolling_7], axis=1)\n    out_df = pd.concat([out_df, rolling_7_year], axis=1)\n    out_df = pd.concat([out_df, rolling_28], axis=1)\n    out_df = pd.concat([out_df, rolling_28_year], axis=1)\n    out_df = pd.concat([out_df, rolling_84], axis=1)\n    out_df = pd.concat([out_df, rolling_84_year], axis=1)\n    out_df = pd.concat([out_df, month_lag], axis=1)\n    \n    ###############################################################################\n    # dept_id\n    group_dept = df.groupby(\"dept_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    dept_day_lag = group_dept.iloc[:,-28:]\n    dept_day_year_lag = group_dept.iloc[:,-393:-365]\n    dept_day_lag.columns = [str(\"dept_lag_{}_day\".format(i)) for i in range(29,57)]\n    dept_day_year_lag.columns = [str(\"dept_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_dept_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_dept = group_dept.iloc[:,-28*i:].T.sum().T\n            month_dept_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_dept\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly = group_dept.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_dept_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_dept\n    # combine out df\n    out_df = pd.merge(out_df, dept_day_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, dept_day_year_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_dept_lag, left_on=\"dept_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################       \n    # cat_id\n    group_cat = df.groupby(\"cat_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    cat_day_lag = group_cat.iloc[:,-28:]\n    cat_day_year_lag = group_cat.iloc[:,-393:-365]\n    cat_day_lag.columns = [str(\"cat_lag_{}_day\".format(i)) for i in range(29,57)]\n    cat_day_year_lag.columns = [str(\"cat_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_cat_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_cat = group_cat.iloc[:,-28*i:].T.sum().T\n            month_cat_lag[\"cat_monthly_lag_{}_month\".format(i)] = monthly_cat\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly_cat = group_cat.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_cat_lag[\"dept_monthly_lag_{}_month\".format(i)] = monthly_cat\n            \n    # combine out df\n    out_df = pd.merge(out_df, cat_day_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, cat_day_year_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_cat_lag, left_on=\"cat_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # store_id\n    group_store = df.groupby(\"store_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    store_day_lag = group_store.iloc[:,-28:]\n    store_day_year_lag = group_store.iloc[:,-393:-365]\n    store_day_lag.columns = [str(\"store_lag_{}_day\".format(i)) for i in range(29,57)]\n    store_day_year_lag.columns = [str(\"store_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_store_lag = pd.DataFrame({})\n    for i in range(1,19):\n        if i == 1:\n            monthly_store = group_store.iloc[:,-28*i:].T.sum().T\n            month_store_lag[\"store_monthly_lag_{}_month\".format(i)] = monthly_store\n        elif i >= 7 and i <13:\n            continue\n        else:\n            monthly_store = group_store.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_store_lag[\"store_monthly_lag_{}_month\".format(i)] = monthly_store\n            \n    # combine out df\n    out_df = pd.merge(out_df, store_day_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, store_day_year_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_store_lag, left_on=\"store_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # state_id\n    group_state = df.groupby(\"state_id\").sum()\n    # day lag 29~57 day and last year's day lag 1~28 day \n    state_day_lag = group_state.iloc[:,-28:]\n    state_day_year_lag = group_state.iloc[:,-393:-365]\n    state_day_lag.columns = [str(\"state_lag_{}_day\".format(i)) for i in range(29,57)]\n    state_day_year_lag.columns = [str(\"state_lag_{}_day_of_last_year\".format(i)) for i in range(1,29)]\n    # monthly lag 1~18 month\n    month_state_lag = pd.DataFrame({})\n    for i in range(1,13):\n        if i == 1:\n            monthly_state = group_state.iloc[:,-28*i:].T.sum().T\n            month_state_lag[\"state_monthly_lag_{}_month\".format(i)] = monthly_state\n        elif i >= 7 and i < 13:\n            continue\n        else:\n            monthly_state = group_state.iloc[:, -28*i:-28*(i-1)].T.sum().T\n            month_state_lag[\"state_monthly_lag_{}_month\".format(i)] = monthly_state\n            \n    # combine out df\n    out_df = pd.merge(out_df, state_day_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, state_day_year_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    out_df = pd.merge(out_df, month_state_lag, left_on=\"state_id\", right_index=True, how=\"left\")\n    \n    ###############################################################################\n    # category flag\n    col_list = ['dept_id', 'cat_id', 'store_id', 'state_id']\n    \n    df_cate_oh = pd.DataFrame({})\n    for i in col_list:\n        df_oh = pd.get_dummies(df[i])\n        df_cate_oh = pd.concat([df_cate_oh, df_oh], axis=1)\n        \n    out_df = pd.concat([out_df, df_cate_oh], axis=1)\n    \n    return out_df","b5bb1fce":"%%time\n# Features\nTrain_data = train.iloc[:,:-56]\nVal_data = train.iloc[:,:-28]\n\nX_train = lag_featrues(Train_data).iloc[:,5:] # select variables\ny_train = train.iloc[:,-56]\nX_test = lag_featrues(Val_data).iloc[:,5:]\ny_test = train.iloc[:,-28]\n\n# Create instance\nlgbm = lgb.LGBMRegressor()\n\n# Training and score\nlearning_rate = [0.15, 0.2, 0.25]\nmax_depth = [15, 20, 25]\n\nparam_grid = {'learning_rate': learning_rate, 'max_depth': max_depth}\n\n# Fitting\ncv_lgbm = GridSearchCV(lgbm, param_grid, cv=10, n_jobs =1)\ncv_lgbm.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lgbm.best_params_))\n\n# best params\nbest_lg = cv_lgbm.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_test_pred_lg = best_lg.predict(X_test)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_lg)))","d3048686":"# Feature importance\nimportance = best_lg.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\n# print importance\nimportance_df = pd.DataFrame({})\ncolumns = []\nimportance_ = []\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %.2f\" %(f+1, 30, X_train.columns[indices[f]], importance[indices[f]]))\n    col = X_train.columns[indices[f]]\n    imp = importance[indices[f]]\n    columns.append(col)\n    importance_.append(imp)\nimportance_df[\"col_name\"] = columns\nimportance_df[\"importance\"] = importance_","82a7989c":"# Visualization of prediction and test data\nplt.figure(figsize=(10,10))\nplt.scatter(y_train, y_train_pred_lg, s=10, color=\"aqua\", label=\"train data prediction\", alpha=0.5)\nplt.scatter(y_test, y_test_pred_lg, s=10, color=\"pink\", label=\"test data prediction\",  alpha=0.5)\nplt.xlabel(\"y real data\")\nplt.xlim([-5,130])\nplt.ylim([-5,130])\nplt.ylabel(\"y_predictin data\")\nplt.title(\"Prediction data plot\")","78a2f3d4":"%%time\n# importance columns (>0)\nimp_col = importance_df[importance_df[\"importance\"]>0][\"col_name\"].values\n\n# Train test split, select by imp_col\n\nX_train = lag_featrues(Train_data).iloc[:,5:][imp_col] # select variables\ny_train = train.iloc[:,-56]\nX_test = lag_featrues(Val_data).iloc[:,5:][imp_col]\ny_test = train.iloc[:,-28]\n\n# Create instance\nlgbm = lgb.LGBMRegressor()\n\n# Training and score\nlearning_rate = [0.15, 0.2, 0.25]\nmax_depth = [15, 20, 25]\n\nparam_grid = {'learning_rate': learning_rate, 'max_depth': max_depth}\n\n# Fitting\ncv_lgbm = GridSearchCV(lgbm, param_grid, cv=10, n_jobs =1)\ncv_lgbm.fit(X_train, y_train)\n\nprint(\"Best params:{}\".format(cv_lgbm.best_params_))\n\n# best params\nbest_lg = cv_lgbm.best_estimator_\n\n# prediction\ny_train_pred_lg = best_lg.predict(X_train)\ny_test_pred_lg = best_lg.predict(X_test)\n\nprint(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_lg)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_lg)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_lg)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_lg)))","6fde39c3":"%%time\n# Prediction last 28 days by roop\n\ndef lgbm_pred(X_train, y_train, X_test):\n    lgbm = lgb.LGBMRegressor(learning_rate=0.2, max_depth=20)\n    # Fitting\n    lgbm.fit(X_train, y_train)\n    # prediction\n    y_pred = lgbm.predict(X_test)\n    return y_pred\n\n# Features\n# Features\nTrain_data = train.iloc[:,:-56]\nY_train = train.iloc[:,-56:-28]\nVal_data = train.iloc[:,:-28]\nY_test = train.iloc[:,-28:]\n\nPred_data = pd.DataFrame({})\nfor d in range(0,28):\n    if d == 0:\n        X_train = lag_featrues(Train_data).iloc[:,5:][imp_col] # select variables\n        y_train = Y_train.iloc[:,d]\n        X_test = lag_featrues(Val_data).iloc[:,5:][imp_col]\n        \n        # Train test split, select by imp_col\n        pred = lgbm_pred(X_train, y_train, X_test)\n        Pred_data[\"pred_{}_day\".format(1+d)] = pred\n    else:\n        X_train = lag_featrues(Train_data.iloc[:,:-1])[imp_col]\n        y_train = Y_train.iloc[:,d]\n        X_test = lag_featrues(Train_data)[imp_col][imp_col]\n        \n        # Train test split, select by imp_col\n        pred = lgbm_pred(X_train, y_train, X_test)\n        Pred_data[\"pred_{}_day\".format(1+d)] = pred","08c8d315":"# Confirming prediction result\nMSE = []\nR2_score = []\n\nfor i in range(Y_test.shape[1]):\n    mse = mean_squared_error(Y_test.iloc[:,i], Pred_data.iloc[:,i])\n    r2 = r2_score(Y_test.iloc[:,i], Pred_data.iloc[:,i])\n    MSE.append(mse)\n    R2_score.append(r2)\n\nmonth_pred_score = pd.DataFrame({\"day\":range(1,29),\n                                 \"MSE\":MSE,\n                                 \"R2\":R2_score})\n\n# Visualization check\nfig, ax = plt.subplots(1,2, figsize=(20, 6))\n\nax[0].bar(month_pred_score[\"day\"], month_pred_score[\"MSE\"])\nax[0].set_xlabel(\"Prediction day\")\nax[0].set_ylabel(\"MSE\")\n\nax[1].bar(month_pred_score[\"day\"], month_pred_score[\"R2\"])\nax[1].set_xlabel(\"Prediction day\")\nax[1].set_ylabel(\"R2 score\")","9a66d4a7":"%%time\n# Prediction next 56 days by roop\n\n# Features\nTrain_data = train.iloc[:,:-28]\nTest_data = train\nY_train = train.iloc[:,-28:]\n\nPred_data_val = pd.DataFrame({})\nPred_data_eval = pd.DataFrame({})\n\nfor d in range(0,28):\n    X_train = lag_featrues(Train_data)[imp_col] # select variables\n    y_train = Y_train.iloc[:,d]\n    X_test = lag_featrues(Test_data)[imp_col]\n        \n    # Train test split, select by imp_col\n    pred = lgbm_pred(X_train, y_train, X_test)\n    Pred_data_val[\"pred_{}_day\".format(1+d)] = pred\n","f9bb94ef":"Train_data = train\nTest_data = pd.concat([train, Pred_data_val], axis=1)\nY_train = Pred_data_val\n\nfor d in range(0,28):\n    X_train = lag_featrues(Train_data)[imp_col]\n    y_train = Y_train.iloc[:,d]\n    X_test = lag_featrues(Test_data)[imp_col]\n        \n    # Train test split, select by imp_col\n    pred = lgbm_pred(X_train, y_train, X_test)\n    Pred_data_eval[\"pred_{}_day\".format(1+d)] = pred","16fd32a7":"Pred_data_val.columns = sample.loc[:,\"F1\":].columns\nPred_data_eval.columns = sample.loc[:,\"F1\":].columns\n\n# concat val data and eval data\nsubmit_data = pd.concat([Pred_data_val, Pred_data_eval])\nsubmit_data = submit_data.reset_index().drop(\"index\", axis=1)\n\ncol = sample.loc[:,'F1':].columns\n\nsample[col] = submit_data\n\n# Submission data\nsample.to_csv(\"submission.csv\", index=False)\n\n# Submission data\nsample.round(0).to_csv(\"submission_r.csv\", index=False)","b528c54e":"# Predictin of validataion\nsample.iloc[:30490,:].describe()","6b63eb33":"# Predictin of evaluation\nsample.iloc[30490:,:].describe()","bb219517":"sample.to_csv(\"mamit.csv\")","440c3b22":"## Component decomposition and autocorrelation\nBefore decomposition, looking at each data, we can see that the sales of one day fell sharply and cut. It is speculated that this is because it is taking a break at Christmas. This data will be a strong noise, so this time it will be deleted.","041b77cf":"* The feature quantity to be created is a lag feature quantity for each item and category feature quantity.\n* Get lag data for each item or category. The acquisition target is the data for the previous year in the same month as the first month of the month by date, the data for the previous 6 months prior to the current month, and the data for 6 months before the same month in the previous year.\n* Results are evaluated by the prediction result of the first date and selected by selecting important feature quantities.\n* Define the obtained results with new data points, reconstruct the model, make a forecast for the next day, and repeat this to obtain the forecast results for January.","bbb31f10":"# Reading and unndersytanding the data","b3663bff":"### Accuracy improvement approach\n1. Flag the day of the weekday and holidays according to the target data.\n1. Review of important features\n1. Noise elimination of lag feature amount. Residuals are generated using statsmodel, and lag features are created from the time series data from which the residuals have been subtracted.","8b32237b":"## By state_id, Count of selling time series analysis","207c4855":"# Price data Analysis","c662f22f":"## Predict using only variables with an importance of 1 or higher.","a58546bf":"### Test prediction by LGBM and feature importance check","b09b25ca":"# Loading the master data","72ae688e":"# Next\nReview the features and create new features to improve accuracy.\n\nAdjust parameters to improve accuracy.","fe1da307":"### Predict sales for next 2 month.\n1st, predict with train data next 28days. 2nd, predict with train data and 1st prediction data next 28days.","41e3f36d":"# Data Relationship","06c2fedd":"# Calender  data Analysis","b2f6fbae":"# Train Data Analysis","657c2196":"# Overview\nWe are going to use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details.","8dffbf10":"# Predection with LGBM\nCreate features and make predictions with LGBM","ebb5625f":"# Model Training plan","241f8268":"# Submission data prediction","eef5d04d":"## Lag features","8ad0a486":"# By cat_id, Count of selling time series analysis","396a131a":"# Time series EDA by each categorical value\nsample Item Id, count of selling time series analysis"}}