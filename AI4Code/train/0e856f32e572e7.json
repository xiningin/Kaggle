{"cell_type":{"a57d1919":"code","4b94a583":"code","10cbcb8e":"code","513b290b":"code","d12164f3":"code","19e49ad4":"code","97274a0a":"code","e150cfe3":"code","35483762":"code","6d1dca6e":"code","625743a9":"code","8e20cc78":"code","b53036c6":"code","886321d4":"code","286e1f1e":"code","3f3ce5e3":"code","1ca81faf":"code","e36c5787":"markdown","00de42f3":"markdown","d1288e28":"markdown","0f38b038":"markdown","ecb6161a":"markdown","e0382418":"markdown","13a310ad":"markdown","b4bfa438":"markdown","b52a47e8":"markdown","19a805c5":"markdown","23adddf9":"markdown","e54cf08a":"markdown"},"source":{"a57d1919":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b94a583":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","10cbcb8e":"data = pd.read_csv(\"..\/input\/tsf-datasets\/student_scores.csv\")\ndata.head()","513b290b":"data.shape","d12164f3":"# check for null values\ndata.isnull().any()","19e49ad4":"# defining the variables\nX = data[\"Hours\"]\ny = data[\"Scores\"]","97274a0a":"plt.scatter(X,y,c = \"r\",marker=\"x\")\nplt.xlabel(\"No. of hours\")\nplt.ylabel(\"Scores\")\nplt.title(\"Hours vs Score\")\nplt.grid(True)\nplt.show()","e150cfe3":"# adding another axis\nX = X[:,np.newaxis]\ny = y[:,np.newaxis]\ntheta = np.zeros([2,1])","35483762":"# allocate valoes to the variables\niterations = 500\nalpha = 0.01\nm = len(y)\nones = np.ones((m,1))\nX = np.hstack((ones, X))","6d1dca6e":"def computeCost(X,y,theta):\n    hx = np.dot(X,theta)\n    prediction = hx - y\n    cost = np.sum(np.power(prediction,2))\/(2*m)\n    return cost\n\nJ = computeCost(X,y,theta)\nprint(J)","625743a9":"def gradientDescent(X,y,theta,alpha,iterations):\n    J_history=[]\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        error = np.dot(X.transpose(),(predictions -y))\n        descent=alpha * 1\/m * error\n        theta-=descent\n        J_history.append(computeCost(X,y,theta))\n    \n    return theta, J_history\n\ntheta,J_history = gradientDescent(X,y,theta,0.01,500)\nprint(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1\")","8e20cc78":"from mpl_toolkits.mplot3d import Axes3D\n\n#Generating values for theta0, theta1 and the resulting cost value\ntheta0_vals=np.linspace(-10,10,100)\ntheta1_vals=np.linspace(-1,4,100)\nJ_vals=np.zeros((len(theta0_vals),len(theta1_vals)))\n\nfor i in range(len(theta0_vals)):\n    for j in range(len(theta1_vals)):\n        t=np.array([theta0_vals[i],theta1_vals[j]])\n        J_vals[i,j]=computeCost(X,y,t)\n\n#Generating the surface plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nsurf=ax.plot_surface(theta0_vals,theta1_vals,J_vals,cmap=\"coolwarm\")\nfig.colorbar(surf, shrink=0.5, aspect=5)\nax.set_xlabel(\"$\\Theta_0$\")\nax.set_ylabel(\"$\\Theta_1$\")\nax.set_zlabel(\"$J(\\Theta)$\")\n\n#rotate for better angle\nax.view_init(30,120)","b53036c6":"def myfit(xval):\n    return theta[0] + theta[1]*xval\n\nplt.figure(figsize=(10,6))\nplt.plot(X[:,1],y[:,0],'rx',markersize=10,label='Training Data')\nplt.plot(X[:,1],myfit(X[:,1]),'b-',label = 'Hypothesis: h(x) = %0.2f + %0.2fx'%(theta[0],theta[1]))\nplt.grid(True) \nplt.ylabel('Percentage')\nplt.xlabel('Hours')\nplt.legend()","886321d4":"def predict(x,theta):\n    \"\"\"\n    Takes in numpy array of x and theta and return the predicted value of y based on theta\n    \"\"\"\n    predictions= np.dot(theta.transpose(),x)\n    \n    return predictions[0]\n\npredict1 = predict(np.array([1,9.25]),theta)\nprint(\"For Hours = 9.25 we predict a percentage of \"+str(round(predict1,2)))","286e1f1e":"actual1 = data.loc[6][\"Scores\"]\nactual1","3f3ce5e3":"MSE = np.square(np.subtract(actual1,predict1)).mean()\nMSE","1ca81faf":"import math\nRMSE = math.sqrt(MSE)\nRMSE","e36c5787":"# Implementing the gradient descent function","00de42f3":"# Import the data","d1288e28":"Student percentage is predicted based on number of hours using **Gradient Descent** function that I learnt in AndrewNg Course.\n\nThe loss can be decreased futher by increasing ***no. of iterations*** or ***changing alpha values*** but I've kept it less as we have a small dataset.\n\nTry to Implement this function on some large dataset\n\nIf you find this notebook useful *upvote* it so that everyone can get benefit.\nAlso, in case of any queries, feel free to ask..","0f38b038":"# Calculating the errors\n## 1. Mean squared error","ecb6161a":"# PLot the data","e0382418":"# Plotting 3D graph","13a310ad":"# Implementing the cost function","b4bfa438":"## 2.Root mean squared error","b52a47e8":"# Conclusion","19a805c5":"# Import libraries","23adddf9":"# Making predictions","e54cf08a":"# Fitting the line over the data"}}