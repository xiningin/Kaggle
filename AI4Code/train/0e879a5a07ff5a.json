{"cell_type":{"e78f8bf2":"code","77ce898a":"code","9c83bad2":"code","262c696a":"code","48450532":"code","19eb44bd":"code","54d00d83":"code","e3beaa75":"code","09dc0f31":"code","7f68818d":"code","91636a33":"code","80a785cf":"code","756905de":"code","141915f4":"code","b9c43e7b":"code","78a62315":"markdown","ad2363c6":"markdown","025fe9cc":"markdown","000e0bd6":"markdown","c12cef2c":"markdown","2e0ef10f":"markdown","cced0b2b":"markdown","e1d8b317":"markdown","4f663a30":"markdown","c3ca5e72":"markdown","e241c48a":"markdown","d2722f92":"markdown","da884d58":"markdown","c1e76e6e":"markdown","ed961725":"markdown","96cb0e65":"markdown","d9e08a61":"markdown","fb593439":"markdown","4346d97d":"markdown","c9fac3d2":"markdown","6a4c7a8b":"markdown","c35b80fd":"markdown","22ea9fc9":"markdown","82e55e49":"markdown","8aab414f":"markdown"},"source":{"e78f8bf2":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVR\nimport math\nfrom sklearn import metrics","77ce898a":"data = datasets.load_boston()\ndf = pd.DataFrame(data['data'], columns=data['feature_names'])\ndf['MEDV'] = data['target']\ndf.head()","9c83bad2":"df.describe()","262c696a":"plt.figure(figsize=(12, 12))\nsns.heatmap(df.corr(), annot=df.corr(), cmap='seismic_r')\nplt.show()","48450532":"price_correlation = df.corr()['MEDV']\ncolumns = list()\n\nfor i, corr in enumerate(price_correlation):\n    if corr >= 0.5 or corr < -0.5:\n        columns.append(df.columns[i])\n        print(df.columns[i], '=', corr)\n        \nnew_df = df[columns]\nnew_df.head()","19eb44bd":"sns.pairplot(new_df)\nplt.show()","54d00d83":"x = new_df.iloc[:, 0].values.reshape((-1, 1))\ny = new_df.iloc[:, -1].values","e3beaa75":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.15, random_state=0)\n\nprint('Tamanho do conjunto de dados de treino:%d'%x_train.size)\nprint('Tamanho do conjunto de dados de teste:%d'%x_test.size)","09dc0f31":"svr_linear = SVR(kernel='linear').fit(x_train, y_train)\nsvr_polyd2 = SVR(kernel='poly', degree=2).fit(x_train, y_train)\nsvr_polyd3 = SVR(kernel='poly', degree=3).fit(x_train, y_train)\nsvr_rbf = SVR(kernel='rbf').fit(x_train, y_train)","7f68818d":"# Obtendo regress\u00e3o para o conjunto de treinamento e teste\nypred_train_linear = svr_linear.predict(x_train)\nypred_test_linear = svr_linear.predict(x_test)\n\n# Configurando estrutura geral do gr\u00e1fico\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plotando informa\u00e7\u00f5es do conjunto de treinamento\nax[0].set_title('SVR com kernel linear\\nDados de treinamento', fontdict=dict(fontsize=14))\nax[0].scatter(x_train, y_train, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[0].scatter(x_train, ypred_train_linear, c='red', alpha=.75, label='SVR linear')\nax[0].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[0].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[0].legend()\n\n# Plotando informa\u00e7\u00f5es do conjunto de teste\nax[1].set_title('SVR com kernel linear\\nDados de teste', fontdict=dict(fontsize=14))\nax[1].scatter(x_test, y_test, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[1].scatter(x_test, ypred_test_linear, c='red', alpha=.75, label='SVR linear')\nax[1].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[1].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[1].legend()\nplt.show()","91636a33":"# Obtendo regress\u00e3o para o conjunto de treinamento e teste para kernel polinomial d2\nypred_train_polyd2 = svr_polyd2.predict(x_train)\nypred_test_polyd2 = svr_polyd2.predict(x_test)\n\n# Obtendo regress\u00e3o para o conjunto de treinamento e teste para kernel polinomial d3\nypred_train_polyd3 = svr_polyd3.predict(x_train)\nypred_test_polyd3 = svr_polyd3.predict(x_test)\n\n# Configurando estrutura geral do gr\u00e1fico\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plotando informa\u00e7\u00f5es do conjunto de treinamento\nax[0].set_title('SVR com kernel polinomial\\nDados de treinamento', fontdict=dict(fontsize=14))\nax[0].scatter(x_train, y_train, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[0].scatter(x_train, ypred_train_polyd2, c='red', alpha=.75, label='SVR poly grau 2')\nax[0].scatter(x_train, ypred_train_polyd3, c='green', alpha=.75, label='SVR poly grau 3')\nax[0].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[0].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[0].legend()\n\n# Plotando informa\u00e7\u00f5es do conjunto de teste\nax[1].set_title('SVR com kernel polinomial\\nDados de teste', fontdict=dict(fontsize=14))\nax[1].scatter(x_test, y_test, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[1].scatter(x_test, ypred_test_polyd2, c='red', alpha=.75, label='SVR poly grau 2')\nax[1].scatter(x_test, ypred_test_polyd3, c='green', alpha=.75, label='SVR poly grau 3')\nax[1].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[1].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[1].legend()\nplt.show()","80a785cf":"# Obtendo regress\u00e3o para o conjunto de treinamento e teste\nypred_train_rbf = svr_rbf.predict(x_train)\nypred_test_rbf = svr_rbf.predict(x_test)\n\n# Configurando estrutura geral do gr\u00e1fico\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plotando informa\u00e7\u00f5es do conjunto de treinamento\nax[0].set_title('SVR com kernel RVF\\nDados de treinamento', fontdict=dict(fontsize=14))\nax[0].scatter(x_train, y_train, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[0].scatter(x_train, ypred_train_rbf, c='red', alpha=.75, label='SVR RBF')\nax[0].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[0].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[0].legend()\n\n# Plotando informa\u00e7\u00f5es do conjunto de teste\nax[1].set_title('SVR com kernel RBF\\nDados de teste', fontdict=dict(fontsize=14))\nax[1].scatter(x_test, y_test, c='blue', ec='black', s=100, alpha=.75, label='Dados')\nax[1].scatter(x_test, ypred_test_rbf, c='red', alpha=.75, label='SVR RBF')\nax[1].set_xlabel('RM', fontdict=dict(fontsize=12))\nax[1].set_ylabel('MEDV', fontdict=dict(fontsize=12))\nax[1].legend()\nplt.show()","756905de":"# Configurando estrutura geral do gr\u00e1fico\nfig, ax = plt.subplots(1, 3, figsize=(17, 5))\n\n# Monstrando compara\u00e7\u00e3o entre os valores reais e o previsto SVR de kernel linear\nax[0].set_title('Comparara\u00e7\u00e3o SVR linear')\nax[0].scatter(y_test, ypred_test_linear, color='k', edgecolor='k', alpha=.75)\nax[0].set_xlabel('Pre\u00e7o real')\nax[0].set_ylabel('Pre\u00e7o previsto')\nax[0].plot([min(y_test), max(y_test)], [min(ypred_test_linear), max(ypred_test_linear)], label='Diagonal secund\u00e1ria', linestyle='--', alpha=.75)\nax[0].legend()\n\n# Monstrando compara\u00e7\u00e3o entre os valores reais e o previsto SVR de kernel polinomial\nax[1].set_title('Comparara\u00e7\u00e3o SVR polinomial')\nax[1].scatter(y_test, ypred_test_polyd3, color='k', edgecolor='k', alpha=.75)\nax[1].set_xlabel('Pre\u00e7o real')\nax[1].set_ylabel('Pre\u00e7o previsto')\nax[1].plot([min(y_test), max(y_test)], [min(ypred_test_polyd3), max(ypred_test_polyd3)], label='Diagonal secund\u00e1ria', linestyle='--', alpha=.75)\nax[1].legend()\n\n# Monstrando compara\u00e7\u00e3o entre os valores reais e o previsto SVR de kernel RBF\nax[2].set_title('Comparara\u00e7\u00e3o SVR polinomial')\nax[2].scatter(y_test, ypred_test_rbf, color='k', edgecolor='k', alpha=.75)\nax[2].set_xlabel('Pre\u00e7o real')\nax[2].set_ylabel('Pre\u00e7o previsto')\nax[2].plot([min(y_test), max(y_test)], [min(ypred_test_rbf), max(ypred_test_rbf)], label='Diagonal secund\u00e1ria', linestyle='--', alpha=.75)\nax[2].legend()\n\nplt.show()","141915f4":"# Configurando estrutura geral do gr\u00e1fico\nfig, ax = plt.subplots(1, 3, figsize=(17, 5))\n\nerror_linear = y_test-ypred_test_linear\nerror_poly3d = y_test-ypred_test_polyd3\nerror_rbf = y_test-ypred_test_rbf\n\n# Monstrando frequ\u00eancia dos erros entre os valores reais e o previsto SVR de kernel linear\nax[0].set_title('Comparara\u00e7\u00e3o SVR linear')\nax[0].hist(error_linear, color='black')\nax[0].set_xlabel('Erro entre valor real e previsto')\nax[0].set_ylabel('Ocorr\u00eancias')\n\n# Monstrando frequ\u00eancia dos erros entre os valores reais e o previsto SVR de kernel linear\nax[1].set_title('Comparara\u00e7\u00e3o SVR polinomial grau 3')\nax[1].hist(error_poly3d, color='black')\nax[1].set_xlabel('Erro entre valor real e previsto')\nax[1].set_ylabel('Ocorr\u00eancias')\n\n# Monstrando frequ\u00eancia dos erros entre os valores reais e o previsto SVR de kernel linear\nax[2].set_title('Comparara\u00e7\u00e3o SVR RBF')\nax[2].hist(error_rbf, color='black')\nax[2].set_xlabel('Erro entre valor real e previsto')\nax[2].set_ylabel('Ocorr\u00eancias')\n\nplt.show()","b9c43e7b":"kerners = ['Linear', 'Poli grau 2', 'Poli grau 3', 'RBF']\n\n# Calculando erro m\u00e9dio absoluto para todos os modelos\nmae_linear = metrics.mean_absolute_error(y_test, ypred_test_linear)\nmae_poly2d = metrics.mean_absolute_error(y_test, ypred_test_polyd2)\nmae_poly3d = metrics.mean_absolute_error(y_test, ypred_test_polyd3)\nmae_rbf = metrics.mean_absolute_error(y_test, ypred_test_rbf)\nmae_kernels = [mae_linear, mae_poly2d, mae_poly3d, mae_rbf]\n\n# Calculando erro m\u00e9dio quadr\u00e1tico para todos os modelos\nmse_linear = metrics.mean_squared_error(y_test, ypred_test_linear)\nmse_poly2d = metrics.mean_squared_error(y_test, ypred_test_polyd2)\nmse_poly3d = metrics.mean_squared_error(y_test, ypred_test_polyd3)\nmse_rbf = metrics.mean_squared_error(y_test, ypred_test_rbf)\nmse_kernels = [mse_linear, mse_poly2d, mse_poly3d, mse_rbf]\n\n# Calculando raiz do erro m\u00e9dio quadr\u00e1tico para todos os modelos\nrmse_linear = math.sqrt(metrics.mean_absolute_error(y_test, ypred_test_linear))\nrmse_poly2d = math.sqrt(metrics.mean_absolute_error(y_test, ypred_test_polyd2))\nrmse_poly3d = math.sqrt(metrics.mean_absolute_error(y_test, ypred_test_polyd3))\nrmse_rbf = math.sqrt(metrics.mean_absolute_error(y_test, ypred_test_rbf))\nrmse_kernels = [rmse_linear, rmse_poly2d, rmse_poly3d, rmse_rbf]\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\nax[0].set_title('Erro m\u00e9dio absoluto')\nax[0].bar(kerners, mae_kernels, color='black')\nax[0].set_xlabel('Kernel')\nax[0].set_ylabel('MAE')\n\nax[1].set_title('Erro m\u00e9dio quadr\u00e1tico')\nax[1].bar(kerners, mse_kernels, color='black')\nax[1].set_xlabel('Kernel')\nax[1].set_ylabel('MSE')\n\nax[2].set_title('Raiz do erro m\u00e9dio quadr\u00e1tico')\nax[2].bar(kerners, rmse_kernels, color='black')\nax[2].set_xlabel('Kernel')\nax[2].set_ylabel('RMSE')\nplt.show()","78a62315":"Como pode ser observado no gr\u00e1fico da figura acima, \u00e9 poss\u00edvel identificar de forma l\u00facida as vari\u00e1veis mais correlacionadas positivamente e negativamente. As cores mais pr\u00f3xima do azul est\u00e3o associadas a correla\u00e7\u00e3o positiva, enquanto que no vermelho est\u00e1 associado \u00e0s correla\u00e7\u00f5es negativas. Essas rela\u00e7\u00f5es s\u00e3o observadas atrav\u00e9s da diagonal superior ou inferior da matriz de corre\u00e7\u00e3o. Afim de facilitar a manipula\u00e7\u00e3o desses dados, ser\u00e3o filtradas somente as vari\u00e1veis com correla\u00e7\u00e3o com o pre\u00e7o dos im\u00f3veis $>=|0.5|$.","ad2363c6":"## Descri\u00e7\u00e3o da atividade <a id=\"description-task\"><\/a> [^](#header-notebook)\n<hr \/>\n\nNeste exemplo ser\u00e1 utilizado o algoritmo de m\u00e1quinas de vetores de suporte (SVR) para tarefas de regress\u00e3o utilizando m\u00faltiplas vari\u00e1veis preditoras para estimar os pre\u00e7os das resid\u00eancias do conjunto de dados Boston House Pricing. Antes da aplica\u00e7\u00e3o do modelo ser\u00e1 feito uma breve an\u00e1lise explorat\u00f3ria dos dados e a sua separa\u00e7\u00e3o em grupos de treinamento e teste. O modelo de regress\u00e3o ser\u00e1 criado utilizando a biblioteca `scikit-learn`. Ao final ser\u00e1 feita a avalia\u00e7\u00e3o do modelo de forma subjetiva, atrav\u00e9s de gr\u00e1ficos e de forma mais objetiva, utilizando as m\u00e9tricas implementadas tamb\u00e9m na biblioteca `scikit-learn`.\n\n> **Aviso**: Para a maioria das atividades que envolvem algoritmos de aprendizagem de m\u00e1quina, \u00e9 recomendado que haja uma certa dedica\u00e7\u00e3o na realiza\u00e7\u00e3o de uma an\u00e1lise explorat\u00f3ria aprofundada, principalmente se os dados n\u00e3o forem muito bem conhecidos. A an\u00e1lise explorat\u00f3ria dos dados pode ajudar a ter uma vis\u00e3o geral r\u00e1pida a respeito do *dataset* que est\u00e1 sendo trabalhado. Para fins did\u00e1ticos, a an\u00e1lise explorat\u00f3ria dos dados n\u00e3o ser\u00e1 muito aprofundada aqui.","025fe9cc":"Outro recurso interessante para ser utilizado na explora\u00e7\u00e3o dos dados s\u00e3o as correla\u00e7\u00f5es entre os atributos, j\u00e1 que as t\u00e9cnicas de regress\u00e3o est\u00e3o diretamente associadas a elas. Com poucas linhas de c\u00f3digo \u00e9 poss\u00edvel obser um gr\u00e1fico com a matriz de correla\u00e7\u00e3o das vari\u00e1veis, como mostrado a seguir:","000e0bd6":"O trecho a seguir, calcula a regress\u00e3o para os dados de treinamento e valida\u00e7\u00e3o para o modelo linear de m\u00e1quinas de vetores de suporte:","c12cef2c":"# Exemplo de aplica\u00e7\u00e3o da regress\u00e3o com m\u00e1quinas de vetores de suporte\n\n**Instrutores**: Adriano Almeida, Felipe Carvalho e Felipe Menino\n\n**Realiza\u00e7\u00e3o**: Dia 15\/09\n\n\n**Descri\u00e7\u00e3o**:Este notebook tem como pr\u00f3posito apresentar a aplica\u00e7\u00e3o da regress\u00e3o com m\u00e1quinas de suporte de vetores no conjunto de dados [*Boston House Prices*](https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/) utilizando a biblioteca [*scikit-learn*](https:\/\/scikit-learn.org) na linguagem de programa\u00e7\u00e3o [Python](http:\/\/python.org).\n\n\n**Sum\u00e1rio**:\n* [Descri\u00e7\u00e3o do conjunto de dados](#description-dataset)\n* [Descri\u00e7\u00e3o da atividade](#description-task)\n* [Importa\u00e7\u00e3o das bibliotecas](#import)\n* [Carregando base de dados](#load-dataset)\n* [An\u00e1lise explorat\u00f3ria dos dados](#eda)\n* [Separando conjunto de dados](#split)\n* [Aplicando a regress\u00e3o com m\u00e1quinas de vetores de suporte](#model)\n* [Avalia\u00e7\u00e3o do modelo](#evaluation)\n* [Agora \u00e9 com voc\u00ea](#homework)\n* [Refer\u00eancias](#references)\n\n**Links \u00dateis**:\n* [Livro **Introdu\u00e7\u00e3o ao Machine Learning**](https:\/\/dataat.github.io\/introducao-ao-machine-learning\/)\n* [Exemplo de **Classifica\u00e7\u00e3o** em Python](https:\/\/www.kaggle.com\/phelpsmemo\/intro-ml-python-knn-worcap2020)\n* [Exemplo de **Regress\u00e3o Linear** em Python](https:\/\/www.kaggle.com\/lordadriano\/mc2-worcap-2020-linear-regression)\n* [Exemplo de **Agrupamento Hierquico** em R](https:\/\/www.kaggle.com\/oldlipe\/intro-ml-r-hiererquico-worcap2020)\n\n> **Aten\u00e7\u00e3o**: Este documento foi redigido apenas para fins did\u00e1ticos, sendo assim, as t\u00e9cnicas e conjunto de dados utilizados aqui podem e devem ser evolu\u00eddos em caso de sua continuidade com cunho cient\u00edfico.","2e0ef10f":"## Separando conjunto de dados <a id=\"split\"><\/a> [^](#header-notebook)\n<hr \/>\n\nDurante as tarefas que envolvem treinamento e testes de modelos de aprendizado de m\u00e1quina, \u00e9 altamente recomendado que o conjunto de dados dispon\u00edvel seja separado em grupos de treinamento e teste ou treinamento, teste e valida\u00e7\u00e3o. A divis\u00e3o do conjunto de dados evita que o modelo fique enviezado, ou seja, que ele fique especialista somente nos dados aos quais ele foi treinado e perca a sua capacidade de generaliza\u00e7\u00e3o, conhecido como *overfitting*. Neste exemplo, n\u00e3o h\u00e1 necessidade de ser utilizado o conjunto de valida\u00e7\u00e3o, pois para preparar o modelo ser\u00e1 utilizado apenas conjunto de treinamento e a avalia\u00e7\u00e3o com o conjunto de testes. A cria\u00e7\u00e3o do terceiro grupo de dados (valida\u00e7\u00e3o) pode ser utilizados quando o modelo ser\u00e1 testado durante o treinamento.\n\nAntes de realizar a divis\u00e3o do conjunto de dados, ser\u00e3o criadas listas separadas para armazenas as informa\u00e7\u00f5es de cada atributo que ser\u00e1 utilizado neste exemplo, conforme mostrado no trecho de c\u00f3digo a seguir:","cced0b2b":"As m\u00e9tricas de erro mais utilizadas para se avaliar os m\u00e9todos de regress\u00e3o s\u00e3o o erro m\u00e9dio absoluto (MAE - sigla do ingl\u00eas, *mean absolute error*), erro quadr\u00e1tico m\u00e9dio (MSE - sigla do ingl\u00eas, *mean square error*) e a raiz do erro quadr\u00e1tico m\u00e9dio (RMSE - sigla do ingl\u00eas, *root mean square error*), Essas e outras m\u00e9tricas est\u00e3o implementadas no `sklearn`. O MAE, como o pr\u00f3prio nome sugere, est\u00e1 relacionado \u00e0 m\u00e9dia dos erros absolutos entre os valores reais e os previstos, obtido pela soma das diferen\u00e7as entre eles. O MSE \u00e9 a m\u00e9dia da diferen\u00e7a entre os valores reais e previstos elevado ao quadrado. J\u00e1 o RMSE \u00e9 a raiz do erro m\u00e9dio quadr\u00e1tico m\u00e9dio entre os valores previstos e os valores reais. No trecho a seguir s\u00e3o apresentadas as tr\u00eas m\u00e9tricas mencionadas:","e1d8b317":"## Agora \u00e9 com voc\u00ea <a id=\"homework\"><\/a> [^](#header-notebook)\n<hr \/>\n\n> Somente a quantidade m\u00e9dia de quartos talvez n\u00e3o seja um bom par\u00e2metro para a estimativa dos pre\u00e7os dos im\u00f3veis, n\u00e3o \u00e9 mesmo?\n\n\ud83d\udcda Como sugest\u00e3o de atividade, aplique a t\u00e9cnica apresentada com m\u00faltiplas vari\u00e1veis preditoras e verifique se com isso o modelo fica mais preciso nas previs\u00f5es.","4f663a30":"Os modelos de aprendizado de m\u00e1quina devem ter uma capacidade de generaliza\u00e7\u00e3o razo\u00e1vel, sendo assim, dificilmente acertar\u00e3o os valores exatos, principalmente em tarefas de regress\u00e3o. O gr\u00e1fico de dispers\u00e3o acima tem como intuito mostrar visualmente a correla\u00e7\u00e3o entre os dados observados e os previstos pela regress\u00e3o. Diante disso, pode ser observado nos gr\u00e1ficos da figura acima que o SVR com *kernel* linear \u00e9 o que pior se ajusta aos dados, enquanto que utilizando o *kernel* polinomial, o resultado fica um pouco melhor. O trecho do gr\u00e1fico a seguir mostra um histograma com a diferen\u00e7a dos erros para cada modelo.","c3ca5e72":"## Descri\u00e7\u00e3o do conjunto de dados <a id=\"description-dataset\"><\/a> [^](#header-notebook)\n<hr \/>\n\n![](https:\/\/raw.githubusercontent.com\/AdrianoPereira\/introducao-ao-machine-learning\/master\/src\/assets\/boston-reg.png)\n\nO conjunto de dados utilizado neste notebook \u00e9 o Boston House Prices, originalmente publicado por Harrison e Rubinfeld (1978). Estes dados foram coletados de diferentes \u00e1reas da capital de Massachusetts, Boston, pelo U.S Census Service em 1970 e possui informa\u00e7\u00f5es associadas aos pre\u00e7os dos im\u00f3veis dessas regi\u00f5es. O Boston House Prices \u00e9 amplamente utilizado como exemplo em exerc\u00edcios de aprendizado de m\u00e1quina, em especial para tarefas de regress\u00e3o. Este conjunto de dados possu\u00ed 506 registros e 13 atributos descritos a seguir:\n\n- **CRIM**: A taxa de crime percapita.\n- **ZN**: Propor\u00e7\u00e3o de terrenos para lotes residenciais com mais de 25000 p\u00e9s quadrados.\n- **INDUS**: Propor\u00e7\u00e3o de acres para neg\u00f3cios n\u00e3o varejistas.\n- **CHAS**: Pr\u00f3ximidade do rio (1 se o rio itercepta a \u00e1rea, 0 caso contr\u00e1rio).\n- **NOX**: Concentra\u00e7\u00e3o de \u00f3xidos n\u00edtricos (partes por 10 milh\u00f5es).\n- **RM**: N\u00famero m\u00e9dio de quartos por resid\u00eancia.\n- **AGE**: Propor\u00e7\u00e3o de unidades ocupadas pelo propriet\u00e1rio antes de 1940.\n- **DIS**: Dist\u00e2ncia ponderada de at\u00e9 cinco grandes centros de trabalho em Boston.\n- **RAD**: \u00cdndice de acessibilidade \u00e0s rodovias radiais.\n- **TAX**: Taxa de imposto sobre a propriedade de valor total por 10 d\u00f3lares.\n- **PTRATIO**: Propor\u00e7\u00e3o de alunos por professores.\n- **B**: Propor\u00e7\u00e3o de Afro-americanos.\n- **LSTAT**: Porcentagem de propriet\u00e1rios no bairro considerados de baixa renda.\n- **MEDV**: Valor m\u00e9dio das casas ocupadas pelos propriet\u00e1rios por 1000 d\u00f3lares.","e241c48a":"O trecho a seguir, calcula a regress\u00e3o para os dados de treinamento e valida\u00e7\u00e3o para o modelo de m\u00e1quinas de vetores de suporte com *kernel* RBF:","d2722f92":"## Carregando base de dados <a id=\"load-dataset\"><\/a> [^](#header-notebook)\n<hr \/>\n\nO bloco a seguir mostra o trecho que faz a importa\u00e7\u00e3o do conjunto de dados que ser\u00e1 utilizado.\n\n> **Dica**: O `Sklearn` possui dispon\u00edveis fun\u00e7\u00f5es para a importa\u00e7\u00e3o alguns dos principais conjuntos de dados utizados em tarefas de aprendizado de m\u00e1quina. Estes dados s\u00e3o provenientes do reposit\u00f3rio [UCI Machine Learning](https:\/\/archive.ics.uci.edu).","da884d58":"<a id=\"header-notebook\"><\/a>\n![](https:\/\/raw.githubusercontent.com\/AdrianoPereira\/adrianopereira.github.io\/master\/assets\/banner_notebook.png)","c1e76e6e":"The Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.","ed961725":"O par\u00e2metro `test_size` da fun\u00e7\u00e3o `train_test_split` indica a porcentagem de dados que ser\u00e1 reservada para o grupo de teste e o atributo `random_state` \u00e9 para garantir que a fun\u00e7\u00e3o sempre ir\u00e1 pegar a mesma amostra aleat\u00f3ria de dados para cada grupo.","96cb0e65":"## Importa\u00e7\u00e3o das bibliotecas <a id=\"import\"><\/a> [^](#header-notebook)\n<hr \/>\n\nAntes de mais nada, devemos importar as bibliotacas e o conjunto de dados que ser\u00e1 utilizado durante a atividade. O conjunto de dados, modelo e m\u00e9tricas de avalia\u00e7\u00e3o ser\u00e3o utilizado a partir dos m\u00f3dulos do `sklearn`. Al\u00e9m disso, ser\u00e1 utilizado a biblioteca `Pandas` para a organiza\u00e7\u00e3o do conjunto de dados. Os gr\u00e1ficos utilizados na an\u00e1lise explorat\u00f3ria dos dados e nos resultados ser\u00e3o criados a partir das bibliotecas `Matplotlib` e `Seaborn`.","d9e08a61":"A c\u00e9lula acima seleciona os nomes das colunas que possuem correla\u00e7\u00e3o razo\u00e1vel com os pre\u00e7os dos im\u00f3veis, e adiciona os dados referentes \u00e0 elas em um novo `Pandas DataFrame`.\n\nOutra ferramenta que pode auxiliar na visualiza\u00e7\u00e3o da correla\u00e7\u00f5es das vari\u00e1veis \u00e9 o gr\u00e1fico de dispers\u00e3o (*scatter plot*). No trecho a seguir \u00e9 executado o comando para visualizar o gr\u00e1fico de disper\u00e7\u00f5es, entre todas as vari\u00e1veis.","fb593439":"O trecho mostrado na c\u00e9lula acima mostra a importa\u00e7\u00e3o da base de dadas a partir da fun\u00e7\u00e3o `load_boston()` do m\u00f3dulo `datasets` dispon\u00edvel no `Sklearn`. Em seguida os dados s\u00e3o transformados em um `Pandas DataFrame`. ","4346d97d":"No gr\u00e1fico acima as rela\u00e7\u00f5es entre as vari\u00e1veis ficam bem mais destacas. Pode ser observado que a vari\u00e1vel referente ao n\u00famero m\u00e9dio de quartos por resid\u00eancia (RM) possui correla\u00e7\u00e3o positiva, enquanto que as vari\u00e1veis referentes a propor\u00e7\u00e3o de alunos por professores (PTRATIO) e porcentagem de propriet\u00e1rios no bairro considerados de baixa renda (LSTAT), possuem correla\u00e7\u00e3o negativa com os pre\u00e7os dos im\u00f3veis.\n\n> **Importante**: Nem sempre a alta correla\u00e7\u00e3o entre as vari\u00e1veis pode indicar necessariamente que elas estejam associadas, isso pode ser uma causalidade. ","c9fac3d2":"## Avalia\u00e7\u00e3o do modelo <a id=\"evaluation\"><\/a> [^](#header-notebook)\n<hr \/>\nAl\u00e9m de auxiliarem na an\u00e1lise explorat\u00f3ria dos dados, os gr\u00e1ficos tamb\u00e9m podem ser uma \u00f3tima ferramenta para an\u00e1lise dos resultados. A seguir \u00e9 mostrado um gr\u00e1fico de dispers\u00e3o dos dados reais em fun\u00e7\u00e3o dos dados previstos por cada um dos modelos.","6a4c7a8b":"## Aplicando SVR pra uma vari\u00e1vel preditora <a id=\"model\"><\/a> [^](#header-notebook)\n<hr \/>\n\nUm vez realizados as an\u00e1lises e preprocessamentos dos dados, a regress\u00e3o poder\u00e1 ser aplicada. O trecho a seguir, cria e treina o modelo com a regress\u00e3o com m\u00e1quinas de vetores de suporte utilizando os *kernels* `linear`, `polinomial` (de graus 2 e 3) e `RBF` (Fun\u00e7\u00e3o gaussiana de base radial):","c35b80fd":"O trecho a seguir, calcula a regress\u00e3o para os dados de treinamento e valida\u00e7\u00e3o para o modelo de m\u00e1quinas de vetores de suporte com *kernels* polinomiais de graus 2 e 3:","22ea9fc9":"## An\u00e1lise explorat\u00f3ria dos dados <a id=\"eda\"><\/a> [^](#header-notebook)\n<hr \/>\n\nO `Pandas DataFrame` possui alguns m\u00e9todos que podem auxiliar no in\u00edcio da explora\u00e7\u00e3o dos dados. O m\u00e9todo `describe` mostra algumas informa\u00e7\u00f5es estat\u00edsticas a repeito dos dados n\u00famericos, conforme apresentado a seguir:","82e55e49":"## Refer\u00eancias <a id=\"references\"><\/a> [^](#header-notebook)","8aab414f":"Para a separa\u00e7\u00e3o dos conjuntos de dados em grupos de treinamento e teste, ser\u00e1 utilizada a fun\u00e7\u00e3o `train_test_split`, dispon\u00edvel no `sklearn`, conforme mostrado no trecho a seguir:"}}