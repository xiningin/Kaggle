{"cell_type":{"490a0494":"code","6ef32e12":"code","6eb80755":"code","54542503":"code","34223fdb":"code","2f4cc56b":"code","4d80d23a":"code","e335dc69":"code","4896c561":"code","aa9ba6f6":"code","9518d08c":"code","6ea74cd9":"code","1db282a5":"code","43a1a918":"code","1e2096dc":"code","8c7dc61b":"code","7156d57b":"code","a61b210d":"code","d16300c5":"code","3d3daef0":"code","7ddb8d49":"code","1520b1ef":"code","661f4824":"code","a2ada8f8":"code","d6b81de4":"code","6abd69e7":"code","3eef0b6d":"code","9a4ea186":"code","a8b27065":"code","372d438a":"code","b247041c":"code","f8bd8b8f":"code","eb74b637":"code","301aba5b":"code","6ef1499f":"code","d5190b94":"code","5de97dfc":"code","429fce3c":"markdown"},"source":{"490a0494":"import tensorflow as tf\nfrom gensim.models import word2vec\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport glob\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","6ef32e12":"os.listdir('..\/input\/')","6eb80755":"os.listdir('..\/input\/102flowersdataset\/')","54542503":"os.listdir('..\/input\/cvpr2016\/cvpr2016_flowers')","34223fdb":"n_input = 100\nn_hidden = 128\nimage_height = 64\nimage_width = 64\nimage_depth = 3\nnoise_dim = 100\nmaxlength = 250\nNUM_EPOCHS = 100\nbatch_size = 64","2f4cc56b":"# \u5c06\u56fe\u7247\u89e3\u538b\u81f3102flowers\u6587\u4ef6\u5939(\u5f53\u524d\u8fd0\u884c\u76ee\u5f55\u4e0b\u521b\u5efa)\nif not os.path.exists('102flowers'):\n    !mkdir 102flowers\n    !tar zxvf ..\/input\/102flowersdataset\/102flowers.tgz -C .\/102flowers\/\n    print()\n# \u53d6\u51fa\u6240\u6709\u63cf\u8ff0\u82b1\u7684TXT\u6587\u672c\nall_text_filename = glob.glob('..\/input\/cvpr2016\/cvpr2016_flowers\/text_c10\/class_*\/image_*.txt')\n# \u5bf9\u6587\u672c\u7f16\u53f7\u8fdb\u884c\u6392\u5e8f\uff0c\u4e3a\u4e86\u65b9\u4fbf\u4f7f\u6587\u672c\u548c\u56fe\u7247\u5bf9\u5e94\u8d77\u6765\nall_text_filename.sort(key=lambda x:x.split('\/')[-1])\n\nprint(all_text_filename[:2])","4d80d23a":"# \u5bfc\u5165\u56fe\u7247\u5e76\u6392\u5e8f\nall_image_filename = glob.glob('.\/102flowers\/jpg\/*.jpg')\nall_image_filename.sort()","e335dc69":"all_image_filename[3001]","4896c561":"all_text_filename[3001]","aa9ba6f6":"# \u8f93\u5165\u961f\u5217\u9700\u8981\u4e24\u90e8\u5206\uff0c\u9996\u5148\u662f\u56fe\u7247\u548c\u5bf9\u5e94\u6587\u672c\u7684\u6b63\u786e\u8f93\u5165\uff0c\u7136\u540e\u662f\u56fe\u7247\u548c\u6587\u672c\u4e0d\u5bf9\u5e94\u7684\u9519\u8bef\u8f93\u5165\nall_text_filename = np.array(all_text_filename)\nall_image_filename = np.array(all_image_filename)\n# \u4e71\u5e8f\u6392\u5217\u56fe\u7247\u5236\u9020\u9519\u8bef\u8f93\u5165\nwrong_image_filename = all_image_filename[np.random.permutation(len(all_image_filename))]","9518d08c":"dataset_image = tf.data.Dataset.from_tensor_slices((all_image_filename, wrong_image_filename))","6ea74cd9":"dataset_image","1db282a5":"# \u5efa\u4e00\u4e2aall_text\u6587\u4ef6\u5b58\u653e\u6240\u6709\u6587\u672c\nif not os.path.exists('..\/input\/text_to_image\/all_text.txt'):\n    with open('all_text.txt', 'at') as f:\n        # \u628a\u6bcf\u4e00\u4e2a\u6587\u672c\u7684\u6362\u884c\u53bb\u6389\uff0c\u53ea\u5728\u961f\u5c3e\u52a0\u6362\u884c\uff0c\u7136\u540e\u5199\u5165all_text\u6587\u4ef6\n        for a_text in all_text_filename:\n            f.write(open(a_text).read().replace('\\n', '') + '\\n')\nif not os.path.exists('..\/input\/text_to_image\/word_model'):\n    # \u7528word2vec.Text8Corpus\u65b9\u6cd5\u8bad\u7ec3\u6587\u672c\u6587\u4ef6\n    sentences = word2vec.Text8Corpus('all_text.txt')\n    # \u5c06\u6bcf\u4e2a\u8bcd\u8f6c\u6362\u4e3a\u957f\u5ea6100\u7684\u5411\u91cf\n    model = word2vec.Word2Vec(sentences, size=100)\n    # \u6bcf\u4e00\u6b21\u8bad\u7ec3\u7ed3\u679c\u90fd\u6709\u968f\u673a\u6027\uff0c\u8fd9\u91cc\u628a\u7b2c\u4e00\u6b21\u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u4e0b\u6765\uff0c\u4fdd\u8bc1\u6bcf\u4e00\u6b21\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\u90fd\u662f\u76f8\u540c\u7684\n    model.save('word_model')\nelse:\n    model = Word2Vec.load('..\/input\/text_to_image\/word_model')\n    !cp ..\/input\/text_to_image\/all_text.txt .\/\n    !cp ..\/input\/text_to_image\/word_model .\/\nword_vectors = model.wv","43a1a918":"# \u83b7\u53d6\u6700\u957f\u6587\u672c\u7684\u957f\u5ea6\u4f5c\u4e3aRNN\u4e2d\u6bcf\u4e00\u4e2a\u8f93\u5165\u961f\u5217\u7684\u957f\u5ea6\nmaxlength = max([len(open(a_text).read().split()) for a_text in all_text_filename])","1e2096dc":"n_steps = maxlength","8c7dc61b":"# \u5411\u91cf\u586b\u5145\ndef pad(x, maxlength=200):\n    x1 = np.zeros((maxlength,100))\n    x1[:len(x)] = x\n    return x1","7156d57b":"# \u628a\u6240\u6709\u6587\u672c\u5411\u91cf\u5316\ndef text_vec(text_filenames):\n    vec = []\n    for a_text in text_filenames:\n        # \u8bfb\u53d6\u6587\u672c\u5e76\u5207\u5206\u5355\u8bcd\n        all_word = open(a_text).read().split()\n        # \u628a\u6bcf\u4e00\u4e2a\u5355\u8bcd\u8f6c\u6362\u6210\u8bcd\u5411\u91cf\n        all_vec = [word_vectors[w] for w in all_word if w in word_vectors]\n        vec.append(all_vec)\n    data = pd.Series(vec)\n    # \u5411\u91cf\u586b\u5145\n    data = data.apply(pad, maxlength=maxlength)\n    # \u5c06\u6240\u6709\u5411\u91cf\u8fde\u63a5\u6210\u4e00\u4e2a\u77e9\u9635 8189*223*100\n    data_ = np.concatenate(data).reshape(len(data),maxlength,100)\n    return data_","a61b210d":"data_text_emb = text_vec(all_text_filename)","d16300c5":"data_text_emb.shape","3d3daef0":"# \u56fe\u7247\u5904\u7406\u51fd\u6570\ndef read_image(image_filename):\n    image = tf.read_file(image_filename)\n    image = tf.image.decode_jpeg(image, channels=3)\n    # \u8f6c\u6362\u6210512*512\u5927\u5c0f\uff0c\u7136\u540e\u8f6c\u6362\u6210256*256\u5927\u5c0f\n    image = tf.image.resize_image_with_crop_or_pad(image, 512, 512)\n    image = tf.image.resize_images(image, (256, 256))\n    #image = tf.image.convert_image_dtype(image, tf.float32)\n    # \u5c06\u56fe\u72470-1\u6807\u51c6\u5316\n    image = (image - tf.reduce_min(image))\/(tf.reduce_max(image) - tf.reduce_min(image))\n    return image","7ddb8d49":"# \u8bfb\u53d6\u56fe\u7247\ndef _pre_func(real_image_name, wrong_image_name):\n    wrong_image = read_image(wrong_image_name)\n    real_image = read_image(real_image_name)\n    return real_image, wrong_image","1520b1ef":"# \u56fe\u7247\u8f93\u5165\u961f\u5217\ndataset_image = dataset_image.map(_pre_func)","661f4824":"dataset_image = dataset_image.batch(batch_size)","a2ada8f8":"# \u901a\u8fc7Iterator\u65b9\u6cd5\u5f97\u5230\u6bcf\u4e00\u4e2abatch\u7684\u6570\u636e\niterator = tf.data.Iterator.from_structure(dataset_image.output_types, dataset_image.output_shapes)\nreal_image_batch, wrong_image_batch = iterator.get_next()","d6b81de4":"input_text = tf.placeholder(tf.float32, [None, n_steps, n_input])\ninputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name='inputs_noise')","6abd69e7":"# \u6c42\u6587\u672c\u957f\u5ea6\ndef length(shuru):\n    return tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(shuru),reduction_indices=2)),reduction_indices=1)","3eef0b6d":"def text_rnn(input_text, batch_size=64, reuse=None):\n    cell = tf.contrib.rnn.GRUCell(n_hidden,\n                                  kernel_initializer = tf.truncated_normal_initializer(stddev=0.0001),\n                                  bias_initializer = tf.truncated_normal_initializer(stddev=0.0001),\n                                  reuse=reuse)\n    output, _ = tf.nn.dynamic_rnn(\n                                  cell,\n                                  input_text,\n                                  dtype=tf.float32,\n                                  sequence_length = length(input_text)\n                                  )\n\n    # \u53d6\u51fa\u4e00\u4e2abatch\u4e2d\u6bcf\u4e00\u4e2a\u6587\u672c\u5bf9\u5e94\u7684output\n    index = tf.range(0,batch_size)*n_steps + (tf.cast(length(input_text),tf.int32) - 1)\n    flat = tf.reshape(output,[-1,int(output.get_shape()[2])])\n    last = tf.gather(flat,index)\n    return last","9a4ea186":"def get_generator(noise_img, image_depth, condition_label, is_train=True, alpha=0.2):\n    with tf.variable_scope(\"generator\", reuse=(not is_train)):\n        # 100 x 1 to 4 x 4 x 512\n        # \u5168\u8fde\u63a5\u5c42\n        noise_img = tf.to_float(noise_img)\n        noise_img = tf.layers.dense(noise_img, n_hidden)\n        noise_img = tf.maximum(alpha * noise_img, noise_img)\n        noise_img_ = tf.concat([noise_img, condition_label], 1)\n        # \u5168\u8fde\u63a5\n        layer1 = tf.layers.dense(noise_img_, 4*4*512)\n        layer1 = tf.reshape(layer1, [-1, 4, 4, 512])\n        layer1 = tf.layers.batch_normalization(layer1, training=is_train)\n        layer1 = tf.nn.relu(layer1)\n        # batch normalization\n        #layer1 = tf.layers.batch_normalization(layer1, training=is_train)\n        # ReLU\n        #layer1 = tf.nn.relu(layer1)\n        # dropout\n        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n        \n        # \u53cd\u5377\u79ef\n        # 4 x 4 x 512 to 8 x 8 x 256\n        layer2 = tf.layers.conv2d_transpose(layer1, 256, 3, strides=2, padding='same')\n        layer2 = tf.layers.batch_normalization(layer2, training=is_train)\n        layer2 = tf.nn.relu(layer2)\n        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n        \n        # 8 x 8 256 to 16x 16 x 128\n        layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding='same')\n        layer3 = tf.layers.batch_normalization(layer3, training=is_train)\n        layer3 = tf.nn.relu(layer3)\n        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n        \n        # 16 x 16 x 128 to 32 x 32 x 64\n        layer4 = tf.layers.conv2d_transpose(layer3, 64, 3, strides=2, padding='same')\n        layer4 = tf.layers.batch_normalization(layer4, training=is_train)\n        layer4 = tf.nn.relu(layer4)\n        \n        # 64 x 64 x 32\n        layer5 = tf.layers.conv2d_transpose(layer4, 32, 3, strides=2, padding='same')\n        layer5 = tf.layers.batch_normalization(layer5, training=is_train)\n        layer5 = tf.nn.relu(layer5)\n        \n        # 128 x 128 x 16\n        layer6 = tf.layers.conv2d_transpose(layer5, 16, 3, strides=2, padding='same')\n        layer6 = tf.layers.batch_normalization(layer6, training=is_train)\n        layer6 = tf.nn.relu(layer6)  \n        \n        #  256 x 256 x 3\n        logits = tf.layers.conv2d_transpose(layer6, image_depth, 3, strides=2, padding='same')\n        outputs = tf.tanh(logits)\n        outputs = (outputs\/2) + 0.5\n        # \u5c06\u8f93\u51fa\u89c4\u8303\u52300,1\u4e4b\u95f4\uff0c\u9632\u6b62\u5f88\u5c0f\u7684\u6570\/2\u4e4b\u540e\u51fa\u9519\n        outputs = tf.clip_by_value(outputs, 0.0, 1.0)\n        return outputs","a8b27065":"def get_discriminator(inputs_img, condition_label, reuse=False, alpha=0.2):\n    with tf.variable_scope(\"discriminator\", reuse=reuse):\n        # 256 x 256 x 3 to 128 x 128 x 16\n        # \u7b2c\u4e00\u5c42\u4e0d\u52a0\u5165BN\n        layer1 = tf.layers.conv2d(inputs_img, 16, 3, strides=2, padding='same')\n        layer1 = tf.maximum(alpha * layer1, layer1)\n        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n        \n        # 128 x 128 x 16 to 64 x 64 x 32\n        layer2 = tf.layers.conv2d(layer1, 32, 3, strides=2, padding='same')\n        layer2 = tf.layers.batch_normalization(layer2, training=True)\n        layer2 = tf.maximum(alpha * layer2, layer2)\n        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n        \n        # 32 x 32 x 64\n        layer3 = tf.layers.conv2d(layer2, 64, 3, strides=2, padding='same')\n        layer3 = tf.layers.batch_normalization(layer3, training=True)\n        layer3 = tf.maximum(alpha * layer3, layer3)\n        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n        \n        # 16*16*128\n        layer4 = tf.layers.conv2d(layer3, 128, 3, strides=2, padding='same')\n        layer4 = tf.layers.batch_normalization(layer4, training=True)\n        layer4 = tf.maximum(alpha * layer4, layer4)\n        \n        \n         # 8*8*256\n        layer5 = tf.layers.conv2d(layer4, 256, 3, strides=2, padding='same')\n        layer5 = tf.layers.batch_normalization(layer5, training=True)\n        layer5 = tf.maximum(alpha * layer5, layer5)\n        \n         # 4*4*512\n        layer6 = tf.layers.conv2d(layer5, 512, 3, strides=2, padding='same')\n        layer6 = tf.layers.batch_normalization(layer6, training=True)\n        layer6 = tf.maximum(alpha * layer6, layer6)\n        \n    \n        \n        text_emb = tf.layers.dense(condition_label, 512)\n        text_emb = tf.maximum(alpha * text_emb, text_emb)\n        text_emb = tf.expand_dims(text_emb, 1)\n        text_emb = tf.expand_dims(text_emb, 2)\n        # tf.tile\u662f\u6309\u7167\u7ed9\u5b9a\u60c5\u51b5\u590d\u5236\u5f20\u91cf\n        text_emb = tf.tile(text_emb, [1,4,4,1])\n        # \u5c06\u56fe\u50cf\u548c\u6587\u672c\u6761\u4ef6\u7ed3\u5408\u5728\u4e00\u8d77\n        layer_concat = tf.concat([layer6, text_emb], 3)\n        \n        layer7 = tf.layers.conv2d(layer_concat, 512, 1, strides=1, padding='same')\n        layer7 = tf.layers.batch_normalization(layer7, training=True)\n        layer7 = tf.maximum(alpha * layer7, layer7)\n        \n        flatten = tf.reshape(layer7, (-1, 4*4*512))\n        logits = tf.layers.dense(flatten, 1)\n        outputs = tf.sigmoid(logits)\n        \n        return logits, outputs","372d438a":"def get_loss(inputs_image, wrong_image, inputs_noise, condition_label, image_depth, smooth=0.1):\n    g_outputs = get_generator(inputs_noise, image_depth, condition_label, is_train=True)\n    d_logits_real, d_outputs_real = get_discriminator(inputs_image, condition_label)\n    d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, condition_label, reuse=True)\n    d_logits_wrong, d_outputs_wrong = get_discriminator(wrong_image, condition_label, reuse=True)\n    \n    print(inputs_image.get_shape(), condition_label.get_shape())\n    \n    # \u8ba1\u7b97Loss\n    # \u751f\u6210\u5668\u5e0c\u671b\u751f\u6210\u7684\u56fe\u7247\u80fd\u88ab\u5224\u5b9a\u4e3a\u63a5\u8fd11\n    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n                                                                    labels=tf.ones_like(d_outputs_fake)*(1-smooth)))\n    \n    #g_loss_l1 = tf.reduce_mean(tf.abs(g_outputs - inputs_image))\n    \n    #g_loss = g_loss_ + g_loss_l1\n    # \u5224\u522b\u5668\u5e0c\u671b\uff1a\u771f\u5b9e\u56fe\u7247\u5224\u5b9a\u4e3a\u63a5\u8fd11\uff0c\u751f\u6210\u56fe\u7247\u548c\u4e0e\u6587\u672c\u63cf\u8ff0\u4e0d\u7b26\u7684\u56fe\u7247\u5224\u5b9a\u4e3a\u63a5\u8fd10\n    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n                                                                         labels=tf.ones_like(d_outputs_real)*(1-smooth)))\n    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n                                                                         labels=tf.ones_like(d_outputs_fake)*smooth))\n    d_loss_wrong = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_wrong,\n                                                                         labels=tf.ones_like(d_outputs_wrong)*smooth))\n    \n    d_loss = d_loss_real + d_loss_fake + d_loss_wrong\n    \n    return g_loss, d_loss","b247041c":"# \u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\ndef get_optimizer(g_loss, d_loss, beta1=0.4, learning_rate=0.001):\n    train_vars = tf.trainable_variables()\n    \n    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n    \n    # Optimizer\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        g_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n        d_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n    \n    return g_opt, d_opt","f8bd8b8f":"# \u753b\u56fe\u51fd\u6570\ndef plot_images(samples):\n    #samples = (samples+1)\/2\n    fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(20,2))\n    for img, ax in zip(samples, axes):\n        ax.imshow(img.reshape((256, 256, 3)))\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    fig.tight_layout(pad=0)","eb74b637":"# \u751f\u6210\u56fe\u7247\u6548\u679c\u5c55\u793a\u51fd\u6570\ndef show_generator_output(sess, n_images, inputs_noise, output_dim, test_text_vec):\n#    condition_text = tf.to_float(condition_text)\n#    last, b_size = sess.run(text_vec(condition_text, batch_size=n_images, reuse=True))\n    samples = sess.run(get_generator(inputs_noise, output_dim, test_text_vec, is_train=False))\n    return samples","301aba5b":"# \u5b9a\u4e49\u53c2\u6570\nn_samples = 10\nlearning_rate = 0.0002\nbeta1 = 0.5","6ef1499f":"losses = []\nstep = 0\nlast = text_rnn(input_text)\ng_loss, d_loss = get_loss(real_image_batch, wrong_image_batch, inputs_noise, last, image_depth, smooth=0.1)\ng_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\nsaver = tf.train.Saver()","d5190b94":"show = 1\nif show:\n    # \u5c55\u793a\u524d\u4e94\u5f20\u56fe\u7247\u548c\u524d\u4e94\u4e2a\u6587\u672c\u63cf\u8ff0\n    for a_text in all_text_filename[:5]:\n            # \u8bfb\u53d6\u6587\u672c\u5e76\u5207\u5206\u5355\u8bcd\n            all_word = open(a_text).read()\n            print(all_word)\n\n    with tf.Session() as sess:\n    #     sess.run(tf.global_variables_initializer())\n        # \u6062\u590d\u68c0\u67e5\u70b9\u6a21\u578b\u53d8\u91cf\n        model_file=tf.train.latest_checkpoint('..\/input\/text-to-iamge')\n        saver.restore(sess, model_file)\n        n_samples = 5\n        # \u53d6\u51fa\u524d\u51e0\u4e2a\u63cf\u8ff0\u770b\u770b\u751f\u6210\u6548\u679c\n        condition_text = data_text_emb[:n_samples]\n        print\n         # \u566a\u58f0\n        test_noise = np.random.uniform(-1, 1, size=[n_samples, noise_dim])\n        # \u63d0\u53d6\u6587\u672c\u7279\u5f81\n        last_test = text_rnn(input_text, batch_size=n_samples, reuse=True)\n        test_text_vec = sess.run(last_test, feed_dict={input_text: condition_text})\n        samples = show_generator_output(sess, n_samples, test_noise, 3, test_text_vec)\n        plot_images(samples)  \n    \nelse:\n    # \u8bad\u7ec3\u8fc7\u7a0b\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n    #     sess.run(tf.global_variables_initializer())\n        # \u6062\u590d\u68c0\u67e5\u70b9\u6a21\u578b\u53d8\u91cf\n        model_file=tf.train.latest_checkpoint('..\/input\/text-to-iamge')\n        saver.restore(sess, model_file)\n\n        for epoch in range(560, 571):\n            # \u9996\u5148\u5bf9\u6240\u6709\u6570\u636e\u4e71\u5e8f\u5904\u7406\uff0c\u8fd9\u65f6\u56fe\u7247\u548c\u6587\u672c\u662f\u5bf9\u5e94\u7684\n            index = np.random.permutation(len(all_image_filename))\n            data_text_emb = data_text_emb[index]\n            all_image_filename = all_image_filename[index]\n            # \u5236\u9020\u9519\u8bef\u8f93\u5165\uff1a\u5c06\u56fe\u7247\u4e71\u5e8f\uff0c\u6587\u672c\u5e8f\u53f7\u4e0d\u53d8\n            wrong_image_filename = all_image_filename[np.random.permutation(len(all_image_filename))] \n            dataset_image = tf.data.Dataset.from_tensor_slices((all_image_filename, wrong_image_filename))\n            dataset_image = dataset_image.map(_pre_func)\n            # \u5728\u6bcf\u4e2aepoch\u91cc\u9762\u6bcf\u5f20\u56fe\u7247\u51fa\u73b0\u4e00\u6b21\n            dataset_image = dataset_image.repeat(1)\n            dataset_image = dataset_image.batch(batch_size)\n            # \u7528\u4e71\u5e8f\u5904\u7406\u4e4b\u540e\u7684\u6570\u636e\u96c6\u521d\u59cb\u5316iterator\n            dataset_image_op = iterator.make_initializer(dataset_image)\n\n            sess.run(dataset_image_op)\n            i = 0\n            while True: \n\n                try:\n                    batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_dim))\n                    text_emb_batch = data_text_emb[i: i + batch_size]\n                    i = i + batch_size\n                    _ = sess.run([g_train_opt, d_train_opt], feed_dict={input_text: text_emb_batch,\n                                                                inputs_noise: batch_noise})\n\n    #               if step % 50 == 0:\n    #                   saver.save(sess, \".\/model10.ckpt\")\n    #                   train_loss_d = d_loss.eval({input_text: text_emb_batch,\n    #                                               inputs_noise: batch_noise})\n    #                   train_loss_g = g_loss.eval({input_text: text_emb_batch,\n    #                                               inputs_noise: batch_noise})\n    #                   \n    #                   losses.append((train_loss_d, train_loss_g))\n    #                   print(\"Step {}....\".format(step+1), \n    #                         \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n    #                         \"Generator Loss: {:.4f}....\". format(train_loss_g))\n\n\n\n                        # \u663e\u793a\u56fe\u7247\n                    step += 1    \n                #except tf.errors.OutOfRangeError as e:\n                # \u6700\u540e\u4e00\u4e2abatch\u4e0d\u591fbatchsize,\u6253\u5370epoch\u7ec8\u6b62\u63d0\u793a\u4fe1\u606f\n                except:\n    #                 saver.save(sess, \".\/model10.ckpt\")\n                    print('epoch', epoch, 'step', step)\n                    #print(e)\n                    #try:\n                    #    sess.run(real_image_batch)\n                    #except Exception as e:\n                    #    print(e)\n                    break\n\n            if epoch%2 == 0:\n                #saver.save(sess, \".\/model10.ckpt\")\n                n_samples = 10\n                # \u53d6\u51fa\u524d10\u4e2a\u63cf\u8ff0\u770b\u770b\u751f\u6210\u6548\u679c\n                condition_text = data_text_emb[:n_samples]\n                # \u566a\u58f0\n                test_noise = np.random.uniform(-1, 1, size=[n_samples, noise_dim])\n                # \u63d0\u53d6\u6587\u672c\u7279\u5f81\n                last_test = text_rnn(input_text, batch_size=n_samples, reuse=True)\n                test_text_vec = sess.run(last_test, feed_dict={input_text: condition_text})\n                samples = show_generator_output(sess, n_samples, test_noise, 3, test_text_vec)\n                plot_images(samples)\n        saver.save(sess, \".\/model11.ckpt\")","5de97dfc":"!rm -rf 102flowers","429fce3c":"vec = []\ntest_word = \"\"\"\nthe petals on this flower are yellow with a red center,the petals on this flower are yellow with a red center\n\"\"\"\nall_vec = [word_vectors[w] for w in test_word if w in word_vectors]\nvec.append(all_vec)\ndata = pd.Series(vec)\ndata = data.apply(pad, maxlength=maxlength)\ndata_ = np.concatenate(data).reshape(len(data),maxlength,100)\ntest_text_vec = data_\n\n\nlosses = []\nstep = 0\nlast = text_rnn(input_text)\ng_loss, d_loss = get_loss(real_image_batch, wrong_image_batch, inputs_noise, last, image_depth, smooth=0.1)\ng_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n\n\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    model_file=tf.train.latest_checkpoint('..\/input\/gan-text-to-image-102flowers-rieyuguanghua')\n    saver.restore(sess, model_file)\n    n_samples = 10\n    test_noise = np.random.uniform(-1, 1, size=[n_samples, noise_dim])\n    last_test = text_rnn(input_text, batch_size=n_samples, reuse=True)\n    test_text_vec = sess.run(last_test, feed_dict={input_text: test_text_vec})\n    samples = show_generator_output(sess, n_samples, test_noise, 3, test_text_vec)\n    plot_images(samples)"}}