{"cell_type":{"cf28745e":"code","b2b5acdd":"code","f286988c":"code","10ee7cfa":"code","813b5c96":"code","a94c0f23":"code","61dc65e2":"code","eaa62371":"code","b77f6576":"code","b1b12578":"code","552eddd5":"code","fb36b5b5":"code","ad31df2a":"code","69ba0d36":"code","30620d43":"code","5b7a8187":"code","6aff1d0a":"code","7fc4738d":"code","3e54c11d":"code","e43b15e2":"code","e58c7270":"code","5f3ec4cc":"code","4737f06c":"code","00102970":"code","38642c89":"code","14d54c9f":"code","284c6581":"code","15894f67":"code","6d361337":"code","3e092c59":"code","19d80563":"code","7dac930d":"code","32eb735d":"code","bc90518e":"code","c7cc8c40":"code","ddfda119":"code","1cf27dfc":"code","3e05c75f":"code","76ac4f68":"code","a23d90e5":"code","f2adc06f":"code","9483c2e6":"code","60aa2dc2":"code","50268bfd":"code","e850d176":"code","b15008fe":"code","ee6e4596":"code","ec2e7934":"code","ce64682d":"code","6e363738":"code","907a4ef8":"code","ab24f4ab":"code","3671072d":"code","555fb96c":"code","960cdb30":"code","c0d194bb":"code","ebcb0973":"code","e9172ed0":"code","0f7bdfab":"code","b779644e":"code","575303d4":"code","a5d7db5c":"code","2c6d0d8e":"code","66954b48":"code","ca22123a":"markdown","c756661f":"markdown","5b68df93":"markdown","4ac38824":"markdown","a05d68b3":"markdown","3af96c3d":"markdown","6e01f233":"markdown","0f3cba3d":"markdown","10947922":"markdown","44d77cc4":"markdown","b108eea0":"markdown","a5d368f6":"markdown","511e66dd":"markdown","5575bb12":"markdown","3a271459":"markdown","634d4ed7":"markdown","75fa00a3":"markdown","a98d7d33":"markdown","93f29916":"markdown","1c9a9619":"markdown","07f773c9":"markdown","e87b7c72":"markdown","a589cbdf":"markdown","baa8f92c":"markdown","69f1901d":"markdown","4bb0c41e":"markdown","0e222b8b":"markdown","f2953b7e":"markdown","300d2b1b":"markdown","cc6cd8b4":"markdown","d7d6cf11":"markdown","070a7c90":"markdown","8d788ff5":"markdown","bc458764":"markdown","de1d7b52":"markdown","57c6c5a2":"markdown","6e3896d8":"markdown","22c20a81":"markdown","0e6edb4d":"markdown","d0f42c0e":"markdown","d0db6540":"markdown","c963254a":"markdown","ca2c3638":"markdown","24d59400":"markdown","5f1831c6":"markdown","402e6f22":"markdown","b1ab37ac":"markdown","02d2d1c4":"markdown","2d10b32e":"markdown","63477783":"markdown","04cf35ce":"markdown","97826fd1":"markdown","ad4d7eda":"markdown","184eec0a":"markdown","a9f35fb2":"markdown","e51bbe03":"markdown","1bc5940c":"markdown","ea79d4a2":"markdown","bf3112bc":"markdown","4ae68553":"markdown","5f35b631":"markdown","f8c39393":"markdown","777c8979":"markdown","4cc497d7":"markdown","71cfa607":"markdown","b8c5f17b":"markdown","6dcf5d45":"markdown"},"source":{"cf28745e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score, confusion_matrix, classification_report\nfrom collections import Counter\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, train_test_split\nplt.rcParams['figure.figsize'] = (16,8)","b2b5acdd":"df = pd.read_csv(\"..\/input\/Banco-PF-DSCOR9\/data_train.csv\") \ndf_test = pd.read_csv('..\/input\/Banco-PF-DSCOR9\/data_test.csv')","f286988c":"df.head(10)","10ee7cfa":"df[\"y\"].replace(to_replace={\"yes\":1,\"no\":0}, inplace=True)","813b5c96":"df.info()","a94c0f23":"df_test.info()","61dc65e2":"# Data set de train\nunknown_list = {}\n\nfor column in df.columns:\n    if(df[column].dtype=='object'):\n        unknown = df[df[column]=='unknown']\n        test = unknown[column].value_counts()\n        unknown_list[column]= test\nunknown_list","eaa62371":"# Data set de test\nunknown_list = {}\n\nfor column in df_test.columns:\n    if(df_test[column].dtype=='object'):\n        unknown = df_test[df_test[column]=='unknown']\n        test = unknown[column].value_counts()\n        unknown_list[column]= test\nunknown_list","b77f6576":"#@title\nsns.boxplot(df.age)","b1b12578":"#@title\nbins = [10, 20, 30, 40, 50, 60, 70, 100]\ncats = pd.cut(df.age, bins)","552eddd5":"#@title\nax = sns.countplot(cats, palette=\"Blues_r\", #hue=df[\"y\"],\n                   order = cats.value_counts().index)\nsns.set(font_scale=1.1)\nax.set_title('Distribuci\u00f3n de clientes por edad')\nvalue=round(cats.value_counts()\/len(cats)*100,2)\nfor p, label in zip(ax.patches, value):\n    ax.annotate(label, (p.get_x()+0.23, p.get_height()+0.25))","fb36b5b5":"jobx = sns.countplot(df[\"job\"], palette=\"Greens_r\", #hue=df['y'],\n                   order = df[\"job\"].value_counts().index)\njobx.set_title('Distribuci\u00f3n de clientes por puesto de trabajo')\nsns.set(font_scale=0.78)\nvalue=round(df[\"job\"].value_counts()\/len(df[\"job\"])*100,2)\nfor p, label in zip(jobx.patches, value):\n    jobx.annotate(label, (p.get_x()+0.23, p.get_height()+0.25))","ad31df2a":"maritx = sns.countplot(df[\"marital\"], palette=\"GnBu_d\", #hue=df['y'],\n                   order = df[\"marital\"].value_counts().index)\nsns.set(font_scale=1)\nmaritx.set_title('Distribuci\u00f3n de clientes por estado civil')\nvalue=round(df[\"marital\"].value_counts()\/len(df[\"marital\"])*100,2)\nfor p, label in zip(maritx.patches, value):\n    maritx.annotate(label, (p.get_x()+0.33, p.get_height()+0.25))","69ba0d36":"edux = sns.countplot(df[\"education\"], palette=\"Oranges_r\", #hue=df[\"y\"], \n                     #hue=(df[\"job\"]==\"blue-collar\"),                 \n                   order = df[\"education\"].value_counts().index)\nedux.set_title('Distribuci\u00f3n de clientes por nivel de educaci\u00f3n')\nvalue=round(df[\"education\"].value_counts()\/len(df[\"education\"])*100,2)\nfor p, label in zip(edux.patches, value):\n    edux.annotate(label, (p.get_x()+0.28, p.get_height()+0.25))","30620d43":"f, axes = plt.subplots(1, 3)\n\nsns.barplot(x=df['default'].value_counts().index, y=df['default'].value_counts(normalize=True), ax=axes[0]).set_title(\"Credito impago\")\nsns.barplot(x=df['housing'].value_counts().index, y=df['housing'].value_counts(normalize=True), ax=axes[1]).set_title(\"Prestamo hipotecado\")\nsns.barplot(x=df['loan'].value_counts().index, y=df['loan'].value_counts(normalize=True), ax=axes[2]).set_title(\"Prestamo personal\")","5b7a8187":"f, axes = plt.subplots(1, 3)\n\nsns.barplot(x= df['contact'].value_counts().index, y= df['contact'].value_counts(normalize=True), ax=axes[0]).set_title(\"Medio de comunicaci\u00f3n\")\nsns.barplot(x= df['month'].value_counts().index, y= df['month'].value_counts(normalize=True), ax=axes[1]).set_title(\"Mes del \u00faltimo contacto\")\nsns.barplot(x= df['day_of_week'].value_counts().index, y= df['day_of_week'].value_counts(normalize=True), ax=axes[2]).set_title(\"Dia del \u00faltimo contacto\")","6aff1d0a":"#@title\nbins2=[0,2,4,6,8,10,12,14,56]\ncampaign_cats = pd.cut(df.campaign, bins2)","7fc4738d":"#@title\nax = sns.countplot(campaign_cats, palette=\"bone_r\", #hue=df[\"y\"],\n                      order = campaign_cats.value_counts().index)\nax.set_title('Distribuci\u00f3n de contacto de \u00faltima campa\u00f1a por cantidad de contactos')\nvalue=round(campaign_cats.value_counts()\/len(campaign_cats)*100,2)\nfor p, label in zip(ax.patches, value):\n    ax.annotate(label, (p.get_x()+0.33, p.get_height()+0.25))","3e54c11d":"bins3=[0,2,4,6,8,10,12,14,999]\npdays_cat = pd.cut(df.pdays, bins3)","e43b15e2":"pdx = sns.countplot(pdays_cat, palette=\"bone_r\", #hue=df[\"y\"],\n                      order = pdays_cat.value_counts().index)\nvalue=round(pdays_cat.value_counts()\/len(pdays_cat)*100,2)\nfor p, label in zip(pdx.patches, value):\n    pdx.annotate(label, (p.get_x()+0.33, p.get_height()+0.25))","e58c7270":"px = sns.countplot(df['previous'], palette=\"bone_r\", #hue=df[\"y\"],\n                      order = df['previous'].value_counts().index)\nvalue=round(df['previous'].value_counts()\/len(df['previous'])*100,2)\nfor p, label in zip(px.patches, value):\n    px.annotate(label, (p.get_x()+0.33, p.get_height()+0.25))","5f3ec4cc":"poutcx = sns.countplot(df['poutcome'], palette=\"bone_r\", #hue=df[\"y\"],\n                      order = df['poutcome'].value_counts().index)\nvalue=round(df['poutcome'].value_counts()\/len(df['poutcome'])*100,2)\nfor p, label in zip(poutcx.patches, value):\n    poutcx.annotate(label, (p.get_x()+0.33, p.get_height()+0.25))","4737f06c":"sns.boxplot(df['emp.var.rate'])","00102970":"df_economic = df.copy()\ndf_economic['month'] = df_economic['month'].map({'mar':3, 'apr':4, 'may':5, 'jun':6, 'jul':7, 'aug':8, 'sep':9, 'oct':10, 'nov': 11, 'dec':12})\nsns.lineplot(x = 'month', y= 'cons.price.idx', data=df_economic)","38642c89":"sns.distplot(df[\"cons.price.idx\"])","14d54c9f":"sns.lineplot(x = 'month', y= 'cons.conf.idx', data=df_economic)","284c6581":"ax=sns.lineplot(x = 'month', y= 'cons.price.idx', data=df_economic,color=\"g\")\nax2 = plt.twinx()\nsns.lineplot(x = 'month', y= 'cons.conf.idx', data=df_economic, ax=ax2, color=\"r\") ","15894f67":"sns.lineplot(x = 'month', y= 'euribor3m', data=df_economic)","6d361337":"sns.boxplot(df['nr.employed'])","3e092c59":"sns.barplot(x = df['y'].value_counts().index, y= df['y'].value_counts())","19d80563":"age = df[df['y']==1]\nage_no= df[df['y']==0]\nsns.distplot(age['age'], label='Si')\nsns.distplot(age_no['age'], label='No')\n#plt.legend()","7dac930d":"#@title\nedad_y={}\na=10\nwhile True:\n    if a==100:\n        break\n    b=a+10\n    df_filt=df[(df[\"y\"]==1)]\n    df_filt=df_filt[(df_filt[\"age\"]>a) & (df_filt[\"age\"]<b)]\n    comp=round(len(df_filt)\/len(df[(df[\"age\"]>a) & (df[\"age\"]<b)])*100,2)\n    inter=str(a)+\"-\"+str(b)\n    print(comp, \"% de los de \",inter, \"compraron el plazo fijo\")\n    edad_y[inter]=comp\n    a+=10","32eb735d":"ax = sns.countplot(data=df, x='job', hue='y')\ntotal = float(len(df)) \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.0f}%'.format((height\/total)*100),\n            ha=\"center\") \nplt.show()","bc90518e":"#@title\njob_y={}\nfor i in df[\"job\"].unique():\n    df_filt=df[(df[\"y\"]==1) & (df[\"job\"]==i)]\n    comp=round(len(df_filt)\/len(df[df[\"job\"]==i])*100,2)\n    print(comp, \"% de los\",i, \"compraron el plazo fijo\")\n    job_y[i]=comp","c7cc8c40":"#@title\nprint(\"Valores nulos: \",(df.job==\"unknown\").sum())\nprint(\"Porcentaje\",(df.job==\"unknown\").sum()\/len(df)*100,\"%\")","ddfda119":"ax = sns.countplot(data=df, x='marital', hue='y')\ntotal = float(len(df)) \nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.0f}%'.format((height\/total)*100),\n            ha=\"center\") \nplt.show()","1cf27dfc":"marital_y={}\nfor i in df[\"marital\"].unique():\n    df_filt=df[(df[\"y\"]==1) & (df[\"marital\"]==i)]\n    ratio=round(len(df_filt)\/len(df[df[\"marital\"]==i])*100,2)\n    print(ratio, \"% de los\",i, \"compraron el plazo fijo\")\n    marital_y[i]=ratio","3e05c75f":"f, ax = plt.subplots(1, 3)\n\nsns.countplot(data=df, x='default',  hue='y', ax = ax[0]).set_title(\"Credito impago\")\nsns.countplot(data=df, x='housing',  hue='y', ax = ax[1]).set_title(\"Prestamo hipotecado\")\nsns.countplot(data=df, x='loan',  hue='y', ax = ax[2]).set_title(\"Prestamo personal\")","76ac4f68":"sns.heatmap(df.corr(), annot=True, fmt= '.2f')","a23d90e5":"df.drop(columns=['id', 'previous', 'default'], inplace=True, axis=1)\ndf_test.drop(columns=['id', 'previous', 'default'], inplace=True, axis=1)","f2adc06f":"plt.rcParams['figure.figsize'] = (20,12)\nsns.heatmap(df.corr(), annot=True, fmt= '.2f')","9483c2e6":"# Para TRAIN\nfor column in df.columns.drop([\"month\", \"day_of_week\"]):\n    if df[column].dtype=='object':\n            df = pd.concat((df.drop(columns=[column]), pd.get_dummies(df[column], prefix=column)) , axis=1)","60aa2dc2":"# Para TEST                            \nfor column in df_test.columns.drop([\"month\", \"day_of_week\"]):\n    if df_test[column].dtype=='object':\n            df_test = pd.concat((df_test.drop(columns=[column]), pd.get_dummies(df_test[column], prefix=column)) , axis=1)","50268bfd":"X = df.drop(columns=[\"y\", \"month\", \"day_of_week\"])\nX_Test = df_test.drop(columns=[\"month\", \"day_of_week\"])\ny = df['y']","e850d176":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)","b15008fe":"def mostrar_metricas(y_test, y_pred_test):\n    #Matriz_de_confusi\u00f3n\n    cm = confusion_matrix(y_test, y_pred_test)\n    cross= pd.crosstab(y_test, y_pred_test, rownames=['True'], colnames=['Predicted'], margins=True)\n    cross\n    sns.heatmap(cm, annot=True, fmt='.2f')\n    plt.show() \n    \n    #Precision_Recall\n    print(classification_report(y_test, y_pred_test))\n    \n    #ROC_AUC\n    #fpr, tpr, _ = roc_curve(y_test, y_score[:,1])\n    #roc_auc = auc(fpr, tpr)\n\n    #plt.figure()\n    #plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    #plt.plot([0, 1], [0, 1], 'k--')\n    #plt.xlim([0.0, 1.0])\n    #plt.ylim([0.0, 1.05])\n    #plt.xlabel('False Positive Rate')\n    #plt.ylabel('True Positive Rate')\n    #plt.title('ROC Curve')\n    #plt.legend(loc=\"lower right\")\n    plt.show()","ee6e4596":"dct = DecisionTreeClassifier(class_weight='balanced', max_depth=10)\ndct.fit(X_train, y_train)\n\ny_pred_test = dct.predict(X_test)\n\ndct_score = f1_score(y_test, y_pred_test, average='macro')\n\nmostrar_metricas(y_test, y_pred_test)","ec2e7934":"rdm = BalancedRandomForestClassifier(max_depth=20, max_features='auto')\nrdm.fit(X_train, y_train)\n\ny_score = rdm.predict_proba(X_test)\n\ny_pred_train = rdm.predict(X_train)\ny_pred_test = rdm.predict(X_test)\nrdm_balanced_score = f1_score(y_test, y_pred_test, average='macro')\n\nmostrar_metricas(y_test, y_pred_test)","ce64682d":"max_depth = np.arange(1, 20, 2)\ntrain_score = []\ntest_score = []\n\nfor depth in max_depth: \n    rdm = RandomForestClassifier(criterion='gini', max_depth=depth, random_state=42, class_weight=\"balanced\")\n    rdm.fit(X_train, y_train)\n    \n    train_score.append(rdm.score(X_train, y_train))\n    test_score.append(rdm.score(X_test, y_test))","6e363738":"plt.figure(figsize=(12,10))\n\nfeat_imp_df = pd.DataFrame(sorted(zip(map(lambda x: round(x, 4), rdm.feature_importances_), X_train.columns), reverse=True))\n\n\nmapping = {feat_imp_df.columns[0]:'Importancia', feat_imp_df.columns[1]: 'Variable'}\nfeat_imp_df = feat_imp_df.rename(columns=mapping)\nsns.barplot(x=feat_imp_df['Importancia'],y=feat_imp_df['Variable'], palette=\"Greens_d\")","907a4ef8":"sns.lineplot(x=max_depth ,y=train_score, label='Train score')\nsns.lineplot(x=max_depth ,y=test_score, label= 'Test score')\nplt.ylabel('Score')\nplt.xlabel('Profundiad')","ab24f4ab":"max_accuracy_rdm_train =  train_score[np.argmax(test_score)]*100\nmax_accuracy_rdm = np.max(test_score)*100\n\n# En la profundidad donde se obtuvo el mejor valor para TEST\nprint('Porcentaje de aciertos sobre el set de entrenamiento:', max_accuracy_rdm_train)\nprint('Porcentaje de aciertos sobre el set de evaluaci\u00f3n:', max_accuracy_rdm)","3671072d":"rdm = RandomForestClassifier(max_depth=max_depth[np.argmax(test_score)], n_jobs=-1, n_estimators=42, class_weight=\"balanced\")\nrdm.fit(X_train, y_train)\n\ny_score = rdm.predict_proba(X_test)\n\ny_pred_train = rdm.predict(X_train)\ny_pred_test = rdm.predict(X_test)\n\nrdm_score = f1_score(y_test, y_pred_test, average='macro')\n\nmostrar_metricas(y_test, y_pred_test)","555fb96c":"#kf = KFold(n_splits=5, shuffle=True)\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=42)\n#cv = StratifiedKFold(n_splits=5, shuffle=True)\n\nparam_grid = {\n    'max_depth' : [5, 10, 20],\n    'max_features' : ['sqrt', 'log2', X_test.shape[1]\/\/2],\n    'n_estimators' :[50, 100, 200],\n    'min_samples_split':[5, 11, 20],\n}","960cdb30":"rdm_balanced = RandomForestClassifier(oob_score=True, random_state=42, class_weight=\"balanced\")\n\nrs_rf = RandomizedSearchCV(rdm_balanced, param_distributions=param_grid, scoring='f1_macro', cv=cv, verbose=1, n_jobs=-1, return_train_score=True)\n#gs2_rf = GridSearchCV(rdm_balanced, param_grid=param_grid, scoring='f1_macro', cv=cv, verbose=1, n_jobs=-1, return_train_score=True)\n\nrs_rf.fit(X_train, y_train)\n\ny_score = rs_rf.predict_proba(X_test)\n\ny_pred_train = rs_rf.predict(X_train)\ny_pred_test = rs_rf.predict(X_test)\n\nrs_rf_score = f1_score(y_test, y_pred_test, average='macro')","c0d194bb":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(weights='uniform', n_neighbors=3)\n\nknn .fit(X_train, y_train)\n\ny_pred_test = knn .predict(X_test)\n\nknn_score = f1_score(y_test, y_pred_test, average='macro')\nknn_score","ebcb0973":"xgb = XGBClassifier(objective='binary:logitraw', gamma=0.02, min_child_weight=4, verbosity=1, random_state=42)\n\nparams = [\n    { # booster gbtree\n    'booster': ['dart'],\n    'max_depth': [2, 4, 5],\n    'learning_rate': np.linspace(1e-5, 1, 3),\n    'n_estimators': [100, 200],\n    'scale_pos_weight': np.arange(1, 15, 4)  # recomendado:  \n    }\n]\n\n#skf = StratifiedKFold(n_splits=3, shuffle = True, random_state = 42)\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3)\n\ngs_xgb = GridSearchCV(xgb, param_grid=params, scoring='f1_macro', n_jobs=-1, cv=cv, verbose=1)\n\ngs_xgb.fit(X_train, y_train)\n\ny_pred_test = gs_xgb.predict(X_test)\ny_score = gs_xgb.predict_proba(X_test)\n\nxgb_score = f1_score(y_test, y_pred_test, average='macro')\n\nmostrar_metricas(y_test, y_pred_test)","e9172ed0":"from sklearn.linear_model import LogisticRegression\n\nlgs = LogisticRegression(penalty='l2', solver='liblinear')\n\nlgs.fit(X_train, y_train)\n\ny_pred_test = lgs.predict(X_test)\n\nlgs_score = f1_score(y_test, y_pred_test, average='macro')","0f7bdfab":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\ngnb.fit(X_train, y_train)\n\ny_pred_test = gnb.predict(X_test)\n\ngnb_score = f1_score(y_test, y_pred_test, average='macro')","b779644e":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators=[('xgboost', gs_xgb), ('random', rs_rf), ('decisionTree', dct)], voting='hard', n_jobs=-1)\n\nvoting_clf.fit(X_train, y_train)\n\ny_pred_test = voting_clf.predict(X_test)\n\nf1_score(y_test, y_pred_test, average='macro')","575303d4":"for p in np.linspace(0, 1, 10):\n    predict_probs = gs_xgb.predict_proba(X_test)[:,1] * p + rs_rf.predict_proba(X_test)[:,1] * ((1-p)\/2) + lgs.predict_proba(X_test)[:,1] * ((1-p)\/2)\n    #predict_probs = gs_rf.predict_proba(X_test)[:,1] * p + dct.predict_proba(X_test)[:,1] * (1-p) \n    \n    predict = np.where(predict_probs >= 0.4, 1, 0)\n\n    print(f'xgboost * {p:.1f} + randomF * {(1-p)\/2:.1f} + decisionT * {(1-p)\/2:.1f}: {f1_score(y_test, predict, average=\"macro\"):.4f}')\n    #print(f'xgboost * {p:.1f} + L * {1-p:.1f}: {f1_score(y_test, predict, average=\"macro\"):.4f}')\n\nmostrar_metricas(y_test, predict)","a5d7db5c":"models = pd.DataFrame({\n    'Model': ['Decision Tree', 'Random Forest Balanceado', 'Random Forest', 'Random Forest Grid Search', 'KNN' ,'XGBoost', 'Regresi\u00f3n Logistica', 'Naive Bayes'],\n    'f1-macro': [dct_score, rdm_balanced_score, rdm_score, rs_rf_score, knn_score, xgb_score, lgs_score, gnb_score]})\nmodels.sort_values(by='f1-macro', ascending=False)","2c6d0d8e":"predictions = xxx.predict(X_Test)\n\n##Si es con el Voting\n#predict_probs = random_search.predict_proba(X_Test)[:,1] * 0.8 + gs_rf.predict_proba(X_Test)[:,1] * 0.1 + lgs.predict_proba(X_Test)[:,1] * 0.1#\n#predict = np.where(predict_probs >= 0.4, 1, 0)\n\noutput = pd.Series(predictions, name='y').to_csv('sample_submit.csv', index_label='id')","66954b48":"import pickle\nwith open('NOMBRE.pkl', 'wb') as fp:\n    pickle.dump(poner_el_modelo_aca, fp)","ca22123a":"#### N\u00famero de empleados. Indicador cuart\u00edlico.","c756661f":"## Ahora analizemos varias variables en conjunto con la variable objetivo","5b68df93":"ros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\nprint (\"Distribution before resampling {}\".format(Counter(y)))\nprint (\"Distribution after resampling {}\".format(Counter(y_resampled)))","4ac38824":"### Numero de contactos realizados previo a esta campa\u00f1a para este cliente\n\nLa mayor\u00eda no hab\u00eda sido contactado con anterioridad.","a05d68b3":"##\u00a0Exportar modelo","3af96c3d":"## \u00bfPodemos hacer feature enginnering?","6e01f233":"### Resumen de otros atributos\n\n* En esta campa\u00f1a, la mayor\u00eda de las veces solo se contact\u00f3 una (> 45%) o dos veces al cliente (> 25%).\n* M\u00e1s del 90% de clientes de la campa\u00f1a actual no hab\u00edan sido contactados en la campa\u00f1a anterior.\n* Antes de esta campa\u00f1a, m\u00e1s del 80% de los clientes no hab\u00eda sido contactado.\n* M\u00e1s del 80 % no participo de la campa\u00f1a anterior.","0f3cba3d":"smote_enn = SMOTEENN(random_state=0)\nX_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n\nprint (\"Distribution before resampling {}\".format(Counter(y)))\nprint (\"Distribution after resampling {}\".format(Counter(y_resampled)))","10947922":"##\u00a0Separamos los datos en train y test","44d77cc4":"\n\n### N\u00famero de d\u00edas que pasaron desde el \u00faltimo contacto al cliente por una campa\u00f1a anterior. \n\n(999 significa que el cliente no ha sido contactado con anterioridad)\n\nEn la mayor\u00eda de los casos el cliente no fue contactado por la campa\u00f1a anterior.","b108eea0":"##\u00a0Tabla general","a5d368f6":"### VotingClassifier","511e66dd":"#### Veamos cuantas variables nulas \"unknown\" existen por feature","5575bb12":"# Entrenamos los modelos","3a271459":"### Random Forest","634d4ed7":"\n\n### Resultado de la campa\u00f1a de marketing anterior\n\nIgual que en `previous` la mayor\u00eda no se contact\u00f3 anteriormente, por ende no tienen resultado","75fa00a3":"### \u00bfQu\u00e9 meses fue m\u00e1s efectiva la campa\u00f1a?","a98d7d33":"### Voting a mano","93f29916":"## Variable objetivo\n\u00bfSe suscribi\u00f3 el cliente al plazo fijo?\n\nSe observa que existe un gran desbalance respecto a nuestro target, esto podr\u00eda ocasionar problemas a la hora de realizar la clasificaci\u00f3n con los modelos de Machine Learning.","1c9a9619":"###\u00a0\u00bfCu\u00e1l es la distribuci\u00f3n de las edades respecto a los que pidieron y los que no pidieron un plazo fijo?\n\nAmbas distribuciones son muy similares, aunque a partir de los 60 a\u00f1os se puede observar que la mayor\u00eda tienen plazo fijo, por lo que esto nos hace pensar que no debemos sacar los valores Outliers de las edades (como vimos al principio del notebook) ya que pueden ayudar al modelo a tener una mejor clasificaci\u00f3n.","07f773c9":"## \u00bfNecesitamos escalar alguna feature?\n\nSi o no, se podr\u00eda pensar que si ya que tenemos algunas features con valores -, otras con + y con valores bastante dispares. \n\nLuego de entrenar los modelos vimos que no era necesario, por lo que se decide dejar la pregunta pero no el c\u00f3digo.","e87b7c72":"### Estado civil\nLa mayor parte de los clientes del banco estan casados y la cantidad de valores desconocidos no es significativa (0.2%)","a589cbdf":"###\u00a02. Oversampling\nCrearemos muestras nuevas \u00absint\u00e9ticas\u00bb de la clase minoritaria.","baa8f92c":"#### Nota-Hipotecado:\n- La proporci\u00f3n de sin hipoteca y con hipoteca es muy similar. 52 y 45 respectivamente\n\n- Existe un 2.35% de valores desconocidos.\n\n- Existe una coincidencia entre los `Unknown` de la Feature `Loan` y de esta feature. (todos los unk de loan son unk en housing)","69f1901d":"### 1. Subsampling en la clase mayoritaria","4bb0c41e":"us = NearMiss(n_neighbors=3, version=2)\nX_train_res, y_train_res = us.fit_resample(X_train, y_train)\n   \nprint (\"Distribution before resampling {}\".format(Counter(y)))\nprint (\"Distribution after resampling {}\".format(Counter(y_train_res)))","0e222b8b":"## Analizando Contexto socio-econ\u00f3mico","f2953b7e":"### \u00bfQu\u00e9 estado civil tienen las personas que se suscribieron al Plazo Fijo?\n Aunque a simple vista parece que los de estado 'married' son los que m\u00e1s tienen plazos fijos, en realidad los `single` tuvieron una mayor proporci\u00f3n de compra de plazo fijo.","300d2b1b":"###\u00a0Random Forest Balanceado con libreria","cc6cd8b4":"#### Nota:\n- Los meses de mayores contactos son: Mayo (33%), Julio (17%), Agosto (15%), Junio (12%) y Noviembre (10%).\n\n- Los meses de menor eficacia de los contactos son: Mayo (6%), Julio (8%), Noviembre (10%), Junio (10%) y Agosto (6%).\n\n- Se puede deducir que las campa\u00f1as de MKT en estos meses son m\u00e1s exahustivas porque existe una menor tasa de efectividad para las mismas.\n\n- Marzo, Diciembre y Septiembre presentan comportamiento opuesto, en el mismo an\u00e1lisis","d7d6cf11":"###\u00a0Naive Bayes","070a7c90":"###\u00a0Regresi\u00f3n Log\u00edstica","8d788ff5":"## \u00bfQu\u00e9 hacemos con los valores unknown?\n\nSe decide dejar las filas que contienen la palabra \"unknown\" ya que no sucede en demasiadas ocasiones.","bc458764":"# Importaci\u00f3n de librerias","de1d7b52":"###\u00a0XGBoost - Con GridSearch","57c6c5a2":"### Relacionado con el \u00faltimo contacto de la campa\u00f1a actual\n- La mayor\u00eda de los contactos se hicieron por celular.\n\n- La mayor\u00eda de los clientes fue contactado por \u00faltima vez en el mes de Mayo.\n\n- Respecto a los d\u00edas de contacto cas\u00ed que se mantienen en igual cantidad.","6e3896d8":"\n\n\n\n### Tipo de trabajo\nEl top de los tipos de trabajos mayoritarios en el data set son \"Administrators\", \"Blue-collar\" and \"Technician\". Los que menos aparecen son \"House maid\", \"unemployed\" and \"Student\". \n\n---\n\n\n\u00bfSignifica que la mayor\u00eda de los clientes del banco trabajan como administradores?","22c20a81":"### \u00bfQu\u00e9 tipo de trabajo tienen las personas que se suscribieron al plazo fijo? \n- Si bien las cateogr\u00edas `admin`, `blue collar` y `technician` son las m\u00e1s abundantes, parece que la relaci\u00f3n de los que adquieren el PF es mayor en otras categor\u00edas.\n\n- 64% de los clientes comprenden estas tres categor\u00edas.\n\n- Los que m\u00e1s adquieren los PF son: `Student` y `Retired`, seguido por `Unemployed`, `Admin` y `Management` .\n\n- La cantidad de Valores nulos no es significativa: 0,8%, pero 11% de estos son de target positivo.","0e6edb4d":"# Carga del Data set","d0f42c0e":"# An\u00e1lisis Exploratorio de datos\nSe observa que el data set de train tiene 28.830 registros m\u00e1s que el data set de test.","d0db6540":"# Competencia Ac\u00e1mica DS-COR-9\n\n**Problema**\n\nUna entidad bancaria necesita predecir el resultado de una llamada telef\u00f3nica para saber si con la informaci\u00f3n recabada (o la que ya se tiene) el cliente contactado o por contactar se suscribir\u00e1 a un plazo fijo.\n\nLink: https:\/\/www.kaggle.com\/c\/Banco-PF-DSCOR9\/overview\n\n\n**DATA SET**\n\n**Datos personales y bancarios:**\n\n1 - age: edad. Num\u00e9rica.\n\n2 - job: tipo de trabajo. Categ\u00f3rica: [\"admin.\",\"blue-collar\",\"entrepreneur\",\"housemaid\",\"management\",\"retired\",\"self-employed\",\"services\",\"student\",\"technician\",\"unemployed\",\"unknown\"].\n\n3 - marital: estado civil. Categ\u00f3rica: [\"divorced\",\"married\",\"single\",\"unknown\"; nota: \"divorced\" es divorciado\/a o viudo\/a].\n\n4 - education: educaci\u00f3n. Categ\u00f3rica: [\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"illiterate\",\"professional.course\",\"university.degree\",\"unknown\"].\n\n5 - default: tiene cr\u00e9dito impago. Categ\u00f3rica: [\"no\",\"yes\",\"unknown\"].\n\n6 - housing: tiene pr\u00e9stamo hipotecario. Categ\u00f3rica: [\"no\",\"yes\",\"unknown\"].\n\n7 - loan: tiene pr\u00e9stamo personal. Categ\u00f3rica: [\"no\",\"yes\",\"unknown\"].\n\n\n**Relacionado con el \u00faltimo contacto de la campa\u00f1a actual:**\n\n8 - contact: medio de contacto. Categ\u00f3rica: [\"cellular\",\"telephone\"].\n\n9 - month: mes del \u00faltimo contacto. Categ\u00f3rica: [\"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\"].\n\n10 - day_of_week: dia de la semana del \u00faltimo contacto. Categ\u00f3rica: [\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"].\n\n\n**Otros atributos:**\n\n11 - campaign: numero de contactos realizados durante esta campa\u00f1a para este cliente. Num\u00e9rica (incluye el \u00faltimo contacto).\n\n12 - pdays: n\u00famero de d\u00edas que pasaron desde el \u00faltimo contacto al cliente por una campa\u00f1a anterior. Num\u00e9rica (999 significa que el cliente no ha sido contactado con anterioridad).\n\n13 - previous: numero de contactos realizados previo a esta campa\u00f1a para este cliente. Num\u00e9rica.\n\n14 - poutcome: resultado de la campa\u00f1a de marketing anterior. Categ\u00f3rica: [\"failure\",\"nonexistent\",\"success\"].\n\n\n**Contexto socio-econ\u00f3mico:**\n\n15 - emp.var.rate: Tasa de variaci\u00f3n de empleo. Indicador cuart\u00edlico. Num\u00e9rica.\n\n16 - cons.price.idx: \u00cdndice de precio de consumidor. Indicador mensual. Num\u00e9rica.\n\n17 - cons.conf.idx: \u00cdndice de confianza de consumidor. Indicador mensual. Num\u00e9rica.\n\n18 - euribor3m: Tasa Eur\u00edbor (3 meses). Indicador diario. Num\u00e9rica.\n\n19 - nr.employed: n\u00famero de empleados. Indicador cuart\u00edlico. Num\u00e9rica.\n\n\n**Variable objetivo:**\n\n20 - y: \u00bfse suscribi\u00f3 el cliente al plazo fijo? Binario [\"yes\",\"no\")","c963254a":"### Educaci\u00f3n\nLa mayor\u00eda de los clientes al menos termino la secundar\u00eda y obtuvo un titulo de grado.\nPor otro lado, el porcentaje de `High School` es del 23%, similar a lo que era el porcentaje de `Blue Collars`","ca2c3638":"#### Tasa Eur\u00edbor (3 meses) - Indicador diario.\n\nAcr\u00f3nimo de European Interbank Offered Rate, tipo europeo de oferta interbancaria,es un tipo de referencia hipotecario que se publica diariamente y refleja el tipo de inter\u00e9s al que las principales entidades financieras se prestan dinero entre s\u00ed en el mercado interbancario","24d59400":"##\u00a0Seleccionamos las variables","5f1831c6":"### Decision Tree","402e6f22":"##\u00a0Modelos","b1ab37ac":"### Resumen de los datos personales y bancarios\n\n* Se observa que no se tiene una distribuci\u00f3n uniforme respecto a la edad, la mayor\u00eda esta entre los 30 y 45 a\u00f1os. Adem\u00e1s, existen valores Outliers luego de los 70 a\u00f1os.\n* El top de los tipos de trabajos mayoritarios en el data set son \"Administrators\", \"Blue-collar\" and \"Technician\". Los que menos aparecen son \"House maid\", \"unemployed\" and \"Student\". \n* La mayor parte de los clientes del banco estan casados.\n* La mayor\u00eda de los clientes al menos termino la secundar\u00eda y obtuvo un titulo de grado.\n* Casi el 80% de los clientes NO tiene un Credito impago.\n* M\u00e1s del 50% tiene un prestamo hipotecado.\n* Casi el 80% de los clientes NO tiene un prestamo personal.\n* M\u00e1s del 60% de los clientes fue contactado por celular.\n* M\u00e1s del 30% fue contactado por \u00faltima vez en el mes de Mayo.\n* Los d\u00edas de \u00faltimo contacto tienen una distribuci\u00f3n muy similar rondando el 20%.","02d2d1c4":"**Basandonos en el mejor score entre el de test y train.**","2d10b32e":"## \u00bfC\u00f3mo podemos balancear el data set?\nUtilizamos algunas de las siguientes t\u00e9cnicas, pero aclarando de que ninguna nos resulto \u00fatil para la competencia, si bien los scores eran muy buenos en el notebook, luego en la realidad daban puntajes muy bajos.","63477783":"### IPC - \u00cdndice de precio de consumidor (Indicador Mensual)\n\nEl \u00cdndice de Precios al Consumidor es un indicador que mide la evoluci\u00f3n promedio de los precios de un conjunto de bienes y servicios representativos del gasto de consumo de los hogares residentes en un \u00e1rea determinada.","04cf35ce":"### 3. SMOTEEN\n\nEl vecino m\u00e1s cercano editado elimina cualquier ejemplo cuya etiqueta de clase difiere de la clase de al menos dos de sus tres vecinos m\u00e1s cercanos. El m\u00e9todo ENN elimina las instancias de la clase mayoritaria cuya predicci\u00f3n realizada por el m\u00e9todo KNN es diferente de la clase mayoritaria. El m\u00e9todo ENN puede eliminar tanto los ejemplos ruidosos como los ejemplos l\u00edmite, proporcionando una superficie de decisi\u00f3n m\u00e1s suave. ENN tiende a eliminar m\u00e1s ejemplos que los enlaces de Tomek, por lo que se espera que proporcione una limpieza de datos m\u00e1s profunda","97826fd1":"**Pasamos las variables categoricas a n\u00famericas**","ad4d7eda":"####\u00a0KNN","184eec0a":"###\u00a0Random Forest - Con RandomizedSearchCV","a9f35fb2":"### \u00bfExisten correlaciones (+ o -) entre las variables?","e51bbe03":"## \u00bfEliminamos alguna columna?\nEliminamos id, previous y default, ya que no nos resultan de utilidad para este problema. Se aclar\u00e1 que la columna \"id\" que corresponden a cada cliente no aporta a nuestro problema.","1bc5940c":"### N\u00famero de contactos realizados durante esta campa\u00f1a para un cliente\n\nSe puede decir que la mayor\u00eda de las veces solo se contact\u00f3 una o dos veces al cliente.","ea79d4a2":"### Tasa de variaci\u00f3n de empleo","bf3112bc":"## Analizando Datos personales de clientes","4ae68553":"### \u00bfLos clientes que tienen plazo fijo tienen creditos impagos o prestamos?","5f35b631":"#### ICC - \u00cdndice de confianza de consumidor (Indicador mensual %)\n\nLa confianza del consumidor es un indicador econ\u00f3mico que mide el grado de optimismo que los consumidores sienten sobre el estado general de la econom\u00eda y sobre su situaci\u00f3n financiera personal.","f8c39393":"### Edad\n- Se observa que no se tiene una distribuci\u00f3n uniforme.\n\n- El 65% de los clientes se encuentra en el rango entre los 30 y 50 a\u00f1os, el 40% de los mismos se concentra entre 30 y 40.\n\n- Mayores de 60 o menores de 20, practicamente nulos\n\n- Al discriminar por target y==1, se obtiene una observaci\u00f3n importante.\n\n- Las personas mayores ten\u00edan una distribuci\u00f3n 50 - 50 entre los que adquirieron y los que no.\n\n- No hay valores nulos o desconocidos","777c8979":"# Preparando el Data set para aplicar Machine Learning","4cc497d7":"##\u00a0Funci\u00f3n general para armar m\u00e9trica","71cfa607":"##\u00a0Guardar modelo","b8c5f17b":"## Analizando Otros atributos\n","6dcf5d45":"\n\n\n### Cr\u00e9ditos y prestamos\n* Casi el 80% de los clientes NO tiene un Credito impago.\n* M\u00e1s del 50% tiene un prestamo hipotecado.\n* Casi el 80% de los clientes NO tiene un prestamo personal."}}