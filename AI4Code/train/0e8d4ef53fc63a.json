{"cell_type":{"c5211fa0":"code","abee4b82":"code","8c6fceb4":"code","38c3fb6f":"code","1b3571ab":"code","22b4a3ad":"code","44cc5bcb":"code","c84034b3":"code","55457d5e":"code","26075da8":"code","709f096d":"code","7256c34c":"code","7eeca349":"code","6ef81aa7":"code","83dd606d":"code","ee748b86":"code","84652d79":"code","58a9ff22":"code","d9c51a5a":"code","08ec0df8":"code","af47cba2":"code","3e938199":"code","4a3099c9":"code","caa4fcc5":"code","8d7ace63":"code","213f011a":"code","03b04840":"code","49f03677":"code","00b791a1":"code","9b39b6dc":"code","d933cf21":"code","faa15bf3":"code","3c54bd68":"code","a2c1966f":"code","88aa4f33":"code","1bb315c0":"code","70035c96":"code","5921c47a":"code","48ac2ea0":"code","9e43843d":"code","76712eb7":"code","521fbf96":"code","58780ed4":"code","4a1eb9b0":"code","38151594":"code","3300a965":"code","f8e3b5bb":"code","fd27f9d2":"code","de456ce1":"code","77b3ac4c":"code","bc30f41c":"code","37a1fa0b":"markdown","5c9b609c":"markdown","ccfc7eab":"markdown","8c98607c":"markdown","a60a82de":"markdown","ef031936":"markdown","fe0b8f30":"markdown","9efea1fb":"markdown","918a2b77":"markdown","9d03126a":"markdown"},"source":{"c5211fa0":"#loading the necessary libraries\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","abee4b82":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","8c6fceb4":"#quick look at our data types & null counts \ntraining.info()","38c3fb6f":"#let's look at the columns of the datasets\ntraining.columns","1b3571ab":"# to better understand the numeric data, we want to use the .describe() method. This gives us an understanding of the central tendencies of the data \ntraining.describe()","22b4a3ad":"# Look at numeric and categorical values separately\n\n\ntraining.select_dtypes(include=[\"int\", \"float64\"]).columns","44cc5bcb":"#for checking the categorical variable, we would execute the below code\ntraining.select_dtypes(include = [\"object\"]).columns","c84034b3":"#let's seperate the numerical values from the datasets\ntraining_num = training[['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n       'train_test']]","55457d5e":"#correlation matrix to find out the most related features to the \"survived\" \ncorrmat = training.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cmap=\"coolwarm\", vmax=.8, square=True, annot=True);","26075da8":"#it is too much features to look at, let's look at the top 10 features related to the survived column\n# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Survived')['Survived'].index\ncm = np.corrcoef(training[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","709f096d":"#for checking the categorical variable, we would execute the below code\ntraining.select_dtypes(include = [\"object\"]).columns","7256c34c":"training_cat = training[['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","7eeca349":"#Distribution for all numeric variables \n\nfor i in training_num.columns:\n    plt.hist(training_num[i])\n    plt.title(i)\n    plt.show()","6ef81aa7":"for i in training_cat.columns:\n    sns.barplot(training_cat[i].value_counts().index, training_cat[i].value_counts()).set_title(i)\n    plt.show()","83dd606d":"# grid = sns.FacetGrid(training, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(training, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","ee748b86":"# grid = sns.FacetGrid(training, col='Embarked')\ngrid = sns.FacetGrid(training, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","84652d79":"# grid = sns.FacetGrid(training, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(training, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","58a9ff22":"training = training.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\nall_data =  pd.concat([training,test])","d9c51a5a":"training.head()","08ec0df8":"#feature engineering on person's title \ntraining.Name.head(50)\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","af47cba2":"training['name_title'].value_counts()","3e938199":"#missing data\ntotal = training.isnull().sum().sort_values(ascending=False)\npercent = (training.isnull().sum()\/training.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)\n","4a3099c9":"all_data.head()","caa4fcc5":"#create all categorical variables that we did above for both training and test sets \n\n\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(training.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","8d7ace63":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","213f011a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","03b04840":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","49f03677":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","00b791a1":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9b39b6dc":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d933cf21":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","faa15bf3":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","3c54bd68":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","a2c1966f":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","88aa4f33":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","1bb315c0":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","70035c96":"svc_poly = SVC( probability = True, kernel='poly', degree=2, gamma='auto', coef0=1, C=5)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5921c47a":"svc_rbf = SVC( probability = True, kernel='rbf', gamma=0.5, C=0.1)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","48ac2ea0":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9e43843d":"#Voting classifier takes all of the inputs and averages the results. \n#For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb),\n                                            ('svc_poly', svc_poly), ('svc_rbf', svc_rbf) ], voting = 'soft') ","76712eb7":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","521fbf96":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","58780ed4":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","4a1eb9b0":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","38151594":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","3300a965":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","f8e3b5bb":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","fd27f9d2":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [300],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","de456ce1":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","77b3ac4c":"# xgb = XGBClassifier(random_state = 1)\n\n# param_grid = {\n#     'n_estimators': [50, 100, 200],\n#     'colsample_bytree': [0.75,0.8,0.85],\n#     'max_depth': [10],\n#     'reg_alpha': [1],\n#     'reg_lambda': [2, 5, 10],\n#     'subsample': [0.55, 0.6, .65],\n#     'learning_rate':[0.5],\n#     'gamma':[.5,1,2],\n#     'min_child_weight':[0.01],\n#     'sampling_method': ['uniform']\n# }\n\n# clf_xgb = GridSearchCV(xgb, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n# clf_performance(best_clf_xgb,'XGB')","bc30f41c":"y_hat_rf = best_clf_rf.best_estimator_.predict(X_test_scaled).astype(int)\nrf_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_rf}\nsubmission_rf = pd.DataFrame(data=rf_submission)\nsubmission_rf.to_csv('rf_submission.csv', index=False)","37a1fa0b":"# Model Tuned Performance","5c9b609c":"Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\nCorrecting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin and Ticket features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent","ccfc7eab":"Correlating categorical features\nNow we can correlate categorical features with our solution goal.\n\nObservations.\n\nFemale passengers had much better survival rate than males. Confirms classifying\n\nException in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily \ndirect correlation between Embarked and Survived.\n\nMales had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing\n\nPorts of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating\n\nDecisions.\n\nAdd Sex feature to model training.\nComplete and add Embarked feature to model trainin","8c98607c":"# For numeric data\n* Made histograms to understand distributions\n* Corrplot\n* Pivot table comparing survival rate across numeric variables\n\n# For Categorical Data\n* Made bar charts to understand balance of classes\n* Made pivot tables to understand relationship with survival","a60a82de":"Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\nObservations.\n\nPclass=3 had most passengers, however most did not survive. Confirms our classifying assumption \n\nInfant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption\n\nMost passengers in Pclass=1 survived. Confirms our classifying assumption \n\nPclass varies in terms of Age distribution of passengers.\n\nDecisions.\n\nConsider Pclass for model training.","ef031936":"# Project Planning\n\n* Understand nature of the data .info() .describe()\n* Histograms and boxplots \n* Value counts \n* Missing data \n* Correlation between the metrics \n* Explore interesting themes \n* Feature engineering \n* preprocess data together or use a transformer? \n* use label for train and test   \n* Scaling?\n* Model Baseline \n* Model comparison with CV ","fe0b8f30":"# Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\nObservations.\n\nHigher fare paying passengers had better survival. Confirms our assumption for creating fare ranges.\n\nPort of embarkation correlates with survival rates. Confirms correlating and completing.\n\nDecisions.\n\nConsider banding Fare feature.","9efea1fb":"# Data Preprocessing for Model\n1) Drop null values from Embarked (only 2)\n\n2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with)\n\nVariables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. \nWe also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n\n4) Impute data with mean for fare and age (Should also experiment with median)\n\n5) Normalized fare using logarithm to give more semblance of a normal distribution\n\n6) Scaled data 0-1 with standard scaler","918a2b77":"In this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. \n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.","9d03126a":"Build the Models"}}