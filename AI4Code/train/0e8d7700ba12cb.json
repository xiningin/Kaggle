{"cell_type":{"8cc961c2":"code","1adc4f7c":"code","d5e36f16":"code","1ce1a563":"code","8af5fc0a":"code","6f97bcdd":"code","43c40725":"code","ce1e3b17":"code","95376722":"code","7b6cfed9":"code","012dba89":"code","798b4fd6":"code","d7eca48c":"code","dca3888a":"code","79bbb021":"code","d6bfa4eb":"code","6d22d8cf":"code","45511fde":"code","c1f95415":"code","5f13ca8c":"code","44f1ef4f":"code","37075277":"code","4acf082c":"code","1b48e9cc":"code","6caee499":"code","a42741eb":"code","db500340":"code","a19bbd90":"code","b0ad6d25":"code","4d1f01ef":"code","5c8e6dc1":"code","8ba72cc5":"code","972a4a47":"code","066f2dd9":"code","cd92c1f8":"code","fb215b0c":"code","d732ad62":"code","d09dcebe":"code","c87ee479":"code","b6b93591":"code","f7808778":"code","d38e8c98":"code","fc9e5d47":"code","0e2cebaa":"code","28878767":"code","5c9cfda1":"code","ea9143a3":"code","6279246e":"code","7576d5bf":"code","644693c5":"markdown","b1f302ba":"markdown","9dcbaf66":"markdown","f4fdd5e3":"markdown","feffe3ae":"markdown","f913f5e1":"markdown","e797ec95":"markdown","87c069c3":"markdown","b5bfaf77":"markdown","75214f1a":"markdown","2beca224":"markdown","b950525f":"markdown","8c4e30f9":"markdown","8ecf23a4":"markdown","679ea76e":"markdown","5be9076d":"markdown","373fc1ec":"markdown","fb808af7":"markdown","8958de4b":"markdown","7a6433d8":"markdown","889b4f65":"markdown","b4bafa51":"markdown","a677682d":"markdown","a6b1f2d1":"markdown","bc6b66cc":"markdown","01127199":"markdown","b9c0f5e9":"markdown","0759646a":"markdown","0dd289bc":"markdown"},"source":{"8cc961c2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA","1adc4f7c":"df = pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\")","d5e36f16":"df.head()","1ce1a563":"df.info()","8af5fc0a":"df.describe()","6f97bcdd":"df.isna().sum()","43c40725":"df.drop(['CUST_ID'], axis=1, inplace=True)","ce1e3b17":"# Fill up the missing elements with mean of the 'MINIMUM_PAYMENT' \ndf['MINIMUM_PAYMENTS'] = df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].mean())","95376722":"# Fill up the missing elements with mean of the 'CREDIT_LIMIT' \ndf['CREDIT_LIMIT'] = df['CREDIT_LIMIT'].fillna(df['CREDIT_LIMIT'].mean())","7b6cfed9":"# Let's see if we have duplicated entries in the data\ndf.duplicated().sum()","012dba89":"n = len(df.columns)\nn","798b4fd6":"numerical_features = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","d7eca48c":"def plot_distribution(feature):\n    sns.displot(x=feature, data=df, kde=True, color='#244747');\n    plt.figtext(0.2, 1, '%s Distribution'%feature, fontfamily='serif', fontsize=17, fontweight='bold');\n\ndef plot_num_cat(feature, target, figsize=None):\n    fig = plt.figure(figsize=(15,6))\n\n    for value in data[target].unique():\n        sns.kdeplot(data[data[target]==value][feature])\n\n    fig.legend(labels=data[target].unique())\n    plt.title('{} distribution based on {}'.format(feature, target))\n    plt.show()\n    \ndef plot_num_num(feature, target):\n    sns.regplot(x=feature, y=target, data=data, color='#244747')\n    plt.show()\n    \ndef plot_cat_cat(feature, target):\n    plot_data = data.groupby([feature, target])[feature].agg({'count'}).reset_index()\n\n    fig = px.sunburst(plot_data, path=[feature, target], values='count', #color_continuous_scale='gray', color=feature, \n                      title='Affect of {} on Customer {}'.format(feature, target), width = 600, height = 600)\n    \n    fig.update_layout(plot_bgcolor='white', title_font_family='Calibri Black', title_font_color='#221f1f', \n                      title_font_size=22, title_x=0.5)\n    fig.update_traces(textinfo = 'label + percent parent')\n    fig.show()","dca3888a":"# looking at the distribution of data\nfor feature in numerical_features:\n    plot_distribution(feature)","79bbb021":"# data is highly skewed and hence taking log transformation\ndf = df.copy()\nskewed_features = ['BALANCE', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS']\n\nfor feature in skewed_features:\n    df[feature] = np.log(1+df[feature])","d6bfa4eb":"# looking at the distribution of data\nfor feature in numerical_features:\n    plot_distribution(feature)","6d22d8cf":"plt.figure(figsize=(15, 8))\nsns.heatmap(round(df[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","45511fde":"# rescaling data\nscale = StandardScaler()\ntrain_data = scale.fit_transform(df)\ntrain_data.shape","c1f95415":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.cluster import KMeans\n#from sklearn.metrics import silhouette_score, rand_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import SpectralClustering \nfrom sklearn.cluster import DBSCAN","5f13ca8c":"scores_1 = []\n\nrange_values = range(1,20)\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(train_data)\n    scores_1.append(kmeans.inertia_)\nplt.plot(scores_1, 'bx-')\nplt.style.use('ggplot')\nplt.title('Finding the right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('Scores') \nplt.show()","44f1ef4f":"kmeans = KMeans(8)\nkmeans.fit(train_data)\nlabels = kmeans.labels_","37075277":"kmeans.cluster_centers_.shape","4acf082c":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_,columns = [df.columns])\ncluster_centers","1b48e9cc":"cluster_centers = scale.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers,columns = [df.columns])\ncluster_centers","6caee499":"# Labels associated to each data point\nlabels.shape","a42741eb":"labels.max()","db500340":"labels.min()","a19bbd90":"y_kmeans = kmeans.fit_predict(train_data)\ny_kmeans","b0ad6d25":"# concatenate the clusters labels to our original dataframe\ncreditcard_df_cluster = pd.concat([df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster.head()","4d1f01ef":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(train_data)\nprincipal_comp","5c8e6dc1":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data=principal_comp,columns=['pca1','pca2'])\npca_df.sample(5)","8ba72cc5":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","972a4a47":"plt.figure(figsize=(10,10))\nplt.style.use('ggplot')\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink','yellow','gray','purple', 'black'])\nplt.show()","066f2dd9":"from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform","cd92c1f8":"encoding_dim = 7\n\ninput_df = Input(shape=(17,))\n\n\n# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution \n\nx = Dense(encoding_dim, activation='relu')(input_df)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nencoded = Dense(10, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\nx = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)\n\ndecoded = Dense(17, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n#encoder - used for our dimention reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer= 'adam', loss='mean_squared_error')","fb215b0c":"train_data.shape","d732ad62":"autoencoder.fit(train_data,train_data,batch_size=128,epochs=25,verbose=1)","d09dcebe":"autoencoder.summary()","c87ee479":"pred_ac = encoder.predict(train_data)","b6b93591":"pred_ac.shape","f7808778":"scores_2 = []\n\nrange_values = range(1,20)\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(pred_ac)\n    scores_2.append(kmeans.inertia_)\nplt.plot(scores_2, 'bx-')\nplt.style.use('ggplot')\nplt.title('Finding the right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('Scores') \nplt.show()","d38e8c98":"plt.plot(scores_1, 'bx-', color = 'r',label='Without Autoencode')\nplt.plot(scores_2, 'bx-', color = 'g',label='With Autoencode')","fc9e5d47":"kmeans = KMeans(4)\nkmeans.fit(pred_ac)\nlabels = kmeans.labels_\nkmeans.cluster_centers_.shape","0e2cebaa":"y_kmeans = kmeans.fit_predict(train_data)\ny_kmeans","28878767":"# concatenate the new reduced clusters labels to our original dataframe\ncreditcard_df_cluster_new = pd.concat([df, pd.DataFrame({'cluster':labels})], axis = 1)\ncreditcard_df_cluster_new.head()","5c9cfda1":"# Obtain the principal components \npca = PCA(n_components=2)\nprincipal_comp_new = pca.fit_transform(pred_ac)\nprincipal_comp_new","ea9143a3":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data=principal_comp_new,columns=['pca1','pca2'])\npca_df.sample(5)","6279246e":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","7576d5bf":"plt.figure(figsize=(10,10))\nplt.style.use('ggplot')\nax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"cluster\", data = pca_df, palette =['red','green','blue','pink'])\nplt.show()","644693c5":"**Apply Kmeans algorithm**","b1f302ba":"**Load the data**","9dcbaf66":"Woah!! There is a lot of skewness and they are varied. It is kind of expected from datasets like these as there will always be a few customers who do very high amount of transactions.\n\nNow it depends on our application whether we want to handle the skewness in our dataset or not for a clustering problem. For instance if we want to do clustering for anamoly detection in that case we wouldn't want to handle the outliers as we would like our model to detect them and group them in a cluster. For our application I am looking for a good visualization so I would like to handle the skewness as much as possible as it will help the model to form better clusters.\n\nLet's see if we can do something about this.","f4fdd5e3":"**Comparing both the results, using Autoencoders and by not using Autoencoders**","feffe3ae":"# Credit Card Customer Segmentation\n","f913f5e1":"- CUSTID : Identification of Credit Card holder (Categorical)\n\n- BALANCE : Balance amount left in their account to make purchases \n- BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n- PURCHASES : Amount of purchases made from account\n- ONEOFFPURCHASES : Maximum purchase amount done in one-go\n- INSTALLMENTSPURCHASES : Amount of purchase done in installment\n- CASHADVANCE : Cash in advance given by the user\n- PURCHASESFREQUENCY : How frequently the Purchases are being made, - score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n- ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n- PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n- CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n- CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n- PURCHASESTRX : Numbe of purchase transactions made\n- CREDITLIMIT : Limit of Credit Card for user\n- PAYMENTS : Amount of Payment done by user\n- MINIMUM_PAYMENTS : Minimum amount of payments made by user\n- PRCFULLPAYMENT : Percent of full payment paid by user\n- TENURE : Tenure of credit card service for user","e797ec95":"\u2022 Marketing is crucial for the growth and sustainability of\nany business.\n\n\u2022 Marketers can help build the company's brand, engage\ncustomers, grow revenue, and increase sales.\n\n\u2022 One of the key pain points for marketers is to know their\ncustomers and identify their needs.\n\n\u2022 By understanding the customer, marketers can launch a\ntargeted marketing campaign that is tailored for specific\nneeds.\n\n\u2022 If data about the customers is available, data science\ncan be applied to perform market segmentation.\n\n\u2022 In this case study, you have been hired as a consultant\nto a bank in New York City.\n\n\u2022 The bank has extensive data on their customers for the\npast 6 months.\n\n\u2022 The marketing team at the bank wants to launch a\ntargeted ad marketing campaign by dividing their\ncustomers into at least 3 distinctive groups.\n","87c069c3":"# 4.Find the optimal number of clusters using elbow method","b5bfaf77":"- Mean balance is 1564\n- Balance frequency is frequently updated on average ~0.9\n- Purchases average is 1000\n- one off purchase average is 600\n- Average purchases frequency is around 0.5\n- average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low\n- Average credit limit ~ 4500\n- Percent of full payment is 15%\n- Average tenure is 11 years\n","75214f1a":"# 5. Apply k-means method","2beca224":"- Mean of balance is 1500\n- 'Balance_Frequency' for most customers is updated frequently ~1\n- For 'PURCHASES_FREQUENCY', there are two distinct group of customers\n- For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently\n- Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0\n- Credit limit average is around 4500\n- Most customers are ~11 years tenure","b950525f":"# 2. Import libraries and datasets","8c4e30f9":"As seen that the shape of our input has reduced. Now using this as an input and repeting the whole process with the reduced input","8ecf23a4":"Customer ID seems to be an unique id for each customer and hence won't play any role in determining the cluster.","679ea76e":"# 1. Understand the problem statement and business case","5be9076d":"# 7. Apply principal component and visualize the results","373fc1ec":"So as per my observations I find the optimal value of clusters to be 4 as the curve seems to be linear after 4","fb808af7":"So to summarize all the steps:\n1. Load the data & just have a brief look at it. Try to find information(.info), use .describe. By doing so you will be able to get good understanding of the data. Try to understand all the features and what do they mean as this is very important to understand which features are the most important or which are the least important. If possible try to ask questions to the team\/person who has provided you the dataset. This step is important in a real world project.\n\n2. Do some exploratory data analysis (EDA). Find missing values. Handling missing values is a critical step. You have to ask youe self this question.\nIs this value missing becuase it wasn't recorded or becuase it dosen't exist?\nIf a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called \"imputation\") :)\n\nMake some really good graphs by extracting information from the data. As a data scientist you might be asked for presentations of your work\/product. Beautiful graphs really helps a lot.\n\n3. Now comes the Machine learning part. For this dataset I used unsupervised learning. Find the optimum number of clusters by using 'Elbow Method'. Apply Kmeans clustering and then use PCA dimensionality reduction technique to make a graph of your clusters.\n\n4. Use Autoencoding technique to encode the original data and reduce its dimensions. Then use the reduced encoded data as a new input and follow step 3 again.","8958de4b":"# 3.Visualize and explore dataset","7a6433d8":"it may not look like an ideal distribution but it is better than what we had and it is our job **help our model as much as possible**.","889b4f65":"- From this we can observe that, 4th cluster seems to be forming the elbow of the curve. \n- However, the values does not reduce linearly until 8th cluster.\n\n- Based on Silhoutte Score and Elbow, we can consider clusters to be 8\n\n\n\n\n\n\n","b4bafa51":"Looking for some correlation now.\n","a677682d":" The problem described in this dataset requires us to extract segments of customers depending on their behaviour patterns provided in the dataset, to focus marketing strategy of the company on a particular segment.\n","a6b1f2d1":"Let's visualize how skewed our dataset is.","bc6b66cc":"\n**Apply PCA and visualize the results of the new encoded reduced data**","01127199":"- 'PURCHASES' have high correlation between one-off purchases, 'installment purchases, purchase transactions, credit limit and payments. \n-  Strong Positive Correlation between 'PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY'","b9c0f5e9":"Here we can see that by using autoencoders I was able to make clusters of data with very less overlapping. This is more meaningful clustering\/segmentations of the customers. I will now be able to tell my clients that they have 4 different types of customers and each can be targeted in a different way. Autoencoding really helped in this case.","0759646a":"**Data Processing**","0dd289bc":"#8. Apply autoencoders"}}