{"cell_type":{"9ee59256":"code","38dddd2f":"code","1077e9ae":"code","4855b9a0":"code","12a6b97d":"code","b15528a6":"code","0523e360":"code","dffa9dc8":"code","fa51d3f6":"code","e182a809":"code","588eb726":"code","1dcdc828":"code","82bb9e4c":"code","9082de81":"code","55a36789":"code","3a7f13f4":"code","d39331de":"code","0fb81984":"code","96ff9919":"code","215d1c7a":"code","c3b7fd63":"code","956479e6":"code","c96162af":"code","b7d750a0":"markdown","71bc21cc":"markdown","f2e87a49":"markdown","b88857d7":"markdown","cfbeb85e":"markdown","9650b0ae":"markdown","96836335":"markdown","c5687e89":"markdown","64a21ec1":"markdown","0fb4f5a1":"markdown","f1d1fc5a":"markdown","641cee32":"markdown","2045dc7c":"markdown","6989d30a":"markdown","597e15e2":"markdown"},"source":{"9ee59256":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38dddd2f":"reddit_data = pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Reddit_Data.csv')\ntwitter_data = pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv')","1077e9ae":"def cleanText(string):\n    punc = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    cleanedText = ' '.join(''.join([i for i in string if not i.isdigit()]).split())\n    \n    for i in punc:\n        cleanedText = cleanedText.replace(i, '')\n        \n    a = [i for i in cleanedText if i.isalpha() or i == ' ']\n            \n    final_text = ' '.join(''.join(a).split())\n    return final_text","4855b9a0":"reddit_data = reddit_data.dropna(how='any')\ntwitter_data = twitter_data.dropna(how='any')","12a6b97d":"reddit_data.rename(columns={'clean_comment':'comment'}, inplace=True)\ntwitter_data.rename(columns={'clean_text':'comment'}, inplace=True)\n\nreddit_data.category = reddit_data.category.apply(lambda x: int(x))\ntwitter_data.category = twitter_data.category.apply(lambda x: int(x))\n\nreddit_data.comment = reddit_data.comment.apply(cleanText)\ntwitter_data.comment = twitter_data.comment.apply(cleanText)","b15528a6":"reddit_data.drop_duplicates(subset='comment', keep=False, inplace=True)\ntwitter_data.drop_duplicates(subset='comment', keep=False, inplace=True)","0523e360":"# Concat reddit data and twiter data\nconcat_df = pd.concat([reddit_data, twitter_data], ignore_index=True)\nconcat_df.drop_duplicates(subset='comment', keep=False, inplace=True)","dffa9dc8":"counts = [len(reddit_data), len(twitter_data), len(concat_df)]\nlabels = ['Reddit', 'Twitter', 'Total']\ncolors = ['#ff4500', '#1DA1F2', '#50C878']","fa51d3f6":"plt.figure(figsize=(8,5))\n\nplt.bar(labels, counts, color=colors, edgecolor='black')\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.grid(axis='y', alpha=0.5)\nplt.tight_layout()","e182a809":"negative_count = [len(reddit_data[reddit_data.category == -1]),\n                 len(twitter_data[twitter_data.category == -1]),\n                 len(concat_df[concat_df.category == -1])]\n\nneutral_count = [len(reddit_data[reddit_data.category == 0]),\n                 len(twitter_data[twitter_data.category == 0]),\n                 len(concat_df[concat_df.category == 0])]\n\npositive_count = [len(reddit_data[reddit_data.category == 1]),\n                 len(twitter_data[twitter_data.category == 1]),\n                 len(concat_df[concat_df.category == 1])]\n\nx = ['Reddit', 'Twitter', 'Total']\nx_indexes = np.arange(len(x))\nwidth = 0.25","588eb726":"plt.figure(figsize=(14,6))\n\nplt.bar(x_indexes - width, negative_count, width=width, label='Negative', edgecolor='white', color='#ff4500')\nplt.bar(x_indexes, neutral_count, width=width, label='Neutral', edgecolor='white', color='#50C878')\nplt.bar(x_indexes + width, positive_count, width=width, label='Positive', edgecolor='white', color='#1DA1F2')\n\nplt.yticks(fontsize=14)\nplt.xticks(ticks=x_indexes, labels=x, fontsize=14)\n\nplt.legend(prop={'size':15})\nplt.grid(axis='y', alpha=0.65)\nplt.tight_layout()","1dcdc828":"negative_avg = int(sum([len(i) for i in concat_df[concat_df.category == -1].comment]) \/ len(concat_df[concat_df.category == 1]))\nneutral_avg = int(sum([len(i) for i in concat_df[concat_df.category == 0].comment]) \/ len(concat_df[concat_df.category == 0]))\npositive_avg = int(sum([len(i) for i in concat_df[concat_df.category == 1].comment]) \/ len(concat_df[concat_df.category == 1]))","82bb9e4c":"plt.figure(figsize=(8,5))\n\nplt.bar(['Negative', 'Neutral', 'Positive'], [negative_avg, neutral_avg, positive_avg], color=colors)\n\nplt.title('Comment Length Average', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.grid(axis='y', alpha=0.5)\nplt.tight_layout()","9082de81":"negative_words = ''\nneutral_words = ''\npositive_words = ''\n\nstopwords = set(STOPWORDS)\n\nfor comment, category in zip(concat_df.comment, concat_df.category):\n    tokens = comment.split()\n    \n    for word in tokens:\n        if category == -1:\n            negative_words += word + ' '\n        elif category == 0:\n            neutral_words += word + ' '\n        else:\n            positive_words += word + ' '","55a36789":"negative_cloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(negative_words)\n\nneutral_cloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(neutral_words)\n\npositive_cloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(positive_words)","3a7f13f4":"fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(18, 5), facecolor=None)\n\nax1.imshow(negative_cloud)\nax1.set_title('Negative', fontsize=18, color='#ff4500')\n\n\nax2.imshow(neutral_cloud)\nax2.set_title('Neutral', fontsize=18, color='#50C878')\n\nax3.imshow(positive_cloud)\nax3.set_title('Positive', fontsize=18, color='#1DA1F2')\n\nplt.tight_layout()","d39331de":"class Data:\n    def __init__(self, df):\n        self.df = df\n        \n    def fixData(self):\n        dataframe = self.df\n        lowest_len = min([i for i in dataframe.category.value_counts()])\n        \n        # Create the final data frame\n        final_df = pd.concat([dataframe[dataframe.category == -1][:lowest_len],\n                             dataframe[dataframe.category == 0][:lowest_len],\n                             dataframe[dataframe.category == 1][:lowest_len]])\n        \n        # To shuffle the rows in the data frame\n        final_df = final_df.sample(frac=1).reset_index(drop=True)\n        return final_df","0fb81984":"a = Data(concat_df)\nfixed_df = a.fixData()\nprint(f'Before: \\n{concat_df.category.value_counts()}\\n')\nprint(f'After: \\n{fixed_df.category.value_counts()}')","96ff9919":"train_X, test_X, train_y, test_y = train_test_split([i for i in fixed_df.comment], [i for i in fixed_df.category], test_size=0.25, random_state=1)","215d1c7a":"vectorizer = TfidfVectorizer()\ntrain_X_vectors = vectorizer.fit_transform(train_X)\ntest_X_vectors = vectorizer.transform(test_X)","c3b7fd63":"clf_svm = LinearSVC()\nclf_svm.fit(train_X_vectors, train_y)\nclf_prediction = clf_svm.predict(test_X_vectors)\n\nprint(f'Accuracy: {clf_svm.score(test_X_vectors, test_y)}')\nprint(f'Accuracy: {f1_score(test_y, clf_prediction, average=None, labels=[-1, 0, 1])}')","956479e6":"svm_mnb = MultinomialNB()\nsvm_mnb.fit(train_X_vectors, train_y)\nmnb_prediction = svm_mnb.predict(test_X_vectors)\n\nprint(f'Accuracy: {svm_mnb.score(test_X_vectors, test_y)}')\nprint(f'Accuracy: {f1_score(test_y, mnb_prediction, average=None, labels=[-1, 0, 1])}')","c96162af":"svm_dtc = DecisionTreeClassifier()\nsvm_dtc.fit(train_X_vectors, train_y)\ndtc_prediction = svm_dtc.predict(test_X_vectors)\n\nprint(f'Accuracy: {svm_dtc.score(test_X_vectors, test_y)}')\nprint(f'Accuracy: {f1_score(test_y, dtc_prediction, average=None, labels=[-1, 0, 1])}')","b7d750a0":"***Count The Negative, Neutral, and Positive Comments***","71bc21cc":"***Vectorize the Comment***","f2e87a49":"***Balance the Data (The Amount of Negative, Neutral, and Positive Data Must be Equivalent)***","b88857d7":"### **Split Test & Train Data**","cfbeb85e":"***Decision Tree Classifier***","9650b0ae":"***Create Word Clouds to See Which Words Appear Frequently***","96836335":"***Linear SVC***","c5687e89":"### **Data Extraction & Cleaning**","64a21ec1":"***Compare Total Comments From both Platforms***","0fb4f5a1":"### **Creating Models**","f1d1fc5a":"***MultinomialNB***","641cee32":"***Count the Average Length of All the 3 Sentiments Comment***","2045dc7c":"### **Data Visualization**","6989d30a":"***Cleaning Data***","597e15e2":"***Split Train and Test Data***"}}