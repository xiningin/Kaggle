{"cell_type":{"96ab079f":"code","6067cb30":"code","adca7552":"code","ae48dd47":"code","ed9b4b1d":"code","37202aa0":"code","7ae6d935":"code","cc36a889":"code","ccab5dd9":"code","bf4ececb":"code","66ca328a":"code","41d5263e":"code","ecebc556":"code","07a053c8":"code","8060d77a":"code","f3ab6fe3":"code","600a4c6a":"code","38be8efe":"code","6039d054":"markdown","952f084e":"markdown","566c5d71":"markdown","ac36aec2":"markdown","478c48ca":"markdown","3576bd48":"markdown","5ed52724":"markdown","c93903c4":"markdown","95a625e8":"markdown","7c8296d9":"markdown","362c1fd2":"markdown","160119ed":"markdown","04b63d01":"markdown","6d66416a":"markdown","0dae7985":"markdown","38bdf3a0":"markdown","dd8dc6ec":"markdown","26b3fdca":"markdown","ec0509c5":"markdown","87bc43d1":"markdown","3350b7bd":"markdown","85e0306b":"markdown","e54252a5":"markdown","11aa863f":"markdown"},"source":{"96ab079f":"# Data processing\nimport numpy as np # data arrays\nimport pandas as pd # data structure and data analysis\nimport functools\nimport datetime as dt # date time\n\n\nfrom mlxtend.frequent_patterns import apriori # Data pattern exploration\nfrom mlxtend.frequent_patterns import association_rules # Association rules conversion","6067cb30":"# Read in data in CSV format\ndf1 = pd.read_csv('..\/input\/bbc-customer-analysis-personal-project\/BBC_analysis.csv')\ndf1.head() ","adca7552":"df1.info() # print to check the dtype","ae48dd47":"# Read in data in CSV format\ndf2 = pd.read_csv('..\/input\/bbc-customer-analysis-personal-project\/User_data.csv')\ndf2.head()","ed9b4b1d":"df2.info()","37202aa0":"# Rename the column to match with above table\ndf2.rename(columns = {'User_Id':'User_id'}, inplace=True)\ndf2.head()","7ae6d935":"#Merge df1 and df2 into one datafram dfs for further analysis\ndfs = [df1, df2]\nUserTable = functools.reduce(lambda left,right: pd.merge(left,right,on='User_id', how='outer'), dfs)\nUserTable.dropna(inplace=True)\nUserTable.head() # print to confirm ","cc36a889":"#convert all service date to datatime64 dtype\nUserTable['FirstServiceDate'] = pd.to_datetime(UserTable['FirstServiceDate'])\nUserTable['SecondServiceDate'] = pd.to_datetime(UserTable['SecondServiceDate'])\nUserTable['LastServiceDate'] = pd.to_datetime(UserTable['LastServiceDate'])\nUserTable['Date'] = pd.to_datetime(UserTable['Date'])\n\n# re-print information to confirm\nUserTable.info()\n\n# print 1st 5 rows to confirm\nUserTable.head()","ccab5dd9":"#Calculate the User_age\nUserTable['User_age'] = 1 + (UserTable['LastServiceDate']-UserTable['FirstServiceDate']).dt.days\n\n# Split users into 5 categories of 30_age, 100_age, 200_age, 300_age, 300_plus_age\nbins = [-1, 30, 100, 200, 300, np.inf]\nnames = ['30_age', '100_age', '200_age', '300_age', '300_plus_age']\nUserTable['Age_bin'] = pd.cut(UserTable['User_age'], bins, labels = names)\n\n#Print the first 10 rows to confirm\nUserTable.head(10)\n","bf4ececb":"UserAgg = UserTable.groupby(['Age_bin','Serviceid'])[['User_id']].count().dropna()\nUserAgg\n\n#We group by the first level of the index:\ng = UserAgg['User_id'].groupby(level=0, group_keys=False)\n\n# Sort 'order' each group and take the first five elements:\ng.nlargest(5)\n","66ca328a":"# Present the UserTable again to find pattern for cross-sell analysis\nUserTable.head()\n","41d5263e":"#create the df cross_sell for further analysis  \nCross_sell = pd.DataFrame(UserTable, columns =['User_id', 'Date', 'Serviceid']) \n\n#Find the association between unique serviceids per day and each User_id\nCross_sell['User_id & Date'] = Cross_sell['User_id'].astype('str') + ' ' + Cross_sell['Date'].astype('str')\nCross_sell.drop(['User_id', 'Date'], axis=1, inplace=True) \nCross_sell.head(10) #print the first ten rows to check ","ecebc556":"Cross_sell.drop_duplicates(inplace = True)\nCross_sell.set_index('User_id & Date', inplace=True)\nCross_sell.head(10) #print the first ten rows to check ","07a053c8":"Cross_sell['Serviceid'] = Cross_sell['Serviceid'].astype('str')\n\nbasket = pd.get_dummies(Cross_sell)\nbasket.head()","8060d77a":"basket_sets = pd.pivot_table(basket, index='User_id & Date', aggfunc='sum')\nbasket_sets","f3ab6fe3":"frequent_itemsets = apriori(basket_sets, min_support = 0.03, use_colnames = True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","600a4c6a":"frequent_itemsets[ (frequent_itemsets['length'] > 1) &\n                   (frequent_itemsets['support'] >= 0.02)]","38be8efe":"rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('support', ascending=False).head(10)","6039d054":"This step is to **group** and **count** customers bases on categories of BBC Age_bin and servicedid in order to know which services were used most by each group, then sort to see the first 5 highest values describing number of BBC customers per serviceid in each Age_group.","952f084e":"**Create** the `frequent_itemsets` through apriori and add a new column that stores the length of each itemset. Frequent itemsets are the ones which occur at least a minimum number of times in the transactions. Technically, these are the itemsets for which support value (fraction of transactions containing the itemset) is above a minimum threshold.\n\nTake this as an example, the number of transactions containing items {Bread, Egg} is greater than or equal to number of transactions containing {Bread, Egg, Vegetables}. If the latter occurs in 30 transactions, former is occurring in all 30 of them and possibly will occur in even some more transactions. So if support value of {Bread, Egg, Vegetables} i.e. (30\/100) = 0.3 is above minsup, then we can be assured that support value of {Bread, Egg} i.e. (>30\/100) = >0.3 is above minsup too. \n\nGet support [here](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/frequent_patterns\/apriori\/)","566c5d71":"**Calculate the User_age**\n\nTo know if different BBC customer ages convert into differents serviceid cluster, we will count the `User_age` first then arrange them into the coresponding `Age_group`. Let's see the results as follows","ac36aec2":"Now that we have all the data in a single result table let's download it as a `CSV` file.\n\nNext step, using `Python` to prepare and process the data.","478c48ca":"Use `drop_duplicates` function to eliminate duplicate rows. For example, a customer use 1 particular serviceid twice a day, all we need is to know the number of unique serviceid per day for each customer.\n\nAnother example, if a customer usually buy razer along with blades, we can cross-sell 2 items in a combo.","3576bd48":"**Create** the df cross_sell for further analysis & **Find** the association between unique serviceids per day and each User_id.","5ed52724":"## \ud83d\udea9 Analysis question 2:\nWe would like to know which serviceid we can cross-sales to users?\n\nFirst, what is cross-selling? \nCross-selling is the practice of selling related or additional products to existing customers. To know which `serviceid`s we can cross-sell for customer, we need to find which items (serviceid) that customers usually buy together then combine these items into combo.\n\nThe analysis below is based on the [Assocations Rules](https:\/\/towardsdatascience.com\/association-rules-2-aa9a77241654) \n and [Apriori principle](https:\/\/towardsdatascience.com\/complete-guide-to-association-rules-2-2-c92072b56c84).\n \nThe Rules do not extract an individual\u2019s preference, rather find **relationships between set of elements of every distinct transaction**. ","c93903c4":"**From the summary, we can see that**\n\n- There are a total of 1464 rows and 3 columns in the data set.\n- Data types are assigned correctly (apart from Date). We will attempt at converting the datatype to a form suitable for analysis in the next section.\n- The are no null values.","95a625e8":"**Selecting** the important parameters for analysis","7c8296d9":"## \ud83d\udccc Observations:\n- If the customer buy serviceid_667, we will cross-sell the serviceid_333 with the Support of 19%, Confidence ~ 92% and Lift >1.\n- If the customer buy serviceid_333, we will cross-sell the serviceid_667 with the Support of 19%, Confidence ~ 99% and Lift >1.\n- If the customer buy serviceid_981, we will cross-sell the serviceid_1014 with the Support of 17%, Confidence ~ 62% and Lift >1.\n- If the customer buy serviceid_1014, we will cross-sell the serviceid_268 with the Support of 17%, Confidence ~ 79% and Lift >1.\n- If the customer buy serviceid_982, we will cross-sell the serviceid_268 with the Support of 11%, Confidence ~ 98% and Lift >1.\n\n...\n\n- If the customer buy serviceid_326, we will cross-sell the serviceid_666 with the Support of 10%, Confidence of 100% and Lift >1.\n","362c1fd2":"Then, we can customize the results that satisfy our desired criteria as follows. In this case we put the `length` > 1 (meaning that there are more than one item in the item_sets and the `support` >=0.02. ","160119ed":"**Convert** categorical serviceid to numeric through get_dummies to see the customers per day buy which `serviceid`s.","04b63d01":"# REFERENCES\n- [Pandas Groupby](https:\/\/stackoverflow.com\/questions\/27842613\/pandas-groupby-sort-within-groups)\n- [Association Rules](https:\/\/towardsdatascience.com\/association-rules-2-aa9a77241654)\n- [Apriori Algorithm](https:\/\/towardsdatascience.com\/complete-guide-to-association-rules-2-2-c92072b56c84)\n- [Cross-sell](https:\/\/www.youtube.com\/watch?v=VMavY0pBo2o&ab_channel=AngossSoftware)\n- [Frequent Itemsets via Apriori Algorithm](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/frequent_patterns\/apriori\/)\n","6d66416a":"## \ud83d\udea9 Analysis question 1\n\nWe would like to know whether customers in different BBC ages have different serviceid cluster?\n\nFirst, we attempt to merge the two tables to see a big picture including the serviceid and the day of service transaction of each customer. In addition, this will help categorize customer into Ages groups and count service transactions for each group. ","0dae7985":"# PROCESS\nDoing SQL query to find the answer for the first goal, we use the combination of three `CTE`s to find the answer for `The first 2 serviceid`, `last serviceid` csutomers used and `Date` \n\nCombine with `COUNT` statement to find the total distinct serviceid that customers used. Then join everything to show the results as the required table above.\n\nNatural join is being used to optimize the time efficiency. No JOIN command, but it is implicit (INNER JOIN).","38bdf3a0":"### **Observation**\n\nFrom the above analysis, we can see that:\n- Customers in 30_age group tend to use Serviceid of 487, 1014, 299, 47, 65.\n- Customers in 100_age group tend to use Serviceid of 333, 667, 981, 1014, 487.\n- Customers in 200_age group tend to use Serviceid of 981, 20, 1014, 271, 326.\n- Customers in 300_age group tend to use Serviceid of 18, 667, 333, 981, 268.\n- Customers in 300_plus_age group tend to use Serviceid of 981, 19, 1014, 2, 271.\n","dd8dc6ec":"**Converting** data into dtype datatime64 to define the correct date for next section, then print the info to double check.","26b3fdca":"```sql\n-- create a ranked_cte to sort data with ROW_NUMBER\nWITH ranked_cte AS( \nSELECT User_Id, date, serviceid,\nROW_NUMBER() OVER(PARTITION BY User_Id\nORDER BY Date) RN,\nROW_NUMBER() OVER(PARTITION BY User_Id\nORDER BY Date DESC) NR\nFROM test\n),\n-- create the first_cte to define the firstservice value by choosing the RN = 1\nfirst_cte AS(\nSELECT t.User_Id, r.serviceid AS FirstServiceid, r.date AS FirstServiceDate\nFROM ranked_cte AS r, test AS t\nWHERE RN = 1 \nAND r.User_Id = t.User_Id\nGROUP BY t.User_Id\n),\n-- create second_cte to define the secondservice value by choosing RN = 2\nsecond_cte AS(\nSELECT f.*, r.serviceid as SecondServiceid, r.date AS SecondServiceDate\nFROM first_cte as f, \n\t ranked_cte as r\nWHERE RN = 2\nAND r.User_Id = f.User_Id\nGROUP BY r.User_Id\n) \n-- Join everything, use the DATE_FORMAT statement to convert the date into your desired format\nSELECT t.User_Id AS User_id, s.FirstServiceid, \n    s.SecondServiceid, DATE_FORMAT(s.FirstServiceDate, \"%e\" \"-\" \"%b\" \"-\" \"%y\") AS FirstServiceDate,\n    DATE_FORMAT(s.SecondServiceDate, \"%e\" \"-\" \"%b\" \"-\" \"%y\") AS SecondServiceDate,\n    r.serviceid AS LastServiceid, DATE_FORMAT(r.date, \"%e\" \"-\" \"%b\" \"-\" \"%y\") AS LastServiceDate,\n    COUNT(DISTINCT t.serviceid) AS TotalService -- find the total distinct serviceid each customer used\nFROM second_cte AS s,\n     ranked_cte AS r,\n     test AS t\nWHERE NR = 1\nAND s.User_Id = r.User_Id\nAND t.User_Id = r.User_Id\nGROUP BY r.User_Id;\n;\n```","ec0509c5":"# Data Exploration\nFile \"BBC_analysis.csv\" has been extracted by querying from dataset \"User_data.csv\" contains info for further analysis. Below, you can see a sample of this info. ","87bc43d1":"**Convert** the service baskets into pivot table type to see the total.","3350b7bd":"**From the summary, we can see that**\n\n- There are a total of 43 rows and 7 columns in the data set.\n- Data types are assigned correctly (apart from Servicedate). We will attempt at converting the datatype to a form suitable for analysis in the next section.\n- The are no null values.","85e0306b":"# \ud83d\udc69\u200d\ud83d\udcbb BBC CUSTOMER ANALYSIS PROJECT\n---\n\n# INTRODUCTION\nThe project aims to know if BBC customers in different BBC ages have different serviceid cluster, and how we do cross-sale to customers. \n\n## Objectives\n1. The first goal of this project is to use SQL to define: \n    - The first 2 serviceid customers used and Date that customer used those services.\n    - The last serviceid users used and Date that users used that service.\n    - The total distinct serviceid that customers used.\n    - Present the findings into one table with the columns as follows:\n \n    \n| User_id  | FirstServiceid | SecondServiceid | FirstServiceDate | SecondServiceDate | LastServiceid | LastServiceDate | TotalService |\n| -------- | -------------- | --------------- | ---------------- | ----------------- | ------------- | --------------- | ------------ |\n| 407901   | 12  |  45 | 1-Jan-18 | 2-Jan-18 | 667 | 22-July-2018| 6 |\n    \n   \n    \n2. The second goal is identifying patterns using Python to answer the business questions below.\n\n## Business question\n1. Determine whether customers in different BBC ages have different serviceid cluster?\n2. Determine which serviceid we can cross-sell to customers? Give recommendation to your findings.\n\n## Database description\nWe have one table `test` with three columns `User_id`, `Date`, and `Serviceid`.\n\n- **User_id**: The identity of the individual customer.\n- **Servicedid**: The identity of the transaction.\n- **Date**: the date that transaction is performed.\n\n_Note: This is the open data._","e54252a5":"### Generating the association rule: \n- Association Rule: Ex. {X \u2192 Y} is a representation of finding Y on the basket which has X on it\n- Itemset: Ex. {X,Y} is a representation of the list of all items which form the association rule\n- Support: Fraction of transactions containing the itemset\n- Confidence:  Measures how often items in Y appear in transactions that contain X.\n- Lift: Ratio of confidence to baseline probability of occurrence of {Y}","11aa863f":"# Importing Libraries\n"}}