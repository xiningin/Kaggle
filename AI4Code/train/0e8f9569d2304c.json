{"cell_type":{"fd9c14cd":"code","87d7c69e":"code","92f8fec9":"code","3e40b250":"code","05fd2447":"code","84602834":"code","7010960b":"code","a7a2e93d":"code","e884b883":"code","2465135d":"code","51a8b6fa":"code","d516ce47":"code","be2f6e70":"code","84c19a71":"code","f25642b3":"code","11fb0dee":"code","55c8dd15":"code","85609a70":"code","44fb33f1":"code","4e49bd8e":"code","d6e83e5f":"code","47f21b1a":"markdown","136bd430":"markdown","145a1696":"markdown","75879fba":"markdown","3f8fd2ef":"markdown","10c01e60":"markdown","8d1babc7":"markdown","f704a112":"markdown","6392f2d9":"markdown","c6e65cc1":"markdown","449e3a54":"markdown","66b73145":"markdown","e808b827":"markdown","c2f5a472":"markdown","0552a18d":"markdown","8875fa8f":"markdown","4186113b":"markdown","f2dbd909":"markdown","e4c87d21":"markdown","031f86eb":"markdown","06c24d89":"markdown","6e3b527b":"markdown","cae47492":"markdown"},"source":{"fd9c14cd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nimport cv2\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.utils import class_weight, shuffle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer,accuracy_score\nfrom keras.utils import to_categorical\nfrom keras import applications, optimizers\n\nfrom keras.models import Sequential, Model, load_model,clone_model\nfrom keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D \nfrom keras.losses import categorical_crossentropy\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom keras import applications\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout","87d7c69e":"foldernames = os.listdir(\"\/kaggle\/input\/animals10\/raw-img\")\ncategories = []      # list of all categories of animals present\nfiles = []           # list of all images from all categories\n\nfor k, folder in enumerate(foldernames):\n    filenames = os.listdir(\"..\/input\/animals10\/raw-img\/\" + folder);\n    for file in filenames:\n        files.append(\"..\/input\/animals10\/raw-img\/\" + folder + \"\/\" + file)\n        categories.append(k)\n# creating a dataframe of images and their respective categories        \ndf = pd.DataFrame({\n    'filename': files,\n    'category': categories\n})\n\n#initializing empty train df\ntrain_df = pd.DataFrame(columns=['filename', 'category'])\n\n#collecting 500 samples from each of 10 categories\ni = 0\nfor i in range(10):\n    train_df = train_df.append(df[df.category == i].iloc[:500,:])\n\nprint(train_df.head())\ntrain_df = train_df.reset_index(drop=True) # reset index of a Data Frame\ntrain_df","92f8fec9":"y = train_df['category']\nx = train_df['filename']\n\nx, y = shuffle(x, y, random_state=8)","3e40b250":"def centering_image(img):\n    size = [256,256]\n    \n#shape:(h\u00d7w\u00d7t)taking the first2 elements(h,w) and unpacking them appropriately \n    img_size = img.shape[:2]\n    \n    # extracting the excess space for centering.\n    row = (size[1] - img_size[0]) \/\/ 2\n    col = (size[0] - img_size[1]) \/\/ 2\n    \n#creating centered image by taking a 0-matrix and then re-assigning intensities\n    resized = np.zeros(list(size) + [img.shape[2]], dtype=np.uint8)\n    resized[row:(row + img.shape[0]), col:(col + img.shape[1])] = img \n\n    return resized\n","05fd2447":"images = []\n\nwith tqdm(total=len(train_df)) as k:  \n    for i, file_path in enumerate(train_df.filename.values):\n        \n        #color order is changed\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n\n        #converting images to square\n        if(img.shape[0] > img.shape[1]):\n            tile_size = (int(img.shape[1]*256\/img.shape[0]),256)\n        else:\n            tile_size = (256, int(img.shape[0]*256\/img.shape[1]))\n\n        #centering the images\n        img = centering_image(cv2.resize(img, dsize=tile_size))\n\n        #output 224*224px \n        img = img[16:240, 16:240]\n        images.append(img)\n        k.update(1)\n\nimages = np.array(images)","84602834":"cols=5\nfig, ax = plt.subplots(2,5, figsize=(20,20))\nfor i in range(10):\n    path = train_df[train_df.category == i].values[1] #takes path in os of 1 value of figure of each category\n#     print(path)\n#     image = cv2.imread(path[0])  #     [i\/\/cols, i%cols]\n    ax[i\/\/cols, i%cols].set_title(path[0].split('\/')[-2] +' '+' Cat:'+ str(path[1])) # -2 is location of name of figure in path\n    ax[i\/\/cols, i%cols].imshow(images[train_df[train_df.filename == path[0]].index[0]])","7010960b":"data_num = len(y)\nrandom_index = np.random.permutation(data_num)\n\nx_shuffle = []\ny_shuffle = []\nfor i in range(data_num):\n    x_shuffle.append(images[random_index[i]])\n    y_shuffle.append(y[random_index[i]])\n    \nx = np.array(x_shuffle) \ny = np.array(y_shuffle)","a7a2e93d":"#train_test split\nval_size = int(round(0.8*len(y)))\nx_train = x[:val_size]\ny_train = y[:val_size]\nx_val = x[val_size:]\ny_val = y[val_size:]\n\nprint('x_train Shape:', x_train.shape, '\\t x_test Shape:',x_val.shape)\nprint('y_train Shape: ', y_train.shape, '\\t y_test Shape:',y_val.shape)\n\n\ny_train = to_categorical(y_train) \ny_val = to_categorical(y_val)\n\n#dividing each intensity in array by 225 ( 225 is max intensity in RGB)\nx_train = x_train.astype('float32')\nx_val = x_val.astype('float32')\nx_train \/= 255 \nx_val \/= 255\n\nprint('x_train Shape:', x_train.shape, '\\t x_test Shape:',x_val.shape)\nprint('y_train Shape: ', y_train.shape, '\\t y_test Shape:',y_val.shape)\nprint('image Shape:', x_train[0].shape)","e884b883":"\nanimal_names = [] \nfor i in range(10):\n    path = train_df[train_df.category == i].values[1]\n    if path[0].split('\/')[-2] == 'scoiattolo':\n        animal_names.append('squirrel')\n    elif path[0].split('\/')[-2] == 'cavallo':\n        animal_names.append('horse')\n    elif path[0].split('\/')[-2] == 'farfalla':\n        animal_names.append('butterfly')\n    elif path[0].split('\/')[-2] == 'mucca':\n        animal_names.append('cow')\n    elif path[0].split('\/')[-2] == 'gatto':\n        animal_names.append('cat')\n    elif path[0].split('\/')[-2] == 'pecora':\n        animal_names.append('sheep')\n    elif path[0].split('\/')[-2] == 'gallina':\n        animal_names.append('chicken')\n    elif path[0].split('\/')[-2] == 'elefante':\n        animal_names.append('elephant')\n    elif path[0].split('\/')[-2] == 'ragno':\n        animal_names.append('spider')\n    elif path[0].split('\/')[-2] == 'cane':\n        animal_names.append('dog')","2465135d":"# ImageDataGenerator for data augmentation \n\ntrain_data_gen = ImageDataGenerator(rotation_range=45, \n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1, \n                                    horizontal_flip=True)\ntrain_data_gen.fit(x_train) ","51a8b6fa":"test_images = []\ntest_df = pd.DataFrame(columns=['filename'])\nactual_pred=pd.DataFrame(columns=['Category'])\nfor i in range(10):\n    test_df = test_df.append(df[df.category == i].iloc[500:502,:1])\n    actual_pred=actual_pred.append(df[df.category == i].iloc[500:502,1:])\n\n    test_df = test_df.reset_index(drop=True)\n    actual_pred = actual_pred.reset_index(drop=True)\nwith tqdm(total=len(test_df)) as k:  \n    for i, file_path in enumerate(test_df.filename.values):\n        \n        #color order is changed\n        img = cv2.imread(file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n\n        #converting images to square\n        if(img.shape[0] > img.shape[1]):\n            tile_size = (int(img.shape[1]*256\/img.shape[0]),256)\n        else:\n            tile_size = (256, int(img.shape[0]*256\/img.shape[1]))\n\n        #centering the images\n        img = centering_image(cv2.resize(img, dsize=tile_size))\n\n        #output 224*224px \n        img = img[16:240, 16:240]\n        test_images.append(img)\n        k.update(1)\ntest_images = np.array(test_images).reshape(-1,224,224,3)\n\n","d516ce47":"# print(test_df.shape())","be2f6e70":"rows,cols,channel = 224, 224, 3\n\nepochs = 25\nlearning_rate = 0.001\ndecay_rate = learning_rate \/ epochs\nmomentum =0.9\nsgd= optimizers.SGD(lr=learning_rate, momentum=momentum,\n                      decay=decay_rate, nesterov=False)\n\nbase_model1 = applications.VGG16(weights='imagenet', include_top=False,\n                                    input_shape=(224,224,3))\nmodel1 = Sequential()\nmodel1.add(Flatten(input_shape= base_model1.output_shape[1:]))\nmodel1.add(Dense(256, activation='relu'))\nmodel1.add(Dense(10, activation='softmax'))\n\nvgg16_model = Model(inputs=base_model1.input,\n                        outputs=model1(base_model1.output))\n\nvgg16_model.compile(loss='binary_crossentropy',\n                        optimizer=sgd,metrics=['accuracy'])\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\nvgg16_model.summary()\n\n","84c19a71":"epochs = 25\nlearning_rate = 0.001\ndecay_rate = learning_rate \/ epochs\nmomentum =0.8\nsgd= optimizers.SGD(lr=learning_rate, momentum=momentum,\n                      decay=decay_rate, nesterov=False)\n\nbase_model2 =ResNet50(weights=None, include_top=False, input_shape=(224,224,3))\nmodel2 = Sequential()\nmodel2.add(Flatten(input_shape= base_model2.output_shape[1:]))\nmodel2.add(Dense(256, activation='relu'))\nmodel2.add(Dense(10, activation='softmax'))\n\nResNet50_model = Model(inputs=base_model2.input,\n                        outputs=model2(base_model2.output))\nResNet50_model.compile(loss='binary_crossentropy',\n                        optimizer= sgd,\n                          metrics=['accuracy'])\nResNet50_model.summary()","f25642b3":"class lr_finder():\n    \n    def __init__(self,model,begin_lr=1e-8, end_lr=1e-1, num_epochs=10, period=5):\n        self.period = period\n        # make a copy of the model to train through a sweep of learning rates\n        self.model = clone_model(model)\n        self.begin_lr = np.log(begin_lr)\/np.log(10)\n        self.end_lr = np.log(end_lr)\/np.log(10)\n        self.num_epochs = num_epochs\n        self.lower_bound = begin_lr\n        self.upper_bound = 1e-2 #end_lr\n        # define learning rates to use in schedules\n        self.lr = np.logspace(self.begin_lr,self.end_lr,self.num_epochs)\n        self.clr = np.logspace(np.log(self.lower_bound)\/np.log(10), np.log(self.upper_bound)\/np.log(10), self.period)\n        \n        \n    def reset_model(self, model):\n        # reset the model to find new lr bounds \n        self.begin_lr = -10 \n        self.end_lr = 0 \n        self.lr = np.logspace(self.begin_lr,self.end_lr,self.num_epochs)\n        self.model = clone_model(model)\n        \n    def lr_schedule(self,epoch):\n        # return lr according to a sweeping schedule\n        if epoch < self.num_epochs:\n            return self.lr[epoch]\n        else:\n            return self.lr[0]\n        \n    def clr_schedule(self,epoch,period=5):\n        # return lr according to cyclical learning rate schedule\n        my_epoch = int(epoch % self.period)\n        return self.clr[my_epoch]\n    \n    def lr_vector(self,epochs):\n        # return the vector of learning rates used in a schedule\n        lrv = []\n        for ck in range(epochs):\n            lrv.append(self.lr_schedule(ck))\n        return lrv\n    \n    def lr_plot(self,history_loss,please_plot=True):\n        # plot the lr sweep results and set upper and lower bounds on learning rate\n        x_axis = self.lr_vector(self.num_epochs)\n        y_axis = history_loss\n                   \n        d_loss = []\n        for cc in range(1,len(y_axis)):\n            if cc == 1:\n                d_loss.append(y_axis[cc] - y_axis[cc-1])\n            else:\n                d_loss.append(0.8*(y_axis[cc] - y_axis[cc-1])+0.2*(y_axis[cc-1] - y_axis[cc-2]))\n        d_loss = np.array(d_loss)\n        \n        self.lower_bound = x_axis[d_loss.argmin()]\n        self.upper_bound = x_axis[np.array(y_axis).argmin()]\n        self.clr = np.logspace(np.log(self.lower_bound)\/np.log(10), np.log(self.upper_bound)\/np.log(10), self.period)\n        \n        print(\"recommended learning rate: more than %.2e, less than %.2e \"%(self.lower_bound, self.upper_bound))\n        if(please_plot):\n            plt.figure(figsize=(10,5))\n            plt.loglog(x_axis,y_axis)\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.title('Loss \/ learning rate progression')\n            plt.show()\n            \n    def get_lr(self,epoch):\n        # return the geometric mean of the upper and lower bound learning rates\n        return (self.lower_bound *self.upper_bound)**(1\/2)\n","11fb0dee":"# lrf = lr_finder(vgg16_model,begin_lr=1e-8, end_lr=1e0, num_epochs=20)\n# lr_rate = LearningRateScheduler(lrf.lr_schedule)\n# steps_per_epoch = int(len(y_train)\/32)\n# max_epochs = 20\n\n# lrf.model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(),\n#                   metrics=['accuracy'])\n\n\n# lr_history = lrf.model.fit_generator(train_data_gen.flow(x_train, y_train,\n#                                             batch_size=32),\n#                                 steps_per_epoch=steps_per_epoch\/20,\n#                                 validation_data=(x_val, y_val),\n#                                 validation_steps=50,\n#                                 epochs=max_epochs,\n#                                 callbacks=[lr_rate],\n#                                 verbose=0)\n# lrf.lr_plot(lr_history.history['loss'])","55c8dd15":"batch_size =32 #32 ,50#hyper parameters obtained by tuning\nepochs = 30  #\nneural_ntwk2= ResNet50_model.fit_generator(train_data_gen.flow(x_train, y_train,\n                                            batch_size=batch_size),\n                    steps_per_epoch= x_train.shape[0] \/\/ batch_size,\n                    validation_data=(x_val, y_val),epochs=epochs,\n                    callbacks=[ModelCheckpoint ('ResNet50transferlearning.model',\n                               monitor='val_acc')])","85609a70":"batch_size =32 \nepochs = 25  \nneural_ntwk1 = vgg16_model.fit_generator(train_data_gen.flow(x_train, y_train,\n                                        batch_size=batch_size),\n                    steps_per_epoch= x_train.shape[0] \/\/ batch_size,\n                    epochs=epochs,validation_data=(x_val, y_val),\n                    callbacks=[ModelCheckpoint('VGG16-transferlearning.model',\n                               monitor='val_acc')])\n\n\n                                    ","44fb33f1":"# print(\"VGG16: Epochs={0:d}, Train accuracy={1:.5f}, Val accuracy={2:.5f}\".format(epochs,neural_ntwk1.history['accuracy'][epochs-1],\n#               neural_ntwk1.history['val_accuracy'][epochs-1]))\nprint(\"ResNet50: Epochs={0:d}, Train accuracy={1:.5f},Val accuracy={2:.5f}\"\n      .format(epochs,neural_ntwk2.history['accuracy'][epochs-1],\n              neural_ntwk2.history['val_accuracy'][epochs-1]))\n\ndef show_plots(neural_ntwk):\n    loss_vals = neural_ntwk['loss']\n    val_loss_vals = neural_ntwk['val_loss']\n    epochs = range(1, len(neural_ntwk['accuracy'])+1)\n    \n    f, ax = plt.subplots(nrows=1,ncols=2,figsize=(16,4))\n    \n    ax[0].plot(epochs, loss_vals, color='R',marker='o',\n               linestyle=' ', label='Train Loss')\n    ax[0].plot(epochs, val_loss_vals, color='B',\n               marker='*', label='Val Loss')\n    ax[0].set(title='Train & Val Loss', xlabel='Epochs',ylabel='Loss')\n    ax[0].legend(loc='best')\n    ax[0].grid(True)\n    \n    # plot accuracies\n    acc_vals = neural_ntwk['accuracy']\n    val_acc_vals = neural_ntwk['val_accuracy']\n\n    ax[1].plot(epochs, acc_vals, color='navy', marker='o',\n               ls=' ', label='Train Accuracy')\n    ax[1].plot(epochs, val_acc_vals, color='firebrick',\n               marker='*', label='Val Accuracy')\n    ax[1].set(title='Train & Val Accuracy',xlabel='Epochs',ylabel='Accuracy')\n    ax[1].legend(loc='best')\n    ax[1].grid(True)\n    \n    plt.show()\n    plt.close()\n    \n    # delete locals from heap before exiting\n    del loss_vals, val_loss_vals, epochs, acc_vals, val_acc_vals\n# show_plots(neural_ntwk1.history)\nshow_plots(neural_ntwk2.history)\nshow_plots(neural_ntwk1.history)","4e49bd8e":"test_prediction1 = vgg16_model.predict(test_images)\ntest_prediction2 = ResNet50_model.predict(test_images)","d6e83e5f":"actual_pred['category']=actual_pred['category'].astype('int')\nfor i in range(20):\n    predict1= test_prediction1[i].argmax()\n    predict2= test_prediction2[i].argmax()\n    actual=actual_pred['category'][i]\n    plt.imshow(test_images[i])\n    plt.title(\"Actual: {1},  VGG: {0},  ResNet:{2}\".format((animal_names[actual]),(animal_names[predict1]),(animal_names[predict2])), fontsize=10)\n    plt.show()\n","47f21b1a":"<a id='step3.4'><\/a>\n3.4. Obtaining optimum Learning Rate for each model","136bd430":"<a id='step2.2'><\/a>\n2.2. Image Resizing and converting them to array","145a1696":"<a id='step3.2'><\/a>\n\n3.2. Creating a Convolutional Neural Network using ResNet50\n","75879fba":"<a id='step2.5'><\/a>\n2.5. Train val split,OneHotVectorizing categories, Input normalization","3f8fd2ef":"> VGG16","10c01e60":"<a id='step3.1'><\/a>\n\n3.1. Creating a Convolutional Neural Network using VGG16 ","8d1babc7":"<a id='step2.8'><\/a>\n2.8. Preprocessing test data","f704a112":"# **Classification of 10 categories of Animal images**\nThe Dataset has been taken from [Animal10](https:\/\/www.kaggle.com\/alessiocorrado99\/animals10).It contains about 28K medium quality animal images belonging to 10 categories. All the images in data set were collected from \"google images\" and have been checked by human. There is some erroneous data to simulate real conditions.\n> CNN using Treansfer Learning Has been implemented. Best model(VGG16) made a training accuracy of 97.040% and validation accuracy of 96.210%. ResNet50 gave a train accuracy=90.222%,Val accuracy=90.170% ","6392f2d9":"<a id='step2.3'><\/a>\n2.3. Viewing Images after preprocessing","c6e65cc1":"<a id='step4'><\/a>\n4. Predicting Categories for Test Data","449e3a54":"<a id='step4.1'><\/a>\n4.1. Viewing Predictions","66b73145":"<a id='step1.1'><\/a>\n\nExplanation for Libraries :\n* > OpenCV(cv2) function for reading image. CV2 reads order of colors as BGR. In PIL, colors order is assumed as RGB .\n* > TQDM is a progress bar library. Inserting tqdm (or python -m tqdm)between pipes will pass through all stdin to stdout while printing progress to stderr\n* > In Utils module, \"class_weight\" parameter penalizes mistakes in samples of class[i] and  \"Shuffle\" parameter for shuffling arrays or sparse matrices in a consistent way \n* > \"categorical\" parameter is used to convert array of labeled data to one-hot vector and \"applications\" can be used for prediction, feature extraction\n* >  \"Dropout\" is used to prevent a model from overfitting. \"Flatten\" is used to preserve weight ordering when switching from one data format to another data format. Dense layer is the regular deeply connected neural network layer","e808b827":"<a id='step1.3'><\/a>\n1.3. Shuffling input and target for optimum training","c2f5a472":"<a id='step2.4'><\/a>\n2.4. Re-Shuffling processed train data and converting to array ","0552a18d":"<a id='step3.5'><\/a>\n3.5. Training Models\n> ResNet50","8875fa8f":"Obtained Accuracy:\n> VGG16: Epochs=25, Train accuracy=0.97040, Val accuracy=0.96210\n> ResNet50: Epochs=25, Train accuracy=0.90222,Val accuracy=0.90170","4186113b":"<a id='step2'><\/a>\n<a id='step2.1'><\/a>\n* 2.1. Function for Resizing and Reshaping the input images","f2dbd909":"<a id='step3.3'><\/a>\n3.3. LEARNING RATE FINDER function","e4c87d21":"<a id='step2.6'><\/a>\n2.6. Renaming Animals (helpful in viewing predictions)","031f86eb":"<a id='step1.2'><\/a>\n1.2.   ****Extracting Train Data****","06c24d89":"> 1. <a href='#step1'>Loading and Exploring Data<\/a>\n     *  1.1.  <a href='#step1.1'>Loading Libraries<\/a>  \n     *  1.2.  <a href='#step1.2'>Extracting Train Data<\/a> \n     *  1.3.  <a href='#step1.3'>Shuffling input and target for optimum training<\/a>    \n>2. <a href='#step2'>Image Preprocessing<\/a>\n     *  2.1. <a href='#step2.1'>Function for Resizing and Reshaping the input images <\/a>     \n     *  2.2. <a href='#step2.2'>Image Resizing and converting them to array<\/a>\n     *  2.3  <a href='#step2.3'>Viewing Images after preprocessing<\/a>\n     * 2.4.  <a href='#step2.4'>Re-Shuffling processed data and converting to array<\/a>\n     * 2.5.  <a href='#step2.5'>Train test split,OneHotVectorizing categories, Input normalization<\/a>              \n     * 2.6.  <a href='#step2.6'>Renaming Animals<\/a>\n     * 2.7.  <a href='#step2.7'>Data Augmentation for training<\/a>\n     * 2.8.  <a href='#step2.8'>Preprocessing Test data<\/a>\n     \n> 3. <a href='#step3.1'>Creating CNN Models with Transfer Learning <\/a>\n     * 3.1 <a href='#step3.1'>CNN using VGG-16 Model <\/a>\n     * 3.2 <a href='#step3.2'>CNN using ResNet50 Model<\/a>\n     * 3.3 <a href='#step3.3'>Defining LEARNING RATE FINDER function <\/a>\n     * 3.4 <a href='#step3.4'>Obtaining Epoch Vs Learning Rate Graph <\/a>\n     * 3.5 <a href='#step3.5'>Training Models <\/a>\n     * 3.6 <a href='#step3.6'>Plotting Loss and Accuracy Curves<\/a> \n> 4. <a href='#step4'>Predicting Categories for Test data<\/a>\n     * 4.1 <a href='#step4.1'>Viewing Predictions<\/a>\n","6e3b527b":"<a id='step3.6'><\/a>\n3.6. Plotting loss and accuracy curves","cae47492":"<a id='step2.7'><\/a>\n2.7. Data Augmentation"}}