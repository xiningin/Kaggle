{"cell_type":{"ee459380":"code","9a65193f":"code","49eebbbb":"code","9cc47a62":"code","82efba49":"code","5b2e4ee2":"code","8199f269":"code","b64b0d7e":"code","c1551ef2":"code","d3f28aad":"code","ff2ffe32":"code","3affa64d":"code","4c9a96a1":"code","e06fb3b1":"code","c1977713":"code","1619530d":"code","240be71e":"code","ef03a6f5":"code","6cd93cd1":"code","541214f3":"code","896f9cf2":"code","d773d63a":"code","fe459ddc":"code","2304a460":"code","14c13ed3":"code","2e0a7b35":"code","380f1114":"code","c97012f5":"code","09d58586":"code","116037ac":"code","cef4963f":"code","9ffca90b":"code","040b398f":"code","77e27252":"code","a1194a3d":"code","3d715dae":"code","7e3b5e7d":"code","e9b08bf3":"code","10232705":"code","15c12303":"code","7bf43138":"code","4960a0c4":"code","edb51e82":"code","2547b0d4":"code","fb37f688":"code","c3b4cc9c":"code","37fc1f53":"code","33ae04aa":"code","b570b7ed":"code","df01bc52":"code","390fb493":"code","de010b9e":"code","0fb70f47":"code","72e2778a":"code","356e525d":"markdown","aa0f0d62":"markdown","47c45501":"markdown","4bed8aa2":"markdown","bc2fa0f0":"markdown","98edc011":"markdown","070702c5":"markdown","de60397c":"markdown","7a2128f7":"markdown","8f165d3c":"markdown","edfa3096":"markdown","440ea553":"markdown","8017d87a":"markdown","a24a6f89":"markdown","6c8fe8ac":"markdown","075c029d":"markdown","8275deeb":"markdown","14faaf5a":"markdown"},"source":{"ee459380":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import plot_importance\nimport pickle\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a65193f":"# load data\nitems=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ncategories=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ntrain=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","49eebbbb":"print(train)","9cc47a62":"boxplot = train.boxplot(column=['item_cnt_day'])","82efba49":"boxplot = train.boxplot(column=['item_price'])","5b2e4ee2":"train = train[(train.item_price < 300000 )& (train.item_price > 0)& (train.item_cnt_day < 1000)]","8199f269":"train.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0","b64b0d7e":"train.item_price","c1551ef2":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","d3f28aad":"shops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )","ff2ffe32":"shops.city","3affa64d":"shops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )","4c9a96a1":"shops.category.unique()","e06fb3b1":"shops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]","c1977713":"shops","1619530d":"categories","240be71e":"categories['item_category_type'] = categories['item_category_name'].str.split('-').map(lambda x: x[0])","ef03a6f5":"categories.item_category_type.unique()","6cd93cd1":"categories['item_category_sub_type'] = categories['item_category_name'].str.split('-').map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())","541214f3":"categories.item_category_sub_type.unique()","896f9cf2":"categories['item_category_type_code'] = LabelEncoder().fit_transform(categories['item_category_type'])\ncategories['item_category_sub_type_code'] = LabelEncoder().fit_transform(categories['item_category_sub_type'])\ncategories = categories[['item_category_id','item_category_type_code','item_category_sub_type_code']]","d773d63a":"categories","fe459ddc":"len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id))))","2304a460":"len(list(set(test.item_id))), len(list(set(test.shop_id))), len(test)","14c13ed3":"matrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","2e0a7b35":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","380f1114":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","c97012f5":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","09d58586":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","116037ac":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, categories, on=['item_category_id'], how='left')\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['item_category_type_code'] = matrix['item_category_type_code'].astype(np.int8)\nmatrix['item_category_sub_type_code'] = matrix['item_category_sub_type_code'].astype(np.int8)","cef4963f":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","9ffca90b":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","040b398f":"group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","77e27252":"group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","a1194a3d":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","3d715dae":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","7e3b5e7d":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)","e9b08bf3":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_sub_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_sub_type_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","10232705":"group = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","15c12303":"group = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","7bf43138":"group = matrix.groupby(['date_block_num', 'item_category_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_category_type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)","4960a0c4":"group = matrix.groupby(['date_block_num', 'item_category_sub_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_category_sub_type_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","edb51e82":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)","2547b0d4":"group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)","fb37f688":"lags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \/\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)","c3b4cc9c":"matrix.drop(['item_name'], axis=1, inplace=True)\nmatrix.drop(['ID'], axis=1, inplace=True)","37fc1f53":"matrix","33ae04aa":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","b570b7ed":"matrix.info()  ","df01bc52":"data = matrix","390fb493":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","de010b9e":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","0fb70f47":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","72e2778a":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nplot_features(model, (10,14))","356e525d":"**Sales_train cleaning**\n* remove item_cnt_day , item_price attributes's outliers\n","aa0f0d62":"Some examples of translated Shop name\n* ! Yakutsk Ordzhonikidze, 56 francs\n* ! Yakutsk TC \"Central\" fran\n* Adygea shopping center \"Mega\"\n* Balashikha TRK \"October-Kinomir\"\n* Volzhsky shopping center \"Volga Mall\"\n* Vologda SEC \"Marmelad\"\n* Voronezh (Plekhanovskaya, 13)\n* Voronezh SEC \"Maksimir\"\n* Voronezh TRC City-Park \"City\"\n* Offsite Trade\n* Zhukovsky st. Chkalov 39m?\n* Zhukovsky st. Chkalov 39m\u00b2\n* Emergency online store\n* Kazan TC \"Behetle\"\n* Kazan TC \"ParkHouse\" II\n* Kaluga SEC \"XXI century\"\n* Kolomna shopping center \"Rio\"\n* Krasnoyarsk TC \"Vzletka Plaza\"\n* Krasnoyarsk TC \"June\"\n* Kursk TC \"Pushkinsky\"\n* Moscow \"Sale\"\n* Moscow MTRC \"Afi Mall\"\n* Moscow Shop C21\n* Moscow TK \"Budenovskiy\" (pav. A2)\n* Moscow TC \"Budenovskiy\" (pav. K7)\n* Moscow TRK \"Atrium\"\n* Moscow TC \"Areal\" (Belyaevo)\n* Moscow TC \"MEGA Belaya Dacha II\"\n* Moscow TC \"MEGA Teply Stan\" II\n* Moscow shopping center \"New Age\" (Novokosino)\n* Moscow TC \"Perlovsky\"\n* Moscow TC \"Semenovsky\"\n* Moscow shopping center \"Silver House\"\n* Mytishchi TRK \"XL-3\"","47c45501":"* remove the items that sold more than 1000 in one day and the item with price greater than 300,000 and it's not negative","4bed8aa2":"The above shop name gives us some hint to the city of each shops","bc2fa0f0":"**Shop Data cleaning**","98edc011":"5100 items , 42 shops and 214,200 pairs of items and shop in test set","070702c5":"**Trainning set and Testing set manipulation**","de60397c":"* 4 records removed from previous conditions","7a2128f7":"**Items' Categories data cleaning****","8f165d3c":"**Translated version**:\n'Headsets \/ Headphones', 'PS2', 'PS3', 'PS4', 'PSP', 'PSVita',\n       'XBOX 360', 'XBOX ONE', 'Tickets (Digit)', 'Delivery of goods',\n       'Other', 'Accessories for games', 'Number', 'Additional editions',\n       'Collector's Editions', 'Standard Editions',\n       'Payment cards (Cinema, Music, Games)', 'Live!', 'Live! (Numeral)',\n       'PSN', 'Windows (Digital)', 'Blu', 'DVD', 'Collectible',\n       'Artbooks, encyclopedias', 'Audiobooks', 'Audiobooks (Digital)',\n       '1C Audiobooks', 'Business Literature', 'Comics, Manga',\n       'Computer literature', 'Methodical materials 1C', 'Postcards',\n       'Educational literature', 'Guides'\n       'Fiction', 'Local CD',\n       'Brand CD', 'MP3', 'Vinyl', 'Music Video',\n       'Gift Editions', 'Attributes', 'Gadgets, Robots, Sports',\n       'Stuffed Toys', 'Board Games',\n       'Board games (compact)', 'Postcards, stickers', 'Development',\n       'Certificates, services', 'Souvenirs', 'Souvenirs (hinged)',\n       'Bags, Albums, Mouse pads', 'Figures', '1C: Enterprise 8',\n       'MAC (Digit)', 'Home & Office', 'Home & Office (Digit)',\n       'Educational', 'Educational (Digit)', 'Service', 'Tickets',\n       'Blank media (spire)', 'Blank media (piece)',\n       'Batteries'","edfa3096":"Translated Version\n\n'PC', 'Accessories', 'Tickets (Digit)', 'Delivery of goods',\n'Game Consoles', 'Games', 'Android Games', 'MAC Games',\n'PC Games', 'Payment Cards (Cinema, Music, Games)', 'Payment Cards',\n'Cinema', 'Books', 'Music', 'Gifts', 'Programs',\n'Service', 'Service', 'Net media (spire)',\n'Blank media (piece)', 'Batteries'","440ea553":"* convert a shop's city and category of it's into the unqiue numerical data","8017d87a":"if none subtype then uses type instead","a24a6f89":"There are a little bit noise in shop city names and categories extract from shop names but since it's unique so it won't significantly effect our prediction","6c8fe8ac":"# Preprocessing ","075c029d":" 363 items are new compared from trainning set to test set","8275deeb":"extract the group of item's categories from the name\n* Seperated by '-' before is type and after is subtype","14faaf5a":"* replace shop_id with the same location and store but may be re-open"}}