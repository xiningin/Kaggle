{"cell_type":{"9f69cff0":"code","5fecac81":"code","dbb62960":"code","98846387":"code","83e7f6b3":"code","f9efb413":"code","5b492d0f":"code","228affff":"code","79d381cc":"code","76e58158":"code","7c26e897":"code","4c6d1ce6":"code","8e66fa2a":"code","3a895c7d":"code","223922ef":"code","0fd524a1":"code","3d367392":"code","c8883c2c":"code","7e3025d7":"code","c49b8dc6":"code","993d4f08":"code","e6a3ef49":"code","3c9ff782":"code","3572ad0d":"code","73d7bdda":"code","4a1e4ebf":"code","0eaae512":"code","11812963":"code","10134083":"code","3abc31ae":"code","5447c9d1":"code","c5de7258":"code","c7deaba7":"code","4b12d4a5":"code","1011d67e":"code","dc5984c8":"code","3d172943":"code","1fa4edfb":"code","d64fb788":"code","fab5d7f4":"code","35404e86":"code","47fcb878":"markdown","e1bb0d1f":"markdown"},"source":{"9f69cff0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n","5fecac81":"tita=pd.read_csv('..\/input\/train.csv')","dbb62960":"tita.head()","98846387":"tita.isnull().sum()","83e7f6b3":"tita[['Pclass','Survived','SibSp','Parch']]=tita[['Pclass','Survived','SibSp','Parch']].astype('str')","f9efb413":"tita.shape","5b492d0f":"tita.info()","228affff":"tita=tita.drop(['Cabin','Name','Embarked','PassengerId','Ticket'],axis=1)\ntita=tita.dropna()\ntita.shape","79d381cc":"tita.Sex=tita.Sex.replace({'male':1,'female':0})","76e58158":"from sklearn.utils import resample\n\n","7c26e897":"\ndf_Parch_0 = resample(tita[tita['Parch']=='0'],n_samples=500,replace=True,random_state=1)\ndf_Parch_1 = resample(tita[tita['Parch']=='1'],n_samples=500,replace=True,random_state=1)\n\ndf_Parch_2 = resample(tita[tita['Parch']=='2'],n_samples=500,replace=True,random_state=1)\n\ndf_Parch_3 = resample(tita[tita['Parch']=='3'],n_samples=500,replace=True,random_state=1)\n\ndf_Parch_4 = resample(tita[tita['Parch']=='4'],n_samples=500,replace=True,random_state=1)\n\ndf_Parch_5 = resample(tita[tita['Parch']=='5'],n_samples=500,replace=True,random_state=1)\ndf_Parch_6 = resample(tita[tita['Parch']=='6'],n_samples=500,replace=True,random_state=1)\n#df_survived_1=resample(tita[tita['Survived']=='1'],n_samples=500,replace=True,random_state=1)\n#df_survived_1=resample(tita[tita['Survived']=='0'],n_samples=500,replace=True,random_state=1)\n\n\n","4c6d1ce6":"tita=pd.concat([df_Parch_0,df_Parch_1,df_Parch_2,df_Parch_3,df_Parch_4,df_Parch_5,df_Parch_6])","8e66fa2a":"from sklearn.model_selection import train_test_split\n\nfirst,second=train_test_split(tita,test_size=0.3,random_state=22)","3a895c7d":"#from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n","223922ef":"ss=StandardScaler()\nss.fit(first[['Age','Fare']])\nfirst[['Age','Fare']]=ss.transform(first[['Age','Fare']])\nsecond[['Age','Fare']]=ss.transform(second[['Age','Fare']])\n","0fd524a1":"first=pd.get_dummies(first)\nsecond=pd.get_dummies(second)\n","3d367392":"first=first.drop(['Survived_0','SibSp_0','SibSp_5','Parch_0'],axis=1)\nsecond=second.drop(['Survived_0','SibSp_0','SibSp_5','Parch_0'],axis=1)\nprint('shape of first and second',first.shape,second.shape)","c8883c2c":"x=first.drop('Survived_1',axis=1)\ny=first['Survived_1']","7e3025d7":"from sklearn.model_selection import train_test_split","c49b8dc6":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=22,stratify=y)\n","993d4f08":"## code to find the best model for the dataset\n\n## Before using this template you should find the hyperparameters for the KNN \n## and Decision and random Forest the add those parameters in the model below \n\n## After selecting the best model we can use KFold CV to check for the bias error and variance error\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom prettytable import PrettyTable\nfrom sklearn import metrics\n\nreport= PrettyTable()\nreport.field_names=['Model name','Accuracy_score','Precision_score','Recall_score','F1_score']\n\n\nregressor=['LogisticRegression','KNN','DecisionTreeClassifier','RandomForestClassifier']\naccuracy=[]\nprecision=[]\nrecall=[]\nf1_score=[]\n\nfor regressor in regressor:\n    if regressor=='LogisticRegression':\n        model1=LogisticRegression()\n        model1.fit(xtrain,ytrain)\n        log_pred=pd.DataFrame(model1.predict(xtest))\n        #Evaluation metrics\n        report.add_row([regressor,\n                    metrics.accuracy_score(ytest,log_pred),\n                    metrics.precision_score(ytest,log_pred,average='weighted'),\n                    metrics.recall_score(ytest,log_pred,average='weighted'),\n                    metrics.f1_score(ytest,log_pred,average='weighted')])\n        \n    elif regressor=='KNN': \n        model2=KNeighborsClassifier(n_neighbors=3)\n        model2.fit(xtrain,ytrain)\n        knn_pred=model2.predict(xtest)\n        #Evaluation metrics\n        report.add_row([regressor,\n                    metrics.accuracy_score(ytest,knn_pred),\n                    metrics.precision_score(ytest,knn_pred,average='weighted'),\n                    metrics.recall_score(ytest,knn_pred,average='weighted'),\n                    metrics.f1_score(ytest,knn_pred,average='weighted')])\n    elif regressor=='DecisionTreeClassifier':\n        model3=DecisionTreeClassifier(criterion='entropy')\n        model3.fit(xtrain,ytrain)\n        dec_pred=model3.predict(xtest)\n        #Evaluation metrics\n        report.add_row([regressor,\n                    metrics.accuracy_score(ytest,dec_pred),\n                    metrics.precision_score(ytest,dec_pred,average='weighted'),\n                    metrics.recall_score(ytest,dec_pred,average='weighted'),\n                    metrics.f1_score(ytest,dec_pred,average='weighted')])\n        \n    elif regressor=='RandomForestClassifier':\n        model4=RandomForestClassifier(criterion='gini')\n        model4.fit(xtrain,ytrain)\n        random_pred=model4.predict(xtest)\n        #Evaluation metrics\n        report.add_row([regressor,\n                    metrics.accuracy_score(ytest,random_pred),\n                    metrics.precision_score(ytest,random_pred,average='weighted'),\n                    metrics.recall_score(ytest,random_pred,average='weighted'),\n                    metrics.f1_score(ytest,random_pred,average='weighted')])\nprint(report)\n","e6a3ef49":"## Code to find the best hyper parameters for the KNN and DecisionTree and Random forest\n\nfrom sklearn.model_selection import RandomizedSearchCV\n## code to find the best model for the dataset\n\n\nbest_par= PrettyTable()\nbest_par.field_names=['Model name','Best Parameters','Best Score']\n\n\nregressor=['KNN','DecisionTreeClassifier','RandomForestClassifier']\n\n\nfor regressor in regressor:\n    if regressor=='KNN': \n        grid1={'n_neighbors': np.arange(1,50),'p': np.arange(1,50)}\n        ran_search1=RandomizedSearchCV(model2,grid1,cv=3)\n        ran_search1.fit(xtrain,ytrain)\n        best_par.add_row([regressor,\n                          ran_search1.best_params_,\n                          ran_search1.best_score_])\n    elif regressor=='DecisionTreeClassifier':\n        \n        \n        grid2={'criterion':['gini','entropy'],'max_depth': np.arange(2,10),'max_leaf_nodes':np.arange(2,10),'min_samples_leaf':np.arange(2,10)}\n        ran_search2=RandomizedSearchCV(model3,grid2,cv=3)\n        ran_search2.fit(xtrain,ytrain)\n        best_par.add_row([regressor,\n                          ran_search2.best_params_,\n                          ran_search2.best_score_])\n        \n    elif regressor=='RandomForestClassifier':\n        \n        \n        grid3={'criterion':['gini','entropy'],'n_estimators':np.arange(1,100),'max_features':np.arange(1,10)}\n        ran_search3=RandomizedSearchCV(model4,grid3,cv=3)\n        ran_search3.fit(xtrain,ytrain)\n        best_par.add_row([regressor,\n                          ran_search3.best_params_,\n                          ran_search3.best_score_])\n        \nprint(best_par)\n\n","3c9ff782":"from sklearn.model_selection import KFold\n\nkf=KFold(n_splits=10)\nrandom_final=PrettyTable()\nrandom_final.field_names=['Model','Accuracy','Precision','Recall','F1_score']\naccuracy=[]\nprecision=[]\nrecall=[]\nf1_score=[]\nfor train,test in kf.split(xtrain,ytrain):\n    \n    xtrain1,xtest1=xtrain.iloc[train,:],xtrain.iloc[test,:]\n    ytrain1=ytrain.iloc[train]\n    ytest1=ytrain.iloc[test]\n    \n    model4=RandomForestClassifier(n_estimators=15,max_features=7,criterion='gini')\n    model4.fit(xtrain1,ytrain1)\n    random_pred=model4.predict(xtest1)\n    #Evaluation metrics\n    accuracy.append(metrics.accuracy_score(ytest1,random_pred))\n    precision.append(metrics.precision_score(ytest1,random_pred,average='weighted'))\n    recall.append(metrics.recall_score(ytest1,random_pred,average='weighted'))\n    f1_score.append(metrics.f1_score(ytest1,random_pred,average='weighted'))\nrandom_final.add_row([regressor,np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1_score)])\n                        \n\n\nprint(random_final)\n\nmetrics.confusion_matrix(ytest1,random_pred)","3572ad0d":"x_sec=first.drop('Survived_1',axis=1)\ny_sec=first['Survived_1']","73d7bdda":"from sklearn.model_selection import KFold\n\nkf=KFold(n_splits=10)\nrandom_final=PrettyTable()\nrandom_final.field_names=['Model','Accuracy','Precision','Recall','F1_score']\naccuracy=[]\nprecision=[]\nrecall=[]\nf1_score=[]\nfor train,test in kf.split(x_sec,y_sec):\n    \n    xtrain1,xtest1=x_sec.iloc[train,:],x_sec.iloc[test,:]\n    ytrain1=y_sec.iloc[train]\n    ytest1=y_sec.iloc[test]\n    \n    model4=RandomForestClassifier(n_estimators=63,max_features=7,criterion='entropy')\n    model4.fit(xtrain1,ytrain1)\n    random_pred=model4.predict(xtest1)\n    #Evaluation metrics\n    accuracy.append(metrics.accuracy_score(ytest1,random_pred))\n    precision.append(metrics.precision_score(ytest1,random_pred,average='weighted'))\n    recall.append(metrics.recall_score(ytest1,random_pred,average='weighted'))\n    f1_score.append(metrics.f1_score(ytest1,random_pred,average='weighted'))\nrandom_final.add_row([regressor,np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1_score)])\n                        \n\n\nprint(random_final)\n\nmetrics.confusion_matrix(ytest1,random_pred)","4a1e4ebf":"test_df=pd.read_csv('..\/input\/test.csv')\ntest_tita=test_df\ntest_tita.head()","0eaae512":"test_tita1=test_tita","11812963":"test_tita.isnull().sum()","10134083":"test_tita.describe()","3abc31ae":"test_tita['Age']=test_tita['Age'].fillna(test_tita['Age'].median())\ntest_tita['Fare']=test_tita['Fare'].fillna(test_tita['Fare'].median())\n\n","5447c9d1":"test_tita[['Pclass','SibSp','Parch']]=test_tita[['Pclass','SibSp','Parch']].astype('str')\n\ntest_tita.shape\n\ntest_tita.info()\n\ntest_tita=test_tita.drop(['Cabin','Name','Embarked','PassengerId','Ticket'],axis=1)\ntest_tita=test_tita.dropna()\ntest_tita.shape\n\ntest_tita.Sex=test_tita.Sex.replace({'male':1,'female':0})","c5de7258":"test_tita=pd.get_dummies(test_tita)","c7deaba7":"test_tita.shape","4b12d4a5":"test_tita.head()","1011d67e":"xtest.head()","dc5984c8":"test_tita.head()","3d172943":"test_tita=test_tita.drop(['SibSp_0','SibSp_5','Parch_0','SibSp_8','Parch_9'],axis=1)\n\ntest_tita.shape","1fa4edfb":"ypred=model4.predict(test_tita)","d64fb788":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": ypred\n    })\n","fab5d7f4":"submission.head()","35404e86":"submission.to_csv('tita_sub.csv',index=False)","47fcb878":"## Building the random forest classifier using the KFold cross validation","e1bb0d1f":"## From the above details we can tell Random forest classifier is performing well in this dataset with the parameters as 'n_estimators': 15, 'max_features': 7, 'criterion': 'gini'"}}