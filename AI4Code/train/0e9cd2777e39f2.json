{"cell_type":{"b60df76f":"code","2a4cea78":"code","b3c99945":"code","76a10535":"markdown","83bc56aa":"markdown","b811ce4c":"markdown"},"source":{"b60df76f":"import pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import f1_score\nimport scipy\n\n\ndef simple_pipeline():\n    print(\"Load data\")\n    train, test = load_data()\n    \n    data = pd.concat([train, test], axis=0, ignore_index=True)\n    print(\"Vectorization\")\n    X = vectorization(data.drop('target', axis=1))\n    if type(X) == scipy.sparse.coo_matrix:\n        X = X.tocsr()\n        \n    test_mask = data.is_test.values\n    \n    X_train = X[~test_mask]\n    y_train = data['target'][~test_mask]\n    \n    X_test = X[test_mask]\n    if scipy.sparse.issparse(X):\n        X_train.sort_indices()\n        X_test.sort_indices()\n\n    model = build_model(X_train, y_train)\n    \n    print(\"Prediction with model\")\n    p = model.predict(X_test)\n    \n    print(\"Generate submission\")\n    make_submission(data[test_mask], p)\n\n\ndef load_data():\n    train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n    train['is_test'] = False\n    \n    test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n    test['target'] = -1\n    test['is_test'] = True\n    \n    return train, test\n\n\ndef calculate_validation_metric(model, X, y, metric):\n    folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n    score = cross_val_score(model, X, y, scoring=metric, cv=folds, n_jobs=4)\n    \n    return np.mean(score), model\n\n\ndef select_model(X, y):\n    models = [\n        LinearSVC(C=30),\n        LinearSVC(C=10),\n        LinearSVC(C=3),\n        LinearSVC(C=1),\n        LinearSVC(C=0.3),\n        LinearSVC(C=0.1),\n        LinearSVC(C=0.03),\n        RidgeClassifier(alpha=30),\n        RidgeClassifier(alpha=10),\n        RidgeClassifier(alpha=3),\n        RidgeClassifier(alpha=1),\n        RidgeClassifier(alpha=0.3),\n        RidgeClassifier(alpha=0.1),\n        RidgeClassifier(alpha=0.03),\n        LogisticRegression(C=30),\n        LogisticRegression(C=10),\n        LogisticRegression(C=3),\n        LogisticRegression(C=1),\n        LogisticRegression(C=0.3),\n        LogisticRegression(C=0.1),\n        LogisticRegression(C=0.03),\n    ]\n    \n    results = [calculate_validation_metric(\n        model, X, y, 'f1_macro',\n    ) for model in models]\n\n    best_result, best_model = max(results, key = lambda x: x[0]) \n    print(\"Best model validation result: {:.4f}\".format(best_result))\n    print(\"Best model: {}\".format(best_model))\n    \n    return best_model\n\n\ndef build_model(X, y):\n    print(\"Selecting best model\")\n    best_model = select_model(X, y)\n    \n    print(\"Refit model to full dataset\")\n    best_model.fit(X, y)\n    \n    return best_model\n\n    \ndef make_submission(data, p):\n    submission = data[['id']].copy()\n    submission['target'] = p\n    submission.to_csv('submission.csv', index=False)","2a4cea78":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom scipy.sparse import hstack\n\ndef vectorization(data):\n    \"\"\"\n    data is concatenated train and test datasets with target excluded\n    Result value \"vectors\" expected to have some number of rows as data\n    \"\"\"\n    \n    tfidf_chars = TfidfVectorizer(\n        analyzer='char',\n        strip_accents='ascii',\n        ngram_range=(3, 6),\n        min_df=0.0015\n    )\n    \n    tfidf_words = TfidfVectorizer(\n        analyzer='word',\n        strip_accents='ascii',\n    )\n    \n    \n    \n    text = data['text'].fillna('').str.lower()\n    keyword = data['keyword'].fillna('').str.lower()\n    \n    vectors_word = tfidf_words.fit_transform(text + ' ' + keyword)\n    vectors_char = tfidf_chars.fit_transform(text)\n\n    return hstack((vectors_word, vectors_char))","b3c99945":"%%time \n\nsimple_pipeline()","76a10535":"# Your part\n## In *vectorization* method you can change everything and use any dependencies","83bc56aa":"# Fixed pipeline\nIn order to participate, the part below need to be unchanged","b811ce4c":"# **Vectorizers** - #1 micro challenge\n# Rules\nI have an idea of an alternative challenge format for a while. I want to test it.\nIn short, it's a short challenge with specific measurable goals to be achieved.\n\nIn this challenge, you are given a fixed pipeline and only can change the vectorization process. The vectorization method interface is fixed, the rest is up to you.\n\nYou need to **fork [original notebook](https:\/\/www.kaggle.com\/dremovd\/micro-challenge-vectorizers)**\nIn order to compete, you also need to **make your Kaggle notebook public**.\n\n# Challenge [data](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data)\nData is the same as for the official competition, you can read description here https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data\n\n# Goals\n- \ud83e\udd49 Bronze. F1-score >= **0.80** at **public** leaderboard \n- \ud83e\udd48 Silver. F1-score >= **0.81** at **public** leaderboard\n- \ud83e\udd47 Gold. F1-score >= **0.81** at **public** leaderboard + runtime is below **1 minute**\n\n# [Submit](https:\/\/forms.gle\/H8MPo4xpu4NDVsX49)\nYou can submit your **public** Kaggle notebook via this [link](https:\/\/forms.gle\/H8MPo4xpu4NDVsX49) \n# [Leaderboard](http:\/\/bit.ly\/36pSp3S) \nThe final leaderboard is sorted by a medal type and then by submission time. The earlier you achieved the goal is better. You can see current leaderboard by this [link](http:\/\/bit.ly\/36pSp3S)"}}