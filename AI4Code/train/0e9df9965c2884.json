{"cell_type":{"46da0293":"code","3fb9e6fb":"code","f216fc96":"code","2582a20b":"code","19e458a0":"code","1c63033f":"code","fc5e3804":"code","a8de99ee":"code","9a2d3a30":"code","b0cd8fb3":"code","2c7680b5":"code","96ac50aa":"code","1c12e98a":"code","64ff768d":"code","fe5e4680":"code","cb27d8a2":"code","fcf20dd7":"code","ac74d0e6":"code","b7733e81":"code","87f44f83":"code","e34a1af0":"code","4a876c9d":"code","dc21c257":"code","c6f45870":"code","2f29e82e":"markdown"},"source":{"46da0293":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3fb9e6fb":"import re\nimport nltk\nnltk.download(\"stopwords\")\nnltk.download('punkt')\nfrom nltk import word_tokenize,sent_tokenize\nnltk.download('wordnet')\nimport nltk as nlp","f216fc96":"df = pd.read_csv('..\/input\/covidvaccine-tweets\/covidvaccine.csv',encoding='utf8')\ndf.head()","2582a20b":"df2=df[['user_favourites','text']].copy()\ndf2","19e458a0":"df2.text[0]","1c63033f":"cult_list=[]\n\nfor cult in df2.text:\n    if type(cult)==str:\n        cult=re.sub(\"[^a-zA-z]\",\" \",cult)\n        cult=cult.lower()\n        cult=nltk.word_tokenize(cult)\n        lemma=nlp.WordNetLemmatizer()\n        cult=[lemma.lemmatize(word) for word in cult]\n        cult=\" \".join(cult)\n        cult_list.append(cult)","fc5e3804":"from sklearn.feature_extraction.text import CountVectorizer\n\nmax_features=800\ncount_vectorizer=CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(cult_list).toarray()","a8de99ee":"sparce_matrix.shape ","9a2d3a30":"sparce_matrix","b0cd8fb3":"print(\"Top {} the most used words: {}\".format(max_features,count_vectorizer.get_feature_names()))","2c7680b5":"data = pd.DataFrame(count_vectorizer.get_feature_names(),columns=[\"Words\"])\ndata[0:10]","96ac50aa":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\nplt.subplots(figsize=(10,10))\nwordcloud=WordCloud(background_color=\"black\",width=1024,height=768).generate(\" \".join(data.Words[0:100]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","1c12e98a":"X=sparce_matrix[0:20000]\ny0=df.user_favourites[0:20000]\n\ny1=[]\nfor item in y0:\n    y1+=[int(np.log1p(float(item)))]\ny=pd.Series(y1)\n\nprint(X.shape)\nprint(y.shape)","64ff768d":"print(y.value_counts())","fe5e4680":"from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss,precision_score\nfrom sklearn.metrics import roc_auc_score,roc_curve\n\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cb27d8a2":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","fcf20dd7":"lgbm_model = LGBMClassifier()\nlgbm_model.fit(X_train,y_train)\ny_pred = lgbm_model.predict(X_test)","ac74d0e6":"print(\"Accuracy:\",accuracy_score(y_test,y_pred))","b7733e81":"df1=df[['user_favourites','text']].copy()\ndf1[0:5]","87f44f83":"df2=df1.dropna(how='any')\ndf2[0:5]","e34a1af0":"badidx=[]\nfor item in df2['user_favourites']:\n    if (type(item) == str) and ('-' in item):\n        badidx+=[df2[df2['user_favourites']==item].index.values[0]]\n        print(item)\nprint(badidx)\n\ndf3=df2.drop(index=badidx)","4a876c9d":"favorites=[]\nfor item in df3['user_favourites']:\n    favorites+=[float(item)]\ndf3['favorites']=favorites\ndf3[0:5]","dc21c257":"df4=df3.sort_values(by=['favorites'], ascending=False).reset_index()\ndf4[0:5]","c6f45870":"for i in range(8):\n    print('['+str(i+1)+'] '+df4['text'][i])","2f29e82e":"NLTK is a leading platform for building Python programs to work with human language data.\nhttps:\/\/www.nltk.org\/"}}