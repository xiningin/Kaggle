{"cell_type":{"81e9f5ec":"code","7473dcbb":"code","2e516362":"code","65d8dcbc":"code","9edb967a":"code","dd73e160":"code","98953ea9":"code","84abe563":"code","56f760ff":"code","ac6d24a9":"code","3850bb95":"code","d888b5d4":"code","828ee355":"code","e7153bfd":"code","5aa786be":"code","64fe166e":"code","70bad882":"code","1123a228":"code","06307b75":"code","d9b9fbc0":"markdown","b8712028":"markdown","309e0c4a":"markdown","7fefb6f8":"markdown","d12fdbe4":"markdown","2ae2dff0":"markdown","b5057de5":"markdown","e53b7472":"markdown","75026cd1":"markdown","3ce98af0":"markdown"},"source":{"81e9f5ec":"import tensorflow as tf\nfrom tensorflow import keras\nprint(tf.__version__)","7473dcbb":"tf.config.experimental.list_physical_devices(device_type=None)","2e516362":"!nvidia-smi","65d8dcbc":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","9edb967a":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)","dd73e160":"import numpy as np                                \nimport matplotlib.pyplot as plt\nimport tensorflow.keras as k\n# from tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import backend as K\nimport time","98953ea9":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\nimg_rows, img_cols = 28,28\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\nx_test=x_test.astype('float32')\nx_train=x_train.astype('float32')\nmean=np.mean(x_train)\nstd=np.std(x_train)\nx_test = (x_test-mean)\/std\nx_train = (x_train-mean)\/std\n\nprint(\"counts of x_train : {}, y_train : {}, x_test : {}, y_test : {}\".format(\n    len(x_train), len(y_train), len(x_test), len(y_test)))","84abe563":"num_classes=10\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\nprint(\"counts of x_train : {}, y_train : {}, x_test : {}, y_test : {}\".format(\n    len(x_train), len(y_train), len(x_test), len(y_test)))","56f760ff":"optimizer = tf.keras.optimizers.SGD()","ac6d24a9":"# don't change anything\nnum_filter=32\nnum_dense=512\ndrop_dense=0.7\nac='relu'\nlearningrate=0.001\n\nwith tf.device(\"\/cpu:0\"):\n    model = Sequential()\n\n    model.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32\n\n    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons\n\n    model.add(Flatten())                        \n    model.add(Dense(num_dense, activation=ac))\n    model.add(BatchNormalization())\n    model.add(Dropout(drop_dense))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optimizer)","3850bb95":"cpu_list=[]\nbatch_sizes = []\nwith tf.device(\"\/cpu:0\"):\n    for i in range(0,7):\n        k=8*2**i\n        print(\"batch size \"+str(k))\n        t1 = time.time()\n        model.fit(x_train, y_train, batch_size=k, epochs=1, validation_data=(x_test, y_test))\n        t2 = time.time()\n        cpu_list.append(int(t2-t1))\n        batch_sizes.append(k)","d888b5d4":"print(cpu_list)","828ee355":"cpu_score = sum(cpu_list)\/len(cpu_list)\ncpu_score","e7153bfd":"!nvidia-smi","5aa786be":"num_filter=32\nnum_dense=512\ndrop_dense=0.7\nac='relu'\nlearningrate=0.001\n\nmodel = Sequential()\n\nmodel.add(Conv2D(num_filter, (3, 3), activation=ac, input_shape=(28, 28, 1),padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(num_filter, (3, 3), activation=ac,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 14x14x32\n\nmodel.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Conv2D(2*num_filter, (3, 3), activation=ac,padding='same'))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))   # reduces to 7x7x64 = 3136 neurons\n\nmodel.add(Flatten())                        \nmodel.add(Dense(num_dense, activation=ac))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drop_dense))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optimizer)","64fe166e":"gpu_list=[]\nbatch_sizes = []\nprint(\"gpu_list : \", gpu_list)\nwith tf.device(\"\/gpu:0\"):\n    for i in range(0,7):\n        k=8*2**i\n        print(\"batch size \"+str(k))\n        t1 = time.time()\n        model.fit(x_train, y_train, batch_size=k, epochs=1, validation_data=(x_test, y_test))\n        t2 = time.time()\n        gpu_list.append(int(t2-t1))\n        batch_sizes.append(k)","70bad882":"gpu_list","1123a228":"gpu_score = sum(gpu_list)\/len(gpu_list)\ngpu_score","06307b75":"plt.plot(batch_sizes,gpu_list,'bo')\nplt.plot(batch_sizes,cpu_list,'ro')\nplt.plot(batch_sizes,gpu_list,'b--')\nplt.plot(batch_sizes,cpu_list,'r--')\nplt.ylabel('training time per epoch (s)')\nplt.xlabel('batch size')\nplt.legend(['gpu', 'cpu'], loc='upper right')\nplt.ylim([0,400])\nplt.savefig('CPUvsGPU.png') \nplt.show()","d9b9fbc0":"As you can see the GPU is way more faster than CPU, it helps us to **reduce training times** for deep learning models","b8712028":"## \ud83d\udd25 If using this in kaggle notebook, don't forget to connect to GPU Accelerator \ud83c\udf0c","309e0c4a":"# \ud83d\udd25 For my GPU NVIDIA-RTX 3060 the score on average is 10\n### \ud83d\udd25 While my CPU took 90 sec per epoch on average, my GPU just finished it in 10 sec","7fefb6f8":"## For my CPU AMD-Ryzen 9 5900 HX the score (lower is better) is 92\n\ud83d\udd25 It takes roughly 92 seconds per epoch, may vary depending on your cpu stats","d12fdbe4":"# CPU ONLY","2ae2dff0":"#  Check the GPU stats","b5057de5":"# Don't forget to comment your CPU and GPU scores \ud83d\ude01\ud83d\udc6f\u200d","e53b7472":"# GPU ONLY","75026cd1":"# Hi Guys today we will see what is the \ud83d\ude0d advantage of using GPU's for DEEP LEARNING","3ce98af0":"The output of the below cell should be \n**1 Physical GPUs, 1 Logical GPUs**"}}