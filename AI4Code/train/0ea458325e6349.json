{"cell_type":{"bc07d858":"code","2d9c12d0":"code","f01b836a":"code","9e30ba06":"code","087575a9":"code","e9e66244":"code","a7f0e07e":"code","8725aa5f":"code","4591c152":"code","f10ac1c0":"code","89b3029b":"code","f8feba67":"code","41659903":"code","53280439":"code","08b6af0a":"code","bdabb51a":"code","981ea542":"code","756c9302":"code","dc454851":"code","fc83d2eb":"code","e2668796":"code","ce4eb8c6":"code","a6d07027":"code","6ab89b16":"code","f0f51ffe":"code","b3ca2d35":"code","a0e1895a":"code","e7b62e7d":"code","68b2a287":"code","93a188d2":"code","0aa2269d":"code","4b7fbe9a":"code","92b3155d":"code","1a8b545e":"code","076bf5c3":"code","4ba1958f":"code","8772c119":"code","4992b5b5":"code","67672bba":"code","e08c4bc3":"code","26d06ced":"code","a910cd38":"code","e44bc6d3":"code","64ed718a":"code","2b3a5660":"code","fcabd9a8":"code","0fd72497":"code","bb01460c":"code","3cabf1ae":"code","215ddfc1":"code","68fae31e":"code","9e96b5dc":"code","153cff49":"code","e610411a":"code","45a34848":"code","0691bdfa":"code","dfa2b7cd":"code","1381d268":"code","b612b834":"code","96d01350":"code","d80fcc5c":"code","5e1422c6":"code","659dd46d":"code","6d57fdfa":"code","d22e7524":"code","89e2bcca":"code","557e7aa0":"code","0e1f6849":"code","cf2c9dcb":"code","e2d7bcf3":"code","354dd01b":"code","cf54b7b3":"code","67ac010a":"code","757fae59":"code","67503b63":"code","2670cd48":"code","29d881cd":"code","cc41b565":"code","e20ecc92":"code","86052bf2":"code","5e37a476":"code","dbcfdac1":"code","c68a9c47":"code","21d86534":"code","7190046b":"code","ec233b90":"code","e52b5530":"code","82e1073a":"code","6a0b2e5e":"code","dc861bd8":"code","787eeaae":"code","2822d4df":"code","67abb289":"code","7338c933":"code","bc6b1f71":"code","5115efe6":"code","240f8580":"code","c864f587":"code","00892d0d":"code","7ca8a6c7":"code","bf8e1a9a":"code","32810250":"code","2914726f":"markdown","701d28ea":"markdown","8caa1d76":"markdown","426375a1":"markdown","2d0cf209":"markdown","1e080807":"markdown","2b67c5a4":"markdown","6f1ac96f":"markdown","61b6599e":"markdown","4c39ad14":"markdown","7c9fbcca":"markdown","25c4f87f":"markdown","f6160ca7":"markdown","91592e2d":"markdown","64c7ff46":"markdown","3894fbac":"markdown","9d25786c":"markdown","0f2f6df7":"markdown","2c15c436":"markdown","f9488647":"markdown","794a591f":"markdown","d01fef91":"markdown","568a9f23":"markdown","9d84f687":"markdown","32e38f15":"markdown","19ffc04b":"markdown","c8d42ae6":"markdown","52134168":"markdown","44b143e2":"markdown","46d51b1e":"markdown","b9093970":"markdown","643a1df9":"markdown","45f04b3b":"markdown","c309d696":"markdown","f52e7131":"markdown","2469bc74":"markdown","29e8b722":"markdown","9642152d":"markdown","2f337f72":"markdown","ba945114":"markdown","43dab802":"markdown","37ae9a89":"markdown","cf8e9dc3":"markdown","13337c3a":"markdown","ef231b32":"markdown","3ff52520":"markdown","0b9fcb3c":"markdown","2fc08762":"markdown","04936e90":"markdown","6879c0b1":"markdown","f0213595":"markdown","ab404673":"markdown","f9af62fc":"markdown","592563ce":"markdown"},"source":{"bc07d858":"# Standard Data Analytic Packages\nimport pandas as pd\nimport numpy as np\n\n# Graphing Packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2d9c12d0":"# Set graphic style and figure size\nsns.set(rc={'figure.figsize':(11,8)})\nsns.set_style(\"white\")","f01b836a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n#train = pd.read_csv('\/Users\/philiposborne\/Documents\/Other Projects\/Competition-Kaggle Titanic\/titanic\/train.csv')\n#test = pd.read_csv('\/Users\/philiposborne\/Documents\/Other Projects\/Competition-Kaggle Titanic\/titanic\/test.csv')","9e30ba06":"train.head()","087575a9":"train.tail()","e9e66244":"train.describe()","a7f0e07e":"test.head()","8725aa5f":"train['Name'][0]","4591c152":"train.Name[0]","f10ac1c0":"train[['Name','Sex']].head()","89b3029b":"# First row, second column\ntrain.iloc[0,3]","f8feba67":"# First row, second and third column\ntrain.iloc[0,[3,4]]","41659903":"# First and second row, second and third column\ntrain.iloc[[0,1],[3,4]]","53280439":"# First to 10th row, second and third column\ntrain.iloc[0:10,[3,4]]","08b6af0a":"# First to 10th row, second to fourth column\ntrain.iloc[0:10,3:5]","bdabb51a":"train.iloc[0,3]","981ea542":"# Find those older than 70   \ntrain[train['Age']>70].head()","756c9302":"# Find those older than 70 OR younger than or equal to 18 \ntrain[(train['Age']>70) | (train['Age']<=18)].head()","dc454851":"# Find those older than or equal to 60 and Female --> only 4 passengers who are above 60 are female\ntrain[(train['Age']>=60) & (train['Sex']==\"female\")].head()","fc83d2eb":"train.describe()","e2668796":"train['Age'].unique()","ce4eb8c6":"train[np.isnan(train['Age'])==True].head()","a6d07027":"train = train.dropna(subset=['Age'])\ntrain.describe()","6ab89b16":"train['Cabin'].unique()","f0f51ffe":"train['Cabin'] = train['Cabin'].fillna('unknown')\ntrain['Embarked'] = train['Embarked'].fillna('unknown')\n\ntrain.head()","b3ca2d35":"print(\"The mean age of passengers is:\", train['Age'].mean())\nprint(\"The max age of passengers is:\", train['Age'].max())\nprint(\"The min age of passengers is:\", train['Age'].min())\n\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.mean.html\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.max.html\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.min.html","a0e1895a":"print(\"The mean age of passengers is:\", np.mean(train['Age']))\nprint(\"The max age of passengers is:\", np.max(train['Age']))\nprint(\"The min age of passengers is:\", np.min(train['Age']))\n\n#https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.ndarray.mean.html\n#https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.ndarray.max.html\n#https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.ndarray.min.html","e7b62e7d":"print(\"The mean age of passengers, rounded to 3 decimal places, is:\", np.round( np.mean(train['Age'])  ,3))","68b2a287":"#e.g. string combining\n\nstring_1 = \"Hello\"\nstring_2 = \"World!\"\n\nprint(string_1 + \" \" + string_2)","93a188d2":"sns.scatterplot(train['PassengerId'], train['Age'])\n\n# Remove some of the plot lines\nsns.despine()\n\n# Graph Titles. \"\\n\" will create a new line in the text if needed\nplt.title(\"A Scatter Plot Showing the Age of Each Passenger ID \\n The Mean Age of Passengers is: \" +  str(np.round( np.mean(train['Age'])  ,3)))\nplt.xlabel(\"Passenger ID\")\nplt.ylabel(\"Age\")\n\nplt.show()","0aa2269d":"# Set all bars to green\n\nage_min = np.min(train['Age'])\nage_mean = np.mean(train['Age'] )\nage_max = np.max(train['Age'])\n                   \nsns.barplot(['min', 'mean', 'max'],[age_min, age_mean, age_max] , color = 'g')\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Summary Statistics of the Age Feature \\n All Bars Set to Green\")\nplt.ylabel(\"Age Value\")\n\nplt.show()","4b7fbe9a":"# Use 'Blues' palette range\n\nage_min = np.min(train['Age'])\nage_mean = np.mean(train['Age'] )\nage_max = np.max(train['Age'])\n                   \nsns.barplot(['min', 'mean', 'max'],[age_min, age_mean, age_max] , palette = 'hls')\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Summary Statistics of the Age Feature \\n Colors Set by Palette Scheme 'hls'\")\nplt.ylabel(\"Age Value\")\n\nplt.show()","92b3155d":"# Manually set each bar's color using hex codes\n\nage_min = np.min(train['Age'])\nage_mean = np.mean(train['Age'] )\nage_max = np.max(train['Age'])\n                   \nsns.barplot(['min', 'mean', 'max'],[age_min, age_mean, age_max] , palette = [\"#808282\", \"#B4ED20\", \"#918BC3\"])\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Summary Statistics of the Age Feature \\n Palette Colors Manually Defined with Hexcodes\")\nplt.ylabel(\"Age Value\")\n\nplt.show()","1a8b545e":"sns.boxplot(x=\"Sex\", y=\"Age\", data=train, palette = ['#0066cc','#cc99ff'])\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Box Plot to Compare the Age of Male and Females on the Titanic\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Age\")\nplt.show()\n","076bf5c3":"sns.boxplot(x=\"Sex\", y=\"Age\", data=train, palette = ['#0066cc','#cc99ff'])\nsns.swarmplot(x=\"Sex\", y=\"Age\", data=train, color=\".25\", alpha = 0.7)\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Box Plot to Compare the Age of Male and Females on the Titanic\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Age\")\nplt.show()\n","4ba1958f":"print(\"Number of passengers that were male: \" , len(train[train['Sex']=='male']))\nprint(\"Number of passengers that were female: \" , len(train[train['Sex']=='female']))","8772c119":"sns.violinplot(x=\"Sex\", y=\"Age\", data=train, palette = ['#0066cc','#cc99ff'])\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Box Plot to Compare the Age of Male and Females on the Titanic\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Age\")\nplt.show()","4992b5b5":"sns.boxplot(x=\"Pclass\", y=\"Age\", data=train, hue=\"Sex\", palette = ['#0066cc','#cc99ff'])\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Box Plot to Compare the Age of Passenger Classes by Gender on the Titanic\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Age\")\nplt.show()\n","67672bba":"sns.boxplot(x=\"Pclass\", y=\"Fare\", data=train, hue=\"Sex\", palette = ['#0066cc','#cc99ff'])\n\n# Remove some of the plot lines\nsns.despine()\n\nplt.title(\"Box Plot to Compare the Fare of Passenger Classes by Gender on the Titanic\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Fare\")\nplt.show()\n","e08c4bc3":"train['Pclass_2'] = np.where(train['Pclass']==1,\"First\", \n                    np.where(train['Pclass']==2,\"Second\", \n                    np.where(train['Pclass']==3,\"Third\",\n                             \"error\")))","26d06ced":"sns.scatterplot(train['Age'], train['Fare'], hue = train['Pclass_2'])\n\n# Remove some of the plot lines\nsns.despine()\n\n# Graph Titles. \"\\n\" will create a new line in the text if needed\nplt.title(\"A Scatter Plot Showing the Age and Fare for each Passenger\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\n\nplt.show()","a910cd38":"sns.scatterplot(train['Age'], train['Fare'], hue = train['Pclass_2'])\n\n# Remove some of the plot lines\nsns.despine()\n\n# Graph Titles. \"\\n\" will create a new line in the text if needed\nplt.title(\"A Scatter Plot Showing the Age and Fare for each Passenger\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\nplt.ylim(0,100)\nplt.show()","e44bc6d3":"sns.distplot(train[train['Pclass_2']==\"First\"][\"Fare\"], label = \"First Class\")\nsns.distplot(train[train['Pclass_2']==\"Second\"][\"Fare\"], label = \"Second Class\")\nsns.distplot(train[train['Pclass_2']==\"Third\"][\"Fare\"], label = \"Third Class\")\n\n# Remove some of the plot lines\nsns.despine()\n\n\nplt.title(\"Histrograms to Compare the Fare of Passenger Classes on the Titanic\")\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Fare\")\n\nplt.ylim(0,0.2)\n\nplt.legend()\n\nplt.show()\n","64ed718a":"# Remind ourselves the features available\ntrain.head()","2b3a5660":"train['counter'] = 1","fcabd9a8":"train[['Sex','Age','Survived','counter']].groupby(['Sex','Age', 'Survived']).count()[0:5]","0fd72497":"train[['Sex','Age','Survived','counter']].groupby(['Sex','Age','Survived']).sum()[0:5]","bb01460c":"train[['Sex','Age','Survived','counter']].groupby(['Sex','Age', 'Survived']).count().reset_index(drop=False)[0:5]","3cabf1ae":"train_gender_age_surv_count = train[['Sex','Age','Survived','counter']].groupby(['Sex','Age', 'Survived']).count().reset_index(drop=False)\ntrain_gender_age_surv_count['prob'] = train_gender_age_surv_count['counter']\/sum(train_gender_age_surv_count['counter'])","215ddfc1":"train_gender_age_count_SURVIVED = train_gender_age_surv_count[train_gender_age_surv_count['Survived']==1].reset_index(drop=True)\ntrain_gender_age_count_DIED = train_gender_age_surv_count[train_gender_age_surv_count['Survived']==0].reset_index(drop=True)","68fae31e":"train_gender_age_count_SURVIVED.head()","9e96b5dc":"male_surv_prob = sum(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['prob'])\nfemale_surv_prob = sum(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['prob'])\n\n\n\n# Remove some of the plot lines\nsns.despine()\n\nsns.barplot( ['Male', 'Female'], [male_surv_prob,female_surv_prob], palette = ['#0066cc','#cc99ff'] )\n\nplt.title(\"Overall Proportion of those that Survived by Gender\")\nplt.ylabel(\"Probability\")\nplt.show()","153cff49":"\nf, axes = plt.subplots(1, 2, sharey=True, sharex=True)\nsns.barplot(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['Age'],\n            train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['counter'], \n            color = '#0066cc', alpha = 0.9, label = 'Male Survived', ax=axes[0] )\n\n\nsns.barplot(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['Age'],\n            train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['counter'], \n            color = '#cc99ff', alpha = 0.9, label = 'Female Survived', ax=axes[1] )\n\n\n\n# Fix axis labels so they are not overlapping, solution found here:\n# https:\/\/stackoverflow.com\/questions\/38809061\/remove-some-x-labels-with-seaborn\/38809632\nimport matplotlib.ticker as ticker\n\nax = plt.gca()\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=20))\n\n# Remove some of the plot lines\nsns.despine()\n\n\nfor ax in axes:\n    ax.set_ylabel(\"Probability of Survival\")\n    ax.set_xlabel(\"Age\")\n    ax.legend()\n\nplt.suptitle(\"Number of Passengers that Survived by Age and Gender \")\n\n\n\nplt.show()","e610411a":"train_gender_age_count = train[['Sex','Age','counter']].groupby(['Sex','Age']).count().reset_index(drop=False)\ntrain_gender_age_count['prob'] = train_gender_age_count['counter']\/sum(train_gender_age_count['counter'])\ntrain_gender_age_count.head()","45a34848":"male_surv_prob = sum(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['prob'])\nfemale_surv_prob = sum(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['prob'])\n\nmale_total_prob = sum(train_gender_age_count[train_gender_age_count['Sex']==\"male\"]['prob'])\nfemale_total_prob = sum(train_gender_age_count[train_gender_age_count['Sex']==\"female\"]['prob'])\n\n\n\n# Remove some of the plot lines\nsns.despine()\n\nsns.barplot( ['Male', 'Female'], [male_surv_prob,female_surv_prob], palette = ['#0066cc','#cc99ff'], alpha = 0.9 )\nsns.barplot( ['Male', 'Female'], [male_total_prob,female_total_prob], palette = ['#0066cc','#cc99ff'], alpha = 0.4 )\n\nplt.title(\"Overall Proportion of those that Survived by Gender Compared to the Total Populations\")\nplt.ylabel(\"Probability\")\nplt.show()","0691bdfa":"\nf, axes = plt.subplots(1, 2, sharey=True, sharex=True)\nsns.barplot(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['Age'],\n            train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"male\"]['counter'], \n            color = '#0066cc', alpha = 0.9, label = 'Male Survived', ax=axes[0] )\n\nsns.barplot(train_gender_age_count[train_gender_age_count['Sex']==\"male\"]['Age'],\n            train_gender_age_count[train_gender_age_count['Sex']==\"male\"]['counter'], \n            color = '#0066cc', alpha = 0.4, label = 'Male Total', ax=axes[0] )\n\n\nsns.barplot(train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['Age'],\n            train_gender_age_count_SURVIVED[train_gender_age_count_SURVIVED['Sex']==\"female\"]['counter'], \n            color = '#9966ff', alpha = 0.9, label = 'Female Survived', ax=axes[1] )\n\n\nsns.barplot(train_gender_age_count[train_gender_age_count['Sex']==\"female\"]['Age'],\n            train_gender_age_count[train_gender_age_count['Sex']==\"female\"]['counter'], \n            color = '#cc99ff', alpha = 0.4, label = 'Female Total', ax=axes[1] )\n\n\n# Fix axis labels so they are not overlapping, solution found here:\n# https:\/\/stackoverflow.com\/questions\/38809061\/remove-some-x-labels-with-seaborn\/38809632\nimport matplotlib.ticker as ticker\n\nax = plt.gca()\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=20))\n\n# Remove some of the plot lines\nsns.despine()\n\n\nfor ax in axes:\n    ax.set_ylabel(\"Probability of Survival\")\n    ax.set_xlabel(\"Age\")\n    ax.legend()\n\nplt.suptitle(\"Number of Passengers that Survived by Age and Gender \")\n\n\n\nplt.show()","dfa2b7cd":"train_gender_age_count.head(3)","1381d268":"train_gender_age_count_SURVIVED.head(3)","b612b834":"train_gender_age_count_2 = train_gender_age_count.merge(train_gender_age_count_SURVIVED[['Sex','Age','prob']], how = 'left', on = ['Sex','Age'])\ntrain_gender_age_count_2 = train_gender_age_count_2.fillna(0)\ntrain_gender_age_count_2.head(3)","96d01350":"prob_surv_given_age_gender = train_gender_age_count_2\n\nprob_surv_given_age_gender['prob'] = train_gender_age_count_2['prob_y']\/train_gender_age_count_2['prob_x']\nprob_surv_given_age_gender.head(3)","d80fcc5c":"\nf, axes = plt.subplots(1, 2, sharey=True, sharex=True)\nsns.barplot(prob_surv_given_age_gender[prob_surv_given_age_gender['Sex']==\"male\"]['Age'],\n            prob_surv_given_age_gender[prob_surv_given_age_gender['Sex']==\"male\"]['prob'], \n            color = '#0066cc', alpha = 0.9, label = 'Male Survived', ax=axes[0] )\n\nsns.barplot(prob_surv_given_age_gender[prob_surv_given_age_gender['Sex']==\"female\"]['Age'],\n            prob_surv_given_age_gender[prob_surv_given_age_gender['Sex']==\"female\"]['prob'], \n            color = '#9966ff', alpha = 0.9, label = 'Female Survived', ax=axes[1] )\n\n\n\n# Fix axis labels so they are not overlapping, solution found here:\n# https:\/\/stackoverflow.com\/questions\/38809061\/remove-some-x-labels-with-seaborn\/38809632\nimport matplotlib.ticker as ticker\n\nax = plt.gca()\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=20))\n\n# Remove some of the plot lines\nsns.despine()\n\n\nfor ax in axes:\n    ax.set_ylabel(\"Probability of Survival\")\n    ax.set_xlabel(\"Age\")\n    ax.legend()\n\nplt.suptitle(\"Probability that a Passenger Survived GIVEN Age and Gender by Conditional Probability\")\n\n\n\nplt.show()","5e1422c6":"pred_surv = prob_surv_given_age_gender[ prob_surv_given_age_gender['prob']>=0.5]\npred_died = prob_surv_given_age_gender[ prob_surv_given_age_gender['prob']<0.5]\n\npred_died.head()","659dd46d":"male_surv_ages = pred_surv[pred_surv['Sex']=='male']['Age']\nfemale_surv_ages = pred_surv[pred_surv['Sex']=='female']['Age']\nmale_surv_ages","6d57fdfa":"test.head()","d22e7524":"test['pred_1'] = np.where( (test['Sex']=='male') & (np.isin(test['Age'], male_surv_ages )), 1,\n                   np.where( (test['Sex']=='female') & (np.isin(test['Age'], female_surv_ages )), 1, 0     ))\n\ntest.head()","89e2bcca":"PassId = [\n892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,\n924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,\n957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,\n990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,\n1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,\n1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,\n1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,\n1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,\n1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,\n1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,\n1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,\n1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,\n1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,\n1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,\n1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309]\n\nsurv_label = [\n0,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,0,0,1,0,1,0,1,0,1,1,1,0,0,0,1,0,1,0,1,1,0,0,1,1,0,0,0,\n1,0,1,0,0,0,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,\n0,0,1,0,0,1,0,1,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,0,0,1,1,0,0,0,\n0,0,1,1,1,1,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,1,0,1,1,0,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,0,0,\n1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,\n0,1,1,0,1,0,0,1,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,1,0,\n1,0,1,1,0,0,0,1,0,1,0,1,0,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,0,1,\n1,0,0,1,0,0,1,0,0,1,0,0,1\n]\n\ntest_labelled = pd.DataFrame({'PassengerId':PassId,\n                              'Survived':surv_label})\n\ntest_labelled.head()                            ","557e7aa0":"test_labelled = test_labelled.merge(test[['PassengerId','pred_1']], how='left', on = 'PassengerId')\ntest_labelled.head(3)","0e1f6849":"# Export Predictions to csv for submission\nsubmission_1 = test_labelled[['PassengerId','pred_1']]\nsubmission_1.index = submission_1['PassengerId']\nsubmission_1 = submission_1[['pred_1']]\nsubmission_1.columns = ['Survived']\n#submission_1.to_csv('...directory...\/titanic\/pred_1.csv')","cf2c9dcb":"pred_1_err = sum(abs(test_labelled['Survived'] - test_labelled['pred_1']))\/len(test_labelled)\npred_1_acc = 1-pred_1_err\n\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_1_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_1_acc*100,2), \"%\" )\n","e2d7bcf3":"from sklearn.metrics import confusion_matrix\n\ntrue = test_labelled['Survived']\npred = test_labelled['pred_1']\n\ntn, fp, fn, tp = confusion_matrix(true, pred).ravel()\nprint(\"True Positive Count = \", tp)\nprint(\"False Positive Count = \", fp)\nprint(\"True Negative Count = \", tn)\nprint(\"False Negative Count = \", fn)","354dd01b":"pred_1_prec = tp\/(tp+fp)\npred_1_recall = tp\/(tp+fn)\n\nprint(\"Prediction 1 Precision = \", np.round(pred_1_prec,4))\nprint(\"Prediction 1 Recall = \", np.round(pred_1_recall,4))\n","cf54b7b3":"train.head()","67ac010a":"train['pred_1'] = np.where( (train['Sex']=='male') & (np.isin(train['Age'], male_surv_ages )), 1,\n                   np.where( (train['Sex']=='female') & (np.isin(train['Age'], female_surv_ages )), 1, 0     ))\ntrain.head()","757fae59":"pred_1_train_err = sum(abs(train['Survived'] - train['pred_1']))\/len(train)\npred_1_train_acc = 1-pred_1_train_err\n\nprint(\"The Training Error of our first prediction using conditional probability is:\", np.round(pred_1_train_err*100,2), \"%\" )\nprint(\"The Training Accuracy of our first prediction using conditional probability is:\", np.round(pred_1_train_acc*100,2), \"%\" )\n","67503b63":"train_overfit = train[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'counter']]\n\nALL_Count = train_overfit.groupby(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']).count().reset_index(drop=False)\nALL_Count['prob'] = ALL_Count['counter']\/sum(ALL_Count['counter'])\nALL_Count.head()","2670cd48":"surv_Count = train_overfit[train_overfit['Survived']==1].groupby(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']).count().reset_index(drop=False)\nsurv_Count['prob'] = surv_Count['counter']\/sum(ALL_Count['counter'])\nsurv_Count.head()","29d881cd":"ALL_Count = ALL_Count.merge(surv_Count, how='left', on=['Survived', 'Pclass',\t'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Cabin','Embarked'])\nALL_Count[ALL_Count['Survived']==1].head()","cc41b565":"ALL_Count_surv = ALL_Count[ALL_Count['Survived']==1]\nALL_Count_surv['prob_surv_given_ALL'] = ALL_Count_surv['prob_y']\/ALL_Count_surv['prob_x']\nALL_Count_surv.head()","e20ecc92":"ALL_Count_surv['prob_surv_given_ALL'].unique()","86052bf2":"test.head()","5e37a476":"test_overfit = test.merge(ALL_Count_surv[['Pclass',\t'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Cabin','Embarked','prob_surv_given_ALL']], \n                          how='left', on = [ 'Pclass',\t'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Cabin','Embarked'])\ntest_overfit = test_overfit.fillna(0)\ntest_overfit.columns = [ 'PassengerId', 'Pclass','Name','Sex', 'Age', 'SibSp', 'Parch','Ticket' ,'Fare','Cabin','Embarked', 'pred_1', 'pred_overfit']\ntest_overfit.head()","dbcfdac1":"test_labelled_2 = test_labelled.merge(test_overfit[['PassengerId','pred_overfit']], how='left', on = 'PassengerId')\ntest_labelled_2.head(3)","c68a9c47":"pred_overfit_err = sum(abs(test_labelled_2['Survived'] - test_labelled_2['pred_overfit']))\/len(test_labelled_2)\npred_overfit_acc = 1-pred_overfit_err\n\n\nprint(\"--.--.--.--.--.--.--.-- Pred 1 --.--.--.--.--.--.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_1_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_1_acc*100,2), \"%\" )\n\nprint(\"\")\n\nprint(\"--.--.--.--.--.--.--.-- Overfitting Prediction --.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_overfit_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_overfit_acc*100,2), \"%\" )\n","21d86534":"from matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 5\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = np.array(train[['Age','Fare']])\ny = train['Survived']\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\n\n# we create an instance of Neighbours Classifier and fit the data.\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\nclf.fit(X, y)\npred_2 = clf.predict(np.array(test[['Age','Fare']].dropna(subset=['Age','Fare'])))\npred_2[0:10]","7190046b":"test_no_na = test.dropna(subset=['Age','Fare'])\ntest_no_na['pred_2'] = pred_2","ec233b90":"sns.scatterplot(test_no_na['Age'], test_no_na['Fare'], hue = test_no_na['pred_2'])\n\n# Remove some of the plot lines\nsns.despine()\n\n# Graph Titles. \"\\n\" will create a new line in the text if needed\nplt.title(\"A Scatter Plot Showing the Age and Fare for each Passenger and the K-Nearest Neighbour Predicted Labels\" )\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\nplt.ylim(0,100)\nplt.show()","e52b5530":"test_labelled = test_labelled.merge(test_no_na[['PassengerId','pred_2']], how='left', on = 'PassengerId')\n# Set any missing values due to not having age or far to 0\ntest_labelled['pred_2'] = test_labelled['pred_2'].fillna(0)\ntest_labelled.head(3)","82e1073a":"pred_2_err = sum(abs(test_labelled['Survived'] - test_labelled['pred_2']))\/len(test_labelled)\npred_2_acc = 1-pred_2_err\n\n\nprint(\"--.--.--.--.--.--.--.-- Pred 1 --.--.--.--.--.--.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_1_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_1_acc*100,2), \"%\" )\n\nprint(\"\")\n\nprint(\"--.--.--.--.--.--.--.-- Pred 2 --.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_2_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_2_acc*100,2), \"%\" )\n","6a0b2e5e":"train.head()","dc861bd8":"train['Sex_2'] = pd.factorize(train['Sex'])[0]\ntrain['Cabin_2'] = pd.factorize(train['Cabin'])[0]\ntrain['Embarked_2'] = pd.factorize(train['Embarked'])[0]\n\ntrain['Pclass'] = train['Pclass'].astype(int)\n              \n\ntrain.head()","787eeaae":"test['Sex_2'] = pd.factorize(test['Sex'])[0]\ntest['Cabin_2'] = pd.factorize(test['Cabin'])[0]\ntest['Embarked_2'] = pd.factorize(test['Embarked'])[0]\n\ntest['Pclass'] = test['Pclass'].astype(int)\n    \n\ntrain.head()","2822d4df":"train[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']].dtypes","67abb289":"n_neighbors = 5\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = np.array(train[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']])\ny = train['Survived']\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\n\n# we create an instance of Neighbours Classifier and fit the data.\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\nclf.fit(X, y)\npred_3 = clf.predict(np.array(test[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']].dropna()))\npred_3[0:10]","7338c933":"test_no_na['pred_3'] = pred_3","bc6b1f71":"test_labelled = test_labelled.merge(test_no_na[['PassengerId','pred_3']], how='left', on = 'PassengerId')\n# Set any missing values due to not having age or far to 0\ntest_labelled['pred_3'] = test_labelled['pred_3'].fillna(0)\ntest_labelled.head(3)","5115efe6":"pred_3_err = sum(abs(test_labelled['Survived'] - test_labelled['pred_3']))\/len(test_labelled)\npred_3_acc = 1-pred_3_err\n\n\nprint(\"--.--.--.--.--.--.--.-- Pred 1 --.--.--.--.--.--.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_1_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_1_acc*100,2), \"%\" )\n\nprint(\"\")\n\nprint(\"--.--.--.--.--.--.--.-- Pred 2 --.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_2_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_2_acc*100,2), \"%\" )\n\nprint(\"\")\n\nprint(\"--.--.--.--.--.--.--.-- Pred 3 --.--.--.--.--.--.--.--\")\nprint(\"The Error of our first prediction using conditional probability is:\", np.round(pred_3_err*100,2), \"%\" )\nprint(\"The Accuracy of our first prediction using conditional probability is:\", np.round(pred_3_acc*100,2), \"%\" )\n\n","240f8580":"import time\n\nKNN_outputs = pd.DataFrame()\nfor i in range(1,10):\n    \n    start_time = time.time()\n\n    n_neighbors = 5*i\n\n    # we only take the first two features. We could avoid this ugly\n    # slicing by using a two-dim dataset\n    X = np.array(train[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']])\n    y = train['Survived']\n\n    h = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\n\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n    clf.fit(X, y)\n    pred_4 = clf.predict(np.array(test[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']].dropna()))\n    test_no_na['pred_4'] = pred_4\n    test_labelled_2 = test_labelled.merge(test_no_na[['PassengerId','pred_4']], how='left', on = 'PassengerId')\n    # Set any missing values due to not having age or far to 0\n    test_labelled_2['pred_4'] = test_labelled_2['pred_4'].fillna(0)\n\n    pred_4_err = sum(abs(test_labelled_2['Survived'] - test_labelled_2['pred_4']))\/len(test_labelled_2)\n    pred_4_acc = 1-pred_4_err\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    KNN_outputs = KNN_outputs.append(pd.DataFrame({'k':n_neighbors,\n                                                   'Accuracy':pred_4_acc,\n                                                   'Error':pred_4_err,\n                                                   'Time':time_taken}, index = [i]))\n    ","c864f587":"KNN_outputs.head()","00892d0d":"sns.lineplot(KNN_outputs['k'], KNN_outputs['Error'], label=\"error\")\n\nplt.title(\"Results of Increase k in K-NN\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Error\")\nplt.show()","7ca8a6c7":"sns.lineplot(KNN_outputs['k'], KNN_outputs['Time'], label=\"time\")\n\nplt.title(\"Results of Increase k in K-NN\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Time taken(s)\")\nplt.show()","bf8e1a9a":"import time\n\nKNN_outputs = pd.DataFrame()\nfor i in range(1,10):\n    \n    start_time = time.time()\n\n    n_neighbors = 50*i\n\n    # we only take the first two features. We could avoid this ugly\n    # slicing by using a two-dim dataset\n    X = np.array(train[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']])\n    y = train['Survived']\n\n    h = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\n\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n    clf.fit(X, y)\n    pred_4 = clf.predict(np.array(test[['Pclass','Sex_2','Age','SibSp','Parch','Fare','Cabin_2','Embarked_2']].dropna()))\n    test_no_na['pred_4'] = pred_4\n    test_labelled_2 = test_labelled.merge(test_no_na[['PassengerId','pred_4']], how='left', on = 'PassengerId')\n    # Set any missing values due to not having age or far to 0\n    test_labelled_2['pred_4'] = test_labelled_2['pred_4'].fillna(0)\n\n    pred_4_err = sum(abs(test_labelled_2['Survived'] - test_labelled_2['pred_4']))\/len(test_labelled_2)\n    pred_4_acc = 1-pred_4_err\n    \n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    KNN_outputs = KNN_outputs.append(pd.DataFrame({'k':n_neighbors,\n                                                   'Accuracy':pred_4_acc,\n                                                   'Error':pred_4_err,\n                                                   'Time':time_taken}, index = [i]))\n    ","32810250":"sns.lineplot(KNN_outputs['k'], KNN_outputs['Error'], label=\"error\")\n\nplt.title(\"Results of Increase k in K-NN\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Error\")\nplt.show()","2914726f":"## Surivival Rates Visually Analysed\n\nAlthough useful, we have yet to create and graphs that would help us deduce whether the person likely survived or not.\n\nIf we followed the famous saying 'Women and children first!' then it woul lead us to believe that priority was given to saving women and children over men.\n\n![Women and Children First!](http:\/\/sciencenordic.com\/sites\/default\/files\/imagecache\/300x\/Titanic-New_York_Herald_front_page_None_0.jpeg)\n\nTo do this, we will use the pandas function '.groupby' to count the number of passengers in each category. First, we introduce an arbitrary column all with values 1 denoted 'counter'. This is not required but makes it easier to follow and use the groupby function.\n\nNext, we subset the dataset for the sex, age and counter column and group these by sex and age. Doing so, whilst applying the count attribute, means our index become the sex and age features (1 row for each unique combination) and each appearance of our counter column for these sub groups is counted. Because we used the counter column which has only values of 1 for each row, we could also use 'sum' instead to return the same values.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.groupby.html","701d28ea":"## Evaluating Parameters\n\nTo evaluate the main parameter, *k*, we will try a range of possible inputs and compare the accuracy output. Furthermore, we will track the time taken to do this for the evaluation.\n\nTo improve the method for evaluating the parameters, we will utilise loops and some neat little tricks that are mentioned in another post that can be found here:\n\nhttps:\/\/towardsdatascience.com\/3-tips-to-improving-your-data-science-workflow-71a6fb8e6f19\n\n\n","8caa1d76":"These graphs have also been combined into violin plots that may be useful but typically box plots are sufficient and easiest to compare.\n\nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html\n","426375a1":"**This is a surprisingly good result and shows, in some cases, even with limited data we can achieve good accuracy results.**\n\n\n### Confusion Matrix Analysis\n\nSomething further to consider is to analyse the number of type 1 or type 2 errors.\n\nThe table in the image below summarises what these are, alternatively you can consider them as true positive\/true negative and false positive\/false negative notation also shown.\n\n![type errors](https:\/\/www.researchgate.net\/profile\/William_Anderegg\/publication\/268035363\/figure\/fig1\/AS:392140141154312@1470504903728\/Graphical-representation-of-type-1-and-type-2-errors.png)\n\n![type errors 2](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/evaluate\/confusion_matrix_files\/confusion_matrix_1.png)\n\n\nWe typically use the latter and this table is known as a **Confusion Matrix**.\n\n\nIn our example, we are making a prediction as to whether a person survived or didnt, therefore, we have the following prediction scenarios:\n\n- Prediction: Survived --> Actual: Survived = True Positive\n- Prediction: Survived --> Actual: Died = False Negative\n- Prediction: Died --> Actual: Died = True Negative\n- Prediction: Died --> Actual: Survived = False Positive\n\nIt helps when understanding to use this example as we can associate surviving with being positive and dying as being negative. \n\n**In many cases, False Negatives are far worse!**\n\nFor example, in medicine predicting someone will survive and then they don't is usually considered worse than them miracurously surviving what was thought to be a fatal illness. \n\nOr another example is spam filters, one or two that spam messages that get through is not the end of the world but if we filter out a genuine message and the person misses this they may be angry that valid information was removed from their inbox.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Type_I_and_type_II_errors\n\nhttps:\/\/en.wikipedia.org\/wiki\/Confusion_matrix\n\n\nWe could find these counts on our own but it is sometimes easier to utilise the analytic packages available. For example, scikit learn has a package that create the matrix automatically.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html","2d0cf209":"prob_x is from the original count and prob_y is from the count of those that survived, therefore we simply find the conditional probability bt eh following:","1e080807":"We also ulilise subplots to compare the graphs side by side.\n\nhttps:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.subplot.html","2b67c5a4":"## What is Overfitting?\n\nWe have taken the data a already being split into a train\/test set and have mentioned that we need to keep the test set 'hidden' from our algorithm but why?\n\nIf you collect any data from an original source it will not be seperated and it is up to you to split into a test and train set. We then develop any algorithms on the training subset (often a large proportion e.g. 80%) and once we think that the predictions are ready, apply it to the test set (that is a smaller proportion e.g. 20%).\n\nWhen we are training the algorithm, we would like this to become generlisable (i.e. can be applied to new informaiton that it has never seen before) and if we force our prediction to achieve too higher accuracy in training it will likely only be good for those inputs.\n\nFor example, if we fit a linear regression line to natural data, we do not expect the points to fall directly on the line but expect their to be some slight gaps from the line to the points. Ideally these gaps are small enough that we would be happy that we can get a good idea of y if we know x. \n\n**The core idea of predicting is that we can try an x value we have never seen before and return a y value from this estimation.** \n\nIf, instead of fitting a line with some errors in training, we forced the line through all the points to achieve a very high accuracy, we see that although the results will be good for the data already seen, it will be much much worse for any other x values we introduced.\n\n![overfitting](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/68\/Overfitted_Data.png\/300px-Overfitted_Data.png)\n\n\nWe can check our training accuracy by predicting the survival of those in the training set. We find this has an accuracy of 82.07% which is high but not so high as to imply that we are overfitting.","6f1ac96f":"**Visual Interpretation: This makes it clear that class 1 is, in general, the most expensive and class 3 is the least. Furthermore, the cost in class one is so high that this makes it hard to visually compare the other classes**\n\nWe can therefore map these numbers to a more traditional categorical system of \"First\" \"Second\" and \"Third\" class labels.\n\nTo do this, we use the numpy function \"np.where()\" that works exactly the same as Excel's if-else statment. Because we have multiple classes to map we nest the if statements so that we correctly map each passenger \n\n\nin Excel where we have the form of if-else statements to be: if( statement = TRUE, x, *else* if( statement = TRUE, x *else* if(........\n\n\n    Pclass_2 = if( Pclass = 1, \"First\" , if( Pclass = 2, \"Second\", if(Pclass = 3, \"Third\", \"error\" )))\n\nin Python:\n\n    Pclass_2 = np.where( data['Pclass'] = 1, \"First\" , np.where( data['Pclass'] = 2, \"Second\", np.where(data['Pclass'] = 3, \"Third\", \"error\" )))\n    \n    \n    \n\n","61b6599e":"## Visual Analysis\n\nNow that we are able to reference individual columns and rows, we can start with some basic plots.\n\nFollowing some of the example graph types, we can compare some of the features directly.\n\nhttps:\/\/seaborn.pydata.org\/introduction.html\n\n\n**Note: It is important to label your graphs with a main title and axis titles.**\n\nIt may also be useful to calculate some summary information.","4c39ad14":"By forcing our prediction to go through all features, we find that those the combinations shown will have a guaranteed chance of surviving and those that dont are guaranteed to die. But does this actually work?\n\n","7c9fbcca":"Alternatively, we can reference columns by their id number, unlike rows this is not clearly shown but they can be easily counted.\n\nTo do this we use the \".iloc[row_index , col_index ]\" function:\n\nIf we want all rows or columns we simply use a colon \":\" instead of a number.","25c4f87f":"To preview the data, we can simply use the functions \".head()\" and \".tail()\" to show the first and last rows respectively.\n\nWe can also use the \".describe()\" function to provide an initial summary of each of the features.","f6160ca7":"# Part 1: Understanding Data with Exploratory Analysis (EDA) and Pre-Processing\n\nA common first step in any analysis is to explore the data and get a better understanding of the features available. \n\n\n","91592e2d":"## Prediction Method: Conditional Probability\n\nFormally defined, the conditional probability is the probability of event B occuring GIVEN A occured and is calculated by:\n\n\\begin{equation}\n    P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\end{equation}\n\ni.e. the probability of A and B occuring divided by the probability of B\n\n\nFor our first prediction, we will simply predict the probability on Age and Gender by the following:\n\n\\begin{equation}\n    P(Survived| Age \\ \\& \\ Gender) = \\frac{P(Survived \\cap (Age \\ \\& \\ Gender))}{P(Age \\ \\& \\ Gender)}\n\\end{equation}","64c7ff46":"---\n\n# Part 3: Understanding Prediction Overfitting\n","3894fbac":"To make this easier to use, I generally use '.reset_index()' to return the index to a list of numbers and the groups as columns that can be referenced.\n\n(drop=False) ensures we keep the columns, if true they are removed entirely.","9d25786c":"The error can be calculated by finding the number of times our prediction differed from the actual value. In our case, because these are binary (1 or 0), we can simply take one away from the other and find the absolute (i.e. abs of 1-0 = 1 and 0-1 = -1) and sum these to find the occurances of miss classifications. ","0f2f6df7":"**Visual Interpretation: The average age decreases as the class id increases**\n\nIt appears that there is an underlying trend causing this, it seems likely that class one is more expensive and is therefore only affordable by those who are older.\n","2c15c436":"####\u00a0Aim: To predict whether the passengers included in the test set survived or not given the features included in data\n\nWith this aim in mind, we can start analysing the data to try to graphically observe any underlying trends that may be useful in our predictions.\n\nBefore we do this however, it may be important to introduce some basic methods for manipulating and pre-processing the data.\n\n","f9488647":"We can develop this further and introduce the 'hue' parameter which splits the data so that we can compare two categorical features together. In this case, we investigate the age of the passengers by the class label and gender.","794a591f":"#### Summary Statistics\nAs with titles, it is often best to clearly explain what the output shown is. We can do this with some basic print functions where text and variables can be seperated by commas.\n\nWe can calculate the mean, max or min with panda's functions or utilise numpy.","d01fef91":"# Conclusion\n\nThis may seem negative, that we weren't able to improve our results from the base prediction that was so simple but this is not unusual in data science. If it was easy to achieve high prediction results then it would be considered a skill to do so. It seems that K-NN was not the best method to use here but it was worth investigating.\n\nThere are many method to use that could improve our results but it is important to remember the basic skills, as well as the importance of not overfitting, demonstrated here when doing so.\n\n\nI hope you find this notebook useful for your learning.\n\nThanks\n\nPhil","568a9f23":"**Subsetting**\n\nWe can utilise the column and row reference to subset the data as required. To do this, we use the following functions where we use the \"&\" to denote AND and \"|\" to denote OR in subsetting notation.","9d84f687":"**Visual Interpretation: The proportion of these that survived was much better for women than men. Furthermore, in the male sub plot we see that those that did survive were typically younger and no male over the age of 50 survived.**","32e38f15":"---\n# Part 2: Base Predictions","19ffc04b":"These can be further analysed using combined metrix precision and recall:\n\n${\\displaystyle {\\text{Precision}}={\\frac {tp}{tp+fp}}\\,}$\n\n${\\displaystyle {\\text{Recall}}={\\frac {tp}{tp+fn}}\\,} $\n\n\n![Prec and Recall](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/700px-Precisionrecall.svg.png)\n\nhttps:\/\/en.wikipedia.org\/wiki\/Precision_and_recall","c8d42ae6":"### Forcing Overfitting\n\nTo show the impact of overfitting, we will attempt to force a very high accuracy for the training set and demonstrate that this makes it difficult to accurately predit new data form the test set.\n","52134168":"## Prediction Method: K-Nearest Neighbour\n\nK-nearest neighbour is a unsupervised method that attempts to clusters data points that could indicate the prediction labels.\n\nWe can apply this by following the basic examples provided in the sklearn package.\n\nWe first attempt this with just two features: age and fare.\n\n\nhttps:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier","44b143e2":"**Subsetting the data may be used to remove outliers (extreme values) but we will not utilise this fully for now. However, it will become useful later when we want to analyse specific sub groups of passengers.**","46d51b1e":"If we want to check this, we should submit these predictions to the competition submission. However, to keep this contained within the notebook, I have manually sourced the correct labels from the original data source and cross referenced these to the given IDs.\n\n**NOTE: Do not use these ever when building your predictions, the test set is designed to be hidden so that results are generalisable and do not simply overfit the data it is provided. In other words, if we used these then we could learn to make very accurate predictions in this case but if new data presented itself then we would do poorly on data we hadn't seen before. The main aim of any machine learning prediction is to make it generlisable, not accurate.**","b9093970":"**Interpretation of Results: This is slightly better but still worse than our original prediction. However, we have some paramters that can be changed and so perhaps varying these will improve the results further.**","643a1df9":"**Visual Interpretation: This scatter clearly shows that there is no correlation between the passenger's id and their age.**\n\nWe can also create variables for the summary statistics and choose sensible names for each. \n\nNote that we cannot simply use the names 'min','mean' and 'max' as these are already used as default functions in Python and will cause issues. \n\nAlso we will likely calculate the min, mean and max values of different variables so is useful to seperate them by the feature.\n\n**We can also introduce some basic styling options for plots, for example, we can change the color of the bars.**\n\n    1. Set all bars to the same color\n    2. Use palette styling to set a range of colors https:\/\/seaborn.pydata.org\/tutorial\/color_palettes.html\n    3. Set each bar manually and use the hex color codes https:\/\/www.w3schools.com\/colors\/colors_picker.asp","45f04b3b":"**Interpretation of Results: It seems that the error continues to reduce and hits the lowest value at k=350 and then rishes sharply after. However, these results are still worse than our original prediction.**","c309d696":"We now name this output something sensible and calculate the probability of being in each sub group.\n\nWe can then compare the probability of survival for men and women overall and by ages.","f52e7131":"To extend this for more features, we need to convert categorical string value into number classifications.","2469bc74":"We can combine these to produce a simple scatter plot of the passenger id against age. We are also able to include our summary statistics in the title but, unlike with the print function, we are required to use some basic string manipulation (convert output to string\/text and simply add this onto the title).","29e8b722":"We can also round these values if needed, for example the mean can be rounded to 3 decimal places by the following numpy function:\n\nhttps:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.round_.html","9642152d":"**Visual Interpretation: From this, we can see that those in the third class are generally between the ages 18 to 45 (approximately) and mostly paid the same price for their ticket whilst the other classes are more varied. In general, there is a trend of those older to have better class tickets but is not conrete enough to make any assumptions with.**\n\n\nWe can analyse this further with the use of of histograms, these show that the cost of the tickets in first class are much more varied than those in first or second. ","2f337f72":"**Interpretation of Results: It seems that the error reduces as we increase k. Furthermore, the time taken varies slightly but is still very small.**\n    \nTherefore, we increase our evaluation for further k values.","ba945114":"Do do this, we merge the groupby tables so that we have a column denoting the probability of being in the age\/gender group and another for the probability of being in the age\/gender group and surviving.\n\nMore examples on how to merge pandas dataframe are shown below but in short, we use the age\/gender count table and LEFT join the age\/gender and survived table on the keys age\/gender.\n\nWhen we use left join, any age\/gender groups that don't have a surviver will produce a nan value, therefore we replace these simply with 0 so we can still using them in the conditional probability calculation.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.merge.html","43dab802":"**Visual Interpretation: We can see from these that the mimum ages are the same but males were generally odler than females and the maximum age for males is much higher than that of females.**\n\nWe can also overlay the box plot with a swarmplot the shows the distribution at each age to improve the comparison. For example, with this we can clearly see that there were more men on board overall and they were slightly older which explains why the median is higher.\n\n**Also we introduce 'alpha' graphical input which fades the color of the plot so that when multiple plots are overalayed the results are still visible.**","37ae9a89":"## Comparing Trends Across Categorical Features\n\nThe visual analysis so far has provided little insight into the data, therefore it may be practical to start considering the trends across categorical features. For example, does the age of the passengers vary across genders?\n\nThe easiest way to compare this is to utilise box plots.\n\nWiki: https:\/\/en.wikipedia.org\/wiki\/Box_plot\n\nSeaborn Plotting: https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html\n\nThese show the min, lower quartile, median, upper quartile and maxmimum values in one plot and can be easily seperated by the genders.\n\nI also generally change to color to match the society standards when comparing genders to make it easier for the viewer to denote which is which.\n\n","cf8e9dc3":"# A Complete Introduction to Data Science and Machine Learning in Python Notebooks\n\nPhilip Osborne\n\nJan 2019\n\n---\n# Overview\n\n### In part 1, we will show the basic methods required to explore data in Python.\n\nWe will therefore demonstrate how to:\n\n1. Import the data\n2. Preview the data\n3. Summarise the features statistically\n4. How to reference features (columns) and elements (rows)\n5. How to subset the data by conditions\n6. Dealing with missing values\n7. Basic plots\n8. Visually analyse across mulitple features\n9. How to draw conclusions that lead to assumptions\n10. Demonstrate visually how conditional probability works\n11. Develop a basic prediction using condiional probability on these assumptions\n\n### In part 2 we will then test whether this prediction is an effective indication of whether or not the passenger survived.\n\nWe find that the base model, even with very little information, provides a decent prediction estimate and achieved 73.5% accuracy when applied to the test set. We then introduce more detailed evaluation metrics including a confusion matrix, precision and recall.  \n\n### In part 3, we will explain what overfitting is and demonstrate why it is important to use a train\/test set and carfully consider how the prediction is trained.\n\n### In part 3, we introduce a more advanced prediction method, K Nearest Neighbors, and demonstrate some of the challenges associated with applying machine learning. \n\nThis includes:\n\n1. Dealing with missing inputs\/outputs\n2. Increasing the range of features used\n3. Varying parameters efficiently\n4. Analysing parameter results\n\nWe find that the K-NN method will be unlikely to improve the results obtained from the base prediction and demonstrates that sometimes a simple assumption can outperform more advanced methods.\n\n\n---","13337c3a":"Likewise, we notice that there are nan values in the cabin and embarked feature. However, unlike with age it would not be ideal to simply remove these rows as this feature is likely not the be as important as we have the passenger class that denotes the quality of their cabin. \n\nTherefore, we instead simply map this to a 'unknown' label using pandas '.fillna()' function.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.0\/generated\/pandas.DataFrame.fillna.html","ef231b32":"We are unable to use any inputs that have 'na' values so will need to remove these from consideration. Also, we cannot predict for test set rows that also have 'na' values, therefore, as before, we simply default these to 0 and aknowledge that we cannot make a prediction for these.","3ff52520":"We can find all the rows with a 'nan' value for age with the 'np.isnan()' function and remove these using panda's '.dropna()' function.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html","0b9fcb3c":"## Indexing and Subsetting the Data\n\n\n**Indexing**\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html\n\nThe data are in the format of Pandas dataframes, therefore to reference a column we can simply refer to its name. Likewise, to extract a row we can refer to the row's id (where in Pandas the first row starts at 0). \n\nThis can either be done inside square brackets of by referencing the column as an attribute. The former is typically better as it means we can eaisly extend this for multiple columns as shown.","2fc08762":"## Dealing with Missing Values\n\nFrom the initial describe function (repeated here for clarity), it appears that some of the age values are missing. We can investigate this further by first finding all unique values of this feature.","04936e90":"---\n# Part 4: Introducing More Advanced Methods for Improved Predictions","6879c0b1":"**Visual Interpretation: We find that the survival probability is higher for women overall and across most ages as expected.**\n\nYou may think it sufficient here to say that you can conclusively deduce that women were somewhat more likely to survive than men. However, there is more to this than what our current graphs show. In our previous visual analysis we found that there were more male passengers than female (approx. 450 to 250). \n\nTherefore, if the overall number of those that survived is higher for women AND there are fewer women in the population, then the probability of surviving given you were a women is even higher.\n\n**This is known as conditional probability and we can use this to form our first predictions.**\n\nhttps:\/\/en.wikipedia.org\/wiki\/Conditioning_(probability)\n\n\nTo further explain what this means, we will go back and find the total number of passengers that were in each age and gender group, irrelevent of whether they survived or not. We then compare this original population to those that did survive and find that the proportion of these that survived was much better for women than men. Furthermore, in the male sub plot we see that those that did survive were typically younger and no male over the age of 50 survived.","f0213595":"**Visual Interpretation: We clearly see that our assumption holds true; young males and females were far more likely to survive than other aged males.**\n\n\n###\u00a0Conditional Probability Prediction\n\nWith these results, we can make our predictions:\n    \n    If the probability of survival, given their age and gender, was less than 0.5 (50%) then we predict they died, otherwise they likely survived.","ab404673":"**By forcing our probability calculations to consider too many classes we reduce the generlisability of the prediction. As shown, although our training accuracy improved, the accuracy when applied the test set is reduced significantly.**","f9af62fc":"**Interpretation of Results: It seems that this method is worse than our first prediction, therefore, we consider what can be achieved if we extend the features used.**","592563ce":"**Visual Interpretation: As noted preiviously, some of the fares are so high it makes it hard to compare the lower cost tickets.**\n\nTherefore, we can limit the y axis so that these are easier to view:"}}