{"cell_type":{"d25dc056":"code","1a6d5173":"code","d998ba40":"code","0152427b":"code","6c0eadbf":"code","4aef0938":"code","24b63118":"code","ff396d6a":"code","d48d0039":"code","dda168b9":"code","78814770":"code","768c6ac9":"code","1ec382e5":"code","dbec1ea8":"code","b2b2d530":"code","545539d8":"code","d17b301c":"code","d09080ff":"code","8eed4488":"code","af5ef7c8":"code","ef0a879c":"code","3b6dd405":"code","097bc6a0":"code","490a3d74":"code","ac59a61c":"code","077d048f":"code","3a7fb0b4":"code","1184fd08":"code","17ad7767":"code","12e8feeb":"code","4ce5d274":"code","3b87c9a6":"markdown","716bc942":"markdown","61db6d5a":"markdown","d1211914":"markdown","f63f0e75":"markdown","6144c6a5":"markdown","7acf554a":"markdown","42dae190":"markdown","78273c2f":"markdown","a5eedc36":"markdown","cf82926c":"markdown","f9085f7f":"markdown","11b55d5b":"markdown","b65bc3f2":"markdown","114441d4":"markdown","c3e391c8":"markdown","29f8dbd6":"markdown","bc2c0804":"markdown","2e8b5d79":"markdown","5ef826e5":"markdown","c2a27407":"markdown","855f14bf":"markdown","a498e7ae":"markdown","18b7ab4a":"markdown","921d32eb":"markdown","47c7deaf":"markdown","c86c4995":"markdown","dd768d98":"markdown","43436c8b":"markdown","8223cb1b":"markdown","c2ba135d":"markdown","fc20634d":"markdown","c055521d":"markdown","70239d77":"markdown","540f2365":"markdown","fbbe7f63":"markdown","4e82531b":"markdown","1eb8e00c":"markdown","306317c5":"markdown","b0af51ae":"markdown","368333c6":"markdown","04976413":"markdown","34b9fa0a":"markdown","85ba8e1f":"markdown","57449dcc":"markdown","0c9bd2ed":"markdown","8dc16d30":"markdown","71c78eea":"markdown"},"source":{"d25dc056":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Sep  1 15:04:31 2021\n\n@author: Rony's PC\n\"\"\"\n","1a6d5173":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\nimport seaborn as sns\nimport missingno as msno","d998ba40":"! pip install openpyxl","0152427b":"x_features = pd.read_excel(\"..\/input\/datas-features\/Features data set.xlsx\") # features \ny_sales = pd.read_excel('..\/input\/datas-features\/sales data-set.xlsx') # labels\nz_stores = pd.read_csv(\"..\/input\/datas-features\/stores data-set.csv\") # stores","6c0eadbf":"print(x_features.head())\nprint(y_sales.head())\nprint(z_stores.head())\nprint('-'*10)\nprint('\\n')","4aef0938":"# First - Optimize the sales via store in a unique date with all Deparments to be preper\ny_sales['Date'] = pd.to_datetime(y_sales['Date'])\n# optimize the sales via store in a unique date with all Deparments to be preper \ny_sales = y_sales.groupby(['Store', 'Date']).sum()\n# Merging of x_features and z_stores\nx_features = pd.merge(x_features, z_stores)\n\n\n# Secondly I  transform categorical 'Type' column' feature needed to be number.\ndef Type2num(x):\n     if x == 'A':\n         return 0\n     if x == 'B':\n         return 1\n     if x == 'C':\n         return 2\nx_features['Type'] = x_features['Type'].apply(Type2num) \n\n\n\nx_features['Date'] = pd.to_datetime(x_features['Date'])\nx_features = x_features.groupby(['Store', 'Date']).sum()\nprint(x_features.dtypes)\nprint(y_sales.dtypes)\nprint(z_stores.dtypes)","24b63118":"x_features['Type'].value_counts() #categorical variable\nx_features.Type.value_counts().plot.barh();","ff396d6a":"Combined_table = pd.merge(x_features, y_sales['Weekly_Sales'], how='inner', right_index=True, left_index=True)\nCombined_table.isna().sum()\nCombined_table.info\nCombined_table['Weekly_Sales'].describe()\n\n\n# Very well... It seems that your minimum price is larger than zero. Excellent","d48d0039":"Combined_table['M^2_per_profit'] = Combined_table['Weekly_Sales']\/Combined_table['Size']\nCombind_graf= Combined_table.copy()","dda168b9":"Combined_table.isnull().sum()\nCombined_table.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\nmsno.bar(Combined_table)   # checking for null ? by plotting.","78814770":"print(\"Skewness: %f\" % Combined_table['Weekly_Sales'].skew())\nprint(\"Kurtosis: %f\" % Combined_table['Weekly_Sales'].kurt())","768c6ac9":"# let see how Weekly_Sales corellative to other features ?\n# sns.distplot(Combined_table['Weekly_Sales'])\nsns.displot(data = Combined_table, x = 'Weekly_Sales', kde=True)","1ec382e5":"# first convert the store from index into a column\n# Takea a look over each sotre outlayers via \"Weekly_Sales'.\nCombined_table = Combined_table.reset_index(level=0)\nvar = 'Store'\ndata = pd.concat([Combined_table['Weekly_Sales'], Combined_table[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Weekly_Sales\", data=data)\nfig.axis(ymin = 180000, ymax = 4000000)","dbec1ea8":"# Let display the relationship between two numerical variables\n# For any combination features. \nsns.set()\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']\nsns_plot = sns.pairplot(Combind_graf[cols].sample(100), height = 2.5) \nplt.show()","b2b2d530":"# I try to see if it's any corralation between the features\nCombind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()\n","545539d8":"# In the following Matrix we will see how mach the features are confusing?.\n# As we see not all features are not corelative. \ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n\n","d17b301c":"# Since I have two Prameters with correlasition, I drop one of them - 'MarkDown1', Why ?\n# To prevent  Linkage featurs that are corelatived.\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()\n","d09080ff":"Combined_table = Combined_table.drop(['MarkDown1'], axis = 1)","8eed4488":"print(\"Dats shape = {}\".format(Combined_table.shape))\nprint()\nprint(\"Lets see some feature:\")\nprint(Combined_table[1:10])","af5ef7c8":"# I try to see if it's any corralation between those tow features\nCombind_graf[['Type', 'Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","ef0a879c":"# I try to see if it's any corralation between 'Type' and 'Weekly_Sales'\nCombind_graf[['Type', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","3b6dd405":"# I try to see if it's any correlation between 'Size' and 'Weekly_Sales'\nCombind_graf[['Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","097bc6a0":"# I try to see if it's any corralation between 'M^2_per_profit' and 'Weekly_Sales'\nCombind_graf[['M^2_per_profit', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","490a3d74":"# Till nuw what we saw ? I saw that 'M2^_per_profit' feature is in correlation with \n#  'Type'----> 40% and 'Size'----> 30%. 'M2^_per_profit' feature was create to check profit.\n# Once I did my exploration with 'M2^_per_profit' and it's artificial feature I need drop it.\n# Reducing features revoke dimensional Data Curse problem.\n# This feature 'M2^_per_profit' could be predicable like 'Size' or 'Type' but both of them are better.\nCombined_table = Combined_table.drop(['M^2_per_profit'], axis=1)\n# Let plot it.\n\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales', 'Type', 'Size']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","ac59a61c":"# Definition of Y (label) and X (inputs)\n# Define label for the new merge table: \"combined_table\"\n# The 'Weekly_Sales' is Indexial so it dosn't take as a label\ny = Combined_table['Weekly_Sales']\n# Define features for the new merge table: \"combined_table\"\nx = Combined_table.drop(['Weekly_Sales'], axis=1)\n","077d048f":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\ny.shape\ny = y.values.reshape(6435, 1)\ny_scaled = scaler.fit_transform(y)\nx.head()\nx.tail()","3a7fb0b4":"# we need to take x_scaled after being notmalized\n# In the first step we will split the data in training and remaining dataset\nx_train, x_valid, y_train, y_valid = train_test_split(x_scaled, y, train_size=0.80, random_state=42)\n","1184fd08":"# Now since we want the valid and test size to be equal (10% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nx_test, x_valid, y_test, y_valid = train_test_split(x_scaled,y, test_size=0.5)\n","17ad7767":"Test_Data = (x_test, y_test)\nprint('-'*10)\nprint('\\n')\nprint(x_train.shape), print(y_train.shape)\nprint(x_valid.shape), print(y_valid.shape)\nprint(x_test.shape), print(y_test.shape)\nprint('-'*10)\nprint('\\n')","12e8feeb":"random_state = 2\nclassifiers = []\n\n\nrandom_forest = RandomForestRegressor() \nrandom_forest.fit(x_train, y_train.ravel())\ny_pred = random_forest.predict(x_test) \n\nrandom_forest_tuning = RandomForestRegressor(random_state = random_state)\nparam_grid = {'n_estimators': [100, 200, 500],'max_features': ['auto', 'sqrt', 'log2'],'max_depth' : [4,5,6,7,8],'criterion' :['mse', 'mae']}\n\n\nGSCV = GridSearchCV(estimator=random_forest_tuning, param_grid=param_grid, cv = 3)\n# By grid serch let look wich are the best choice to be taken ?.\nGSCV.fit(x_train, y_train.ravel())\nGSCV.best_params_ \n\nrandom_forest = RandomForestRegressor(random_state = random_state)\nrandom_forest.fit(x_train, y_train.ravel())\ny_pred_random = random_forest.predict(x_test)\nprint('MAE: ', mean_absolute_error(y_test, y_pred))\nprint('MSE: ', mean_squared_error(y_test, y_pred))\n\n\nrandom_forest_out_of_bag = RandomForestRegressor(oob_score=True)\nrandom_forest_out_of_bag.fit(x_train, y_train.ravel())\nprint('Random forest score')\nprint(random_forest_out_of_bag.oob_score_) ","4ce5d274":"x_train, y_train = make_regression(n_features = 12, n_informative = 2, random_state = 0, shuffle=False)\nregr = AdaBoostRegressor(random_state = 0, n_estimators = 100)\n\n# Fit regression model\nregr.fit(x_train, y_train)\n\n# Predict\ny_pred_boost = regr.predict(x_test)\n\nprint('Adaboost score')\n# score\nregr.score(x_test, y_test)","3b87c9a6":"**6.** Feature engineering\n   -------------------","716bc942":"**7.2 splitting the Datas to train and test** ","61db6d5a":"**7.** Modeling\n----------","d1211914":"**2.4 Merging all Data**\n","f63f0e75":"**Random Forest\nAre an ensemble learning method for classification, regression. In a Random Forest, each tree has an equal vote on the\nfinal decision. In a Random Forest each decision tree is made independently of the others.\nFor regression tasks, the mean or average prediction of the individual trees is returned.\nRandom forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees.\nHowever, data characteristics can affect their performance.**","6144c6a5":"**4.2 Plotting Confusional Correlation matrix with no features correlative.**","7acf554a":"**1.** Introduction\n   -------------","42dae190":"**3.3 scatterplot - Data Visualization**","78273c2f":"# **5. Core business analysis and inquery by plotting.**","a5eedc36":"**This is my second kernel at Kaggle. I choosed economic data tabular which\nis a good way to introduce feature engineering and ensemble modeling.\nFirstly, I will display some feature analyses than i will focus on the feature\nengineering. Last part concerns modeling and predicting sales marketing as\nusing an voting procedure. As an economic label problem, I prefer to do a regression model Random forest Grid Search with Cross Validation.\n\nThis script follows three main parts:\n\n    Feature analysis\n    Feature engineering\n    Modeling**","cf82926c":"**5.1.1**","f9085f7f":"As an inner mean - marge just the common Datas.","11b55d5b":"**2.5 inquery by: Skewness @ skew**","b65bc3f2":"**7.1 preprocessing normalization values between 0 and 1**","114441d4":"**3.2 Inquery of outlier by box plot store \/ Weekly_Sales**","c3e391c8":"**7.4** Random forest\n   --------------","29f8dbd6":"In other way plotting all features again with 'Combind_graf' no relations between all of them.","bc2c0804":"By inquiring with \"Boxplot\" I see that all the features are outlier.","2e8b5d79":"**7.3 splitting the test to test and val**","5ef826e5":"**4.** Feature analysis\n   ----------------","c2a27407":"Conclusion: Finally The linckage is no so big.\n-----------\nAfter checking the accuracy with and without 'MarkDown1' the different seem to be small.","855f14bf":"**4.1 Plotting Confusional Correlation matrix between numerical values.**","a498e7ae":"The groupby drop all missing rows","18b7ab4a":"**5.2 Drop unuse feature**","921d32eb":"**2.1 Load data**","47c7deaf":"# **Else let run another model as a bonus**","c86c4995":"I see that feature \"Syze\", 'type' and M^2 per profit could and mostlyy the two first could be predictable for sales but all the features together can be predictable that no growing is expectable. Else Random Forest show accuracy of around 94%.","dd768d98":"**3.** Inquery by Plotting \n   ---------------------","43436c8b":"**conclusion**\n--------------","8223cb1b":"**2.2 Inquering all Data**","c2ba135d":"We could see above that 'Size' store and 'Type' store could be good predictable \nfor business growing marketing. I need to see if those two features could be \nimplemented with deep learning model. Finally as showing plotting above \n'M^2_per_profit' have no good correlation with other features, and along \nthe time the profit per m^2 get down. So m^2 and can not pridictable. To be on \nthe safe side I take correlasition Pearson Metric as another referance bias the \nRandom forest ot Adaboost\n model.","fc20634d":"**5.1.3**","c055521d":"**5.1.2**","70239d77":"**2.4.1 let Calculating profitability per square meter**","540f2365":"**2.3.1 Plotting 'Type' categorical variable above**","fbbe7f63":"**2.5.2 Have appreciable positive kurt.**","4e82531b":"**2.4.2 Inquery by Plotting isnull ?**","1eb8e00c":"**3.4 Plotting all features via the label (prediction of sales)**","306317c5":"**2.3 Groupby working**\n","b0af51ae":"After looking the data we have a problem with missing data\nSo to fix it we need apply the dates in both columns to same type date.\nI choose as best as I can the featurs that fit the economic aspects predict.\nFollowing this vision: \n    1)I didn't take data size stores\n    2) I merge only the full cells data","368333c6":"# **8. Adaboost**","04976413":"**5.1 Plotting 'M^2_per_profit' 'Type' and 'Size\" via the label (prediction of main sales features)**","34b9fa0a":"**3.1 Histogram plot**","85ba8e1f":"Retail chain efficiency is measured as efficiency per square meter of profit. \nLet's see what the efficiency is per square meter? What is the rate of increase \nin efficiency? Next we will examine whether it is possible to predict the rate of \nsales growth based on this figure, and what is the percentage of accuracy of the model?","57449dcc":"Here by 'Scatterplot' I could see the relations between all features. The calculating takes a lot of time so it's suggested to take small number sample. Finally, we haven't correlation between the features","0c9bd2ed":"**4.3 Let see the first 10 variable (for example)**","8dc16d30":"**2.5.1 Have appreciable positive skewness.**","71c78eea":"**2.** Load and check data\n   --------------------"}}