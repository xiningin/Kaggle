{"cell_type":{"218c787c":"code","3d379841":"code","70b2c9bd":"code","49b38aba":"code","d27b034f":"code","cced98d4":"code","854b5ddc":"code","5538b127":"code","dba21924":"code","87f5f56e":"code","33848b90":"code","0d95f949":"code","31470fe5":"code","7e73a716":"code","c3243db4":"code","1beaddbd":"code","af1b2cb3":"code","9ad5acf3":"code","2eb46579":"code","fbfc19bf":"code","10acb510":"code","01fa9419":"code","d6528117":"code","eb46d869":"code","84f6730d":"code","8ece9567":"code","c7de6c01":"markdown","06d0b25a":"markdown","693ef48d":"markdown","8a32d118":"markdown","9b58a0d8":"markdown","26d50014":"markdown","d45294f8":"markdown","15ae52dc":"markdown","d0f80284":"markdown","0c7dcab5":"markdown","53ee07ba":"markdown","fdfd69ba":"markdown","525847c3":"markdown","3c70bf00":"markdown","0e02b5fd":"markdown","279769db":"markdown","6bc8ce9b":"markdown","5b782ba3":"markdown","178d00c3":"markdown"},"source":{"218c787c":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport nltk\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings('ignore')\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","3d379841":"messages = pd.read_csv(\"..\/input\/spam.csv\",encoding='latin-1')\nmessages.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\nmessages.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)\nmessages.head()","70b2c9bd":"messages.describe()","49b38aba":"messages.groupby('label').describe()","d27b034f":"messages['length'] = messages['message'].apply(len)\nmessages.head()","cced98d4":"messages['length'].plot(bins=10, kind='hist') ","854b5ddc":"messages.length.describe()","5538b127":"messages[messages['length'] == 910]['message'].iloc[0]","dba21924":"messages.hist(column='length', by='label', bins=50,figsize=(12,4))","87f5f56e":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","33848b90":"from sklearn.model_selection import train_test_split\n\nmsg_train, msg_test, label_train, label_test = \\\ntrain_test_split(messages['message'], messages['label'], test_size=0.2)\n\nprint(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))","0d95f949":"from sklearn.pipeline import Pipeline\n\npipeline1 = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])","31470fe5":"pipeline1.fit(msg_train,label_train)","7e73a716":"predictions1 = pipeline1.predict(msg_test)","c3243db4":"print(classification_report(predictions1,label_test))\nnbscore=accuracy_score(predictions1,label_test)\nprint(nbscore)","1beaddbd":"pipeline2 = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w\/ Logistic regression\n])","af1b2cb3":"pipeline2.fit(msg_train,label_train)\npredictions2=pipeline2.predict(msg_test)\nprint(classification_report(predictions2,label_test))\nlrscore=accuracy_score(predictions2,label_test)\nprint(lrscore)","9ad5acf3":"pipeline3 = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', KNeighborsClassifier()),  # train on TF-IDF vectors w\/ K-nn\n])","2eb46579":"pipeline3.fit(msg_train,label_train)\npredictions3=pipeline3.predict(msg_test)\nprint(classification_report(predictions3,label_test))\nknnscore=accuracy_score(predictions3,label_test)\nprint(knnscore)","fbfc19bf":"pipeline4 = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', SVC()),  # train on TF-IDF vectors w\/ SVM\n])","10acb510":"pipeline4.fit(msg_train,label_train)\npredictions4=pipeline4.predict(msg_test)\nprint(classification_report(predictions4,label_test))\nsvmscore=accuracy_score(predictions4,label_test)\nprint(svmscore)","01fa9419":"pipeline5 = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', DecisionTreeClassifier()),  # train on TF-IDF vectors w\/ DecisionTreeClassifier\n])","d6528117":"pipeline5.fit(msg_train,label_train)\npredictions5=pipeline5.predict(msg_test)\nprint(classification_report(predictions5,label_test))\ndtscore=accuracy_score(predictions5,label_test)\nprint(dtscore)","eb46d869":"results=[nbscore,lrscore,knnscore,svmscore,dtscore]\nn=['Naive-B','Log. Reg.','KNN','SVM','Dtree']\n","84f6730d":"ndf=pd.DataFrame(n)\nrdf=pd.DataFrame(results)\nrdf[1]=n","8ece9567":"print('Accuracy')\nrdf","c7de6c01":"## Logistic regression","06d0b25a":"## Naive Bayes","693ef48d":"**Looks like some sort of Love letter! But let's focus back on the idea of trying to see if message length can act as a good feature to disting between ham and spam:**","8a32d118":"\n## Train Test Split","9b58a0d8":"**Let's create the function that tockenize's our datset by removing unwanted punctuations and stopwords.**","26d50014":"Let's look at labels closely.","d45294f8":"The test size is 20% of the entire dataset (1115 messages out of total 5572), and the training is the rest (4457 out of 5572).\n\n## Creating a Data Pipelines\n**Let's create data pipelines that vectorizes and converts to bag-of-words(BOW), applies TF-IDF, and fits a classifier to the data**\n","15ae52dc":"## Exploratory Data Analysis","d0f80284":"## Thank You!","0c7dcab5":"**It is very clear that Naive-bayes, Logistic regression, and Decision tree are the best fit for this dataset with accuracies around 95%**","53ee07ba":"Woah! 910 characters, let's use masking to find this message:","fdfd69ba":"## K-Nearest Neigbors","525847c3":"## Decision Tree","3c70bf00":"**We can clearly see there IS a trend in the the text length of 'spam' and 'ham messages' **","0e02b5fd":"**The new feature looks normally distributed and hence can act as a pretty good feature**","279769db":"**To better understand the data and improve the accuracy with our ML model, lets add a synthetic feature 'length'**","6bc8ce9b":"### Data Visualization\n","5b782ba3":"## Text Pre-processing","178d00c3":"## Support Vector Machine\n\n\n\n\n\n\n\n\n\n\n\n\n"}}