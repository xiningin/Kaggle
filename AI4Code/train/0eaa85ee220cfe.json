{"cell_type":{"a41a9ad7":"code","96ab5464":"code","7156ee94":"code","80d037f7":"code","146e7204":"code","0fdfecad":"code","78c01dfb":"code","1f3f9e7f":"code","4f22ab49":"code","5c24fb1c":"code","7087fe8a":"code","233aca8e":"code","40cfcda7":"code","9f518407":"code","3a29516f":"code","06705e56":"code","60fd36ad":"code","ff5a6a19":"code","46596721":"code","0660d473":"code","a42435fe":"markdown","39c00ec9":"markdown","aabbb0e4":"markdown","fb282f9b":"markdown","6ded1b6d":"markdown","7a5db530":"markdown","ee5e4bc0":"markdown","ac1e7fba":"markdown"},"source":{"a41a9ad7":"import re\nimport nltk\nimport copy\nimport scipy\nimport tensorflow\nimport numpy as np \nimport pandas as pd \nfrom tqdm.auto import tqdm \nfrom bs4 import BeautifulSoup \nfrom scipy.sparse import vstack\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom sklearn.pipeline import Pipeline\nfrom tensorflow.keras import activations\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow. keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, Input, Bidirectional, TimeDistributed, Dense, Conv1D, MaxPool1D, concatenate, \\\n    Dropout, add, InputSpec, PReLU, Flatten, multiply, Reshape, Permute, BatchNormalization,LeakyReLU","96ab5464":"data_train=pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ndata_val=pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\ndata_train.head()","7156ee94":"weight_toxic={'toxic':1.2,'severe_toxic':1.3,'obscene':0.5,'threat':0.4,'insult':0.3,'identity_hate':0.5}\ndata=copy.deepcopy(data_train)\ndata['weight_rate']=0\nfor col,val in weight_toxic.items():\n    data['weight_rate']+=val*data[col]","80d037f7":"#Balance through down_scaling\ndata_insulting=data[data['weight_rate']>0]\ndata_val_0=data[data['weight_rate']==0].sample(len(data_insulting)+5000)\ndata_under_sampled=pd.concat([data_insulting,data_val_0])\ndata=data_under_sampled","146e7204":"# data['rate']=data['toxic']+data['severe_toxic']+data['obscene']+data['threat']+data['insult']+data['identity_hate']\n# data.rate.unique()\n# print(data.rate.value_counts())","0fdfecad":"# data['rate']=data.rate.apply(lambda x:1 if x>0 else 0)\n# data.head()","78c01dfb":"#Cleaning Template copied from-\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) \n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    return data","1f3f9e7f":"data_clean = clean(data,'comment_text')      #Training data\ndata_val=clean(data_val,'less_toxic')        #Validation data\ndata_val=clean(data_val,'more_toxic')","4f22ab49":"data_clean.head()","5c24fb1c":"# pipeline = Pipeline(\n#     [\n#         (\"vect\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n#         (\"clf\", Ridge()),\n#     ]\n# )\n# pipeline.fit(data_clean['comment_text'],data_clean['rate'])\n# p1 = pipeline.predict(val_less_toxic['less_toxic'])\n# p2 = pipeline.predict(val_more_toxic['more_toxic'])","7087fe8a":"#To vectorize the text(we can also use keras functions for this)\ndef text_to_vector(df,col_name,w2v):\n    w2v={}\n    text=list(df[col_name])\n    rate=list(df['weight_rate'])\n    count=1\n    if len(w2v)==0:\n        for txt in text:\n            for word in txt.split(' '):\n                if word not in w2v:\n                    w2v[word]=count\n                    count+=1\n    X=np.zeros((len(text),30),dtype='float32')\n    Y=np.zeros((len(text),1),dtype='float32')\n    for i,txt in enumerate(text):\n        j=0\n        Y[i][0]=rate[i]\n        for word in txt.split(' '):\n            if word in w2v and j<30:\n                X[i][j]=w2v[word]\n                j+=1\n                \n    return X,Y,w2v ","233aca8e":"x_train,y_train,w2v=text_to_vector(data_clean,col_name='comment_text',w2v={})\nprint(x_train.shape,y_train.shape,len(w2v))","40cfcda7":"# def lstm_model(V1, V2, K, sequence_length):\n#     inputs = Input(shape=(sequence_length,))\n#     l0 = Embedding(V1, K, input_length=sequence_length,mask_zero=True)(inputs)\n#     l0 = BatchNormalization()(l0)\n#     l0 = LSTM(356)(l0)\n#     l0 = BatchNormalization()(l0)\n#     l0=Dense(20)(l0)\n#     l0=LeakyReLU()(l0)\n#     l0 = BatchNormalization()(l0)\n#     out = Dense(V2, activation='linear')(l0)\n#     model = Model(inputs=inputs, outputs=out)\n#     opt=tensorflow.keras.optimizers.Adam(learning_rate=0.004)\n# #     opt=tensorflow.keras.optimizers.SGD(learning_rate=0.001)\n#     model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n#     return model\n\n# model= lstm_model(len(w2v)+1, 1, 300, 30)\n# model.summary()","9f518407":"def lstm_model(V1, V2, K, sequence_length):\n    inputs = Input(shape=(sequence_length,))\n    l0 = Embedding(V1, K, input_length=sequence_length,mask_zero=True)(inputs)\n    l0 = BatchNormalization()(l0)\n    l0 = Bidirectional(LSTM(256))(l0)\n    l0 = BatchNormalization()(l0)\n    l0=Dense(10)(l0)\n    l0=LeakyReLU()(l0)\n    l0 = BatchNormalization()(l0)\n    out = Dense(V2, activation='linear')(l0)\n    model = Model(inputs=inputs, outputs=out)\n    opt=tensorflow.keras.optimizers.Adam(learning_rate=0.004)\n#     opt=tensorflow.keras.optimizers.SGD(learning_rate=0.001)\n    model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n    return model\n\nmodel= lstm_model(len(w2v)+1, 1, 200, 30)\nmodel.summary()","3a29516f":"model.fit(x_train,y_train,epochs=10)","06705e56":"def text2vect(df,col):\n    X=np.zeros((len(list(df[col])),30),dtype='float32')\n    for i,txt in enumerate(list(df[col])):\n        j=0\n        for word in txt.split(' '):\n            if word in w2v and j<30:\n                X[i][j]=w2v[word]\n                j+=1\n    return X\n\n\ndef validation_result(df,col1,col2):\n    less_toxic=text2vect(df,col1)\n    more_toxic=text2vect(df,col2)\n    match=0\n    if len(less_toxic)==len(more_toxic):\n        \n        pred_less_toxic=model.predict(less_toxic)\n        pred_more_toxic=model.predict(more_toxic)\n        \n        for i in tqdm(range(len(pred_less_toxic))):\n            if pred_less_toxic[i]<=pred_more_toxic[i]:\n                match+=1\n        return match\/len(pred_less_toxic)\n    return 'length not same'\n    ","60fd36ad":"print(validation_result(data_val,'less_toxic','more_toxic'))","ff5a6a19":"df_sub=pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ndf_sub=clean(df_sub,'text')","46596721":"score=model.predict(text2vect(df_sub,'text'))\nscore.shape","0660d473":"df_sub['score']=score\ndf_sub['score'] = df_sub['score'].rank(method='first')\ndf_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","a42435fe":"# Tf-Idf+Ridge","39c00ec9":"# Data Load","aabbb0e4":"# Validation Number Check","fb282f9b":"# Give Weight to different toxicity level","6ded1b6d":"# Submission Part","7a5db530":"# LSTM+Embedding Layer","ee5e4bc0":"# Data Cleaning","ac1e7fba":"# Clean the data"}}