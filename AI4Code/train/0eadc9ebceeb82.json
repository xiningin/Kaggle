{"cell_type":{"45d84778":"code","9f82c1e4":"code","5a73ef87":"code","cbedf0e9":"code","930f3f3b":"code","c38b0123":"code","1b5a29bd":"code","fd3e2cb3":"code","e236ad0d":"code","68800e6d":"code","ff0e39aa":"code","951ad752":"code","df671b50":"code","3d1ffa0c":"code","49958cf1":"code","6720a6ca":"markdown","d1d998fa":"markdown","b7cd8e28":"markdown","477d49fe":"markdown","f01cc7b1":"markdown"},"source":{"45d84778":"!pip install keras-segmentation\n!pip install segmentation-models","9f82c1e4":"import tensorflow as tf\nimport os\nimport random\nimport numpy as np\n\nfrom tqdm import tqdm \n\nfrom keras.layers import *\nfrom keras.models import *\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\n\nfrom keras_segmentation.models.fcn import *\nfrom keras_segmentation.models.unet import *\nfrom keras_segmentation.models.segnet import *\nfrom keras_segmentation.models.pspnet import *\nfrom keras_segmentation.models.unet import *\n\n%env SM_FRAMEWORK=tf.keras\nimport segmentation_models as sm\n\n\nfrom random import randint\nfrom segmentation_models.metrics import IOUScore\nimport seaborn as sns\niou_score = IOUScore()\nimport cv2\nseed = 42\nnp.random.seed = seed\n\nIMG_WIDTH = 224\nIMG_HEIGHT = 224\n\n\nIMG_PATH = \"..\/input\/cityscape\/ds\/images_prepped_train\"\nANN_PATH = \"..\/input\/cityscape\/ds\/annotations_prepped_train\" ","5a73ef87":"# We read the image paths and put them in arrays\nimg_ids = []\nfor a,b,c in os.walk(IMG_PATH):\n    for i in c:\n        img_ids.append(a+'\/'+i)\n\nprint(len(img_ids))\n\nann_ids = []\nfor a,b,c in os.walk(ANN_PATH):\n    for i in c:\n        ann_ids.append(a+'\/'+i)\n\nprint(len(ann_ids))","cbedf0e9":"# This function will be used in order to read the image annotations as arrays: they will be of a format 288x288x12 since we have 12 classes\n\ndef getSegmentationArr( path , nClasses , height , width  ):\n\n    seg_labels = np.zeros((  height , width  , nClasses ))\n    img = cv2.imread(path, 1)\n    img = cv2.resize(img, ( height , width ))\n    img = img[:, : , 0]\n\n    for c in range(nClasses):\n        seg_labels[: , : , c ] = (img == c ).astype(int)\n    #seg_labels = np.reshape(seg_labels, ( width*height,nClasses  ))\n    return seg_labels\n","930f3f3b":"# This function will be used to color the segmentation images\ndef get_colored_segmentation_image(seg,n_classes=12):\n    \n    seg_img = np.zeros( (seg.shape[0],seg.shape[1],3) ).astype('float')\n    colors = sns.color_palette(\"hls\", n_classes)\n\n    for c in range(n_classes):\n        segc = (seg == c)\n        seg_img[:,:,0] += (segc*( colors[c][0] ))\n        seg_img[:,:,1] += (segc*( colors[c][1] ))\n        seg_img[:,:,2] += (segc*( colors[c][2] ))\n\n    return(seg_img)\n","c38b0123":"# We read the images and place them in arrays\n\nX = []\nfor path in img_ids:\n    img = imread(path,)\n    img = resize(img, (IMG_HEIGHT,IMG_WIDTH), mode='constant', preserve_range=True)\n    X.append(img)\nX = np.array(X,np.uint8)  \n#X= X\/255.0\n\ny = []\nfor path in ann_ids:\n    y.append(getSegmentationArr(path,12,IMG_HEIGHT,IMG_WIDTH))\ny = np.array(y,np.uint8)    ","1b5a29bd":"# We shuffle the data for better performance\n\nrandomize = np.arange(len(X))\nnp.random.shuffle(randomize)\nX = X[randomize]\ny = y[randomize]","fd3e2cb3":"\nimshow(X[320])","e236ad0d":"test = np.argmax(y,axis=3)\ntest1 = get_colored_segmentation_image( test[320]  , n_classes=12  )\nimshow(test1)","68800e6d":"# we do the train\/test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ff0e39aa":"\ndef Fcn8():\n    model = fcn_8_mobilenet(n_classes=12, input_height=IMG_HEIGHT, input_width=IMG_WIDTH)\n    input_layer = model.layers[0].input\n    final_layer = model.layers[-4].output\n    add_layer = Conv2DTranspose(12, kernel_size=8, strides=(8, 8), activation=\"softmax\")(final_layer)\n    new_model = Model(input_layer, add_layer)\n    return new_model\n\ndef Fcn32():\n    model = fcn_32_mobilenet(n_classes=12, input_height=IMG_HEIGHT, input_width=IMG_WIDTH)\n    input_layer = model.layers[0].input\n    final_layer = model.layers[-4].output\n    add_layer = Conv2DTranspose(12, kernel_size=32, strides=(32, 32), activation=\"softmax\")(final_layer)\n    new_model = Model(input_layer, add_layer)\n    return new_model    \n\ndef Segnet():\n    model = vgg_segnet(n_classes=12, input_height=IMG_HEIGHT, input_width=IMG_WIDTH)\n    input_layer = model.layers[0].input\n    final_layer = model.layers[-4].output\n    add_layer = Conv2DTranspose(12, kernel_size=2, strides=2, activation=\"softmax\")(final_layer)\n    new_model = Model(input_layer, add_layer)\n    return new_model     \n\ndef Psp_resnet():\n    model = sm.PSPNet( backbone_name='resnet34',input_shape=(288, 288, 3),\n                      classes=12, activation='softmax',encoder_weights='imagenet')\n    return model\n\ndef Unet_resnet():\n    model = sm.Unet( backbone_name='resnet34',input_shape=(288, 288, 3),\n                    classes=12, activation='softmax',encoder_weights='imagenet')\n    return model","951ad752":"#model = Psp_resnet()\n#model= Fcn32()\nmodel = Fcn8()\n#model = Segnet()\n#model = Unet_resnet()\nmodel.summary()","df671b50":"callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')] # If the validation loss does not improve in three ephoces we stop training\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_score])\n\nresults = model.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=32,\n                    epochs=50, callbacks=callbacks)","3d1ffa0c":"# Results\nprint(model.metrics_names)\nscore = model.evaluate(X_test, y_test)\n\nprint(\"Test loss: \", score[0])\nprint(\"Test accuracy: \", score[1])\nprint('IoU: ', score[2])","49958cf1":"# we check out the results on some random images\n\npredictions_test = model.predict(X_test)\nfor a in range(10):\n    i = randint(0,93)\n    orig = X_test[i]\n    annot = np.argmax(y_test[i], axis=2)\n    pred = np.argmax(predictions_test[i],axis=2)\n    \n    \n    fig = plt.figure(figsize=(20,60))    \n    ax = fig.add_subplot(1,3,1)\n    ax.imshow(orig)\n    ax.set_title(\"original\")\n    \n    ax = fig.add_subplot(1,3,2)\n    ax.imshow(get_colored_segmentation_image(pred))\n    ax.set_title(\"predicted class\")\n    \n    ax = fig.add_subplot(1,3,3)\n    ax.imshow(get_colored_segmentation_image(annot))\n    ax.set_title(\"true class\")\n    plt.show()","6720a6ca":"Let's start the training","d1d998fa":"# Image semantic segmentation\nIn this kernel we will see how we can use different deep learning architectures for image semantic segmentation tasks. We'll import networks from:\n\nhttps:\/\/github.com\/divamgupta\/image-segmentation-keras and\n\nhttps:\/\/github.com\/qubvel\/segmentation_models","b7cd8e28":"We check if everything is okay","477d49fe":"We select one of the networks available!","f01cc7b1":"We are going to change the last layers of the 3 first networks imported in order to match with our y_train dimensions."}}