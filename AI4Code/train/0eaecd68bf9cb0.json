{"cell_type":{"0c63cf32":"code","5e631cf1":"code","f7f07bf7":"code","66853f03":"code","9b8f94f8":"code","13b48f51":"code","1c3bbaf0":"code","aa8fe516":"code","7bd11657":"code","e34c6540":"code","b93a16dd":"code","bb4212e9":"code","f999377c":"code","ef2110c3":"code","33ffa498":"code","f788aa17":"code","054886c5":"code","37e57423":"code","4ace5ef7":"code","1834cf34":"code","a19bfb1a":"code","c6ad4cc7":"code","42a9accb":"code","b566e777":"markdown","5d1960b9":"markdown","a6b2653d":"markdown","c5bf801e":"markdown","9b8009a2":"markdown","a50c8c84":"markdown"},"source":{"0c63cf32":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nimport numpy as np","5e631cf1":"def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n\n  # first layer\n  x = input_tensor\n  for i in range(2):\n    x = tf.keras.layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n            kernel_initializer = 'he_normal', padding = 'same')(x)\n    x = tf.keras.layers.Activation('relu')(x)\n  \n  return x\n\n\ndef encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n \n\n  f = conv2d_block(inputs, n_filters=n_filters)\n  p = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(f)\n  p = tf.keras.layers.Dropout(0.3)(p)\n\n  return f, p\n\ndef encoder(inputs):\n\n  f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3)\n  f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2,2), dropout=0.3)\n  f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2,2), dropout=0.3)\n  f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2,2), dropout=0.3)\n\n  return p4, (f1, f2, f3, f4)","f7f07bf7":"def bottleneck(inputs):\n  \n  bottle_neck = conv2d_block(inputs, n_filters=1024)\n\n  return bottle_neck","66853f03":"def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):\n    \n  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides = strides, padding = 'same')(inputs)\n  c = tf.keras.layers.concatenate([u, conv_output])\n  c = tf.keras.layers.Dropout(dropout)(c)\n  c = conv2d_block(c, n_filters, kernel_size=3)\n\n  return c\n\n\ndef decoder(inputs, convs, output_channels):\n  \n  f1, f2, f3, f4 = convs\n\n  c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n  c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n  c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n  c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3,3), strides=(2,2), dropout=0.3)\n\n  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n\n  return outputs","9b8f94f8":"OUTPUT_CHANNELS = 3\n\ndef unet():\n\n  # specify the input shape\n  inputs = tf.keras.layers.Input(shape=(128, 128,3,))\n\n  # feed the inputs to the encoder\n  encoder_output, convs = encoder(inputs)\n\n  # feed the encoder output to the bottleneck\n  bottle_neck = bottleneck(encoder_output)\n\n  # feed the bottleneck and encoder block outputs to the decoder\n  # specify the number of classes via the `output_channels` argument\n  outputs = decoder(bottle_neck, convs, output_channels=OUTPUT_CHANNELS)\n  \n  # create the model\n  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n  return model\n\n# instantiate the model\nmodel = unet()\n\n# see the resulting model architecture\nmodel.summary()\n","13b48f51":"!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=oxford_iiit_pet:3.1.0\n\n# download the dataset and get info\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","1c3bbaf0":"print(dataset.keys())","aa8fe516":"print(info)","7bd11657":"# Preprocessing Utilities\n\ndef random_flip(input_image, input_mask):\n  '''does a random flip of the image and mask'''\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)\n    input_mask = tf.image.flip_left_right(input_mask)\n\n  return input_image, input_mask\n\n\ndef normalize(input_image, input_mask):\n  '''\n  normalizes the input image pixel values to be from [0,1].\n  subtracts 1 from the mask labels to have a range from [0,2]\n  '''\n  input_image = tf.cast(input_image, tf.float32) \/ 255.0\n  input_mask -= 1\n  return input_image, input_mask\n\n\n@tf.function\ndef load_image_train(datapoint):\n  '''resizes, normalizes, and flips the training data'''\n  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n  input_image, input_mask = random_flip(input_image, input_mask)\n  input_image, input_mask = normalize(input_image, input_mask)\n  \n  return input_image, input_mask\n\n\ndef load_image_test(datapoint):\n  '''resizes and normalizes the test data'''\n  input_image = tf.image.resize(datapoint['image'], (128, 128), method='nearest')\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128), method='nearest')\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask","e34c6540":"# preprocess the train and test sets\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest = dataset['test'].map(load_image_test)","b93a16dd":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\n\n# shuffle and group the train set into batches\ntrain_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\n# do a prefetch to optimize processing\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n# group the test set into batches\ntest_dataset = test.batch(BATCH_SIZE)","bb4212e9":"# class list of the mask pixels\nclass_names = ['pet', 'background', 'outline']\n\n\ndef display_with_metrics(display_list, iou_list, dice_score_list):\n  '''displays a list of images\/masks and overlays a list of IOU and Dice Scores'''\n  \n  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n  \n  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n  display_string = \"\\n\\n\".join(display_string_list)\n\n  display(display_list, [\"Image\", \"Predicted Mask\", \"True Mask\"], display_string=display_string) \n\n\ndef display(display_list,titles=[], display_string=None):\n  '''displays a list of images\/masks'''\n\n  plt.figure(figsize=(15, 15))\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\n    if display_string and i == 1:\n      plt.xlabel(display_string, fontsize=12)\n    img_arr = tf.keras.preprocessing.image.array_to_img(display_list[i])\n    plt.imshow(img_arr)\n  \n  plt.show()\n\n\ndef show_image_from_dataset(dataset):\n  '''displays the first image and its mask from a dataset'''\n\n  for image, mask in dataset.take(1):\n    sample_image, sample_mask = image, mask\n  display([sample_image, sample_mask], titles=[\"Image\", \"True Mask\"])\n\n\ndef plot_metrics(metric_name, title, ylim=5):\n  '''plots a given metric from the model history'''\n  plt.title(title)\n  plt.ylim(0,ylim)\n  plt.plot(model_history.history[metric_name],color='blue',label=metric_name)\n  plt.plot(model_history.history['val_' + metric_name],color='green',label='val_' + metric_name)","f999377c":"# display an image from the train set\nshow_image_from_dataset(train)\n\n# display an image from the test set\nshow_image_from_dataset(test)","ef2110c3":"# configure the optimizer, loss and metrics for training\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","33ffa498":"# configure the training parameters and train the model\n\nTRAIN_LENGTH = info.splits['train'].num_examples\nEPOCHS = 20\nVAL_SUBSPLITS = 5\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\nVALIDATION_STEPS = info.splits['test'].num_examples\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\n# this will take around 20 minutes to run\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_dataset)","f788aa17":"# Prediction Utilities\n\ndef get_test_image_and_annotation_arrays():\n  '''\n  Unpacks the test dataset and returns the input images and segmentation masks\n  '''\n\n  ds = test_dataset.unbatch()\n  ds = ds.batch(info.splits['test'].num_examples)\n  \n  images = []\n  y_true_segments = []\n\n  for image, annotation in ds.take(1):\n    y_true_segments = annotation.numpy()\n    images = image.numpy()\n  \n  y_true_segments = y_true_segments[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))]\n  \n  return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments\n\n\ndef create_mask(pred_mask):\n  '''\n  Creates the segmentation mask by getting the channel with the highest probability. Remember that we\n  have 3 channels in the output of the UNet. For each pixel, the predicition will be the channel with the\n  highest probability.\n  '''\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0].numpy()\n\n\ndef make_predictions(image, mask, num=1):\n  '''\n  Feeds an image to a model and returns the predicted mask.\n  '''\n\n  image = np.reshape(image,(1, image.shape[0], image.shape[1], image.shape[2]))\n  pred_mask = model.predict(image)\n  pred_mask = create_mask(pred_mask)\n\n  return pred_mask ","054886c5":"# Prediction Utilities\n\ndef get_test_image_and_annotation_arrays():\n  '''\n  Unpacks the test dataset and returns the input images and segmentation masks\n  '''\n\n  ds = test_dataset.unbatch()\n  ds = ds.batch(info.splits['test'].num_examples)\n  \n  images = []\n  y_true_segments = []\n\n  for image, annotation in ds.take(1):\n    y_true_segments = annotation.numpy()\n    images = image.numpy()\n  \n  y_true_segments = y_true_segments[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))]\n  \n  return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments\n\n\ndef create_mask(pred_mask):\n  '''\n  Creates the segmentation mask by getting the channel with the highest probability. Remember that we\n  have 3 channels in the output of the UNet. For each pixel, the predicition will be the channel with the\n  highest probability.\n  '''\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0].numpy()\n\n\ndef make_predictions(image, mask, num=1):\n  '''\n  Feeds an image to a model and returns the predicted mask.\n  '''\n\n  image = np.reshape(image,(1, image.shape[0], image.shape[1], image.shape[2]))\n  pred_mask = model.predict(image)\n  pred_mask = create_mask(pred_mask)\n\n  return pred_mask ","37e57423":"def class_wise_metrics(y_true, y_pred):\n  class_wise_iou = []\n  class_wise_dice_score = []\n\n  smoothening_factor = 0.00001\n  for i in range(3):\n    \n    intersection = np.sum((y_pred == i) * (y_true == i))\n    y_true_area = np.sum((y_true == i))\n    y_pred_area = np.sum((y_pred == i))\n    combined_area = y_true_area + y_pred_area\n    \n    iou = (intersection + smoothening_factor) \/ (combined_area - intersection + smoothening_factor)\n    class_wise_iou.append(iou)\n    \n    dice_score =  2 * ((intersection + smoothening_factor) \/ (combined_area + smoothening_factor))\n    class_wise_dice_score.append(dice_score)\n\n  return class_wise_iou, class_wise_dice_score","4ace5ef7":"# get the ground truth from the test set\ny_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n\n# feed the test set to th emodel to get the predicted masks\nresults = model.predict(test_dataset, steps=info.splits['test'].num_examples\/\/BATCH_SIZE)\nresults = np.argmax(results, axis=3)\nresults = results[..., tf.newaxis]","1834cf34":"# compute the class wise metrics\ncls_wise_iou, cls_wise_dice_score = class_wise_metrics(y_true_segments, results)","a19bfb1a":"# show the IOU for each class\nfor idx, iou in enumerate(cls_wise_iou):\n  spaces = ' ' * (10-len(class_names[idx]) + 2)\n  print(\"{}{}{} \".format(class_names[idx], spaces, iou)) ","c6ad4cc7":"# show the Dice Score for each class\nfor idx, dice_score in enumerate(cls_wise_dice_score):\n  spaces = ' ' * (10-len(class_names[idx]) + 2)\n  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score)) ","42a9accb":"# Please input a number between 0 to 3647 to pick an image from the dataset\ninteger_slider = 3646\n\n# Get the prediction mask\ny_pred_mask = make_predictions(y_true_images[integer_slider], y_true_segments[integer_slider])\n\n# Compute the class wise metrics\niou, dice_score = class_wise_metrics(y_true_segments[integer_slider], y_pred_mask)  \n\n# Overlay the metrics with the images\ndisplay_with_metrics([y_true_images[integer_slider], y_pred_mask, y_true_segments[integer_slider]], iou, dice_score)","b566e777":"## Compile and Train the model","5d1960b9":"## Make predictions","a6b2653d":"### Show Predictions","c5bf801e":"## Prepare the Dataset","9b8009a2":"### Compute class wise metrics\n","a50c8c84":"## Download the Oxford-IIIT Pets dataset"}}