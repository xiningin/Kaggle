{"cell_type":{"b0ba5486":"code","01659794":"code","e7a01edc":"code","3b125fbc":"code","67bc939c":"code","0fed4c0e":"code","720a2d56":"code","4fd2d093":"code","5e4664d4":"code","349f715c":"code","b6b60130":"code","87931676":"code","97ea19b0":"code","98bfc078":"code","9f324544":"code","3684947e":"code","3badafe5":"code","8f0fc2f5":"code","8b8d093e":"code","91ec2d15":"code","3d701e23":"markdown","df641d21":"markdown","7142ff3a":"markdown","6ac27166":"markdown","2af2af8a":"markdown","b71a11e7":"markdown","6d7390d6":"markdown","08b66302":"markdown","e7d24ecb":"markdown","45a9a086":"markdown","716a6f49":"markdown","37e3e507":"markdown","9a654f8a":"markdown","098c798b":"markdown","5ea31873":"markdown","17182e19":"markdown","f262c41d":"markdown","c42c5a89":"markdown","002c93d0":"markdown","48b38898":"markdown","1cdf095c":"markdown","3eced7fb":"markdown","cc724062":"markdown"},"source":{"b0ba5486":"# For printing all the outputs of a cell in the same output window\n\n# from IPython.core.interactiveshell import InteractiveShell  \n# InteractiveShell.ast_node_interactivity = \"all\"         #for enabling\n# InteractiveShell.ast_node_interactivity = \"last_expr\"   #for disabling\n\n\n# Basic Libraries\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport re\nimport string\nimport random\nimport math\nimport time\nimport os\nfrom os import listdir\nimport itertools\nimport collections\nfrom collections import Counter, defaultdict\nfrom tqdm import tqdm\nfrom sklearn import utils\n\n# Visualization\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Vector Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# evaluation metrics\nfrom sklearn import metrics\n\n\n# Deep learning\n\n# import tensorflow\n# from tensorflow import keras\n# from tensorflow.keras import backend as KB\n# # from tensorflow.keras import models, layers, preprocessing as keras_processing\n\n\n# Bert language model\n\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n!pip install transformers\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# model serialization\nimport pickle","01659794":"train_df = pd.read_csv('..\/input\/quora-question-pairs\/train.csv.zip')\nprint(\"Train Dataframe:\")\ntrain_df.head(3)\nprint(f'Train dataframe contains {train_df.shape[0]} samples.')\nprint('Number of features in train data : ', train_df.shape[1])\nprint('Train Features : ', train_df.columns.values)","e7a01edc":"train_df.profile_report()","3b125fbc":"# Null values and Data types\nprint('Train Set:\\n')\nprint(train_df.info())\nprint('')","67bc939c":"# basic stats\nprint('Train set basic stats:')\ntrain_df.describe(include='all')","0fed4c0e":"print('Train data Null values :')\ntrain_df[train_df.isnull().any(1)]","720a2d56":"train_df = train_df.fillna(value=\"\")\ntrain_df[train_df.isnull().any(1)]","4fd2d093":"train_df.is_duplicate.value_counts(normalize=True)","5e4664d4":"plt.figure(figsize=(8,6))\ntrain_df.is_duplicate.value_counts().plot(kind='bar', color=['r','g'])\n\nD = mpatches.Patch(color='r', label='Duplicate')\nND = mpatches.Patch(color='g', label='Non-Duplicate')\n\nplt.legend(handles=[D,ND], loc='best')\n\nplt.xlabel('Type of Labels')\nplt.ylabel('Count of Data per Label Category')\nplt.title('Distribution of labels')\nplt.show()","349f715c":"q1_lengths = [len(q1)for q1 in train_df.question1]\nprint(\"Mean sentence length for Question1:\", np.mean(q1_lengths))\n\nplt.figure(figsize=(8,6))\nplt.hist(q1_lengths,bins=50,density=True,color='b')\n# sns.distplot(q1_lengths,bins=50,kde=True,color='b')\nplt.xlabel('Question1 lengths')\nplt.ylabel('Count of Question1 lengths')\nplt.title('Distribution of Question1 sentence lengths')\nplt.show()","b6b60130":"q2_lengths = [len(q2)for q2 in train_df.question2]\nprint(\"Mean sentence length for Question2:\", np.mean(q2_lengths))\n\nplt.figure(figsize=(8,6))\nplt.hist(q2_lengths,bins=50,density=True,color='r')\n#sns.distplot(q2_lengths,bins=50,kde=True,color='r')\nplt.xlabel('Question2 lengths')\nplt.ylabel('Count of Question2 lengths')\nplt.title('Distribution of Question2 sentence lengths')\nplt.show()","87931676":"sentences_question1 = list(sent for sent in train_df['question1'].values)\nsentences_question2 = list(sent for sent in train_df['question2'].values)","97ea19b0":"st_model = SentenceTransformer('bert-base-nli-mean-tokens')","98bfc078":"def generate_sent_embeddings(data):\n    return st_model.encode(data)","9f324544":"if os.path.isfile('..\/input\/quora-question-pairs-sentence-transformers\/question1_sent_embeddings.pkl'):\n    #retrieve the question1_sent_embeddings list for usage.\n    with open('..\/input\/quora-question-pairs-sentence-transformers\/question1_sent_embeddings.pkl', 'rb') as f: \n        question1_sent_embeddings = pickle.load(f)\nelse:\n    question1_sent_embeddings = generate_sent_embeddings(sentences_question1)\n    #save the question1_sent_embeddings list for later usage.\n    with open('question1_sent_embeddings.pkl', 'wb') as f: \n        pickle.dump(question1_sent_embeddings, f)\n\nprint(\"shape of question1 sentence embeddings:\", question1_sent_embeddings.shape)\ntrain_df['question1_sent_embeddings'] = pd.DataFrame({'question1_sent_embeddings' : list(question1_sent_embeddings)})","3684947e":"if os.path.isfile('..\/input\/quora-question-pairs-sentence-transformers\/question2_sent_embeddings.pkl'):\n    #retrieve the question2_sent_embeddings list for usage.\n    with open('..\/input\/quora-question-pairs-sentence-transformers\/question2_sent_embeddings.pkl', 'rb') as f: \n        question2_sent_embeddings = pickle.load(f)\nelse:\n    question2_sent_embeddings = generate_sent_embeddings(sentences_question2)\n    #save the question2_sent_embeddings list for later usage.\n    with open('question2_sent_embeddings.pkl', 'wb') as f: \n        pickle.dump(question2_sent_embeddings, f)\n\nprint(\"shape of question2 sentence embeddings:\", question2_sent_embeddings.shape)\ntrain_df['question2_sent_embeddings'] = pd.DataFrame({'question2_sent_embeddings' : list(question2_sent_embeddings)})","3badafe5":"questions_similarity = []\nfor index, row in train_df.iterrows():\n    questions_similarity.append(cosine_similarity([row['question1_sent_embeddings']],[row['question2_sent_embeddings']]))\n\n#convert the question similarity array into 1d array \nquestions_similarity = np.stack(questions_similarity,axis=0)\n# questions_similarity = questions_similarity.tolist()\nques_sim = np.array(questions_similarity).ravel()\n\n# store the question similarity scores in our dataframe\ntrain_df['questions_similarity'] = pd.DataFrame({'questions_similarity' : ques_sim})\ntrain_df['questions_similarity']","8f0fc2f5":"def similarity_to_predictions(cos_sim, threshold):\n    \"\"\"\n    This function converts the predicted similarities to predicted labels based on the threshold value\n    \"\"\"\n    if (cos_sim >= threshold):\n        return 1\n    else:\n        return 0\n    \ntrain_df['predicted_result'] = train_df['questions_similarity'].apply(similarity_to_predictions, threshold=0.87)","8b8d093e":"train_df.head(3)","91ec2d15":"metrics.accuracy_score(train_df['is_duplicate'], train_df['predicted_result'])","3d701e23":"## Implementing Sentence Transformers","df641d21":"**Dataset contains below data fields:**\n\n- id:  a simple rowID\n- qid(1, 2):  unique IDs of each question in the pair\n- question(1, 2):  actual text contents of the questions.\n- is_duplicate:  the label we are trying to Predict, i.e. whether the two questions are duplicates of each other.","7142ff3a":"## Import Libraries","6ac27166":"We know that the cosine similarity values ranges between 0 and 1, so we can convert these sentence similarity values into predicted labels (0 and 1) by setting an appropriate threshold for similarity.","2af2af8a":"Generating sentence embeddings using pretrained sentence transformers is a very time exhaustive process and the time complexity increases with the increase in the data size. So here I have generated the sentence embeddings at first and then reusing the generated embeddings using pickle serialisation.\nSo let's use sentence embeddings for question1 and question2 text generated using pre-trained sentence transformers.","b71a11e7":"## Dataset Analysis","6d7390d6":"**Check the distribution sentence lengths for Question 1 and Question 2 :**","08b66302":"Generate sentences lists for question1 and question2.","e7d24ecb":"- There are 3 null values for question1 and question2 texts which are present across only 3 samples, so we will fill these null values with empty strings.","45a9a086":"<h1 style=\"text-align:center;font-size:30px;\" >Quora Question Pairs : Sentence Transformers and BERT for Semantic Similarity<\/h1>","716a6f49":"**Concluding sentence transformers approach :**\n\nIn this work, I have implemented the Sentence Transformers model based on pre-trained \"BERT\" embeddings. The accuracy of predictions can be adjusted using an appropriate threshold value for sentence similarity. This implementation focuses on simplistic usage of the pre-trained \"BERT\" embeddings and cosine-similarity for evaluating the textual similarity between pair of sentences.","37e3e507":"In this approach I have defined a model for **Sentence Transformation** using '[bert-base-nli-mean-tokens](http:\/\/huggingface.co\/sentence-transformers\/bert-base-nli-mean-tokens)' repository. The sentence-transformers repository allows to train and use Transformer models for generating sentence and text embeddings. The model is described in the paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](http:\/\/arxiv.org\/abs\/1908.10084)","9a654f8a":"At last, we can compare the predictions ('predicted_result') with actual results ('is_duplicate') to check accuracy of the Quora question pairs similarity.","098c798b":"This work demonstrates how to find textual similarity between a pair of documents using Sentence Transformers and pre-trained BERT model. In this work, I have used the \"Quora Question Pairs\" dataset, the details about which can be found from [here](http:\/\/www.kaggle.com\/c\/quora-question-pairs) . ","5ea31873":"Let's define a method for generating sentence embeddings for each sentence using Sentnce Transformers model.","17182e19":"**Dataset Complete Information at a glance Using Pandas Profiling:**\n\nPandas profiling is a python package which helps us understand our data. It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The Pandas Profiling function extends the pandas DataFrame with df.profile_report() for quick data analysis. It displays a lot of information with a single line of code and that too in an interactive HTML report","f262c41d":"**Check the basic stats of the data:**\n\nThe pandas df.describe() and df.info() functions gives us a basic overview of the entire dataset.","c42c5a89":"Now let's check the our dataset after processing.","002c93d0":"**Check the distribution of output labels:**","48b38898":" ## Load Dataset","1cdf095c":"Now let's generate textual similarity values for question1 and question2 sentence embeddings using cosine similarity","3eced7fb":"### Handling the Missing Values","cc724062":"- We can observe missing values are present in the data, let's handle these values."}}