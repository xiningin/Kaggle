{"cell_type":{"7ad517bb":"code","45f5394c":"code","f272f820":"code","2879ed6f":"code","54b264d3":"code","572f462d":"code","ed0eda7f":"code","cc7b30c7":"code","15aa3c1f":"code","33c0ea20":"code","6f21cde1":"code","79a319f8":"code","d46d00fc":"code","4288038d":"code","56ac7cb1":"code","67f6a884":"code","fe370f23":"code","e311786c":"code","007220a5":"code","df86b12c":"code","3493d2bd":"code","e13954a3":"code","0a0fbf6c":"code","a8c7c81e":"code","b23efcba":"code","05b0466e":"code","443b3c84":"code","9de39c13":"code","6b57d015":"code","e702b4f4":"code","ea028634":"code","49bb2c27":"code","bab3243b":"code","4a6c2cd4":"code","57562223":"code","83eac596":"code","26295c98":"code","38990abf":"code","605cfc18":"code","e7460bfb":"code","9b1efa1b":"code","10d04c62":"code","186ad1db":"code","fb353fd0":"code","45146ea9":"code","eaa50bf4":"code","0ef0223d":"code","956c898d":"code","7c6fd221":"code","266ae974":"code","b7f65825":"code","2c03f90b":"code","cbfd162b":"code","697063bc":"code","e0a149d3":"code","c28a47db":"code","42f045dd":"code","02893555":"code","0cf27336":"markdown","bad33b8c":"markdown","e6208fba":"markdown","642036a1":"markdown","dee5d0a4":"markdown","e441e4c7":"markdown","72571b7b":"markdown","643d0018":"markdown","561d2b7c":"markdown","360c2e43":"markdown","66fdc83c":"markdown","6d34912f":"markdown","4228ef9b":"markdown","ecf252c4":"markdown","301ba4cf":"markdown","0ee1835d":"markdown","aba8708b":"markdown","d80a105d":"markdown","0f686402":"markdown","e20703c1":"markdown","c1b48ebb":"markdown","66e6d520":"markdown","2e67b663":"markdown","329e4e67":"markdown","dfc5ab33":"markdown","7797be2d":"markdown","7cc1cecb":"markdown","68ebcf31":"markdown","f8500d44":"markdown"},"source":{"7ad517bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45f5394c":"pip install --upgrade pip","f272f820":"!pip install swifter\n!pip install bayesian-optimization\n!pip install scikit-optimize","2879ed6f":"import numpy as np\nimport pandas as pd\nimport random\nseed = 777\nrandom.seed(seed)\nnp.random.seed(seed)","54b264d3":"#Analysis and data preparation\n\nimport math\nimport matplotlib as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nimport numpy as np  \nimport pandas as pd\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom scipy.stats import mode\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom itertools import permutations\nfrom tqdm import tqdm\nimport swifter\nfrom itertools import combinations\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nimport operator\nimport os\nimport warnings\nimport pandas as pd\n# import geopandas as gpd # For loading the map of road segments\n\n\n#tuning hyperparameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \n\n#graph, plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib inline\nplt.rcParams['figure.figsize'] = (20,15)     \n# warnings.filterwarnings('ignore')\n# from typing import List\n\n#building models\nimport lightgbm as lgb\nimport xgboost as xgb\nimport tensorflow as tf\nimport torch\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, \\\nKFold, GridSearchCV, StratifiedKFold, RepeatedStratifiedKFold\nfrom torch import tensor\n\n\n\nimport time\nimport sys\n\n#metrics \nfrom sklearn.metrics import balanced_accuracy_score, auc, mean_squared_error, roc_curve, confusion_matrix, precision_score, recall_score, f1_score,\\\nlog_loss, roc_auc_score\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom torch import nn\nfrom collections import OrderedDict\n\n\n#Clustering\n\nfrom sklearn.cluster import KMeans\n\n# #import shap\n# warnings.simplefilter(action='ignore', category=FutureWarning)","572f462d":"# Load the data\ndf = pd.read_csv('\/kaggle\/input\/uber-nairobi-ambulance-data\/Train (67).csv', parse_dates=['datetime'])\nprint(df.shape)\ndf.head(10)\n\nrand = 303\n","ed0eda7f":"# Plot the crashes\ndf.plot(x='longitude', y='latitude', kind='scatter', figsize=(12, 12), alpha=0.9, title='Crash Locations')","cc7b30c7":"sns.boxplot(y = 'latitude', data = df)","15aa3c1f":"sns.boxplot(y = 'longitude', data = df)","33c0ea20":"# View the submission format\nss = pd.read_csv('\/kaggle\/input\/uber-nairobi-ambulance-data\/SampleSubmission (45).csv', parse_dates=['date'])\nss.head()","6f21cde1":"ss.columns","79a319f8":"def score(sub, ref):\n    total_distance = 0\n    for date, c_lat, c_lon in ref[['datetime', 'latitude', 'longitude']].values: #For each accident period in the test set\n        \n        row = sub.loc[sub.date < date].tail(1) #The closest ambulance schedule \n        #time is picked\n        \n        dists = [] \n        for a in range(6):\n            dist = (((c_lat - row[f'A{a}_Latitude'].values[0])**2)+((c_lon - row[f'A{a}_Longitude'].values[0])**2))**0.5 \n            #the distance from lat, long is calulated for 6 ambulances\n            \n            dists.append(dist) \n        total_distance += min(dists) #the closest ambulance is addeed to the overall distance\n    return total_distance\n","d46d00fc":"df.describe()","4288038d":"weather = pd.read_csv('\/kaggle\/input\/uber-nairobi-ambulance-data\/Weather_Nairobi_Daily_GFS.csv', parse_dates=['Date'])\nweather.head()","56ac7cb1":"#Fill using forward fill\n\nweather = weather.fillna(method = 'ffill')","67f6a884":"def bin_weather(df, col):\n    \n    bins = np.linspace(min(df[col]), max(df[col]), 4)\n\n    group_names = ['low', 'mid', 'high']\n\n    df[col + '-bin'] = pd.cut(df[col], bins, labels = group_names, include_lowest = True)\n    \n","fe370f23":"weather.columns","e311786c":"weather.head()","007220a5":"for feature in weather.columns:\n    \n    if feature != 'Date':\n    \n        bin_weather(weather, feature)","df86b12c":"df['Date'] = df['datetime'].dt.date\n\nweather['Date'] = weather['Date'].astype(str)\n\ndf['Date'] = df['Date'].astype(str)\n\n#merge weather data\n\ndf = pd.merge(df, weather, on= ['Date'],  how='left')","3493d2bd":"# Example:\n\n# Test set\nreference = df.loc[df.datetime > '2019-01-01'] # Using 2019 as our test set\n\n# Date range covering test set\ndates = pd.date_range('2019-01-01', '2020-01-01', freq='3h')\n\n# # Create submission dataframe\nsub = pd.DataFrame({\n    'date':dates\n})\nfor ambulance in range(6):\n    sub['A'+str(ambulance)+'_Latitude'] = 0\n    sub['A'+str(ambulance)+'_Longitude'] = 0\n\n# Place an ambulance in the center of the city:\n\n    if ambulance == 0: \n        \n        sub['A'+str(ambulance)+'_Latitude'] = -1.23\n        sub['A'+str(ambulance)+'_Longitude'] = 36.8\n        \n    else:\n        sub['A'+str(ambulance)+'_Latitude'] =  sub['A'+str(ambulance-1)+'_Latitude'] - 0.05\n        sub['A'+str(ambulance)+'_Longitude'] = sub['A'+str(ambulance-1)+'_Longitude'] + 0.05\n\n\n\n\n\nscore(sub, reference)\n","e13954a3":"\nsub['Date'] = sub['date'].dt.date.astype(str)\n\n#merge weather data\n\nsub = pd.merge(sub, weather, on= ['Date'],  how='left')\n","0a0fbf6c":"sub.head()","a8c7c81e":"#Create time features\n\ndf['3h'] = (df.datetime.dt.hour  \/ 3).astype(int)* 3 \n\nsub['3h'] = (sub.date.dt.hour  \/ 3).astype(int)* 3 \n\ndf['year'] = df.datetime.dt.year\n\nsub['year'] = sub.date.dt.year\n\ndf['weekend\/week'] = (df.datetime.dt.dayofweek\/\/5 == 1).astype('float')\n\nsub['weekend\/week'] = (sub.date.dt.dayofweek\/\/5 == 1).astype('float')\n\ndf['dayofweek'] = df.datetime.dt.dayofweek\n\nsub['dayofweek'] = sub.date.dt.dayofweek\n\ndf['month'] = df.datetime.dt.month\n\nsub['month'] = sub.date.dt.month\n\n","b23efcba":"df['year']","05b0466e":"# Ambulance Locations\namb_locs = torch.randn(6, 2) * 0.02  # 6 pairs of random numbers, made fairly small\namb_locs = amb_locs + tensor((-1.27, 36.85)) # Centered around the middle of town\namb_locs.requires_grad_() # Tell pytorch to calculate gradients\ninitial_locs = amb_locs.clone() # Save originals for later\namb_locs # View our set of initial locations\n","443b3c84":"# Crash locations - turn the training data into a tensor we can work with\ncrash_locs = tensor(df[['latitude', 'longitude']].values) # To Tensor\n\ncrash_locs = crash_locs.type('torch.FloatTensor')\n\nlen(crash_locs)","9de39c13":"def loss_fn(crash_locs, amb_locs):\n  \"\"\"For each crash we find the dist to the closest ambulance, and return the mean of these dists.\"\"\"\n  # Dists to first ambulance\n  total_dists = []\n  dists_split = crash_locs-amb_locs[0]\n  dists = (dists_split[:,0]**2 + dists_split[:,1]**2)**0.5\n  min_dists = dists\n  for i in range(1, 6):\n    # Update dists so they represent the dist to the closest ambulance\n    dists_split = crash_locs-amb_locs[i]\n    dists = (dists_split[:,0]**2 + dists_split[:,1]**2)**0.5\n    min_dists = torch.min(min_dists, dists)\n  \n  return min_dists.mean()","6b57d015":"# Calculate loss:\nloss = loss_fn(crash_locs, amb_locs)\nloss","e702b4f4":"# We can use .backward to find gradients\nloss.backward()","ea028634":"# View gradients - they've been calculated for us!!!\namb_locs.grad","49bb2c27":"df.columns","bab3243b":"def extract_condition(data_df, cat, condition, kind_num, cat_2, condition_2, cat_3, condition_3):\n    \n    if kind_num == 'single':\n    \n        data_cat = data_df[data_df[cat].isin (condition)]\n        data_cat = tensor(data_cat[['latitude', 'longitude']].values) # To Tensor\n        data_cat = data_cat.type('torch.FloatTensor')\n        \n    elif kind_num == 'double':\n        \n        data_cat = data_df[(data_df[cat] == condition) & (data_df[cat_2] == condition_2)]\n        data_cat = tensor(data_cat[['latitude', 'longitude']].values) # To Tensor\n        data_cat = data_cat.type('torch.FloatTensor')\n        \n    else:\n        \n        data_cat = data_df[(data_df[cat] == condition) & (data_df[cat_2] == condition_2) & (data_df[cat_3] == condition_3)]\n        data_cat = tensor(data_cat[['latitude', 'longitude']].values) # To Tensor\n        data_cat = data_cat.type('torch.FloatTensor')\n        \n        \n        \n    return data_cat","4a6c2cd4":"def extract_data(df, cat, condition, kind, kind_num = 'single', cat_2 = 1 , condition_2 = 1, cat_3 = 1, condition_3 = 1):\n    \n    \n        if kind == 'train':\n    \n            data_df = df[(df.datetime < '2019-01-01')]\n            \n            \n        elif kind == 'dynamic':\n            \n            data_df = df\n            \n        \n        else:\n            \n            data_df = df[(df.datetime > '2019-01-01')]\n            \n                              \n        data_cat = extract_condition(data_df, cat, condition, kind_num, cat_2, condition_2, cat_3, condition_3)\n        \n                             \n        return data_cat","57562223":"def training_routine(train_locs, val_locs, learn_rate, epochs, amb_locs, shuffle_type):\n        \n    # Load crash locs from train into a dataloader\n    dl = DataLoader(train_locs, batch_size= 64, shuffle= shuffle_type)\n        \n    amb_locs.requires_grad_()\n\n    lr = learn_rate\n    \n    n_epochs = epochs\n\n    # Store loss over time\n    train_losses = []\n    val_losses = []\n\n    # Training loop\n    for epoch in range(n_epochs):\n        # Run through batches\n        for crashes in dl:\n            loss = loss_fn(crashes, amb_locs) # Find loss for this batch of crashes\n            loss.backward() # Calc grads\n    \n            amb_locs.data -= lr * amb_locs.grad.data # Update locs\n            amb_locs.grad = None # Reset gradients for next step\n            \n                \n            train_loss = loss_fn(train_locs, amb_locs).item()\n                \n            val_loss = loss_fn(val_locs, amb_locs).item()\n            \n\n                \n            train_losses.append(loss_fn(train_locs, amb_locs).item())\n            val_losses.append(loss_fn(val_locs, amb_locs).item()) \n\n        # Print validation loss\n\n        print('Epoch : {}... Train loss: {:4f}... Val loss : {:4f}...'.\\\n              format(epoch, train_loss, val_loss))\n        \n    return amb_locs, train_losses, val_losses","83eac596":"#Train on 2018, validate on 2019 for static locations\n\n\ntorch.manual_seed(rand)\n    \namb_locs = torch.randn(6, 2) * 0.02\namb_locs = amb_locs + tensor((-1.27, 36.85))\n\n\n# Set up ambulance locations\n\n\n    \nall_df = list(df['3h'].unique())\n\ntrain_locs = extract_data(df, '3h', all_df, 'train')\n\nval_locs = extract_data(df, '3h', all_df, 'val')\n    \namb_locs, train_losses, val_losses = training_routine(train_locs, val_locs, 0.003, 185, amb_locs, shuffle_type = True)       ","26295c98":"amb_locs","38990abf":"plt.figure(num=None, figsize=(16, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(train_losses, label='train_loss')\nplt.plot(val_losses, c='red', label='val loss')\nplt.legend()","605cfc18":"def set_static():\n\n    for ambulance in range(6):\n    \n        sub['A'+str(ambulance)+'_Latitude'] = amb_locs[ambulance].detach().numpy()[0]\n    \n        sub['A'+str(ambulance)+'_Longitude'] = amb_locs[ambulance].detach().numpy()[1]","e7460bfb":"years = df['year']","9b1efa1b":"set_static()","10d04c62":"#Distance Baseline score on test set for all 2019 accidents using static ambulance locations\n\nscore(sub, reference)","186ad1db":"sub.to_csv('baseline_static.csv')","fb353fd0":"#Initial static ambulance locations\n\namb_locs","45146ea9":"#Function for setting new optimized static locations with single condition\n\ndef set_ambulance_single(train_locs, val_locs, learn_rate, epochs, amb_locs, cat, condition):\n    \n    amb_locs_t, _, _ = training_routine(train_locs, val_locs, learn_rate, epochs, amb_locs, shuffle_type = False)\n    \n    for ambulance in range(6):\n        \n        sub.loc[sub[cat] == condition, ['A'+str(ambulance)+'_Latitude']] = \\\n        amb_locs_t[ambulance].detach().numpy()[0]\n    \n        sub.loc[sub[cat] == condition, ['A'+str(ambulance)+'_Longitude']] = \\\n        amb_locs_t[ambulance].detach().numpy()[1]","eaa50bf4":"#Function for setting new optimized static locations with double condition\n\ndef set_ambulance_double(train_locs, val_locs, learn_rate, epochs, amb_locs, cat, condition, cat2, condition2):\n    \n    amb_locs_t, _, _ = training_routine(train_locs, val_locs, learn_rate, epochs, amb_locs, shuffle_type = False)\n    \n    for ambulance in range(6):\n        \n        sub.loc[((sub[cat] == condition) & (sub[cat2] == condition2)), ['A'+str(ambulance)+'_Latitude']] = \\\n        amb_locs_t[ambulance].detach().numpy()[0]\n    \n        sub.loc[((sub[cat] == condition) & (sub[cat2] == condition2)), ['A'+str(ambulance)+'_Longitude']] = \\\n        amb_locs_t[ambulance].detach().numpy()[1]\n        \n\n\n        \n\n        ","0ef0223d":"#Function for setting new optimized static locations with triple condition\n\ndef set_ambulance_triple(train_locs, val_locs, learn_rate, epochs, amb_locs, cat, condition, \\\n                         cat2, condition2, cat3, condition3):\n    \n    amb_locs_t, _, _ = training_routine(train_locs, val_locs, learn_rate, epochs, amb_locs, shuffle_type = False)\n    \n    for ambulance in range(6):\n        \n        sub.loc[((sub[cat] == condition) & (sub[cat2] == condition2) & (sub[cat3] == condition3)), \\\n        ['A'+str(ambulance)+'_Latitude']] = amb_locs_t[ambulance].detach().numpy()[0]\n    \n        sub.loc[((sub[cat] == condition) & (sub[cat2] == condition2) &  (sub[cat3] == condition3)), \\\n        ['A'+str(ambulance)+'_Longitude']] = amb_locs_t[ambulance].detach().numpy()[1]","956c898d":"def create_week_categories(df):\n    \n    if df in ([1, 2]):\n        \n        return 'peak-days'\n    \n    if df in ([3, 4, 5]):\n        \n        return 'mid-peaks'\n    \n    else:\n        \n        return 'none'\n    \n    ","7c6fd221":"df['day-zone'] = df['dayofweek'].apply(create_week_categories)\n\nsub['day-zone'] = sub['dayofweek'].apply(create_week_categories)","266ae974":"sub.to_csv('exp.csv')","b7f65825":"#Setting optimized static locations and dayofweek new best one\n\ndef set_static_week():\n    \n    set_static()\n\n    elements = [(0, 0.009, 400, 'dayofweek'), (6, 0.009, 450, 'dayofweek'), ('mid-peaks', 0.0001, 150, 'day-zone')]\n\n\n    for element in elements:\n    \n    \n        day, lr, epoch, category = element[0], element[1], element[2], element[3]\n\n\n        train_cat = extract_data(df, category,  [day], 'dynamic', 'single')\n    \n        val_cat = extract_data(df, category, [day], 'val', 'single')\n\n        amb_locs_new = amb_locs.clone().detach()\n    \n    \n    \n        set_ambulance_single(train_cat, val_cat,  lr, epoch, amb_locs_new, category, day)\n    \n        print(score(sub, reference))\n        \n    return sub\n        \n    \n        \n    ","2c03f90b":"set_static_week()","cbfd162b":"def create_time_categories(df):\n    \n    if df in ([0, 3, 21]):\n        \n        return 'off-time'\n    \n    if df in ([6, 9]):\n        \n        return 'peak-time'\n    \n    else:\n        \n        return 'mid-time'\n    \n    ","697063bc":"df['time-zone'] = df['3h'].apply(create_time_categories)\n\nsub['time-zone'] = sub['3h'].apply(create_time_categories)\n","e0a149d3":"#Train with best LB one\n\nelements = [(0, 'peak-time', 0.00004, 500), (0, 'mid-time', 0.002, 500), ('peak-days', 'peak-time', 0.007, 700), \\\n            (1, 'mid-time', 0.00004, 600), ('mid-peaks', 'mid-time', 0.00004, 500), (3, 'peak-time', 0.00015, 500),\n            (5, 'mid-time', 0.008, 500)]\n\nfor element in elements:\n    \n    day, time, lr, epoch = element[0], element[1], element[2], element[3]\n    \n    print(day, time)\n    \n    if day in list(df['day-zone'].unique()):\n        \n        day_ver = 'day-zone'\n        \n    else:\n        \n        day_ver = 'dayofweek'\n\n\n    train_cat = extract_data(df, 'time-zone',  time, 'dynamic', 'double', \\\n                                     day_ver, day)\n    \n    val_cat = extract_data(df, 'time-zone', time, 'val', 'double', \\\n                                   day_ver, day)\n        \n    amb_locs_new = amb_locs.clone().detach()\n    \n    \n    set_ambulance_double(train_cat, val_cat, lr, epoch, amb_locs_new, \\\n                                 'time-zone', time, day_ver, day)\n        \n        \n        \n    print('present score {}'.format(score(sub, reference)))\n        ","c28a47db":"#train with new one\n\n#testing\n\nelements = [(0, 3, 0.03, 549), (0, 12, 0.00085, 795), (0, 18, 0.009, 424), (1, 15, 0.008, 450),\n            (2, 3, 0.0002, 400), (2, 6, 0.008, 540), (2, 9,  0.00008, 200), \\\n            (3, 9, 0.001, 400), (3, 12, 0.008, 580), (4, 0, 0.00045, 621), (5, 12, 0.009, 540),\\\n           (6, 12, 0.0035, 569)]\n\nfor element in elements:\n    \n    day, hr, lr, epoch = element[0], element[1], element[2], element[3]\n    \n    print(day, hr)\n    \n    \n    if day == 'mid-peaks':\n        \n        day_ver = 'day-zone'\n        \n    else:\n        \n        day_ver = 'dayofweek'\n\n\n    train_cat = extract_data(df, '3h',  hr, 'dynamic', 'double', \\\n                                     day_ver, day)\n    \n    val_cat = extract_data(df, '3h', hr, 'val', 'double', \\\n                                   day_ver, day)\n        \n    amb_locs_new = amb_locs.clone().detach()\n    \n    \n    set_ambulance_double(train_cat, val_cat, lr, epoch, amb_locs_new, \\\n                                 '3h', hr, day_ver, day)\n        \n        \n        \n    print('present score {}'.format(score(sub, reference)))\n        ","42f045dd":"print(score(sub, reference))","02893555":"sub.to_csv('24_2021_New_optimization.csv')","0cf27336":"# Scoring\n\nYou can re-create the scoring process by creating your own test set and using the following function:","bad33b8c":"### Setting different locations for conditions","e6208fba":"# Overview of Notebook\n\nThe goal of this notebook is to create an optimized ambulance positioning deployment from first optimizing static locations using all the accident locations. We further finetune this static locations in response to external conditions, utilizing cross-validation to judge performance of fine-tuning on certain conditions. \n\nThe idea is for certain conditions, the ambulances can be positioned differently. Not all conditions favour a different positioning but we identify conditions worth optimizing for using a cross-validation approach.","642036a1":"#### Cross Validation:- From analysis, certain days and times are worth setting separate locations but we need to cross-validate these days and not overfit to non-generalizable conditions. We adopt a two step validation approach of first using repeated cross-validation to find days possibly worth setting seperate locations, settings which improve beyond the static baseline approach. We will next train on all data and compare the distance score improvement. We have the highest trust in conditions which improve distance score and mean cross val score and check performance on the LB. Mild trust in conditions which only improve distance score using all the data.","dee5d0a4":"# Upvote this Kernel if you find it useful. \n# More of these are coming to share and to learn from.","e441e4c7":"##### Cross-Validation routine for selected dayofweek & time-zone period (Hyper-parameter tuning)","72571b7b":"#### Cross-Validation of Selected Days of Week and 3h combination Tuning (Hyper-parameter Tuning)","643d0018":"# Helper Functions for setting new ambulance locations per groupings:","561d2b7c":"Cross-Val score of Mid-peaks is better than individual days 3, 4, 5. Minor boost using Mid-peaks to represent mid-accident activity days even on LB.\nMid-peak improves overall score when fine-tuned on entire data. Overall performance improvement setting different times for the mid-peak days.","360c2e43":"##### Cross-Validation Routine for Single Category - Dayofweek\/Day-zone","66fdc83c":"### DATA OVERVIEW:","6d34912f":"#### Single Condition Settings: Day of Week (Using as initial condition to move ambulance and identify days of week worth setting separate locations)","4228ef9b":"From the plot, most accidents are concentrated at Longitude 36.8-37.0 (Centered around 36.9) and Latitude -1.25 to -1.29\n\n\nRather than trying to predict accident locations or the number of accidents in a given timeframe, we are instead asked to come up with an ambulance deployment schedule that will minimise distance travelled. \n\nThis is of course a simplification of the problem, but by solving this you will hopefully develop strategies that can be used going forward.","ecf252c4":"These gradients tell us which way to move the locations to reduce the loss. This is all we need to do some gradient descent! We'll pick a learning rate, and then run through 1000 steps, updating the locations each time:","301ba4cf":"# Stochastic Gradient Descent and a Validation Set\n\nWe can train with mini-batches (SGD) to speed things up. We'll also use the 2019 crashes as a validation set and track how well we do on that over time. ","0ee1835d":" ##### Validation Routine on full dataset for Selected Days of Week:","aba8708b":"**Distance Baseline score on test set for all 2019 accidents using static ambulance locations**","d80a105d":"# What Next?\n\nThere are many ways you could approach this challenge. For example, you could:\n- Create a model to predict the liklihood of an accident given a location, the features of the nearby road segments, the weather, the traffic speed and the time of day\n- Use this model to predict the probability of crashes for different times+locations over the test period, and then sample from this probability distribution to generate plausible crash locations.\n- Use an optimization technique to minimise the travel distance for the ambulances to your fake crash points. \n\nThis might seem too complicated - perhaps simply picking 6 fixed locations based on the existing data will be enough? Or maybe it's worth setting separate locations during busy traffic times? \n\nPerhaps a Reinforcement Learning aproach will do well? \n\nOr maybe you can see a totally different way to solve this!\n\nShare your ideas on the discussion boards, and let's see how innovative we can be :)","0f686402":"### Selected Days of Week and Time-zone combination Tuning:","e20703c1":"# Uber Nairobi Ambulance Perambulation Challenge\n\n# Can you use ML to create an optimised ambulance deployment strategy in Nairobi?","c1b48ebb":"# Static Ambulance positioning","66e6d520":"# Finding static locations with Gradient Descent (Baseline Approach) \n\nCredits: JonathanWhitaker\n\nTo do the gradient descent we'll rely on PyTorch to calculate gradients. If you're not familiar with tensors and this part looks scary, check out the fastai lossons that cover SGD from scratch (Lessons 3 and 4 of the new course and this book chapter:https:\/\/github.com\/fastai\/fastbook\/blob\/master\/04_mnist_basics.ipynb). This is where I got a lot of the inspiration for this code.\n\nI start with 6 random locations that are close to the center of the crashes - plot these over the crashes and see if this initialization seems sensible.","2e67b663":"# Testing our scoring function: Positioning the ambulances at 6 locations around center of the street","329e4e67":"Static Distance - 90.78796\n\nStatic Cross-Val mean - 18.17185896606233\n\nObservation- Based on cross-validation results and distance validation results, certain days of the week are better individually tuned. Significant boost in validation performance compared to other days of the week.\n\nSelected Days of Week from tuning- 0 & 6 show the largest boost from cross-validation and LB performance. 3, 4, 5 show small improvement when tuned individually.\n\nTry binning the 3,4,5 days of relatively high accidents, mid-peaks.\n\nTry binning other daysofweek & check cross-validation performance","dfc5ab33":"### Build Baseline Model: Building baseline model of determining static locations not dependent on external factors \n### and optimzing static distance to accident locations in train data.","7797be2d":"Helper Functions: For data extraction and training-","7cc1cecb":"# The Challenge\n\nThe main dataset for this challenge (in Train.csv) is a record of crashes within Nairobi over 2018-19. Each crash has an associated datetime and location. \n\nWe are challenged to determine locations to position 6 ambulances that will minimize the distance to recorded accidents. Train data contains an accident datetime with the associated latitude and longitude of the accident location between 2018 to early 2019.\n\nWhile test set contains the accident datetime without the associated latitude and longitude. In order to build a useful model, we need a way to evaluate for each datetime and locations, the probability of an accident occuring in order to recommend ambulance locations for a 3hrs duration.\n\nWe approach the problem by checking our score when we position our ambulances randomly in train data, then position our ambulances at the accident hotspots in train data. We compare the performance then optimize the distance using gradient descent.\n\nThe solution utilizes the optimized static locations from gradient descent and shifts the locations in reponse to different external conditions.","68ebcf31":"# Moving static locations in response to external factors with Gradient Descent (Conditions Groupings) \n\nUsing the ambulance locations determined on the general datatset, can we move the ambulance postions in response to surrounding group factors. e.g dayofweek, 3h, weather conditions.","f8500d44":"# Setting optimized static locations"}}