{"cell_type":{"a4a2bd42":"code","ad0dddc3":"code","428b0011":"code","6dc79147":"code","03524e0c":"code","d6a4cb2f":"code","86c8da29":"code","e80a5542":"code","ff62a687":"code","e6fbc2c5":"code","5a67e490":"code","1ba3f9ff":"code","fcbf6f2d":"code","dd784ce2":"code","d2f68a1e":"code","243fa2f2":"code","632fa522":"code","aebba9a8":"code","b464efc7":"code","1beedd28":"code","a573d3d7":"markdown","bf4cc9d8":"markdown","7cef3e72":"markdown","a80e8dcc":"markdown","4bbfcfa7":"markdown","00833664":"markdown"},"source":{"a4a2bd42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad0dddc3":"# Import all necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","428b0011":"# Read our data\ndata = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","6dc79147":"# Take a look on our data\ndata.head()","03524e0c":"# All features scaled but Time and Amount.\ndata.describe()","d6a4cb2f":"# No missing values\ndata.isnull().sum()","86c8da29":"# All features of type float64\ndata.info()","e80a5542":"# Check how many samples we have from each class\nnum_fraud = len(data.loc[data['Class'] == 1])\nnum_valid = len(data.loc[data['Class'] == 0])\nprint(f'{num_fraud} deals are fraud.')\nprint(f'{num_valid} deals are valid.')","ff62a687":"# Let's make it even - downsample\nfrauds = data.loc[data['Class'] == 1]\n\nvalid = data.loc[data['Class']==0].sample(len(frauds))\n\nnew_data = frauds.append(valid)","e6fbc2c5":"# Sanity check\nnum_fraud = len(new_data.loc[new_data['Class'] == 1])\nnum_valid = len(new_data.loc[new_data['Class'] == 0])\nprint(f'{num_fraud} deals are fraud.')\nprint(f'{num_valid} deals are valid.')","5a67e490":"# Normalize Time and Amount\n\ntime_mean, time_std = new_data.Time.mean(), new_data.Time.std()\namount_mean, amount_std = new_data.Amount.mean(), new_data.Amount.std()\n\nnorm_Time = (new_data['Time'] - time_mean) \/ time_std\nnorm_Amount = (new_data['Amount'] - amount_mean) \/ amount_std","1ba3f9ff":"# Replace the normalized features with the old ones\nnew_data['norm_Time'] = norm_Time\nnew_data['norm_Amount'] = norm_Amount\n\nnorm_data = new_data.drop(['Time', 'Amount'], axis=1)","fcbf6f2d":"norm_data.describe()","dd784ce2":"# Shuffle before making any splits and turn into numpy tensor\nnorm_data = norm_data.sample(frac=1)\n\n# Split to independent and dependent features + convert to numpy tensor\nX = norm_data.drop(['Class'], axis=1).to_numpy(dtype='float32')\ny = norm_data['Class'].to_numpy(dtype='float32')","d2f68a1e":"# Split the data into train & test.\nsplit_idx = round(len(X)*0.2)\n\nx_test = X[:split_idx]\nx_train = X[split_idx:]\ny_test = y[:split_idx]\ny_train = y[split_idx:]\n\nprint(f'Train size={len(x_train)}\\nTest size={len(x_test)}')","243fa2f2":"from keras import models\nfrom keras import layers\n\n# Given train data and train labels, performs kfold cross validation and returns the list of accuracies.\n\ndef build_model(input_size):\n    model = models.Sequential()\n    model.add(layers.Dense(32, activation='relu', input_shape=(input_size,)))\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\ndef kfold_cv(X, y, k, epochs):\n    k = 4\n    num_val_samples = len(X) \/\/ k\n    num_epochs = epochs\n    all_scores = []\n    for i in range(k):\n        print('processing fold #', i)\n        val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n        val_targets = y[i * num_val_samples: (i + 1) * num_val_samples]\n        partial_train_data = np.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n        partial_train_targets = np.concatenate([y[:i * num_val_samples],y[(i + 1) * num_val_samples:]],axis=0)\n        \n        model = build_model(X.shape[1])\n        model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n        model.fit(partial_train_data, partial_train_targets,\n        epochs=num_epochs, batch_size=1, verbose=1)\n        val_loss, val_acc = model.evaluate(val_data, val_targets, verbose=0)\n        all_scores.append(val_acc)\n    return all_scores","632fa522":"results = kfold_cv(x_train, y_train, 4, 20)","aebba9a8":"print(f'Cross validation accuracies = {results}')\nprint(f'Mean accuracy = {np.mean(results)}')","b464efc7":"# Train again, but on the entire training set to use more samples\n\nmodel = build_model(x_train.shape[1])\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.fit(x_train, y_train,epochs=20, batch_size=1, verbose=1)\n","1beedd28":"# Evaluate on test set\nevaluation = model.evaluate(x_test, y_test, verbose=0)\nprint(f'Evaluation on test set:\\nloss = {evaluation[0]},\\naccuracy = {evaluation[1]}')","a573d3d7":"**Important things to remember**:\n\n1. Don't forget to convert your DataFrame to a numpy tensor using to_numpy() and the dtype to float32.\n2. You should experiment with the architecture you choose for your model in order to see which one fits your task best. By that I mean playing with the number of layers, number of units per layer etc.\n3. On binary classification tasks like this, the last layer should have 1 unit and the activation function should be sigmoid as it squeezes the output to a value between [0,1] which represents probabilities. Also, your loss should be binary_crossentropy.\n4. rmsprop is a good optimizer usually but you can also use Adam.\n5. Overfitting - If your model overfits your training set after just a few epochs, it may be too complex. Try reducing the number of layers or units per layer.\n6. Underfitting - If your model underfits your training set, try adding layers or increasing the number of units per layer.\n7. There is no magic formula for which architecture is best. You should always try to overfit your training set first and from there start experimenting with your hyperparameters (model size, number of epochs, batch_size etc..)\n8. When you have enough data for training you can use a Holdout set, but when you have a small dataset you should use KFold CV.\n9. When you're done training and ready to evaluate your model on the test set you should first train 1 more time on the entire training set (train & validation).","bf4cc9d8":"There are two common ways to evaluate our model's performance before checking on the test set:\n1. Hold-out set: Split the training data into train & validation sets (usually 80-20 is a good ratio), train on the training set and evaluate on validation set. This method is good when we have enough data. In our case we only have 787 samples on the training set so a different approach is recommended:\n2. KFold cross validation: When we don't have enough labeled data - the results on the validation set may vary significantly (as we will soon see). In such cases this is the recommended way to evaluate model's performance.\n\nTo read more on those methods you should refer to the book mentioned in the 'References' section, or google it - there are many sources that explain it well.","7cef3e72":"**Note:**\nI myself am pretty new to this field. All I did was to try and implement the basics I've learned in the book from the 'References' section on this binary classification task.\n\nAny comment\/tip\/critisism would be appreciated - Thank you for reading!","a80e8dcc":"The data is highly unbalanced. In order to get more accurate models we should always aim to feed it data as balanced as possible, i.e equal number of samples per class.","4bbfcfa7":"We can see that the accuracies vary from 90% to almost 95%. This is due to the small size of our training set. The mean accuracy - 92% is the most accurate measure among all scores.","00833664":"**References**:\n\nDeep Learning with Python by Fran\u00e7ois Chollet."}}