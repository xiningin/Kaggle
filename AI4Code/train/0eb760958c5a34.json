{"cell_type":{"12d0bbc1":"code","d47e2fb7":"code","a54d7acc":"code","934df691":"code","57ad19ee":"code","c919bc9d":"code","5936f823":"markdown","b11f9f62":"markdown","c9062d42":"markdown"},"source":{"12d0bbc1":"from typing import Optional, Tuple\n\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\n\nfrom sklearn.base import BaseEstimator, ClusterMixin\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.datasets import make_blobs, make_regression\nimport holoviews as hv\n\nhv.extension('bokeh')","d47e2fb7":"class SOM(BaseEstimator, ClusterMixin):\n    def __init__(self, shape: Tuple[int] = (3, 3), \n                 lattice: str = 'hexagonal_lattice_graph', \n                 n_iter: int = 100, \n                 learning_rate: float = 0.25, \n                 cooperative_learning_rate: float = 0.125, \n                 n_jobs: Optional[int] = None):\n        \n        self.lattice = lattice\n        self.shape = shape\n        self.learning_rate = learning_rate\n        self.cooperative_learning_rate = cooperative_learning_rate\n        self.n_jobs = n_jobs\n        self.n_iter = n_iter\n        \n    def fit(self, X: np.ndarray):\n        # initialize graph\n        self.graph_ = getattr(nx.generators.lattice, self.lattice)(*self.shape)\n        \n        # initialize weights\n        mean, var = X.mean(0), X.var(0)\n        for key in self.graph_.nodes:\n            self.graph_.nodes[key]['weight'] = np.random.normal(mean, var)\n            \n        for schedule in range(1, self.n_iter):\n            for x in X:\n                weights_dict = nx.get_node_attributes(self.graph_, 'weight')\n                self.weights_ = np.vstack(list(weights_dict.values()))\n\n                # competition\n                argmin = int(pairwise_distances(X = x.reshape(1, -1), Y = self.weights_,\n                                                n_jobs=self.n_jobs)\n                             .argmin())\n                min_node_key = list(weights_dict.keys())[argmin]\n\n                self.graph_.nodes[key]['weight'] -= self.learning_rate * (self.graph_.nodes[key]['weight'] - x) \/ schedule\n\n                # cooperation\n                for key in self.graph_.neighbors(min_node_key):\n                    self.graph_.nodes[key]['weight'] -= self.cooperative_learning_rate * (self.graph_.nodes[key]['weight'] - x) \/ schedule\n                    \n    def predict(self, X):\n        return (pairwise_distances(X = X,\n                                   Y = self.weights_,\n                                   n_jobs=self.n_jobs)\n                .argmin(1))\n        \n            \n            ","a54d7acc":"def get_blobs_on_scurve(n_samples = 2500,\n                        noise = 0.01,\n                        centers=2):\n    x_gaussian_blobs, y_gaussian_blobs = make_blobs(n_samples,  n_features=1, centers=centers)\n    x_gaussian_blobs = x_gaussian_blobs.flatten()\n    \n    clipped_ = (x_gaussian_blobs - x_gaussian_blobs.min())\/x_gaussian_blobs.max()\n\n    t = 3 * np.pi * ( clipped_ - clipped_.mean() )\n    x = np.sin(t)\n    y = 2.0 * (np.random.rand(1, n_samples) - 0.5)\n    z = np.sign(t) * (np.cos(t) - 1)\n\n    X = np.column_stack((x.reshape(-1,1),\n                         y.reshape(-1,1),\n                         z.reshape(-1,1)))\n    X += noise * np.random.randn(1, n_samples).reshape(-1,1)\n    t = np.squeeze(t)\n    \n    return X, y_gaussian_blobs\n\nX, y = get_blobs_on_scurve()","934df691":"hv.extension('matplotlib')\n(hv.Scatter3D(pd.DataFrame(X, columns=['x','y','z'])\n              .assign(cluster=y), kdims=['x','y'], vdims=['z', 'cluster'])\n .opts(color='cluster', title='Blobs along S-curve Manifold'))","57ad19ee":"som = SOM((15, 15), 'grid_2d_graph', learning_rate= 0.2, cooperative_learning_rate=0.1, n_iter=25, n_jobs=None)\nsom.fit(X)","c919bc9d":"hv.extension('bokeh')\ngrid = np.zeros(som.shape)\nfor key in som.graph_.nodes:\n    row, column = list(key)\n    for neighbour in som.graph_.neighbors(key):\n        grid[row, column] += pairwise_distances(X = som.graph_.nodes[key]['weight'].reshape(1, -1),\n                                                Y = som.graph_.nodes[neighbour]['weight'].reshape(1, -1))\n        \nhv.Image(grid).opts(title='Density along Lattices')","5936f823":"In order to better understand this idea of wrapping a manifold with a lattice, we are going to have to generate data. Here we are going to take our classical S-curve manifold and try to generate Gaussian Blobs along the surface of the this manifold. ","b11f9f62":"After fitting this model, we can then compute the distances between neighbours along the lattice to visualize the density accross this lattice and Voila. ","c9062d42":"What are Self-organising Maps (SOMS)?  That's a question I always stuggle to answer.  Are they a culstering algorithm, are they meant for dimensionality reduction, are they meant just for visualization? They are used for all of the above and what's wild is that they are an artificial neural network unlike any you have seen before. I have often been asked to describe SOMS to people before and have struggled, but a year ago discovered a great analogy for how SOMs work.  \n  \nA common assumption in the Data Sciences is the Manifold Assumption. This assumption proposes that that data which exists in high-dimensions actually exists on a low-dimensional subspace. For humans, we live on a low dimensional subspace. The earth is a big 3d ball but we only live on the surface and seldom travel straight through the centre of earths melton core. Using SOMs we are looking to model the data not in the original space but in this subspace.  In order to do this we initialize a lattice and slowly, but surely, move and spread this net over our data points. This process of spreading relies on a two methods of competitive and cooperative learning, and is can be thought of as a mechanism to wrap our subspace using our lattice- like wrapping a ball with a piece of paper. \n\nUsing our competitive and cooperative learning steps, we slowly iterate through our dataset, find the node on our lattice closest to it and move both it and its neighbours along the lattice closer to that datapoints. This learning rate, the mechanism for deciding which neighbours get moved and by how much and the shape of our lattice are the critical hyperparameters which are critical to the success and insights of SOM.  "}}