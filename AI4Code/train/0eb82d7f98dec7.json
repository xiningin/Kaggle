{"cell_type":{"4ad24940":"code","dc997aa4":"code","6a199547":"code","8bf67b8c":"code","a4c4a6a7":"code","88fb476e":"code","9ca71e1a":"code","3b808062":"code","3ca70bc2":"code","93f05942":"code","78ff275f":"code","867c34ce":"code","db781238":"code","8f93150c":"code","8e748bc9":"code","615a42d5":"code","fab2e763":"code","7a5558ec":"code","3800bd59":"code","f0513f67":"code","25442169":"code","90028fe0":"code","2ef99fdb":"code","6cfdd43c":"code","9cd1de5c":"code","57d1cc9d":"code","87499305":"code","e819013d":"code","d77aac7d":"code","669c33a1":"code","42afc016":"code","5245958a":"code","42487e6f":"markdown","b54b0056":"markdown","f87d043e":"markdown","dc7aa524":"markdown","88f0e37a":"markdown","8fa849e9":"markdown","3e568f7a":"markdown","d35949e7":"markdown","85d11d1a":"markdown","2617913b":"markdown","3058611e":"markdown","da0afb7f":"markdown","ec4c9a56":"markdown","b6841219":"markdown","d44b9ab0":"markdown"},"source":{"4ad24940":"#Importing all the necessary libraries\nimport numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","dc997aa4":"df=pd.read_csv('..\/input\/disaster-tweets\/tweets.csv')\nprint('The shape of the dataset=',df.shape)\ndf.head()","6a199547":"#Basic EDA\ndf.isnull().sum()","8bf67b8c":"#Exploring the target columns\ndf['target'].value_counts()","a4c4a6a7":"sns.barplot(df['target'].value_counts().index,df['target'].value_counts(),palette='rocket')","88fb476e":"# A Disaster tweet\ndisaster_tweets=df[df['target']==1]['text']\ndisaster_tweets.values[1]","9ca71e1a":"# Non Disaster tweets\nnondisaster_tweets=df[df['target']==0]['text']\nnondisaster_tweets.values[1]","3b808062":"sns.barplot(y=df['keyword'].value_counts()[:30].index,x=df['keyword'].value_counts()[:30])","3ca70bc2":"df.loc[df['text'].str.contains('disaster',na=False,case=False)].target.value_counts()","93f05942":"df['location'].value_counts().head(10)","78ff275f":"# Replacing the ambigious locations name with Standard names\ndf['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                           \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"England, United Kingdom\":'UK',\n                            \"Mumbai, India\":'India',\n                            \"Melbourne,Victoria\":'Australia'},inplace=True)\nsns.barplot(y=df['location'].value_counts()[:10].index,x=df['location'].value_counts()[:10])","867c34ce":"# Checking the text data\ndf['text'][:5]","db781238":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ndf['text'].head()","8f93150c":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(nondisaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","8e748bc9":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","615a42d5":"#Tokenize the dataset\ntokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ndf['text']=df['text'].apply(lambda x:tokenizer.tokenize(x))\ndf['text'].head()","fab2e763":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ndf['text'] = df['text'].apply(lambda x : remove_stopwords(x))\ndf.head()","7a5558ec":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","3800bd59":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf['text'] = df['text'].apply(lambda x : combine_text(x))\ndf.head()","f0513f67":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","25442169":"# Splitting the data into independent and dependent features\nX=df['text']\ny=df['target']","90028fe0":"X.head()","2ef99fdb":"X_train,X_test,y_train,y_test=model_selection.train_test_split(X,y,test_size=0.2,random_state=1)\nvectorizer=CountVectorizer()\nx_train_vectors=vectorizer.fit_transform(X_train)\nx_test_vectors=vectorizer.transform(X_test)","6cfdd43c":"X_train.head()","9cd1de5c":"x_train_vectors.todense()","57d1cc9d":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(X_train)\ntest_tfidf = tfidf.transform(X_test)","87499305":"train_tfidf","e819013d":"test_tfidf","d77aac7d":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=0.5,random_state=1,class_weight={0:1,1:5})\nclf.fit(x_train_vectors,y_train)\npred=clf.predict(x_test_vectors)\naccuracy_score_train=metrics.accuracy_score(y_train,clf.predict(x_train_vectors))\nprint(accuracy_score_train)\naccuracy_score_test=metrics.accuracy_score(y_test,pred)\nprint(accuracy_score_test)\nclassification_report_train=metrics.classification_report(y_train,clf.predict(x_train_vectors))\nprint(classification_report_train)\nclassification_report_test=metrics.classification_report(y_test,pred)\nprint(classification_report_test)\nroc_auc_score_train=metrics.roc_auc_score(y_train,clf.predict(x_train_vectors))\nprint(roc_auc_score_train)\nroc_auc_score_test=metrics.roc_auc_score(y_test,pred)\nprint(roc_auc_score_test)\nconfusion_matrix_train=metrics.confusion_matrix(y_train,clf.predict(x_train_vectors))\nprint(confusion_matrix_train)\nconfusion_matrix_test=metrics.confusion_matrix(y_test,pred)\nprint(confusion_matrix_test)","669c33a1":"# Fitting A Logistic Regression model on TF-IDF\nclf_tfidf = LogisticRegression(C=1.0,random_state=1,class_weight={0:1,1:5})\nclf_tfidf.fit(train_tfidf,y_train)\npred=clf_tfidf.predict(test_tfidf)\naccuracy_score_train=metrics.accuracy_score(y_train,clf_tfidf.predict(train_tfidf))\nprint(accuracy_score_train)\naccuracy_score_test=metrics.accuracy_score(y_test,pred)\nprint(accuracy_score_test)\nclassification_report_train=metrics.classification_report(y_train,clf_tfidf.predict(train_tfidf))\nprint(classification_report_train)\nclassification_report_test=metrics.classification_report(y_test,pred)\nprint(classification_report_test)\nconfusion_matrix_train=metrics.confusion_matrix(y_train,clf_tfidf.predict(train_tfidf))\nprint(confusion_matrix_train)\nconfusion_matrix_test=metrics.confusion_matrix(y_test,pred)\nprint(confusion_matrix_test)\nroc_auc_score_train=metrics.roc_auc_score(y_train,clf_tfidf.predict(train_tfidf))\nprint(roc_auc_score_train)\nroc_auc_score_test=metrics.roc_auc_score(y_test,pred)\nprint(roc_auc_score_test)","42afc016":"clf_naive=MultinomialNB(alpha=0.2,fit_prior=False)\nclf_naive.fit(x_train_vectors,y_train)\npred=clf_naive.predict(x_test_vectors)\naccuracy_score_train=metrics.accuracy_score(y_train,clf_naive.predict(x_train_vectors))\nprint(accuracy_score_train)\naccuracy_score_test=metrics.accuracy_score(y_test,pred)\nprint(accuracy_score_test)\nclassification_report_train=metrics.classification_report(y_train,clf_naive.predict(x_train_vectors))\nprint(classification_report_train)\nclassification_report_test=metrics.classification_report(y_test,pred)\nprint(classification_report_test)\nroc_auc_score_train=metrics.roc_auc_score(y_train,clf_naive.predict(x_train_vectors))\nprint(roc_auc_score_train)\nroc_auc_score_test=metrics.roc_auc_score(y_test,pred)\nprint(roc_auc_score_test)\nconfusion_matrix_train=metrics.confusion_matrix(y_train,clf_naive.predict(x_train_vectors))\nprint(confusion_matrix_train)\nconfusion_matrix_test=metrics.confusion_matrix(y_test,pred)\nprint(confusion_matrix_test)","5245958a":"clf_naive_tf=MultinomialNB(alpha=0.2,fit_prior=False)\nclf_naive_tf.fit(train_tfidf,y_train)\npred=clf_naive_tf.predict(test_tfidf)\naccuracy_score_train=metrics.accuracy_score(y_train,clf_naive_tf.predict(train_tfidf))\nprint(accuracy_score_train)\naccuracy_score_test=metrics.accuracy_score(y_test,pred)\nprint(accuracy_score_test)\nclassification_report_train=metrics.classification_report(y_train,clf_naive_tf.predict(train_tfidf))\nprint(classification_report_train)\nclassification_report_test=metrics.classification_report(y_test,pred)\nprint(classification_report_test)\nroc_auc_score_train=metrics.roc_auc_score(y_train,clf_naive_tf.predict(train_tfidf))\nprint(roc_auc_score_train)\nroc_auc_score_test=metrics.roc_auc_score(y_test,pred)\nprint(roc_auc_score_test)\nconfusion_matrix_train=metrics.confusion_matrix(y_train,clf_naive_tf.predict(train_tfidf))\nprint(confusion_matrix_train)\nconfusion_matrix_test=metrics.confusion_matrix(y_test,pred)\nprint(confusion_matrix_test)","42487e6f":"Let's see how often the word 'disaster' come in the dataset and whether this help us in determining \nwhether a tweet belongs to a disaster category or not.","b54b0056":"# Naives Bayes Classifier\n\nWell, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes.","f87d043e":"Exploring the 'keyword' column\n\nThe keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data","dc7aa524":"# Importing All the necessary Libraries","88f0e37a":"Exploring the 'location' column\n\nEven though the column location has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning.","8fa849e9":"3. Stopwords Removal\u00b6\n\nNow, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like a, an, the, are etc.","3e568f7a":"Token normalization\n\nToken normalisation means converting different tokens to their base forms. This can be done either by:\n\nStemming : removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\n\nLemmatization : Returns the base or dictionary form of a word, which is known as the lemma","d35949e7":"The columns denote the following:\n\n1. The text of a tweet\n\n2. A keyword from that tweet\n\n3. The location the tweet was sent from\n\n4. target-Whether it is a disaster tweet or not","85d11d1a":"Exploring the Target Column\n\n-Distribution of the Target Column\n-We have to predict whether a given tweet is about a real disaster or not. - If so, predict a 1. If not, predict a 0.","2617913b":"Getting it all together- A Text Preprocessing Function\n\nThis concludes the pre-processing part. It will be prudent to convert all the steps undertaken into a function for better reusability.","3058611e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da0afb7f":"Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets.","ec4c9a56":"Exploring the Target Column Let's look at what the disaster and the non disaster tweets look like","b6841219":"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques","d44b9ab0":"**Reading the Dataset**"}}