{"cell_type":{"08d82019":"code","3ae2779b":"code","35a8bd7f":"code","e54dc1b4":"code","47857c86":"code","aabd3cee":"code","e89a4f2e":"code","11209eec":"code","77e0738d":"code","73d85aa4":"code","79bbf80c":"code","f577bf95":"markdown","ed3a6b4b":"markdown","ff3d681b":"markdown","6c0a626e":"markdown","ecd72d59":"markdown","57be5e6f":"markdown","7bb77ef7":"markdown","f4d12d5b":"markdown","f33c556c":"markdown"},"source":{"08d82019":"# !nvidia-smi\n# from google.colab import drive\n\n# drive.mount('\/content\/drive', force_remount=True)\n# !mkdir dataset\n\n# !cp \/content\/drive\/MyDrive\/Research\/commonlit\/*.csv dataset\n\n\n# !ls dataset\n# !pip3 install -q transformers tensorboard_logger seqeval sentencepiece tokenizers sentence_transformers\n","3ae2779b":"# import sys\n# DRIVE_DIR=\"\/content\/drive\/MyDrive\/Research\/commonlit\/\"\n# sys.path.insert(0, DRIVE_DIR)\n# from utils import seed_everything, save_model_weights, count_params","35a8bd7f":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" \nimport time, torch, random, glob, re, gc, datetime, tokenizers, pdb\n\nimport numpy as np\nimport transformers\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tokenizers import *\nfrom transformers import *\nfrom functools import partial\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom torch.nn import functional as F\nfrom itertools import cycle, chain\nfrom torch.utils.data import Dataset, DataLoader, IterableDataset, TensorDataset\nfrom sklearn.model_selection import train_test_split, RepeatedKFold, KFold\nfrom ast import literal_eval as eval","e54dc1b4":"class Config:\n\n    random_state=2021\n    k=4\n    device=\"cuda\"\n    # selected_folds=[3,4]\n    selected_folds=list(range(k))\n    seed = 2021\n    model=\"bert-base-uncased\"\n    # checkpoint=\"\/content\/drive\/MyDrive\/Research\/commonlit\/2021-06-24\/Regression_roberta-base_ fold - 5_bs768.pt\"\n    checkpoint=\"\/content\/drive\/MyDrive\/Research\/commonlit\/2021-06-20\/Regression_bert-base-cased_fold-1_epoch-10_full_model.pt\"\n#     checkpoint=None\n    task=\"Regression\"\n    pretrained=True\n    lowercase = False\n\n    num_labels=1\n    batch_size = 768\n    batch_size_val = int(batch_size * 1.5)\n    weight_decay = 1.\n    \n    epochs = 100\n    lr = 1e-3\n    warmup_prop = 0.1\n    save_every_epoch=10\n\n    \n    freeze_transformer=True #freeze the transformer model and train only the final dense layer\n    model_names=[\"transformer.\"+i for i in [\"roberta\", \"bert\", \"albert\", \"transformer\", \"distilbert\"]]\n\n\nCP_DIR=Path(\"\/content\/drive\/MyDrive\/Research\/commonlit\")\nNUM_WORKERS = 2\nDATA_DIR=Path(\"..\/input\/commonlitreadabilityprize\/\")\nTEST=DATA_DIR\/\"test.csv\"\nTRAIN=DATA_DIR \/ \"train.csv\"","47857c86":"TRANSFORMERS={\n    \"roberta-base\":{\n        \"model_config\":(RobertaForSequenceClassification, RobertaConfig),\n        \"tokenizer\":RobertaTokenizer,\n    },\n    \"bert-base-cased\":{\n        \"model_config\":(BertForSequenceClassification, BertConfig),\n        \"tokenizer\":BertTokenizer,\n    },\n    \"bert-base-uncased\":{\n        \"model_config\":(BertForSequenceClassification, BertConfig),\n        \"tokenizer\":BertTokenizer,\n        },\n    \"albert-base-v2\":{\n        \"model_config\":(AlbertForTokenClassification,AlbertConfig),\n        \"tokenizer\":AlbertTokenizer,\n    },\n    \"gpt2\":{\n        \"model_config\":(GPT2ForSequenceClassification, GPT2Config),\n        \"tokenizer\":GPT2Tokenizer,\n    },\n    \"distilbert-base-cased\":{\n        \"model_config\":(DistilBertForSequenceClassification, DistilBertConfig),\n        \"tokenizer\":DistilBertTokenizer,\n        \n    }\n}","aabd3cee":"def get_checkpoint_dir():\n    today=str(datetime.date.today())\n    checkpoint_dir=CP_DIR\/today\n\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n    return checkpoint_dir\n\ndef checkpoint_name():\n      return Config.task+\"_\"+Config.model\n\ndef save_log(list_, logdir):\n    if os.path.exists(logdir):\n        mode=\"a\"\n    else:\n        mode=\"w\"\n    with open(logdir, mode) as f:\n        f.writelines(\"\\n\".join(list_))\n        f.writelines(\"\\n\")\n    \ndef load(model, with_checkpoint=None):\n    model=Transformer(model)\n    if with_checkpoint:\n        checkpoint=torch.load(with_checkpoint, map_location=\"cpu\")\n        model.load_state_dict(checkpoint)\n        print(\"Checkpoint loaded!\", end=\"\\r\")\n    return model\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n        \n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    \ndef save_model_weights(model, filename, cp_folder=\"\"):\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))\n    print(f\"Saved weights to {os.path.join(cp_folder, filename)}!!!\\n\")","e89a4f2e":"class ComprehensionDataset(IterableDataset):\n    def __init__(self, df, tokenizer, shuffle=False, max_len=256, train=True, normalize_label=False):\n        if shuffle:\n            self.df=df.sample(frac=1).reset_index(drop=True)\n        else:\n            self.df=df\n            \n        targets=df['target']\n            \n        self.tokenizer=tokenizer\n        self.ids=list(range(len(df)))\n        self.maxlen=max_len\n        self.train=train\n\n        if not tokenizer.pad_token_id:\n            #GPT2\n            self.pad_token=tokenizer.eos_token_id\n        else:\n            self.pad_token=tokenizer.pad_token_id\n          \n    def __len__(self):\n        return len(self.ids)\n    \n    def pad(self, array):\n        end_=len(array)%self.maxlen\n        if end_>0:\n            full_len=((len(array)\/\/self.maxlen)+1)*self.maxlen\n        else:\n            full_len=self.maxlen\n             \n        newArray=np.full(full_len, self.pad_token)\n        end=end_+(len(array)\/\/self.maxlen)*self.maxlen\n        newArray[:end]=array\n        return newArray, end_\n    \n    def chunks(self, list_):\n\n        \"for an array longer than the maxlen, this function returns a 2d array bending the 1d\"\n        l=len(list_)\n        n=self.maxlen\n        if l%n>0:\n            N=n*((l\/\/n)+1)\n        else:\n            N=l-l%n\n        for i in range(0, N, n):\n            yield np.array(list_[i:i+n], dtype=\"long\")\n\n    \n    \n    def getitems(self, idx):\n        row=self.df.iloc[idx]\n        row_id=row['id']\n        input_ids=self.tokenizer.encode(row['excerpt'])\n        target=self.targets.iloc[idx]\n  \n        input_ids, end=self.pad(input_ids)\n        input_ids=torch.LongTensor(list(self.chunks(input_ids)))\n        shape=input_ids.shape\n        \n        attention_mask=torch.ones((shape[0]-1, shape[1]))\n        last_mask=torch.zeros(self.maxlen)\n        last_mask[:end]=torch.ones(end)\n        attention_mask=torch.cat([attention_mask, last_mask.unsqueeze(0)]).long()\n        \n        for i, batch in enumerate(input_ids):\n            yield {\n                \"id\":row_id,\n                \"input_ids\":torch.as_tensor(input_ids[i], dtype=torch.long),\n                \"attention_mask\":torch.as_tensor(attention_mask[i], dtype=torch.long),\n                \"target\":torch.as_tensor(target, dtype=torch.float),\n            }\n\n    \n    \n    def get_stream(self, ids):\n        yield from chain.from_iterable(map(self.getitems, ids))\n        \n    def __iter__(self):\n        return self.get_stream(self.ids)\n    ","11209eec":"class Transformer(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.name = model\n        model_type, config_type=TRANSFORMERS[model]['model_config']\n        if Config.pretrained:\n            self.transformer=model_type.from_pretrained(model, output_hidden_states=True, num_labels=Config.num_labels)\n        else:\n            config_file=TRANSFORMERS[model]['config']\n            config=config_type.from_json_file(config_file)\n            config.num_labels=Config.num_labels\n            config.output_hidden_states=True\n            self.transformer=model_type(config)\n\n    def forward(self, input_ids, attention_mask=None):\n        output=self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n        )\n        return output\n","77e0738d":"def fit(model,train_dataset,val_dataset, fold, epochs,batch_size, weight_decay=0,warmup_prop=0.0,lr=5e-4):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=NUM_WORKERS)\n\n    #just the iterabledataset things :D\n    val_loader_len=0\n    train_loader_len=0\n    for batch in train_loader:\n        train_loader_len+=1\n    for batch in val_loader:\n        val_loader_len+=1\n\n    \n    opt_params = []\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    for n, p in model.named_parameters():\n        wd = 0 if any(nd in n for nd in no_decay) else weight_decay\n        opt_params.append(\n            {\"params\": [p], \"weight_decay\": wd, \"lr\": lr}\n        )\n\n    optimizer = AdamW(opt_params, lr=lr, betas=(0.5, 0.999))\n\n    n_steps=epochs*train_loader_len\n    num_warmup_steps = int(warmup_prop * n_steps)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, n_steps)\n\n    total_steps = 0\n    epoch=0\n\n    avg_transformer_loss=0\n    loss_function=nn.MSELoss()\n    # loss_function=nn.L1Loss()\n\n    save_log([\"\\n\",str(datetime.datetime.now()).split(\".\")[0],\"\\n\", checkpoint_name()+f\"_fold_{fold+1}\"], logdir=get_checkpoint_dir()\/\"log.txt\")\n    with tqdm(total=epochs, desc=\"Epoch {}\/{}\".format(epoch + 1, epochs), unit=\"sections\", position=0,leave=True) as pbar:\n        for epoch in range(epochs):\n            model.train()\n            start_time = time.time()\n            optimizer.zero_grad()\n            avg_loss = 0\n\n            with tqdm(total=train_loader_len, desc=\"training iterations\", unit=\"batch\", position=1, leave=True) as pbar2:\n                for step, data in enumerate(train_loader):\n                  total_steps+=1\n                  input_ids=data['input_ids']\n                  labels=data['target']\n                  attention_mask=data['attention_mask']\n                  logits=model(input_ids=input_ids.to(Config.device), attention_mask=attention_mask.to(Config.device))['logits']\n                  #pdb.set_trace()\n                  loss=loss_function(logits.squeeze(1), labels.to(Config.device))\n  \n                  avg_loss += loss.item() \/ train_loader_len\n                  nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n                  loss.backward()\n                  optimizer.step()\n                  scheduler.step()\n                  model.zero_grad()\n                  pbar2.update()\n\n            model.eval()\n            avg_val_loss = 0.\n            avg_transformer_val_loss=0.\n            preds, truths = [], []\n            with torch.no_grad():\n                with tqdm(total=val_loader_len, desc=\"validation iterations\", unit=\"batch\", position=2, leave=True) as pbar3:\n                    for idx_val, data in enumerate(val_loader):\n                      input_ids=data['input_ids']\n                      labels=data['target']\n                      attention_mask=data['attention_mask']\n                      logits=model(input_ids=input_ids.to(Config.device), attention_mask=attention_mask.to(Config.device))['logits']\n                      loss=loss_function(logits.squeeze(1), labels.to(Config.device))\n                      avg_val_loss += loss.item() \/ val_loader_len\n                      pbar3.update()\n                      \n            dt = time.time() - start_time\n            lr = scheduler.get_last_lr()[0]\n            if epochs!=1:\n              if (epoch+1)%Config.save_every_epoch==0:\n                save_model_weights(model, f'{checkpoint_name()}_fold-{fold+1}_epoch-{epoch+1}_{CHECKPOINT_KEYWORD}.pt', cp_folder=get_checkpoint_dir())\n\n            log_lr=f\"Epoch {epoch + 1}\/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t \\n\"\n            print(log_lr)\n            log_score=f\"loss={avg_loss:.3f}\\t val_loss={avg_val_loss:.3f}  \\n\"\n            print(log_score)\n            save_log([log_lr, log_score], logdir=get_checkpoint_dir()\/\"log.txt\")\n            pbar.update()\n\n\n    del loss, data, avg_val_loss, avg_loss, train_loader, val_loader\n    if DEVICE != \"cpu\":\n        torch.cuda.empty_cache()\n    gc.collect()\n\n    return preds\n\n\ndef fit(model,train_dataset,val_dataset, fold, epochs,batch_size, weight_decay=0,warmup_prop=0.0,lr=5e-4):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=NUM_WORKERS)\n\n    val_loader_len=0\n    train_loader_len=0\n\n    #just the iterabledataset things :D\n    for batch in train_loader:\n        train_loader_len+=1\n    for batch in val_loader:\n        val_loader_len+=1\n\n    \n    opt_params = []\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    for n, p in model.named_parameters():\n        wd = 0 if any(nd in n for nd in no_decay) else weight_decay\n        opt_params.append(\n            {\"params\": [p], \"weight_decay\": wd, \"lr\": lr}\n        )\n\n    optimizer = AdamW(opt_params, lr=lr, betas=(0.5, 0.999))\n\n    n_steps=epochs*train_loader_len\n    num_warmup_steps = int(warmup_prop * n_steps)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, n_steps)\n\n    total_steps = 0\n    epoch=0\n\n    avg_transformer_loss=0\n    loss_function=nn.MSELoss()\n    # loss_function=nn.L1Loss()\n\n    save_log([\"\\n\",str(datetime.datetime.now()).split(\".\")[0],\"\\n\", checkpoint_name()+f\"_fold_{fold+1}\"], logdir=get_checkpoint_dir()\/\"log.txt\")\n    with tqdm(total=epochs, desc=\"Epoch {}\/{}\".format(epoch + 1, epochs), unit=\"sections\", position=0,leave=True) as pbar:\n        for epoch in range(epochs):\n            model.train()\n            start_time = time.time()\n            optimizer.zero_grad()\n            avg_loss = 0\n\n            with tqdm(total=train_loader_len, desc=\"training iterations\", unit=\"batch\", position=1, leave=True) as pbar2:\n                for step, data in enumerate(train_loader):\n                  total_steps+=1\n                  input_ids=data['input_ids']\n                  labels=data['target']\n                  attention_mask=data['attention_mask']\n                  logits=model(input_ids=input_ids.to(Config.device), attention_mask=attention_mask.to(Config.device))['logits']\n                  loss=loss_function(logits.squeeze(1), labels.to(Config.device))\n  \n                  avg_loss += loss.item() \/ len(train_loader)\n                  nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n                  loss.backward()\n                  optimizer.step()\n                  scheduler.step()\n                  model.zero_grad()\n                  pbar2.update()\n\n            model.eval()\n            avg_val_loss = 0.\n            avg_transformer_val_loss=0.\n            preds, truths = [], []\n            with torch.no_grad():\n                with tqdm(total=val_loader_len, desc=\"validation iterations\", unit=\"batch\", position=2, leave=True) as pbar3:\n                    for idx_val, data in enumerate(val_loader):\n                      input_ids=data['input_ids']\n                      labels=data['target']\n                      attention_mask=data['attention_mask']\n                      logits=model(input_ids=input_ids.to(Config.device), attention_mask=attention_mask.to(Config.device))['logits']\n                      loss=loss_function(logits.squeeze(1), labels.to(Config.device))\n                      avg_val_loss += loss.item() \/ len(val_loader)\n                      pbar3.update()\n                      \n            dt = time.time() - start_time\n            lr = scheduler.get_last_lr()[0]\n            if epochs!=1:\n              if (epoch+1)%Config.save_every_epoch==0:\n                save_model_weights(model, f'{checkpoint_name()}_fold-{fold+1}_epoch-{epoch+1}_{CHECKPOINT_KEYWORD}.pt', cp_folder=get_checkpoint_dir())\n\n            log_lr=f\"Epoch {epoch + 1}\/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t \\n\"\n            print(log_lr)\n            log_score=f\"loss={avg_loss:.3f}\\t val_loss={avg_val_loss:.3f}  \\n\"\n            print(log_score)\n            save_log([log_lr, log_score], logdir=get_checkpoint_dir()\/\"log.txt\")\n            pbar.update()\n\n\n    del loss, data, avg_val_loss, avg_loss, train_loader, val_loader\n    if DEVICE != \"cpu\":\n        torch.cuda.empty_cache()\n    gc.collect()\n\n    return preds\n","73d85aa4":"df=pd.read_csv(\"dataset\/train.csv\")\ndf=df[df['standard_error']!=0.0]\nlen(df)","79bbf80c":"import pdb\nCHECKPOINT_KEYWORD=\"frozenTransformer\"\nk_fold(df,save=True, config=Config)","f577bf95":"# Dataset","ed3a6b4b":"# Configs and Globals","ff3d681b":"# Model","6c0a626e":"Much of the code below is borrowed from The\u00f2.","ecd72d59":"# Fitting and K-fold","57be5e6f":"## Imports","7bb77ef7":"# Train\/Tune","f4d12d5b":"# Helper functions","f33c556c":"## COLAB One-timers"}}