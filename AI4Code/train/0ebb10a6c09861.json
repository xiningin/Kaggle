{"cell_type":{"cf95a64f":"code","27552c9d":"code","eed1d80b":"code","d29c165b":"code","eb513cf7":"code","3bf139e9":"code","b22faae1":"code","457a274f":"code","66d7d60f":"code","70cafbaa":"code","35e4de3c":"code","7ddf39c2":"code","8f6ff0f5":"code","53047c2e":"code","9717cd35":"code","9bdeedac":"code","c40a7974":"code","5dbea4ca":"code","5a129c14":"code","c063d897":"code","66679078":"markdown","e5b308e7":"markdown","ebdd7857":"markdown","b83e8087":"markdown","1bbc5c84":"markdown","3b29fd05":"markdown","2de9aa11":"markdown","e12ad478":"markdown","cfe51682":"markdown","1740aae3":"markdown","00dbf0b4":"markdown","abfda699":"markdown","30892680":"markdown"},"source":{"cf95a64f":"# pip install dataprep","27552c9d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from dataprep.eda import plot\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n\n# encoding\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.glmm import GLMMEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\ncolour = [\"#1A5276\",\"#A0A0A0\"]\nsns.set_palette(colour)","eed1d80b":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d29c165b":"data_null = pd.DataFrame(data.isnull().sum()\/data.shape[0],columns=['Null_percent'])\ndata_null.query('Null_percent > 0').sort_values(by='Null_percent',ascending=False).plot(kind='barh')\n\nsns.despine()\nplt.show()","eb513cf7":"v = ['LotFrontage','MasVnrArea']\nv_1 = ['Alley','PoolQC','MiscFeature','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n\ndata[v] = data[v].fillna(0)\ndata[v_1] = data[v_1].fillna('None')\ntest[v] = test[v].fillna(0)\ntest[v_1] = test[v_1].fillna('None')\ndata['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['GarageYrBlt'].mean())\ntest['GarageYrBlt'] = test['GarageYrBlt'].fillna(test['GarageYrBlt'].mean())\n\ndata.dropna(inplace=True)\n\ndata['GarageYrBlt']\ndata.loc[data['GarageYrBlt']==2207, 'GarageYrBlt'] = 2007","3bf139e9":"# plot(data, 'SalePrice')\n\ndata['LogSalePrice'] = np.log(data['SalePrice'])\nsns.distplot(data['LogSalePrice'])\n\ndata_copy = data.copy()\ntest_copy = test.copy()   # copy the data to prepare for the comparison at the coming steps","b22faae1":"data_type = pd.read_csv('..\/input\/house-price-prediction-2021-oct\/ames-variables.csv',index_col='variable')\n\ncontinuous = list(data_type.query('type == \"continuous\"').index)\nnominal = list(data_type.query('type == \"nominal\"').index)\nordinal = list(data_type.query('type == \"ordinal\"').index)\ndiscrete  = list(data_type.query('type == \"discrete\"').index)\nresponse = ['LogSalePrice']","457a274f":"# plot(data[continuous])\ntransf = continuous\n\nyj_transf = PowerTransformer(method='yeo-johnson').fit(data[transf])\ndata_copy[transf] = yj_transf.transform(data_copy[transf])\ntest_copy[transf] = yj_transf.transform(test_copy[transf])\n\nlabels = continuous\nscaler = StandardScaler().fit(data[labels])\ndata_copy.loc[:, labels] = scaler.transform(data.loc[:, labels])\ntest_copy.loc[:, labels] = scaler.transform(test.loc[:, labels])","66d7d60f":"ordinal_v = ['ExterQual','KitchenQual','BsmtCond','BsmtQual','ExterCond','BsmtExposure','HeatingQC','FireplaceQu','GarageQual','GarageCond','PoolQC','LandSlope']\n\nkeys = {\n    'None':0,\n    'No':0,\n    'Po':0,\n    'Mn':1,\n    'Fa':1,\n    'Gtl':1,\n    'Av':2,\n    'TA':2,\n    'Mod':2,\n    'Gd':3,\n    'Sev':3,\n    'Ex':4\n}\n\ndata_copy[ordinal_v] = data_copy[ordinal_v].replace(keys)\ntest_copy[ordinal_v] = test_copy[ordinal_v].replace(keys)\n\ndata_copy.drop(['BsmtFinType1','BsmtFinType2','Functional'],axis=1,inplace=True)\ntest_copy.drop(['BsmtFinType1','BsmtFinType2','Functional'],axis=1,inplace=True)","70cafbaa":"# nonimal\n\n# for i in range(len(nominal)):\n#     cate = data[nominal[i]].value_counts().shape[0]\n#     print('{} has {} categories.'.format(nominal[i],cate))\n\ndata_Target      = data_copy.copy()\ndata_LeaveOneOut = data_copy.copy()\ndata_GLMM        = data_copy.copy()\ndata_CatBoost    = data_copy.copy()\n\npredictor = nominal\n\nencoder_Target = TargetEncoder().fit(data_Target[predictor], data_Target[response])\ndata_Target[predictor] = encoder_Target.transform(data_Target[predictor])\n\nencoder_LeaveOneOut = LeaveOneOutEncoder().fit(data_LeaveOneOut[predictor], data_LeaveOneOut[response])\ndata_LeaveOneOut[predictor] = encoder_LeaveOneOut.transform(data_LeaveOneOut[predictor])\n\nencoder_GLMM = GLMMEncoder().fit(data_GLMM[predictor], data_GLMM[response])\ndata_GLMM[predictor] = encoder_GLMM.transform(data_GLMM[predictor])\n\nencoder_CatBoost = CatBoostEncoder().fit(data_CatBoost[predictor], data_CatBoost[response])\ndata_CatBoost[predictor] = encoder_CatBoost.transform(data_CatBoost[predictor])\n\n\npredictor = nominal.copy()\npredictor.remove('MSSubClass')\npredictor.remove('YrSold')\n\ndata_dummy = data_copy.copy()\ndummies = pd.get_dummies(data_dummy[predictor], drop_first=True)\ndata_dummy = data_dummy.join(dummies)","35e4de3c":"# Accessing\nTarget_train, Target_test = train_test_split(data_Target, train_size=0.7, random_state=11)\nLeaveOneOut_train, LeaveOneOut_test = train_test_split(data_LeaveOneOut, train_size=0.7, random_state=11)\nGLMM_train, GLMM_test = train_test_split(data_GLMM, train_size=0.7, random_state=11)\nCatBoost_train, CatBoost_test = train_test_split(data_CatBoost, train_size=0.7, random_state=11)\nDummy_train, Dummy_test = train_test_split(data_dummy, train_size=0.7, random_state=11)\n\nvar = ordinal_v + ['OverallQual','OverallCond'] + continuous + discrete + nominal\nvar_dummy = ordinal_v + ['OverallQual','OverallCond'] + continuous + discrete + list(dummies.columns)\n    \n# model = RandomForestRegressor()\n\n# param_random_f = {'n_estimators': np.arange(1,100),'min_samples_leaf': np.arange(1,10), 'min_samples_split': np.arange(2,10)}\n# rf_search =  RandomizedSearchCV(model, param_random_f, cv = 5, n_iter=10, scoring = 'neg_mean_squared_error', \n#                                  random_state=42)\n\n# rf_search.fit(Target_train[var],Target_train['LogSalePrice'])\n\nrows = ['Target', 'LeaveOneOut', 'GLMM', 'CatBoost', 'Dummy']\ncolumns=['RMSE']\nresults = pd.DataFrame(0.0, columns=columns, index=rows)\n\nregressor_0 = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nregressor_0.fit(Target_train[var], Target_train[response])\ny_pred = regressor_0.predict(Target_test[var])\nresults.iloc[0, 0] = np.sqrt(mean_squared_error(Target_test[response], y_pred))\n\nregressor_1 = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nregressor_1.fit(LeaveOneOut_train[var], LeaveOneOut_train[response])\ny_pred = regressor_1.predict(LeaveOneOut_test[var])\nresults.iloc[1, 0] = np.sqrt(mean_squared_error(LeaveOneOut_test[response], y_pred))\n\nregressor_2 = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nregressor_2.fit(GLMM_train[var], GLMM_train[response])\ny_pred = regressor_2.predict(GLMM_test[var])\nresults.iloc[2, 0] = np.sqrt(mean_squared_error(GLMM_test[response], y_pred))\n\nregressor_3 = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nregressor_3.fit(CatBoost_train[var], CatBoost_train[response])\ny_pred = regressor_3.predict(CatBoost_test[var])\nresults.iloc[3, 0] = np.sqrt(mean_squared_error(CatBoost_test[response], y_pred))\n\nregressor_4 = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nregressor_4.fit(Dummy_train[var_dummy], Dummy_train[response])\ny_pred = regressor_4.predict(Dummy_test[var_dummy])\nresults.iloc[4, 0] = np.sqrt(mean_squared_error(Dummy_test[response], y_pred))\n\nresults","7ddf39c2":"encoder = LeaveOneOutEncoder().fit(data_copy[nominal], data_copy[response])\ndata_copy[nominal] = encoder.transform(data_copy[nominal])\ntest_copy[nominal] = encoder.transform(test_copy[nominal])","8f6ff0f5":"# model = RandomForestRegressor()\n\n# param_random_f = {'n_estimators': np.arange(1,100),'min_samples_leaf': np.arange(1,10), 'min_samples_split': np.arange(2,10)}\n# rf_search =  RandomizedSearchCV(model, param_random_f, cv = 5, n_iter=10, scoring = 'neg_mean_squared_error', \n#                                  random_state=42)\n\n# rf_search.fit(Target_train[var],Target_train['LogSalePrice'])\n# rf_search.best_params_","53047c2e":"# XGB_model = xgb.XGBRegressor(use_label_encoder = False, \n#                              eval_metric = 'rmse', \n#                              n_estimators = 10000)\n\n# XGB_param_Random = {'reg_alpha': [0.01, 0.05, 0.1, 0.2, 0.5],\n#                     'reg_lambda': [0.01, 0.05, 0.1, 0.2, 0.5],\n#                     'learning_rate': [x \/ 400 for x in range(1, 10, 1)],\n#                     'max_depth': list(range(2, 15, 1)),\n#                     'min_child_weight': list(range(2, 20, 1)),\n#                     'gamma': [x \/ 200 for x in range(0, 20, 1)],\n#                     'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n#                     'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9]}\n\n# XGB_random_grid = RandomizedSearchCV(XGB_model, XGB_param_Random, cv = 5, n_iter=10, scoring = 'neg_mean_squared_error', \n#                                  random_state=42)\n\n# XGB_fit = XGB_random_grid.fit(LeaveOneOut_train[var], LeaveOneOut_train[response], \n#                               early_stopping_rounds = 200, \n#                               eval_set = [[LeaveOneOut_test[var], LeaveOneOut_test[response]]], \n#                               eval_metric = 'rmse', verbose = False)\n\n# XGB_fit.best_params_","9717cd35":"reg = RandomForestRegressor(n_estimators=69, min_samples_leaf=2, min_samples_split=2, random_state = 0)\nreg.fit(LeaveOneOut_train[var], LeaveOneOut_train[response])","9bdeedac":"XGB_model = xgb.XGBRegressor(use_label_encoder = False, \n                             eval_metric = 'rmse',                   \n                             n_estimators = 5000,\n                             reg_alpha = 0.5,\n                             reg_lambda = 0.01,\n                             learning_rate = 0.005,\n                             max_depth = 5,\n                             min_child_weight = 3,\n                             gamma = 0.005,\n                             subsample = 0.8,\n                             colsample_bytree = 0.8)\n\nXGB_model.fit(LeaveOneOut_train[var], LeaveOneOut_train[response])","c40a7974":"stack = StackingCVRegressor(regressors=[reg, XGB_model], meta_regressor=LinearRegression(), \n                           cv=5, random_state=1, store_train_meta_features=True)\nstack.fit(LeaveOneOut_train[var], LeaveOneOut_train[response])\n\nnp.sqrt(mean_squared_error(LeaveOneOut_test[response], stack.predict(LeaveOneOut_test[var])))","5dbea4ca":"test_copy['GarageArea'] = test_copy['GarageArea'].fillna(test_copy['GarageArea'].mean())\ntest_copy['TotalBsmtSF'] = test_copy['TotalBsmtSF'].fillna(test_copy['TotalBsmtSF'].mean())\ntest_copy['KitchenQual'] = test_copy['KitchenQual'].fillna(test_copy['KitchenQual'].mean())\ntest_copy['GarageCars'] = test_copy['GarageCars'].fillna(test_copy['GarageCars'].mean())\n\nnull = pd.DataFrame(test_copy.isnull().sum()\/test_copy.shape[0],columns=['Null_percent'])\npred = list(null.query('Null_percent > 0').index)\ntest_copy[pred] = test_copy[pred].fillna(0)","5a129c14":"stack.fit(LeaveOneOut_train[var], LeaveOneOut_train[response])\n\ntest_copy['SalePrice'] = np.exp(stack.predict(test_copy[var]))\ntest_copy[['Id','SalePrice']].sort_values(by='Id').set_index(['Id'])  #.to_csv('..\/output\/test_pred.csv')","c063d897":"test_copy[['Id','SalePrice']].sort_values(by='Id').set_index(['Id']).to_csv('test_pred.csv')","66679078":"#### (2) Stacking Model","e5b308e7":"## 2. Data understanding and processing","ebdd7857":"#### (2) Inspecting the response variable and do some transformation if neccessary.","b83e8087":"#### (4) Processing the ***ordinal*** data","1bbc5c84":"### Thanks for reading!","3b29fd05":"## 3. Model choosing","2de9aa11":"#### (5) Processing the ***nominal*** data. As the number of categories for every nominal variables are quiet large, some other number encoding may perform better than dummy encoding.","e12ad478":"#### (3) Proccessing the test data and predicting using the model with lowest testing error.","cfe51682":"#### (3) Manully assign varibles into different types. Inspecting the ***continuous*** data firstly, then doing some Yeo-Johnson transformation and standard transformation.","1740aae3":"## 1. Package importing","00dbf0b4":"#### (1) Filling missing values based on the real meaning of variables and inspecting some outliers.","abfda699":"#### (1) Applying random search to find the optimal hyperparameter of different regression methods.","30892680":"#### Hi there, this notebook mainly contains EDA, data processing, hyperparameter and model choosing. The final model is XGboost.\n#### It my first attempt at Kaggle. Please feel free to leave your precious comments :)"}}