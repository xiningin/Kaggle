{"cell_type":{"a85872bf":"code","f042fdf0":"code","5157f87c":"code","b9d001b7":"code","63ff3a82":"code","115958e6":"code","6b8c7fc1":"code","61021f34":"code","24816162":"code","acad064c":"markdown","af769de4":"markdown","85ea315e":"markdown","77340c80":"markdown","8146a4f9":"markdown","b9dd2269":"markdown","a3b296bc":"markdown","75149970":"markdown","498b81e7":"markdown"},"source":{"a85872bf":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import Dataset, Subset, DataLoader, RandomSampler, SequentialSampler\n\nfrom transformers import LongformerTokenizerFast\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt","f042fdf0":"# load the train text data\n\nconfig = {\n    'model_name': 'allenai\/longformer-base-4096',\n    'batch_size': 4,\n}\n\nTEXT_FILES = os.listdir('..\/input\/feedback-prize-2021\/train')\nTEXT_FILES = [f'..\/input\/feedback-prize-2021\/train\/{file}' for file in TEXT_FILES]\n\ntext_data = dict()\nfor file_path in tqdm(TEXT_FILES):\n    with open(file_path, 'r') as file:\n        idx = os.path.basename(file_path).split('.txt')[0]\n        text_data[idx] = file.read()\n        \n# 1. delete spaces from texts ends\nfor key, value in text_data.items():\n    text_data[key] = value.rstrip()\n","5157f87c":"tokenizer = LongformerTokenizerFast.from_pretrained(config['model_name'])\n\ndata_tokenized = []\n\nfor idx, text in tqdm(text_data.items()):\n    \n    # get inputs\n    inputs = tokenizer(text, add_special_tokens=True)\n        \n    data_tokenized.append([inputs['input_ids'], inputs['attention_mask']])\n    \ntokenized_df = pd.DataFrame(data_tokenized, columns=['input_ids', 'attention_mask'])\ntokenized_df.head()","b9d001b7":"seq_len = tokenized_df['attention_mask'].apply(len)\n\nplt.rcParams['figure.figsize'] = (17, 8)\nbins = np.linspace(0, 2000, 100)\n\nplt.hist(seq_len, bins=bins, alpha=0.75, label='sequence length')\nplt.vlines(seq_len.mean(), ymin=0, ymax=700, colors='red', label='mean sequence length')\nplt.legend(loc='upper right')\nplt.show()","63ff3a82":"class LongformerDataset(Dataset):\n    \"\"\"Dataset for the longformer model.\"\"\"\n    \n    def __init__(self, data: pd.DataFrame):\n        self.data = data        \n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        return {\n            'input_ids': self.data.loc[idx, 'input_ids'],\n            'attention_mask': self.data.loc[idx, 'attention_mask']\n        }\n      ","115958e6":"from typing import List\n\nclass Collate:\n    \n    def __call__(self, batch: List[dict]) -> dict:\n        \n        output = dict()\n        \n        # since our custom Dataset's __getitem__ method returns dictionary\n        # the collate_fn function will receive list of dictionaries\n        output['input_ids'] = [sample['input_ids'] for sample in batch]\n        output['attention_mask'] = [sample['attention_mask'] for sample in batch]\n        \n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output['input_ids']])\n        \n        # add padding\n        output['input_ids'] = [sample + (batch_max-len(sample)) * [tokenizer.pad_token_id] for sample in output['input_ids']]\n        output['attention_mask'] = [sample + (batch_max-len(sample)) * [0] for sample in output['attention_mask']]\n        \n        # convert to tensors\n        output['input_ids'] = torch.tensor(output['input_ids'], dtype=torch.long)\n        output['attention_mask'] = torch.tensor(output['attention_mask'], dtype=torch.long)\n    \n        return output\n","6b8c7fc1":"collate = Collate()\ndataset = LongformerDataset(tokenized_df)\n\ntrain_data, val_data = train_test_split(dataset, test_size=0.2, shuffle=True)\n\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=config['batch_size'],\n                              collate_fn=collate,\n                              shuffle=True)\n\nval_dataloader = DataLoader(val_data,\n                            batch_size=config['batch_size'],\n                            collate_fn=collate\n                           )","61021f34":"train_batch_sizes = pd.Series([batch['input_ids'].size(1) for batch in train_dataloader])\n\nprint(f'Mean: {round(train_batch_sizes.mean(), 2)}')\nprint(f'Mean absolute deviation: {round(train_batch_sizes.mad(), 2)}')","24816162":"config['batch_size'] = 8\n\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=config['batch_size'],\n                              collate_fn=collate,\n                              shuffle=True)\n\ntrain_batch_sizes = pd.Series([batch['input_ids'].size(1) for batch in train_dataloader])\n\nprint(f'Mean: {round(train_batch_sizes.mean(), 2)}')\nprint(f'Mean absolute deviation: {round(train_batch_sizes.mad(), 2)}')","acad064c":"### 4. Create Dataloaders","af769de4":"### Lets see the average batch max len","85ea315e":"Now instead of using a PyTorch Sampler, we need to make a class that processes a batch ourselfs, and call it as collate_fn parameter when making a DataLoader.\n\nSee this discusion: [Link](https:\/\/discuss.pytorch.org\/t\/how-to-use-collate-fn\/27181)","77340c80":"### 2. Create custom PyTorch Dataset","8146a4f9":"With higher batch sizes, there should be more padding tokens on average, therefore less runtime improvements!","b9dd2269":"### 1 . Dataloading and preprocessing","a3b296bc":"Here, I am going to use the Longformer tokenizer.","75149970":"#### Sequence length histogram","498b81e7":"### Sequence bucketing - PyTorch implementation\n\nI've unsuccessfully tried a couple of times to implement Sequence Bucketing in PyTorch. Recently I found this notebook that solves this problem: [Notebook](https:\/\/www.kaggle.com\/shahules\/guide-pytorch-data-samplers-sequence-bucketing\/notebook)\n\nThanks [Shahules](https:\/\/www.kaggle.com\/shahules)\n\nI just modified it for this competition and made it a little bit more pythonic.\n\nIn my case it speeds up training from 18:40 min to 8:54 min for one epoch and batch size 4. If you are allready truncating your input, you may see less improvements than me.\n\nIf you have any questions or ideas for improvements, please let me know!"}}