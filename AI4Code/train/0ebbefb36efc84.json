{"cell_type":{"b5922269":"code","4dd98de9":"code","e26f0237":"code","e0e2bf82":"code","2b250933":"code","9f4281cd":"code","2696d940":"code","1dacdaf9":"code","6d81528c":"code","df2f8269":"code","fcbf4da8":"code","44334166":"code","39c6abf7":"code","dbebfdd0":"code","df84832a":"code","37327cbb":"code","ce4e6073":"code","e094c944":"code","ea4258a2":"code","4a995a8c":"code","9f963d3a":"code","d8c47c84":"code","fe63c14b":"code","1140df02":"code","606c2897":"code","5549c75e":"code","ef0e9fec":"code","ce594ffb":"code","6e3aac92":"code","3fbb4e07":"code","886a34d1":"code","48618c99":"code","c7a51a74":"code","71851a28":"code","27ad3e90":"code","a5e122d0":"code","49d2dc04":"code","4f085609":"code","a872cba4":"markdown","29a56d46":"markdown","52288fb0":"markdown","cf673169":"markdown","785bbd8c":"markdown","94c85995":"markdown","bf219f74":"markdown","57dffb58":"markdown","6b9549f0":"markdown","41319d3f":"markdown","90e38de3":"markdown","4d848feb":"markdown"},"source":{"b5922269":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nsns.set()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4dd98de9":"stops = pd.read_csv('..\/input\/stop-data-april-2013-to-sept.-2015.csv')\nstops.sample(10)","e26f0237":"sns.heatmap(stops.isnull(), cbar=False, cmap=sns.color_palette(\"GnBu_d\"))","e0e2bf82":"def strip_comma(text):\n    if pd.isna(text):\n        return np.nan\n    else:\n        return text[:-1] if text[-1] is ',' else text","2b250933":"stops['ReasonForEncounter'] = stops['ReasonForEncounter'].apply(strip_comma)\nstops['ResultOfEncounter'] = stops['ResultOfEncounter'].apply(strip_comma)\nstops['ResultOfSearch'] = stops['ResultOfSearch'].apply(strip_comma)\nstops['SDRace'] = stops['SDRace'].apply(strip_comma)\nstops['SearchConducted'] = stops['SearchConducted'].apply(strip_comma)\nstops['TypeOfSearch'] = stops['TypeOfSearch'].apply(strip_comma)\nstops['Sex'] = stops['Sex'].apply(strip_comma)","9f4281cd":"print('Before:')\nprint(stops['ReasonForEncounter'].value_counts())\nstops = stops[~(stops['ReasonForEncounter'] == 'Other-Consensual')]\nprint()\nprint('After:')\nprint(stops['ReasonForEncounter'].value_counts())","2696d940":"print('Before:')\nprint(stops['ResultOfEncounter'].value_counts())\nstops['ResultOfEncounter'].replace('Report Taken-No Action,Report Taken-No Action', 'Report Taken-No Action', inplace=True)\nstops['ResultOfEncounter'].replace('FI Report,FI Report', 'FI Report', inplace=True)\nprint()\nprint('After:')\nprint(stops['ResultOfEncounter'].value_counts())","1dacdaf9":"print('Before:')\nprint(stops['ResultOfSearch'].value_counts())","6d81528c":"stops['ResultOfSearch'] = stops['ResultOfSearch'].str.replace('&',',')\nstops['ResultOfSearch'] = stops['ResultOfSearch'].str.replace(' , ',',')","df2f8269":"stops = pd.concat([stops, stops['ResultOfSearch'].str.get_dummies(sep=',')], axis=1, sort=False); ","fcbf4da8":"print('Before:')\nprint(stops['TypeOfSearch'].value_counts())\nprint()\nprint('After:')\nstops = stops[~(stops['TypeOfSearch'] == 'ContactDate')]\nstops['TypeOfSearch'].value_counts()","44334166":"race = stops['SDRace']\nencounters = pd.get_dummies(stops, columns=['ReasonForEncounter','ResultOfEncounter', 'SDRace', 'SearchConducted', 'Sex', 'TypeOfSearch'])\nencounters.drop(columns=['ContactDate','ContactTime', 'Location 1','ResultOfSearch'], inplace=True)\nencounters.head()","39c6abf7":"def plot_corr_seaborn(df_corr,threshold):\n    mask = np.zeros_like(df_corr)\n    mask[np.triu_indices_from(mask)] = True\n    with sns.axes_style(\"white\"):\n        fig, ax = plt.subplots()\n        fig.set_size_inches(15, 15)\n        fig.dpi = 100\n        ax = sns.heatmap(df_corr, mask=mask, vmax=1, square=True)","dbebfdd0":"pd.Series.to_frame(encounters.corr().replace(1,0).max()).T","df84832a":"plot_corr_seaborn(encounters.corr(), 1)","37327cbb":"felony_arrests = {'Male': len(encounters[(encounters['Sex_M'] == 1) & (encounters['ResultOfEncounter_Felony Arrest'] == 1)])\/len(encounters[encounters['Sex_M'] == 1]), \n                  'Female': len(encounters[(encounters['Sex_F'] == 1) & (encounters['ResultOfEncounter_Felony Arrest'] == 1)])\/len(encounters[encounters['Sex_F'] == 1])}\nplt.bar(felony_arrests.keys(), felony_arrests.values())\n\nplt.ylabel('Percentage of all interactions')\nplt.xlabel('Genders')\nplt.title('Felony Arrest frequency')","ce4e6073":"felony_arrests = {'Male': len(encounters[(encounters['Sex_M'] == 1) & (encounters['ResultOfEncounter_Misdemeanor Arrest'] == 1)])\/len(encounters[encounters['Sex_M'] == 1]), \n                  'Female': len(encounters[(encounters['Sex_F'] == 1) & (encounters['ResultOfEncounter_Misdemeanor Arrest'] == 1)])\/len(encounters[encounters['Sex_F'] == 1])}\nplt.bar(felony_arrests.keys(), felony_arrests.values())\nplt.title('Felony Misdemeanor frequency')\nplt.ylabel('Percentage of all interactions')\nplt.xlabel('Genders')","e094c944":"# clustering for k = 2 to 32 by doubling values.\nfrom sklearn.cluster import KMeans\nmax_k = 7\nks = [2**k for k in range(1,max_k+1)]\nscores = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k, n_jobs=-1, init='k-means++')\n    model.fit_predict(encounters.values)\n    scores.append(-model.score(encounters.values))","ea4258a2":"fig = plt.figure(figsize = (8,8))\nplt.plot(ks, scores)\nplt.title('cost-clusters tradeoff')\nplt.ylabel('total intra-cluster distance')\nplt.xlabel('k')\nplt.show()","4a995a8c":"kmeans = KMeans(n_clusters=8, n_init=50, n_jobs=-1, init='k-means++', random_state=0)\nkmeans.fit_predict(encounters.values);","9f963d3a":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(encounters)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n\ny_kmeans = kmeans.predict(encounters)","d8c47c84":"fig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 10)\nax.set_ylabel('Principal Component 2', fontsize = 10)\nax.set_title('2 component PCA', fontsize = 20)\nax.scatter(principalDf['principal component 1'], principalDf['principal component 2'], c= y_kmeans)\nplt.show()","fe63c14b":"encounters['Cluster'] = y_kmeans","1140df02":"encounters[encounters['Cluster'] == 0].describe().loc[['mean']]","606c2897":"encounters[encounters['Cluster'] == 1].describe().loc[['mean']]","5549c75e":"encounters[encounters['Cluster'] == 2].describe().loc[['mean']]","ef0e9fec":"encounters[encounters['Cluster'] == 3].describe().loc[['mean']]","ce594ffb":"encounters[encounters['Cluster'] == 4].describe().loc[['mean']]","6e3aac92":"encounters[encounters['Cluster'] == 5].describe().loc[['mean']]","3fbb4e07":"encounters[encounters['Cluster'] == 6].describe().loc[['mean']]","886a34d1":"encounters[encounters['Cluster'] == 7].describe().loc[['mean']]","48618c99":"assert len(encounters) == len(race)","c7a51a74":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.model_selection import train_test_split","71851a28":"x_cols = [ 'ReasonForEncounter_Consensual Encounter', 'ReasonForEncounter_Probable Cause', 'ReasonForEncounter_Probation\/Parole', 'ReasonForEncounter_Reasonable Suspicion',\n          'ReasonForEncounter_Traffic Violation', 'SearchConducted_No', 'SearchConducted_Yes', 'Sex_F', 'Sex_M',\n          'ResultOfEncounter_Citation','ResultOfEncounter_FI Report', 'ResultOfEncounter_Felony Arrest', 'ResultOfEncounter_Misdemeanor Arrest', \n          'ResultOfEncounter_Report Taken-No Action', 'ResultOfEncounter_Warning']\noutput = pd.Categorical(race).codes\nprint(output)\nprint(dict( enumerate(pd.Categorical(race).categories) ))","27ad3e90":"X_train, X_test, y_train, y_test = train_test_split(encounters[x_cols].values, output, test_size=0.3)\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","a5e122d0":"model = Sequential([\n    Dense(15),\n    Activation('relu'),\n    Dense(11),\n    Activation('relu'),\n    Dense(7),\n    Activation('softmax')\n])","49d2dc04":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=15)\nmodel.evaluate(X_test, y_test)","4f085609":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, max_depth=7)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","a872cba4":"Cluster size of 8 seems to be optimal, given that we are happier off maintining a higher score with less amount of clusters. k=16 may as well be good choice too. it is usually not a good sign if there's no clear sign of an elbow point, but for this data k=8 or k=16 are arguably the best sizes to pick here.","29a56d46":"Let's see the highest correlation values for each attribute.","52288fb0":"This hints that it's not possible to classify stop consequences based on logistics","cf673169":"This will require a more complicated means of One Hot Encoding because each value in the current dataframe may need multiple values to be hot.","785bbd8c":"<a id=\"Encoding\"><\/a> \n# **1. One-Hot-Encoding:**\n\nLet's start by removing all the trailing commas in the dataframe","94c85995":"<h1> Welcome to my stop encounters kernel! <\/h1>\n<h2>This kernel will go over some data mining concepts to understand this data better<\/h2>\n\nTable of Contents:<br>\n**1. [One-Hot-Encoding](#Encoding)** <br>\n**2. [Visualization](#Visualization)** <br>\n**3. [Clustering](#Clustering)** <br>\n**4. [Exploring classification](#Classification)**<br>\n\nLet's start by looking at the data. ","bf219f74":"We can see that there's two values `FI Report,FI Report ` and `Report Taken-No Action,Report Taken-No Action ` which seem to be typos","57dffb58":"It seems that we only have missing values when a Search wasn't conducted. Then there would be no TypeOfSearch or ResultOfSearch. From the given data, we can see that there's examples where the columns we are interested in have multiple comma seperated features, such as `ResultOfSearch: 'Other Evidence,Narcotics,Firearms` ","6b9549f0":"<h3>We've successfully one-hot-encoded this column!<h3>\n<a id=\"Visualization\"><\/a> \n# **2. Visualization:**\nLet's generate a correlation matrix","41319d3f":"It seems that women are much more likely to get charged with Felony Misdemeanor than Felony Arrest, \nwhereas men are more likely to be charged with Felony Arrest than Felony Misdemeanor\n\n<a id=\"Clustering\"><\/a> \n# **3. Clustering:**\nLet's perform k-means clustering with doubling k-values to see which value of k will be the optimal cluster numbers. This will be when adding more clusters will not raise the model score by much.","90e38de3":"`ReasonForEncounter` looks ready to be one-hot-encoded, but let's just drop the one row which has `Other-Consensual` as it's reason","4d848feb":"<h3>For now we will be using the following fields fields<\/h3>\n* ReasonForEncounter\n* ResultOfEncounter\n* ResultOfSearch\n* SDRace\n* SearchConducted\n* Sex\n* TypeOfSearch<br>\n\nLet us look at the null values of the data"}}