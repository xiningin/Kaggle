{"cell_type":{"435a03fe":"code","33d6bfe7":"code","d58a40fd":"code","00baede9":"code","4d9745ca":"code","b4063cac":"code","50800a41":"code","f7432c60":"code","d9a72e75":"code","c5652dd5":"code","440d9912":"code","a66ab199":"code","edfce34f":"code","2402be94":"code","20bd1710":"code","8e4057f9":"code","849201ca":"code","73c85d9a":"code","8ff38da6":"code","a61a5047":"code","00a21f0f":"code","ca837278":"code","50b0db8b":"code","0b8ea742":"code","0bf487cf":"code","d1c6ba5f":"code","e3d89c9e":"code","ca3c2307":"code","ad9f60c3":"code","6229cf61":"code","1e1b6225":"code","d27b3fb5":"code","1135d700":"code","fd394967":"code","59d715ea":"code","9c342d07":"code","fc12dd10":"code","34b6c5c9":"code","1155253f":"code","fef94d6a":"code","6282fa7b":"code","98068c0e":"code","303df2cb":"code","2bcd17d6":"code","d134c5d5":"code","dc4e977f":"code","a087290d":"code","7f63db3d":"code","4a9d9966":"code","aef43a46":"code","cdd60aa2":"code","943d7beb":"code","51b251ea":"code","e92f72db":"code","59f7a47a":"code","11334c93":"code","6df1b7b9":"code","f00c6a1f":"code","8d0d63ed":"code","5645d284":"code","1f68c9fd":"code","eafd0c84":"code","c78d03f3":"code","d800e42e":"code","61a90ed8":"code","68af41c0":"code","38eb5e3e":"code","9da8fc70":"code","6bd9c585":"markdown","3d798d89":"markdown","d86ffbf8":"markdown","4b861455":"markdown","28a532dc":"markdown","cb7ad6e9":"markdown","19027542":"markdown","2d8dbd2e":"markdown","302df93e":"markdown","529ffc0f":"markdown","ff9e7433":"markdown","fcea31da":"markdown","16bf34df":"markdown","d6390ba4":"markdown","e230c8c5":"markdown","8c2f5f64":"markdown","b458849b":"markdown","a99824a9":"markdown","f55bbc74":"markdown","1ed8ad0c":"markdown","f7b408c2":"markdown","c6dcca76":"markdown","d15d8bab":"markdown","f4bd6cd6":"markdown","f19f2551":"markdown","b1b9864b":"markdown","c43c4e45":"markdown"},"source":{"435a03fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nsns.set(rc=({'figure.figsize':(11,15)}))\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33d6bfe7":"#-- importing files\ndb_features = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ndb_sampleSubmission = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\ndb_stores = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ndb_test = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\ndb_train = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')","d58a40fd":"#-- printing head\ndb_features.head()","00baede9":"#-- checking db dimension\nprint(f'Rows: {db_features.shape[0]}')\nprint(f'\\nColumns: {db_features.shape[1]}')","4d9745ca":"#-- checking features type\ndb_features.info()","b4063cac":"#-- converting date field\ndb_features['Date'] = db_features['Date'].apply(pd.to_datetime)","50800a41":"#-- checking convertion\ndb_features.info()","f7432c60":"#-- checking missing values\ndb_features.isnull().sum().sort_values(ascending=False).to_frame() \/ len(db_features)","d9a72e75":"#-- checking data\ndb_features.describe()","c5652dd5":"#-- printing date values\nprint(f\"Min Date value: {min(db_features['Date'])}\")\nprint(f\"\\nMax Date value: {max(db_features['Date'])}\")","440d9912":"#-- checking % of registers before Nov 2011\nprint(\"Values which will be removed, considering markdown availability: \" +\n      f\"{len(db_features[db_features['Date'] < '2011-12-01']) \/ len(db_features):.2f}\")","a66ab199":"#-- printing head\ndb_stores.head()","edfce34f":"#-- checking db dimension\nprint(f'Rows: {db_stores.shape[0]}')\nprint(f'\\nColumns: {db_stores.shape[1]}')","2402be94":"#-- checking features type\ndb_stores.info()","20bd1710":"#-- checking missing values\ndb_stores.isnull().sum().sort_values(ascending=False).to_frame() \/ len(db_stores)","8e4057f9":"#-- checking data\ndb_stores.describe()","849201ca":"#-- counting stores types\ndb_stores['Type'].value_counts()","73c85d9a":"#-- printing head\ndb_train.head()","8ff38da6":"#-- checking db dimension\nprint(f'Rows: {db_train.shape[0]}')\nprint(f'\\nColumns: {db_train.shape[1]}')","a61a5047":"#-- checking features type\ndb_train.info()","00a21f0f":"#-- converting date field\ndb_train['Date'] = db_train['Date'].apply(pd.to_datetime)","ca837278":"#-- checking convertion\ndb_train.info()","50b0db8b":"#-- checking missing values\ndb_train.isnull().sum().sort_values(ascending=False).to_frame() \/ len(db_train)","0b8ea742":"#-- checking data\ndb_train.describe()","0bf487cf":"#-- grouping stores\ndb_train_g = db_train.groupby(['Store', 'Dept'])['Dept'].count().to_frame().rename(columns={'Dept':'count'})\ndb_train_g.reset_index(inplace=True)","d1c6ba5f":"#-- checking number of departments by Store\ndb_train_g['Store'].value_counts().to_frame().sort_index()","e3d89c9e":"#-- merging db_train + db_store\ndb_train_store = pd.merge(left=db_train, right=db_stores, on='Store', how='left')\ndb_train_store.head()","ca3c2307":"#-- grouping weekly sales by store type\ndb_train_store_g = db_train_store.groupby(['Date', 'Type'])['Weekly_Sales'].sum().reset_index()\ndb_train_store_g.index = db_train_store_g['Date']","ad9f60c3":"#-- ploting seasonal for stores A\nresult_a = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_a.observed.plot(ax=ax1)\nresult_a.trend.plot(ax=ax2)\nresult_a.seasonal.plot(ax=ax3)\nresult_a.resid.plot(ax=ax4)\nplt.tight_layout()","6229cf61":"#-- ploting seasonal for stores B\nresult_b = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_b.observed.plot(ax=ax1)\nresult_b.trend.plot(ax=ax2)\nresult_b.seasonal.plot(ax=ax3)\nresult_b.resid.plot(ax=ax4)\nplt.tight_layout()","1e1b6225":"#-- ploting seasonal for stores C\nresult_c = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_c.observed.plot(ax=ax1)\nresult_c.trend.plot(ax=ax2)\nresult_c.seasonal.plot(ax=ax3)\nresult_c.resid.plot(ax=ax4)\nplt.tight_layout()","d27b3fb5":"#-- function to define stationarity\ndef estacionario(x):\n    \"\"\"Fun\u00e7\u00e3o para avaliar se os dados s\u00e3o estacion\u00e1rios\"\"\"\n    \n    if adfuller(x)[1] <= 0.05:\n        print('Conjunto de dados \u00e9 estacion\u00e1rio')\n        print(f'N\u00famero de lags utilizado: {adfuller(x)[2]}')\n    else:\n        print('Conjuntos de dados n\u00e3o \u00e9 estacion\u00e1rio')","1135d700":"#-- function to define normality\ndef normal(x):\n    \"\"\"Fun\u00e7\u00e3o para avaliar normalidade\"\"\"\n    p_normal = stats.shapiro(x)[1]\n    \n    if p_normal >= 0.05:\n        print(f'Dados seguem uma distribui\u00e7\u00e3o normal - p_value = {p_normal:.2}')\n    else:\n        print(f'Dados n\u00e3o seguem uma distribui\u00e7\u00e3o normal - p_value = {p_normal:.2}')","fd394967":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'])","59d715ea":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'])","9c342d07":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'])","fc12dd10":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'])","34b6c5c9":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'])","1155253f":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'])","fef94d6a":"#-- merging train_stores + features\ndb_train_store_features = pd.merge(left=db_train_store, right=db_features, on=['Store', 'Date'], how='left', suffixes=('_train', '_features'))\ndb_train_store_features.head()","6282fa7b":"db_train_store_features.info()","98068c0e":"#-- creating db bi store type\ntype_a = db_train_store_features.loc[db_train_store_features['Type'] == 'A']\ntype_b = db_train_store_features.loc[db_train_store_features['Type'] == 'B']\ntype_c = db_train_store_features.loc[db_train_store_features['Type'] == 'C']","303df2cb":"#-- EDA\n_ = sns.pairplot(db_train_store_features_n)","2bcd17d6":"#-- checking correlation between features\n_ = plt.clf()\n_ = plt.style.use('fivethirtyeight')\n_ = font_opts = {'fontsize':15, 'fontweight':'bold'}\n_ = plt.figure(figsize=(20,10))\n\nx = sns.heatmap(\n    db_train_store_features.corr(), \n    annot=db_train_store_features.corr(), \n    fmt='.2f', \n    annot_kws={'fontsize':10, 'fontweight':'bold'},\n    cmap='RdPu'\n)\n\n_ = plt.title(\"Correlation Matrix\\n\", **font_opts)\n_ = plt.xticks(**font_opts)\n_ = plt.yticks(**font_opts)\n\n\n_ = plt.tight_layout();\n_ = plt.plot();","d134c5d5":"#-- converting strings to number\ndb_train_store_features.Type = db_train_store_features.Type.map({'A':1, 'B':2, 'C':3})","dc4e977f":"#-- selecting features\nX_train = db_train_store_features[['Store','Dept','IsHoliday_train','Size', 'Type']]\nY_train = db_train_store_features['Weekly_Sales']","a087290d":"#-- creating function to ml\ndef random_forest(n_estimators, max_depth):\n    resultado = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes = []\n            for i in range(1,5):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)\n                ml_rf = RandomForestRegressor(n_estimators=estimator, max_depth=depth, random_state=0)\n                ml_rf.fit(x_train, y_train)\n                predicted = ml_rf.predict(x_test)\n                wmaes.append(wmae(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes))\n            resultado.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes)})\n    return pd.DataFrame(resultado)","7f63db3d":"#-- creating fuction to calculate error\ndef wmae(db, y, y_h):\n    weights = db.IsHoliday_train.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(y-y_h))\/(np.sum(weights)), 2)","4a9d9966":"#-- applying ml\nn_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","aef43a46":"#-- creating columns based on date, I'll test if those columns will improve models evaluation\ndb_train_store_features['week'] = db_train_store_features['Date'].dt.week\ndb_train_store_features['month'] = db_train_store_features['Date'].dt.month\ndb_train_store_features['day_year'] = db_train_store_features['Date'].dt.dayofyear","cdd60aa2":"#-- selecting features\nX_train = db_train_store_features[['Store','Dept','IsHoliday_train','Size', 'Type', 'week', 'month', 'day_year']]\nY_train = db_train_store_features['Weekly_Sales']","943d7beb":"#-- running second model\nn_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","51b251ea":"#-- defining model to use in test db\nml_rf = RandomForestRegressor(n_estimators=60, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1, random_state=0)\nml_rf.fit(X_train, Y_train)","e92f72db":"#-- printing head\ndb_test.head()","59f7a47a":"#-- checking data\ndb_test.info()","11334c93":"#-- converting date field\ndb_test.Date = db_test.Date.apply(pd.to_datetime)","6df1b7b9":"#-- checking convertion\ndb_test.info()","f00c6a1f":"#-- merging db_train + db_store\ndb_train_store = pd.merge(left=db_train, right=db_stores, on='Store', how='left')\ndb_train_store.head()","8d0d63ed":"#-- merging dbs\ndb_test_store = pd.merge(left=db_test, right=db_stores, on='Store', how='left' )\ndb_test_store.head()","5645d284":"#-- merging dbs\ndb_test_store_features = pd.merge(left=db_test_store, right=db_features, on=['Store', 'Date'], how='left', suffixes=('_train', '_features'))\ndb_test_store_features.head()","1f68c9fd":"#-- converting\ndb_test_store_features.Type = db_test_store_features.Type.map({'A':1, 'B':2, 'C':3})","eafd0c84":"#-- creating same features\ndb_test_store_features['week'] = db_test_store_features['Date'].dt.week\ndb_test_store_features['month'] = db_test_store_features['Date'].dt.month\ndb_test_store_features['day_year'] = db_test_store_features['Date'].dt.dayofyear","c78d03f3":"#-- selecting features\nX_test = db_test_store_features[['Store','Dept','IsHoliday_train','Size', 'Type', 'week', 'month', 'day_year']]","d800e42e":"#-- running prediction\npredict = ml_rf.predict(X_test)","61a90ed8":"#-- print prediction\nprint(predict)","68af41c0":"#-- concat store + dept + date\ndb_test_store_features['Id'] = db_test_store_features['Store'].astype(str) + '_' + db_test_store_features['Dept'].astype(str) + '_' + db_test_store_features['Date'].astype(str)","38eb5e3e":"#-- concat ID + Prediction\ndb_Submission = pd.concat([db_test_store_features['Id'], pd.DataFrame(predict)], axis=1)\ndb_Submission.columns = ['Id', 'Weekly_Sales']","9da8fc70":"#-- saving results into .csv\ndb_Submission.to_csv('walmart_thiago_mauricio.csv', index=False)","6bd9c585":"> Irei testar se os dados s\u00e3o estacion\u00e1rios","3d798d89":"## Next steps\n1. Fill missing values\n2. User more featuring engineering as: difference between weekly sales and moving average\n3. Use cross validation\n4. Calculate feature importance\n5. Test a different algorithm\n6. Test splitting data by Store Type (A, B and C) ","d86ffbf8":"## Conclusions\n1. some unmployment rate has more weekly sales, temperature and store size\n2. store size has a little influence in weekly sales\n3. week, month and day of the year have a strong impact in model's prediction","4b861455":"## Analyzing Stores","28a532dc":"With the figure above, it is possible to verify:\n- some unmployment rate has more weekly sales, temperature and store size","cb7ad6e9":"## Analyzing Features","19027542":"> Date field was not recognized as data, let's convert it","2d8dbd2e":"## Applying into test db","302df93e":"> Same issue, date field is been considerde as object. Let's convert it","529ffc0f":"> Date field converted","ff9e7433":"### Creating file to save results","fcea31da":"- This model, with featuring engineering was considered improved in comparison with first model. So, I'll use it.","16bf34df":"> Features seems to be consistent","d6390ba4":"> I'll test normality into the data","e230c8c5":"The databases are not normal, so I can not calculate confidential interval.","8c2f5f64":"## MLII","b458849b":"> Convertion OK!","a99824a9":"> Number of department are not the same by store","f55bbc74":"Analyzig the plots above, we can notice a difference in the total sell by store type. \n- All kind of stores presents a tendence to increase sales. \n- We can notice sazonality too, as black fridays and periods before Christmas.","1ed8ad0c":"## MLI - BaseLine","f7b408c2":"- The data set has missing values and I didn't test for outliers (due to restrict time to solve this case). So, I'm gonna use random forest model, because it is not affect by those issues.","c6dcca76":"- Only C stores are not stacionary. This is important if I'll use ARIMA's model.","d15d8bab":"> MarkDownn fields are filled after Nov 2011, that's something we must check after. On the other hand, there are a lot of missing values. Unemployment and CPI have missing values, but we can just remove the lines.","f4bd6cd6":"## Merging dbs","f19f2551":"- Store size plays a little role in weekly sales","b1b9864b":"### Feature engineering","c43c4e45":"## Analyzing Train"}}