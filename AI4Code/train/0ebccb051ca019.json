{"cell_type":{"20835bfc":"code","e2d1a453":"code","74325146":"code","6bd04b69":"code","638613f0":"code","5e186308":"code","0840d12e":"code","a1b84847":"code","bf0192dc":"code","53795f6e":"code","b7397d95":"code","89726222":"code","72afbb9e":"code","b806b600":"code","7d590276":"code","2e39cf1e":"code","d159317c":"code","622a40b6":"markdown","beac6561":"markdown","1fd88f56":"markdown","7ce1931e":"markdown","d28a2f3d":"markdown","b73712c1":"markdown","ba81806e":"markdown","12254931":"markdown","8bff6627":"markdown","11bd8971":"markdown","c86f0771":"markdown"},"source":{"20835bfc":"import pickle #permet la lecture des donn\u00e9es binaires \nimport pandas as pd\nimport numpy as np                                              \nfrom textblob import TextBlob #Correcteur orthographique \nfrom sklearn.feature_extraction.text import CountVectorizer   \nfrom sklearn.model_selection import train_test_split           \nfrom sklearn.linear_model import LogisticRegression            \nfrom sklearn.metrics import confusion_matrix                   \nfrom sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF\nimport nltk\nfrom nltk.corpus import stopwords #R\u00e9ccup\u00e8re les mots vides \nfrom wordcloud import WordCloud #Cr\u00e9ation de nuage de mots \nimport matplotlib.pyplot as plt ","e2d1a453":"# Enregistrement des donn\u00e9es dans des variables: pos_data = critiques positives    | neg_data = critiques n\u00e9gatives \n\n#with open(\"C:\/Users\/..\/imdb_raw_pos.pickle\",'rb') as file_1:\n    #pos_data = pickle.load(file_1)\n\n#with open(\"C:\/Users\/..\/\/imdb_raw_neg.pickle\",'rb') as file_2:\n    #neg_data = pickle.load(file_2)\n    \npos_data = pd.read_pickle(\"..\/input\/imdb-pos\/imdb_raw_pos.pickle\")\nneg_data = pd.read_pickle(\"..\/input\/imdb-neg\/imdb_raw_neg.pickle\")\n    \nprint(\"Le dataset de critiques positives contient \" + str(len(pos_data)) + \" observations.\")\nprint(\"Le dataset de critiques n\u00e9gatives contient \" + str(len(neg_data)) + \" observations.\")","74325146":"#Lecture de la premi\u00e8re critique positive et derni\u00e8re critique n\u00e9gative.\n\nprint(pos_data[0])\nprint(\"------------------\")\nprint(neg_data[12499])","6bd04b69":"caracteres_speciaux=[',','\"','<br \/>',';','--','---','`','\/',\"'\",\"]\",'[','?',':','(',')','<','>','|','{','}','*','%','_','!',\".\",'~','1','2','3','4','5','6','7','8','9','0','$']\n\nfor i in range(0,len(pos_data)):\n    #Correction de l'orthographe (prend beaucoup de temps malheureusement)\n    #pos_data[i] = str(TextBlob(pos_data[i]).correct())                    \n    #neg_data[i] = str(TextBlob(neg_data[i]).correct()) \n    \n    #Changer toutes les majuscules en minuscules \n    pos_data[i] = pos_data[i].lower()                                      \n    neg_data[i] = neg_data[i].lower()   \n    \n    #Remplacer tous les caract\u00e8res sp\u00e9ciaux par des espaces, j'ai laiss\u00e9 '-' car certains mots compos\u00e9s sont pertinents \n    for char in range(0,len(caracteres_speciaux)):                         \n        pos_data[i] = pos_data[i].replace(caracteres_speciaux[char],\" \")\n        neg_data[i] = neg_data[i].replace(caracteres_speciaux[char],\" \")                        \n    i+=1\n    ","638613f0":"Data= np.hstack((pos_data,neg_data))\n\n#nombre de lignes de Data\nprint(\"La matrice Data contient donc \" + str(Data.shape[0]) + \" lignes.\")  \nprint(\"---------------------\")\n #Affichage de la premi\u00e8re critique de Data = premi\u00e8re critique de pos_data\nprint(Data[0]) \nprint(\"---------------------\")\n#Affichage de la derni\u00e8re critique de Data = derni\u00e8re critique de neg_data\nprint(Data[24999])                                                         \n","5e186308":"#stopwords\nstop = set(stopwords.words(\"english\"))\n#Mise \u00e0 jour des stopwords car ceux ci ont sensiblement le m\u00eame poids dans les deux set de critiques.\n#stop.update(['movie','film','like','one','see','story','time','would','also','people','movies','show','even','good','really','much','make',\n                  #'watch','first','think','characters','way','films','many','could','seen','made','character','little','get','know','two','well','ever','never','say','end','10','plot','scene',\n                  #'scenes','great','acting','better','funny','actually','go','life','makes','going','man','actors'])\n\n\n#Cr\u00e9ation de la Matrice X avec le module counvectorizer\ncv = CountVectorizer(binary=True,stop_words=(stop)) #Stop_words pour supprimer les mots vides \ncv.fit(Data)\nX = cv.transform(Data) \n\n#print(stop)\nprint(\"La matrice X cr\u00e9\u00e9e a donc \"+ str(X._shape[0]) + \" lignes et \" + str(X._shape[1]) + \" colonnes.\")\nprint(str(X._shape[1]) + \" est donc la taille de notre dictionnaire (Vocabulaire) c'est \u00e0 dire le nombre de mots uniques trouv\u00e9s dans les diff\u00e9rentes critiques .\")\n","0840d12e":"#Les diff\u00e9rents mots de notre dictionnaire \/ vocabulaire\n#print(cv.get_feature_names()) \nFeatures_names = cv.get_feature_names()\nFeatures_names_300=[]\nfor i in range(0,300):\n        Features_names_300.append(Features_names[i])\n\n#Affiche Les 300 premiers mots du vocabulaire\nprint(Features_names_300)","a1b84847":"Y = np.vstack((np.ones(shape=(12500,1)),np.zeros(shape=(12500,1))))\nprint(Y)","bf0192dc":"X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size =0.15,random_state=0)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","53795f6e":"#Comptons le nombre de critiques n\u00e9gatives dans Y_train \nnb_critique = 0 \nfor i in range(0,len(y_train)):\n     if y_train[i] ==0:\n        nb_critique +=1\n        \nprint(\"Il y'a donc \" + str(nb_critique) + \" critiques n\u00e9gatives sur \" + str(y_train.shape[0]) + \" critiques totales dans Y_train.\") \nprint(\"\u25baSoit \" + str(nb_critique \/ y_train.shape[0] *100) + \" % de critiques n\u00e9gatives et \" + str(100 - (nb_critique \/ y_train.shape[0] *100 )) + \" % de critiques positives dans Y_train\")\n\nprint(\"---------------\")\n\n#Comptons le nombre de critiques n\u00e9gatives dans y_test \nnb_critique = 0 \nfor i in range(0,len(y_test)):\n     if y_test[i] ==0:\n        nb_critique +=1\n        \nprint(\"Il y'a donc \" + str(nb_critique) + \" critiques n\u00e9gatives sur \" + str(y_test.shape[0]) + \" critiques totales dans Y_test.\") \nprint(\"\u25baSoit \" + str(nb_critique \/ y_test.shape[0] *100) + \" % de critiques n\u00e9gatives et \" + str(100 - (nb_critique \/ y_test.shape[0]*100 )) + \" % de critiques positives dans Y_test\")\n\n","b7397d95":"Lr =LogisticRegression()\nLr.fit(X_train,y_train)","89726222":"y_pred =Lr.predict(X_test)\n\n#Affichage de Y_test et Y_pred\n#for i in range(0,len(y_test)):\n    #print(str(y_pred[i])+ \" | \" + str(y_test[i]))\nprint(y_pred)          ","72afbb9e":"cm = confusion_matrix(y_test,y_pred)    \n\nscore = (cm[0,0]+cm[1,1])\/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0]) * 100\n\n#Matrice de confusion \nprint(\"Matrice de confusion:\")\nprint(cm)\nprint(\"----------------\")\nprint(\"Notre mod\u00e8le de r\u00e9gression logistique pr\u00e9dit bien les donn\u00e9es \u00e0 \" + str(score) + \" %.\")","b806b600":"#Cr\u00e9ation d'une fonction pour r\u00e9ccup\u00e9rer les mots et leurs fr\u00e9quences dans un texte\ndef get_top_n_words(texte, n=None):\n    # Initialiser la variable des mots vides\n     stop = set(stopwords.words(\"english\"))\n        \n     #Mise \u00e0 jour des stopwords car ceux ci ont sensiblement le m\u00eame poids dans les deux set de critiques.\n     stop.update(['movie','film','like','one','see','story','time','would','also','people','movies','show','even','good','really','much','make',\n                  'watch','first','think','characters','way','films','many','could','seen','made','character','little','get','know','two','well','ever','never','say','end','10','plot','scene',\n                  'scenes','great','acting','actually','go','better','funny','life','makes','going','man','actors'])\n    \n     # Cr\u00e9ation de la transformation TF-IDF\n     vectorizer = TfidfVectorizer(analyzer=u'word',stop_words=set(stop), lowercase=True, max_features=15000)\n     vectorizer.fit(texte)\n     Bag_of_words = vectorizer.fit_transform(texte)\n     Sum_words = Bag_of_words.sum(axis=0)\n     words_freq = [(word, Sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n     words_freq =sorted(words_freq, key=lambda x: x[1], reverse=True)\n     return words_freq[:n]\n \n#Top 10 mots ayant plus de poids pour classifier une critique positive\nTop_10_positifs = get_top_n_words(pos_data, 10)\n\n#Top 10 mots ayant plus de poids pour classifier une critique n\u00e9gative\nTop_10_negatifs = get_top_n_words(neg_data, 10)\n\nprint('Top 10 mots ayant plus de poids pour classifier une critique positive et leurs poids :')\nfor i in range(0,10):\n    print(Top_10_positifs[i])\n    \nprint(\"----------------\")\n\nprint('Top 10 mots ayant plus de poids pour classifier une critique n\u00e9gative et leurs poids :')\nfor i in range(0,10):\n    print(Top_10_negatifs[i])\n","7d590276":"#Affichage des mots n\u00e9gatifs avec Wordcloud \ntext_neg =\"\"\n\nfor i in range(0,len(neg_data)):\n    text_neg+= str(neg_data[i])","2e39cf1e":"stop = set(stopwords.words(\"english\"))\nstop.update(['movie','film','like','one','see','story','time','would','also','people','movies','show','even','good','really','much','make',\n                  'watch','first','think','characters','way','films','many','could','seen','made','character','little','get','know','two','well','ever','never','say','end','10','plot','scene',\n                  'scenes','great','acting','better','funny','actually','go','life','makes','going','man','actors'])\n\n\nwordcloud = WordCloud(stopwords=stop, background_color=\"white\")\nwordcloud.generate(text_neg)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","d159317c":"#Affichage des mots positifs avec Wordcloud\ntext_pos =\"\"\n\nfor i in range(0,len(pos_data)):\n    text_pos+= str(pos_data[i])  \nwordcloud = WordCloud(stopwords=stop, background_color=\"white\")\nwordcloud.generate(text_pos)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","622a40b6":"<h3> 6. Performance du mod\u00e8le <\/h3>\n<h5> \u2022 Matrice de confusion <\/h5>","beac6561":"<h2> SENTIMENTS ANALYSIS (IMBD) \n\n<h5> par <b> Vada ZAMBLE <b> | vadazamble@yahoo.fr \n\n<h3> Contexte \n    \n<h5> Il s'agit de r\u00e9pondre \u00e0 une probl\u00e9matique de <b>sentiment analysis <\/b> sur des critiques de films. Les donn\u00e9es pour entra\u00eener et tester le mod\u00e8le de <b>classification binaire<\/b> sont issues du site IMDB et peuvent \u00eatre r\u00e9cup\u00e9r\u00e9es sous la forme de fichiers textes depuis le site de Stanford 2.\n\n\n**Dataset**:        <a>ai.stanford.edu\/  <\/a>  |   <a> www.imdb.com <\/a>\n\n<h3> 1. Import des modules \n","1fd88f56":"<h3> 7. Top 10 mots ayant plus de poids pour classifier une critique 'POSITIVE' et 'NEGATIVE'<\/h3>","7ce1931e":"\u25ba <b> Cr\u00e9ation du vecteur Y contenant 1 si la critique est positive et 0 si la critique est n\u00e9gative <\/b> en tenant compte de la position des critiques positives et n\u00e9gatives dans la matrice X.","d28a2f3d":"<h3> 4. R\u00e9pr\u00e9sentation du corpus \u00e0 l'aide des sacs de mots (<i>Bag of words<\/i>) <\/h3>\n\nOn utilisera une repr\u00e9sentation en sac de mots (bag of words) des critiques par le processus de \"tokenisation\" consistant \u00e0 r\u00e9p\u00e9rer les mots uniques du corpus qui vont constituer le dictionnaire. Les mots deviennent ainsi des descripteurs (features) et l'objectif est d'associer l'absence ou la pr\u00e9sence de chaque mot \u00e0 chaque critique dans une matrice.","b73712c1":"\u25ba <b>Rassembler les deux listes de critiques positives et n\u00e9gatives dans une seule matrice<\/b>","ba81806e":"<h5> <b> \u2022 Mod\u00e8le de r\u00e9gression logistique <\/b> <\/h5>","12254931":"#### \u2022 Pr\u00e9diction \u00e0 partir de X_Test ","8bff6627":"<h3> 5. Mod\u00e9lisation : R\u00e9gression logistique <\/h3>\n\n<h5> <b>\u2022 Division de X et Y en TRAIN et TEST <\/b> <i>(85%,15%)<\/i> <\/h5>","11bd8971":"<h3> 2. Chargement des donn\u00e9es et Visualisation <\/h3>","c86f0771":"<h3> 3. Pr\u00e9traitements des donn\u00e9es <\/h3>\n\nLe but est de supprimer tous les \u00e9l\u00e9ments ind\u00e9sirables du texte avant de charger les donn\u00e9es dans le mod\u00e8le.\nCes \u00e9tapes de pr\u00e9traitements sont les suivantes:\n<ol><li><i> Correction de l'orthographe des mots afin de s'assurer que tous les mots ont la m\u00eame orthographe. <\/i> <\/li>\n<li><i> Suppression des caract\u00e8res sp\u00e9ciaux, de certains signes de ponctuations, des chiffres (optionnel) et des sauts de lignes inutiles sinon <b>\"mot.\"<\/b>,<b>\"mot,\"<\/b> et <b>\"mot\"<\/b> seront trait\u00e9s diff\u00e9remment par le mod\u00e8le.  <\/i><\/li>\n<li><i> Transformation de toutes les majuscules en miniscules sinon <b>\"mot\"<\/b>, <b>\"Mot\"<\/b> et <b>\"MOT\"<\/b> seront trait\u00e9s diff\u00e9remment par le mod\u00e8le.<\/i><\/li>\n<li><i> Suppression des mots vides (Stopwords) car il s'agit de mots n'ayant pas de r\u00e9elles significations pertinentes n\u00e9cessaires au mod\u00e8le. Ils sont tr\u00e8s courants et ne permettent pas de caract\u00e9riser un texte par rapport \u00e0 un autre.  <\/i><\/ol>"}}