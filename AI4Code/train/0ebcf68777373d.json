{"cell_type":{"a75505ec":"code","dc17f243":"code","ba371cf2":"code","96a35e42":"code","f46c11d9":"code","9fd5f82f":"code","46511995":"code","8dc04e05":"code","666fb718":"code","1652a93d":"code","0d0b3e8f":"code","aecf87bb":"code","b610689f":"code","b4aed7f8":"code","538ab40e":"code","66730868":"code","cb410675":"code","44346106":"code","c7a7ac54":"markdown","d202244b":"markdown","ae9d7e55":"markdown","35ad8e7a":"markdown","6b689575":"markdown","a261386d":"markdown","95484a18":"markdown","b1fc79e8":"markdown","62ec91b7":"markdown","d8d4dd51":"markdown","5517d9de":"markdown","0ca8c19b":"markdown","b0de3c98":"markdown","0f2f6322":"markdown","bab8dae2":"markdown","c6da98e3":"markdown","7a100c50":"markdown","54aa0925":"markdown","15fedf44":"markdown","eedddff2":"markdown","916f3739":"markdown","ce3367f2":"markdown","59ebb3fc":"markdown"},"source":{"a75505ec":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport sklearn.datasets\nimport h5py\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # tama\u00f1o de los gr\u00e1ficos\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'","dc17f243":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/scripttaller6\/init_utils.py\", dst = \"..\/working\/init_utils.py\")\n\n# import all our functions\nfrom init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\nfrom init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n\n# cargamos los datos: puntos azules\/{rojos en c\u00edrculos\ntrain_X, train_Y, test_X, test_Y = load_dataset()","ba371cf2":"def load_cat_dataset():\n    train_dataset = h5py.File('datasets\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # entrenamiento\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n\n    test_dataset = h5py.File('datasets\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # prueba\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # clases\n    \n    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n    \n    train_set_x = train_set_x_orig\/255\n    test_set_x = test_set_x_orig\/255\n\n    return train_set_x, train_set_y, test_set_x, test_set_y, classes","96a35e42":"def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n    \"\"\"\n    Implementa una red neuronal de 3 capas: LINEAL->RELU->LINEAL->RELU->LINEAL->SIGMOIDE.\n    Input:\n    X: datos de entrada, con dimensi\u00f3n (2, n\u00famero de ejemplos)\n    Y: vector con las etiquetas observadas (contiene 0 para los puntos rojos y 1 para los azules), con dimensi\u00f3n (1, n\u00famero de ejemplos)\n    learning_rate: tasa de aprendizaje para el G.D. \n    num_iterations: n\u00famero de iteraciones para ejecutar G.D.\n    print_cost: si es verdadero (True), muestra el coste cada 1000 iteraciones\n    initialization: permite elegir el m\u00e9todo a utilizar (\"zeros\",\"random\" or \"he\")\n    Output:\n    parameters: los par\u00e1metros aprendidos por el modelo\n    \"\"\"\n        \n    grads = {}\n    costs = []     \n    m = X.shape[1]        # n\u00famero de ejemplos\n    layers_dims = [X.shape[0], 10, 5, 1]\n    \n    # Inicializa los par\u00e1metros del diccionario.\n    if initialization == \"ceros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n\n    # Bucle (G.D.)\n\n    for i in range(0, num_iterations):\n\n        # Propagaci\u00f3n hacia delante: LINEAL -> RELU -> LINEAL -> RELU -> LINEAL -> SIGMOIDE.\n        a3, cache = forward_propagation(X, parameters)\n        \n        # P\u00e9rdida\n        cost = compute_loss(a3, Y)\n\n        # Retro-propagaci\u00f3n.\n        grads = backward_propagation(X, Y, cache)\n        \n        # Actualizaci\u00f3n de par\u00e1metros.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Imprimir la p\u00e9rdida cada 1000 iteraciones\n        if print_cost and i % 1000 == 0:\n            print(\"Coste tras la iteraci\u00f3n {}: {}\".format(i, cost))\n            costs.append(cost)\n            \n    # grafica de la p\u00e9rdida\n    plt.plot(costs)\n    plt.ylabel('Coste')\n    plt.xlabel('Iteraciones ')\n    plt.title(\"Tasa de aprendizaje =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","f46c11d9":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters_zeros \n\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Inputs:\n    layer_dims: arreglo (lista) python con el tama\u00f1o de cada capa.\n    Output:\n    parameters: diccionario python con los par\u00e1metros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1: matriz de pesos de dimensi\u00f3n (layers_dims[1], layers_dims[0])\n                    b1: vector de sesgo de dimensi\u00f3n (layers_dims[1], 1)\n                    ...\n                    WL: matriz de pesos de dimensi\u00f3n  (layers_dims[L], layers_dims[L-1])\n                    bL: vector de sesgo de dimensi\u00f3n (layers_dims[L], 1)\n    \"\"\"\n    \n    parameters = {}\n    L = len(layers_dims)            # numero de capas de la red\n    \n    for l in range(1, L):\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        parameters['W' + str(l)] = \n        parameters['b' + str(l)] = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    return parameters","9fd5f82f":"parameters = initialize_parameters_zeros([3,2,1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","46511995":"parameters = model(train_X, train_Y, initialization = \"ceros\")\nprint (\"Sobre el conjunto de entrenamiento:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"Sobre el conjunto de prueba:\")\npredictions_test = predict(test_X, test_Y, parameters)","8dc04e05":"print (\"Predicciones en entrenamiento = \" + str(predictions_train))\nprint (\"Predicciones en prueba = \" + str(predictions_test))","666fb718":"plt.title(\"Modelo con inicializaci\u00f3n con ceros\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nY2=train_Y.flatten()\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, Y2)","1652a93d":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters_random\n\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Input:\n    layer_dims: arreglo (lista) python con el tama\u00f1o de cada capa.\n    Output:\n    parameters: diccionario python con los par\u00e1metros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1: matriz de pesos de dimensi\u00f3n (layers_dims[1], layers_dims[0])\n                    b1: vector de sesgo de dimensi\u00f3n (layers_dims[1], 1)\n                    ...\n                    WL: matriz de pesos de dimensi\u00f3n  (layers_dims[L], layers_dims[L-1])\n                    bL: vector de sesgo de dimensi\u00f3n (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)               # Semilla para replicar la inicializaci\u00f3n aleatoria\n    parameters = {}\n    L = len(layers_dims)            # N\u00famero de capas\n    \n    for l in range(1, L):\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        parameters['W' + str(l)] = \n        parameters['b' + str(l)] = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    return parameters","0d0b3e8f":"parameters = initialize_parameters_random([3, 2, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","aecf87bb":"parameters = model(train_X, train_Y, initialization = \"random\")\nprint (\"Sobre el conjunto de entrenamiento:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"Sobre el conjunto de prueba:\")\npredictions_test = predict(test_X, test_Y, parameters)","b610689f":"print (predictions_train)\nprint (predictions_test)","b4aed7f8":"plt.title(\"Modelo con inicializaci\u00f3n aleatoria con valores grandes\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, Y2)","538ab40e":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters_he\n\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Input:\n    layer_dims: arreglo (lista) python con el tama\u00f1o de cada capa.\n    Output:\n    parameters: diccionario python con los par\u00e1metros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1: matriz de pesos de dimensi\u00f3n (layers_dims[1], layers_dims[0])\n                    b1: vector de sesgo de dimensi\u00f3n (layers_dims[1], 1)\n                    ...\n                    WL: matriz de pesos de dimensi\u00f3n  (layers_dims[L], layers_dims[L-1])\n                    bL: vector de sesgo de dimensi\u00f3n (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # n\u00famero de capas\n     \n    for l in range(1, L + 1):\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        parameters['W' + str(l)] = \n        parameters['b' + str(l)] = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n    return parameters","66730868":"parameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","cb410675":"parameters = model(train_X, train_Y, initialization = \"he\")\nprint (\"Sobre el conjunto de entrenamiento:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"Sobre el conjunto de prueba:\")\npredictions_test = predict(test_X, test_Y, parameters)","44346106":"plt.title(\"Modelo con la inicializaci\u00f3n de He\")\naxes = plt.gca()\naxes.set_xlim([-1.5,1.5])\naxes.set_ylim([-1.5,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, Y2)","c7a7ac54":"Si aparece un \"inf\" en el coste tras la iteraci\u00f3n 0, es por un redondeo num\u00e9rico que se podr\u00eda resolver por una implementaci\u00f3n m\u00e1s detallada (que no es tan relevante para este ejercicio). \n\nDe todos modos, se ha logrado romper la simetr\u00eda. Esto tambi\u00e9n lleva a conseguir mejores resultados. ","d202244b":"<font color='blue'>\n**En este taller debe haber aprendido a:**\n- Inicializaciones diferentes llevan a resultados distintos\n- La incializaci\u00f3n aleatoria es usada para romper la simetr\u00eda y asegurarse que las distintas unidades escondidas (neuronas) puedan aprender funciones distintas\n- No inicialice con valores muy grandes\n- La inicilizaci\u00f3n de He funciona bien con redes con activaci\u00f3n tipo ReLU. \n    \nEspero te haya gustado este Notebook. Por favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","ae9d7e55":"El modelo est\u00e1 prediciendo 0 para todo ejemplo (no hay frontera, todo es un mismo fondo)\n\nEn general, la inicializaci\u00f3n a cero de los pesos resulta en una red que falla en romper la simetr\u00eda. Esto significa que todas las neuronas de cada capa aprenden lo mismo, por lo cual dar\u00eda lo mismo entrenar una red neuronal con una sola unidad, tal que $n^{[l]}=1$ en cada capa. De esta manera, la red no tiene un mayor poder discriminatorio que cualquier otro clasificador lineal como e.g. la regresi\u00f3n log\u00edstica. ","35ad8e7a":"# Inicializaci\u00f3n de pesos\n\nEntrenar una red neuronal requiere especificar unos valores iniciales para los pesos. Un m\u00e9todo de inicializaci\u00f3n bien elegido va a mejorar bastante el aprendizaje.  \n\nEn los talleres anteriores hemos implementado algunas inicializaciones que han dado buen resultado. Pero c\u00f3mo se elige el m\u00e9todo de inicializaci\u00f3n para una red neuronal? En este taller vamos a ver c\u00f3mo distintos m\u00e9todos de inicializaci\u00f3n llevan a obtener resultados diferentes.  \n\nUn m\u00e9todo de inicializaci\u00f3n bien elegido permite:\n- Incrementar la velocidad de convergencia de la b\u00fasqueda por los par\u00e1metros (mediante G.D) \n- Incrementar las probabilidades de que el m\u00e9todo de b\u00fasqueda (G.D.) converja hacia un error de entrenamiento menor, y una mejor generalizaci\u00f3n  \n\nEmepecemos cargando los paquetes y el conjunto de datos para su clasificaci\u00f3n.","6b689575":"<font color='blue'>\n**Lo que debe aprender**:\n- Los pesos $W^{[l]}$ deben ser inicializados aleatotriamente para romper la simetr\u00eda. \n- No hay problema si se inicializan a cero los sesgos $b^{[l]}$. La simetr\u00eda se rompe mientras $W^{[l]}$ sea inicializada aleatoriamente. \n","a261386d":"## 4 - Inicializaci\u00f3n de He\n\nFinalmente, apliquemos la inicializaci\u00f3n de He. Este m\u00e9todo fue presentado en el paper publicado por He et al., 2015. Es muy similar a la inicializaci\u00f3n de Xavier, s\u00f3lo que en la de Xavier los pesos  $W^{[l]}$ se multiplican por `sqrt(1.\/layers_dims[l-1])`, mientras que en la de He se multiplican por `sqrt(2.\/layers_dims[l-1])`.)\n\n**Ejercicio**: Implemente la siguiente funci\u00f3n para inicializar sus par\u00e1metros con el m\u00e9todo de He.\n\n**Ayuda**: Esta funci\u00f3n es similar a la que previamente escribi\u00f3 `initialize_parameters_random(...)`. La \u00fanica diferencia va a ser que en lugar de multiplicar `np.random.randn(..,..)` por 10, ahora se multiplica por $\\sqrt{\\frac{2}{\\text{dimensi\u00f3n de la capa previa}}}$, que la que la inicializaci\u00f3n de He recomienda para las capas con una activaci\u00f3n ReLU. ","95484a18":"Ejecute la celda abajo para entrenar el modelo en 15,000 iteraciones utilizando la inicializaci\u00f3n en ceros.","b1fc79e8":"**Ejercicio:** Verifique si es posible mejorar los resultados anteriores, obtenidos mediante la funci\u00f3n `initialize_parameters_random(layers_dims)`, incializando los pesos de manera aleatoria pero con valores peque\u00f1os.\n**Ayuda:** Utilice `np.random.randn(..,..) * k`, donde k es una constante que toma un valor peque\u00f1o.","62ec91b7":"El desempe\u00f1o es bastante malo, y el coste  realmente no disminuye. De hecho, el algoritmo no mejora en nada unos resultados completamente aleatorios. Veamos con mayor detalle el resultado de las predicciones y la frontera de decisi\u00f3n\/clasificaci\u00f3n:","d8d4dd51":"**Salida esperada**:\n\n<table> \n    <tr>\n    <td>\n    **W1**\n    <\/td>\n        <td>\n    [[ 0.  0.  0.]\n [ 0.  0.  0.]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b1**\n    <\/td>\n        <td>\n    [[ 0.]\n [ 0.]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **W2**\n    <\/td>\n        <td>\n    [[ 0.  0.]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b2**\n    <\/td>\n        <td>\n    [[ 0.]]\n    <\/td>\n    <\/tr>\n\n<\/table> ","5517d9de":"## 2 - Inicializaci\u00f3n con ceros\n\nHay dos tipos de par\u00e1metros para inicializar una red neuronal:\n- las matrices de pesos $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n- los vectores de sesgo $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n\n**Ejercicio**: Implemente la siguiente funci\u00f3n para incializar todos los par\u00e1metros a cero. Como hemos visto, esta inicializaci\u00f3n no va a funcionar bien pues falla en \"romper la simetr\u00eda\". Para verificarlo, utilice np.zeros((..,..)) con las dimensiones adecuadas.","0ca8c19b":"Se quiere desarrollar un clasificador que separe los puntos azules de los rojos. ","b0de3c98":"**Observaciones**:\n- El modelo con la inicializaci\u00f3n de He separa muy bien los puntos rojos de los azules con unas pocas iteraciones.\n","0f2f6322":"Ha visto tres maneras diferentes de incializar los pesos. Para el mismo n\u00famero de iteraciones y los mismos hiper-par\u00e1metros, la comparaci\u00f3n es la siguiente:\n\n<table> \n    <tr>\n        <td>\n        **Modelo**\n        <\/td>\n        <td>\n        **Precisi\u00f3n de entrenamiento**\n        <\/td>\n        <td>\n        **Comentario**\n        <\/td>\n\n    <\/tr>\n        <td>\n        Red de 3 capas con inicializaci\u00f3n en ceros\n        <\/td>\n        <td>\n        50%\n        <\/td>\n        <td>\n        falla en romper la simetr\u00eda\n        <\/td>\n    <tr>\n        <td>\n        Red de 3 capas con inicializaci\u00f3n aleatoria con valores altos\n        <\/td>\n        <td>\n        83%\n        <\/td>\n        <td>\n        Pesos muy grandes \n        <\/td>\n    <\/tr>\n    <tr>\n        <td>\n        Red de 3 capas con inicializaci\u00f3n de He\n        <\/td>\n        <td>\n        99%\n        <\/td>\n        <td>\n        m\u00e9todo recomendado\n        <\/td>\n    <\/tr>\n<\/table> ","bab8dae2":"Ejecute la siguiente celda para entrenar el modelo sobre 15,000 iteraciones utilizando la incializaci\u00f3n aleatoria.","c6da98e3":"**Observaciones**:\n- El coste empieza bastante alto. Esto es porque se han utilizado valores aleatorios altos, y con valores altos, la salida de la \u00faltima activaci\u00f3n (sigmoide) obtiene valores muy cercanos a 0 o 1 para algunos ejemplos. Entonces, cuando clasifica mal esos ejemplos, incurre en una p\u00e9rdida muy alta. De hecho, cuando $\\log(a^{[3]}) = \\log(0)$, la p\u00e9rdida tiende a infinito.\n- Una inicializaci\u00f3n pobre puede llevar al problema de gradientes que desaparecen o que explotan, lo cual tambi\u00e9n llevar\u00eda a que el algoritmo de optimizaci\u00f3n vaya m\u00e1s despacio.  \n- Si entrena esta red por m\u00e1s tiempo puede conseguir mejores resultados, pero inicializ\u00e1ndola con valores aleatorios demasiado grandes hace que el tiempo de optimizaci\u00f3n sea m\u00e1s prolongado.\n\n<font color='blue'>\n**En resumen**:\n- Inicializar los pesos con valores muy grandes no da buenos resultados. \n- Ser\u00eda de esperar que inicializar con valores aleatorios peque\u00f1os obtenga mejores resultados. Ser\u00eda relevante saber qu\u00e9 tan peque\u00f1os tendr\u00edan que ser estos valores aleatorios. \n    \n<font color='black'>\nVeamos el siguiente ejercicio.. ","7a100c50":"## 1 - Modelo de red neuronal","54aa0925":"## 3 - Inicializaci\u00f3n aleatoria\n\nCon el fin de romper la simetr\u00eda, inicialicemos los pesos de manera aleatoria. De este modo, cada neurona puede aprender una funci\u00f3n diferente a partir de los mismos datos de entrada. En este ejercicio, puede verificar lo que ocurre si los pesos son inicializados aleatoriamente, pero con valores muy grandes.  \n\n**Ejercicio**: Implemente la siguiente funci\u00f3n para inicializar los pesos con valores aleatorios grandes (multiplicados \\*10) y los sesgos con ceros. Utilize `np.random.randn(..,..) * 10` para los pesos y `np.zeros((.., ..))` para los sesgos. La semilla `np.random.seed(..)` es utilizada para poder replicar los resultados sobre la inicializaci\u00f3n aleatoria. ","15fedf44":"Dada una red neuronal de 3 capas, experimente con los siguientes m\u00e9todos de inicializaci\u00f3n:  \n- *Inicializaci\u00f3n con ceros:*  se define `initialization = \"zeros\"` en el argumento de entrada.\n- *Inicializaci\u00f3n aleatoria:* se define `initialization = \"random\"` en el argumento de entrada. Los pesos se inicializan con valores aleatorios grandes.  \n- *Inicializaci\u00f3n de He:* se define `initialization = \"he\"` en el argumento de entrada. Los pesos se inicializan con valores aleatorios en una escala definida por He et al., 2015. \n\n**Instrucciones**: Ejecute el c\u00f3digo a continuaci\u00f3n. Luego implementar\u00e1 los tres m\u00e9todos de inicializaci\u00f3n a los que hace referencia la funci\u00f3n `model()`.","eedddff2":"**Salida esperada**:\n\n<table> \n    <tr>\n    <td>\n    **W1**\n    <\/td>\n        <td>\n    [[ 1.78862847  0.43650985]\n [ 0.09649747 -1.8634927 ]\n [-0.2773882  -0.35475898]\n [-0.08274148 -0.62700068]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b1**\n    <\/td>\n        <td>\n    [[ 0.]\n [ 0.]\n [ 0.]\n [ 0.]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **W2**\n    <\/td>\n        <td>\n    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b2**\n    <\/td>\n        <td>\n    [[ 0.]]\n    <\/td>\n    <\/tr>\n\n<\/table> ","916f3739":"Ejecute la celda abajo para entrenar el modelo sobre 15,000 iteraciones utilizando la inicializaci\u00f3n de He.","ce3367f2":"## 5 - Conclusiones","59ebb3fc":"**Salida esperada**:\n\n<table> \n    <tr>\n    <td>\n    **W1**\n    <\/td>\n        <td>\n    [[ 17.88628473   4.36509851   0.96497468]\n [-18.63492703  -2.77388203  -3.54758979]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b1**\n    <\/td>\n        <td>\n    [[ 0.]\n [ 0.]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **W2**\n    <\/td>\n        <td>\n    [[-0.82741481 -6.27000677]]\n    <\/td>\n    <\/tr>\n    <tr>\n    <td>\n    **b2**\n    <\/td>\n        <td>\n    [[ 0.]]\n    <\/td>\n    <\/tr>\n\n<\/table> "}}