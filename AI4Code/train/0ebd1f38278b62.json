{"cell_type":{"aad1a7cb":"code","ddf04eed":"code","82541aa4":"code","a267fa10":"code","183f9d12":"code","890fb069":"code","71ff2589":"code","9c279218":"code","0d4d40a7":"code","a21fb353":"code","6273649e":"code","b1ab344b":"code","e1b1e287":"code","ede91137":"code","74557ed2":"code","d419c8a5":"code","66310c20":"code","6c47e84c":"code","ee014a53":"code","e11eeafd":"code","dc1e56d3":"code","a5c4bf24":"code","9d195ac0":"code","9d66b7af":"code","8c1d0d81":"code","5b0f9084":"code","2ef84054":"code","690cdfdd":"code","358b9523":"markdown"},"source":{"aad1a7cb":"import numpy as np\nimport pandas as pd\nfrom umap import UMAP\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.manifold import TSNE\n# Clearing up memory\nimport gc\n\n# Featuretools for automated feature engineering\nimport featuretools as ft\nimport featuretools.variable_types as vtypes\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import Imputer\n\n# Modeling\nimport lightgbm as lgb\n\n# Evaluation of the model\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score","ddf04eed":"feature_matrix = pd.read_csv('..\/input\/costa-rican-poverty-derived-data\/ft_2000.csv')\nfeature_matrix.shape","82541aa4":"feature_matrix['SUM(ind.rez_esc \/ escolari)'] = feature_matrix['SUM(ind.rez_esc \/ escolari)'].astype(np.float32)\nfeature_matrix['SUM(ind.age \/ escolari)'] = feature_matrix['SUM(ind.age \/ escolari)'].astype(np.float32)","a267fa10":"for col in feature_matrix:\n    if feature_matrix[col].dtype == 'object':\n        if col != 'idhogar':\n            feature_matrix[col] = feature_matrix[col].astype(np.float32)","183f9d12":"feature_matrix.columns[np.where(feature_matrix.dtypes == 'object')]","890fb069":"missing_threshold = 0.95\ncorrelation_threshold = 0.99\n\n\ntrain = feature_matrix[feature_matrix['Target'].notnull()]\ntest = feature_matrix[feature_matrix['Target'].isnull()]\n\ntrain_ids = list(train.pop('idhogar'))\ntest_ids = list(test.pop('idhogar'))\n\nfeature_matrix = feature_matrix.replace({np.inf: np.nan, -np.inf:np.nan})\nn_features_start = feature_matrix.shape[1]\nprint('Original shape: ', feature_matrix.shape)\n\n# Find missing and percentage\nmissing = pd.DataFrame(feature_matrix.isnull().sum())\nmissing['fraction'] = missing[0] \/ feature_matrix.shape[0]\nmissing.sort_values('fraction', ascending = False, inplace = True)\n\n# Missing above threshold\nmissing_cols = list(missing[missing['fraction'] > missing_threshold].index)\nn_missing_cols = len(missing_cols)\n\n# Remove missing columns\nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in missing_cols]]\nprint('{} missing columns with threshold: {}.'.format(n_missing_cols, missing_threshold))\n\n# Zero variance\nunique_counts = pd.DataFrame(feature_matrix.nunique()).sort_values(0, ascending = True)\nzero_variance_cols = list(unique_counts[unique_counts[0] == 1].index)\nn_zero_variance_cols = len(zero_variance_cols)\n\n# Remove zero variance columns\nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in zero_variance_cols]]\nprint('{} zero variance columns.'.format(n_zero_variance_cols))\n\n# Correlations\ncorr_matrix = feature_matrix.corr()\n\n# Extract the upper triangle of the correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# Select the features with correlations above the threshold\n# Need to use the absolute value\nto_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n\nn_collinear = len(to_drop)\n\nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]\nprint('{} collinear columns removed with correlation above {}.'.format(n_collinear,  correlation_threshold))\n\ntotal_removed = n_missing_cols + n_zero_variance_cols + n_collinear\n\nprint('Total columns removed: ', total_removed)\nprint('Shape after feature selection: {}.'.format(feature_matrix.shape))\n\n# Remove columns derived from the Target\ndrop_cols = []\nfor col in feature_matrix:\n    if col == 'Target':\n        pass\n    else:\n        if 'Target' in col:\n            drop_cols.append(col)\n\nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]    \n\n# Extract out training and testing data\ntrain = feature_matrix[feature_matrix['Target'].notnull()]\ntest = feature_matrix[feature_matrix['Target'].isnull()]\n\ntrain_ids = list(train.pop('idhogar'))\ntest_ids = list(test.pop('idhogar'))\n\ntrain_labels = np.array(train.pop('Target')).reshape((-1, ))\ntest = test.drop(columns = 'Target')\n\ntrain = train.replace({np.inf: np.nan, -np.inf: np.nan})\ntest = test.replace({np.inf: np.nan, -np.inf: np.nan})","71ff2589":"from sklearn.impute import SimpleImputer\n\nfeature_list = list(train.columns)\n\nimputer = SimpleImputer(strategy = 'median')\ntrain = imputer.fit_transform(train)\ntest = imputer.transform(test)\n\ntrain_df = pd.DataFrame(train, columns = feature_list)\ntest_df = pd.DataFrame(test, columns = feature_list)\ntrain.shape","9c279218":"train_df = train_df.astype(np.float32)\ntest_df = test_df.astype(np.float32)","0d4d40a7":"from timeit import default_timer as timer\n\nn_components = 3","a21fb353":"umap = UMAP(n_components=n_components)\npca = PCA(n_components=n_components)\nica = FastICA(n_components=n_components)\ntsne = TSNE(n_components=n_components)","6273649e":"for method, name in zip([umap, pca, ica], ['umap', 'pca', 'ica']):\n    \n    if name == 'umap':\n        start = timer()\n        reduction = method.fit_transform(train, train_labels)\n        test_reduction = method.transform(test)\n        end = timer()\n    \n    else:\n        start = timer()\n        reduction = method.fit_transform(train)\n        test_reduction = method.transform(test)\n        end = timer()\n        \n    print(f'Method: {name} {round(end - start, 2)} seconds elapsed.')\n    train_df['%s_c1' % name] = reduction[:, 0]\n    train_df['%s_c2' % name] = reduction[:, 1]\n    train_df['%s_c3' % name] = reduction[:, 2]\n    \n    test_df['%s_c1' % name] = test_reduction[:, 0]\n    test_df['%s_c2' % name] = test_reduction[:, 1]\n    test_df['%s_c3' % name] = test_reduction[:, 2]","b1ab344b":"train_df['label'] = train_labels","e1b1e287":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\ncmap = plt.get_cmap('tab10', 4)\n\nfor method, name in zip([umap, pca, ica], ['umap', 'pca', 'ica']):\n    fig = plt.figure(figsize = (8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    p = ax.scatter(train_df['%s_c1' % name], train_df['%s_c2'  % name], train_df['%s_c3'  % name], c = train_df['label'].astype(int), cmap = cmap)\n    plt.title(f'{name.capitalize()}')\n    fig.colorbar(p, aspect = 4, ticks = [1, 2, 3, 4])\n    ","ede91137":"test_comp = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\nsubmission_base = test_comp.loc[:, ['idhogar', 'Id']]","74557ed2":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","d419c8a5":"def model_gbm(features, labels, test_features, test_ids, nfolds = 5, return_preds = False):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n    \n    # Model with hyperparameters selected from previous work\n    model = lgb.LGBMClassifier(boosting_type = 'gbdt', n_estimators = 10000, max_depth = -1,\n                               learning_rate = 0.025, metric = 'None', min_child_samples = 30,\n                               reg_alpha = 0.35, reg_lambda = 0.6, num_leaves = 15, \n                               colsample_bytree = 0.85, objective = 'multiclass', \n                               class_weight = 'balanced', \n                               n_jobs = -1)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        # Dataframe for \n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        predictions = predictions.append(fold_predictions)\n        \n        importances += model.feature_importances_ \/ nfolds    \n\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    valid_scores = np.array(valid_scores)\n    print(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], \n                                       on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances\n    return submission, feature_importances, valid_scores","66310c20":"train_df.head()","6c47e84c":"for col in train_df:\n    if 'Target' in col:\n        print(col)","ee014a53":"predictions, fi = model_gbm(train_df.drop(columns = 'label'), train_labels, \n                                   test_df, test_ids, return_preds = True)","e11eeafd":"fi.sort_values('importance').dropna().tail()","dc1e56d3":"submission, fi, scores = model_gbm(train_df.drop(columns = 'label'), train_labels, \n                                   test_df, test_ids, return_preds = False)\n\nsubmission.to_csv('dimension_reduction.csv', index = False)","a5c4bf24":"fi.sort_values('importance').dropna().tail(25)","9d195ac0":"scores.mean()","9d66b7af":"scores.std()","8c1d0d81":"for method, name in zip([umap, pca, ica], ['umap', 'pca', 'ica']):\n    start = timer()\n    reduction = method.fit_transform(train)\n    test_reduction = method.transform(test)\n    end = timer()\n    print(f'Method: {name} {round(end - start, 2)} seconds elapsed.')\n    train_df['%s_c1' % name] = reduction[:, 0]\n    train_df['%s_c2' % name] = reduction[:, 1]\n    train_df['%s_c3' % name] = reduction[:, 2]\n    \n    test_df['%s_c1' % name] = test_reduction[:, 0]\n    test_df['%s_c2' % name] = test_reduction[:, 1]\n    test_df['%s_c3' % name] = test_reduction[:, 2]","5b0f9084":"cmap = plt.get_cmap('tab10', 4)\n\nfor method, name in zip([umap, pca, ica], ['umap', 'pca', 'ica']):\n    fig = plt.figure(figsize = (8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    p = ax.scatter(train_df['%s_c1' % name], train_df['%s_c2'  % name], train_df['%s_c3'  % name], c = train_df['label'].astype(int), cmap = cmap)\n    plt.title(f'{name.capitalize()}')\n    fig.colorbar(p, aspect = 4, ticks = [1, 2, 3, 4])","2ef84054":"submission, fi, scores = model_gbm(train_df.drop(columns = 'label'), train_labels, \n                                   test_df, test_ids, return_preds = False)\n\nsubmission.to_csv('dimension_reduction_nolabels.csv', index = False)","690cdfdd":"fi.sort_values('importance').dropna().tail(25)","358b9523":"# Try without giving labels"}}