{"cell_type":{"8a393717":"code","3d899477":"code","5a726ec7":"code","92c75978":"code","d42ce4cd":"code","1d0127f9":"code","b13d063d":"code","be0b8ba3":"code","83a0e11d":"code","a4ee8162":"code","eee4030c":"code","67cfe912":"code","1e85da2c":"code","d1d0ae4c":"code","0bd8931f":"code","675a8be0":"code","7bba7006":"code","66a424de":"code","0a43f34d":"code","4e656beb":"code","dad0f272":"code","c9ad51f6":"code","32514aea":"code","55ab4efc":"code","0015c9ea":"code","77390469":"code","8fd15cfb":"code","99480f64":"markdown","4e7815a4":"markdown","18a724f3":"markdown","c6fdb1a7":"markdown","8d70d3a2":"markdown","5292f40c":"markdown","34d38153":"markdown","1632aea0":"markdown","27beed4b":"markdown","4813f82b":"markdown","ac974539":"markdown","09892353":"markdown","4b954648":"markdown","04d41a46":"markdown"},"source":{"8a393717":"%matplotlib inline\n#\u9019\u662fjupyter notebook\u7684magic word\u02d9\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display","3d899477":"import os\n#\u5224\u65b7\u662f\u5426\u5728jupyter notebook\u4e0a\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#\u5224\u65b7\u662f\u5426\u5728colab\u4e0a\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#\u5224\u65b7\u662f\u5426\u5728kaggke_kernal\u4e0a\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('\/content\/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = '.\/trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '\/content\/gdrive\/My Drive\/trident'","5a726ec7":"#\u70ba\u78ba\u4fdd\u5b89\u88dd\u6700\u65b0\u7248 \n\n!pip uninstall tridentx -y\n!pip install ..\/input\/trident\/tridentx-0.7.3.22-py3-none-any.whl --upgrade\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport numpy as np\n#\u8abf\u7528trident api\n\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\nimport trident as T\nfrom trident import *\nfrom trident.models import rfbnet","92c75978":"class_names=['\u672a\u914d\u6234\u53e3\u7f69','\u6b63\u78ba\u914d\u6234\u53e3\u7f69','\u672a\u6b63\u78ba\u914d\u6234\u53e3\u7f69','\u914d\u6234\u7121\u9632\u8b77\u80fd\u529b\u53e3\u7f69']\npalette=[ (0,255,255),(255,255,128),(255,0,255), (0, 255, 0)]\nimgs=glob.glob('..\/input\/maskedfacesdetection\/imgs\/*.*g')\npath_dict={}\nfor im_path in imgs:\n    folder,file,ext=split_path(im_path)\n    path_dict[file]=im_path","d42ce4cd":"annos=OrderedDict()\njsons=glob.glob('..\/input\/maskedfacesdetection\/tags\/*.json')\nimages=[]\nboxes=[]\nif os.path.exists('..\/input\/masked-face-detection\/annos.pkl'):\n    annos=unpickle('..\/input\/masked-face-detection\/annos.pkl')\n    images=annos['images']\n    boxes=annos['boxes']\nelse:\n\n    for j in tqdm(range(len(jsons))) :\n        j_item=jsons[j]\n\n        try:\n\n            folder,file,ext=split_path(j_item)\n            box_collection = []\n            label_collection=[]\n            if file in path_dict and os.path.exists(path_dict[file]):\n                img_path=path_dict[file]\n\n                f = open(j_item, 'r',encoding='utf-8-sig')\n                im=image2array(img_path)\n                tagging = json.load(f)\n                w=im.shape[1]\n                h = im.shape[0]\n                if len(tagging['outputs']) > 0 and len(tagging['outputs']['object']) > 0:\n                    folder, file, ext = split_path(img_path)\n                    annos[file+ext] = []\n\n                    for  i in range(len(tagging['outputs']['object'])):\n                        tag=tagging['outputs']['object'][i]\n                        label=int(tag['name'])+1\n                        x1=max(tag['bndbox']['xmin'],0)\n                        y1=max(tag['bndbox']['ymin'],0)\n                        x2=min(tag['bndbox']['xmax'],w)\n                        y2=min(tag['bndbox']['ymax'],h)\n\n\n                        bbox=[x1,y1,x2,y2,label]\n\n                        box_collection.append(bbox)\n\n                box_collection=np.array(box_collection, dtype=np.float32)\n                boxes.append(box_collection)\n                images.append(img_path)\n\n        except :\n            print(j_item)\n    annos['images']=images\n    annos['boxes']=boxes\n    print('boxes',len(boxes),'images',len(images))\n#\u5099\u4efd\u9019\u6a23\u4e0b\u6b21\u5c31\u4e0d\u9700\u8981\u91cd\u65b0\u8b80\u53d6\npickle_it('.\/annos.pkl',annos)","1d0127f9":"print(boxes[:20])\nprint('')\nprint(images[:20])","b13d063d":"import string\n\n\nannos_widerfaces=OrderedDict()\nwider_images=[]\nwider_boxes=[]\nif os.path.exists('..\/input\/masked-face-detection\/annos_widerfaces.pkl'):\n    print('annos_widerfaces.pkl   exist')\n    annos_widerfaces=unpickle('..\/input\/masked-face-detection\/annos_widerfaces.pkl')\n    wider_images=annos_widerfaces['images']\n    wider_boxes=annos_widerfaces['boxes']\n#     for box in wider_boxes:\n#         box=box.astype(np.float32)\n#         box[:,4]=(box[:,2]-box[:,0]+1)\/(box[:,3]-box[:,1]+1).astype(np.float32)\n#         if (box[:,4]>5).sum()>0:\n#             print(box)\n#     print(wider_boxes)\nelse:\n\n    data=open('..\/input\/widerfaces\/wider_face_train_bbx_gt.txt',encoding='utf-8-sig').readlines()\n    print(len(data))\n\n\n\n\n    img_path = None\n    bboxes = None\n    num = None\n    skip = None\n    current_num = 0\n    im = None\n\n    is_start=False\n    for i in tqdm(range(len(data))):\n\n        line=data[i].strip()\n\n        if '--' in line :\n            is_start=True\n            img_path=os.path.join('..\/input\/widerfaces\/WIDER_train\/WIDER_train\/images\/',line)\n            im=image2array(img_path)\n            width= im.shape[1]\n            height= im.shape[0]\n\n            bboxes = []\n        elif line.isnumeric():\n            num=int(line)\n        else:\n            is_start=False\n            arr=line.split(' ')\n\n            box=np.array([float(item) for item in arr][:5]).astype(np.float32)\n            box[4]=1.0\n            bboxes.append(box)\n            if i==len(data)-1 or  '--' in data[i+1].strip():\n                wider_images.append(img_path)\n                bboxes=np.stack(bboxes,axis=0)\n                bboxes[:,2:4]=bboxes[:,0:2]+bboxes[:,2:4]\n                #print(bboxes)\n                wider_boxes.append(bboxes)\n    annos_widerfaces['images']=wider_images\n    annos_widerfaces['boxes']=wider_boxes\n    print('boxes',len(wider_boxes),'images',len(wider_images))\n\n\n    #\u5099\u4efd\u9019\u6a23\u4e0b\u6b21\u5c31\u4e0d\u9700\u8981\u91cd\u65b0\u8b80\u53d6\npickle_it('.\/annos_widerfaces.pkl',annos_widerfaces)\n\n      \n","be0b8ba3":"\nfrom PIL import Image, ImageDraw\nimport builtins\n\n\ndef draw_sample(annotations,idx=0):\n    img_path=annotations['images'][idx]\n    boxes=annotations['boxes'][idx]\n\n    print(img_path)\n    #\u8b80\u53d6\u5716\u7247\n    pillow_img=array2image(image2array(img_path))\n    for box in boxes:\n        this_box = box[:4]\n\n        this_label =  int(box[4])\n\n        thiscolor=palette[this_label-1]\n        pillow_img=plot_bbox(this_box, pillow_img, thiscolor,'', line_thickness=2)\n\n    draw = ImageDraw.Draw(pillow_img)\n\n    return pillow_img.resize((pillow_img.width*2,pillow_img.height*2))\n\n","83a0e11d":"draw_sample(annos,random.choice(list(range(len(images)))))","a4ee8162":"draw_sample(annos_widerfaces,random.choice(list(range(len(wider_images)))))","eee4030c":"\ndef intersect(box_a, box_b):\n    \"\"\" We Resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    \"\"\"\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:4].unsqueeze(1).expand(A, B, 2), box_b[:, 2:4].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, 0:2].unsqueeze(1).expand(A, B, 2), box_b[:, 0:2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \u2229 B \/ A \u222a B = A \u2229 B \/ (area(A) + area(B) - A \u2229 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter,union  # [A,B]\n\n\n\ndef match(truths, priors, variances, labels, keypoints=None,threshold=0.3):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w\/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w\/ matched indices for conf preds.\n        idx: (int) current batch index\n        threshold: (float) The overlap threshold used when mathing boxes.\n\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    \"\"\"\n    # jaccard index\n    priors2 = priors.clone()\n    num_priors = len(priors)\n    overlaps = jaccard(truths, xywh2xyxy(priors.clone()))\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n\n    # ignore hard gt\n    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n    if best_prior_idx_filter.shape[0] <= 0:\n        return np.zeros((num_priors, 4)).astype(np.float32), np.zeros((num_priors,1)).astype(np.int64),np.zeros((num_priors, 16)).astype(np.float32)\n\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_idx_filter.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]  # Shape: [num_priors,14]\n    conf = labels[best_truth_idx]  # Shape: [num_priors]\n    keypoints=keypoints[best_truth_idx]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n\n    loc = encode(matches, priors2, variances)\n    return loc, conf,keypoints\n\n\n\ndef encode(matched, priors, variances):\n    \"\"\"\n\n    Args:\n        matched (tensor): Coords of ground truth for each prior in xyxy Shape: [num_priors, 4].\n        priors (tensor): Prior boxes in center-offset form Shape: [num_priors,4].\n        variances (list[float]):  Variances of priorboxes\n\n    Returns:\n        encoded boxes and landmarks (tensor), Shape: [num_priors, 14]\n\n    \"\"\"\n\n    # dist b\/t match center and prior's center\n    priors = priors.clone()\n    g_cxcy = (matched[:, 0:2] + matched[:, 2:4]) \/ 2 - priors[:, 0:2]\n    # encode variance\n    g_cxcy \/= (variances[0] * priors[:, 2:4])\n    # match wh \/ prior wh\n    g_wh = (matched[:, 2:4] - matched[:, 0:2]) \/ priors[:, 2:4]\n    g_wh = torch.log(g_wh) \/ variances[1]\n\n    # # landmarks\n    # g_xy1 = (matched[:, 4:6] - priors[:, 0:2]) \/ (variances[0] * priors[:, 2:4])\n    # g_xy2 = (matched[:, 6:8] - priors[:, 0:2]) \/ (variances[0] * priors[:, 2:4])\n    # g_xy3 = (matched[:, 8:10] - priors[:, 0:2]) \/ (variances[0] * priors[:, 2:4])\n    # g_xy4 = (matched[:, 10:12] - priors[:, 0:2]) \/ (variances[0] * priors[:, 2:4])\n    # g_xy5 = (matched[:, 12:14] - priors[:, 0:2]) \/ (variances[0] * priors[:, 2:4])\n\n    # return target for loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,14]\n\n\ndef decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Adapted from https:\/\/github.com\/Hakuyume\/chainer-ssd\n\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n\n    Return:\n        decoded bounding box predictions\n\n    \"\"\"\n    boxes = torch.cat([priors.unsqueeze(0)[:, :, 0:2] + loc[:, :, 0:2] * variances[0] * priors.unsqueeze(0)[:, :, 2:4],\n                       priors.unsqueeze(0)[:, :, 2:4] * torch.exp(loc[:, :, 2:4] * variances[1])], -1)\n    boxes[:, :, 0:2] -= boxes[:, :, 2:4] \/ 2\n    boxes[:, :, 2:4] += boxes[:, :, 0:2]\n    return boxes\n\n\nnp.set_printoptions(precision=3)\n#\u78ba\u8a8d\u8f38\u5165\u7684\u9ede\u662f\u5426\u6709\u5728\u6307\u5b9a\u7684\u908a\u754c\u6846\u4e2d\u3002\ndef point_in_box(point,box):\n    if ndim(box)+1==ndim(point):\n        box=np.expand_dims(box,1)\n    if ndim(point)==ndim(box)==1:\n        x,y=point\n        x1,y1,x2,y2=box\n        if x1<x<x2 and y1<y<y2:\n            return True\n        else:\n            return False\n    elif  ndim(point)==ndim(box)==2:\n        x=point[:,0]\n        y=point[:,1]\n        x1=box[:,0]\n        y1=box[:,1]\n        x2=box[:,2]\n        y2=box[:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)\n    elif  ndim(point)==ndim(box)==3:\n        x=point[:,:,0]\n        y=point[:,:,1]\n        x1=box[:,:,0]\n        y1=box[:,:,1]\n        x2=box[:,:,2]\n        y2=box[:,:,3]\n        return np.greater_equal(x,x1)*np.greater_equal(x2,x)*np.greater_equal(y,y1)*np.greater_equal(y2,y)","67cfe912":"#\u8f09\u5165\u6a21\u578b\u7d50\u69cb(\u9019\u662f\u662f\u6253\u7b97\u5e36\u5927\u5bb6\u5f9e\u65b0\u8a13\u7df4\u800c\u975e\u57fa\u65bc\u65bc\u8a13\u7df4\u6a21\u578b)\nnew_rfbmodel=rfbnet.RfbNet(pretrianed=False,num_classes=5, num_regressors=4)\n#\u5c07\u6240\u6709\u6b0a\u91cd\u6307\u5b9a\u70ba\u53ef\u8a13\u7df4\nnew_rfbmodel.model.trainable=True\n#\u6aa2\u8996\u6a21\u578b\u7d50\u69cb\nnew_rfbmodel.summary()","1e85da2c":"\nclass SsdBboxDataset(BboxDataset):\n    def __init__(self, boxes=None, image_size=(480, 640), priors=None, center_variance=0.1, size_variance=0.2,\n                 gt_overlap_tolerance=0.5, object_type=ObjectType.absolute_bbox, class_names=None,\n                 symbol='bbox', name=''):\n        super().__init__(boxes=boxes, image_size=image_size, object_type=object_type, class_names=class_names,\n                         symbol=symbol, name=name)\n        self._element_spec = TensorSpec(shape=TensorShape([None, 21]), name=self.symbol, object_type=self.object_type, is_spatial=True)\n        self.priors = priors\n        self.image_size = image_size\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.label_transform_funcs = []\n        self.gt_overlap_tolerance = gt_overlap_tolerance\n        self.bbox_post_transform_funcs = []\n\n    def __getitem__(self, index: int):\n\n        # \u53d6\u51faxyxy\u5b9a\u7fa9boxes\u6216\u662f[]\n        # \u70ba\u4e86\u907f\u514d[]\u6e05\u55ae\u8207np.ndarray\u8a08\u7b97\u5dee\u7570\uff0c\u56e0\u6b64\u88dc\u4e0anp.array([[-1,-1,-1,-1,-1]])\u66ff\u4ee3\n        if self.items[index] is None or self.items[index] == []:\n            return np.expand_dims(np.ones((5))*-1,0)\n        else:\n            return self.items[index].astype(np.float32)\n\n    def area_of(self, left_top, right_bottom):\n        \"\"\"Compute the areas of rectangles given two corners.\n\n        Args:\n            left_top (N, 2): left top corner.\n            right_bottom (N, 2): right bottom corner.\n\n        Returns:\n            area (N): return the area.\n        \"\"\"\n        hw = np.clip(right_bottom - left_top, 0.0, None)\n        return hw[..., 0] * hw[..., 1]\n\n    def iou_of(self, boxes0, boxes1, eps=1e-5):\n        \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n        Args:\n            boxes0 (N, 4): ground truth boxes.\n            boxes1 (N or 1, 4): predicted boxes.\n            eps: a small number to avoid 0 as denominator.\n        Returns:\n            iou (N): IoU values.\n        \"\"\"\n        overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n        overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n        overlap_area = self.area_of(overlap_left_top, overlap_right_bottom)\n        area0 = self.area_of(boxes0[..., :2], boxes0[..., 2:])\n        area1 = self.area_of(boxes1[..., :2], boxes1[..., 2:])\n        return overlap_area \/ (area0 + area1 - overlap_area + eps)\n\n    def convert_boxes_to_locations(self, center_form_boxes, center_form_priors):\n        if len(center_form_priors.shape) + 1 == len(center_form_boxes.shape):\n            center_form_priors = np.expand_dims(center_form_priors, 0)\n        return np.concatenate([(center_form_boxes[..., :2] - center_form_priors[..., :2]) \/ center_form_priors[..., 2:] \/ self.center_variance,\n                               np.log(np.clip(center_form_boxes[..., 2:] \/ center_form_priors[..., 2:], 1e-8, 640)) \/ self.size_variance],\n                              axis=len(center_form_boxes.shape) - 1)\n\n    def assign_priors(self, gt_boxes, gt_labels, center_form_priors, iou_threshold):\n   \n        corner_form_priors = xywh2xyxy(center_form_priors)\n        ious = self.iou_of(np.expand_dims(gt_boxes, 0), np.expand_dims(corner_form_priors, 1))\n\n        \n        # size: num_priors\n        best_target_per_prior, best_target_per_prior_index = np.max(ious, axis=1), np.argmax(ious, axis=1)\n        #best_target_per_prior_index[best_target_per_prior == 0] = -1\n\n        best_prior_per_target, best_prior_per_target_index = np.max(ious, axis=0), np.argmax(ious, axis=0)\n    \n\n\n        for target_index, prior_index in enumerate(best_prior_per_target_index):\n            best_target_per_prior_index[prior_index] = target_index\n            overlaps = best_target_per_prior[best_target_per_prior_index == target_index]\n            overlaps.sort()\n            overlaps = overlaps[::-1]\n            threshould=overlaps[1 if len(overlaps)>2 else len(overlaps)-1]\n            mask1=((best_target_per_prior_index == target_index)*(best_target_per_prior<threshould)).astype(np.bool)\n            best_target_per_prior[best_prior_per_target_index[target_index]] = 2\n            best_target_per_prior[mask1]=0\n            best_target_per_prior_index[mask1]=-1\n\n        # 2.0 is used to make sure every target has a prior assigned\n        best_prior_per_target_index_list = best_prior_per_target_index.tolist()\n        for i in range(best_target_per_prior.shape[0]):\n            if i in best_prior_per_target_index_list:\n                best_target_per_prior[i] = 2\n\n        #labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n\n       \n        labels= gt_labels[best_target_per_prior_index]\n        labels[best_target_per_prior==0]=0\n        \n        boxes = gt_boxes[best_target_per_prior_index]\n        boxes[best_target_per_prior==0]=0\n        # boxes = encode(boxes, corner_form_priors, (0.1,0.2))\n        \n        \n        return boxes, labels\n\n    def data_transform(self, data):\n        if data is None or len(data) == 0:\n         \n            return np.zeros((self.priors.shape[0], 5)).astype(np.float32)\n        elif isinstance(data, np.ndarray):\n            height, width = self.image_size\n            #print('data traznsform data',data.shape,data)\n\n            data[:, 0] = np.clip(data[:, 0], 0, width)\n            data[:, 2] = np.clip(data[:, 2], 0, width)\n            data[:, 1] = np.clip(data[:, 1], 0, height)\n            data[:, 3] = np.clip(data[:, 3], 0, height)\n            \n            small_box_mask1=np.round(data[:, 0])==np.round(data[:, 2],0)\n            data[:, 2:3][small_box_mask1,:]+=1\n            small_box_mask2=np.round(data[:, 1])==np.round(data[:, 3],0)\n            data[:, 3:4][small_box_mask2,:]+=1\n            \n            box_w = np.expand_dims(data[:, 2] - data[:, 0], -1)\n            box_h = np.expand_dims(data[:, 3] - data[:, 1], -1)\n            box_left = np.expand_dims(data[:, 0].copy(), -1)\n            box_top = np.expand_dims(data[:, 1].copy(), -1)\n            # print('box_w',box_w.shape)\n            # print('box_left',box_left.shape)\n            \n            gt_box = data[:, :4]\n            \n            gt_label = data[:, 4:5]\n           \n            #if len([n for n in gt_label.reshape(-1) if n>1])>0:\n            #    print(data)\n            \n            \n\n            \n\n            gt_box[:, 0] = gt_box[:, 0] \/ width\n            gt_box[:, 2] = gt_box[:, 2] \/ width\n            gt_box[:, 1] = gt_box[:, 1] \/ height\n            gt_box[:, 3] = gt_box[:, 3] \/ height\n\n        \n            if gt_box is not None and len(gt_box) > 0:\n                truths = to_tensor(gt_box).float()\n                labels = to_tensor(gt_label).long()\n                \n\n                boxes, confidences= self.assign_priors(gt_box, gt_label, to_numpy(self.priors), 0.5)\n                boxes = xyxy2xywh(boxes)\n                locations = self.convert_boxes_to_locations(boxes, to_numpy(self.priors))\n\n                locations = np.concatenate([to_numpy(locations).astype(np.float32), to_numpy(confidences).astype(np.float32)], axis=-1)\n                return to_numpy(locations).astype(np.float32)\n            \n            num_priors = self.priors.shape[0]\n            return np.zeros((num_priors, 5)).astype(np.float32)\n\n\n    def bbox_transform(self, bbox):\n        return self.data_transform(bbox)\n","d1d0ae4c":"def draw_dataprovider(img_data,label_data,priors):\n    \n    boxes=to_tensor(label_data[:,:,:4])\n    keypoints=label_data[:,:,4:-1]\n    target_confidences=to_tensor(label_data[:,:,-1]).long()\n    #print(boxes.shape)\n    #print(keypoints.shape)\n    #print(target_confidence.shape)\n    confidences=to_numpy(make_onehot(target_confidences,5,-1).float())\n    #print(confidence.shape)\n    gc.collect()\n    locations = decode(boxes,priors, (0.1,0.2))\n    for i in range(8):\n        pillow_img=array2image(reverse_image_backend_adaption(img_data[i]*127.5+127.5))\n        \n        mask=np.argmax(confidences[i],-1)>0\n        \n        this_locations = to_numpy(locations[i][mask, :])\n\n        print(this_locations.shape)\n       \n        if len(this_locations)>0:\n            this_locations[:, 0] = this_locations[:, 0]*640\n            this_locations[:, 2] = this_locations[:, 2] *640\n            this_locations[:, 1] = this_locations[:, 1] *480\n            this_locations[:, 3] = this_locations[:, 3] *480\n\n            this_confidence=confidences[i][mask,:]\n            this_keypoints=keypoints[i][mask,:]\n            print(np.round(this_locations.copy()).astype(np.int64),target_confidences[i][mask])\n            out,keep=np.unique(np.round(this_locations.copy()), axis=0, return_index=True)\n            #print(out,idx)\n\n            \n            draw = ImageDraw.Draw(pillow_img)\n\n            box_probs = np.concatenate([this_locations.astype(np.float32), np.argmax(this_confidence,-1).reshape(-1, 1).astype(np.float32), np.max(this_confidence,-1).reshape(-1, 1).astype(np.float32)], axis=1)\n            #print(box_probs.shape)\n            if len(boxes) > 2:\n                \n                #box_probs, keep = rfbnet.model.hard_nms(box_probs, nms_threshold=0.9, top_k=-1, )\n                this_locations=this_locations[keep,:]\n                this_confidence=this_confidence[keep,:]\n            \n\n            #print(len(this_locations),len(this_keypoints),len(this_confidence))\n            for box,conf in zip(this_locations,this_confidence):\n                #print([int(v) for v in box],['{0:.2f}'.format(v) for v in conf],[v for v in kps.reshape((-1,2))])\n                #print(box)\n               \n                pillow_img =plot_bbox(box, pillow_img, palette[np.argmax(conf[1:])] ,  '', line_thickness=2)\n             \n\n\n        display.display(pillow_img)\n            #pillow_img.save('data_provider_next_{0}.jpg'.format(get_time_suffix()))","0bd8931f":"from trident.data.transform import Transform,VisionTransform\nfrom typing import Sequence, Tuple, Dict, Union, Optional\nfrom collections import Iterable\nclass ImageMosaic(VisionTransform):\n    \"\"\"Perform gamma correction on an image.\n        Also known as Power Law Transform. Intensities in RGB mode are adjusted\n        based on the following equation:\n        I_out = 255 * gain * ((I_in \/ 255) ** gamma)\n        See https:\/\/en.wikipedia.org\/wiki\/Gamma_correction for more details.\n    Args:\n        gamma (float): Non negative real number. gamma larger than 1 make the\n            shadows darker, while gamma smaller than 1 make dark regions\n            lighter.\n        gain (float): The constant multiplier.\n    \"\"\"\n\n    def __init__(self, output_size=None, keep_prob=0.7, name='image_mosaic', **kwargs):\n        super().__init__(name)\n        self.output_size = output_size\n        self.keep_prob = keep_prob\n        self.memory_cache = []\n\n    def apply(self, input: Tuple, spec: TensorSpec):\n        return super().apply(input, spec)\n\n    def _apply_image(self, image, spec: TensorSpec):\n        image = unpack_singleton(image)\n\n        if self._shape_info is None:\n            self._shape_info = self._get_shape(image)\n        keep, other_idxes, center, img1_offsetx, img1_offsety, img2_offsetx, img2_offsety, img3_offsetx, img3_offsety = self._shape_info\n        if keep:\n            return image\n        height, width = self.output_size\n        p1 = self.memory_cache[other_idxes[0]][spec].copy()[img1_offsety:img1_offsety + center[0], img1_offsetx:img1_offsetx + width - center[1], :]\n        p2 = self.memory_cache[other_idxes[1]][spec].copy()[img2_offsety:img2_offsety + height - center[0], img2_offsetx:img2_offsetx + center[1], :]\n        p3 = self.memory_cache[other_idxes[2]][spec].copy()[img3_offsety:img3_offsety + height - center[0], img3_offsetx:img3_offsetx + width - center[1], :]\n        # print('p2',p2.shape)\n        # print('p3',p3.shape)\n\n        base_img = image.copy()\n        base_img[:p1.shape[0], center[1]:center[1] + p1.shape[1], :] = p1\n        base_img[center[0]:center[0] + p2.shape[0], :p2.shape[1], :] = p2\n        base_img[center[0]:center[0] + p3.shape[0], center[1]:center[1] + p3.shape[1]:] = p3\n\n        return clip(base_img, 0, 255).astype(np.float32)\n\n    def _apply_coords(self, coords, spec: TensorSpec):\n        return coords\n\n    def _apply_boxes(self, boxes, spec: TensorSpec):\n        keep, other_idxes, center, img1_offsetx, img1_offsety, img2_offsetx, img2_offsety, img3_offsetx, img3_offsety = self._shape_info\n        if keep:\n            return boxes\n        height, width = self.output_size\n        concate_list_all=[]\n        box0 = boxes.copy() if boxes is not None else None\n        if box0 is None or len(box0)==0:\n            pass\n        else:\n            location0 = box0[:, :4].reshape(box0.shape[0], 2, 2)\n            location0[:, :, 0] = np.clip(location0[:, :, 0], 0, center[1])\n            location0[:, :, 1] = np.clip(location0[:, :, 1], 0, center[0])\n\n            class_info0 = box0[:, 4:5] if box0.shape[-1] > 4 else None\n            keypoints0 = box0[:, 5:] if box0.shape[-1] > 5 else None\n            if box0.shape[-1] > 5:\n                keypoints0 = box0[:, 5:].reshape(box0.shape[0], -1, 2)\n                invisible_keypoints0 = np.less_equal(keypoints0, 0).astype(np.bool)\n                keypoints0[:, :, 0] = np.clip(keypoints0[:, :, 0], 0, center[1])\n                keypoints0[:, :, 1] = np.clip(keypoints0[:, :, 1], 0, center[0])\n                keypoints0[invisible_keypoints0] = -1\n            else:\n                pass\n            concate_list0=None\n            if keypoints0 is None:\n                #print('concate_list0',location0.reshape((box0.shape[0], 4)).shape,class_info0.shape)\n                concate_list0=[location0.reshape((box0.shape[0], 4)), class_info0]\n            else:\n                concate_list0=[location0.reshape((box0.shape[0], 4)), class_info0, keypoints0.reshape((box0.shape[0], -1))] \n\n            box0 = np.concatenate(concate_list0, axis=-1)\n            concate_list_all.append(box0)\n\n        \n        \n        \n        \n        box1 = self.memory_cache[other_idxes[0]][spec].copy() if  self.memory_cache[other_idxes[0]][spec] is not None else None\n        if box1 is None or len(box1)==0:\n            pass\n        else:\n            location1 = box1[:, :4].reshape(box1.shape[0], 2, 2)\n            location1[:, :, 0] = np.clip(location1[:, :, 0] - img1_offsetx + center[1], center[1], width)\n            location1[:, :, 1] = np.clip(location1[:, :, 1] - img1_offsety, 0, center[0])\n\n            class_info1 = box1[:, 4:5] if box1.shape[-1] > 4 else None\n            keypoints1 =None\n            if box1.shape[-1] > 5:\n                keypoints1 = box1[:, 5:].reshape(box1.shape[0], -1, 2)\n                invisible_keypoints1 = np.less_equal(keypoints1, 0).astype(np.bool)\n                keypoints1[:, :, 0] = np.clip(keypoints1[:, :, 0] - img1_offsetx + center[1], center[1], width)\n                keypoints1[:, :, 1] = np.clip(keypoints1[:, :, 1] - img1_offsety, 0, center[0])\n                keypoints1[invisible_keypoints1] = -1\n\n            else:\n                pass\n            concate_list1=None\n            if keypoints1 is None:\n                #print('concate_list1',location1.reshape((box1.shape[0], 4)).shape,class_info1.shape)\n                concate_list1=[location1.reshape((box1.shape[0], 4)), class_info1]\n            else:\n                concate_list1=[location1.reshape((box1.shape[0], 4)), class_info1, keypoints1.reshape((box1.shape[0], -1))]\n            box1 = np.concatenate(concate_list1, axis=-1)\n            concate_list_all.append(box1)\n\n        box2 = self.memory_cache[other_idxes[1]][spec].copy() if  self.memory_cache[other_idxes[1]][spec] is not None else None\n        if box2 is None or len(box2)==0:\n            pass\n        else:\n            location2 = box2[:, :4].reshape(box2.shape[0], 2, 2)\n            location2[:, :, 0] = np.clip(location2[:, :, 0] - img2_offsetx, 0, center[1])\n            location2[:, :, 1] = np.clip(location2[:, :, 1] - img2_offsety + center[0], center[0], height)\n\n            class_info2 = box2[:, 4:5] if box2.shape[-1] > 4 else None\n            keypoints2 = None\n            if box2.shape[-1] > 5:\n                keypoints2 = box2[:, 5:].reshape(box2.shape[0], -1, 2)\n                invisible_keypoints2 = np.less_equal(keypoints2, 0).astype(np.bool)\n                keypoints2[:, :, 0] = np.clip(keypoints2[:, :, 0] - img2_offsetx, 0, center[1])\n                keypoints2[:, :, 1] = np.clip(keypoints2[:, :, 1] - img2_offsety + center[0], center[0], height)\n                keypoints2[invisible_keypoints2] = -1\n\n\n            else:\n                pass\n            concate_list2=None\n            if keypoints2 is None:\n                #print('concate_list2',location2.reshape((box2.shape[0], 4)).shape,class_info2.shape)\n                concate_list2=[location2.reshape((box2.shape[0], 4)), class_info2]\n            else:\n                concate_list2=[location2.reshape((box2.shape[0], 4)), class_info2, keypoints2.reshape((box2.shape[0], -1))]\n\n\n            box2 = np.concatenate(concate_list2, axis=-1)\n            concate_list_all.append(box2)\n\n        box3 =self.memory_cache[other_idxes[2]][spec].copy() if  self.memory_cache[other_idxes[2]][spec] is not None else None\n        if box3 is None or len(box3)==0:\n            pass\n        else:\n            #print('box3',box3)\n            location3 = box3[:, :4].reshape(box3.shape[0], 2, 2)\n            location3[:, :, 0] = np.clip(location3[:, :, 0] - img3_offsetx + center[1], center[1], width)\n            location3[:, :, 1] = np.clip(location3[:, :, 1] - img3_offsety + center[0], center[0], height)\n\n            class_info3 = box3[:, 4:5] if box3.shape[-1] > 4 else None\n            keypoints3 =None\n            if box3.shape[-1] > 5:\n                keypoints3 = box3[:, 5:].reshape(box3.shape[0], -1, 2)\n                invisible_keypoints3 = np.less_equal(keypoints3, 0).astype(np.bool)\n                keypoints3[:, :, 0] = np.clip(keypoints3[:, :, 0] - img3_offsetx + center[1], center[1], width)\n                keypoints3[:, :, 1] = np.clip(keypoints3[:, :, 1] - img3_offsety + center[0], center[0], height)\n                keypoints3[invisible_keypoints3] = -1\n\n\n            else:\n                pass\n            concate_list3=None\n            if keypoints3 is None:\n                #print('concate_list3',location3.reshape((box3.shape[0], 4)).shape,class_info3.shape)\n                concate_list3=[location3.reshape((box3.shape[0], 4)), class_info3]\n            else:\n                concate_list3=[location3.reshape((box3.shape[0], 4)), class_info3, keypoints3.reshape((box3.shape[0], -1))]\n            box3 = np.concatenate(concate_list3, axis=-1)\n            concate_list_all.append(box3)\n\n        trans_boxes = np.concatenate(concate_list_all, axis=0)\n\n        hw = clip(trans_boxes[:, :4][..., 2:] - trans_boxes[:, :4][..., :2], 0.0, None)\n        area = hw[..., 0] * hw[..., 1]\n        area_mask = area >= 1\n        trans_boxes = trans_boxes[area_mask, :]\n\n        return trans_boxes\n\n    def _apply_mask(self, mask, spec: TensorSpec):\n        keep, other_idxes, center, img1_offsetx, img1_offsety, img2_offsetx, img2_offsety, img3_offsetx, img3_offsety = self._shape_info\n        height, width = self.output_size\n        if keep:\n            return mask\n        mp1 = self.memory_cache[other_idxes[0]][spec].copy()[img1_offsety:img1_offsety + center[0], img1_offsetx:img1_offsetx + width - center[1], :]\n        mp2 = self.memory_cache[other_idxes[1]][spec].copy()[img2_offsety:img2_offsety + height - center[0], img2_offsetx:img2_offsetx + center[1], :]\n        mp3 = self.memory_cache[other_idxes[2]][spec].copy()[img3_offsety:img3_offsety + height - center[0], img3_offsetx:img3_offsetx + width - center[1], :]\n\n        base_msk = mask.copy()\n        base_msk[:mp1.shape[0], center[1]:center[1] + mp1.shape[1], :] = mp1\n        base_msk[center[0]:center[0] + mp2.shape[0], :mp2.shape[1], :] = mp2\n        base_msk[center[0]:center[0] + mp3.shape[0], center[1]:center[1] + mp3.shape[1]:] = mp3\n        return base_msk\n\n    def _get_shape(self, image):\n        if self.output_size is None:\n            self.output_size = image.shape[:2]\n        rr = random.random()\n        keep = rr < self.keep_prob or len(self.memory_cache[:-1]) <= 3\n        other_idxes = None\n        center = None\n        img1_offsetx = img1_offsety = img2_offsetx = img2_offsety = img3_offsetx = img3_offsety = 0\n        if not keep:\n            other_idxes = list(range(len(self.memory_cache[:-1])))\n            random.shuffle(other_idxes)\n            other_idxes = other_idxes[:3]\n\n            height, width = self.output_size\n            rs = np.random.uniform(0.5, 1.5, [2])  # random shift\n            center = (int(height * rs[0] \/ 2), int(width * rs[1] \/ 2))\n\n            img1 = self.memory_cache[other_idxes[0]].value_list[0]\n            img1_offsetx = img1.shape[1] if img1.shape[1] - 1 < (width - center[1]) else np.random.choice(np.arange(0, img1.shape[1] - (width - center[1]), 1))\n            img1_offsety = img1.shape[0] if img1.shape[0] - 1 < center[0] else np.random.choice(np.arange(0, img1.shape[0] - center[0], 1))\n\n            img2 = self.memory_cache[other_idxes[1]].value_list[0]\n            img2_offsetx = img2.shape[1] if img2.shape[1] - 1 < center[1] else np.random.choice(np.arange(0, img2.shape[1] - center[1], 1))\n            img2_offsety = img2.shape[0] if img2.shape[0] - 1 < (height - center[0]) else np.random.choice(np.arange(0, img2.shape[0] - (height - center[0]), 1))\n\n            img3 = self.memory_cache[other_idxes[2]].value_list[0]\n            img3_offsetx = img3.shape[1] if img3.shape[1] - 1 < (width - center[1]) else np.random.choice(np.arange(0, img3.shape[1] - (width - center[1]), 1))\n            img3_offsety = img3.shape[0] if img3.shape[0] - 1 < (height - center[0]) else np.random.choice(np.arange(0, img3.shape[0] - (height - center[0]), 1))\n\n        return (keep, other_idxes, center, img1_offsetx, img1_offsety, img2_offsetx, img2_offsety, img3_offsetx, img3_offsety)\n    \n    \n\n\n\nclass RandomGridMask(VisionTransform):\n    \"\"\"Perform gamma correction on an image.\n        Also known as Power Law Transform. Intensities in RGB mode are adjusted\n        based on the following equation:\n        I_out = 255 * gain * ((I_in \/ 255) ** gamma)\n        See https:\/\/en.wikipedia.org\/wiki\/Gamma_correction for more details.\n    Args:\n        gamma (float): Non negative real number. gamma larger than 1 make the\n            shadows darker, while gamma smaller than 1 make dark regions\n            lighter.\n        gain (float): The constant multiplier.\n    \"\"\"\n\n    def __init__(self, output_size=None, d1=None, d2=None, max_d1=None, rotate_range=(0.2, 1), ratio=0.2, mode=0, keep_prob=0.5, name='gridmask', **kwargs):\n        super().__init__(name)\n        self.output_size = output_size\n        self.d1 = d1\n        self.d2 = d2\n        self.max_d1 = max_d1\n        self.rotate_range = rotate_range\n        self.ratio = ratio\n        self.keep_prob = keep_prob\n        self.mode = mode\n        self.current_mask = None\n\n    def set_prob(self, epoch, max_epoch):\n        self.prob = self.keep_prob * min(1, epoch \/ max_epoch)\n\n    def get_grid(self, image):\n        h, w = image.shape[:2]\n        if self.output_size is None:\n            self.output_size = (h, w)\n        hh = math.ceil((math.sqrt(h * h + w * w)))\n        d2 = self.d2\n        d1 = None\n        if self.d2 is None:\n            d2 = minimum(h, w)\n        elif isinstance(self.d2, numbers.Number):\n            pass\n        elif isinstance(self.d2, (tuple, list)):\n            d2 = random.choice(list(self.d2))\n\n        if self.d1 is None:\n            divisors = get_divisors(minimum(h, w))\n\n            divisors = list(sorted(set(divisors)))\n            divisors = [d for d in divisors if d <= minimum(h, w) \/ 2.0]\n            if self.max_d1 is not None:\n                divisors = [d for d in divisors if d <= self.max_d1]\n            if minimum(h, w) in divisors:\n                divisors.remove(minimum(h, w))\n            d1 = random.choice(divisors)\n        elif isinstance(self.d1, (tuple, list)):\n            d1 = random.choice(list(self.d1))\n\n        d = np.random.randint(d1, d2)\n        # d = self.d\n\n        # maybe use ceil? but i guess no big difference\n        self.l = math.ceil(d * self.ratio)\n\n        mask = np.ones((hh, hh), np.float32)\n        st_h = np.random.randint(d)\n        st_w = np.random.randint(d)\n        for i in range(-1, hh \/\/ d + 1):\n            s = d * i + st_h\n            t = s + self.l\n            s = max(min(s, hh), 0)\n            t = max(min(t, hh), 0)\n            mask[s:t, :] *= 0\n        for i in range(-1, hh \/\/ d + 1):\n            s = d * i + st_w\n            t = s + self.l\n            s = max(min(s, hh), 0)\n            t = max(min(t, hh), 0)\n            mask[:, s:t] *= 0\n        r = np.random.randint(self.rotate_range[0], self.rotate_range[1], )\n        mask = Image.fromarray(np.uint8(mask))\n        mask = mask.rotate(r)\n        mask = np.asarray(mask)\n        mask = mask[(hh - h) \/\/ 2:(hh - h) \/\/ 2 + h, (hh - w) \/\/ 2:(hh - w) \/\/ 2 + w]\n\n        if self.mode == 1:\n            mask = 1 - mask\n        # \u66ab\u5b58mask\u4ee5\u8655\u7406bbox\n        self.current_mask = mask.copy()\n        image = image * np.expand_dims(mask, -1)\n\n        return image\n\n    def apply(self, input: Tuple, spec: TensorSpec):\n        return super().apply(input, spec)\n\n    def _apply_image(self, image, spec: TensorSpec):\n        if self._shape_info is None:\n            self._shape_info = self._get_shape(image)\n        h, w, rr, keep = self._shape_info\n        height, width = self.output_size\n        if keep:\n            return image\n        else:\n            return self.get_grid(image)\n\n    def _apply_boxes(self, boxes, spec: TensorSpec):\n        h, w, rr, keep = self._shape_info\n        if keep:\n            return boxes\n        elif boxes is None:\n            return boxes\n        else:\n            keep_boxes = []\n            location = boxes[:, :4].copy()\n            class_info = boxes[:, 4:5] if boxes.shape[-1] > 4 else None\n            keypoints = boxes[:, 5:] if boxes.shape[-1] > 5 else None\n            # \u5982\u679c\u525b\u597d\u683c\u7dda\u906e\u853d\u4e86\u5c0f\u6846\uff0c\u5247\u6392\u9664\u5c0f\u6846\u5b9a\u7fa9\n            boxes[:,2] = np.clip(boxes[:,2], 0, w)\n            boxes[:,3] = np.clip(boxes[:,3], 0, h)\n            for i in range(len(boxes)):\n                box=np.round(location[i]).astype(np.int32)\n                if self.current_mask[box[1]:box[3] + 1, box[0]:box[2] + 1].sum() == 0:\n                    pass\n                else:\n                    keep_boxes.append(boxes[i])\n            if len(keep_boxes)>1:\n                return np.stack(keep_boxes, 0)\n            elif len(keep_boxes)==1:\n                return np.array(keep_boxes)\n            else:\n                return None\n\n\n\n    def _apply_coords(self, coords, spec: TensorSpec):\n        h, w, rr, keep = self._shape_info\n        return coords\n\n    def _apply_mask(self, mask, spec: TensorSpec):\n        h, w, rr, keep = self._shape_info\n        height, width = self.output_size\n        return mask\n\n    def _get_shape(self, image):\n        h, w = image.shape[:2]\n\n        rr = random.random()\n        keep = rr < self.keep_prob\n        if self.output_size is None:\n            self.output_size = (h, w)\n        return h, w, rr, keep\n\n","675a8be0":"def draw_infer(img_data,label_data,priors):\n    \n    new_rfbmodel.model.train(False)\n    #print(traindata.keys())\n    imagedata=img_data.copy()\n    labeldata=label_data.copy()\n    confidences, locations=new_rfbmodel.model(to_tensor(img_data))\n    \n    print(exp(confidences).mean(),exp(confidences).max())\n    #print(softmax(traindata['confidences'].float(),-1))\n    #\u57fa\u65bc\u8a13\u7df4\u968e\u6bb5\u8f38\u51fa\u7e6a\u5716\uff0c\u4fe1\u5fc3\u6c34\u6e96\u61c9\u8a72\u8981\u53d6exp\n    \n    if (confidences<0).sum()>0:\n        confidences =to_numpy(exp(confidences).copy().float())\n    else:\n        confidences =to_numpy(confidences.copy().float())\n\n    #target_confidences=to_numpy(traindata['target_confidences'].copy())\n    boxes =to_tensor(locations[:,:,:4])\n\n    gc.collect()\n    #\u57fa\u65bc\u8a13\u7df4\u968e\u6bb5\u8f38\u51fa\u7e6a\u5716\uff0cboxes\u61c9\u8a72\u8981decode\n    locations = to_numpy(decode(boxes, priors, (0.1,0.2)))\n    for i in range(8):\n        pillow_img=array2image(reverse_image_backend_adaption(img_data[i]*127.5+127.5))\n        #probs=confidence[i].max().item()\n\n        #print('confidence',confidence[i])\n        #print('probs',probs)\n        print(confidences[i].shape)\n        mask=confidences[i][:,1:].max(-1)>=0.3\n        \n        print('mask.shape',mask.shape)\n        #print(confidence[i].shape)\n        #print(locations[i].shape)\n        this_locations = locations[i][mask, :]\n        \n        \n        \n        #print(this_locations.shape)\n        if len(this_locations)>0:\n            \n            this_locations[:, 0] = this_locations[:, 0] *640\n            this_locations[:, 2] = this_locations[:, 2] *640\n            this_locations[:, 1] = this_locations[:, 1] *480\n            this_locations[:, 3] = this_locations[:, 3] *480\n            this_confidence=confidences[i][mask]\n            \n#             aspect_ratio=(this_locations[:,2]-this_locations[:,0])\/(this_locations[:,3]-this_locations[:,1]).astype(np.float32)\n#             aspect_mask=((aspect_ratio<3.5)*(this_locations[:,3]-this_locations[:,1]<3)).astype(np.bool)\n#             print('aspect_mask',aspect_mask.shape)\n\n#             this_locations=this_locations[aspect_mask,:]\n#             this_confidence=this_confidence[aspect_mask,:]\n\n            \n\n\n            draw = ImageDraw.Draw(pillow_img)\n\n            box_probs = np.concatenate([this_locations.astype(np.float32), (np.expand_dims(np.argmax(this_confidence[:,1:],-1)+1,-1)).astype(np.float32), np.max(this_confidence,-1).reshape(-1, 1).astype(np.float32)], axis=1)\n            print('box_probs',box_probs.shape)\n            if len(boxes) > 1:\n                box_probs, keep = hard_nms(box_probs, 0.5, top_k=-1, )\n                print('box_probs',box_probs.shape)\n                this_locations=this_locations[keep,:]\n                this_confidence=this_confidence[keep,:]\n\n            #print(len(this_locations),len(this_keypoints),len(this_confidence))\n            if len(this_locations) > 0:\n                for box,conf in zip(this_locations,this_confidence):\n                    try:\n                        print('box',[int(v) for v in box],'aspect ratio',(box[2]-box[0])\/float(box[3]-box[1]),'conf',['{0:.3f}'.format(v) for v in conf])\n                        #print(box)\n\n                        pillow_img =plot_bbox(box, pillow_img, palette[np.argmax(conf[1:])] ,  '', line_thickness=2)\n                    except Exception as e:\n                        print(e)\n                        print('box',box,'conf',conf)\n\n\n\n        display.display(pillow_img)\n            #pillow_img.save('data_provider_next_{0}.jpg'.format(get_time_suffix()))\n","7bba7006":"small_faces_images=[]\nsmall_faces_boxes=[]\nfor img,box in zip(images,boxes):\n    if len(box)>10:\n        small_faces_images.append(img)\n        small_faces_boxes.append(box)\n\nprint(len(small_faces_images))\n\n\nds1=ImageDataset(wider_images+5*small_faces_images+20*images,symbol='images')\nds2=SsdBboxDataset(boxes=wider_boxes+5*small_faces_boxes+20*boxes,priors=new_rfbmodel.model.priors,symbol='boxes')\n\n\ndata_provider=DataProvider(traindata=Iterator(data=ds1,label=ds2))\ndata_provider.batch_transform_funcs=[ImageMosaic((480,640),keep_prob=0.4)]\ndata_provider.paired_transform_funcs=[\n    Resize((600,800)),\n    RandomRescaleCrop((480,640),scale_range=(0.5,1.5)),\n    RandomTransform(rotation_range=25,zoom_range=0.1, shift_range=0.1,  shear_range= 0.4,random_flip= 0.2),\n    RandomGridMask(size_range=(480,640),max_d1=64,rotation_range=(10,30),keep_prob=0.6)]\n#\u8a2d\u5b9a\u5f71\u50cf\u8f49\u63db\ndata_provider.image_transform_funcs=[\n    RandomAdjustGamma(scale=(0.5,1.5)),#\u8abf\u6574\u660e\u6697\n    RandomAdjustHue(scale=(-0.3,0.3)),#\u8abf\u6574\u8272\u76f8\n    RandomAdjustContrast(scale=(0.6,1.4)),#\u8abf\u6574\u5c0d\u6bd4\n    RandomAdjustSaturation(scale=(0.6,1.4)),#\u8abf\u6574\u98fd\u548c\u5ea6\n    RandomErasing(size_range=(0.05, 0.1), transparency_range=(0.4, 0.8), transparancy_ratio=1, keep_prob=0.5), #\u52a0\u5165\u96a8\u6a5f\u64e6\u53bb\n    RandomBlur(scale=(3,7)),#\u96a8\u6a5f\u6a21\u7cca\n    SaltPepperNoise(prob=0.002),#\u6912\u9e7d\u566a\u97f3\n    RandomErasing(size_range=(0.05, 0.1), transparency_range=(0.4, 0.8), transparancy_ratio=1, keep_prob=0.5), #\u52a0\u5165\u96a8\u6a5f\u64e6\u53bb\n    Normalize(127.5,127.5)] #\u6a19\u6e96\u5316","66a424de":"\n\nimg_data,label_data=data_provider.next()\nprint(img_data.shape)\nprint(label_data.shape)\n","0a43f34d":"draw_dataprovider(img_data,label_data,new_rfbmodel.model.priors)\n","4e656beb":"data_provider.preview_images()","dad0f272":"def get_actual_target(target_locations,target_confidences,priors,center_variance=0.1, size_variance=0.2):\n    relative_target_boxes = decode(target_locations[:,:,:4].copy(),priors, (center_variance,size_variance))\n    relative_target_boxes=to_numpy(relative_target_boxes)\n    target_boxes = decode(target_locations[:,:,:4].copy(),priors, (center_variance,size_variance))*to_tensor([[640,480,640,480]])\n    target_boxes=np.round(to_numpy(target_boxes),0)\n    target_confidences=to_numpy(target_confidences)\n    all_sorted_boxes=[]\n    all_target_confidences=[]\n    for i in range(len(target_boxes)):\n        this_target_boxes=target_boxes[i][target_confidences[i]>0,:]\n        this_relative_target_boxes=relative_target_boxes[i][target_confidences[i]>0,:]\n        arr_idx=this_target_boxes[:,0]*1000000+this_target_boxes[:,1]*10000+this_target_boxes[:,2]*100+this_target_boxes[:,3]\n        np.sort(arr_idx)\n        sorted_indexes=np.argsort(arr_idx)\n        \n        sorted_target_confidences=target_confidences[i][target_confidences[i]>0][sorted_indexes]\n        \n        sorted_target_boxes=this_target_boxes[sorted_indexes]\n        sorted_relative_target_boxes=this_relative_target_boxes[sorted_indexes]\n        \n        mask=np.ones(len(sorted_target_boxes))\n        for n in range(len(sorted_target_boxes)):\n            if n>0 and np.array_equal(sorted_target_boxes[n],sorted_target_boxes[n-1]):\n                mask[n]=0\n        mask=mask.astype(np.bool)\n        sorted_target_confidences=sorted_target_confidences[mask]\n        sorted_target_boxes=sorted_target_boxes[mask,:]\n        sorted_relative_target_boxes=sorted_relative_target_boxes[mask,:]\n        #print(sorted_target_confidences)\n        label=np.zeros((priors.shape[0]))\n        if len(sorted_target_boxes)>0:\n            corner_form_priors = xywh2xyxy(to_numpy(priors)).astype(np.float32)\n            \n            ious = ds2.iou_of(np.expand_dims(sorted_relative_target_boxes.astype(np.float32), 0), np.expand_dims(corner_form_priors, 1))\n            best_target_per_prior, best_target_per_prior_index = np.max(ious, axis=1), np.argmax(ious, axis=1)\n            best_prior_per_target, best_prior_per_target_index = np.max(ious, axis=0), np.argmax(ious, axis=0)\n            #print(best_prior_per_target_index)\n            #print(sorted_target_confidences)\n            \n            for n,p in enumerate(best_prior_per_target_index):\n                label[p]=sorted_target_confidences[n]\n            #print(label)\n            all_target_confidences.append(label )\n            all_sorted_boxes.append(sorted_target_boxes)\n        else:\n            all_target_confidences.append(label)\n            all_sorted_boxes.append(sorted_target_boxes)\n        \n        \n    return all_sorted_boxes,all_target_confidences","c9ad51f6":"import torch.nn.functional as F\ndef preprocess_data(training_context):\n    traindata=training_context['train_data']\n    boxes=traindata['boxes']\n\n    traindata['target_confidences']=boxes[:,:,-1].long().detach()\n    traindata['target_locations']=boxes[:,:,:-1].detach()    \n    \n    \ndef hard_negative_mining(loss, labels, neg_pos_ratio):\n    \"\"\"\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example\/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    \"\"\"\n    pos_mask = labels > 0\n    \n\n    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n    #\u907f\u514d\u6b64\u5716\u7247\u7121\u6b63\u6848\u4f8b\n    num_neg = clip(num_pos,min=1) * neg_pos_ratio\n\n    \n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=1,descending=True)\n    _, orders = indexes.sort(dim=1)\n    neg_mask = orders < num_neg\n\n    return pos_mask | neg_mask\n\ndef area_of(left_top, right_bottom):\n    \"\"\"Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    \"\"\"\n    hw = clip(right_bottom - left_top, 0.0, None)\n    return hw[..., 0] * hw[..., 1]\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    \"\"\"\n    overlap_left_top = maximum(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area =area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area \/ (area0 + area1 - overlap_area + eps),overlap_area ,area0 + area1 - overlap_area\n\ndef hard_nms(box_scores, nms_threshold, top_k=-1, candidate_size=200):\n    \"\"\"\n\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        nms_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    \"\"\"\n    if box_scores is None or len(box_scores) == 0:\n        return None, None\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :4]\n    picked = []\n    # _, indexes = scores.sort(descending=True)\n    indexes = np.argsort(-scores)\n    # indexes = indexes[:candidate_size]\n    indexes = indexes[-candidate_size:]\n    while len(indexes) > 0:\n        # current = indexes[0]\n        current = indexes[-1]\n        picked.append(current)\n        if 0 < top_k == len(picked) or len(indexes) == 1:\n            break\n        current_box = boxes[current, :]\n        # indexes = indexes[1:]\n        indexes = indexes[:-1]\n        rest_boxes = boxes[indexes, :]\n        iou, inter, union = iou_of(rest_boxes, expand_dims(current_box, axis=0), )\n        iot = inter \/ area_of(current_box[..., :2], current_box[..., 2:])\n\n        indexes = indexes[((iou <= nms_threshold) * (iot <= nms_threshold)).astype(np.bool)]\n    return box_scores[picked, :], picked\n\n\n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self, priors, neg_pos_ratio=3.5, center_variance=0.1, size_variance=0.2):\n        \"\"\"Implement SSD Multibox Loss.\n\n        Basically, Multibox loss combines classification loss\n         and Smooth L1 regression loss.\n        \"\"\"\n        super(MultiBoxLoss, self).__init__()\n        self.neg_pos_ratio = neg_pos_ratio\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.priors = to_tensor(priors)\n\n    def forward(self, confidences, locations, target_confidences, target_locations):\n        \"\"\"Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            locations (batch_size, num_priors, 4): predicted locations.\n            target_confidence (batch_size, num_priors): real labels of all the priors.\n            target_locations (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        \"\"\"\n        \n        num_classes = confidences.size(2)\n        batch= confidences.size(0)\n        boxes =locations[...,:4]\n\n        target_boxes =target_locations[...,:4]\n\n        pos_mask = target_confidences > 0\n       \n        \n        \n        with torch.no_grad():\n            loss =binary_cross_entropy(exp(confidences).gather(1,target_confidences.unsqueeze(-1)).squeeze(-1),1)\n            mask = hard_negative_mining(loss, target_confidences, self.neg_pos_ratio)\n        masked_confidences =confidences[mask, :].reshape(-1, num_classes)\n        masked_target_confidences = target_confidences[mask].reshape(-1).detach()\n        \n        \n        classification_loss = CrossEntropyLoss(axis=-1, label_smooth=True,reduction='sum')(masked_confidences,masked_target_confidences.detach())\\\n    \n        masked_boxes = locations[...,:4][pos_mask, :]\n        masked_target_boxes = target_locations[...,:4][pos_mask, :].detach()\n        \n        #\u8a08\u7b97\u6bd4\u5c0d\u5230\u7684\u6578\u91cf\n        num_match=len(masked_target_confidences)+1\n        num_pos=len(target_confidences[pos_mask])+1\n        #\u4e0d\u4e00\u5b9a\u6bcf\u500b\u908a\u754c\u6846\u90fd\u6709\u95dc\u9375\u9ede\uff0c\u800c\u4e14\u95dc\u9375\u9ede\u6709\u53ef\u80fd\u7121\u6548\uff0c\u6240\u4ee5\u62c6\u958b\u8a08\u7b97\n        if len(masked_target_boxes) > 0:\n            #\u908a\u754c\u6846\n            regression_loss=F.smooth_l1_loss(masked_boxes,masked_target_boxes.detach()).sum()\n        \n        return (regression_loss)\/num_pos+(classification_loss)\/num_match\n    \n    \n\n    \n# def diou_loss(confidences, locations, target_confidences, target_locations):\n#     loss=to_tensor(0.0)\n#     boxes1 =locations[:,:,:4]\n#     boxes1 = decode(boxes1, new_rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]])\n    \n#     boxes2 =target_locations[:,:,:4]\n#     boxes2 = decode(boxes2, new_rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]])\n#     for i in range(len(locations)):\n    \n#         mask1=reduce_max(clip(exp(confidences),1e-7,1-1e-7)[i,:,1:],-1)>=0.3\n#         mask2=target_confidences[i,:]>0\n \n#         this_boxes1=boxes1[i,:,:][mask1,:]\n#         this_boxes2=boxes2[i,:,:][mask2,:]\n#         #\u5176\u4e2d\u70ba\u4e86\u9f13\u52f5\u5728\u6b63\u78ba\u4f4d\u7f6e\u51fa\u6846\uff0c\u61c9\u6709\u6846\u4f4d\u7f6e\u7684diou_loss\u518d\u4e58\u4e0a(1+\u985e\u5225\u70ba\u96f6\u7684\u6a5f\u7387) \u985e\u5225\u70ba\u96f6\u6a5f\u7387\u8d8a\u4f4e\uff0c\u640d\u5931\u8d8a\u4f4e\n#         #\u70ba\u4e86\u6291\u5236\u4e0d\u8a72\u51fa\u6846\u7684\u5730\u65b9\uff0c\u4e0d\u8a72\u6709\u6846\u4f4d\u7f6ediou_loss\u518d\u4e58\u4e0a(1+(1-\u985e\u5225\u70ba\u96f6\u7684\u6a5f\u7387)) \u985e\u5225\u70ba\u96f6\u6a5f\u7387\u8d8a\u9ad8\uff0c\u640d\u5931\u8d8a\u4f4e\n        \n#         this_target_confidences=target_confidences[i,:][mask2].unsqueeze(-1)\n#         this_confidences=clip(exp(confidences),1e-7,1-1e-7)[i,:,:][mask2,:]\n        \n#         this_probs=this_confidences.gather(-1, this_target_confidences)\n#         #print(this_probs.shape)\n        \n#         this_probs=1+(1-this_probs)\n#         if len(this_boxes1)>0 and len(this_boxes2)>0:\n#             diou=expand_dims(reduce_max(bbox_diou(this_boxes1,this_boxes2),0),-1)\n#             #print('diou',diou.shape)\n#             #print('diou',diou)\n#             #print('this_boxes2',this_boxes2.shape)\n#             #print('this_boxes1',this_boxes1.shape)\n#             loss=loss+ (this_probs*(1-diou)).mean()#+binary_cross_entropy(this_confidences.gather(-1, this_target_confidences),1).mean()\n#     return loss\n\ndef diou_loss(confidences, locations, target_confidences, target_locations):\n    batch=len(confidences)\n    loss=to_tensor(0.0,requires_grad=True)\n    boxes1 =locations[:,:,:4]\n    boxes1 = decode(boxes1, new_rfbmodel.model.priors, (0.1,0.2))#*to_tensor([[[640,480,640,480]]])\n    \n    boxes1=stack([clip(boxes1[:,:,0],0,640),clip(boxes1[:,:,1],0,480),clip(boxes1[:,:,2],0,640),clip(boxes1[:,:,3],0,480)],axis=-1)\n  \n    \n    \n    boxes2 =target_locations[:,:,:4]\n    #print('boxes2',boxes2)\n    boxes2 = decode(boxes2, new_rfbmodel.model.priors, (0.1,0.2))\n    #print('boxes2',boxes2)\n    #boxes2 = boxes2*to_tensor([[[640,480,640,480]]])\n    #print('boxes2',boxes2)\n    \n    #confidences=clip(exp(confidences),1e-7,1-1e-7)\n    pos_mask = target_confidences.reshape((-1)) > 0\n       \n    #\u8a08\u7b97\u6bd4\u5c0d\u5230\u7684\u6578\u91cf\n    num_match=len(target_confidences.reshape((-1))[pos_mask])\n    num_pred=0\n    \n    for i in range(batch):\n        boxes1_mask=(((boxes1[i,:,2]-boxes1[i,:,0])>=1)*((boxes1[i,:,3]-boxes1[i,:,1])>=1)).bool()\n        mask1=(((exp(confidences)[i][:,1:].max(-1)>=0.3)+(target_confidences[i,:]>0)>0)*boxes1_mask).bool()\n        \n        #boxes2_mask=(((boxes2[i,:,2]-boxes2[i,:,0])>=1)*((boxes2[i,:,3]-boxes2[i,:,1])>=1)).bool()\n        #print('boxes2_mask',boxes2_mask.shape)\n        mask2=target_confidences[i,:]>0#)*boxes2_mask).bool()\n        #\u9810\u6e2c\u6846\u906e\u7f69\u5305\u62ec\u61c9\u6709\u6846\u4f4d\u7f6e(mask2)\u4ee5\u53ca\u9810\u6e2c\u51fa\u6846(mask1)\u5169\u8005\u7684\u806f\u96c6\n        \n        this_boxes1=boxes1[i,:,:][mask1,:]\n        this_boxes2=boxes2[i,:,:][mask2,:]\n        #print('this_boxes2',this_boxes2.shape)\n        #print('this_boxes1',this_boxes1.shape)\n        #\u5176\u4e2d\u70ba\u4e86\u9f13\u52f5\u5728\u6b63\u78ba\u4f4d\u7f6e\u51fa\u6846\uff0c\u61c9\u6709\u6846\u4f4d\u7f6e\u7684diou_loss\u518d\u4e58\u4e0a(1+\u985e\u5225\u70ba\u96f6\u7684\u6a5f\u7387) \u985e\u5225\u70ba\u96f6\u6a5f\u7387\u8d8a\u4f4e\uff0c\u640d\u5931\u8d8a\u4f4e\n        #\u70ba\u4e86\u6291\u5236\u4e0d\u8a72\u51fa\u6846\u7684\u5730\u65b9\uff0c\u4e0d\u8a72\u6709\u6846\u4f4d\u7f6ediou_loss\u518d\u4e58\u4e0a(1+(1-\u985e\u5225\u70ba\u96f6\u7684\u6a5f\u7387)) \u985e\u5225\u70ba\u96f6\u6a5f\u7387\u8d8a\u9ad8\uff0c\u640d\u5931\u8d8a\u4f4e\n\n        this_target_confidences=target_confidences[i,:][mask2].detach()\n        this_confidences=confidences[i,:,:][mask2,:]\n\n \n        \n        if len(this_boxes1)>0 and len(this_boxes2)>0:\n            diou=1-bbox_diou(this_boxes1,this_boxes2.detach())\n            if any_abnormal_number(diou):\n                diou=where(is_abnormal_number(diou),zeros_like(diou),diou)\n                #print('diou',diou.shape)\n                print('diou',diou)\n                #print('this_probs',this_probs)\n                print('this_boxes2',this_boxes2)\n                print('this_boxes1',this_boxes1)\n         \n            \n            loss=loss+diou.mean()\n            \n    return loss\/batch\n\ndef eiou_loss(confidences, locations, target_confidences, target_locations):\n    smooth_point=0.1\n    num_classes = confidences.size(2)\n    batch=len(confidences)\n    loss=to_tensor(0.0,requires_grad=True)\n    \n    boxes1 =locations[:,:,:4]\n    boxes1 = decode(boxes1, new_rfbmodel.model.priors, (0.1,0.2))\n    boxes1=stack([clip(boxes1[:,:,0],0,1),clip(boxes1[:,:,1],0,1),clip(boxes1[:,:,2],0,1),clip(boxes1[:,:,3],0,1)],axis=-1)\n    \n    boxes2 =target_locations[:,:,:4]\n    #print('boxes2',boxes2)\n    boxes2 = decode(boxes2, new_rfbmodel.model.priors, (0.1,0.2)).detach()\n  \n    \n    \n\n\n    px1, py1, px2, py2 =split(boxes1,4,axis=-1)\n    tx1, ty1, tx2, ty2 = split(boxes2,4,axis=-1)\n        \n    # extent top left\n    ex1 = torch.min(px1, tx1)\n    ey1 = torch.min(py1, ty1)\n    # intersection coordinates\n    ix1 = torch.max(px1, tx1)\n    iy1 = torch.max(py1, ty1)\n    ix2 = torch.min(px2, tx2)\n    iy2 = torch.min(py2, ty2)\n    # extra\n    xmin = torch.min(ix1, ix2)\n    ymin = torch.min(iy1, iy2)\n    # xmax = ix2\n    # ymax = iy2\n    xmax = torch.max(ix1, ix2)\n    ymax = torch.max(iy1, iy2)\n\n    # Intersection\n    intersection = (ix2 - ex1) * (iy2 - ey1) +   \\\n                   (xmin - ex1) * (ymin - ey1) - \\\n                   (ix1 - ex1) * (ymax - ey1) -   \\\n                   (xmax - ex1) * (iy1 - ey1)\n    # Union\n    union = (px2 - px1) * (py2 - py1) + \\\n            (tx2 - tx1) * (ty2 - ty1) - intersection + 1e-7\n    # IoU\n    iou = intersection \/ union\n\n    # EIoU\n    smooth_sign = ((1 - iou) < smooth_point).detach().float()\n    eiou = 0.5 * smooth_sign * ((1 - iou) ** 2) \/ smooth_point + \\\n           (1 - smooth_sign) * ((1 - iou) - 0.5 * smooth_point)\n \n    eiou *= union\n  \n    \n    return eiou.mean()\n\ndef dice_loss(confidences, locations, target_confidences, target_locations):\n\n    num_class=confidences.shape[-1]\n    target_confidences=target_confidences.detach()\n    pos_mask=target_confidences>0\n  \n    target_confidences=make_onehot(target_confidences,5,-1).float().detach()\n    \n    confidences=exp(confidences)#.reshape((-1,5))\n    \n    #sample_weight=to_tensor([1,1,1.5,1.25]).reshape((1,4))\n    intersection = reduce_sum(target_confidences * confidences, axis=1)\n    den1 = reduce_sum(confidences,axis=1)\n    den2 = reduce_sum(target_confidences, axis=1)\n    dice = 1.0 - (2.0 * intersection + 1) \/ (den1 + den2 + 1)\n    \n    return (dice*to_tensor([[1,1,1.2,1.5,1.5]])).sum(-1).mean()\n\n@torch.no_grad()\ndef classification_recall(confidences,target_confidences):\n    confidences=to_numpy(exp(confidences))\n    target_confidences=to_numpy(target_confidences)\n    mask=target_confidences>0\n    #print('np.argmax(confidences[:,:,1:],-1)+1',(np.argmax(confidences[:,:,1:],-1)+1).shape)\n    #print('confidences[:,:,0]<0.6',(confidences[:,:,0]<0.6).shape)\n    masked_confidences=(confidences[:,:,1:].max(-1)>=0.3)[mask]\n    masked_target_confidences=target_confidences[mask]\n    return equal(masked_confidences,masked_target_confidences).mean()\n\n@torch.no_grad()\ndef classification_accuracy(confidences,target_confidences):\n    confidences=to_numpy(exp(confidences))\n    target_confidences=to_numpy(target_confidences)\n    mask=confidences[:,:,1:].max(-1)>=0.3\n    #print('np.argmax(confidences[:,:,1:],-1)+1',(np.argmax(confidences[:,:,1:],-1)+1).shape)\n    #print('confidences[:,:,0]<0.6',(confidences[:,:,0]<0.6).shape)\n    masked_confidences=(confidences[:,:,1:].max(-1)>=0.3)[mask]\n    masked_target_confidences=target_confidences[mask]\n    return equal(masked_confidences,masked_target_confidences).mean()\n\n@torch.no_grad()\ndef bbox_rmse(locations,target_locations,target_confidences):\n    bboxes=locations[:,:,:4].copy().detach()\n    target_bboxes=target_locations[:,:,:4].copy().detach()\n    bboxes = decode(bboxes, new_rfbmodel.model.priors, (0.1,0.2))\n    target_bboxes = decode(target_bboxes, new_rfbmodel.model.priors, (0.1,0.2))\n    \n    \n    mask=target_confidences>0\n    masked_bboxes=bboxes[mask,:]\n    masked_target_bboxes=target_bboxes[mask,:]\n    if len(masked_target_bboxes)>0:\n        return ((masked_bboxes-masked_target_bboxes)**2).mean().sqrt()\n    else:\n        return 0\n    \ndef bbox_iou(bboxes1, bboxes2):\n    \"\"\"\n\n    Args:\n        bboxes1 (Tensor): shape (n, 4)\n        bboxes2 (Tensor): shape (k, 4)\n\n    Returns:\n         ious(Tensor): shape (n, k)\n\n    Examples;\n    >>> boxes1=to_tensor(np.array([[39, 63, 203, 112], [49, 75, 203, 125],[31, 69, 201, 125],[50, 72, 197, 121],[35, 51, 196, 110]]))\n    >>> boxes2=to_tensor(np.array([[54, 66, 198, 114], [42, 78, 186, 126], [18, 63, 235, 135],[54, 72, 198, 120],[36, 60, 180, 108]]))\n    >>> iou_loss=(1-bbox_iou(boxes1,boxes2)).sum()\/(boxes1.shape[0]*boxes2.shape[0])\n    >>> print(iou_loss.cpu())\n    tensor(0.3802)\n\n    >>> boxes1=to_tensor(np.array([[39, 63, 203, 112], [49, 75, 203, 125],[31, 69, 201, 125],[50, 72, 197, 121],[35, 51, 196, 110]]))\n    >>> boxes2=to_tensor(np.array([[54, 66, 198, 114], [42, 78, 186, 126], [18, 63, 235, 135],[54, 72, 198, 120]]))\n    >>> iou_loss=(1-bbox_iou(boxes1,boxes2)).sum()\/(boxes1.shape[0]*boxes2.shape[0])\n    >>> print(iou_loss.cpu())\n    tensor(0.3703)\n\n\n\n\n    \"\"\"\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = torch.zeros((rows, cols))\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = torch.zeros((cols, rows))\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n\n    lt = maximum(bboxes1[:, None, :2], bboxes2[:, :2])  # [N,M,2]\n    rb = minimum(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [N,M,2]\n    wh = torch.clamp(rb - lt, min=0)  # [N,M,2]\n    inter_area = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n    #print('inter_area',inter_area)\n\n\n    union =  area1[:, None] + area2 -inter_area\n    #print('union',union)\n    ious = inter_area \/ union\n    ious = torch.clamp(ious,min=0,max = 1.0)\n    if exchange:\n        ious = ious.T\n    #print('ious',ious)\n    return reduce_max(ious,0).mean()\n\n@torch.no_grad()\ndef iou(confidences, locations, target_confidences, target_locations):\n    iou=0\n    confidences=to_numpy(exp(confidences))\n    target_confidences=to_numpy(target_confidences)\n    boxes1 =locations[:,:,:4]\n    boxes1 = to_numpy(decode(boxes1, new_rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]]))\n    \n    boxes2 =target_locations[:,:,:4]\n    boxes2 =  to_numpy(decode(boxes2, new_rfbmodel.model.priors, (0.1,0.2))*to_tensor([[640,480,640,480]]))\n    all_sorted_boxes,all_target_confidences=get_actual_target(target_locations[:,:,:4],target_confidences,new_rfbmodel.model.priors,center_variance=0.1, size_variance=0.2)\n    \n    num_batch=0\n    for i in range(len(locations)):\n        mask1=confidences[i][:,1:].max(-1)>=0.3\n        mask2=target_confidences[i,:]>0\n        #\u9810\u6e2c\u6846\u906e\u7f69\u5305\u62ec\u61c9\u6709\u6846\u4f4d\u7f6e(mask2)\u4ee5\u53ca\u9810\u6e2c\u51fa\u6846(mask1)\u5169\u8005\u7684\u806f\u96c6\n   \n        this_boxes1=boxes1[i,:,:][mask1,:]\n        this_boxes1_conf=np.expand_dims(np.argmax(confidences[i][:,:],-1)[mask1],-1)\n        box_probs = np.concatenate([this_boxes1.astype(np.float32),this_boxes1_conf.astype(np.float32)], axis=1)\n        #print(box_probs.shape)\n        if len(box_probs) > 1:\n            box_probs, keep = hard_nms(box_probs, 0.5, top_k=-1, )\n            this_boxes1=this_boxes1[keep,:]\n            this_boxes1_conf=this_boxes1_conf[keep,:]\n        #this_boxes2=boxes2[i,:,:][mask2,:]\n        this_boxes2=all_sorted_boxes[i]\n        \n        if this_boxes2 is not None and len(this_boxes1)>0 and len(this_boxes2)>0:\n            iou+=to_numpy(bbox_iou(to_tensor(this_boxes1),to_tensor(this_boxes2))).mean()\n            num_batch+=1\n        elif  len(this_boxes2)>0:\n            num_batch+=1\n    return iou\/num_batch\n\n\n# @torch.no_grad()\n# def iou(confidences, locations, target_confidences, target_locations):\n#     iou=0\n#     confidences=to_numpy(exp(confidences))\n#     target_confidences=to_numpy(target_confidences)\n#     boxes1 =locations[:,:,:4]\n#     boxes1 = to_numpy(decode(boxes1, new_rfbmodel.model.priors, (0.1,0.2)))\n    \n#     boxes2 =target_locations[:,:,:4]\n#     boxes2 =  to_numpy(decode(boxes2, new_rfbmodel.model.priors, (0.1,0.2)))\n#     num_batch=0\n#     for i in range(len(locations)):\n#         mask1=np.argmax(confidences[i],-1)>0\n#         mask2=target_confidences[i,:]>0\n     \n#         this_boxes1=boxes1[i,:,:][mask1,:]\n#         this_boxes2=boxes2[i,:,:][mask2,:]\n        \n#         if len(this_boxes1)>0 and len(this_boxes2)>0:\n#             iou+=bbox_iou(this_boxes1,this_boxes2).mean()\n#             num_batch+=1\n#         elif  len(this_boxes2)>0:\n#             num_batch+=1\n#     return iou\/num_batch\n\n","32514aea":"\n\ndef draw_detection(training_context):\n    if training_context['current_epoch']>=0:\n        traindata=training_context['train_data']\n        #print(traindata.keys())\n        imagedata=traindata['images'].copy()\n        labeldata=traindata['locations'].copy()\n        #print(softmax(traindata['confidences'].float(),-1))\n        #\u57fa\u65bc\u8a13\u7df4\u968e\u6bb5\u8f38\u51fa\u7e6a\u5716\uff0c\u4fe1\u5fc3\u6c34\u6e96\u61c9\u8a72\u8981\u53d6exp\n        confidences =to_numpy(exp(traindata['confidences'].copy().float()))\n\n        target_boxes=to_tensor(traindata['target_locations'].copy()[:,:,:4])\n        boxes =to_tensor(labeldata[:,:,:4])\n\n        gc.collect()\n        #\u57fa\u65bc\u8a13\u7df4\u968e\u6bb5\u8f38\u51fa\u7e6a\u5716\uff0cboxes\u61c9\u8a72\u8981decode\n        locations = to_numpy(decode(boxes, new_rfbmodel.model.priors, (0.1,0.2)))\n        target_locations = to_numpy(decode(target_boxes, new_rfbmodel.model.priors, (0.1,0.2)))\n        for i in range(6):\n            pillow_img=None\n            #probs=confidences[i].max().item()\n\n            #print('confidence',confidences[i])\n            #print('probs',probs)\n            print(confidences[i].shape)\n            mask=confidences[i][:,1:].max(-1)>=0.3\n            #print(mask.shape)\n            #print(confidences[i].shape)\n            #print(locations[i].shape)\n            this_locations = locations[i][mask, :]\n            this_target_locations=target_locations[i][mask, :]\n            #print(this_locations.shape)\n            if len(this_locations)>0:\n                this_locations[:, 0] = this_locations[:, 0]*640\n                this_locations[:, 2] = this_locations[:, 2] *640\n                this_locations[:, 1] = this_locations[:, 1] *480\n                this_locations[:, 3] = this_locations[:, 3] *480\n\n                this_confidence=confidences[i][mask]\n            \n#                 aspect_ratio=(this_locations[:,2]-this_locations[:,0])\/(this_locations[:,3]-this_locations[:,1]).astype(np.float32)\n#                 aspect_mask=((aspect_ratio<3.5)*(this_locations[:,3]-this_locations[:,1]<3)).astype(np.bool)\n\n#                 this_locations=this_locations[aspect_mask,:]\n#                 this_confidence=this_confidence[aspect_mask,:]\n\n\n\n                pillow_img=array2image(reverse_image_backend_adaption(imagedata[i]*127.5+127.5))\n                draw = ImageDraw.Draw(pillow_img)\n\n                box_probs = np.concatenate([this_locations.astype(np.float32), (np.argmax(this_confidence[:,1:],-1)+1).reshape(-1, 1).astype(np.float32), np.max(this_confidence,-1).reshape(-1, 1).astype(np.float32)], axis=1)\n                #print(box_probs.shape)\n                if len(boxes) > 1:\n                    box_probs, keep = hard_nms(box_probs, 0.5, top_k=-1, )\n                    this_locations=this_locations[keep,:]\n                    this_confidence=this_confidence[keep,:]\n\n                #print(len(this_locations),len(this_keypoints),len(this_confidence))\n                if len(this_locations) > 0:\n                    if len(this_locations)>20:\n                        print('target locations')\n                        print(this_target_locations)\n                    for box,conf in zip(this_locations,this_confidence):\n                        try:\n                            print('box',[int(v) for v in box],'aspect ratio',(box[2]-box[0])\/float(box[3]-box[1]),'conf',['{0:.3f}'.format(v) for v in conf])\n                            #print(box)\n\n                            pillow_img =plot_bbox(box, pillow_img, palette[np.argmax(conf[1:])] ,  '', line_thickness=2)\n                        except Exception as e:\n                            print(e)\n                            print('box',box,'conf',conf)\n                        \n                      \n\n                       \n\n\n\n            display.display(pillow_img)\n                #pillow_img.save('data_provider_next_{0}.jpg'.format(get_time_suffix()))\n","55ab4efc":"from trident.callbacks.lr_schedulers import AdjustLRCallbackBase\n\nclass PolyLR(AdjustLRCallbackBase):\n    def __init__(self,max_lr=1e-3,  max_iter=10000):\n        super().__init__()\n        self.max_lr = max_lr\n        self.max_iter=max_iter\n    def on_batch_end(self, training_context):\n        current_step =training_context['steps']\n        lr = self.max_lr * (1 - (current_step\/ self.max_iter)) * (1 - (current_step \/ self.max_iter))\n        if (lr < 1.0e-7):\n            lr = 1.0e-7\n        self.adjust_learning_rate(training_context, lr, verbose=False)\n","0015c9ea":"#\u662f\u5426\u70ba\u63a5\u7e8c\u8a13\u7df4\nis_resume=True\nif is_resume and os.path.exists('.\/Models\/rfbnet_ssd.pth'):\n    new_rfbmodel.load_model('.\/Models\/rfbnet_ssd.pth')\n    print('.\/Models\/rfbnet_ssd.pth loaded')\n    \nelse:\n    if os.path.exists('..\/input\/masked-face-detection\/Models\/rfbnet_ssd.pth'):\n        new_rfbmodel.load_model('..\/input\/masked-face-detection\/Models\/rfbnet_ssd.pth')\n\ndraw_infer(img_data,label_data,new_rfbmodel.model.priors)","77390469":"\n        \ndef punishment(confidences,target_confidences):\n    \n    #return ((exp(confidences).gather(-1,target_confidences.unsqueeze(-1))-1)**2).mean()\n    \n    confidences=exp(confidences).reshape((-1,5))\n    target_confidences=target_confidences.reshape(-1).detach()\n    pos_mask=target_confidences>0\n    not_pos_mask=target_confidences==0\n    #\u61c9\u8a72\u6aa2\u51fa\u537b\u6c92\u6709\n    falsenegative_mask=((target_confidences>0)*not_equal(argmax(confidences,-1),target_confidences)).bool()\n    #\u61c9\u8a72\u6c92\u6709\u537b\u6aa2\u51fa\n    falsepositive_mask=((target_confidences==0)*not_equal(argmax(confidences,-1),target_confidences)).bool()\n    \n    masked_confidences=confidences[pos_mask,:]\n    masked_not_confidences=confidences[not_pos_mask,:]\n    masked_falsenegative_confidences=confidences[falsenegative_mask,:]\n    masked_falsepositive_confidences=confidences[falsepositive_mask,:]\n    \n    masked_target_confidences=target_confidences[pos_mask]\n    masked_not_target_confidences=target_confidences[not_pos_mask]\n    masked_falsenegative_target_confidences=target_confidences[falsenegative_mask]\n    masked_falsepositive_target_confidences=target_confidences[falsepositive_mask]\n    loss=0\n    if len(masked_target_confidences)>0:\n        loss=loss+binary_cross_entropy(masked_confidences.gather(1,masked_target_confidences.unsqueeze(-1)).squeeze(-1),1).mean()\n        \n    if len(masked_falsenegative_target_confidences)>0:\n        loss=loss+8.5*binary_cross_entropy(masked_falsenegative_confidences.gather(1,masked_falsenegative_target_confidences.unsqueeze(-1)).squeeze(-1),1).mean()\n    \n    if len(masked_not_target_confidences)>0:\n        loss=loss+1.0*binary_cross_entropy(masked_not_confidences.gather(1,masked_not_target_confidences.unsqueeze(-1)).squeeze(-1),1).mean()\n    \n    if len(masked_falsepositive_target_confidences)>0:\n        loss=loss+2.0*binary_cross_entropy(masked_falsepositive_confidences.gather(1,masked_falsepositive_target_confidences.unsqueeze(-1)).squeeze(-1),1).mean()\n    return loss\n    \n    \n    \n#\u8a2d\u5b9a\u6a21\u578b\u5c64\u7d1a\u8a13\u7df4\u6240\u9700\u8cc7\u8a0a\n#\u5176\u4e2d\u900f\u904etrigger_when\u53ef\u4ee5\u52d5\u614b\u6307\u5b9a\u5728\u751a\u9ebc\u5834\u5408\u57f7\u884c\u54ea\u500b\u51fd\u6578(\u5176\u5f15\u6578\u5fc5\u9808\u662ftraining_context)\nnew_rfbmodel.with_optimizer(optimizer=DiffGrad,lr=5e-5,betas=(0.9, 0.999),weight_decay=4e-5)\\\n.with_loss(MultiBoxLoss(new_rfbmodel.model.priors,neg_pos_ratio=3.5),loss_weight=1)\\\n.with_loss(dice_loss,loss_weight=1.5)\\\n.with_loss(punishment,loss_weight=0.1)\\\n.with_metric(classification_accuracy, name='accuracy',print_only=True)\\\n.with_metric(classification_recall, name='recall',print_only=True)\\\n.with_metric(iou)\\\n.with_metric(bbox_rmse,print_only=True)\\\n.with_regularizer('l2',reg_weight=5e-5)\\\n.with_accumulate_grads(5)\\\n.trigger_when(when='on_loss_calculation_start',frequency=1,unit='batch',action=preprocess_data)\\\n.trigger_when(when='on_batch_end',frequency=50,unit='batch',action=draw_detection)\\\n.with_learning_rate_scheduler(PolyLR(max_lr=5e-5,max_iter=5000))\\\n.with_model_save_path('.\/Models\/rfbnet_ssd.pth')\\\n.with_automatic_mixed_precision_training()","8fd15cfb":"#\u8a2d\u5b9a\u5b78\u7fd2\u8a08\u756b\n#\u52a0\u5165\u8a13\u7df4\u6d41\u7a0b\u6240\u9700\u4e4b\u4fe1\u606f\nplan=TrainingPlan()\\\n    .add_training_item(new_rfbmodel,name='rfbnet')\\\n    .with_data_loader(data_provider)\\\n    .with_batch_size(32)\\\n    .repeat_epochs(30)\\\n    .print_progress_scheduling(5,unit='batch') \\\n    .display_loss_metric_curve_scheduling(100)\\\n    .save_model_scheduling(50,unit='batch')\\\n    .start_now()","99480f64":"\u5b9a\u7fa9\u53e3\u7f69\u4eba\u81c9\u8b58\u5225\u7684\u6a19\u7c64\u3001\u8abf\u8272\u76e4\u4e26\u8b80\u53d6\u5716\u7247\u6e05\u55ae","4e7815a4":"\u7d44\u88dd\u51fa\u672c\u6b21\u8a13\u7df4\u8981\u7528\u5230\u7684data provider\uff0c\u4e26\u52a0\u5165\u6578\u64da\u589e\u5f37\u6280\u8853\uff0c\u9019\u6b21\u6539\u7248\u4e2d\u5fc3\u52a0\u5165\u4e86\u5169\u7a2e\u5f37\u5927\u7684\u6578\u64da\u589e\u5f37\u6280\u8853:   \n1. ImageMosaic(\u5716\u7247\u99ac\u8cfd\u514b) [YOLOv4: Optimal Speed and Accuracy of Object Detection](https:\/\/arxiv.org\/abs\/2004.10934) \n2. GridMask [GridMask Data Augmentation](https:\/\/arxiv.org\/abs\/2001.04086)\n\n\u524d\u8005\u56e0\u70ba\u5fc5\u9808\u8981\u5c07\u4e00\u500b\u6279\u6b21\u4e2d\u7684\u6578\u5f35\u5716\u7247\u91cd\u7d44\uff0c\u56e0\u6b64\u5c6c\u65bcbatch transform","18a724f3":"\u8abf\u7528next()\u4f86\u78ba\u8a8ddata_provider\u80fd\u5920\u7522\u51fa\u6578\u64da\u7121\u8aa4","c6fdb1a7":"\u7d44\u88dd\u51fa\u5716\u7247\u8207\u908a\u969b\u6846\u5c0d\u61c9\u7684dataset\uff0c\u518d\u5c07\u5b83\u7d44\u6210data_provoder\uff0c\u4ee5\u53ca\u8a2d\u5b9a\u5c0d\u61c9\u7684\u6578\u64da\u589e\u5f37\u6216\u6578\u64da\u8f49\u63db\u3002\u5176\u4e2d  \n\npaired_transform_funcs: \u6307\u7684\u662f\u6703\u5f71\u97ff\u7a7a\u9593\u4f4d\u7f6e\u7684\u8f49\u63db\uff0c\u9700\u8981\u5716\u7247\u8207\u908a\u754c\u6846\u6210\u5c0d\u7684\u540c\u6b65\u8f49\u63db\uff0c\u6240\u5982\u7e2e\u653e\u3001\u4eff\u5c04\u8f49\u63db   \nbatch_transform_funcs: \u662fpaired_transform_funcs\u7684\u4e00\u7a2e\u7279\u4f8b\uff0c\u6d89\u53ca\u591a\u5f35\u5716\u7247\u7684\u878d\u5408\uff0c\u4f8b\u5982Mixup\u6216\u662f\u5716\u50cf\u99ac\u8cfd\u514b   \nimages_transform_funcs: \u5176\u5b83\u4e0d\u6703\u9020\u6210\u7a7a\u9593\u4f4d\u7f6e\u8b8a\u5316\u7684\u8f49\u63db\uff0c\u50c5\u9700\u8981\u5c0d\u5716\u7247\u57f7\u884c\uff0c\u4e0d\u9700\u8981\u8003\u616e\u908a\u754c\u6846   \n","8d70d3a2":"\u4e0b\u65b9\u51fd\u6578\u5247\u662f\u53ef\u4ee5\u6839\u64dadata provider\u5410\u51fa\u4f86\u7684\u6578\u64da\u7e6a\u88fd\u6210\u5177\u908a\u754c\u6846\u7684\u5716\u7247\uff0c\u9019\u500b\u7528\u610f\u662f\u7528\u4f86\u6aa2\u67e5\u524d\u9762\u6211\u5011\u5ba2\u88fd\u5316\u7684SsdBboxDataset\u662f\u5426\u6b63\u78ba\u7121\u8aa4","5292f40c":"\u63a5\u4e0b\u4f86\u5247\u662f\u5ba2\u88fd\u5316\u51fa\u6839\u64da\u672c\u60c5\u5883\u6240\u9700\u7684\u908a\u754c\u6846Dataset\u3002","34d38153":"\u63a5\u8457\u6211\u5011\u9664\u4e86\u53e3\u7f69\u4eba\u81c9\u5916\uff0c\u4e5f\u9700\u8981\u88dc\u5145\u4e00\u822c\u4eba\u81c9\u53ca\u5404\u7a2e\u9ad8\u96e3\u5ea6\u4eba\u81c9\u6aa2\u6e2c\u5834\u666f\uff0c\u9019\u6642\u5019widerface\u662f\u6700\u9069\u5408\u7684\u8cc7\u6599\u4f86\u6e90\uff0c\u4ee5\u4e0b\u7684\u8a9e\u6cd5\u5247\u793a\u7bc4\u5982\u4f55\u8b80\u53d6\u8207\u6574\u7406widerface\u7684\u6578\u64da\u6210\u524d\u9762\u4e00\u81f4\u7684\u683c\u5f0f\u3002","1632aea0":"\u6e96\u5099\u8a13\u7df4\u6642\u6240\u9700\u7684\u640d\u5931\u51fd\u6578\u8207\u8a55\u4f30\u51fd\u6578","27beed4b":"SSD\u6aa2\u6e2c\u6a21\u578b\u6240\u9700\u8981\u7684\u8a08\u7b97\u51fd\u6578\uff0c\u5176\u4e2dencode\u662f\u5c07\u5b9a\u7fa9\u70bax1y1x2y2\u7684\u908a\u754c\u6846(\u76f8\u5c0d\u5927\u5c0f)\u57fa\u65bcpriors\u7de8\u78bc\u6a21\u578b\u7279\u5fb5\u5716\u7684\u76ee\u6a19\u8f38\u51fa\u3002\u800cdecode\u5247\u662f\u5c07\u6a21\u578b\u7684\u7279\u5fb5\u5716\u8f38\u5165\uff0c\u57fa\u65bcpriors\u89e3\u78bc\u70ba\u5be6\u969b\u7684\u908a\u754c\u6846(\u76f8\u5c0d\u5927\u5c0fx1y1x2y2)","4813f82b":"\u7e3d\u8a08\u8a13\u7df4\u53c3\u6578\u4e5f\u624d299,926\uff0c\u50c5\u63a5\u8fd130\u842c\u5de6\u53f3\uff0c\u76f8\u8f03\u65bc\u4e00\u822c\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u6578\u767e\u842c\u3001\u5343\u842c\u3001\u5104\u7d1a\u5225\u7684\u6b0a\u91cd\uff0c\u5c31\u62ffYOLO V4\u4f86\u8aaa\u6b0a\u91cd\u6578\u5c31\u67096436\u842c\u500b\uff0c\u76f8\u8f03\u4e4b\u4e0b\u9019\u500bRFBNET\u771f\u7684\u8d85\u7d1a\u8ff7\u4f60\u3002","ac974539":"\u9019\u88e1\u4f7f\u7528\u7684\u6578\u64da\u96c6\u662f\u6211\u57282020\/2\u6708\u958b\u6e90\u7684\u53e3\u7f69\u4eba\u81c9\u6578\u64da\u96c6\uff0c\u8b80\u53d6\u5c0d\u61c9\u7684\u6a19\u8a3bjson\u6a94\u6848\u683c\u5f0f\uff0c\u8f49\u63db\u6210\u5c0d\u61c9\u7684\u5716\u7247\u8207\u908a\u754c\u5bec\u6e05\u55ae\u3002","09892353":"\u793a\u7bc4\u5982\u4f55\u8b80\u53d6\u908a\u754c\u6846\u5b9a\u7fa9\u4f86\u756b\u51fa\u5c0d\u61c9\u5f71\u50cf\u8996\u89ba\u5316\u6548\u679c","4b954648":"\u9019\u6b21\u8981\u4f7f\u7528\u7684\u662f\u5927\u5c0f\u624d1m\u7684\u8d85\u9ad8\u901f\u4eba\u81c9\u6aa2\u6e2c\u6a21\u578b\uff0c\u4f46\u7531\u65bc\u6211\u5011\u9700\u8981\u5206\u8fa8\u51fa\u6c92\u6234\u53e3\u7f69\u3001\u6234\u53e3\u7f69\u3001\u5e36\u4e0d\u6b63\u78ba\u53e3\u7f69\u3001\u6234\u53e3\u7f69\u65b9\u5f0f\u4e0d\u6b63\u78ba\u9019\u5e7e\u7a2e\u72c0\u6cc1\uff0c\u56e0\u6b64\u8981\u628a\u5206\u985e\u982d\u5f9e\u539f\u4f862\u64f4\u5145\u52305\u3002\u6211\u5011\u53ef\u4ee5\u9ad4\u9a57\u4e00\u4e0b\u5f9e\u982d\u8a13\u7df4\u7684\u904e\u7a0b\u3002","04d41a46":"\u70ba\u4e86\u80fd\u5920\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u6bcf\u6578\u500b\u6279\u6b21\u5c31\u81ea\u52d5\u6839\u64da\u7576\u4e0b\u6a21\u578b\u7d50\u679c\u8f38\u51fa\u7e6a\u88fd\u8996\u89ba\u5316\u5f71\u50cf\uff0c\u9019\u908a\u9700\u8981\u8a2d\u8a08\u4e00\u500b\u51fd\u6578\uff0c\u5176\u552f\u4e00\u7684\u8f38\u5165\u5f15\u6578\u662ftraining_context\uff0c\u57fa\u672c\u4e0a\u6b64\u51fd\u6578\u53ef\u4ee5\u900f\u904etraining_context\u4f5c\u70ba\u901a\u4fe1\u4e4b\u57fa\u790e\uff0c\u4f86\u53d6\u5f97\u7576\u4e0b\u6240\u6709\u9700\u8981\u7684\u72c0\u614b\u4fe1\u606f\u3002"}}