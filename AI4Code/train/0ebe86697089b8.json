{"cell_type":{"1cf98038":"code","891c0324":"code","8d19f73a":"code","1e6e13be":"code","d4750eea":"code","821d01c4":"code","dbd9a905":"code","ed9e36fd":"code","20c0da2a":"code","c82d4a6b":"code","5a26a5c5":"code","983caadd":"code","74d6702a":"code","d4d2691f":"code","eec2196e":"code","3b0ffb54":"code","6ce2bd1a":"code","14ea549e":"code","b5bd349f":"code","ab2277dc":"code","83cf835c":"code","bf323b70":"code","7e73cea5":"code","67b6c13a":"code","1658b58e":"code","31f6246a":"code","6b50d3ad":"code","742b6096":"code","6ded17b7":"code","e2564aa7":"code","5f1546b6":"code","ab49498b":"code","cdf7770d":"code","05759de4":"code","c7f6789a":"code","6a944381":"code","31302629":"code","58f85bb5":"code","bc8f190a":"code","5147b91a":"code","6b38313f":"code","6e369292":"code","8210c7fa":"code","76b59601":"code","e5bc3e6b":"code","80959bbc":"code","f62d9edf":"code","0e9ecbd4":"code","50620d8a":"code","77e95408":"code","4a7f4c42":"code","d7e9a01d":"code","a7c240d4":"code","0c63cf40":"code","34b760c9":"code","a00f988b":"code","788a3d8e":"code","d9bc76f2":"code","903e3a1c":"code","57cfaf1e":"code","1d516b4a":"code","7bbdfc36":"code","bf327a30":"code","27f220d1":"code","5c995794":"code","12d65b35":"code","066147ad":"code","1efb2d1d":"code","4a36aed6":"code","e1c12c5e":"code","5fb0163a":"code","d6dbcf9d":"code","874a3b1a":"markdown","3a28f45c":"markdown","315b3c42":"markdown","350fb30e":"markdown","ceec134f":"markdown","6ccddd47":"markdown","2ee3a205":"markdown","d79eb76a":"markdown","10a0175c":"markdown","bbb00984":"markdown","2564cf40":"markdown","67b4c633":"markdown","d39f9543":"markdown","617b32ae":"markdown","93cd5da2":"markdown","c5abfb35":"markdown","b57f2c79":"markdown","6d1d8185":"markdown","35535cf0":"markdown","17c575b1":"markdown","90ede078":"markdown","dba16266":"markdown","fe373871":"markdown","254a2e48":"markdown","c741e5e3":"markdown","affe3911":"markdown","f64e838d":"markdown","365e3030":"markdown","a60c6c8a":"markdown","c9372641":"markdown","2ecc4adb":"markdown","b4209939":"markdown","16a12f56":"markdown","52e149ad":"markdown","5cd149c9":"markdown","fefacf20":"markdown","195b76c5":"markdown","aaf5010a":"markdown","fc4d865c":"markdown","eb13eeb0":"markdown","f466cf51":"markdown","231fe73b":"markdown","b2919b54":"markdown","4e80ce30":"markdown","1952b341":"markdown","d982db24":"markdown","c09946f5":"markdown","a5fef95c":"markdown","1e92d9d7":"markdown","eeb7a714":"markdown","57bc0f56":"markdown","9bed898e":"markdown","4fab336b":"markdown"},"source":{"1cf98038":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.available","891c0324":"style.use(\"seaborn-dark\")","8d19f73a":"data = pd.read_csv(\"..\/input\/forest-cover-type-prediction\/train.csv\")","1e6e13be":"data.shape","d4750eea":"data.head()","821d01c4":"data.drop(\"Id\", axis=1, inplace=True)","dbd9a905":"x = data.drop(\"Cover_Type\", axis=1)\ny = data[\"Cover_Type\"]","ed9e36fd":"x.head(10)","20c0da2a":"x.columns","c82d4a6b":"print('Soil_Type37'[:9])\nprint('Wilderness_Area3'[:15])","5a26a5c5":"categoricals = []\nnumericals = []\nfor col in x.columns:\n    if col[:9]==\"Soil_Type\" or col[:15]=='Wilderness_Area':\n        categoricals.append(col)\n    else:\n        numericals.append(col)","983caadd":"numericals","74d6702a":"fig, axes = plt.subplots(nrows=len(numericals), ncols=1, figsize=(20, 9*len(numericals)))\nfor i in range(len(numericals)):\n    col = numericals[i]\n    sns.histplot(x=col, data=x, ax=axes[i], color=\"blueviolet\", kde=True)\nplt.show()","d4d2691f":"fig, axes = plt.subplots(nrows=len(categoricals), ncols=1, figsize=(20,9*len(categoricals)))\nfor i in range(len(categoricals)):\n    col = categoricals[i]\n    sns.countplot(x=col, data=x, ax=axes[i], palette=\"Set2\")\n    axes[i].set_yticks(x[col].value_counts())\nplt.show()","eec2196e":"y.value_counts()","3b0ffb54":"plt.figure(figsize=(20,9))\nsns.countplot(y)\nplt.show()","6ce2bd1a":"x.drop([\"Soil_Type7\", \"Soil_Type15\"], axis=1, inplace=True)\ncategoricals.remove(\"Soil_Type7\")\ncategoricals.remove(\"Soil_Type15\")","14ea549e":"from scipy.stats import zscore","b5bd349f":"uni_out = x.copy(deep=True)","ab2277dc":"zs = zscore(uni_out[numericals])","83cf835c":"zs","bf323b70":"scores = np.abs(zs)","7e73cea5":"non_outlier_indices = (scores<3).all(axis=1)\nprint(non_outlier_indices)","67b6c13a":"uni_out[non_outlier_indices]","1658b58e":"uni_out = x.copy(deep=True)","31f6246a":"x[\"Soil_Type13\"].value_counts()","6b50d3ad":"probably_useless_features = []\nfor col in categoricals:\n    if x[col].value_counts()[0] < 100 or x[col].value_counts()[1] < 100:\n        probably_useless_features.append(col)","742b6096":"probably_useless_features","6ded17b7":"uni_out.drop(probably_useless_features, axis=1, inplace=True)","e2564aa7":"uni_out","5f1546b6":"feat = []\nfor i in range(15120):\n    r = np.random.rand()\n    if r < 0.25:\n        feat.append(\"value1\")\n    elif r < 0.53:\n        feat.append(\"value2\")\n    elif r < 0.85:\n        feat.append(\"value3\")\n    elif r < 0.99:\n        feat.append(\"value4\")\n    else:\n        feat.append(\"value5\")\nfeat = pd.Series(feat)","ab49498b":"sns.countplot(feat)","cdf7770d":"feat.shape","05759de4":"feat","c7f6789a":"feat!=\"value5\"","6a944381":"feat = feat[feat!=\"value5\"]","31302629":"sns.countplot(feat)","58f85bb5":"feat.shape","bc8f190a":"from sklearn.ensemble import IsolationForest\niso = IsolationForest(contamination=0.05) # I want to remove the 5% most extreme rows\/datapoints","5147b91a":"iso.fit(x)","6b38313f":"outlier_indices = iso.predict(x)","6e369292":"pd.Series(outlier_indices).value_counts()","8210c7fa":"outlier_indices == 1","76b59601":"x[outlier_indices == 1]","e5bc3e6b":"!pip install category_encoders","80959bbc":"from category_encoders import CountEncoder","f62d9edf":"feat","0e9ecbd4":"enc = CountEncoder()\nfeat_enc = enc.fit_transform(feat)\nprint(feat_enc)","50620d8a":"enc = CountEncoder(normalize=True)\nfeat_enc = enc.fit_transform(feat)\nprint(feat_enc)","77e95408":"v = pd.DataFrame()\nv[\"Soil_Type10\"] = x[\"Soil_Type10\"].map({0:\"no\", 1:\"yes\"})\nv[\"Other_feature\"] = feat\nt = x[\"Wilderness_Area3\"]","4a7f4c42":"v","d7e9a01d":"t","a7c240d4":"from category_encoders import WOEEncoder","0c63cf40":"enc = WOEEncoder()","34b760c9":"enc.fit(v, t)","a00f988b":"enc.transform(v)","788a3d8e":"from sklearn.preprocessing import QuantileTransformer, StandardScaler, RobustScaler, MinMaxScaler","d9bc76f2":"NQT=QuantileTransformer(output_distribution='normal')\nUQT=QuantileTransformer(output_distribution='uniform')\nRS=RobustScaler()\nSS=StandardScaler()\nMMS=MinMaxScaler()\n\nscalers = [NQT,UQT,RS,SS,MMS]\nnames = [\"Gaussian\", \"Uniform\", \"Robust\", \"Standard\", \"Min-Max\"]","903e3a1c":"fig, axes = plt.subplots(nrows=len(numericals), ncols=1+len(scalers), figsize=(20, 5*len(numericals)))\nfor i in range(len(numericals)):\n    col = numericals[i]\n    sns.histplot(x=col, data=x, ax=axes[i,0], color=\"blueviolet\")\n    axes[i,0].set_title(\"Original\")\n    for j in range(len(scalers)):\n        scaler = scalers[j]\n        reshaped_col = np.expand_dims(x[col], axis=1)\n        transformed_col = scaler.fit_transform(reshaped_col)\n        sns.histplot(x=transformed_col[:,0], ax=axes[i,j+1], color=\"crimson\")\n        axes[i,j+1].set_title(names[j])\nplt.show()","57cfaf1e":"rs = RobustScaler(quantile_range=(5,95))\nscaled_data = rs.fit_transform(x)","1d516b4a":"xcopy = x.copy(deep=True)\nxcopy.drop(probably_useless_features, axis=1, inplace=True)","7bbdfc36":"xcopy = xcopy.sample(n=3000, axis=0)","bf327a30":"for col in xcopy.columns:\n    for i in xcopy.index:\n        r = np.random.rand()\n        if r < 0.05:\n            xcopy.loc[i,col] = np.nan","27f220d1":"xcopy.shape","5c995794":"xcopy.isna().sum()","12d65b35":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","066147ad":"from sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(max_depth = 5)","1efb2d1d":"mice = IterativeImputer(estimator=tree, n_nearest_features=10)","4a36aed6":"impdata = mice.fit_transform(xcopy)","e1c12c5e":"impdataframe = pd.DataFrame(impdata, columns=xcopy.columns)","5fb0163a":"impdataframe.isna().sum()","d6dbcf9d":"for col in numericals:\n    sns.histplot(xcopy[col])\n    plt.show()","874a3b1a":"### Notes:\n* Some are gaussian\/normal, some are skewed.  \n* There are outliers that we can trim\/remove.","3a28f45c":"Encoding means replacing categorical values to numerical ones.","315b3c42":"# Scaling Methods","350fb30e":"In this section we will explore a few different scaling methods.","ceec134f":"Then it makes another random split.  \nNotice that after the second split, the outlier is now isolated.  \nIt would take, on average, many more splits to isolate points that are not outliers.","6ccddd47":"**Univariate means we will try to detect outliers by looking at each feature by itself.**  \n**For every feature, any point that has a value that is very different from the remaining will be considered an outlier**","2ee3a205":"# Pre-Processing Tutorial: Level 2  \n**So this is a tutorial I presented as an instructor at GDSC Enet'Com, Tunisia.**  \n**I considered calling it \"Advanced pre-processing tutorial\", but then again, whenever I see such a title on a kaggle notebook it ends up being not as \"advanced\" as it claims. I didn't want to fall in a category that i myself criticize xD**  \n**Plus, almost no matter what you do in this field, there's something much more advanced.**","d79eb76a":"Say the first feature is represented in green (the x feature) and the second is in orange (the y feature).  \n(x2-x1) will be a value in [-1 , 1] (cause feature x has values between 0 and 1), and (y2-y1) will have values in [-4000, 14000].  \nThe value of the distance will be almost equal to sqrt( (y2 - y1)\u00b2 ), and the x term will have no effect.  \nSo basically the first feature will be ignored by default.  \nWe don't want that since it could be an important feature.  \nThe solution is to make all features have similar ranges, for example from 0 to 1.  \nThis is called feature scaling.  \nHowever, if you're not going to use a model\/method that depends on distances, you don't need scaling.  ","10a0175c":"Make sure you use absolute values","bbb00984":"The id is useless here so we drop it.","2564cf40":"-1 values indicate outliers","67b4c633":"# MICE: Multiple\/Multivariate Imputation by Chained Equations","d39f9543":"I'll be honest, I don't want to explain this. Maybe another time, but not right now.  \nHowever, [This article](https:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html) explains it really nicely, so make sure you check it out.","617b32ae":"**Every point that is situated far from the others in a histogram is considered an outlier and removed**  \n**Since we wouldn't do this manually, we can automate the process by calculating z-scores**  \n**The z-score of an observation\/datapoint with regards to a certain continuous feature, characterizes how far it is from the mean\/average value**  \n**The z-score is \"how many standard deviations away from the mean is this value?\"**  \n**Or, more simply, \"How far away from the mean is this value?\", the unit being 1 standard deviations**  \n**So if the z-score of a particular value with regards to a particular feature is 2.3, we would say that the difference between that value and the mean value is equal to 2.3 times the standard deviation of the feature**  \n**To get the z-score, we simply subtract the mean from the value of the observation, then divide it by the standard deviation**  \n![](https:\/\/toptipbio.com\/wp-content\/uploads\/2020\/02\/Z-score-formula.jpg)","93cd5da2":"![](https:\/\/miro.medium.com\/max\/768\/1*6Aw782wiyiFtzvK7EOY8CA.png)","c5abfb35":"**Outliers are observations\/datapoints that are very different from the majority**  \n**They can often negatively affect the performance of some models\/algorithms**  \n**So we usually just get rid of them**","b57f2c79":"## Numerical Features","6d1d8185":"The following cell shows how to scale one feature with the RobustScaler.  \nThe syntax is the same for all features.  \nYou can write a loop to scale all features.  \nOr you can pass the whole dataset to the scaler, but keep in mind it will return an array, not a dataframe.","35535cf0":"Count encoding consists of simply replacing a categorical value with how many times it was observed. It literally counts it.  \nFrequency encoding is the same thing, it just divides the count by the total number of rows to get a proportion\/probability.  \nSo we just replace every categorical value with how common it is.","17c575b1":"# Frequency\/Count Encoding","90ede078":"**Some outliers can not be detected by looking at histograms**  \n**In the following image we have an example.**  \n**The red point is clearly an outlier**  \n**If we plot the histogram of the x-axis variable\/feature or the y-axis variable\/feature, it would be in the middle, not somewhere extreme**","dba16266":"The Isolation Forest algorithm chooses a feature\/variable randomly, then makes a split at a random value.  \nThis separates the space\/points into two parts.  ","fe373871":"# Weight Of Evidence Encoding","254a2e48":"**If the zscore is negative then the value x is less than the mean, and if the zscore is positive then it's higher that the mean**  \n**Usually, if the zscore (in absolute value) is above 3, the point\/value is considered an outlier and removed**  \n**You can choose other values though. Higher values mean you will tolerate more outliers and only remove the most extreme ones, whereas lower values will remove more points and only keep points that are close to the mean.**","c741e5e3":"MICE is a method used to estimate missing values with machine learning.  \nThis is often better than dropping them or replacing them with means or modes etc..  \nI cannot explain mice here, but [THIS VIDEO RIGHT HERE](https:\/\/www.youtube.com\/watch?v=WPiYOS3qK70) does a good job at doing that so check it out.","affe3911":"**Are there categorical outliers????**  \n**Well if any point that is different from the majority is an outlier then any categorical value with a low number points could be considered an anomaly and any point corresponding to it would be an outlier**  \n**If that feature is binary, then removing one categorical value will only leave one, and thus that feature will be uninformative and get removed**  \n**So basically, any binary categorical feature that is so imbalanced that it has only a very small number of observations with a value of 1 (or 0) will be removed**  \n#### Note:\n**This isn't always a good idea, it depends on what you mean by \"a very small number of observations\"**  \n**Here, i considered anything less that 100 to be a very small number of observations**","f64e838d":"Can't do pre-processing without understanding the data first so gotta do this.  \nFeel free to skip if uninterested.","365e3030":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1766233\/2883347\/image3.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211208%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211208T105422Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=9c04436e5d15a77f3baed0ccdb2cdebfd525595c9e53a67a6eee6ee9a84abd3485bd77555f3f56db98934fd045b0e3cdbea1062646064771b118cbdf57393e78325d4dae8e41c7e8635a7996720c5d946e110c2aede0156aaab896ade5b4c37a876caf9d469d97e6c7ab111e1267ffea2cb83fcba59750cb09d9211570f10fe7cbf0b25efde88a39c1ca0cce141116f7343769edad8bee906824938a223f49ed7d91ff1216b2010fb5e6acf5710cb2019e7b5255fb69aa062ed299fd07a810624427d246d7cc62389837565f30169fe67917739375b98198e2b72d9c026df963fc73c0fc51102f27d67b43611b35c0fb4d1a03264125df14f8ed80c901d92aa7)","a60c6c8a":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1766233\/2883347\/image4.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211208%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211208T105400Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=5cb2167fba1c4c1154a7dda81eb382b168a39b47fe471279aa662a5e9cf027f5e751d0948cc853af48ec963c25244f4b5c0d05c49aa745e09f3de0cfd00db78e3a7ffa84bb75352ec370d1bccc98e07865407043775c06c1c01126a213a83382f8bf934affb5b1ce56bfc96faa1d4812b1185a685b786ddc94eefdccc4a78c7a44df9742c279da61dacf7739c52c71356395333dc703c6cdfb8ff1ee44a492f0b4e9b0c298c82507733950f91246d64c60de5711f41db3d1665b816b93cee96b82cdb86190d91165e2914a05432ee1e59bc599fad5d721827ca3ee0f04659d708966e79375257e0a479a10575d778eeb3c4b7b9068f6d10fb62d079c3e9b84ae)","c9372641":"### Notes:\n* Many have low variation: most observations are in the \"0\" category.  \n* Some have almost no variation: about 40 or less instances (out of ~15000) in the \"1\" category.\n* Some have no variation whatsoever (soil types 7 & 15). These will be removed since they don't differentiate between instances\/forests.","2ecc4adb":"The dataset that we'll be using here is about forests. The purpose is, from what i've understood, to predict the cover type of a forest. However, the purpose of this tutorial is just to demonstrate different processing methods, so we won't be doing any predictions.","b4209939":"# Content\n### Exploration \n### Univariate Outlier Detection\n### Multivariate Outlier Detection\n### Frequency\/Count Encoding\n### Weight of Evidence Encoding\n### Different Scaling Methods\n### MICE imputation","16a12f56":"![](https:\/\/cdn.kastatic.org\/googleusercontent\/UPUY_dSWBpH3LM_ujmZAHhiFQdArEwklCUA-wOFSqBRo1Y4SFtnD5io397_Iw3YREocm_EkDPEUgKU3sDIMnZdU)","52e149ad":"## Categorical Features ??","5cd149c9":"As you can see, we're left with 14364 rows; 5% were removed.","fefacf20":"The algorithm keeps making splits until each point is isolated (or until some early-stopping criteria is met).  \nThis builds an isolation tree.  \nThen another tree is built, and another, and so on..  \nThen an average score across all trees is calculated for every point: how many splits do we need, on average, to isolate that point?  \nOutliers will have a low score since they're often isolated in just a few splits.  \nNote that the isolation forest doesn't automatically remove outliers but rather ranks points\/observations by how likely they are to be outliers.  \nYou can set a percentage that you would like to remove. For example you might want to remove the 5% most extreme points\/observations in your data.  \nIn the sklearn version of the isolation forest, you would need to set the \"contamination\" parameter to the percentage you choose.","195b76c5":"**But how about categorical features that aren't binary?**  \n**Well we can remove categorical values with \"a very small number of observations\" and leave the rest as it is**  \n**Since all categorical features in our dataset are binary, i will create an artificial feature**","aaf5010a":"Notice that were left with 13990 rows instead of 15120.  \nIf we used 1 instead of 3 we would get around 600 values only.  \nIf we used 5 we would remove almost nothing. Only the most extreme ones.","fc4d865c":"* Y is balanced so no need to worry about imbalance in the target feature.","eb13eeb0":"### Notes:\n* StandardScaler, Min-Max Scaler & RobustScaler are \"linear transformations\"; they don't change the shape of the distributions.  \n* Min-Max scaling isn't suitable for very skewed features because most values would get mapped between [0.2 , 0.5] for example.  \n* Standard Scaler might not be very suitable for skewed features as well, since it uses the mean and the std which are affected by outliers.  \n* Robust Scaler is more suited for skewed distributions since it isn't affected by outliers.  \n* Uniform and Gaussian transformations might distort dependencies between features; features that are originally dependent might become less dependent and vice-versa. So if you have high correlations\/dependencies between features then these two might not be suitable. If not, however, they could be perhaps give better results since many methods\/models could work better with normal\/uniform distributions","f466cf51":"The following table contains the zscores of every point for every feature.  \nAny row that contains a high absolute value (which *usually* means any value bigger than 3) will be removed.","231fe73b":"**Soil types 7 & 15 are not to be considered since they show no variance. All values are the same**","b2919b54":"Here, i used 1 as a threshold instead of 3.  \nThe following code creates a variable that indicates which indices ***do not*** correspond to outliers","4e80ce30":"We get which indices don't correspond to \"value5\"","1952b341":"# Univariate Outlier Detection","d982db24":"![](https:\/\/www.intechopen.com\/media\/chapter\/47833\/media\/image2.jpeg)","c09946f5":"Consider the following: Your dataset contains 2 features. One feature has values ranging from 0 to 1, whereas the other has values ranging from 5000 to 9000.  \nNow say you want to measure the \"similarity\" between two data points. This is usually done using euclidean distance.  \nThe following is the formula for the euclidean distance between two points in 2 dimensions\/features:","a5fef95c":"# Exploring the Dataset (feel free to skip)","1e92d9d7":"Separating feature names into two lists: one for the categoricals and one for the numericals.","eeb7a714":"# Multivariate Outlier Detection with Isolation Forest","57bc0f56":"### Definitions: (from right to left)  \n* Min-Max Scaling makes all features have values in [0 , 1]. It does this by subtracting the minimum value then dividing by the maximum value.  \n* Standardization subtracts the mean then divides by the standard deviation. This assures all features have an average of 0 and a standard deviation of 1.  \n* RobustScaler is the same as standardization but instead of using the mean and the standard deviation it uses the median and the IQR, which is the range between the 25th percentile and the 75th percentile. This assures that the central 50% of the values have a range of 1 and the median becomes 0. This ensures that the central 50% is between -0.5 and 0.5. You can use other percentiles if you wish.  \n* The uniform quantile transformer transforms distributions into uniform ones with a range from 0 to 1.  \n* The normal\/gaussian quantile transformer transforms distributions into gaussian ones with mean 0 and a standard deviation of 1.  ","9bed898e":"**Observations that have a value equal to \"value5\" could be considered as outliers**  \n**You're throwing away data so be think well before you do this**  \n**If these observations are significant despite their small number you might want to keep them**","4fab336b":"**There are any ways to detect the kinds of outliers, and one of them is the Isolation Forest**  \n"}}