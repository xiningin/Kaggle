{"cell_type":{"0add7dec":"code","f22c6705":"code","bd20ec4c":"code","5f0c0a8e":"code","c7b5a544":"code","e00dcfea":"code","fdf09d07":"code","76b96d97":"code","41d64336":"code","1e7f976d":"code","0a5ea1fa":"code","a0267ca4":"code","de40db19":"code","3b324510":"code","752099e4":"code","907bb272":"code","5556a96a":"code","c6d9d803":"code","7f429aa2":"code","bd1badbc":"code","199349ea":"code","c860e1f4":"code","2e668099":"code","d6af6ef7":"code","22f39d55":"code","52193698":"code","b87ed53e":"code","c9146145":"code","eef85f91":"markdown","e1a9c2b1":"markdown","d73f3063":"markdown","e85b83d4":"markdown","53906ffe":"markdown","4addfefe":"markdown","df6bf806":"markdown","99fc5ba1":"markdown","c7bfeb93":"markdown","8bf0c958":"markdown","f05322ce":"markdown","e65aced6":"markdown","004dc08e":"markdown","e7a6399b":"markdown","2fb8bd56":"markdown","ccc51d2e":"markdown","ecdd175b":"markdown","f71281d8":"markdown","3b89039c":"markdown","2dac8be1":"markdown","53c595f1":"markdown"},"source":{"0add7dec":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation","f22c6705":"news_data = pd.read_csv(\"..\/input\/news-data.csv\")\nnews_data.shape","bd20ec4c":"news_data.head()","5f0c0a8e":"NUM_SAMPLES = 12000 # The number of sample to use \nsample_df = news_data.sample(NUM_SAMPLES, replace=False).reset_index(drop=True)","c7b5a544":"sample_df.shape","e00dcfea":"sample_df.head()","fdf09d07":"cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\ndtm = cv.fit_transform(sample_df['headline_text'])","76b96d97":"dtm","41d64336":"feature_names = cv.get_feature_names()\nlen(feature_names) # show the total number of distinct words","1e7f976d":"feature_names[6500:]","0a5ea1fa":"NUM_TOPICS = 7 \nLDA_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=30, random_state=42)","a0267ca4":"LDA_model.fit(dtm)","de40db19":"len(feature_names)","3b324510":"import random \nfor index in range(15):\n    random_word_ID = random.randint(0, 6506)\n    print(cv.get_feature_names()[random_word_ID])","752099e4":"len(LDA_model.components_[0])","907bb272":"# Pick a single topic \na_topic = LDA_model.components_[0]\n\n# Get the indices that would sort this array\na_topic.argsort()","5556a96a":"# The word least representative of this topic\na_topic[597]","c6d9d803":"# The word most representative of this topic\na_topic[3598]","7f429aa2":"top_10_words_indices = a_topic.argsort()[-10:]\n\nfor i in top_10_words_indices:\n    print(cv.get_feature_names()[i])","bd1badbc":"for i, topic in enumerate(LDA_model.components_):\n    print(\"THE TOP {} WORDS FOR TOPIC #{}\".format(10, i))\n    print([cv.get_feature_names()[index] for index in topic.argsort()[-10:]])\n    print(\"\\n\")","199349ea":"final_topics = LDA_model.transform(dtm)\nfinal_topics.shape","c860e1f4":"final_topics[0]","2e668099":"final_topics[0].argmax()","d6af6ef7":"sample_df[\"Topic N\u00b0\"] = final_topics.argmax(axis=1)","22f39d55":"sample_df.head()","52193698":"import pyLDAvis.sklearn","b87ed53e":"pyLDAvis.enable_notebook() # To enable the visualization on the notebook","c9146145":"panel = pyLDAvis.sklearn.prepare(LDA_model, dtm, cv, mds='tsne') # Create the panel for the visualization\npanel","eef85f91":"## LDA     \nFrom our DTM matrix, we can now build our LDA to extract topics from the underlined texts. The number of topic to be extracted is a hyperparameter, so we do not know it a a glance. In our case, we will be using 7 topics.   \nLDA is an iterative algorithm, we will have 30 iterations in our case, but the default value is 10.  ","e1a9c2b1":"### Attach Discovered Topic Labels to Original News","d73f3063":"We are not interested in the **publish_data** column, since we will only be using **headline_text** data.    \n\n**`max_df`**` : float in range [0.0, 1.0] or int, default=1.0`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\n**`min_df`**` : float in range [0.0, 1.0] or int, default=1`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.     \n\n\nBe defining the **CountVectorizer** object as below, we ignore:   \n- all terms that occur over 95% times in our document corpus. We say in this case that the terms occuring more than this threshold are not significant, most of them are  `stopwords`.   \n\n- all the terms that occur fewer than twice in the entire corpus.  ","e85b83d4":"We can observe that our Document X Term Matrix (dtm) has:  \n- 12000 documents, and.  \n- 6506 distinct words   \n\nWe can also get all those words using the `get_feature_names()` function","53906ffe":"### Combination with the original data     \nLet's create a new column called **Topic N\u00b0** that will correspond to the topic value to which each document belongs to.","4addfefe":"`Zoumana KEITA, Data Scientist`\n\n# Latent Dirichlet Allocation \/ Analysis (LDA)     \n\n**Note**: you will need to unzip the data from the `data` folder in order to follow this notebook.  \n\nThis is a probabilistic model used to find clusters assigments for documents.  \nIt uses two probability values to cluster documents: \n- **P(word | topic)**: the probability that a particular word is associated with a particular topic. This first set of probability is also considered as the **Word X Topic** matrix.  \n- **P(topics | documents)**: the topics associated with documents. This second set of probability is considered as **Topics X Documents** matrix.   \nThese probability values are calculated for all words, topics and documents.    \n\nFor this tutorial, we will be using the dataset of the Australian Broadcasting Corporation, available on kaggle:   \nhttps:\/\/www.kaggle.com\/therohk\/million-headlines ","df6bf806":"## Some Visualization       \nWe will be using the `pyldavis` module to visualize the topics associated to our documents.   ","99fc5ba1":"**If you liked this kernel, please upvote. I am also open to suggestions**","c7bfeb93":"This looks like Government Article. Let's have a look at all the 7 topics found. ","8bf0c958":"Let's have a look at some of the features that have been extracted from the documents.  ","f05322ce":"## Preprocessing.    ","e65aced6":"Let have a look at the top 10 words for the topic we previously took","004dc08e":"### Top Words Per Topic","e7a6399b":"## Load the Dataset","2fb8bd56":"According to our LDA model:   \n- the first document belongs to 4th topic.  \n- the second document belongs to 4th topic. \n- the third document belongs to 6th topic.  \netc.   ","ccc51d2e":"This value (4) means that our LDA model thinks that the first document belongs to the 4th topic.","ecdd175b":"**final_topics** contains, for each of our 12000 documents, the probability score of how likely a document belongs to each of the 7 topics.  This is a Document X Topics matrix. \nFor example, below is the probability values for the first document.","f71281d8":"### Show Stored Words.   \nLet's randomnly have a look at some words of that have been stored.  ","3b89039c":"## Import Useful Libraries ","2dac8be1":"### Some Comments On The Graphic     \n\n- By selecting a particular term on the right, we can see which topic(s) it belongs.    \n- Vice-versa, by choosing a topic on the left, we can see all the terms, from most to least relevant term.  ","53c595f1":"Our data have over a million of records, and there are two columns: \n- the date a particular headline have been published.  \n- the actual headline.   \nBy looking at the first 5 rows, we can see that we don't have the topic of the headline text! So, we will use LDA to attempt to figure out clusters of the news.   \n**A million** of record, that is a lot of data. To do so, we will use only **12000** records to make the computation faster.   "}}