{"cell_type":{"b9bd8701":"code","7571a2b5":"code","e8778baf":"code","794d55bb":"code","d922f325":"code","8d529682":"code","10437175":"code","f834f269":"code","7aeff5d1":"code","9a39b522":"code","1af81909":"code","c1dc8ffd":"code","2754f3f6":"code","550167d5":"code","63ccf503":"code","857ee86f":"code","d537e238":"code","7adc6e5f":"code","e70f83f6":"code","2a382c3f":"code","0d2890ab":"code","9c7d9171":"code","41dcde07":"code","a9b53633":"code","9691a793":"code","0bc6ba84":"code","daeb7365":"code","a362b9a4":"code","e911992c":"code","fe988906":"code","baf01437":"code","079aedd9":"code","91f8c370":"code","08b0e69d":"code","e0777944":"code","40136cce":"code","81480673":"code","258c358b":"code","59666fc3":"code","03ededa8":"code","d198842f":"code","c1899d36":"code","29b36d48":"code","6ee2253e":"code","cdce7adc":"code","20261342":"code","0ca23124":"code","ab9e4ab2":"code","6542c9a1":"code","db60a4a4":"code","3bae4161":"code","af4f6b0b":"code","c124161e":"code","636ce8e6":"code","4196796c":"markdown","960d09c8":"markdown","ca7b8d34":"markdown"},"source":{"b9bd8701":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"\/kaggle\/input\/tgs-salt-identification-challenge\"))\n#os.getcwd()","7571a2b5":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nfrom pathlib import Path\nimport json\ntorch.cuda.set_device(0)","e8778baf":"MASKS_FN = 'train.csv'\nTRAIN_DN = Path('train\/images\/')\nMASKS_DN = Path('train\/masks\/')\nTEST = Path('test\/images\/')\n\nPATH = Path('\/kaggle\/input\/tgs-salt-identification-challenge\/')\nPATH128 = Path('\/tmp\/128\/')\nTMP = Path('\/tmp\/')\nMODEL = Path('\/tmp\/model\/')\n# PRETRAINED = Path('\/kaggle\/input\/is-there-salt-resnet34\/model\/resnet34_issalt.h5')\nseg = pd.read_csv(PATH\/MASKS_FN).set_index('id')\nseg.head()\n\nsz = 128\nbs = 64\nnw = 4","794d55bb":"train_names_png = [TRAIN_DN\/f for f in os.listdir(PATH\/TRAIN_DN)]\ntrain_names = list(seg.index.values)\nmasks_names_png = [MASKS_DN\/f for f in os.listdir(PATH\/MASKS_DN)]\ntest_names_png = [TEST\/f for f in os.listdir(PATH\/TEST)]","d922f325":"train_names_png[0], masks_names_png[0], test_names_png[0]","8d529682":"TMP.mkdir(exist_ok=True)\nPATH128.mkdir(exist_ok=True)\n(PATH128\/'train').mkdir(exist_ok=True)\n(PATH128\/'test').mkdir(exist_ok=True)\n(PATH128\/MASKS_DN).mkdir(exist_ok=True)\n(PATH128\/TRAIN_DN).mkdir(exist_ok=True)\n(PATH128\/TEST).mkdir(exist_ok=True)","10437175":"def resize_mask(fn):\n    Image.open(PATH\/fn).resize((128,128)).save(PATH128\/fn)","f834f269":"with ThreadPoolExecutor(4) as e: e.map(resize_mask, train_names_png)","7aeff5d1":"with ThreadPoolExecutor(4) as e: e.map(resize_mask, masks_names_png)","9a39b522":"with ThreadPoolExecutor(4) as e: e.map(resize_mask, test_names_png)","1af81909":"PATH = PATH128 #just for sanity","c1dc8ffd":"def show_img(im, figsize=None, ax=None, alpha=None):\n    if not ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, alpha=alpha)\n    ax.set_axis_off()\n    return ax","2754f3f6":"im, mk = Image.open(PATH\/train_names_png[0]), Image.open(PATH\/masks_names_png[0])","550167d5":"ax = show_img(im);\nshow_img(mk, alpha=0.3);","63ccf503":"class CustomDataset(FilesDataset):\n    def __init__(self, fnames, y, transform, path):\n        self.y=y\n        assert(len(fnames)==len(y))\n        super().__init__(fnames, transform, path)\n        \n    def get_x(self, i): \n        return open_image(os.path.join(self.path,self.fnames[i]))\n    def get_y(self, i): \n        return open_image(os.path.join(self.path,self.y[i]))\n    def get_c(self): return 0","857ee86f":"def dice(pred, targs):\n    pred = (pred>0).float()\n    return 2. * (pred*targs).sum() \/ (pred+targs).sum()\n\ndef IoU(pred, targs):\n    pred = (pred>0).float()\n    intersection = (pred*targs).sum()\n    return intersection \/ ((pred+targs).sum() - intersection + 1.0)","d537e238":"x_names = [f'{x}.png' for x in train_names]\nx_names_path = np.array([str(TRAIN_DN\/x) for x in x_names])\ny_names = [x for x in x_names]\ny_names_path = np.array([str(MASKS_DN\/x) for x in x_names])","7adc6e5f":"im = open_image(str(PATH\/y_names_path[1]))","e70f83f6":"im[:,:,0][:,:,None].shape","2a382c3f":"# val_idxs = list(range(200))\nval_idxs=list(range(20))\n((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names_path, y_names_path)\ntest_x = np.array(test_names_png)","0d2890ab":"trn_x, trn_y, test_x","9c7d9171":"aug_tfms = [RandomRotate(4, tfm_y=TfmType.CLASS),\n            RandomFlip(tfm_y=TfmType.CLASS),\n            RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]\n# aug_tfms = []","41dcde07":"tfms = tfms_from_model(resnet34, sz=128, crop_type=CropType.NO, tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)\ndatasets = ImageData.get_ds(CustomDataset, (trn_x,trn_y), (val_x,val_y), tfms, (test_x, test_x), path=PATH)\nmd = ImageData(PATH, datasets, bs=16, num_workers=nw, classes=None)\ndenorm = md.trn_ds.denorm","a9b53633":"x,y = next(iter(md.trn_dl))\nx.shape, y.shape","9691a793":"from fastai.models.unet import *","0bc6ba84":"def get_encoder(f, cut):\n    base_model = (cut_model(f(True), cut))\n    return nn.Sequential(*base_model)","daeb7365":"# Wrap everything nicely\nclass UpsampleModel():\n    def __init__(self, model, cut_lr, name='upsample'):\n        self.model,self.name, self.cut_lr = model, name, cut_lr\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.encoder), [self.cut_lr]))\n        return lgs + [children(self.model)[1:]]","a362b9a4":"class DynamicUnet2(nn.Module):\n    \"\"\"\n    A dynamic implementation of Unet architecture, because calculating connections\n    and channels suck!. When an encoder is passed, this network will\n    automatically construct a decoder after the first single forward pass for any\n    given encoder architecture.\n\n    Decoder part is heavily based on the original Unet paper:\n    https:\/\/arxiv.org\/abs\/1505.04597.\n\n    Inputs:\n        encoder(nn.Module): Preferably a pretrained model, such as VGG or ResNet\n        last (bool): Whether to concat only last activation just before a size change\n        n_classes (int): Number of classes to output in final step of decoder\n\n    Important Note: If architecture directly reduces the dimension of an image as soon as the\n    first forward pass then output size will not be same as the input size, e.g. ResNet.\n    In order to resolve this problem architecture will add an additional extra conv transpose\n    layer. Also, currently Dynamic Unet expects size change to be H,W -> H\/2, W\/2. This is\n    not a problem for state-of-the-art architectures as they follow this pattern but it should\n    be changed for custom encoders that might have a different size decay.\n    \"\"\"\n\n    def __init__(self, encoder, last=True, n_classes=3):\n        super().__init__()\n        self.encoder = encoder\n        self.n_children = len(list(encoder.children()))\n        self.sfs = [SaveFeatures(encoder[i]) for i in range(self.n_children)]\n        self.last = last\n        self.n_classes = n_classes\n\n    def forward(self, x):\n        dtype = x.data.type()\n\n        # get imsize\n        imsize = x.size()[-2:]\n\n        # encoder output\n        x = F.relu(self.encoder(x))\n\n        # initialize sfs_idxs, sfs_szs, middle_in_c and middle_conv only once\n        if not hasattr(self, 'middle_conv'):\n            self.sfs_szs = [sfs_feats.features.size() for sfs_feats in self.sfs]\n            self.sfs_idxs = get_sfs_idxs(self.sfs, self.last)\n            middle_in_c = self.sfs_szs[-1][1]\n            middle_conv = nn.Sequential(*conv_bn_relu(middle_in_c, middle_in_c * 2, 3, 1, 1),\n                                        *conv_bn_relu(middle_in_c * 2, middle_in_c, 3, 1, 1))\n            self.middle_conv = middle_conv.type(dtype)\n\n        # middle conv\n        x = self.middle_conv(x)\n\n        # initialize upmodel, extra_block and 1x1 final conv\n        if not hasattr(self, 'upmodel'):\n            x_copy = Variable(x.data, requires_grad=False)\n            upmodel = []\n            for idx in self.sfs_idxs[::-1]:\n                up_in_c, x_in_c = int(x_copy.size()[1]), int(self.sfs_szs[idx][1])\n                unet_block = UnetBlock(up_in_c, x_in_c).type(dtype)\n                upmodel.append(unet_block)\n                x_copy = unet_block(x_copy, self.sfs[idx].features)\n                self.upmodel = nn.Sequential(*upmodel)\n\n            if imsize != self.sfs_szs[0][-2:]:\n                extra_in_c = self.upmodel[-1].conv2.out_channels\n                self.extra_block = nn.ConvTranspose2d(extra_in_c, extra_in_c, 2, 2).type(dtype)\n\n            final_in_c = self.upmodel[-1].conv2.out_channels\n            self.final_conv = nn.Conv2d(final_in_c, self.n_classes, 1).type(dtype)\n\n        # run upsample\n        for block, idx in zip(self.upmodel, self.sfs_idxs[::-1]):\n            x = block(x, self.sfs[idx].features)\n        if hasattr(self, 'extra_block'):\n            x = self.extra_block(x)\n\n        out = self.final_conv(x)\n        if self.n_classes == 1:\n            out = out.squeeze(1)\n        return out","e911992c":"f = vgg16\ncut,lr_cut = model_meta[f]\ncut,lr_cut","fe988906":"encoder= get_encoder(f, 30)\n# m_base = get_base()\nm = DynamicUnet2(encoder, n_classes=1).cuda()\n# m = to_gpu(Unet34(m_base))\n# models = UnetModel(m)","baf01437":"inp = torch.ones(1, 3, 128, 128)\nout = m(V(inp))","079aedd9":"out.shape","91f8c370":"models = UpsampleModel(m, cut_lr=9)","08b0e69d":"learn = ConvLearner(md, models, tmp_name=TMP, models_name=MODEL)\nlearn.opt_fn=optim.Adam\nlearn.crit=nn.BCEWithLogitsLoss()\nlearn.metrics=[accuracy_thresh(0.5),dice, IoU]","e0777944":"learn.freeze_to(1)","40136cce":"learn.lr_find()\nlearn.sched.plot()","81480673":"lr=1e-2\nwd=1e-4\n\nlrs = np.array([lr\/100,lr\/100,lr\/10])","258c358b":"learn.fit(lr,1,wds=wd,cycle_len=20,use_clr=(5,8))","59666fc3":"learn.save('128unet1')","03ededa8":"learn.load('128unet1')","d198842f":"learn.unfreeze()\nlearn.bn_freeze(True)","c1899d36":"learn.fit(lrs\/10, 1, wds=wd, cycle_len=10,use_clr=(20,10))","29b36d48":"# learn.fit(lrs\/3, 1, wds=wd, cycle_len=10,use_clr=(8,5))","6ee2253e":"py,ay = learn.predict_with_targs()","cdce7adc":"fig, axes = plt.subplots(5, 4, figsize=(12, 12))\nfor i,ax in enumerate(axes.flat):\n    ax = show_img((ay[i]), ax=ax)\nplt.tight_layout(pad=0.1)","20261342":"fig, axes = plt.subplots(5, 4, figsize=(12, 12))\nfor i,ax in enumerate(axes.flat):\n    ax = show_img((py[i]>0), ax=ax)\nplt.tight_layout(pad=0.1)","0ca23124":"# Predict on Test data\nout = learn.predict(is_test=True)\nout.shape","ab9e4ab2":"fig, axes = plt.subplots(6, 6, figsize=(12, 12))\nfor i,ax in enumerate(axes.flat):\n#     ax = show_img(Image.open(PATH\/test_names_png[i]), ax=ax)\n    show_img(out[i]>0, ax=ax, alpha=0.8)\nplt.tight_layout(pad=0.1)","6542c9a1":"def rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","db60a4a4":"tmp_list = []\nname_list = []\nfor i in range(18000):\n    img = cv2.resize(out[i,:,:], dsize=(101,101), interpolation = cv2.INTER_CUBIC)\n    tmp_list.append(rle_encode(img>0))\n    name_list.append(test_names_png[i].name[0:-4])","3bae4161":"sub = pd.DataFrame(list(zip(name_list, tmp_list)), columns = ['id', 'rle_mask'])","af4f6b0b":"sub.head()","c124161e":"sub.to_csv('submission.csv', index=False)","636ce8e6":"#last version 1PM - 03\/10\/","4196796c":"# Predict","960d09c8":"# VGG - UNET","ca7b8d34":"Checking FastAI"}}