{"cell_type":{"ab8b4278":"code","0f954cf4":"code","91b659b9":"code","e311cf4b":"code","7208f876":"code","0831ff93":"code","0a86567b":"code","c3ab90ac":"code","ddaeb74e":"code","2e1dba48":"code","55d73651":"code","1174efc4":"code","a00054ec":"code","01082791":"code","a77b344c":"code","67b551f2":"code","bd30202e":"code","55bcc3e4":"code","d3b640c1":"code","f9b0e1f4":"markdown"},"source":{"ab8b4278":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0f954cf4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","91b659b9":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.DataFrame(columns=['Id','winPlacePerc'])\nsubmission.Id = df_test.Id\ndf_train.head()","e311cf4b":"df_train.shape","7208f876":"df_train.info()","0831ff93":"df_train.describe()","0a86567b":"df_train.drop('Id',axis=1,inplace=True)\ndf_test.drop('Id',axis=1,inplace=True)","c3ab90ac":"# for col in df_train.columns:\n#     sns.distplot(df_train[col])\n#     plt.show()","ddaeb74e":"# df_train_corr = df_train.corr().abs()","2e1dba48":"# plt.figure(figsize=(20,15))\n# sns.heatmap(df_train_corr,annot=True)\n# plt.show()","55d73651":"df_train.drop(['numGroups','killPlace', 'roadKills','swimDistance', 'teamKills', 'vehicleDestroys'],axis=1,inplace=True)\ndf_test.drop(['numGroups','killPlace', 'roadKills','swimDistance', 'teamKills', 'vehicleDestroys'],axis=1,inplace=True)\n# df_train.drop(['numGroups','groupId','matchId'],axis=1,inplace=True)\n# df_test.drop(['numGroups','groupId','matchId'],axis=1,inplace=True)\n\ndf_train_mean = df_train.groupby(['matchId','groupId']).mean().reset_index()\ndf_test_mean = df_test.groupby(['matchId','groupId']).mean().reset_index()\n\ndf_train_min = df_train.groupby(['matchId','groupId']).min().reset_index()\ndf_test_min = df_test.groupby(['matchId','groupId']).mean().reset_index()\n\ndf_train_max = df_train.groupby(['matchId','groupId']).max().reset_index()\ndf_test_max = df_test.groupby(['matchId','groupId']).max().reset_index()\n\ndf_train = pd.merge(df_train,df_train_mean,suffixes=['','_mean'],how='left',on=['matchId','groupId'])\ndf_test = pd.merge(df_test,df_test_mean,suffixes=['','_mean'],how='left',on=['matchId','groupId'])\ndf_train = pd.merge(df_train,df_train_min,suffixes=['','_min'],how='left',on=['matchId','groupId'])\ndf_test = pd.merge(df_test,df_test_min,suffixes=['','_min'],how='left',on=['matchId','groupId'])\ndf_train = pd.merge(df_train,df_train_max,suffixes=['','_max'],how='left',on=['matchId','groupId'])\ndf_test = pd.merge(df_test,df_test_max,suffixes=['','_max'],how='left',on=['matchId','groupId'])\n\ncol_list = []\nfor col in df_test.columns:\n    if '_' in col:\n        col_list.append(col)\n        \ny = df_train.winPlacePerc\nX = df_train[col_list]\n\ndf_test = df_test[col_list]\n\ndel df_train,df_train_mean,df_test_mean,df_test_min,df_train_min,df_train_max,df_test_max","1174efc4":"from sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error as MAE\nx_train,x_test,y_train,y_test = train_test_split(X,y,random_state=64,test_size=0.2)\n\n# forest = RandomForestRegressor()\n# forest.fit(x_train,y_train)\n# y_predict_ = forest.transform(x_train)\n# y_predict = forest.transform(s_test)\n# print('forest_MAE in train : {}'.format(MAE(y_train,y_predict_)))\n# print('forest_MAE in test :{}'.format(MAE(y_test,y_predict)))\n","a00054ec":"from sklearn.feature_selection import SelectKBest,f_classif\n\n# selector = SelectKBest(f_classif,k=10)\n# selector.fit(x_train,y_train)\n# score_p = selector.pvalues_\n# score_s = selector.scores_","01082791":"# plt.figure(figsize=(18,9))\n# plt.subplot(211)\n# plt.bar(range(len(score_p)),-np.log(score_p))\n# plt.xticks(range(len(score_p)),X.columns,rotation=45)\n# plt.title('')\n# plt.subplot(212)\n# plt.bar(range(len(score_s)),score_s)\n# plt.xticks(range(len(score_s)),X.columns,rotation=45)\n# plt.show()","a77b344c":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X,y)\ny_predict_ = lr.predict(x_train)\ny_predict = lr.predict(x_test)\n\nprint('forest_MAE in train : {}'.format(MAE(y_train,y_predict_)))\nprint('forest_MAE in test :{}'.format(MAE(y_test,y_predict)))","67b551f2":"from sklearn.linear_model import LassoCV\nfrom lightgbm import LGBMRegressor\n\nlgb1 = LGBMRegressor(max_depth=6,learning_rate=0.2)\nlgb1.fit(X,y)\ny_predict_ = lgb1.predict(x_train)\ny_predict = lgb1.predict(x_test)\n\nprint(\"MAE in train: {}\".format(MAE(y_train,y_predict_)))\nprint('MAE in test : {}'.format(MAE(y_test,y_predict)))","bd30202e":"submission.winPlacePerc = lgb1.predict(df_test)\nsubmission.to_csv('samble_submission.csv',index=False)","55bcc3e4":"# from xgboost import XGBRegressor\n\n# xgb1 = XGBRegressor(max_depth=8,learning_rate=0.05)\n# xgb1.fit(x_train,y_train)\n# y_predict_ = xgb1.predict(x_train)\n# y_predict = xgb1.predict(x_test)\n# print('MAE in train :{}'.format(MAE(y_train,y_predict_)))\n# print('MAE in test  :{}'.format(MAE(y_test,y_predict)))\n\n","d3b640c1":"params = {\n          'boosting_type': 'gbdt', \n          'objective':'regression',\n          'silent': 0,\n          'learning_rate': 0.1, \n          'max_depth': 6,\n          'max_bin': 127, \n          'subsample_for_bin': 50000,\n          'subsample': 0.8, \n          'colsample_bytree': 0.8, \n          'min_child_weight': 1, \n}\nimport lightgbm as lgb\ndef modelfit(params,model,x_train,y_train,early_stopping_rounds=10):\n    \n    lgb_params = params.copy()\n    \n    lgb_train = lgb.Dataset(x_train,y_train,silent=False)\n    \n    cv_result = lgb.cv(\n        lgb_params,\n        lgb_train,\n        num_boost_round=10000,\n        nfold=10,\n        stratified=False,\n        shuffle=True,\n        metrics='mean_absolute_error',\n        early_stopping_rounds=early_stopping_rounds,\n    )\n    cv.to_csv('cv_result.csv')\n    \n# modelfit(params,lgb1,x_train,y_train)","f9b0e1f4":"*\u518d\u67e5\u770btest\u6570\u636e\u96c6\u4e0a\u6570\u636e\uff0c\u9a8c\u8bc1\u6570\u636e\u5b8c\u6574\u6027\uff0c\u6b64\u5904\u7701\u7565*"}}