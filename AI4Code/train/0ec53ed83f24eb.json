{"cell_type":{"a38bed2f":"code","2056e507":"code","c41b22b4":"code","456bbeaf":"code","533778d3":"code","da756db0":"code","5dc3c14b":"code","3ffb0910":"code","2fe27b85":"code","fde4d4bb":"code","66e1f961":"code","347c77bd":"code","794d5433":"code","05e83e00":"code","98e1aaa1":"code","edb211d1":"code","970504cf":"code","1ac7116e":"code","2fde75e6":"code","97d7706b":"code","136d1f4a":"code","98d7006e":"code","7b750192":"code","678f0f6e":"code","bc528032":"code","db7a2cb4":"code","28707b91":"code","f5f4d55f":"code","f20bd9b8":"code","b92d8f77":"code","2188403d":"code","84216d03":"code","bd2505a1":"code","53fa9bb3":"code","0f45d9a5":"code","436de37f":"code","60f2d1da":"code","07737554":"code","7118ef43":"code","1d70ed96":"code","ee8ac300":"code","6072aaaa":"code","d752f004":"code","6197909d":"code","dd1e969d":"code","bc269f38":"code","b8d9ebd2":"code","39f4c5cd":"code","b7ba113f":"code","50828e57":"code","b9a4d3c5":"code","ef1a05ce":"code","96656694":"code","e9df0c12":"code","affc6f13":"code","5539a789":"code","d6db1cd0":"markdown","557b9bef":"markdown","3b69b4a8":"markdown","eef50850":"markdown","ab9b9743":"markdown","db059489":"markdown","f58a1d14":"markdown","77dcc1a2":"markdown","6038cbca":"markdown","f9452753":"markdown","f8d1714d":"markdown","7122cdbb":"markdown","a761e09f":"markdown"},"source":{"a38bed2f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","2056e507":"dataset=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndataset.head()","c41b22b4":"dataset.isnull().sum()","456bbeaf":"dataset.shape","533778d3":"dataset.describe()","da756db0":"dataset.info()","5dc3c14b":"dataset=dataset.drop('Ticket',axis=1)","3ffb0910":"dataset['Cabin']=dataset['Cabin'].fillna('U')\ndataset['Cabin']=dataset.Cabin.apply(lambda c:c[0])\ndataset.head()","2fe27b85":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=dataset,palette='RdBu_r')","fde4d4bb":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=dataset,palette='RdBu_r')","66e1f961":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=dataset,palette='rainbow')","347c77bd":"sns.distplot(dataset['Age'].dropna(),kde=False,color='darkred',bins=30)","794d5433":"sns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='viridis')","05e83e00":"dataset.head()","98e1aaa1":"dataset['Title']=dataset.Name.apply(lambda name:name.split(',')[1].split('.')[0].strip())","edb211d1":"dataset['Title'].value_counts()","970504cf":"group=dataset.groupby(['Sex','Pclass','Title'])\ngroup.Age.median()","1ac7116e":"#filling the age with the median values according to the salutation\ndataset.Age=group.Age.apply(lambda x:x.fillna(x.median()))","2fde75e6":"sns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='viridis')","97d7706b":"#remove passenger id as it is of no use\ndataset.drop('PassengerId',inplace=True,axis=1)","136d1f4a":"dataset.head()","98d7006e":"# here i will fill the missing values in embarked with the most frequent values\n# in it.  \n\nemb_most=dataset.Embarked.value_counts().index[0]\ndataset.Embarked=dataset.Embarked.fillna(emb_most)\nsns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='viridis')","7b750192":"#removing the cabin section\ndataset.drop('Cabin',inplace=True,axis=1)\n\n# encoding categorical variable and removing one of the dummies variable to avoid\n# dummies variable trap.\nembark=pd.get_dummies(dataset['Embarked'],drop_first=True)\nsex=pd.get_dummies(dataset['Sex'],drop_first=True)\n\n# now we will concatenate the dummies variable with the original dataset.\ndataset.drop(['Sex','Embarked'],axis=1,inplace=True)\ndataset=pd.concat([dataset,sex,embark],axis=1)\n\n#Similarly we will encode pclass feature\npclass=pd.get_dummies(dataset['Pclass'],drop_first=True)\ndataset.drop(['Pclass'],axis=1,inplace=True)\ndataset=pd.concat([dataset,pclass],axis=1)\n\n#renaming the dummies variable for passenger class for simplicity\ndataset=dataset.rename(columns={2:'Pclass2',3:'Pclass3'})","678f0f6e":"dataset.head()","bc528032":"#Now we will analyze the survival rate of passengers according to their Family size\n# including siblings(SibSp) and Parent-Child(Parch)\ndataset.groupby(['SibSp','Parch']).Survived.median()","db7a2cb4":"# Creating new feature of Family size\ndataset['Family_Size']=dataset.SibSp+dataset.Parch+1\ndataset[['Family_Size','Survived']].groupby('Family_Size').Survived.mean().sort_values(ascending=False)","28707b91":"# Now with the help of heatmap we will see correlations between the features\nplt.figure(figsize=(10,10))\nsns.heatmap(dataset.corr(),annot=True)","f5f4d55f":"dataset.drop(['Title','Name','SibSp','Parch'],axis=1,inplace=True)","f20bd9b8":"dataset.head()","b92d8f77":"testdata=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntestdata.head()","2188403d":"testdata.isnull().sum()","84216d03":"testdata.shape","bd2505a1":"testdata=testdata.drop('Cabin',axis=1)","53fa9bb3":"group=testdata.groupby(['Pclass'])\ngroup.Fare.median()","0f45d9a5":"testdata.Fare=group.Fare.apply(lambda x:x.fillna(x.median()))","436de37f":"plt.figure(figsize=(12,8))\nsns.heatmap(testdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')","60f2d1da":"testdata['Title']=testdata.Name.apply(lambda name:name.split(',')[1].split('.')[0].strip())\ngroup1=testdata.groupby(['Sex','Pclass','Title'])\ngroup1.Age.median()\ntestdata.Age=group1.Age.apply(lambda x:x.fillna(x.median()))\ntestdata['Title'].value_counts()","07737554":"testdata[testdata['Title'].isnull()]","7118ef43":"grp=testdata.groupby(['Pclass','Sex'])\ngrp.Age.median()\ntestdata.Age=grp.Age.apply(lambda x:x.fillna(x.median()))","1d70ed96":"testdata.isnull().sum()","ee8ac300":"embark=pd.get_dummies(testdata['Embarked'],drop_first=True)\nsex=pd.get_dummies(testdata['Sex'],drop_first=True)\ntestdata.drop(['Sex','Embarked'],axis=1,inplace=True)\ntestdata=pd.concat([testdata,sex,embark],axis=1)\npclass=pd.get_dummies(testdata['Pclass'],drop_first=True)\ntestdata.drop(['Pclass'],axis=1,inplace=True)\ntestdata=pd.concat([testdata,pclass],axis=1)\ntestdata=testdata.rename(columns={2:'Pclass2',3:'Pclass3'})","6072aaaa":"testdata['Family Size']=testdata.SibSp+testdata.Parch+1\ntestdata.head()","d752f004":"testdata.drop(['Title','Name','Ticket','SibSp','Parch'],axis=1,inplace=True)\ntestdata.head()","6197909d":"from sklearn.model_selection import train_test_split as t\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nX=dataset.drop('Survived',axis=1)\ny=dataset['Survived']","dd1e969d":"#first we will divide our train dataset to further train and test dataset to check its accuracy\nX_train,X_test,y_train,y_test=t(X,y,test_size=.2,random_state=0)","bc269f38":"#defining parameters of Random forrest classifier for hyperParameter tuning.\nforrest_params = { 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 10],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'bootstrap': [True, False],     \n    \n                 }","b8d9ebd2":"forrest = RandomForestClassifier()\nforest_cv = GridSearchCV(estimator=forrest,param_grid=forrest_params, cv=5) \nforest_cv.fit(X_train, y_train)","39f4c5cd":"y_pred = forest_cv.predict(X_test)","b7ba113f":"forest_cv.score(X_test,y_test)","50828e57":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","b9a4d3c5":"confusion_matrix(y_test,y_pred)","ef1a05ce":"print(classification_report(y_test,y_pred))","96656694":"forest_cv.fit(X,y)\ny_pred=forest_cv.predict(X)\nforest_cv.score(X,y)","e9df0c12":"#now at last we will predict on test data set\ny_pred=forest_cv.predict(testdata.drop('PassengerId',axis=1))","affc6f13":"data=pd.DataFrame({'PassengerId':testdata['PassengerId'],'Survived':y_pred})","5539a789":"data.to_csv('submission.csv',index=False)","d6db1cd0":"I got a accuracy of .81818 on a test dataset and i am trying to improve it by doing more exploratory data analysis.\nPlease be free to share some suggestions if possible.\nThank you!","557b9bef":"# Now we will similarly handle the test data set","3b69b4a8":"Passenger class is important feature as we can see that lower class \npassengers have more fatalities than higher class passengers","eef50850":"Name can be handy in deciding the proper values to be filled in the age section \nso i decided to extract the salutation from the name and compute their median age","ab9b9743":"Most of the missing values are there in cabin section so we can drop it for now\nbut we have some missing values in age section so we have to replace it with proper \nvalues","db059489":"We can see from above that the one with large family size has less survival rate","f58a1d14":"Most of the passengers are young and ranges from 20-30 years of age","77dcc1a2":"# Now we will train our Machine learning model on training dataset and fit it over test data set to predict the survival using Random Forest Classifier.","6038cbca":"For simplicty i m going to define a new feature called Family Size to analyze \nit with ease","f9452753":"Now we will fit it over all train dataset","f8d1714d":"Now we wil use heatmap to account for the missing values","7122cdbb":"From above plot we can conclude that females have survived more than the males \n","a761e09f":"From the above correlation matrix we can see that  family size have high correlation with the sibling and Parch which is obvious so we will drop it."}}