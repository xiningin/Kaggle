{"cell_type":{"d5196f80":"code","5c7d92ca":"code","903025b8":"code","83a5fcfd":"code","287065f1":"code","ce15b592":"code","3f845bc2":"code","0cec605b":"code","b05213d8":"code","e681379d":"code","c19f4926":"code","6c374713":"code","3d79e8c3":"code","4c53a7a3":"code","78a52837":"code","f2ed1a75":"code","7d6fc70a":"code","63006c82":"code","e16df5a8":"code","de1bd6c9":"code","16f78d39":"code","4799976b":"code","7416aded":"code","9b37375e":"code","16369ea3":"code","10757fe6":"code","e807caf5":"code","a58455cb":"code","803add61":"code","f8e4d95a":"code","7e91501f":"code","dd1fc0cf":"code","eef752f9":"markdown","1143987a":"markdown","8f78deeb":"markdown","0c6e6974":"markdown","0118da7d":"markdown","af4f2c67":"markdown","c9a40ce8":"markdown","b6f94fa6":"markdown","562a9352":"markdown","795ac028":"markdown","c3ce4acc":"markdown","f2f6e0ca":"markdown"},"source":{"d5196f80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pylab as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5c7d92ca":"train = pd.read_csv(\"..\/input\/train\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test\/test.csv\")\ncolor_labels = pd.read_csv(\"..\/input\/color_labels.csv\")\nbreed_labels = pd.read_csv(\"..\/input\/breed_labels.csv\")\nstate_labels = pd.read_csv(\"..\/input\/state_labels.csv\")\ntrain.describe()","903025b8":"test.sample(5)","83a5fcfd":"train_simple = pd.read_csv(\"..\/input\/train\/train.csv\")\ntrain_simple.drop(['Name', 'RescuerID', 'Description',\"PetID\"], axis=1, inplace=True)\ntest.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\ntrain.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)","287065f1":"train_simple.sample(5)","ce15b592":"test.sample(5)","3f845bc2":"train_simple[\"NewState\"] = train_simple[\"State\"]-41323\ntrain_simple.drop([\"State\"],inplace = True,axis=1)","0cec605b":"test[\"NewState\"] = test[\"State\"]-41323\ntest.drop([\"State\"],inplace = True,axis=1)","b05213d8":"train_simple.sample(3)","e681379d":"train_simple[\"Mixed\"] = 0\ntest[\"Mixed\"] = 0","c19f4926":"train_simple.sample(5)","6c374713":"indexer = 0 \nfor x in train_simple[\"Breed2\"]:\n    if x > 0:\n        train_simple.loc[[indexer],\"Mixed\"] = 1\n        indexer +=1","3d79e8c3":"indexer = 0 \nfor x in test[\"Breed2\"]:\n    if x > 0:\n        test.loc[[indexer],\"Mixed\"] = 1\n        indexer +=1","4c53a7a3":"train_simple[\"Exp\"] = 0\nfor x in range(len(train[\"PhotoAmt\"])):\n    train_simple.loc[[x],\"Exp\"] = train_simple.loc[[x],\"PhotoAmt\"] + train_simple.loc[[x],\"VideoAmt\"]\ntrain_simple.drop([\"PhotoAmt\",\"VideoAmt\"],inplace=True,axis=1)","78a52837":"test[\"Exp\"] = 0\nfor x in range(len(test[\"PhotoAmt\"])):\n    test.loc[[x],\"Exp\"] = test.loc[[x],\"PhotoAmt\"] + test.loc[[x],\"VideoAmt\"]\ntest.drop([\"PhotoAmt\",\"VideoAmt\"],inplace=True,axis=1)","f2ed1a75":"indexer = 0 \nfor x in train_simple[\"Fee\"]:\n    if x > 0:\n        train_simple.loc[[indexer],\"Fee\"] = 1\n        indexer +=1\n    else:\n        train_simple.loc[[indexer],\"Fee\"] = 0","7d6fc70a":"indexer = 0 \nfor x in test[\"Fee\"]:\n    if x > 0:\n        test.loc[[indexer],\"Fee\"] = 1\n        indexer += 1\n    else:\n        test.loc[[indexer],\"Fee\"] = 0","63006c82":"from sklearn.model_selection import train_test_split\n\ntarget = train_simple[\"AdoptionSpeed\"]\npredict = train_simple.drop([\"AdoptionSpeed\"],axis = 1)\n\nx_train, x_val, y_train, y_val = train_test_split(predict, target, test_size = 0.22, random_state = 0)","e16df5a8":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train,y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","de1bd6c9":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","16f78d39":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","4799976b":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","7416aded":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","9b37375e":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","16369ea3":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","10757fe6":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","e807caf5":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","a58455cb":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","803add61":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","f8e4d95a":"full_train = train_simple.drop([\"AdoptionSpeed\"],axis = 1)\nfull_test = train_simple[\"AdoptionSpeed\"]\ngbk.fit(full_train, full_test)","7e91501f":"sample = pd.read_csv(\"..\/input\/test\/sample_submission.csv\")\nsample.sample(5)","dd1fc0cf":"#set ids as PassengerId and predict survival \nids = test[\"PetID\"]\npredictions = gbk.predict(test.drop([\"PetID\"],axis = 1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({\"PetID\": ids ,\"AdoptionSpeed\": predictions})\noutput.to_csv('submission.csv', index=False)","eef752f9":"So I noticed while going through the data on my own that the majority of the adoptions were free, and a few had absurd numbers like 3000, so I just made sure that the arrays are as simple as they can possibly be.","1143987a":"I know this is just scratching the surface of this competition but it's a nice little start in my opinion.  \nI plan on adding the image recognition and feature detection (Beginner level again of course) based on if the kernel is received well\/viewed at all.  \nI hope you all have a great day, keep coding everyone :)","8f78deeb":"**Wlcome to my little Tutorial on the Pet adoption competition.**  \nI'm sorry for the bad formatting, I know there are many ways of making a kernel look fancy and cool,  \nbut It's pretty late at night and I just wanted to add a few notes on my kernel and share with others who may be interested in the competition.\nAs you can see the code isn't as high end as some other kernels you'll come across, but it's just how a starter coder tried to achieve results.  \nThis is a little intro to the competition for anyone who may be interested.  \nSo let's get started :)","0c6e6974":"Now I prepare the data to be fit and read by the different models I managed to find online to see which one worked base for the model we had :)","0118da7d":"Here we drop the columns that only mess up the data and don't contribute to the chance of the animal being adopted in anyway.","af4f2c67":"We just took a little look at the data we're working with, and below we have the test array.","c9a40ce8":"Now that we cab see that the Gradient Boosting Classifier did the best work out of all of them, let's use that for the submission :)  \nBut before that, I want to train it with the entire dataset to maximize our chances","b6f94fa6":"Just checking the format of the output file so we don't get a zero percent in our first submission","562a9352":"The next two columns are the \"PhotoAmt\" and \"VideoAmt\"  \nThese were just representation of how much coverage these puppies got on the website, so I almost dumbed down the array a bit to make the number crunching easier and to simply things.","795ac028":"We fit all the data in the next blocks and print the values on a final board so we can see easily :)","c3ce4acc":"I wanted to simplfy the data a tad bit further since it's a starter kernel, so I simply looked if the dog being adopted was a Mixed breed or a pure breed to see if that had any effect.","f2f6e0ca":"Just cleaning the state data,  \nthe minimum value was 41322, so I simply used that value as a gauge value to simplify the state collumn a bit"}}