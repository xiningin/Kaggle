{"cell_type":{"64bf8183":"code","842d10ce":"code","41a7ea2c":"code","22de0718":"code","1304e7fe":"code","591ef85b":"code","284970db":"code","3c1b2396":"code","9e94522f":"code","193b978c":"code","489c5c0e":"code","a57e33bc":"code","85e37126":"code","f8d39402":"code","2406f8e5":"code","4fcdc1b3":"code","d7cc6303":"code","0368c81b":"code","5f8f747d":"code","a7a0b6a3":"code","353dd10c":"code","cd07e6fe":"code","b91104b7":"code","8adcbe65":"code","c6745c56":"code","5acf6bb0":"code","d8b1074b":"code","0c7ec655":"code","39a09e1f":"code","81517275":"code","1562f831":"code","e977f7fd":"code","92719489":"code","94b38f5a":"code","f95dcd86":"code","0cbb464e":"code","e9cd182f":"code","1122ed64":"code","f351a9eb":"markdown","e8787c15":"markdown","8be07ef0":"markdown","733ac679":"markdown","b59301a6":"markdown","8d15384b":"markdown","6a7b9f3b":"markdown","880089ee":"markdown","5a2ba561":"markdown","8684df6f":"markdown","1689576c":"markdown","0d9cbba5":"markdown","600fd998":"markdown","d6a61c16":"markdown","1afaa5d8":"markdown","48fb484d":"markdown","7803dff1":"markdown","2d14f41c":"markdown","1847bd60":"markdown","82728baa":"markdown","a968f665":"markdown","c0816b1e":"markdown","6ce66e7f":"markdown","22c45e99":"markdown","cf769302":"markdown","082fada5":"markdown","2bee4be2":"markdown","94580ee4":"markdown","97df6b28":"markdown","d63c550c":"markdown","7b0ddda8":"markdown","3fb53143":"markdown","e813422f":"markdown","5630d124":"markdown","5322f3dd":"markdown","d44a8368":"markdown","a9d9bbb0":"markdown","78a13ab6":"markdown","6a8028f6":"markdown","85c01841":"markdown","8f892d1f":"markdown"},"source":{"64bf8183":"# Import all the libraries we need\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 5000)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB,ComplementNB\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom scipy import stats, linalg\nimport networkx as nx\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain, combinations\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder","842d10ce":"# helper methods from Kaggle EDA\n\ndef load_csv(base_dir,file_name):\n    \"\"\"Loads a CSV file into a Pandas DataFrame\"\"\"\n    file_path = os.path.join(base_dir,file_name)\n    df = pd.read_csv(file_path,low_memory=False)\n    return df\n\ndef count_then_return_percent(dataframe,column_name):\n    '''\n    A helper function to return value counts as percentages.\n    \n    It has the following dependencies:\n    numpy: 1.18.5; pandas: 1.1.3\n\n    import numpy as np \n    import pandas as pd \n    '''\n    \n    counts = dataframe[column_name].value_counts(dropna=False)\n    percentages = round(counts*100\/(dataframe[column_name].count()),1)\n    return percentages\n\ndef count_then_return_percent_for_multiple_column_questions(dataframe,list_of_columns_for_a_single_question,dictionary_of_counts_for_a_single_question):\n    '''\n    A helper function to convert counts to percentages.\n    \n    It has the following dependencies:\n    numpy: 1.18.5; pandas: 1.1.3\n\n    import numpy as np \n    import pandas as pd \n    '''\n    \n    df = dataframe\n    subset = list_of_columns_for_a_single_question\n    df = df[subset]\n    df = df.dropna(how='all')\n    total_count = len(df) \n    dictionary = dictionary_of_counts_for_a_single_question\n    for i in dictionary:\n        dictionary[i] = round(float(dictionary[i]*100\/total_count),1)\n    return dictionary \n\ndef create_dataframe_of_counts(dataframe,column,rename_index,rename_column,return_percentages=False):\n    '''\n    A helper function to create a dataframe of either counts \n    or percentages, for a single multiple choice question.\n    \n    It has the following dependencies: \n    numpy: 1.18.5; pandas: 1.1.3\n    \n    import numpy as np \n    import pandas as pd  \n    '''\n    df = dataframe[column].value_counts().reset_index() \n    if return_percentages==True:\n        df[column] = (df[column]*100)\/(df[column].sum())\n    df = pd.DataFrame(df) \n    df = df.rename({'index':rename_index, 'Q3':rename_column}, axis='columns')\n    return df\n\ndef sort_dictionary_by_percent(dataframe,list_of_columns_for_a_single_question,dictionary_of_counts_for_a_single_question): \n    ''' \n    A helper function that can be used to sort a dictionary.\n    \n    It is an adaptation of a similar function\n    from https:\/\/www.kaggle.com\/sonmou\/what-topics-from-where-to-learn-data-science.\n    \n    It has the following dependencies:\n    numpy: 1.18.5; pandas: 1.1.3\n\n    import numpy as np \n    import pandas as pd \n    '''\n    dictionary = count_then_return_percent_for_multiple_column_questions(dataframe,\n                                                                list_of_columns_for_a_single_question,\n                                                                dictionary_of_counts_for_a_single_question)\n    dictionary = {v:k    for(k,v) in dictionary.items()}\n    list_tuples = sorted(dictionary.items(), reverse=False) \n    dictionary = {v:k for (k,v) in list_tuples}   \n    return dictionary\n\ndef plotly_bar_chart(response_counts,title,y_axis_title,orientation):\n    '''\n    This function creates a bar chart.\n    \n    It has the following dependencies:\n    plotly express: 0.4.1\n    \n    import plotly.express as px\n    '''\n    response_counts_series = pd.Series(response_counts)\n    fig = px.bar(response_counts_series,\n             labels={\"index\": '',\"value\": y_axis_title},\n             text=response_counts_series.values,\n             orientation=orientation,)\n    fig.update_layout(showlegend=False,\n                      title={'text': title,\n                             'y':0.95,\n                             'x':0.5,})\n    fig.show()\n\ndef plotly_choropleth_map(df, column, title, max_value):\n    '''\n    This function creates a choropleth map.\n    \n    It has the following dependencies:\n    plotly express: 0.4.1\n    \n    import plotly.express as px\n    '''\n    fig = px.choropleth(df, \n                    locations = 'country',  \n                    color = column,\n                    locationmode = 'country names', \n                    color_continuous_scale = 'viridis',\n                    title = title,\n                    range_color = [0, max_value])\n    fig.update(layout=dict(title=dict(x=0.5)))\n    fig.show()\n        \ndef plotly_bar_chart_with_x_axis_limit(response_counts,title,y_axis_title,orientation,limit_for_axis):\n    '''\n    A slightly modified version of plotly_bar_chart().\n    \n    It has the following dependencies:\n    plotly express: 0.4.1\n    \n    import plotly.express as px\n    '''\n    response_counts_series = pd.Series(response_counts)\n    fig = px.bar(response_counts_series,\n             labels={\"index\": '',\"value\": y_axis_title},\n             text=response_counts_series.values,\n             orientation=orientation,)\n    fig.update_xaxes(range=[0, limit_for_axis])\n    fig.update_layout(showlegend=False,\n                      title={'text': title,\n                             'y':0.95,\n                             'x':0.5,})\n    fig.show()","41a7ea2c":"# Load the data\n\nbase_dir = '\/kaggle\/input\/'\nfile_name = 'kaggle_survey_2020_responses.csv'\nsurvey_df = load_csv(base_dir,file_name)\n\n# Get the responses only\nresponses_df = survey_df[1:]","22de0718":"bayesian_df = survey_df[survey_df['Q17_Part_4']=='Bayesian Approaches']\nba_responses_df = bayesian_df[1:]","1304e7fe":"ba_responses_per_country_df = create_dataframe_of_counts(bayesian_df,'Q3','country','# of respondents',return_percentages=False)\nba_percentages_per_country_df = create_dataframe_of_counts(bayesian_df,'Q3','country','% of respondents',return_percentages=True)","591ef85b":"responses_per_country_df = create_dataframe_of_counts(survey_df,'Q3','country','# of respondents',return_percentages=False)\nmerged_res = ba_responses_per_country_df.merge(responses_per_country_df, how='inner', on='country')\nba_responses_per_country_df['normalized'] = merged_res['# of respondents_x'].values\/merged_res['# of respondents_y'].values","284970db":"plotly_choropleth_map(ba_responses_per_country_df, \n                       'normalized', \n                       'Proportion of Bayesian usage per country (2020 Kaggle DS & ML Survey)',\n                        max_value = np.max(ba_responses_per_country_df.normalized.values)+0.02)\n\nprint('Note that countries with less than 50 responses were replaced with the country name \"other\" (which does not show up on this map)')","3c1b2396":"question_name = 'Q4'\nresponses_in_order = [\"I prefer not to answer\",\"No formal education past high school\",\n                      \"Some college\/university study without earning a bachelor\u2019s degree\",\n                      \"Bachelor\u2019s degree\",\"Master\u2019s degree\",\"Doctoral degree\",\"Professional degree\"]\nsorted_percentages = count_then_return_percent(ba_responses_df,question_name)[responses_in_order]\ntitle_for_chart = 'Most Common Degree Type'\ntitle_for_y_axis = '% of respondents'\norientation_for_chart = 'v'\n  \nplotly_bar_chart(response_counts=sorted_percentages,\n                 title=title_for_chart,\n                 y_axis_title=title_for_y_axis,\n                 orientation=orientation_for_chart) ","9e94522f":"question_name = 'Q4'\nresponses_in_order = [\"I prefer not to answer\",\"No formal education past high school\",\n                      \"Some college\/university study without earning a bachelor\u2019s degree\",\n                      \"Bachelor\u2019s degree\",\"Master\u2019s degree\",\"Doctoral degree\",\"Professional degree\"]\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df['Q4'] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df['Q4'] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\n\nfig = px.line(mini_df, x='responses', y='fractions', title='Fraction of people using BA, given their degree')\nfig.show()","193b978c":"#Same analysis as in 2, wrt gender \nquestion_name = 'Q2'\nresponses_in_order = [\"Man\", \"Woman\", \"Nonbinary\", \"Prefer not to say\", \"Prefer to self-describe\"]\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of gender')\nfig.show()","489c5c0e":"#Same analysis as in 2, wrt bucketed age \nquestion_name = 'Q1'\nresponses_in_order = [\"18-21\",\"22-24\", \"25-29\", \"30-34\", \"35-39\",\"40-44\", \"45-49\", \"50-54\",\"55-59\",\"60-69\"]\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of age')\nfig.show()","a57e33bc":"#Same analysis as in 2, wrt company size\nquestion_name = 'Q20'\nresponses_in_order = [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\"]\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of company size')\nfig.show()","85e37126":"question_name = 'Q20'\nresponses_in_order = ['0-49 employees',\n                      '50-249 employees','250-999 employees',\n                      \"1000-9,999 employees\"]\nsorted_percentages = count_then_return_percent(responses_df,question_name)[responses_in_order]\n\nmini_df['weights'] = sorted_percentages\nmini_df.fillna(0, inplace=True)\nmini_df['weighted'] = sorted_percentages.values\/100.0 * mini_df['fractions'].values\n\nfig = px.line(mini_df, x='responses', y='weighted', title='Usage of BA, as a weighted fraction of company size')\nfig.show()","f8d39402":"#programming experience and usage of BA\nquestion_name = 'Q6'\nresponses_in_order = ['I have never written code',\n                      '< 1 years','1-2 years','3-5 years',\n                      '5-10 years','10-20 years','20+ years']\n\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of Programming Experience')\nfig.show()","2406f8e5":"#ML experience and usage of BA\nquestion_name = 'Q15'\nresponses_in_order = ['I do not use machine learning methods',\n                      'Under 1 year','1-2 years','2-3 years',\n                      '3-4 years','4-5 years','5-10 years',\n                     '10-20 years', '20 or more years']\n\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of ML Experience')\nfig.show()","4fcdc1b3":"#Salary and usage of BA\nquestion_name = 'Q24'\nresponses_in_order = ['$0-999',\n                        '1,000-1,999',\n                        '2,000-2,999',\n                        '3,000-3,999',\n                        '4,000-4,999',\n                        '5,000-7,499',\n                        '7,500-9,999',\n                        '10,000-14,999',\n                        '15,000-19,999',\n                        '20,000-24,999',\n                        '25,000-29,999',\n                        '30,000-39,999',\n                        '40,000-49,999',\n                        '50,000-59,999',\n                        '60,000-69,999',\n                        '70,000-79,999',\n                        '80,000-89,999',\n                        '90,000-99,999',\n                        '100,000-124,999',\n                        '125,000-149,999',\n                        '150,000-199,999',\n                        '200,000-249,999',\n                        '250,000-299,999',\n                        '300,000-500,000',\n                        '> $500,000']\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction salary')\nfig.show()","d7cc6303":"# Let's look at the usage of Bayesian approaches as a fraction of title\n\n\n#Salary and usage of BA\nquestion_name = 'Q5'\nresponses_in_order = [ 'Student', 'Data Engineer', 'Software Engineer', 'Data Scientist',\n                        'Data Analyst', 'Research Scientist', 'Other',\n                        'Currently not employed', 'Statistician',\n                        'Product\/Project Manager', 'Machine Learning Engineer',\n                        'Business Analyst', 'DBA\/Database Engineer']\nnum_responses_in_order = []\nnum_responses_in_order_ba = []\n\nfor r in responses_in_order:\n    \n    temp = survey_df[survey_df[question_name] == r]\n    num_responses_in_order.append(temp.shape[0])\n    \n    temp_ba = survey_df[(survey_df[question_name] == r) & (survey_df['Q17_Part_4']=='Bayesian Approaches')]\n    num_responses_in_order_ba.append(temp_ba.shape[0])\n    \nfractions = np.asarray(num_responses_in_order_ba)\/np.asarray(num_responses_in_order)\n\nmini_df = pd.DataFrame({'responses': responses_in_order, 'fractions': fractions})\nfig = px.line(mini_df, x='responses', y='fractions', title='Usage of BA, as a fraction of title')\nfig.show()","0368c81b":"target = responses_df['Q17_Part_4']=='Bayesian Approaches'\nprint(target.value_counts())","5f8f747d":"le_age = LabelEncoder()\nage_feature = le_age.fit_transform(survey_df['Q1'].values[1:]) # to remove the first entry which is the question\nprint(le_age.classes_)","a7a0b6a3":"le_gender = LabelEncoder()\ngender_feature = le_gender.fit_transform(survey_df['Q2'].values[1:]) # to remove the first entry which is the question\nprint(le_gender.classes_)","353dd10c":"le_degree = LabelEncoder()\nsurvey_df['Q4'].fillna(value='NA', inplace=True)\ndegree_feature = le_degree.fit_transform(survey_df['Q4'].values[1:]) # to remove the first entry which is the question\nprint(le_degree.classes_)","cd07e6fe":"le_compsize = LabelEncoder()\nsurvey_df['Q20'].fillna(value='NA', inplace=True)\ncompsize_feature = le_compsize.fit_transform(survey_df['Q20'].values[1:]) # to remove the first entry which is the question\nprint(le_compsize.classes_)","b91104b7":"le_prgexp = LabelEncoder()\nsurvey_df['Q6'].fillna(value='NA', inplace=True)\nprgexp_feature = le_prgexp.fit_transform(survey_df['Q6'].values[1:]) # to remove the first entry which is the question\nprint(le_prgexp.classes_)","8adcbe65":"le_mlexp = LabelEncoder()\nsurvey_df['Q15'].fillna(value='NA', inplace=True)\nmlexp_feature = le_mlexp.fit_transform(survey_df['Q15'].values[1:]) # to remove the first entry which is the question\nprint(le_mlexp.classes_)","c6745c56":"le_salary = LabelEncoder()\nsurvey_df['Q24'].fillna(value='NA', inplace=True)\nsalary_feature = le_salary.fit_transform(survey_df['Q24'].values[1:]) # to remove the first entry which is the question\nprint(le_salary.classes_)","5acf6bb0":"le_title = LabelEncoder()\nsurvey_df['Q5'].fillna(value='NA', inplace=True)\ntitle_feature = le_title.fit_transform(survey_df['Q5'].values[1:]) # to remove the first entry which is the question\nprint(le_title.classes_)","d8b1074b":"nb_df = pd.DataFrame({'Age': age_feature, \n                      'Gender': gender_feature, \n                      'Degree': degree_feature, \n                      'CompanySize': compsize_feature, \n                      'ProgrammingExp': prgexp_feature, \n                      'MLExp': mlexp_feature, \n                      'Salary': salary_feature, \n                      'Title': title_feature,\n                      'Target': target})","0c7ec655":"nb_df.head()","39a09e1f":"X = np.copy(nb_df[['Age', 'Gender', 'Degree', 'CompanySize', 'ProgrammingExp', 'MLExp', 'Salary', 'Title']])\ny = np.copy(nb_df['Target'])\n\n# split the data into training and test sets with a 20%-80% split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n\ngnb = GaussianNB() \ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\nprint(\"Test Accuracy: \",  1-(y_test != y_pred).sum()\/X_test.shape[0])","81517275":"print(\"Precision: %.2f; Recall: %.2f; Fscore: %.2f\" % precision_recall_fscore_support(y_test, y_pred, average='micro')[:3])","1562f831":"\ndef powerset(iterable):\n    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\n\ndef partial_corr(C):\n    \"\"\"\n    Returns the sample linear partial correlation coefficients between pairs of variables in C, controlling \n    for the remaining variables in C.\n    \"\"\"\n    \n    C = np.asarray(C)\n    p = C.shape[1]\n    P_corr = np.zeros((p, p), dtype=np.float)\n    for i in range(p):\n        P_corr[i, i] = 1\n        for j in range(i+1, p):\n            idx = np.ones(p, dtype=np.bool)\n            idx[i] = False\n            idx[j] = False\n            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n\n            res_j = C[:, j] - C[:, idx].dot( beta_i)\n            res_i = C[:, i] - C[:, idx].dot(beta_j)\n            \n            corr = stats.pearsonr(res_i, res_j)[0]\n            P_corr[i, j] = corr\n            P_corr[j, i] = corr\n        \n    return P_corr\n\n\ndef get_mb(mb_size, pcorr):\n    \n    mb = np.empty((pcorr.shape[1], mb_size), dtype=np.int64)\n    #print(\"shape of mb: \", mb.shape)\n    \n    for i in range(pcorr.shape[1]):\n        \n        mb_for_i = np.argsort(-pcorr[:,i])[:mb_size]\n        #print(pcorr[:,i][:mb_size])\n        #print(mb_for_i)\n        #temp = [int(x) for x in mb_for_i]\n        mb[i, :] = mb_for_i\n    \n    return mb\n\n\ndef pc_method(C, mb, M, N,istart, iend):\n    \"\"\"\n    Tries to separate two variables i and j given a set S\n    S is a subset of the variables in C\n    C: matrix containing features\n    N: C.shape[0]\n    \"\"\"\n    \n    C = np.asarray(C)\n    scores = np.zeros((N, N), dtype=np.float)\n    errors1 = []\n    errors2 = []\n    for i in range(istart, iend):\n        for j in range(i+1, iend):\n         \n            ij_score = 1\n            #now need to create all possible subsets \n            \n            sets = list(powerset(set(mb[[i,j],1:M+1].flatten())))\n            for s in sets:\n                if i not in list(s) and j not in list(s):\n                    \n                    idx = np.zeros(C.shape[1], dtype=np.bool)\n                    idx[list(s)] = 1\n                   \n                    if len(s) == 0:\n                        \n                        X1 = C[:,j].reshape(-1,1)#Lags[:, idx]\n                        y1 = C[:,i]\n                        regr = LinearRegression(fit_intercept = False)#SGDRegressor(max_iter=1000, tol=1e-3)\n                        yh1 = regr.fit(X1, y1).predict(X1)\n                        error1 = np.sum(np.square(y1 - yh1))\n                        \n                        error2 = np.sum(np.square(y1))\n                        fscore = 1 - float(error1)\/error2\n                        new_ij_score = fscore#-np.linalg.inv(np.corrcoef(Cn.T))[0,1]\n                        if new_ij_score < ij_score:\n                            ij_score = new_ij_score\n                    else:\n                    \n                        Cs = np.copy(C[:, idx])\n                        Cn = np.concatenate(( np.expand_dims(C[:,i], axis=1), \n                                              np.expand_dims(C[:,j], axis=1), \n                                              Cs), axis=1)\n                        idx = np.ones(Cn.shape[1], dtype=np.bool)\n                        idx[0] = False\n\n\n                        X1 = Cn[:,idx]\n                        y1 = Cn[:,0]\n                        regr = LinearRegression(fit_intercept = False)\n                        yh1 = regr.fit(X1, y1).predict(X1)\n                        error1 = np.sum(np.square(y1 - yh1))\n\n\n                    \n                        #then the error two is when we exclude j                     \n                        idx[1] = False\n                        X2 = Cn[:,idx]#Lags[:, idx]\n                        yh2 = regr.fit(X2, y1).predict(X2)\n                        error2 = np.sum(np.square(y1 - yh2))\n\n\n                     \n                        errors1.append(error1)\n                        errors2.append(error2)\n                        if error1 > error2:\n                            fscore = 0\n                        else:\n\n                            fscore = 1 - float(error1)\/error2\n                        new_ij_score = fscore\n                        if new_ij_score < ij_score:\n                            ij_score = new_ij_score\n\n            scores[i,j] = ij_score\n            scores[j,i] = ij_score\n            \n        \n    return scores, errors1, errors2\n\n\ndef get_feature_graph(data, mb_size, thresh_for_graph):\n    scaler = StandardScaler() \n    data_md = scaler.fit_transform(data)\n    \n    C = np.copy(data_md)\n    \n   \n    N= data_md.shape[1]\n    print(\"N is: \", N)\n    M = mb_size\n    istart = 0\n    iend =N\n    pcorr_matrix = partial_corr(C)\n    \n    mb = get_mb(N, pcorr_matrix)\n    \n    pc_scores_ind0 = pc_method(C, mb, M, N,istart, iend)\n    \n    pc_scores_ind0 = pc_scores_ind0[0]\n\n    \n    pc_graph_present=np.zeros((pc_scores_ind0.shape[0], pc_scores_ind0.shape[1]))\n    thresh = thresh_for_graph\n    \n    for i in range(pc_scores_ind0.shape[0]):\n        for j in range(pc_scores_ind0.shape[1]):\n            if abs(pc_scores_ind0[i,j]) > thresh:\n                pc_graph_present[i,j] = 1\n\n    PCg = nx.from_numpy_matrix(pc_graph_present)\n    \n    return PCg, pc_scores_ind0","e977f7fd":"labels = {}\nfor i in range(nb_df.columns.values.shape[0]):\n    labels[i] = nb_df.columns.values[i]\n\nmb_size = 3\nthresh_for_graph = 0.01\n\ndata = nb_df.to_numpy()\nPCg, pc_scores_ind0  = get_feature_graph(data, mb_size, thresh_for_graph)\n\npos = nx.spring_layout(PCg, seed=4321, k=2)\nnx.draw(PCg, pos=pos, node_size=40, node_color='red', labels=labels)","92719489":"corr_matrix_sample = nb_df.corr()\nsns.heatmap(corr_matrix_sample, cmap='coolwarm_r', annot_kws={'size':20})","94b38f5a":"corr_matrix_sample","f95dcd86":"X = np.copy(nb_df[['Age', 'Degree', 'CompanySize', 'MLExp', 'Title']])\ny = np.copy(nb_df['Target'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\ngnb = GaussianNB() \ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\nprint(\"Test Accuracy: \",  1-(y_test != y_pred).sum()\/X_test.shape[0])\n\nprint(\"Precision: %.2f; Recall: %.2f; Fscore: %.2f\" % precision_recall_fscore_support(y_test, y_pred, average='micro')[:3])","0cbb464e":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"Naive BayesClassifier\": GaussianNB()\n}","e9cd182f":"X = np.copy(nb_df[['Age', 'Degree', 'CompanySize', 'MLExp', 'Title']])\ny = np.copy(nb_df['Target'])\n\nfrom sklearn.model_selection import cross_val_score\npredictions = []\n#note cross_val_score with cv uses StratifiedKFold by default\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    cv_score = cross_val_score(classifier, X, y, cv=5, n_jobs=-1)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a cross-validation score of\", round(cv_score.mean(), 2) * 100, \"% accuracy score\")","1122ed64":"X = np.copy(nb_df[['Age', 'Degree', 'CompanySize', 'MLExp', 'Title']])\ny = np.copy(nb_df['Target'])\n\nfrom sklearn.model_selection import cross_val_score\npredictions = []\n#note cross_val_score with cv uses StratifiedKFold by default\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X, y, cv=5, n_jobs=-1)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","f351a9eb":"We observe that the models are still able to perform at least as accurately as the case where we used the two variables \"Age\" and \"Gender\". \nTherefore, we do not consider those two variables as features in our model to prevent creating a biased model. ","e8787c15":"* Job Title","8be07ef0":"Interestingly, it seems that in the Americas, there is higher usage of Bayesian approaches!","733ac679":"* Company Size","b59301a6":"# <a class=\"anchor\" id=\"Part5\"><\/a> Concluding Remarks \n\n1. Based on the large fraction of PhDs, as well as people with 20+ years of programming and machine learning experience, it seems that Kagglers use much more than just Naive Bayes from the Bayesian approaches spectrum. \n\n2. However, Naive Bayes is not to be discareded - for example, our Naive Bayes model for the usage of Bayesian approaches performs just as well as other models such as logistic regression or decision trees. \n\n3. We can use Bayesian network modeling for choosing optimal features, as well as for other network inference tasks. \n\n4. To practice responsible and ethical AI we took some preemptive action to prevent our model from performing biased predictions. \n\nWe hope you liked our work and learned something fun and new! We know we did :)\n","8d15384b":"### Let's check whether BAs are more used by people with higher education.","6a7b9f3b":"Wow, almost as if, the older you are, the more likely you use BA! ","880089ee":"## <a class=\"anchor\" id=\"Part2-2\"><\/a> Model Training \nLet's apply Naive Bayes to nb_df!","5a2ba561":"So people with Doctoral Degree use BA the most! Notice they use it 7% more than people with MS!","8684df6f":"Let's see some more metrics, such as precision, recall and fscore","1689576c":"# <a class=\"anchor\" id=\"Part1\"><\/a> Exploratory Data Analysis \n\nWe are interested in the subpopulation that uses Bayesian Aproaches (BA). \nLet's get some ideas about who that subpopulation is.\nThe spectrum of Bayesian approaches is wide. \nBoth Naive Bayes as well as sophisticated graphical models and counterfactual reasoning are in that spectrum. \nSo on which end of the spectrum are our Bayesian Kaggle users? ","0d9cbba5":"Data scientists, reseach scientists and statisticians have the highest number of people using Bayesian approaches. ","600fd998":"## <a class=\"anchor\" id=\"Part3\"><\/a> Comparing the Bayesian Model Performance with Other Methods \nNext, let's examine some other classifiers, and see how they compare with the Naive Bayes classifier.\nHere, we consider Logistic Regression, K-Nearest Neighbors (kNN), Support Vector Machine (SVM), and Decision Tree approaches. ","d6a61c16":"### Let's see the usage of BA per country (normalized by total number of responses per country).","1afaa5d8":"It seems the more programming experience you have, the more you use BA!","48fb484d":"* Machine Learning Experience","7803dff1":"## <a class=\"anchor\" id=\"Part2-3\"><\/a> Revisiting Feature Extraction \n\n**Let's get the most important features via a Bayesian approach!**\n**We will use the Peter-Clark (PC) algorithm to create a graph of direct dependencies among the feature and target variables.**\n\nTo create the dependency graph (which in this case will be an undirected graph), for every two variables $X$ and $Y$, the PC algorithm searches for a 'separating' set $Z$ such that $X$ and $Y$ are conditionally independent given $Z$. For more details on the PC method, see for example [[1](#ref1)].\n\nIn general, $Z$ is a subset of all the variables we are considering in our model (except $X,Y$), but here we use an optimization that makes the search space smaller. That optimization is via the so called Markov Blanket. For more details see [[2](#ref2)]. \n\nLastly, as an approximate conditional independence test, we use f-test to check whether $X$ is independent of $Y$ given $Z$. Intuitively, the f-test is a check whether $Y$ significantly reduces the variance of the error when we estimate $X$ from $Y \\cup Z$. For more details on the f-test, see [[3](#ref3)]. ","2d14f41c":"* Educational Level","1847bd60":"### Let's do the same analysis with respect to gender, bucketed age and the size of the company people work for. ","82728baa":"While the distribution of degrees in the 'BA' subpopulation is similar to the degree distribution in the population, let's ask a bit of a different question. \nFor each educational group, what is the fraction of people using BA?","a968f665":"## <a class=\"anchor\" id=\"Part2\"><\/a> Feature Extraction \nLet's create the features.\n* Age","c0816b1e":"Based on the PC graph, let's remove \"Salary\", \"Gender\", and \"Programming Experience\".","6ce66e7f":"Again, the more ML experience one has, the more BA one uses! ","22c45e99":"Some of the top earners use BA!","cf769302":"# Motivation\n\nWe are two friends that have used Bayesian approaches in our research. Specifically, our research uses a lot of reasoning on Bayesian networks and ideas developed by the Turing Award winner, Judea Pearl. Since Bayesian approaches can range from the usage of Naive Bayes to sophisticated AI algorithms using counterfactuals on Bayesian networks, we wanted to investigate on which side of the spectrum do Kaggle users lean. Spoiler alert, we do use Bayesian approaches in our analysis! ","082fada5":"Here we combine all the different features we extracted from the available data. ","2bee4be2":"Here we use the PC method and draw a dependency graph among the features. We use the PC method with $\\epsilon = 0.01$ as the threshold for the f-score.","94580ee4":"* Programming Experience","97df6b28":"# Analysis of the Usage of Bayesian Approaches from 2020 Kaggle ML and DS Survey Responses\n![kaggle_intro.png](attachment:kaggle_intro.png)","d63c550c":"### Now let's do the same analysis with respect to years of programming experience, machine learning experience, yearly compensation (salary), and the job title. ","7b0ddda8":"# <a class=\"anchor\" id=\"Part6\"><\/a> References \n\n\n[1] <a class=\"anchor\" id=\"ref1\"><\/a>\nKalisch, Markus, and Peter B\u00fchlmann. \"Estimating high-dimensional directed acyclic graphs with the PC-algorithm.\" Journal of Machine Learning Research 8.Mar (2007): 613-636.\n    \n[2] <a class=\"anchor\" id=\"ref2\"><\/a>\nGao, Tian, and Qiang Ji. \"Efficient Markov blanket discovery and its application.\" IEEE transactions on Cybernetics 47.5 (2016): 1169-1179.\n\n[3] <a class=\"anchor\" id=\"ref3\"><\/a>\nhttps:\/\/en.wikipedia.org\/wiki\/F-test\n\n[4] <a class=\"anchor\" id=\"ref4\"><\/a>\nTolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai. \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\", arXiv, 2016.\n\n[5] <a class=\"anchor\" id=\"ref5\"><\/a>\nEmmanouil Krasanakis, Eleftherios Spyromitros-Xioufis, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification. In Proceedings of the 2018 World Wide Web Conference (WWW '18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 853\u2013862. DOI:https:\/\/doi.org\/10.1145\/3178876.3186133\n\n[6] <a class=\"anchor\" id=\"ref6\"><\/a>\nShah, Deven Santosh, and Schwartz, H. Andrew, and Hovy, Dirk, \"Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview\", Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, 5248-5264. \n\n    ","3fb53143":"Note that in this case, we could also use the correlation matrix as a way to discard less important features, but the PC graph tells us something more: for example, though in the correlation matrix we see that Salary and Company Size are correlated and their correlation with the target variable is approximately the same, we don't have much information as to which feature should we remove. \nThe graph obtained with the PC method however, tells us that by removing Salary we won't be losing much information, as Salary is only connected to Company Size. \n\nBelow we report the correlation matrix as well. ","e813422f":"So it's used almost equally across different sizes of companies. So if we weight the fractions by the percentages of population presense of each of the company size buckets ","5630d124":"* Salary","5322f3dd":"* Gender","d44a8368":"Looking at the weighted fraction, it seems BA are used the most in small companies and equally across bigger companies.","a9d9bbb0":"# <a class=\"anchor\" id=\"Part2\"><\/a> Using Bayesian Models to Analyze Bayesian Usage \n    \n**Now let's use some Bayesian approaches to study Bayesian usage!**\n\nWe will start by a simple Naive Bayes and then we will also use some graphical model techniques :) \nWe're gonna make a model predicting whether a Kaggler uses BA or doesn't. We'll use Naive Bayes for starters :) ","78a13ab6":"The accuracy metrics show an improved performance compared to the previous run which included the features \"Salary\", \"Gender\", and \"Programming Experience\".","6a8028f6":"# <a class=\"anchor\" id=\"Part4\"><\/a> Eliminating AI Bias from the Model \nResponsible and ethical AI is an important issue that we would like to address in our analysis. \nEliminating bias from predictive models has gained more traction during the recent years in many different domains from creating word embeddings [[4](#ref4)] and predictive classifiers [[5](#ref5)] to Natural Language Processing (NLP) tasks [[6](#ref6)].\n\nOne effective way to combat bias in our study is to use less bias and\/or more inclusive data.\nTherefore, we remove some of the features that might lead the model to have a biased prediction in regards to some protected categories. \nNote that the PC method already pointed out that \"Gender\" is not a relevant feature. \nHere, we also remove \"Age\" and repeat the same experiments to compare the model performance. ","85c01841":"# Notebook Content\n* [Importing the Required Libraries and Functions](#Part0)\n* [Exploratory Data Analysis](#Part1): We examine what fraction of people with certain age, degree, salary, title, or experience use Bayesian Approaches. \n* [Using Bayesian Models to Analyze Bayesian Usage](#Part2)\n    * [Feature Extraction](#Part2-1): We extract features from the data that are related to our analysis. \n    * [Model Training](#Part2-2): We create a Bayesian model that predicts whether a Kaggler uses Bayesian approaches or not. \n    * [Revisiting Feature Extraction](#Part2-3): We use a Bayesian approach to choose the most informative features for the model.\n* [Comparing the Bayesian Model Performance with Other Methods](#Part3): We compare our simple Bayesian model with other methods, such as logistic regression, SVM, and Decision Tree classifiers.\n* [Eliminating AI Bias from the Model](#Part4): We eliminate some features that might make our model biased towards some protected categories. \n* [Concluding Remarks](#Part5)\n* [References](#Part6)","8f892d1f":"# <a class=\"anchor\" id=\"Part0\"><\/a> Importing the Required Libraries and Functions "}}