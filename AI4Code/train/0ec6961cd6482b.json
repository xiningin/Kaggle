{"cell_type":{"a27a466a":"code","630c37b4":"code","305c6cd3":"code","5ad9bc8f":"code","1ad88018":"code","df8569ab":"code","0a2b3524":"code","f2845cd1":"code","e92e4440":"code","6b24665b":"code","e75a0d56":"code","071e70b2":"code","812556ec":"code","72e49b1b":"code","f743859c":"code","3d24cd27":"code","2a860b87":"code","a55ea153":"code","ce70c0ba":"code","8e5d9611":"code","0d30e9d4":"code","af51da50":"code","468e44c7":"markdown","8b3c67d0":"markdown","66aac4d4":"markdown","e664296d":"markdown","4ac679c9":"markdown","31375308":"markdown","3d6d23c8":"markdown","4cc3e3f8":"markdown","30d87c5f":"markdown"},"source":{"a27a466a":"import pandas as pd\nimport numpy as np\nimport re\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import f1_score,hamming_loss\n\nfrom sklearn.multiclass import OneVsRestClassifier","630c37b4":"!pip install xlrd\n!pip install openpyxl","305c6cd3":"data = pd.read_excel(\"..\/input\/multilabel-entity-identification\/data_set.xlsx\")\ndata.head()","5ad9bc8f":"# for use in classwise classification report\nlabels_list = list()\nfor i in range(data.shape[0]):\n    if data.iloc[i,0] not in labels_list:\n        labels_list.append(data.iloc[i,0])\n# print(labels_list)","1ad88018":"print(\"Number of rows-->\",data.shape[0])\nprint(\"Number of columns-->\",data.shape[1])","df8569ab":"data['intent'].value_counts()","0a2b3524":"figure(figsize=(12, 8), dpi=80)\ndata['intent'].value_counts().plot(kind='bar')","f2845cd1":"sentence_list = data['message'].tolist()\nword_count = 0\nfor sentence in sentence_list:\n    word_count+=len(sentence.split(\" \"))\naverage_word_count = word_count\/data.shape[0]\nprint(\"Average word count-- \",average_word_count)","e92e4440":"# function for text cleaning \ndef clean_text(text):\n    # remove backslash-apostrophe \n    text = re.sub(\"\\'\", \"\", text) \n    # remove everything except alphabets \n    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n    # remove whitespaces \n    text = ' '.join(text.split()) \n    # convert text to lowercase \n    text = text.lower() \n    \n    return text\n\ndata['message'] = data['message'].apply(lambda x:clean_text(x))\n","6b24665b":"for i in range(data.shape[0]):\n    data.iloc[i,0] = [data.iloc[i,0]]\n    \nmultilabel = MultiLabelBinarizer()\ny = multilabel.fit_transform(data['intent'])","e75a0d56":"multilabel.classes_","071e70b2":"# using tfidf vectorizer for feature extraction as no embedding can be used \n# also with small data samples no good embedding can be created\ntfidf = TfidfVectorizer(analyzer='word', max_features=1500, ngram_range=(1,3), stop_words='english')\nX = tfidf.fit_transform(data['message'])\nprint(X.shape, y.shape)","812556ec":"#splitting into 60:40 ratio as per assignment\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4)","72e49b1b":"sgd = SGDClassifier()\nsvc = LinearSVC()\ndecision_tree = DecisionTreeClassifier()","f743859c":"from sklearn.metrics import accuracy_score,hamming_loss,precision_score,recall_score,f1_score\ndef print_metrics_report(clf,y_test,y_pred):\n    print(\"Clf: \", clf.__class__.__name__)\n    print('Exact Match Ratio: {0}'.format(np.round(accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)*100,2)))\n\n    print('Hamming loss: {0}'.format(np.round(hamming_loss(y_test, y_pred)*100,2))) \n    print('Recall: {0}'.format(np.round(precision_score(y_test, y_pred, average='micro')*100,2)))\n    print('Precision: {0}'.format(np.round(recall_score(y_test,y_pred, average='micro')*100,2)))\n    print('F1 Measure: {0}'.format(np.round(f1_score(y_test, y_pred, average='micro')*100,2))) \n    print('-------------')\n    print(\"\")","3d24cd27":"for classifier in [svc,sgd,decision_tree]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print_metrics_report(classifier,y_test,y_pred)","2a860b87":"# Hyper parameters range intialization for tuning \n\nparameters={\n    \"estimator__splitter\":[\"best\",\"random\"],\n            \"estimator__max_depth\" : [1,9,17,24],\n           \"estimator__min_samples_leaf\":[1,3,5],\n#            \"min_weight_fraction_leaf\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n           \"estimator__max_features\":[\"auto\",\"sqrt\",None],\n           \"estimator__max_leaf_nodes\":[None,10,20,30,40,50,70] \n           }","a55ea153":"import warnings\nwarnings.filterwarnings('ignore')\nclf = OneVsRestClassifier(decision_tree)\nclf_tuned=GridSearchCV(clf,param_grid=parameters,scoring='neg_mean_squared_error',cv=3,verbose=0)\n\nclf_tuned.fit(X_train, y_train)","ce70c0ba":"clf_tuned.best_params_","8e5d9611":"decision_tree_tuned = DecisionTreeClassifier(max_depth=9,min_samples_leaf=1,splitter='best',max_leaf_nodes=30,max_features=None)\nclf_tuned = OneVsRestClassifier(decision_tree_tuned)\nclf_tuned.fit(X_train, y_train)\ny_pred = clf_tuned.predict(X_test)\nprint_metrics_report(decision_tree_tuned,y_test,y_pred)","0d30e9d4":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,y_pred,target_names=labels_list))","af51da50":"x = [\n    ['how much is engine displacement'],\n    ['What is the fuel tank capacity?'],\n    ['I want to know about engine displacement and fuel tank capacity']\n]\nfor sentence in x:\n    xt = tfidf.transform(sentence)\n    value = multilabel.inverse_transform(clf_tuned.predict(xt))\n    print(value)","468e44c7":"# Data Preprocessing\n\n1. It includes cleaning the text i.e removing non alphabetic characters and converting the text to lower case etc.\n\n### Note ###\n1. Not removing stopwords here as the text size is already small and removing it degrades model performance","8b3c67d0":"# Testing on sample test sentences","66aac4d4":"# Extrapolatory Data Analysis\n\n1. We find the number of rows,columns in the dataset\n2. We find the classwise distribution of the text\n3. Find the average word count in the dataset to know whether the text classification is to be done on short or long sentences\n\n## Observations\n\n1. We have very less samples per class for some classes\n2. Number of rows is 915\n3. Text size is small i.e average word count is 4-5 words per sentence\n","e664296d":"# Data preparation\n\n1. It involves encoding the data class label into multiclass label form\n2. Creating the TF IDF vectorizer","4ac679c9":"# Hyperparameter Tuning","31375308":"# Testing on tuned model metrics","3d6d23c8":"# Model Training\n\n1. We first split the dataset in 60:40 ratio as asked in document\n2. We use three classifiers\n        * SGD Classifier\n        * SVM with Linear kernel\n        * Decision Tree \n3. Do the model training on 60% of the samples and testing on 40% of samples and generate metrics report","4cc3e3f8":"# Importing Libraries","30d87c5f":"# Results and Conclusion\n\nWe have used following metrics for analyzing the model performance\n\n1. **Exact Match Ratio**\n2. **Hamming Loss**\n3. **Recall**\n4. **Precision**\n5. **F1 Score**\n\nWe got the **75.48** as F1 Score using tf idf + Decision Tree classifier\n\nResults can be improved further by\n1. Having atleast 25 samples per class for each of the classes\n2. Using weighted word2Vec for creating the feature vector for a sentence"}}