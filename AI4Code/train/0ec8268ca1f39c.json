{"cell_type":{"f8ce17f4":"code","7d1b3dd6":"code","55a0c762":"code","7bb2b534":"code","eac2cf65":"code","297fbc0c":"code","58b95127":"code","b399bc90":"code","1fa74434":"markdown","72292533":"markdown","1c4aaa59":"markdown","525314dc":"markdown","f7111ee4":"markdown","77ff21e1":"markdown","d91d2988":"markdown","26763cd6":"markdown","38029b3b":"markdown","4d9103da":"markdown","5811996e":"markdown","c15932f1":"markdown","e219f71c":"markdown","ada1aec0":"markdown","90b48892":"markdown"},"source":{"f8ce17f4":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC as SVM\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestCentroid as NC\nfrom sklearn.model_selection import train_test_split as data_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA","7d1b3dd6":"classes = {\n\t\"circle\"   : 1,\n\t\"square\"   : 2,\n\t\"star\"     : 3,\n\t\"triangle\" : 4\n}\n\nclassifiers = {\n\t\"NC\"         : NC(),\n\t\"LDA\"        : LDA(),\n\t\"QDA\"        : QDA(),\n\t\"SVM_linear\" : SVM(kernel=\"linear\"),\n\t\"SVM_radial\" : SVM(kernel=\"rbf\")\n}","55a0c762":"def feature_extraction(data_file, dataset=\"..\/input\/shapes\/\"):\n\tdump = []\n\t\n\tprint(\"Extracting Hu moments...\")\n\t\n\tfor c, idx in classes.items():\n\t\tclass_folder = dataset + \"{}\/\".format(c)\n\t\t\n\t\tfor f in os.listdir(class_folder):\n\t\t\tfpath = class_folder + f\n\t\t\tsample = int(f.replace(\".png\", \"\"))\n\t\t\t\n\t\t\timg = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n\t\t\timg = cv2.bitwise_not(img)\n\t\t\thu = cv2.HuMoments(cv2.moments(img))\n\t\t\t\n\t\t\tfor i in range(0, 7):\n\t\t\t\thu[i] = -1 * np.sign(hu[i]) * np.log10(np.abs(hu[i]))\n\t\t\t\n\t\t\thu = hu.reshape((1, 7)).tolist()[0] + [sample, idx]\n\t\t\tdump.append(hu)\n\t\t\n\t\tprint(c, \"ok!\")\n\n\tcols = [\"hu1\", \"hu2\", \"hu3\", \"hu4\", \"hu5\", \"hu6\", \"hu7\", \"sample\", \"class\"]\n\t\n\tdf = pd.DataFrame(dump, columns=cols)\n\tdf.to_csv(data_file, index=None)\n\t\n\tprint(\"Extraction done!\")","7bb2b534":"data_file = \"hu_moments.csv\"\n\nfeature_extraction(data_file)","eac2cf65":"def classification(data_file, rounds=100, remove_disperse=[]):\n\tdf = pd.read_csv(data_file)\n\tdf = df.drop([\"sample\"], axis=1)\n\t\n\tif remove_disperse:\n\t\tdf = df.drop(remove_disperse, axis=1)\n\t\n\tX = df.drop([\"class\"], axis=1)\n\ty = df[\"class\"]\n\t\n\tans = {key: {\"score\" : []} for key, value in classifiers.items()}\n\t\n\tprint(\"Classifying...\")\n\t\n\tfor i in range(rounds):\n\t\tX_train, X_test, y_train, y_test = data_split(X, y, test_size=0.3)\n\t\t\n\t\tfor name, classifier in classifiers.items():\n\t\t\tscaler = StandardScaler()\n\t\t\tscaler.fit(X_train)\n\t\t\tX_train = scaler.transform(X_train)\n\t\t\tX_test = scaler.transform(X_test)\n\t\t\t\n\t\t\tclassifier.fit(X_train, y_train)\n\t\t\tscore = classifier.score(X_test, y_test)\n\t\t\t\n\t\t\tans[name][\"score\"].append(score)\n\t\t\n\tprint(\"Classification done!\")\n\t\n\treturn ans","297fbc0c":"ans = classification(data_file)","58b95127":"def sumary(ans, title=\"Summary\"):\n\tsize = 70\n\tseparator = \"-\"\n\t\n\tprint(separator*size)\n\tprint(\"SUMARY: {}\".format(title))\n\tprint(separator*size)\n\tprint(\"CLASSIF\\t\\tMEAN\\tMEDIAN\\tMINV\\tMAXV\\tSTD\")\n\tprint(separator*size)\n\t\n\tfor n in ans:\n\t\tm = round(np.mean(ans[n][\"score\"])*100, 2)\n\t\tmed = round(np.median(ans[n][\"score\"])*100, 2)\n\t\tminv = round(np.min(ans[n][\"score\"])*100, 2)\n\t\tmaxv = round(np.max(ans[n][\"score\"])*100, 2)\n\t\tstd = round(np.std(ans[n][\"score\"])*100, 2)\n\t\t\n\t\tprint(\"{:<16}{}\\t{}\\t{}\\t{}\\t{}\".format(n, m, med, minv, maxv, std))\n\t\n\tprint(separator*size)\n\tprint()","b399bc90":"sumary(ans)","1fa74434":"By executing this function and asking to store the results in ```hu_moments.csv```, we have:","72292533":"### Centroid Localization\n\nWe can use the raw moments to extract important information of an image. By doing $M_{00}$, we are accumulating the non-zero intensities. It's like describing the spatial information of the pixel \"blob\". Similary, doing for the $X$ and $Y$ dimensions ($M_{10}$ and $M_{01}$), one can pinpoint the centroid coordinates $(\\bar{x}, \\bar{y})$ of the blob by doing:\n\n$$\\bar{x} = \\frac{M_{10}}{M_{00}}$$\n$$\\bar{y} = \\frac{M_{01}}{M_{00}}$$","1c4aaa59":"The next function performs the classification. The feature vectors were split in training and test sets with 70\/30 proportion, respectively, with a default number of test iterations set to 100. This function returns a dataframe with the results of each round.","525314dc":"### Hu Moments\n\nWe call **Hu Moments** the set of 7 values propesed by Hu in his 1962 work _Visual Pattern Recognition by Moment Invariants_:\n\n$h_1 = \\eta_{20} + \\eta_{02}$\n\n$h_2 = (\\eta_{20} - \\eta_{02})^2 + 4(\\eta_{11})^2$\n\n$h_3 = (\\eta_{30} - 3\\eta_{12})^2 + 3(\\eta_{03} - 3\\eta_{21})^2$\n\n$h_4 = (\\eta_{30} + \\eta_{12})^2 + (\\eta_{03} + \\eta_{21})^2$\n\n$h_5 = (\\eta_{30} - 3\\eta_{12})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{03} + \\eta_{21})^2] + (3\\eta_{21} - \\eta_{03})(\\eta_{03} + \\eta_{21})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{03} + \\eta_{21})^2]$\n\n$h_6 = (\\eta_{20} - \\eta_{02})[(\\eta_{30} + \\eta_{12})^2 - 7(\\eta_{03} + \\eta_{21})^2] + 4\\eta_{11}(\\eta_{30} + \\eta_{12})(\\eta_{03} + \\eta_{21})$\n\n$h_7 = (3\\eta_{21} - \\eta_{03})(\\eta_{30} + \\eta_{12})[(\\eta_{30} + \\eta_{12})^2 - 3(\\eta_{03} + \\eta_{21})^2] + (\\eta_{30} - 3\\eta_{12})(\\eta_{03} + \\eta_{21})[3(\\eta_{30} + \\eta_{12})^2 - (\\eta_{03} + \\eta_{21})^2]$","f7111ee4":"### Translation Invariance\n\nThe centroid can be used to rewrite the raw moment equation to achieve the **translation invariant** momento $\\mu_{pq}$:\n\n$$\\mu_{pq} = \\sum_{x=0}^{N-1}\\sum_{y=0}^{N-1}(x - \\bar{x})^p(y - \\bar{y})^qI(x, y)$$\n\nNow, the relative spatial information of the centroid is being take in consideration, so no matter where the blob is localized the moments will be (roughly) the same.","77ff21e1":"### Raw Moments\n\nWe define _Image Moments_ as **the weighted average of all pixel intensities of a image**. Consider a binary image described by the function $I(x, y)$ with dimenisons $NxN$ pixels (any size is possible, not just square ones), we can calculate the raw moments using:\n\n$$M_{pq} = \\sum_{x=0}^{N-1}\\sum_{y=0}^{N-1}x^py^qI(x, y)$$\n\nThe above expression shows a summation of all pixel intensities pondered by its location $(x, y)$ over the powers $p$ and $q$. In other words, image moments are values that carry both spatial and intensity information, e.g, **shape**. $p$ and $q$ are the weights of the horizontal and vertical dimensions, respectivelly. The sum $p+q$ is the _moment order_.","d91d2988":"### Scale Invariance\n\nScaling (change of size) is another very common transformation performed in images. This scaling can be uniform (the same in both dimensions) or non-uniform. Hu showed that you can relate the zero order translate invariant moment to get scale invariants $\\eta_{pq}$:\n\n$$\\eta_{pq} = \\frac{\\mu_{pq}}{\\mu_{00}^{1 + \\frac{p+q}{2}}}$$","26763cd6":"### The Code\n\nThe code is written in Python 3.x and makes use of the following packages: ```opencv-python```, ```scikit-learn```, ```numpy``` and ```pandas```. First, let's import the all the necessary packages:","38029b3b":"### Conclusion\n\nAs we can see, all classifiers performed almost perfectly in this task, with the only exception being the NC. The SVM classifier configured with Linear Kernel had the highest mean accuracy and lowest standard deviation, making it the best choice in this simple application. With simple math involved and a rather fast extraction\/classification time, the Hu Moments are a good benchmark for visual pattern recognition, as well as a good entry example for those who are starting with computer vision.","4d9103da":"The next line executes it and stores the result in ```ans```:","5811996e":"Next, we define two dictionaries. The first represents the relationship between the class name literal and a numerical label. The second, the set of classifiers chosen for this example:","c15932f1":"The first function we gonna create is responsible to extract the Moments of all samples, storing the class label as a eight column and save everything in a ```CSV``` file. Also, a simple logarithmic transformation is performed to equalize the orders of the moments, wich is a good pre-processing step when handling this type of feature.","e219f71c":"### Example: The Four Shapes Dataset\n\nTo illustrate the use of Hu Moments in pattern recognition, we will use the **Four Shapes Dataset**. This dataset contains 14970 samples of four classes: circles, squares, stars and triangles. You can get a free copy of the data in [Kaggle](https:\/\/www.kaggle.com\/smeschke\/four-shapes). We created the bellow animations using the first 400 samples of each class to showcase the diversity of examples:\n\n|                Circles               |                Squares               |                 Stars                |               Triangles              |\n|:------------------------------------:|:------------------------------------:|:------------------------------------:|:------------------------------------:|\n| ![](https:\/\/i.imgur.com\/XcY2ehb.gif) | ![](https:\/\/i.imgur.com\/hDzvMYj.gif) | ![](https:\/\/i.imgur.com\/FRE8Pl1.gif) | ![](https:\/\/i.imgur.com\/ThGXvP8.gif) |","ada1aec0":"Let's visualize the classification performance in a more frendly way using the ```summary()``` function:","90b48892":"### Hu Moments for Image Recognition: Intro\n\nMoments are very common features extracted from images to be used in pattern recogntion tasks, such as face recognition and shape retrieval. In this short notebook, let's take a look at Hu Moments, undoubtedly the most important work of this domain. The main reference is the work **Visual Pattern Recognition by Moment Invariants** wrote by MK Hu in [1962](https:\/\/ieeexplore.ieee.org\/document\/1057692). Both the math and Python code are provided to better understanding of the subject, alongside more detailed explanations. Please feel free to help, share and improve this document."}}