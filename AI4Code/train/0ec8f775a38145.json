{"cell_type":{"73f95548":"code","ff7d4a7e":"code","7578fcb0":"code","5f05938a":"code","4ab75d01":"code","7fb191dc":"code","d25d6be0":"code","cf2349a4":"code","e6395bce":"code","3c7a72ae":"code","f13a5b8a":"code","d71bab13":"code","4ea9cab3":"code","6fe05746":"code","0f3ee4d0":"code","eacbb8dc":"code","ee7a80ab":"code","a13af6b2":"code","7f3c50fb":"code","0b4ac90b":"code","4cbe13c0":"code","03d1f785":"code","cd0faeae":"code","2c7466b0":"code","c4c72456":"code","22cc64c6":"code","ea18c42e":"code","3c5d905c":"code","88fcd597":"markdown","f1187de4":"markdown","82ee9321":"markdown","89925429":"markdown","38d6f492":"markdown","20aace54":"markdown","f5750631":"markdown","10ad4605":"markdown","a0080a5a":"markdown","bc696ea6":"markdown","467da4e4":"markdown","a63f7cf4":"markdown","3ef75f9f":"markdown","52816dda":"markdown","8c57a2d7":"markdown","344b63a8":"markdown","0b70f0ad":"markdown","ef510b97":"markdown","a9c652ec":"markdown","57b892a0":"markdown","6f7f0673":"markdown"},"source":{"73f95548":"import numpy as np\nimport pandas as pd\n#from sklearn.linear_model import LinearRegression\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re","ff7d4a7e":"# Data path\npath = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\n\n#Load\ntrain = pd.read_csv(path + 'sales_train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample_submission = pd.read_csv(path + 'sample_submission.csv')\nitems = pd.read_csv(path + 'items.csv')\nitem_categories = pd.read_csv(path + 'item_categories.csv')\nshops = pd.read_csv(path + 'shops.csv')","7578fcb0":"#look at data\nprint(\"************** SALES_TRAIN **************\")\nprint(train.describe())\nprint(train.head(6))\nprint(\"************** TEST **************\")\nprint(test.head(3))\nprint(test.describe())\n#print(test[46359:46362])\n#print(test[45:65])\nprint(\"************** OTHERS **************\")\nprint(sample_submission.head(3))\nprint(items.head(1))\nprint(item_categories.head(1))\nprint(shops.head(1))","5f05938a":"#check nulls\nprint(\"Check for Nulls:\")\nprint(train.isnull().sum())\nprint(test.isnull().sum())","4ab75d01":"#Difference Stores and Items between train and test sets\ndif1_a = list(set(train['shop_id']) - set(test['shop_id']))\nprint(\"Dif shop_id - TRAIN NOT TEST: \", dif1_a)\n\ndif1_b = list(set(test['shop_id']) - set(train['shop_id']))\nprint(\"Dif shop_id - TEST NOT TRAIN: \", dif1_b)\n\ndif2_a = list(set(train['item_id']) - set(test['item_id']))\nprint(\"Amount Dif item_id - TRAIN NOT TEST: \", len(dif2_a))\n\ndif2_b = list(set(test['item_id']) - set(train['item_id']))\nprint(\"Amount Dif item_id - TEST NOT TRAIN: \", len(dif2_b))\n\nprint(\"There are \", len(dif2_a), \" items not sold and \" , len(dif2_b) ,\" new.\")","7fb191dc":"#Groupby boughts on the same month\ntrain_grouped_d_s = train.groupby([\"date_block_num\", \"shop_id\"], as_index=False)[\"item_cnt_day\"].sum()\ntrain_grouped_d_i = train.groupby([\"date_block_num\", \"item_id\"], as_index=False)[\"item_cnt_day\"].sum()\ntrain_grouped_d_s_i = train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"], as_index=False)[\"item_cnt_day\"].sum()\ntrain_grouped_d = train.groupby([\"date_block_num\"], as_index=False)[\"item_cnt_day\"].sum()\nprint(\"************** GROUPED BY DATE AND SHOP_ID **************\")\nprint(train_grouped_d_s.describe())\nprint(train_grouped_d_s.head(3))\nprint(\"************** GROUPED BY DATE AND ITEM_ID **************\")\nprint(train_grouped_d_i.describe())\nprint(train_grouped_d_i.head(3))\nprint(\"************** GROUPED BY DATE AND SHOP_ID AND ITEM_ID **************\")\nprint(train_grouped_d_s_i.describe())\nprint(train_grouped_d_s_i.head(3))\nprint(\"************** GROUPED BY DATE **************\")\nprint(train_grouped_d.describe())\nprint(train_grouped_d.head(3))","d25d6be0":"#Groupby boughts of the same item and\/or shop\ntrain_grouped_s = train.groupby([\"shop_id\"], as_index=False)[\"item_cnt_day\"].sum()\ntrain_grouped_i = train.groupby([\"item_id\"], as_index=False)[\"item_cnt_day\"].sum()\ntrain_grouped_s_i = train.groupby([\"shop_id\",\"item_id\"], as_index=False)[\"item_cnt_day\"].sum()\nprint(\"************** GROUPED BY SHOP_ID **************\")\nprint(train_grouped_s.describe())\nprint(train_grouped_s.head(3))\nprint(\"************** GROUPED BY ITEM_ID **************\")\nprint(train_grouped_i.describe())\nprint(train_grouped_i.head(3))\nprint(\"************** GROUPED BY SHOP_ID AND ITEM_ID **************\")\nprint(train_grouped_s_i.describe())\nprint(train_grouped_s_i.head(3))","cf2349a4":"#PLOT MONTLY TOTAL ITEMS SOLD - cheack seasonality\nfig1 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d)\nfig1a = fig1.fig \nfig1a.suptitle(\"Total solds Time Series\", fontsize=12)\n\n#ITEMS SOLD PER ID - check outliers\nfig2 = sns.relplot(x='item_id', y='item_cnt_day', data=train_grouped_i)\nfig2a = fig2.fig \nfig2a.suptitle(\"Items sold\", fontsize=12)\n\n#ITEMS SOLD PER STORE - check outliers\nfig3 = sns.relplot(x='shop_id', y='item_cnt_day', data=train_grouped_s)\nfig3a = fig3.fig \nfig3a.suptitle(\"Solds per Shop\", fontsize=12)","e6395bce":"#Check what items are outliers\nprint(\"*** Item outlier ***\")\nfor i in range(1,len(train_grouped_i)):\n    if train_grouped_i.iloc[i,1] >=25000: \n        print(train_grouped_i.iloc[i,0] , \" -> \" , items.iloc[i,0])\n                \n#Check what stores were outliers\nprint(\"*** Biggest shop ***\")\nfor i in range(1,len(train_grouped_s)):\n    if train_grouped_s.iloc[i,1] >=250000: \n        print(train_grouped_s.iloc[i,0] , \" -> \" , shops.iloc[i,0])","3c7a72ae":"#Check if item outlier is legitim\nprint(\"*** Check if Best Seller item (outlier) is 'legitim' ***\")                               \n#ITEMS SOLD PER ID 20949\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 20949])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 20949\", fontsize=12)\n\nprint(\"*** Check Biggest shop follows the typical trend ***\")   \n#SHOPS SOLD PER SHOP 31\nfig5 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s.loc[train_grouped_d_s['shop_id'] == 31])\nfig5a = fig5.fig \nfig5a.suptitle(\"Solds Shop 31\", fontsize=12)\n\nprint(\"*** Check Best Seller item at Biggest shop ***\")  \n#TEMS SOLD AT SHOP 31\nfig6 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s_i.loc[(train_grouped_d_s_i['item_id'] == 20949) & (train_grouped_d_s_i['shop_id'] == 31)])\nfig6a = fig6.fig \nfig6a.suptitle(\"Solds of item 20949 at Shop 31\", fontsize=12)\n","f13a5b8a":"#Check other Shops\nprint(\"*** Check other Shops ***\")    \nfig5 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s.loc[train_grouped_d_s['shop_id'] == 2])\nfig5a = fig5.fig \nfig5a.suptitle(\"Solds Shop 2\", fontsize=12)\n\nfig6 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s_i.loc[(train_grouped_d_s_i['item_id'] == 20949) & (train_grouped_d_s_i['shop_id'] == 2)])\nfig6a = fig6.fig \nfig6a.suptitle(\"Solds of item 20949 at Shop 2\", fontsize=12)\n\nfig5 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s.loc[train_grouped_d_s['shop_id'] == 12])\nfig5a = fig5.fig \nfig5a.suptitle(\"Solds Shop 12\", fontsize=12)\n\nfig6 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s_i.loc[(train_grouped_d_s_i['item_id'] == 20949) & (train_grouped_d_s_i['shop_id'] == 12)])\nfig6a = fig6.fig \nfig6a.suptitle(\"Solds of item 20949 at Shop 12\", fontsize=12)","d71bab13":"#Check other items outliers \nprint(\"*** Check other items ***\")    \nprint(items.loc[items[\"item_id\"] == 7223])\nprint(train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 7223, 'item_cnt_day'].sum())\nprint(items.loc[items[\"item_id\"] == 3731])\nprint(train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 3731, 'item_cnt_day'].sum())\nprint(items.loc[items[\"item_id\"] == 3733])\nprint(train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 3733, 'item_cnt_day'].sum())\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 3731])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 3731\", fontsize=12)\n\nfig6 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s_i.loc[(train_grouped_d_s_i['item_id'] == 3731) & (train_grouped_d_s_i['shop_id'] == 12)])\nfig6a = fig6.fig \nfig6a.suptitle(\"Solds of item 3731 at Shop 12\", fontsize=12)\n\nfig6 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_s_i.loc[(train_grouped_d_s_i['item_id'] == 3731) & (train_grouped_d_s_i['shop_id'] == 31)])\nfig6a = fig6.fig \nfig6a.suptitle(\"Solds of item 3731 at Shop 31\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 3733])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 3733\", fontsize=12)\n","4ea9cab3":"#Check other items atypical \nprint(\"*** Identify type of items atypical ***\")  \n\nprint(10447, \" -> \", items.loc[10447]['item_name'])\nprint(17717, \" -> \", items.loc[17717]['item_name'])\nprint(2293, \" -> \", items.loc[2293]['item_name'])\nprint(3460, \" -> \", items.loc[3460]['item_name'])\nprint(1555, \" -> \", items.loc[1555]['item_name'])\n\nprint(6675, \" -> \", items.loc[6675]['item_name'])\nprint(20404, \" -> \", items.loc[20404]['item_name'])\nprint(20405, \" -> \", items.loc[20405]['item_name'])\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 10447])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 10447\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 17717])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 17717\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 2293])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 2293\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 3460])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 3460\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 1555])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 1555\", fontsize=12)\n\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 6675])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 6675\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 20404])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 20404\", fontsize=12)\n\nfig4 = sns.relplot(x='date_block_num', y='item_cnt_day', data=train_grouped_d_i.loc[train_grouped_d_i['item_id'] == 20405])\nfig4a = fig4.fig \nfig4a.suptitle(\"Solds item 20405\", fontsize=12)","6fe05746":"aty_items =  []\nprint(\"\u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u0441\u0443\u0431\u0442\u0438\u0442\u0440 - Russian subtitles \") \nfor i in range(0,len(items['item_name'])):\n    txt = items.loc[i]['item_name']\n    x = re.search(\"\u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u044b\", txt)\n    if x:\n        #print(items.loc[i]['item_name'])\n        aty_items.append(i)\n\nprint(\"\u0440\u0443\u0441\u0441\u043a\u0438\u0435 \u0441\u0443\u0431\u0442\u0438\u0442\u0440 - Russian version \")\nfor i in range(0,len(items['item_name'])):\n    txt = items.loc[i]['item_name']\n    x = re.search(\"\u0440\u0443\u0441\u0441\u043a\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f\", txt)\n    if x:\n        #print(items.loc[i]['item_name'])\n        aty_items.append(i)\n        \nprint(aty_items)","0f3ee4d0":"#drop Shop that are not in the test set\n#[0, 1, 32, 33, 8, 9, 40, 11, 43, 13, 17, 51, 20, 54, 23, 27, 29, 30]\nstg_dsi = train_grouped_d_s.copy()\nfor i in dif1_a:\n    stg_dsi.drop(stg_dsi[stg_dsi['shop_id'] == i].index, inplace = True)\n\n#CONFIRM: Difference Stores and Items between train and test sets\ndif10_a = list(set(stg_dsi['shop_id']) - set(test['shop_id']))\nprint(\"Dif shop_id A: \", dif10_a)","eacbb8dc":"#solds every shops\nprint(\"*** All shops ***\")   \ng = sns.FacetGrid(stg_dsi, col=\"shop_id\", col_wrap=4, height=2,  xlim=(10, 35), ylim=(0, 5000)) #ylim=(-1000, 10000))\ng.map(sns.regplot, \"date_block_num\", \"item_cnt_day\") #, order=[1, 2, 3], color=\".3\", ci=None)","ee7a80ab":"#1 #Make regression on each shop of the test set \ntest_s_list = np.unique(test['shop_id'])\nprint(test_s_list)\n\nshop_func1 = []\n# stg_dsi1 = stg_dsi.loc[(stg_dsi['date_block_num'] == 10) | (stg_dsi['date_block_num'] == 22)]   #IF CONSIDERING ONLY NOVEMBER MONTHS\n# stg_dsi1 = stg_dsi.loc[(stg_dsi['date_block_num'] != 11) & (stg_dsi['date_block_num'] != 23)]   #NOT CONSIDERING DECEMBER MONTHS\nfor j in range(0,len(test_s_list)):\n    #if ( (test_s_list[j] == 34) | (test_s_list[j] == 36) | (test_s_list[j] == 39) | (test_s_list[j] == 48) | (test_s_list[j] == 49) ): #don't have both those months      #IF CONSIDERING ONLY NOVEMBER MONTHS\n    #    train_shopj1 = stg_dsi.loc[(stg_dsi['shop_id'] == test_s_list[j])]     #IF CONSIDERING ONLY NOVEMBER MONTHS      \n    #train_shopj1 = stg_dsi1.loc[(stg_dsi1['shop_id'] == test_s_list[j]) & (stg_dsi1['date_block_num'] >= 1)] #Ignore month 0, contains some 0 data at some shops\n    train_shopj1 = stg_dsi.loc[(stg_dsi['shop_id'] == test_s_list[j])]\n    t_data1 = np.array(train_shopj1['date_block_num'])\n    y_data1 = np.array(train_shopj1['item_cnt_day'])\n\n    def func(t_data1, a, b): \n        #return a * (b ** np.exp(t_data)) + c  #exponential\n        return a * t_data1 + b   #linear\n\n    InitialParams1 = [0.1, 1000]\n    fitParams1, pcov1 = curve_fit(func, t_data1, y_data1, p0=InitialParams1, method='dogbox')\n    \n    shop_func1.append(fitParams1)\n","a13af6b2":"#2 #TOTAL SELLS IN THE THE MONTH TO PREDICT, for each store\nprint(\"November 15th, 2015 , #1 ->\")\ny_month1 = []\n\nfor j in range(0,len(test_s_list)):\n    sells1 = shop_func1[j][0] * 34 + shop_func1[j][1]\n    print(sells1)\n    y_month1.append(sells1)\n    #sign if negative\n    if sells1 <= 0:\n        print(j)","7f3c50fb":"#3 #Make regression on each shop of the test set, considering only the last 3 months\n\nshop_func2 = []\nfor j in range(0,len(test_s_list)):\n    train_shopj2 = stg_dsi.loc[(stg_dsi['shop_id'] == test_s_list[j]) & (stg_dsi['date_block_num'] >= 31)]\n    t_data2 = np.array(train_shopj2['date_block_num'])\n    y_data2 = np.array(train_shopj2['item_cnt_day'])\n    \n    def func(t_data2, a, b): \n        #return a * (b ** np.exp(t_data)) + c  #exponential\n        return a * t_data2 + b   #linear\n\n    InitialParams2 = [0.1, 1000]\n    fitParams2, pcov2 = curve_fit(func, t_data2, y_data2, p0=InitialParams2, method='dogbox')\n    \n    shop_func2.append(fitParams2)\n","0b4ac90b":"#4 #TOTAL SELLS IN THE THE MONTH TO PREDICT, for each store\nprint(\"November 15th, 2015, #2 ->\")\ny_month2 = []\n\nfor j in range(0,len(test_s_list)):\n    sells2 = shop_func2[j][0] * 34 + shop_func2[j][1]\n    print(sells2)\n    y_month2.append(sells2)\n    #sign if negative\n    if sells2 <= 0:\n        print(j)","4cbe13c0":"y_month = []\n\nfor j in range(0,len(y_month1)):\n    y_month.append((y_month1[j] + y_month2[j]) \/ 2)\n    \nprint(y_month)","03d1f785":"#Create new Sets to use\n\nperc_item_s = train_grouped_d_s_i.copy() \nprint(\"perc_item_s - Before - All months\")\nprint(\"Total = \", perc_item_s[\"item_cnt_day\"].sum())\nprint(\"Shop2 = \", perc_item_s.loc[(perc_item_s['shop_id'] == 2), 'item_cnt_day'].sum())\nprint(\"Item 20949 at Shop2 = \", perc_item_s.loc[(perc_item_s['item_id'] == 20949) & (perc_item_s['shop_id'] == 2), 'item_cnt_day'].sum())\n\n#Consider only last 3 months and November month\nperc_item_s = perc_item_s.loc[ (perc_item_s['date_block_num'] == 22) | (perc_item_s['date_block_num'] >= 31)]  #(perc_item_s['date_block_num'] == 10) | \n#correct outliers atypical items (such as item id = 3733) - replace old by most recent\nfor i in range(len(aty_items)): \n    perc_item_s.loc[(perc_item_s['date_block_num'] == 22) & (perc_item_s['item_id'] == aty_items[i])] = perc_item_s.loc[(perc_item_s['date_block_num'] == 33) & (perc_item_s['item_id'] == aty_items[i])]   \n#correct 20949 and 6675\nperc_item_s.loc[(perc_item_s['date_block_num'] == 22) & (perc_item_s['item_id'] == 20949)] = perc_item_s.loc[(perc_item_s['date_block_num'] == 33) & (perc_item_s['item_id'] == 20949)] \nperc_item_s.loc[(perc_item_s['date_block_num'] == 22) & (perc_item_s['item_id'] == 6675)] = perc_item_s.loc[(perc_item_s['date_block_num'] == 33) & (perc_item_s['item_id'] == 6675)] \n\nprint(\"perc_item_s - After - Reduced months\")\nprint(\"Total = \", perc_item_s[\"item_cnt_day\"].sum())\nprint(\"Shop2 = \", perc_item_s.loc[(perc_item_s['shop_id'] == 2), 'item_cnt_day'].sum())\nprint(\"Item 20949 at Shop2 = \", perc_item_s.loc[(perc_item_s['item_id'] == 20949) & (perc_item_s['shop_id'] == 2), 'item_cnt_day'].sum())\nperc_item_s = perc_item_s.groupby([\"shop_id\",\"item_id\"], as_index=False)[\"item_cnt_day\"].sum()\n\n#drop Shop that are not in the test set\n#[0, 1, 32, 33, 8, 9, 40, 11, 43, 13, 17, 51, 20, 54, 23, 27, 29, 30]\nfor i in dif1_a:\n    perc_item_s.drop(perc_item_s[perc_item_s['shop_id'] == i].index, inplace = True)\nprint(\"perc_item_s drop shops - head: \", perc_item_s.head(5))\nprint(\"Total sells in these shops  = \" , perc_item_s['item_cnt_day'].sum())\n\nprint(\"Amount of items in TEST: \", len(test['item_id'].unique()))\nprint(\"Amount of items in perc_item_s: \", len(perc_item_s['item_id'].unique()))\ndif3_b = list(set(test['item_id']) - set(perc_item_s['item_id']))\nprint(\"Amount Dif item_id - TEST NOT perc_item_s: \", len(dif3_b))\ndif4_b = list(set(perc_item_s['item_id']) - set(test['item_id']))\nprint(\"Amount Dif item_id - perc_item_s NOT TEST: \", len(dif4_b))\n\n#equivalent dataframe with sum of items for each shop\ntrain_s2 = train.copy()\ntrain_s2 = train_s2.loc[ (train_s2['date_block_num'] == 22) |  (train_s2['date_block_num'] >= 31)]  #(train_s2['date_block_num'] == 10) | \nprint(len(train_s2))\nprint(\"train_s2 - After - Reduced months\")\nprint(\"Total = \", train_s2[\"item_cnt_day\"].sum())\nprint(\"Shop2 = \", train_s2.loc[(train_s2['shop_id'] == 2), 'item_cnt_day'].sum())\nprint(\"Item 20949 at Shop2 = \", train_s2.loc[(train_s2['item_id'] == 20949) & (train_s2['shop_id'] == 2), 'item_cnt_day'].sum())\ntrain_s2 = train_s2.groupby([\"shop_id\"], as_index=False)[\"item_cnt_day\"].sum()\nprint(\"train_s2 - head: \", train_s2.head(5))\n\ndif5_a = list(set(perc_item_s['shop_id']) - set(train_s2['shop_id']))\ndif5_b = list(set(train_s2['shop_id']) - set(perc_item_s['shop_id']))\nprint(\"Amount Dif item_id - perc_item_s NOT train_s2: \", dif5_a)\nprint(\"Amount Dif item_id - train_s2 NOT perc_item_s: \", dif5_b)","cd0faeae":"#Calculate Percentages\n\nperc_item_s['sh_sum'] = np.nan\n\n#use shop sum for from train_s2 to perc_item_s\nfor i in range(len(perc_item_s['shop_id'])):\n    aa = train_s2.loc[train_s2['shop_id'] == perc_item_s.iat[i,0]]['item_cnt_day'].tolist()\n    perc_item_s.iat[i,3] = aa[0]    #['sh_sum']\n#calculate percentage\nperc_item_s['percent'] =  perc_item_s['item_cnt_day']\/ perc_item_s['sh_sum']\n\nprint(\"perc_item_s - head: \", perc_item_s.head(5))\nprint(\"perc_item_s - tail: \", perc_item_s.tail(5))\nprint(perc_item_s.loc[(perc_item_s['shop_id'] == 31)])\nprint(perc_item_s.loc[(perc_item_s['shop_id'] == 31)  &  (perc_item_s['item_id'] == 20949)])\nprint(\"shop 31 percentage sum: \", perc_item_s.loc[(perc_item_s['shop_id'] == 31)]['percent'].sum())\nprint(perc_item_s.loc[(perc_item_s['shop_id'] == 5)])\nprint(\"shop 5 percentage sum: \", perc_item_s.loc[(perc_item_s['shop_id'] == 5)]['percent'].sum())","2c7466b0":"test_res = pd.merge(test, perc_item_s, how=\"left\", on=[\"shop_id\",\"item_id\"])\n\nprint(\"Check if percentages total in new test_res set are lower\")\nprint(test_res.loc[(test_res['shop_id'] == 31)]['percent'].sum())\nprint(test_res.loc[(test_res['shop_id'] == 5)]['percent'].sum())\n\ntest_res['final_res'] = np.nan\nprint(test_res)\n\nfor i in range(len(test_res['shop_id'])):\n    bb = test_res.loc[(test_res['shop_id'] == test_res.iat[i,1])]\n    \nprint(\"Sells per shop per item in Nov 2015\")\nfor i in range(0,len(test_res['ID'])):   \n    posit = np.where(test_s_list == test_res.iat[i,1])[0][0] #shop_id position in test_s_list\n    test_res.iat[i,6] = y_month[posit] * test_res.iat[i,5] \n    \nprint(test_res)\n\ntot_estimate = test_res['final_res'].sum()\nprint(\"Total sells estimated = \", tot_estimate)\n","c4c72456":"print(\"There are several NAs, that correspond to new products, or products that where not in previous months used in estimation\")\nprint(test_res.head(5))\n\nprint(\"Amount of items in TEST: \", len(test['item_id'].unique()))\nprint(\"Amount of items in test_res: \", len(test_res['item_id'].unique()))\ndif5_b = list(set(test['item_id']) - set(test_res['item_id']))\nprint(\"Amount Dif item_id - TEST NOT test_res: \", len(dif5_b))\ndif6_b = list(set(test_res['item_id']) - set(test['item_id']))\nprint(\"Amount Dif item_id - test_res NOT TEST: \", len(dif6_b))\n#print(\"total shop 5 with NA: \", test_res.loc[(test_res['shop_id'] == 5)]['final_res'].sum())\n\n#Replacing NAs whith a value lower (20%) of that shop mean\nt_aux = []\nfor j in range(0,len(test_s_list)):\n    t_aux0 = test_res.loc[test_res['shop_id'] == test_s_list[j]]  #test_res.iat[i,1]\n    t_aux.append([test_s_list[j], t_aux0['final_res'].mean() * 0.20]) \nprint(t_aux)\n\nfor i in range(0,len(test_res['final_res'])):   \n    if np.isnan(test_res.iat[i,6]) == True:\n        posaux = np.where(test_s_list == test_res.iat[i,1])[0][0]\n        test_res.iloc[i,6] = t_aux[posaux][1]\n\n#print(\"total shop 5 without NA: \", test_res.loc[(test_res['shop_id'] == 5)]['final_res'].sum())\n        \nprint(\"test_res final\", test_res)       \n\nprint(\"Final total sells estimated = \", test_res['final_res'].sum())","22cc64c6":"#Check results\ntest_res_grouped = test_res.groupby([\"shop_id\"], as_index=False)[\"final_res\"].sum()\ntest_res_grouped[\"date_block_num\"] = 34\ntest_res_grouped.rename(columns = {'final_res' : 'item_cnt_day'}, inplace = True)\ntrain_test_res = pd.concat([stg_dsi,test_res_grouped])\nprint(train_test_res.head())\nprint(train_test_res.tail())\n\n#solds every shops\nprint(\"*** All shops ***\")   \ng = sns.FacetGrid(train_test_res, col=\"shop_id\", col_wrap=4, height=2, xlim=(25, 36), ylim=(100, 7500))\ng.map(sns.regplot, \"date_block_num\", \"item_cnt_day\") #, order=[1, 2, 3], color=\".3\", ci=None)","ea18c42e":"#TOTAL SOLDS AT NOV 2015\nfig7 = sns.relplot(x='ID', y='final_res', data=test_res)\nfig7a = fig7.fig \nfig7a.suptitle(\"Solds at 11\/2015\", fontsize=12)\n\n#SHOPS SOLD PER SHOP 31\nfig9 = sns.relplot(x='ID', y='final_res', data=test_res.loc[test_res['shop_id'] == 31])\nfig9a = fig9.fig \nfig9a.suptitle(\"Solds Shop 31 at 11\/2015\", fontsize=12)\n\n#TEMS SOLD AT SHOP 31\nfig10 = sns.relplot(x='ID', y='final_res', data=test_res.loc[(test_res['item_id'] == 20949) & (test_res['shop_id'] == 31)])\nfig10a = fig10.fig \nfig10a.suptitle(\"Solds of item 20949 at Shop 31 at 11\/2015\", fontsize=12)\n","3c5d905c":"#Submission table\nsubmit = test_res[['ID','final_res']]\nsubmit.columns= ['ID','item_cnt_month']\n\n#True target values are clipped into [0,20] range.\nprint(\"Clip into [0,20] range\")\nsubmit['item_cnt_month'][submit['item_cnt_month'] > 20] = 20\nsubmit['item_cnt_month'][submit['item_cnt_month'] < 0] = 0\nprint(submit.head(10))\nprint(submit.tail(10))\nprint(\"Final checks\")\nprint(submit[46359:46362])\nprint(submit[45:55])\nprint(submit[460:465])\n\nprint(\"Final total sells estimated, after clip = \", submit['item_cnt_month'].sum())\n\nsubmit.to_csv('submission.csv', index=False)","88fcd597":"The total results for sells at each Shop was predicted above. <br>\nNow, let's find Percentage of sells of each Item at each Shop in the past data. <br>\nTo be more accurate, let's consider only the last 3 months and previous November month. <br>\nNote: don't consider last November for the item outliers, such as id=3733 and the identified videogames. <br>\n\nThen, using the total sells and the percentages, the predicted sells of each Item at each Shop can be calculated. <br>\nConsidering only some months will increase the amount of Items without prediction that will appear in the test set. Those NAs will have to be throughourly replaced. <br> \nThus, the NA values, corresponding to items there haven't been sold in the past (considered), are estimated according to the mean of sells at the same shop.\n\n","f1187de4":"**Check Outliers**","82ee9321":"# #2 Considering only the last 3 months","89925429":"# #1 Considering all months","38d6f492":"# APPLY PREDICTION TO TEST SET","20aace54":"**Replace NAs whith 20% of that shop mean**","f5750631":"# Regression model","10ad4605":"Most of these are Videogames and have atypical seasonal trends - they could be identified by item_category, but that could cover a very large part of the train set. <br>\nThese videogames names all share part of their name, regarding their language or translation. <br>\nThe sells of these type of items will be forecasted only based only on their latter months.","a0080a5a":"**Drop unused Shops and check final processed data**","bc696ea6":"There was a item identified as outlier (id=20949), but it was legitime and followed a similar trend as the others. <br>\nThere are several items, particularly videogames, with a high seasonal trend, that should be taken into account. They were identified through the keywords \"subtitles\" and \"version\", in Russian. <br>\nThere are smaller and larger shops, being the larger one the shop id=31. But its seasonality is similar to the others. <br>\nThe sells seems to be decreasing. They have a yearly seasonal trend, with a peak around at december, which might be related with the christmas season. The prediction desired is for November, and therefore, the christmas peak could be considered as an outlier. <br>\nThere are several possible methods to deal with seasonality in time series forecast. Looking at the Figure, it seems reasonable that it could be use the november month alone, from 2014 and 2015, to predict the November 2015 sales. Or simply apply a linear regression with all the months included. Or apply a linear regression for the recent trend, such as considering only the last 3 months.<br>\nOr both! Let's do both and average it.","467da4e4":"**Percentage of sells of each Item at each Shop!** <br>\nOnly from the last 3 months and last November month","a63f7cf4":"# Predictive Sales using simple Linear Regressions\nFrom daily historical data of sales from January 2013 to October 2015, <br>\nthe goal is to forecast the sales for the shops and products for November 2015.  <br>\n![Best_Seller](https:\/\/i.ebayimg.com\/images\/g\/GkEAAOSwxH1ULE5R\/s-l300.jpg)","3ef75f9f":"There was a item identified as outlier (id=20949), but it seems legitim. <br>\nThe largest shop (shop_id=31) also seems to follow a typical trend. <br>\nThere are some other shops and items that deserve further investigation. <br>","52816dda":"# Check Results and Submit","8c57a2d7":"**Validate Data**","344b63a8":"# #Average results from both trends to a final result","0b70f0ad":"# Load Data","ef510b97":"# Data Cleaning and Analysis","a9c652ec":"**Look at data**","57b892a0":"**Estimate predicted sells based on the total sells and the percentages.**","6f7f0673":"**Group data to useful sets**"}}