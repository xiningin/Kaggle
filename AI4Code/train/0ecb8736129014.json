{"cell_type":{"573b5c7d":"code","8dc281d2":"code","69f4d9cd":"code","95bc9cc7":"code","58a3487c":"code","742b6aaf":"code","be799c5c":"code","926318c5":"code","d3f7b0a2":"code","6db0379a":"code","4de46d80":"code","3aa8bd8a":"code","b9e4f916":"code","c6837c97":"code","e54d79d0":"code","c22f00cf":"code","5b12e73d":"code","42add460":"code","6cdcc641":"code","9c8f033e":"code","e71a3a6a":"code","e7fa080c":"code","392e953f":"code","ca2aaa74":"code","8fd986c6":"code","90e7d61e":"code","586a6515":"code","ba463da3":"code","5e26aea7":"code","d58826f0":"code","1ebf35c4":"code","6efef22d":"code","4bb948d7":"code","fd19f0b1":"code","649b11b1":"code","e07ed02f":"code","f7a5d9f5":"code","dd282939":"code","7165c394":"code","148f4da9":"code","2a9c454f":"code","05661d02":"code","825b598b":"code","4380a4cc":"code","d8b97ce3":"code","66f0cb43":"code","81b2ab6b":"code","cbf804fb":"code","4c16ca9b":"code","d993204a":"code","16986a24":"code","51446a0c":"code","61dd511d":"code","5b0b40b8":"code","5092324a":"code","6a106202":"code","7c3007af":"code","89357dba":"code","c29ec359":"code","4d78030f":"code","22a857be":"code","0b9ce659":"code","f619310d":"markdown","931d34ce":"markdown","5f507875":"markdown","984a87b7":"markdown","b910274e":"markdown","b05890f6":"markdown","af7b0b22":"markdown","56b0766c":"markdown","087cf89f":"markdown","6b8454e7":"markdown","04ad7fa1":"markdown","2d9ed9ef":"markdown","a81d8427":"markdown"},"source":{"573b5c7d":"import os\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8dc281d2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","69f4d9cd":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","95bc9cc7":"test_data.head()\ntrain_data.head()","58a3487c":"print(train_data.shape)\nprint(test_data.shape)","742b6aaf":"# feature names\nprint(train_data.columns.values)","be799c5c":"train_data.info()","926318c5":"train_data.describe()","d3f7b0a2":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6db0379a":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4de46d80":"train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3aa8bd8a":"train_data[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b9e4f916":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","c6837c97":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","e54d79d0":"sns.factorplot(x='Survived', col='Sex', kind='count', data=train_data)","c22f00cf":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","5b12e73d":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","42add460":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_data, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","6cdcc641":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","9c8f033e":"train_data.isnull().sum()","e71a3a6a":"test_data.isnull().sum()","e7fa080c":"med_fare = test_data.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ntest_data['Fare'] = test_data['Fare'].fillna(med_fare)","392e953f":"train_data['Age'] = train_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ntest_data['Age'] = test_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","ca2aaa74":"train_data['Embarked'] = train_data['Embarked'].fillna('S')\ntest_data['Embarked'] = test_data['Embarked'].fillna('S')","8fd986c6":"train_data['Deck'] = train_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntest_data['Deck'] = test_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ntrain_data['Deck'] = train_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\ntrain_data['Deck'] = train_data['Deck'].replace(['D', 'E'], 'DE')\ntrain_data['Deck'] = train_data['Deck'].replace(['F', 'G'], 'FG')\n\ntest_data['Deck'] = test_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\ntest_data['Deck'] = test_data['Deck'].replace(['D', 'E'], 'DE')\ntest_data['Deck'] = test_data['Deck'].replace(['F', 'G'], 'FG')\n\ntest_data['Deck'].value_counts()","90e7d61e":"# combine = [train_data, test_data]\n# print(\"Before\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)\n\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)\ncombine = [train_data, test_data]\n\nprint(\"After\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)","586a6515":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","ba463da3":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], \n                                                'Miss\/Mrs\/Ms')\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], \n                                                'Dr\/Military\/Noble\/Clergy')\n#     dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n#                                                  'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n#     dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","5e26aea7":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","d58826f0":"train_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape","1ebf35c4":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_data.head()","6efef22d":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","4bb948d7":"guess_ages = np.zeros((2,3))\nguess_ages","fd19f0b1":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_data.head()","649b11b1":"train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","e07ed02f":"for dataset in combine:    \n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_data.head()","f7a5d9f5":"train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","dd282939":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], \n                                               as_index=False).mean().sort_values(by='Survived', ascending=False)","7165c394":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","148f4da9":"# train_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n# test_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n# combine = [train_data, test_data]\n\n# train_data.head()","2a9c454f":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","05661d02":"freq_port = train_data.Embarked.dropna().mode()[0]\nfreq_port","825b598b":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4380a4cc":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_data.head()","d8b97ce3":"test_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntest_data.head()","66f0cb43":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","81b2ab6b":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]","cbf804fb":"test_data.head(10)","4c16ca9b":"train_data['Ticket'] = LabelEncoder().fit_transform(train_data['Ticket'])\ntrain_data['Deck'] = LabelEncoder().fit_transform(train_data['Deck'])\n\ntest_data['Ticket'] = LabelEncoder().fit_transform(test_data['Ticket'])\ntest_data['Deck'] = LabelEncoder().fit_transform(test_data['Deck'])","d993204a":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.copy()\nX_test = X_test.drop(['PassengerId'], axis=1)\nX_train.shape, Y_train.shape, X_test.shape\n\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","16986a24":"X_train.shape","51446a0c":"train_x, val_x, train_y, val_y = train_test_split(X_train, Y_train, random_state = 0)\nkfold = StratifiedKFold(n_splits=10)\nval_y","61dd511d":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_x, train_y)\npred_y = logreg.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\nacc","5b0b40b8":"coeff_df = pd.DataFrame(train_data.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","5092324a":"logreg = LogisticRegression()\nn_folds = 10\nclf = GridSearchCV(logreg, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","6a106202":"# Support Vector Machines\n\nsvc = SVC()\n\nn_folds = 10\nclf = GridSearchCV(svc, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","7c3007af":"# KNN\n\nknn = KNeighborsClassifier(algorithm='auto', \n                           leaf_size=30,\n                           metric='minkowski',                                           \n                           metric_params=None, \n                           n_neighbors=7, \n                           p=2,\n                           n_jobs=4,\n                           weights='uniform')\n                                            \nn_folds = 10\nclf = GridSearchCV(knn, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","89357dba":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\nn_folds = 10\nclf = GridSearchCV(gaussian, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","c29ec359":"# Perceptron\n\nperceptron = Perceptron()\nn_folds = 10\nclf = GridSearchCV(perceptron, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","4d78030f":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nn_folds = 10\nclf = GridSearchCV(sgd, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","22a857be":"# Decision Tree\n\ndt = DecisionTreeClassifier(random_state=0)\nn_folds = 10\nclf = GridSearchCV(dt, param_grid = {}, cv=n_folds, refit=True)\nclf.fit(train_x, train_y)\npred_y = clf.predict(val_x)\nacc = accuracy_score(val_y, pred_y)\ncm = classification_report(val_y, pred_y)\nprint(acc)\nprint(cm)","0b9ce659":"Y_pred = clf.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f619310d":"replace titles","931d34ce":"check missing data","5f507875":"**Load data**","984a87b7":"drop the Name feature from training and testing datasets","b910274e":"**Data exploring**","b05890f6":"drop Parch, SibSp, and FamilySize features in favor of IsAlone","af7b0b22":"convert a categorical feature","56b0766c":"**Data Preproccessing**","087cf89f":"**Single Model**","6b8454e7":"save to csv","04ad7fa1":"replace age with ordinals","2d9ed9ef":"create new features","a81d8427":"convert the categorical titles to ordinal"}}