{"cell_type":{"db89dc60":"code","dde5d204":"code","fe7fe9af":"code","9f7120b9":"code","5e6eb4aa":"code","b7628f02":"code","4d1edcb2":"code","65247740":"code","55e569f9":"markdown","458c5b21":"markdown","7ac57707":"markdown","80c532ef":"markdown","fb597bc7":"markdown","ce200753":"markdown","e6c6f04b":"markdown","b24b65fb":"markdown","4774deed":"markdown","3685d070":"markdown","aa67f850":"markdown"},"source":{"db89dc60":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pylab import meshgrid\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input\nfrom keras.initializers import he_normal, glorot_normal","dde5d204":"def approximate_2d(y_func, epochs = 10, batch_size = 4, hidden_layers = [4], test_size = 0.5, init = 'glorot_normal', act = 'sigmoid'):\n    # Train\/test data\n    x = np.arange(-10, 10, 0.1).reshape(-1, 1)\n    y = y_func(x).reshape(-1, 1)\n    \n    # Data to see how model will handle unseen data\n    unseen_x = np.arange(10, 20, 0.1).reshape(-1, 1)\n    unseen_y = y_func(unseen_x).reshape(-1, 1)\n    \n    # Train test split\n    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = test_size, shuffle = True)\n    \n    # Scaling data\n    scaler_x = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    train_x = scaler_x.fit_transform(train_x)\n    test_x = scaler_x.transform(test_x)\n    unseen_x = scaler_x.transform(unseen_x)\n    \n    train_y = scaler_y.fit_transform(train_y)\n    test_y = scaler_y.transform(test_y)\n    unseen_y = scaler_y.transform(unseen_y)\n    \n    # Model\n    if init == 'he_normal':\n        init = he_normal(seed = 666)\n    elif init == 'glorot_normal':\n        init = glorot_normal(seed = 666)\n    \n    model = Sequential()    \n    for i, l in enumerate(hidden_layers):\n        model.add(Dense(l, input_shape = (1, ), kernel_initializer = init, activation = act)) if i == 0 else\\\n        model.add(Dense(l, kernel_initializer = init, activation = act))\n    model.add(Dense(1, kernel_initializer = init))\n    model.compile(optimizer = 'sgd', loss = 'mse')\n    model.fit(train_x, train_y, epochs = epochs, batch_size = batch_size, validation_split = 0.1, verbose = 0)\n    \n    # Predictions\n    preds = scaler_y.inverse_transform(model.predict(test_x)) # Preds on test data\n    preds_train = scaler_y.inverse_transform(model.predict(train_x)) # Preds on train data\n    unseen_preds = scaler_y.inverse_transform(model.predict(unseen_x)) # Preds on unseen data\n    \n    # Inverse transform of data\n    unseen_x = scaler_x.inverse_transform(unseen_x)\n    unseen_y = scaler_y.inverse_transform(unseen_y)\n    train_x = scaler_x.inverse_transform(train_x)\n    test_x = scaler_x.inverse_transform(test_x)\n    \n    # Plotting results\n    fig = plt.figure(figsize = (19, 6))\n    \n    # Learning curves plot\n    plt.subplot(121)\n    H = model.history.history\n    plt.plot(H['loss'], label = 'loss')\n    plt.plot(H['val_loss'], label = 'val_loss')\n    plt.grid(); plt.legend()\n    \n    # Predictions plot\n    plt.subplot(122)\n    plt.plot(x, y, label = '$f(x)$')\n    plt.scatter(test_x, preds, label = 'Test_preds', s = 15, c = 'g', marker = 'x')\n    plt.scatter(train_x, preds_train, label = 'Train_preds', s = 5, c = 'y', alpha = 0.9)\n    plt.scatter(unseen_x, unseen_preds, label = 'unseen_preds', s = 10, c = 'k', marker = '1')\n    plt.plot(unseen_x, unseen_y, label = 'Unseen data')\n    plt.grid(); plt.legend()\n    plt.title(f'{len(hidden_layers)} hidden: {hidden_layers} neurons, {act} activation')\n    plt.show()   ","fe7fe9af":"def quadratic(x):\n    return x**2 + 2*x + 5\n\napproximate_2d(quadratic, hidden_layers = [24], epochs = 500) ","9f7120b9":"approximate_2d(quadratic, hidden_layers = [10, 20], epochs = 200, act = 'relu', init = 'he_normal') ","5e6eb4aa":"def sinusoid(x):\n    return np.sin(x)\n\napproximate_2d(sinusoid, hidden_layers = [16, 32, 64, 128], epochs = 500, act = 'relu', init = 'he_normal') ","b7628f02":"# f(x)=0.2x2+0.5\\sin(5x)+2\\cos(x)\ndef tricky_one(x):\n    return (0.2 * x**2) + (5 * np.sin(5 * x)) + (4 * np.cos(x))\n\napproximate_2d(tricky_one, hidden_layers = [64, 128, 256], epochs = 100, act = 'relu', init = 'he_normal') ","4d1edcb2":"def approximate_3d(y_func, epochs = 10, batch_size = 4, hidden_layers = [4], test_size = 0.5, init = 'glorot_normal', act = 'sigmoid'):\n    # Train\/test data\n    x = np.arange(-1.5, 1.6, 0.1)\n    y = np.arange(-1.5, 1.6, 0.1)\n    \n    X, Y = meshgrid(x, y)\n    Z = f(X, Y)    \n    \n    train_data = np.array(list(zip(X.flatten(), Y.flatten())))\n    labels = Z.flatten().reshape(-1, 1)\n    \n    # Train test split\n    train_x, test_x, train_y, test_y = train_test_split(train_data, labels, test_size = test_size, shuffle = True)\n    \n    # Scaling data\n    scaler_x = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    train_x = scaler_x.fit_transform(train_x)\n    test_x = scaler_x.transform(test_x)\n    \n    train_y = scaler_y.fit_transform(train_y)\n    test_y = scaler_y.transform(test_y)\n    \n    # Model\n    if init == 'he_normal':\n        init = he_normal(seed = 666)\n    elif init == 'glorot_normal':\n        init = glorot_normal(seed = 666)\n    \n    model = Sequential()    \n    for i, l in enumerate(hidden_layers):\n        model.add(Dense(l, input_shape = (2, ), kernel_initializer = init, activation = act)) if i == 0 else\\\n        model.add(Dense(l, kernel_initializer = init, activation = act))\n    model.add(Dense(1, kernel_initializer = init))\n    model.compile(optimizer = 'sgd', loss = 'mse')\n    model.fit(train_x, train_y, epochs = epochs, batch_size = batch_size, validation_split = 0.1, verbose = 0)\n    \n    # Predictions\n    preds = scaler_y.inverse_transform(model.predict(test_x)) # Preds on test data\n    preds_train = scaler_y.inverse_transform(model.predict(train_x)) # Preds on train data\n        \n    # Inverse transform of data\n    train_x = scaler_x.inverse_transform(train_x)\n    test_x = scaler_x.inverse_transform(test_x)\n    \n    t_x = train_x[:, 0]\n    t_y = train_x[:, 1]\n    \n    tr_x = test_x[:, 0]\n    tr_y = test_x[:, 1]\n    \n    # Plotting results\n    fig = plt.figure(figsize = (20, 8))\n    \n    # Learning curves plot\n    plt.subplot(121)\n    H = model.history.history\n    plt.plot(H['loss'], label = 'loss')\n    plt.plot(H['val_loss'], label = 'val_loss')\n    plt.grid(); plt.legend()\n    \n    # Predictions plot    \n    ax = fig.add_subplot(1, 2, 2, projection = '3d')\n    ax.plot_wireframe(X, Y, Z, color = 'k', label = '$f(x)$', alpha = 0.4)\n    ax.scatter(t_x, t_y, preds_train, c = 'y', label = 'Train_preds')\n    ax.scatter(tr_x, tr_y, preds, c = 'g', label = 'Test_preds')\n#     ax.legend()\n    ax.view_init(25, 100)\n    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n    plt.show()","65247740":"def f(x, y):\n    return np.sin(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.cos(2 * x + 1 - np.e**y)\n\napproximate_3d(f, hidden_layers = [64, 128, 256], epochs = 100, act = 'relu', init = 'he_normal')","55e569f9":"### $0.2x^2+5sin(5x)+4cos(x)$ approximation\n\nNow let's try tricky one:","458c5b21":"Now we can start experiment. After some attempts, I managed to get a good approximation with next configuration: relu, 3 hidden layers with 64, 128, 256 neurons.","7ac57707":"After many attemts I decided to stop on this variant: relu, 3 hidden layers with 64, 128, 256 neurons. It not follow function directly, but this is still descent approximation.","80c532ef":"### $x^2+2x+5$ approximation\n\nI want to start with something simple - quadratic function. So our ANN will have one input and one output. ","fb597bc7":"As we can see - the ANN can approximate different functions pretty well, but only on a specific range, that is limited by training data range.","ce200753":"### Goal\n\nIn this kernel I want to exeriment with approximation of different functions using ANN, to get better undersatanding how they work.\n\nTo achieve this goal I will create a function, that will do all work for us:\n1. As input the function takes a function, that we need to approximate.\n2. The function creates \"x\" values using [-10, 10) range and \"y\" values, using passed as argument function.\n3. 50% of generated data will be used as training dataset and 50% as test dataset.\n4. In addition to train\/test data, a new \"unseen\" data in range [10, 20) will be generated to test, how model will handle data, that outside the train data range.\n5. Train model using train data.\n6. Make predictions using train, test and unseen data.\n7. Plot results.\n\nSo, let's start coding:","e6c6f04b":"### $sin(x)$ approximation\n\nNext let's try $sin(x)$ function:","b24b65fb":"In this experiment I used 1 hidden layer with 24 neurons and sigmoid activation. After 500 epochs, I got a preety good approximation but, at values close to -10 and 10, the approximation starts to look like sinusoid, not a parabola. Let's try relu now:","4774deed":"After some experiments, I decided to stop at next configuration: relu as activation, 2 hidden layers - first with 10 neurons and second with 20 neurons. After 200 epochs I got a very good approximation even on unseen data. ","3685d070":"### Approximation of multivariable function\n\nNow lets take something more interesting - the function with 2 inputs and 1 output:\n\n$sin(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)cos(2x+1-e^y)$\n\nFirst - we need to edit our main function a little bit:","aa67f850":"The $sin(x)$ function took much more layers and neurons to approximate: 4 hidden layers with, 16, 32, 64, 128 neurons. The approximation on train data is very good, but the model cant predict unseen data properly."}}