{"cell_type":{"61bc6461":"code","3c69f6b7":"code","33b84f6c":"code","292a35b6":"code","3c14e9b3":"code","a9afbf0e":"code","3f86a8d1":"code","de3d8c41":"code","c4688582":"code","25633234":"code","f5030d5f":"code","85633642":"code","67deb81f":"code","bd8cbd74":"code","a253f479":"code","417d9348":"code","b4ae1eb4":"code","944cbf37":"code","06224b50":"code","03108d6a":"code","5f16c69c":"code","33716180":"code","4351c5a7":"code","9dcb675d":"code","e2387c3d":"code","0076b075":"code","8cbc7285":"code","45140b79":"code","fcb8ff41":"code","0adb8ba1":"code","7e607f6b":"code","2df55c0b":"code","9fbc5e1a":"code","ebe6ebb1":"code","541caf4e":"code","f1b03b97":"code","70a7e016":"code","e442592c":"code","4c31eefc":"code","e4b53048":"code","e1854bb3":"code","c72457df":"code","bfb3341b":"code","d5a3b165":"code","ee04c983":"code","f429a075":"code","6336adb4":"code","5f930484":"code","b49f95b6":"code","d7db7788":"code","71117d97":"code","629437ef":"code","dd03a1e3":"code","3356af62":"code","8aa762c0":"code","315ebe0d":"code","408d9d85":"code","4452b43b":"code","aeca6c36":"code","ee93945f":"code","9a522c2e":"code","a6399675":"code","8d43a85b":"code","b3c214a9":"code","7b2bec52":"code","c47715b9":"code","4494b898":"code","99147b06":"code","cbbe4de1":"code","0c8e641a":"code","848da936":"code","c90a7f2d":"code","3a4fdab4":"code","198a4c44":"code","00e3b721":"code","2259fb8c":"code","fb7ef0e0":"code","9e8e1fb8":"code","11e3cc79":"code","b8f6b9f4":"code","e686864b":"code","6268de1e":"code","b390ca72":"code","c577bff9":"code","7425fcf2":"code","b9441c6e":"code","0d7edcbe":"code","66666528":"code","92b2a2c3":"code","a37673c4":"code","1bf03295":"code","5ca5f90a":"code","a95a3e4e":"code","4c2ca21e":"code","199d85cf":"code","45248023":"code","702aea47":"code","dc8827ce":"code","c1277768":"code","689a22a2":"code","fc7a2a05":"code","71e41137":"code","725a1dd4":"code","c7e7fac8":"code","6e1c3031":"code","2851ca46":"code","4a1fdd42":"code","9f909dc0":"code","81082d0a":"code","39f1ce1e":"code","5114095d":"code","9f317865":"code","3f697abb":"code","0b551cd5":"code","edebee09":"code","a60697d3":"code","080089c2":"code","e3abe23d":"code","31c0f275":"code","50f0c544":"code","020e878f":"code","c76f170e":"code","524902b0":"code","662eed2c":"code","8099e7c3":"code","bf21c738":"code","6a49d263":"code","749b9adc":"code","6e6eab79":"code","1a3eea07":"code","ea1ef025":"code","2aef40a7":"code","c9cae989":"code","0b632625":"code","e008f0a8":"code","4c3ce3f2":"code","70345126":"code","3eb2163d":"code","03e1c4c3":"code","3638a9ec":"code","19067c15":"code","1a796342":"code","0f7c87f0":"code","b7f1c6e9":"code","d20ea240":"code","7fd55c4e":"code","8f0e3f22":"code","892c9766":"code","a1986a6e":"code","67d46dbb":"code","dd915fee":"code","0502e738":"code","8544efcb":"code","3754837a":"code","a3510627":"code","afc51bd1":"code","3fc74696":"code","250a94b9":"code","973616ef":"code","e0f10f28":"code","07f6ffe1":"code","073f987d":"code","9f420329":"code","8fdc158f":"code","2899f91d":"code","a780c29b":"code","cbd81da1":"code","032fd2ba":"code","08b4b445":"code","9f7be285":"code","978a8527":"code","8c108538":"code","ef8d2dcc":"code","0749650a":"code","1571ddf2":"code","0a661f43":"code","9468d394":"code","f21cd5f3":"code","4ec13464":"code","1fcf4ef0":"code","bd99fa7d":"code","ec8a6ab5":"code","55900c25":"code","ef0932f6":"code","883d5b15":"code","6045d2f4":"code","1c2a6d49":"code","17d24d9f":"code","68e0ecc1":"code","e76ef2e9":"code","3130f08e":"code","14fbdc9e":"code","8f02df57":"code","9cfac132":"code","eb8d7452":"code","23460ea8":"code","8481cbdd":"code","c599b56a":"code","0a347411":"code","db5a4085":"code","a521a580":"code","b2fb3185":"code","97bd965d":"markdown","eb00b6ef":"markdown","402f6ba0":"markdown","1ae62c2e":"markdown","ba7ae065":"markdown","65910459":"markdown","7635673e":"markdown","962b117a":"markdown","254cac7c":"markdown","a1b7c6ba":"markdown","c056aaad":"markdown","f26569a7":"markdown","205a5fd7":"markdown","ea9652d0":"markdown","68ba3862":"markdown","1480947a":"markdown","6cce86b9":"markdown","862b78d7":"markdown","f2c02570":"markdown","cbf4e7ce":"markdown","865bb0cb":"markdown","c2484533":"markdown","f449d0d0":"markdown","b2fa125d":"markdown","07309086":"markdown","f960bad9":"markdown","3a8839e1":"markdown","3b1ff1e9":"markdown","2d391afa":"markdown","9e9a8df1":"markdown","cae315cb":"markdown","17330038":"markdown","dbfbd338":"markdown","95256d94":"markdown","a2479a24":"markdown","1ab09ac1":"markdown","cbc84d44":"markdown","8df1241a":"markdown","36e23f43":"markdown","42f84ab3":"markdown","0dd2a586":"markdown","5701ff15":"markdown","e9ba0cfc":"markdown","25e0ac1a":"markdown","3495eeb6":"markdown","1cd98d14":"markdown","00220dcd":"markdown","30523c62":"markdown","7ecf61a4":"markdown","55c0fb15":"markdown","a9c5d15b":"markdown","e13868da":"markdown","8b6ce7e4":"markdown","ddafee30":"markdown","37125a4a":"markdown","0713f275":"markdown","ad74a3d8":"markdown","5a3b256a":"markdown","18845348":"markdown","821a5a96":"markdown","d7f0beb1":"markdown","3d927781":"markdown","d1476b07":"markdown","031fddb3":"markdown","96c44ae0":"markdown","52ebfef4":"markdown","2ee3f52f":"markdown","b6951c60":"markdown","a79335fe":"markdown","4afd582d":"markdown","345563a9":"markdown","a8b80f35":"markdown","2be7aee3":"markdown","7085f490":"markdown","c07cca0b":"markdown","48aaf4ea":"markdown","8ec8db00":"markdown","e3eacf53":"markdown","442c3390":"markdown","6b9cff4c":"markdown","987694f0":"markdown","234d3287":"markdown","bf75d625":"markdown","8a31e2f1":"markdown","9372d3cc":"markdown","c723c227":"markdown","522fd714":"markdown","6cfe792a":"markdown","186266cf":"markdown","47151774":"markdown","ddfc7cd1":"markdown","756241bd":"markdown","c07ae0b4":"markdown","1c0242e3":"markdown","a60d33b0":"markdown","e5c6b78f":"markdown","6dafcfae":"markdown","edcaa6c8":"markdown","0a7b8736":"markdown","b97da2e2":"markdown","b0a67cb3":"markdown","b367b8a7":"markdown","b4a9dca6":"markdown","4a79956c":"markdown","e0044a1f":"markdown","8b1f162c":"markdown","382a141b":"markdown","05c92a99":"markdown","aa47f6c9":"markdown","b3e4d606":"markdown","ad32fecc":"markdown","313a691e":"markdown","17022819":"markdown","6c349181":"markdown","c63641b2":"markdown","a45af54e":"markdown","84929348":"markdown","f9a71992":"markdown","c8030238":"markdown","702791b1":"markdown","e9329487":"markdown","6dde3485":"markdown","744842de":"markdown","9af3d88e":"markdown","0575be3c":"markdown","e52ffcf1":"markdown","65c2b8bb":"markdown","9b36bdfd":"markdown","f6ab5d5a":"markdown","64ab7490":"markdown","f5bb4364":"markdown","669e503d":"markdown","a946e92e":"markdown","5d8722a9":"markdown","d4721d28":"markdown","e10facde":"markdown","18e93744":"markdown","8fac572e":"markdown","152679e5":"markdown","2005207c":"markdown","861c4c75":"markdown","4ca84c15":"markdown","75056ffd":"markdown","7dbf0f88":"markdown","6aedf790":"markdown","ac6bae58":"markdown","a457d209":"markdown","86b2d0d6":"markdown","af011b78":"markdown","b7eb9945":"markdown","20985b5f":"markdown","6041db95":"markdown","7d82b636":"markdown","0067916e":"markdown","26e2e35f":"markdown","7f908f93":"markdown","5379e036":"markdown","876106b3":"markdown","422b3e80":"markdown","231993ab":"markdown","a0c1ec71":"markdown","d9aa9a5a":"markdown","8301d0bc":"markdown","9d93a41d":"markdown","5a0e9078":"markdown","8fcba972":"markdown","367465f9":"markdown","f1314a93":"markdown","2c5961ff":"markdown","1c1e4fee":"markdown","c46c1db9":"markdown","22b5432f":"markdown","2b6b6808":"markdown","c888ee63":"markdown","46f8ed7a":"markdown","47acd572":"markdown","2dadff62":"markdown","3bf2a5b0":"markdown","42c6d5ab":"markdown","2c026fdd":"markdown","ae8d1692":"markdown","107c38c7":"markdown","67950bb0":"markdown","4aa5f912":"markdown","0f3a7771":"markdown","ebb9bf6e":"markdown","9bb3fcca":"markdown","4042eab1":"markdown","a3cb1eb8":"markdown","9451d438":"markdown"},"source":{"61bc6461":"# basic libraries for data acquisition, handling and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# libraries for modelling\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import explained_variance_score, r2_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","3c69f6b7":"os.listdir('..\/input')","33b84f6c":"diam = pd.read_csv('..\/input\/diamonds.csv', index_col=0)","292a35b6":"diam.head()","3c14e9b3":"data = diam.copy()","a9afbf0e":"diam.sample(10)","3f86a8d1":"# check fror null values on target variable\ndiam.price.isnull().any()","de3d8c41":"print(\"We have {:.0f} priced diamonds.\".format(diam.price.count()))","c4688582":"diam.price.describe()","25633234":"f, ax = plt.subplots(ncols=2, figsize=(12,6))\nsns.boxplot(y='price', data=diam, ax=ax[0])\nsns.boxenplot(y='price', data=diam, ax=ax[1])\nplt.show()","f5030d5f":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.kdeplot(diam.price, color='b', shade=True, ax=ax[0])\nsns.kdeplot(diam.price, color='r', shade=True, bw=100, ax=ax[1])\n\nax[0].set_title('KDE')\nax[1].set_title('KDE, bandwidth = 100')\n\nplt.show()","85633642":"diam.columns","67deb81f":"diam.info()","bd8cbd74":"for col in ['cut', 'color', 'clarity']:\n    print(\"Column : {}\".format(col))\n    print(diam[col].value_counts())\n    print()","a253f479":"# turn to 'categorical' data type and order\ncut_dtype = pd.api.types.CategoricalDtype(\n    categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'], \n    ordered=True)\ndata['cut'] = diam.cut.astype(cut_dtype)","417d9348":"data.cut.head()","b4ae1eb4":"# turn to 'categorical' data type and order\ncolor_dtype = pd.api.types.CategoricalDtype(\n    categories=['J', 'I', 'H', 'G', 'F', 'E', 'D'], \n    ordered=True)\ndata['color'] = diam.color.astype(color_dtype)","944cbf37":"data.color.head()","06224b50":"clar_dtype = pd.api.types.CategoricalDtype(\n    categories=['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], \n    ordered=True)\ndata['clarity'] = diam.clarity.astype(clar_dtype)","03108d6a":"data.clarity.head()","5f16c69c":"diam.carat.describe()","33716180":"ax = sns.boxplot(y='carat', data=diam)","4351c5a7":"print(diam[diam.carat < 3].sample(5))\nprint()\nprint(diam[diam.carat > 3].sample(5))","9dcb675d":"print(\"At least one null entry? {}\".format(diam.depth.isnull().any()))","e2387c3d":"diam.depth.describe()","0076b075":"ax = sns.boxplot(y='depth', data=diam)","8cbc7285":"print(\"# diamonds, depth > 65:\", diam[diam.depth > 65].depth.count())\nprint()\nprint('Sample')\nprint(diam[diam.depth > 65].sample(10))","45140b79":"print(\"# diamonds, depth < 58:\", diam[diam.depth < 58].depth.count())\nprint()\nprint('Sample')\nprint(diam[diam.depth < 58].sample(10))","fcb8ff41":"print(\"At least one null entry? {}\".format(diam.table.isnull().any()))","0adb8ba1":"diam.table.describe()","7e607f6b":"ax = sns.boxplot(y='table', data=diam)","2df55c0b":"print(\"# diamonds, table% > 65:\", diam[diam.table > 65].table.count())\nprint()\nprint(\"Sample:\")\nprint(diam[diam.table > 65].sample(10))","9fbc5e1a":"print(\"# diamonds, table% < 50:\", diam[diam.table < 50].table.count())\nprint()\nprint(\"Sample:\")\nprint(diam[diam.table < 50].sample(4))","ebe6ebb1":"print(\"Any null entry for \\'x\\'? {}\".format(diam.x.isnull().any()))\nprint(\"Any null entry for \\'y\\'? {}\".format(diam.y.isnull().any()))\nprint(\"Any null entry for \\'z\\'? {}\".format(diam.z.isnull().any()))","541caf4e":"diam.loc[:, ['x', 'y', 'z']].describe()","f1b03b97":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.scatterplot(x=diam.x, y=diam.y, ax=ax[0])\nax[0].set_title(\"y vs x\")\n\nsns.scatterplot(x=diam.x, y=diam.y, ax=ax[1])\nax[1].set_xlim(0, 15)\nax[1].set_ylim(0, 15)\nax[1].set_title(\"y vs x - Zoomed in\")\n\nplt.show()","70a7e016":"ax = sns.scatterplot(x='carat', y='y', data=diam)","e442592c":"diam[(diam.y > 10) | (diam.z > 10)]","4c31eefc":"diam[(diam.x < 1) | (diam.y < 1) | (diam.z < 1)].sample(10)","e4b53048":"absurdx_i = ((diam.x == 0) | (diam.x > 15)) & (diam.y != 0)\ndata.loc[absurdx_i, 'x'] = diam.loc[absurdx_i, 'y']","e1854bb3":"# create Boolean mask to subset DataFrame\nabsurdxy_i = ((data.x == 0) | (data.y == 0))\n\n# compute mean value of x\nmean_x = np.mean(data.x)\n\n# substitute on the dataFrame\ndata.loc[absurdxy_i, ['x', 'y']] = data.loc[absurdxy_i, ['x', 'y']].replace(0, mean_x)","c72457df":"absurdy_i = (((data.y > 15) | (data.y == 0)) & (data.x != 0))\ndata.loc[absurdy_i, 'y'] = data.loc[absurdy_i, 'x']","bfb3341b":"data[(data.z == 0) | (data.z > 15)]","d5a3b165":"# find rows where z is absurd\nabsurd_z = ((diam.z == 0) | (diam.z > 15))\n\n# define function to calculate z\ncalc_z = lambda row: (row['depth']\/100) * (row['x'] + row['y'])\/2\n\n# apply on dataframe\ndata.loc[absurd_z, 'z'] = data.loc[absurd_z, :].apply(calc_z, axis=1)","ee04c983":"data[['x', 'y', 'z']].describe()","f429a075":"data.info()","6336adb4":"data.describe()","5f930484":"data['depth_table_ratio'] = data['depth'] \/ data['table']","b49f95b6":"data.sample(10)","d7db7788":"ax = sns.boxplot(y='carat', data=data)\nprint(data[['carat']].describe())","71117d97":"ax = sns.scatterplot(x='carat', y='price', data=data,\n                     edgecolors='k', alpha=0.3)","629437ef":"ax = sns.violinplot(y='price', data=data)","dd03a1e3":"f, ax = plt.subplots(ncols=2, figsize=(10, 5))\nsns.regplot(x='carat', y='price', data=data, ax=ax[0],\n            x_bins=10, x_estimator=np.mean, ci=None)\nsns.regplot(x='carat', y='price', data=data, ax=ax[1],\n            x_bins=10, x_estimator=np.mean, ci=None, order=3)\n\nax[0].set_title(\"Price vs Carat - 1st order linear\")\nax[1].set_title(\"Price vs Carat - 3rd order polynomial\")\nplt.show()","3356af62":"data['log_price'] = data.price.apply(np.log)","8aa762c0":"f, ax = plt.subplots(ncols=3, figsize=(15,5))\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[0],\n                 x_estimator=np.mean, ci=None)\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[1],\n                 x_estimator=np.mean, ci=None, order=2)\nsns.regplot(x='carat', y='log_price', data=data, x_bins=10, ax=ax[2],\n                 x_estimator=np.mean, ci=None, order=3)\n\nax[0].set_title(\"Log(Price) vs Carat - 1st order polynom.\")\nax[0].set_ylabel(\"Log (price)\")\n\nax[1].set_title(\"Log(Price) vs Carat - 2nd order polynom.\")\nax[1].set_ylabel(\"Log (price)\")\n\nax[2].set_title(\"Log(Price) vs Carat - 3rd order polynom.\")\nax[2].set_ylabel(\"Log (price)\")\n\nplt.show()","315ebe0d":"data['carat_bin'] = pd.cut(data.carat, range(6))","408d9d85":"ax = sns.countplot(x='carat_bin', data=data)\n\nprint(data.carat_bin.value_counts(normalize=True, sort=True, ascending=False)*100)","4452b43b":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='carat_bin', y='log_price', data=data, palette='husl', ax=ax[0])\nsns.pointplot(x='carat_bin', y='log_price', data=data, ax=ax[1])\n\nax[0].set_title(\"Distribution - log (Price) vs Carat category\")\nax[1].set_title(\"Mean log(price) vs Carat category\")\nplt.show()\n\nprint(\"Mean price per carat_bin:\")\nprint(data.groupby('carat_bin').price.mean())","aeca6c36":"data[(data.carat <= 1) & (data.price > 12000)]","ee93945f":"f, ax = plt.subplots(ncols=3, figsize=(18, 5))\nsns.countplot(x='cut', data=data, ax=ax[0])\nsns.boxplot(x='cut', y='log_price', data=data, ax=ax[1])\nsns.stripplot(x='cut', y='log_price', data=data, ax=ax[1],\n              size=1, edgecolor='k', linewidth=.1)\nsns.pointplot(x='cut', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='cut', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per carat\")\nax[1].set_title(\"log(Price) vs Cut - Distribution\")\nax[2].set_title(\"Mean log(price) vs Cut (blue)\\nMedian log(price) vs Cut (red)\")\nplt.show()","9a522c2e":"print('% diamonds per cut grade:')\nprint(data.cut.value_counts(normalize=True, sort=True, ascending=False)*100)","a6399675":"print('Mean log(price) per cut grade:')\nprint(data.groupby('cut').log_price.mean())","8d43a85b":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='color', data=data, ax=ax[0])\nsns.boxplot(x='color', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='color', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='color', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per color grade\")\nax[1].set_title(\"log(Price) vs Color - Distribution\")\nax[2].set_title(\"Mean log(price) vs Color (blue)\\nMedian log(price) vs Color (red)\")\nplt.show()","b3c214a9":"print(\"# diamonds per color grade\")\nprint(data.color.value_counts(normalize=True, sort=True, ascending=True) * 100)","7b2bec52":"print(\"Mean log(price) of diamonds per color grade\")\nprint(data.groupby('color').log_price.mean())","c47715b9":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='clarity', data=data, ax=ax[0])\nsns.boxplot(x='clarity', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='clarity', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='clarity', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per clarity grade\")\nax[1].set_title(\"log(Price) vs clarity - Distribution\")\nax[2].set_title(\"Mean log(price) vs clarity (blue)\\nMedian log(price) vs clarity (red)\")\nplt.show()","4494b898":"data[['depth']].describe()","99147b06":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(y='depth', data=data, ax=ax[0])\nax[0].set_title(\"Depth distribution\")\n\nsns.boxplot(y='depth', data=data, ax=ax[1])\nax[1].set_ylim(55, 70)\nax[1].set_title(\"Depth distribution - Zoomed in\")\n\nplt.show()","cbbe4de1":"ax = sns.scatterplot(x='depth', y='log_price', data=data,\n                     alpha=0.3, edgecolor='k')","0c8e641a":"# create bins\ndepth_desc = data[['depth']].describe()\ndepth_bins = depth_desc['min':'max'].depth.tolist()\n\n# create column for bins\ndata['depth_bin'] = pd.cut(data.depth, depth_bins)\ndata.depth_bin.value_counts()","848da936":"f, ax = plt.subplots(ncols=3, figsize=(15, 5))\nsns.countplot(x='depth_bin', data=data, ax=ax[0])\nsns.boxplot(x='depth_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='depth_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='depth_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per depth bin\")\nax[1].set_title(\"log(Price) vs depth_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs depth_bin (blue)\\nMedian log(price) vs depth_bin (red)\")\nplt.show()","c90a7f2d":"data[['table']].describe()","3a4fdab4":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(y='table', data=data, ax=ax[0])\nax[0].set_title(\"Table distribution\")\n\nsns.boxplot(y='table', data=data, ax=ax[1])\nax[1].set_ylim(50, 65)\nax[1].set_title(\"Table distribution - Zoomed in\")\n\nplt.show()","198a4c44":"ax = sns.scatterplot(x='table', y='log_price', data=data,\n                     alpha=0.3, edgecolor='k')","00e3b721":"# create bin list\ntable_desc = data[['table']].describe()\ntable_bins=table_desc['min':'max'].table.tolist()\ntable_bins.append(65)\ntable_bins.sort()\n\n# create column for bins\ndata['table_bin'] = pd.cut(data.table, table_bins)\ndata.table_bin.value_counts()","2259fb8c":"f, ax = plt.subplots(ncols=3, figsize=(18, 5))\nsns.countplot(x='table_bin', data=data, ax=ax[0])\nsns.boxplot(x='table_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='table_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='table_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per table bin\")\nax[1].set_title(\"log(Price) vs table_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs table_bin (blue)\\nMedian log(price) vs table_bin (red)\")\nplt.show()","fb7ef0e0":"data['depth_table_ratio'].describe()","9e8e1fb8":"ax = sns.boxplot(y='depth_table_ratio', data=data)","11e3cc79":"ax = sns.scatterplot(x='depth_table_ratio', y='log_price', data=data,\n                     edgecolor='k', alpha=.3, s=10)","b8f6b9f4":"# create bins list\ndt_ratio_desc = data[['depth_table_ratio']].describe()\ndt_ratio_bins = dt_ratio_desc['min':'max'].depth_table_ratio.tolist()\n\n# create columns for bins\ndata['dt_ratio_bin'] = pd.cut(data.depth_table_ratio, dt_ratio_bins)\ndata.dt_ratio_bin.value_counts()","e686864b":"f, ax = plt.subplots(nrows=3, figsize=(6, 18))\nsns.countplot(x='dt_ratio_bin', data=data, ax=ax[0])\nsns.boxplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[1])\nsns.pointplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[2])\nsns.pointplot(x='dt_ratio_bin', y='log_price', data=data, ax=ax[2],\n              estimator=np.median, color='r')\n\nax[0].set_title(\"Count diamonds per dt_ratio_bin\")\nax[1].set_title(\"log(Price) vs dt_ratio_bin - Distribution\")\nax[2].set_title(\"Mean log(price) vs dt_ratio_bin (blue)\\nMedian log(price) vs dt_ratio_bin (red)\")\nplt.show()","6268de1e":"data.price.describe()","b390ca72":"ax = sns.boxplot(y='price', data=data)","c577bff9":"data['high_price'] = data.price.apply(lambda x: 1 if x >= 10000 else 0)","7425fcf2":"ax = sns.boxplot(x='high_price', y='carat', data=data)","b9441c6e":"ax = sns.countplot(x='cut', data=data, hue='high_price')","0d7edcbe":"pricebin_cut_ct = pd.crosstab(data.high_price, data.cut, values=data.price, \n                              aggfunc='count', normalize='index')\npricebin_cut_ct.style.background_gradient(cmap='autumn', axis=1)","66666528":"pricebin_cut_ct[['Very Good', 'Premium', 'Ideal']].sum(axis=1)","92b2a2c3":"ax = sns.countplot(x='color', data=data, hue='high_price')","a37673c4":"pricebin_color_ct = pd.crosstab(data.high_price, data.color, values=data.price, \n                                aggfunc='count', normalize='index')\npricebin_color_ct.style.background_gradient(cmap='autumn', axis=1)","1bf03295":"ax = sns.countplot(x='clarity', data=data, hue='high_price')","5ca5f90a":"pricebin_clarity_ct = pd.crosstab(data.high_price, data.clarity, values=data.price, \n                                  aggfunc='count', normalize='index')\npricebin_clarity_ct.style.background_gradient(cmap='autumn', axis=1)","a95a3e4e":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='high_price', y='depth', data=data, ax=ax[0])\nsns.boxplot(x='high_price', y='depth', data=data, ax=ax[1])\n\nax[0].set_title(\"Depth distribution by high_price\")\nax[1].set_ylim((58, 66))\nax[1].set_title(\"Depth distribution by high_price\\nZoomed in\")\n\nplt.show()","4c2ca21e":"f, ax = plt.subplots(ncols=2, figsize=(10,5))\nsns.boxplot(x='high_price', y='table', data=data, ax=ax[0])\nsns.boxplot(x='high_price', y='table', data=data, ax=ax[1])\n\nax[0].set_title(\"Table distribution by high_price\")\nax[1].set_ylim((50, 65))\nax[1].set_title(\"Table distribution by high_price\\nZoomed in\")\n\nplt.show()","199d85cf":"data['cut_encod'] = LabelEncoder().fit_transform(np.asarray(data.cut))\ndata['color_encod'] = LabelEncoder().fit_transform(np.asarray(data.color))\ndata['clarity_encod'] = LabelEncoder().fit_transform(np.asarray(data.cut))","45248023":"cor_mat = data.corr()","702aea47":"f, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(cor_mat, cmap='autumn', annot=True)","dc8827ce":"g = sns.pairplot(data, vars=['log_price', 'price', 'carat', 'x', 'y', 'z'])","c1277768":"data.drop(['carat_bin', 'high_price', 'x', 'y', 'z'], axis=1, inplace=True)","689a22a2":"g = sns.pairplot(data, vars=['log_price', 'price', 'depth', 'table', 'depth_table_ratio'])","fc7a2a05":"data.drop(['depth_table_ratio', 'dt_ratio_bin', 'depth_bin', 'table_bin'], axis=1, inplace=True)","71e41137":"data.head()","725a1dd4":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)","c7e7fac8":"ct = pd.crosstab(data.cut, data.clarity, data.log_price, aggfunc=np.mean)\nprint(\"Table 1. Mean log(price) map - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","6e1c3031":"ct = pd.crosstab(data.cut, data.clarity, data.carat, aggfunc=np.mean)\nprint(\"Table 2. Mean carat map - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","2851ca46":"ct = pd.crosstab(data.cut, data.clarity)\nprint(\"Table 3. Count diamonds - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","4a1fdd42":"ct = pd.crosstab(data.cut, data.color, data.log_price, aggfunc=np.mean)\nprint(\"Table 4. Mean log(price) - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","9f909dc0":"ct = pd.crosstab(data.cut, data.color, data.carat, aggfunc=np.mean)\nprint(\"Table 5. Mean carat - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","81082d0a":"ct = pd.crosstab(data.cut, data.color)\nprint(\"Table 6. Count of diamonds - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","39f1ce1e":"ct = pd.crosstab(data.clarity, data.color, data.log_price, aggfunc=np.mean)\nprint(\"Table 7. Mean log(price) - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","5114095d":"ct = pd.crosstab(data.clarity, data.color, data.carat, aggfunc=np.mean)\nprint(\"Table 8. Mean carat - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","9f317865":"ct = pd.crosstab(data.clarity, data.color)\nprint(\"Table 9. Count of diamonds - clarity vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","3f697abb":"g = sns.catplot(y='depth', kind='violin', hue='cut', data=data,\n                col='cut', col_wrap=3)","0b551cd5":"g = sns.catplot(y='table', kind='violin', hue='cut', data=data,\n                col='cut', col_wrap=3)","edebee09":"g = sns.relplot(x='depth', y='table', data=data, hue='cut',\n                col='cut', col_wrap=3)","a60697d3":"data.carat.value_counts(sort=True, ascending=False).head()","080089c2":"data_fixcarat = data[data.carat == 0.3]\ndata_fixcarat.carat.describe()","e3abe23d":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 10. Mean log(price) for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","31c0f275":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 11. Mean log(price) for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=0)","50f0c544":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.color, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 12. Count of diamonds for carat = 0.3 - cut vs color\")\nct.style.background_gradient(cmap=cmap, axis=1)","020e878f":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 13. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","c76f170e":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 14. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=0)","524902b0":"ct = pd.crosstab(data_fixcarat.cut, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 15. Mean log(price) for carat = 0.3 - cut vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","662eed2c":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 16. Mean log(price) for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","8099e7c3":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc=np.mean)\nprint(\"Table 17. Mean log(price) for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=0)","bf21c738":"ct = pd.crosstab(data_fixcarat.color, data_fixcarat.clarity, data_fixcarat.log_price, aggfunc='count')\nprint(\"Table 18. Count of diamonds for carat = 0.3 - color vs clarity\")\nct.style.background_gradient(cmap=cmap, axis=1)","6a49d263":"data.head()","749b9adc":"data.drop(['cut_encod', 'color_encod', 'clarity_encod'], axis=1, inplace=True)\ndata.head()","6e6eab79":"data = pd.get_dummies(data, drop_first=True)\ndata.head()","1a3eea07":"X = data.drop(['price', 'log_price'], axis=1).values\ny = data.price.values\n\nassert X.ndim == 2\nassert y.ndim == 1","ea1ef025":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n                                                    random_state=42)","2aef40a7":"train_X = sm.add_constant(X_train[:, 0])\ntrain_y = y_train.copy()\n\nlm1 = sm.OLS(train_y, train_X).fit()\nlm1.summary()","c9cae989":"fitted_y = lm1.fittedvalues\nres = lm1.resid\nres_student = lm1.get_influence().resid_studentized_internal","0b632625":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.2, edgecolors='k', linewidths=.1)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=1, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","e008f0a8":"train_X = sm.add_constant(X_train[:, 0])\ntrain_y = np.log(y_train)\n\nlm2 = sm.OLS(train_y, train_X).fit()\nlm2.summary()","4c3ce3f2":"fitted_y = lm2.fittedvalues\nres = lm2.resid","70345126":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.2, edgecolors='k', linewidths=.1)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=1, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","3eb2163d":"train_X = X_train[:, 0].reshape(-1, 1)\ntrain_X = PolynomialFeatures(degree=4).fit_transform(train_X)\n\ntrain_X = sm.add_constant(train_X[:, 1:])\ntrain_y = y_train.copy()\n\nlm3 = sm.OLS(train_y, train_X).fit()\nlm3.summary()","03e1c4c3":"fitted_y = lm3.fittedvalues\nres = lm3.resid\nres_student = lm3.get_influence().resid_studentized_internal","3638a9ec":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=3, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","19067c15":"train_X = X_train[:, 0].reshape(-1, 1)\ntrain_X = PolynomialFeatures(degree=4).fit_transform(train_X)\n\ntrain_X = sm.add_constant(train_X[:, 1:])\ntrain_y = np.log(y_train)\n\nlm4 = sm.OLS(train_y, train_X).fit()\nlm4.summary()","1a796342":"fitted_y = lm4.fittedvalues\nres = lm4.resid\nres_student = lm4.get_influence().resid_studentized_internal","0f7c87f0":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nsns.regplot(fitted_y, res, ci=None, scatter=False, lowess=True,\n            line_kws=dict(linewidth=3, color='r'))\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","b7f1c6e9":"f = plt.figure(figsize=(8, 8))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","d20ea240":"data.head()","7fd55c4e":"columns = data.drop(['carat', 'price', 'log_price'], axis=1).columns\ncarat_columns = ['carat', 'carat^2', 'carat^3', 'carat^4']\n\n# build polynomial carats, exclude cons\ncarat_poly = X_train[:, 0].reshape(-1, 1)\ncarat_poly = PolynomialFeatures(degree=4, include_bias=False).fit_transform(carat_poly)\ncarat_poly_df = pd.DataFrame(data=carat_poly, columns=carat_columns)\n\n# take quality features + concatenate carat and quality features \ntrain_X_df = pd.DataFrame(data=X_train[:, 1:], columns=columns)\ntrain_X_df = pd.concat([carat_poly_df, train_X_df], axis=1)\n\n# get responde DataFrame\ntrain_y_df = pd.DataFrame(y_train, columns=['log_price']).apply(np.log)","8f0e3f22":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm5 = sm.OLS(train_y_df, train_X_df).fit()\nlm5.summary()","892c9766":"fitted_y = lm5.fittedvalues\nres = lm5.resid\nres_student = lm5.get_influence().resid_studentized_internal","a1986a6e":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","67d46dbb":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","dd915fee":"train_X_df.head()","0502e738":"# take quality features + concatenate carat and quality features \ntrain_X_df = train_X_df.drop(['depth', 'table'], axis=1)\n\n# train_y_df remains as used in the previous example","8544efcb":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm6 = sm.OLS(train_y_df, train_X_df).fit()\nlm6.summary()","3754837a":"fitted_y = lm6.fittedvalues\nres = lm6.resid\nres_student = lm6.get_influence().resid_studentized_internal","a3510627":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","afc51bd1":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","3fc74696":"columns_cut = [col for col in train_X_df.columns if 'cut' in col]\n\n# remove cut columns from DataFrame\ntrain_X_df = train_X_df.drop(columns_cut, axis=1)\n\n# train_y_df is the same as used for the previous model","250a94b9":"# train model\ntrain_X_df = sm.add_constant(train_X_df)\nlm7 = sm.OLS(train_y_df, train_X_df).fit()\nlm7.summary()","973616ef":"fitted_y = lm7.fittedvalues\nres = lm7.resid\nres_student = lm7.get_influence().resid_studentized_internal","e0f10f28":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Residuals\", fontdict=dict(fontsize=16))\nplt.show()","07f6ffe1":"f = plt.figure(figsize=(6, 6))\nplt.scatter(fitted_y, res_student, s=70, alpha=.4)\nplt.plot(fitted_y, np.zeros_like(fitted_y), linestyle='--', color='k')\nplt.xlabel(\"Fitted values\", fontdict=dict(fontsize=16))\nplt.ylabel(\"Studentized Residuals\", fontdict=dict(fontsize=16))\nplt.show()","073f987d":"data.head()","9f420329":"X = data.drop(['price', 'log_price'], axis=1)    # as DataFrame\ny = data[['log_price']]    # as DataFrame\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n\nprint(type(X_train), type(X_test))\nprint(type(y_train), type(y_test))","8fdc158f":"# create identifiers for polynomial features and linear features\nPOLY_COLS = ['carat']\nLIN_COLS = [col for col in data.columns if col not in ['carat', 'price', 'log_price']]","2899f91d":"# create the functions to get each subset of the data\nget_polyfeatures = FunctionTransformer(lambda x: x[POLY_COLS], validate=False)\nget_linfeatures = FunctionTransformer(lambda x: x[LIN_COLS], validate=False)","a780c29b":"poly_pl = Pipeline([\n    ('selector', get_polyfeatures),\n    ('polynomial', PolynomialFeatures(degree=4, include_bias=False))\n])\n\n# display polynomial features after transformation\npoly_pl.fit_transform(X_train)[:5, :]","cbd81da1":"lin_pl = Pipeline([('selector', get_linfeatures)])\n\n# display first few lines of linear features (in this case, no other operation is performed)\nlin_pl.fit_transform(X_train).head()","032fd2ba":"# join both pipelines into one\nprep_join = FeatureUnion([\n    ('polynomial', poly_pl),\n    ('linear', lin_pl)\n])\n\n# display final resulting array's first 5 rows\nprep_join.fit_transform(X_train)","08b4b445":"# create identifiers for polynomial features and linear features\nPOLY_COLS = ['carat']\nLIN_COLS = [col for col in data.columns if col not in ['carat', 'price', 'log_price']]\n\n# create the functions to get each subset of the data\nget_polyfeatures = FunctionTransformer(lambda x: x[POLY_COLS], validate=False)\nget_linfeatures = FunctionTransformer(lambda x: x[LIN_COLS], validate=False)","9f7be285":"POLY_DEG=4\n\n# join both pipelines into one\nprep_join = FeatureUnion([\n    ('polynomial', Pipeline([\n        ('selector', get_polyfeatures),\n        ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n    ])),\n    ('linear', Pipeline([('selector', get_linfeatures)]))\n])","978a8527":"POLY_DEG = 4\n\n# create the regressor pipeline\nridge_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', Ridge(alpha=0.1))\n])","8c108538":"# perform first fit and use as starting point\nridge_pl.fit(X_train, y_train)\nridge_pl.score(X_test, y_test)","ef8d2dcc":"# set up grid of alphas to search\nalphas = np.logspace(-4, 4, 9)\n\n# set up GridSearch object to select best alpha and fit to data\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas}\nSCORE = 'neg_mean_squared_error'\ngs = GridSearchCV(ridge_pl, cv=CV_FOLDS, param_grid=PARAM_GRID, scoring=SCORE)","0749650a":"# fit to data and print best scores and parameters\ngs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","1571ddf2":"ridge_pl.set_params(regressor__alpha=1.0)\n\nridge_pl.fit(X_train, y_train)","0a661f43":"ridge_coef = np.squeeze(ridge_pl.named_steps['regressor'].coef_)","9468d394":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(ridge_coef)), width=ridge_coef)\nplt.yticks(range(len(ridge_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Ridge Coefficient Estimates\", fontdict=dict(fontsize=16))\nplt.show()","f21cd5f3":"# predictions and actual values as arrays\ny_pred = ridge_pl.predict(X_test)\ny_true = y_test.values","4ec13464":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","1fcf4ef0":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","bd99fa7d":"POLY_DEG = 4\n\n# create the regressor pipeline\nlasso_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', Lasso(alpha=0.1))\n])","ec8a6ab5":"# perform first fit and use as starting point\nlasso_pl.fit(X_train, y_train)\nlasso_pl.score(X_test, y_test)","55900c25":"# set up grid of alphas to search\nalphas = np.logspace(-4, 4, 9)\n\n# set up GridSearch object to select best alpha and fit to data\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas}\nSCORE = 'neg_mean_squared_error'\ngs = GridSearchCV(lasso_pl, cv=CV_FOLDS, param_grid=PARAM_GRID, scoring=SCORE)","ef0932f6":"# fit to data and print best scores and parameters\ngs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","883d5b15":"lasso_pl.set_params(regressor__alpha=0.0001)\n\nlasso_pl.fit(X_train, y_train)","6045d2f4":"lasso_coef = np.squeeze(lasso_pl.named_steps['regressor'].coef_)","1c2a6d49":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(lasso_coef)), width=lasso_coef)\nplt.yticks(range(len(lasso_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Lasso Coefficient Estimates, alpha = %.4f\" % 0.0001, fontdict=dict(fontsize=16))\nplt.show()","17d24d9f":"# get prediction and actual values as arrays\ny_pred = lasso_pl.predict(X_test).reshape(-1, 1)\ny_true = y_test.values","68e0ecc1":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","e76ef2e9":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","3130f08e":"en_pl = Pipeline([\n    ('union', FeatureUnion([\n        ('polynomial', Pipeline([\n            ('selector', get_polyfeatures),\n            ('polynomial', PolynomialFeatures(degree=POLY_DEG, include_bias=False))\n        ])),\n        ('linear', Pipeline([\n            ('selector', get_linfeatures)\n        ]))\n    ])),\n    ('regressor', ElasticNet())\n])","14fbdc9e":"en_pl.fit(X_train, y_train)\nen_pl.score(X_test, y_test)","8f02df57":"# create alphas space for search\nalphas = np.logspace(-4, 4, 9)\nl1_ratios = np.linspace(0, 1, 6)","9cfac132":"# prepare GridSearch arguments\nCV_FOLDS = 5\nPARAM_GRID = {'regressor__alpha': alphas, 'regressor__l1_ratio': l1_ratios}\nSCORE = 'neg_mean_squared_error'\n\ngs = RandomizedSearchCV(en_pl, cv=CV_FOLDS, param_distributions=PARAM_GRID, scoring=SCORE)","eb8d7452":"gs.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", gs.best_params_)\nprint(\"Best RMSE:           \", (-gs.best_score_) ** 0.5)","23460ea8":"en_pl.set_params(regressor__alpha=0.0001, regressor__l1_ratio=0.8)\n\nen_pl.fit(X_train, y_train)","8481cbdd":"en_coef = np.squeeze(en_pl.named_steps['regressor'].coef_)\nen_coef","c599b56a":"predictors = ['carat', 'carat^2', 'carat^3', 'carat^4']\npredictors.extend(X_train.columns[1:].tolist())\n\nplt.figure(figsize=(6, 8))\nplt.barh(y=range(len(en_coef)), width=en_coef)\nplt.yticks(range(len(en_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"ElasticNet Coefficient Estimates\\nalpha = %.4f, l1_ratio = %.1f\" % (0.0001,0.8), \n          fontdict=dict(fontsize=16))\nplt.show()","0a347411":"# get prediction and actual values as arrays\ny_pred = en_pl.predict(X_test).reshape(-1, 1)\ny_true = y_test.values","db5a4085":"print('R^2: %.4f' % (r2_score(y_test, y_pred)))\nprint('Exp. Var.: %.4f' % (explained_variance_score(y_test, y_pred)))\nprint('RMSE: %.4f' % (mean_squared_error(y_test, y_pred) ** .5))\nprint('MAE: %.4f' % (mean_absolute_error(y_test, y_pred)))","a521a580":"resid = (y_true - y_pred)\nsns.jointplot(x=y_pred, y=resid, kind='reg', \n              joint_kws=dict(fit_reg=False))\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()","b2fb3185":"plt.figure(figsize=(10, 10))\nplt.barh(y=range(len(ridge_coef)), width=ridge_coef, color='r')\nplt.barh(y=range(len(lasso_coef)), width=lasso_coef, color='b')\nplt.yticks(range(len(ridge_coef)), predictors)\nplt.xlabel(\"Coefficient estimate\")\nplt.ylabel(\"Predictors\")\nplt.title(\"Comparison of Coefficient Estimates\\nRidge in red, Lasso in blue\", \n          fontdict=dict(fontsize=16))\nplt.show()","97bd965d":"Alone, doesn't help predicting price. Let's try turning to bins (\"discretizing\"):","eb00b6ef":"### 7.1.5 5th model: log(price) ~ ALL features\nThis time will use data as DataFrame for better interpretation of results.","402f6ba0":"Analog to depth, doesn't seem to help predicting price. Like for depth, let's try \"discretizing\" and check for patterns.","1ae62c2e":"Again same overall pattern:\n- majority of cases receving low price\n- lower grades with higher mean prices","ba7ae065":"No null entries, as observed with the `.info()` method. Next, turn into `category` and set an order of importance.","65910459":"What we've learned so far:\n- overall, higher the weight, higher the price\n- relationship between mean price and carat bin is not linear:\n    - first three bins: mean value increases far more than the proportional increase in carat\n    - diamonds between 2 and 3 carats, and between 3 and 4 carats: apporximately same mean price\n    - diamonds heavier than 3 carats: the price still goes up with carat but the increase is more moderate\n- there are lighter diamonds that were highly valued: why?\n\nFor the last observation, let's check highly priced diamonds with small carat weight.","7635673e":"### 5.1.5. Depth and Table\nFrom a different __[reference](https:\/\/beyond4cs.com\/grading\/depth-and-table-values\/)__ I got information about depth and table.","962b117a":"## 5.2. Data cleaning\nThis step is crucial. Machine Learning algorithms don't work with NaNs (how missing values are encoded in `pandas` and `numpy`, and some are very sensitive to outliers and absurd values.\n\nMoreover, Data Cleaning focuses on removing problematic data entries whenever possible, be it for computational and\/or statistical reasons.","254cac7c":"Interpretation:\n- Table 13. for a given carat and cut, higher clarity grade related to increase in price.\n- Table 14. for a given carat and clarity, higher cut grade doesn't necessarily mean increase in price.\n\nLooking at the count of diamonds below.","a1b7c6ba":"Since two parameters are going to be tested, nw we use `RandomizedSearchCV` to reduce the workload (instead of testing 6 x 9 = 54 combinations, it sample a number of them and return the best results).","c056aaad":"Residuals plot behaves similarly but now we see a few more high studentized residuals appearing.","f26569a7":"**1. Build Pipeline**","205a5fd7":"### 5.1.6. x, y and z\n- x: length, in mm\n- y: width, in mm\n- z: depth, in mm\n\nThose are simply the dimensions of the diamonds. As most diamonds are approximately round-shaped, we expect `x` ~ `y`. `z` is the absolute value of depth, and should be coherent with the `depth`, `x` and `y` values.\n\nAlso, approximating a diamond for a prism, we should expect `carat` to be proportional to $x * y * z$.","ea9652d0":"(1) When 'y' is availabele but 'x' is absurd:","68ba3862":"### Depth\/Table ratio","1480947a":"Taking a look at the dataset as a whole with heavy weight and light weight carats:","6cce86b9":"## 5.3. Feature engineering","862b78d7":"Indeed log-transforming seems to bring the data closer to fit (with the help of a third order polynomial).\n\nNext, we do indeed create a partition in carat weight, but we split by 1.0 carat.","f2c02570":"# 7. Modelling","cbf4e7ce":"In order to leave the whole functionality in just one cell, I reproduce the \"complete\" pipeline below:","865bb0cb":"Again, similar behavior.","c2484533":"Interpretation:\n- Table 16. for a given carat and cut, higher clarity grade related to increase in price.\n- Table 17. for a given carat and clarity, higher cut grade doesn't necessarily mean increase in price.\n\nHere, different than what has been seen previously, both color and clarity grades seem to increase price very clearly.\n\nBelow we look at the count of diamonds once again.","f449d0d0":"**2. Set up GridSearchCV, find best parameter(s)**","b2fa125d":"The `.info()` method helps analysing a lot on information regarding data preparation.","07309086":"**3. Fit w\/ best parameter(s)**","f960bad9":"**5. Predict and print metrics on test set**","3a8839e1":"### Table","3b1ff1e9":"**2. Set up GridSearchCV, find best parameter(s)**","2d391afa":"Residuals still ehxibit highly non-linear pattern and heteroscadacity wasn't really taken care for. Regarding statistical metrics, $R^2$ got smaller and no improvement is found. We then try the polynomial approach on carat.","9e9a8df1":"On the basis purely of color, higher grades seem to decrease the value. Unexpected behavior.\n\n**Hypothesis:**\n\nLike with 'cut', the overall behvior is possibly being adversily affected by the majority of diamonds being light.\n\nNeed to look at combination between 'color' and other quality factors and how they affect diamond prices.","cae315cb":"### 7.1.7 7th model: log(price) ~ ALL features - depth - table - cut\nOne last model, now I will try removing cut: since it seemed to display a low effect on price in the multivariate analysis (cross tables), want to check.","17330038":"# 1. Load relevant libraries","dbfbd338":"**1. Build Pipeline**","95256d94":"Let's check the potentially absurd values.","a2479a24":"## 7.1. Statsmodels","1ab09ac1":"**Depth**","cbc84d44":"### Scikit-Learn: Conclusions","8df1241a":"(4) When `z` is absurd:","36e23f43":"Residuals plot resembles that of Ridge Regression as well.","42f84ab3":"**Carat**\n\nContinuous numerical variable then it's good to check for: missing values, absurd values, scale and possible errors.","0dd2a586":"# 4. Target variable inspection","5701ff15":"**6. Inspect residuals**","e9ba0cfc":"For better understanding it's good to go step by step.","25e0ac1a":"Combines the concepts of L1 and L2 regularizations by letting one chose the \"weight\" of each through the `l1_ratio` parameter. By default, uses `alpha=1` and `l1_ratio=0.5`.\n\nMinimizes:\n\n$1 \/ (2 * n_{samples}) * ||y - Xw||^2_2 + alpha * l1_{ratio} * ||w||_1 + 0.5 * alpha * (1 - l1_{ratio}) * ||w||^2_2$","3495eeb6":"Now we use this 'union pipeline' alongside other functionalities. It just lazily treats the data on demand for the algorithms and makes it easier to change the polynomial features degree. ","1cd98d14":"We begin by taking a look at some 10 random observations:","00220dcd":"'Color' seems to matter more for low 'cut' grades.","30523c62":"## Clarity vs Color","7ecf61a4":"Mean price seems to decrease as depth to table ratio increases.","55c0fb15":"## 5.1. Feature Explanation\nAs I know absilutely nothing about diamonds beforehand, checking some literature is always a good idead. The following quote from the Gemological Institute of America summarizes the __[diamond quality factors](https:\/\/www.gia.edu\/diamond-quality-factor)__: \"Diamonds with certain qualities are more rare\u2014and more valuable\u2014than diamonds that lack them. These are known as the 4Cs. When used together, they describe the quality of a finished diamond. The value of a finished diamond is based on this combination.\"\n\nIn the American Gem Society there's a comprehensive explanation of the __[4 C's of diamonds](https:\/\/www.americangemsociety.org\/page\/4cs)__, which is presented below, very summarized.","a9c5d15b":"#### 5.1.5.2. Table\nThe table refers to the flat facet of the diamond which can be seen when the stone is face up. It also happens to be the largest facet on a diamond and plays a vital role on brilliance and light performance of a stone.\n\nThe main purpose of the table facet is to refract light rays entering the diamond and to allow reflected light rays from the pavilion facets back into the observer\u2019s eye.\n\n![Table_percentage](https:\/\/beyond4cs.com\/wp-content\/uploads\/2013\/02\/tableandtablepercentagesofdiamond.png)\n\nIn a grading report, table percentage is calculated based on the size of the table divided by the average girdle diameter of the diamond. So, a 60 percent table means that the table is 60 percent wide as the diamond\u2019s outline.","e13868da":"Like depth and table, doesn't seem to be correlate with price. Again we trun to \"discretization\" for better visualization.","8b6ce7e4":"### 7.1.4. 4th model: log(price) ~ carat + carat^2 + carat^3 + carat^4","ddafee30":"Checking the data type and some statistics.","37125a4a":"Taking a quick glance at the distribution:","0713f275":"- Residuals plot displays that variance of error terms around 0 got more or less constant\n- Studentized Residuals Plot display there are still high residuals","ad74a3d8":"### 7.1.1. First Model: price ~ carat\nAs a first model, I start with the simplest possible as a starting point.","5a3b256a":"In Table 1: for a given value of cut, higher mean prices are more associated with lower clarity grades.\n\nHowever, when we look at Tables 2 and 3, we verify that the mean price is driven mainly by mean carat weight. In fact, the Table for price seems a combination of the effects of carat and count of diamonds.","18845348":"No null entries,as observed with the `.info()` method. Next, turn into `category` and set an order of importance.","821a5a96":"So we select carat = 0.3 for the maximum number of diamonds with same carat.","d7f0beb1":"What we learn:\ni. No missing values.\n\nii. No data type errors or typos.\n\niii. Seemingly high number of high values above 1.5 * Inter-Quartile Range, but there are too many to be outliers: probrably are highly priced diamonds, therefore, can't lose this information.\n\niv. About the distribution:\n- highly skewed to the right\n- 1\/4 of the diamonds below 950\n- 50% of the diamonds below 2,400\n- 1\/4 of the diamonds between 2,400 and 5,300\n- 50% of the diamonds between 950 and 5,300\n\nGiven the skewness of the distribution, I've seen in a lot of kernel authors perfoming a log-transformation on the target variable. After this transformation, the distribution of values allows for better statistical analysis and seems to improve models' performances. We'll see with Statsmodels that this will in fact improve the model and why it is so, but for now, I won't do that.","3d927781":"**4. Inspect coefficients visually**","d1476b07":"**3. Fit w\/ best parameter(s)**","031fddb3":"**4. Inspect coefficients visually**","96c44ae0":"### 5.1.4. Carat\n__[Carat](https:\/\/www.americangemsociety.org\/page\/diamondcarat)__ is the unit of measurement for the physical weight of diamonds. One carat equals 0.200 grams or 1\/5 gram and is subdivided into 100 points.\n\n![carat_ags](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/116161409755629.jpg)\n\nLarge diamonds are rarer than smaller ones, and as the carat weight increases, the value of the diamond increases as well. However, the increase in value is not proportionate to the size increase. For example, a 1-carat diamond will cost more than twice that of a \u00bd-carat diamond (assuming Color, Clarity and Cut grade are the same). Weight does not always enhance the value of a diamond, either. Two diamonds of equal weight may be unequal in value, depending upon other determining factors such as Cut, Color and Clarity.","52ebfef4":"####  Coefficient comparison: Ridge and Lasso","2ee3f52f":"Interesting result: it seems that high 'cut' grades tend to be found more often around 60 and 65% (more concentrated).","b6951c60":"The `.describe()` method reveals that there are at least three suposedly absurd situations:\n- x = 0: no sense in a diamond with zero length\n- y = 58.9: very weird when maximum x is about 10.7\n- z = 31.8: very weird when maximum x is about 10.7\n\nVisually inspecting:","a79335fe":"From the heatmap, we see that carat, x, y and z are highly correlated with price and between themselves. Drawing a pair plot in order to visualize as scatter plots:","4afd582d":"### 7.2.2. Ridge Regression: L2-norm penalization\nOn top of the Ordinary Least Squares, Ridge Regression adds a penalty term that is proportional to the square of the coefficients from the regression. The equation below is taken from Scikit-Learn's __[documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html)__\n\n$||y - Xw||^2_2 + alpha * ||w||^2_2$\n\nThis additional term penalizes large coefficients. The consequence is that smaller coefficients tend to produce smaller penalties and the coefficients are shrinked.\n\nThe 'alpha' term controls the regularization strength: higher alpha ~ stronger regularization ~ smaller coefficients","345563a9":"Values highly concentrated between ~58 and 65, but there are observations above and below. How many?","a8b80f35":"**Table %**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors.\n\nFrom `.info()` performed at the beginning of the cleaning stage, we know there are no missign entries, but if we didn't remember that:","2be7aee3":"### Color vs Clarity","7085f490":"Now the U-shaped is practically gone, suggesting a better fit, but heteroscedacity is still present. Next, both approach are put to work together.","c07cca0b":"I am not very skilled in this aspect, but one idea comes to mind: since 'cut' is related to 'depth' and 'table', I combine depth and table into one variable.","48aaf4ea":"## 6.2. Multivariate","8ec8db00":"Taking a look at some summary statistics:","e3eacf53":"## Table vs Cut","442c3390":"With larger diamonds come higher prices.","6b9cff4c":"### 7.2.1. Get the data","987694f0":"### Cut vs Clarity","234d3287":"## 6.1. Univariate\nHow is each predictor alone related to price?","bf75d625":"As observed before, the L1 penalty term in this particular dataset makes the alpha go very low, almost \"turning off\" the regularization.","8a31e2f1":"1. LOWESS (Locally WEighted Scatterplot Smoothing) curve shows a U-shape, suggesting a non-linear relationship is present\n2. Residuals should be equally scattered around zero-line for all fitted values (constant variance of residuals or homoscedacity): the plot displays a non-equally distributed behavior,i.e., hetroscedacity.\n\n(1) was expected, since we saw visually that price and carat relationship was not linear. One way to solve it would be a non-linear transformation on carat.\n\n(2) could be solved using a non-linear transformation on price, such as $\\sqrt{price}$ or $log(price)$.\n\nBoth are suggested by Hastie T. et al. in their book *Introduction to Statistical Learning*. We've visually tested both, so we'll now model them to see the results.","9372d3cc":"**Cut**\n![cut_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/135781409756963.jpg)","c723c227":"**6. Inspect residuals**","522fd714":"### Nulls\nNo values missing or encoded as `NaN`.","6cfe792a":"Aside from having less points outside the box-and-whiskers, the distributions seem analogous.","186266cf":"### Conclusions: what to expect from the models\n1. Carat is highly related to price in a non-linear  fashion\n    - log-transforming price + adding polynomial terms improved the fit visually\n2. Cut, Color and Clarity: given a carat value, we expect them to increase price as the grade is higher, but:\n    - Color and clarity clearly displayed that increase when combined for a fixed carat value\n    - Cut, from previous literature, is expected to increase price, but we couldn't see it so clearly (other aspects may be affecting)\n3. Table: from univariate analysis, increase in table is related to decrease in price - expect to see negative coefficient\n4. Depth: no very clear relationship with price","47151774":"Similar behavior to the observed for cut vs clarity: price is mainly affected by carat, but the count of diamonds shifts towards higher grades.","ddfc7cd1":"As expected, higher `carat` weights linked to greater dimensions of diamonds. Apparently large values (above 4) are not absurds but big diamonds.","756241bd":"Although some preliminary analysis found no missing values (NaNs), it seems that 0.0 encodes missing values in this dataset.\n\nSince we have confirmed that `x` ~ `y`, I propose a set of steps to deal with absurd dimension values, in the following order:\n1. When 'y' is available but 'x' is absurd (> 15 or 0.0): do 'x' = 'y'\n2. For the remaining observations, when 'x' is absurd (> 15 or 0.0): replace with the mean 'x' value over the entire dataset\n3. Next, when 'y' is absurd (>15 or 0.0): do 'y' = 'x'\n4. Finally, when 'z' is absurd (> 15 or 0.0): aproximate using $depth = z \/ average(x, y) * 100$ (showed in the Feature Explanation).","c07ae0b4":"### 7.2.3. Lasso Regression: L1-norm penalization\nOn top of the Ordinary Least Squares, Lasso Regression adds a penalty term that is proportional to the absolute magnitude of the coefficients of the regression. The equation below is taken from Scikit-Learn's __[documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html)__\n\n$(1 \/ (2 * n_{samples})) * ||y - Xw||^2_2 + alpha * ||w||_1$\n\nThis additional term penalizes large coefficients, but, different from Ridge, it actually shrinks smaller coefficients down to zero (sparsity). As a consequence, Lasso is sometimes used to perform feature selection (\"least important\" are left out).\n\nAs in Ridge, 'alpha' term controls the regularization strength: higher alpha ~ stronger regularization ~ smaller coefficients","1c0242e3":"**5. Predict and print metrics on test set**","a60d33b0":"As observed in the Statsmodels' models, although there are some very high residuals, there are some good aspects to this Residuals plot:\n- Residusl resemble normality (kind of)\n- Residuals seem to be equally dispersed around zero mean for all fitted values (homoscedacity)","e5c6b78f":"It's a continuous variable. A few points we may want to check:\n1. Are there missing values?\n2. Are there absurd values, i.e., negative, zero, strings, data type problems etc.?\n3. Are there outliers?\n4. How is it distributed over the range of values? Does it seem to follow a particular distribution?","6dafcfae":"- $R^2$ increased to 0.936, a very significant increase\n- Coefficient estimates still significant\n- F statistic display there is high relationship between predictors and response still\n- Omnibus and JarqueBera suggest residuals are still far from normal\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean","edcaa6c8":"Up until now I've only looked at the residuals plot, but now we take a look at the Studentized Residuals as well (the Residuals divied\n\n- Residuals plot displays that:\n    - variance of error terms around 0 got more or less constant\n    - LOWESS line is almost a straight line\n- Studentized Residuals Plot display there are high residuals","0a7b8736":"Through Statsmodels we've already tried out a few models and, using its statistical API, came to a few conclusions regarding important features.\n\nTherefore, I won't start with a simple model here and build it up: rather, I'll use Ridge and Lasso regressors from Scikit-Learn (regularized regression) on the model with all features and see if I come to similar conclusions regarding feature importance and prediction capability.\n\nAlso, I'll use some of the Scikit-Learn's funcionalities to speed the preprocessing steps.","b97da2e2":"# 2. Problem definition\nWe want to predict diamond price (continuous, numerical) based on certain measurements (features) using use prices already available. It's a Supervised Regression task.\n\nQuestions we may want to answer:\n1. Is there a relationship between the predictors and price?\n2. If so, how strong it this relationship?\n3. Which predictors seem to have greater impact?\n4. Can we predict price with the predictors available?","b0a67cb3":"- $R^2$ got to 0.985, a very significant increase\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera suggest residuals are far from normal\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean\n- Condition Number is very high, indicating high collinearity between terms.\n    - Although polynomial terms tend to be somewhat collinear, this value wasn't so high when only the carat terms were used\n    - Looking back at the pairplot, we see that depth and table are fairly collinear.\n\nTo reduce collinearity and increase the accuracy of the coefficient estimates, we'll try next removing depth and table.","b367b8a7":"# Workflow\n1. Load relevant libraries\n2. Problem Definition\n3. Data acquisition\n4. Target variable inspection\n5. Features inspection\n6. Exploratory Data Analysis\n    - 6.1. Univariate\n    - 6.2. Bivariate\n7. Modelling\n    - 7.1. Statsmodels\n    - 7.2. Scikit-Learn","b4a9dca6":"#### 5.1.5.1. Depth\n\nThe depth of a diamond is its height (in millimeters) measured from the culet to the table. On a grading report, there are normally two measurements of depth \u2013 the first is the actual depth measurement in millimeters, and the second is the depth percentage, which shows how deep the diamond is in relation to its width.\n\n*As explained Dataset page, `depth` here is the depth percentage, which can be approximated by $depth = [z \/ average(x, y)] * 100$.*\n\n![Depth percentage](https:\/\/beyond4cs.com\/wp-content\/uploads\/2013\/02\/depthpercentagesofdiamond.png)\n\nThe ideal depth percentage varies with the shape of the diamond. A depth percentage that may be too much for one shape might be essential for another. For instance, a princess cut with a 75 or 77 percent depth would still be considered acceptable and can yield an attractive diamond. On the other hand, a depth of 65 percent for a round diamond would be excessive and be detrimental to its beauty.","4a79956c":"**Color**","e0044a1f":"**Carat**","8b1f162c":"Partiotining carat into multiple bins for visualization shows:\n- 'carat' is positively related to 'price'\n- relationship seems non-linear\n\nLooking at how carat changes with price seems to hint yet again that log-transforming price might be a good idea (because of the shape of the relationship). Let's try.","382a141b":"## Cut vs Clarity\nHow does the interaction between 'cut' and 'clarity' affect the mean price?","05c92a99":"In the case of color highly priced diamonds seem to appear more often alongdside \"bad\" color grades than with \"good\" color grades.","aa47f6c9":"As expected from the feature explanation section, these entries are categories written in the form of text. Chaging data type is good practice here for visualization and data analysis.","b3e4d606":"Now all values seem to be fine!","ad32fecc":"# 3. Data Acquisition","313a691e":"As observed, better cuts tend to be more restrictive in terms of the range of depth and table values. However, being inside this range does not guarantee a good cut.\n\nFrom this, we learn that cut may help estimate depth and table, but the other way around is not true.","17022819":"This is my first practice Kernel in Data Science. My main goal was to use some basic Statsmodels API to evaluate some simple regression models. Later, I go from what I've learned and build on top of that with Scikit-Learn API as it's, to me, more friendly and more flexible.\n\nHope to be contributing to the Data Science community. Feel free to make on this Kernel better as well by comenting and suggesting.","6c349181":"## Fixed carat: how is price related to other features?\nHow quality features fare for a fixed carat: does increase in quality mean increase in price?","c63641b2":"### Conclusions: What we've learned\n1. All predictors, when used solely, have large and small priced diamonds over the entire range of their values\/categories.\n1. Carat weight:\n    - has positive correlation with price and relationship seems highly non-linear\n    - carat distribution is different between more valuable and less valuable diamonds.\n2. Cut, Color, Clarity:\n    - appear to have negative correlation with mean price\n    - apart from Color, frequency of diamonds is very similar across all grades.\n3. Depth:\n    - don't seem to have any clear relationsip with price\n4. Table:\n    - appear to have a negative correlation with price","a45af54e":"Depth% > 65:\n- around 835 diamonds\n- values of 'x', 'y', and 'z' don't particularly stand out: 'z' just seems to be close to them, probrably due to the shape of the cut\n\nDepth% < 58:\n- around 581 diamonds\n- values of 'x', 'y', and 'z' don't particularly stand out: 'z' just seems to be far from them, probrably due to the shape of the cut\n\nNo reason to cut out points outside Inter-Quartile Range of boxplot, but it's probably a good thing to verify how cut and depth interact in Exploratory Data Analysis.","84929348":"Let's arbitrarily look at diamonds above 10,000.","f9a71992":"**Clarity**","c8030238":"**Color**\n\n![color_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/164901433526626.JPG)","702791b1":"(3) Where `x` is available but `y` is absurd:","e9329487":"**The same kind of relationship found between 'price' and 'carat' is reproduced between 'price' and the dimensional features 'x', 'y', and 'z'.** This is largely due to these dimensions being highly correlated with 'carat' and between themselves.\n\nFor simplicity, and to prevent high collinearity from affecteing the coefficient estimates of the model I'll drop 'x', 'y', 'z', since carat seem to","6dde3485":"**5. Predict and print metrics on test set**","744842de":"No clear difference in distributions on the basis of clarity.","9af3d88e":"Interpretation:\n- Table 10. for a given carat and cut, higher color grade related to increase in price.\n- Table 11. for a given carat and color, higher cut grade doesn't necessarily mean increase in price.\n\nLooking at the count of diamonds below.","0575be3c":"Since 'depth_table_ratio' doesn't add information, will also drop (alongside epth and table bins.","e52ffcf1":"### Data types\nThree variables with `object` data type. This is the data type of strings\/text in Python. It is relavant to dig a little deeper here and understanding why might it be that these entries are encoded as strings:\n1. are they text data?\n2. are they categories written as text?\n3. are they numeric that due to errors got coerced into `object`?\n\nIt's important to differentiate case 2 from case 1 as there's a specific `category` data type in Python, that saves memory and allows for ordering, which helps in data analysis tasks. In the case 3, it might be that we'll need to perform some more cleaning steps.\n\nBelow, I summarize the entries on each column to see unique values.","65c2b8bb":"Of highly priced diamonds:\n- Three highest prices: maximum grades for 'clarity' and 'color'\n- Fourth highest price: maximum grades for 'cut' and color, 2nd highest for 'clarity'\n\nBetween diamonds close in 'carat', exceptionally good quality drives price up. Seems to confirm that high quality grades increase price.","9b36bdfd":"### What we learn:\nFrom the basic reserach of the literature, we expect:\n- carat, clarity, color and cut to play a big role diamond price\n- depth and table are also important, but not clearly how\n- x,y and z are important as they help determine carat and depth, but seem to be of secondary importance\n\nI won't assume an order of importance between the 4 Cs since for me it isn't quite clear from the previous explanations which order this should be. On the other hand, I don't expect to any of the dimensions to be largely relevant as other variables already capture their importanca with them.","f6ab5d5a":"Not really good to see as there are way less highly priced diamonds. Let's try using a table, normalization and some colors:","64ab7490":"### 5.1.2. Color\nThe __[color](https:\/\/www.americangemsociety.org\/page\/diamondcolor)__ of a diamond actually refers to the lack of color in a diamond, with perfectly colorless diamonds considered the highest quality with the highest value, and brown or yellow diamonds being the lowest quality. Using a master set of diamonds specifically chosen based on their range of color, a grader picks up the diamond and places it next to the individual diamonds in the master set. The diamond grader then decides the color grade based on the saturation of the color compared to the master set.\n\n![color_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/164901433526626.JPG)","f5bb4364":"Model didn't suffer much:\n- $R^2$ dropped to 0.983, one indication that cut didn't help much\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera still high, but we'll see from residuals that theyare \"better behaved\"\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean","669e503d":"- 67.6% weight 1 carat or less.\n- 96.5% weight 2 carats or less.\n- approximately 0.06% weight more than 3 carats.\n\nExpect to see majority of low prices, because majority of diamonds are are light.\nVisualizaing:","a946e92e":"Very similar distributions as well.","5d8722a9":"**4. Inspect coefficients visually**","d4721d28":"**x, y and z**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors.","e10facde":"First it's better to encode quality, categorical variables so they get visible.","18e93744":"## Depth vs Table vs Cut","8fac572e":"- $R^2$ achieved of 0.850, good for a starting value\n- Coefficients are significant, as p-values are low\n- F-test for regression states there is a correlation between predictors and response\n- residuals are non-normal (form Omnibus and Jarque-Bera)\n\nProceeding to examine residuals:","152679e5":"## Depth vs Cut","2005207c":"'Depth' values balanced across bins. No particular relationship. Alone, not a good predictor of price.","861c4c75":"- $R^2$ stayed at 0.985, one indication that depth and table didn't help much\n- Coefficient estimates are all significant\n- F statistic's p-value suggests there is high relationship between predictors and response\n- Omnibus and JarqueBera still high, but we'll see from residuals that theyare \"better behaved\"\n- Skew is very close to zero, suggesting more symmetrical residuals\n- Kurtosis is somewhat large, indicating more concentration of residuals around zero mean\n- Condition Number got reduced and warning is now gone, indicating that collinearity is not a huge issue now (would need to see VIF statistic ti be sure, but for simplicity won't do that now).","4ca84c15":"## Cut vs Color","75056ffd":"When turned to bins, 'table' seems to display a positive correlation with price, except for depths greater than 65%.","7dbf0f88":"Instead of preparing a different, complete DataFrame as I did for Statsmodels, I'll use `FunctionTransformer()` and `FeatureUnion()` functionalities to transform the data \"on the fly\" for each model.","6aedf790":"**Table**","ac6bae58":"### Cut vs Color","a457d209":"In the case of the top 3 cut grades:","86b2d0d6":"Create pipelines to extract and treat differently each subset of the data.\n\nBelow, I extract the features of interest for polynomial transformation and separately the feaures to go untouched (in this case).","af011b78":"### 7.1.6 6th model: log(price) ~ ALL features - depth - table\nFor this model I tried first removing depth, since it has weaker relationship with price. In doing so, the coefficient estimate for table got a p-value > 0.5, meaning it became non-significant.\n\nThefere, the next model presents only the version where I removed both depth and table.","b7eb9945":"### 7.1.3. 3rd model: price ~ carat +carat^2 + carat^3 + carat^4","20985b5f":"(2) When 'x' is absurd but 'y' is not available\n\nFor simplicity, as this only seems to happen when both are zero, I do:","6041db95":"### 5.1.1. Cut\nThe __[cut](https:\/\/www.americangemsociety.org\/page\/diamondcut)__ of a diamond refers to how well the diamond\u2019s facets interact with light, the proportions of the diamond, and the overall finish of the diamond.\n\nIt is not to be confused with the shape, (like emerald or round,) or facet arrangement, (like brilliant, or step cut), but is instead a reference to the craftsmanship of the diamond and how it factors into the diamond\u2019s brilliance. AGS grades cut on a scale from 0 to 10, with 0 being \u201cIdeal\u201d and 10 being \u201cPoor.\u201d AGS has a proprietary numeric and verbal descriptors for cut. The numeric descriptors for the Diamond Cut Grade follow the American Gem Society's standards for how well a diamond is cut. The verbal descriptors are AGS Ideal, Excellent, Very Good, Good, Fair, and Poor.\n\n![cut_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/135781409756963.jpg)","7d82b636":"I separate a DataFrame `data` so that the changes I perform are recorded on it, but there's still one version of the data untouched.","0067916e":"Setting the axes limits between 0 and 15 reveals that the hypotheses that x is approximately equal to y was somewhat accurate. Now looking at `y` vs `carat`.","26e2e35f":"**3. Fit w\/ best parameter(s)**","7f908f93":"### Carat Weight","5379e036":"### 5.1.3. Clarity\n__[Clarity](https:\/\/www.americangemsociety.org\/page\/diamondclarity)__ is the state of being clear or transparent. Diamond clarity is the presence or absence of characteristics called inclusions in the diamond. In short, inclusions are the internal or external flaws of the diamond. The size and severity of these flaws determines the grade. Since many inclusions and blemishes are very small, and can be difficult to see with the naked eye, they are graded at 10x magnification. Clarity grade is determined on a scale of decreasing clarity from the highest clarity (Flawless or FL) to the lowest clarity (Included 3, or I3).\n\n![clarity_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/79661461782004.png)\n\n**AGS 0 - Flawless or Internally Flawless:** no inclusions or blemishes visible under 10x; Internally Flawless diamonds have no inclusions visible under 10x, but can have very minor blemishes (marks and features confined to the surface only).\n\n**AGS 1 or 2 - VVS:** has minute inclusions that are difficult for a skilled grader to see under 10x magnification.\n\n**AGS 3 or 4 - VS:** have minor inclusions.\n\n**AGS 5, 6, or 7 - SI:** have noticeable inclusions that are fairly easy to see under 10x magnification; sometimes, these inclusions can be visible to the unaided eye.\n\n**AGS (7, 8, 9, or 10) - I:** have inclusions that are obvious at 10x magnification; sometimes, they can be seen with the naked eye. At the lower clarities, may have an effect on the diamond\u2019s durability.\n\nThe modern clarity scale was invented in the 1950s, by a former president of GIA, Richard T. Liddicoat, Jr. With minor modifications, it has been the universal standard ever since, using verbal descriptors most are now familiar with: Flawless, Internally Flawless, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, and I3.","876106b3":"No null entries, as observed with the `.info()` method. Next, turn into `category` and set an order of importance.","422b3e80":"Here a very similar finding to that of table: higher cut grades tend to be found around specific table values. Specifically, it seems to be between 55 and 63.","231993ab":"### Cut","a0c1ec71":"Residuals plot resembles that of Ridge Regression as well.","d9aa9a5a":"### Clarity","8301d0bc":"After using Statsmodels to investigate linear models, non-linear transformation and analyse some easily available statistics, we've used Scikit-Learn's API to:\n- build Pipelines to create polynomial terms \"on th fly\" and \"grid search\" for the best parameters\n- perform regularized regressions with Ridge and Lasso\n- investigate results on test sets\n\nBoth API's are great tools to fit and analyse models, with some differences on outputs and capabilities. This is as far as my expertise of both APIs go for now, so I stop here. Hope you've enjoyed and that you feel like contributing, commenting, and upvoting!","9d93a41d":"Well, that's pretty much as far as my knowledge in Linear Regression Statistics and Statsmodels go.\n\n**Regarding the best model**\n\nModel 5 ( log(price) ~ polynomial carat) presented the best behavior in terms of residuals. However, by including categorical features like clarity and color, model 7 seemed to outperform model 5 in terms of prediction (as measured by the $R^2$), although there are high studentized residuals for all fitted values.\n\n**What we've done:**\n- used Statsmodels to build some less complex models for the price task regression\n- evaluated some statistical aspects of each model\n- developed some knowledge about the importane of the features\n- got an $R^2$ of about 0.985 on the training set\n- a few statistics pointed out that the model used might not be the best one, however the ease of interpretation is in favor of the model built.\n\n**What we do next**\n- use some powerful tools like Cross-Validation in Scikit-Learn to evaluate other models\n- evaluate the generalization capacity of each model on the test set","5a0e9078":"### 7.1.2. Second model: log(price) ~ carat\nFirst just the log-transformation of the response.","8fcba972":"### Color","367465f9":"Only three rows present values above 12. For simplicity, I'll arbitrarily establish that above 15 is absurd. Domain and historical knowledge would be useful here, but in the absence of any, I'll simplify.\n\nNext,let's look at the other end (low values).","f1314a93":"### Valuable diamonds: are they distributed differently?\nAs all boxplots displaying price display a large number of diamonds outside Inter-Quartile Range, I investigate a little further highly priced diamonds. Is there any particular pattern for the other variables?","2c5961ff":"- Residuals plot displays that variance of error terms around 0 got more or less constant\n- Studentized Residuals Plot display there are some very high residuals, even higher then before","1c1e4fee":"**6. Inspect residuals**","c46c1db9":"Values of 'x', 'y' and 'z' don't particularly stand out. Table is high or low probrably due to shape of cut.","22b5432f":"Notice that even with a very low alpha, Lasso shrinks the coefficients even more.","2b6b6808":"`dtype` is `float64`, so only numeric entries.\n\nWhat about the values themselves? Some very low and some very high. We follow with a visual inspection on the distribution of prices.","c888ee63":"## 7.2. Scikit-Learn","46f8ed7a":"About 'cut':\n- 87.9% above 'Very Good'\n- seems to be negatively related to 'price'\n\nHere, a non-expected behavior appears. Why would higher 'cut' grades have lower prices?\n\n**Hypotheses:**\n\nMajority of diamonds have good 'cut' grade. Also, majority of diamonds have low carat weight. 'cut' is \"receiving bad reputation\" for something it may be not responsible for.\n- most of the diamonds ~ carat < 1 ~ on average, small price\n- most of the diamonds ~ Ideal cut\n\nBy association: Ideal cut ~ small prices\n\nIt should be true, then, that 'cut' only drives up the price:\n- when comparing same carat diamonds\n- when alongside other distinctive quality factors\n\nOn Bivariate\/Multivariate analysis (predictors relationships between themselves), we wnat to check that.","47acd572":"**Clarity**\n\n![clarity_grading](https:\/\/cdn.ymaws.com\/www.americangemsociety.org\/resource\/resmgr\/images\/GemsJewelry\/79661461782004.png)","2dadff62":"### 7.2.4 Elastic Net: Ridge + Lasso ","3bf2a5b0":"**Depth %**\n\nContinuous numerical variable: check for missing values, absurd values, scale and possible errors.\n\nNo null entries,as observed with the `.info()` method. In case we forgot that:","42c6d5ab":"And the result is very similar to the that obtained with Statsmodels:\n- carat has the biggest average weight on price, followed by the 2nd order term of carat\n- depth, table and cut have very low weight on price\n- the quality features have increasing weight with increasing quality grade, as expected.","2c026fdd":"### Depth","ae8d1692":"**Procedure**\n1. build pipeline to transform the data and apply a \"grid search\" for best parameters\n2. set up and perform GridSearchCV\n3. fit model with best paramater(s)\n4. inspect coefficients\n5. predict on test set\n6. print and store metrics\n    - $R^2$,\n    - Explained Variance,\n    - RSME, \n    - MAE\n7. inspect residuals","107c38c7":"### Statsmodels: Conclusions","67950bb0":"As we have observed, the GridSearchCV actually dound the best value for alpha to be very low. In practice, this means regularization is almost absent. AS a result, the coefficients are not shrinked and feature selection is not performed.\n\nFrom the plot, in fact, the coefficients resemble those of Ridge regression.","4aa5f912":"Highly priced diamonds don't seem to present a different distribution across 'cut' grades: both have majority of diamonds on high grades (pprobably because overall there are more good grades than bad ones).","0f3a7771":"Values highly concentrated between 50 and 65. How many?","ebb9bf6e":"**Cut**","9bb3fcca":"**Notice:** the ouput of lin_pl pipeline is a DataFrame whereas the output of poly_pl pipeline is an array. When they're put together using FeatureUnion, the final output is coerced to an array.","4042eab1":"# 6. Exploratory Data Analysis - EDA","a3cb1eb8":"# 5. Feature inspection","9451d438":"Define cmap for bivariate visualization:"}}