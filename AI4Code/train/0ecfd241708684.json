{"cell_type":{"83716395":"code","9ce36f04":"code","29f9637e":"code","f6cfaaf0":"code","5a1f53c9":"code","1cbb81a1":"code","0874497a":"code","cde253ef":"code","50a75af7":"code","e06e2ee8":"code","3d2b39ba":"code","98c28e74":"code","92598fde":"code","be1f3a5c":"code","488ce137":"code","762f14c9":"code","4e63602c":"code","eedeba89":"code","a718ce42":"code","c991414f":"code","041cbe77":"code","d321a389":"code","624ad58f":"code","05ce989f":"code","282fc742":"code","501ff2f7":"code","d6d0440c":"markdown","1c0080e0":"markdown","f2020bc6":"markdown","6596da67":"markdown","3ea3b2ea":"markdown","6b005028":"markdown","01384740":"markdown"},"source":{"83716395":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9ce36f04":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nimport scipy.misc \nfrom math import sqrt \nimport itertools\nfrom IPython.display import display\n%matplotlib inline","29f9637e":"#Loading the dataset\ndata= pd.read_csv('..\/input\/facial-expression-recognitionferchallenge\/fer2013\/fer2013\/fer2013.csv')\ndata.head()","f6cfaaf0":"#Number of training samples of each emotions\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10","5a1f53c9":"#Assigning Names to Emotions in labels\n\nnum_classes = 7\nwidth = 48\nheight = 48\nemotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\nclasses=np.array((\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"))\ndata.Usage.value_counts() ","1cbb81a1":"depth = 1\nheight = int(sqrt(len(data.pixels[0].split()))) \nwidth = int(height)\nfor i in range(0, 10): \n    array = np.mat(data.pixels[i]).reshape(height, width) \n    image = scipy.misc.toimage(array, cmin=0.0) \n    display(image)\n    print(emotion_labels[data.emotion[i]]) ","0874497a":"train_set = data[(data.Usage == 'Training')] \nval_set = data[(data.Usage == 'PublicTest')]\ntest_set = data[(data.Usage == 'PrivateTest')] \nX_train = np.array(list(map(str.split, train_set.pixels)), np.float32) \nX_val = np.array(list(map(str.split, val_set.pixels)), np.float32) \nX_test = np.array(list(map(str.split, test_set.pixels)), np.float32) \nX_train = X_train.reshape(X_train.shape[0], 48, 48, 1) \nX_val = X_val.reshape(X_val.shape[0], 48, 48, 1)\nX_test = X_test.reshape(X_test.shape[0], 48, 48, 1)","cde253ef":"num_train = X_train.shape[0]\nnum_val = X_val.shape[0]\nnum_test = X_test.shape[0]","50a75af7":"from keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator ","e06e2ee8":"y_train = train_set.emotion \ny_train = np_utils.to_categorical(y_train, num_classes) \ny_val = val_set.emotion \ny_val = np_utils.to_categorical(y_val, num_classes) \ny_test = test_set.emotion \ny_test = np_utils.to_categorical(y_test, num_classes)","3d2b39ba":"datagen = ImageDataGenerator( \n    rescale=1.\/255,\n    rotation_range = 10,\n    horizontal_flip = True,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    fill_mode = 'nearest')\n\ntestgen = ImageDataGenerator( \n    rescale=1.\/255\n    )\ndatagen.fit(X_train)\nbatch_size = 64","98c28e74":"for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n    for i in range(0, 9): \n        pyplot.axis('off') \n        pyplot.subplot(330 + 1 + i) \n        pyplot.imshow(X_batch[i].reshape(48, 48), cmap=pyplot.get_cmap('gray'))\n    pyplot.axis('off') \n    pyplot.show() \n    break","92598fde":"train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \nval_flow = testgen.flow(X_val, y_val, batch_size=batch_size) \ntest_flow = testgen.flow(X_test, y_test, batch_size=batch_size)","be1f3a5c":"from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import Adam, SGD\nfrom keras.regularizers import l1, l2\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix","488ce137":"def FER_Model(input_shape=(48,48,1)):\n    # first input model\n    visible = Input(shape=input_shape, name='input')\n    num_classes = 7\n    #the 1-st block\n    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n    conv1_1 = BatchNormalization()(conv1_1)\n    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n    conv1_2 = BatchNormalization()(conv1_2)\n    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n\n    #the 2-nd block\n    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n    conv2_1 = BatchNormalization()(conv2_1)\n    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n    conv2_2 = BatchNormalization()(conv2_2)\n    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n    conv2_2 = BatchNormalization()(conv2_3)\n    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)\n\n     #the 3-rd block\n    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n    conv3_1 = BatchNormalization()(conv3_1)\n    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n    conv3_2 = BatchNormalization()(conv3_2)\n    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n    conv3_3 = BatchNormalization()(conv3_3)\n    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n    conv3_4 = BatchNormalization()(conv3_4)\n    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)\n\n    #the 4-th block\n    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n    conv4_1 = BatchNormalization()(conv4_1)\n    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n    conv4_2 = BatchNormalization()(conv4_2)\n    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n    conv4_3 = BatchNormalization()(conv4_3)\n    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n    conv4_4 = BatchNormalization()(conv4_4)\n    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n    \n    #the 5-th block\n    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n    conv5_1 = BatchNormalization()(conv5_1)\n    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n    conv5_2 = BatchNormalization()(conv5_2)\n    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n    conv5_3 = BatchNormalization()(conv5_3)\n    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n    conv5_3 = BatchNormalization()(conv5_3)\n    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)\n\n    #Flatten and output\n    flatten = Flatten(name = 'flatten')(drop5_1)\n    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)\n\n    # create model \n    model = Model(inputs =visible, outputs = ouput)\n    # summary layers\n    print(model.summary())\n    \n    return model","762f14c9":"model = FER_Model()\nopt = Adam(lr=0.0001, decay=1e-6)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","4e63602c":"from keras.callbacks import ModelCheckpoint\nfilepath=\"weights_min_loss.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","eedeba89":"# we iterate 200 times over the entire training set\nnum_epochs = 200  \nhistory = model.fit_generator(train_flow, \n                    steps_per_epoch=len(X_train) \/ batch_size, \n                    epochs=num_epochs,  \n                    verbose=2,  \n                    callbacks=callbacks_list,\n                    validation_data=val_flow,  \n                    validation_steps=len(X_val) \/ batch_size)","a718ce42":"%matplotlib inline\n\ntrain_loss=history.history['loss']\nval_loss=history.history['val_loss']\ntrain_acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nepochs = range(len(train_acc))\n\nplt.plot(epochs,train_loss,'r', label='train_loss')\nplt.plot(epochs,val_loss,'b', label='val_loss')\nplt.title('train_loss vs val_loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs,train_acc,'r', label='train_acc')\nplt.plot(epochs,val_acc,'b', label='val_acc')\nplt.title('train_acc vs val_acc')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.figure()","c991414f":"loss = model.evaluate_generator(test_flow, steps=len(X_test) \/ batch_size) \nprint(\"Test Loss \" + str(loss[0]))\nprint(\"Test Acc: \" + str(loss[1]))","041cbe77":"model.save('..\/working\/Fer2013.h5')","d321a389":"loss = model.evaluate(X_test\/255., y_test) \nprint(\"Test Loss \" + str(loss[0]))\nprint(\"Test Acc: \" + str(loss[1]))","624ad58f":"loss = model.evaluate(X_val\/255., y_val) \nprint(\"Validation Loss \" + str(loss[0]))\nprint(\"Validation Acc: \" + str(loss[1]))","05ce989f":"def plot_confusion_matrix(y_test, y_pred, classes,\n                          normalize=False,\n                          title='Unnormalized confusion matrix',\n                          cmap=plt.cm.Blues):\n    cm = confusion_matrix(y_test, y_pred)\n    \n    if normalize:\n        cm = np.round(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], 2)\n        \n    np.set_printoptions(precision=2)\n        \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.min() + (cm.max() - cm.min()) \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True expression')\n    plt.xlabel('Predicted expression')\n    plt.show()","282fc742":"y_pred_ = model.predict(X_test\/255., verbose=1)\ny_pred = np.argmax(y_pred_, axis=1)\nt_te = np.argmax(y_test, axis=1)","501ff2f7":"fig = plot_confusion_matrix(y_test=t_te, y_pred=y_pred,\n                      classes=classes,\n                      normalize=True,\n                      cmap=plt.cm.Greys,\n                      title='Average accuracy: ' + str(np.sum(y_pred == t_te)\/len(t_te)) + '\\n')","d6d0440c":"# Building Neural Network Model","1c0080e0":"# Image Preprocessing","f2020bc6":"# Checking the performance of the model","6596da67":"# Training the DataSet","3ea3b2ea":"# Splitting the dataset","6b005028":"# Visualisation of Dataset","01384740":"The dataset we are using for Emotion Detection for Facial Expressions is \"FER2013\"  "}}