{"cell_type":{"4802743d":"code","fe6c6583":"code","63e4982e":"code","304c4896":"code","1f77370d":"code","e23c4dfd":"code","fe110d62":"code","bf416af7":"code","cbacb5cd":"code","b6004ba7":"code","f575b46e":"code","89e450b4":"code","706be95b":"code","25bfd8e1":"code","29a931d8":"code","c8e6bb25":"code","3a1ea2b5":"code","8d49f084":"code","f05c50ac":"code","723adbc9":"code","4e8838a3":"code","ba521e87":"code","ddf80ea0":"code","f7d7f545":"code","6448ea89":"code","ab61dcb4":"code","61068973":"code","5ad1db91":"code","2a04e3aa":"code","6cabaef5":"code","69461080":"code","a877058e":"code","60cbea84":"code","fc43b8d3":"code","4e83d9e4":"code","32768027":"code","d2547cd6":"code","73276413":"code","1ad6edaf":"code","d1d9ace6":"code","89ebf442":"code","b5aa6f80":"code","83c78797":"code","d9e49c39":"code","d2258a8e":"code","5cbad7d2":"code","6b141ccd":"code","34762418":"code","06347e4f":"code","db980ea1":"code","f93f9ba0":"code","fdc02807":"code","79adbe81":"code","11cc1590":"code","01bc2aab":"code","dd181451":"code","5d8ad80c":"code","e725b0e4":"code","6f7b6a19":"code","710e24e7":"code","00bf951b":"code","9f5934e4":"code","d04cb88f":"code","fb13e1a4":"code","b0335679":"code","9cea2e74":"code","5dc717dd":"code","23b1783a":"code","04715b72":"code","c4f6460e":"code","8fb86389":"code","4f3b324f":"code","13748395":"code","537e7427":"code","6a68e210":"code","5fe1e828":"code","bbf15322":"code","f528acde":"code","64163cd5":"code","436e9590":"code","425d1427":"markdown","4096f223":"markdown","800b8e59":"markdown","1506d0a8":"markdown","4e6ccfce":"markdown","3bba0f1f":"markdown","d0c19e4b":"markdown","0570c0e9":"markdown","0d6ece48":"markdown","28478d90":"markdown","de23a560":"markdown","e5b82c15":"markdown","f8e4ba54":"markdown","4f1313fa":"markdown","7c20462c":"markdown","40051ca4":"markdown","dcc4df2f":"markdown","b47b09b6":"markdown","3e0058f0":"markdown","811e9439":"markdown","2ef9444f":"markdown","d3acbed5":"markdown","556d1758":"markdown","1292f853":"markdown","38161dfe":"markdown","54b017bf":"markdown","1f68bc29":"markdown","572a024f":"markdown","be1820ff":"markdown"},"source":{"4802743d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fe6c6583":"# Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","63e4982e":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","304c4896":"# Importing train and test data.\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","1f77370d":"# before doing any analysis we will check our data that it imported properly or not.\n\ntrain_data.head()","e23c4dfd":"test_data.shape","fe110d62":"train_data.shape","bf416af7":"# as can see train_data having 891 unique values with 12 feature. and test_data having 418 unique values with 11 feature.\n# in test_data one feature is not included \"survived\" which is DV and we need to identify.\n# for doing further few cleaning and munging i am combining both data.","cbacb5cd":"titanic = [train_data, test_data]","b6004ba7":"# checking train columns\n\ntrain_data.columns","f575b46e":"train_data.info()\n\nprint(\"------------------------------------------\")\n\ntest_data.info()\n\n# Information collected from info - \n#Seven features are integer or floats. Six in case of test dataset.\n#Five features are strings (object).","89e450b4":"# from above ingormation can see there is null value in data. lets check by once again by using isnull \ntrain_data.isnull().sum() \n\n# Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n# Cabin > Age are incomplete in case of test dataset.","706be95b":"test_data.isnull().sum()","25bfd8e1":"# Checking some statistics information about data\n\ntrain_data.describe()\n\n","29a931d8":"train_data.describe(include=['O'])","c8e6bb25":"test_data.describe()","3a1ea2b5":"test_data.describe(include=['O'])","8d49f084":"# Lets create some pivot to see more detail analysis","f05c50ac":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","723adbc9":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","4e8838a3":"# as shown from above pivot female are most likely to survived around(74%)of given data.","ba521e87":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index = False ).mean().sort_values(by = 'Survived', ascending = False)","ddf80ea0":"# passenger who is trvelling with one sibling have chances to survived (53%) compared to passenger who have more than one family member.","f7d7f545":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index = False ).mean().sort_values(by = 'Survived', ascending = False)","6448ea89":"# Correlating numerical features \n#I will use Histogram chart for analyzing continuous numerical variable like age.","ab61dcb4":"g = sns.FacetGrid(train_data, col = 'Survived')\ng.map(plt.hist, 'Age', bins = 20)","61068973":"g = sns.FacetGrid(train_data, col = 'Survived', row = 'Pclass')\ng.map(plt.hist, 'Age', bins = 20)\ng.add_legend()","5ad1db91":"grid = sns.FacetGrid(train_data, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex')\ngrid.add_legend()","2a04e3aa":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived')\ngrid.map(sns.barplot, 'Sex', 'Fare')\ngrid.add_legend()","6cabaef5":"train_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_data, test_data]","69461080":"train_data.columns","a877058e":"test_data.columns","60cbea84":"combine[0].shape","fc43b8d3":"combine[1].shape","4e83d9e4":"# Title feature added\n# can see majorly passenger use Miss, Mrs, Mr, master for their title.\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","32768027":"# We can replace many titles with a more common name or classify them as Rare.\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    \n    \n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d2547cd6":"# We can convert the categorical titles to ordinal.\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","73276413":"train_data.head(10)","1ad6edaf":"test_data.head(3)","d1d9ace6":"# Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.\n\ntrain_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape","89ebf442":"# Encoding categorical feature into Numeric\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n","b5aa6f80":"train_data.head()","83c78797":"train_data.corr().sort_values(by = \"Survived\",ascending = True)","d9e49c39":"corr = train_data.corr()\nprint (corr['Survived'].sort_values(ascending=False)[:10], '\\n')\nprint (corr['Survived'].sort_values(ascending=False)[-10:])","d2258a8e":"#correlation matrix\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corr,  annot=True,annot_kws={'size': 15})","5cbad7d2":"s = corr.unstack()\ns","6b141ccd":"grid = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","34762418":"# Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.\n\nguess_ages = np.zeros((2,3))\nguess_ages","06347e4f":"# Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n           \n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n","db980ea1":"\ntrain_data.head()","f93f9ba0":"train_data.isnull().sum()","fdc02807":"# Let us create Age group and determine correlations with Survived.\n\ntrain_data['AgeGroup'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean().sort_values(by='AgeGroup', ascending=True)","79adbe81":"# Let us replace Age with ordinals based on these groups.\n\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\n","11cc1590":"train_data.head()","01bc2aab":"# Now removing agegroup as its not required after converting age into different groups\ntrain_data = train_data.drop(['AgeGroup'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","dd181451":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5d8ad80c":"# We can create another feature called IsAlone.\n\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","e725b0e4":"# Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.\n\ntrain_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_data, test_data]\n\n","6f7b6a19":"train_data.head()","710e24e7":"# We can also create an artificial feature combining Pclass and Age.\n\nfor dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","00bf951b":"# Completing a categorical feature\n# Embarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.\n\nfreq_port = train_data.Embarked.dropna().mode()[0]\nfreq_port","9f5934e4":" for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d04cb88f":"train_data.isnull().sum()\n# all missing value filled","fb13e1a4":"# Converting categorical feature to numeric\n# We can now convert the EmbarkedFill feature by creating a new numeric Port feature.\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_data.head()","b0335679":"test_data.isnull().sum()","9cea2e74":"# Filling missing vale in fare by median\ntest_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntest_data.head()","5dc717dd":"train_data['FareGroup'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean().sort_values(by='FareGroup', ascending=True)","23b1783a":"# Convert the Fare feature to ordinal values based on the FareGroup.\n\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareGroup'], axis=1)\ncombine = [train_data, test_data]\n    \n","04715b72":"train_data.head(10)","c4f6460e":"test_data.head(10)","8fb86389":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","4f3b324f":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","13748395":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","537e7427":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","6a68e210":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","5fe1e828":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","bbf15322":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","f528acde":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'KNN', \n              'Naive Bayes', 'Decision Tree','Random Forest'], \n             \n    'Score': [acc_log,acc_svc, acc_knn,  acc_gaussian, \n              acc_decision_tree,acc_random_forest]})\nmodels.sort_values(by='Score', ascending=False)","64163cd5":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n","436e9590":" submission.to_csv('\/kaggle\/working\/submission.csv', index=False)  ","425d1427":"# Correlating Categorical Feature","4096f223":"# Logistic Regression with Python in Machine Learning - Titanic Dataset\nLogistic Regression is a Classification Algorithym used to assign observations to a discrete set of classes. In linear we having continuous observations but in logistic regresssion we have 0 and 1 and True \/ False. logistic regression transforms it output using the logistic sigmid function to return a probability value. if probability is greater than 0.5 will predict it towards 1. Ex - rain forecasting, fraud detection, cancer detection, spam mail or not.\n\n# TITANIC DATA INFORMATION -\nThe RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died.\n\n# Lets go ahead and build a model which can predict if a passenser is gonna survive","800b8e59":"# Random Forest","1506d0a8":"# Support Vector Machines\n","4e6ccfce":"# Explanation of above histogram \n- Survival number is much higher in case of PClass 1.\n- most unsurvived passenger belongs to Pclass 3.\n- mostly infant passenger belongs to Pclass 1, 2 and survived \n- major number of passenger between age group 15-30 boarded in PClass 3 and non survived too.","3bba0f1f":"# Algorithym application \/ Model Building","d0c19e4b":"# Filling Missing value","0570c0e9":"# Logistic Regression","0d6ece48":"# Removing some feature\nNow after understanding data by making differnt assumptions checking through visualization and correlation we got to know which feature is non relevant with survival and can remove them.\n\n","28478d90":"Now all data cleaning, wrangling part done. our data is ready to build model. as in this data we are trying to find out correlation of other feature with survived. and also training our model with training dataset. so its classification and regression problem.\nSo I will use some model app;ication on this dataset for checking which one is working better.","de23a560":"# Gaussian Naive Bayes","e5b82c15":"# can see from above pivot based on Pcassenger Class first class passenger survived around 62%.","f8e4ba54":"# Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem.","4f1313fa":"# Explanation of age histogram based on survival\n- Survived = 1 and nonsurvived = 0\n- age of 80 survived.\n- non survival passenger are mostly from age group 15-30.\n- maximum number of passenger who boarded from age group 15-35 age range.\n    ","7c20462c":"# Correlating categorical and numerical features\n# Bar plot","40051ca4":"# Understanding of describe from categorical feature\n\n- Names are unique acroos the dataset (count = 891)\n- Sex variable as two possible values with 65% male(top = male, freq = 577 \/ count = 891)\n- Cabin values have several duplicate across samples. freq = 4\n- Embarked take 3 possible values. S port used by most passenger (top = s)\n-Ticket feature has high ratio(22 %) of duplicate values (unique = 681 , count = 891)","dcc4df2f":"# Assumptions based on Data Analysis\n### now i would like to check coorelation of all features with survival to check is there any linear correlation or not. \n### is there any multicollineaity between predictors.\n### also fill missing value in some features.\n### do required encoding for categorical feature.\n### if needed do feature scaling too\n### as ticket feature contains high ratio of duplicates(22%) may be can drop this.\n### cabin feature may be dropped as it having many null values both in training and test dataset.\n### PassengerId not useful for giving any information so drop this.\n### Name feature also not directly corelated to survival, so can drop this too.\n### can create some new feature based on class, Fare Range, family which include detail of person and sibling . children.\n### can crete new feature as \"Title\" and adjust all name based on the title.\n### classify category based on female as we know already female survive mostly.\n### need to find out range of children age who survived by classifying them.\n### we knew also upper class passenger survived mostly, also high fair paid passenger survived mostly.\n","b47b09b6":"### Create new feature combining existing features(Parch & SibSp)\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","3e0058f0":"# Correlating numerical and ordinal features\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values","811e9439":"# KNN k-Nearest Neighbors algorithm ","2ef9444f":"# Creating Some new feature ","d3acbed5":"# Assumptions based on above graph between survival and Embarked\n- Cherbourg passenger paid higher fare and survived to. Female have higher survival than male.\n- Queenstown passenger have equal proprotion of survival and non survival.\n- Port of embarkation correlates with survival rates. and its shows correlation with survival****","556d1758":"### can see ticket and cabin removed from data","1292f853":"# # Understanding of describe from categorical feature\n-Names are unique across the dataset (count=unique=418)\n\n-Sex variable as two possible values with 63% male (top=male, freq=266\/count=418).\n\n-Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n\n-Embarked takes three possible values. S port used by most passengers (top=S)\n\n-Ticket feature has high ratio (13%) of duplicate values (unique=363).","38161dfe":"# Decision Tree","54b017bf":"#  Information Gained from describe -\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and\/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80.","1f68bc29":"# Visualizaton of Data","572a024f":"# Observations from above correlation\n- In this graph checked passenger board place (C - Cherbourg, S - Southampton, Q = Queenstown) correlation with survival\n- passenger who boarded from Cherbourg and part of passenger class 1have more male survival rate.\n- from Southampton more female survived with passenger class 1,2.\n- in all board place female have higher survival rate.","be1820ff":"# Explanation of all features\n\n### PassengerId - Unique ID of the passenger\n\n### Survived - Survived (1) or died (0)\n\n### Pclass - Passenger's class (1st, 2nd, or 3rd)\n\n### Name- Passenger's name\n\n### Sex- Passenger's sex\n\n### Age - Passenger's age\n\n### SibSp - Number of siblings\/spouses aboard the Titanic\n\n### Parch - Number of parents\/children aboard the Titanic\n\n### Ticket- Ticket number\n\n### Fare - Fare paid for ticket\n\n### Cabin - Cabin number\n\n### Embarked - Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown)"}}