{"cell_type":{"1e5b7d7d":"code","0a9585b1":"code","9d221bd0":"code","753cdb17":"code","21fb8ed8":"code","5ff9baee":"code","5cd45e0a":"code","3f397a9a":"code","9aa03b61":"code","c35a03c9":"code","e52c7176":"code","bcc67f62":"code","ca5e2707":"code","294113dd":"code","67e1584c":"code","566590b6":"code","c07b3511":"code","9a09cb9b":"code","4a44f933":"code","37ea98dd":"code","822bf45f":"code","a63e5016":"code","671d72a3":"code","d4f0990e":"code","aa66e480":"code","1a4c071c":"code","c396c2a8":"code","dfab6309":"code","67472303":"code","14e7b709":"code","412d8e5b":"code","4192f442":"code","d15ebe07":"code","96d7251e":"code","c464c5d6":"markdown","fe73262e":"markdown","b1ff1a08":"markdown","8972655f":"markdown","e296dd2e":"markdown","0318bc98":"markdown","a01a3e02":"markdown","0afbeaeb":"markdown","60d3cdbf":"markdown","110be7b2":"markdown","bb616428":"markdown"},"source":{"1e5b7d7d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","0a9585b1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d221bd0":"# read the dataset\nretail_df = pd.read_csv(r\"\/kaggle\/input\/learning-data\/OnlineRetail.csv\", sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail_df.head()","753cdb17":"# basics of the df\nretail_df.info()","21fb8ed8":"# missing values\nround(100*(retail_df.isnull().sum())\/len(retail_df), 2)","5ff9baee":"# drop all rows having missing values\nretail_df = retail_df.dropna()\nretail_df.shape","5cd45e0a":"retail_df.head()","3f397a9a":"# new column: amount \nretail_df['amount'] = retail_df['Quantity']*retail_df['UnitPrice']\nretail_df.head()","9aa03b61":"# monetary\ngrouped_df = retail_df.groupby('CustomerID')['amount'].sum()\ngrouped_df = grouped_df.reset_index()\ngrouped_df.head()","c35a03c9":"# frequency\nfrequency = retail_df.groupby('CustomerID')['InvoiceNo'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['CustomerID', 'frequency']\nfrequency.head()","e52c7176":"# merge the two dfs\ngrouped_df = pd.merge(grouped_df, frequency, on='CustomerID', how='inner')\ngrouped_df.head()","bcc67f62":"retail_df.head()","ca5e2707":"# recency\n# convert to datetime\nretail_df['InvoiceDate'] = pd.to_datetime(retail_df['InvoiceDate'], \n                                          format='%d-%m-%Y %H:%M')","294113dd":"retail_df.head()","67e1584c":"# compute the max date\nmax_date = max(retail_df['InvoiceDate'])\nmax_date","566590b6":"# compute the diff\nretail_df['diff'] = max_date - retail_df['InvoiceDate']\nretail_df.head()","c07b3511":"# recency\nlast_purchase = retail_df.groupby('CustomerID')['diff'].min()\nlast_purchase = last_purchase.reset_index()\nlast_purchase.head()","9a09cb9b":"# merge\ngrouped_df = pd.merge(grouped_df, last_purchase, on='CustomerID', how='inner')\ngrouped_df.columns = ['CustomerID', 'amount', 'frequency', 'recency']\ngrouped_df.head()","4a44f933":"# number of days only\ngrouped_df['recency'] = grouped_df['recency'].dt.days\ngrouped_df.head()","37ea98dd":"# 1. outlier treatment\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(y=grouped_df['recency'])\nplt.subplot(1,2,2)\nsns.boxplot(y=grouped_df['recency'])\nplt.show()","822bf45f":"# two types of outliers:\n# - statistical\n# - domain specific","a63e5016":"# removing (statistical) outliers\nQ1 = grouped_df.amount.quantile(0.05)\nQ3 = grouped_df.amount.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.amount >= Q1 - 1.5*IQR) & (grouped_df.amount <= Q3 + 1.5*IQR)]\n\n# outlier treatment for recency\nQ1 = grouped_df.recency.quantile(0.05)\nQ3 = grouped_df.recency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.recency >= Q1 - 1.5*IQR) & (grouped_df.recency <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = grouped_df.frequency.quantile(0.05)\nQ3 = grouped_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.frequency >= Q1 - 1.5*IQR) & (grouped_df.frequency <= Q3 + 1.5*IQR)]\n\n","671d72a3":"# 2. rescaling\nrfm_df = grouped_df[['amount', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","d4f0990e":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['amount', 'frequency', 'recency']\nrfm_df_scaled.head()","aa66e480":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","1a4c071c":"kmeans.labels_","c396c2a8":"# help(KMeans)","dfab6309":"# elbow-curve\/SSD\nfig=plt.figure(figsize=(20,8))\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd);","67472303":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n    \n    ","14e7b709":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\nimport pandas as pd\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","412d8e5b":"#Use the Hopkins Statistic function by passing the above dataframe as a paramter\nhopkins(rfm_df_scaled)","4192f442":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","d15ebe07":"kmeans.labels_","96d7251e":"# assign the label\ngrouped_df['cluster_id'] = kmeans.labels_\ngrouped_df.head()","c464c5d6":"## K-Means Clustering","fe73262e":"# 2. Clean the data","b1ff1a08":"# 1. Read and visualise the data","8972655f":"**Overview**<br>\n<a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail\">Online retail is a transnational data set<\/a> which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nThe steps are broadly:\n1. Read and understand the data\n2. Clean the data\n3. Prepare the data for modelling\n4. Modelling\n5. Final analysis and record observation ","e296dd2e":"# Hopkins statistic","0318bc98":"# 3. Prepare the data for modelling","a01a3e02":"The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set. It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. A value close to 1 tends to indicate the data is highly clustered, random data will tend to result in values around 0.5, and uniformly distributed data will tend to result in values close to 0","0afbeaeb":"- R (Recency): Number of days since last purchase\n- F (Frequency): Number of tracsactions\n- M (Monetary): Total amount of transactions (revenue contributed)","60d3cdbf":"## Finding the Optimal Number of Clusters\n\n### SSD","110be7b2":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","bb616428":"# 4. Modelling"}}