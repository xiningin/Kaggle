{"cell_type":{"3b64c8c9":"code","2b3add94":"code","1e29c169":"code","02b99853":"code","c4ea792f":"code","bf183851":"code","d43ff7b4":"code","eff2292f":"code","a8de038e":"code","cd4cd015":"code","f47e7897":"code","f983eb51":"code","01578838":"code","34e55d1e":"code","0e179c14":"code","fe8a6c3f":"code","19fa9cd5":"code","5db77ce5":"code","7aeed850":"code","f0c3725d":"code","058ba23d":"code","d172d3f4":"code","2685f1ea":"code","b7bf2fa6":"code","dda4a732":"code","0f7c7102":"code","a8f6ffe8":"code","7f6d012e":"code","36d4736c":"code","3ae5928c":"code","3d8d7366":"code","8abae02f":"code","5034a0a4":"code","75f041ab":"code","55b437cf":"code","22057497":"code","33fd2628":"code","50f313d8":"code","6234632e":"code","72b52d47":"code","55186cfb":"code","2d0cc2fc":"code","facbd11e":"code","b4a8f0c4":"code","748c1ab2":"code","a8a9777d":"code","ae755fcc":"code","38e3c661":"code","cf08679d":"code","b1f738dc":"code","ab72b361":"code","6d1b027a":"code","c80956fc":"code","7b1f860d":"code","1f3a765d":"code","c9faaf9c":"code","a3d967a5":"code","3a265c8c":"code","c09420d1":"code","19cbd6d7":"code","a4e7a3d3":"code","b586f695":"code","3bb14d4e":"code","0e79e077":"code","6d6a5b11":"code","c1a2cd98":"code","cc2620ce":"code","4549035d":"code","abf9a2a9":"code","bafcb718":"code","25962787":"code","6b0e89ee":"code","6d5745e9":"code","ab073b19":"code","2e65ac64":"code","7e3b2952":"code","486695c7":"code","b0b67c22":"code","a3f92789":"markdown","ad942eb5":"markdown","8a2895a3":"markdown","ab905a20":"markdown","29193162":"markdown","dbeb0f6c":"markdown","30027553":"markdown","38939d3e":"markdown","a4084341":"markdown","13b6c34e":"markdown","fbca1b7c":"markdown","b2635de0":"markdown","97e8e949":"markdown","23a5b802":"markdown","326dd0bd":"markdown","71700422":"markdown","22aff9b7":"markdown","43222d73":"markdown","95aaa338":"markdown","eb9976b6":"markdown","bd86bacb":"markdown","20c30b17":"markdown","917f9227":"markdown","39a7b896":"markdown","3e748a11":"markdown","f0816388":"markdown","ee2453d1":"markdown"},"source":{"3b64c8c9":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torch import autograd\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline","2b3add94":"class FashionMNIST(Dataset):\n    def __init__(self, transform=None):\n        self.transform = transform\n        fashion_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n        self.labels = fashion_df.label.values\n        self.images = fashion_df.iloc[:,1:].values.astype('uint8').reshape(-1,28,28)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        img = Image.fromarray(self.images[idx])\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        return img, label","1e29c169":"dataset = FashionMNIST()","02b99853":"# __getitem__(idx:0) -> return (img, label)\ndataset[0]","c4ea792f":"# show img  == Image.fromarray(self.images[idx])\ndataset[0][0]","bf183851":"!ls ..","d43ff7b4":"fashion_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')","eff2292f":"fashion_df","a8de038e":"fashion_df.iloc[:,1:].values.astype('uint8').shape","cd4cd015":"fashion_df.iloc[:,1:].values.astype('uint8').reshape(-1,28,28).shape","f47e7897":"images = fashion_df.iloc[:,1:].values.astype('uint8').reshape(-1,28,28)","f983eb51":"images[1].shape","01578838":"Image.fromarray(images[1])","34e55d1e":"img = Image.fromarray(images[1])","0e179c14":"img","fe8a6c3f":"transform = transforms.Compose([ # torchvision.transforms\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5), std=(0.5))\n])\ndataset = FashionMNIST(transform=transform)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)","19fa9cd5":"count = 0\nfor i,j in iter(data_loader):\n    print(i.shape, j.shape)\n    count += 1\n    if count == 5:\n        break","5db77ce5":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.label_emb = nn.Embedding(10,10)\n        \n        self.model = nn.Sequential(\n            nn.Linear(794, 1024), # x = torch.cat([x,c], axis=1) ## x.shape == (64,784), c.shape == (64,10), torch.cat([x,c],axis=1).shape == (64,794)\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            \n            nn.Linear(1024, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            \n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            \n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x, labels):\n        c = self.label_emb(labels)\n        x = x.view(x.size(0), 784)\n        x = torch.cat([x,c], axis=1)\n        output = self.model(x)\n        return output.squeeze()","7aeed850":"count = 0\nprint(next(iter(data_loader))[0].shape)\n","f0c3725d":"sample_imgs = next(iter(data_loader))[0]","058ba23d":"# number of images\nsample_imgs.size(0)","d172d3f4":"print(next(iter(data_loader))[0].shape) # [n, c, h, w]\nprint(next(iter(data_loader))[1].shape) # [n]","2685f1ea":"sample_imgs = next(iter(data_loader))[0]\nsample_labels = next(iter(data_loader))[1]","b7bf2fa6":"sample_labels","dda4a732":"nn.Embedding(10,10)(sample_labels).shape","0f7c7102":"nn.Embedding(10,10)(sample_labels)[0]","a8f6ffe8":"sample_imgs.view(64, 784).shape\nsample_x = sample_imgs.view(64, 784)\nsample_c = nn.Embedding(10,10)(sample_labels)","7f6d012e":"sample_x.shape","36d4736c":"sample_c.shape","3ae5928c":"sample_output = torch.cat([sample_x,sample_c], axis=1)\nsample_output.shape","3d8d7366":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.label_emb = nn.Embedding(10,10)\n        \n        self.model = nn.Sequential(\n            nn.Linear(110, 256), # x = torch.cat([z,c], axis=1) ## z.shape == (64,100), c.shape == (64,10), torch.cat([z,c], axis=1).shape == (64,110)\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Linear(1024, 784),\n            nn.Tanh()\n        )\n        \n    def forward(self, z, labels):\n        z = z.view(z.size(0), 100)\n        c = self.label_emb(labels)\n        x = torch.cat([z,c], axis=1)\n        output = self.model(x)\n        return output.view(x.size(0), 28, 28)","8abae02f":"generator = Generator().cuda()\ndiscriminator = Discriminator().cuda()","5034a0a4":"criterion = nn.BCELoss()","75f041ab":"d_optimizer = torch.optim.Adam(\n    discriminator.parameters(),\n    lr = 1e-4\n)\ng_optimizer = torch.optim.Adam(\n    generator.parameters(),\n    lr = 1e-4\n)","55b437cf":"print('1e-1 == ', 1e-1)\nprint('1e-2 == ', 1e-2)\nprint('1e-3 == ', 1e-3)\nprint('1e-4 == ', 1e-4)","22057497":"discriminator","33fd2628":"print(discriminator.parameters())\nprint(discriminator.parameters)","50f313d8":"generator","6234632e":"print(generator.parameters())\nprint(generator.parameters)","72b52d47":"d_optimizer","55186cfb":"g_optimizer","2d0cc2fc":"def train_generator(batch_size, discriminator, generator, g_optimizer, criterion):\n    g_optimizer.zero_grad()\n    z = Variable(torch.randn(batch_size,100)).cuda() # tensor to GPU\n    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 10, batch_size))).cuda() # tensor to GPU\n    fake_images = generator(z, fake_labels)\n    validity = discriminator(fake_images, fake_labels) # valid that is image(generated image by generator) is real or fake\n    g_loss = criterion(validity, Variable(torch.ones(batch_size)).cuda()) # get loss\n    g_loss.backward() # backward pass, back-propagation\n    g_optimizer.step() # gradient descent, adjust model parameters\n    return g_loss.data\n    ","facbd11e":"test_batch_size = 32","b4a8f0c4":"torch.randn(test_batch_size, 100)","748c1ab2":"torch.randn(test_batch_size, 100).shape","a8a9777d":"Variable(torch.randn(test_batch_size,100)) # same result. it is a tensor-wrapper for calculating gradient.","ae755fcc":"Variable(torch.randn(test_batch_size,100)).shape","38e3c661":"np.random.randint(0, 10, test_batch_size)","cf08679d":"np.random.randint(0, 10, test_batch_size).shape","b1f738dc":"torch.LongTensor(np.random.randint(0,10,test_batch_size))","ab72b361":"torch.LongTensor(np.random.randint(0,10,test_batch_size)).shape","6d1b027a":"z = Variable(torch.randn(test_batch_size,100)).cuda()\nfake_labels = Variable(torch.LongTensor(np.random.randint(0,10,test_batch_size))).cuda()\ngenerator(z, fake_labels).shape # [N, H, W]","c80956fc":"generator(z, fake_labels).unsqueeze(1).shape # [N, C, H, W]","7b1f860d":"generator(z, fake_labels).unsqueeze(1).permute(0,2,3,1).shape # [N, H, W, C]","1f3a765d":"generator(z, fake_labels).unsqueeze(1).permute(0,2,3,1)[0].shape","c9faaf9c":"plt.imshow(generator(z, fake_labels).unsqueeze(1).permute(0,2,3,1)[0].cpu().detach().numpy(), cmap='gray');","a3d967a5":"z = Variable(torch.randn(test_batch_size, 100), requires_grad=True).cuda()\nfake_labels = Variable(torch.LongTensor(np.random.randint(0,10,test_batch_size))).cuda()\nfake_images = generator(z, fake_labels)\ntest_validity = discriminator(fake_images, fake_labels)","3a265c8c":"z.shape","c09420d1":"z","19cbd6d7":"z.retain_grad() # there is no grad","a4e7a3d3":"test_validity","b586f695":"test_validity.shape","3bb14d4e":"torch.ones(test_batch_size)","0e79e077":"torch.ones(test_batch_size).shape","6d6a5b11":"test_g_loss = criterion(test_validity, Variable(torch.ones(test_batch_size)).cuda())\ntest_g_loss","c1a2cd98":"test_g_loss.backward()","cc2620ce":"test_g_loss.data","4549035d":"g_optimizer","abf9a2a9":"g_optimizer.step()","bafcb718":"print(z.grad)","25962787":"z.grad.shape","6b0e89ee":"def train_discriminator(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n    d_optimizer.zero_grad()\n    \n    # train with real images\n    real_validity = discriminator(real_images, labels)\n    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).cuda())\n    \n    # train with fake images\n    z = Variable(torch.randn(batch_size, 100)).cuda()\n    fake_labels = Variable(torch.LongTensor(np.random.randint(0,10,batch_size))).cuda()\n    fake_images = generator(z, fake_labels)\n    fake_validity = discriminator(fake_images, fake_labels)\n    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).cuda())\n    \n    d_loss = real_loss + fake_loss\n    d_loss.backward()\n    d_optimizer.step()\n    return d_loss.data # get value(scalar)","6d5745e9":"from tqdm import tqdm\n\nnum_epochs = 10\nn_critic = 5\ndisplay_step = 300\nfor epoch in range(num_epochs):\n    print('epoch : {}'.format(epoch))\n    for _, (images, labels) in enumerate(tqdm(data_loader)):\n        real_images = Variable(images).cuda()\n        labels = Variable(labels).cuda()\n        generator.train()\n        batch_size = real_images.size(0)\n        d_loss = train_discriminator(len(real_images), discriminator, generator, d_optimizer, criterion, real_images, labels)\n        g_loss = train_generator(batch_size, discriminator, generator, g_optimizer, criterion)\n        \n    generator.eval()\n    print('g_loss : {}, d_loss : {}'.format(g_loss, d_loss))\n    \n    z = Variable(torch.randn(9,100)).cuda()\n    labels = Variable(torch.LongTensor(np.arange(9))).cuda()\n    \n    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n    plt.imshow(grid)\n    plt.show()","ab073b19":"z = Variable(torch.randn(100,100)).cuda()\nlabels = Variable(torch.LongTensor([i for i in range(10) for j in range(10)])).cuda()\nsample_images = generator(z, labels).unsqueeze(1).data.cpu()\n\ngrid = make_grid(sample_images, nrow=10, normalize=True).permute(1,2,0).numpy()\nfig, ax = plt.subplots(figsize=(15,15))\nax.imshow(grid)\nplt.xticks([])\nplt.yticks(np.arange(15,300,30),['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'], fontsize=20)\nplt.show();","2e65ac64":"[i for i in range(10) for j in range(10)][:20]","7e3b2952":"torch.LongTensor([i for i in range(10) for j in range(10)])","486695c7":"torch.LongTensor([i for i in range(10) for j in range(10)]).shape","b0b67c22":"np.arange(15,300,30) # text position of y-axis ","a3f92789":"### 3-a. Loss Function","ad942eb5":"reference :\n[PyTorch: Variables and autograd](http:\/\/seba1511.net\/tutorials\/beginner\/examples_autograd\/two_layer_net_autograd.html)\n- A PyTorch Variable is a wrapper around a PyTorch Tensor, and represents a node in a computational graph. \n- If x is a Variable then x.data is a Tensor giving its value, and x.grad is another Variable holding the gradients of x with respect to some scalar value.\n\n[PyTorch 0.4.0 Migration Guide](https:\/\/pytorch.org\/blog\/pytorch-0_4_0-migration-guide\/)\n- `torch.Tensor` and `torch.autograd.Variable` are now the same class. \n- More precisely, `torch.Tensor` is capable of tracking history and behaves like the old `Variable`; `Variable` wrapping continues to work as before but returns an object of type `torch.Tensor`. This means that you don't need the `Variable` wrapper everywhere in your code anymore\n","8a2895a3":"> Index\n```\nStep 1. Import Libraries and Load Dataset\n     1-a. basic way to load dataset\n     1-b. more efficient way to load dataset(with transform) + batch\nStep 2. Define Discriminator and Generator\n     2-a. discriminator\n     2-b. generator\nStep 3. Define Loss Function and Optimizing Function\n     3-a. loss function\n     3-b. optimizing function\nStep 4. Define Training Functions\n     4-a. training gerenator (get 1 loss and 1 gradient descent)\n     4-b. training discriminator (get 1 loss and 1 gradient descent)\nStep 5. Train Model\nStpe 6. Test Model\n```","ab905a20":"\n> TEST CODE | LINE BY LINE\n","29193162":"Since the first differentiation has not yet been performed(before training), the generated fake image is poor enough to be 100% distingushable from the real image(fashion mnist).","dbeb0f6c":"### 1-a. Basic way to load dataset","30027553":"> reference :\n\n1. [Data Loading and Processing Tutorial on PyTorch's documentation](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n\n- `torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset`and override the following methods:\n    - `__len__` so that `len(dataset)` returns the size of the dataset.\n    - `__getitem__` to support the indexing such that `dataset[i]` can be used to get i-th sample.\n    \n2. [torchvision](https:\/\/github.com\/pytorch\/vision)\n\n- the `torchvision` package consists of popular datasets, model architectures, and common image transformations for computer vision.\n","38939d3e":"## Step 4. Define Training Function","a4084341":"## Step 6. Test Model","13b6c34e":"### 2-a. Discriminator","fbca1b7c":"### 4-a. training gerenator (get 1 loss and 1 gradient descent)","b2635de0":"This kernel is PyTorch GAN Tutorial that based on [PyTorch Conditional GAN by Artur Machado Lacerda](https:\/\/www.kaggle.com\/arturlacerda\/pytorch-conditional-gan). \n\nFor Tutorial, Added step-by-step configuration and comments and test code lines to the base code to make learning easier. And Fixed some code according to PyTorch version update.\n\nThank you for nice reference!","97e8e949":"> TEST CODE | LINE BY LINE","23a5b802":"> TEST CODE | LINE BY LINE","326dd0bd":"> TEST CODE | LINE BY LINE\n\n","71700422":"Thanks for studying this kernel with me so far, and I hope you continue to enjoy Pytorch!","22aff9b7":"## Step 5. Train Model","43222d73":"### 2-b. Generator","95aaa338":"### 3-b. Optimizing Function","eb9976b6":"## Step 3. Define Loss Function and Optimizing Function","bd86bacb":"## Step 2. Define Discriminator and Generator","20c30b17":"# Generate Fasion Images with Conditional GAN","917f9227":"### 1-b. More efficient way to Load dataset(with transform) + batch","39a7b896":"## Step 1. Import Libraries and Load Dataset","3e748a11":"> TEST CODE | LINE BY LINE","f0816388":"> TEST CODE | LINE BY LINE","ee2453d1":"### 4-b. training discriminator (get 1 loss and 1 gradient descent)"}}