{"cell_type":{"22adda8b":"code","68f7614a":"code","7144d138":"code","6d26190f":"code","c3e29fa7":"code","4630a53c":"code","0207f1d3":"code","1a6e2774":"code","f9783b91":"code","74934fa8":"code","65ac6b09":"code","db077036":"code","8f3ea6f6":"code","d339f8a7":"markdown"},"source":{"22adda8b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import Parallel, delayed\nimport lightgbm as lgb\nfrom scipy import stats\n\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntqdm.pandas()","68f7614a":"def split_and_label(rows_labels):\n    \n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((80))\n        \n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        \n        row_labels_list.append(labels_array)\n    \n    return row_labels_list","7144d138":"train_curated = pd.read_csv('..\/input\/train_curated.csv')\ntrain_noisy = pd.read_csv('..\/input\/train_noisy.csv')\ntrain_noisy = train_noisy[['fname','labels']]\ntest = pd.read_csv('..\/input\/sample_submission.csv')\nprint(train_curated.shape, train_noisy.shape, test.shape)","6d26190f":"label_columns = list( test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\nlabel_mapping","c3e29fa7":"train_curated_labels = split_and_label(train_curated['labels'])\ntrain_noisy_labels   = split_and_label(train_noisy  ['labels'])\nlen(train_curated_labels), len(train_noisy_labels)","4630a53c":"for f in label_columns:\n    train_curated[f] = 0.0\n    train_noisy[f] = 0.0\n\ntrain_curated[label_columns] = train_curated_labels\ntrain_noisy[label_columns]   = train_noisy_labels\n\ntrain_curated['num_labels'] = train_curated[label_columns].sum(axis=1)\ntrain_noisy['num_labels']   = train_noisy[label_columns].sum(axis=1)\n\ntrain_curated['path'] = '..\/input\/train_curated\/'+train_curated['fname']\ntrain_noisy  ['path'] = '..\/input\/train_noisy\/'+train_noisy['fname']\n\ntrain_curated.head()","0207f1d3":"train = pd.concat([train_curated, train_noisy],axis=0)\n\ndel train_curated, train_noisy\ngc.collect()\n\ntrain.shape","1a6e2774":"def create_features( pathname ):\n\n    var, sr = librosa.load( pathname, sr=44100)\n    # trim silence\n    if 0 < len(var): # workaround: 0 length causes error\n        var, _ = librosa.effects.trim(var)\n    xc = pd.Series(var)\n    \n    X = []\n    X.append( xc.mean() )\n    X.append( xc.median() )\n    X.append( xc.std() )\n    X.append( xc.max() )\n    X.append( xc.min() )\n    X.append( xc.skew() )\n    X.append( xc.mad() )\n    X.append( xc.kurtosis() )\n    \n    X.append( np.mean(np.diff(xc)) )\n    X.append( np.mean(np.nonzero((np.diff(xc) \/ xc[:-1]))[0]) )\n    X.append( np.abs(xc).max() )\n    X.append( np.abs(xc).min() )\n    \n    X.append( xc[:4410].std() )\n    X.append( xc[-4410:].std() )\n    X.append( xc[:44100].std() )\n    X.append( xc[-44100:].std() )\n    \n    X.append( xc[:4410].mean() )\n    X.append( xc[-4410:].mean() )\n    X.append( xc[:44100].mean() )\n    X.append( xc[-44100:].mean() )\n    \n    X.append( xc[:4410].min() )\n    X.append( xc[-4410:].min() )\n    X.append( xc[:44100].min() )\n    X.append( xc[-44100:].min() )\n    \n    X.append( xc[:4410].max() )\n    X.append( xc[-4410:].max() )\n    X.append( xc[:44100].max() )\n    X.append( xc[-44100:].max() )\n    \n    X.append( xc[:4410].skew() )\n    X.append( xc[-4410:].skew() )\n    X.append( xc[:44100].skew() )\n    X.append( xc[-44100:].skew() )\n    \n    X.append( xc.max() \/ np.abs(xc.min()) )\n    X.append( xc.max() - np.abs(xc.min()) )\n    X.append( xc.sum() )\n    \n    X.append( np.mean(np.nonzero((np.diff(xc[:4410]) \/ xc[:4410][:-1]))[0]) )\n    X.append( np.mean(np.nonzero((np.diff(xc[-4410:]) \/ xc[-4410:][:-1]))[0]) )\n    X.append( np.mean(np.nonzero((np.diff(xc[:44100]) \/ xc[:44100][:-1]))[0]) )\n    X.append( np.mean(np.nonzero((np.diff(xc[-44100:]) \/ xc[-44100:][:-1]))[0]) )\n    \n    X.append( np.quantile(xc, 0.95) )\n    X.append( np.quantile(xc, 0.99) )\n    X.append( np.quantile(xc, 0.10) )\n    X.append( np.quantile(xc, 0.05) )\n    \n    X.append( np.abs(xc).mean() )\n    X.append( np.abs(xc).std() )\n             \n    return np.array( X )","f9783b91":"\nX = Parallel(n_jobs= 4)(delayed(create_features)(fn) for fn in tqdm(train['path'].values) )\nX = np.array( X )\nX.shape","74934fa8":"Xtest = Parallel(n_jobs= 4)(delayed(create_features)( '..\/input\/test\/'+fn) for fn in tqdm(test['fname'].values) )\nXtest = np.array( Xtest )\nXtest.shape","65ac6b09":"\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=69)\n\nparams = {'num_leaves': 15,\n         'min_data_in_leaf': 200, \n         'objective':'binary',\n         \"metric\": 'auc',\n         'max_depth': -1,\n         'learning_rate': 0.05,\n         \"boosting\": \"gbdt\",\n         \"bagging_fraction\": 0.85,\n         \"bagging_freq\": 1,\n         \"feature_fraction\": 0.20,\n         \"bagging_seed\": 42,\n         \"verbosity\": -1,\n         \"nthread\": -1,\n         \"random_state\": 69}\n\nPREDTRAIN = np.zeros( (X.shape[0],80) )\nPREDTEST  = np.zeros( (Xtest.shape[0],80) )\nfor f in range(len(label_columns)):\n    y = train[ label_columns[f] ].values\n    oof      = np.zeros( X.shape[0] )\n    oof_test = np.zeros( Xtest.shape[0] )\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n        model = lgb.LGBMClassifier(**params, n_estimators = 20000)\n        model.fit(X[trn_idx,:], \n                  y[trn_idx], \n                  eval_set=[(X[val_idx,:], y[val_idx])], \n                  eval_metric='auc',\n                  verbose=0, \n                  early_stopping_rounds=25)\n        oof[val_idx] = model.predict_proba(X[val_idx,:], num_iteration=model.best_iteration_)[:,1]\n        oof_test += model.predict_proba(Xtest          , num_iteration=model.best_iteration_)[:,1]\/5.0\n\n    PREDTRAIN[:,f] = oof    \n    PREDTEST [:,f] = oof_test\n    \n    print( f, str(roc_auc_score( y, oof ))[:6], label_columns[f] )\n","db077036":"from sklearn.metrics import roc_auc_score\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\nprint( 'lwlrap cv:', calculate_overall_lwlrap_sklearn( train[label_columns].values, PREDTRAIN ) )","8f3ea6f6":"test[label_columns] = PREDTEST\ntest.to_csv('submission.csv', index=False)\ntest.head()","d339f8a7":">v0.1 This code implements a simple feature extraction and train using Lightgbm.\n\nFeature extraction is very simple and can be improved."}}