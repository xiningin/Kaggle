{"cell_type":{"ad1034a2":"code","61e9824f":"code","dacf0815":"code","4f36a764":"code","41631b13":"code","9246c084":"code","aa36eb11":"code","32a50f4e":"code","61a11fb1":"code","679b44a9":"code","11213919":"code","08e1cb49":"code","a01b612b":"code","af22e0bc":"code","b498b13e":"code","3b32c12a":"code","d5ced282":"code","0e03cf78":"code","52dc9328":"code","25076d8c":"code","b3626669":"code","400aade5":"code","b273494b":"code","ac63d84b":"code","57107f14":"code","3036fa5f":"code","4294d3e8":"code","833c44b8":"code","5e7b3be4":"code","42903535":"code","b4029ba4":"code","d80a92a1":"code","f5b7a0a4":"code","3ba6385c":"code","0911c619":"code","31eae5d9":"code","b2eed347":"code","a6b23135":"code","ea1740f7":"code","9e726344":"code","aeb5d023":"code","02b51d81":"code","f5a74275":"code","787102b1":"code","33748557":"code","afbf85b3":"code","34bd9787":"code","5ec42d5c":"code","13afd7d7":"code","c565e3eb":"code","dfad13ab":"code","15dabcff":"code","a0b49d20":"code","80380db8":"code","fc65bf85":"code","8682040c":"code","a7643772":"code","d0757d2a":"code","d089537f":"markdown","760425a6":"markdown","0be3b57f":"markdown","e0986428":"markdown","f6fed7db":"markdown","46969272":"markdown","4ac9c31e":"markdown","9aa6f692":"markdown","95e1cdb8":"markdown","ad7511b2":"markdown","3d3c4b22":"markdown","7d4666ae":"markdown","228c55a7":"markdown","75e7314a":"markdown","b46c4ab5":"markdown","bab8e086":"markdown","6d6be89d":"markdown","27249a97":"markdown","3d43e2bb":"markdown","5c795c7b":"markdown","2dbe9f4c":"markdown","b9442490":"markdown","eeeda73f":"markdown","09f74549":"markdown","60e07e40":"markdown","02dbad01":"markdown","2d769d4a":"markdown","2b588e41":"markdown","52a10103":"markdown","c02bfdd4":"markdown","dfe1bd7b":"markdown"},"source":{"ad1034a2":"#Library\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport datetime\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\npy.init_notebook_mode(connected = True)\nimport plotly.express as px","61e9824f":"col = ['Name','Age']\ndf = pd.DataFrame(columns = col)\ndf","dacf0815":"# using list of lists\ndata = [['John', 29], ['Hannah', 35], ['Juli', 26]]\ndf = pd.DataFrame(data, columns = ['Name', 'Age'])\ndf","4f36a764":"# using dictionaries\nData = {'Name':['John','Hannah','Juli'],'Age':[29,35,26]}\ndf = pd.DataFrame(Data)\ndf","41631b13":"df = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\nprint('Dataset has ',df.shape[0],' records and ',df.shape[1], ' columns' )\nprint(' ')\ndf.head() # head shows the first 5 rows by default","9246c084":"print('Before Rename: ',df.columns)\ndf = df.rename(columns = {'SellerG':'Real_Estate_Agent'})\n# Column names in the dataset\nprint('After Rename: ',df.columns)","aa36eb11":"# new column\ndf['test'] = 'test column'\ndf.head(2)","32a50f4e":"filters = [\n   (df.Rooms == 3) & (df.Price > 1400000),\n   (df.Rooms == 2) & (df.Price < 1400000),\n   (df.Price < 900000)\n]\nvalues = [\"class1\", \"class2\", \"class3\"]\n\ndf[\"conditional_column\"] = np.select(filters, values, default=\"no_class\")\ndf.head()","61a11fb1":"df.drop('test' , axis = 1,inplace = True) # you can set df equal to this statement here to overwrite or use inplace = True\ndf.head(2)","679b44a9":"new_row = {'Suburb': 'Abbotsford', 'Address': '26 Bloomburg St', 'Rooms': 4, 'Type': 'h', 'Price': 1033030.0, 'Method': 'S', 'Real_Estate_Agent': 'Biggin', 'Date': '4\/02\/2016', 'Distance': 2.5, 'Postcode': 3067.0, 'Bedroom2': 2.0, 'Bathroom': 1.0, 'Car': 0.0, 'Landsize': 156.0, 'BuildingArea': 79.0, 'YearBuilt': 1900.0, 'CouncilArea': 'Yarra', 'Lattitude': -37.8079, 'Longtitude': 144.9934, 'Regionname': 'Northern Metropolitan', 'Propertycount': 4019.0}\nprint(len(df))\ndf = df.append(new_row , ignore_index = True)  # ignore_index should be set to True to append a dict to the dataframe\nprint(len(df))\ndf.tail(2)","11213919":"df.drop(len(df)-1,inplace = True)\ndf.tail(2)","08e1cb49":"df.info()","a01b612b":"df.describe()","af22e0bc":"memory_usage0 = df['Type'].memory_usage()  #select a column with square brackets [ ]\nmemory_usage1 = df.Type.memory_usage() #select a column with .col_name\nprint('Initial memory usage: ',memory_usage1,memory_usage0)\ndf.Type.unique() #Unique values in the column","b498b13e":"# If the number of distinct categories are very few compared to the number of rows, we can save a substantial amount of memory by using category data type\n\ndf['Type'] = df['Type'].astype('category')\nmemory_usage2 = df['Type'].memory_usage()\nprint('Memory usage after changing to categorical type: ',memory_usage2)\nprint('Changing to categorical type reduced the used memory by:' ,(1- memory_usage2\/memory_usage1)*100,'%')","3b32c12a":"print('Initial Type values: ',df.Type.unique())\ndf.Type.replace({'h':'house','u':'unit','t':'town_house'}, inplace = True)\nprint('The more representative values: ',df.Type.unique())\nprint(' ')\ndf['Type'].value_counts(normalize = True)","d5ced282":"#Categorical Data\ndf['Suburb'] = df['Suburb'].astype('category')\ndf['Postcode'] = df['Postcode'].astype('category')\ndf['Regionname'] = df['Regionname'].astype('category')\ndf['Real_Estate_Agent'] = df['Real_Estate_Agent'].astype('category')\ndf['Type'] = df['Type'].astype('category')\ndf['Method'] = df['Method'].astype('category')","0e03cf78":"# finding the columns with missing data\npd.isnull(df).any(axis = 0) ","52dc9328":"df.isnull().sum()","25076d8c":"df.drop(columns = ['YearBuilt', 'BuildingArea', 'CouncilArea'], inplace = True) #Dropped this column since this data point was poorly sourced","b3626669":"#df = df.fillna(df.mean())  # fill with the mean of the column\n# df = df.fillna(df.max())  # fill with the max of the column\ndf['Car'] = df['Car'].fillna(df['Car'].mean())    # setting missing values to zero\npd.isnull(df).any(axis = 0)\n","400aade5":"#Integer Data\ndf['Car'] = df['Car'].astype('int64')\ndf['Rooms'] = df['Rooms'].astype('int64')\ndf['Bedroom2'] = df['Bedroom2'].astype('int64')\ndf['Bathroom'] = df['Bathroom'].astype('int64')\ndf['Price'] = df['Price'].astype('float64')","b273494b":"print(df.Date.dtypes)  # Columns with mixed types are stored with the\u00a0object dtype.\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'].dtypes","ac63d84b":"#extracting month\ndf['Month'] = df['Date'].dt.month\ndf.head()","57107f14":"# After sorting, we have to reset the index:\ndf = df.sort_values('Price',ascending = False).reset_index() # Default is ascending\ndf","3036fa5f":"# The split function, as the name suggests, splits a string at the specified character \ndf['road_type'] = df['Address'].str.split(' ').str[-1]\ndf.road_type.unique()","4294d3e8":"# Standardizing the text formats. \ndf['Address'].str.upper()\ndf['Type'] = df['Type'].str.capitalize()","833c44b8":"print(\"\\n -- loc -- \\n\")\nprint(df.loc[df['Price'] < 150000, ['Type']])\n \nprint(\"\\n -- iloc -- \\n\")\nprint(df.iloc[(df['Price'] < 150000).values, [4]])","5e7b3be4":"df.iloc[[1]].to_dict('records')","42903535":"df.loc[df['Type'] == 'House',['Type','Price','Distance','Regionname']]","b4029ba4":"df_filtered = df.loc[(df['Type'] == 'House') & (df['Distance'] <= 0.5 * np.median(df['Distance'])) & (df['Rooms'] <= 2),['Type','Price','Distance','Regionname','Rooms']]\ndf_filtered","d80a92a1":"print('The mean Price of a property meeting our criteria is: ',\"${:,.2f}\".format(np.mean(df_filtered['Price'])))","f5b7a0a4":"# distribution of the available data over collected dates  \ndf['Date_only']=df['Date'].dt.date\nax = df.groupby(['Date_only'])['Price'].count().plot(kind = 'bar', figsize = (30,10))\n\nax.set_ylabel(\"Number of records\")\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)   \n","3ba6385c":"# Mean price on each of the collected dates \nax = df.groupby(['Date_only'])['Price'].mean().plot(kind = 'bar', figsize = (30,10))\n\nax.set_ylabel(\"Price\")\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)   \n","0911c619":"# Price distribution of each property type \nsns.displot( \n    data = df,\n    x = \"Price\",\n    hue = \"Type\",\n    kind = \"hist\",\n    aspect = 1.5,\n    log_scale = 10,\n    bins = 20\n             )","31eae5d9":"# Visualization\nplt.figure(figsize=(10, 8), dpi=80)\nbox_plot = sns.boxplot(x = 'Type',y = 'Price',data = df.sort_values('Type'))\nplt.ylabel('Price')\nplt.xlabel('Property Type')\n\nax = box_plot.axes\nlines = ax.get_lines()\ncategories = ax.get_xticks()\n\nfor cat in categories:\n    # every 4th line at the interval of 6 is median line\n    # 0 -> p25 1 -> p75 2 -> lower whisker 3 -> upper whisker 4 -> p50 5 -> upper extreme value\n    y = round(lines[cat*6+2].get_ydata()[0],1) \n    y2 = round(lines[cat*6+4].get_ydata()[0],1) \n\n    ax.text(\n        cat, \n        y, \n        f'{y}', \n        ha='center', \n        va='center', \n        fontweight='bold', \n        size=10,\n        color='white',\n        bbox=dict(facecolor='#445A64'))\n    ax.text(\n        cat, \n        y2, \n        f'{y2}', \n        ha='center', \n        va='center', \n        fontweight='bold', \n        size=10,\n        color='white',\n        bbox=dict(facecolor='#445A64'))\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)   \n\nbox_plot.figure.tight_layout()\n\nfig = box_plot.get_figure()\n\n# fig.savefig(\"age.png\",dpi=300)","b2eed347":"max_price = np.max(df['Price'])\nmin_price = np.min(df['Price'])\nmid_price = np.median(df['Price'])\n\ndf[df['Price'].isin([min_price,mid_price,max_price])].sort_values(by = ['Price'])","a6b23135":"min_Lattitude = df[df['Price'] == min_price]['Lattitude'].values[0]\nmin_Longtitude = df[df['Price'] == min_price]['Longtitude'].values[0]\n\nfig = px.density_mapbox(df[df['Price'] == min_price], lat='Lattitude', lon='Longtitude', z='Price', radius=15,\n                        center=dict(lat=min_Lattitude, lon=min_Longtitude), zoom=14,\n                        mapbox_style=\"stamen-terrain\", opacity = 1, title = 'Cheapest Property Neighborhood')\nfig.show()","ea1740f7":"plt.figure(figsize=(10, 8), dpi=80)\nax = sns.violinplot(x = 'Type',y = 'Rooms',data = df.sort_values('Type'), split = False)\nplt.ylabel('Number of Rooms')\nplt.xlabel('Type')\nplt.title('A kernel density estimation of Number of Rooms for each Property Type',fontweight='bold', fontfamily='serif')\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  \n\n# fig = ax.get_figure()\n# fig.savefig(\"mas.png\",dpi=300)","9e726344":"# we can use pivot table and then plot it to compare the price of \n# properties in different regions grouped by their type\n\nax = df.pivot_table(index='Type', values='Price', aggfunc='mean',columns='Regionname')\\\n.plot(kind=\"bar\",figsize=(10, 8))\n\nplt.ylabel('Mean Price')\nplt.title('Southern Metropolitan is constistanly the most expensive region', fontweight='bold', fontfamily='serif')\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left') \n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  ","aeb5d023":"# another approach is using groupby:\nax = df[['Price','Type','Regionname']]\\\n.groupby(['Type','Regionname']) .agg(['mean']).sort_values(by=(\"Price\", \"mean\"))\\\n.dropna().unstack().plot(kind='bar',figsize=(10, 8))\nplt.ylabel('Mean Price')\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  ","02b51d81":"df[['Type','Rooms','Bedroom2']].groupby('Type') .agg(['max','min','mean']).round(1)","f5a74275":"fig = px.density_mapbox(df, lat='Lattitude', lon='Longtitude', z='Price', radius=10,\n                        center=dict(lat=-37.8, lon=145), zoom=10,\n                        mapbox_style=\"stamen-terrain\", opacity = 0.5, title = 'Melbourne Price Heatmap')\nfig.show()","787102b1":"fig = px.density_mapbox(df, lat='Lattitude', lon='Longtitude', z='Landsize', radius=10,\n                        center=dict(lat=-37.8, lon=145), zoom=10,\n                        mapbox_style=\"stamen-terrain\", opacity = 0.5, title = 'Melbourne Landsize Heatmap',range_color=(0,3000))\nfig.show()","33748557":"monthDict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n            7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\ndf.pivot_table(index='Type',columns='Month', values=['Price', 'Propertycount'], aggfunc={'Price':'count'}).rename(columns=monthDict, level=1)","afbf85b3":"import calendar\ndf['Month2'] = df['Month'].apply(lambda x: calendar.month_abbr[x])\n\n\nax = df.pivot_table(index='Type',columns='Month', values=['Price', 'Propertycount'], aggfunc={'Price':'count'})\\\n.rename(columns=monthDict, level=1).plot(kind=\"bar\",figsize=(10, 8))\nplt.ylabel('Count')\nplt.title('Distributions are normal like for all the types, peaking in the middle of the year',fontweight='bold', fontfamily='serif')\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  \n\nax2 = df.pivot_table(index='Type',columns='Month', values=['Price', 'Propertycount'], aggfunc={'Price':'mean'})\\\n.rename(columns=monthDict, level=1).plot(kind=\"bar\",figsize=(10, 8))\nplt.ylabel('Mean Price')\nplt.title('The Supply and demand is reflected in that the price drops in the middle of the year',fontweight='bold', fontfamily='serif')\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n\nfor s in ['top', 'left', 'right']:\n    ax2.spines[s].set_visible(False)\n\nax2.grid(axis='y', linestyle='-', alpha=0.4)  ","34bd9787":"#Numerical Dataset\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_columns = list(df.select_dtypes(include=numerics).columns)[1:-4]\n                                                            \ndf_numerical = df[numerical_columns]\ndf_numerical","5ec42d5c":"import scipy.stats as scs \n\ndef test_normality(col,data):\n  stat, p = scs.shapiro(data)\n  if p < 0.05:\n    print(f'p-Value: {p}. {col} is not normaly distributed.')\n  else:\n    print(f'p-Value: {p}. {col} is normaly distributed.')\n    \nfor col in numerical_columns:\n    test_normality(col,df[col])    ","13afd7d7":"fig, ax = plt.subplots(figsize=(15,10))\nfig.text(.75,.84,'Melbourne House Price Correlations', fontfamily='serif',fontweight='bold',fontsize=15,ha='right')\nfig.text(.75,.665,\n            '''\n             Distance to Central Business District\n             has a high negative correlation  \n             with the price.\n             \n             ''', fontfamily='serif',fontsize=13,ha='right')\n\ncorr = df[numerical_columns].corr(method='spearman')\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\npl = sns.heatmap(corr, mask=mask, annot = True, square=True, linewidths=2.5, cbar=False)","c565e3eb":"sns.pairplot(df_numerical)","dfad13ab":"#Removing the outlier from Landsize var, I am going to remove the row exhibiting the Landsize outlier completely\ndf_numerical_2 = df_numerical.drop(df_numerical['Landsize'].idxmax())","15dabcff":"ax = sns.boxplot(y = df_numerical.Landsize, data = df_numerical)\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  ","a0b49d20":"# removing all the landsize outliers  (3% of the data)\ndf_numerical_2 = df_numerical.drop(df_numerical.index[df_numerical['Landsize'] > 1200])\n#df.loc[df['B'] == 19]\nprint(np.round(100-(df_numerical_2.shape[0]\/df.shape[0])*100), '% of data were dropped for being outliers' )","80380db8":"ax = sns.boxplot(y = df_numerical_2.Landsize, data = df_numerical_2)\n\nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n\nax.grid(axis='y', linestyle='-', alpha=0.4)  ","fc65bf85":"sns.pairplot(df_numerical_2)","8682040c":"len(df_numerical_2)","a7643772":"# Hovering on each of the points displays the number of rooms and the region name\ndf_alt = df[['Type','Regionname','Price','Distance','Rooms','Bathroom','Car','Landsize']]\n\nimport altair as alt\n\nalt.data_transformers.disable_max_rows()\n(alt.\n  Chart(df_alt).\n  mark_circle(size=40).\n  encode(x='Price', y='Distance', color='Type', tooltip=   \n  ['Rooms','Regionname']).\n  properties(height=400, width=500).\n  interactive())","d0757d2a":"## Filterred to only display Town houses\n(alt.\n  Chart(df_alt).\n  mark_circle(size=40).\n  encode(x='Price', y='Distance', color='Regionname', tooltip=   \n  ['Rooms','Landsize']).\n  transform_filter(alt.FieldEqualPredicate(field='Type', equal='Town_house')).\n  properties(height=400, width=500))","d089537f":"### Inserting a new column","760425a6":"# Exploratory Data Analysis & Visualization\n\n## **Groupby**\n#### Groupby is a versatile and extremely useful function for exploratory data analysis","0be3b57f":"### Interactive plots and dynamic filtering with Altair","e0986428":"### Dropping a column","f6fed7db":"### **Data type conversions** _ to categorical","46969272":"### **Data type conversions** _ to integer","4ac9c31e":"### iloc vs. loc","9aa6f692":"# Example dataset: Melbourne House Prices\n### Analysing housing data in Melbourne\n![](https:\/\/cdn.britannica.com\/64\/190464-050-B74E1FD9\/view-central-business-district-Melbourne-train-station.jpg)","95e1cdb8":"### But, the most common case is to read the data from an Excel or CSV file into the dataframe using **pd.read_csv()**:","ad7511b2":"### Dropping a row","3d3c4b22":"## In Jupyter, one can easily document and report what they have done and many use it to present their results. \n\n#### Some useful Jupyter **shortcuts** are: \n##### $\\;\\;\\;\\;\\;\\;$- Ctrl + Enter (run current cell) \n##### $\\;\\;\\;\\;\\;\\;$- Alt+Enter (run the current and make a new cell) \n##### $\\;\\;\\;\\;\\;\\;$- Shift + Enter (run and move to the next cell) \n\n#### When in the **command mode**: \n##### $\\;\\;\\;\\;\\;\\;$- **Use h to pull up the list of all the shortcuts.** Some of the useful ones are:\n##### $\\;\\;\\;\\;\\;\\;$- m (turn the cell into a markdown cell),1 (turn the text into a large header), y (turn it to code cell), k and j are to move up and down, a and b to insert cell above and below \n\n#### In the **code cells**: \n##### $\\;\\;\\;\\;\\;\\;$- Tab (for code completion)\n##### $\\;\\;\\;\\;\\;\\;$- Shift + Tab (for pulling up the help documentation)\n##### $\\;\\;\\;\\;\\;\\;$- Ctrl + ]    (indent) ","7d4666ae":"### Inserting conditional columns","228c55a7":"### ![](https:\/\/i.pinimg.com\/originals\/91\/db\/a8\/91dba80a5419f1bf4700ec99ab6081bb.jpg)","75e7314a":"### Inserting a new row","b46c4ab5":"#### We can use **px.density_mapbox**, and  **Lattitude** and **Longtitude** columns in the data, to overlay the data points on map.\n\n#### Each row of the DataFrame is represented as a point smoothed with a given radius of influence.","bab8e086":"#### Let's now check if the numerical columns in the data are normally distributed. If they are, we will use Pearson for correlation calculation. If the data is not normally distributed, the Spearman method is more appropriate.","6d6be89d":"### Sorting records based on a column values","27249a97":"### More on Altair and Interactive Visualization on my notebook \"Interactive plots with Altair\"!","3d43e2bb":"#### There are a few outliers after the max value and the minium price of all is \\\\$85,000. \n#### \\\\$85,000 for a property is very low for Melbourne. Let's find this cheapest property: ","5c795c7b":"Orighty, this pairplot has lot going on so lets take it step-by-step. Firstly, Landsize data has an outlier sticking out. We can get rid of the outlier which will give us some more meaningful insights (will do this in a bit).\n\nNow lets look at Distance. The Distance variable is exhibiting a sort of positively skewed bell curve characteristics when plotted against Price. It seems that as distance reduces prices increase not strictly linearly, this maybe why the correlation matrix was showing strange values. But from the pairplot we can observe the 4th assumption in full effect which is awesome! There are obvious relationships with distance such as distance increases (moving out of CBD) Rooms, Bathrooms and Car space will increase. I do not want to spend too much time on this at the moment.\n\nFinally, looking at Rooms, Bathrooms and Car we can observe a loosely positive increase in price as there is a positive 1-unit change in the three variables. Once again this does not indicate that there is a strict linear relationship, we have to always take this kind of analysis with a grain of salt.\n","2dbe9f4c":"### Handling missing values","b9442490":"### DataFrame Summary","eeeda73f":"### There are multiple ways to create a DataFrames:","09f74549":"## The notebook is as follows:\n#### $\\;\\;\\;\\;\\;\\;$- Reading Data into DataFrames\n#### $\\;\\;\\;\\;\\;\\;$- Inserting, Dropping, and Renaming rows and columns\n#### $\\;\\;\\;\\;\\;\\;$- Data Cleaning (Handling missing values, and converting data formats)\n#### $\\;\\;\\;\\;\\;\\;$- Exploratory Data Analysis \n#### $\\;\\;\\;\\;\\;\\;$- Data Visualization\n#### $\\;\\;\\;\\;\\;\\;$- Hypothesis formulation\/Conclusion","60e07e40":"### Correlation of Price with other features\n\n#### Correlation matrix can only be used for numberical variables. \n\n#### Here are some common sense assumptions:\n1. **Landsize and Price are probably highly correlated.** \n2. **Rooms, Bathroom and Carpark should be highly correlated.** \n3. **In Australia the CBD(Central Business District) tends to be prime property.** The reason for that is every aminety is readily avialable and being close to work is a huge advantage consdering all work offices tend to be within the CBD.\n","02dbad01":"### **Data type conversions** _ to date","2d769d4a":"## Introduction to DataFrames  \n### Let's create an empty dataframe first!","2b588e41":"### Renaming a column","52a10103":"#### We can see the irregularity in the number of data points for the recorded dates. ","c02bfdd4":"#### Bedroom2 (Scraped # of Bedrooms (from different source)) is not consistent with the other source:\n","dfe1bd7b":"### Descriptive statistics of the DataFrame"}}