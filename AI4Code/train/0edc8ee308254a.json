{"cell_type":{"f0920987":"code","4d6861eb":"code","a325f0b1":"code","335e3f99":"code","52dfe683":"code","9d1092d0":"code","6e1bc51f":"code","22110d44":"code","a2413898":"code","6134e2d9":"code","2ed36687":"code","1693416d":"code","600eaf86":"code","78c56de2":"code","a94d9141":"code","65c51608":"code","87593e33":"code","a0b0940b":"code","c59fa993":"code","19983475":"code","4013afd4":"code","f5b1cbd2":"code","f079eff1":"code","f381cb0a":"code","58bec0ae":"code","89b0fd20":"code","53bb399e":"code","c1b9c09d":"code","d5bb36f7":"markdown","5c73ed67":"markdown","b54818c3":"markdown","3682f3b2":"markdown","af108018":"markdown","56e99484":"markdown"},"source":{"f0920987":"import os\nimport random\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as D\nimport torch.nn.functional as F\n\nfrom torchvision import transforms, models\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntorch.cuda.empty_cache()","4d6861eb":"def display_pil_images(\n    images, \n    labels,\n    columns=5, width=20, height=8, max_images=15, \n    label_wrap_length=50, label_font_size=8):\n\n    if not images:\n        print(\"No images to display.\")\n        return \n\n    if len(images) > max_images:\n        print(f\"Showing {max_images} images of {len(images)}:\")\n        images=images[0:max_images]\n\n    height = max(height, int(len(images)\/columns) * height)\n    plt.figure(figsize=(width, height))\n        \n    for i, image in enumerate(images):\n\n        plt.subplot(len(images) \/ columns + 1, columns, i + 1)\n        plt.imshow(image)\n\n        plt.title(labels[i], fontsize=label_font_size); \n        \ndef show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n    \ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a325f0b1":"DATA_DIR = '\/kaggle\/input\/colorectal-histology-mnist\/'\nSMALL_IMG_DATA_DIR = os.path.join(DATA_DIR, 'kather_texture_2016_image_tiles_5000\/Kather_texture_2016_image_tiles_5000')\nLARGE_IMG_DATA_DIR = os.path.join(DATA_DIR, 'kather_texture_2016_larger_images_10\/Kather_texture_2016_larger_images_10')\n\n# for inceptionv3, xception 229, other models 224\nIMAGE_SIZE = 224\nSEED = 2000\nBATCH_SIZE = 64\nNUM_EPOCHS = 15\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","335e3f99":"classes = os.listdir(SMALL_IMG_DATA_DIR)\nclasses","52dfe683":"os.listdir(LARGE_IMG_DATA_DIR)","9d1092d0":"samples_to_view = []\nfor label in classes:\n    num_samples = len(os.listdir(os.path.join(SMALL_IMG_DATA_DIR,label)))\n    print(label + '\\t' + str(num_samples))\n    samples_to_view.append(random.choice(np.arange(num_samples)))","6e1bc51f":"imgs = []\nfor idx, label in enumerate(classes):\n    show_idx = samples_to_view[idx]\n    file_name = os.listdir(os.path.join(SMALL_IMG_DATA_DIR,label))[show_idx] \n    print(file_name)\n    imgs.append(Image.open(os.path.join(SMALL_IMG_DATA_DIR,label,file_name)))","22110d44":"display_pil_images(imgs, classes)","a2413898":"imgs_paths, labels = [], []\nfor label in classes:\n    file_names = os.listdir(os.path.join(SMALL_IMG_DATA_DIR,label))\n    for file_name in file_names:\n        imgs_paths.append(os.path.join(SMALL_IMG_DATA_DIR,label,file_name))\n        labels.append(label)","6134e2d9":"df = pd.DataFrame(data={'img_path': imgs_paths, 'label': labels})\ndf.head()","2ed36687":"label_num = {}\nfor idx, item in enumerate(np.unique(df.label)):\n    label_num[item] = idx ","1693416d":"df['label_num'] = df['label'].apply(lambda x: label_num[x])","600eaf86":"class HistologyMnistDS(D.Dataset):\n    def __init__(self, df, transforms, mode='train'):\n\n        self.records = df.to_records(index=False)\n        self.transforms = transforms\n        self.mode = mode\n        self.len = df.shape[0]\n    \n    @staticmethod\n    def _load_image_pil(path):\n        return Image.open(path)\n        \n    def __getitem__(self, index):\n        path = self.records[index].img_path\n\n        img = self._load_image_pil(path)\n        \n        if self.transforms:\n            img = self.transforms(img)\n\n        if self.mode in ['train', 'val', 'test']:\n            return img, torch.from_numpy(np.array(self.records[index].label_num))\n        else:\n            return img\n\n    def __len__(self):\n        return self.len","78c56de2":"train_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","a94d9141":"train_df, tmp_df = train_test_split(df,\n                                      test_size=0.2,  \n                                      random_state=SEED,\n                                      stratify=df['label'])\n\nvalid_df, test_df = train_test_split(tmp_df,\n                                      test_size=0.8,  \n                                      random_state=SEED,\n                                      stratify=tmp_df['label'])","65c51608":"print(\"Train DF shape:\", train_df.shape)\nprint(\"Valid DF shape:\", valid_df.shape)\nprint(\"Test DF shape:\", test_df.shape)","87593e33":"ds_train = HistologyMnistDS(train_df, train_transforms)\nds_val = HistologyMnistDS(valid_df, val_transforms, mode='val')\nds_test = HistologyMnistDS(test_df, val_transforms, mode='test')","a0b0940b":"train_loader = D.DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = D.DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = D.DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","c59fa993":"X_batch, y_batch = next(iter(train_loader))\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nplt.imshow(X_batch[0].permute(1, 2, 0).numpy() * std + mean)\nplt.title(y_batch[0]);","19983475":"import copy \n\ncheckpoints_dir = '\/kaggle\/working\/'\n\nhistory_train_loss, history_val_loss = [], []\n    \ndef train_model(model, loss, optimizer, scheduler, num_epochs):\n    best_model_wts = copy.deepcopy(model.state_dict())\n       \n    best_loss = 10e10\n    best_acc_score = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_loader\n                scheduler.step()\n                model.train()\n            else:\n                dataloader = val_loader\n                model.eval()\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n            \n            if phase == 'train':\n                history_train_loss.append(epoch_loss)\n            else:\n                history_val_loss.append(epoch_loss)\n            \n            if phase == 'val':\n                def save_checkpoint(name):\n                    checkpoint = {\n                        'state_dict': best_model_wts\n                    }\n\n                    model_file_name = name + '.pth.tar'\n\n                    model_file = checkpoints_dir + model_file_name\n\n                    if not os.path.exists(checkpoints_dir):\n                        os.mkdir(checkpoints_dir)\n\n                    # saving best weights of model\n                    torch.save(checkpoint, model_file)\n\n                if epoch_loss < best_loss:\n                    best_loss = epoch_loss\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    print(\"Saving model for best loss\")\n                    save_checkpoint('best_model')\n                \n                if epoch_acc > best_acc_score:\n                    best_acc_score = epoch_acc\n                    \n                print('Best_loss: {:.4f}'.format(best_loss))\n                print('Best_acc_score: {:.4f}'.format(best_acc_score))\n\n    return model","4013afd4":"model = models.resnet50(pretrained=False)\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, len(classes))\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","f5b1cbd2":"train_model(model, loss, optimizer, scheduler, num_epochs=NUM_EPOCHS);","f079eff1":"x = np.arange(NUM_EPOCHS)\nplt.plot(x, history_train_loss)\nplt.plot(x, history_val_loss)","f381cb0a":"filename = \"best_model.pth.tar\"\n\nmodel.load_state_dict(torch.load(os.path.join(checkpoints_dir, filename))['state_dict'])\n    \nmodel.eval()\n\ny_preds = []\nfor inputs, labels in tqdm(test_loader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    y_preds.append(preds.argmax(dim=1).data.cpu().numpy())\n    \ny_preds = np.concatenate(y_preds)","58bec0ae":"inputs, labels = next(iter(test_loader))\n\nfor img, label, pred in zip(inputs, labels, y_preds):\n    title = f\"True label: {label}\\nPredicted label: {pred}\"\n    show_input(img, title=title)","89b0fd20":"cm = confusion_matrix(test_df.label_num.values, y_preds)","53bb399e":"plt.figure(figsize=(10,10))\nplot_confusion_matrix(cm, label_num)","c1b9c09d":"print(classification_report(test_df.label_num.values, \n                            y_preds, \n                            target_names=list(label_num.keys())))","d5bb36f7":"# PyTorch Dataset, Dataloaders and Transforms","5c73ed67":"# Validation and test results","b54818c3":"# Train loop","3682f3b2":"# Model setup and training","af108018":"# Data exploration","56e99484":"# Helpers"}}