{"cell_type":{"48a4352b":"code","5610078f":"code","95307dba":"code","16ae1423":"code","08c48432":"code","f33831db":"code","62f25a50":"code","c04f14ab":"code","8e645afe":"code","c1a229c6":"code","e73c5c58":"code","7e1d34fd":"code","5d77b081":"code","5d57dc4c":"code","124475ae":"code","39a2f573":"code","a1269999":"code","19a484b8":"code","09e04f16":"code","2503358d":"markdown","ba54129f":"markdown","e01d441a":"markdown","2d3a3812":"markdown","cad0b25d":"markdown","07946922":"markdown","7cab4fca":"markdown","24f5cf4c":"markdown","4eac0efd":"markdown","4c3ba7cb":"markdown","36e2fcc5":"markdown","d64ed55e":"markdown","0a74567c":"markdown"},"source":{"48a4352b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5610078f":"# Getting the data into a dataframe and taking a first look\ndata = pd.read_csv('..\/input\/multi-organ-failure-prediction\/mods_dataset.csv')\ndata.head()","95307dba":"sns.heatmap(data.isnull(), xticklabels = False, cbar = False, cmap = 'Blues')","16ae1423":"ratios = data.isnull().sum()\/len(data)*100\nratios","08c48432":"cols = []\n\nfor i in range(data.columns.shape[0]):\n    if ratios[i] < 10:\n        cols.append(data.columns[i])\n\n# The length of the new list is the amount of columns with less than 10% missing vlaues\nlen(cols)","f33831db":"# Storing the remaining columns in a new dataframe\nnew_data = data[cols].copy()\n\n# Let's take another look\nnew_data.head()","62f25a50":"sns.heatmap(new_data.isnull(), cbar = False, cmap = \"Blues\", xticklabels = False)","c04f14ab":"new_data.interpolate(method = \"linear\", limit = 5, inplace = True)","8e645afe":"sns.heatmap(new_data.isnull(), cbar = False, cmap = \"Blues\", xticklabels = False)","c1a229c6":"# Separate the training data from the target data\nX = new_data.iloc[:, 0:-1]\ny = new_data.loc[:, 'Label']","e73c5c58":"from sklearn.model_selection import train_test_split  \nfrom sklearn.linear_model import LogisticRegression\n\n# Split the dataset into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4 ,random_state= 0)\n\n# Training the logistic regression model\nlogreg = LogisticRegression(max_iter = 2500)\nlogreg.fit(X_train, y_train)","7e1d34fd":"from sklearn.metrics import accuracy_score, plot_confusion_matrix\ny_pred = logreg.predict(X_test)\nprint(\"Test accuracy:\", np.round(accuracy_score(y_test, y_pred), 2))","5d77b081":"plot_confusion_matrix(logreg, X_test, y_test)  \nplt.show()","5d57dc4c":"from sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = None)\npca.fit(X_scaled)","124475ae":"plt.plot(np.cumsum(pca.explained_variance_ratio_*100))\nplt.xlabel(\"Number of components (Dimensions)\")","39a2f573":"# Notice that now we specify the amount of components we wish to use\npca = PCA(n_components = 30)","a1269999":"# Storing the scaled dataset into a new dataframe\nX_pca = pca.fit_transform(X_scaled)\nX_pca = pd.DataFrame(X_pca)\n\n# Splitting our dataset with the same size and random state\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size = 0.4, random_state = 0)\n\n# Training the new model with the same number of iterations\nmodel2 = LogisticRegression(max_iter = 2500)\nmodel2.fit(X_train_pca, y_train)\ny_pred2 = model2.predict(X_test_pca)","19a484b8":"print(\"Test accuracy:\", np.round(accuracy_score(y_test, y_pred2), 2))","09e04f16":"plot_confusion_matrix(model2, X_test_pca, y_test)  \nplt.show()","2503358d":"## Taking a first look and cleaning the dataset\n\nThe process begins with importing the commonly used libraries","ba54129f":"## Training and testing the new model\n\nThe process will be almost entirely the same to train this model, with the exception that now the scaled dataset is used for training the model.","e01d441a":"Setting the n_components argument to None will yield the maximum amount of principal components, which is equal to the amount of columns in the original dataset, this becomes useful when combined with a visualization to see how much of the variation can be explained with the least quantity of principal components.","2d3a3812":"Now that the dataset is free of missing values, it can be used to create machine learning models.\nLet's begin by creating a logistic regression model to predict the target label.\n\n## Training the first regression model","cad0b25d":"Now, since the goal is to eliminate as many columns as possible, the columns that are missing over 10% of the values will be dropped. ","07946922":"From the plot above, it's seen that 10 components account for about 60% of the variation, while 20 account for 80% and the plot plateaus at about 30 components, which is the number of dimensions that will be chosen for the new regression model. ","7cab4fca":"Now that a model has been trained, let's see how it performs.","24f5cf4c":"Our data is comprised of 197 different attributes including the target label, performing exploratory data analysis won't be as easy as it is with small-dimensioned data. Also, training a model with this amount of dimensions will very likely lead to overfitting.\n\nFrom the table above it's evident that there may be columns and rows with several missing values, let's take a quick look to get an idea of just how many values are missing.","4eac0efd":"After dropping all of the columns with more than 10% of missing values, the heatmap is looking a lot cleaner, but there are still a few missing values, the interpolation method will be used for filling in the remaining nulls.","4c3ba7cb":"The dark blue portions of the dataset indicate missing values.\n\nSince this dataset is small as it is, it'd be wise to avoid removing rows and instead remove columns based on a Missing Value Ratio, this way no rows are deleted and reduction of the dimensionality of the dataset is achieved.\nThe missing value ratio for each column is obtained with the following code:","36e2fcc5":"## Improving accuracy by dimensionality reduction with principal component analysis\n\nThe last model was not high, but all is not lost, the dimensions of this dataset can still be reduced while maintaining test size to improve the model's accuracy. \n\nPrincipal Component Analysis is an unsupervised ML algorithm that works by creating a new variable (or group of them, depending on what the programmer wishes), in which the first variable accounts for the most variation in the dataset, the second one accounts for the second largest amount variation and so on and so forth. \n\nUsing this type of technique allows to reduce the amount of variables used in the model whilst also eliminating multicolinearity, which is especially useful in linear models such as the one used for this dataset.\n\nThe process begins by scaling the dataset to stop one variable with a larger range from dominating over another one with a smaller range. e.g: a variable that goes from 0 to 100 over another one with range 0 to 1","d64ed55e":"A baseline accuracy of 68% was achieved, let's see if that score can be improved upon by using other methods.","0a74567c":"Success!\n\nBy eliminating multicolinearity between attributes and reducing the amount of dimensions in the dataset, a model with a higher percentage of accuracy, from 68% to 80%, could be trained. "}}