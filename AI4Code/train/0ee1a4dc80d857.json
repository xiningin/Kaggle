{"cell_type":{"7b0cf5d0":"code","65e76466":"code","ad7b994c":"code","1912c8d8":"code","ca442565":"code","704fe9c6":"code","1009132c":"code","55e05e85":"code","efc20c89":"code","4b01b029":"code","cc755f18":"code","90aa2605":"code","5ca6f83c":"code","027fa546":"code","9ba84859":"code","1bd5fbae":"code","cc7d97bc":"code","d1284594":"code","21cb70e7":"code","e87c6684":"code","681008c6":"code","290260d8":"code","95d0bd08":"code","812fb4de":"code","742cfc97":"code","cf08217b":"code","1510c334":"code","3cbe3076":"code","c92afa15":"code","3addd648":"code","d06f00e4":"code","dbde6777":"code","472de32b":"code","d4e51fa0":"code","468af523":"code","03027885":"code","9326db38":"code","fe0ac75e":"code","fe08fede":"code","5a9c57f9":"code","d77e59ad":"code","04215d94":"code","67772a8a":"code","cafb6910":"code","66b324ef":"code","3feaf98d":"code","85f349ac":"code","3984e7c1":"code","bb4ca946":"code","6f18b33a":"code","eb01566e":"code","381d90c4":"markdown","215fdb2d":"markdown","5d355f76":"markdown","94859d7e":"markdown","e927415b":"markdown","e1615aaa":"markdown","f2c4939c":"markdown","755af673":"markdown","c2893060":"markdown","beacd069":"markdown","60eb7eeb":"markdown","1d65be44":"markdown","897b5ab8":"markdown","de1bc7f1":"markdown","aa9bcf50":"markdown","7d04764d":"markdown","634b2d18":"markdown","6323520f":"markdown","e9d11bc8":"markdown","96e51c08":"markdown","69f7dc20":"markdown","ba249548":"markdown","30e3633c":"markdown","a3ec3971":"markdown","908f0a56":"markdown","56987e87":"markdown","a9a1932d":"markdown","aad1f9a4":"markdown","6080ca36":"markdown","d8c8b9d6":"markdown","eebee4c7":"markdown","f60d5c39":"markdown","52b66953":"markdown","9bff0c94":"markdown","e0741923":"markdown","a177ece1":"markdown","d995e851":"markdown","f63fe000":"markdown"},"source":{"7b0cf5d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","65e76466":"features = pd.read_csv(\"..\/input\/Features data set.csv\")\nsales = pd.read_csv(\"..\/input\/sales data-set.csv\")\n#stores = pd.read_csv(\"..\/input\/stores data-set.csv\")","ad7b994c":"features['Date'] = pd.to_datetime(features['Date'])\nfeatures = features.fillna(0)\nfeatures.head()","1912c8d8":"sales['Date'] = pd.to_datetime(sales['Date'])\nsales.head()","ca442565":"sales","704fe9c6":"import matplotlib.pyplot as plt\n\ndata_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')","1009132c":"filter_dept = data_in['Dept']== 5\nfiltered_data = data_in[filter_dept]\n\nfilter_sale = filtered_data['Weekly_Sales'] == 259955.820000\nfiltered_data[filter_sale]","55e05e85":"data_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\n\nmax_sales_per_store = data_in.groupby(by=['Store'], as_index=False)['Weekly_Sales'].max()\nmax_sales_per_store\n\nmax_sale_features = pd.DataFrame(columns=list(data_in))\nfor entry in max_sales_per_store['Weekly_Sales']:\n    filter_sale =data_in['Weekly_Sales']== entry\n    max_sale_features = max_sale_features.append(pd.DataFrame(data_in[filter_sale], columns=list(data_in)))\n\nmax_sale_features","efc20c89":"data_in_analysis = data_in.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\ndata_in_analysis.sort_values('Weekly_Sales', ascending=False)\n\nplt.figure(figsize=(20,5))\nplt.plot(data_in_analysis.Date, data_in_analysis.Weekly_Sales)\nplt.show()","4b01b029":"data_analysis = data_in.groupby(by=['Store'], as_index=False)['Weekly_Sales'].sum()\ndata_analysis","cc755f18":"def scatter(dataset, feature):\n    plt.figure()\n    plt.scatter(data_in[feature] , data_in['Weekly_Sales'])\n    plt.ylabel('weeklySales')\n    plt.xlabel(feature)","90aa2605":"scatter(data_in, 'Fuel_Price')\nscatter(data_in, 'Store')\nscatter(data_in, 'Dept')\nscatter(data_in, 'Temperature')\nscatter(data_in, 'CPI')\nscatter(data_in, 'IsHoliday')\nscatter(data_in, 'Unemployment')","5ca6f83c":"scatter(data_in, 'MarkDown1')\nscatter(data_in, 'MarkDown2')\nscatter(data_in, 'MarkDown3')\nscatter(data_in, 'MarkDown4')\nscatter(data_in, 'MarkDown5')","027fa546":"import seaborn as sns\n\ncorr = data_in.corr()\ncorr = abs(corr)\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, \n            annot=True, fmt=\".3f\",\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.show()","9ba84859":"corr['Weekly_Sales'].sort_values(ascending=False)","1bd5fbae":"data_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\ndata_in = data_in.drop(columns = ['Date'])\n\nprint(data_in.values.shape)\ndata_in[:][100:105]","cc7d97bc":"from sklearn.model_selection import train_test_split\n\nx = data_in.drop(columns=['Weekly_Sales'])\ny = data_in['Weekly_Sales']\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)\nprint(xTrain.shape, yTrain.shape)","d1284594":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, make_scorer\n\nmodel = LinearRegression()\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","21cb70e7":"model.score(xTest, yTest)","e87c6684":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\nmodel = MLPRegressor(hidden_layer_sizes=(20,),  activation='tanh', solver = 'adam', max_iter=30, alpha=0.001, batch_size='auto',\n    verbose=3)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))\n\n","681008c6":"from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor(max_depth=5)\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","290260d8":"model.score(xTest, yTest)","95d0bd08":"from sklearn.ensemble import ExtraTreesRegressor\n\nmodel = ExtraTreesRegressor(n_estimators=100,max_features='auto', verbose=1)\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","812fb4de":"model.score(xTest, yTest)","742cfc97":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=50, max_features=0.99, min_samples_leaf=2,\n                          n_jobs=-1, oob_score=True)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","cf08217b":"model.score(xTest, yTest)","1510c334":"from sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor(n_estimators= 400, max_depth=4, min_samples_split=2,\n          learning_rate=0.1, loss='ls')\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","3cbe3076":"model.score(xTest, yTest)","c92afa15":"data_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\ndata_in = data_in.drop(columns = ['Date'])\ndata_in.describe()","3addd648":"x = data_in.drop(columns=['Weekly_Sales'])\ny = data_in['Weekly_Sales']\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)\nprint(xTrain.shape, yTrain.shape)","d06f00e4":"from sklearn.ensemble import ExtraTreesRegressor\n\nmodel = ExtraTreesRegressor(n_estimators=100,max_features='auto', verbose=1)\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","dbde6777":"score_tree_1 = model.score(xTest, yTest)\nprint(score_tree_1)","472de32b":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=50, max_features=0.99, min_samples_leaf=2,\n                          n_jobs=-1, oob_score=True)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","d4e51fa0":"score_forest_1 = model.score(xTest, yTest)\nprint(score_forest_1)","468af523":"data_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\ndata_in = data_in.drop(columns = ['Date'])\ndata_in.describe()","03027885":"data_in = data_in.drop(columns=['Fuel_Price', 'Temperature',  'IsHoliday', 'MarkDown2', 'CPI',  'Unemployment',])","9326db38":"x = data_in.drop(columns=['Weekly_Sales'])\ny = data_in['Weekly_Sales']\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)\nprint(xTrain.shape, yTrain.shape)","fe0ac75e":"from sklearn.ensemble import ExtraTreesRegressor\n\nmodel = ExtraTreesRegressor(n_estimators=100,max_features='auto', verbose=1)\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","fe08fede":"score_tree_2 = model.score(xTest, yTest)\nprint(score_tree_2)","5a9c57f9":"model = RandomForestRegressor(n_estimators=50, max_features=0.99, min_samples_leaf=2,\n                          n_jobs=-1, oob_score=True)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","d77e59ad":"score_forest_2 = model.score(xTest, yTest)\nprint(score_forest_2)","04215d94":"data_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\ndata_in = data_in.drop(columns = ['Date'])\ndata_in.describe()","67772a8a":"data_in = data_in.drop(columns=['Fuel_Price', 'Temperature',  'IsHoliday', 'MarkDown2', 'CPI',  'Unemployment', 'MarkDown1', 'MarkDown3', 'MarkDown4', 'MarkDown5'])","cafb6910":"x = data_in.drop(columns=['Weekly_Sales'])\ny = data_in['Weekly_Sales']\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)\nprint(xTrain.shape, yTrain.shape)","66b324ef":"data_in.describe()","3feaf98d":"from sklearn.ensemble import ExtraTreesRegressor\n\nmodel = ExtraTreesRegressor(n_estimators=100,max_features='auto', verbose=1)\n\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","85f349ac":"score_tree_3 = model.score(xTest, yTest)\nprint(score_tree_3)","3984e7c1":"model = RandomForestRegressor(n_estimators=50, max_features=0.99, min_samples_leaf=2,\n                          n_jobs=-1, oob_score=True)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))","bb4ca946":"score_forest_3 = model.score(xTest, yTest)\nprint(score_forest_3)","6f18b33a":"data_scores = [[score_tree_1, score_forest_1],\n              [score_tree_2, score_forest_2],\n              [score_tree_3, score_forest_3]]\n\ndata_scores","eb01566e":"from sklearn.ensemble import RandomForestRegressor\n\ndata_in = pd.merge(features, sales, on=['Store', 'Date', 'IsHoliday'], how='inner')\ndata_in = data_in.drop(columns = ['Date'])\n\nx = data_in.drop(columns=['Weekly_Sales'])\ny = data_in['Weekly_Sales']\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.1, random_state = 0)\nprint(xTrain.shape, yTrain.shape)\n\nmodel = RandomForestRegressor(n_estimators=50, max_features=0.99, min_samples_leaf=2,\n                          n_jobs=-1, oob_score=True)\nmodel.fit(xTrain, yTrain)\nyPredict = pd.Series(model.predict(xTest))\nerr = mean_absolute_error(yTest, yPredict)\nprint('{:.3f}'.format(err))\n\nnames = xTrain.columns\nprint(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), \n             reverse=True))","381d90c4":"#### Modelul de baza - Linear Regression","215fdb2d":"Comentarii: Observam ca eliminarea de coloane duce la o performanta sub optima. Acest lucru se datoreaza si faptului ca cele doua modele, atat Random Forest cat si Random Trees au metoda lor optimizata de selectare si \"weighting\" a celor mai importante coloane.","5d355f76":"Comentarii: Se poate observa ca eliminarea anumitor coloane nu duce la o imbunatatire.","94859d7e":"#### Cele mai corelate 2 coloane","e927415b":"#### Random Forest","e1615aaa":"# Part 2 - Data modeling and prediction","f2c4939c":"Matricea de corelate - cum sunt coloanele corelate intre ele.","755af673":"Rulam Random Trees pe toate coloanele folosite","c2893060":"Rulam Random Trees pe coloanele corelate","beacd069":"Nu avem date test, asa ca voi folosi din datele puse la dispozitie 10% pentru validarea modelului.","60eb7eeb":"*Comentarii*:\n\nSe observa deci ca unele din variabilele puternic corelate se regasesc si in cele selectate de model, insa nu toate. De exemplu CPI si Unemployment sunt coloanele cu cele mai slabe corelatii, dar care sunt pe locul 3, respectiv 4 in importanta variabilelor gasite de model. Acest lucru inseamna ca sunt corelatii pe portiuni de date, care nu sunt vizibile in matricea de corelatie. De asemenea, uneori un model poate optimiza folosirea acestor variabile mai bine decat analiza manuala de date. Insa atunci cand nu sunt resurse pentru un astfel de model, analiza manuala poate ajuta.","1d65be44":"*Comentarii*: \n\nModelul nu converge, sau pare sa convearga foarte lent. Loss-function are o evolutie foarte inceata. Am incercat sa miscorez learning-rate (pasul gradientului) si sa maresc numarul de iteratii, insa comportamentul este asemanator. In concluzie, acest tip de retele neuronale nu se comporta cum trebuie la tipul nostru de date.","897b5ab8":"*Comentarii:* \n\nAm luat cateva departamente sa vad cum arata celelalte variabile cand vanzarile sunt maxime. Apoi am facut un tabel cu vanzarile maxime per magazin ca sa vad cum variaza celelalte variabile. Se observa ca MarkDowns au influente intamplatoare la unele magazine (se va observa acelasi lucru si in graficele de mai jos). In schimb se observa la coloanele de Temperature, CPI si Unemployment ca variatiile sunt mai mici. De asemenea, temperatura pare a ramane medie, lucru care se observa si din graficul de mai jos: cand temperaturile sunt extreme, vanzarile scad. Un lucru iarasi de notat este faptul ca unele departamente au cu preponderenta vanzari maxime.","de1bc7f1":"### B) Experimente pe date","aa9bcf50":"*Comentarii: *\n\nAm incercat sa aplic boosting pe gradient dar nu ajuta la predictie. Observam ca eroarea creste. Ne vom opri aici si vom alege Random Decision Trees si Random Forests ca modele candidat pentru a doua parte: explorarea de features.","7d04764d":"#### Decision Trees","634b2d18":"### A) Experimente pe modele","6323520f":"Cum evolueaza vanzarile per total in timp.","e9d11bc8":"*Comentarii:* converge rapid dar eroarea este prea mare. Practic nici nu se apropie de o predictie.","96e51c08":"\nEvolutia vanzarilor in functie de features.\n\n","69f7dc20":"Rulam Random Forest pe toate coloanele","ba249548":"Rulam pe Random Trees","30e3633c":"\nEvolutia vanzarilor in functie de oferte.\n\n","a3ec3971":"Voi mai face un ultim comentariu. Pentru analiza de date am folosit doar factorul de corelatie implicat direct din variabilele disponibile. Insa algoritmul Random Forest face acest lucru deja: cauta variabilele cele mai importante si le asigneaza greutati in timpul invatarii. Vom incerca sa vedem daca importanta variabilelor date de algoritm corespund cu cele date de matricea de corelatie.","908f0a56":"Pentru o vedere de ansamblu se pot lua toate combinatiile de features si incerca cate un model pe fiecare. Apoi selectat cel cu cea mai buna performanta. Aceasta metoda insa este costisitoare si creste exponential cu numarul de noi features. Pentru a incerca cateva experimente insa ne vom uita la matricea de corelatie de mai sus si vom incerca cateva variante:\n* Toate coloanele folosite\n* Coloanele mai corelate: vom elimina 6 coloane din mijloc din cele 12 sortate\n* Coloanele cele mai corelate: vom pastra doar 2 coloane, cele mai corelate din matrice.\n\nObservatie: Am scos coloana 'Date' pentru ca acesta va fi indexul. Coloanele 'Store' si 'IsHoliday' sunt comune celor doua tabele, deci vor fi combinate. ","56987e87":"# Part 1 - Data Analysis","a9a1932d":"*Comentarii: *\n\nDecision Trees incep sa convearga iar eroarea absoluta scade. Scorul insa este inca foarte mic. Vom incerca si alte variatiuni de decision trees sa vedem daca putem imbunatati scorul.","aad1f9a4":"Pentru modelare vom explora 2 experimente: variatii de model si variatii de date.\n\n* Experimente pe modele: <br\/>\n    Vom incerca mai multe tipuri de modele si le vom evalua folosind mai multe metrici; la final vom selecta 1-2 modele care converg si ofera o performanta rezonabila, in termeni de acuratete.\n* Experimente pe date: <br\/>\n    Vom incerca sa folosim diverse combinatii de features din datele puse la dispozitie si vom selecta cele unde modelele propuse la 2.a se comporta cel mai bine: ofera o convergenta relativ rapida si o performanta rezonabila\n    \nDupa cele doua analize vom selecta un candidat cu cea mai mica eroare de predictie.","6080ca36":"#### Random decision trees","d8c8b9d6":"Cum arata datele si ce contin. Combinarea surselor","eebee4c7":"*Comentarii:* \n\nModelul incepe sa aiba mai mult sens. Eroarea totala a scazut iar scorul a ajuns la o valoare foarte buna, aproape de 94%. Putem sa ne declaram multumiti sau putem incerca mai multe variante de modele. De asemenea, putem incerca si o serie de parametri pentru acest model, precum numarul de estimatori.","f60d5c39":"Rulam pe Random Forest","52b66953":"#### Retea de neuroni","9bff0c94":"*Comentarii:* \n\nPana aici random forest pare a avea cea mai buna performanta. Tin sa precizez ca am incercat mai multe variante pentru numarul de estimatori, dar 40 pare a fi cel mai potrivit. Coeficientul de determinare este de 0.94, ceea ce inseamna ca pentru datele x (in cazul nostru datele calendaristice) putem prezice y (in cazul nostru Weekly_Sales) cu o eroare foarte mica.","e0741923":"#### Coloanele mai corelate","a177ece1":"#### Boosting","d995e851":"#### Toate coloanele folosite","f63fe000":"Pregatim datele pentru experiment"}}