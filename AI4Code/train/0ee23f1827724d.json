{"cell_type":{"35339d88":"code","9d2cf80e":"code","1863ce9c":"code","de50ef36":"code","485f540d":"code","0b257ec0":"code","6971f2e7":"code","5161508a":"code","062ff31e":"code","262eebf5":"code","b28046f1":"code","1315e368":"code","a3a26bfa":"code","89bd65e2":"code","cb2b4fcb":"code","9a349734":"code","62fef9c6":"code","29a2e45b":"code","2de71f8a":"code","eb256992":"code","108af093":"code","634fdd45":"code","7ee7ffec":"markdown","4c58b189":"markdown","5f039a44":"markdown","238d88a4":"markdown","84f6d1cb":"markdown","82da571b":"markdown","febd5790":"markdown","0e4870e9":"markdown","b182c9aa":"markdown"},"source":{"35339d88":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom pprint import pprint\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/cap-4611-2021-fall-assignment-2'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\", low_memory=False)\neval_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\", low_memory=False)\nsample_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/sample_submission.csv\", low_memory=False)\n\ndf = pd.DataFrame(train_data)","9d2cf80e":"df","1863ce9c":"missing_train_data = train_data.isna().sum()\nprint(missing_train_data)","de50ef36":"from sklearn.model_selection import train_test_split\n\nfeatures = list(df)\nfeatures = features[2:-1]\nx = pd.get_dummies(train_data[features])\ny = train_data['esrb_rating']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1)","485f540d":"#Baseline Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(max_iter = 500)\nlog_reg.fit(x_train, y_train)\ny_pred = log_reg.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)) + \"\\n\")\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","0b257ec0":"#Testing hyperparameters for logistic Regression\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000]}\n\nlog_reg2 = LogisticRegression(max_iter = 500)\nlog_reg_grid_search = GridSearchCV(log_reg2, param_grid = param_grid,  verbose = 1, n_jobs = -1)\nlog_reg_grid_search.fit(x_train, y_train)\n\nprint(\"Best parameters: \", log_reg_grid_search.best_params_)","6971f2e7":"#logistic regression with chosen hyperparameters\n\nlog_reg3 = LogisticRegression(C = 100, max_iter = 500)\nlog_reg3.fit(x_train, y_train)\ny_pred = log_reg3.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)) + \"\\n\")\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","5161508a":"#Baseline Support Vector Machine\nfrom sklearn.svm import SVC\n\nsupport = SVC()\nsupport.fit(x_train, y_train)\ny_pred = support.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","062ff31e":"#Testing hyperparameters for Support Vector Machine\n\nparam_grid = { 'C': [.01, 1, 10, 100, 1000], 'kernel':['rbf', 'poly', 'sigmoid', 'linear'],\n               'degree':[1, 2, 3, 4, 5, 6], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n\nsupport2 = SVC()\nsupport_grid_search = GridSearchCV(support2, param_grid = param_grid, verbose = 1,  n_jobs = -1)\nsupport_grid_search.fit(x_train, y_train)\n\nprint(\"Best parameters: \", support_grid_search.best_params_)","262eebf5":"#Support Vector Machine with chosen parameters\n\nsupport3 = SVC(C = 100, degree = 1, gamma = .1, kernel = 'rbf')\nsupport3.fit(x_train, y_train)\ny_pred = support3.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" +str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","b28046f1":"#baseline Decision Tree model\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\ny_pred = tree.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" +str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","1315e368":"#Testing hyperparameters for Decision Tree model\n\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'ccp_alpha': [0.1, .01, .001],\n              'max_depth' : [5, 6, 7, 8, 9],\n              'criterion' :['gini', 'entropy']}\n\ntree2 = DecisionTreeClassifier()\ntree_grid_search = GridSearchCV(estimator = tree2, param_grid = param_grid, verbose = 1,  n_jobs = -1)\ntree_grid_search.fit(x_train, y_train)\nprint(tree_grid_search.best_params_)","a3a26bfa":"#Decision Tree Model with chosen parameters\ntree3 = DecisionTreeClassifier(ccp_alpha = 0.001, criterion = 'entropy', max_depth = 9, max_features = 'auto')\ntree3.fit(x_train, y_train)\ny_pred = tree3.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","89bd65e2":"#baseline Random Forest model\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier()\nforest.fit(x_train, y_train)\ny_pred = forest.predict(x_test)\n\nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","cb2b4fcb":"#Testing hyperparameters for Random Forest Model\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 20)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 22)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10, 20]\nmin_samples_leaf = [1, 2, 4, 8, 16]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrforest = RandomForestClassifier()\nforest_random_search = RandomizedSearchCV(estimator = rforest, param_distributions = random_grid, n_iter = 10, verbose = 1, random_state = 42, n_jobs = -1)\nforest_random_search.fit(x_train, y_train)\n\nprint(\"\\nBest parameters: \")\nforest_random_search.best_params_","9a349734":"#I realized this cell was redundant and does effectively nothing but I'm too close to deadline to delete it and risk breaking my code so just ignore it lol\nparam_grid = {\n        'n_estimators': [600],\n        'min_samples_split': [10],\n        'min_samples_leaf': [1],\n        'max_features': ['sqrt'],\n        'max_depth': [40],\n        'bootstrap': [False] }\n\nforest2 = RandomForestClassifier()\nforest_grid_search = GridSearchCV(estimator = forest2, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\nforest_grid_search.fit(x_train, y_train)\nprint(forest_grid_search.best_params_)","62fef9c6":"#Random Forest Model with chosen parameters\n#I originally did many more iterations in my random search but then imported the notebook to kaggle and do not have time to rerun the full search,\n #so these parameters don't match the ones above\n    \nforest3 = RandomForestClassifier(bootstrap = False, max_depth = 100, max_features = 'sqrt',\n                                 min_samples_leaf = 1, min_samples_split = 10, n_estimators = 200)\nforest3.fit(x_train, y_train)\ny_pred = forest3.predict(x_test)\n\nprint(\"\\nMetrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)) + \"\\n\")\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","29a2e45b":"#baseline K Nearest Neighbors Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\n    \nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","2de71f8a":"#Testing parameters for K Nearest Neighbors Model\n\nparam_grid = {'n_neighbors': [int(x) for x in np.linspace(start = 3, stop = 25, num = 12)],\n              'weights': ['uniform', 'distance'],\n              'metric': ['euclidean', 'manhattan']}\n\nknn2 = KNeighborsClassifier()\nknn_grid_search = GridSearchCV(estimator = knn2, param_grid = param_grid, verbose = 1)\nknn_grid_search.fit(x_train, y_train)\nprint(knn_grid_search.best_params_)","eb256992":"#K Nearest Neighbors Model with chosen parameters\n\nknn3 = KNeighborsClassifier(n_neighbors = (9), weights = 'distance', metric = 'manhattan')\nknn3.fit(x_train, y_train)\ny_pred = knn3.predict(x_test)\n    \nprint(\"Metrics Accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred)))\nscores_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nscores_df.describe()","108af093":"#Random Forest model did the best, submitting that\n\nx_test = pd.get_dummies(eval_data[features])\npredictions = forest3.predict(x_test)\noutput = pd.DataFrame({'id': sample_data.id, 'esrb_Rating': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","634fdd45":"print(output)","7ee7ffec":"# Support Vector Machine","4c58b189":"# Check for missing data","5f039a44":"# Logistic Regression","238d88a4":"# Final Winner and Submission","84f6d1cb":"# Random Forest Model","82da571b":"No missing data.\nSince all the data is boolean, there won't be any outliers.\nThere is also little feature engineering that can be done to this dataset.","febd5790":"# prepping data for training","0e4870e9":"# K Nearest Neighbors Model","b182c9aa":"# Decision Tree Model"}}