{"cell_type":{"8f36bada":"code","a1b75cf5":"code","602edfda":"code","c03dddbc":"code","ded2de8a":"code","0a929a9f":"code","1f32d3c4":"code","0fca3b11":"code","4104fdb5":"code","3bbf4f42":"code","8122d2ec":"code","65a01353":"code","8d99bf9b":"markdown","964412aa":"markdown","c75d196e":"markdown","ff3721b5":"markdown","3095f3a1":"markdown","45949be1":"markdown","d2b6de97":"markdown","14e80489":"markdown","a88c7ef5":"markdown"},"source":{"8f36bada":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1b75cf5":"!pip install wavenet_vocoder","602edfda":"!pip install webrtcvad","c03dddbc":"import torch\nfrom tqdm import tqdm\nfrom wavenet_vocoder import builder\n\nimport random\nimport struct\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport librosa\nimport numpy as np\nimport webrtcvad\nfrom scipy.ndimage.morphology import binary_dilation\n\nimport soundfile as sf\nfrom scipy import signal\nfrom librosa.filters import mel\nfrom numpy.random import RandomState\nimport os\nimport pickle\n\nfrom scipy import signal\nfrom scipy.signal import get_window\nfrom librosa.filters import mel\nfrom numpy.random import RandomState","ded2de8a":"class DictWithDotNotation(dict):\n    \"\"\"\n    a dictionary that supports dot notation\n    as well as dictionary access notation\n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = DictWithDotNotation(value)\n            self[key] = value\n\n\nclass GetDictWithDotNotation(DictWithDotNotation):\n\n    def __init__(self, hp_dict):\n        super(DictWithDotNotation, self).__init__()\n\n        hp_dotdict = DictWithDotNotation(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n\n    __getattr__ = DictWithDotNotation.__getitem__\n    __setattr__ = DictWithDotNotation.__setitem__\n    __delattr__ = DictWithDotNotation.__delitem__\n","0a929a9f":"PROJECT_DIR = \"\"\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device type available = '{device}'\")\n\nhparam_dict = {\n\n    # genereal parameters\n    \"general\": {\n        # small error\n        \"small_err\": 1e-6,\n\n        \"is_training_mode\": True,\n        \"device\": device,\n        \"project_root\": PROJECT_DIR,\n\n    },\n\n    # path to the raw audio file\n    \"raw_audio\": {\n        \"raw_audio_path\": \"static\/raw_data\/wavs\",\n        \"train_spectrogram_path\": \"static\/spectrograms\/train\",\n        \"test_spectrogram_path\": \"static\/spectrograms\/test\",\n        \"train_percent\": .8,\n    },\n\n    ## Audio\n    # same audio settings to be used in the wavenet model to reconstruct the audio from mel-spectrogram\n    \"audio\": {\n        \"sampling_rate\": 16000,\n        # Number of spectrogram frames in a partial utterance\n        \"partials_n_frames\": 180,  # 1600 ms\n\n        \"n_fft\": 1024,  # 1024 seems to work well\n        \"hop_length\": 1024 \/\/ 4,  # n_fft\/4 seems to work better\n\n        \"mel_window_length\": 25,  # In milliseconds\n        \"mel_window_step\": 10,  # In milliseconds\n        \"mel_n_channels\": 80,\n\n    },\n\n    ## Voice Activation Detection\n    \"vad\": {\n        # Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n        # This sets the granularity of the VAD. Should not need to be changed.\n        \"vad_window_length\": 30,  # In milliseconds\n        # Number of frames to average together when performing the moving average smoothing.\n        # The larger this value, the larger the VAD variations must be to not get smoothed out.\n        \"vad_moving_average_width\": 8,\n        # Maximum number of consecutive silent frames a segment can have.\n        \"vad_max_silence_length\": 6,\n\n        ## Audio volume normalization\n        \"audio_norm_target_dBFS\": -30,\n        \"rate_partial_slices\": 1.3,\n        \"min_coverage\": 0.75,\n    },\n\n    \"m_wave_net\": {\n        \"gen\": {\n            \"best_model_path\": \"static\/model_chk_pts\/wavenet_model\/checkpoint_step001000000_ema.pth\"\n        },\n        \"hp\": {\n            # DO NOT CHANGE THESE HP\n            'name': \"wavenet_vocoder\",\n\n            # Convenient model builder\n            'builder': \"wavenet\",\n\n            # Input type:\n            # 1. raw [-1, 1]\n            # 2. mulaw [-1, 1]\n            # 3. mulaw-quantize [0, mu]\n            # If input_type is raw or mulaw, network assumes scalar input and\n            # discretized mixture of logistic distributions output, otherwise one-hot\n            # input and softmax output are assumed.\n            # **NOTE**: if you change the one of the two parameters below, you need to\n            # re-run preprocessing before training.\n            'input_type': \"raw\",\n            'quantize_channels': 65536,  # 65536 or 256\n\n            # Audio: these 4 items to be same as used to create mel out of audio\n            # 'sample_rate': 16000,\n            # 'fft_size': 1024,\n            # # shift can be specified by either hop_size or frame_shift_ms\n            # 'hop_size': 256,\n            # 'num_mels': 80,\n\n            # this is only valid for mulaw is True\n            'silence_threshold': 2,\n\n            'fmin': 125,\n            'fmax': 7600,\n            'frame_shift_ms': None,\n            'min_level_db': -100,\n            'ref_level_db': 20,\n            # whether to rescale waveform or not.\n            # Let x is an input waveform, rescaled waveform y is given by:\n            # y = x \/ np.abs(x).max() * rescaling_max\n            'rescaling': True,\n            'rescaling_max': 0.999,\n            # mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n            # happen depends on min_level_db and ref_level_db, causing clipping noise.\n            # If False, assertion is added to ensure no clipping happens.o0\n            'allow_clipping_in_normalization': True,\n\n            # Mixture of logistic distributions:\n            'log_scale_min': float(-32.23619130191664),\n\n            # Model:\n            # This should equal to `quantize_channels` if mu-law quantize enabled\n            # otherwise num_mixture * 3 (pi, mean, log_scale)\n            'out_channels': 10 * 3,\n            'layers': 24,\n            'stacks': 4,\n            'residual_channels': 512,\n            'gate_channels': 512,  # split into 2 gropus internally for gated activation\n            'skip_out_channels': 256,\n            'dropout': 1 - 0.95,\n            'kernel_size': 3,\n            # If True, apply weight normalization as same as DeepVoice3\n            'weight_normalization': True,\n            # Use legacy code or not. Default is True since we already provided a model\n            # based on the legacy code that can generate high-quality audio.\n            # Ref: https:\/\/github.com\/r9y9\/wavenet_vocoder\/pull\/73\n            'legacy': True,\n\n            # Local conditioning (set negative value to disable))\n            'cin_channels': 80,\n            # If True, use transposed convolutions to upsample conditional features,\n            # otherwise repeat features to adjust time resolution\n            'upsample_conditional_features': True,\n            # should np.prod(upsample_scales) == hop_size\n            'upsample_scales': [4, 4, 4, 4],\n            # Freq axis kernel size for upsampling network\n            'freq_axis_kernel_size': 3,\n\n            # Global conditioning (set negative value to disable)\n            # currently limited for speaker embedding\n            # this should only be enabled for multi-speaker dataset\n            'gin_channels': -1,  # i.e., speaker embedding dim\n            'n_speakers': -1,\n\n            # Data loader\n            'pin_memory': True,\n            'num_workers': 2,\n\n            # train\/test\n            # test size can be specified as portion or num samples\n            'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n            'test_num_samples': None,\n            'random_state': 1234,\n\n            # Loss\n\n            # Training:\n            'batch_size': 2,\n            'adam_beta1': 0.9,\n            'adam_beta2': 0.999,\n            'adam_eps': 1e-8,\n            'amsgrad': False,\n            'initial_learning_rate': 1e-3,\n            # see lrschedule.py for available lr_schedule\n            'lr_schedule': \"noam_learning_rate_decay\",\n            'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n            'nepochs': 2000,\n            'weight_decay': 0.0,\n            'clip_thresh': -1,\n            # max time steps can either be specified as sec or steps\n            # if both are None, then full audio samples are used in a batch\n            'max_time_sec': None,\n            'max_time_steps': 8000,\n            # Hold moving averaged parameters and use them for evaluation\n            'exponential_moving_average': True,\n            # averaged = decay * averaged + (1 - decay) * x\n            'ema_decay': 0.9999,\n\n            # Save\n            # per-step intervals\n            'checkpoint_interval': 10000,\n            'train_eval_interval': 10000,\n            # per-epoch interval\n            'test_eval_epoch_interval': 5,\n            'save_optimizer_state': True,\n\n            # Eval:\n        }\n\n    }\n}\n\n# this hp will be used throughout the project\nhp = GetDictWithDotNotation(hparam_dict)\n\n# few calculated values from wavenet model\nhp.m_wave_net.hp.sample_rate = hp.audio.sampling_rate\nhp.m_wave_net.hp.fft_size = hp.audio.n_fft\nhp.m_wave_net.hp.hop_size = hp.audio.hop_length\nhp.m_wave_net.hp.num_mels = hp.audio.mel_n_channels\n","1f32d3c4":"\n\nint16_max = (2 ** 15) - 1\n\n\ndef preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray], hp, source_sr: Optional[int] = None):\n    \"\"\"\n    Applies preprocessing operations to a waveform either on disk or in memory such that\n    The waveform will be resampled to match the data hyperparameters.\n\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not\n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before\n    preprocessing. After preprocessing, the waveform'speaker sampling rate will match the data\n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and\n    this argument will be ignored.\n    \"\"\"\n    # Load the wav from disk if needed\n    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n    else:\n        wav = fpath_or_wav\n\n    # Resample the wav\n    if source_sr is not None:\n        wav = librosa.resample(wav, source_sr, hp.audio.sampling_rate)\n\n    # Apply the preprocessing: normalize volume and shorten long silences\n    wav = normalize_volume(wav, hp.vad.audio_norm_target_dBFS, increase_only=True)\n    wav = trim_long_silences(wav, hp)\n\n    return wav\n\n\ndef trim_long_silences(wav, hp):\n    \"\"\"\n    Ensures that segments without voice in the waveform remain no longer than a\n    threshold determined by the VAD parameters in params.py.\n\n    :param wav: the raw waveform as a numpy array of floats\n    :return: the same waveform with silences trimmed away (length <= original wav length)\n    \"\"\"\n    # Compute the voice detection window size\n    samples_per_window = (hp.vad.vad_window_length * hp.audio.sampling_rate) \/\/ 1000\n\n    # Trim the end of the audio to have a multiple of the window size\n    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n\n    # Convert the float waveform to 16-bit mono PCM\n    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n\n    # Perform voice activation detection\n    voice_flags = []\n    vad = webrtcvad.Vad(mode=3)\n    for window_start in range(0, len(wav), samples_per_window):\n        window_end = window_start + samples_per_window\n        voice_flag = vad.is_speech(pcm_wave[window_start * 2:window_end * 2], sample_rate=hp.audio.sampling_rate)\n        voice_flags.append(voice_flag)\n\n    voice_flags = np.array(voice_flags)\n\n    # Smooth the voice detection with a moving average\n    def moving_average(array, width):\n        array_padded = np.concatenate((np.zeros((width - 1) \/\/ 2), array, np.zeros(width \/\/ 2)))\n        ret = np.cumsum(array_padded, dtype=float)\n        ret[width:] = ret[width:] - ret[:-width]\n        return ret[width - 1:] \/ width\n\n    audio_mask = moving_average(voice_flags, hp.vad.vad_moving_average_width)\n    audio_mask = np.round(audio_mask).astype(np.bool)\n\n    # Dilate the voiced regions\n    audio_mask = binary_dilation(audio_mask, np.ones(hp.vad.vad_max_silence_length + 1))\n    audio_mask = np.repeat(audio_mask, samples_per_window)\n\n    return wav[audio_mask == True]\n\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n    wave_dBFS = 20 * np.log10(rms \/ int16_max)\n    dBFS_change = target_dBFS - wave_dBFS\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return wav * (10 ** (dBFS_change \/ 20))\n\n\ndef compute_partial_slices(n_samples: int, hp):\n    \"\"\"\n    Computes where to split an utterance waveform and its corresponding mel spectrogram to\n    obtain partial utterances of <partials_n_frames> each. Both the waveform and the\n    mel spectrogram slices are returned, so as to make each partial utterance waveform\n    correspond to its spectrogram.\n\n    The returned ranges may be indexing further than the length of the waveform. It is\n    recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\n\n    :param n_samples: the number of samples in the waveform\n    :param rate: how many partial utterances should occur per second. Partial utterances must\n    cover the span of the entire utterance, thus the rate should not be lower than the inverse\n    of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n    the minimum rate is thus 0.625.\n    :param min_coverage: when reaching the last partial utterance, it may or may not have\n    enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n    then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n    it will be discarded. If there aren't enough frames for one partial utterance,\n    this parameter is ignored so that the function always returns at least one slice.\n    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n    respectively the waveform and the mel spectrogram with these slices to obtain the partial\n    utterances.\n    \"\"\"\n    assert 0 < hp.vad.min_coverage <= 1\n\n    # Compute how many frames separate two partial utterances\n    samples_per_frame = int((hp.audio.sampling_rate * hp.mel_fb.mel_window_step \/ 1000))\n    n_frames = int(np.ceil((n_samples + 1) \/ samples_per_frame))\n    frame_step = int(np.round((hp.audio.sampling_rate \/ hp.vad.rate_partial_slices) \/ samples_per_frame))\n\n    min_frame_step = (hp.audio.sampling_rate \/ (samples_per_frame * hp.audio.partials_n_frames))\n    assert 0 < frame_step, \"The rate is too high\"\n    assert frame_step <= hp.audio.partials_n_frames, \"The rate is too low, it should be %f at least\" % min_frame_step\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - hp.audio.partials_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + hp.audio.partials_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n\n    # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) \/ (last_wav_range.stop - last_wav_range.start)\n    if coverage < hp.vad.min_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n\ndef pySTFT(x, fft_length=1024, hop_length=256):\n    \"\"\"\n    this function returns spectrogram\n    :param x: np array for the audio file\n    :param fft_length: fft length for fast fourier transform (https:\/\/www.youtube.com\/watch?v=E8HeD-MUrjY)\n    :param hop_length: hop_lenght is the sliding overlapping window size normally fft\/\/4 works the best\n    :return: spectrogram in the form of np array\n    \"\"\"\n    x = np.pad(x, int(fft_length \/\/ 2), mode='reflect')\n\n    noverlap = fft_length - hop_length\n    shape = x.shape[:-1] + ((x.shape[-1] - noverlap) \/\/ hop_length, fft_length)\n    strides = x.strides[:-1] + (hop_length * x.strides[-1], x.strides[-1])\n    result = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n\n    fft_window = get_window('hann', fft_length, fftbins=True)\n    result = np.fft.rfft(fft_window * result, n=fft_length).T\n\n    return np.abs(result)\n\n\ndef butter_highpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff \/ nyq\n    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n    return b, a\n\n\ndef wav_to_mel_spectrogram(wav, hp):\n    \"\"\"\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    \"\"\"\n\n    # creating mel basis matrix\n    mel_basis = mel(hp.audio.sampling_rate,\n                    hp.audio.n_fft,\n                    fmin=90,\n                    fmax=7600,\n                    n_mels=hp.audio.mel_n_channels).T\n\n    min_level = np.exp(-100 \/ 20 * np.log(10))\n\n    # getting audio as a np array\n    pp_wav = preprocess_wav(wav, hp)\n\n    # Compute spect\n    spectrogram = pySTFT(pp_wav).T\n    # Convert to mel and normalize\n    mel_spect = np.dot(spectrogram, mel_basis)\n    D_db = 20 * np.log10(np.maximum(min_level, mel_spect)) - 16\n    norm_mel_spect = np.clip((D_db + 100) \/ 100, 0, 1)\n\n    return norm_mel_spect\n\n\ndef shuffle_along_axis(a, axis):\n    \"\"\"\n    :param a: nd array e.g. [40, 180, 80]\n    :param axis: array axis. e.g. 0\n    :return: a shuffled np array along the given axis\n    \"\"\"\n    idx = np.random.rand(*a.shape).argsort(axis=axis)\n    return np.take_along_axis(a, idx, axis=axis)\n","0fca3b11":"torch.set_num_threads(4)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n\nhparams = hp.m_wave_net.hp\n\ndef build_model():\n    model = getattr(builder, hparams.builder)(\n        out_channels=hparams.out_channels,\n        layers=hparams.layers,\n        stacks=hparams.stacks,\n        residual_channels=hparams.residual_channels,\n        gate_channels=hparams.gate_channels,\n        skip_out_channels=hparams.skip_out_channels,\n        cin_channels=hparams.cin_channels,\n        gin_channels=hparams.gin_channels,\n        weight_normalization=hparams.weight_normalization,\n        n_speakers=hparams.n_speakers,\n        dropout=hparams.dropout,\n        kernel_size=hparams.kernel_size,\n        upsample_conditional_features=hparams.upsample_conditional_features,\n        upsample_scales=hparams.upsample_scales,\n        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n        scalar_input=True,\n        legacy=hparams.legacy,\n    )\n    return model\n\n\ndef wavegen(model, c=None, tqdm=tqdm):\n    \"\"\"Generate waveform samples by WaveNet.\n    \n    \"\"\"\n\n    model.eval()\n    model.make_generation_fast_()\n\n    Tc = c.shape[0]\n    upsample_factor = hparams.hop_size\n    # Overwrite length according to feature size\n    length = Tc * upsample_factor\n\n    # B x C x T\n    c = torch.FloatTensor(c.T).unsqueeze(0)\n\n    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n\n    # Transform data to GPU\n    initial_input = initial_input.to(device)\n    c = None if c is None else c.to(device)\n\n    with torch.no_grad():\n        y_hat = model.incremental_forward(\n            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n            log_scale_min=hparams.log_scale_min)\n\n    y_hat = y_hat.view(-1).cpu().data.numpy()\n\n    return y_hat\n\nmodel = build_model().to(device)","4104fdb5":"c = \"..\/input\/wave-net-model-ckk-pt\/checkpoint_step001000000_ema.pth\"\ncheckpoint = torch.load(c, map_location=device)\nmodel.load_state_dict(checkpoint[\"state_dict\"])","3bbf4f42":"import soundfile as sf\n\nwav_path = \"..\/input\/wave-net-model-ckk-pt\/Donald_trumph_01.mp3\"\nmel = wav_to_mel_spectrogram(wav_path, hp)\nmel.shape","8122d2ec":"# slicing the audio to smaller portion, you can use the whole audio mel as well\nc = mel[:128, :]\nc.shape","65a01353":"waveform = wavegen(model, c=c)\nsf.write('reconstructed_audio.wav', waveform, 16000, 'PCM_24')","8d99bf9b":"# Convert mel-spect to audio and save the file","964412aa":"# How to convert an audio file to mel-spectrogram and then back to an audio file\n**Following steps are followed**\n* Read an audio file\n* Convert audio file to spectrogram\n* Convert spectrogram to mel-spectrogram [read here for details](https:\/\/towardsdatascience.com\/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8)\n* using the mel-spectrogram *one can run multiple analysis e.g. Voice cloning*\n* Using cloned voice, create a new audio file using **Wavenet** pretrained model\n* In this notebook analysis is skipped and audio is created using original audio\n* For End-to-End solution of Auto Voice Cloning refer to [this](https:\/\/github.com\/gkv856\/end2end_auto_voice_conversion)","c75d196e":"# Building Wavenet model\n**Wavenet model is used to convert mel-spectrogram to audio file**","ff3721b5":"# Audio to mel-spectrogram","3095f3a1":"# Loading pre-trained wavenet model","45949be1":"# Creating a class to access dictionary as a dot notation","d2b6de97":"# Imports","14e80489":"# Installing required dependencies","a88c7ef5":"# Audio utility functions"}}