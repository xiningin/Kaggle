{"cell_type":{"142c43c8":"code","1a3487f2":"code","6e04f6e5":"code","bde021fc":"code","b574ff4c":"code","7129c427":"code","b4443e0a":"code","622e6be0":"code","7c81e44c":"code","8905028c":"code","f1b0b26d":"code","6106bd58":"code","36c155ca":"code","5b0e78ea":"code","fcb47ee6":"code","db016e6f":"code","0d5dd94f":"code","e29dba8b":"code","a8be44ea":"code","7cb13d96":"code","a0514778":"code","d9042aa6":"code","63d52a4b":"code","d8b46bc5":"code","c84941ff":"code","19f1981c":"code","64f869eb":"code","725ba7e5":"code","93a962ee":"code","5e1358a0":"code","c6848f57":"code","3014205f":"code","dfb79350":"code","812fa262":"code","f4f922f4":"code","5e3e9be8":"code","d813a1e1":"code","6fa65e60":"code","1f5a45f4":"code","63b19e10":"code","919094d1":"code","97960c63":"code","3ba90943":"code","82f43320":"code","5323da51":"code","f8ddb2b5":"code","bcba9e99":"code","2fbebd94":"code","46c81a35":"code","ea917fdd":"code","e6dfdfb5":"code","b5646fe6":"code","1bf2196b":"code","e4c7929f":"code","51317c5c":"code","9d8591cb":"markdown","2a4fc022":"markdown","f40952a5":"markdown","8abfab87":"markdown","44e56a2f":"markdown","0216026e":"markdown","8c571529":"markdown","2ed9e619":"markdown","a7d91539":"markdown","6001301b":"markdown","7b6dfb8d":"markdown","b8361da5":"markdown","b3edef6c":"markdown","0286ac2c":"markdown","32781764":"markdown","39e5132d":"markdown","8d0968ea":"markdown","ffec6a54":"markdown","7b463246":"markdown","05f4ac50":"markdown","170cdeca":"markdown"},"source":{"142c43c8":"!pip install tensor-sensor[torch]","1a3487f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tsensor\nfrom transformers import pipeline\nfrom matplotlib import pyplot as plt","6e04f6e5":"%config InlineBackend.figure_format='retina'","bde021fc":"unmasker = pipeline(\"fill-mask\", model=\"roberta-base\")","b574ff4c":"unmasker.tokenizer.mask_token","7129c427":"unmasker(\"I don't like ball sports, I prefer <mask>\")","b4443e0a":"unmasker(\"I don't like <mask> sports, I prefer swimming\")","622e6be0":"unmasker(\"It's late so I'm going to <mask>\")","7c81e44c":"generator = pipeline(\n    'text-generation', \n    #model='EleutherAI\/gpt-neo-2.7B', # mejores resultados pero no cabe en la RAM de kaggle\n    #model='EleutherAI\/gpt-neo-1.3B',\n    model='gpt2', # 117M par\u00e1metros\n    device=0,\n)","8905028c":"generator('Once upon a time there was a little girl named Little Red Riding Hood.')","f1b0b26d":"generator(\"I don't like ball sports, I prefer\")","6106bd58":"generator(\"I don't like ball sports, I prefer\", \n    max_length = 12,\n    num_return_sequences=5,\n)","36c155ca":"generator(\"In a place of La Mancha, whose name I don't want to remember\")","5b0e78ea":"qa = pipeline(\n    \"question-answering\", \n    'bert-large-uncased-whole-word-masking-finetuned-squad',\n    device=0\n)","fcb47ee6":"text_en = r\"\"\"\nThe respiratory system is made up of the airways and the lungs.\n\nThe airways are a set of tubes through which air enters and leaves. \nThey are the nostrils, pharynx, trachea and bronchial tubes. \nThe latter branch out into increasingly thin tubes until they end up \nin millions of tiny sacs called alveoli.\n\nThe lungs are two organs formed by the branches of the bronchial \ntubes and by the alveoli.\n\nThis is how our respiratory system works:\n\nThe process of respiration consists of introducing air into the lungs, \nexchanging oxygen and carbon dioxide with the blood, and expelling air outside.\n\nThe entry of air into the lungs; The inspiration:\n\nBeneath the lungs is a muscle called the diaphragm. When this muscle \ncontracts, it widens the lungs and air enters through the airways to \nthe alveoli.\n\nThe exchange of oxygen and carbon dioxide:\n\nIn the alveoli, oxygen from the air passes into the blood. \n\nAt the same time, the carbon dioxide that we produce as waste and that \nis in the blood passes into the alveolus.\n\nThe expulsion of air to the outside; expiration:\n\nThe diaphragm relaxes, the lungs narrow, and the carbon dioxide-laden air\nfrom the alveoli is pushed out through the airways.\n\"\"\"\n\nquestions_en = [\n    #\"Name in an orderly manner the different airways that carbon dioxide-rich air passes through from the moment it leaves the alveolus until it is expelled outside.\",\n    \"What are the airways that carbon dioxide-rich air passes through from the moment it leaves the alveolus until it is expelled outside in an orderly manner?\",\n    \"Explain how gas exchange occurs between air and blood during respiration.\",\n    \"What are the alveoli?\",\n    \"Explain what role the diaphragm plays during inspiration.\",\n    \"Explain what role the diaphragm plays during exhalation.\",\n]\n","db016e6f":"qa(context=text_en, question=questions_en[4])","0d5dd94f":"text_es = r\"\"\"\nEl aparato respiratorio est\u00e1 formado por las v\u00edas respiratorias y por los pulmones.\n\nLas v\u00edas respiratorias son un conjunto de tubos por los que el aire entra y sale. Son las fosas nasales, la faringe, la traquea y los bronquis. Estos \u00faltimos se ramifican en tubos cada vez m\u00e1s finos hasta acabar en millones de diminutos sacos llamados alv\u00e9olos.\n\nLos pulmones son dos \u00f3rganos formados por las ramificaciones de los bronquios y por los alv\u00e9olos.\n\nAs\u00ed funciona nuestro aparato respiratorio:\n\nEl proceso de la respiraci\u00f3n consiste en introducir aire en los pulmones, intercambiar ox\u00edgeno y di\u00f3xido de carbono con la sangre y expulsar aire al exterior.\n\nLa entrada de aire en los pulmones; la inspiraci\u00f3n:\n\nBajo los pulmones hay un m\u00fasculo llamado diafragma. Cuando este m\u00fasculo se contrae, ensancha los pulmones y el aire entra por las v\u00edas respiratorias hasta los alv\u00e9olos.\n\nEl intercabio de ox\u00edgeo y di\u00f3xido de carbono:\n\nEn los alv\u00e9olos, el ox\u00edgeno del aire pasa a la sangre. Al mismo tiempo, el di\u00f3xido de carbono que producimos como desencho y que est\u00e1 en la sange pasa al alv\u00e9olo.\n\nLa expulsi\u00f3n del aire al exterior; la espiraci\u00f3n:\n\nEl diafragma se relaja, los pulmones se estrechan y el aire cargado de di\u00f3xido de carbono de los alv\u00e9olos se expulsa al exterior a traves de las v\u00edas respiratorias.\n\"\"\"\n\nquestions_es = [\n    \"Nombra de manera ordenada las diferentes v\u00edas respiratorias que atraviesa el aire rico en di\u00f3xido de carbono desde que sale del alv\u00e9olo hasta que es expulsado al exterior.\",\n    \"Explica c\u00f3mo se produce el intercambio de gases entre el aire y la sangre durante la respiraci\u00f3n.\",\n    \"\u00bfQu\u00e9 son los alv\u00e9olos?\",\n    \"Explica que papel desempe\u00f1a el diafragma durante la inspiraci\u00f3n.\",\n    \"Explica qu\u00e9 papel desempe\u00f1a el diafragma durante la espiraci\u00f3n.\",\n]","e29dba8b":"unmasker.tokenizer","a8be44ea":"unmasker.tokenizer.mask_token","7cb13d96":"tokens = unmasker.tokenizer.encode(\"It's late so I'm going to <mask>\")\ntokens","a0514778":"unmasker.tokenizer.decode(tokens)","d9042aa6":"import torch","63d52a4b":"x = torch.LongTensor(tokens) # convertimos a tensor\nx = x.reshape(1, -1) # le a\u00f1adimos la dimensi\u00f3n del batch\nx","d8b46bc5":"with tsensor.explain(fontname='Ubuntu Mono'): \n    x","c84941ff":"unmasker.model","19f1981c":"embs = unmasker.model.roberta.embeddings(x)\nembs","64f869eb":"with tsensor.explain(fontname='Ubuntu Mono'):\n    embs","725ba7e5":"tokens = unmasker.tokenizer.encode(\"The dog ate the cat\")\nembs = unmasker.model.roberta.embeddings(torch.LongTensor(tokens).unsqueeze(0))\nembs # <s>, The, dog, ..., the, cat, <\/s>","93a962ee":"tokens = unmasker.tokenizer.encode(\"The cat ate the dog\")\nembs = unmasker.model.roberta.embeddings(torch.LongTensor(tokens).unsqueeze(0))\nembs # <s>, The, cat, ..., the, dog, <\/s>","5e1358a0":"embs = unmasker.model.roberta.embeddings.word_embeddings(torch.LongTensor(tokens).unsqueeze(0))\nembs # <s>, The, cat, ..., the, dog, <\/s>","c6848f57":"words = [ unmasker.tokenizer.decode(token) for token in tokens ]\nwords","3014205f":"fig, ax = plt.subplots(figsize=(6,6))\ncorr = torch.matmul(embs[0], embs[0].T).detach().numpy()\nim = plt.imshow(corr)\nax.set_xticks(np.arange(len(words)))\nax.set_yticks(np.arange(len(words)))\nax.set_xticklabels(words)\nax.set_yticklabels(words)\n# Loop over data dimensions and create text annotations.\nfor i in range(len(words)):\n    for j in range(len(words)):\n        text = ax.text(j, i, f'{corr[i, j]:.1f}', ha=\"center\", va=\"center\", color=\"w\", fontsize=12)\nfig.tight_layout()\nplt.show()","dfb79350":"tokens = unmasker.tokenizer.encode(\"It's late so I'm going to <mask>\")\nx = torch.LongTensor(tokens) # convertimos a tensor\nx = x.reshape(1, -1) # le a\u00f1adimos la dimensi\u00f3n del batch\nunmasker.tokenizer.decode(x[0])","812fa262":"y = unmasker.model(x, output_hidden_states=True, output_attentions=True)\ny.keys()","f4f922f4":"explain = lambda: tsensor.explain(fontname='Ubuntu Mono')","5e3e9be8":"len(y.hidden_states)","d813a1e1":"# Los embeddings:\ny.hidden_states[0]","6fa65e60":"# \u00daltima capa del transformer:\ny.hidden_states[12]","1f5a45f4":"# Vamos a ver qu\u00e9 dimensiones tienen:\nwith explain():\n    (y.hidden_states[0], y.hidden_states[12])","63b19e10":"with explain():\n    y.logits","919094d1":"y_tokens = y.logits.argmax(-1)[0]\ny_tokens","97960c63":"unmasker.tokenizer.decode(y_tokens)","3ba90943":"y_probs = y.logits.softmax(-1)","82f43320":"plt.plot(y_probs[0,-2].detach().numpy())","5323da51":"y_probs[0,-2].argmax()","f8ddb2b5":"y_probs[0,-2,3267]","bcba9e99":"y_probs[0,-2].topk(5)","2fbebd94":"for tok in y_probs[0,-2].topk(5).indices:\n    print(unmasker.tokenizer.decode(tok))","46c81a35":"len(y.attentions)","ea917fdd":"with explain():\n    y.attentions[0]","e6dfdfb5":"# Dimensiones: (batch_size, num_heads, sequence_length, sequence_length)\ny.attentions[0].shape","b5646fe6":"# Vamos a inspeccoinar la matriz de atenci\u00f3n de la primera capa (el primer 0 indexa el lote, el segundo)\ny.attentions[0][0,0]","1bf2196b":"def plot_attention(a, words):\n    a = a.detach().numpy()\n    fig, ax = plt.subplots(figsize=(6,6))\n    im = plt.imshow(a)\n    ax.set_xticks(np.arange(len(words)))\n    ax.set_yticks(np.arange(len(words)))\n    ax.set_xticklabels(words)\n    ax.set_yticklabels(words)\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(words)):\n        for j in range(len(words)):\n            text = ax.text(j, i, f'{a[i, j]:.2f}', ha=\"center\", va=\"center\", color=\"w\", fontsize=12)\n    fig.tight_layout()\n    plt.show()","e4c7929f":"words = [ unmasker.tokenizer.decode(token) for token in tokens ]\nwords","51317c5c":"# cabeza 0: parece que intenta completar el significado de las contracciones\n# cabeza 5: sintaxis?\n# cabeza 9: se fija en la palabra siguiente\n# cabeza 10: ???\n# cabeza 11: se fija en la palabra anterior\nplot_attention(y.attentions[0][0,10], words)","9d8591cb":"## Estados ocultos (salidas de las capas del transformer)\nEn `y.hidden_states` hay 13 tensores con las salidas de cada una de las capas intermedias del modelo: la primera son los embeddings (`y.hidden_states[0]`) y las 12 siguientes corresponden a cada una de las capas del transformer:","2a4fc022":"La matriz de embeddings es un **par\u00e1metro** del modelo, por lo que sus valores no est\u00e1n predefinidos (se inicializan de forma aleatoria) sino que se aprenden durante el entrenamiento en funci\u00f3n de la tarea. Para un modelo MLM, podemos conjeturar que los embeddings almacenan informaci\u00f3n sobre:\n\n* Significados\n* Funciones sint\u00e1cticas de la palabra (sustantivo, adverbio, preposici\u00f3n, etc.)\n* Tiempo verbal\n* G\u00e9nero\n* N\u00famero\n* etc.\n\nEn el siguiente ejemplo podemos ver como, de todas las palabras de la frase, la palabra `cat` guarda mayor relaci\u00f3n con `dog`.\n\nVamos a sacar los embeddings de las palabras (sin la codificaci\u00f3n posicional):","f40952a5":"## Entrada al modelo: los embeddings\nLos embeddings ([nn.Embedding](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Embedding.html)) traducen una variable categ\u00f3rica (ej. el `id` de una palabra) a un vector continuo. La traducci\u00f3n consiste sencillamente en indexar con el `id` de cada palabra en una matriz de dimensiones `n` x `d` donde `n` es el n\u00famero de palabras del vocabulario y `d` es la dimensi\u00f3n de vector.","8abfab87":"# Los modelos por dentro","44e56a2f":"## Tarea: generaci\u00f3n de texto (`text-generation`)\nEn este caso el modelo se entrena pas\u00e1ndole un texto y se le pide que prediga la siguiente palabras. Tambi\u00e9n se entrenar de forma no supervisada (no hay nada que etiquetar). A diferencia del modelo MLM, los modelos generativos solo pueden ver el texto que antecede a la palabra a predecir, por ello tambi\u00e9n a veces se les llama modelos **autoregresivos** o **causales**.\n\nEjemplos de modelos: todos los GPT.","0216026e":"## El modelo","8c571529":"# Ejercicios\n* \u00bfQu\u00e9 probabilidad tiene asociada la palabra \"bed\"? (pista [torch.softmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Softmax.html))\n* \u00bfCu\u00e1les son las siguientes palabras a \"bed\" en orden de probabilidad? (pista: [torch.topk](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.topk.html))\n* \u00bfQu\u00e9 longitud tiene el vocabulario del tokenizador de BERT?\n* Muestra algunas palabras del vocabulario\n* Probar a tokenizar la palabra \"Sunday\" o \"Monday\". \u00bfCu\u00e1ntos tokens salen? \u00bfPor qu\u00e9? \u00bfC\u00f3mo funciona el tokenizador? \u00bfQu\u00e9 pasa si se encuentra palabras que no existe? Probar con nombres propios o palabras en castellano, con tildes o con \u00f1.\n* ","2ed9e619":"## El tokenizador","a7d91539":"# Soluciones","6001301b":"## Tarea: rellenar huecos (`fill-mask`)\nEs una tarea que se puede entrenar de forma no supervisada: durante el entrenamiento se le pasa a la red un texto escondi\u00e9ndole palabras (por defecto con p=0.15), y se le pide a la red que prediga las palabras enmascaradas. Para predecir las palabras enmascaradas, el modelo puede ver **todo** el texto, tanto el anterior a la palabra oculta como el posterior.\n\nAl modelo entrenado con esta tarea se le llama _Masked Language Model_ (MLM). \n\nAdem\u00e1s de la tarea MLM, a algunos modelos, como BERT, se les entrena simult\u00e1neamente para predecir si dos fragmentos de texto son consecutivos o no _(Next Sentence Prediction)_ o NSP.\n\nEjemplos de arquitecturas dise\u00f1adas para esta tarea: BERT, RoBERTa, DeBERTA, XLNet, etc.\n\nPaper de BERT: https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n\n\nDiferencias BERT vs RoBERTa:\n![image.png](attachment:c85eb5a2-5519-4b5a-90d9-dbad0021fa0a.png)\n\nGLUE: \u00bfqu\u00e9 es GLUE? \u00bfC\u00f3mo se eval\u00faan los modelos de lenguaje? https:\/\/mccormickml.com\/2019\/11\/05\/GLUE\/","7b6dfb8d":"## Tarea: preguntas y respuestas (`question-answering`)\n\nLe pasamos un texto y una pregunta y nos devuelve el fragmento del texto que mejor responde a la pregunta. Ejemplo:\n\nhttps:\/\/twitter.com\/info_libertas\/status\/1393321710368198662\n\n\n\nEn hugging face hay modelos entrenados con el dataset SQUAD 2.0: https:\/\/rajpurkar.github.io\/SQuAD-explorer\/, que contiene 100 000 preguntas con sus respuestas y 50 000 ejemplos adversarios de preguntas sin respuestas cuya finalidad es dar la opci\u00f3n al modelo de no responder si no sabe la respuesta.","b8361da5":"El valor m\u00e1ximo en la \u00faltima dimensi\u00f3n indica qu\u00e9 palabra de las 50265 del vocabulario tiene la mayor probabilidad de ir en cada una de las 11 posiciones.","b3edef6c":"# Tareas pre-entrenadas en Hugging \ud83e\udd17 Face\n- Pipelines disponibles: https:\/\/huggingface.co\/transformers\/main_classes\/pipelines.html\n- Modelos pre-entrenados: https:\/\/huggingface.co\/transformers\/pretrained_models.html","0286ac2c":"# Ejercicios\n* Probar alguno de los otros pipelines disponibles en https:\/\/huggingface.co\/transformers\/main_classes\/pipelines.html, por ejemplo:\n    * `SummarizationPipeline` (para resumir textos)\n    * `ConversationalPipeline`\n    * `TextClassificationPipeline` (an\u00e1lisis de sentimientos)\n* [Buscar](https:\/\/huggingface.co\/models?filter=es&pipeline_tag=question-answering) alg\u00fan modelo de preguntas y respuestas pre-entrenado en **castellano** y hacer la prueba con el texto y las pregutnas en castellano (ver m\u00e1s abajo)\n* Test de Turing (para hacer en parejas): \u00bfpuedes discriminar texto sint\u00e9tico generado por ordenador del texto escrito por un humano? Una persona escoge un fragmento de texto, genera su continuaci\u00f3n con GPT-x (cuanto m\u00e1s grande el modelo, mejor ser\u00e1 el resultado) y le pasa a la otra: 1) el texto original, 2) la continuaci\u00f3n original, 3) la continuaci\u00f3n generada por GPT y debe intentar diferenciar el texto original del sint\u00e9tico.","32781764":"# El mecanismo de atenci\u00f3n\n\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V $$\n\nV\u00eddeo explicativo de BERT con visualizaciones muy buenas, tambi\u00e9n explica el mecanismo de atenci\u00f3n: https:\/\/www.youtube.com\/watch?v=-9vVhYEXeyQ","39e5132d":"## Salida del modelo","8d0968ea":"* \u00bfCu\u00e1les son las siguientes palabras a \"bed\" en orden de probabilidad? (pista: [torch.topk](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.topk.html))","ffec6a54":"El resultado de `unmasker.model.roberta.embeddings` codifica no solo las palabras sino la posici\u00f3n:","7b463246":"## Salida de la capa clasificadora\nA la salida de la capa clasificadora tenemos los _logits_, que son los valores que salen de la \u00faltima capa `nn.Linear` antes de pasar por la funci\u00f3n softmax:","05f4ac50":"## \u00bfPodemos visualizar el contenido de las matrices de atenci\u00f3n?\nLas matrices de atenci\u00f3n son el resultado de $\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})$, nos permiten ver en qu\u00e9 se est\u00e1 fijando el transformer para construir cada elemento de la secuencia de salida a partir de los elementos de la secuencia de entrada.","170cdeca":"* \u00bfQu\u00e9 probabilidad tiene asociada la palabra \"bed\"? (pista [torch.softmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Softmax.html))* \u00bfQu\u00e9 probabilidad tiene asociada la palabra \"bed\"? (pista [torch.softmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Softmax.html))"}}