{"cell_type":{"ed257059":"code","fa0d3adc":"code","c15e75bb":"code","4d542390":"code","e1f66797":"code","caa114cd":"code","2db04c71":"code","8131e796":"code","11cff7d5":"code","450721ea":"code","6f677468":"code","08d535a4":"code","498966c5":"code","3d3d19e8":"code","da4a087e":"markdown","6cd7938b":"markdown","323507d6":"markdown","eda30d15":"markdown"},"source":{"ed257059":"import pandas as pd, numpy as np, tensorflow as tf\nfrom keras.preprocessing import text, sequence","fa0d3adc":"TEXT_COL = 'comment_text'\nBATCH_SIZE = 512\nMAX_LEN = 220","c15e75bb":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","4d542390":"df = None","e1f66797":"def preprocess(data):\n    '''\n    Credit goes to https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n    '''\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","caa114cd":"x_train = preprocess(train[TEXT_COL])\ny_train = np.where(train['target'] >= 0.5, 1, 0)\nx_test = preprocess(test[TEXT_COL])","2db04c71":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train_seq = tokenizer.texts_to_sequences(x_train)\nx_test_seq = tokenizer.texts_to_sequences(x_test)","8131e796":"x_train = pd.DataFrame.from_dict({\n    'text': x_train,\n    'as_numbers': x_train_seq\n})","11cff7d5":"x_train['length'] = x_train.as_numbers.str.len()\nx_train['target'] = y_train","450721ea":"x_train.head()","6f677468":"class BucketedDataIterator():\n    def __init__(self, df, num_buckets = 5):\n        df = df.sort_values('length').reset_index(drop=True)\n        self.size = len(df) \/ num_buckets\n        self.dfs = []\n        for bucket in range(num_buckets):\n            self.dfs.append(df.loc[bucket*self.size: (bucket+1)*self.size - 1])\n        self.num_buckets = num_buckets\n\n        # cursor[i] will be the cursor for the ith bucket\n        self.cursor = np.array([0] * num_buckets)\n        self.shuffle()\n\n        self.epochs = 0\n\n    def shuffle(self):\n        #sorts dataframe by sequence length, but keeps it random within the same length\n        for i in range(self.num_buckets):\n            self.dfs[i] = self.dfs[i].sample(frac=1).reset_index(drop=True)\n            self.cursor[i] = 0\n\n    def next_batch(self, n):\n        if np.any(self.cursor+n+1 > self.size):\n            self.epochs += 1\n            self.shuffle()\n\n        i = np.random.randint(0,self.num_buckets)\n\n        res = self.dfs[i].loc[self.cursor[i]:self.cursor[i]+n-1]\n        self.cursor[i] += n\n\n        # Pad sequences with 0s so they are all the same length\n        maxlen = max(res['length'])\n        x = np.zeros([n, maxlen], dtype=np.int32)\n        for i, x_i in enumerate(x):\n            x_i[:res['length'].values[i]] = res['as_numbers'].values[i]\n\n        return x, res['target'], res['length']","08d535a4":"tr = BucketedDataIterator(x_train, 5)\npadding = 0\nfor i in range(100):\n    lengths = tr.next_batch(BATCH_SIZE)[2].values\n    max_len = max(lengths)\n    padding += np.sum(max_len - lengths)\nprint(\"Average padding with bucketing:\", padding\/(BATCH_SIZE*100))","498966c5":"#If x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN) is used then the padding results will be\nprint(\"Average padding without bucketing :\", np.sum(MAX_LEN - x_train['length'])\/len(x_train))","3d3d19e8":"#Sample usage to extract batch for training\nbatch = tr.next_batch(BATCH_SIZE)\nx = batch[0]\ny = batch[1]","da4a087e":"**Bucketing** - A technique that can significantly improve model's training time.\n<br>Idea is to pad sequences on the batch level instead of padding sequences on the whole data.","6cd7938b":"Reference : [Variable length sequences](https:\/\/r2rt.com\/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html)","323507d6":"Inspiration -> [Quora insincere questions classification discussion](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568#latest-516532)","eda30d15":"**Bucketing**\n<br>Excerpt from the reference sample itself\n<br>\n> The key point to keep in mind is that we should not \u201cbias\u201d the order in which different sequence lengths are sampled any more than necessary to achieve bucketing. E.g., sorting our data by sequence length might seem like a good solution, but then each epoch would be trained on short sequences before longer sequences, which could harm results. Here is one solution, which uses a predetermined batch_size:"}}