{"cell_type":{"f5a558cb":"code","0c4323c8":"code","3cf0756f":"code","41d81219":"code","cb02e818":"code","5eab0df6":"code","9f667dde":"code","5e0f6ffb":"code","dfe867c8":"code","d1380cb4":"code","92402fe1":"code","7826e5a5":"code","9902b2da":"code","05b33f18":"code","829c0bad":"code","c600ed24":"code","515a8913":"code","7f9a6aa9":"code","23f25fde":"code","a01b34e6":"code","e9a4d9ff":"code","0a8fcbdd":"code","71d1d07a":"markdown","53ef2ddb":"markdown","7be8f5f4":"markdown","c42c96f0":"markdown","14173243":"markdown","a456aba0":"markdown","def23d95":"markdown","2b885a8b":"markdown","3768af17":"markdown","d68c5ab1":"markdown","2b47acd4":"markdown","8f899cf7":"markdown","80659aa7":"markdown","bf2a297f":"markdown","97545f6a":"markdown","7239cb4a":"markdown"},"source":{"f5a558cb":"ABELVALIDATIONSIZE = 4000\nSCIBERTDATASETNAME = \"uniquevalidation-10neg-1pos\"\n\nvalidation_version = True\n","0c4323c8":"# Install offline libraries\n!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl\n","3cf0756f":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","41d81219":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\n# Load and display the train data from train.csv\nunique = True\n\nif unique:\n    train = pd.read_csv(train_path)\n    list_unique = ['Baccalaureate and Beyond','National Assessment of Education Progress','World Ocean Database','Survey of Industrial Research and Development','Survey of Doctorate Recipients',\"COVID-19 Deaths data\",\"The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)\",'National Assessment of Education Progress','National Teacher and Principal Survey','Beginning Postsecondary Student','Rural-Urban Continuum Codes', 'NOAA Tide Gauge','National Education Longitudinal Study'] \n   \n    boolean_series_test = train.dataset_title.isin(list_unique)\n    boolean_series_train = ~train.dataset_title.isin(list_unique)\n    test = train[boolean_series_test]\n    train = train[boolean_series_train]\nelse:\n    train = pd.read_csv(train_path)\n\n#display(train)\n#display(test)\n\n# Load all the train papers into a dictionary\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\npapers = {}\nfor paper_id in tqdm(train['Id'].unique(), ascii=True, desc=\"Loading papers\"):\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","cb02e818":"# load and display sample submission\nsample_submission_path = '..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\ndisplay(sample_submission)\n\n# load the test data\nif validation_version:\n    paper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n    papers_test = {}\n    for paper_id in tqdm(test['Id'].unique(), ascii=True, desc=\"Loading papers\"):\n        with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n            papers_test[paper_id] = paper\n    sample_submission = test\nelse:\n    paper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\n    papers_test = {}\n    for paper_id in sample_submission['Id']:\n        with open(f'{paper_test_folder}\/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n            papers_test[paper_id] = paper","5eab0df6":"all_labels = set() # all dataset names\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')\nif len(all_labels) > 5:\n    print(\"Examples:\")\n    print(list(dict.fromkeys(all_labels))[:10])","9f667dde":"# Cleaning text by removing all but single spaces and lowercase characters\/letters\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt","5e0f6ffb":"# loop through all papers, lowercase\/clean the text and match it with the set of datasets\nliteral_preds = []\n\nfor paper_id in tqdm(sample_submission['Id'], ascii=True, desc=\"matching papers\"):\n    paper = papers_test[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2 or clean_text(label) in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n\nprint(len(literal_preds))\nliteral_preds[:10]","dfe867c8":"# TODO: look into this preprocessing for prediction\nMAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = '..\/input\/' + SCIBERTDATASETNAME + '\/output'\nTEST_INPUT_SAVE_PATH = '.\/input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '..\/input\/' + SCIBERTDATASETNAME + '\/train_ner.json'\nVAL_PATH = '..\/input\/' + SCIBERTDATASETNAME + '\/train_ner.json'\n\n# Output of the kaggle_run_ner.py predictions\nPREDICTION_SAVE_PATH = '.\/pred'\nPREDICTION_FILE = 'test_predictions.txt'","d1380cb4":"# note: train there is the train.csv dataframe\ntrain = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')\n\ndisplay(train)","92402fe1":"# does NOT do lowercasing. Which is important as we use SciBERT-cased\ndef clean_SciBERT_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\n# for sliding window. Sci-BERT cannot handle too long inputs\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","7826e5a5":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in tqdm(sample_submission['Id'], ascii=True, desc=\"preprocessing papers\"): # loop over test papers\n    # load paper\n    paper = papers_test[paper_id]\n    \n    # extract sentences from the text part of the paper\n    sentences = [clean_SciBERT_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    #sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 words\n    \n    # only look at sentences which contain these words\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study', 'statistics', 'compilation', 'dossier', 'dataset', 'reports', 'studies', 'measurements', 'file', 'archive', 'set', 'public', 'toy', 'synthetic'])]\n    #sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n    \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words) # TODO: figure out why this happens\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","9902b2da":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","05b33f18":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/\n\n# make necessary directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","829c0bad":"#--seed 123 \\\ndef bert_predict():\n    !python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","c600ed24":"# load the outputs from the prediction file\nbert_outputs = [] # these contain the ner labels for each sentence\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}\/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","515a8913":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\n#print(test_sentences)\n#print(bert_outputs)","7f9a6aa9":"bert_dataset_labels = [] # store all dataset labels for each publication\n\n# Check for B and I tags to reconstruct dataset names\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]\n\n#bert_dataset_labels","23f25fde":"# TODO: look into this\ndef jaccard_similarity(s1, s2):\n    l1 = s1.lower().split(\" \")\n    l2 = s2.lower().split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\nfiltered_bert_labels = []\n#test['cleaned_label']\n\n\nif validation_version:\n    validatable_labels = []\n    truths = [x for x in test['cleaned_label']]\n    for labels in bert_dataset_labels:\n        filtered = []\n        for label in sorted(labels, key=len):\n            label = clean_text(label)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n                filtered.append(label)\n        validatable_labels.append(filtered)\n    \n    TP = 0\n    FP = 0\n    FN = 0\n    for i in range(len(validatable_labels)):\n        for p in validatable_labels[i]:\n            if jaccard_similarity(p, truths[i]) >= 0.5:\n                TP+=1\n            else:\n                FP+=1        \n        if len(validatable_labels[i]) == 0:\n            FN+=1\n    \n    print(\"score is:\")\n    print((5*TP)\/((5*TP)+(4*FP)+FN))\n        \nelse:\n    for labels in bert_dataset_labels:\n        filtered = []\n\n        for label in sorted(labels, key=len):\n            label = clean_text(label)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n                filtered.append(label)\n\n        filtered_bert_labels.append('|'.join(filtered))\n    \n#filtered_bert_labels[:5]","a01b34e6":"#final_predictions = literal_preds\nfinal_predictions = filtered_bert_labels\n\n\n\n#final_predictions = []\n#for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n#    if literal_match:\n#        final_predictions.append(literal_match)\n#    else:\n#        final_predictions.append(bert_pred)","e9a4d9ff":"sample_submission['PredictionString'] = final_predictions\nsample_submission.head()","0a8fcbdd":"sample_submission.to_csv(f'submission.csv', index=False)","71d1d07a":"### Create a set of all found datasets","53ef2ddb":"# Sci-BERT prediction","7be8f5f4":"# Settings","c42c96f0":"### Paths and Hyperparameters","14173243":"# Load data from competition","a456aba0":"# Literal string matching","def23d95":"### Filter based on Jaccard score and clean","2b885a8b":"### Matching on test data","3768af17":"# Aggregate final predictions and write submission file","d68c5ab1":"# Import","2b47acd4":"### Transform data to NER format","8f899cf7":"# MLiP Group 15\nUsing this notebook, we will perform NER using a fine-tuned Sci-BERT model, combined with string matching.\n\nNote: we used a notebook from tungmphung (https:\/\/www.kaggle.com\/tungmphung\/coleridge-matching-bert-ner) as a starting point.","80659aa7":"# Install offline packages","bf2a297f":"### Do predict and collect results using kaggle_run_ner.py","97545f6a":"### Restore Dataset labels from predictions","7239cb4a":"Group by publication, training labels should have the same form as expected output."}}