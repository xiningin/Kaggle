{"cell_type":{"3c8896a7":"code","fed86529":"code","51bd689a":"code","b8415676":"code","0883a075":"code","07f4e52c":"code","399a4bdc":"code","217b1cf2":"code","c5056678":"code","41b01109":"code","e13a7e21":"code","3cf98379":"code","04716bde":"code","b959eceb":"code","9abec1af":"markdown","ce287299":"markdown","f5222ad5":"markdown","94ddbecb":"markdown","22109ef5":"markdown","c5f4337d":"markdown","4f117873":"markdown","e60e6f89":"markdown","deb82684":"markdown","49d4455e":"markdown","614dc45a":"markdown","e26ba67f":"markdown","5d923558":"markdown"},"source":{"3c8896a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","fed86529":"import subprocess\nfrom ast import literal_eval\nimport multiprocessing\nfrom pynvml import *\nimport torch\n\n\n\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\n    \n\n    \nprint('### CPU ###')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^model name\"')\nprint(\"cpu count       :\", multiprocessing.cpu_count())\n#run('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu cores\"')\n\n\n\n# These codes executes only in a GPU env\nprint('\\n### RAM ###')\n#run('cat \/proc\/meminfo | egrep \"^MemTotal\"')\nnvmlInit()\nh = nvmlDeviceGetHandleByIndex(0)\ninfo = nvmlDeviceGetMemoryInfo(h)\nprint(f'Total    : {info.total \/ 1073741824} GB')\nprint(f'Used     : {info.used  \/ 1073741824} GB')\nprint(f'Free     : {info.free  \/ 1073741824} GB')\n\n\n\nprint('\\n### OS ###')\nrun('uname -a')\n\n\n\nprint('\\n### GPU ###')\n#run('lspci | grep VGA')\n# setting device on GPU if available, else CPU\nprint('Device name      :', torch.cuda.get_device_name(0))\nprint('Memory Allocated :', round(torch.cuda.memory_allocated(0)\/1024**3,1), 'GB')\nprint('Memory Cached    :', round(torch.cuda.memory_reserved(0)\/1024**3,1), 'GB')","51bd689a":"%%time\n\nimport cudf as cu\nfrom sklearn.metrics import classification_report\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.feature_selection import SelectFromModel\nfrom tqdm.notebook import tqdm\nimport gc","b8415676":"%%time\n\ndata_cudf = cu.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv') # cudf is sensitivee about tyoe and use at and iat instead of loc and iloc\ndata = data_cudf.to_pandas()\ndel data_cudf\n\n#backup = data\n#train = backup_train","0883a075":"# # drop the colomns that have +10% na\n\n# print(train.columns.shape)\n# missing_val = pd.DataFrame(train.isna().sum().sort_values(ascending=True)*100\/train.shape[0],columns=['missing %'])[:138-14]\n# missing_val.style.background_gradient(cmap='Oranges_r')\n# features = missing_val.index\n# print(features.shape)\n\n# train = train[features]\n# print(train.shape)","07f4e52c":"features = [c for c in data.columns if 'feature' in c]\nfor i in features:\n    x = data[i].mean()     \n    data[i] = data[i].fillna(x)","399a4bdc":"print(data.shape)\n\n# prepare the data before splitting to train and test\ndata = data[data['weight']!=0]\ndata['action'] = (data['resp']>0)*1\nprint(data.shape)\n#features = data.columns.str.contains('feature')\n\n# with we will create the model\ntrain = data.sample(frac = 0.75, random_state = 73)\nprint(train.shape)\n#train = data.reset_index(drop=True) # if we keep it here it will raise an error in the valid set, cause if the change of the index\n\nvalid = data.drop(train.index) # take the data that does not exist in train\nprint(valid.shape)\n\n\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\n\n# it will help us track the model\n#test  = data.tail(500).reset_index(drop=True)\n\n# split the train set\nX_train = train.loc[:,features] \ny_train = train.loc[:, 'action']\n\n# split the test set\n#X_test = test.loc[:, features]\n#y_test = test.loc[:, 'action']\n\n\nX_valid = valid.loc[:, features]\ny_valid = valid.loc[:, 'action']","217b1cf2":"%%time\n\nselector = CatBoostClassifier(thread_count = -1, task_type = \"GPU\", devices = '-1', random_seed = 73).fit(Pool(X_train, y_train), verbose = 100) # without task_type='GPU' the fit time is 42min with GPU is 46s\nlist_of_tuples = list(zip(X_train.columns.values, selector.get_feature_importance())) \n\ndf = pd.DataFrame(list_of_tuples).sort_values(by = [1]).reset_index(drop = True).rename(columns = {0: 'feat_labels', 1: 'feature_importances'})\ndf.head()","c5056678":"df1 = df.tail(111) # features importance > 0.003\nfeatures = list(df1[\"feat_labels\"])\n\nX_train = X_train.loc[:, features]\n#X_test = X_test.loc[:, features]\n#print(train.shape)\nprint(X_train.shape)\n#print(X_test.shape)\n\nX_valid = X_valid.loc[:, features]\nprint(X_valid.shape)","41b01109":"#%%time\n\n#model = CatBoostClassifier(\n#                thread_count = -1, task_type = \"GPU\", devices = '-1', random_seed = 73, \n#                bootstrap_type = 'Poisson', verbose = 10, name = 'V1')\n\n# my core number is 4\n# I have only one GPU, so no need to -1 for activating all the gpu's but instead 0:1\n# save_snapshot=True, snapshot_file=\"V0\", snapshot_interval=600 not supported for randomized search\n\n#grid = {  'depth'           :[3, 1, 2, 6, 4, 5, 7, 8, 9, 10],\n#          'iterations'      :[1000, 250, 100, 500],\n#          'learning_rate'   :[0.03, 0.001, 0.01, 0.1, 0.2, 0.3],\n#          'l2_leaf_reg'     :[3, 1, 5, 10, 100],\n#          'border_count'    :[32, 5, 10, 20, 50, 100, 200],\n#          }\n# 'loss_function'   :['Logloss', 'CrossEntropy'], currently not supported in grid search\n# 'thread_count'    :4 error non iterable\n# 'ctr_border_count':[50,5,10,20,100,200] error not a map\n\n#grid_search_result = model.randomized_search(  grid, \n#                                               Pool(X_train, y_train), # X = X_train, y = y_train\n#                                               cv         = 3,\n#                                               n_iter     = 10,\n#                                               refit      = False,\n#                                               shuffle    = True,\n#                                               stratified = True,\n#                                               train_size = 0.75,\n#                                               plot       = True)\n# search_by_train_test_split = Split the source dataset into train and test parts. \n# Models are trained on the train part, while parameters are compared by the loss function score on the test dataset.\n\n\n#print(grid_search_result['params'])","e13a7e21":"# {'border_count': 200, 'depth': 10, 'l2_leaf_reg': 1, 'iterations': 250, 'learning_rate': 0.5}\n\n# Dataset processing.The fastest way to pass the features data to the Pool\ntrain_data = Pool(data = X_train,\n                  label = y_train,\n                  ) #weight=[0.1, 0.2...]\n\nvalid_data = Pool(data = X_valid,\n                  label = y_valid)\n\nmodel = CatBoostClassifier(border_count = 32, depth = 5, l2_leaf_reg = 3.5, \n                           thread_count = -1,iterations = 100, learning_rate = 0.5, \n                           task_type = \"GPU\",devices = '0:1', bootstrap_type = 'Poisson', \n                           random_seed = 73, verbose = 100, name = 'V3', \n                           use_best_model=True, loss_function= 'Logloss', eval_metric='AUC',) \n# rsm=0.98 not supported on GPU\n# use_best_model=True This option requires a validation dataset to be provided. \n# Use the validation dataset to identify the iteration with the optimal value of the metric specified in  --eval-metric (eval_metric).\n\nmodel.fit(train_data, eval_set = valid_data) # eval_set=(X_test, y_test) Pool(X_train, y_train)\n#y_pred = model.predict(X_test)\n\n#print(\"\\n\\n\",model.predict(X_test))\n#print(\"\\n\\n\",classification_report(y_test, y_pred))","3cf98379":"#model.save_model(fname = \"\/model_v2\",\n#                   format=\"cbm\",\n#                   export_parameters=None, # Additional format-dependent parameters for Apple CoreML ONNX-ML PMML\n#                   pool=None) # This parameter is required if the model contains categorical features and the output format is cpp, python, or JSON.","04716bde":"#model = CatBoostClassifier()      \n#model.load_model(\"\/model_v1\")","b959eceb":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    \n    X_test = test_df.loc[:, features]\n    \n    for i in features:\n        x = X_test[i].mean()     \n        X_test[i] = X_test[i].fillna(x)\n\n    y_preds = model.predict(X_test)\n    \n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","9abec1af":"# Hyper Pram tuning\n### using CatBoostClassifier Randomized Search\n##### dont run this it takes alot of time. The result is used in the next cell.\n### V1: I have changed some grid params","ce287299":"# Sys\n\nThe used cmds are designed for a GPU env, and I have collected them from various notebooks.\n\nKaggle provides free access to NVidia K80 GPUs in kernels.\n\nA GPU Kernel will give you Tesla P100 16gb VRAM as GPU, with 13gb RAM + 2-core of Intel Xeon as CPU. No-GPU option will give you 4-cores + 16gb RAM, hence more CPU power.","f5222ad5":"# Feature Selection\n### using CatBoostClassifier","94ddbecb":"# Kaggle Code","22109ef5":"# Nan","c5f4337d":"# Model","4f117873":"# Submission","e60e6f89":"# Data","deb82684":"# Save and Load","49d4455e":"# Update Splitted data","614dc45a":"This notebook was implemented using the official documentation of Catboost\n\nhttps:\/\/catboost.ai\/","e26ba67f":"# Libs","5d923558":"# Split"}}