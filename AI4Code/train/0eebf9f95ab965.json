{"cell_type":{"a8acecd5":"code","44824a2e":"code","c76fd152":"code","655d3013":"code","66958a0f":"code","cf79270f":"code","eb82249b":"code","f21bc339":"code","8f1e20ed":"code","79334a25":"code","ed702015":"code","e1b7276c":"code","65438679":"code","042ce6fc":"code","e22a842a":"code","3463138a":"code","059a6846":"code","2cae7e83":"code","447f215f":"code","cd87c522":"code","afcde40b":"code","bb1bc01d":"code","ff9d245f":"code","c019df1e":"code","3834bbba":"code","9437aacc":"code","08c06bb2":"code","bd063ac3":"code","47ba3089":"code","26e7b1b9":"code","15e41a22":"code","279c791f":"code","10a360a8":"code","cd4fb136":"code","5983bae6":"code","4cae631a":"code","9be14310":"code","26880aee":"code","e8f73d0f":"code","02069dbf":"code","018aee21":"code","35efaf57":"code","013d6c4e":"code","ed3ba2fe":"code","cdd65f46":"code","8b5e0533":"code","73d3e6d7":"code","27b5afea":"code","7499e9f5":"code","e85be8bc":"code","b91b5660":"code","2d05823b":"code","46e59f3a":"code","24d539d6":"code","5179afb2":"code","6bda248d":"code","10022b9a":"code","7e41214e":"code","316ce5cc":"code","acfd555c":"code","7d7257d9":"code","e5f0f9d5":"code","411fa4ad":"code","30d78796":"code","296ca0d4":"markdown","ef4e92fe":"markdown","ac053af1":"markdown","3e3bbdc5":"markdown","24eba56e":"markdown","f16270b6":"markdown","531b31b9":"markdown","f603bdcb":"markdown","4a6228d7":"markdown","adee8b3a":"markdown","138561b7":"markdown","845cd381":"markdown","fb2fa01f":"markdown","98fdf30b":"markdown"},"source":{"a8acecd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44824a2e":"import tensorflow as tf\nimport matplotlib.pyplot as plt","c76fd152":"data = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")","655d3013":"data.head()","66958a0f":"print(len(data.text), len(data.textID))","cf79270f":"for idx, value in enumerate(data.text): ## remove hyperlinks\n    words = str(value).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    data[\"text\"][idx] = \" \".join(words)","eb82249b":"import string\n\ndef clean_text(dataset, field):\n    for index, strin in enumerate(dataset[field]):\n        if not strin:\n            strin = strin.lower()\n            strin = strin.replace(\"'\", \"\")\n            strin = strin.replace(\"\\n\", \"\")\n            strin = strin.strip()\n            strin = strin.replace('[{}]'.format(string.punctuation), '')\n            dataset[field][index] = strin\n\n\nclean_text(data, 'text')\nclean_text(data, 'selected_text')","f21bc339":"print(len(data.text), len(data.textID))","8f1e20ed":"data = data[pd.notnull(data.selected_text)]","79334a25":"print(data.text[data.textID == \"a88287bbda\"])\nprint(len(data.text), len(data.textID))\n","ed702015":"from sklearn.model_selection import train_test_split\n\ntrain, validation = train_test_split(data, test_size = 0.25)\nprint(len(data), len(train), len(validation))","e1b7276c":"train.head()","65438679":"validation.head()","042ce6fc":"print(train.text[train.textID == \"a88287bbda\"])","e22a842a":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train.text)\nword_index = tokenizer.word_index","3463138a":"training_sequences = tokenizer.texts_to_sequences(np.array(train.text))\ntraining_padded = pad_sequences(training_sequences,truncating=trunc_type, padding=pad_type)\n\nmax_length = len(training_padded[0])\n\nvalidation_sequences = tokenizer.texts_to_sequences(np.array(validation.text))\nvalidation_padded = pad_sequences(validation_sequences, padding=pad_type, maxlen = max_length)","059a6846":"training_selected_sequences = tokenizer.texts_to_sequences(np.array(train.selected_text))\nvalidation_selected_sequences = tokenizer.texts_to_sequences(np.array(validation.selected_text))","2cae7e83":"def get_list(padded, sequence):\n    return np.array([1 if x in sequence else 0 for x in padded])","447f215f":"training_padded[4]","cd87c522":"training_selected_sequences[4]","afcde40b":"get_list(training_padded[4], training_selected_sequences[4])","bb1bc01d":"train_y = np.array([get_list(i,j) for i,j in zip(training_padded, training_selected_sequences)])\nvalidate_y = np.array([get_list(i,j) for i,j in zip(validation_padded, validation_selected_sequences)])","ff9d245f":"train_y","c019df1e":"np.array(train.sentiment).shape","3834bbba":"training_padded.shape","9437aacc":"rev_word_index = {v: k for k, v in word_index.items()}\ndef get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index]]])","08c06bb2":"print(train.text.values[4])\nprint(str(get_phrase(training_padded, train_y, 4)))","bd063ac3":"train_x = np.copy(training_padded)\nvalidate_x = np.copy(validation_padded)","47ba3089":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout","26e7b1b9":"plt.style.use('dark_background')\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \n# plot_graphs(history, \"accuracy\")\n# plot_graphs(history, \"loss\")","15e41a22":"#!pip install -U keras-tuner","279c791f":"training_padded = np.array(training_padded)\nvalidation_padded = np.array(validation_padded)\ntrain_y = np.array(train_y)\nvalidate_y = np.array(validate_y)","10a360a8":"training_padded","cd4fb136":"print(np.array(training_padded)[(train.sentiment == \"positive\")].shape)\nprint(training_padded[(train.sentiment == \"neutral\")].shape)\ntraining_padded[(train.sentiment == \"negative\")].shape","5983bae6":"train_positive_x = training_padded[(train.sentiment == \"positive\")]\ntrain_neutral_x = training_padded[(train.sentiment == \"neutral\")]\ntrain_negative_x = training_padded[(train.sentiment == \"negative\")]\ntrain_positive_y = train_y[(train.sentiment == \"positive\")]\ntrain_neutral_y = train_y[(train.sentiment == \"neutral\")]\ntrain_negative_y = train_y[(train.sentiment == \"negative\")]\n\nvalidate_positive_x = validation_padded[(validation.sentiment == \"positive\")]\nvalidate_neutral_x = validation_padded[(validation.sentiment == \"neutral\")]\nvalidate_negative_x = validation_padded[(validation.sentiment == \"negative\")]\nvalidate_positive_y = validate_y[(validation.sentiment == \"positive\")]\nvalidate_neutral_y = validate_y[(validation.sentiment == \"neutral\")]\nvalidate_negative_y = validate_y[(validation.sentiment == \"negative\")]","4cae631a":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)","9be14310":"!pip install -U keras-tuner","26880aee":"import kerastuner\ndef build_model(hp):\n    model = Sequential()\n    model.add(Embedding(vocab_size, hp.Int('units', min_value = 5, max_value = 200, step = 25), input_length=max_length))\n    model.add(Dropout(0.5))\n    model.add(Bidirectional(LSTM(20)))\n    model.add(Dropout(0.5))\n    model.add(Dense(max_length, activation='softmax'))\n    model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate',\n                      values=[1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n    return model\n\ntuner = kerastuner.tuners.RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3)\n\n    ","e8f73d0f":"tuner.search(train_positive_x, train_positive_y, epochs = 40,verbose = 2,validation_data = (validate_positive_x, validate_positive_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","02069dbf":"positive_model = tuner.get_best_models()[0]\npositive_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\npositive_history = positive_model.fit(np.array(train_positive_x), np.array(train_positive_y), epochs=60, verbose=2,\n                    validation_data = (np.array(validate_positive_x), np.array(validate_positive_y)),callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)] )","018aee21":"# positive_model = Sequential()\n# positive_model.add(Embedding(vocab_size, 16, input_length=max_length))\n# positive_model.add(Dropout(0.5))\n# positive_model.add(Bidirectional(LSTM(20)))\n# positive_model.add(Dropout(0.5))\n# positive_model.add(Dense(max_length, activation='softmax'))\n# positive_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n# positive_history = positive_model.fit(np.array(train_positive_x), np.array(train_positive_y), epochs=60, verbose=2,\n#                    validation_data = (np.array(validate_positive_x), np.array(validate_positive_y)))","35efaf57":"plot_graphs(positive_history, \"accuracy\")\nplot_graphs(positive_history, \"loss\")","013d6c4e":"tuner.search(train_negative_x, train_negative_y, epochs = 40,verbose = 2,validation_data = (validate_negative_x, validate_negative_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","ed3ba2fe":"tuner.results_summary()","cdd65f46":"negative_model = tuner.get_best_models()[0]\nnegative_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nnegative_history = negative_model.fit(train_negative_x, train_negative_y, epochs=100, verbose=2,\n                   validation_data = (validate_negative_x, validate_negative_y), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=6)])","8b5e0533":"# negative_model = Sequential()\n# negative_model.add(Embedding(vocab_size, 16, input_length=max_length))\n# negative_model.add(Dropout(0.5))\n# negative_model.add(Bidirectional(LSTM(20)))\n# negative_model.add(Dropout(0.5))\n# negative_model.add(Dense(max_length, activation='softmax'))\n# negative_model.compile(loss=l, optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n# negative_history = negative_model.fit(train_negative_x, train_negative_y, epochs=60, verbose=2,\n#                    validation_data = (validate_negative_x, validate_negative_y))","73d3e6d7":"plot_graphs(negative_history, \"accuracy\")\nplot_graphs(negative_history, \"loss\")","27b5afea":"val_neg_preds = [np.round(negative_model.predict(item[np.newaxis])) for item in validate_negative_x]","7499e9f5":"test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")","e85be8bc":"for idx, value in enumerate(test.text):\n    words = str(value).split()\n    words = [x for x in words if not x.startswith(\"http\")]\n    test[\"text\"][idx] = \" \".join(words)\ntest","b91b5660":"clean_text(test, 'text')","2d05823b":"test","46e59f3a":"test_sequences = tokenizer.texts_to_sequences(np.array(test.text))\ntest_padded = pad_sequences(test_sequences,truncating=trunc_type, maxlen = max_length,padding=pad_type)","24d539d6":"def get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index]]])","5179afb2":"preds = []\nfor index, item in enumerate(test_padded):\n    if test.sentiment[index] == \"positive\":\n        p = np.round(positive_model.predict(item[np.newaxis]))\n        preds.append(p)\n    elif test.sentiment[index] == \"negative\":\n        p = np.round(negative_model.predict(item[np.newaxis]))\n        preds.append(p)\n    else:\n        #p = np.round(neutral_model.predict(item[np.newaxis]))\n        preds.append(test_padded[index].astype(bool).astype(int)[np.newaxis])","6bda248d":"def get_phrase(array_x, array_y, index): \n    return np.array([rev_word_index[i] for i in array_x[index][array_y.astype(bool)[index][0]]if i != 0])","10022b9a":"test[\"prediction\"] = np.zeros(len(test))\n\nfor index, item in enumerate(preds):\n    test['prediction'][index] = str(get_phrase(test_padded, np.array(preds), index))","7e41214e":"test.prediction[test.sentiment == \"neutral\"] = test.text[test.sentiment == \"neutral\"]","316ce5cc":"test.prediction = test.prediction.str.replace(\"[\", \"\")\ntest.prediction = test.prediction.str.replace(\"]\", \"\")\ntest.prediction = test.prediction.str.replace(\"'\", \"\")\ntest.prediction = test.prediction.str.replace(\"<OOV>\", \"\")","acfd555c":"test.prediction[(test.prediction) == ''] = test.text[(test.prediction) == '']","7d7257d9":"test","e5f0f9d5":"evaluation = test.textID.copy().to_frame()","411fa4ad":"evaluation['selected_text'] = test['prediction']\nevaluation","30d78796":"evaluation.to_csv(\"submission.csv\", index=False)","296ca0d4":"So the output array will look like this:","ef4e92fe":"The padded sequences are going to be the inputs. The output will be an array of the same length, but with ones at the indexes of the words that we keep (that embody the sentiment), and zeros for the rest of the words.","ac053af1":"To get the phrases back from the predicted arrays:","3e3bbdc5":"So for this tweet:","24eba56e":"I was having some trouble figuring out how to incorporate the sentiment and the input text into the model, but I noticed that for neutral tweets, in almost every case the selected text is just the whole tweet. So, I decided to just go with that, and make two separate models, one for negative and one for positive tweets.","f16270b6":"## Tune and Train models","531b31b9":"## Separate into training and validation sets","f603bdcb":"These words are important:","4a6228d7":"## Clean text","adee8b3a":"This notebook is for tweet sentiment analysis- we are given the text of the tweet, as well as the sentiment, and are asked to generate the part of the tweet that embodies that sentiment.","138561b7":"## Predicting for test set","845cd381":"I read at this link: (https:\/\/medium.com\/huggingface\/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d) that this is the correct loss function to use for multilabel classification, which I think is what I'm doing here.","fb2fa01f":"## Tokenize and create vocabulary","98fdf30b":"## Prepare data"}}