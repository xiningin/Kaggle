{"cell_type":{"440878c1":"code","1f1917b0":"code","e64078f9":"code","45bbb5da":"code","a6a1cb4c":"code","bb5fa246":"code","926eec81":"code","0a838e4f":"code","e5baaf91":"code","8a4624ac":"code","14c56250":"code","40ef7af4":"code","ddcc30ae":"code","f768f692":"code","29476510":"code","0f251550":"code","9dc65174":"code","2cdc2a71":"code","d8e7d5b5":"code","2e685be0":"code","fc230a84":"code","359e7aae":"code","72ae6071":"code","650e4eb4":"code","88154a9f":"code","f1ed4684":"markdown","ea6f65f4":"markdown","54b5e797":"markdown","3e3acc74":"markdown","03e79e4e":"markdown","31bb9d35":"markdown","46315e2c":"markdown","4880b826":"markdown","3fea6ed8":"markdown","c893f0dc":"markdown","1abbfa2a":"markdown","528faf48":"markdown","801b4a28":"markdown","1c6fad1a":"markdown","a3b40744":"markdown","f83d7694":"markdown","0af5e36d":"markdown","3dabb140":"markdown","793f8906":"markdown","fcfa71e9":"markdown","f72e439d":"markdown","16588c90":"markdown"},"source":{"440878c1":"import numpy as np\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly import figure_factory as ff\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom multiprocessing import Pool, cpu_count\nfrom tqdm.notebook import tqdm, trange\n\n\nnp.random.seed(42)\nrandom.seed(42)\npd.options.plotting.backend = \"plotly\"\nsns.set(style='darkgrid', context='notebook', rc={'figure.frameon': False, 'figure.figsize': (12, 8)})\nn_jobs = cpu_count()\n\ny_true = np.random.uniform(size=1140000) < .5125\n\npd.Series(y_true, name='y_true').value_counts(normalize=True).plot.bar(title=f'mean(y_true) = {y_true.mean():.5f}')","1f1917b0":"y_pred = np.copy(y_true)\n\nroc_auc_score(y_true, y_pred)","e64078f9":"flip = np.random.uniform(size=y_true.size) < .25\ny_true[flip] = 1 - y_true[flip]","45bbb5da":"pd.Series(y_true, name='y_true').value_counts(normalize=True).plot.bar(title=f'AUC {roc_auc_score(y_true, y_pred):.5f}, mean(y_true) = {y_true.mean():.5f}')","a6a1cb4c":"y_train_true, y_test_true, y_train_pred, y_test_pred = train_test_split(y_true, y_pred, test_size=540000)","bb5fa246":"cvs = pd.DataFrame({'auc': [\n    roc_auc_score(y_train_true[idx_val], y_train_pred[idx_val])\n    for _, idx_val in StratifiedKFold(5, shuffle=True).split(y_train_true, y_train_pred)\n]})\ncvs.plot.bar(y='auc')","926eec81":"cvs","0a838e4f":"y_private_true, y_public_true, y_private_pred, y_public_pred = train_test_split(y_test_true, y_test_pred, test_size=.19)\n\nlb = pd.DataFrame({'auc': [\n    roc_auc_score(y_public_true, y_public_pred),\n    roc_auc_score(y_private_true, y_private_pred)\n]}, index=['public', 'private'])\nlb.plot.bar(title=f'public AUC {lb.loc[\"public\", \"auc\"]:.5f}, private AUC {lb.loc[\"private\", \"auc\"]:.5f}')","e5baaf91":"def simulate_tabfun_november(clf_accuracy=1.0, metric=roc_auc_score):\n    y_true = (np.random.uniform(size=1140000) < 0.5125).astype(np.float32)\n    y_pred = np.copy(y_true)\n    if clf_accuracy < 1.0:\n        make_wrong = clf_accuracy < np.random.uniform(size=y_pred.size)\n        y_pred[make_wrong] = 1 - y_pred[make_wrong]\n    flip = np.random.uniform(size=y_true.size) < .25\n    y_true[flip] = 1 - y_true[flip]\n    y_train_true, y_test_true, y_train_pred, y_test_pred = train_test_split(y_true, y_pred, test_size=540000)\n    out = {\n        f'cv_{i}': metric(y_train_true[idx_val], y_train_pred[idx_val])\n        for i, (_, idx_val) in enumerate(StratifiedKFold(5, shuffle=True).split(y_train_true, y_train_pred))\n    }\n    out['oof_auc'] = metric(y_train_true, y_train_pred)\n    y_private_true, y_public_true, y_private_pred, y_public_pred = train_test_split(y_test_true, y_test_pred, test_size=.19)\n    out['public'] = metric(y_public_true, y_public_pred)\n    out['private'] = metric(y_private_true, y_private_pred)\n    return out\n\none_run = pd.DataFrame(simulate_tabfun_november(), index=pd.RangeIndex(1))\none_run.melt().plot.bar(x='variable', y='value')","8a4624ac":"one_run","14c56250":"summary = pd.DataFrame([\n    simulate_tabfun_november() for _ in trange(10)\n])\nsummary","40ef7af4":"summary.describe()","ddcc30ae":"px.box(pd.melt(summary).rename(columns={'variable': 'split', 'value': 'auc'}), x='split', y='auc')","f768f692":"tasks = [1.0] * 1000\nwith Pool(n_jobs) as pool:\n    summary = pd.DataFrame(list(tqdm(pool.imap(simulate_tabfun_november, tasks), total=1000)))","29476510":"px.box(pd.melt(summary).rename(columns={'variable': 'split', 'value': 'auc'}), x='split', y='auc', title='Splits by AUC after 1000 competitions')","0f251550":"px.imshow(summary.corr(), title=f'Correlation heatmap for {len(tasks)} competitions')","9dc65174":"ff.create_distplot(\n    [summary.private - summary.public, summary.oof_auc - summary.public, summary.oof_auc - summary.private],\n    ['Private - Public', 'CV - Public', 'CV - Private'],\n    bin_size=.001,\n)\n","2cdc2a71":"tasks = [.995] * 1000\nwith Pool(n_jobs) as pool:\n    summary2 = pd.DataFrame(list(tqdm(pool.imap(simulate_tabfun_november, tasks), total=1000)))\n\npx.box(pd.melt(summary2).rename(columns={'variable': 'split', 'value': 'auc'}), x='split', y='auc')","d8e7d5b5":"d = pd.concat([summary.assign(clf='perfect'), summary2.assign(clf='good')]).melt(id_vars=['clf'], var_name='split', value_name='auc')\npx.box(d, x='split', y='auc', color='clf')","2e685be0":"px.box(\n    summary.loc[summary.public <= .7488].melt(var_name='split', value_name='auc'), \n    x='split', y='auc', title=f'Public LB <= .7488 ({100 * np.mean(summary.public <= .7488):.2f}% of simulations)',\n)","fc230a84":"px.box(\n    summary.loc[summary.oof_auc > .75].melt(var_name='split', value_name='auc'), \n    x='split', y='auc', title=f'OOF AUC > .75 ({100 * np.mean(summary.oof_auc > .75):.2f}% of simulations)',\n)","359e7aae":"d = summary.loc[summary.oof_auc.between(summary.oof_auc.quantile(.25), summary.oof_auc.quantile(.75))]\n\npx.box(\n    d.melt(var_name='split', value_name='auc'), \n    x='split', y='auc', title=f'OOF near median',\n)","72ae6071":"from sklearn.metrics import accuracy_score\n\ntasks = [1.0] * 1000\ndef simulate_tabfun_november_accuracy(accuracy):\n    return simulate_tabfun_november(accuracy, metric=accuracy_score)\n\nwith Pool(n_jobs) as pool:\n    accuracy_summary = pd.DataFrame(list(tqdm(pool.imap(simulate_tabfun_november_accuracy, tasks), total=1000))).rename(columns={'oof_auc': 'oof_accuracy'})\n\n    \npx.box(\n    accuracy_summary.melt(var_name='split', value_name='accuracy'),\n    x='split', y='accuracy', title='Perfect classifier accuracy'\n)","650e4eb4":"tasks = [.995] * 1000\n\nwith Pool(n_jobs) as pool:\n    accuracy_summary2 = pd.DataFrame(list(tqdm(pool.imap(simulate_tabfun_november_accuracy, tasks), total=1000))).rename(columns={'oof_auc': 'oof_accuracy'})\n\nd = pd.concat([accuracy_summary.assign(clf='perfect'), accuracy_summary2.assign(clf='good')]).melt(id_vars=['clf'], var_name='split', value_name='accuracy')\npx.box(d, x='split', y='accuracy', color='clf')","88154a9f":"summary.to_csv('\/kaggle\/working\/summary_perfect.csv', index=False)\nsummary2.to_csv('\/kaggle\/working\/summary_good.csv', index=False)\naccuracy_summary.to_csv('\/kaggle\/working\/summary_accuracy_perfect.csv', index=False)\naccuracy_summary2.to_csv('\/kaggle\/working\/summary_accuracy_good.csv', index=False)","f1ed4684":"Well, wow. I *am* seeing CVs in this range regularly. Does this mean I should just quit trying? Hard to say, but it turns out that for a perfect classifier, Public LB might be less than this score a fair bit of the time...\n\nI'm regularly hitting `oof_auc` over `.75` locally, but I never submit to that score. How's that look with a perfect classifier?","ea6f65f4":"This changes the distribution of `y_true` to what we'd recognize from the training data set:","54b5e797":"What kind of variation can we expect?","3e3acc74":"Writing the results\n==\n\nIn case somebody else might want to play with these results, I'm writing out:\n\n- `summary_perfect.csv` is 1000 rounds of simulated scores for the perfect classifier\n- `summary_accuracy.csv` is 1000 rounds of simulated accuracy scores for the perfect classifier\n- `summary_good.csv` is 1000 rounds of simulated scores for the good classifier\n\nIf you followed along this far, thanks a lot for reading, let me know if you learned something, or have suggestions for improvements!","03e79e4e":"Private vs Public LB score\n==\n\n[This](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/285503) posts suggests that the labels for this competition have been tampered with, which seems very likely. For the sake of doing some experiments, let's assume that the post is correct, and that 25% of the labels have been selected at random, and flipped. This seems like the most likely explanation for why we're hitting around AUC .75.\n\nBut does that actually tell us anything? Well, let's assume for a moment, that we have generated the 1 140 000 total labels to be about balanced, like this: `y_true = np.random.uniform(1140000) < 0.5125`. And that we randomly flipped 25% of them. Can that tell us anything about the maximum AUC that we \"should\" be scoring on the Public LB, before we're overfitting? To try to answer the question, I will try to run some simulations. A disclaimer here, is that I wrote this code so that it's hopefully easy to follow, it's certainly possibly to make it more efficient.\n\nOriginal label distribution\n--\n\nDid you notice that I wrote `.5125` above? But `mean(y) = .506` according to train set! Well, it turns out, that flipping 25% of the labels will bring `mean(y)` closer to `.5`. In fact, we're likely to end up in the vicinity around `.506` when we do this. What does that mean? Well, the flipped labels have a different distribution than the correct labels. That'll impact practically everything.","31bb9d35":"Notice the enormous spans here, especially in public LB!\n\nSplit AUC score correlations\n--\n\nNow, it might be interesting to check out some correlations here.","46315e2c":"So, it turns out, you should actually be able to tell the difference between the perfect classifier, and the good classifier. Let's plot both together:\n\n\n","4880b826":"But we don't know that yet, of course. We've only _really_ seen the train set, and been tested on 19% of the test set. We're pretty sure though, because our CV score is 1.0 too.\n\nSomeone messes up the labels!\n==\n\nBut anyway, Kaggle staff found out, and decided that labels must be made more difficult, so they randomly flip about 25% of the labels. Note that, at this point, we make the assumption that the labelflip is random, and completely independent from the features! If that's not the case, the rest of this notebook won't be making much sense.","3fea6ed8":"And how do the splits distribute for the oof_auc within 25% of the median?","c893f0dc":"There's some variation here, but, by and large the scores are similar. Later on, we should check how much more variation there would be if our classifier was not perfect.\n\nBut there's one more split going on! The public vs private LB split of the test sets, where the public split is smaller than the private at 19%. In fact, it's actually smaller even than our individual CVs, so it's probably less stable!\n\nThe Perfect Classifier Leaderboard Scores\n==\n\nSo, with that CV performance, how well do we perform on Public LB compared to Private LB with our perfect classifier?","1abbfa2a":"That seems to be quite a range, right? I've certainly had plenty of models that score in this sort of range on local CVs, and I also have submits that score in that min-to-max range of the public LB.\n\nCurrently, the public LB is very near .75 AUC. And it's very possible that a perfect classifier might only be able to achieve, for example, .7485 AUC, according to this.\n\nBut maybe that's very unlikely. A sample size of 10 is way too low to get us any estimates of how likely that is, anyway. So let's run some more.\n\nSimulating 1000 competitions\n==\n\n![d07e44e436aa5b9ef5c80950c8702959.png](attachment:dcb8d3a0-2b6b-47c9-8d82-9af2e87120c8.png)\n\nThis is going to take a while, because we didn't write this code to be efficient. There's probably also a way to express this with formulas and statistics, rather than doing it this, but my brain is just way too fried to think about that right now and the code came naturally. But let's throw more CPU cores into the mix, for some extra speed:","528faf48":"The Good Classifier\n==\n\nLet's relax one assumption we've had so far. We've used a perfect classifier for our study up until now. But what if it turns out that it's only possible to make one that is almost perfect? What happens to our estimates if it turns out that our classifier is only 99.5% accurate? Does that lead to greater variation? Would we be able to tell the difference between the perfect one, and the almost perfect one?","801b4a28":"The perfect classifier from earlier, can now only classify around 75% of the data correctly, so it ends up with AUC roughly .75, just like what we're seeing at the top of the leaderboard.\n\nBut this isn't actually how the leaderboard score is calculated, so let's make this a bit more complicated, to better model reality.\n\n\nSimulating data set splits\n==\n\nLet's assume, that was kaggle did, was to generate a single, big data set. Then they flipped the labels randomly. After which they split the dataset, first into train\/test parts, then they split the test part into public and private. So let's do the same thing.\n\nFirst we split out the test set, we'll use `sklearn.model_selection.train_test_split` for clarity.","1c6fad1a":"CV results from train set\n--\n\nSo, suppose we still have the perfect classifier, and we validate on some CV splits locally -- we might see these scores:","a3b40744":"One thing that I'm personally very interested in knowing, is which one out of CV score and Public LB is the best indicator of Private LB score, so let's plot the difference between these, ie plot the three distributions:\n\n- `public - private`\n- `oof_auc - public`\n- `oof_auc - private`","f83d7694":"And there you go, now we have a reusable simulation to go on. Let's run it a few times:","0af5e36d":"Good classifier accuracy vs perfect comparison\n--","3dabb140":"The Perfect Classifier\n==\n\nNow, let's assume that we've built the **perfect** classifier for that. So, it actually predicts, exactly `y_true`, for all of these. It's just that good.\n\nObviously, the AUC is 1:","793f8906":"If you're seeing CV scores at around .75 AUC, you're probably doing better than the good classifier is. But it's your CV scores you should be looking at, public LB is much more unstable, and has a bigger overlap!\n\nShould I even keep trying?\n==\n\nPersonally I'm hitting .75 AUC on CVs, but my best on the Public LB is 0.74880. Does this mean I'm overfitting to my CV, or does it mean that we got unlucky, and got a Public LB selection which is hard, even for the perfect classifier?\n\nCan we inspect our simulation results to find out? Let's find out what the other values distribute to, when Public LB is lower, or equal to .7488:","fcfa71e9":"Perfect classifier accuracy\n==\n\nWe've been spending a lot of time looking at AUC, which can seemingly end up over .75.\n\nBut how's it look for accuracy? I've ran hundreds of models by now and I am not getting out of fold accuracy over 74.78% (train accuracy is 75% for those). So I want to run some simulations tracking accuracy too:","f72e439d":"According to this, you should trust your CV scores more than the public LB by far, and it's OK if public LB and CV scores disagree. Because they do, even for the perfect classifier!","16588c90":"In this case, we score worse on Public LB, than on Private LB with our perfect classifier. By quite a margin too, going by the score differences we're seeing right now. \n\nWe don't **know** that it **actually** is like this. But can we try to find out how likely it is?\n\nWell, we can run as many simulations as we'd like, to kind of figure out how all of these numbers relate to each other.\n\nMaking a reusable simulation\n==\n\nWe'll basically do exactly the same thing, except of course, many times. That sounds like exactly the right job for a function, so let's make that. We'll make sure to add a parameter so that we can later experiment with a worse-than-perfect classifier:"}}