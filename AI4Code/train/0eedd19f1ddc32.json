{"cell_type":{"5f8469b1":"code","35e2bb3a":"code","a79ae2bd":"code","34ccbdbc":"code","ab08937f":"code","750d63ee":"code","0e13d04f":"code","003333ca":"code","3611b131":"code","1af6bdf8":"code","50b2a22a":"code","f5570e4c":"code","64fbc9b5":"code","94a3d9a9":"code","e181400d":"code","cdfe5adf":"code","d0377e87":"code","ba73eee2":"code","d68908b6":"code","350634ad":"code","f7de1287":"code","f22ea538":"code","899dd7d8":"code","da8196e5":"code","b6e8b712":"code","15bdc913":"code","e441f2fb":"code","9e940b6e":"code","e14693cb":"code","c0086fe3":"code","53f383d6":"code","15575a57":"code","38024b8f":"code","afc3411a":"code","98000827":"code","ea204a79":"code","dba5b0d9":"code","78b6d8ed":"code","965bb850":"code","438862e8":"code","b6b42aba":"code","28d8bf9a":"code","5d9e4cf1":"code","d10b37c0":"code","8ddfbc3d":"code","3fc2b41c":"code","974d2cb5":"code","0f88a04d":"code","e707017d":"code","f08351db":"code","fda0d9ad":"code","ab49758b":"code","8dcda4be":"markdown","ae64d4d4":"markdown","108110f0":"markdown","58ed0226":"markdown","84565e8a":"markdown","561d7fd8":"markdown","34ab015b":"markdown","57d7a0b5":"markdown","90c2b203":"markdown","0a60aa1e":"markdown","3149e1d8":"markdown","acdaa249":"markdown","004e266e":"markdown","c18d9c2b":"markdown","108aac08":"markdown","2a186fbc":"markdown","daba8558":"markdown","4f347ac0":"markdown","8963fd01":"markdown","cb64c8ca":"markdown","d6a7d52e":"markdown","96aae869":"markdown","bd833b43":"markdown","4132c139":"markdown","e252c249":"markdown","41bf6069":"markdown","ad3e9189":"markdown","4ff461ae":"markdown","8c508117":"markdown","a21dcd9e":"markdown","a61864b2":"markdown","ff723001":"markdown"},"source":{"5f8469b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35e2bb3a":"trainingset = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntestset = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\no_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","a79ae2bd":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nimport missingno as msno\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \n\npd.set_option(\"display.float_format\", lambda x: \"{:.3f}\".format(x)) #Limiting floats output to 3 decimal points\npd.set_option(\"display.max_columns\", None)","34ccbdbc":"print(trainingset.shape)\nprint(testset.shape)","ab08937f":"#trainingset.describe()","750d63ee":"# checking duplication\ntrainingset.duplicated().sum()","0e13d04f":"train_null = trainingset.isnull().sum().sort_values(ascending=False)\ntrain_null = train_null[train_null>0]\n\ntest_null = testset.isnull().sum().sort_values(ascending=False)\ntest_null = test_null[test_null>0]\nmissing_df = pd.concat([train_null,test_null, train_null\/len(trainingset),test_null\/len(testset), trainingset[train_null.index].dtypes, testset[test_null.index].dtypes], axis=1)\nmissing_df.rename({0: \"train_null\", 1: \"test_null\", 2: \"train_null raito\", 3: \"test_null raito\", 4: \"dtype_train\", 5: \"dtype_test\" },  axis='columns')","003333ca":"trainingset.SalePrice.describe()","3611b131":"sns.distplot(trainingset.SalePrice)","1af6bdf8":"skew = trainingset.SalePrice.skew()\nkurt = trainingset.SalePrice.kurt()\nprint(\"Skew: {} \/  Kurt: {}\".format(skew, kurt))","50b2a22a":"# Replace missging Age values based on other columns\n# check correlation between different variables\nmatrix = trainingset.select_dtypes(exclude=\"object\").corr()\nmask = np.triu(np.ones_like(matrix, dtype=bool))\ncmap = sns.diverging_palette(220, 25, s=80, n=9, as_cmap=True, center=\"light\")\nplt.figure(figsize=(12, 10))\nsns.heatmap(matrix, mask = mask, annot=False, cmap=cmap, square=True, fmt='.2f',linewidth=.2, center=0, vmin=-0.15, vmax=0.55)\nplt.show()\n","f5570e4c":"# Pick top 10 features that highly corelated to SalePrice\nk = 10 #number of variables for heatmap\ncol_names = matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncoef_matrix = np.corrcoef(trainingset[col_names].values.T)\nmask = np.triu(np.ones_like(coef_matrix, dtype=bool))\ncmap = sns.diverging_palette(220, 25, s=80, n=9, as_cmap=True, center=\"light\")\nplt.figure(figsize=(10, 10))\nsns.heatmap(coef_matrix, cbar=True, mask = mask, annot=True, cmap=cmap, square=True, fmt='.2f',linewidth=.2, center=0, vmin=-0.15, vmax=0.55, yticklabels=col_names.values, xticklabels=col_names.values)\nplt.show()","64fbc9b5":"# plot multiple subplots to validate correlations\nfig, axs = plt.subplots(2, 2, figsize=(10,8))\naxs[0, 0].plot(trainingset.GrLivArea, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[0, 0].set_title('GrLivArea vs SalePrice')\naxs[0, 1].plot(trainingset.GarageArea, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[0, 1].set_title('GarageArea vs SalePrice')\naxs[1, 0].plot(trainingset.TotalBsmtSF, trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[1, 0].set_title('TotalBsmtSF vs SalePrice')\naxs[1, 1].plot(trainingset[\"1stFlrSF\"], trainingset.SalePrice, marker=\"o\", linestyle=\"\", markersize=1.2, alpha=.8)\naxs[1, 1].set_title('1stFlrSF vs SalePrice')\n\nfig.tight_layout()","94a3d9a9":"fig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Categorical data vs SalePrice')\n\nsns.boxplot(ax=axes[0, 0], data=trainingset, x='OverallQual', y='SalePrice')\nsns.boxplot(ax=axes[0, 1], data=trainingset, x='GarageCars', y='SalePrice')\nsns.boxplot(ax=axes[1, 0], data=trainingset, x='FullBath', y='SalePrice')\nsns.boxplot(ax=axes[1, 1], data=trainingset, x='TotRmsAbvGrd', y='SalePrice')\n\n\n","e181400d":"# removing two outliers in GrLivArea\nprint(\"Before: {}\".format(trainingset.shape))\noutlier_index = trainingset[(trainingset.GrLivArea > 4000) & (trainingset.SalePrice < 200000)].index\ntrainingset = trainingset.drop(outlier_index)\nprint(\"After: {}\".format(trainingset.shape))\n","cdfe5adf":"#### Investigate 'LotFrontage'\n# After checking plots below, droped two outliers in LotArea(one row)  and LotFrontage (two rows)\n# sns.scatterplot(trainingset.LotArea, trainingset.SalePrice)\n# sns.scatterplot(trainingset.LotFrontage, trainingset.SalePrice)\n\n# removing outliers in LotFrontage and LotArea\ntrainingset = trainingset.loc[trainingset.LotArea != max(trainingset.LotArea), :] # one row\ntrainingset = trainingset.loc[trainingset.LotFrontage != max(trainingset.LotFrontage), :] # one row\nprint(\"trainingset data size: {}\".format(trainingset.shape))\nprint(\"After: {}\".format(trainingset.shape))","d0377e87":"nullcols_train = [col for col in trainingset.columns if trainingset[col].isnull().any()]\nnullcols_test = [col for col in testset.columns if testset[col].isnull().any()]\nprint(nullcols_train, len(nullcols_train))\nprint(nullcols_test, len(nullcols_test))","ba73eee2":"# Visualise missing values.\nmsno.matrix(trainingset.loc[:, nullcols_train])\n# #msno.matrix(trainingset.select_dtypes(include=\"object\")) \n# #msno.matrix(testset.loc[:, test_null.index])","d68908b6":"# PoolQC : data description says NA means \"No Pool\". \n# MiscFeature : data description says NA means \"no misc feature\".\n# Alley : data description says NA means \"no alley access\".\n# Fence: data description says NA means \"no fence\". \n# FireplaceQu: data description says NA meand \"NoFireplace\". \n# Features start with 'Garage' in the trainingset have the same number of missing data. \n#-----GarageType, GarageFinish,GarageCond, GarageQual : data description says NA means \"No Garage\". \n#-----GarageYrBlt has NA beacause the property does not have garage.\n#-----GarageArea and GarageCars in the testset has one missing value each beacuse the property does not have a garage according to GarageQual column.\n# Features start with 'Bsmtuniquere missing same data.\n#-----BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 : data description says NA means \"No Basement\". \n#-----'BsmtFullBath','BsmtHalfBath','BsmtUnfSF', 'TotalBsmtSF' : data is missing because there is no basement.\n# MasVnrType, MasVnrArea : Data is missing probably there is no veneer. (8 values are missing in the trainingset.)\nvalues = {\n    \"PoolQC\": \"None\",\n    \"MiscFeature\": \"None\",\n    \"Alley\": \"None\",\n    \"Fence\": \"None\",\n    \"FireplaceQu\": \"None\",\n    \"GarageType\": \"None\",\n    \"GarageFinish\": \"None\",    \n    \"GarageCond\": \"None\",\n    \"GarageQual\": \"None\",\n    \"GarageYrBlt\": 1880.0, #No garage, give an artificial year here    \n    \"GarageArea\": 0,#No garage\n    \"GarageCars\": 0,#No garage\n    \"BsmtExposure\": \"None\",#No basement\n    \"BsmtFinType2\": \"None\",#No basement\n    \"BsmtFinType1\": \"None\",#No basement\n    \"BsmtCond\": \"None\",#No basement\n    \"BsmtQual\": \"None\",#No basement\n    \"BsmtFullBath\":0, #No basement\n    \"BsmtHalfBath\":0, #No basement\n    \"BsmtUnfSF\":0, #No basement\n    \"BsmtFinSF2\":0, #No basement\n    \"BsmtFinSF1\":0, #No basement\n    \"TotalBsmtSF\":0, #No basement\n    \"MasVnrArea\": 0,#No Veneer\n    \"MasVnrType\": \"None\"\n}\n\ntrainingset.fillna(value=values, inplace=True)\ntestset.fillna(value=values, inplace=True)\n","350634ad":"# Electrical, MSZoning,Exterior2nd, Exterior1st, SaleType, KitchenQual: these columns have one or two missing values, \n# imputing missing values with the most common category in the corresponding column.\nmost_common_cols = [\"Electrical\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"KitchenQual\"]\nfor col in most_common_cols:\n    trainingset[col] = trainingset[col].fillna(trainingset[col].mode()[0])\n    testset[col] = testset[col].fillna(testset[col].mode()[0])\n\n#data description says \"Assume typical unless deductions are warranted\"\ntestset[\"Functional\"] = testset[\"Functional\"].fillna(\"Typ\") \n\n#LotFrontage : Since the area of each street connected to the house property most likely have \n# a similar area to other houses in its neighborhood, fill in missing values by the median LotFrontage of the same neighborhood.\ntrainingset[\"LotFrontage\"] = trainingset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\ntestset[\"LotFrontage\"] = testset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n","f7de1287":"# Check low variance columns in categorical columns\nresults = {}\ndef valueCount(df):\n    df_filter = df.select_dtypes(include=\"object\")\n    for col in df_filter.columns:\n        result = df_filter[col].value_counts(normalize=True) \n        results[\"------\" + col + \"------\"] = result\n    return results\n\nvalueCount(trainingset)","f22ea538":"# 99% of Street has only one value \"Pave\" \nlow_variance_cols = ['Street','Utilities', 'Heating','RoofMatl','Condition2', 'PoolQC', 'MiscFeature'] \nfor col in low_variance_cols:\n    print(trainingset[col].value_counts(normalize=True)) \ntrainingset = trainingset.drop(low_variance_cols,axis=1) # drop 7 columns\ntestset = testset.drop(low_variance_cols,axis=1) # drop 7 columns","899dd7d8":"# check if all missing data are imputed.\nprint(trainingset.isnull().sum().any())\nprint(testset.isnull().sum().any())\n# Check data shapes\nprint(trainingset.shape)\nprint(testset.shape)","da8196e5":"#Before handling skewness in SalePrice\n#histogram and normal probability plot\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\nsns.distplot(ax=axs[0], x=trainingset['SalePrice'], fit=norm);\nstats.probplot(x= trainingset['SalePrice'], plot=plt)\nplt.show()\n\n# After handling skewness\n# Applying log transformation on columns without zero values\ntrainingset[\"SalePrice\"] = np.log(trainingset[\"SalePrice\"])\n\n# Transformed histogram and normal probability plot\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\nsns.distplot(ax=axs[0], x=trainingset['SalePrice'], fit=norm);\nstats.probplot(x= trainingset['SalePrice'], plot=plt)\nplt.show()","b6e8b712":"numerical_cols = trainingset.select_dtypes(exclude=\"object\").columns\nnumerical_cols = numerical_cols.drop(\"Id\")\nhighly_skewed_cols = [col for col in numerical_cols if np.abs(trainingset[col].skew()) > 0.75] #20 columns\n\n# Using boxcox from scipy library to fix skewness\nlam = 0.15\nfor col in highly_skewed_cols:\n    trainingset[col] = boxcox1p(trainingset[col], lam)\n    testset[col] = boxcox1p(testset[col], lam)","15bdc913":"trainingset[highly_skewed_cols].hist(figsize=(12,12))\nplt.show()","e441f2fb":"sns.scatterplot(trainingset.GrLivArea, trainingset.SalePrice)","9e940b6e":"sns.scatterplot(trainingset[\"1stFlrSF\"], trainingset.SalePrice)","e14693cb":"ordinal_cols = [\"ExterQual\", \"ExterCond\", \"BsmtCond\", \"BsmtQual\",\"HeatingQC\", \"KitchenQual\", \n                \"FireplaceQu\",\"GarageQual\", \"GarageCond\", \"BsmtFinType1\", \"BsmtFinType2\"]\nbin_map  = {'Ex':4, 'Gd':3,'TA':2, 'Fa':1,'Po':1,  'None':0, \n            \"GLQ\" : 6, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5\n            }\nfor col in ordinal_cols:\n    trainingset[col] = trainingset[col].map(bin_map)\n    testset[col] = testset[col].map(bin_map)\n\n    \nBsmtExposure = {\"Gd\" : 4, \"Av\": 3, \"Mn\" : 2, 'No': 1, \"None\":0}\ntrainingset['BsmtExposure'] = trainingset['BsmtExposure'].map(BsmtExposure)\ntestset['BsmtExposure'] = testset['BsmtExposure'].map(BsmtExposure)\n\nPavedDrive =   {\"Y\" : 2, \"N\" : 0, \"P\" : 1}\ntrainingset['PavedDrive'] = trainingset['PavedDrive'].map(PavedDrive)\ntestset['PavedDrive'] = testset['PavedDrive'].map(PavedDrive)","c0086fe3":"print(trainingset.shape)\nprint(testset.shape)","53f383d6":"print(trainingset.isnull().sum().any())\nprint(testset.isnull().sum().any())","15575a57":"# print(\"{}{}\".format(trainingset.shape, testset.shape))\n\n# # Spliting Data\n# X = trainingset.drop([\"Id\",\"SalePrice\"], axis=1)\n# y = trainingset.SalePrice\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# print(\"{}{}{}{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n\n# result = {}\n# def evaluation(y, y_pred):\n#     r_squared = r2_score(y, y_pred),\n#     mae = mean_absolute_error(y, y_pred),\n#     mse = mean_squared_error(y, y_pred),\n#     rmse = np.sqrt(mean_squared_error(y, y_pred))\n#     return r_squared, mae, mse, rmse\n\n# low_cardinality_cols = [col for col in X_train.columns if (X_train[col].dtype==\"object\") & (X_train[col].nunique()<26)]\n# #print(len(low_cardinality_cols), low_cardinality_cols)\n\n# column_trans = make_column_transformer(\n#     (OneHotEncoder(handle_unknown=\"ignore\", sparse=False),low_cardinality_cols),\n#     remainder='passthrough'\n# )\n# sc = StandardScaler()\n\n\n# model = Lasso() \n# pipe = make_pipeline(column_trans, sc, model) \n# params = {\"lasso__alpha\": np.arange(0.001,0.005,0.001)} #print(pipe.get_params().keys())\n# cv_lasso = GridSearchCV(pipe, param_grid=params)\n# cv_lasso.fit(X_train, y_train)\n# preds_lasso = cv_lasso.predict(X_test)\n# preds_lasso = np.exp(preds_lasso) #inverse logrithm on predicted SalePrice.\n\n\n# lasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(y_test, preds_lasso) \n# print(lasso_r2,lasso_mae, lasso_mse, lasso_rmse)\n# print(cv_lasso.best_params_, cv_lasso.best_score_)\n\n# ############################\n# # Plot feature graph\n\n# temp_df = pd.DataFrame(column_trans.fit_transform(X_train), columns=column_trans.get_feature_names())\n# cols = temp_df.columns \n# temp_df = pd.DataFrame(sc.fit_transform(temp_df), columns=cols)\n# names = cols\n# lasso = Lasso(alpha=0.003)\n# lasso_coef = lasso.fit(temp_df, y_train).coef_\n\n# # pick important features\n# names_n = np.array(names)[np.abs(lasso_coef)>0.005]\n# lasso_coef_n = lasso_coef[np.abs(lasso_coef)>0.005]\n# print(len(names_n),len(lasso_coef_n))\n\n# _ = plt.figure(figsize=(8,8))\n# _ = plt.plot(range(len(names_n)), lasso_coef_n,  'bo-')\n# _ = plt.xticks(range(len(names_n)), names_n, rotation=90)\n# _ = plt.ylabel('Coefficients')\n# plt.show()","38024b8f":"# cat_names = ['MSZoning', 'Alley', 'LotShape','LandContour', 'LotConfig', 'LandSlope',\n#                    'Neighborhood', 'Condition1', 'BldgType','HouseStyle', \n#                    'RoofStyle', 'Exterior1st','Exterior2nd', 'MasVnrType', 'Foundation',\n#                    'CentralAir', 'Electrical', 'Functional',\n#                    'GarageType', 'GarageFinish', 'Fence','SaleType', 'SaleCondition']\n# for i, v in enumerate(cat_names):\n#     print(i, \":\", v)\n\n# #names_n\n\n############# Selected features after fixing the skewness ############# \n# # # # Threshold: abs(Lasso coef)> 0 (after fixed skewness)\n# features_53 = ['MSZoning','Alley','LotShape', 'LandContour','LotConfig', 'LandSlope', 'Neighborhood',\n# 'Condition1','BldgType', 'HouseStyle','RoofStyle', 'Exterior1st', 'Exterior2nd','MasVnrType','Foundation',\n# 'CentralAir', 'Functional', 'GarageType', 'GarageFinish','Fence',  'SaleType','SaleCondition',\n# 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond','YearBuilt', 'YearRemodAdd', 'ExterQual', 'BsmtQual',\n# 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'HeatingQC', '1stFlrSF', 'GrLivArea',\n# 'BsmtFullBath', 'FullBath', 'HalfBath', 'KitchenAbvGr','KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', \n#  'FireplaceQu','GarageCars', 'GarageArea', 'GarageQual', 'WoodDeckSF','OpenPorchSF', 'ScreenPorch', 'PoolArea']\n\n# # find highly correlated features\n# corr_matrix = trainingset[features_53].corr().abs()\n# # Select upper triangle of correlation matrix\n# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# # Find features with correlation greater than 0.8\n# to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n# print(to_drop)\n# feature_selection = set(features_53) - set(to_drop)\n# print(len(feature_selection))\n\n\n############# Selected features before fixing the skewness ############# \n# # Threshold: abs(Lasso coef)> 0\n# selected_features_64 = ['MSZoning', 'Alley', 'LotShape','LandContour', 'LotConfig', 'LandSlope', \n#                         'Neighborhood', 'Condition1', 'BldgType','HouseStyle', 'RoofStyle', \n#                         'Exterior1st','Exterior2nd', 'MasVnrType', 'Foundation',\n#                         'Functional','GarageType', 'GarageFinish', 'Fence','SaleType', 'SaleCondition',\n#                         'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual','OverallCond', \n#                         'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'ExterQual', 'ExterCond', \n#                         'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', \n#                         'TotalBsmtSF','HeatingQC', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', \n#                         'HalfBath', 'BedroomAbvGr','KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', \n#                         'Fireplaces', 'FireplaceQu', 'GarageYrBlt', 'GarageCars', 'GarageArea',\n#                         'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF','OpenPorchSF', \n#                         '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MoSold', 'YrSold']\n\n# # Threshold: abs(Lasso coef)> 1000\n# selected_features_48 = ['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'Condition1', 'BldgType','HouseStyle', 'Exterior1st',\n#  'Exterior2nd', 'MasVnrType', 'Foundation', 'Functional','GarageType',  'Fence','SaleCondition', 'MSSubClass', 'LotArea','OverallQual', 'OverallCond', 'YearBuilt', 'MasVnrArea','ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n# 'BsmtFinType1', 'BsmtFinSF1', 'TotalBsmtSF', '2ndFlrSF','LowQualFinSF', 'GrLivArea', \n# 'BsmtFullBath', 'HalfBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces',\n# 'FireplaceQu', 'GarageCars', 'GarageArea', 'GarageQual','WoodDeckSF', 'OpenPorchSF', \n# '3SsnPorch', 'ScreenPorch','PoolArea', 'MoSold']\n\n\n# # Threshold: abs(Lasso coef)> 2000\n# selected_features_31 = ['LandSlope', 'Neighborhood','BldgType','HouseStyle', \n#                         'Exterior1st','MasVnrType', 'Functional','GarageFinish','SaleCondition',\n#                         'MSSubClass', 'LotArea','OverallQual', 'OverallCond', 'YearBuilt', \n#                         'MasVnrArea','ExterQual', 'ExterCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinSF1',\n#                         'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'BedroomAbvGr','KitchenQual', \n#                         'TotRmsAbvGrd', 'GarageCars', 'GarageArea','GarageQual', 'OpenPorchSF', 'PoolArea']\n\n# # Threshold: abs(Lasso coef)> 7000\n# selected_features_5 = ['SaleType', 'OverallQual', 'YearBuilt', 'BsmtFinSF1','GrLivArea']\n\n\n","afc3411a":"# # Threshold: abs(Lasso coef)> 0.005 (after fixed skewness)\n# features_36 = ['MSZoning', 'LotConfig','Neighborhood', 'Condition1', 'BldgType', 'Exterior1st','MasVnrType', \n#  'Foundation','CentralAir', 'Functional','SaleType', 'SaleCondition', 'LotArea', 'OverallQual',\n#  'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual','BsmtQual', 'BsmtExposure', \n#  'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC', '1stFlrSF', 'GrLivArea', 'BsmtFullBath',\n#  'HalfBath', 'KitchenAbvGr', 'KitchenQual','Fireplaces', 'GarageCars', 'GarageArea', 'GarageQual',\n#  'WoodDeckSF', 'ScreenPorch']\n\n############# Remove redundant features #################\n# removed two highly correlated features: 1stFlrSF and GarageCards\n# 1stFlrSF and TotalBsmtSF are highly correlated. (0.82)\n# TotRmsAbvGrd and GrLiveArea are also highly correlated (0.83)\n# GarageArea and GarageCards are highly correlated (0.88)\nfeature_selection = ['MSZoning', 'LotConfig','Neighborhood', 'Condition1', 'BldgType', 'Exterior1st','MasVnrType', \n 'Foundation','CentralAir', 'Functional','SaleType', 'SaleCondition', 'LotArea', 'OverallQual',\n 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'ExterQual','BsmtQual', 'BsmtExposure', \n 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC',  'GrLivArea', 'BsmtFullBath',\n 'HalfBath', 'KitchenAbvGr', 'KitchenQual','Fireplaces', 'GarageArea', 'GarageQual',\n 'WoodDeckSF', 'ScreenPorch']\n\n\n","98000827":"print(\"{}{}\".format(trainingset.shape, testset.shape))\n\n# Spliting Data\nX = trainingset[feature_selection]\ny = trainingset.SalePrice\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"{}{}{}{}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))","ea204a79":"result = {}\ndef evaluation(y, y_pred):\n    r_squared = r2_score(y, y_pred),\n    mae = mean_absolute_error(y, y_pred),\n    mse = mean_squared_error(y, y_pred),\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    return r_squared, mae, mse, rmse","dba5b0d9":"low_cardinality_cols = [col for col in X_train.columns if (X_train[col].dtype==\"object\") & (X_train[col].nunique()<26)]\n#print(len(low_cardinality_cols), low_cardinality_cols)\n\ncolumn_trans = make_column_transformer(\n    (OneHotEncoder(handle_unknown=\"ignore\", sparse=False),low_cardinality_cols),\n    remainder='passthrough'\n)\nsc = StandardScaler()","78b6d8ed":"model = LinearRegression() \npipe = make_pipeline(column_trans, model) \npipe.fit(X_train, y_train)\npreds_lm = pipe.predict(X_test)\npreds_lm = np.exp(preds_lm) #inverse logrithm on predicted SalePrice.\nprint(pipe.score(X_test, y_test))\npd.DataFrame(preds_lm.reshape(len(preds_lm),1)).describe()","965bb850":"model = Lasso() \npipe = make_pipeline(column_trans, sc, model) \nparams = {\"lasso__alpha\": np.arange(0.001,0.002,0.0001)} #print(pipe.get_params().keys())\ncv_lasso = GridSearchCV(pipe, param_grid=params)\ncv_lasso.fit(X_train, y_train)\npreds_lasso = cv_lasso.predict(X_test)\npreds_lasso = np.exp(preds_lasso) #inverse logrithm on predicted SalePrice.\nlasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(y_test, preds_lasso) \nprint(cv_lasso.best_params_, cv_lasso.best_score_)","438862e8":"model = Ridge() \npipe = make_pipeline(column_trans, sc, model) \nparams = {\"ridge__alpha\": np.arange(1,100,10)} #print(pipe.get_params().keys())\ncv_ridge = GridSearchCV(pipe, param_grid=params)\ncv_ridge.fit(X_train, y_train)\npreds_ridge = cv_ridge.predict(X_test)\npreds_ridge = np.exp(preds_ridge) #inverse logrithm on predicted SalePrice.\nprint(cv_ridge.best_params_, cv_ridge.best_score_)\n","b6b42aba":"model = ElasticNet()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"elasticnet__alpha\": np.arange(0,0.2,0.01)} \ncv_elastic = GridSearchCV(pipe, param_grid=params)\ncv_elastic.fit(X_train, y_train)\npreds_elastic = cv_elastic.predict(X_test)\npreds_elastic = np.exp(preds_elastic) #inverse logrithm on predicted SalePrice.\nprint(cv_elastic.best_params_, cv_elastic.best_score_)\n","28d8bf9a":"model = SVR()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"svr__C\": np.arange(1,1.6,0.1)} \ncv_svr = GridSearchCV(pipe, param_grid=params)\ncv_svr.fit(X_train, y_train)\npreds_svr = cv_svr.predict(X_test)\npreds_svr = np.exp(preds_svr) #inverse logrithm on predicted SalePrice.\nprint(cv_svr.best_params_, cv_svr.best_score_)\n","5d9e4cf1":"model = SGDRegressor()\npipe = make_pipeline(column_trans, sc, model) \n#print(pipe.get_params().keys())\nparams = {\"sgdregressor__tol\": [1e-3], #stopping criteria\n          \"sgdregressor__n_iter_no_change\": [200],\n          \"sgdregressor__eta0\": [0.006, 0.009, 0.01, 0.03], \n          \"sgdregressor__alpha\": np.arange(0.005,0.1,0.005),\n          \"sgdregressor__random_state\": [42]\n         }\ncv_sgd = GridSearchCV(pipe, param_grid=params)\ncv_sgd.fit(X_train, y_train)\npreds_sgd = cv_sgd.predict(X_test)\npreds_sgd = np.exp(preds_sgd) #inverse logrithm on predicted SalePrice.\nprint(cv_sgd.best_params_, cv_sgd.best_score_)\n\n\npd.DataFrame(preds_sgd.reshape(len(preds_sgd),1)).describe()","d10b37c0":"model = RandomForestRegressor()\npipe = make_pipeline(column_trans, model) \n#print(pipe.get_params().keys())\nparams = {\"randomforestregressor__n_estimators\": np.arange(50,150,50)} \ncv_rf = GridSearchCV(pipe, param_grid=params)\ncv_rf.fit(X_train, y_train)\npreds_rf = cv_rf.predict(X_test)\npreds_rf = np.exp(preds_rf) #inverse logrithm on predicted SalePrice.\npd.DataFrame(preds_rf.reshape(len(preds_rf),1)).describe()\nprint(cv_rf.best_params_, cv_rf.best_score_)\n","8ddfbc3d":"model = XGBRegressor()\npipe = make_pipeline(column_trans, model) \n#print(pipe.get_params().keys())\nparams = {\"xgbregressor__n_estimators\": np.arange(100,200,50), \"xgbregressor__learning_rate\": np.arange(0.08,0.12,0.01)} \ncv_xg = GridSearchCV(pipe, param_grid=params)\ncv_xg.fit(X_train, y_train)\npreds_xg = cv_xg.predict(X_test)\npreds_xg = np.exp(preds_xg) #inverse logrithm on predicted SalePrice.\npd.DataFrame(preds_xg.reshape(len(preds_xg),1)).describe()\nprint(cv_xg.best_params_, cv_xg.best_score_)\n","3fc2b41c":"lm_r2, lm_mae, lm_mse, lm_rmse = evaluation(np.exp(y_test), preds_lm) \nresult[\"Liner\"] = [lm_r2, lm_mae, lm_mse, lm_rmse]\n\nlasso_r2, lasso_mae, lasso_mse, lasso_rmse = evaluation(np.exp(y_test), preds_lasso) \nresult[\"Lasso\"] = [lasso_r2,lasso_mae, lasso_mse, lasso_rmse]\n\nridge_r2, ridge_mae, ridge_mse, ridge_rmse = evaluation(np.exp(y_test), preds_ridge) \nresult[\"Ridge\"] = [ridge_r2, ridge_mae, ridge_mse, ridge_rmse]\n\nelastic_r2, elastic_mae, elastic_mse, elastic_rmse = evaluation(np.exp(y_test), preds_elastic) \nresult[\"Elastic\"] = [elastic_r2, elastic_mae, elastic_mse, elastic_rmse]\n\nsvr_r2, svr_mae, svr_mse, svr_rmse = evaluation(np.exp(y_test), preds_svr) \nresult[\"SVR\"] = [svr_r2, svr_mae, svr_mse, svr_rmse]\n\nrf_r2, rf_mae, rf_mse, rf_rmse = evaluation(np.exp(y_test), preds_rf) \nresult[\"RandomForest\"] = [rf_r2, rf_mae, rf_mse, rf_rmse]\n\nxg_r2, xg_mae, xg_mse, xg_rmse = evaluation(np.exp(y_test), preds_rf) \nresult[\"XGBoost\"] = [xg_r2, xg_mae, xg_mse, xg_rmse]\n\n\nsgd_r2, sgd_mae, sgd_mse, sgd_rmse = evaluation(np.exp(y_test), preds_sgd) \nresult[\"Stochastic\"] = [sgd_r2, sgd_mae, sgd_mse, sgd_rmse]\n\n\n","974d2cb5":"result = pd.DataFrame.from_dict(result, orient='index', columns=[\"R2 Score\",\"MAE\",\"MSE\",\"RMSE\"])\nresult.sort_values(by=\"RMSE\")\n","0f88a04d":"plt.figure(figsize=(8,8))\nsns.barplot(result.index,result.RMSE)","e707017d":"test_X = testset[feature_selection]\n# model = RandomForestRegressor(n_estimators=100)\n# pipe = make_pipeline(column_trans, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# pd.DataFrame(predictions.reshape(len(predictions),1)).describe()\n\n\n# model = XGBRegressor(n_estimators=150, learning_rate=0.08)\n# pipe = make_pipeline(column_trans, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# predictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\n\nmodel = Lasso(alpha=0.0014) \npipe = make_pipeline(column_trans, sc, model) \npipe.fit(X_train, y_train)\npredictions = pipe.predict(test_X)\npredictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\npd.DataFrame(predictions.reshape(len(predictions),1)).describe()\n\n\n# model = Ridge(alpha=21) \n# pipe = make_pipeline(column_trans, sc, model) \n# pipe.fit(X_train, y_train)\n# predictions = pipe.predict(test_X)\n# predictions = np.exp(predictions) #inverse logrithm on predicted SalePrice.\n# pd.DataFrame(predictions.reshape(len(predictions),1)).describe()","f08351db":"output_dic = {\"Id\": testset[\"Id\"], \"SalePrice\": predictions}\noutput_df = pd.DataFrame(output_dic)\noutput_df.to_csv(\"submission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","fda0d9ad":"from sklearn.model_selection import learning_curve\n# Use learning curve to get training and test scores along with train sizes\ntrain_sizes, train_scores, test_scores = learning_curve(\n    estimator=pipe, \n    X=X_train, \n    y=y_train,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    n_jobs=1,\n    cv=5\n)\n\n#\n# Calculate training and test mean and std\n#\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n#\n# Plot the learning curve\n#\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.title('Learning Curve')\nplt.xlabel('Training Data Size')\nplt.ylabel('Accuracy')\nplt.grid()\nplt.legend(loc='lower right')\nplt.show()","ab49758b":"############# Fix Skewness ##################\n# #trainingset.loc[:,highly_skewed_cols].describe()\n# has_zero_cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '2ndFlrSF', 'LowQualFinSF', \n# 'KitchenAbvGr', 'BsmtHalfBath',  'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n#  'MiscVal']\n# non_zero_cols = ['MSSubClass', 'LotArea', '1stFlrSF', 'GrLivArea','GarageYrBlt']\n\n# for col in non_zero_cols:\n#     trainingset[col] = np.log(trainingset[col])\n\n# # Handle cols with zero values\n# for col in has_zero_cols:\n#     trainingset[col + '_flag'] = trainingset[col]>0\n#     trainingset.loc[trainingset[col + '_flag']==1,col] = np.log(trainingset[col])\n\n# # Drop columns with _flag where I created to perform log transformation on the columns contain zero values.\n# trainingset = trainingset[trainingset.columns.drop(list(trainingset.filter(regex='_flag')))]\n\n\n","8dcda4be":"### Check homoscedasticity","ae64d4d4":"Tried with different threshold of lasso coef and finally selected the best performing threshold.\n* np.abs(lasso_coef)>0.005, which gave me 36 important features.","108110f0":"#### Handle Low Variance columns","58ed0226":"#### Ridge","84565e8a":"I am staring out my DS journey to practice what I have leaned so far. This competetion helped me to learn how to impute missing data, how to fix skewness, how to select features using Lasso, and how to fine-tune the parameters for the simple ML models such as Lasso and Ridge. \n\n\n\n\n\n","561d7fd8":"#### ElasticNet","34ab015b":"Note: Train and test data have missing values in diffent columns. Handle all missing values later in the data processing stage.","57d7a0b5":"### Handling Outliers","90c2b203":"# Build Models with selected features","0a60aa1e":"## Basic EDA","3149e1d8":"## Data Processing","acdaa249":"### Handling Missing Values\n##### Notes:\n* NA values can cause errors with machine learning later down the line, so I will impute the missing values.\n* PoolQC, MiscFeature, Alley, Fence columns have more than 80% of missing values. If there are no good ways to figure out the values, I will drop these columns. The rest columns (FireplaceQu, Lotfrontage etc.) need to impute missing values.\n* Based on MSNO graph, features start with 'Garage' in the trainingset have the same number of missing data. \n* Based on MSNO graph, Features start with 'Bsmt' in the trainingset have the same number of missing data. ","004e266e":"#### 1.2 Bivariate analysis","c18d9c2b":"#### Randome Forest Resgression","108aac08":"* SalePrice is highly corelated with \n'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt'","2a186fbc":"#### Handle oridinal columns","daba8558":"### Experiment: Learning Curve","4f347ac0":"#### Stochstic Gradient Descent","8963fd01":"### Model comparison","cb64c8ca":"#### 1.1 Analyse target variable: SalePrice\n* SalePrice has positive skewness.","d6a7d52e":"#### Linear Regression","96aae869":"* There are outliers in GrLivArea (bottom right corner), TotalBsmtSF, and 1stFlrSF\n* After checking these outliers, ID 1298 is the common outliers for all three features, remove this later.\n* GrLivArea also has another outlier ID 523, removing this later too.\n\n* 1stFlrSF and TotalBsmtSF are highly correlated. (0.82)\n* TotRmsAbvGrd and GrLiveArea are also highly correlated (0.83)\n* GarageArea and GarageCards are highly correlated (0.88)\n\n","bd833b43":"## References","4132c139":"# Feature selection with Lasso","e252c249":"* Select low variance columns from the value_counts above and remove them as these columns are dominated by one single value and not informative to our target at all.\n* For instance, 99% of Street has only one value \"Pave\" ","41bf6069":"Feature selection lecture: [UW Machine Learning: Regression](https:\/\/www.coursera.org\/learn\/ml-regression?specialization=machine-learning)\n    \nI've read some great notebooks in Kaggle. Regarding imputing missing values, I refered the notebook below.\n* [Handling Missing Values](https:\/\/www.kaggle.com\/dansbecker\/handling-missing-values ) by DANB.\n* [Using Categorical Data with One Hot Encoding](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding) by DANB.\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne.\n\nThe note below has a stragetic approach about EDA. \n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by PEDRO MARCELINO\n\nTransforming skewed data (why and how)\n* [Skewed Data: A problem to your statistical model](https:\/\/towardsdatascience.com\/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37)\n* [Transforming Skewed Data for Machine Learning](https:\/\/odsc.medium.com\/transforming-skewed-data-for-machine-learning-90e6cc364b0)\n* https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa\n\n","ad3e9189":"#### XGBoost","4ff461ae":"\n### Output","8c508117":"#### SVM","a21dcd9e":"#### Handling Skewness in data\nIn skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model\u2019s performance especially regression-based models. There are statistical model that are robust to outlier like a Tree-based models but it will limit the possibility to try other models. So there is a necessity to transform the skewed data to close enough to a Gaussian distribution or Normal distribution. This will allow us to try more number of statistical model.\n\n\n* Before fixing skewness in data, tree-based models performed better.\n* After fixing skewness in data, linear models performed better.\n\nThe rule of thumb seems to be:\n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n* If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n* If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.","a61864b2":"I ran the following code to select important features first and then fit various models only with the selected features.","ff723001":"#### Lasso"}}