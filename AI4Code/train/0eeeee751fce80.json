{"cell_type":{"7c005b5e":"code","2b8ba942":"code","55570f94":"code","5350ad00":"code","95ef8f55":"code","556a0295":"code","2c31faa5":"code","c1b6cc85":"code","82daaeb6":"code","7bf327ca":"code","ee10ffda":"code","af6778d2":"code","24b013ea":"code","b983b1aa":"markdown","95fe0e9f":"markdown","913566e7":"markdown","c0ca5fbe":"markdown","9f1dacdd":"markdown","2ca93da5":"markdown","c9534152":"markdown","bea6579f":"markdown","d62a1ff6":"markdown","6ca0d25e":"markdown","21b172b1":"markdown","8d582bcf":"markdown"},"source":{"7c005b5e":"import torch.nn as nn\nimport torch\nimport transformers\nimport pandas as pd \nimport numpy as np\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AdamW\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import logging\nlogging.set_verbosity_warning()","2b8ba942":"# this is the maximum number of tokens in the sentence\nMAX_LEN = 512\n# batch sizes for pytorch dataloaders\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 10\nTRAINING_FILE = \"..\/input\/imdb-sentiment-10k-reviews-binary-classification\/imdb_10K_sentimnets_reviews.csv\"","55570f94":"TOKENIZER = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","5350ad00":"class BertDataSetPytorch:\n    def __init__(self, review, sentiment):\n        self.review = review\n        self.sentiment = sentiment\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    def __len__(self):\n        return len(self.review)\n    def __getitem__(self, item):\n        review = str(self.review[item])\n        review = \" \".join(review.split())\n        # Tokenize with padding and max lenght of the sentence\n        inputs = self.tokenizer.encode_plus(review, None, add_special_tokens=True, truncation=True, max_length=self.max_len, pad_to_max_length=True)\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n                \"mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n                \"targets\": torch.tensor(self.sentiment[item], dtype=torch.float)}\n# Test\ntest_data_ = pd.read_csv(TRAINING_FILE, nrows=3) # just 3 reviews\ntest_bert_class_ = BertDataSetPytorch(test_data_.review, test_data_.sentiment)\nlen(test_bert_class_[0]['ids'])\nprint(f'The number of tokens in the sentence is {MAX_LEN}')","95ef8f55":"class BertPretrainedUncased(nn.Module):\n    def __init__(self):\n        super(BertPretrainedUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", return_dict=False) # Here we load from pretrained\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n    def forward(self, ids, mask, token_type_ids):\n        ids, results = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        drop_out = self.drop(results)\n        out_layer = self.out(drop_out)\n        return out_layer\n# test model and print # params\ntest_bert_model_ = BertPretrainedUncased()\nprint(f'number of parameters {test_bert_model_.bert.num_parameters()}')","556a0295":"def loss(results, targets):\n    return nn.BCEWithLogitsLoss()(results, targets.view(-1,1))\n\n# test loss with random results \ninp_ = torch.randn(3, requires_grad=True).view(-1,1)\nout_ = torch.empty(3).random_(2)\nff_ = loss(inp_, out_)\nprint(ff_)","2c31faa5":"def train(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    for data in data_loader:\n        # prepare data        \n        ids = data[\"ids\"]\n        token_type_ids = data[\"token_type_ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n        # to device\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        # grads to zero\n        optimizer.zero_grad()\n        # get the model result\n        out = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        # loss\n        ls = loss(out, targets)\n        # backword gradients\n        ls.backward()\n        # steps of optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\n        \ndef validate(data_loader, model, device):\n    model.eval()\n    target_list = []\n    output_list = []\n    with torch.no_grad():\n        for data in data_loader:\n            # prepare data        \n            ids = data[\"ids\"]\n            token_type_ids = data[\"token_type_ids\"]\n            mask = data[\"mask\"]\n            targets_ = data[\"targets\"]\n            # to device\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets_.to(device, dtype=torch.float)\n            # get results of current model\n            outs = model(ids, mask, token_type_ids)\n            # convert targets to cpu and extend the final list\n            targets__ = targets.cpu().detach()\n            target_list.extend(targets__.numpy().tolist())\n            # convert outs to cpu\n            out_ = torch.sigmoid(outs).cpu().detach()\n            output_list.extend(out_.numpy().tolist())\n    return output_list, target_list","c1b6cc85":"data = pd.read_csv(TRAINING_FILE).fillna('none')\n# We need to stratify (not distributed well)\ndata_train, data_valid = model_selection.train_test_split(data, test_size=0.10, random_state=11, stratify=data.sentiment.values)\ndata_train.reset_index(drop=True)\ndata_valid.reset_index(drop=True)\n\nprint(len(data_train), len(data_valid))\ndata_train.head(2)","82daaeb6":"train_data_set = BertDataSetPytorch(data_train.review.values, data_train.sentiment.values)\ntest_data_set = BertDataSetPytorch(data_valid.review.values, data_valid.sentiment.values)\n\nprint('params of the data tokenized')\nprint()\nprint(train_data_set.tokenizer)","7bf327ca":"train_data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=TRAIN_BATCH_SIZE, num_workers=2)\ntest_data_loader = torch.utils.data.DataLoader(test_data_set, batch_size=VALID_BATCH_SIZE, num_workers=1)\n\nprint(train_data_loader.dataset.review[0], 'AND REVIEW IS ', train_data_loader.dataset.sentiment[0]) # negative?","ee10ffda":"num_train_steps = int(len(data_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\nprint(num_train_steps)","af6778d2":"# cuda if exits \nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nprint(device)    \n# set model\nMODEL = BertPretrainedUncased()\n# send model to device \nMODEL.to(device)\n# params of the model\nparam_optimizer = list(MODEL.named_parameters())\n# get names of paraments to search \nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n# params space to optimize \noptimizer_parameters = [{\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\\\n                         \"weight_decay\": 0.001,},\n                        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\\\n                         \"weight_decay\": 0.0,},]\n# set optimizer\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\n# set scheduler to stop\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n\nprint(optimizer_parameters[0]['params'][0])","24b013ea":"accuracy = 0 # set initial \nsave_model = True # if we need to save model\n\nfor epoch in range(EPOCHS):\n    # train\n    print('start train')\n    train(train_data_loader, MODEL, optimizer, device, scheduler)\n    # evaluate\n    print('start validate')\n    outputs, targets = validate(test_data_loader, MODEL, device)\n    # all that less 0.5 is negative\n    outputs = np.array(outputs) >= 0.5\n    acc = accuracy_score(outputs, targets)\n    print(f'accuracy for {epoch} is {acc}')\n    if acc > accuracy:\n        accuracy = acc\n        if save_model:\n            torch.save(MODEL.state_dict(), '.\/model_bert.bin')\n            \nprint(f'best accuraccy is {acc}')","b983b1aa":"### Set number of training steps","95fe0e9f":"### Class that helps load data for Pytorch\n##### more here https:\/\/pytorch.org\/docs\/stable\/data.html#dataset-types","913566e7":"## How quickly train BERT model to solve binary classification task","c0ca5fbe":"### Train and validate functions","9f1dacdd":"### Prepare model and set paraments space\n#### more here \"Approaching (Almost) Any Machine Learning Problem\" https:\/\/github.com\/abhishekkrthakur\/approachingalmost, by p.270","2ca93da5":"### Load torch dataloader","c9534152":"### Loss Function","bea6579f":"### Prepate and split data","d62a1ff6":"### Conver data into tensors after being tokenezed","6ca0d25e":"### Class for BERT model\n#### more here \"Approaching (Almost) Any Machine Learning Problem\" https:\/\/github.com\/abhishekkrthakur\/approachingalmost, by p.256","21b172b1":"### Run model with params ","8d582bcf":"### Load pretained BERT model\n#### More models and docs here https:\/\/huggingface.co\/models"}}