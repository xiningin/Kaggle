{"cell_type":{"d96429ab":"code","2618051a":"code","18448b6b":"code","f1f3f567":"code","f745436b":"code","494a13d1":"code","5efa8fc7":"code","7c465940":"code","e843f7bd":"code","0ed79026":"code","ec1f5416":"code","a7b3a7cb":"code","f50d978b":"code","65add7d1":"code","9301c488":"code","6aea02ca":"markdown","d87fcfd9":"markdown","f39e1af4":"markdown","544b007c":"markdown","ceeca86e":"markdown","afc1d9ee":"markdown"},"source":{"d96429ab":"import os\nimport time\nimport shutil\nimport random\nimport cv2\nimport pandas as pd\nimport seaborn as sn\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.efficientnet import preprocess_input, EfficientNetB5\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom kaggle_datasets import KaggleDatasets\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2618051a":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","18448b6b":"EPOCHS = 15\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nWIDTH = 480\nHEIGHT = 480\nCHANNELS = 3\nLEARNING_RATE = 0.001\nCLASSES = 6\nSEED = 32\ntop_dropout_rate = 0.2\nAUTO = tf.data.experimental.AUTOTUNE","f1f3f567":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('fgvc8aug')\nTRAIN_PATH = GCS_DS_PATH + \"\/data_full_augmentation_images\/data_full_augmentation\/images\/\"\nprint(GCS_DS_PATH)","f745436b":"final_model = 'EfficientNet_B5_Fullaug.h5'\nhist_path = 'EfficientNet_B5_Fullaug_Hist.log'\ntrain_image = '..\/input\/fgvc8aug\/data_full_augmentation_images\/data_full_augmentation\/images'\ntrain_df = pd.read_csv('..\/input\/fgvc8aug\/data.csv', )","494a13d1":"train_df = train_df[[\"image\", \"labels\"]]\nmlb = MultiLabelBinarizer().fit(train_df.labels.apply(lambda x : x.split()))\nlabels = pd.DataFrame(mlb.transform(train_df.labels.apply(lambda x : x.split())), columns = mlb.classes_)\n\nlabels = pd.concat([train_df['image'], labels], axis=1)\nlabels.head()","5efa8fc7":"def format_path(st):\n    return TRAIN_PATH + st\n\ntrain_paths = labels.image.apply(format_path).values\n\ntrain_labels = np.float32(labels.loc[:, 'complex':'scab'].values)\ntrain_paths, valid_paths, train_labels, valid_labels =\\\ntrain_test_split(train_paths, train_labels, test_size=0.15, random_state=2020)","7c465940":"def process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label","e843f7bd":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(process_img, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(process_img, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","0ed79026":"with strategy.scope():  \n    base_model = EfficientNetB5(include_top = False, weights = 'imagenet')\n    base_model.trainabe = True\n\n    inputs = Input((HEIGHT, WIDTH, 3))\n    x = base_model(inputs, training = True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(top_dropout_rate)(x)\n    outputs = Dense(CLASSES, activation='sigmoid')(x)\n    \n    model = Model(inputs, outputs)\n    model.compile(optimizer ='adam' , loss='binary_crossentropy', metrics=['accuracy'])\n    model.summary()","ec1f5416":"checkpoint = ModelCheckpoint(\n    final_model,\n    monitor = 'val_accuracy',\n    mode = 'max',\n    save_best_only = True,\n    save_weights_only= False ,\n    perior = 1,\n    verbose = 1\n)\n\nearly_stopping = EarlyStopping(\n    monitor = 'val_accuracy',\n    mode = 'auto',\n    min_delta = 0.0001,\n    patience = 5,\n    baseline = None,\n    restore_best_weights = True,\n    verbose = 1\n)\ndef build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","a7b3a7cb":"lrfn = build_lrfn()\nSTEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","f50d978b":"params = model.fit(\n    train_dataset, \n    validation_data = valid_dataset, \n    epochs = EPOCHS,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    callbacks = [lr_schedule, early_stopping, checkpoint, CSVLogger(hist_path)]\n)","65add7d1":"def plot_hist(path):\n  history = pd.read_csv(path)\n\n  acc = history['accuracy']\n  val_acc = history['val_accuracy']\n\n  loss = history['loss']\n  val_loss = history['val_loss']\n  plt.style.use('fivethirtyeight')\n  plt.figure(figsize=(20, 10))\n\n  plt.subplot(1, 2, 1)\n  plt.plot(acc, label='Training Accuracy')\n  plt.plot(val_acc, label='Validation Accuracy')\n  plt.legend(loc='lower right')\n  plt.ylabel('Accuracy')\n  plt.ylim([min(plt.ylim()), 1])\n  plt.title('Training and Validation Accuracy')\n  plt.xlabel('epoch')\n\n  plt.subplot(1, 2, 2)\n  plt.plot(loss, label='Training Loss')\n  plt.plot(val_loss, label='Validation Loss')\n  plt.legend(loc='upper right')\n  plt.ylabel('Categorical Crossentropy')\n  plt.ylim([min(plt.ylim()), max(plt.ylim())])\n  plt.title('Training and Validation Loss')\n\n  plt.xlabel('epoch')\n  plt.savefig('evaluation.jpg')\n  plt.show()","9301c488":"plot_hist(\".\/EfficientNet_B5_Fullaug_Hist.log\")","6aea02ca":"# Set path","d87fcfd9":"# Preprocess Dataset","f39e1af4":"# Config TPU","544b007c":"# Training model","ceeca86e":"# Plot history","afc1d9ee":"# Import usefull library"}}