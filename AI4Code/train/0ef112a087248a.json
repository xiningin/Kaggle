{"cell_type":{"e3358baf":"code","fd64e9b2":"code","ffd84b1e":"code","fa2f1b2e":"code","59097ccc":"code","7be1d074":"code","c5fb4728":"code","35c41ea7":"code","eea185db":"code","8a767fad":"code","0b87378d":"code","7d3687cf":"code","2f561102":"code","faa5b5cd":"code","7765d175":"code","b3aa54b5":"code","b7c4d79a":"code","df6f40c3":"code","6000c6a9":"code","a93277cf":"markdown","aa163229":"markdown","17fd75a6":"markdown","d64ff768":"markdown","116864cf":"markdown","4695e36d":"markdown","bf8d34bc":"markdown","06b2349d":"markdown","25c0ac9b":"markdown","0ca8d33a":"markdown","d930f932":"markdown"},"source":{"e3358baf":"# From eda-preprocessing-pipeline-and-submission\n# OS\nimport os\n\n# Data format\nimport datetime\n\n# Tying\nfrom copy import copy\n\n# Data processing\nimport pandas as pd\n\n# Data virtualisation\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\n\n# Widgets\nimport ipywidgets as widgets\n\n# Exporter\nfrom inspect import getsource\n\n# Math and model\nimport numpy as np\nimport scipy\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\n\n# Normaliser\nfrom scipy.special import (\n    boxcox,\n    inv_boxcox\n)\n\n# Modules in this notebook\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom lightgbm import LGBMRegressor\nimport tensorflow as tf\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport xgboost\n\n## Optuna tuner\nimport optuna\nfrom sklearn.metrics import mean_squared_error\n\n# Submission plot\nimport plotly.figure_factory as ff","fd64e9b2":"Train_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\nTest_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')","ffd84b1e":"pipeline_path = '..\/input\/eda-preprocessing-pipeline-and-submission\/pipeline\/'\nfor node in os.listdir(pipeline_path):\n    with open(pipeline_path + node, 'r') as f:\n        exec(f.read())","fa2f1b2e":"Train_data = preprocess_test(Train_data)\n# Train_data['month'] = list(pd.Series(Train_data.index).apply(lambda date: date.month))\n# Train_data['week_day*hour'] = Train_data['week_day']*Train_data['hour']\nTrain_data = sort_columns(Train_data)\n# Train_data.drop('hour', axis=1, inplace=True)\nTrain_data.head(5)","59097ccc":"Test_data = preprocess_test(Test_data)\n# Test_data['month'] = list(pd.Series(Test_data.index).apply(lambda date: date.month))\n# Test_data['week_day*hour'] = Test_data['week_day']*Test_data['hour']\n# Test_data.drop(['hour'], axis=1, inplace=True)\nTest_data.head(5)","7be1d074":"fig = px.parallel_coordinates(\n    Train_data,\n    color='target_benzene',\n    dimensions=[\n        'hour',\n        'relative_humidity',\n        'sensor_3',\n        'absolute_humidity',\n        'deg_C',\n        # 'month'\n        # 'week_day',\n        'sensor_1',\n        'sensor_2',\n        'sensor_4',\n        'sensor_5',\n        # 'week_day*hour',\n        'target_benzene',\n    ],\n    labels={\n            'target_benzene': 'Benzene',\n            'target_nitrogen_oxides': 'Nitrogen oxides',\n            \"target_carbon_monoxide\": \"Carbon monoxide\"\n    },\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2)\nfig.show()","c5fb4728":"fig = px.parallel_coordinates(\n    Train_data,\n    color='target_nitrogen_oxides',\n    dimensions=[\n        'hour',\n        'relative_humidity',\n        'sensor_3',\n        'absolute_humidity',\n        'deg_C',\n        'sensor_1',\n        'sensor_2',\n        # 'week_day',\n        # 'week_day*hour',\n        'target_benzene',\n        \"target_carbon_monoxide\",\n        'sensor_4',\n        'sensor_5',\n        'target_nitrogen_oxides',\n    ],\n    labels={\n            'target_benzene': 'Benzene',\n            'target_nitrogen_oxides': 'Nitrogen oxides',\n            \"target_carbon_monoxide\": \"Carbon monoxide\"\n    },\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2,\n    range_color=[\n        min(Train_data.target_nitrogen_oxides),\n        max(Train_data.target_nitrogen_oxides)\n    ]\n)\nfig.show()","35c41ea7":"fig = px.parallel_coordinates(\n    Train_data,\n    color='target_carbon_monoxide',\n    dimensions=[\n        'hour',\n        'relative_humidity',\n        'sensor_3',\n        'absolute_humidity',\n        'deg_C',\n        'sensor_1',\n        'sensor_2',\n        'sensor_4',\n        'sensor_5',\n        # 'week_day',\n        'target_carbon_monoxide',\n    ],\n    labels={\n            'target_benzene': 'Benzene',\n            'target_nitrogen_oxides': 'Nitrogen oxides',\n            \"target_carbon_monoxide\": \"Carbon monoxide\"\n    },\n    color_continuous_scale=px.colors.diverging.Tealrose,\n    color_continuous_midpoint=2,\n    range_color=[\n        min(Train_data.target_carbon_monoxide),\n        max(Train_data.target_carbon_monoxide)\n    ]\n)\nfig.show()","eea185db":"# Classic K fold\nnum_fold = 5\nkf = KFold(n_splits=num_fold, shuffle=True, random_state=1234)\nkf.get_n_splits(Train_data)\n\nprint(kf)\n\nK_FOLD = []\nfor train_index, test_index in kf.split(Train_data):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    K_FOLD.append((train_index, test_index))","8a767fad":"# # Back testing\n# num_fold = 5\n# kf = KFold(n_splits=num_fold, shuffle=False)\n# tscv = TimeSeriesSplit()\n# kf.get_n_splits(Train_data)\n\n# print(kf)\n\n# K_FOLD = []\n# for train_index, test_index in tscv.split(Train_data):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     K_FOLD.append((train_index, test_index))\n","0b87378d":"import pickle\n\ndef save_model(model, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(model, f)","7d3687cf":"def fit_svr(X_Train, y_Train, X_Test, y_Test, verbose=True, **params):\n    model = make_pipeline(\n        StandardScaler(),\n        SVR(**params)\n        # SVR(C=1.0, epsilon=0.3)\n    )\n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    if verbose: print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    if verbose: print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\ndef objective_decorate(X_Train, y_Train, X_Test, y_Test):\n    def objective(trial):\n        params = {\n            # 'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n            'C': trial.suggest_loguniform('C', 1e-3, 10.0),\n            'epsilon': trial.suggest_loguniform('epsilon', 1e-3, 10.0),\n        }\n\n        train_predictions = np.array([])\n        y_trains = np.array([])\n        predictions = np.array([])\n        y_validations = np.array([])\n\n\n        # print(f'Columns: {y_col}')\n        model, prediction, y_validation = fit_svr(\n            X_Train,\n            y_Train,\n            X_Test,\n            y_Test,\n            verbose=False,\n            **params\n        )\n        train_prediction = model.predict(X_Train).flatten()\n        train_predictions = np.concatenate([train_predictions, train_prediction])\n        y_trains = np.concatenate([y_trains, y_Train.values])\n        predictions = np.concatenate([predictions, prediction])\n        y_validations = np.concatenate([y_validations, y_validation])\n\n        rmsle_train = RMSLE(train_predictions, y_trains)\n        rmsle_test = RMSLE(predictions, y_validations)\n        overfit_protect = rmsle_test * (rmsle_test\/rmsle_train)**(2)\n#         print(f\"Overall rmsle train: {rmsle_train}\")\n#         print(f\"Overall rmsle test: {rmsle_test}\")\n#         print(f\"Overfit protect: {overfit_protect}\")\n        return overfit_protect\n    \n    return objective\n\n# study = optuna.create_study(direction='minimize')\n# optuna.logging.disable_default_handler()\n# study.optimize(\n#     objective_decorate(\n#         X_Train=X_Train,\n#         y_Train=y_Train[y_col],\n#         X_Test=X_Test,\n#         y_Test=y_Test[y_col],\n#     ),\n#     n_trials=50,\n# )\n\n# print('Best trial:', study.best_trial.params)\n\n","2f561102":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_log_error as rmsle\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Metric\ndef RMSLE(pred, act):\n    pred = inv_boxcox(pred, 0.0001)\n    act = inv_boxcox(act, 0.0001)\n    return (np.mean(\n        (np.log(pred + 1) - np.log(act + 1))**2\n    )) ** 0.5\n\n# Fit\ndef fit_catboosts(X_Train, y_Train, X_Test, y_Test):\n    param = {'iterations':5}\n    model = CatBoostRegressor(\n        iterations=50, \n        depth=5, \n        learning_rate=0.1, \n        l2_leaf_reg=0.15, #0.3,\n        loss_function='RMSE'\n    )\n    \n    train_pool = Pool(\n        X_Train,\n        y_Train, \n        cat_features=None\n    )\n    \n    test_pool = Pool(\n        X_Test, \n        cat_features=None\n    )\n    model.fit(X_Train, y_Train, verbose=0)\n    prediction = model.predict(test_pool)\n    \n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\ndef fit_lgbm(X_Train, y_Train, X_Test, y_Test):\n    model = LGBMRegressor(\n        reg_lambda=2,\n        reg_alpha=6,\n        random_state=1234\n    )\n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\ndef fit_svr(X_Train, y_Train, X_Test, y_Test, verbose=True, **params):\n    if not params:\n        params = {'C': 1.5395074336816164, 'epsilon': 0.19287719821166877}\n    model = make_pipeline(StandardScaler(), SVR(\n        # C=1.0, epsilon=0.3\n        **params\n    ))\n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    if verbose: print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    if verbose: print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\ndef fit_linear(X_Train, y_Train, X_Test, y_Test):\n    model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    \n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    \n    feature_importance = dict(\n        zip(X_Train.columns, model[-1].coef_)\n    )\n    feature_importance = {\n        k: v\n        for k, v\n        in sorted(\n            feature_importance.items(),\n            key=lambda item: item[1], reverse=True\n        )\n    }\n    print(\"Feature important\")\n    print(feature_importance)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\n\ndef fit_randomForest(X_Train, y_Train, X_Test, y_Test):\n    model = RandomForestRegressor(max_features=3)\n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    \n    feature_importance = dict(\n        zip(X_Train.columns, model.feature_importances_)\n    )\n    feature_importance = {\n        k: v\n        for k, v\n        in sorted(\n            feature_importance.items(),\n            key=lambda item: item[1], reverse=True\n        )\n    }\n    print(\"Feature important\")\n    print(feature_importance)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\n# KNeighborsRegressor(10, weights='uniform')\ndef fit_knn(X_Train, y_Train, X_Test, y_Test):\n    # model = make_pipeline(StandardScaler(), LinearRegression())\n    model = make_pipeline(StandardScaler(), KNeighborsRegressor(10, weights='uniform'))\n    \n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\n# def fit_svrWithOptunaTune(X_Train, y_Train, X_Test, y_Test):\n#     study = optuna.create_study(direction='minimize')\n#     optuna.logging.disable_default_handler()\n#     study.optimize(\n#         objective_decorate(\n#             X_Train=X_Train,\n#             y_Train=y_Train,\n#             X_Test=X_Test,\n#             y_Test=y_Test,\n#         ),\n#         n_trials=50,\n#     )\n    \n    \n#     model = make_pipeline(\n#         StandardScaler(),\n#         SVR(\n#             **study.best_params\n#         )\n#     )\n#     print(model)\n    \n#     model.fit(X_Train, y_Train)\n#     prediction = model.predict(X_Test)\n#     rmsle = RMSLE(prediction, y_Test)\n#     rmsle = RMSLE(model.predict(X_Train), y_Train)\n#     print(f\"rmsle Train: {rmsle}\")\n#     rmsle = RMSLE(prediction, y_Test)\n#     print(f\"rmsle validation: {rmsle}\")\n#     return model, prediction, y_Test\n\n\n\ndef fit_xgboost(X_Train, y_Train, X_Test, y_Test, **params):\n    if not params:\n        params = {'reg_lambda': 8.806514467534535, 'reg_alpha': 5.10815789088487}\n    model = xgboost.XGBRegressor(\n        **params\n#         reg_lambda=2,\n#         reg_alpha=5\n    )\n    model.fit(X_Train, y_Train)\n    prediction = model.predict(X_Test)\n    rmsle = RMSLE(prediction, y_Test)\n    rmsle = RMSLE(model.predict(X_Train), y_Train)\n    print(f\"rmsle Train: {rmsle}\")\n    rmsle = RMSLE(prediction, y_Test)\n    print(f\"rmsle validation: {rmsle}\")\n    return model, prediction, y_Test\n\n\n# def fit_stacking(X_Train, y_Train, X_Test, y_Test):\n#     estimators = [\n#         ('svr_rbf', make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))),\n#         (\n#             'lgbm', LGBMRegressor(\n#                 reg_lambda=2,\n#                 reg_alpha=6,\n#                 random_state=1234\n#             )\n#         )\n#     ]\n#     model = StackingRegressor(\n#         estimators=estimators,\n#         final_estimator= make_pipeline(StandardScaler(), Ridge(alpha=0.1))\n#     )\n#     model.fit(X_Train, y_Train)\n#     prediction = model.predict(X_Test)\n#     rmsle = RMSLE(prediction, y_Test)\n#     rmsle = RMSLE(model.predict(X_Train), y_Train)\n#     print(f\"rmsle Train: {rmsle}\")\n#     rmsle = RMSLE(prediction, y_Test)\n#     print(f\"rmsle validation: {rmsle}\")\n#     return model, prediction, y_Test\n\n\n\ndef get_fit_function(key: str):\n    return key.startswith('fit_')\n\nmodels = {}\ncv_report = pd.DataFrame()\n\nfor k, (Train, Test) in enumerate(K_FOLD):\n    print(f\"K: {k}\")\n    \n    # Ge\n    y_columns = list(\n        Train_data.columns[\n            Train_data.columns.str.startswith('target')\n        ]\n    )\n    X_Train = Train_data.iloc[Train].drop(y_columns, axis=1)\n    y_Train = Train_data.iloc[Train][y_columns]\n\n    X_Test = Train_data.iloc[Test].drop(y_columns, axis=1)\n    y_Test = Train_data.iloc[Test][y_columns]\n    \n    \n    \n    for fit in filter(get_fit_function,dict(globals())):\n        _, model_name = fit.split('_')\n        train_predictions = np.array([])\n        y_trains = np.array([])\n        predictions = np.array([])\n        y_validations = np.array([])\n        model = {}\n        for y_col in y_columns:\n            print(y_col)\n            print(f'Model {model_name}')\n            model[y_col], prediction, y_validation = globals()[fit](\n                X_Train, y_Train[y_col],\n                X_Test, y_Test[y_col]\n            )\n            train_prediction = model[y_col].predict(X_Train)\n            train_predictions = np.concatenate([train_predictions, train_prediction])\n            y_trains = np.concatenate([y_trains, y_Train[y_col].values])\n            predictions = np.concatenate([predictions, prediction])\n            y_validations = np.concatenate([y_validations, y_validation])\n            \n            save_model(model[y_col], f\"Fold_{k}_Model_{model_name}_Col_{y_col}.pickle\")\n        \n        rmsle_train = RMSLE(train_predictions, y_trains)\n        rmsle_test = RMSLE(predictions, y_validations)\n        \n        cv_report.loc[k, f'{model_name}_train'] = rmsle_train\n        cv_report.loc[k, f'{model_name}_test'] = rmsle_test\n        if model_name in models:\n            models[model_name].append(model)\n        else:\n            models[model_name] = [model]\n        print('-' * 36)\n        \n    \n#     catboost_models.append(catboost)\n#     svr_models.append(svr)\n#     linear_models.append(linear)\n#     lgbm_models.append(lgbm)","faa5b5cd":"feature_importance = dict(zip(X_Train.columns, models['linear'][0]['target_nitrogen_oxides'][-1].coef_))","7765d175":"from pprint import pprint\n\nfor name, model in models.items():\n    print(f\"{name}:\")\n    pprint(model[0])","b3aa54b5":"cv_report","b7c4d79a":"cv_report.mean()","df6f40c3":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n\nfig = go.Figure()\n\n\nmean_submission = pd.read_csv(\n        '..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv'\n    )\nmean_submission = mean_submission.set_index('date_time')\nmean_submission['target_carbon_monoxide'] = 0\nmean_submission['target_benzene'] = 0\nmean_submission['target_nitrogen_oxides'] = 0\n\nfor sub_count, fit in enumerate(filter(get_fit_function,dict(globals()))):\n    _, model_name = fit.split('_')\n\n    submission = pd.read_csv(\n        '..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv'\n    )\n    submission = submission.set_index('date_time')\n    submission['target_carbon_monoxide'] = 0\n    submission['target_benzene'] = 0\n    submission['target_nitrogen_oxides'] = 0\n    \n    for k in range(num_fold):\n        for y_col in y_columns:\n            submission[y_col] += models[model_name][k][y_col].predict(\n                Test_data\n            )\/num_fold\n\n\n    submission = inv_boxcox(submission, 0.0001)\n    submission.to_csv(f'{model_name}_submission.csv')\n    for y_col in y_columns:\n        fig.add_trace(\n            go.Scatter(\n                x=submission.index,\n                y=submission[y_col],\n                mode='lines',\n                name=f'{model_name}_{y_col}')\n        ) \n    mean_submission += submission\n\n    \nmean_submission = mean_submission \/ (sub_count + 1)\nmean_submission.to_csv('mean_submission.csv')\nfor y_col in y_columns:\n    fig.add_trace(\n        go.Scatter(\n            x=mean_submission.index,\n            y=mean_submission[y_col],\n            mode='lines',\n            name=f'mean_submission_{y_col}')\n    )\nfig.show()","6000c6a9":"mean_submission","a93277cf":"<a id=\"import\"><\/a>\n# Import\n<a id=\"modules\"><\/a>\n## modules","aa163229":"### Nitrogen oxides","17fd75a6":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 Introduction](#Introduction)\n- [2 Import](#import)","d64ff768":"# Make submission","116864cf":"# Modeling","4695e36d":"### Carbon monoxide","bf8d34bc":"## Pipeline\n\nIn this section, the preprocessing pipelines made in [EDA, preprocessing pipeline and submission](https:\/\/www.kaggle.com\/batprem\/eda-preprocessing-pipeline-and-submission?scriptVersionId=67990508) are imported","06b2349d":"More feature engineering and model impplementdation from [EDA, preprocessing pipeline and submission](https:\/\/www.kaggle.com\/batprem\/eda-preprocessing-pipeline-and-submission).","25c0ac9b":"<a id=\"introduction\"><\/a>\n# Introduction","0ca8d33a":"## Data","d930f932":"## Visualise Preprocessing\n### Benzene"}}