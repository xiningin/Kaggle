{"cell_type":{"73dbc01d":"code","441c9b3d":"code","34def53b":"code","4d78468a":"code","6dafe913":"code","69ffb65a":"code","6567fd87":"code","dcc3ff89":"code","93600fc7":"code","ef098b3c":"code","5556d754":"code","f25777ed":"code","88f0b9d5":"code","9a601057":"code","71e04714":"code","e9a5fc21":"code","3b45f1fe":"code","97db97c3":"code","58c610eb":"code","bcfe2d7e":"code","6cdd2282":"code","0fade2d4":"code","b9d96a9c":"code","7fabe44d":"code","55824518":"code","d400c1e6":"code","dd144083":"code","1731b8b9":"code","348c2ebf":"code","a6d647c6":"code","e2b8bcf7":"code","ee33c96c":"code","0a47369f":"code","c26cc9e9":"code","779af87c":"code","e7508c22":"code","13a683a3":"markdown","8f490413":"markdown","aea84c0f":"markdown","6c7e451f":"markdown","cd08219b":"markdown","26640f7b":"markdown","99e80d82":"markdown","b9d70d45":"markdown","4fe46625":"markdown","9c5b3c2a":"markdown","d6e50fdd":"markdown","a39de91e":"markdown","0d9abccb":"markdown","3e3e256b":"markdown","d4e0af03":"markdown","d8ceec10":"markdown","556cd30e":"markdown","ef625ff7":"markdown","af9b538e":"markdown","7f10e1ba":"markdown","b3e457b3":"markdown","0901d9ab":"markdown","9a51fd58":"markdown","0ed61dce":"markdown","a63d0060":"markdown","ddca8bd0":"markdown","a0f370fc":"markdown","fdb6c462":"markdown","e56ef969":"markdown","95705a30":"markdown","7be82148":"markdown","22044f1f":"markdown"},"source":{"73dbc01d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))","441c9b3d":"# Remove Warnings in JupyterNotebook (I hate those warnings!)\nimport warnings\nwarnings.simplefilter('ignore')","34def53b":"df = pd.read_csv('..\/input\/heart.csv')\ndf.sample(3)","4d78468a":"# Let's change the column names based on information on dataset description. It would be much more understandable.\ndf.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target' ]","6dafe913":"df.shape","69ffb65a":"# There's no Null value inside the data frame\ndf.isnull().sum().value_counts()","6567fd87":"# Check the Correlation between variables\ndf_corr = df.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(df_corr, cmap='coolwarm', annot=True)","dcc3ff89":"# Turn our data into matrix form\ndfmat = df.pivot_table(columns='age', index='st_depression', values='target')\n\n# Use heatmap to draw the chart. I also used invert_yaxis() to change the order of y_axis,\n# you can delete this part to see the difference.\nsns.heatmap(dfmat, cmap='coolwarm', cbar=False).invert_yaxis()\n\n# Let's have more clever chart! illustrating labels and legend\nplt.ylabel('ST Depression')\nplt.xlabel('Age')\n\nfrom matplotlib.patches import Patch\nred_patch = Patch(color='#B40426', label='Target= 1')\nblue_patch = Patch(color='blue', label='Target= 0')\nplt.legend(handles=[red_patch, blue_patch])","93600fc7":"sns.jointplot(x=df.age, y=df.st_depression, data=df, kind='kde')","ef098b3c":"sns.lmplot(data=df, x='age', y='max_heart_rate_achieved', hue='sex', markers=['o','v'])","5556d754":"FG = sns.FacetGrid(data=df, row='sex', col='chest_pain_type')\nFG.map(sns.distplot, 'age')","f25777ed":"i = df.groupby(['chest_pain_type'])['sex']\ni\n#sns.jointplot(x=df.age, y=df.chest_pain_type, data=df, kind='kde')","88f0b9d5":"sns.violinplot(data=df, x='chest_pain_type', y='age', hue='sex', split=True)","9a601057":"df.columns","71e04714":"from sklearn.model_selection import train_test_split\n\nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)","e9a5fc21":"from sklearn.linear_model import LinearRegression\n\n# create an instance from LinearRegression name \"lm\" and fit the model\nlm = LinearRegression()\nlm.fit(X_train, y_train)","3b45f1fe":"coeff_df = pd.DataFrame((lm.coef_)*100, X.columns, columns=['Coefficient (percentage %)'])\ncoeff_df","97db97c3":"prediction = lm.predict(X_test)","58c610eb":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nprint(classification_report(y_test, prediction.round()))","bcfe2d7e":"print('The Accuarcy Score of our Model would be: ', accuracy_score(y_test, prediction.round())*100, '%')","6cdd2282":"from sklearn.svm import SVC\n\n# create an instance from SCV and fit the model\nSVM_model = SVC()\nSVM_model.fit(X_train, y_train)","0fade2d4":"prediction2= SVM_model.predict(X_test)","b9d96a9c":"print(confusion_matrix(y_test, prediction2))\nprint(classification_report(y_test,prediction2))","7fabe44d":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)","55824518":"grid.fit(X_train,y_train)","d400c1e6":"grid.best_params_","dd144083":"grid.best_estimator_","1731b8b9":"grid_predictions = grid.predict(X_test)","348c2ebf":"print(confusion_matrix(y_test,grid_predictions))\nprint(classification_report(y_test,grid_predictions))","a6d647c6":"print('The Accuarcy Score of our Model would be: ', accuracy_score(y_test, grid_predictions.round())*100, '%')","e2b8bcf7":"y = (accuracy_score(y_test, grid_predictions.round())*100, accuracy_score(y_test, prediction.round())*100)\nx = ['SVM', 'Linear Regression']\nplt.bar(x, y)\nplt.ylabel('Accuracy Score (%)')","ee33c96c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('target', axis=1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n\ndata = df\nmy_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)","0a47369f":"row_to_show = 5\ndata_for_prediction = X_test.iloc[row_to_show]\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nmy_model.predict_proba(data_for_prediction_array)","c26cc9e9":"import shap\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","779af87c":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","e7508c22":"explainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","13a683a3":"Before start our analysis, we should visit Google and search for hear disease causes and the variables we have in our data set. Based on our Google-study and above heatmap, we can continue to analyze a few of these variables, like set_depression with a noticable correlation with target. ","8f490413":"Now, we can inspect the *best parameters* found by **GridSearchCV** in the **best_params_** attribute, and the *best estimator* in the **best_estimator_** attribute:","aea84c0f":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","6c7e451f":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","cd08219b":"Now it's time to re-run predictions on this grid object","26640f7b":"**Gridsearch**","99e80d82":"### Data Visualization","b9d70d45":"**Predict Model**","4fe46625":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","9c5b3c2a":"There are four ways to check if the predictions are right or wrong:\n1. TN \/ True Negative: case was negative and predicted negative\n2. TP \/ True Positive: case was positive and predicted positive\n3. FN \/ False Negative: case was positive but predicted negative\n4. FP \/ False Positive: case was negative but predicted positive\n\nIn evaluation of our model, when we use Classification_Report, we need to know the meaning of each one of these parameters:\n\n**Precision** - What percent of your predictions were correct?\n- Precision: Accuracy of positive predictions\n\n*Precision = TP\/(TP + FP)*\n\n\n**Recall** \u2013 What percent of the positive cases did you catch?\n- Recall: Fraction of positives that were correctly identified\n\n*Recall = TP\/(TP+FN)*\n\n**F1 score** \u2013 What percent of positive predictions were correct?\n- The F1 score is a weighted harmonic mean of precision and recall\n- As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n\n*F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)*","d6e50fdd":"**Further Study**","a39de91e":"### Machine Learning:\n**SVM Model**\n   (Support Vector Machines)","0d9abccb":"From above chart(heatmap), we can see there's no meaningful relation between ST Depression and Age, but as soon as ST Depression gets higher than 2 (approximately), the Target color tends to turn into blue\/0 which means higher ST Depression, lower risk of hear disease.\n\nThere's another way to check this possibility from JointPlot:","3e3e256b":"### Initial Steps","d4e0af03":"Like any other regression model, we need to **split** our data frame into **train** and **test** using scikit-learn","d8ceec10":"### Machine Learning:\n**Linear Regression**","556cd30e":"Most of the times, I would like to check the **correlation** between all the variables to have an initial thougth of possible relation between them and heatmap is the best way for that.","ef625ff7":"**Evaluate Model**","af9b538e":"The next plot is a linear relation between **age** & **max_hear_rate_achieved**. It shows that the max_hear_rate decreased slightly as the age goes high. But the deviation is high.","7f10e1ba":"So it means that the patient is 70% likely to have heart disease (target equal to 1)","b3e457b3":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","0901d9ab":"Dataset Link: [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)\n\n*Attribute Information*\n- **cp**: chest pain type (4 values) \n- **tresttbps**: resting blood pressure \n- **chol**: serum cholestoral in mg\/dl \n- **fbs**: fasting blood sugar > 120 mg\/dl\n- **restecg**: resting electrocardiographic results (values 0,1,2)\n- **thalach**: maximum heart rate achieved \n- **exang**: exercise induced angina \n- **oldpeak**: ST depression induced by exercise relative to rest \n- **slope**: the slope of the peak exercise ST segment \n- **ca**: number of major vessels (0-3) colored by flourosopy \n- **thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect","9a51fd58":"So it means:\n*  Holding all features fixed, a *1 unit increase* in **Chest_Pain_Type** (CP) is associated with an increase of 7.7% chance of having heart disease.\n*  Holding all features fixed, a *1 unit increase* in **ST Slope** is associated with an increase of 8% chance of having heart disease.\n*  And because in sex category male are 1 and female are 0, then holding all features fixed, the chance of having heart disease for female are higher than male","0ed61dce":"Here is the Summary Plot:","a63d0060":"So, we examined two ML model: Linear Regression & SVM\n\nHere is the accuray of each one of them:","ddca8bd0":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","a0f370fc":"FacetGrid allows us to see the difference between **male** and **female** at different **types** of chest pain (from 0 to 3)","fdb6c462":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","e56ef969":"![](https:\/\/ui-ex.com\/images\/transparent-dividers-horizontal-line-6.png)","95705a30":"### SHAP Values ###\nCheck this Kernel in Kaggle for full description: [LINK](https:\/\/www.kaggle.com\/dansbecker\/shap-values)","7be82148":"Notice that we are classifying everything into a single class!\nThis means our model needs to have it parameters adjusted (*it may also help to normalize the data*)\n\nWe can search for parameters using a **GridSearch**!","22044f1f":"**SHAP Values** break down a prediction to show the impact of each feature.\n\nWhere could you use this?\n* A model says a bank shouldn't loan someone money\n* A healthcare provider wants to identify what factors are driving each patient's risk of some disease\n\n**SHAP values** interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n\n> Feature values causing **increased predictions** are in **pink**, and their visual size shows the magnitude of the feature's effect.\n\n> Feature values **decreasing the prediction** are in **blue**."}}