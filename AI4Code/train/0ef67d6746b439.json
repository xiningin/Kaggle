{"cell_type":{"1eb7ce60":"code","bb09cd99":"code","bd81f4fb":"code","e9d94636":"code","5a0b2833":"code","bae79dcb":"code","efad4685":"code","57e6f54c":"code","af18d615":"code","8efe22a0":"code","7684d40f":"code","303813d1":"markdown","9ef3e853":"markdown","27786abd":"markdown","a6b43f4e":"markdown","5105b8f6":"markdown","4dc573ec":"markdown","0a7917d0":"markdown","36d7b2ab":"markdown","8e3751e6":"markdown","e79f3452":"markdown","8aba2860":"markdown","2ed3d1ea":"markdown"},"source":{"1eb7ce60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nfrom PIL import Image\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bb09cd99":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head(5)","bd81f4fb":"print('size of train data',train.shape)\nprint('size of test data',test.shape)","e9d94636":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of image categories(Target Variable)\")\nax = sns.distplot(train[\"category_id\"])","5a0b2833":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of train image locations(Train location Variable)\")\nax = sns.distplot(train[\"location\"])","bae79dcb":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of test image locations(Test location Variable)\")\nax = sns.distplot(test[\"location\"])","efad4685":"sns.FacetGrid(train, hue=\"height\", size=10).map(plt.scatter, \"category_id\", \"location\").add_legend()","57e6f54c":"train['date'] = train['date_captured'].str.split('\\s+').str[0]\ntrain['time'] = train['date_captured'].str.split('\\s+').str[-1]    \ntrain['hour'] = pd.to_numeric(train['time'].str[:2], errors='coerce')\nsns.FacetGrid(train, hue=\"category_id\", size=10).map(plt.scatter, \"hour\", \"location\").add_legend()","af18d615":"night = train[(train['hour'] > 19) | (train['hour'] < 7)]\nday = len(train) - len(night)\nlabels = 'Day', 'Night'\nsizes = [len(night), day]\ncolors = ['lightcoral', 'lightskyblue']\nexplode = (0.1, 0) \nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()","8efe22a0":"# sample night images\nfig = plt.figure(figsize=(25, 60))\nimgs = [np.random.choice(night.loc[night['category_id'] == i, 'file_name'], 4) for i in night.category_id.unique()]\nimgs = [i for j in imgs for i in j]\nlabels = [[i] * 4 for i in train.category_id.unique()]\nlabels = [i for j in labels for i in j]\nfor idx, img in enumerate(imgs):\n    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n    im = Image.open(\"..\/input\/train_images\/\" + img)\n    plt.imshow(im)\n    ax.set_title(f'Label: {labels[idx]}')","7684d40f":"day = train[(train['hour'] < 19) & (train['hour'] > 7)]\n# sample night images\nfig = plt.figure(figsize=(25, 60))\nimgs = [np.random.choice(day.loc[day['category_id'] == i, 'file_name'], 4) for i in day.category_id.unique()]\nimgs = [i for j in imgs for i in j]\nlabels = [[i] * 4 for i in train.category_id.unique()]\nlabels = [i for j in labels for i in j]\nfor idx, img in enumerate(imgs):\n    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n    im = Image.open(\"..\/input\/train_images\/\" + img)\n    plt.imshow(im)\n    ax.set_title(f'Label: {labels[idx]}')","303813d1":"# Can we do something with Image size?","9ef3e853":"![](https:\/\/tse4.mm.bing.net\/th?id=OIP.zN1wmN4DIre-HT2j1tT0dwHaCu&pid=15.1&P=0&w=451&h=167)\n\n# Let's have a quick look at the 'iWildCam 2019 - FGVC6' data...\n\nCamera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automating the species classification challenge in camera traps, but as we try to expand the scope of these models from specific regions where we have collected training data to nearby areas we are faced with an interesting probem: how do you classify a species in a new region that you may not have seen in previous training data?\n\n","27786abd":"# To be continued...","a6b43f4e":"# Let's read data...","5105b8f6":"# Target Distribution in the Train dataset:","4dc573ec":"# Location Distribution in the Train dataset:","0a7917d0":"# Time to display some image samples collected during the night: ","36d7b2ab":"# Dataset size:","8e3751e6":"# Location Distribution in the Test dataset:","e79f3452":"# Let's have a look at timestamps...","8aba2860":"# Day \/ Night samples ratio:","2ed3d1ea":"# Time to display some image samples collected during the day: "}}