{"cell_type":{"a12a949f":"code","40e458f2":"code","d9a31754":"code","bea29778":"code","9e00150c":"code","55603b45":"code","e850ab42":"code","ff688194":"code","e5150114":"code","68f541ba":"code","7e25d19e":"code","12d83031":"code","cc258ac4":"code","7732633e":"code","c7c4ca70":"code","1a48032a":"code","292c152a":"code","469ca4b3":"code","d5cd3d18":"code","571c5c7a":"code","3ca21582":"code","daeda97c":"code","99f0680a":"code","e4c20316":"code","8ec0f2d5":"code","34532674":"code","7dd45c30":"code","57f966d0":"code","d683384c":"code","a07f6204":"code","9f9da05c":"markdown","90027633":"markdown","ae1d3eaf":"markdown","5566718d":"markdown","f130a1e2":"markdown","2eb0697c":"markdown","201be48e":"markdown","d47003d7":"markdown","09c80656":"markdown","dd86387f":"markdown","c49f29d1":"markdown","49166192":"markdown"},"source":{"a12a949f":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","40e458f2":"# Load the paths of the images\ntrainpaths = os.listdir(\"..\/input\/fingers\/train\")\ntestpaths = os.listdir(\"..\/input\/fingers\/test\")\n\n# Add the all path to the paths of the images\ntrain_str = \"..\/input\/fingers\/train\/\"\ntrainpaths = [\"..\/input\/fingers\/train\/\" + p for p in trainpaths]\n\ntest_str = \"..\/input\/fingers\/test\/\"\ntestpaths = [\"..\/input\/fingers\/test\/\" + p for p in testpaths]\n\n# Create a DataFrame with the paths\ndf_train = pd.DataFrame(trainpaths, columns=['Filepath'])\ndf_train['set'] = 'train'\n\ndf_test = pd.DataFrame(testpaths, columns=['Filepath'])\ndf_test['set'] = 'test'\n\nimage_df = pd.concat([df_train,df_test])\nimage_df.reset_index(drop = True, inplace = True)\n\n# Display the first rows\nimage_df.head(5)","d9a31754":"# The number of fingers is the second character on the left of .png\n# In this example, it is 4 (fingers)\nimage_df.iloc[0][0]","bea29778":"# Create the labels (number of fingers)\nimage_df['Label'] = image_df['Filepath'].apply(lambda x: x[-6])\n\n# Create the labels for a right or a left hand\nimage_df['Label_LR'] = image_df['Filepath'].apply(lambda x: x[-5])\n\n# Shuffle the DataFrame\nimage_df = image_df.sample(frac=1,random_state=0)\n\n# Show the result\nimage_df.head(5)","9e00150c":"# Display some pictures of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","55603b45":"# Separe the image_df in the training DataFrame and into the testing DataFrame\ntrain_df = image_df[image_df['set'] == 'train']\ntest_df = image_df[image_df['set'] == 'test']","e850ab42":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","ff688194":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","e5150114":"# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images = create_gen()","68f541ba":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(6, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=3,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=1,\n            restore_best_weights=True\n        )\n    ]\n)","7e25d19e":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","12d83031":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","cc258ac4":"results = model.evaluate(test_images, verbose=0)","7732633e":"printmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))","c7c4ca70":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","1a48032a":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","292c152a":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","469ca4b3":"# Display some pictures of the dataset with their labels and the predictions\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","d5cd3d18":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","571c5c7a":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","3ca21582":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label_LR',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label_LR',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label_LR',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","daeda97c":"# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images = create_gen()","99f0680a":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","e4c20316":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()\n\npd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","8ec0f2d5":"results = model.evaluate(test_images, verbose=0)\nprintmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))","34532674":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","7dd45c30":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label_LR)\nprint(classification_report(y_test, pred))","57f966d0":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","d683384c":"# Display some pictures of the dataset with their labels and the predictions\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","a07f6204":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None\n# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(15, 30),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label_LR.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","9f9da05c":"## 3.2. Train the model<a class=\"anchor\" id=\"9\"><\/a>","90027633":"## 2.2. Train the model<a class=\"anchor\" id=\"4\"><\/a>","ae1d3eaf":"# 2. Teach a Neural Network how to count fingers  <a class=\"anchor\" id=\"2\"><\/a>\n## 2.1. Load the Images with a generator<a class=\"anchor\" id=\"3\"><\/a>","5566718d":"<h1>Table of contents<\/h1>\n\n<ul>\n<li><a href=\"#1\"><strong>1. Load and transform the dataset<\/strong><\/a>\n<\/ul>\n    \n<ul>\n<li><a href=\"#2\"><strong>2. Teach a Neural Network how to count fingers<\/strong><\/a>\n<ul>\n \n<li><a href=\"#3\">2.1. Load the Images with a generator<\/a><\/li>\n<li><a href=\"#4\">2.2. Train the model<\/a><\/li>\n<li><a href=\"#5\">2.3. Visualize the result<\/a><\/li>\n<li><a href=\"#6\">2.4. Class activation heatmap for image classification<\/a><\/li>\n  \n    \n<br>\n    \n<\/ul>\n<li><a href=\"#7\"><strong>3. Teach a Neural Network to differenciate the left and the right hands<\/strong><\/a>\n<ul>\n  \n<li><a href=\"#8\">3.1. Load the Images with a generator<\/a><\/li>\n<li><a href=\"#9\">3.2. Train the model<\/a><\/li>\n<li><a href=\"#10\">3.3. Visualize the result<\/a><\/li>\n<li><a href=\"#11\">3.4. Class activation heatmap for image classification<\/a><\/li>\n\n<\/ul>\n","f130a1e2":"## 3.3. Visualize the result<a class=\"anchor\" id=\"10\"><\/a>","2eb0697c":"# Teach a Neural Network to count Fingers and to differenciate a left hand from right hand\n## \\# Class activation heatmap for image classification\n## \\# Grad-CAM class activation visualization\n\nHaving 21.600 images of left and right hands fingers\n\n![finger hand](https:\/\/i.imgur.com\/9tCNaOr.png)","201be48e":"# 3. Teach a Neural Network to differenciate the left and the right hands<a class=\"anchor\" id=\"7\"><\/a>\n## 3.1. Load the Images with a generator<a class=\"anchor\" id=\"8\"><\/a>","d47003d7":"# 1. Load and transform the dataset<a class=\"anchor\" id=\"1\"><\/a>","09c80656":"## 3.4. Class activation heatmap for image classification<a class=\"anchor\" id=\"11\"><\/a>\n### Grad-CAM class activation visualization\n*Code adapted from keras.io*","dd86387f":"# Content of the dataset\n21600 images of left and right hands fingers.\n\nAll images are 128 by 128 pixels.\n\n- Training set: 18000 images\n- Test set: 3600 images\n- Images are centered by the center of mass\n- Noise pattern on the background\n\n## Labels\nLabels are in 2 last characters of a file name. L\/R indicates left\/right hand; 0,1,2,3,4,5 indicates number of fingers.\n\n## Note\nImages of a left hand were generated by flipping images of right hand.\n","c49f29d1":"## 2.4. Class activation heatmap for image classification<a class=\"anchor\" id=\"6\"><\/a>\n### Grad-CAM class activation visualization\n*Code adapted from keras.io*","49166192":"# 2.3. Visualize the result<a class=\"anchor\" id=\"5\"><\/a>"}}