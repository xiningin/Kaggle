{"cell_type":{"53d828d9":"code","641087cf":"code","40a93d91":"code","84f8171c":"code","7b92246b":"code","16dcff32":"code","54bd40b7":"code","e5c15224":"code","d9714d3d":"code","8f76cf25":"code","de59d1b0":"code","5dab3745":"code","95a8285e":"code","2e453271":"code","4736aeaa":"code","61b2206f":"code","25bc6ed6":"markdown","16ac2d35":"markdown","da7a7203":"markdown","c08fd7b0":"markdown","e4958b41":"markdown","23dcdb01":"markdown","c482c164":"markdown","6f50843e":"markdown","0bc916d6":"markdown","f185ebee":"markdown","dc99a3a0":"markdown","b986c309":"markdown","2bdabbcb":"markdown","9b4e87fe":"markdown","1c2a8b1f":"markdown","c9458e7f":"markdown","9cde41e7":"markdown","5c1fd811":"markdown","d699c614":"markdown","bd474923":"markdown","7cdf2919":"markdown","c89259d6":"markdown","1371c5d7":"markdown","9ddbb746":"markdown","18bd9e9d":"markdown","1bc29ca6":"markdown","7eda108e":"markdown","7b861a3c":"markdown","b7569b29":"markdown","2f7d8891":"markdown"},"source":{"53d828d9":"\n# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n","641087cf":"pd.set_option(\"display.max_columns\", None)","40a93d91":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","84f8171c":"pip install autoviz","7b92246b":"pip install xlrd","16dcff32":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\nfilename = \"\"\nsep = \",\"\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"Class\",\n    dfte=df,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=30,\n)","54bd40b7":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\n\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","e5c15224":"# # New_df is from the random undersample data (fewer instances)\n# X = df.drop('Class', axis=1)\n# y = df['Class']\n\n\n# # T-SNE Implementation\n# t0 = time.time()\n# X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n# t1 = time.time()\n# print(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# # PCA Implementation\n# # t0 = time.time()\n# # X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n# # t1 = time.time()\n# # print(\"PCA took {:.2} s\".format(t1 - t0))\n\n# # # TruncatedSVD\n# # t0 = time.time()\n# # X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n# # t1 = time.time()\n# # print(\"Truncated SVD took {:.2} s\".format(t1 - t0))\n\n\n\n\n\n# f, (ax1) = plt.subplots(1, 3, figsize=(24,6))\n# f, (ax1) = plt.subplots(1, 3, figsize=(24,6))\n# # labels = ['No Fraud', 'Fraud']\n# f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\n# blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n# red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# # t-SNE scatter plot\n# ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n# ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n# ax1.set_title('t-SNE', fontsize=14)\n\n# ax1.grid(True)\n\n# ax1.legend(handles=[blue_patch, red_patch])\n\n\n# # # PCA scatter plot\n# # ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n# # ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n# # ax2.set_title('PCA', fontsize=14)\n\n# # ax2.grid(True)\n\n# # ax2.legend(handles=[blue_patch, red_patch])\n\n# # # TruncatedSVD scatter plot\n# # ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n# # ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n# # ax3.set_title('Truncated SVD', fontsize=14)\n\n# # ax3.grid(True)\n\n# # ax3.legend(handles=[blue_patch, red_patch])\n\n# plt.show()","d9714d3d":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n","8f76cf25":"model_name = list()\nresample = list()\nprecision = list()\nrecall = list()\nF1score = list()\nAUCROC = list()","de59d1b0":"def test_eval(clf_model, X_test, y_test, algo=None, sampling=None):\n    # Test set prediction\n    y_prob=clf_model.predict_proba(X_test)\n    y_pred=clf_model.predict(X_test)\n\n    print('Confusion Matrix')\n    print('='*60)\n    print(confusion_matrix(y_test,y_pred),\"\\n\")\n    print('Classification Report')\n    print('='*60)\n    print(classification_report(y_test,y_pred),\"\\n\")\n    print('AUC-ROC')\n    print('='*60)\n    print(roc_auc_score(y_test, y_prob[:,1]))\n          \n    model_name.append(algo)\n    precision.append(precision_score(y_test,y_pred))\n    recall.append(recall_score(y_test,y_pred))\n    F1score.append(f1_score(y_test,y_pred))\n    AUCROC.append(roc_auc_score(y_test, y_prob[:,1]))\n    resample.append(sampling)\n","5dab3745":"#Comparison of classifiers with imbalanced data\nclassifiers = {\"CatBoost\" : CatBoostClassifier(verbose=False),\n              \"LGBM\" : LGBMClassifier()}\nfor classifier in classifiers:\n    model = classifiers[classifier]\n    model.fit(x_train, y_train)\n    print('='*60)\n    print (classifier, \"imbalanced\")\n    print('='*60)\n    test_eval(model, x_test, y_test, classifier, \"imbalanced\")\n","95a8285e":"#Comparison of classifiers with class weighted model\nclassifiers = {\"CatBoost\" : CatBoostClassifier(verbose=False, auto_class_weights=\"Balanced\"),\n              \"LGBM\" : LGBMClassifier(is_unbalance=True)}\nfor classifier in classifiers:\n    model = classifiers[classifier]\n    model.fit(x_train, y_train)\n    print('='*60)\n    print (classifier, \"class_balance\")\n    print('='*60)\n    test_eval(model, x_test, y_test, classifier, \"class_balance\")\n","2e453271":"classifiers = {\"CatBoost\" : CatBoostClassifier(verbose=False),\n              \"LGBM\" : LGBMClassifier()}\n\nsampling_methods = {\"SMOTE\" : SMOTE(),\n                    \"SMOTEENN\" : SMOTEENN()\n                   }\nfor sampling_method in sampling_methods:\n    method = sampling_methods[sampling_method]\n    x_res, y_res = method.fit_resample(x_train, y_train)\n    for classifier in classifiers:           \n        model = classifiers[classifier]\n        model.fit(x_res, y_res)\n        print('='*60)\n        print (classifier, sampling_method)\n        print('='*60)\n        test_eval(model, x_test, y_test, classifier, sampling_method)\n    ","4736aeaa":"clf_eval_df = pd.DataFrame({'model':model_name,\n                            'resample':resample,\n                            'precision':precision,\n                            'recall':recall,\n                            'f1-score':F1score,\n                            'AUC-ROC':AUCROC})\nclf_eval_df","61b2206f":"sns.set(font_scale=1.2)\n#sns.palplot(sns.color_palette())\ng = sns.FacetGrid(clf_eval_df, col=\"model\", height=5)\ng.map(sns.barplot, \"resample\", \"recall\", palette='twilight', order=[\"imbalanced\", \"class_balance\", \"SMOTE\", \"SMOTEENN\"])\ng.set_xticklabels(rotation=30)\ng.set_xlabels(' ', fontsize=14)\ng.set_ylabels('Recall', fontsize=14)","25bc6ed6":"![image.png](attachment:image.png)","16ac2d35":"from imblearn.over_sampling import ADASYN\n\nmethod = ADASYN()\n\nx_res, y_res = method.fit_resample(x_train, y_train)","da7a7203":"from imblearn.under_sampling import EditedNearestNeighbours\n\nmethod = EditedNearestNeighbours()\n\nx_res, y_res = method.fit_resample(x_train, y_train)","c08fd7b0":"\n\nTomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n\nTomek\u2019s link exists if the two samples are the nearest neighbors of each other\n\n\n![image.png](attachment:image.png)","e4958b41":"<h3> ADASYN vs SMOTE<\/h3>\n1. ADASYN generates more synthetic data from samples in the minority class, that are harder to classify and less synthetic data where it's less harder to classify, whereas in SMOTE, there is a uniform weight for all minority points.\n\n2. SMOTE uses only the minority class to train the KNN, on the contrary ADASYN uses all the samplesto train the KNN\n\n\n![image.png](attachment:image.png)\n\nADASYN creates more samples that are closer to the majority class, as they are harder to classify","23dcdb01":"<h3> Edited Nearest Neighbor (ENN) <\/h3>\n\n![image.png](attachment:image.png)\n\nJust like Tomek, Edited Nearest Neighbor removes any example whose class label differs from the class of at least two of its three nearest neighbors. The ENN method removes the instances of the majority class whose prediction made by KNN method is different from the majority class. ENN method can remove both the noisy examples as borderline examples, providing a smoother decision surface. ENN tends to remove more examples than the Tomek links does, so it is expected that it will provide a more in depth data cleaning\n","c482c164":"from imblearn.over_sampling import SMOTE\n\nmethod = SMOTE(sampling_strategy=\"minority\")\n\nx_res, y_res = method.fit_resample(x_train, y_train)","6f50843e":"<h3>SMOTE + ENN <\/h3>","0bc916d6":"from imblearn.over_sampling import SMOTE\n\nmethod = SMOTE()\n\nx_res, y_res = method.fit_resample(x_train, y_train)","f185ebee":"<h3>Common Mistakes from Imbalanced Datasets:<\/h3>\n<a id=\"defineoversamplingundersampling\"><\/a>\n\n[Visit outline](#outline) <br>\nNever test on the oversampled or undersampled dataset.\nIf we want to implement cross validation, remember to oversample or undersample your training data during cross-validation, not before!\nDon't use accuracy score as a metric with imbalanced datasets (will be usually high and misleading), instead use f1-score, precision\/recall score or confusion matrix\n\n\n![image.png](attachment:image.png)\n\nAs you see above, SMOTE occurs \"during\" cross validation and not \"prior\" to the cross validation process. Synthetic data are created only for the training set without affecting the validation set.","dc99a3a0":"<h3> SMOTE + Tomek links <\/h3>","b986c309":"# Oversampling techniques SMOTE, ADASYN\n<a id=\"oversampling\"><\/a>\n<h3>SMOTE <\/h3>\n\n[Visit outline](#outline) <br>","2bdabbcb":"# Identify cluster using t-sne, PCA, TruncatedSVD\n<a id=\"datadecomposition\"><\/a>\n[Visit outline](#outline)","9b4e87fe":"<h1>Undersampling techniques Tomek Links, ENN<\/h1>\n<a id=\"undersampling\"><\/a>\n<h3> Tomek links <\/h3>\n\n[Visit outline](#outline) <br>","1c2a8b1f":" \n<h2> Overview <\/h2>\n   \n* Get familiar with **AutoViz** - performs automatic visualization of any dataset with one line\n* Understand the theory behind different **Oversampling and Undersampling techniques and use them together**\n* Compare different Sampling techniques on different ML models\n\n\n<b>Please consider to upvote this notebook if you find this useful,<span style=\"color:red\"> UPVOTES<\/span> would be highly appreciated<\/b>\n\n\n\n","c9458e7f":"![image.png](attachment:image.png)","9cde41e7":"\n\nThe hybrid techniques that aims to clean overlapping data points for each of the classes distributed in sample space. <p>After the oversampling is done by SMOTE, the class clusters may be invading each other\u2019s space. As a result, the classifier model will be overfitting. <\/p> <p> Now, to get better class clusters, Tomek links \/ ENN are applied to oversampled minority class samples done by SMOTE.Thus instead of removing the observations only from the majority class, we generally remove both the class observations <\/p>\n","5c1fd811":"<h2> Outline: <\/h2>\n<a id=\"outline\"><\/a>","d699c614":"References :\n\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/07\/10-techniques-to-deal-with-class-imbalance-in-machine-learning\/\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/overcoming-class-imbalance-using-smote-techniques\/\n\nhttps:\/\/www.udemy.com\/course\/machine-learning-with-imbalanced-data\/\n","bd474923":"<h1> Hybridized Sampling: <\/h1>\n<a id=\"samplingimplementation\"><\/a>\n\n[Visit outline](#outline) <br>\n\n![image.png](attachment:image.png)","7cdf2919":"\n\nI. <b>Understanding our data<\/b><br>\na) [Gather sense of our data using AutoViz](#autoviz)<br>\nb) [Identify cluster using t-sne, PCA, TruncatedSVD](#datadecomposition)<br><br>\n\nII. <b>Preprocessing<\/b><br>\na) [Scaling and Distributing](#distributing)<br>\nb) [Splitting the Data](#splitting)<br><br>\n\nIII. <b>Oversampling and Undersampling methods<\/b><br>\na) [Need for imbalanced data handling](#needforimbalanced)<br>\nb) [Common Mistakes from Imbalanced Datasets](#defineoversamplingundersampling)<br>\nc) [Oversampling techniques SMOTE, ADASYN](#oversampling)<br>\nd) [Undersampling techniques Tomek Links, ENN](#undersampling)<br>\ne) [Hybrid sampling (SMOTE + Tomek Links, SMOTE + ENN)](#samplingimplementation)<br>\n\n<br>\n\n\n\nIV. <b>Compare different sampling technqiues performance <\/b> <br>\n[Compare performance of different sampling technqiues on best performing models](#comparesamplingmethods)<br>","c89259d6":"from imblearn.combine import SMOTEENN\n\nmethod = SMOTEENN(sampling_strategy=\"minority\", n_jobs= -1)\n\nx_res, y_res = method.fit_resample(x_train, y_train)","1371c5d7":"# Need for imbalanced data handling\n<a id=\"needforimbalanced\"><\/a>\n[Visit outline](#outline) <br>\nWhen observation in one class is higher than the observation in other classes then there exists a class imbalance. Example: To detect fraudulent credit card transactions. As you can see in the below graph fraudulent transaction is around 400 when compared with non-fraudulent transaction around 90000.\n\nThe Problem with Class Imbalance\nMost machine learning algorithms work best when the number of samples in each class are about equal.\n<br>\nStandard classifier algorithms like **Decision Tree and Logistic Regression have a bias towards classes which have number of instances**. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.\n\n\n> **In a dataset with highly unbalanced classes, the classifier will always \u201cpredicts\u201d the most common class without performing any analysis of the features and it will have a high accuracy rate, obviously not the correct one.**\n\n![image.png](attachment:image.png)\n\n\n**Advantages and disadvatages of under-sampling**\n**Advantages**\n<br>It can help improve run time and storage problems by reducing the number of training data samples when the training data set is huge.\n\n**Disadvantages**\n<br> It can discard potentially useful information which could be important for building rule classifiers.\n<br> The sample chosen by random under-sampling may be a biased sample. And it will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.\n\n**Advantages and Disadvantage of over-sampling**\n<br> **Advantages**\n<br> Unlike under-sampling, this method leads to no information loss.\n\n**Disadvantages**\n<br> It increases the likelihood of overfitting since it replicates the minority class events.","9ddbb746":"from imblearn.combine import SMOTETomek\n\nmethod = SMOTETomek()\n\nx_res, y_res = method.fit_resample(x_train, y_train)","18bd9e9d":"<h1>Compare performance of different sampling technqiues on best performing models<\/h1>\n<a id=\"comparesamplingmethods\">\n    \n[Visit outline](#outline) <br>    \n            ","1bc29ca6":"# Splitting the Data\n<a id=\"splitting\"><\/a>\n[Visit outline](#outline)","7eda108e":"from imblearn.under_sampling import TomekLinks\n\nmethod = TomekLinks()\n\nx_res, y_res = method.fit_resample(x_train, y_train)\n","7b861a3c":"# Gather sense of our data using AutoViz\n<a id=\"autoviz\"><\/a>\n\n[Visit outline](#outline)","b7569b29":"# Scaling and Distributing\n<a id=\"distributing\"><\/a>\n\n[Visit outline](#outline)","2f7d8891":"![image.png](attachment:image.png)\nNew sample = Original sample - (factor * (Original sample - Neighbour))\n<br> Factor = 1 if you want to acheive the same number of observations as that of the majority class\n\nSo, this process continues till the time the no of minority sample doesn't equals to that of majority sample\n"}}