{"cell_type":{"ae67267f":"code","85cfa989":"code","61a891b5":"code","e112115e":"code","0d5ebd5f":"code","97ce0fd2":"code","3d7dd138":"code","bc0bea89":"code","6356bf54":"code","6918dafd":"code","3e3d476f":"code","051470ac":"code","e3fbdf28":"code","ce12c361":"code","fac3e7e2":"code","d18bce0a":"code","ef1cf19e":"code","634c3184":"code","113374f0":"code","72ca3048":"code","cfd6a7b2":"code","d46752b2":"code","afb5d26a":"code","08b65182":"code","a5837b03":"code","19320710":"code","3bb2d569":"code","2ca11626":"code","84af1817":"code","1c7c23c2":"code","4fa34172":"code","c468578d":"markdown","378c2139":"markdown","583bcfbf":"markdown","40d1fa03":"markdown","67122306":"markdown","604b0a4f":"markdown","d2d467a8":"markdown","ef3a957c":"markdown","79060b4b":"markdown","01dc60eb":"markdown","aa06a291":"markdown","36bd3253":"markdown","1e21b66f":"markdown","ceb86ee8":"markdown","1566871c":"markdown","56064522":"markdown"},"source":{"ae67267f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85cfa989":"import nltk\n\nimport keras\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split","61a891b5":"tagged_sentences = nltk.corpus.treebank.tagged_sents()\nprint(f\"Number of sentences we can use >> {len(tagged_sentences)}\\n\")\nprint(f\"Sample data >>\\n{tagged_sentences[15]}\")","e112115e":"words =[]\ntags=[]\n\nwords_bert=[]\n\nfor sentence in tagged_sentences:\n    word,tag = zip(*sentence)\n    words.append(list(word))\n    tags.append(list(tag))\n    words_bert.append(' '.join(list(word)))\n    \nprint(\"Separation finished!\")\nprint(f\"Length of words list >> {len(words)}\")\nprint(f\"Length of tags list >> {len(tags)}\\n\")\n\nprint(\"Sample words\\n\")\nprint(words[22])\nprint(\"Sample tags\\n\")\nprint(tags[22])\n\nprint(\"\\n\\n\")\nprint(words_bert[22])","0d5ebd5f":"print(words_bert[0])\nprint(type(words_bert[0]))","97ce0fd2":"from keras.preprocessing.text import Tokenizer\n\nword_tok = Tokenizer()\nword_tok.fit_on_texts(words)\nprint(f\"Number of items in word Tokenizer >> {len(word_tok.word_index)}\\n\")\nword_size = len(word_tok.word_index)\nvocab_size = word_size+1\n\ntag_tok = Tokenizer()\ntag_tok.fit_on_texts(tags)\nprint(f\"Number of items in tag Tokenizer >> {len(tag_tok.word_index)}\\n\")\ntag_size = len(tag_tok.word_index) +1\n\nprint(\"Tokenizing words and tags\")\nwords = word_tok.texts_to_sequences(words)\ntags = tag_tok.texts_to_sequences(tags)\nprint(\"Tokenizing Finished!\")","3d7dd138":"sns.set_style(\"whitegrid\")\n\nlens = [len(s) for s in words]\nprint(f\"Maximum length of sentence >> {np.max(lens)}\")\nprint(f\"Average length of sentence >> {int(np.round(np.mean(lens)))}\")\n\nplt.hist(lens,bins=100)\nplt.show()\n\nsequence_size = 128","bc0bea89":"from keras.preprocessing.sequence import pad_sequences\n\nwords = pad_sequences(words,padding=\"post\",truncating=\"post\",maxlen=sequence_size)\ntags = pad_sequences(tags,padding=\"post\",truncating=\"post\",maxlen=sequence_size)\ntags_bert = pad_sequences(tags,padding=\"post\",truncating=\"post\",maxlen=128)\n\nprint(\"Samples\\n\")\nprint(words[0])\nprint()\nprint(tags[0])","6356bf54":"from keras.utils import to_categorical\n\ntags_1hot = to_categorical(tags,num_classes=tag_size)","6918dafd":"train_data,test_data,train_label,test_label = train_test_split(words,tags_1hot,test_size=0.3,random_state=42)\n\nprint(f\"train data shape >> {train_data.shape}\")\nprint(f\"train label shape >> {train_label.shape}\\n\")\nprint(f\"test data shape >> {test_data.shape}\")\nprint(f\"test label shape >> {test_label.shape}\")","3e3d476f":"test_label[0]","051470ac":"from keras.layers import Input,Embedding,LSTM,Bidirectional,Dense,Dropout,BatchNormalization,TimeDistributed\nfrom keras.models import Model\nfrom keras.utils import plot_model\n\n# vocab_size\n# tag_size\n# sequence_size\nword_vec_size = 512\nhidden_size = 512\n\ndef create_lstm1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    \n    Y = TimeDistributed(Dense(tag_size,activation='softmax'))(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","e3fbdf28":"lstm1 = create_lstm1()\nhist = lstm1.fit(train_data,train_label,batch_size=64,epochs=8,validation_split=0.1)\nscores = lstm1.evaluate(test_data,test_label)\n\nprint(\"\\n\\nScores on test dataset\")\nprint(f\"Loss >> {scores[0]}\")\nprint(f\"Accuracy >> {scores[1]}\")","ce12c361":"plot_model(lstm1)","fac3e7e2":"import os\n\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","d18bce0a":"embedding_dict = dict()\n\nf =  open(os.path.join('glove.6B.300d.txt'),encoding='utf-8')\n\nfor line in f:\n    tokens = line.split()\n    word = tokens[0]\n    word_vector = np.asarray(tokens[1:],dtype='float32')\n    embedding_dict[word] = word_vector\n\nf.close()\n\nprint(f\"There are {len(embedding_dict)} embedding vectors in total\")\nembedding_size = len(embedding_dict['love'])\nprint(f\"Dimension of each vector >> {embedding_size}\")\n\nembedding_matrix = np.zeros((vocab_size,embedding_size))\n\nfor word,idx in word_tok.word_index.items():\n    if idx <= word_size:\n        vector = embedding_dict.get(word)\n        if vector is not None:\n            embedding_matrix[idx] = np.asarray(vector,dtype='float32')","ef1cf19e":"def create_lstm2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,trainable=False,weights=[embedding_matrix],mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = TimeDistributed(Dense(128))(H)\n    \n    Y = TimeDistributed(Dense(tag_size,activation='softmax'))(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","634c3184":"lstm2 = create_lstm2()\nhist = lstm2.fit(train_data,train_label,batch_size=64,epochs=8,validation_split=0.1)\nscores = lstm2.evaluate(test_data,test_label)\n\nprint(\"Scores on test dataset\")\nprint(f\"Loss >> {scores[0]}\")\nprint(f\"Accuracy >> {scores[1]}\")","113374f0":"plot_model(lstm2)","72ca3048":"def create_lstm3():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.2)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = TimeDistributed(Dense(128))(H)\n    \n    Y = TimeDistributed(Dense(tag_size,activation='softmax'))(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","cfd6a7b2":"lstm3 = create_lstm3()\nhist = lstm3.fit(train_data,train_label,batch_size=64,epochs=8,validation_split=0.1)\nscores = lstm3.evaluate(test_data,test_label)\n\nprint(\"Scores on test dataset\")\nprint(f\"Loss >> {scores[0]}\")\nprint(f\"Accuracy >> {scores[1]}\")","d46752b2":"plot_model(lstm3)","afb5d26a":"index_to_word = word_tok.index_word\nindex_to_tag = tag_tok.index_word\n\ni = 33\npredicted = lstm1.predict(np.array(test_data[i]))\npredicted = np.argmax(predicted,axis=-1)\nlabel = np.argmax(test_label[i],axis=-1)\n\nprint(\"{:15}|{:10}|{:10}\".format(\"word\",\"label\",\"predicted\"))\nprint(\"=\"*40)\n\nfor w,l,p in zip(test_data[i],label,predicted[0]):\n    if w != 0:\n        print(\"{:17}|{:12}|{:12}\".format(index_to_tag[p]))","08b65182":"!pip install -q tensorflow-text","a5837b03":"!pip install -q tf-models-official","19320710":"import os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom official.nlp import optimization\n\ntf.get_logger().setLevel('ERROR')","3bb2d569":"bert_model_name = 'small_bert\/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-768_A-12\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/google\/electra_small\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/google\/electra_base\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/pubmed\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/wiki_books\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/3',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'electra_small':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'electra_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","2ca11626":"bert_process_model = hub.KerasLayer(tfhub_handle_preprocess)\n\ndef create_pos_bert():\n    X = tf.keras.layers.Input(shape=(),dtype=tf.string,name=\"Input\")\n    \n    preprocess = hub.KerasLayer(tfhub_handle_preprocess,name=\"Preprocessing\")(X)\n    encoders = hub.KerasLayer(tfhub_handle_encoder,trainable=True,name=\"BERT_encoder\")(preprocess)\n    \n    result = encoders['sequence_output']\n    \n    H = tf.keras.layers.Dropout(0.1)(result)\n    Y = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tag_size,activation='softmax',name='Classifier'))(H)\n    \n    model = tf.keras.models.Model(X,Y)\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","84af1817":"#words_bert\n#tags_bert\n\ntrain_x,test_x,train_y,test_y = train_test_split(words_bert,tags_bert,test_size=0.3)\ntrain_x = np.array(train_x)\ntest_x = np.array(test_x)","1c7c23c2":"bert_clf = create_pos_bert()\nhist = bert_clf.fit(train_x,train_y,epochs=10,batch_size=64,validation_split=0.1)\ntest_score = bert_clf.evaluate(test_x,test_y)\n\nprint(f\"\\n\\nloss : {test_score[0]}\")\nprint(f\"acc : {test_score[1]}\")","4fa34172":"history_dict = hist.history\nprint(history_dict.keys())\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nfig = plt.figure(figsize=(10,6))\nfig.tight_layout()\n\nplt.subplot(2,1,1)\nplt.plot(range(1,len(acc)+1),acc,'b',label='Training Acc')\nplt.plot(range(1,len(acc)+1),val_acc,'r',label='Validation Acc')\nplt.legend()\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.subplot(2,1,2)\nplt.plot(range(1,len(acc)+1),loss,'b',label='Training Loss')\nplt.plot(range(1,len(acc)+1),val_loss,'r',label='Validation Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.show()","c468578d":"* (4) One-hot encode tags data (label data)","378c2139":" (1) Load Glove dataset","583bcfbf":"[3-3] Build Stacked Bidirectional LSTM (with or without pre-trained embedding vectors)","40d1fa03":"* (3) Padding both words and taggings","67122306":"-> Conclusion : Using pre-trained embedding vectors from Glove dataset doesn't improve final results on this task","604b0a4f":"[1] Load Dataset","d2d467a8":" (2) Extract embedding vectors for words we will use and make them into a single matrix","ef3a957c":" (4) Train and test lstm2 ","79060b4b":"[3-4] BERT","01dc60eb":"* (2) Tokenize both words and taggings","aa06a291":"[3-1] Build Bidirectional LSTM (not stacked) model (without using pre-trained embedding vectors)","36bd3253":"-> Train and test lstm1","1e21b66f":"[2] Preprocessing\n*     (1) Separate words and taggings","ceb86ee8":" (3) Build Bidirectional LSTM model using pre-trained embedding vectors ","1566871c":"[3-2] Build Bidirectional LSTM (not stacked) model (using pre-trained embedding vectors)","56064522":"* (5) Split train data and test data"}}