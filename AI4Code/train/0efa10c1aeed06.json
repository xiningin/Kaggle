{"cell_type":{"a971a7a6":"code","1ab78bbb":"code","ff4ee72a":"code","74199eef":"code","869f7667":"code","7e2c83f8":"code","fe718e14":"code","2cbb8b86":"code","f091a874":"code","c1c9387f":"code","63243a93":"markdown","0d79d2ca":"markdown","f1d7aa7b":"markdown","2c19dd06":"markdown","5ba55139":"markdown","9793eef9":"markdown","0eed24e8":"markdown"},"source":{"a971a7a6":"import tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\n\nhv.extension('bokeh')","1ab78bbb":"num_components = 2\nk = 2","ff4ee72a":"X = tf.concat([tf.random.normal(mean=[10,10], shape=(200,2)), \n               tf.random.normal(mean=[-10,-10], shape=(100,2))], axis=0)","74199eef":"hv.Scatter(pd.DataFrame(X.numpy(), columns=['x','y']), kdims='x',vdims='y')","869f7667":"class Gamma:\n    def __init__(self, concentration, rate):\n        super(Gamma, self).__init__()\n        \n        self.concentration = concentration\n        self.rate = rate\n    \n    def __call__(self, shape, dtype=None):\n        return tf.random.gamma(shape, \n                               alpha = self.concentration, \n                               beta = self.rate, \n                               dtype=getattr(tf.dtypes, dtype))","7e2c83f8":"class VariationalBayesianGaussianMixture(tf.keras.layers.Layer):\n    def __init__(self, num_outputs, convert_to_tensor_fn = tfp.distributions.Distribution.sample):\n        super(VariationalBayesianGaussianMixture, self).__init__()\n        self.num_outputs = num_outputs\n        self.convert_to_tensor_fn = convert_to_tensor_fn\n        \n\n    def build(self, input_shape):\n        _input_shape = input_shape[1]\n        self._input_shape = _input_shape\n        \n        # Variational distribution variables for means\n        locs_init = tf.keras.initializers.RandomNormal()\n        self.locs = tf.Variable(initial_value=locs_init(shape=(self.num_outputs, _input_shape),\n                                                        dtype='float32'),\n                                trainable=True)\n        scales_init = Gamma(5., 5.)\n        self.scales = tf.Variable(initial_value=scales_init(shape=(self.num_outputs, _input_shape),\n                                                  dtype='float32'),\n                                  trainable=True)\n\n        # Variational distribution variables for standard deviations\n        alpha_init = tf.keras.initializers.RandomUniform(4., 6.)\n        self.alpha = tf.Variable(initial_value=alpha_init(shape=(self.num_outputs, _input_shape), dtype='float32'),\n                                 trainable=True)\n        beta_init = tf.keras.initializers.RandomUniform(4., 6.)\n        self.beta = tf.Variable(initial_value=beta_init(shape=(self.num_outputs, _input_shape), dtype='float32'),\n                                trainable=True)\n\n        counts_init = tf.keras.initializers.Constant(2)\n        self.counts = tf.Variable(initial_value=counts_init(shape=(self.num_outputs,), dtype='float32'),\n                                  trainable=True)\n\n        # priors\n        mu_mu_prior = tf.zeros((self.num_outputs, _input_shape))\n        mu_sigma_prior = tf.ones((self.num_outputs, _input_shape))\n        self.mu_prior = tfp.distributions.Normal(mu_mu_prior, mu_sigma_prior)\n\n        sigma_concentration_prior = 5.*tf.ones((self.num_outputs, _input_shape))\n        sigma_rate_prior = 5.*tf.ones((self.num_outputs, _input_shape))\n        self.sigma_prior = tfp.distributions.Gamma(sigma_concentration_prior, sigma_rate_prior)\n\n        theta_concentration = 2.*tf.ones((self.num_outputs,))\n        self.theta_prior = tfp.distributions.Dirichlet(theta_concentration)\n    \n\n    def call(self, inputs, sampling=True):\n        n_samples = tf.dtypes.cast(tf.reduce_mean(tf.reduce_sum(inputs ** 0., 0)), 'float32') # TODO: get rid of expensie hack\n        \n        # The variational distributions\n        mu = tfp.distributions.Normal(self.locs, self.scales)\n        sigma = tfp.distributions.Gamma(self.alpha, self.beta)\n        theta = tfp.distributions.Dirichlet(self.counts)\n        \n        # Sample from the variational distributions\n        if sampling:\n            mu_sample = mu.sample(n_samples)\n            sigma_sample = tf.pow(sigma.sample(n_samples), -0.5)\n            theta_sample = theta.sample(n_samples)\n        else:\n            mu_sample = tf.reshape(mu.mean(), (1, self.num_outputs, self._input_shape))\n            sigma_sample = tf.pow(tf.reshape(sigma.mean(), (1, self.num_outputs, self._input_shape)), -0.5)\n            theta_sample = tf.reshape(theta.mean(), (1, self.num_outputs))\n        \n        # The mixture density\n        density = tfp.distributions.MixtureSameFamily(\n                        mixture_distribution=tfp.distributions.Categorical(probs=theta_sample),\n                        components_distribution=tfp.distributions.MultivariateNormalDiag(loc=mu_sample,\n                                                                                         scale_diag=sigma_sample))\n                \n        # Compute the mean log likelihood\n        log_likelihoods = density.log_prob(inputs)\n        \n        # Compute the KL divergence sum\n        mu_div    = tf.reduce_sum(tfp.distributions.kl_divergence(mu,    self.mu_prior))\n        sigma_div = tf.reduce_sum(tfp.distributions.kl_divergence(sigma, self.sigma_prior))\n        theta_div = tf.reduce_sum(tfp.distributions.kl_divergence(theta, self.theta_prior))\n        kl_sum = sigma_div + theta_div + mu_div\n                \n        self.add_loss(kl_sum\/n_samples - log_likelihoods)\n        \n        return tfp.layers.DistributionLambda(lambda x: density, \n                                             convert_to_tensor_fn=self.convert_to_tensor_fn)(inputs)\n        ","fe718e14":"model = tf.keras.models.Sequential([VariationalBayesianGaussianMixture(2, 'sample')])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-1), \n              loss = lambda y_true, y_pred: tf.zeros_like(y_pred))\n\nmodel.fit(X, X, batch_size=300, epochs=1000)","2cbb8b86":"%matplotlib inline\nhv.Scatter(pd.DataFrame(model.predict(X),columns=['x','y']), kdims='x',vdims='y', label='samples from model') *\\\nhv.Scatter(pd.DataFrame(X.numpy(),columns=['x','y']), kdims='x',vdims='y', label='original data').opts(width=800, height=400)","f091a874":"distribution = model.layers[0](X, sampling=False)","c1c9387f":"# mixing probability\ndistribution.mixture_distribution.probs_parameter()","63243a93":"I would love to get people ideas whether they would like to see these kinds of tools in Tensorflow for model prototyping and their experience with TF Probability and TF2. ","0d79d2ca":"Six months after the release of Tensorflow 2.0 (TF2) its has been amazing to see the energy it has injected into the ecosystem.  I think for many new-comers Tensorflow was quite an intimidating tool without the necessary abstractions to make Tensorflow feel effortless. For a long time I was reluctant to dive deep into the Tensorflow ecosystem and dabbled in PyTorch, Chainer and MxNet, as I hoped the Tensorflow ecosystem with be revived with dynamic graphs for experimentation and expressiveness. TF2 came with many great refreshes to TF Lite, TF Serving and the 0.8 release of TF Probability.  TF Probablity is an amazing tool which I have used not in development and experiments to describe and estimate probabilistic models, which provides great tooling for fast MCMC sampling and Variational Inference.  \n  \nWhile I love Tensorflow I do consider it an investment, as you swap many components of the conventional Python Data Science Stack, for Tensorflow loading and preprocessing conventions. Scikit-learn in particular has an amazing suite of tools for quickly doing clustering, manifold learning and active learning and despite Tensorflow's best efforts with TF Estimators to offer Histogram Gradient Boosting I have always seen a gap to provide a wider quite of models in the tensorflow ecosystem- the problem is, they should probably be differentiable.  \n\nVariational Bayesian Gaussian Mixture Models are a family of probability models commonly used for clustering, unlike there Frequentist cousins- which efficiently rely on the Expectation Maximization Algorithm- these models can typically be estimated using gradient descent by minimizing both the log-likelihood of our mixture model and the ELBO KL-divergence between our estimates and priors. \n\nThis gave me the idea what would it be like to have this as a layer for Keras. While you would not want to use this is a deep network, without some stop gradient for fear of learning the most perfect and beutiful blobs you have ever seen, you may want to use this layer to do anomaly detection or clustering with all the wonderful benefits of GPU computation, TF Serving, Tensorboard and TF Data. ","f1d7aa7b":"As this notebook is largely an excercise in TF Probability I opted to start with something simple- two very well seperated 2D gaussian blobs. This makes it easy for us to visually inspect and test.  I also opted to sample twice the number of datapoints from the one gaussian, to make incorrect estimation a little more aparent. ","2c19dd06":"Sadly this did take a little work in defining new probability distributions for weight initializion. ","5ba55139":"I have tried my best to comment the code as much as possible for those to follow. There are manu components one would want to tidy up for a production implementation, such as the hard-coded dtypes. ","9793eef9":"Looking at the mixing probabilities, they seem well in-line with our simulated data.  ","0eed24e8":"Looking at our samples, we seem to have fairly consistent parameter estimates for our data. "}}