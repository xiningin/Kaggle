{"cell_type":{"4647682b":"code","9ecddf0f":"code","996ea8f1":"code","6dbc39dc":"code","43908cac":"code","4f7158a7":"code","d0372409":"code","26872a5c":"code","66489ef5":"code","d02ba5c0":"code","1366f9f6":"code","dec7f00f":"code","c5e53662":"code","c0ae5d38":"code","2d96d47e":"code","cdd53f5e":"code","f1536c7d":"code","0c0bdd16":"code","ca808827":"code","f8deead4":"code","2d758aa2":"markdown","08ca7590":"markdown","7d6c5624":"markdown","1a9236ae":"markdown","04d2f542":"markdown","543091c9":"markdown","65f6232a":"markdown","e18e2b8c":"markdown","174afeb7":"markdown","aa48372b":"markdown","05b90147":"markdown","53e16239":"markdown"},"source":{"4647682b":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","9ecddf0f":"from fastai.vision import *\nfrom fastai.metrics import error_rate\nfrom pathlib import Path\nfrom ipywidgets import IntProgress\nfrom IPython.display import display","996ea8f1":"path = '\/kaggle\/input\/lego-brick-images'\nimagePath = path +'\/dataset'","6dbc39dc":"#Test if the dataset images are found\nfnames = get_image_files(imagePath)\nfnames[:4]","43908cac":"# Test the regular expression to filter out the classification name. \n# The space at the end is trimmed during import into ImageDataBunch, so don't care.\nimport re\nre.search(r'([^\/]+) ', fnames[0].name)[0] ","4f7158a7":"data = ImageDataBunch.from_name_re(imagePath, \n                                   fnames, \n                                   r'\/([^\/]+) ', \n                                   ds_tfms=get_transforms(), \n                                   size=224, \n                                   bs=64\n                                  ).normalize(imagenet_stats)","d0372409":"#Check the number of training anf validation items.\n#The validation.txt file is not used as input, the ImageDataBunch does this.\nlen(data.train_ds.x.items), len(data.valid_ds.x.items)","26872a5c":"#Check the data classes\nprint([len(data.classes), data.classes])","66489ef5":"#Be sure to enable under Kaggle NoteBook Settings: 'Internet' to On and 'GPU' to On\nlearn = cnn_learner(data, models.resnet34, metrics=error_rate, model_dir='\/kaggle\/output')","d02ba5c0":"lr_find(learn)\nlearn.recorder.plot()","1366f9f6":"learn.fit_one_cycle(10, slice(6e-3), pct_start=0.9)","dec7f00f":"learn.lr_find()\nlearn.recorder.plot()","c5e53662":"learn.unfreeze()\nlearn.fit_one_cycle(10, max_lr=slice(1e-6,1e-4))\nlearn.freeze()","c0ae5d38":"interp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\ninterp.plot_top_losses(9, figsize=(15,11))","2d96d47e":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","cdd53f5e":"fig, ax = plt.subplots(1,2)\nax[0].imshow(plt.imread(f'{imagePath}\/3046 roof corner inside tile 2x2 007R.png'));\nax[1].imshow(plt.imread(f'{imagePath}\/3003 brick 2x2 000L.png'));","f1536c7d":"#Determine the error rate with one camera as verification. \n#This must be equal to the last outcome of training epoch.\n\nprg = IntProgress(min=0, max=len(data.valid_ds.x.items)) # instantiate the progress bar\ndisplay(prg) # display the progress bar\n\nerr = 0\nfor f in data.valid_ds.x.items:\n    cat = f.name[:-9]\n    pred_class,pred_idx,outputs = learn.predict(open_image(f))\n    pred_cat = learn.data.classes[pred_class.data.item()]\n    if pred_cat != cat:\n        err += 1\n    prg.value += 1\n    \nprint(f'Error rate with one camera: {err\/len(data.valid_ds.x.items)}')","0c0bdd16":"fnamesR = [f for f in data.valid_ds.x.items if f.name[-5:] == 'R.png']\nfnamesL = [f for f in data.valid_ds.x.items if f.name[-5:] == 'L.png']\nprint([len(fnamesR), len(fnamesL)])","ca808827":"if len(fnamesR) < len(fnamesL):\n    suffix = 'L'\n    fnames2 = fnamesR\nelse:\n    suffix = 'R'\n    fnames2 = fnamesL\nprint([suffix, len(fnames2)])","f8deead4":"#Determine the error rate with two cameras\n\nprg = IntProgress(min=0, max=len(fnames2)) # instantiate the progress bar\ndisplay(prg) # display the progress bar\n\nerr = 0\nfor fA in fnames2:\n    fB = Path(f'{imagePath}\/{fA.name[:-5]}{suffix}.png')\n    cat = fA.name[:-9]\n    pred_classA,pred_idxA,outputsA = learn.predict(open_image(fA))\n    pred_catA = learn.data.classes[pred_classA.data.item()]\n    pred_classB,pred_idxB,outputsB = learn.predict(open_image(fB))\n    pred_catB = learn.data.classes[pred_classB.data.item()]\n    outputs = outputsA+outputsB\n    arr = outputs.numpy()\n    maxval = np.amax(arr)\n    maxind = np.where(arr == maxval)[0][0]\n    if data.classes[maxind] != cat:\n        err += 1\n    prg.value += 1\nprint(f'Error validation set with two cameras: {err\/len(fnames2)}')","2d758aa2":"And that is a real improvement going to <1%! ","08ca7590":"The Resnet34 model is trained using all images regardless the 2 camera orientation. During the validation phase the two cameras will be considered.","7d6c5624":"For example, check row '3046 roof corner inside tile 2x2' with column '3003 brick 2x2' where the cell shows 13 errors. Let look up images of these 2 bricks what is going on:  ","1a9236ae":"## Result with one camera\nThe result of the final training epoch shows a error rate of ~5%. \nLet's take a look which images go most wrong:  ","04d2f542":"If you examine these images then you could easily see what goes wrong. These viewpoints can occur with several bricks.\nThe confusion matrix makes even more clear:","543091c9":"# Conclusion\nIt does pay off to use a multiple camera setup.","65f6232a":"The error rate is the same as last epoch. We are on the right track here.\n\nThere is always an image pair in the two camera setup. Let's select the images of one camera and lookup the twin image of the other camera.\nThe ImageDataBunch object used during the training, selected 20% of all images regardless of camera orientation. The selection is at random, but statistically this should be a close to 50%\/50% even split:","e18e2b8c":"Now it is time for the final number crunch:","174afeb7":"## Two camera recognition\n![](http:\/\/)Let's first start with a verification of how th error rate is determined on the final epoch to ensure we understand how this is calculated and to avoid we use different methods:","aa48372b":"# One versus two camera setup to recognize LEGO bricks\nThis simple kernel is to show the advantage of a two camera versus one camera setup. \n\nVersion 2 of the [Images of LEGO bricks dataset](https:\/\/www.kaggle.com\/joosthazelzet\/lego-brick-images) are rendered with 2 different camera positions as explained in the [dataset creation article](https:\/\/www.kaggle.com\/joosthazelzet\/how-to-create-a-lego-bricks-dataset-using-maya). \nThis kernel will show how to create a Resnet34 model to recognize LEGO bricks. Next, the one camera versus two camera setup is demonstrated and shown how the two camera approach results in a much smaller error rate. \n\nIn order to run this kernel you need to enable under the Kaggle NoteBook Settings: \n- 'Internet' to on to be able to download the basic Resnet34 model \n- 'GPU' to On to speed up the learning process","05b90147":"The smallest set must be leading and selected to ensure always an matching twin can be found. So for the two camera we end up with a validation set of 3976 paired images:","53e16239":"With a little fantasy you can image why from some viewpoint angles will become an issue if only one camera is used.\n\nLet's now invoke the two cameras and see what this improves."}}