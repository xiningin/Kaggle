{"cell_type":{"eaeca1fe":"code","e467f4e3":"code","d273e769":"code","47830a15":"code","02d02838":"code","4ede3a91":"code","365e14c3":"code","5d72d152":"code","d2a0a8b1":"code","39589518":"code","a804038a":"code","06fb3d22":"code","f3bdd96c":"code","57ad5730":"code","171e5156":"code","efbd7ef6":"code","f310844e":"code","10ccf5e5":"code","4bf8e48c":"code","9d8b71bc":"code","4e4eec24":"code","3eeacfad":"code","9d7b9723":"code","afd9f4cb":"code","939253ad":"code","82e34909":"code","0a56edfd":"code","f01b094c":"code","9e543f58":"code","5a5fdadb":"code","1c0dbcec":"code","0dcb8f3e":"code","21f97904":"code","d8e28bc5":"code","307e5efd":"code","13d15f8c":"code","2ea9b0ff":"code","203a4ff8":"code","c8248f58":"markdown","4a0f1190":"markdown","49764548":"markdown","15376725":"markdown","d544e06c":"markdown","802cdd25":"markdown","11f2986f":"markdown","c7e286a9":"markdown","8c3e9f85":"markdown","4a5c41e1":"markdown","bca355b7":"markdown","bdd02030":"markdown","ee4676af":"markdown","0d85ed19":"markdown","7d5030e9":"markdown","9b6b60e2":"markdown","46623dfc":"markdown","550b62d4":"markdown","4caaf9fa":"markdown","97d3e0c4":"markdown","b87b2cc2":"markdown","769f5d8e":"markdown","2083b332":"markdown","95c8d0bd":"markdown"},"source":{"eaeca1fe":"import gc\nimport numpy as np\nimport pandas as pd\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\n# Scaling\nfrom sklearn.preprocessing import RobustScaler\n\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n# Cross-Validation\nfrom sklearn.model_selection import StratifiedKFold\n\n# Scoring\nfrom sklearn.metrics import accuracy_score","e467f4e3":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")","d273e769":"train.head()","47830a15":"train.describe()","02d02838":"print(\"Columns: \\n{0}\".format(list(train.columns)))","4ede3a91":"pseudo = pd.read_csv(\"..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv\")\ntrain = pd.concat([train, pseudo], axis=0)\ntrain.reset_index(drop=True)","365e14c3":"del pseudo\ngc.collect()","5d72d152":"print('Train data shape:', train.shape)\nprint('Test data shape:', test.shape)","d2a0a8b1":"missing_values_train = train.isna().any().sum()\nprint('Missing values in train data: {0}'.format(missing_values_train[missing_values_train > 0]))\n\nmissing_values_test = test.isna().any().sum()\nprint('Missing values in test data: {0}'.format(missing_values_test[missing_values_test > 0]))","39589518":"duplicates_train = train.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))\n\nduplicates_test = test.duplicated().sum()\nprint('Duplicates in test data: {0}'.format(duplicates_test))","a804038a":"del missing_values_train\ndel missing_values_test\ndel duplicates_train\ndel duplicates_test\ngc.collect()","06fb3d22":"categorical_features = train.columns[11:-1:]\nprint(\"Categorical Columns: \\n{0}\".format(list(categorical_features)))","f3bdd96c":"numerical_features = train.columns[1:11]\nprint(\"Numerical Columns: \\n{0}\".format(list(train.columns[1:11])))\ntrain[numerical_features].describe()","57ad5730":"plt.figure(figsize=(10, 6))\nplt.title('Target distribution')\nax = sns.countplot(x=train['Cover_Type'], data=train)","171e5156":"cType5 = train[train['Cover_Type'] == 5].index\nprint(\"Number of rows with Cover_Type = 5: {0}\".format(len(cType5)))","efbd7ef6":"print(\"Unique values in Soil_Type7 column train data: {0}\".format(train['Soil_Type7'].unique()))\nprint(\"Unique values in Soil_Type15 column train data: {0}\".format(train['Soil_Type15'].unique()))\n\nprint(\"Unique values in Soil_Type7 column test data: {0}\".format(test['Soil_Type7'].unique()))\nprint(\"Unique values in Soil_Type15 column test data: {0}\".format(test['Soil_Type15'].unique()))","f310844e":"# Dropping the row Cover_Type = 5,\n# causes problems during kfold (least populated class)\n# Also, it appears there is no label 5 in test data, \n# Check out, https:\/\/www.kaggle.com\/baekseungyun\/tps-dec-there-is-no-label-5-in-test-data\ntrain.drop(cType5, axis=0, inplace=True)\n\n# Dropping columns Soil_Type7 and Soil_Type15, they are zero\ntrain.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","10ccf5e5":"del categorical_features\ndel cType5\ndel ax\ngc.collect()","4bf8e48c":"encoder = LabelEncoder()\ntrain[\"Cover_Type\"] = encoder.fit_transform(train[\"Cover_Type\"])","9d8b71bc":"for data in [train, test]:\n    data[\"Aspect\"][data[\"Aspect\"] < 0] += 360\n    data[\"Aspect\"][data[\"Aspect\"] > 359] -= 360","4e4eec24":"for data in [train, test]:\n    data.loc[data[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    data.loc[data[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    data.loc[data[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    data.loc[data[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    data.loc[data[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    data.loc[data[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","3eeacfad":"for data in [train, test]:\n    # Manhhattan distance to Hydrology\n    data[\"Manhhattan_Distance_To_Hydrology\"] = np.abs(data[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(data[\"Vertical_Distance_To_Hydrology\"])\n    # Euclidean distance to Hydrology\n    data[\"Euclidean_Distance_To_Hydrology\"] = (data[\"Horizontal_Distance_To_Hydrology\"]**2 + data[\"Vertical_Distance_To_Hydrology\"]**2)**0.5","9d7b9723":"features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\nfor data in [train, test]:\n    # Thanks @mpwolke : https:\/\/www.kaggle.com\/mpwolke\/tooezy-where-are-you-no-camping-here\n    data[\"Soil_Count\"] = data[soil_features].apply(sum, axis=1)\n    \n    # Thanks @yannbarthelemy : https:\/\/www.kaggle.com\/yannbarthelemy\/tps-december-first-simple-feature-engineering\n    data[\"Wilderness_Area_Count\"] = data[wilderness_features].apply(sum, axis=1)\n    data[\"Hillshade_mean\"] = data[features_Hillshade].mean(axis=1)\n    data['amp_Hillshade'] = data[features_Hillshade].max(axis=1) - data[features_Hillshade].min(axis=1)","afd9f4cb":"cols = test.columns\nfor data in [train, test]:\n    data['sum_na'] = data.isna().sum(axis = 1)\n    data['mean'] = data[cols].mean(axis=1)\n    data['min'] = data[cols].min(axis=1)\n    data['max'] = data[cols].max(axis=1)","939253ad":"new_features = [\n    \"Manhhattan_Distance_To_Hydrology\",\n    \"Euclidean_Distance_To_Hydrology\",\n    \"Soil_Count\",\n    \"Wilderness_Area_Count\",\n    \"Hillshade_mean\",\n    \"amp_Hillshade\",\n    \"sum_na\",\n    \"mean\",\n    \"min\",\n    \"max\"\n]\nfeatures = np.concatenate((new_features, numerical_features))\n\nscaler = RobustScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","82e34909":"del wilderness_features\ndel features_Hillshade\ndel numerical_features\ndel soil_features\ndel new_features\ndel features\ndel scaler\ndel cols\ngc.collect()","0a56edfd":"# This code snippet is taken from https:\/\/www.kaggle.com\/desalegngeb\/december-2021-tps-eda-models\n# Originally https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275854\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","f01b094c":"UNITS = train[\"Cover_Type\"].nunique()\nTEST_ID = test.Id.copy()","9e543f58":"# Get train data without the target and ids\nX = train.drop(['Id', 'Cover_Type'], axis=1).copy()\n# Get the target\ny = train.Cover_Type.copy()\n# Get the test data without ids\ntest_X = test.drop(['Id'], axis=1).copy()","5a5fdadb":"del train\ndel test\ngc.collect()","1c0dbcec":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_accuracy\",     # Quantity to be monitored\n    patience=20,                # How many epochs to wait before stopping\n    restore_best_weights=True)","0dcb8f3e":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.5,                # Factor by which the learning rate will be reduced\n    patience=5)                # Number of epochs with no improvement","21f97904":"CALLBACKS = [early_stopping, reduce_lr]","d8e28bc5":"# Credits to https:\/\/www.kaggle.com\/samuelcortinhas\/tps-dec-feat-eng-pseudolab-clean-version\nN_SPLITS = 8\nEPOCHS = 100\nBATCH_SIZE = 250","307e5efd":"# Credits to https:\/\/www.kaggle.com\/chryzal\/features-engineering-for-you\n# Credits to https:\/\/www.kaggle.com\/samuelcortinhas\/tps-dec-feat-eng-pseudolab-clean-version\nmodel = keras.Sequential([\n    layers.Dense(units=256, kernel_initializer=\"lecun_normal\", activation=\"selu\", input_shape=[X.shape[1]]),\n    layers.BatchNormalization(),\n    layers.Dense(units=256, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n    layers.BatchNormalization(),\n    layers.Dense(units=128, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n    layers.BatchNormalization(),\n    layers.Dense(units=64, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n    layers.BatchNormalization(),\n    layers.Dense(units=UNITS, activation=\"softmax\")])","13d15f8c":"model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])","2ea9b0ff":"fold = 1\nscores = []\ntest_predictions = np.zeros((1, 1))\ncv = StratifiedKFold(n_splits=N_SPLITS, random_state=48, shuffle=True)\nfor train_idx, test_idx in cv.split(X, y):\n    train_X, val_X = X.iloc[train_idx], X.iloc[test_idx]\n    train_y, val_y = y.iloc[train_idx], y.iloc[test_idx]\n\n    model.fit(\n        train_X, train_y,\n        validation_data=(val_X, val_y),\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=CALLBACKS,        # Put your callbacks in a list\n        verbose=0)                  # Turn off training log\n\n    predictions = np.argmax(model.predict(val_X), axis=1)\n    score = accuracy_score(val_y, predictions)\n    scores.append(score)\n    print(f\"Fold {fold} \\t\\t Accuracy: {score}\")\n\n    # Get the average values from each fold to the prediction\n    test_predictions = test_predictions + model.predict(test_X)\n    fold += 1\nprint('Overall Accuracy: ', np.mean(scores))","203a4ff8":"test_predictions = np.argmax(test_predictions, axis=1)\ntest_predictions = encoder.inverse_transform(test_predictions)\noutput = pd.DataFrame({'Id': TEST_ID, 'Cover_Type': test_predictions})\noutput.to_csv('submission.csv', index=False)","c8248f58":"## Callbacks","4a0f1190":"# Explore Data","49764548":"## Arrange the range of `Aspect` column\n\nSets the `Aspect` columns' value range to 0 to 359.","15376725":"# Reduce memory usage\n\nThis code snippet is taken from https:\/\/www.kaggle.com\/desalegngeb\/december-2021-tps-eda-models  \nOriginally https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275854","d544e06c":"## Model","802cdd25":"# Basic Data Check","11f2986f":"# Pseudo Labeling\n\nCredits to [remekkinas](https:\/\/www.kaggle.com\/remekkinas) for his notebook [TPS-12 NN (TPU) + Pseudolabeling](https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95690) and his dataset [TPS-12 - Pseudolabels](https:\/\/www.kaggle.com\/remekkinas\/tps12-pseudolabels).","c7e286a9":"## Duplicates","8c3e9f85":"## Categorical Features","4a5c41e1":"## Dropping rows and columns","bca355b7":"## Target Distribution","bdd02030":"# Submission","ee4676af":"## Encoding labels","0d85ed19":"## Missing values","7d5030e9":"## Arrange the range of `Hillshade` columns\n\nSets the `Hillshade` columns' value range to 0 to 255.","9b6b60e2":"## Training","46623dfc":"## Numerical Features","550b62d4":"## Creating new features\n\nCreating the following new features:  \n* Sum of all the soil types\n* Sum of all the wilderness area types","4caaf9fa":"# Importing Libraries and Loading datasets","97d3e0c4":"## Creating distance based features","b87b2cc2":"# Features","769f5d8e":"# Feature Engineering\n\nCredits to [chryzal](https:\/\/www.kaggle.com\/chryzal) for his notebook [\ud83e\udd47Features Engineering For You \ud83e\udd47](https:\/\/www.kaggle.com\/chryzal\/features-engineering-for-you).","2083b332":"## Scaling features","95c8d0bd":"# Modelling"}}