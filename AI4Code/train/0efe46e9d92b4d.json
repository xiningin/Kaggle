{"cell_type":{"f0863b47":"code","a3022903":"code","a56abad2":"code","4fdad7b6":"code","983b402b":"code","dd31fd6e":"code","76dfdfba":"code","32476286":"code","b27f35b4":"code","919bc56c":"code","c70f86bb":"code","5c855ace":"code","3050f705":"code","0c50bc6b":"code","102653e7":"code","e062cecb":"code","04f6b6c3":"code","f1abdbfd":"code","3aba497d":"code","995e8a1b":"code","a1e733d1":"code","12ee5500":"code","170272dd":"code","9253bb5b":"code","8957430f":"code","3c42491f":"code","5ac28937":"code","2148556f":"code","3e8bd9d0":"code","bb151240":"code","306467a2":"code","6046fd5e":"code","03fa6fb5":"code","b1b8f3ee":"code","42269e1c":"code","38f056d1":"code","cfcfe218":"code","9f724e4a":"code","20ead8f2":"code","7cd86ffa":"code","2037c392":"code","43961293":"code","c0987eb2":"code","ac67ed59":"code","200f507e":"code","c8ff9d19":"markdown","f0ba6547":"markdown","5b0d71f5":"markdown","c7a667a2":"markdown","7ee9c7dc":"markdown","91cfa29b":"markdown"},"source":{"f0863b47":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a3022903":"train_df=pd.read_csv(\"\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv\")\nsample_df=pd.read_csv(\"\/kaggle\/input\/bigquery-geotab-intersection-congestion\/sample_submission.csv\")","a56abad2":"train_df.head()","4fdad7b6":"test_df.head()","983b402b":"train_df.info()","dd31fd6e":"#total number of intersections in our dataset\nprint(\"The total number of unique intersections in our dataset are : {}\".format(train_df['IntersectionId'].nunique()))\nprint(\"The total number of Cities in our dataset are : {}\".format(train_df['City'].nunique()))","76dfdfba":"#Number of intersections in each city\ntrain_df.groupby('City')['IntersectionId'].nunique().plot(kind='barh')","32476286":"print(\"The maximum number of entry streets for intersections in our dataset is : {}\".format(train_df.groupby('IntersectionId')['EntryStreetName'].nunique().max()))\nprint(\"The average number of entry streets for intersections in our dataset is : {}\".format(train_df.groupby('IntersectionId')['EntryStreetName'].nunique().mean()))","b27f35b4":"print(\"The maximum number of exit streets for intersections in our dataset is : {}\".format(train_df.groupby('IntersectionId')['ExitStreetName'].nunique().max()))\nprint(\"The average number of exit streets for intersections in our dataset is : {}\".format(train_df.groupby('IntersectionId')['ExitStreetName'].nunique().mean()))","919bc56c":"#Transforming CAtegorical features","c70f86bb":"import seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\ncorr = train_df.corr()\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)\n","5c855ace":"# Drop rowID\ntrain_df.drop('RowId', axis=1, inplace=True)\ntest_df.drop('RowId', axis=1, inplace=True)","3050f705":"#Check cardinality of all the categorical variables\nfor y in train_df.columns:\n    if(train_df[y].dtype == object):\n          print(\"The caridinality for {} is : {}\".format(y, train_df[y].nunique()))","0c50bc6b":"X = train_df[['IntersectionId', 'Latitude', 'Longitude', 'EntryStreetName', \n              'ExitStreetName', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'Path','City']]\ny1 = train_df[\"TotalTimeStopped_p20\"]\ny2 = train_df[\"TotalTimeStopped_p50\"]\ny3 = train_df[\"TotalTimeStopped_p80\"]\ny4 = train_df[\"DistanceToFirstStop_p20\"]\ny5 = train_df[\"DistanceToFirstStop_p50\"]\ny6 = train_df[\"DistanceToFirstStop_p80\"]","102653e7":"#Creating Dummies for train Data\ndfen = pd.get_dummies(X[\"EntryHeading\"],prefix = 'entry')\ndfex = pd.get_dummies(X[\"ExitHeading\"],prefix = 'exit')\ncity = pd.get_dummies(X[\"City\"],prefix = 'city')\n\nX = pd.concat([X,dfen],axis=1)\nX = pd.concat([X,dfex],axis=1)\nX = pd.concat([X,city],axis=1)\n\nX.drop(\"EntryHeading\", axis=1, inplace=True)\nX.drop(\"ExitHeading\", axis=1, inplace=True)\nX.drop(\"City\", axis=1, inplace=True)\n\n#Creating Dummies for test Data\ndfent = pd.get_dummies(test_df[\"EntryHeading\"],prefix = 'entry')\ndfext = pd.get_dummies(test_df[\"ExitHeading\"],prefix = 'exit')\ncity_test = pd.get_dummies(test_df[\"City\"],prefix = 'city')\n\ntest_df = pd.concat([test_df,dfent],axis=1)\ntest_df = pd.concat([test_df,dfext],axis=1)\ntest_df = pd.concat([test_df,city_test],axis=1)\n\ntest_df.drop(\"EntryHeading\", axis=1, inplace=True)\ntest_df.drop(\"ExitHeading\", axis=1, inplace=True)\ntest_df.drop(\"City\", axis=1, inplace=True)","e062cecb":"X.head()","04f6b6c3":"#Visualizing rows having NaN values for EntryStreetName and ExitStreetName\n#Path being concatenation of EntryStreetName_EntryHeading_ExitStreetName_ExitHeading\ntrain_df[train_df.isnull().any(axis=1)].head()","f1abdbfd":"#filling rows with NaN's\nX.fillna(\"nan\", inplace=True)\ntest_df.fillna(\"nan\", inplace=True)","3aba497d":"X.drop('Path', axis=1, inplace=True)\ntest_df.drop('Path', axis=1, inplace=True)","995e8a1b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX['EntryStreetName'] = le.fit_transform(X['EntryStreetName'])\ntest_df['EntryStreetName'] = le.fit_transform(test_df['EntryStreetName'])\n\nX['ExitStreetName'] = le.fit_transform(X['ExitStreetName'])\ntest_df['ExitStreetName'] = le.fit_transform(test_df['ExitStreetName'])\n\n# X['Path'] = le.fit_transform(X['Path'])\n# test_df['Path'] = le.fit_transform(test_df['Path'])","a1e733d1":"X.shape","12ee5500":"y1.shape","170272dd":"# import lightgbm as lgb\n# from sklearn.model_selection import KFold, StratifiedKFold\n# def kfold_lightgbm(target, num_folds= 10):\n#     print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(X.shape, test_df.shape))\n#     folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n#     # Create arrays and dataframes to store results\n#     oof_preds = np.zeros(X.shape[0])\n#     sub_preds = np.zeros(test_df.shape[0])\n\n#     for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X,target)):\n#         train_x, train_y = X.iloc[train_idx], target.iloc[train_idx]\n#         valid_x, valid_y = X.iloc[valid_idx], target.iloc[valid_idx]\n\n#         # LightGBM parameters found by Bayesian optimization\n#         clf = lgb.LGBMRegressor(\n#             nthread=4,\n#             n_estimators=10000,\n#             learning_rate=0.001,\n#             num_leaves=34,\n#             colsample_bytree=0.9497036,\n#             subsample=0.8715623,\n#             max_depth=8,\n#             reg_alpha=0.041545473,\n#             reg_lambda=0.0735294,\n#             min_split_gain=0.0222415,\n#             min_child_weight=39.3259775,\n#             silent=-1,\n#             verbose=-1)\n\n#         clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n#             eval_metric= 'rmse', verbose= 500, early_stopping_rounds= 200)\n\n#         oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)\n#         sub_preds += clf.predict(test_df, num_iteration=clf.best_iteration_) \/ folds.n_splits\n#         return sub_preds","9253bb5b":"# %%time\n# pred1 = kfold_lightgbm(y1)\n# pred2 = kfold_lightgbm(y2)\n# pred3 = kfold_lightgbm(y3)\n# pred4 = kfold_lightgbm(y4)\n# pred5 = kfold_lightgbm(y5)\n# pred6 = kfold_lightgbm(y6)","8957430f":"# # Appending all predictions\n# prediction = []\n# for i in range(len(pred1)):\n#     for j in [pred1,pred2,pred3,pred4,pred5,pred6]:\n#         prediction.append(j[i])\n        \n# sample_df[\"Target\"] = prediction\n# sample_df.to_csv(\"Submission_CB.csv\",index = False)","3c42491f":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","5ac28937":"df = h2o.import_file(\"\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv\")\nh2o_test = h2o.import_file(\"\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv\")","2148556f":"y1 = \"TotalTimeStopped_p20\"\ny2 = \"TotalTimeStopped_p50\"\ny3 = \"TotalTimeStopped_p80\"\ny4 = \"DistanceToFirstStop_p20\"\ny5 = \"DistanceToFirstStop_p50\"\ny6 = \"DistanceToFirstStop_p80\"","3e8bd9d0":"splits = df.split_frame(ratios = [0.8], seed = 1)\ntrain = splits[0]\ntest = splits[1]","bb151240":"aml_1 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_1.train(y = y1, training_frame = df)\nprint(aml_1.leader.model_performance(test))\naml_1.leaderboard.head()","306467a2":"aml_2 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_2.train(y = y2, training_frame = df)\nprint(aml_2.leader.model_performance(test))\naml_2.leaderboard.head()","6046fd5e":"aml_3 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_3.train(y = y3, training_frame = df)\nprint(aml_3.leader.model_performance(test))\naml_3.leaderboard.head()","03fa6fb5":"aml_4 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_4.train(y = y4, training_frame = df)\nprint(aml_4.leader.model_performance(test))\naml_4.leaderboard.head()","b1b8f3ee":"aml_5 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_5.train(y = y5, training_frame = df)\nprint(aml_5.leader.model_performance(test))\naml_5.leaderboard.head()","42269e1c":"aml_6 = H2OAutoML(max_runtime_secs = 300, seed = 1)\naml_6.train(y = y6, training_frame = df)\nprint(aml_6.leader.model_performance(test))\naml_6.leaderboard.head()","38f056d1":"y1 = train_df[\"TotalTimeStopped_p20\"]\ny2 = train_df[\"TotalTimeStopped_p50\"]\ny3 = train_df[\"TotalTimeStopped_p80\"]\ny4 = train_df[\"DistanceToFirstStop_p20\"]\ny5 = train_df[\"DistanceToFirstStop_p50\"]\ny6 = train_df[\"DistanceToFirstStop_p80\"]","cfcfe218":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\ny6_val = pd.DataFrame({\"y6\":train_df[\"DistanceToFirstStop_p80\"], \"log(y6 + 1)\":np.log1p(train_df[\"DistanceToFirstStop_p80\"])})\ny6_val.hist()","9f724e4a":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model,y_value):\n    rmse= np.sqrt(-cross_val_score(model, X, y_value, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","20ead8f2":"model_ridge = Ridge()","7cd86ffa":"alphas = [0.01, 0.05, 0.1, 0.3]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha),y1).mean() \n            for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","2037c392":"cv_ridge_2 = [rmse_cv(Ridge(alpha = alpha),y2).mean() \n            for alpha in alphas]\n\ncv_ridge_2 = pd.Series(cv_ridge_2, index = alphas)\ncv_ridge_2.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","43961293":"cv_ridge_3 = [rmse_cv(Ridge(alpha = alpha),y3).mean() \n            for alpha in alphas]\n\ncv_ridge_3 = pd.Series(cv_ridge_3, index = alphas)\ncv_ridge_3.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","c0987eb2":"cv_ridge_4 = [rmse_cv(Ridge(alpha = alpha),y4).mean() \n            for alpha in alphas]\n\ncv_ridge_4 = pd.Series(cv_ridge_4, index = alphas)\ncv_ridge_4.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","ac67ed59":"cv_ridge_5 = [rmse_cv(Ridge(alpha = alpha),y5).mean() \n            for alpha in alphas]\n\ncv_ridge_5 = pd.Series(cv_ridge_5, index = alphas)\ncv_ridge_5.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","200f507e":"y6 = np.log1p(train_df[\"DistanceToFirstStop_p80\"])\ncv_ridge_6 = [rmse_cv(Ridge(alpha = alpha),y6).mean() \n            for alpha in alphas]\n\ncv_ridge_6 = pd.Series(cv_ridge_6, index = alphas)\ncv_ridge_6.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","c8ff9d19":"To be continued\n\n**Kindly UPVOTE if you find it insightful. Will mean a lot to me!**","f0ba6547":"# Objective : \n\nWe\u2019ve all been there: Stuck at a traffic light, only to be given mere seconds to pass through an intersection, behind a parade of other commuters. Imagine if you could help city planners and governments anticipate traffic hot spots ahead of time and reduce the stop-and-go stress of millions of commuters like you.\n\nThe task here is to predict congestion, based on an aggregate measure of stopping distance and waiting times, at intersections in 4 major US cities: Atlanta, Boston, Chicago & Philadelphia.\n\n# About the data\n\nThe data consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks. The data have been grouped by :\n\n1. intersection\n2. month\n3. hour of day\n4. direction driven through the intersection\n5. whether the day was on a weekend or not\n\nFor each grouping in the test set, you need to make predictions for three different quantiles, that is, **20th, 50th, and 80th percentiles** for:\n\n1. The total time stopped at an intersection \n2. The distance between the intersection and the first place a vehicle stopped while waiting.","5b0d71f5":"Since Path column is just concatenation of texts from EntryStreetName_EntryHeading_ExitStreetName_ExitHeading it doesn't provide any significant info to the model so let's drop it.","c7a667a2":"From the above info, the categorical features in the data are : \n* EntryStreetName            \n* ExitStreetName             \n* EntryHeading               \n* ExitHeading \n* City\n* Path","7ee9c7dc":"# Trying Linear models with regularization","91cfa29b":"# Trying H2o AutoML and evaluating it's performance on our dataset"}}