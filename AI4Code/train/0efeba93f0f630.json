{"cell_type":{"aec35185":"code","8b9a716f":"code","1e5df1ac":"code","965fa7c5":"code","33c65de7":"code","3af47cc4":"code","1414446e":"code","5ce8624b":"code","d535eaa3":"markdown","6fa50377":"markdown","ec5fc870":"markdown","4135a39b":"markdown","65632207":"markdown","a90c4fe6":"markdown"},"source":{"aec35185":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\n","8b9a716f":"nRowsRead = 500 # prendo le prime 501 righe\n# winequality-red.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndata = pd.read_csv('\/kaggle\/input\/winequality-red.csv', delimiter=',', nrows = nRowsRead)\ndata.dataframeName = 'winequality-red.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","1e5df1ac":"data.head(10)","965fa7c5":"print(data.quality.unique())","33c65de7":"ranges = (2, 5, 6, 8) # da 3 a 5 bad, 6 average e da 7 a 8 good\nclasses_names = ['bad', 'average', 'good']\ndata['quality'] = pd.cut(data['quality'], bins = ranges, labels = classes_names)\n\nprint(data.quality.unique())","3af47cc4":"data.head(10)","1414446e":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size = 0.3) \n","5ce8624b":"from sklearn import tree\n\ndepths = [1, 3, 5, 8, 10, None]\nminsamples = [1, 5, 10, 20, 40, None]\n\nfor d in depths:\n    for m in minsamples:\n        t = tree.DecisionTreeClassifier(max_depth = d, min_samples_leaf = m)\n#        t.fit(train.drop('quality', axis = 1), train['quality']) ","d535eaa3":"Controllo quali valori pu\u00f2 assumere la y target\n","6fa50377":"#### Preparo i dati per il training","ec5fc870":"#### Operazioni preliminari sui dati \n\ndataset in: \/kaggle\/input\/winequality-red.csv","4135a39b":"Modifico i possibili valori target raggruppandoli in 3 classi: bad, average, good\n","65632207":"#### Creo l'albero di decisione\n\nNota: il metodo *DecistionTreeClassifier* offerto da scikit mette a disposizione alcuni parametri da settare in maniera appropriata per evitare overfitting, tramite il controllo della crescita dell'albero di decisione.\nIn particolare mostrer\u00f2 come cambia l'accuratezza al variare di *min_samples_leaf* e *max_depth*.","a90c4fe6":"# Decision tree in sklearn - analisi parametri (min_samples_leaf and max_depth )\nAttraverso questo kernel far\u00f2 il mio primo approccio agli alberi di decisione in sklearn; in particolare, voglio vedere come i parametri *min_samples_leaf* e *max_depth* influiscano sull'accuratezza e sul problema dell'*overfitting*, confrontando poi anche con la tecnica del **reduced error pruning**."}}