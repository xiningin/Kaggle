{"cell_type":{"952f7515":"code","2a2267d0":"code","10043535":"code","ba04a06d":"code","bab1726f":"code","77065472":"code","15a88bd6":"code","b067781d":"code","f2575a8f":"code","856c9eb6":"code","0ca2b237":"code","3f83f041":"code","157b650a":"code","29185f40":"code","3230a4ef":"code","2e6bb998":"code","79cf95e7":"code","b9e2570c":"code","da1323c7":"code","6262bdd2":"code","59be4cdb":"code","98814d7e":"code","49030646":"code","00b7b971":"markdown","2498f447":"markdown","0bb74027":"markdown","b96c03e0":"markdown","e4f98f37":"markdown","9316467d":"markdown","662b27f0":"markdown","e39209bf":"markdown","e6aa3639":"markdown","1ec71072":"markdown","91bf3cdc":"markdown","4eac5d6f":"markdown","02d189bb":"markdown","0f323d71":"markdown","92377c5d":"markdown"},"source":{"952f7515":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action='ignore')\nplt.rcParams['figure.dpi'] = 200 #high resolution\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport optuna\nfrom math import sqrt","2a2267d0":"#Let's read data\ntrain=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\nsub=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")","10043535":"print(\"Shape of train data:\",train.shape)\nprint(\"Number of columns in train data:\",train.shape[1])\nprint(\"Number of rows in train data:\",train.shape[0])\ntrain.head()","ba04a06d":"print(\"Shape of test data:\",test.shape)\nprint(\"Number of columns in test data:\",test.shape[1])\nprint(\"Number of rows in test data:\",test.shape[0])\ntest.head()","bab1726f":"print(\"Shape of sub data:\",sub.shape)\nprint(\"Number of columns in sub data:\",sub.shape[1])\nprint(\"Number of rows in sub data:\",sub.shape[0])\nsub.head()","77065472":"train.info()","15a88bd6":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(22, 10))\nsns.distplot(train['target'],color=\"darkred\",kde=True,bins=120,label='target',ax=ax[0])\nsns.boxplot(train['target'],color=\"darkred\",ax=ax[1])\nplt.legend();\nplt.show()","b067781d":"train['target'].describe()\nprint(round(train['target'].median(),1))\ntrain = train[train['target'] != 0].reset_index(drop=True)","f2575a8f":"#let we analyze our all the features and see they have bimdal nature or unimodal nature\n#Feature Analysis\nfeatures=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7','cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor i,col in enumerate(features):\n    plt.subplot(5,3,i+1)\n    sns.distplot(train.loc[:,col], kde =True,color=\"darkred\")\nfig.tight_layout(pad=1.0)","856c9eb6":"\nfeatures=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7','cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor i,col in enumerate(features):\n    plt.subplot(5,3,i+1)\n    sns.distplot(test.loc[:,col], kde =True,color=\"darkred\")\nfig.tight_layout(pad=1.0)","0ca2b237":"for i in range(1,15):\n    sns.jointplot(x='cont'+str(i),y='target',data=train,kind='reg',color='crimson')","3f83f041":"%%time\ntrain_features=train.drop(['id'],axis=1)\ncorr=train_features.corr()","157b650a":"#sns.set_theme(style=\"white\")\ncorr=train_features.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 7))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","29185f40":"X=train.drop(['id','target'],axis=1)\ny=train['target']","3230a4ef":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","2e6bb998":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nimport warnings\nwarnings.simplefilter('ignore')","79cf95e7":"def rmse_score(yreal, yhat):\n    return sqrt(mean_squared_error(yreal, yhat))    ","b9e2570c":"#standardizing the data\ntrain_std = StandardScaler().fit_transform(X_train)\nval_std = StandardScaler().fit_transform(X_val)\n    \n#hyper-paramater tuning\nclf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\nvalues = [10**-14, 10**-12, 10**-10, 10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6]\nhyper_parameter = {\"alpha\": values}\nbest_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\nbest_parameter.fit(train_std, y_train)\nalpha = best_parameter.best_params_[\"alpha\"]\n    \n#applying linear regression with best hyper-parameter\nclf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\nclf.fit(train_std, y_train)\ntrain_pred = clf.predict(train_std)\ntrain_RMSE_lr = rmse_score(y_train, train_pred)\nval_pred = clf.predict(val_std)\nval_RMSE_lr = rmse_score(y_val, val_pred)\n\nprint(\"linear_regression_score:\",train_RMSE_lr,val_RMSE_lr)","da1323c7":"import xgboost as xgb\n#hyper-parameter tuning\nhyper_parameter = {\"max_depth\":[ 1,3,2, 4], \"n_estimators\":[40, 80, 150, 400]}\nclf_Xg = xgb.XGBRegressor()\nbest_parameter = GridSearchCV(clf_Xg, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\nbest_parameter.fit(X_train, y_train)\nestimators = best_parameter.best_params_[\"n_estimators\"]\ndepth = best_parameter.best_params_[\"max_depth\"]\n\n#applying xgboost regressor with best hyper-parameter\nclf_Xg = xgb.XGBRegressor(max_depth = depth, n_estimators = estimators)\nclf_Xg.fit(X_train, y_train)\ntrain_pred = clf_Xg.predict(X_train)\ntrain_RMSE_Xg = rmse_score(y_train, train_pred)\nval_pred = clf_Xg.predict(X_val)\nval_RMSE_Xg = rmse_score(y_val, val_pred)\n\nprint(\"XG_regression_score:\",train_RMSE_Xg,val_RMSE_Xg)","6262bdd2":"error_table_regressions = pd.DataFrame(columns = [\"Model\",\"TrainRMSE\",\"ValRMSE\"])\n\nerror_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Linear Regression\", train_RMSE_lr,val_RMSE_lr ]], columns = [\"Model\",\"TrainRMSE\",\"ValRMSE\"]))\n\nerror_table_regressions = error_table_regressions.append(pd.DataFrame([[\"XGBoost Regressor\",train_RMSE_Xg,val_RMSE_Xg]], columns = [\"Model\",\"TrainRMSE\",\"ValRMSE\"]))\nerror_table_regressions.reset_index(drop = True, inplace = True)","59be4cdb":"error_table_regressions","98814d7e":"test=test.drop(['id'],axis=1)\ny_test=clf_Xg.predict(test)","49030646":"sub = pd.DataFrame({\n        \"id\": sub[\"id\"],\n        \"target\":y_test\n    })\nsub.to_csv('Xgb_submission.csv', index=False)","00b7b971":"# Work in Progress:","2498f447":"1. In train data we have 16 columns in which one is target column.\n2. All the columns except id are in continous manner.\n3. We have all 14 columns which we going to analyze further but we not have their proper names.","0bb74027":"1. This our test data so obvious we not have any target columns.\n2. Here also all the columns are continous data. ","b96c03e0":"1. All the features are follow mulimodal distribution having more than two peaks,and already we know our target feature also follow bimodal distribution.","e4f98f37":"1. This is our submission data after complete our modeling task our submission file should be look similar file as above which having two columns one is id and other is target. ","9316467d":"1. Our target is kind of bimodal in nature.\n2. Target column has one oulier that is 0.\n3. We can break bimodal distribution target column to univariate at target median which is near about 8\nBimodal: A probability distribution with two different modes, which may also be referred to as a bimodal distribution.","662b27f0":"# Features VS target Distribution:\nNow we are going to see how all the 14 features are behaves with respect to target ","e39209bf":"1. From all the above 14 continous features no one is highly correlated with target feature.\n2. Multicollinearity is exist in this dataset.","e6aa3639":"![image.png](attachment:image.png)","1ec71072":"**Let's start the Work**\n1. So let me gives you idea what I am going to do in this notebook below.\n* **Step1**: We do Exploratory data analysis for checking null values,relation between the columns and how the columns behave with target column with giving           brief idea about statistical analysis.\n* **Step2**: Whatever we do analysis next step we present that analysis with help of some cool grpahs so we can unserstand better.\n* **Step3**: During this we also check correlation matrix and if some outliers are there we try to remove in case if we use model except trees model,Outliers           can impact our model.\n* **Step4**: We will come up with feature enginerring in this step.\n* **Step5**: We developed one basline model.\n   ","91bf3cdc":"1. Test data also contains multimodal distribution features","4eac5d6f":"# Let's Do Modeling:\nStill we not do any kind of feature engineering.First we try without feature enginerring and check performance of model","02d189bb":"# Linear Regression with Hyperparameter-Tuning:\n1. I performed first standardization to make data standard in one scale\n2. Used squred_loss(due to OLS) with l2 penality for regularization.\n3. Used GridSearch for finding best hyperparameter.","0f323d71":"1. There is no missing values in our data set.RELAX!","92377c5d":"# XGBoost Regressor with Hyperparameter tuning"}}