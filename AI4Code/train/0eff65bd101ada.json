{"cell_type":{"dfe6673b":"code","2b264bf5":"code","c1da762f":"code","938a3e37":"code","d39d8831":"code","562153c4":"code","923abe58":"code","eb23e16a":"code","472a8d9a":"code","dd1d66a7":"code","0959e1af":"code","1900de5d":"markdown","8f2497b1":"markdown","fb97a502":"markdown","f07293bd":"markdown","15152ee7":"markdown","23136262":"markdown","7c3ee589":"markdown","d111081c":"markdown","020008d9":"markdown","d7d1e4b8":"markdown"},"source":{"dfe6673b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b264bf5":"import time\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset, DataLoader","c1da762f":"epochs = 50\nbatch_size = 32\n\nlr = 1e-3\nmomentum = 0.9\nweight_decay = 1e-4\n\ntrain_ratio = 0.8\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","938a3e37":"# read from csv\ntrain_df = np.array(pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv'))\nimgs = train_df[:,1:].reshape(-1, 28, 28)\nlabels = train_df[:,0]\n\n# totensor, normalization\nimgs = np.expand_dims(imgs, axis=1) \/ 255.0\n\n# split train and validation\nnum_train = int(imgs.shape[0] * train_ratio)\ntrain_imgs = imgs[:num_train]\ntrain_labels = labels[:num_train]\nval_imgs = imgs[num_train:]\nval_labels = labels[num_train:]\n\n# dataloader\ntrain_dataset = TensorDataset(torch.tensor(train_imgs, dtype=torch.float), torch.tensor(train_labels))\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\nval_dataset = TensorDataset(torch.tensor(val_imgs, dtype=torch.float), torch.tensor(val_labels))\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","d39d8831":"# a basic convolution-batchnorm-activation block\ndef conv_cell(cin, cout, k, s, p=None):\n    if not p:\n        p = (k-1) \/\/ 2\n    cell = nn.Sequential()\n    cell.add_module('conv', nn.Conv2d(in_channels=cin, out_channels=cout, kernel_size=k, stride=s, padding=p, bias=False))\n    cell.add_module('bn', nn.BatchNorm2d(cout))\n    cell.add_module('act', nn.ReLU())\n    return cell\n\n# define the network\nnet = nn.Sequential(\n    conv_cell(1, 32, 3, 2), # 14x14\n    conv_cell(32, 32, 3, 1),\n    conv_cell(32, 32, 3, 1),\n    conv_cell(32, 64, 3, 2), # 7x7\n    conv_cell(64, 64, 3, 1),\n    conv_cell(64, 64, 3, 1),\n    nn.AdaptiveAvgPool2d(1), # 1x1\n    nn.Flatten(),\n    nn.Linear(64, 10)\n)\nprint(net)\n\nnet = net.to(device)","562153c4":"# loss\nloss_fn = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)","923abe58":"def accuracy(dataloader, net):\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, labels = data[0].to(device), data[1].to(device)\n            outputs = net(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return correct\/total","eb23e16a":"best = 0\ntrain_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(epochs):\n    tic = time.time()\n    epoch_loss = 0\n    \n    for i, data in enumerate(train_loader):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        \n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # For every 100 batches, print the accumulated average loss.\n        epoch_loss += loss.item()\n        if i % 100 == 99:\n            print(\"[Epoch {}\/{} Batch {}] lr: {}, loss: {:.4f}\".format(\n                  epoch+1, epochs, i+1, optimizer.param_groups[0]['lr'], epoch_loss\/(i+1)))\n    \n    # Let's see how our model performs after each epoch.\n    toc = time.time()\n    train_acc = accuracy(train_loader, net)\n    val_acc = accuracy(val_loader, net)\n    print(\"Epoch {}, time: {:.2f}s, train accuracy: {:.4f}, val accuracy: {:.4f}\".format(\n          epoch+1, toc-tic, train_acc, val_acc))\n    print('-'*30)\n    \n    # get losses and accuracies for plotting and analysis\n    train_losses.append(epoch_loss\/(i+1))\n    train_accuracies.append(train_acc)\n    val_accuracies.append(val_acc)\n    \n    # save the best and the last model\n    if val_acc > best:\n        best = val_acc\n        torch.save(net.state_dict(), 'mnist_best.pt')\n    torch.save(net.state_dict(), 'mnist_last.pt')\n    \nprint(\"Training complete. Best validation accuracy:\", best)","472a8d9a":"fig = plt.figure()\nax = fig.add_subplot()\nax.plot(range(epochs), train_losses, 'blue', label='train loss')\nax.legend(loc='upper left')\nax.set_xlabel('epochs')\nax2 = ax.twinx()\nax2.plot(range(epochs), train_accuracies, 'orange', label='train acc')\nax2.plot(range(epochs), val_accuracies, 'red', label='val acc')\nax2.legend(loc='upper right')\nax2.set_ylim(0.9, 1)\nplt.show()","dd1d66a7":"# read test data\ntest_df = np.array(pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv'))\ntest_imgs = test_df.reshape(-1, 28, 28)\ntest_imgs = np.expand_dims(test_imgs, axis=1) \/ 255.0\n\ntest_dataset = TensorDataset(torch.tensor(test_imgs, dtype=torch.float))\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# load the best model and do prediction\nnet.load_state_dict(torch.load('mnist_best.pt'))\nnet.eval()\n\ntest_results = []\nfor data in test_loader:\n    inputs = data[0].to(device)\n    outputs = net(inputs)\n    _, predicted = torch.max(outputs.data, axis=1)\n    test_results.extend(predicted.tolist())","0959e1af":"num_test = len(test_results)\ntest_df = pd.DataFrame(np.vstack((np.arange(1, num_test+1), np.array(test_results))).T, columns=['ImageId', 'Label'])\ntest_df.to_csv('submission.csv', index=False)","1900de5d":"Define some hyper parameters and the device.","8f2497b1":"Define the metric for this task which is accuracy.","fb97a502":"Read data from csv, convert it to torch Tensor and split it to train and validation sets. Then use TensorDataset to make it iterable and DataLoader for training.","f07293bd":"Plot losses and accuracies.","15152ee7":"Write the results to csv and submit!","23136262":"Define the loss function and optimizer. Let's use CrossEntropyLoss for this multi-classification task and stochastic gradient descent for parameter updating.","7c3ee589":"Design a simple convolution neural network with typical convolution-batchnorm-activation blocks, followed by average pooling and fully connected layers.","d111081c":"Now we can train our neural network! (It will take ~5 minutes with GPU)","020008d9":"First, let's import some necessary packages.","d7d1e4b8":"Now use our best model to do prediction on test set."}}