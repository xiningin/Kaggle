{"cell_type":{"39f678c8":"code","9c1001e3":"code","f3b703b6":"code","abd0398c":"code","e6d54959":"code","9a0b555f":"code","96e01775":"code","95b85b30":"code","642310c9":"code","f9709260":"code","b52e16b0":"code","465bcd23":"code","932c2af4":"code","7200ce4b":"code","43bdb49b":"code","c830932b":"code","c67ef738":"code","6acd88bf":"code","c94dd2e9":"code","f346b48d":"code","c6871183":"code","b436ec78":"code","91904a39":"code","c350be6a":"code","bb7e3bd7":"code","8b50ce8a":"code","a566a955":"code","4fe4b051":"code","978c7c0b":"code","e6d9eb66":"code","e0f4b014":"code","d334dcc7":"code","95296667":"code","2fcd8a68":"code","761a8686":"code","2649b6a7":"code","2393ce7e":"markdown","95c13238":"markdown","7fbab182":"markdown","f6a76091":"markdown","a5961988":"markdown","f990ac0e":"markdown","e225896f":"markdown","e48823e0":"markdown","c752dc44":"markdown","9e5633c0":"markdown","25e3297b":"markdown","0e827f66":"markdown","414a259c":"markdown","fa48ea5a":"markdown","858a3117":"markdown","43a7e14f":"markdown","49b123f6":"markdown","ffd5de2d":"markdown","7f7c3e5a":"markdown","99516a9f":"markdown","d21f64ef":"markdown","538d7559":"markdown","e5c501d6":"markdown","dfaa1b8d":"markdown","1a176994":"markdown","8bcc3957":"markdown","8180e9b2":"markdown","05b56db4":"markdown","a627b525":"markdown","4e3577c3":"markdown","af63e566":"markdown","3809a65d":"markdown","6b74a8fa":"markdown","43cb779e":"markdown","a10f2aa9":"markdown","ab5d0f73":"markdown","0fa7149e":"markdown","1600f9c8":"markdown","1498a9a9":"markdown","400c154c":"markdown","09bc41d9":"markdown","9ff4092d":"markdown","91a4f7a2":"markdown","2fed0db9":"markdown","c7094652":"markdown","28a1e63c":"markdown","8739b1a0":"markdown","788827cf":"markdown"},"source":{"39f678c8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.models import Model","9c1001e3":"train = pd.read_csv('\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/train.csv')\nprint(train.shape)","f3b703b6":"train.head()","abd0398c":"train['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48) for x in train['pixels']]","e6d54959":"pixels = np.concatenate(train['pixels'])\nlabels = train.emotion.values\n\nprint(pixels.shape)\nprint(labels.shape)","9a0b555f":"emotion_prop = (train.emotion.value_counts() \/ len(train)).to_frame().sort_index(ascending=True)\n\nemotion_prop","96e01775":"emotions = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']","95b85b30":"palette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotions, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Emotion Label Proportions')\nplt.show()","642310c9":"plt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nrow = 0\nfor emotion in np.unique(labels):\n\n    all_emotion_images = train[train['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels.reshape(48,48)\n        lab = emotions[emotion]\n\n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.text(-30, 5, s = str(lab), fontsize=10, color='b')\n        plt.axis('off')\n    row += 5\n\nplt.show()","f9709260":"X_train, X_valid, y_train, y_valid = train_test_split(\n    pixels, labels, test_size=0.2, stratify=labels, random_state=1\n)\n\n\nprint('X_train Shape:', X_train.shape)\nprint('y_train Shape:', y_train.shape)\nprint()\nprint('X_valid Shape:', X_valid.shape)\nprint('y_valid Shape:', y_valid.shape)","b52e16b0":"rgb_X_train = np.repeat(X_train[..., np.newaxis], 3, -1)\nprint(rgb_X_train.shape)\n\nrgb_X_valid = np.repeat(X_valid[..., np.newaxis], 3, -1)\nprint(rgb_X_valid.shape)","465bcd23":"train_datagen = ImageDataGenerator(\n    rotation_range = 40,\n    width_shift_range = 0.3, \n    height_shift_range = 0.3, \n    zoom_range = 0.3, \n    horizontal_flip = True, \n    fill_mode = 'reflect'\n)\n\ntrain_loader = train_datagen.flow(rgb_X_train, y_train, batch_size=64)","932c2af4":"resnet_model = tf.keras.applications.resnet50.ResNet50(\n    include_top=False, weights='imagenet', input_shape=(48,48,3))\n\nresnet_model.trainable = False","7200ce4b":"resnet_model.summary()","43bdb49b":"tf.keras.utils.plot_model(resnet_model, show_shapes=True)","c830932b":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    resnet_model,\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.7),\n    \n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.7),\n    \n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Dense(7, activation='softmax')\n])\n\ncnn.summary()\n","c67ef738":"opt = tf.keras.optimizers.Adam(0.0001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","6acd88bf":"%%time \n\nh1 = cnn.fit(\n    train_loader, \n    batch_size=32,\n    epochs = 30,\n    verbose = 1,\n    validation_data = (rgb_X_valid, y_valid)\n)","c94dd2e9":"history = h1.history\nprint(history.keys())","f346b48d":"epoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","c6871183":"resnet_model.trainable = True\ntf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.00001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","b436ec78":"cnn.summary()","91904a39":"%%time \n\nh2 = cnn.fit(\n    train_loader, \n    batch_size=32,\n    epochs = 30,\n    verbose = 1,\n    validation_data = (rgb_X_valid, y_valid)\n)","c350be6a":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","bb7e3bd7":"resnet_model = tf.keras.applications.resnet50.ResNet50(\n    include_top=False, weights='imagenet', input_shape=(48,48,3))\n\nresnet_model.summary()","8b50ce8a":"print('Number of layers in base model:', len(resnet_model.layers), '\\n')\n\nprint('Names of last ten layers:')\nfor layer in resnet_model.layers[-10:]:\n    print(layer.name)","a566a955":"resnet_model.trainable = True\n\nfor layer in resnet_model.layers[:-10]:\n    layer.trainable = False","4fe4b051":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    resnet_model,\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.7),\n    \n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.7),\n    \n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Dense(7, activation='softmax')\n])\n\ncnn.summary()","978c7c0b":"opt = tf.keras.optimizers.Adam(0.0001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","e6d9eb66":"%%time \n\nh1 = cnn.fit(\n    train_loader, \n    batch_size=32,\n    epochs = 30,\n    verbose = 1,\n    validation_data = (rgb_X_valid, y_valid)\n)","e0f4b014":"history = h1.history\nprint(history.keys())","d334dcc7":"epoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","95296667":"tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.00001)","2fcd8a68":"%%time \n\nh2 = cnn.fit(\n    train_loader, \n    batch_size=32,\n    epochs = 30,\n    verbose = 1,\n    validation_data = (rgb_X_valid, y_valid)\n)","761a8686":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","2649b6a7":"cnn.save('fer_model_v06.h5')\npickle.dump(history, open(f'fer_v06.pkl', 'wb'))","2393ce7e":"## Save Model and History","95c13238":"## Summary","7fbab182":"We import all necessary packages.","f6a76091":"Overall, the ResNet50 did not perform well on the facial expression recognition data. The top accuracy was ~44%. This dataset may benefit from another transfer learning model or simple architecture with the addition of image augmentation. ","a5961988":"[A Comparison of 4 Popular Transfer Learning Models](https:\/\/analyticsindiamag.com\/a-comparison-of-4-popular-transfer-learning-models\/)<br\/>\n[Facial Expression Detection 2](https:\/\/www.kaggle.com\/haneenabdelmaguid\/facial-expression-detection-2)<br\/>\n[How can I use a pre-trained neural network with grayscale images?](https:\/\/stackoverflow.com\/questions\/51995977\/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images)<br\/>\n[Transfer learning & fine-tuning](https:\/\/keras.io\/guides\/transfer_learning\/#do-a-round-of-finetuning-of-the-entire-model)<br\/>\n[Transfer Learning Tutorial (CIFAR 10)](https:\/\/www.kaggle.com\/drbeane\/transfer-learning-tutorial-cifar-10)","f990ac0e":"ResNet50 was trained with RGB images, and our data is in grayscale. In order to use the pretrained weights of the ResNet50 model we need to convert the single grayscale channel of our images into 3 channels (RGB).","e225896f":"From the model we can see the accuracy improved from ~35% to ~37%; however, there is still underfitting present. ","e48823e0":"## Transfer Learning with ResNet50","c752dc44":"We load the training data and view the first 5 rows.","9e5633c0":"## Label Distribution","25e3297b":"In an effort to enhance the model's performance, we will retrain the top convolutional layers of the ResNet50 model. ","0e827f66":"From the learning curves we can see there is slight overfitting, and the model could benefit from additional epochs.","414a259c":"## Configure Model","fa48ea5a":"In an effort to enhance our model's performance, we will allow the ResNet50 model to retrain.","858a3117":"### Training Run 2","43a7e14f":"We train the model for 30 epochs. ","49b123f6":"From the learning curves we can see retraining the model increase the accuracy from ~30% to ~44%; however, there is still unerfitting present. ","ffd5de2d":"We train for another 30 epochs for the second training run.","7f7c3e5a":"To the ResNet50 model we add densely-connected layers, incorporating dropout and batch normalization.","99516a9f":"We load the pretrained ResNet50 model and set trainable to false. ","d21f64ef":"We save the model and training history for future reference.","538d7559":"## Split, Reshape, and Scale Datasets","e5c501d6":"## Import Packages","dfaa1b8d":"We train the model using the Adam optimizer, a learning rate of 0.0001, and sparse categorical crossentropy loss.","1a176994":"## Train Final Convolutional Layers","8bcc3957":"## Resources","8180e9b2":"## Train Model","05b56db4":"As we can see from the distribution of labels, there is a class imbalance within this training set: the emotion happy accounts for about 25% of the data.","a627b525":"We split the data into training and validation sets using a stratified fashion.","4e3577c3":"## View Sample of Images","af63e566":"From the learning curves we can see there is slight overfitting, and the model could benefit from additional epochs.","3809a65d":"## Load Training DataFrame","6b74a8fa":"We view a sample of images for each emotion: angry, disgust, fear, happy, sad, surprise, and neutral.","43cb779e":"In this notebook we will explore the effects of the pretrained model ResNet50 on the facial expression recognition data. ResNet50 is a transfer learning model that was trained on ImageNet, a large dataset of annotated photographs. The benefits of ResNet50 include accelerated training and help with the vanishing gradient problem.","a10f2aa9":"Let's view the distribution of labels.","ab5d0f73":"## Fine-Tune Model","0fa7149e":"In an effort to prevent overfitting, we use image augmentation to create additional training observations.","1600f9c8":"In order to enhance the performance of the model, we will increase the learning rate to 0.00001.","1498a9a9":"The images for this training set are stored as a string. In order to train the model and visualize the images we need to process these strings into a 4D array of pixel values.","400c154c":"## Preprocess Data","09bc41d9":"## Image Augmentation","9ff4092d":"### Training Run 1","91a4f7a2":"To the ResNet50 model we add densely-connected layers, incorporating dropout and batch normalization.","2fed0db9":"We train the model using the Adam optimizer, a learning rate of 0.0001, and sparse categorical crossentropy loss.","c7094652":"We train the model for 30 epochs. ","28a1e63c":"We train for 30 epochs for the first training run.","8739b1a0":"We view the model summary and plot the model's architecture. ","788827cf":"# **Facial Expression Recognition Training Notebook**\n## **Week 6**\n### Sara Manrriquez"}}