{"cell_type":{"b27e4e0b":"code","c7ad1db6":"code","0777f7d3":"code","86195542":"code","b8e15a7e":"code","57dfff66":"code","0044487f":"code","645cc803":"code","cbd51efb":"code","10b6d689":"code","604c5c05":"code","73f54123":"code","9db87906":"code","4ab69e67":"code","200a8d9d":"code","aeb48f66":"code","c87b55df":"code","63445db5":"code","1826fa67":"code","64da6942":"code","f2534fd3":"code","c1c7710f":"code","0f7a0e03":"code","e67434bf":"code","ed8efee4":"code","9fdc91f5":"code","fffaa83b":"code","7c230c6a":"code","ec4da705":"code","17088a82":"code","81278283":"markdown","fa63e29d":"markdown","ffa55cbe":"markdown","43bd5a94":"markdown","a41df298":"markdown","c959b204":"markdown","6e5f62e7":"markdown","e78d3f16":"markdown","b7e4341b":"markdown","43e051fd":"markdown","ff1bee26":"markdown","bbdf8b9b":"markdown","a0d83ed8":"markdown","a663fccb":"markdown","81fd1ede":"markdown","4ca864d8":"markdown","b9c8e1d7":"markdown","d86ebfdb":"markdown","71ce0448":"markdown","1a8d30fb":"markdown","6ee55d4b":"markdown","e4d870e8":"markdown","99d59cac":"markdown","d220bbc4":"markdown","4249f599":"markdown","e5cb3fb6":"markdown","60b667b6":"markdown","2eb9d357":"markdown","d5e766b1":"markdown"},"source":{"b27e4e0b":"import numpy as np # for linear algebra\nimport pandas as pd # for dealing data files\n%matplotlib inline\nimport matplotlib.pyplot as plt # for plotting\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c7ad1db6":"data = pd.read_csv(\"\/kaggle\/input\/car-advertisement\/Car_Advertisement.csv\");\nm = len(data) # 400\ndata[['Age', 'EstimatedSalary']] = data[['Age', 'EstimatedSalary']].astype(float)\n# Need to explicitly convert \"numerical\" entries to \"float\" type","0777f7d3":"# scikit learn\nfrom sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(data, test_size=0.2)\ntrain_data.head()","86195542":"train_data.info()","b8e15a7e":"train_data.describe()","57dfff66":"train_data[\"Gender\"].value_counts()","0044487f":"train_data[\"Purchased\"].value_counts()","645cc803":"Y_train = train_data[\"Purchased\"].copy()\nX_train = train_data.drop([\"Purchased\", \"UserID\"], axis=1) \nm_train = len(X_train)\nX_train.head()","cbd51efb":"Y_train.head()","10b6d689":"def plot_scatter_with_labels(X1, X2, Y, xlabel=\"Age\", ylabel=\"Estimated Salary\"): \n    df = pd.DataFrame(dict(x1=X1, x2=X2, label=Y))\n    groups = df.groupby(\"label\")\n    for name, group in groups:\n        plt.plot(group.x1, group.x2, \n                 marker=\"o\", linestyle=\"\", ms=3,\n                 label=name)\n    plt.legend()\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n\nplot_scatter_with_labels(train_data[\"Age\"],\n                         train_data[\"EstimatedSalary\"], \n                         train_data[\"Purchased\"])","604c5c05":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nGender_encoded = ordinal_encoder.fit_transform(X_train[[\"Gender\"]])\nGender_encoded[:5]","73f54123":"ordinal_encoder.categories_","9db87906":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train[[\"Age\", \"EstimatedSalary\"]])\n\nX_train[:3, :]","4ab69e67":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nda_clf = LinearDiscriminantAnalysis()\nda_clf.fit(X_train, Y_train)","200a8d9d":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(da_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\nscores","aeb48f66":"np.average(scores) # averaged cv scores of 10 cross validations","c87b55df":"avg_cv_scores = {} # dictionary to store all avg scores\navg_cv_scores[\"DA\"] = np.average(scores)","63445db5":"def plot_decision_boundary(clf, axes, inverse=False):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_pred = y_pred ==\"Purchased\"\n    if inverse:\n        X = scaler.inverse_transform(np.c_[x0s, x1s])\n        x0, x1 = np.meshgrid(X[:,0], X[:,1])\n        \n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n\ndef plot_prediction_results(clf, X, Y):\n    plot_decision_boundary(clf, [-3, 3, -3, 3])\n    plot_scatter_with_labels(X[:,0], X[:,1], Y, \n                             xlabel=\"Scaled Age\", ylabel=\"Scaled Salary\")\n\nplot_prediction_results(da_clf, X_train, Y_train)","1826fa67":"from sklearn.naive_bayes import GaussianNB\nnb_clf = GaussianNB()\nnb_clf.fit(X_train, Y_train)\nscores = cross_val_score(nb_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"NB\"] = np.average(scores)\nplot_prediction_results(nb_clf, X_train, Y_train)","64da6942":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=3) # change n_neighbors, see what happens!\nknn_clf.fit(X_train, Y_train)\nscores = cross_val_score(knn_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"KNN3\"] = np.average(scores)\nprint(avg_cv_scores[\"KNN3\"] )\nplot_prediction_results(knn_clf, X_train, Y_train)","f2534fd3":"from sklearn.svm import SVC\nsvmln_clf = SVC(kernel=\"linear\", C=1)\nsvmln_clf.fit(X_train, Y_train)\nscores = cross_val_score(svmln_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"SVM-linear\"] = np.average(scores)\nplot_prediction_results(svmln_clf, X_train, Y_train)","c1c7710f":"svmpoly_clf = SVC(kernel=\"poly\", gamma='auto') # Play with degree and C\nsvmpoly_clf.fit(X_train, Y_train)\nscores = cross_val_score(svmpoly_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"SVM-poly\"] = np.average(scores)\nprint(avg_cv_scores[\"SVM-poly\"] )\nplot_prediction_results(svmpoly_clf, X_train, Y_train)","0f7a0e03":"#Gaussian Radial Basis Function (RBF) Kernel\nsvmgauss_clf = SVC(kernel=\"rbf\", gamma=1, C=1) # Play with hyperparameters\nsvmgauss_clf.fit(X_train, Y_train)\nscores = cross_val_score(svmgauss_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"SVM-gauss\"] = np.average(scores)\nprint(avg_cv_scores[\"SVM-gauss\"] )\nplot_prediction_results(svmgauss_clf, X_train, Y_train)","e67434bf":"from sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth=3) # Play with hyperparameter\ntree_clf.fit(X_train, Y_train)\nscores = cross_val_score(tree_clf, X_train, Y_train,\n                        scoring=\"accuracy\", cv=10)\navg_cv_scores[\"D Tree\"] = np.average(scores)\nprint(avg_cv_scores[\"D Tree\"] )\nplot_prediction_results(tree_clf, X_train, Y_train)","ed8efee4":"avg_cv_scores","9fdc91f5":"# Note you need to use \"transform\" not \"fit_transform\". \n# This will standardize the test_data using the mean & std of the train_data.\nX_test = scaler.transform(test_data[[\"Age\", \"EstimatedSalary\"]])\nY_test_pred = svmgauss_clf.predict(X_test)\naccuracy = np.sum(Y_test_pred==test_data[\"Purchased\"])\/len(Y_test_pred)\nprint(\"Accuracy on test set: \",accuracy)","fffaa83b":"plot_scatter_with_labels(data[\"Age\"],\n                         data[\"EstimatedSalary\"], \n                         data[\"Purchased\"])\nplot_decision_boundary(svmgauss_clf, [-3, 3, -3, 3], inverse=True)\nplt.axis([15, 65, 0, 160000])","7c230c6a":"from sklearn.pipeline import Pipeline\n\nfull_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n    ('svm_gauss_clf', SVC(kernel=\"rbf\", gamma=1, C=1)),\n])\n\nfull_pipeline.fit(data[[\"Age\", \"EstimatedSalary\"]], data[\"Purchased\"])","ec4da705":"X_new_customers = pd.DataFrame({'Age':  [27., 55.], 'EstimatedSalary': [24000., 60000.]})\nX_new_customers","17088a82":"full_pipeline.predict(X_new_customers)","81278283":"### Model 4: SVM-Linear ","fa63e29d":"### Model 3: KNN\n`n_neighbors` is a hyperparameter that you can tune to get a better result.\n\nLower n_neighbors = more DOF = more likely to overfit","ffa55cbe":"This `full_pipeline` is the final model that we are going to launch. Not let's make some predictions using this model for some future cutomers (The first one is me):","43bd5a94":"### Train-Test Split ","a41df298":"### Feature Selection\nAs I have explained in the MATLAB LiveScript, we decide not to include \"Gender\" as input features.","c959b204":"## Building Pipelines\nIn sklearn, you can build a \"Pipeline\", to do some process sequencially.\n\nBelow we are building a pipeline using our best choice of classifiers, and then train it on all the data we have:","6e5f62e7":"### Visualization: Decision Boundary","e78d3f16":"### Check Performace on Test Set","b7e4341b":"# Tutorial 8. Machine Learning - Python Version\nAthena Liu\n\nApril 1, 2020","43e051fd":"### Visualize the data ","ff1bee26":"The encoder automatically set Female = 0, Male = 1","bbdf8b9b":"## Step 5: Try out Different ML Models\n### Model 1: Discriminant Analysis Classifier\n","a0d83ed8":"And report that our classifier can predict roughly 90% of the customers if they will purchase or not correctly based on their salary and age.\nAlso we can report that Gender is not a critical factor in the prediction and the company does not need to collect that information in the future.","a663fccb":"### Model 6 (Optional): Decision Tree","81fd1ede":"# More Learning Resources\n\nIf you want to learn more about practical machine learning in Python, you should checkout Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow by Aur\u00e9lien G\u00e9ron: https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/ . \n\nBe sure to work through Chapter 2, where you will do an end-to-end ML project on a Regression problem. Actually, I based my tutorial on that chapter! You will defintely learn more by reading the book.\n\nUBC students can read the online version for free. The author also uploaded all his Notebooks to Github, open sourced for everyone to download and learn.","4ca864d8":"Summary of each numerical attribute:","b9c8e1d7":"## Step 7: Present Your Solution & Launch","d86ebfdb":"### Feature Scaling\nFirst, we standardize all the data using their mean and standard deviation.\n$$ \\hat{x_i} = \\frac{x_i-\\mu}{\\sigma_i}$$\nScikit-learn provides the `StandardScaler` class that does this automatically.","71ce0448":"## Step 6: Fine-Tune the System\nIn this step we compare the cross-validation errors and choose our final model.\n\nThere are other things one can do in this step, such as\n- Tuning Hyperparameters using Grid Search\n- Using Ensemble Models\nHowever, we won't have time to cover them. If you are interested, you can check the references at the bottom of this notebook and learn more.","1a8d30fb":"The classifier tells us that I will not be purchasing a car. That sounds correct cuz I can't afford it!","6ee55d4b":"## Background Scenario\nIn this tutorial, we will build machine learning models to help a car retail company to \"learn\" their customer behaviour. \nThey have collected some data on their previous customers and listed in \"Car_Advertisement.csv\". In the datasheet, you can see 5 columns:\n+ User ID (instead of names, the company is using an ID to protect their privacy!)\n+ Gender \n+ Age\n+ Salary\n+ Purchased (whether or not they purchased a car - this is the target for our predictions)\nUsing these data, the company want to be build a ML model that can predict if a new customer will purchase a car based on their gender, age and salary.\n\n## Machine Learning Project Procedures\nWe want to demonstrate to you the standard procedures to do a machine learning project using the car advertisement case.\nWe are going through the following steps:\n- Frame the problem and look at the big picture.\n- Get the data. Split into training and testing set, and set aside the testing set for now.\n- Explore the data. Get some insight!\n- Pre-processing the data to better expose the underlying data patterns to ML algorithms.\n- Try out different ML models. In this tutorial we will try 5 models: discriminant analysis model, KNN, Naive Bayes, SVM-linear, SVM-nonlinear.\n- Fine-tune your models. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n- Present your solution. Lauch your model.\n\n_This checklist is adapted from Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurelien Geron. It is free to read online for all UBC students. I recommend it if you want to learn more about doing ML in Python!_\n\n## Step 1: Frame the Problem\nUsing \"Car_Advertisement.csv\", we want to help the company to build a ML model that can predict if a new customer will purchase a car based on their gender, age and salary as accurately as possible.\n\n## Step 2: Get the Data","e4d870e8":"## Step 4: Pre-processing the Data\n### Handling Categorical Features\nWe can convert category text to numbers using sklearn's OrdinalEncoder class:","99d59cac":"For the same reasoning as I presented in the MATLAB LiveScript, I am going to go with SVM-Gaussian kernel.","d220bbc4":"### Evaluate Using Cross-Validation ","4249f599":"### Model 2: Naive Bayes Classifier ","e5cb3fb6":"Value counts of categorical attributes: (You might noticed that these values are different than the MATLAB version. This is because the train-test split is done randomly and we may get different male\/female proportions. As long as the total number is 320, we are good to go!)","60b667b6":"Let's create a dictionary to save the average cv scores of different classifiers so that we can compare later:","2eb9d357":"## Step 3: Explore the Training Data","d5e766b1":"### Model 5: SVM-Non-linear "}}