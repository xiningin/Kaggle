{"cell_type":{"f4ce15d1":"code","704baefc":"code","89a989f1":"code","1d956659":"code","de9992de":"code","8c8ab342":"code","38a89bff":"code","e980e588":"code","e5240295":"code","db0f486a":"code","8707fbbc":"code","ad758d9d":"code","af2486e1":"code","7c4005b1":"code","02203b8c":"code","98ad5f75":"code","c76f1812":"code","b882ed45":"code","d2af99a0":"code","6c4f2a83":"markdown","0f741070":"markdown","49d0f280":"markdown","6bd8b9d9":"markdown"},"source":{"f4ce15d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import imdb\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","704baefc":"#Downloading IMDB data\n(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words=10000)","89a989f1":"#Converting integer into English word\nword_index = imdb.get_word_index()\n\n#Reversing the dictionary so that index maps to word\nindex_word = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_review = ' '.join([index_word.get(i-3, '?') for i in train_x[0]])\n","1d956659":"#Decoded review text\ndecoded_review","de9992de":"#Code training to one-hot encoding\ndef seq_to_one_hot_encoding(sequences, dim=10000):\n    vect_seq = np.zeros((len(sequences), dim))\n    for i, sequence in enumerate(sequences):\n        vect_seq[i, sequence] = 1\n    return vect_seq\n\nx_train = seq_to_one_hot_encoding(train_x)\nx_test = seq_to_one_hot_encoding(test_x)","8c8ab342":"print(\"The shape of x_train and x_test are {} & {}\".format(x_train.shape,x_test.shape))","38a89bff":"#Vectorize test data\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","e980e588":"#The network architecture\nmodel = keras.Sequential(([\n    layers.Dense(16, activation=\"relu\", input_shape = (10000,)),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation = 'sigmoid')\n]))","e5240295":"#The model compilation\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])","db0f486a":"#Creating a validation set\n\nval_x = x_train[:10000]\npartial_x_train = x_train[10000:]\n\nval_y = y_train[:10000]\npartial_y_train = y_train[10000:]","8707fbbc":"model_train = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(val_x, val_y))\n","ad758d9d":"hist_dict = model_train.history\nhist_dict.keys()","af2486e1":"#Ploting the accuracy over time\nimport matplotlib.pyplot as plt\nloss_values = hist_dict['loss']\nval_loss_values = hist_dict['val_loss']\n\nepochs = range(1, len(loss_value)+1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n","7c4005b1":"plt.clf()\nacc_values = hist_dict['accuracy']\nval_acc_values = hist_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","02203b8c":"#Recreating a new model with an L2 regularization\nfrom tensorflow.keras.regularizers import l2\nmodel = keras.Sequential(([\n    layers.Dense(32, kernel_regularizer=l2(0.02), activation=\"relu\", input_shape = (10000,)),\n    layers.Dense(16, kernel_regularizer=l2(0.02),activation=\"relu\"),\n     layers.Dense(8, kernel_regularizer=l2(0.02), activation=\"relu\"),\n    layers.Dense(1, activation = 'sigmoid')\n]))","98ad5f75":"#Re-compiling the model\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])","c76f1812":"model_train = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(val_x, val_y))","b882ed45":"#plotting the accuracy\nacc_values = hist_dict['accuracy']\nval_acc_values = hist_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d2af99a0":"#Predicting values of model\ny_pred = model.predict(x_test)\n\ny_pred = np.where((y_pred>=0.5), 1, 0)\nprint(y_pred)\n","6c4f2a83":"This is clearly a case of over-fitting as the validation accuracy is decreasing over the epochs. \n\nThe model needs to be re-trained. ","0f741070":"The overfitting has a bit because of adding L2 regularization parameters.\n\nLets predict the labels from the test data. ","49d0f280":"The model has reached an accuracy of 1, while the accuracy of the validation set is 0.86. It could be a case of overfitting. ","6bd8b9d9":"The data is already pre-processed in keras. Each word is representated as an integer. "}}