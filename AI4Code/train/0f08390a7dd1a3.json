{"cell_type":{"4e513992":"code","3cef0c72":"code","e33034cb":"code","17904ceb":"code","c1be1510":"code","df2447b4":"code","ccc36b01":"code","31b8a732":"code","e53690ed":"code","8ed996ad":"code","f456ef68":"code","3e79a7f0":"code","89531fec":"code","2c7c0669":"code","09239114":"code","70e0e740":"code","43b8adfb":"code","1ea53871":"code","bcbc5039":"code","8fc02a1c":"code","baca8f8d":"code","f77d5d44":"code","bbe88a29":"code","5bbd1bea":"code","1de1777a":"code","6dfaaf9e":"code","6e779a87":"code","2651f1ab":"code","d6396b37":"code","4bee6d19":"code","15fc3e34":"code","1e3f1016":"code","213e7275":"code","6cec3338":"code","70a96db4":"markdown","980c46c5":"markdown","71374630":"markdown","53396be9":"markdown","0eb7cabd":"markdown","a5fb9792":"markdown","39430076":"markdown","2f99168d":"markdown","24a8fd8d":"markdown","2fd287ab":"markdown","82d1a64b":"markdown","16bbce1f":"markdown","948d8a28":"markdown","fe94aea0":"markdown","f3191f1e":"markdown","c1b2cc2b":"markdown","c9b5698e":"markdown"},"source":{"4e513992":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport torch.nn as nn\nimport numpy as np\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tt\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline","3cef0c72":"project_name = '005facialexpressorecogresnetnreguapproach'","e33034cb":"# Defining the Data directory\ndata_dir = '..\/input\/facial-expression-recog-image-ver-of-fercdataset\/Dataset'\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"\/train\")\nprint(classes)","17904ceb":"# Data transform (normalization & data augmentation)\ntrain_tfms = tt.Compose([\n                         #tt.RandomCrop(32, padding=4, padding_mode='reflect'),\n                         #tt.Grayscale(num_output_channels=1),\n                         tt.RandomHorizontalFlip(),\n                         tt.RandomRotation(30),\n                         tt.ColorJitter(brightness=0.1, contrast=0.25, saturation=0.35, hue=0.05),\n                         tt.RandomRotation(10, resample=False, expand=False, center=None, fill=None),\n                         tt.ToTensor()\n                        ])\n\nvalid_tfms = tt.Compose([tt.ToTensor()])","c1be1510":"train_ds = ImageFolder(data_dir+'\/train', train_tfms)\nvalid_ds = ImageFolder(data_dir+'\/test', valid_tfms)","df2447b4":"batch_size = 64","ccc36b01":"# PyTorch data loaders\ntrain_dl = DataLoader(\n    train_ds, \n    batch_size, \n    shuffle=True, \n    num_workers=3, \n    pin_memory=True\n                     )\n\nvalid_dl = DataLoader(\n    valid_ds, \n    batch_size*2, \n    num_workers=3, \n    pin_memory=True\n                     )","31b8a732":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]) \n        ax.set_yticks([])\n        ax.imshow(make_grid(images[:64], nrow=16).permute(1, 2, 0))\n        break","e53690ed":"show_batch(train_dl)","8ed996ad":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","f456ef68":"device = get_default_device()\ndevice","3e79a7f0":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","89531fec":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","2c7c0669":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)\n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)\n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Linear(512, num_classes))\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out\n","09239114":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass FacialRecogModel2(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet50(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 7)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))","70e0e740":"model = to_device(ResNet9(3, 7), device)\nmodel","43b8adfb":"#model = to_device(FacialRecogModel2(), device)\n#model","1ea53871":"from tqdm.notebook import tqdm","bcbc5039":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","8fc02a1c":"history = [evaluate(model, valid_dl)]\nhistory","baca8f8d":"epochs = 25\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","f77d5d44":"%%time\nhistory += fit_one_cycle(\n    epochs, \n    max_lr, \n    model, \n    train_dl, \n    valid_dl, \n    grad_clip=grad_clip, \n    weight_decay=weight_decay, \n    opt_func=opt_func\n                        )","bbe88a29":"train_time = '15min 35s'","5bbd1bea":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    \ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.')","1de1777a":"plot_accuracies(history)","6dfaaf9e":"plot_losses(history)","6e779a87":"plot_lrs(history)","2651f1ab":"Stop Here","d6396b37":"evaluate(model, valid_dl)","4bee6d19":"!pip install jovian --upgrade --quiet","15fc3e34":"import jovian","1e3f1016":"jovian.log_hyperparams(arch='resnet9', \n                       epochs=[25], \n                       lr=max_lr, \n                       scheduler='one-cycle', \n                       weight_decay=weight_decay, \n                       grad_clip=grad_clip,\n                       opt=opt_func.__name__)","213e7275":"jovian.log_metrics(val_loss=history[-1]['val_loss'], \n                   val_acc=history[-1]['val_acc'],\n                   train_loss=history[-1]['train_loss'],\n                   time=train_time)","6cec3338":"jovian.commit(project=project_name, environment=None)","70a96db4":"-----------Pre Trained------------","980c46c5":"### Lets have a look at the batch of data","71374630":"# Facial Expression Recognition with ResNet and Regularization Techniques","53396be9":"- **User**: [@manishshah120](https:\/\/www.kaggle.com\/manishshah120)\n- **LinkedIn**: https:\/\/www.linkedin.com\/in\/manishshah120\/\n- **GitHub**: https:\/\/github.com\/ManishShah120\n- **Twitter**: https:\/\/twitter.com\/ManishShah120\n\n> This Notebook was created while working on project for a course \"**Deep Learning with PyTorch: Zero to GANs**\" from \"*jovian.ml*\" in collaboratoin with \"*freecodecamp.org*\"\n","0eb7cabd":"## Data Loaders","a5fb9792":"## Imports","39430076":"## Commiting to Jovian","2f99168d":"### Moving the batches of data to GPU if available","24a8fd8d":"## Preparing the Data","2fd287ab":"### Declaration of the `train_ds` and `test_ds`","82d1a64b":"## Model with Residual Blocks and Batch Normalization\nOne of the key changes to our CNN model this time is the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers.\n![img](https:\/\/miro.medium.com\/max\/1140\/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)","16bbce1f":"### Data transformations","948d8a28":"## Lets work on Pre-trained model","fe94aea0":"## Model Defintion","f3191f1e":"## Using a GPU","c1b2cc2b":"## Training the model\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\n- **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the \"One Cycle Learning Rate Policy\", which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n- **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab\n\n- **Gradient clipping**: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","c9b5698e":"## Plotting Functions"}}