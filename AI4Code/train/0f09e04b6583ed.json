{"cell_type":{"ad0a4e31":"code","75fe9309":"code","29d46393":"code","1ee72c14":"code","49da9334":"code","a4fca3d7":"code","787b9f20":"code","4a35fa90":"code","78401a44":"code","1462ed7d":"code","1c398e7f":"code","7f6cc9bb":"code","a7f70897":"code","e0f40b39":"code","060490d8":"code","cb29c614":"code","7e058e83":"markdown","81d40a77":"markdown","df153ca5":"markdown","c64103d2":"markdown","f287ed2f":"markdown","8bb04159":"markdown","582eb7bc":"markdown","4cf66c85":"markdown","b5e51561":"markdown","46a343b9":"markdown"},"source":{"ad0a4e31":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","75fe9309":"#read the data from input and output files\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\n#Drop features we are not going to use\n#train_df = train_df.drop(['Name','SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],axis=1)\n#test_df = test_df.drop(['Name','SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],axis=1)\n\n#train_df.head(3)","29d46393":"train_df['Fare'] = train_df['Fare'].fillna(0)\ntest_df['Fare'] = test_df['Fare'].fillna(0)\n\ntarget = 'Survived'\n#train_df.head(3)","1ee72c14":"#test_df.head(3)","49da9334":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","a4fca3d7":"freq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","787b9f20":"#Fill in missing age values with 0 (presuming they are a baby if they do not have a listed age)\nage_port = train_df.Age.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Age'] = dataset['Age'].fillna(age_port)\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']","4a35fa90":"#train_df.head()","78401a44":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","1462ed7d":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","1c398e7f":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n","7f6cc9bb":"#train_df.head()","a7f70897":"train_df = train_df.drop(['Name','Ticket', 'Cabin','FamilySize','SibSp','Parch' ],axis=1)\ntest_df = test_df.drop(['Name','Ticket', 'Cabin','FamilySize','SibSp','Parch' ],axis=1)","e0f40b39":"#train_df = pd.get_dummies(train_df)\n#test_df = pd.get_dummies(test_df)","060490d8":"#Create classifier object with default hyperparameters\nclf = RandomForestClassifier()\n#clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf=2)\n\n\nactual = train_df[target]\ntrain_df=train_df.drop('Survived', axis=1)\n\n\n#Fit our classifier using the training features and the training target values\nclf.fit(train_df,actual) \n\n#Make predictions using the features from the test data set\npredictions = clf.predict(test_df)\nsubmission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':predictions})\n#submission.head()","cb29c614":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","7e058e83":"Writing down our approach to solve the problem.\n> Let's break it down to pieces and also we will explain what can be done in each section\n1. Problem Definition\n2. Acquire training and test data\n3. Prepare,Clean Data\n4. Analyse, corelation, identify patterns\n5. Model and fit the data\n6. Predict the results\n7. Visualize and create reports\n8. Submit solution","81d40a77":"**SEX feature correction**","df153ca5":"combine = [train_df, test_df]\nfor dataset in combine:\n     dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","c64103d2":"Importing everything required","f287ed2f":"**ONE HOT ENCODING**","8bb04159":"Dropping FamilySize ,SibSp and Parch since we have IsAlone feature now","582eb7bc":"> My approach is KISS( Keep it stupid simple)","4cf66c85":"**Embarked feature**","b5e51561":"** TTILE ,,AT THE MOMENT THIS NEW FEATURE IS reducing the accuracy**","46a343b9":"**DROP USELESS FEATURES BEFORE FITTING**"}}