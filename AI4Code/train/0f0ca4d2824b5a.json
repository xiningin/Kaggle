{"cell_type":{"d87eae84":"code","fb32d0e2":"code","4d491ec2":"code","471bdb15":"code","d4d48d64":"code","17e59b16":"code","93f2890a":"code","9e61b672":"code","de40299e":"code","8bf7d52d":"code","fbe72301":"code","3e17d4ca":"code","3b3fefaf":"code","139eb75f":"code","d6152a37":"code","7d9487a9":"code","d6675b82":"code","2112392c":"code","db30df88":"code","30b19c64":"code","30c3b84f":"code","907b7046":"code","fcfb311d":"code","5e323f07":"code","6d1cfe05":"code","e138fc09":"code","66e5e9c0":"code","f40ff90f":"code","172c8fe1":"code","7b395168":"code","bc6b0e4a":"code","ac1d98a1":"code","b292198e":"code","c8804dbb":"code","e4d74402":"code","b5eaed1b":"code","8a88d761":"code","eda4d45c":"code","7e954e90":"markdown","8a89d5e8":"markdown","0ccaf7c2":"markdown","7d01d506":"markdown","c016d91d":"markdown","0cdf8e11":"markdown","a6c46850":"markdown","3b9d66e1":"markdown","3e846dc6":"markdown","6111fc41":"markdown","1d895432":"markdown","abb4399f":"markdown","1b1b883e":"markdown","49565d7b":"markdown","983ecc8d":"markdown","1a0027de":"markdown","ac8b110e":"markdown","41a12d56":"markdown","5ba269ae":"markdown","c9ebdb9a":"markdown","db4393e1":"markdown","401e2515":"markdown","310d01f1":"markdown","3a1658ee":"markdown","36aec825":"markdown","d351c09a":"markdown","a37a91b5":"markdown","f14a6a94":"markdown","6cef6ea0":"markdown","e32d1100":"markdown","388f4c8b":"markdown","e3831d47":"markdown","9c97c140":"markdown"},"source":{"d87eae84":"!pip install ..\/input\/tabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl","fb32d0e2":"N_FOLDS = 5\nSEED = 33","4d491ec2":"import os, random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\nimport lightgbm as lgb","471bdb15":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nsub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","d4d48d64":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)","17e59b16":"data = pd.concat([train, test], sort=False)\n\ndata['Sex'] = data['Sex'].replace(['male','female'], [0, 1])\ndata['Embarked'] = data['Embarked'].fillna(('S'))\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'] = data['Fare'].fillna(np.mean(data['Fare']))\ndata['Age'] = data['Age'].fillna(data['Age'].median())","93f2890a":"del_col = ['Name', 'PassengerId','Ticket', 'Cabin']\ndata.drop(del_col, axis=1, inplace=True)","9e61b672":"train = data[:len(train)]\ntest = data[len(train):]","de40299e":"feature_col = [col for col in train.columns.tolist() if col != 'Survived']","8bf7d52d":"categorical_columns = ['Pclass','Sex','SibSp','Parch','Embarked']\n\n# class num\ncategorical_dims = {}\n\nfor col in categorical_columns:\n    categorical_dims[col] = train[col].nunique()","fbe72301":"cat_idxs = [ i for i, f in enumerate(feature_col) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(feature_col) if f in categorical_columns]","3e17d4ca":"# comment out when Unsupervised\n\ntabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                     n_independent=2, n_shared=2,\n                     seed=SEED, lambda_sparse=1e-3, \n                     optimizer_fn=torch.optim.Adam, \n                     optimizer_params=dict(lr=2e-2),\n                     mask_type=\"entmax\",\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10\n                    )\n\npretrainer = TabNetPretrainer(**tabnet_params)\n\npretrainer.fit(\n    X_train=train.drop('Survived',axis=1).values,\n    eval_set=[train.drop('Survived',axis=1).values],\n    max_epochs=200,\n    patience=20, batch_size=256, virtual_batch_size=128,\n    num_workers=1, drop_last=True)","3b3fefaf":"skf = StratifiedKFold(n_splits=N_FOLDS)\nfor f, (t_idx, v_idx) in enumerate(skf.split(train.drop('Survived',axis=1), train['Survived'])):\n    train.loc[v_idx, 'fold'] = int(f)","139eb75f":"%%time\noof = np.zeros((len(train),))\ntest_preds_all = np.zeros((len(test),))\nmodels = []\n\nfor fold_num in range(N_FOLDS):\n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    print(\"FOLDS : \", fold_num)\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train['Survived'].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train['Survived'].values[valid_idx,].astype(float)\n    \n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                         n_independent=2, n_shared=2,\n                         seed=SEED, lambda_sparse=1e-3,\n                         optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2,\n                                               weight_decay=1e-5\n                                              ),\n                         mask_type=\"entmax\",\n                         scheduler_params=dict(max_lr=0.05,\n                                               steps_per_epoch=int(X_train.shape[0] \/ 256),\n                                               epochs=200,\n                                               is_batch_level=True\n                                              ),\n                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                         verbose=10,\n                         cat_idxs=cat_idxs, # comment out when Unsupervised\n                         cat_dims=cat_dims, # comment out when Unsupervised\n                         cat_emb_dim=1 # comment out when Unsupervised\n                        )\n\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_name = [\"valid\"],\n              eval_metric = [\"auc\"],\n              max_epochs=200,\n              patience=20, batch_size=256, virtual_batch_size=128,\n              num_workers=0, drop_last=False,\n              from_unsupervised=pretrainer # comment out when Unsupervised\n             )\n    \n    # Make Oof\n    oof[valid_idx] = model.predict_proba(X_valid)[:,1]\n    # Model\n    models.append(model)\n\n    # for save weight\n    # name = f\"fold{fold_num}\"\n    # model.save_model(name)    \n\n    # preds on test\n    preds_test = model.predict_proba(test[feature_col].values)[:,1]\n    test_preds_all += preds_test \/ N_FOLDS","d6152a37":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","7d9487a9":"for fold_num, model in enumerate(models):\n    # Feature Importance\n    feat_imp_fold = pd.DataFrame(model.feature_importances_,index=feature_col, columns= [f'imp_{fold_num}'])\n    if fold_num == 0:\n        feature_importance = feat_imp_fold.copy()\n    else:\n        feature_importance = pd.concat([feature_importance, feat_imp_fold], axis=1)\n        \nfeature_importance['imp_mean'] = feature_importance.mean(axis=1)\nfeature_importance = feature_importance.sort_values('imp_mean')\n\nplt.tick_params(labelsize=18)\nplt.barh(feature_importance.index.values,feature_importance['imp_mean']);\nplt.title('feature_importance',fontsize=18);","d6675b82":"explain_matrix, masks = model.explain(test[feature_col].values)","2112392c":"fig, axs = plt.subplots(1, 3, figsize=(10,7))\n\nfor i in range(3):\n    axs[i].imshow(masks[i][:25])\n    axs[i].set_title(f\"mask {i}\")","db30df88":"# 0 = 'Pclass', 1 = 'Sex', 2 = 'Age',3 =  'SibSp', 4 = 'Parch', 5 = 'Fare', 6 = 'Embarked'","30b19c64":"plt.hist(test_preds_all);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","30c3b84f":"sub['Survived'] = (test_preds_all > 0.5).astype('int')\nsub.to_csv(\"submission.csv\", index=False)","907b7046":"# change to category\nfor col in categorical_columns:\n    train[col] = train[col].astype('category')\n    test[col] = test[col].astype('category')","fcfb311d":"%%time\noof = np.zeros((len(train),))\ntest_preds_all = np.zeros((len(test),))\nmodels = []\n\nfor fold_num in range(N_FOLDS):\n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    print(\"FOLDS : \", fold_num)\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train['Survived'].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train['Survived'].values[valid_idx,].astype(float)\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n    params = {\n        'objective': 'binary'\n    }\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n    \n    # Calc Oof\n    oof[valid_idx] = model.predict(X_valid, num_iteration=model.best_iteration)\n    # Model\n    models.append(model)\n\n    # for save weight\n    # name = f\"fold{fold_num}\"\n    # model.save_model(name)    \n\n    # preds on test\n    test_preds_all += model.predict(test[feature_col], num_iteration=model.best_iteration) \/ N_FOLDS","5e323f07":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","6d1cfe05":"# Feature Importance\n\nfor fold_num, model in enumerate(models):\n    # Feature Importance\n    feat_imp_fold = pd.DataFrame(model.feature_importance(), index=feature_col, columns= [f'imp_{fold_num}'])\n    if fold_num == 0:\n        feature_importance = feat_imp_fold.copy()\n    else:\n        feature_importance = pd.concat([feature_importance, feat_imp_fold], axis=1)\n        \nfeature_importance['imp_mean'] = feature_importance.mean(axis=1)\nfeature_importance = feature_importance.sort_values('imp_mean')\n\nplt.tick_params(labelsize=18)\nplt.barh(feature_importance.index.values,feature_importance['imp_mean']);\nplt.title('feature_importance',fontsize=18);","e138fc09":"plt.hist(test_preds_all);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","66e5e9c0":"# sub['Survived'] = (test_preds_all > 0.5).astype('int')\n# sub.to_csv(\"submission.csv\", index=False)","f40ff90f":"target_cols = ['Survived']","172c8fe1":"class TitanicDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","7b395168":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","bc6b0e4a":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nNFOLDS = 5\n\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_col)\nnum_targets=1\nhidden_size =150","ac1d98a1":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","b292198e":"def run_training(fold_num, seed, train, test, target_cols):\n    \n    seed_everything(seed)\n    \n    print(\"FOLDS : \", fold_num)\n    \n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train[['Survived']].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train[['Survived']].values[valid_idx,].astype(float)\n    \n    train_dataset = TitanicDataset(X_train, y_train)\n    valid_dataset = TitanicDataset(X_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)        \n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), num_targets))\n    predictions = np.zeros((len(test), num_targets))\n    \n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"seed{seed}, FOLD: {fold_num}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            print('Update best loss')\n            best_loss = valid_loss\n            oof[valid_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold_num}_seed{seed}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                print(f'Early Stop epoch{epoch} early_step{early_step}')\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test[feature_col].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n\n    model.load_state_dict(torch.load(f\"FOLD{fold_num}_seed{seed}_.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","c8804dbb":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    for fold_num in range(NFOLDS):\n        oof_, pred_ = run_training(fold_num, seed, train, test, target_cols)\n\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","e4d74402":"%%time\nSEED = [33]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nprint(\"######################## Training ############################\")\nfor seed in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)","b5eaed1b":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","8a88d761":"plt.hist(predictions);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","eda4d45c":"# sub['Survived'] = (predictions > 0.5).astype('int')\n# sub.to_csv(\"submission.csv\", index=False)","7e954e90":"<img src=\"https:\/\/storage.googleapis.com\/zenn-user-upload\/mts8q51v792anu3e0cwkbsq0dob6\" width=75%>","8a89d5e8":"# Implementation","0ccaf7c2":"- The output distribution of TabNet has a shape similar to that of NN.","7d01d506":"![Feature Importance for each Model](https:\/\/storage.googleapis.com\/zenn-user-upload\/ezsqiabborqn8jmil0odvrevipbc)","c016d91d":"### Summary","0cdf8e11":"### Comparison Predict Distribution","a6c46850":"- TabNet is a highly interpretive model by using Mask and Feature Importance.\n\n- In Titanic data, the accuracy of TabNet was equivalent to that of LightBM and NN. Furthermore, since the dissociation between OOF and LB values is small, it may be a model that is less prone to overlearning.\n\n- The top features in TabNet's Feature Importance were different from those of LightGBM. Therefore, TabNet can be expected to be useful for Ensemble.","3b9d66e1":"# NN(For comparison)","3e846dc6":"# Summary of the Results","6111fc41":"# In this Notebook, you will find the following\n\n- SelfSupervised TabNet Implementation Examples\n- TabNet Feature Importance, Mask, output examples\n- Comparison of accuracy with TabNet with pretrain, TabNet without pretrain, LighGBM, and NN\n- Comparison of feature importance with TabNet with pretrain, TabNet without pretrain, LighGBM\n- Comparison of Output distribution for TabNet with pretrain, TabNet without pretrain, LighGBM, NN","1d895432":"### Comparison OOF_ROC-AUC, OOF_Accuracy, LB_Accuracy\n\n | - | TabNet with Pretrain| TabNet without Pretrain| LightGBM | NN |\n| ------------ | ------------- | ------------- | ------------- | ------------- |\n| OOF ROC-AUC | 0.8620 | 0.8354 | 0.8750 | 0.8643 |\n| OOF Accuracy | 0.8114 | 0.7564 | 0.8260 | 0.8249 |\n| LB Accuracy | 0.7775 | 0.7560 | 0.7608 |0.7656 |\n| Time(s) | 34.6 | 37.3 | 0.24 | 6.86 | ","abb4399f":"- TabNet is lower than LightGBM and NN for the accuracy of OOF.\n- TabNet is higher than LightGBM and NN for the accuracy of LB.\n- TabNet has a smaller difference between OOF and LB accuracy than LightGBM and NN.\n- TabNet with Pretrain has a higher accuracy of OOF and a lower accuracy of LB than TabNet without Pretrain.\n- When using CPUs, TabNet is more time consuming","1b1b883e":"![Feature Importance for each Model](https:\/\/storage.googleapis.com\/zenn-user-upload\/k7wzkbishovexicgah2hex4mwoig)","49565d7b":"# Ref","983ecc8d":"# Make Folds","1a0027de":"# TabNet Pretraining","ac8b110e":"### Comparison Feature Importance","41a12d56":"- TabNet (with Pretrain or without Pretrain) has a smaller difference in importance between features than LightGBM.\n\n- In both TabNet with Pretrain and LightGBM, Fare ranks first. On the other hand, there is a rlarge variance for features between TabNet with Pretrain and LightGBM","5ba269ae":"# Fix seed","c9ebdb9a":"# LightGBM(For comparison)","db4393e1":"# Preprocess","401e2515":"### Original Article(Japanese)\nhttps:\/\/zenn.dev\/sinchir0\/articles\/9228eccebfbf579bfdf4","310d01f1":"# Setting","3a1658ee":"- This Mask is created each time you make a decision (deciding which features to use), and the number of times you make a decision is determined by\nIt can be specified by n_steps. In this case, n_steps=3, so the number of Masks is 3.\n\n- The horizontal axis is the features (0='Pclass', 1='Sex', 2='Age', 3='SibSp', 4='Parch', 5='Fare', 6='Embarked') and the vertical axis is the number of test lines (25 lines from the top).\n\n- At a glance, we can see that 5='Fare' is strongly colored for mask2. This seems to be a reasonable result, considering that Fare came first in the feature importance.","36aec825":"https:\/\/www.kaggle.com\/optimo\/selfsupervisedtabnet\/data?select=pytorch_tabnet-2.0.1-py3-none-any.whl\n\nhttps:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/develop\/census_example.ipynb","d351c09a":"# Install","a37a91b5":"# Feature cols","f14a6a94":"### TabNet's Mask\n","6cef6ea0":"# TabNet Training","e32d1100":"# Import","388f4c8b":"# Global Explainability : Feature Importance","e3831d47":"# Local explainability and masks","9c97c140":"# Define categorical features for categorical embeddings"}}