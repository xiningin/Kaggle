{"cell_type":{"cb570f32":"code","aa111724":"code","a139a6d6":"code","8939ac58":"code","c9e88baa":"code","57091eb5":"code","fb11a321":"code","cdb2c308":"code","2d371d38":"code","42cc2189":"code","b36f90be":"code","73875114":"code","88f30b69":"code","324f6be2":"code","e2f112db":"code","1abd447c":"code","b9d0ca1e":"code","07ed59b5":"code","17cc3012":"code","5c690c5a":"code","54f6dc87":"code","2f47787f":"code","62e05f27":"code","1d3fae36":"code","2d215344":"code","88779db3":"code","019b2a89":"code","aeff072a":"code","7e366815":"code","1028b099":"code","9d5ccf89":"code","0ee1b5ec":"code","84f45e76":"code","3ec59478":"code","267b1e0c":"code","f9ebd55c":"code","602a368f":"code","ee53fc12":"code","5d48421c":"code","684fd0c7":"code","df400607":"code","70434eb4":"code","b87ad216":"code","6a41e1f3":"code","07c743c4":"code","6def54fa":"code","1332c0be":"code","e53e9bce":"code","fe6ebe32":"code","11952910":"code","d1551a14":"markdown","d987f01e":"markdown","57974b9d":"markdown","cb0e2c61":"markdown","b891eaf4":"markdown","98858379":"markdown","4715a7dc":"markdown","77428730":"markdown","54b32022":"markdown","2c98efe5":"markdown","083486e5":"markdown"},"source":{"cb570f32":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import OrdinalEncoder","aa111724":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","a139a6d6":"test.head()","8939ac58":"print('the shape of train is: ',train.shape)\nprint('the shape of test is: ',test.shape)","c9e88baa":"train.isnull().sum().sort_values(ascending = False)","57091eb5":"train.info()","fb11a321":"train = train.drop('Id', axis=1)","cdb2c308":"train.columns","2d371d38":"plt.figure(figsize=(10,7))\nsns.distplot(train['SalePrice'])","42cc2189":"num_cols = train.select_dtypes(include=[np.int64]).columns\nnum_cols.append(train.select_dtypes(include=[np.float64]).columns)\ncat_cols = train.select_dtypes(include=[np.object]).columns","b36f90be":"num_cols","73875114":"#to check the distribution of numeric cols","88f30b69":"plt.figure(figsize=(30,50))\nfor i in range(len(num_cols)):\n  plt.subplot(15,3,i+1)\n  sns.distplot(train[num_cols[i]], kde= False)\n  plt.show","324f6be2":"plt.figure(figsize=(30,50))\nfor i in range(len(num_cols)):\n  plt.subplot(15,3,i+1)\n  sns.boxplot(train[num_cols[i]])\n  plt.show","e2f112db":"plt.figure(figsize=(30,50))\nfor i in range(len(cat_cols)):\n  plt.subplot(17,3,i+1)\n  sns.barplot(train[cat_cols[i]], train['SalePrice'])\n  plt.show","1abd447c":"plt.figure(figsize=(30,50))\nfor i in range(len(num_cols)):\n  plt.subplot(15,3,i+1)\n  sns.scatterplot(train[num_cols[i]], train['SalePrice'])\n  plt.show","b9d0ca1e":"plt.figure(figsize=(20,15))\nsns.heatmap(train[num_cols].corr(), annot= True, cbar=True)","07ed59b5":"plt.figure(figsize=(20,7))\nsns.lineplot(x = 'YearBuilt', y= 'SalePrice', data= train)\nplt.xticks(rotation = 90, ha = 'right');","17cc3012":"plt.figure(figsize=(10,6))\nsns.lineplot(x = 'YrSold', y= 'SalePrice', data= train)","5c690c5a":"plt.figure(figsize=(10,6))\nsns.lineplot(x = 'MoSold', y= 'SalePrice', data= train)","54f6dc87":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([percent], axis=1, keys=['Percent'])\nmissing.head(20)","2f47787f":"#we are reomoving top 5 missing data columns since there are lot of missing values(i.e more than 10-15% missing values)\ntrain = train.drop(columns =['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage'], axis =1)\ntest = test.drop(columns =['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage'], axis =1)\n#Lets explore other variable if they are important ones","62e05f27":"train[['GarageCond', 'GarageQual', 'GarageFinish', 'GarageType', 'GarageYrBlt', 'BsmtFinType1', 'BsmtExposure', 'BsmtQual','BsmtCond', 'BsmtFinType1', 'MasVnrArea', 'MasVnrType', 'Electrical']].isnull().sum()","1d3fae36":"train =train.drop(columns =['GarageQual', 'GarageFinish', 'GarageType', 'GarageYrBlt', 'BsmtFinType1', 'BsmtExposure', 'BsmtQual','BsmtFinType1', 'MasVnrType'], axis =1)","2d215344":"train =train.dropna(axis =0)","88779db3":"train.info()","019b2a89":"train.select_dtypes(np.object).head(2)","aeff072a":"cat_cols = train.select_dtypes(np.object).columns\ncat_cols","7e366815":"for i in range(len(cat_cols)):\n  train[cat_cols[i]] = train[cat_cols[i]].astype('category')\n  train[cat_cols[i]] = train[cat_cols[i]].cat.codes","1028b099":"for i in range(len(cat_cols)):\n  test[cat_cols[i]] = test[cat_cols[i]].astype('category')\n  test[cat_cols[i]] = test[cat_cols[i]].cat.codes","9d5ccf89":"plt.figure(figsize=(40,15))\nsns.heatmap(train.corr(), annot= True, cbar=True)","0ee1b5ec":"train = train.drop(columns = ['LotArea', 'YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', '2ndFlrSF', 'OpenPorchSF', 'WoodDeckSF', 'PoolArea', 'YrSold', 'MoSold'], axis =1)\n\ntest = test.drop(columns = ['LotArea', 'YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', '2ndFlrSF', 'OpenPorchSF', 'WoodDeckSF', 'PoolArea', 'YrSold', 'MoSold'], axis =1)","84f45e76":"#normality test on target variable\nplt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['SalePrice'], plot=plt)\npylab.show()","3ec59478":"# Shapiro-Wilk Test\nfrom scipy.stats import shapiro\ndata = train['SalePrice']\n# normality test\nstat, p = shapiro(data)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian')\nelse:\n\tprint('Sample does not look Gaussian')","267b1e0c":"train['SalePrice'] = np.log(train['SalePrice'])\n#normality test on target variable\nplt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['SalePrice'], plot=plt)\npylab.show()","f9ebd55c":"train.head()","602a368f":"plt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['TotalBsmtSF'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['TotalBsmtSF'], plot=plt)\npylab.show()","ee53fc12":"train['TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\n#normality test on target variable\nplt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['TotalBsmtSF'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['TotalBsmtSF'], plot=plt)\npylab.show()","5d48421c":"plt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['GrLivArea'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['GrLivArea'], plot=plt)\npylab.show()","684fd0c7":"train['GrLivArea'] = np.log(train['GrLivArea'])","df400607":"plt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['1stFlrSF'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['1stFlrSF'], plot=plt)\npylab.show()","70434eb4":"train['1stFlrSF'] = np.log(train['1stFlrSF'])","b87ad216":"plt.figure(figsize=(12,5))\nimport pylab\nplt.subplot(1,2,1)\nsns.distplot(train['GarageArea'])\nplt.subplot(1,2,2)\n#calculates a best-fit line for the data and plots the results using Matplotlib or a given plot function.\nres = stats.probplot(train['GarageArea'], plot=plt)\npylab.show()","6a41e1f3":"train['GarageArea'] = np.log(train['GarageArea'])","07c743c4":"train.shape","6def54fa":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error","1332c0be":"X = train.drop('SalePrice', axis =1)\ny = train['SalePrice']","e53e9bce":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state =100, test_size =0.2)","fe6ebe32":"model_Rf = RandomForestRegressor()\nmodel_Rf.fit(X_train, y_train)\ny_pred = model_Rf.predict(X_test)\n\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\nSSE = np.sum((y_pred-y_test)**2)\nSST = np.sum((y_test-np.mean(y_train))**2)\nr2_test = 1 - SSE\/SST\nprint(\"Test RMSE : \", rmse_test)\nprint(\"Test SSE : \", SSE)\nprint(\"Test SST : \", SST)\nprint(\"Test R2 : \", r2_test)","11952910":"model_GB = GradientBoostingRegressor()\nmodel_GB.fit(X_train, y_train)\ny_pred = model_GB.predict(X_test)\n\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\nSSE = np.sum((y_pred-y_test)**2)\nSST = np.sum((y_test-np.mean(y_train))**2)\nr2_test = 1 - SSE\/SST\nprint(\"Test RMSE : \", rmse_test)\nprint(\"Test SSE : \", SSE)\nprint(\"Test SST : \", SST)\nprint(\"Test R2 : \", r2_test)","d1551a14":"Exploratory data Analysis","d987f01e":"Lets explore about missing values\n\n","57974b9d":"convert category to codes","cb0e2c61":"there are lot of outliers in the dataset","b891eaf4":"from the above analysis, the important variables seems to be OverallQual, TotalBsmtSF, 1stFlrSF, GrLivArea , FullBath, TotRmsAbvGrd, GarageCars, GarageArea, ","98858379":"lets check the distribution of target variable","4715a7dc":"If the P-Value of the Shapiro Wilk Test is larger than 0.05, we assume a normal distribution\nIf the P-Value of the Shapiro Wilk Test is smaller than 0.05, we do not assume a normal distribution","77428730":"The above hypothesis says that first we have to make the target sample normal using log normal","54b32022":"Lets calculate the skewness of the continous features and seperate the features that are highly skewed","2c98efe5":"Assumption To be Followed by the model:\n1. The first assumption was that\nthe shape of the distribution of\nthe continuous variables in the\nmultiple regression correspond to\na normal distribution.\nThat is, each variable\u2019s frequency\ndistribution of values roughly\napproximates a bell-shaped curve.","083486e5":"We can achieve lot more by doing hyper parameter tuning and cross validation. Hope you all like a EDA and preprocessig part that i was tried. If you really like..kindly do upvote\n\n"}}