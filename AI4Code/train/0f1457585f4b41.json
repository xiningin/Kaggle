{"cell_type":{"4126a519":"code","ab8392be":"code","e46030d9":"code","10f106d1":"code","7651625e":"code","5f412b5d":"code","2bb40222":"code","092edced":"code","d6336b0d":"code","82234f8c":"code","d16fdd9a":"code","67d04f4d":"code","f43b0d2c":"code","47b6b23e":"code","b833785f":"markdown","8fc7dc04":"markdown","ab1413d8":"markdown","f925310a":"markdown","1596e74e":"markdown","d9f58f0d":"markdown","e127044c":"markdown","31bcf641":"markdown","1571928e":"markdown","b6b718db":"markdown","749813a0":"markdown","ced60b27":"markdown"},"source":{"4126a519":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab8392be":"train = pd.read_csv('..\/input\/pubg-finish-placement-prediction\/train_V2.csv')\ntest = pd.read_csv('..\/input\/pubg-finish-placement-prediction\/test_V2.csv')","e46030d9":"\"\"\"\nWanted to first look at the different data types in the dataset to \nunderstand which features could be used in a regression equation\nSince I am building a linear regression equation, I can't use any object types (categorical)\n\"\"\"\n\ntrain.info()","10f106d1":"null_columns = train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","7651625e":"#Seeing how there is only 1 null in winPlacePerc I sorted the data to view what index the null occurs in.\n\ntrain['winPlacePerc'].isnull().sort_values(ascending=False).head(10)","5f412b5d":"#Now we need to drop the values for index 2744604\n\ntrain.drop([train.index[2744604]], inplace=True)","2bb40222":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nnumVar = train.drop(['winPlacePerc', 'Id', 'groupId','matchId', 'matchType'], axis=1)\nscaler.fit(numVar)\n\nscaled_feat = pd.DataFrame(scaler.transform(numVar), columns = numVar.columns)\nscaled_feat.head()\n\n#Now that we have the equivalent Z scores for all of our variables,\n#I wanted to find outliers in the features and potentially remove them\n\nboolVal = (scaled_feat > 3) | (scaled_feat < -3)\n\nOutliers = 0\n\nfor i in scaled_feat:\n    boolVal_i = boolVal[boolVal[i] == True].sum()\n    Outliers = Outliers + boolVal_i\n    \nprint(Outliers)\n\n\"\"\"\nWhere there are true values, shows us how many outliers we have\nas these values are more than 3 standard deviations from the mean\n\nReturning Outliers shows us there is a massive amount of values past 3 standard deviations.\nAccording to the empirical rules, 99.7% of our data falls within 3 standard deviations from the mean. \n\nWith this massive amount of outliers, we can't remove them as this would significantly alter\nthe results of the data as we would be reducing the size of our training set by too much. \n\nTo conclude, we will not remove outliers.\n\"\"\"\n","092edced":"col = list(train.columns)\ncol = col[3:] #removes the 3 categorical variables that we can't\n#use in a scatterplot\ncol.remove('winPlacePerc') #removes our dependent variables\ncol.remove('matchType') #remove another categorical variable\n\n#using a for loop, we print out jointplots for each potential\n#independent variables for our regression equation on the x axis and\n#dependent varaible on our y axis\nfor i in col:\n    sns.jointplot(x=i, y='winPlacePerc', data=train)\n    plt.show()","d6336b0d":"plt.figure(figsize=(18,18))\nsns.heatmap(train.corr(), cmap='coolwarm', linecolor='white',\n           linewidth=.5, annot=True, fmt='.2f')","82234f8c":"#Now we will train the model for linear regression\n#First step is to seperate our independent variables and dependent variables\n\nX = train[['killPlace', 'boosts', 'walkDistance', 'weaponsAcquired']]\ny = train['winPlacePerc']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3)\n","d16fdd9a":"from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)","67d04f4d":"#Using the regression equation, we predict winPlacePerc off the 30% of data that our model hasn't seen before.\npredictions=linear.predict(X_test)\n","f43b0d2c":"c_mat = np.corrcoef(y_test, predictions)\nc_xy = c_mat[0,1]\nr_squared = c_xy**2\nprint(\"R Squared: {:.4f}\".format(r_squared))\n","47b6b23e":"X_test2 = test[['killPlace', 'boosts', 'walkDistance', 'weaponsAcquired']]\ntest_pred = linear.predict(X_test2)\n\nresults = pd.DataFrame(index=test['Id'], data=test_pred)\nresults.rename(columns={0: 'winPlacePerc'}, inplace=True)\n\nresults.to_csv('submission.csv')","b833785f":"Now that we have a regression equation that is a rather good predictor on Win Placement, we apply this equation to our test dataset and then convert our predictions to a csv file.","8fc7dc04":"To evaluate the goodness of fit, I used R^2. The resulting R^2 value of .7604 is rather good. This states that our linear regression equation explains approximately 76% of the variance in our data.","ab1413d8":"<a id=\"section-one\"><\/a>\n# Introduction\n\n![](https:\/\/www.cerillion.com\/cerillioncom\/media\/cerillionMedia\/BlogImages\/PUBG-Mobile.png?ext=.png?width=350)**With this notebook, our goal is to build a regression equation to predict a players Win Place Percentile given different independent variables for the game PUBG.**\n\nPUBG is a battle royale video game where multiple teams land on a map and fight each other until 1 team comes out victorious. There are multiple different game modes in PUBG including solos, duos, and squads. For simplicity of this model, I did not tailor my results to each match type which may provide more detail. My model explains PUBG as a whole and what independent variables have the highest impact on the Win Place Percentile of a team.","f925310a":"Looking at the different jointplots above, we see a few variables that look to have some correlation with our winPlacePerc. These include: boosts, damageDealt, heals, killPlace, longestKill, rideDistance, and walkDistance. To analyze these specific variables further I built a heatmap to look at correlation values. ","1596e74e":"<a id=\"section-four\"><\/a>\n# Model Creation","d9f58f0d":"As stated above, there are too many potential outliers. We can't remove any due to this issue of potentially altering the results of the dataset. So we can now move onto exploratory data analysis for feature selection. ","e127044c":"<a id=\"section-two\"><\/a>\n# Pre-Processing\n\nHere in the pre-processing, we will upload our data and clean our data. The cleaning process includes checking for null values, removing outliers, and feature selection. ","31bcf641":"Now that I have the training set split into a train and test model, I can use these variables to fit a linear regression equation. After fitting the regression equation to this training set, I can apply it to the test set and view the goodness of fit. ","1571928e":"Next step is exploring the features to see if there are any null values.","b6b718db":"<a id='Introduction'><\/a>","749813a0":"Now that we cleaned our data of null values, let's check for outliers in our dataset. To see this I created a dataframe that looks at all values within 3 standard deviations of the mean for each independent variable.","ced60b27":"Looking at the heatmap above I see that there are a few independent variables that have a strong correlation with winPlacePerc. Any absolute value of correlation above .5 I viewed as having a strong effect on the winPlacePerc. These included boosts (.63), killPlace (-.72), walkDistance (.81), and weaponsAcquired (.58). These are the independent variables that I will use in a linear regression equation."}}