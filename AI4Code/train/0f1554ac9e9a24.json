{"cell_type":{"9b67893f":"code","5d68c65f":"code","4ad075b0":"code","90c6e644":"code","709ede88":"code","20300151":"code","690297bd":"code","0157a112":"code","639617ad":"code","8405d6c2":"code","692e007f":"code","10a875ed":"code","f842ed6f":"code","114bc3b6":"code","72bd2726":"code","3584344b":"code","ead8cc77":"markdown","29d249d2":"markdown","b642a408":"markdown","39f8051a":"markdown","b280f18b":"markdown","aeb9187a":"markdown","9226db78":"markdown","63f13be9":"markdown","bbc65d26":"markdown","40cb5bb0":"markdown","3441ef08":"markdown"},"source":{"9b67893f":"# Required installs\n!pip install prince\n\n# Required imports\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None, 'display.max_rows', None, 'display.max_colwidth', -1)  \n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import KMeans\nfrom kmodes import kmodes\n\nfrom sklearn.decomposition import PCA\nimport prince\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5d68c65f":"train_data = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\nfeat_df = train_data.iloc[:,8:-1]\nfeat_df.fillna(0, inplace = True)\nfeat_names = feat_df.columns\nfeat_df.head()","4ad075b0":"# Calculating feature correlation\ncorr_feat_df = feat_df.corr()\ncorr_feat_mtx = corr_feat_df.to_numpy()\nplt.figure()\nplt.imshow(corr_feat_mtx, interpolation='nearest')\nplt.colorbar()\nplt.title('Feature correlation')","90c6e644":"# Determine optimun number of clusters for kmeans\nwcss = []\nmax_num_clusters = 15\nfor i in range(1, max_num_clusters):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(corr_feat_mtx)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1, max_num_clusters), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","709ede88":"# Using kmeans to cluster the features based on their correlation\nn_clusters_kmeans = 5\nkmeans = KMeans(n_clusters = n_clusters_kmeans, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ncorr_feat_labels = kmeans.fit_predict(corr_feat_mtx)\n\n# Preparing a dataframe to collect some cluster stats\ncorr_feat_clust_df = pd.DataFrame(np.c_[feat_names, corr_feat_labels])\ncorr_feat_clust_df.columns = [\"feature\", \"cluster\"]\ncorr_feat_clust_df['feat_list'] = corr_feat_clust_df.groupby([\"cluster\"]).transform(lambda x: ', '.join(x))\ncorr_feat_clust_df = corr_feat_clust_df.groupby([\"cluster\", \"feat_list\"]).size().reset_index(name = 'feat_count')\ncorr_feat_clust_df\n","20300151":"corr_node_dist = kmeans.transform(corr_feat_df)\ncorr_clust_dist = np.c_[feat_names, np.round(corr_node_dist.min(axis=1),3), np.round(corr_node_dist.min(axis=1)\/np.max(corr_node_dist.min(axis=1)),3), corr_feat_labels]\ncorr_clust_dist_df = pd.DataFrame(corr_clust_dist)\ncorr_clust_dist_df.columns = ['feature', 'dist_corr', 'dist_corr_norm', 'cluster_corr']\ncorr_clust_dist_df","690297bd":"# Method to group together in correlation matrix features with same labels\ndef clustering_corr_matrix(corrMatrix, clustered_features):\n    npm = corrMatrix.to_numpy()\n    npm_zero = np.zeros(shape=(len(npm), len(npm)))\n    n = 0\n    for i in clustered_features:\n        m = 0\n        for j in clustered_features:\n            npm_zero[n, m] = npm[i-1, j-1] # TODO: remove the -1 if including again feature 0\n            m += 1\n        n += 1\n    return npm_zero\n\n# Preprocessing the correlation matrix before starting the the clustering based on labels\ndef processing_clusterd_corr_matrix(feat_labels, corrMatrix):\n    \n    lst_lab = list(feat_labels)\n    lst_feat = corrMatrix.columns\n\n    lab_feat_map = {lst_feat[i].replace(\"feature_\" , \"\") : lst_lab[i] for i in range(len(lst_lab))} \n    lab_feat_map_sorted = {k: v for k, v in sorted(lab_feat_map.items(), key=lambda item: item[1])}\n    \n    clustered_features = list(map(int,lab_feat_map_sorted.keys()))\n    print(len(clustered_features))\n    return clustering_corr_matrix(corrMatrix, clustered_features)\n\n# Function to plot the clustered \ndef plot_clustered_matrix(clust_mtx, feat_clust_list):\n    plt.figure()\n    \n    fig, ax = plt.subplots(1)\n    \n    im = ax.imshow(clust_mtx, interpolation='nearest')\n    \n    corner = 0\n    for s in feat_clust_list:\n        rect = patches.Rectangle((float(corner),float(corner)), float(s), float(s), angle=0.0, linewidth=2,edgecolor='r',facecolor='none')\n        ax.add_patch(rect)\n        corner += s\n        ax.add_patch(rect)\n    \n    fig.colorbar(im)\n    \n    plt.title('Clusterd Feature by Correlation')\n    plt.show()    \n    ","0157a112":"# Plotting Clustered Correlation Matrix Heat Map\nclust_mtx = processing_clusterd_corr_matrix(corr_feat_labels, corr_feat_df)\nplot_clustered_matrix(clust_mtx, corr_feat_clust_df['feat_count'].to_numpy())","639617ad":"# Scatter plot of the different centroids along with observations once clusterd\ndef plotting_scatter(n_clusters, centroids, labels_mtx, title):\n    \n    # Size and alpha values \n    obsv_lw, obsv_alp = 2, .9\n    cntr_lw, cntr_apl = 55, .55\n    \n    # Generating cluster names for the legend and colors\n    target_names = ['k'+str(i) for i in range(n_clusters)]\n    colors = colors = cm.rainbow(np.linspace(0, 1, n_clusters))\n    \n    # Printing the centroids\n    for color, i, target_name in zip(colors, range(n_clusters), target_names):\n        plt.scatter(centroids[i, 0], centroids[i, 1], color = color , alpha = cntr_apl,  s = cntr_lw**2)\n    \n    # Printing observation\n    for color, i, target_name in zip(colors, range(n_clusters), target_names):\n        cur_label = labels_mtx[np.where(labels_mtx[:,2] == i)]\n        plt.scatter(cur_label[:, 0], cur_label[:, 1], color = color, alpha = obsv_alp , lw = obsv_lw, label = target_name)\n    \n    plt.legend(loc='best', shadow=False, scatterpoints=1)\n    plt.title(title)\n    plt.figure()","8405d6c2":"# Visualizing the dispersion of each cluster in \"2D\"\npca_2 = PCA(n_components=2)\ncorr_pca = pca_2.fit(corr_feat_df).transform(corr_feat_df)\ncorr_centr_pca = pca_2.fit(kmeans.cluster_centers_).transform(kmeans.cluster_centers_)\n\n# Concatenating the pca values with their labels\ncorr_labels_mtx = np.c_[corr_pca, corr_feat_labels]\nplotting_scatter(n_clusters_kmeans, corr_centr_pca, corr_labels_mtx,'PCA of Clustered Features')","692e007f":"tags_feat_df = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv', skiprows=[1])\ntags_feat_df.replace({False: \"False\", True: \"True\"}, inplace = True)\ntags_feat_df['feature'] = tags_feat_df['feature'].apply(lambda x : x.replace('eature_', ''))\ntags_feat_df.head()","10a875ed":"# Looking for best number of clusters\ncost = []\nmax_clust = 15\nfor num_clusters in list(range(1,max_num_clusters)):\n    kmode = kmodes.KModes(n_clusters = num_clusters, init = \"Cao\", n_init = 5, verbose=1)\n    kmode.fit_predict(tags_feat_df.iloc[:,7:])\n    cost.append(kmode.cost_)","f842ed6f":"plt.plot(range(1, max_num_clusters), cost)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Cost')\nplt.show()","114bc3b6":"n_clusters_kmodes = 4\nkmode_cao = kmodes.KModes(n_clusters = n_clusters_kmodes, init = \"Cao\", n_init = 10, verbose=1)\ntags_feat_labels = kmode_cao.fit_predict(tags_feat_df.iloc[:,7:])\n\n# Preparing a dataframe to collect some cluster stats\ntags_feat_clust_df = pd.DataFrame(np.c_[feat_names, tags_feat_labels])\ntags_feat_clust_df.columns = [\"feature\", \"cluster\"]\ntags_feat_clust_df['feat_list'] = tags_feat_clust_df.groupby([\"cluster\"]).transform(lambda x: ', '.join(x))\ntags_feat_clust_df = tags_feat_clust_df.groupby([\"cluster\", \"feat_list\"]).size().reset_index(name = 'feat_count')\ntags_feat_clust_df","72bd2726":"def nodes_distances(nodes_clustered, centroids):\n    distances = [] \n    for node in nodes_clustered:\n        # Centroid value of the current node\n        centroid = centroids[node[-1]]     \n        distances.append([node[0], node[-1], np.sum(centroid != node[1:-1])])\n    return np.array(distances)\n\n\ntags_nodes_clustered = np.c_[tags_feat_df.to_numpy(), tags_feat_labels]\ntags_nodes_dist = nodes_distances(tags_nodes_clustered, kmode_cao.cluster_centroids_)\n\ntag_clust_dist = np.c_[feat_names, tags_nodes_dist[:,2], tags_feat_labels]\ntag_clust_dist_df = pd.DataFrame(tag_clust_dist)\ntag_clust_dist_df.columns = ['feature', 'dist_tags', 'cluster_tags']\ntag_clust_dist_df","3584344b":"mca = prince.MCA(n_components = 2)\ncentr_kmodes_df = pd.DataFrame(kmode_cao.cluster_centroids_)\n\ntags_mca = mca.fit(tags_feat_df.iloc[:,7:]).transform(tags_feat_df.iloc[:,7:])\ntags_mca_centr = mca.fit(centr_kmodes_df).transform(centr_kmodes_df)\ntags_labels_mtx = np.c_[tags_mca, tags_feat_labels]\nplotting_scatter(n_clusters_kmodes, tags_mca_centr.to_numpy(), tags_labels_mtx, 'MCA for Clustered Features')","ead8cc77":"## 1- Clustering Features based on their Correlation\n\n<u>Steps followed<\/u>:\n\n* 1.1- Calculating the Correlation across the different features.\n* 1.2- Determining the number of clusters\n* 1.3- Clustering the Correlation matrix using Kmeans\n* 1.4- Visualizing features once clustered with a correaltion matrix heatmap\n* 1.5- Visualizing features with a 2D representation after applying PCA on the kmeans results","29d249d2":"### 1.4- Visualizing features once clustered with a correaltion matrix heatmap\n\nThen the correlation matrix was reordered so features belonging to same cluster stay together in the heatmap.","b642a408":"### 2.2- Clustering the Tags Matrix using Kmodes\n\nThe Tags Matrix is clustered using 4 cluster in Kmodes. Below you can see to which cluster each feature has been assign to and the distance of each feature to its cluster centroid","39f8051a":"### 1.3- Clustering the Correlation matrix using Kmeans\n\nThe correlation matrix is clustered using 5 cluster in Kmeans. Below you can see to which cluster each feature has been assign to and the distance of each feature to its cluster centroid","b280f18b":"## 2- Clustering Features based on their Tags\n\n<u>Steps followed<\/u>:\n\n* 2.1- Determining the number of clusters\n* 2.2- Clustering the Tag matrix using Kmodes\n* 2.3- Visualizing features with a 2D representation after applying MCA on the Kmodes results","aeb9187a":"### 2.3- Visualizing features with a 2D representation after applying MCA on the Kmodes results\n\nIt was applied MCA to the feature Clustered Tag Matrix to reduce the number of dimensions to two. The same was applied to the centroids of the cluster. Finally both, features and centroids were plotted together to see the distribution of the different clusters.\n","9226db78":"### 1.2- Determining the number of clusters\n\nBy plotting the cluster cost evolution when increasing the number of clusters we can see the best **number of cluster is 5**","63f13be9":"### 2.1- Determining the number of clusters\nBy using the Cao initialization method it is determined that 4 is the best number of clusters. The first 6 tags are not taken into account since they show some sort of cycle in most of the features. ","bbc65d26":"### 1.1- Calculating the Correlation across the different features\n\nI dropped ```feature_0``` since it seems to be a sign flag. Below there is a heat map of the correlation matrix of the other features.","40cb5bb0":"### 1.5- Visualizing features with a 2D representation after applying PCA on the kmeans results\nIt was applied PCA to the feature correlation matrix to reduce the number of dimensions to two. The same was applied to the centroids of the cluster. Finally both, features and centroids were plotted together to see the distribution of the different clusters.","3441ef08":"## Description\n\nDuring the description of the challenge it is mentioned that the dataset may contain **potential redundancy & strong feature correlation**, so it may be interesting to analyse the relationship among the different features in the dataset by clustering them following different approaches.\n\nFirstly I will cluster the features in the train set based on how they are correlated with each other. Then I will use the feature's metadata to cluster them based on their tags.\n\nSince first approach requires to work with numerical data and second with categorical, I will use the following technologies:\n\n* kmeans\n* kmodes\n* PCA\n* MCA"}}