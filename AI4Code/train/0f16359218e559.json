{"cell_type":{"9fff0b93":"code","0624bbf5":"code","7dc482a7":"code","59804a0f":"code","f3feec9f":"code","f913dbad":"code","ea83118a":"code","bfb2ab99":"code","de3df15d":"code","8eaa29c6":"code","1dd98537":"code","2ffd561d":"markdown","fa768aec":"markdown","4efca8c5":"markdown","661d87f9":"markdown","04f4a088":"markdown"},"source":{"9fff0b93":"!pip install -q efficientnet # Efficientnet not supposed in tensorflow yet\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.linear_model import LogisticRegression\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.models import save_model, Sequential, load_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","0624bbf5":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path(\"plant-pathology-2020-fgvc7\")","7dc482a7":"# Configs\nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIMG_SIZE = 800","59804a0f":"# Show random set of images\ndef show_images_in(path, n=16):\n    files_to_show = os.listdir(path)[:n]\n    assert files_to_show[0].endswith((\".jpg\",\".jpeg\",\".png\"))\n    np.random.shuffle(files_to_show)\n    img_paths = [os.path.join(path,file) for file in files_to_show]\n    plt.figure(figsize=(12,12))\n    for i in range(n):\n        img = plt.imread(img_paths[i])\n        plt.subplot(4,4,i+1)\n        plt.imshow(img)\n    plt.tight_layout()\nshow_images_in(\"..\/input\/plant-pathology-2020-fgvc7\/images\/\",n=16)","f3feec9f":"# Learning rate scheduler\ndef build_lrfn(lr_start=0.00001, lr_max=0.00005, \n               lr_min=0.00001, lr_rampup_epochs=5, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlr_schedule = LearningRateScheduler(lrfn, verbose=1)","f913dbad":"def read_image(filename, label=None, image_size=(IMG_SIZE, IMG_SIZE)):\n    # Read file as binary\n    bits = tf.io.read_file(filename)\n    # Decode binary file into pixel values\n    image = tf.image.decode_jpeg(bits, channels=3)\n    # Scale pixel values into floats in [0,1]\n    image = tf.cast(image, tf.float32) \/ 255.0\n    # Resize image to correct size\n    image = tf.image.resize(image, image_size)\n \n    if label is None: # Used for testing data, label is not given in dataframe because of competition\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    # Data augmentation, nothing fancy here.\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    if label is None:\n        return image\n    else:\n        return image, label","ea83118a":"train = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv')\n\n# Turning filepaths into absolute paths in our dataframe\ntrain_paths = train.image_id.apply(lambda x: GCS_DS_PATH + \"\/images\/\" + x +\".jpg\").values\ntest_paths = test.image_id.apply(lambda x: GCS_DS_PATH + \"\/images\/\" + x +\".jpg\").values\ntrain_labels = train.loc[:, 'healthy':].values","bfb2ab99":"# Creating the dataset objects for TPU feeding\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(read_image, num_parallel_calls=AUTO)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(IMG_SIZE, reshuffle_each_iteration=True)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(read_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(read_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","de3df15d":"arrs = [] # For model ensemebling. After every trained model, we append predictions here\n\n# Train 2 EfficientNetB7's and append predictions to arr\nfor i in range(2):\n    with strategy.scope():\n        model = Sequential()\n        base = efn.EfficientNetB7(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False, weights=\"imagenet\")\n        #base.trainable=False\n        model.add(base)  \n        model.add(Dropout(0.5))\n        model.add(GlobalAvgPool2D())\n        model.add(Dense(4, activation=\"softmax\"))\n        model.compile(optimizer=\"adam\",loss=CategoricalCrossentropy(label_smoothing=0.1), metrics=[\"categorical_accuracy\"])\n        early_stop = EarlyStopping(monitor=\"loss\", patience=8, min_delta=0.01)\n        reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n        H = model.fit(train_dataset, epochs=15,steps_per_epoch=len(train_paths)\/\/BATCH_SIZE, callbacks=[early_stop,lr_schedule])\n        preds = np.array(model.predict(test_dataset))\n        arrs.append(preds)","8eaa29c6":"# Train 2 IncepResNets and append predictions to arr\nfor i in range(2):\n    with strategy.scope():\n        model = Sequential()\n        base = InceptionResNetV2(input_shape=(IMG_SIZE,IMG_SIZE,3), include_top=False, weights=\"imagenet\")\n        model.add(base)\n        model.add(Dropout(0.5))\n        model.add(GlobalAveragePooling2D())\n        model.add(Dense(4, activation=\"softmax\"))\n        model.compile(optimizer=\"adam\",loss=CategoricalCrossentropy(label_smoothing=0.1), metrics=[\"categorical_accuracy\"])\n        early_stop = EarlyStopping(monitor=\"loss\", min_delta=.01, patience=8)\n        H = model.fit(train_dataset, epochs=15,steps_per_epoch=len(train_paths)\/\/BATCH_SIZE, callbacks=[early_stop,lr_schedule])\n        preds = model.predict(test_dataset)\n        # Predictions\n        preds = np.array(model.predict(test_dataset))\n        # Averaging out all predictions then appending to the end of submission dataframe\n        arrs.append(preds)","1dd98537":"# Inference function, averages all predictions then creates a submission dataframe for competition\ndef make_new_preds(arr_list, NUM_OLD_MODELS=1, submission_name=\"new_preds.csv\"):\n    arrs = np.asarray(arr_list)\n    avg = np.sum(arrs, axis=0)\n    old_preds = pd.read_csv(\"..\/input\/incesres\/2IncResNets  2EFNB7.csv\").loc[:,\"healthy\":].values * NUM_OLD_MODELS\n    new_preds = (old_preds + avg) \/ (len(arr_list) + NUM_OLD_MODELS)\n    sub.loc[:,\"healthy\":] = new_preds\n    sub.head()\n    sub.to_csv(submission_name, index=False)\n    \nmake_new_preds(arrs, 4)","2ffd561d":"# Testing","fa768aec":"# Kaggle TPU Activation","4efca8c5":"# Training","661d87f9":"# Helper Functions","04f4a088":"# Dependencies"}}