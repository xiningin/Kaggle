{"cell_type":{"809de5e4":"code","c7fd5055":"code","7f5b1122":"code","05e45c30":"code","6a5f0cad":"code","0e8a9827":"code","a6cb8027":"code","d25f0b3c":"code","15cf3bb3":"code","c761dc16":"code","960e8fa8":"code","aac55eca":"code","33073398":"code","df75e5c8":"code","93c1ad2d":"code","eb4bfd37":"code","95ed1cd1":"code","0b256533":"code","e3fe5176":"code","6d9fde0c":"code","2dc9926a":"code","194ad05f":"code","838a63ab":"code","acc06dee":"code","d024b17c":"code","e4f8ccfe":"code","aa34424c":"code","bb546cbc":"code","6cdaf41e":"code","cfdf366b":"code","86ffdf91":"code","0abb3fd9":"code","cf72c38e":"code","7c17ff16":"code","12e9f0c9":"code","96431d1c":"code","7800c344":"code","c45b27d4":"code","4472b5b8":"code","e9079de0":"code","2d01f48b":"code","2aeb0b72":"code","3227ee45":"code","f0c88291":"code","ebcbd855":"code","534af0c7":"code","e6418b99":"code","55173675":"code","5e5ed054":"code","1ceb174a":"code","a74b04b9":"code","56050583":"code","dc125a87":"code","97d4cb64":"code","42fa84b8":"code","035743d1":"code","4ef546f2":"code","7b4f6e16":"markdown","476db4fa":"markdown","23a46a50":"markdown","256b55c2":"markdown","cd94b023":"markdown","d8c64f1d":"markdown","0120ab2f":"markdown","a3cba095":"markdown","f577eea6":"markdown","8b41c845":"markdown","3eb3aff5":"markdown","76416206":"markdown","b69f7ae3":"markdown","7c3fd5c4":"markdown","a2eb9d72":"markdown","02abd263":"markdown","e2387884":"markdown","7ecac99f":"markdown","96ec054d":"markdown","9e3ffbc3":"markdown","a7a24899":"markdown","ac0a4f9c":"markdown","02d9d42b":"markdown"},"source":{"809de5e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV,KFold,train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import CatBoostEncoder, TargetEncoder\n\nfrom sklearn.linear_model import SGDClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nfrom time import time\nimport datetime\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,roc_curve,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c7fd5055":"#df_id_tr=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n#df_id_ts=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ndf_tran_tr=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ndf_tran_ts=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n#df_sample=pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv\")","7f5b1122":"#Exploring data about the nature and types with memory consumption\ndf_tran_tr.info(verbose=True, null_counts=True, memory_usage='deep')","05e45c30":"df_tran_ts.info(verbose=True, null_counts=True, memory_usage='deep')","6a5f0cad":"#df_id_tr.info()","0e8a9827":"df_tran_tr.describe()","a6cb8027":"#df_id_ts.describe()","d25f0b3c":"# #making a joint dataframe: tran+id\n# df_joint_tr=pd.merge(df_tran_tr,df_id_tr,on='TransactionID')\n# df_joint_ts=pd.merge(df_tran_ts, df_id_ts, on=\"TransactionID\")","15cf3bb3":"%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(18,9))\ntrain_full_num = df_tran_tr.filter(regex='isFraud|TransactionDT|TransactionAmt|dist|C|D')\nsns.heatmap(train_full_num.isnull(), cbar= False)","c761dc16":"%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(18,9))\ntrain_full_num = df_tran_tr.filter(regex='M')\nsns.heatmap(train_full_num.isnull(), cbar= False)","960e8fa8":"# #this can be avoided as most often this gives memory error\n# %matplotlib notebook\n# %matplotlib inline\n# train_full_Vesta = df_tran_tr.filter(regex='V')\n# plt.figure(figsize=(18,9))\n# sns.heatmap(train_full_Vesta.isnull(), cbar= False)","aac55eca":"# %matplotlib notebook\n# %matplotlib inline\n# train_full_id = df_id_tr.filter(regex='id')\n# plt.figure(figsize=(18,9))\n# sns.heatmap(train_full_id.isnull(), cbar= False)","33073398":"#balanced\/imbalanced?\n%matplotlib notebook\n%matplotlib inline\nplt.figure(figsize=(10,5))\nsns.countplot(x='isFraud',data=df_tran_tr)","df75e5c8":"# pd.options.display.max_rows=500\n# df_joint_tr.info()","93c1ad2d":"#checking each feature count relation with fraud\ndf_tran_tr.groupby('ProductCD')['isFraud'].value_counts(normalize=True).unstack()[1].sort_values(ascending=False)","eb4bfd37":"df_tran_tr.groupby('P_emaildomain')['isFraud'].value_counts(normalize=True).unstack().fillna(0)[1].sort_values(ascending=False)","95ed1cd1":"df_tran_tr.groupby('R_emaildomain')['isFraud'].value_counts(normalize=True).unstack().fillna(0)[1].sort_values(ascending=False)","0b256533":"df_tran_tr.groupby('card4')['isFraud'].value_counts(normalize=True).unstack()[1].sort_values(ascending=False)","e3fe5176":"df_tran_tr.groupby('card6')['isFraud'].value_counts(normalize=True).unstack().fillna(0)[1].sort_values(ascending=False)","6d9fde0c":"df_tran_tr.groupby('M8')['isFraud'].value_counts(normalize=True).unstack().dropna()[1].sort_values(ascending=False)","2dc9926a":"df_tran_tr.groupby('M9')['isFraud'].value_counts(normalize=True).unstack().dropna()[1].sort_values(ascending=False)","194ad05f":"df_tran_tr.groupby('M7')['isFraud'].value_counts(normalize=True).unstack().dropna()[1].sort_values(ascending=False)","838a63ab":"%matplotlib notebook\n%matplotlib inline\ndef cor_heat(df):\n    cor=df.corr()\n    plt.figure(figsize=(20,10),dpi=100)\n    sns.heatmap(data=cor,annot=True,square=True,linewidths=0.1,cmap='YlGnBu')\n    plt.title(\"Pearson Co-relation: Heat Map\")\ncor_heat(df_tran_tr.filter(regex='C|isFraud'))","acc06dee":"%matplotlib notebook\n%matplotlib inline\ndef cor_heat(df):\n    cor=df.corr()\n    plt.figure(figsize=(20,7),dpi=100)\n    sns.heatmap(data=cor,annot=True,square=True,linewidths=0.1,cmap='YlGnBu')\n    plt.title(\"Pearson Co-relation: Heat Map\")\ncor_heat(df_tran_tr.filter(regex='Tran|isFraud'))","d024b17c":"pd.set_option('display.max_rows',400)\n#using absolute to figure out features with higher co-relation irrespective of their +\/- value\nabs(df_tran_tr.filter(regex='V|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","e4f8ccfe":"abs(df_tran_tr.filter(regex='D[0-9]|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","aa34424c":"abs(df_tran_tr.filter(regex='C|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)[0:11]","bb546cbc":"abs(df_tran_tr.filter(regex='add|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","6cdaf41e":"abs(df_tran_tr.filter(regex='dist|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","cfdf366b":"abs(df_tran_tr.filter(regex='card1|card2|card3|card5|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","86ffdf91":"#This code will save your memory which will be required to run SVC\/KNN within allocated memory and grid_search\n## This cell is for advanced model building with D features selected from their null counts and correlation values\ndf_tran_tr=df_tran_tr.filter(regex='V|addr|C2|C8|C12|C1|C4|C10|C11|C6|C7|Tran|card1|card3|card4|card5|card6|ProductCD|P_emaildomain|R_emaildomain|isFraud|D[0-9]')\n#dropping these columns due to their very low correlation with target\ndf_tran_tr=df_tran_tr.drop(['D7','D13','D8'],axis=1)\n#df_tran_tr.info(verbose=True, null_counts=True)\ndf_tran_ts=df_tran_ts.filter(regex='V|addr|C2|C8|C12|C1|C4|C10|C11|C6|C7|Tran|card1|card3|card4|card5|card6|ProductCD|P_emaildomain|R_emaildomain|D[0-9]')\ndf_tran_ts=df_tran_ts.drop(['D7','D13','D8'],axis=1)\nprint(\"Train data set shape: {0}\".format(df_tran_tr.shape))\nprint(\"Test data set shape: {0}\".format(df_tran_ts.shape))","0abb3fd9":"# # use it when you don't want auto imputation for missing values\n# #imputing D features missing values with median\n# for p in df_tran_tr.filter(regex='D[0-9]'):\n#     df_tran_tr[p]=df_tran_tr[p].fillna(df_tran_tr[p].median())\n\n# for q in df_tran_ts.filter(regex='D[0-9]'):\n#     df_tran_ts[q]=df_tran_ts[q].fillna(df_tran_ts[q].median())","cf72c38e":"# # use it when you don't want auto imputation for missing values\n# #imputing V features missing values with median\n# for x in df_tran_tr.filter(regex='V'):\n#     df_tran_tr[x]=df_tran_tr[x].fillna(df_tran_tr[x].median())\n\n# for y in df_tran_ts.filter(regex='V'):\n#     df_tran_ts[y]=df_tran_ts[y].fillna(df_tran_ts[y].median())","7c17ff16":"# #checking the co-relation after imputations\n# pd.set_option('display.max_rows',400)\n# abs(df_tran_tr.filter(regex='V|isFraud').fillna(0).corr())['isFraud'].sort_values(ascending=False)","12e9f0c9":"# # use it when you don't want auto imputation for missing values\n# #filling numerical card features with median\n# for a in df_tran_tr.filter(regex='card1|card3|card5'):\n#     df_tran_tr[a]=df_tran_tr[a].fillna(df_tran_tr[a].median())\n\n# for b in df_tran_ts.filter(regex='card1|card3|card5'):\n#     df_tran_ts[b]=df_tran_ts[b].fillna(df_tran_ts[b].median())","96431d1c":"# #filling null C values in test tran dataset with median\n# for c in df_tran_ts.filter(regex='C2|C8|C12|C1|C4|C10|C11|C6|C7'):\n#     df_tran_ts[c]=df_tran_ts[c].fillna(df_tran_ts[c].median())","7800c344":"# # use it when you don't want auto imputation for missing values\n# #filling null addr values with median\n# df_tran_tr.addr1=df_tran_tr.addr1.fillna(df_tran_tr.addr1.median())\n# df_tran_tr.addr2=df_tran_tr.addr2.fillna(df_tran_tr.addr2.median())\n\n# df_tran_ts.addr1=df_tran_ts.addr1.fillna(df_tran_ts.addr1.median())\n# df_tran_ts.addr2=df_tran_ts.addr2.fillna(df_tran_ts.addr2.median())","c45b27d4":"# If you want to R_emaildomain in your model\n#dropping null values for R_emaildomain; this is a very critical indicator to fruad; so have to utilize it\n##df_tran_tr=df_tran_tr.dropna(subset=['R_emaildomain'])\ndf_tran_tr['R_emaildomain']=df_tran_tr['R_emaildomain'].fillna('gmail.com')\n#as we can't drop any row from test data, filling it with mode\ndf_tran_ts['R_emaildomain']=df_tran_ts['R_emaildomain'].fillna('gmail.com')","4472b5b8":"#df_tran_tr=df_tran_tr.dropna(subset=['P_emaildomain'])\ndf_tran_tr['P_emaildomain']=df_tran_tr['P_emaildomain'].fillna('gmail.com')\n#as we can't drop any row from test data, filling it with mode\ndf_tran_ts['P_emaildomain']=df_tran_ts['P_emaildomain'].fillna('gmail.com')","e9079de0":"#checking max present card4 type to fill in null values\ndf_tran_tr.groupby('card4')['isFraud'].value_counts().unstack()\nprint(df_tran_tr.card4.mode())\nprint(df_tran_ts.card4.mode())","2d01f48b":"df_tran_tr.groupby('card6')['isFraud'].value_counts().unstack()\nprint(df_tran_tr.card6.mode())\nprint(df_tran_ts.card6.mode())","2aeb0b72":"df_tran_tr.card4=df_tran_tr.card4.fillna('visa')\ndf_tran_tr.card6=df_tran_tr.card6.fillna('debit')\n\ndf_tran_ts.card4=df_tran_ts.card4.fillna('visa')\ndf_tran_ts.card6=df_tran_ts.card6.fillna('debit')","3227ee45":"# for x in df_tran_tr.filter(regex='M'):\n#     df_tran_tr[x]=df_tran_tr[x].fillna(df_tran_tr[x].value_counts().index[0])\n# for y in df_tran_ts.filter(regex='M'):\n#     df_tran_ts[y]=df_tran_ts[y].fillna(df_tran_ts[y].value_counts().index[0])","f0c88291":"#Label Encoding R_emaildomain\ndf_tran_tr.R_emaildomain=LabelEncoder().fit_transform(df_tran_tr.R_emaildomain)\ndf_tran_ts.R_emaildomain=LabelEncoder().fit_transform(df_tran_ts.R_emaildomain)\n\n#Label Encoding P_emaildomain\ndf_tran_tr.P_emaildomain=LabelEncoder().fit_transform(df_tran_tr.P_emaildomain)\ndf_tran_ts.P_emaildomain=LabelEncoder().fit_transform(df_tran_ts.P_emaildomain)\n\n#Label Encoding ProductCD\ndf_tran_tr.ProductCD=LabelEncoder().fit_transform(df_tran_tr.ProductCD)\ndf_tran_ts.ProductCD=LabelEncoder().fit_transform(df_tran_ts.ProductCD)\n\n#Label encoding card features\ndf_tran_tr.card4=LabelEncoder().fit_transform(df_tran_tr.card4)\ndf_tran_tr.card6=LabelEncoder().fit_transform(df_tran_tr.card6)\ndf_tran_ts.card4=LabelEncoder().fit_transform(df_tran_ts.card4)\ndf_tran_ts.card6=LabelEncoder().fit_transform(df_tran_ts.card6)\n\n\n# for z in df_tran_tr.filter(regex=\"M\"):\n#     df_tran_tr[z]=LabelEncoder().fit_transform(df_tran_tr[z])\n\n# for a in df_tran_ts.filter(regex=\"M\"):\n#     df_tran_ts[a]=LabelEncoder().fit_transform(df_tran_ts[a])","ebcbd855":"cat_features=['R_emaildomain','P_emaildomain','ProductCD','card4','card6']\n\ncbe=CatBoostEncoder(cols=cat_features)\n# X= df_tran_tr.drop(['isFraud'],axis=1)\n# y= df_tran_tr[['isFraud']]\ncbe.fit(df_tran_tr[cat_features],df_tran_tr[['isFraud']])\n\n# #Train & Test Set transforming\ndf_tran_tr=df_tran_tr.join(cbe.transform(df_tran_tr[cat_features]).add_suffix('_target'))\ndf_tran_tr.drop(['R_emaildomain','P_emaildomain','ProductCD','card4','card6'],axis=1,inplace=True)\n\ndf_tran_ts=df_tran_ts.join(cbe.transform(df_tran_ts[cat_features]).add_suffix('_target'))\ndf_tran_ts.drop(['R_emaildomain','P_emaildomain','ProductCD','card4','card6'],axis=1,inplace=True)\n","534af0c7":"#commented out for memory error and directly fed to model below\n#making df ready for model training\n# inp_df=df_tran_tr.drop(['isFraud'],axis=1)\n# out_df=df_tran_tr[['isFraud']]","e6418b99":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","55173675":"#Reducing memory without any data loss\ndf_tran_tr=reduce_mem_usage(df_tran_tr)\ndf_tran_ts=reduce_mem_usage(df_tran_ts)","5e5ed054":"X=df_tran_tr.drop(['isFraud'],axis=1)\ny=df_tran_tr[['isFraud']]\n## as this is an imbalanced problem, we need to stratify the splitting so that training is well distributed\n#no need for LGB hyper model selection\n# X_train, X_test, y_train, y_test = train_test_split(df_tran_tr.drop(['isFraud'],axis=1), df_tran_tr[['isFraud']], test_size=0.2, random_state=0,stratify=df_tran_tr[['isFraud']])","1ceb174a":"# this gives memory error; so I will be trying to iterate manually with parameter values\n# param_test = {'n_estimators':np.arange(2,100,1), \n#               'max_depth':np.arange(3,10,1),\n#               'gamma':np.arange(0,0.5,0.1),\n#               'learning_rate':np.arange(0,0.1,0.01),\n#              'min_child_weight':np.arange(1,10,1)\n#              }\n\n\n# gs=GridSearchCV(estimator=XGBClassifier(),scoring='roc_auc',param_grid=param_test,cv=3)\n# gs.fit(X_train,np.ravel(y_train)) #used ravel to convert to 1-D array\n# print(\"best parameters are: {0} for the best score of {1}\".format(gs.best_params_,gs.best_score_))","a74b04b9":"# #you can try cv using below method\n# params_lgb={'boosting_type':'gbdt',\n#            'objective': 'binary',\n#            'random_state':42}\n# k_fold=10\n# train_data=lgb.Dataset(X_train,label=y_train)\n# validation_data=lgb.Dataset(X_test,label=y_test)\n# time_to_train=time()\n# lgbmc=lgb.cv(params_lgb,train_data,num_boost_round=10000,nfold=k_fold,metrics='auc',\n#              verbose_eval=True, early_stopping_rounds=500)\n# print(\"Training is completed!\")\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - time_to_train))))\n# print('-',30)\n# print(lgbmc.best_score_)\n# print(lgbmc.best_params)\n","56050583":"params_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':42,\n           'metric':'auc'}\nk_fold=10\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=42)\ntraining_start_time = time()\naucs=[]\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X,y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[val_idx], label=y.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n    aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 30)\nprint('Training is completed!.')\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\nprint(clf.best_params_)\nprint('-' * 30)\n","dc125a87":"# def testing_model(my_model):\n#     my_model.fit(X_train,np.ravel(y_train))\n#     my_model_pred=my_model.predict(X_test)\n#     print(\"accuracy score is {0}% and roc \n#     print(classification_report(y_test,my_model_pred))\n#     print(my_model)","97d4cb64":"# xgbc_test=XGBClassifier(gamma=0.5,learning_rate=0.03,max_depth=9,n_estimators=200,min_child_weight=3, reg_alpha=0.005)\n# testing_model(xgbc_test)","42fa84b8":"print(df_tran_tr.shape)\nprint(df_tran_ts.shape)","035743d1":"#saving the model parameters\nparams=clf.best_params_\nbest_iter=clf.best_iteration","4ef546f2":"def model_output(your_model):\n    #training the model with inp and out df\n    your_model.fit(df_tran_tr.drop(['isFraud'],axis=1),np.ravel(df_tran_tr[['isFraud']]))\n    your_model_pred= your_model.predict_proba(df_tran_ts)[:,1]\n    your_model_df= pd.DataFrame({'TransactionID':df_tran_ts['TransactionID'],'isFraud': your_model_pred.astype(float)})\n    your_model_df.to_csv('submission_fraud.csv',index=False)\n\nrdf_model=RandomForestClassifier(warm_start=True)\nxgb_model=XGBClassifier()\nnbg_model=GaussianNB()\nmplc_model=MLPClassifier()\nadb_model= AdaBoostClassifier()\ngbb_model=GradientBoostingClassifier()\nsvc_model= SVC()\nknn_model=KNeighborsClassifier()\nsgd_model=SGDClassifier()\nlgbmc_model= LGBMClassifier(**params,num_boost_round=best_iter)\ncatb_model=CatBoostClassifier(eval_metric = 'AUC')\n\nmodel_output(lgbmc_model)","7b4f6e16":"# Imputing and transforming Categorical features","476db4fa":"### Missing values imputations with most common values","23a46a50":"# Use below cells if you want to manually impute the numeric missing features for models other than XGBoost\/LGBM","256b55c2":"### CatBoost Encoding","cd94b023":"# Hyperparameters Tuning","d8c64f1d":"# Pre-processing & Feature Engineering","0120ab2f":"# Read Me Please!\n### 1. Only necessary cells are kept in code to run with highest submission score and rest and commented. You can try to run your portion of the code uncommenting but be careful of the memory crash :(\n### 2. I could not complete the whole grid_search due to out of memory. So, I had to try manually with parameters in various ranges\n### 3. I've a plan to make the model optimized in terms of memory. If you can add some piece of code to do that, please suggest in the comment box.\n### 4. Hyperparameter tuning is important but trust me that won't improve your Accuracy or AUC substantially. But it's very important to tune the model in case of over fitting and calculating efficiently so that run time is less and we don't suffer from our available resources.\n### 5. The highest submission score 92.1% is a LGMC in default parameters! As this is an imbalanced dataset problem, under\/over sampling may be needed. I've checked the classfication report also and it suggests the same as F1 score is quite low.\n## My goal is to make this Notebook an End to End Pipeline for a Novice ML Engineer to understand what are the steps are taken typically in a classification problem! If you can contribute, you are welcome! :)","a3cba095":"# Co-relating numeric features through Heat Maps","f577eea6":"### Splitting Train & Test Sets","8b41c845":"## Details of XGB classifier hyper parameters\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","3eb3aff5":"## Details of Light GBMC hyperparameters","76416206":"### with lgb.cv which will require model in test data","b69f7ae3":"# Exploratory Data Aanalysis\n## Some Burning Questions to answer before we start building models...\n### 1. What types of data are there? how many categorical & numeric features?\n### 2. how much missing values?\n### 3. is it a balanced or imbalanced classification problem?\n### 4. how are the features related with the labelled classification (output)?\n### 5. should we merge transaction and identity datasets? if yes\/not, how do we proceed?\n### 6. which features to give it a try?\n### 7. to what extent we preprocess the data? how do we deal nulls?\n### 8. how is the data distribution? Skewed? normal dist?","7c3fd5c4":"### Label Encoding categorical features","a2eb9d72":"### train and validation in same iteration with lgb.train","02abd263":"# Visualizing the missing values","e2387884":"# Importing Data sets","7ecac99f":"# Insights so far from EDA to build initial Model...\n### 1. M values are to be ignored for initial model due to very high number of null values and relate to fraud lesser comparatively\n### 2. joining the transaction and identity (on TransactionID) leaves very small dataset (~144k). so, we will drop identity dataset as of now and work only with Transaction Dataset\n### 3. Vxxx features are very good co-relator of fraud.need to maximize their utilization in model building\n### 4. D features need to be considered due to their high correlation [needs imputations though] (top 3 initially); We drop D9,D2 even though they show high correlation due to high null values\n### 5. TransactionDT, TransactionAMT will be considered\n### 6. Top 5 C features (sorted by the co-relation) will be considered as no null values\n### 7. From categorical features ProductCD, card (excluding card2), P_emaildomain (R_emaildomain excluded due to high null values) will be included and need label encoding\n### 8. dist1,dist2 features are excluded due to high null & low co-relation\n### 9. addr1, addr2 features should be included\n### 10. P_emaildoamin has very high correlation and we will drop all the null values of it from main Dataset\n\n","96ec054d":"# Observations and findings after Iterations on Model Score on test data\n### 1. XGBoost and RandomForest give 89.3% and 83.4% accuracy on default parameters run. Seems Ensemble(checked AdaBoost, GradientBoosting) has a good prediction on this data.\n### 2. Missing values for P & R emaildomain can be imputed by mode and missing values for numeric features can be imputed through XGBoost auto imputation (auto imputation gave .03% higher score for me. so I'm following that).\n### 3. we will check if the id dataset can add values to prediction through Heatmap correlation\n### 4. We can try out MLPC, KNN, SVC, NB algos. but ensemble seems to a winner here.\n### 5. are all V rich features needed to be included in the model? which columns can we drop? we can set a certain co-relation score and allow all V features meeting that threshold...\n### 6. tried with 137k train set ( size of R_emaildomain non-null values) and got a score of 86.5% which is less than our auto imputed XGBoost classifier score of 89.6%.","9e3ffbc3":"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html#for-better-accuracy\nhttps:\/\/www.kaggle.com\/nroman\/lgb-single-model-lb-0-9419","a7a24899":"# Which algorithm to tune finally?","ac0a4f9c":"# Testing tuned model in Test data","02d9d42b":"## Categorical features and their co-relation with predictor"}}