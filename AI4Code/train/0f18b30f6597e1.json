{"cell_type":{"bb79b3ca":"code","28fd6827":"code","b6a7fda6":"code","d21fa01f":"code","d77373b0":"code","ccc06051":"code","484cb446":"code","ddb65e5e":"code","b921ee15":"code","2ea2b0c7":"code","7df5961d":"code","a85adc56":"code","f6f63c76":"code","1a3f016d":"code","59c5f8e6":"code","bbfe6905":"code","daa0f9da":"code","f1a4caed":"code","515bd1dd":"code","32ccf804":"code","a5a9bce0":"code","fdafe446":"code","6ed4ad89":"code","bc43b9ac":"code","64758aab":"code","26978833":"code","9b1fe98a":"code","f3c09676":"code","991c5861":"code","00c8f7b0":"markdown","2101755a":"markdown","20756d7e":"markdown","ba71b8af":"markdown","71d3aaa8":"markdown","6ab56e9e":"markdown","8ad1f76e":"markdown","f4309744":"markdown","97177b33":"markdown","333e00eb":"markdown","446953ce":"markdown","20411ae4":"markdown","199871c5":"markdown","3d8ed18c":"markdown","f4d92db2":"markdown","fbd66868":"markdown","2efe1183":"markdown","6d76cff7":"markdown"},"source":{"bb79b3ca":"import warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport itertools\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn import svm","28fd6827":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nsns.set_style('white') ","b6a7fda6":"data = pd.read_csv('..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')\n#stopword_list = [k.strip() for k in open(\"E:\/MaLearning\/souhu\/stopwords.txt\", encoding='utf8').readlines() if k.strip() != '']\nstopword_list = stopwords.words('english')","d21fa01f":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d77373b0":"data.head()","ccc06051":"data[\"Category\"] = data[\"Category\"].map({'ham': 0,'spam':1})","484cb446":"data.head()","ddb65e5e":"description_list = []\nfor article in data[\"Message\"]:\n    article = re.sub(\"[^a-zA-Z]\",\" \",article)\n    article = article.lower()   # low case letter\n    article = word_tokenize(article)\n    lemma = WordNetLemmatizer()\n    article = [ lemma.lemmatize(word) for word in article]\n    article = \" \".join(article)\n    description_list.append(article) #we hide all word one section\n    \n    \ndef text_replace(text):\n    '''some text cleaning method'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","b921ee15":"count_vectorizer = CountVectorizer(max_features = 100, stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\ntokens = count_vectorizer.get_feature_names()","2ea2b0c7":"print(type(sparce_matrix))\nsparce_matrix = pd.DataFrame(sparce_matrix, columns=tokens)\nsparce_matrix.head()","7df5961d":"vectorizer = TfidfVectorizer(max_features = 100)\ntfidfmatrix = vectorizer.fit_transform(description_list)\ncname = vectorizer.get_feature_names()\ntfidfmatrix = pd.DataFrame(tfidfmatrix.toarray(),columns=cname)\ntfidfmatrix.head()","a85adc56":"tfidfmatrix.columns","f6f63c76":"count_vectorizer = CountVectorizer(max_features = 100, stop_words = \"english\",ngram_range=(2, 2),)\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\ntokens = count_vectorizer.get_feature_names()\ngram2 = pd.DataFrame(sparce_matrix, columns=tokens)\ngram2.head()","1a3f016d":"\ny = data.iloc[:,0].values   \nx = sparce_matrix\ntfidfx = tfidfmatrix\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 2019)\ntf_x_train, tf_x_test, tf_y_train, tf_y_test = train_test_split(tfidfmatrix ,y,\n                                                                test_size = 0.3,\n                                                                random_state = 2019)\n\ngm_x_train, gm_x_test, gm_y_train, gm_y_test = train_test_split(gram2 ,y,\n                                                                test_size = 0.3,\n                                                                random_state = 2019)","59c5f8e6":"nb = GaussianNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","bbfe6905":"nb = MultinomialNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","daa0f9da":"nb = BernoulliNB()\nnb.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',nb.score(x_test,y_test))\nnb.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',nb.score(tf_x_test,tf_y_test))\nnb.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',nb.score(gm_x_test,gm_y_test))","f1a4caed":"%%time\nsvmmodel = svm.SVC(kernel='linear', C = 1)\nsvmmodel.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',svmmodel.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',svmmodel.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',svmmodel.score(gm_x_test,gm_y_test))","515bd1dd":"svmmodel = svm.SVC(kernel='linear', C = 1)\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',svmmodel.score(tf_x_test,tf_y_test))","32ccf804":"%%time\nlogit = LogisticRegression(random_state=0, solver='lbfgs')\nlogit.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',logit.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',logit.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',logit.score(gm_x_test,gm_y_test))","a5a9bce0":"%%time\nclf = GradientBoostingClassifier(n_estimators=50)\nclf.fit(x_train, y_train)\nprint('CountVectorizer Accuracy Score',clf.score(x_test,y_test))\nsvmmodel.fit(tf_x_train, tf_y_train)\nprint('TF-IDF Vectorizer Accuracy Score',clf.score(tf_x_test,tf_y_test))\nsvmmodel.fit(gm_x_train, gm_y_train)\nprint('bi-gram Vectorizer Accuracy Score',clf.score(gm_x_test,gm_y_test))","fdafe446":"description_list = []\nfor article in data[\"Message\"]:\n    article = re.sub(\"[^a-zA-Z]\",\" \",article)\n    article = article.lower() \n    cutWords = [k for k in word_tokenize(article) if k not in stopword_list]\n    cutWords = [ lemma.lemmatize(word) for word in cutWords]\n    description_list.append(cutWords)\n#description_list","6ed4ad89":"def getVector_v2(cutWords, word2vec_model):\n    vector_list = [word2vec_model[k] for k in cutWords if k in word2vec_model]\n    vector_df = pd.DataFrame(vector_list)\n    cutWord_vector = vector_df.mean(axis=0).values\n    return cutWord_vector\n\nword2vec_model = Word2Vec(description_list, size=100, iter=10, min_count=20)","bc43b9ac":"vector_list = []\nfor c in description_list:\n    vec = getVector_v2(c, word2vec_model)\n    vector_list.append(vec)","64758aab":"X = pd.DataFrame(vector_list)\nX.shape","26978833":"Y = data[\"Category\"]\nY = pd.DataFrame(Y)\nY.shape","9b1fe98a":"X = X.fillna(X.mean())\nY = Y.dropna()","f3c09676":"train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.3)\nlogistic_model = LogisticRegression()\nlogistic_model.fit(train_X, train_y)\ny_predict = logistic_model.predict(test_X)\n\nprint('CountVectorizer Accuracy Score',accuracy_score(y_test, y_predict))\npd.DataFrame(confusion_matrix(y_test,y_predict))","991c5861":"clf = GradientBoostingClassifier(n_estimators=50)\ngbdt = clf.fit(train_X, train_y)\ny_predict = gbdt.predict(test_X)\nprint('CountVectorizer Accuracy Score',accuracy_score(y_test, y_predict))\npd.DataFrame(confusion_matrix(y_test,y_predict))","00c8f7b0":"## Tf-Idf\n\n Term Frequency-Inverse Document Frequency","2101755a":"# Text Classification","20756d7e":"# Preprocessing","ba71b8af":"See this notebook: <https:\/\/www.kaggle.com\/rikdifos\/bert-test>","71d3aaa8":"# word2vec","6ab56e9e":"# Data Import","8ad1f76e":"## LogisticRegression","f4309744":"## GBDT","97177b33":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\">Data Import<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Define-some-functions\" data-toc-modified-id=\"Define-some-functions-1.1\">Define some functions<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\">Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Feature-Extraction\" data-toc-modified-id=\"Feature-Extraction-3\">Feature Extraction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Word-Count\" data-toc-modified-id=\"Word-Count-3.1\">Word Count<\/a><\/span><\/li><li><span><a href=\"#Tf-Idf\" data-toc-modified-id=\"Tf-Idf-3.2\">Tf-Idf<\/a><\/span><\/li><li><span><a href=\"#N-gram\" data-toc-modified-id=\"N-gram-3.3\">N-gram<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Text-Classification\" data-toc-modified-id=\"Text-Classification-4\">Text Classification<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-4.1\"><a href=\"https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_spam_filtering\" target=\"_blank\">Naive Bayes<\/a><\/a><\/span><\/li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-4.2\">SVM<\/a><\/span><\/li><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-4.3\">LogisticRegression<\/a><\/span><\/li><li><span><a href=\"#GBDT\" data-toc-modified-id=\"GBDT-4.4\">GBDT<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#word2vec\" data-toc-modified-id=\"word2vec-5\">word2vec<\/a><\/span><\/li><li><span><a href=\"#Bert-TensorFlow\" data-toc-modified-id=\"Bert-TensorFlow-6\">Bert-TensorFlow<\/a><\/span><\/li><\/ul><\/div>","333e00eb":"#  Feature Extraction\n\nThere is several ways to extract features from text data, including word count method and tf-idf encoding. Now I will do both of them and compare their effect of predicting.","446953ce":"## Define some functions","20411ae4":"# Bert-TensorFlow","199871c5":"## N-gram ","3d8ed18c":"## [Naive Bayes](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_spam_filtering)\n\nNaive Bayes gives us a baseline accuracy of predicting.","f4d92db2":"<font size=5>SMS Text classification<\/font>","fbd66868":"## SVM","2efe1183":"<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"https:\/\/i.loli.net\/2019\/11\/18\/kdH1gfSlezstUwL.png\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">Word Count Vectorizer<\/div>\n<\/center>","6d76cff7":"## Word Count"}}