{"cell_type":{"501d6b54":"code","b2a4502b":"code","58903fb9":"code","166fb7fb":"code","acb63816":"code","82025829":"code","6865ed52":"code","9af9aa8b":"code","87c03485":"code","a2c0f023":"markdown","42b13ca9":"markdown","198e5ccd":"markdown","f11feb54":"markdown","1624f32d":"markdown","180929f3":"markdown","29dd08a6":"markdown","5938cf6c":"markdown"},"source":{"501d6b54":"import numpy as np \nimport pandas as pd \nimport os\nimport random\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport pickle","b2a4502b":"df_train = pd.read_csv('..\/input\/part-2-preprocessing-feature-engineering\/train_preprocessed.csv')\ndf_test = pd.read_csv('..\/input\/part-2-preprocessing-feature-engineering\/test_preprocessed.csv')\ndf_sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","58903fb9":"df_train.head()","166fb7fb":"df_test.head()","acb63816":"def seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything(seed=42)","82025829":"clf = LogisticRegression()\nprint(clf)\nprint(clf.get_params())","6865ed52":"# function to perform cross-validation\ndef cross_validate(method=None, method_name=None, X=None, y=None):\n    '''\n    - method: KFold, StratifiedKFold, etc\n    - X: training data without labels\n    - y: labels for training data\n    '''\n    if method_name == 'kfold':\n        split_method = method.split(X)\n    elif method_name == 'stratifiedkfold':\n        split_method = method.split(X, y)\n    models = []\n    cv_scores = []\n    for i, (train_index, val_index) in enumerate(split_method):\n        X_train, X_val = X[train_index], X[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n\n        # instantiate and fit model \n        clf = LogisticRegression(random_state=SEED)\n        clf.fit(X_train, y_train)\n\n        # inference on validation set\n        preds = clf.predict(X_val)#clf.predict_proba(X_val)[:, 1] # obtain predictions of positive class\n\n        # evaluate predictions using competition metric (binary accuracy)\n        score = accuracy_score(y_val, preds)\n        cv_scores.append(score)\n        models.append(clf)\n\n        # save model to load during inference later on (optional)\n        with open(f'logistic_regression_{method_name}_fold_{i}.pkl','wb') as f:\n            pickle.dump(clf, f)\n\n        print('---'*30)\n        print(f'Fold {i} accuracy score: {np.round(score, 4)}')\n\n    print('---'*30 + '\\n')\n    print(f'Mean CV accuracy score: {np.round(np.mean(cv_scores), 4)}')","9af9aa8b":"# specify number of folds (suppose 5 folds)\nN_FOLDS = 5 \n# set seed for reproducibility\nSEED = 42 # favourite number lol\n\n# prepare train\/test split\ny = df_train['Survived']\nX = df_train.drop('Survived', axis=1).values\n\n# K-fold cross-validation scheme\nkf = KFold(n_splits=N_FOLDS, shuffle=False)\ncross_validate(method=kf, method_name='kfold', X=X, y=y)","87c03485":"# Stratified K-fold cross-validation scheme\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=False)\ncross_validate(method=skf, method_name='stratifiedkfold', X=X, y=y)","a2c0f023":"# Imports","42b13ca9":"# Finishing Remarks\n\nThanks for reading and I welcome your feedback and suggestions for improvement. The notebook will be updated periodically as well.\n\nHappy Kaggling!\n\n---------------------------------------------------------------------\nMy notebooks in this series can be found in the links below:\n- [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/khoongweihao\/part-1-exploratory-data-analysis-eda)\n- [Preprocessing & Feature Engineering](https:\/\/www.kaggle.com\/khoongweihao\/part-2-preprocessing-feature-engineering)\n- [Model Training & Validation Strategies](https:\/\/www.kaggle.com\/khoongweihao\/part-3-model-training-validation-strategies)\n- [Inference & Post-processing Techniques](https:\/\/www.kaggle.com\/khoongweihao\/part-4-inference-and-post-processing-techniques)\n\nBonus notebooks include adoption of recent research in terms of models, hyperparameter search, etc. They can be found in the links below:\n- Hyperparameter optimization with Optuna\n- TabNet ","198e5ccd":"# Cross-Validation","f11feb54":"# K-Fold Cross-Validation","1624f32d":"# Some Settings\n\nHere, we set the seed for reproducibility purposes, by ensuring the same pseudorandomness is produced for the affected modules.","180929f3":"# Introduction\n\nIn this notebook series, I'll be sharing my typical approach to machine learning (ML) problems (in this case competitions), in an end-to-end ML pipeline starting with the data analyses, to feature engineering, validation strategies, model training and finally inference (with post-processing techniques).\n\nMy notebooks in this series can be found in the links below:\n- [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/khoongweihao\/part-1-exploratory-data-analysis-eda)\n- [Preprocessing & Feature Engineering](https:\/\/www.kaggle.com\/khoongweihao\/part-2-preprocessing-feature-engineering)\n- [Model Training & Validation Strategies](https:\/\/www.kaggle.com\/khoongweihao\/part-3-model-training-validation-strategies)\n- [Inference & Post-processing Techniques](https:\/\/www.kaggle.com\/khoongweihao\/part-4-inference-and-post-processing-techniques)\n\nBonus notebooks include adoption of recent research in terms of models, hyperparameter search, etc. They can be found in the links below:\n- Hyperparameter optimization with Optuna\n- TabNet ","29dd08a6":"# Stratified K-Fold Cross-Validation","5938cf6c":"# The Classification Model\n\nIn this notebook we use the logistic regression model. More complex and 'better' models may be introduced in later versions."}}