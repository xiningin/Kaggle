{"cell_type":{"31b3214f":"code","1a9bd2d6":"code","758263b7":"code","66753fff":"code","011ad8c7":"code","7969f463":"code","92ce764b":"code","b1cfcc43":"code","392eff71":"code","a8f5d9c2":"code","394277a7":"code","fc035061":"code","adbcb829":"code","114effda":"code","cb6e9fe2":"code","cfd84ecb":"code","bb62c34e":"code","9f5a70da":"code","ea194401":"code","1f0f4035":"code","a89ca589":"code","e7763cec":"code","b38dbc20":"code","d652808c":"code","63a17a52":"code","2e3b11e0":"code","bae940b3":"code","c4ae3b32":"code","66d60b0e":"code","fb775fa8":"code","46b80727":"code","fb9e7aa0":"code","8dfb547c":"code","65e67d56":"code","cda40c2c":"code","9724235d":"code","1619b2f3":"code","5217f163":"code","6c7ec5ad":"code","d672ee8a":"code","080b1ab5":"code","21faa8b5":"code","11a28363":"code","d1cdf6ed":"markdown"},"source":{"31b3214f":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns","1a9bd2d6":"data_df = pd.read_csv(\"\/kaggle\/input\/bu-cs542-fall19\/data.csv\")\ndata_df","758263b7":"test_ind = pd.read_csv(\"\/kaggle\/input\/bu-cs542-fall19\/test.csv\")\ntrain_ind = pd.read_csv(\"\/kaggle\/input\/bu-cs542-fall19\/train.csv\")\nval_ind = pd.read_csv(\"\/kaggle\/input\/bu-cs542-fall19\/val.csv\")\nprint(\"Train set: %i, Val set: %i, Test set: %i\" % (len(train_ind), len(val_ind), len(test_ind)))","66753fff":"train_df = data_df.merge(train_ind, on=[\"id\"])\nval_df = data_df.merge(val_ind, on=[\"id\"])\ntest_df = data_df.merge(test_ind, on=[\"id\"])\ntrain_df","011ad8c7":"train_df[\"host_response_time\"].unique()","7969f463":"train_df.info()","92ce764b":"data_missing = train_df.isna()\ndata_missing = data_missing.sum()\ndata_missing = data_missing \/ len(data_df)\nprint(\"Train Missing field rate (sorted):\")\ndata_missing[data_missing > 0.0].sort_values(ascending=False)\n","b1cfcc43":"train_df[[\"square_feet\"]].plot()\ntrain_df[[\"price\"]].plot()\n# plt.scatter(train_df[\"square_feet\"], train_df[\"price\"])","392eff71":"# Unique ID\nprint(len(data_df[\"id\"].unique()) == len(data_df))\nlen(data_df[\"id\"].unique())","a8f5d9c2":"train_df[\"host_response_time\"].value_counts()","394277a7":"train_df.columns","fc035061":"train_df[\"host_since_timestamp\"] = train_df[\"host_since\"].apply(lambda x: datetime.timestamp(datetime.strptime(x, \"%Y-%m-%d\")) if not pd.isnull(x) else 0.0)\ntrain_df[\"first_review_timestamp\"] = train_df[\"first_review\"].apply(lambda x: datetime.timestamp(datetime.strptime(x, \"%Y-%m-%d\")) if not pd.isnull(x) else 0.0)\ntrain_df[\"last_review_timestamp\"] = train_df[\"last_review\"].apply(lambda x: datetime.timestamp(datetime.strptime(x, \"%Y-%m-%d\")) if not pd.isnull(x) else 0.0)","adbcb829":"plt.scatter(train_df[\"id\"], train_df[\"host_since_timestamp\"], s=0.5, alpha=0.4)","114effda":"plt.scatter(train_df[\"id\"], train_df[\"first_review_timestamp\"], s=0.5, alpha=0.4)\n","cb6e9fe2":"plt.scatter(train_df[\"id\"], train_df[\"last_review_timestamp\"], s=0.5, alpha=0.4)","cfd84ecb":"\n# plt.scatter(train_df[\"id\"], train_df[\"last_review_timestamp\"], s=0.5, alpha=0.4)\n\ntrain_df[\"host_response_rate\"] = train_df[\"host_response_rate\"].apply(lambda x: -0.1 if pd.isnull(x) else float(x.replace(\"%\", \"\"))\/100)\ntrain_df[\"host_response_rate\"].unique()","bb62c34e":"plt.scatter(train_df[\"id\"], train_df[\"host_response_rate\"], s=0.8, alpha=0.8)","9f5a70da":"sns.distplot(train_df[\"host_response_rate\"].tolist(), bins=500)\nplt.title(\"Host response rate value-frequency\")\nplt.xlim(-0.25, 1.1)","ea194401":"def process_binary_category(x):\n    if pd.isnull(x):\n        return -1\n    elif x == \"t\":\n        return 1\n    else:\n        return 0\ntrain_df[\"host_is_superhost_encode\"] = train_df[\"host_is_superhost\"].apply(lambda x: process_binary_category(x))\nsns.distplot(train_df[\"host_is_superhost_encode\"].tolist(), bins=20)","1f0f4035":"train_df[\"host_is_superhost\"].value_counts()","a89ca589":"train_df[\"host_listings_count\"] = train_df[\"host_listings_count\"].apply(lambda x: -1 if pd.isnull(x) else int(x))\nsns.distplot(train_df[\"host_listings_count\"].tolist(), bins=30)","e7763cec":"print(max(train_df[\"host_listings_count\"].tolist()))\nprint(min(train_df[\"host_listings_count\"].tolist()))","b38dbc20":"print(train_df[\"host_identity_verified\"].unique())\nprint(train_df[\"host_identity_verified\"].value_counts())\ntrain_df[\"host_identity_verified_encode\"] = train_df[\"host_identity_verified\"].apply(lambda x: process_binary_category(x))\nsns.distplot(train_df[\"host_identity_verified_encode\"].tolist(), bins=30)","d652808c":"lats = train_df[\"latitude\"].tolist()\nlongs = train_df[\"longitude\"].tolist() \nprint(max(lats), min(lats), np.median(lats), np.median(lats)-min(lats))\nprint(max(longs), min(longs), np.median(longs), np.median(longs)-min(longs))\n","63a17a52":"img = plt.imread(\"\/kaggle\/input\/shawn-airbnb-feature-eng\/austin_map5.png\")\nplt.figure(figsize=(60, int(60\/1004*1884)))\nplt.imshow(img, extent=[-98.38806,-97.09717,29.97040,30.56344], alpha=0.3) \nplt.scatter(longs, lats, c=\"b\", s=30, alpha=0.3)\n# scat_plot = sns.scatterplot(lats, longs)\n# scat_plot.imshow(img, extent=[-98.38806,-97.09717,29.97040,30.56344], zorder=0, alpha=0.3)","2e3b11e0":"from matplotlib.colors import LinearSegmentedColormap\n\nncolors = 256\ncolor_array = plt.get_cmap('viridis')(range(ncolors))\n# change alpha values\ncolor_array[:,-1] = np.linspace(1.0,0.0,ncolors)\n# create a colormap object\nmap_object = LinearSegmentedColormap.from_list(name='viridis_alpha',colors=color_array)\n# register this new colormap with matplotlib\nplt.register_cmap(cmap=map_object)","bae940b3":"from scipy.stats.kde import gaussian_kde\n\nimg = plt.imread(\"\/kaggle\/input\/shawn-airbnb-feature-eng\/austin_map5.png\")\n\nk = gaussian_kde(np.vstack([longs, lats]))\nxi, yi = np.mgrid[-98.38806:-97.09717:0.005,29.97040:30.56344:0.005]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n\nfig, axs = plt.subplots(1, 1, figsize=(60, int(60\/1004*1884)))\naxs.imshow(img, extent=[-98.38806,-97.09717,29.97040,30.56344], alpha=0.4) \nh = axs.pcolormesh(xi, yi, zi.reshape(xi.shape), vmin=0, alpha=0.7, cmap=map_object)\n","c4ae3b32":"train_df[\"is_location_exact_encode\"] = train_df[\"is_location_exact\"].apply(lambda x: process_binary_category(x))\nsns.distplot(train_df[\"is_location_exact_encode\"].tolist(), bins=10, kde=False)","66d60b0e":"train_df[\"property_type\"].value_counts()\nlen(train_df[\"property_type\"].unique())\nsns.catplot(y=\"property_type\", kind=\"count\", data=train_df);","fb775fa8":"train_df[\"room_type\"].value_counts()\nlen(train_df[\"room_type\"].unique())\nsns.catplot(y=\"room_type\", kind=\"count\", data=train_df);\n# len(train_df[\"room_type\"].unique())","46b80727":"train_df[\"accommodates\"].value_counts()\nlen(train_df[\"accommodates\"].unique())\nsns.distplot(train_df[\"accommodates\"], bins=50, kde=False)","fb9e7aa0":"train_df[\"bathrooms\"].value_counts()\nlen(train_df[\"bathrooms\"].unique())\nsns.distplot(train_df[\"bathrooms\"], bins=50, kde=False)","8dfb547c":"train_df[\"bedrooms\"].value_counts()\nlen(train_df[\"bedrooms\"].unique())\nsns.distplot(train_df[\"bedrooms\"], bins=50, kde=False)","65e67d56":"train_df[\"beds\"].value_counts()\nlen(train_df[\"beds\"].unique())\nsns.distplot(train_df[\"beds\"], bins=25, kde=False)","cda40c2c":"# The relationship btw `bedrooms` and `beds`\nsns.scatterplot(train_df[\"bedrooms\"], train_df[\"beds\"])","9724235d":"train_df[\"bed_type\"].value_counts()\nlen(train_df[\"bed_type\"].unique())\nsns.catplot(y=\"bed_type\", kind=\"count\", data=train_df);\n# len(train_df[\"room_type\"].unique())","1619b2f3":"import itertools\nfrom collections import Counter\namenities = train_df[\"amenities\"].tolist()#.value_counts()\nl = [[tok.strip() for tok in a.replace(\"\\\"\", \"\")[1:-1].split(\",\") if tok.strip() != \"\"] for a in amenities]\nplt.figure(figsize=(6, 40))\nsns.countplot(y=list(itertools.chain(*l)))\n","5217f163":"train_df[\"square_feet\"] = train_df[\"square_feet\"].apply(lambda x: -1 if pd.isnull(x) else x)\n# train_df[\"square_feet\"].value_counts()\nprint(\"Non-nan entry:\", len(train_df[train_df[\"square_feet\"] > -1][\"square_feet\"]))\nsns.distplot(train_df[train_df[\"square_feet\"] > -1][\"square_feet\"], bins=50, kde=False)","6c7ec5ad":"# train_df[\"number_of_reviews\"].unique()\nprint(max(train_df[\"number_of_reviews\"]))\nsns.distplot(train_df[\"number_of_reviews\"], bins=50, kde=False)","d672ee8a":"print(max(train_df[\"number_of_reviews_ltm\"]))\nsns.distplot(train_df[\"number_of_reviews_ltm\"], bins=50, kde=False)","080b1ab5":"print(min(train_df[\"review_scores_rating\"]))\ntrain_df[\"review_scores_rating\"].unique()\ntrain_df[\"review_scores_rating\"] = train_df[\"review_scores_rating\"].apply(lambda x: -1 if pd.isnull(x) else x)\nsns.distplot(train_df[\"review_scores_rating\"], bins=50, kde=False)","21faa8b5":"scores = [\"review_scores_rating\", \"review_scores_cleanliness\", \"review_scores_checkin\", \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\"]\nsns.distplot(train_df[scores[1:]], color=[\"r\", \"g\", \"b\", \"y\", \"purple\"], label=scores[1:], bins=15, kde=False)\nplt.legend()\ntrain_df[scores[1:]].mean()","11a28363":"train_df[\"price\"].unique()\nprint(max(train_df[\"price\"]))\nprint(min(train_df[\"price\"]))\nprint(max(val_df[\"price\"]))\nprint(min(val_df[\"price\"]))\nsns.distplot(train_df[\"price\"])\n","d1cdf6ed":"## Data Visualization & Feature Anaylysis\nSome visualization code for understanding the dataset better.  \nWelcome to give some feedback on how to further process the features.  \nCheers!\n\n\n#### Feature Description:\n1. `host_since`: datetime, need to transform to timestamp. Assign `min_in_column` timestamp to nan fields\n2. `host_response_time`: categorical(4) 20% nan rate. Need to assign dummy class to hold nan value?\n3. `host_response_rate`: float(0.0, 1.0)  20% nan rate. Need to assign negative value to hold nan value (-0.1)?\n4. `host_is_superhost`: categorical(2) .03% nan rate.\n5. `host_listings_count`: int(1, 1717) .03% nan rate.\n6. `host_identity_verified` categorical(2) .03% nan rate.\n7. `zipcode`: hash_str (geo-encoded)\n8. `latitude` location [need extra feature engineering]\n9. `longitude` location [need extra feature engineering]\n10. `is_location_exact` categorical(2)\n11. `property_type` categorical(34) [sparse and skewness]\n12. `room_type` categorical(4)\n13. `accommodates` int(1, 17)\n14. `bathrooms` double(0, 6) (not pretty sure what 0.5 bathrooms means, shared w\/t someone?)\n15. `bedrooms` int(0, 10) (probably means the amount of private rooms)\n16. `beds` int(0, 24)\n17. `bed_type` categorical(5) [sparse and skewness]\n18. `amenities` unstructured_text [probably need some feature engineering?]\n19. `square_feet` double(0, 3000) 59.5% nan rate!!\n20. `number_of_reviews` int(0, 856) [outlier?]\n21. `number_of_reviews_ltm` int(0, 223)\n22. `first_review` datetime 12.8% nan rate\n23. `last_review` datetime 12.8% nan rate\n24. `review_scores_rating` int(0, 100) 12.8% nan rate\n25. `review_scores_accuracy` int(0, 10)\n26. `review_scores_cleanliness` int(0, 10)\n27. `review_scores_checkin` int(0, 10)\n28. `review_scores_communication` int(0, 10)\n29. `review_scores_location` int(0, 10)\n30. `review_scores_value` int(0, 10)\n31. `price` int(0, 700?)"}}