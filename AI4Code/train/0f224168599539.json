{"cell_type":{"d5c92d61":"code","c008b549":"code","5a6ccb0f":"code","94e23a12":"code","df777e42":"code","e808a500":"code","0e7212e8":"code","16b719d3":"code","6c739f32":"code","f1128175":"code","f8b54718":"code","cffbc00c":"code","bd7c2043":"code","e74c7dea":"code","d4594e76":"code","06272fd8":"code","1143727c":"code","c9d51c5e":"code","8d83d186":"code","83bfe8e9":"code","a3f3e438":"code","941fc523":"code","359c6b0d":"code","d7922163":"code","3fff4f39":"code","048cba6c":"code","253aaf95":"code","681f2d0d":"code","06d26546":"code","8986d949":"code","8097661c":"code","de9eef08":"code","7e043885":"code","1a195bc5":"code","fd424e09":"code","425b5905":"code","b0d90ac3":"code","090f1a55":"code","0db0a0a2":"code","a31f6973":"code","03453298":"code","43469e22":"code","85f9d6c2":"code","104be469":"code","795aa606":"code","f4eb12c8":"code","0152920f":"code","1737b57c":"code","29cc5376":"code","a2ebd9f5":"code","615fb949":"code","cb37bb5e":"code","a96e455a":"code","4594856c":"code","32bc6e62":"code","ba55b036":"code","cde0910f":"code","f70d753c":"code","7e5dfa4e":"code","12140a82":"code","6d28a57c":"code","ff0c197a":"code","bd7aa40d":"code","ea06357e":"code","086bc4a4":"code","b4614f37":"code","1e45bfdf":"code","988ed92f":"code","17665a40":"code","0ade77d5":"code","6a2d955f":"code","d850f9a9":"code","c7c6a3aa":"code","c9bd0725":"code","b42d402a":"code","84e93612":"code","2803cd00":"code","c681c040":"code","7729e072":"code","5b072b48":"code","ceabdfdc":"code","ae028350":"code","44a6d7ed":"code","4bac79ad":"code","a3346d2c":"code","8787c079":"code","a0bd45f1":"code","93d7099d":"markdown","a71d8045":"markdown","207bfd67":"markdown","73723b42":"markdown","7e40c8ee":"markdown","75348761":"markdown","1a57ee0a":"markdown","4ddff1cb":"markdown","15f0d8fd":"markdown","1f386b70":"markdown","ccb92869":"markdown","a28ee147":"markdown","8ef6213d":"markdown","985fcb68":"markdown","6044fa32":"markdown","b9588e04":"markdown","fe7906b1":"markdown","512936bf":"markdown","ec2ea788":"markdown","840dcfbf":"markdown","b5018c9c":"markdown","ab7260e5":"markdown","c24a4f75":"markdown","9da86fbf":"markdown","2f8c6fa7":"markdown","c7218aaa":"markdown","9011b195":"markdown","b9fdb502":"markdown","ffec81b7":"markdown","59c72b6b":"markdown","3c740e02":"markdown","f6d561d1":"markdown","cfa8dee1":"markdown","d193ac38":"markdown","2f5c386d":"markdown","16116478":"markdown","7ab73f4d":"markdown","94013081":"markdown","cc36467b":"markdown"},"source":{"d5c92d61":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport re\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nsns.set(style='darkgrid')\n%matplotlib inline","c008b549":"pd.read_csv('..\/input\/diabetic-data\/diabetic_data.csv').head()","5a6ccb0f":"# csv contains \"?\" for missing values. We replace it with NaN\ndata = pd.read_csv('..\/input\/diabetic-data\/diabetic_data.csv', na_values=[\"?\"])\ndf= data.copy()\ndf.head()","94e23a12":"features = pd.read_csv('..\/input\/diabetic-data-features-description\/features.csv',index_col='Unnamed: 0')\ninfo = lambda attribute:print(f\"{attribute.upper()} : {features[features['Feature']==attribute]['Description'].values[0]}\\n\")\nfeatures.head()","df777e42":"info('encounter_id')","e808a500":"df.duplicated().value_counts()\n# df = df.drop_duplicates()","0e7212e8":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\ndisplay(summary(df).sort_values(by='Nulls', ascending=False))","16b719d3":"df = df.drop(['citoglipton','examide','encounter_id'],axis=1)","6c739f32":"df.gender.value_counts(dropna=False)","f1128175":"gender_index = df[df.gender == 'Unknown\/Invalid'].index\ndf = df.drop(gender_index, axis=0)","f8b54718":"# confirm removal\ndf.gender.value_counts(dropna=False)","cffbc00c":"df.readmitted.value_counts(dropna=False)","bd7c2043":"df = df.replace(['<30', '>30'], 'YES')","e74c7dea":"info('patient_nbr')","d4594e76":"df['patient_nbr'].duplicated().value_counts(dropna=False)","06272fd8":"# total unique patients\nlen(df.patient_nbr), df.patient_nbr.nunique()","1143727c":"# locate number of patient visits using patient_id\ndf.patient_nbr.value_counts()","c9d51c5e":"# keep only one record for each patient, the first visit\ndf = df.drop_duplicates(['patient_nbr'], keep='first')\ndf.shape","8d83d186":"df.patient_nbr.nunique()","83bfe8e9":"df = df.drop('patient_nbr', axis=1)","a3f3e438":" def null_values(df):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)","941fc523":"# columns with missing values\nnull_values(df)","359c6b0d":"for i in ['weight','medical_specialty','payer_code']: info(i)","d7922163":"df = df.drop(['weight','medical_specialty','payer_code'], axis=1)","3fff4f39":"summary(df).sort_values(by='Uniques', ascending=False)[:20]","048cba6c":"for i in ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']: info(i)","253aaf95":"# drop columns\ndrop_cols = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\ndf = df.drop(drop_cols, axis=1)","681f2d0d":"null_values(df)","06d26546":"df.race.value_counts(dropna=False)","8986d949":"df = df.dropna(axis=0, subset=['race'])\nnull_values(df)","8097661c":"for i in ['diag_1', 'diag_2', 'diag_3']: info(i)","de9eef08":"info('number_diagnoses')","7e043885":"df[['diag_1', 'diag_2', 'diag_3','number_diagnoses']][df.diag_1.isnull() & df.diag_2.notnull() & df.diag_3.notnull() & df.number_diagnoses.notnull()]","1a195bc5":"# remove rows where diagnosis 1 is missing\ndf = df.dropna(axis=0, subset=['diag_1'])","fd424e09":"null_values(df)","425b5905":"df[['diag_1','diag_2', 'diag_3','number_diagnoses']][df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))]","b0d90ac3":"# remove rows where diagnosis 2 is missing and number of diagnoses is greater than 1\ndiag_2_indexes = df[df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))].index\ndf = df.drop(index = diag_2_indexes, axis=0)","090f1a55":"null_values(df)","0db0a0a2":"# list of affected rows\ndf[['diag_1','diag_2', 'diag_3', 'number_diagnoses']][df.diag_3.isnull() & (df.number_diagnoses > 2)]","a31f6973":"# remove rows with missing diagnosis 3 and number of diagnoses is greater than 2\ndiag_3_indexes = df[(df.diag_3.isnull()) & (df.number_diagnoses > 2)].index\ndf = df.drop(index=diag_3_indexes, axis=0)","03453298":"null_values(df)","43469e22":"sns.heatmap(df[['diag_1','diag_2', 'diag_3','number_diagnoses']].isnull(),yticklabels=False,cbar=False,cmap='viridis');","85f9d6c2":"# replace NaN with None in diagnosis 2 and 3 to show there is no additional diagnosis\ndf.fillna('None', inplace=True)","104be469":"# confirm there are no more NaN values\nnull_values(df)","795aa606":"summary(df[['diag_1','diag_2', 'diag_3']])","f4eb12c8":"# Circulatory\ncodes =[str(i) for i in list(range(390,460)) + [785]]\ndf = df.replace(codes, 'Circulatory')","0152920f":"# Respiratory\ncodes =[str(i) for i in list(range(460,520)) + [786]]\ndf = df.replace(codes, 'Respiratory')","1737b57c":"# Digestive\ncodes =[str(i) for i in list(range(520,580)) + [787]]\ndf = df.replace(codes, 'Digestive')","29cc5376":"# Diabetes\ndf = df.replace(regex=r'^250.*', value='Diabetes')","a2ebd9f5":"# Injury\ncodes =[str(i) for i in range(800,1000)]\ndf = df.replace(codes, 'Injury')","615fb949":"# Musculoskeletal\ncodes =[str(i) for i in range(710,740)]\ndf = df.replace(codes, 'Musculoskeletal')","cb37bb5e":"# Genitourinary\ncodes =[str(i) for i in list(range(580,630)) + [788]]\ndf = df.replace(codes, 'Genitourinary')","a96e455a":"# Neoplasms\ncodes =[str(i) for i in range(140,240)]\ndf = df.replace(codes, 'Neoplasms')","4594856c":"# Other\ndf = df.replace(regex=r'^[E,V].*', value='Other')\n\ncodes =[str(i) for i in range(0,1000)]\ndf = df.replace(codes, 'Other')","32bc6e62":"df[['diag_1', 'diag_2', 'diag_3']].head()","ba55b036":"# Unique Values of Each Features:\nfor i in df[['diag_1', 'diag_2', 'diag_3']]:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","cde0910f":"# need to add 365.44 to Other\ndf = df.replace('365.44', 'Other') ","f70d753c":"plt.figure(figsize=(20, 8))\nfor diag in ['diag_1','diag_2','diag_3']:\n    sns.lineplot(x=df[diag].value_counts().sort_index().index, y= df[diag].value_counts().sort_index().values, marker='o')\nplt.legend(['diag_1','diag_2','diag_3'])\nplt.show()","7e5dfa4e":"# drop diagnoses 2 and 3\ndf = df.drop(columns=['diag_2', 'diag_3'])","12140a82":"plt.figure(figsize=(8,5))\nax = df.number_diagnoses.value_counts().sort_index().plot.bar()\ndef labels(ax, df=df):\n    for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), \n                        (p.get_x(), p.get_height()+100),size=10)\nlabels(ax)","6d28a57c":"df.number_diagnoses = df.number_diagnoses.replace([10,11,12,13,14,15,16],9)","ff0c197a":"df.describe().T","bd7aa40d":"features = df.describe().columns","ea06357e":"def col_plot(df,col_name):\n    plt.figure(figsize=(15,6))\n    \n    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n    plt.hist(df[col_name], bins = 20)\n    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n    \n    # \u00fc\u00e7 sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n    plt.xlabel(col_name)\n    plt.tight_layout\n    plt.xlabel(\"Histogram \u00b13z\")\n    plt.ylabel(col_name)\n\n    plt.subplot(142)\n    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n    plt.xlabel(\"IQR=1.5\")\n\n    plt.subplot(143)\n    plt.boxplot(df[col_name].apply(f), whis = 2.5)\n    plt.xlabel(\"ROOT SQUARE - IQR=2.5\")\n\n    plt.subplot(144)\n    plt.boxplot(np.log(df[col_name]+0.1), whis = 2.5)\n    plt.xlabel(\"LOGARITMIC - IQR=2.5\")\n    plt.show()","086bc4a4":"for i in features:\n    col_plot(df,i)","b4614f37":"from scipy.stats.mstats import winsorize\n\ndef plot_winsorize(df,col_name,up=0.1,down=0):\n    plt.figure(figsize = (15, 6))\n\n    winsor=winsorize(df[col_name], (down,up))\n    logr=np.log(df[col_name]+0.1)\n\n    plt.subplot(141)\n    plt.hist(winsor, bins = 22)\n    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n    plt.xlabel('Winsorize_Histogram')\n    plt.ylabel(col_name)\n    plt.tight_layout\n\n    plt.subplot(142)\n    plt.boxplot(winsor, whis = 1.5)\n    plt.xlabel('Winsorize - IQR:1.5')\n    \n    plt.subplot(143)\n    plt.hist(logr, bins=22)\n    plt.axvline(x=logr.mean()+3*logr.std(),color='red')\n    plt.axvline(x=logr.mean()-3*logr.std(),color='red')\n    plt.xlabel('Logr_col_name')\n\n    plt.subplot(144)\n    plt.boxplot(logr, whis = 1.5)\n    plt.xlabel(\"Logaritmic - IQR=1.5\")\n    plt.show()    \n","1e45bfdf":"for i in features:\n    plot_winsorize(df,i)","988ed92f":"df_winsorised=df.copy()\nfor i in features:\n    df_winsorised[i]=winsorize(df_winsorised[i], (0,0.1))","17665a40":"df_log=df.copy()\nfor i in features:\n    df_log[i]=np.log(df_log[i])","0ade77d5":"df_root=df.copy()\nf=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\nfor i in features:\n    df_root[i]=df_root[i].apply(f)","6a2d955f":"from numpy import percentile\nfrom scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n    z_scores = zscore(df[col].dropna())\n    threshold_list = []\n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))\/df_outlier.outlier_count*100\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n               colors=\"r\", ls = \":\"\n              )\n    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n                                                                   (np.round(percentile_threshold, 3), \n                                                                    np.round(100-percentile_threshold, 3))), \n                 (best_treshold, df_outlier.outlier_count.max()\/2))\n    #plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, outlier_limit, percentile)","d850f9a9":"from scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n   \n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n    \n    plt.subplot(1,3,1)\n    plt.axvline(x=df[col].mean() + z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.axvline(x=df[col].mean() - z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.show()\n    \n    return z_score_inspect","c7c6a3aa":"def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n    ''' \n    this function detects outliers based on 3 time IQR and\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(p*IQR)\n    lower_limit = first_quartile-(p*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","c9bd0725":"k=3\nprint(f\"Number of Outliers for {k}*IQR\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df, col)[2] > 0:\n        outliers=detect_outliers(df, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","b42d402a":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Root Square\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_root, col)[2] > 0:\n        outliers=detect_outliers(df_root, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","84e93612":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Winsorised\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_winsorised, col)[2] > 0:\n        outliers=detect_outliers(df_winsorised, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","2803cd00":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Logarithmed\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_log, col)[2] > 0:\n        outliers=detect_outliers(df_log, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","c681c040":"z_scores=[]\nfor i in features:\n    z_scores.append(outlier_inspect(df,i)[2])","7729e072":"z_scores","5b072b48":"features","ceabdfdc":"# Alternative 1\n# df_3z=df.copy()\n# for i in features:\n#     down_limit= df_3z[i].mean() - (3*df_3z[i].std())\n#     upper_limit= df_3z[i].mean() + (3*df_3z[i].std())\n#     condition= (df_3z[i] > down_limit) & (df_3z[i] < upper_limit)\n#     df_3z=df_3z[condition]\n\n# print('Number of Outliers:',len(df)-len(df_3z))","ae028350":"# Alternative 2\n\n# create columns for z scores, new column with z score\ndf_3z=df.copy()\n\nfor x in features:\n    df_3z[x + '_z'] = stats.zscore(df_3z[x])\n\nfor x in df_3z.columns[-len(features):]:\n    df_3z = df_3z[(df_3z[x] < 3) & (df_3z[x] > -3)]\n    \n# drop _z columns\ndf_3z = df_3z.drop(columns=df_3z.columns[-8:])\n\nprint('Number of Outliers:',len(df)-len(df_3z))","44a6d7ed":"df_3z.describe().T.round(2)","4bac79ad":"df.describe().T.round(2)","a3346d2c":"summary(df_3z)","8787c079":"df_3z = df_3z.drop(['acetohexamide','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1)","a0bd45f1":"df_3z = df_3z.reset_index(drop=True)\ndf_3z.to_csv('.\/diabetic_data_cleaned.csv')","93d7099d":"### Dropping irrelevant columns","a71d8045":"# General Overview","207bfd67":"Investigate the unique values of each column and look for error entries.","73723b42":"* Since patient_nbr is unique, it is no longer needed.","7e40c8ee":"> Drop the columns that the number of uniques is 1","75348761":"* You can reach the extensive diagnosis description on this website by querying with the ICD9 code:\nhttp:\/\/icd9.chrisendres.com\/","1a57ee0a":"### Outlier Detection","4ddff1cb":"The number of diagnoses column shows the total number of conditions a patient is diagnosed with. Only the first three are recorded, so those that are missing the first diagnosis but still a second or third are in error.","15f0d8fd":"### FOCUS  ON \" patient_nbr \"","1f386b70":"### Export Cleaned Dataset","ccb92869":"Dataset Resources: [UC Irvine's Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008)\n\n[Kaggle Link](https:\/\/www.kaggle.com\/pavan2029\/diabetic-data)\n\n**Objective:**\nHospital readmission rates for certain conditions are now considered an indicator of hospital quality, and also affect the cost of care adversely. Hospital readmissions of diabetic patients are expensive as hospitals face penalties if their readmission rate is higher than expected and reflects the inadequacies in health care system. For these reasons, it is important for the hospitals to improve focus on reducing readmission rates. Identify the key factors that influence readmission for diabetes and to predict the probability of patient readmission. \n\nThe dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.*","a28ee147":"**Group Names**\n\n    1-Circulatory\n    2-Respiratory\n    3-Digestive\n    4-Diabetes\n    5-Injury\n    6-Musculoskeletal\n    7-Genitourinary\n    8-Neoplasms\n    9-Other","8ef6213d":"Now, we are down to three columns with missing information: diagnosis 1, 2, and 3. \n* Diagnosis 1 is described as the primary diagnosis made during the patient's visit while diagnosis 2 is the second and 3 is an any additional diagnoses made after that. \n* Looking at the patients' rows that are missing a primary diagnosis, most of them have a second diagnosis or even a third. \n* Since it doesn't make sense to have a second (or third) but not a primary diagnosis, we will remove these columns from the dataset.","985fcb68":"### FOCUS ON \"readmitted\"","6044fa32":"* Looking at the graph above, we can say that there is a high correlation between the diagnoses. So we drop diag_2 and diag_3.\n* Also since the most common diagnoses are prevalent in all three diagnoses listed, We are only using the primary diagnosis variable to build the machine learning model","b9588e04":"### Import Features Dataset\nDescriptions of the features:\nhttps:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/tab1\/","fe7906b1":"Diagnosis 3 is the last column left with unaccounted missing values. Since some patients have 1 or 2 diagnosed conditions, the diagnosis 3 column is left intentionally blank. The goal here is to remove the rows that have a diagnoses number greater than two.","512936bf":"* The majority of patients do not have a weight listed so this column can be dropped. \n* Medical specialty and payer code are also missing for about half of the patients. \n* We do not need to know how the patients paid for their treatments.\n* we do not have enough information to figure out which medical unit they went to.","ec2ea788":"> We regard the observations of \"Unknown\/Invalid\" gender as null values and drop them.","840dcfbf":"* 'citoglipton' and 'examide' features that the number of uniques is 1 are droped.\n* all values of 'encounter_id' column are unique. It has to be droped.","b5018c9c":"### Analysis of Diagnosis","ab7260e5":"* Since there is no way to know the race of the patient using existing information, the best option is to remove those rows.","c24a4f75":"### FOCUS ON \"`number_diagnoses`\"","9da86fbf":"> Patients readmitted to the hospital within and after 30 days will be combined into one column, because these patients ultimately returned.","2f8c6fa7":"### Import Libraries","c7218aaa":"* For a small number of observations with number of diagnoses greater than 9, let's change the number of diagnoses to 9.","9011b195":"There are two remaining diagnosis columns with missing values. Each number correlates to a specific condition so if there is a missing value, then it is likely that the patient only has one diagnosed condition. The number of diagnoses column lists the total number of diagnosed conditions. When looking at all three diagnosis columns, if the number is one, then diagnosis 2 and 3 can be filled in with a 0 to show that there is no additional diagnosis. If diagnosis 2 or 3 is missing a value and the number of diagnoses is greater than one, then some diagnoses were not recorded and the rows should be removed.","b9fdb502":"> no duplicates detected!","ffec81b7":"### FOCUS ON \"Gender\"","59c72b6b":"* We dont need 'admission_type_id', 'discharge_disposition_id', 'admission_source_id' columns","3c740e02":"* we can think of 'patient_nbr' as the id number of each patient.\n* It turned out that the dataset is the data of 71515 unique patients.\n* Some patients visited the hospital multiple times for treatment so to avoid over-representing any particular individual, only the first encounter with a patient will be used \/ kept in this dataset.","f6d561d1":"Based on the basic statistics describing the dataset, it looks there are outliers that influence skewness in the data. In order to represent the majority of samples and build clean models, we are going to remove outliers that have [z-scores](https:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/z-score\/) greater than 3.0 or less than -3.0. This means that we are removing samples that are more (or less) than 3 times the standard deviation from the mean.","cfa8dee1":"### Handling Missing Values","d193ac38":"### Grouping Diagnosis Codes","2f5c386d":"### Descriptive Analysis","16116478":"### Check Unique Values","7ab73f4d":"* 'diag_1','diag_2' and 'diag_3' columns contain codes for the types of conditions patients are diagnosed with. \n* There are too much unique codes throughout this dataset.\n* We can group the related icd9 diagnosis codes among themselves. In this way, we use categorical group names instead of numerical codes.\n* The grouping is based on the research paper table (https:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/tab2\/)\n![image.png](attachment:image.png)","94013081":"### Check Duplicates","cc36467b":"### Import Dataset"}}