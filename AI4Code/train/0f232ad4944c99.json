{"cell_type":{"6d4f513e":"code","5bfb5b86":"code","291c7af8":"code","214e2f23":"code","19def299":"code","6809c6b3":"code","af6c64f1":"code","636b78d5":"code","e7e3914f":"code","f65e4ead":"code","1d259e61":"code","a47a2781":"code","c018c150":"markdown","1151a122":"markdown","6603ecd4":"markdown","0cde9a8a":"markdown","c957dedb":"markdown","bfeb8691":"markdown","2aca1190":"markdown","189f6f9d":"markdown","551007f6":"markdown","79b64a4a":"markdown","5fc668e7":"markdown","be032f52":"markdown"},"source":{"6d4f513e":"\n# importing essential libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder #encoder\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import OneHotEncoder # encoding to overcome the trap of assigning values according to category number\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\n\ndf = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')    ","5bfb5b86":"encoder = LabelEncoder()\nencoder1 = LabelEncoder()\ndf['status'] = encoder.fit_transform(df['status'])\ndf.groupby('ssc_b').mean()","291c7af8":"df.groupby('gender').mean()","214e2f23":"print(df.groupby('hsc_b').mean()) # grouped by HSC board\ndf.groupby('hsc_s').mean() # grouped by HSC Stream ie Science, comm and arts\n","19def299":"df.groupby('degree_t').mean()","6809c6b3":"df.groupby('specialisation').mean()","af6c64f1":"df.iloc[:,1] = encoder1.fit_transform(df.iloc[:,1])\ndf.iloc[:,3] = encoder1.fit_transform(df.iloc[:,3])\ndf.iloc[:,5] = encoder1.fit_transform(df.iloc[:,5])\ndf.iloc[:,6] = encoder1.fit_transform(df.iloc[:,6])\ndf.iloc[:,8] = encoder1.fit_transform(df.iloc[:,8])\ndf.iloc[:,9] = encoder1.fit_transform(df.iloc[:,9])\ndf.iloc[:,11] = encoder1.fit_transform(df.iloc[:,11])","636b78d5":"df = df.iloc[:,1:]\ndf_chance = df.iloc[:,:13]\nenc = OneHotEncoder()\nenc1 = OneHotEncoder()","e7e3914f":"array1 = df_chance.iloc[:,5].values\narray2 = df_chance.iloc[:,5].values\nenc.fit(array1.reshape(-1,1))\nenc1.fit(array1.reshape(-1,1))\nenc1.categories_\nenc.categories_\narray1 = enc.transform(array1.reshape(-1,1)).toarray()\narray2 = enc.transform(array2.reshape(-1,1)).toarray()\ndf_new = pd.DataFrame(array1)\ndf_chance = df_chance.join(df_new)\ndf_chance = df_chance.drop(columns = 0, axis = 1)\ndf_chance = df_chance.rename(columns={1: 'new_1', 2: \"new_2\"})\ndf_new = pd.DataFrame(array2)\ndf_chance = df_chance.join(df_new)\ndf_chance = df_chance.drop(columns = 0, axis = 1)\ndf_chance = df_chance.drop(columns = 'hsc_s', axis = 1)\ndf_chance = df_chance.drop(columns = 'degree_t', axis = 1)\ndf_chance = df_chance.rename(columns={1: 'new_3', 2: \"new_4\"})\n#df_chance.iloc[:,5] = enc.fit_transform(df_chance.iloc[:,5]).toarray()\nresult = df['status'].values\ndf_chance = df_chance.drop(columns = 'status', axis = 1)\n\nscaler = StandardScaler()\ndf_chance = df_chance.values\ndf_chance[:,[1,3,5,7,9]] = scaler.fit_transform(df_chance[:,[1,3,5,7,9]])","f65e4ead":"clf = SVC(gamma = 'auto')\n\nX_train = df_chance[:172, :]\nX_test = df_chance[172:, :]\ny_train = result[:172]\ny_test = result[172:]\n\nclf.fit(X_train, y_train)\nclf1 = RandomForestClassifier(n_estimators = 200)\nclf1.fit(X_train, y_train)\npreds = clf.predict(X_test)\npred1 = clf1.predict(X_test)\ncm = confusion_matrix(y_test, preds)\ncm1 = confusion_matrix(y_test, pred1)\nprint(cm)\nprint(cm1)","1d259e61":"array2 = df.iloc[:,13].values\n\nresults2 = []\nfactors = []\n\nfor i in range(len(result)):\n    if(result[i] == 1):\n        results2.append(array2[i])\n        factors.append(df_chance[i])\n        \nfactors = np.array(factors)\nresults2 = np.array(results2)\nresults2 = scaler.fit_transform(results2.reshape(-1,1))\n\nX_train = factors[:113, :]\nX_test = factors[113:, :]\ny_train = results2[:113]\ny_test = results2[113:]\nreg = LinearRegression()\nreg.fit(X_train, y_train)\npred_lin = reg.predict(X_test)\n\nregressor = SVR()\nregressor.fit(X_train, y_train)\npred = regressor.predict(X_test)\nscore = mean_squared_error(y_test, pred)\nscore_1 = mean_squared_error(y_test, pred_lin)","a47a2781":"print(score)\nprint(score_1)","c018c150":"We have used two classification methods here. Namely, Random forest classifier and the Support vector classifier. The working of these classifers is very different. While one deals with a network of decision trees, the other deals with a boundary range or support range across the (n-1) dimensional plane for n dimensional data. We will now use the data we initially discarded in order to make regression predictions. Only if a candidate is placed, will he earn money. Thus, we use a for loop and create an array of the candidates who were placed and apply regression model to predict their salaries","1151a122":"We can see that Science and commerce are almost neck and neck in terms of jobs occupied and salary availed while Arts lags behind significatly","6603ecd4":"To summarize, we started with EDA and speculated the parameters that can contribute to the results. We then used classification to understand and predict the possibility of a candidate getting a job. Out of the candidates that might get a job, we also predicted their salaries using a regression model. For regression, we used support vector regression and linear regression","0cde9a8a":"This shows the difference  between the average salaries of males and females. There is a remarkable difference of aboyr 22,700 units of currency. Also a significant 8 percent of difference in placement probabilities . Females have a lower chance of getting a job and even if they do, they are paid less despite scoring more than men almost throughout the educational process.","c957dedb":"These are the mse values of the results that we have obtained. ","bfeb8691":"The differences in degrees and streams is shown above. Science and Tech is clearly the best industry","2aca1190":"We donot require the serial number as it contributes to nothing. We will thus omit the serial number. Also, for now we are only interested in the classification of the candidate getting a job or not. So, we will also omit the regression column with exact salary data. and One Hot Encode the columns with more than two categories.","189f6f9d":"Marketing and Finance is clearly the more rewarding option. \nWe will now encode the data and categorize the descriptive data","551007f6":"We split the train and test sets to carry out the classification process. Intuitively, the process should give a better result than just random guess because of all the EDA that we carried out. The data is certainly insightful","79b64a4a":"We will first encode the required data in order to be able to use it in the initial findings we gather using the data. We will be using the Group By function for this purpose. We will group by the HSC board , SSC Board, the stream chosen and also the gender of the student. The GroupBy results will only be based on the categorical variables. ","5fc668e7":" We clearly observe that the salary difference between Central and Other Boards is not significant. However, Other boards are marginally better than Central Board in terms of Placement percentage and salary","be032f52":"We will now reshape the data and complete the prerequisites . We will scale the values that are continuous to make them normal"}}