{"cell_type":{"1498c7f6":"code","b25fb4e3":"code","9624b23e":"code","44e2b48d":"code","a81e9789":"code","eb272992":"code","774dbc98":"code","067dcb56":"code","3dfb999d":"code","ec08fff3":"code","a11b713b":"code","d7967f12":"code","99cc699c":"markdown","7dc01186":"markdown","a8f69ed3":"markdown","a48d16d7":"markdown","b7b95144":"markdown","8c33de4e":"markdown","4b9d2cc1":"markdown","5c3863c0":"markdown","ce4c0ad9":"markdown","e303c09f":"markdown","2d0effec":"markdown","84bb7431":"markdown","c6b31335":"markdown","babeacf1":"markdown","2f07bad5":"markdown","58eba7bf":"markdown","d13db624":"markdown","02e0f212":"markdown"},"source":{"1498c7f6":"#importing necessary libraries\n\nimport pandas as pd\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndf = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\n\nprint ('Importing and reading done successfully!')","b25fb4e3":"#checking for nan\n\nnan_column_count = 0\nfor column in df.isna().sum():\n    if column>0:\n        print(column)\n        nan_column_count+=1\nif nan_column_count == 0:\n    print('No missing values in your dataset!')\n    \n#checking dtypes and listing features\nprint (df.dtypes)","9624b23e":"#check blueWins distribution\nprint (df.blueWins.value_counts())\n","44e2b48d":"#creating new features\n\ndf['WardPlaceDiff']=df['blueWardsPlaced']-df['redWardsPlaced']\ndf['WardDestroyDiff']=df['blueWardsDestroyed']-df['redWardsDestroyed']\ndf['FirstBloodDiff']=df['blueFirstBlood']-df['redFirstBlood']\ndf['KillDiff']=df['blueKills']-df['redKills']\ndf['DeathDiff']=df['blueDeaths']-df['redDeaths']\ndf['AssistDiff']=df['blueAssists']-df['redAssists']\ndf['EliteMonsterDiff']=df['blueEliteMonsters']-df['redEliteMonsters']\ndf['DragonDiff']=df['blueDragons']-df['redDragons']\ndf['HeraldDiff']=df['blueHeralds']-df['redHeralds']\ndf['TowerDestroyDiff']=df['blueTowersDestroyed']-df['redTowersDestroyed']\ndf['AvgLevelDiff']=df['blueAvgLevel']-df['redAvgLevel']\ndf['MinionsDiff']=df['blueTotalMinionsKilled']-df['redTotalMinionsKilled']\ndf['JungleMinionsDiff']=df['blueTotalJungleMinionsKilled']-df['redTotalJungleMinionsKilled']\ndf['CSdiff']=df['blueCSPerMin']-df['redCSPerMin']\ndf['GPMdiff']=df['blueGoldPerMin']-df['redGoldPerMin']\n\n#selecting relevant features\n\nrelevant=[\n          'blueWins',\n          'WardPlaceDiff',\n          'WardDestroyDiff',\n          'FirstBloodDiff',\n          'KillDiff',\n          'DeathDiff',\n          'AssistDiff',\n          'EliteMonsterDiff',\n          'DragonDiff',\n          'HeraldDiff',\n          'TowerDestroyDiff',\n          'AvgLevelDiff',\n          'MinionsDiff',\n          'JungleMinionsDiff',\n          'blueGoldDiff',\n          'blueExperienceDiff',\n          'CSdiff',\n          'GPMdiff'\n           ]\n\nprint ('Step saved successfully!')","a81e9789":"dados = df[relevant]\n\nscaler2 = MaxAbsScaler()\nscaler2.fit(dados)\nanalisedados=scaler2.transform(dados)\nanalisedf = pd.DataFrame(data=analisedados)\nprint (pd.DataFrame(data=analisedados).groupby(by=0).mean().T)\n","eb272992":"#getting the subset of our elected features and randomizing it using a seed to get reproductable results\n\ndados = df[relevant]\n\ndados_embaralhados=dados.sample(frac=1, random_state = 4234)\n\n#splitting the target column out of the dataframe\n\nx = dados_embaralhados.loc[:,dados_embaralhados.columns!='blueWins'].values\ny = dados_embaralhados.loc[:,dados_embaralhados.columns=='blueWins'].values\n\n#defining our training sample size and splitting our data\n\nq = 7750\n\nx_treino = x[:q,:]\ny_treino = y[:q].ravel()\n\nx_teste = x[q:,:]\ny_teste = y[q:].ravel()\n\n#scaling the features\n\nscaler = MaxAbsScaler()\nscaler.fit(x_treino)\n\nx_treino = scaler.transform(x_treino)\nx_teste = scaler.transform(x_teste)\n\nprint ('Step saved successfully!')","774dbc98":"print ( \"\\n  K TRAINING  TEST\")\nprint ( \" -- ------ ------\")\n\nfor k in range(40,60):\n\n    classificador = KNeighborsClassifier(\n        n_neighbors = k,\n        weights     = 'uniform',\n        p           = 1\n        )\n    classificador = classificador.fit(x_treino,y_treino)\n\n    y_resposta_treino = classificador.predict(x_treino)\n    y_resposta_teste  = classificador.predict(x_teste)\n    \n    acuracia_treino = sum(y_resposta_treino==y_treino)\/len(y_treino)\n    acuracia_teste  = sum(y_resposta_teste ==y_teste) \/len(y_teste)\n    \n    print(\n        \"%3d\"%k,\n        \"%6.1f\" % (100*acuracia_treino),\n        \"%6.1f\" % (100*acuracia_teste)\n        )\n    \n    ","067dcb56":"classificador = KNeighborsClassifier(\n    n_neighbors = 58,\n    weights     = 'uniform',\n     p           = 1\n    )\nclassificador = classificador.fit(x_treino,y_treino)\n\ny_resposta_treino = classificador.predict(x_treino)\ny_resposta_teste  = classificador.predict(x_teste)\n    \nacuracia_treino = sum(y_resposta_treino==y_treino)\/len(y_treino)\nacuracia_teste  = sum(y_resposta_teste ==y_teste) \/len(y_teste)\n    \nprint(\n        \"%3d\"%k,\n        \"%6.1f\" % (100*acuracia_treino),\n        \"%6.1f\" % (100*acuracia_teste)\n        )","3dfb999d":"relevant=['blueWins',\n          # 'WardPlaceDiff',\n          # 'WardDestroyDiff',\n          # 'FirstBloodDiff',\n          'KillDiff',\n          'DeathDiff',\n          # 'AssistDiff',\n          'EliteMonsterDiff',\n          'DragonDiff',\n          # 'HeraldDiff',\n          # 'TowerDestroyDiff',\n          'AvgLevelDiff',\n          'MinionsDiff',\n          #'JungleMinionsDiff',\n          'blueGoldDiff',\n          'blueExperienceDiff',\n          'CSdiff',\n          'GPMdiff'\n          ]\nprint ('Step saved successfully!')","ec08fff3":"#getting the subset of our elected features and randomizing it using a seed to get reproductable results\n\ndados = df[relevant]\n\ndados_embaralhados=dados.sample(frac=1, random_state = 4234)\n\n#splitting the target column out of the dataframe\n\nx = dados_embaralhados.loc[:,dados_embaralhados.columns!='blueWins'].values\ny = dados_embaralhados.loc[:,dados_embaralhados.columns=='blueWins'].values\n\n#defining our training sample size and splitting our data\n\nq = 7750\n\nx_treino = x[:q,:]\ny_treino = y[:q].ravel()\n\nx_teste = x[q:,:]\ny_teste = y[q:].ravel()\n\n#scaling the features\n\nscaler = MaxAbsScaler()\nscaler.fit(x_treino)\n\nx_treino = scaler.transform(x_treino)\nx_teste = scaler.transform(x_teste)\n\nprint ('Step saved successfully!')","a11b713b":"print ( \"\\n  K TRAINING  TEST\")\nprint ( \" -- ------ ------\")\n\nfor k in range(40,60):\n\n    classificador = KNeighborsClassifier(\n        n_neighbors = k,\n        weights     = 'uniform',\n        p           = 1\n        )\n    classificador = classificador.fit(x_treino,y_treino)\n\n    y_resposta_treino = classificador.predict(x_treino)\n    y_resposta_teste  = classificador.predict(x_teste)\n    \n    acuracia_treino = sum(y_resposta_treino==y_treino)\/len(y_treino)\n    acuracia_teste  = sum(y_resposta_teste ==y_teste) \/len(y_teste)\n    \n    print(\n        \"%3d\"%k,\n        \"%6.1f\" % (100*acuracia_treino),\n        \"%6.1f\" % (100*acuracia_teste)\n        )\n    ","d7967f12":"classificador = KNeighborsClassifier(\n    n_neighbors = 48,\n    weights     = 'uniform',\n     p           = 1\n    )\nclassificador = classificador.fit(x_treino,y_treino)\n\ny_resposta_treino = classificador.predict(x_treino)\ny_resposta_teste  = classificador.predict(x_teste)\n    \nacuracia_treino = sum(y_resposta_treino==y_treino)\/len(y_treino)\nacuracia_teste  = sum(y_resposta_teste ==y_teste) \/len(y_teste)\n    \nprint(\n        'accuracy:',\n        \"%6.1f\" % (100*acuracia_teste)\n        )","99cc699c":"League of Legends is a competitive multiplayer online game in which the blue and red teams (composed of 5 players each) rush to destroy each other's base (and Nexus, its central structure and final goal of the game). Each match usually lasts from 20 to 50 minutes. The objective of this study is **to develop a K-nearest-neighbors model to predict, based on game data of the first 10 minutes, whether the winner is the blue team or the red team**. The dataset used contains information from roughly 10000 matches of high-ranked players, and includes statistics extracted at the 10-minute mark of the game plus the final match result.","7dc01186":"Now, with the feature designing out of the way, we have to split the data into our training and testing subsets.\n\nI have chosen to use, for training, a subset of 7750 of the 9879 samples we have (~78%).\n","a8f69ed3":"Now all that's left is to build the classifier itself:\n","a48d16d7":"Brief glossary about the game:\n\n* Warding totem: An item that a player can put on the map to reveal the nearby area. Very useful for map\/objectives control.\n* Minions: NPC that belong to both teams. They give gold when killed by players.\n* Jungle minions: NPC that belong to NO TEAM. They give gold and buffs when killed by players.\n* Elite monsters: Monsters with high hp\/damage that give a massive bonus (gold\/XP\/stats) when killed by a team.\n* Dragons: Elite monster which gives team bonus when killed. The 4th dragon killed by a team gives a massive stats bonus. The 5th dragon (Elder Dragon) offers a huge advantage to the team.\n* Herald: Elite monster which gives stats bonus when killed by the player. It helps to push a lane and destroys structures.\n* Towers: Structures you have to destroy to reach the enemy Nexus. They give gold.\n* Level: Champion level. Start at 1. Max is 18.","b7b95144":"# Considerations\n\nWe have to consider in our analysis some highly relevant aspects of this complex game:\n\n* Most features are highly interdependant: Kills and Elite Monster kills, for example, also give gold and XP. This makes it very hard to isolate each variable's influence to build a model.\n* Very influent human factors: players are susceptible to making decisive bad plays and lose games they should have won.\n* Small time gap analyzed: 10 minutes can be as little as 20% of the total duration of a regular game, so we're extrapolating a lot of info here.\n* Some characters are better at different time stages of the game, creating a certain bias involving exactly which ones were being played at that specific match.\n\nConsidering the factors above and the amount of delicate points that determine the outcome of a match, i would say 75.3% of accuracy using a simple model such as this one is a very decent result. It also reflects a certain level of predictability that reiterates how games at a higher skill level tend to have less room for human mistakes, because each player is playing much closer to their character and situation's maximum potential. It's probably much harder to predict results involving average players because their games involve a lot more random human mistakes, thus being less predictable.\n\nOne very relevant conclusion, however, is that over this statistical approach it becomes really clear that, at the end of the day, wards placed and destroyed at the early game don't make a lot of difference on deciding the game winner. Based on my game knowledge, my guess is that the warding relevance ramps up over the course of the game, because at the start both teams have all the towers, so the players have a lot less space to do vision-based outplays. This, combined with the fact that high-level players, with their experience, can guess very well what is happening without necessarily having vision of the enemy, may justify this fact.\n\nIf you liked this notebook and found it useful in any way, i'd greatly appreciate your upvote :)\n","8c33de4e":"We can see here that this new model with selected features did perform a little better, reaching 75.3% accuracy for k=48!\n\nAs such, we can see that more data does not always equal better models. Sometimes discarding potentially irrelevant or difficult to analyze features can be healthy to the result, not to mention reducing waste of computational power. This is a very important conclusion!\n\nOur final model, therefore, is:","4b9d2cc1":"First we are going to take a look at the dataset:\n\n1) Checking for null values.\n\n2) Checking the features we have and its datatypes.","5c3863c0":"We have no null values AND all our data is numeric, so we don't have to worry about mapping categorical features.\nNow, we should check our target variable to see if it is evenly distributed. If it isn't, we may have some problems regarding bias. However, it's important to notice that the FirstBlood feature has a categorical nature, so we should be careful with this one because it may introduce weird behavior.","ce4c0ad9":"# KNN modeling for LOL match result predictions (75.3% accuracy)","e303c09f":"We will discard the absolute features (such as redKills and blueDeaths) in favor of the \"difference\" features we created. We will also use the \"GoldDiff\" and \"ExperienceDiff\" already included in the dataset, which leaves us with 17 features and the target column (blueWins).","2d0effec":"So we have roughly 50% wins for each team in our dataset, which is very good.\n\nNow, to the feature engineering:\nWe have to select (and create) the most relevant features to be used in our model. League of Legends is a competitive game, and this means that each and every metric available is only meaningful with context: There is no point in analyzing how much gold the blue team has by itself, for example. However, how much **more** gold the blue team has than the red team is most definitely relevant. In that respect, the feature engineering here will focus on creating \"Difference\" features that express the **lead** each team has in the match.","84bb7431":"# Testing with discarded features","c6b31335":"Now, with all features scaled into comparable range, we can observe which of them have higher relation to which team wins by viewing their numerical distance. With this criteria, for example, we're hinted that maybe we could discard the features WardPlaceDiff, WardDestroyDiff, AssistDiff, HeraldDiff, TowerDestroyDiff, JungleMinionsDiff, because their numerical difference is relatively small. Also, it's important to test carefully with the FirstBloodDiff because of its categorical nature. We will test these variations later.","babeacf1":"As the K choice is highly experimental, we will build a testing loop to see which K favors us the most, printing our % accuracy in each instance of the loop:","2f07bad5":"Now, we should try to comment out some of the features and repeat the modeling to see if our model's performance improves. I've done some testing and the best combination i have found is the following:","58eba7bf":"It's also important to:\n\n* Observe the scaled data distribution and try to have some intuition about which features are more important\n* Randomize our dataset order to minimize possible bias.\n* Transform our numerical features to the same scale.\n\n\nThe scaling step is important because KNN modeling uses distance between numerical features as its criteria, and using raw data will naturally make it so larger numbers weigh a lot more even if they are not necessarily more relevant.\n","d13db624":"<img src=\"https:\/\/image.winudf.com\/v2\/image\/Y29tLmxvbHdhbGxwYXBlci5oZC5sb2xwaWN0dXJlcy5waG90b3MuYmFja2dyb3VuZC5jdXRlLmNvb2wuYXJ0LmxvbGltYWdlcy5oZC5mcmVlX3NjcmVlbl8zXzE1MzEyNjgyNDhfMDgx\/screen-3.jpg?fakeurl=1&type=.jpg\"> <\/img>","02e0f212":"That gives us, for the k-range tested, an optimal K-value of 58. That being, our best model yet, with rough accuracy of 74,2%, is:"}}