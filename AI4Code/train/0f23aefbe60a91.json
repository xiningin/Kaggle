{"cell_type":{"d6862370":"code","75f9ed71":"code","2cdcb46a":"code","aca16d32":"code","428f9896":"code","12d28c1b":"code","1cc4a859":"code","a6be280d":"code","d0df03f4":"markdown","659c4595":"markdown","ea8e9bee":"markdown"},"source":{"d6862370":"import pandas as pd\n\ncreditcard = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","75f9ed71":"X_train = creditcard.drop(\"Class\", axis=1).values[:int(0.8 * len(creditcard))]\ny_train = creditcard[\"Class\"].values[:int(0.8 * len(creditcard))]\nX_test = creditcard.drop(\"Class\", axis=1).values[int(0.8 * len(creditcard)):]\ny_test = creditcard[\"Class\"].values[int(0.8 * len(creditcard)):]","2cdcb46a":"import numpy as np\n\n\nclass StandardScaler:\n    def __init__(self):\n        self.u = 0\n        self.s = 0\n    \n    def fit(self, X):\n        self.u = np.mean(X, axis=0)\n        self.s = np.std(X, axis=0)\n    \n    def transform(self, X):\n        return (X - self.u) \/ self.s","aca16d32":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","428f9896":"import numpy as np\n\n\nclass NeuralNetwork:\n    def __init__(self, layer_dims=np.array([1]), learning_rate=1.0, num_iterations=1000):\n        self.layer_dims = layer_dims\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n\n    def feedforward(self, X):\n        def sigmoid(z):\n            return 1 \/ (1 + np.exp(-z)), z\n\n        def relu(z):\n            return np.maximum(0, z), z\n\n        def linear_forward(a, W, b):\n            return np.dot(W, a) + b, (a, W, b)\n\n        def linear_activation_forward(a_prev, W, b, activation):\n            z, linear_cache = linear_forward(a_prev, W, b)\n\n            if activation == \"sigmoid\":\n                a, activation_cache = sigmoid(z)\n\n                return a, (linear_cache, activation_cache)\n            elif activation == \"relu\":\n                a, activation_cache = relu(z)\n\n                return a, (linear_cache, activation_cache)\n\n        caches = []\n        L = len(self.parameters) \/\/ 2\n\n        a = X\n        for l in range(1, L):\n            a_prev = a\n            a, cache = linear_activation_forward(a_prev, self.parameters['W' + str(l)], self.parameters['b' + str(l)],\n                                                 \"relu\")\n            caches.append(cache)\n\n        aL, cache = linear_activation_forward(a, self.parameters['W' + str(L)], self.parameters['b' + str(L)],\n                                              \"sigmoid\")\n        caches.append(cache)\n\n        return aL, caches\n\n    def fit(self, X, y):\n        def initialize_parameters():\n            parameters = {}\n            L = len(self.layer_dims)\n\n            for l in range(1, L):\n                parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l - 1]) * 0.01\n                parameters['b' + str(l)] = np.zeros((self.layer_dims[l], 1))\n\n            return parameters\n\n        def compute_cost(aL):\n            return - np.sum(np.multiply(np.log(aL), self.y) + np.multiply(np.log(1 - aL), 1 - self.y)) \/ self.m\n\n        def backprop(aL, caches):\n            def sigmoid_backward(da, cache):\n                s = 1 \/ (1 + np.exp(-cache))\n\n                return da * s * (1 - s)\n\n            def relu_backward(da, cache):\n                dz = np.array(da, copy=True)\n                dz[cache <= 0] = 0\n\n                return dz\n\n            def linear_backward(dz, cache):\n                a_prev, W, b = cache\n\n                return np.dot(W.T, dz), 1 \/ self.m * (np.dot(dz, a_prev.T)), 1 \/ self.m * (\n                    np.sum(dz, axis=1, keepdims=True))\n\n            def linear_activation_backward(da, cache, activation):\n                linear_cache, activation_cache = cache\n\n                if activation == \"relu\":\n                    return linear_backward(relu_backward(da, activation_cache), linear_cache)\n                elif activation == \"sigmoid\":\n                    return linear_backward(sigmoid_backward(da, activation_cache), linear_cache)\n\n            grads = {}\n            L = len(caches)\n            daL = - (np.divide(self.y, aL) - np.divide(1 - self.y, 1 - aL))\n\n            current_cache = caches[L - 1]\n            grads[\"da\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(daL,\n                                                                                                              current_cache,\n                                                                                                              \"sigmoid\")\n            for l in reversed(range(L - 1)):\n                current_cache = caches[l]\n                grads[\"da\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = linear_activation_backward(\n                    grads[\"da\" + str(l + 1)], current_cache, \"relu\")\n\n            return grads\n\n        def update_parameters(grads):\n            L = len(self.parameters) \/\/ 2\n\n            for l in range(L):\n                self.parameters[\"W\" + str(l + 1)] -= self.learning_rate * grads[\"dW\" + str(l + 1)]\n                self.parameters[\"b\" + str(l + 1)] -= self.learning_rate * grads[\"db\" + str(l + 1)]\n\n        self.X = X.T\n        self.m = self.X.shape[1]\n        self.y = y.reshape(1, self.m)\n        self.parameters = initialize_parameters()\n        self.costs = []\n\n        for i in range(self.num_iterations):\n            aL, caches = self.feedforward(self.X)\n            cost = compute_cost(aL)\n            grads = backprop(aL, caches)\n            update_parameters(grads)\n            self.costs.append(cost)\n\n    def predict(self, X):\n        aL, caches = self.feedforward(X.T)\n\n        return aL > 0.5","12d28c1b":"nn_clf = NeuralNetwork(layer_dims=np.array([30, 1]), learning_rate=1, num_iterations=100)\nnn_clf.fit(X_train, y_train)\ny_pred = nn_clf.predict(X_test)","1cc4a859":"accuracy_score = np.mean(y_pred == y_test)\naccuracy_score","a6be280d":"import matplotlib.pyplot as plt\n\nplt.plot(nn_clf.costs)\nplt.title('Gradient Descent')\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost J')\nplt.show()","d0df03f4":"# Credit Card","659c4595":"# Normalize Features","ea8e9bee":"# Neural Network"}}