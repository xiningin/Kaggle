{"cell_type":{"3d76e9e0":"code","6f50d3a6":"code","24f22d3a":"code","fad1b37f":"code","0cda40e9":"code","8909ed74":"code","8745f2f8":"markdown","8cd5faef":"markdown","e832e23b":"markdown","bd8ca64f":"markdown"},"source":{"3d76e9e0":"!pip install lm-dataformat #Library for easy retrieval of The pile data from jsonl.zstd files\n!pip install transformers #Library for easy interaction with EleutherAI\/gpt-neo-125M model","6f50d3a6":"from transformers import GPTNeoForCausalLM, GPT2Tokenizer\nmodel = GPTNeoForCausalLM.from_pretrained(\"EleutherAI\/gpt-neo-125M\").cuda()\ntokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI\/gpt-neo-125M\")","24f22d3a":"import lm_dataformat\npile = lm_dataformat.Reader('..\/input\/the-pile-train-00-dataset\/the-eye.eu\/public\/AI\/pile\/train\/29.jsonl.zst')","fad1b37f":"docsiter = iter(pile.stream_data())\ndocs = []\nfrom tqdm import tqdm \nwith tqdm(total=10000, position=0, leave=True) as pbar:\n    for i in range(10000):\n        docs.append(next(docsiter))\n        pbar.update()","0cda40e9":"import numpy as np\nimport torch\nacc = [[],[],[]]\nfrom sklearn.metrics import accuracy_score\nwith tqdm(total=10000, position=0, leave=True) as pbar:\n    for idx in range(10000):\n        doc = docs[idx]\n        input_ids = tokenizer(doc, return_tensors=\"pt\").input_ids\n        \n        if(input_ids.shape[1] < 40):\n            acc[0].append(idx)\n            acc[1].append(input_ids.shape[1])\n            acc[2].append(np.nan) #Input tokens are less than 40, append nan as logit\n            continue\n        \n        length = 20 #Length of subsequent tokens, the tokens utilized to calculate number of correctly predicted tokens\n        \n        gen_tokens = model.generate(input_ids[:,:20].cuda(), do_sample=True, temperature=0.1,pad_token_id=50256,eos_token_id=50256,\n                                    min_length=20+length,max_length=40+length).cpu()[0] #Generating tokens from the model\n        \n        score = torch.sum(input_ids[0,20:length+20] == gen_tokens[20:length+20]).item() #Calculating the number of correctly predicted tokens \n        \n        \n        #Appending the document index, length of input tokens and the score to the logit array, @param acc\n        acc[0].append(idx)\n        acc[1].append(input_ids.shape[1])\n        acc[2].append(score)\n        \n        idx+=1 #Temporary index, keeps count of current iteration\n        pbar.update()","8909ed74":"import joblib\njoblib.dump(acc,'data29.pkl') #Saving @param acc","8745f2f8":"* LM_Dataformat allows for retrieval of data by providing an iterator method \"stream_data\"","8cd5faef":"# About This Notebook\n* This notebook evaluates the number of tokens generated by the model EleutherAI\/gpt-neo-125M with The Pile data\n* The Evaluation is done by the following methodology\n\n* Select documents within THE PILE 29 dataset with more than or equal to 40 tokens, replace the skipped logits with np.nan\n* Utilize the first 20 tokens of each document as input to the model and use the subsequent 20 tokens to calculate the number of correctly predicted tokens\n* Correctly predicted tokens are found by comparing the tokens in range 20..40 of the input document with the tokens in range 20--40 of generated document\n* This data is stored in the array acc and saved in the file 'data29.pkl'","e832e23b":"* Creating the @param model using Huggingface api","bd8ca64f":"* Creating the array @param docs, which contains the first 10,000 documents of THE PILE dataset"}}