{"cell_type":{"e706b748":"code","b0d3f981":"code","dd471a4b":"code","a0b51a93":"code","98593d69":"code","8542c4ff":"code","da9090aa":"code","ed44643a":"code","95773f90":"code","f4b86aa4":"code","8f2f97da":"code","7953d42d":"code","ba7a57cc":"code","deb3ef79":"code","3d321d81":"code","4a637109":"code","88c94ddb":"code","0b9a691f":"code","e18d416b":"code","326425aa":"code","d22c510d":"markdown","23b5f84a":"markdown","78a76335":"markdown","54bb1522":"markdown","c34e9d1c":"markdown","baced247":"markdown","f4f13e29":"markdown","a79e2d6d":"markdown","2bbb2b91":"markdown","baf10247":"markdown"},"source":{"e706b748":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0d3f981":"# libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dd471a4b":"# Reading the dataset\ndata_train = pd.read_csv(\"\/kaggle\/input\/letterrecognition-using-svm\/letter-recognition.csv\")\ndata_train.head()","a0b51a93":"data_train.info()","98593d69":"# dimension of dataset\ndata_train.shape","8542c4ff":"# columns of dataset\nprint(data_train.columns)","da9090aa":"# printing the letter in correct sequence\nsequence = list(np.sort(data_train['letter'].unique()))\nprint(sequence)","ed44643a":"# getting mean of columns for each alphabet\ndata_train_mean = data_train.groupby('letter').mean()\ndata_train_mean.head()","95773f90":"X = data_train.drop(['letter'],axis = 1)\ny = data_train['letter']","f4b86aa4":"# Scaling\nfrom sklearn.preprocessing import scale\nX = scale(X)","8f2f97da":"from sklearn.model_selection import train_test_split\n\n# Spliting the dataset into train-test\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,test_size=0.3,random_state=101)","7953d42d":"from sklearn.svm import SVC\n\n# Building a linear SVM model\nlinear_model = SVC(kernel='linear')\nlinear_model.fit(X_train,y_train)\n\ny_pred = linear_model.predict(X_test)","ba7a57cc":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\n# accuracy and confusion matrix\nprint(\"accuracy :\", metrics.accuracy_score(y_true = y_test,y_pred = y_pred), \"\\n\")\nprint(\"confusion_matrix :\", metrics.confusion_matrix(y_true = y_test,y_pred = y_pred))","deb3ef79":"#Building a non-linear SVM model\nnon_linear_model = SVC(kernel = 'rbf')\nnon_linear_model.fit(X_train,y_train)\n\n# Predict\ny_pred = non_linear_model.predict(X_test)","3d321d81":"# accuracy and confusion matrix for non-linear SVM model\nprint(\"accuracy :\", metrics.accuracy_score(y_true = y_test,y_pred = y_pred), \"\\n\")\nprint(\"confusion_matrix :\", metrics.confusion_matrix(y_true = y_test,y_pred = y_pred))","4a637109":"# hypertuning\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  \n","88c94ddb":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","0b9a691f":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n","e18d416b":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","326425aa":"# Building and Evaluating a final model\n# model with optimal hyperparameters\n\n# model\nmodel = SVC(C=1000, gamma=0.01, kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")\nprint(metrics.confusion_matrix(y_test, y_pred), \"\\n\")","d22c510d":"SVM is the supervised learning method which can be used for both classification and regression challenges.Mostly it is used for classification problem.\nThe main function of SVM is to check the hyperplane between two classes is distinguish between two classes.\n\nThere are mainly three kernal functions represent typical decision boundaries:\n1. linear kernal \n2. Polynomial kernal\n3. Radial basis function kernal","23b5f84a":"# 4. Hypertuning using Grid-Search","78a76335":"Hyperparameters tuning :\nFor given problem statement we have multiple hyperparameters to optimise -\n1. Selection of Kernel(linear, rbf)\n2. C\n3. Gamma","54bb1522":" **Observations -** \n 1. When  the value of Gamma is low then accuracy is also quite low.\n 2. Gamma value increases, accuracy also increases.\n 3. Accordingly we will be select the C- values.","c34e9d1c":"# 1. Reading, Understanding and Visualising the dataset","baced247":"# 3. Training a Model","f4f13e29":"# 5. Building and Evaluating a final model with optimal C and gamma value","a79e2d6d":"Hence optimal value of Gamma and C are 0.01 and 1000 respectively.Therefore building a model with optimal value give the accuracy score of 0.956.","2bbb2b91":"# 2. Data Preparation","baf10247":"1. Accuracy Score by linear SVM - 85.2\n2. Accuracy Score by Non linear SVM -93.8"}}