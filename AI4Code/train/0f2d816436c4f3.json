{"cell_type":{"87a7db74":"code","4f393398":"code","97b5e775":"code","e3e47131":"code","e7786f60":"code","4b6cc12a":"code","41e892f2":"code","88d6e4f5":"code","eae653c7":"code","26759815":"code","89d8a4c0":"code","f5398511":"code","f1afce21":"code","f1875978":"code","32403d39":"code","089dcd82":"code","05d0e40a":"code","d07313d2":"code","eebf731f":"code","dec6dcf7":"code","0b5f4607":"code","486f2e1f":"code","163b13ea":"code","7c8eb34f":"code","fc5a3f70":"code","38be2238":"code","baeb2c5a":"markdown","36cb7bf7":"markdown"},"source":{"87a7db74":"# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, VerboseCallback, DeltaXStopper\nfrom skopt.space import Real, Categorical, Integer\nfrom time import time\n\n# Metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import make_scorer","4f393398":"import pandas as pd\npd.set_option('display.max_columns', 500)\nimport time\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score","97b5e775":"train = pd.read_csv('\/kaggle\/input\/cascade-cup-debye\/Cascade_cup\/train_age_dataset.csv')\ntest = pd.read_csv('\/kaggle\/input\/cascade-cup-debye\/Cascade_cup\/test_age_dataset.csv')\nsample = pd.read_csv('\/kaggle\/input\/cascade-cup-debye\/Cascade_cup\/sample_submission.csv')","e3e47131":"train","e7786f60":"train = pd.get_dummies(train, columns=['tier','gender'])\ntest = pd.get_dummies(test, columns=['tier','gender'])","4b6cc12a":"train['age_group'] = train['age_group']-1","41e892f2":"feature_cols = [col for col in train.columns.tolist() if col not in ['age_group']]\ntarget_cols = ['age_group']","88d6e4f5":"# KFOLD\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n\nfor i, (trn, val) in enumerate(skf.split(train[target_cols], train[target_cols])):\n    train.loc[val, 'kfold'] = i\ntrain['kfold'] = train['kfold'].astype(int)","eae653c7":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time.time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    print(best_params)\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time.time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  best_score_std))    \n    print('Best parameters:')\n    print(best_params)\n    print()\n    return best_params","26759815":"train['age_group'].value_counts()","89d8a4c0":"f1 = make_scorer(f1_score, average='weighted', greater_is_better=True)\n# Defining your search space\nsearch_spaces = {'iterations': Integer(10, 1000),\n                 'depth': Integer(1, 8),\n                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(1, 255),\n                 'l2_leaf_reg': Integer(2, 30)}","f5398511":"1\/train['age_group'].value_counts(normalize=True)","f1afce21":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(task_type='GPU',       \n                         od_type = 'Iter',\n                         verbose= False,\n                         classes_count = 4,\n                         class_weights = [1.58, 8.05, 8.09, 8.23],\n                        )","f1875978":"# Setting up BayesSearchCV\nopt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=f1,\n                    cv=skf,\n                    n_iter=100,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=42)","32403d39":"#best_params = report_perf(opt, train[feature_cols], train[target_cols],'CatBoost', \n#                           callbacks=[VerboseCallback(100),DeadlineStopper(60*10)])","089dcd82":"best_params = {'bagging_temperature': 0.6170793585023163,\n              'border_count':198,\n              'depth':4,\n              'iterations':876,\n              'l2_leaf_reg':19,\n              'learning_rate':0.15461088645165447,\n              'random_strength':0.0030745042451505583}","05d0e40a":"import catboost\n\nclass TripleCat():\n    def __init__(self):\n        self.mod1 = catboost.CatBoostClassifier(**best_params,\n                                            task_type='GPU', \n                                            classes_count = 4,\n                                             class_weights = [1.58, 8.05, 8.09, 8.23],\n                                            verbose=0,)\n        self.mod2 = catboost.CatBoostClassifier(**best_params,\n                                            task_type='GPU', \n                                            classes_count = 4,\n                                             class_weights = [1.58, 8.05, 8.09, 8.23],\n                                            verbose=0,)\n        self.mod3 = catboost.CatBoostClassifier(**best_params,\n                                            task_type='GPU', \n                                            classes_count = 4,\n                                             class_weights = [1.58, 8.05, 8.09, 8.23],\n                                            verbose=0,)\n        \n    def fit(self, x, y):\n        self.mod1.fit(x, y)\n        self.mod2.fit(x, y)\n        self.mod3.fit(x, y)\n        \n    def predict_proba(self, x):\n        pred1 = self.mod1.predict_proba(x)\n        pred2 = self.mod2.predict_proba(x)\n        pred3 = self.mod3.predict_proba(x)\n        \n        return (pred1+pred2+pred3)\/3\n    \n    def predict(self, x):\n        pred1 = self.mod1.predict_proba(x)\n        pred2 = self.mod2.predict_proba(x)\n        pred3 = self.mod3.predict_proba(x)\n        \n        pred = (pred1+pred2+pred3)\/3\n        \n        pred_classes = np.argmax(pred, axis=1)\n        return pred_classes","d07313d2":"import catboost\n\ndef run_training():\n    oof = np.zeros((train.shape[0], 4))\n    pred = np.zeros((test.shape[0], 4))\n    \n    for fold in range(5):\n        print(f\"\\nStarting FOLD: {fold}\")\n        start = time.time()\n        \n\n        trn_idx = train['kfold'] != fold\n        val_idx = train['kfold'] == fold\n        trn = train.loc[trn_idx, :]\n        val = train.loc[val_idx, :]\n\n        xtr, ytr = trn[feature_cols].values, trn[target_cols].values\n        xval, yval = val[feature_cols].values, val[target_cols].values\n        xtest = test[feature_cols].values\n\n        #model = xgb.XGBClassifier(random_state=42, tree_method='gpu_hist')\n        #model = catboost.CatBoostClassifier(**best_params,\n        #                                    task_type='GPU', \n        #                                    classes_count = 4,\n        #                                     class_weights = [1.58, 8.05, 8.09, 8.23],\n        #                                    verbose=0,)\n        model = catboost.CatBoostClassifier(task_type='GPU',\n                                           verbose=0)\n                                            \n                                          \n        print(\"Fittting model ...\")\n        model.fit(xtr, ytr.reshape(-1,))\n        \n        print(\"Calculating training preds ...\")\n        train_preds = model.predict(xtr)\n        print(\"Calculating validation preds ...\")\n        val_preds = model.predict(xval)\n        print(\"Training Accuracy Score - \", accuracy_score(ytr, train_preds ))\n        print(\"Training F1 Score - \", f1_score(ytr, train_preds, average='weighted'))\n        \n        print(\"Validation Accuracy Score - \", accuracy_score(yval, val_preds))\n        print(\"Validation F1 Score - \", f1_score(yval, val_preds, average='weighted'))\n        \n        oof[val_idx, :] += model.predict_proba(xval)\n        \n        # Test preds\n        pred += model.predict_proba(xtest)\/5\n        \n        print(f\"FOLD {fold} completed in {time.time()-start} seconds\")\n        \n    return oof, pred","eebf731f":"oof, pred = run_training()","dec6dcf7":"def find_oof_score(oof):\n    predictions = oof.argmax(axis=1)\n    print(\"OOF Accuracy Score - \", accuracy_score(train[target_cols], predictions))\n    print(\"OOF F1 Score - \", f1_score(train[target_cols], predictions, average='weighted'))","0b5f4607":"find_oof_score(oof)\n\n# xgboost without onehot - 0.7432\n# xgboost with tier and gender ohencoded - 0.7434\n\n# catboost - 0.7458 (with class weights)\n# 0.7459","486f2e1f":"np.save('oof_cat.npy', oof)\nnp.save('pred_cat.npy', pred)","163b13ea":"final_preds = pred.argmax(axis=1)+1","7c8eb34f":"pred_csv = pd.DataFrame(final_preds.reshape(-1), columns=['prediction'] )\npred_csv","fc5a3f70":"pred_csv.value_counts()","38be2238":"pred_csv.to_csv('Submission.csv', index=False)","baeb2c5a":"Catboost documentation\n\nhttps:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html","36cb7bf7":"# REMEMBER TO ADD 1 IN THE END"}}