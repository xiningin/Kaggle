{"cell_type":{"f39b5464":"code","a3b90f43":"code","01fa5917":"code","a32f5cfb":"code","5f3efb5a":"code","a7a59a95":"code","999e8af5":"code","a77232cd":"code","d86ebe19":"code","d91d3064":"code","41ecadf1":"code","66991afa":"code","d464efcc":"code","5d55ec16":"code","714cacd0":"code","c9c804a5":"code","e94107aa":"code","7cad18b3":"code","1e17a792":"code","b2a2a485":"code","bde61662":"code","b7dba2ab":"code","12ee0d29":"code","88910e86":"code","9e2cd0e5":"code","c73fd766":"code","6ffb12fc":"code","91e57408":"code","66cd1855":"code","610d5ca2":"code","eb81a8ad":"code","9320a18a":"code","ad2bba49":"code","3d01ddb1":"code","2822c0a6":"code","b8cf0276":"code","59724f85":"code","41b126b4":"code","8f6c80f8":"code","ad85801a":"code","b11fc49a":"code","ffc1fee1":"code","eef032b1":"code","f34c1320":"code","9c2c50a3":"code","78352097":"code","b2c9208c":"markdown","78fc5337":"markdown","bb908a7e":"markdown","9dbc1222":"markdown","1ef9ac43":"markdown","4ad2b2aa":"markdown","b50a9c68":"markdown","f8be9ca1":"markdown","eefbe68a":"markdown","332a8c1f":"markdown","2272b31e":"markdown","e2fbdb69":"markdown","c791880b":"markdown","16dd1d73":"markdown","b1686d90":"markdown","4f1f65f3":"markdown"},"source":{"f39b5464":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3b90f43":"df_cancer = pd.read_csv(\"..\/input\/breast-cancer.csv\")","01fa5917":"df_cancer.head()","a32f5cfb":"df_cancer.loc[:,'diagnosis'] = df_cancer.diagnosis.map({'M':1, 'B':0})","5f3efb5a":"df_cancer.head()","a7a59a95":"df_cancer.tail()","999e8af5":"df_cancer.shape","a77232cd":"df_cancer.isnull().sum()","d86ebe19":"sns.pairplot(df_cancer, hue = 'diagnosis', vars = ['radius_mean', 'texture_mean', 'area_mean', 'perimeter_mean', 'smoothness_mean'] )","d91d3064":"sns.countplot(df_cancer['diagnosis'], label = \"Count\") ","41ecadf1":"sns.scatterplot(x = 'area_mean', y = 'smoothness_mean', hue = 'diagnosis', data = df_cancer)","66991afa":"sns.lmplot('area_mean', 'smoothness_mean', hue ='diagnosis', data = df_cancer, fit_reg=False)","d464efcc":"fig = sns.FacetGrid(df_cancer, hue=\"diagnosis\",aspect=4)\n\n# Next use map to plot all the possible kdeplots for the 'Age' column by the hue choice\nfig.map(sns.kdeplot,'smoothness_mean',shade= True)\n\n# Set the x max limit by the oldest passenger\noldest = df_cancer['smoothness_mean'].max()\n\n#Since we know no one can be negative years old set the x lower limit at 0\nfig.set(xlim=(0,oldest))\n\n#Finally add a legend\nfig.add_legend()","5d55ec16":"fig = sns.FacetGrid(df_cancer, hue=\"diagnosis\",aspect=4)\n\n# Next use map to plot all the possible kdeplots for the 'Age' column by the hue choice\nfig.map(sns.kdeplot,'texture_mean',shade= True)\n\n# Set the x max limit by the oldest passenger\noldest = df_cancer['texture_mean'].max()\n\n#Since we know no one can be negative years old set the x lower limit at 0\nfig.set(xlim=(0,oldest))\n\n#Finally add a legend\nfig.add_legend()","714cacd0":"fig = sns.FacetGrid(df_cancer, hue=\"diagnosis\",aspect=4)\n\n# Next use map to plot all the possible kdeplots for the 'Age' column by the hue choice\nfig.map(sns.kdeplot,'area_mean',shade= True)\n\n# Set the x max limit by the oldest passenger\noldest = df_cancer['area_mean'].max()\n\n#Since we know no one can be negative years old set the x lower limit at 0\nfig.set(xlim=(0,oldest))\n\n#Finally add a legend\nfig.add_legend()","c9c804a5":"sns.factorplot('texture_mean','area_mean',hue='diagnosis',data=df_cancer)","e94107aa":"sns.scatterplot('concavity_se', 'radius_mean', hue ='diagnosis', data = df_cancer)\n","7cad18b3":"sns.scatterplot('compactness_se', 'radius_mean', hue ='diagnosis', data = df_cancer)","1e17a792":"plt.figure(figsize=(24,12)) \nsns.heatmap(df_cancer.corr(), annot=True) ","b2a2a485":"unwantedcolumnlist=[\"diagnosis\",\"Unnamed: 32\",\"id\"]","bde61662":"X = df_cancer.drop(unwantedcolumnlist,axis=1)","b7dba2ab":"y = df_cancer['diagnosis']","12ee0d29":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=5)","88910e86":"from sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)","9e2cd0e5":"y_predict = svc_model.predict(X_test)\ncm = confusion_matrix(y_test, y_predict)\ncm","c73fd766":"sns.heatmap(cm, annot=True)","6ffb12fc":"print(classification_report(y_test, y_predict))","91e57408":"min_train = X_train.min()\nmin_train","66cd1855":"range_train = (X_train - min_train).max()\nrange_train","610d5ca2":"X_train_scaled = (X_train - min_train)\/range_train","eb81a8ad":"X_train_scaled.head()","9320a18a":"sns.scatterplot(x = X_train['area_mean'], y = X_train['smoothness_mean'], hue = y_train)","ad2bba49":"sns.scatterplot(x = X_train_scaled['area_mean'], y = X_train_scaled['smoothness_mean'], hue = y_train)","3d01ddb1":"min_test = X_test.min()\nrange_test = (X_test - min_test).max()\nX_test_scaled = (X_test - min_test)\/range_test","2822c0a6":"from sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train_scaled, y_train)","b8cf0276":"y_predict = svc_model.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_predict)\n\nsns.heatmap(cm,annot=True,fmt=\"d\")","59724f85":"print(classification_report(y_test,y_predict))","41b126b4":"param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} ","8f6c80f8":"from sklearn.model_selection import GridSearchCV","ad85801a":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)","b11fc49a":"grid.fit(X_train_scaled,y_train)","ffc1fee1":"grid.best_params_","eef032b1":"grid.best_estimator_","f34c1320":"grid_predictions = grid.predict(X_test_scaled)","9c2c50a3":"cm = confusion_matrix(y_test, grid_predictions)\nsns.heatmap(cm,annot=True,fmt=\"d\")","78352097":"print(classification_report(y_test,grid_predictions))","b2c9208c":"**As we have seen there are no null entries except Unnamed:32 so we will delete it later before training **","78fc5337":"**Checking if there are any nulls in any column**","bb908a7e":"I beleive for this problem **Support Vector Machines** are good classification algorithm for this problem.\nNow , lets know <br>\n**What is a Support Vector and what is SVM?**\nA Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are more commonly used in classification problems and as such, this is what we will focus on in this post.\n\nSVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, as shown in the image below\n<img src=\"https:\/\/image.ibb.co\/g0rmWp\/svm1.png\" alt=\"class\">\n\nSupport vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.\n\n**What is a hyperplane?**\n \nAs a simple example, for a classification task with only two features (like the image above), you can think of a hyperplane as a line that linearly separates and classifies a set of data.\n\nIntuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it.\n\nSo when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n\n**How do we find the right hyperplane?**\n \nOr, in other words, how do we best segregate the two classes within the data?\n\nThe distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly.\n<img src=\"https:\/\/image.ibb.co\/mU6Lrp\/svm2.png\" alt=\"class\">","9dbc1222":"Apart from visualization. But if you want to  see numbers and stats, then there are other ways to find out how data correlates.\n\nPearson\u2019s Correlation Coefficient helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson\u2019s Correlation Coefficient can be between -1 to +1.\n\n1 means that they are highly correlated and 0 means no correlation. -1 means that there is a negative correlation. Think of it as an inverse proportion.\n\nThe t-test is a correlation coefficient testing for any correlation between two values.\n\nOther popular correlation coefficients include\n\n**Spearman rank order correlation**\n**Pearson\u2019s Rank Correlation.**\n\nThe importance of data correlation has an effect when you have a dataset with many features. It\u2019s tempting to think that a larger number of features will help a model make better predictions. But that\u2019s incorrect.\n\nIf you try to train a model on a set of features with no or very little correlation, you will get inaccurate results.","1ef9ac43":"**Checking the correlation among different features and target variable diagnosis**","4ad2b2aa":"**Just looking at the tail or last 5 entries to know the distribution of data **","b50a9c68":"As we have seen Unnamed: 32 is a column with full of NAN and some of the other features such as id is also non contributing feature in cancer prediction so we will be dropping those features before training the model.","f8be9ca1":"**Prediction of Breast Cancer : Benign or Malignant**\n\nTo Predict if the cancer diagnosis is benign or malignant based on several observations\/features\n30 features are used, examples:\n\n  - radius (mean of distances from center to points on the perimeter)\n  - texture (standard deviation of gray-scale values)\n  - perimeter\n  - area\n  - smoothness (local variation in radius lengths)\n  - compactness (perimeter^2 \/ area - 1.0)\n  - concavity (severity of concave portions of the contour)\n  - concave points (number of concave portions of the contour)\n  - symmetry \n  - fractal dimension (\"coastline approximation\" - 1)\nDatasets are linearly separable using all 30 input features\n\nNumber of Instances: 569\nClass Distribution: 212 Malignant, 357 Benign\nTarget class:\n   - Malignant\n   - Benign","eefbe68a":"Looking at the Visualization of important features in relation to target variable diagnosis to see on which features it is more related","332a8c1f":"**Reference **  <br>\n1.   https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)\n2.  https:\/\/www.kdnuggets.com\/2016\/07\/support-vector-machines-simple-explanation.html","2272b31e":"<img src=\"https:\/\/image.ibb.co\/kxSX49\/c1PNG.png\" alt=\"class\">","e2fbdb69":"**Now, we will split training and testing dataset using sklearn to X_train, X_test,y_train,y_test**","c791880b":"**Means there are 569 rows and 33 columns**","16dd1d73":"That's Great we have achieved 97% accuracy and precision to detect malignant or benign breast cancer.","b1686d90":"**Improving the model using** **GridSearchCV** <br>\nwhat is **GridSearch ?** <br>\nGrid Search is an algorithm with the help of which we can tune hyper-parameters of a model. We pass the hyper-parameters to tune, the possible values for each hyper-parameter and a performance metric as input to the grid search algorithm. The algorithm will then place all the possible hyper-parameter combination in a grid and then find the performance of the model for each combination against some cross-validation set. Then it outputs the hyper-parameter combination that gives the best result.\n\nGridSearch is generally used when you are not sure of good values for a parameter. We might have a range of parameter values that you think would work out for the model and you want to test which one of them to use. In that situation it becomes tedious for us to train the model again and again with different parameters and in that situation we use gridSearch.\n\nLet us suppose that : To solve the optimization problem for a fixed set of values of hyperparameters \u03b1 and \u03b2 gives us a value of w. Since the optimal value of w (call it w\u2217) is a function of \u03b1 and \u03b2, we can write it as follows:\n\n**w\u2217(\u03b1,\u03b2)=argminwP(w,\u03b1,\u03b2,Strain)**\n\nNow we use this w\u2217 to predict on the validation sample to get validation error. We can view this scenario in terms of a \"validation error function\": the function takes as inputs the hyperparameters \u03b1 and \u03b2, and returns the validation error corresponding to w\u2217(\u03b1,\u03b2).\n\nSo the goal of hyperparameter optimization is to find the set of values of \u03b1 and \u03b2, that minimize this validation error function.\n\nSo, in Grid Search technique, it picks a bunch of values of \u03b1 -- (\u03b11,\u03b12,\u2026) and a bunch of values of \u03b2 -- (\u03b21,\u03b22,\u2026) and for each pair of values, evaluates the validation error function. Then pick the pair that gives the minimum value of the validation error function.\n\nThe pairs (\u03b11,\u03b21),(\u03b11,\u03b22),\u2026,(\u03b12,\u03b21),(\u03b12,\u03b22),\u2026 when plotted in space look like a grid, hence the name.\n<img src=\"https:\/\/image.ibb.co\/gJFfSU\/grid123.png\" alt=\"claass\" border=\"0\">","4f1f65f3":"**Mapping Diagnosis variable which is our Target variable to 0,1 : 1 for Malignant 0: Benign**"}}