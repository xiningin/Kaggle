{"cell_type":{"000e42a1":"code","8ff06f12":"code","a4d8bd63":"code","9d23285c":"code","0b1fb1b9":"code","7e6045bd":"code","079881f9":"code","56495d63":"code","97f97eb0":"code","ce528f33":"code","09b58bfb":"code","b75f15ee":"code","2dde376b":"code","e1eecfd6":"code","42bc0f95":"code","96269e9e":"code","9718230c":"code","436fa347":"code","dd3b1454":"code","ceb7ff73":"code","74e03178":"code","ec7fd041":"code","67240077":"code","c0d792ce":"code","4c0b4ddb":"code","ed4e85c7":"code","9b723954":"code","7aa54326":"code","a122caa8":"code","a4d9a90d":"markdown","5b72da85":"markdown","d5313409":"markdown","969c35c6":"markdown","332dff77":"markdown","bf6c0caf":"markdown","b71b63a1":"markdown","0b849ed7":"markdown","d10e2e23":"markdown"},"source":{"000e42a1":"# Exploratory Data Analysis","8ff06f12":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport os\nimport random\nfrom tqdm import tqdm\nimport cv2 as cv\nimport PIL\nfrom PIL import Image\n!pip install plotly\nimport plotly.express as px\nfrom IPython import display\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","a4d8bd63":"base_dir = '..\/input\/bored-apes-yacht-club\/'\nos.listdir(base_dir)","9d23285c":"# data_dir = '..\/input\/cryptopunks\/txn_history-2021-10-07.jsonl'\nimage_dir = \"..\/input\/bored-apes-yacht-club\/bayc\"\nimage_root = \"..\/input\/bored-apes-yacht-club\"","0b1fb1b9":"# df = pd.read_json(base_dir + 'txn_history-2021-10-07.jsonl', lines=True)\n# df.head()","7e6045bd":"#look at 100 samples\n\n#define number of rows and cols\nno_plots = 10*10\n\n#define path\nimages = glob.glob(\"..\/input\/bored-apes-yacht-club\/bayc\/*.png\")\n\nplt.rcParams['figure.figsize'] = (30, 30)\nplt.subplots_adjust(wspace=0, hspace=0)\n\nprint(\"Sample 100 Bored Apes\")\nfor idx,image in enumerate(images[:no_plots]):\n    sample_img = cv.imread(image)\n    plt.subplot(10, 10, idx+1)\n    plt.axis('off')\n    plt.imshow(cv.cvtColor(sample_img,cv.COLOR_BGR2RGB)) #covert color space\nplt.show()","079881f9":"#display tensor image\ndef tensor_imshow(img, dnorm=True):\n    img = img.to('cpu')\n    npimg = img.detach().numpy()\n    if dnorm:\n        npimg = npimg*0.5+0.5\n    plt.figure(figsize=(3, 3))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","56495d63":"def get_dataloader(batch_size,           #batch size during training\n                   image_size,           #spatial size of training images\n                   data_dir=image_dir,   #root directory for dataset\n                   num_workers=3):       #number of sub-processes\n    \n    stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \n    \n    #create transformer to transform images\n    transform = transforms.Compose([transforms.Resize((image_size, image_size)),  #resize\n                                    transforms.ToTensor(),                        #convert to tensor\n                                    transforms.Normalize(*stats)])                #normalize to be between -1 and 1\n    \n    #create the dataset\n    dataset = datasets.ImageFolder(root=data_dir,\n                                   transform=transform)\n    \n    #create the dataloader\n    data_loader = torch.utils.data.DataLoader(dataset,\n                                              batch_size=batch_size,\n                                              shuffle=True,\n                                              num_workers=num_workers,\n                                              pin_memory=True)\n    \n    return data_loader","97f97eb0":"#test dataloader\nbatch_size, image_size = 5, 24\n\ntrain_loader = get_dataloader(batch_size,\n                              image_size,\n                              image_root)\n\ndataiter = iter(train_loader) #dataloader is an iterator\n\nimg,_ = next(dataiter)\nsample_img = img[-1]\n\n#display tensor image\ntensor_imshow(sample_img)","ce528f33":"class Generator(nn.Module):       #signals neural network\n    def __init__(self, \n                 z_dim=100,      #noise vector\n                 im_chan=3,      #color chanel, 3 for red green blue\n                 hidden_dim=64): #spatial size of feature map (conv)\n        \n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        self.im_chan = im_chan\n        self.hidden_dim = hidden_dim\n        \n        self.generator_cnn = nn.Sequential(self.make_gen_block(z_dim, hidden_dim*8, stride=1, padding=0),   \n                                           #(64*8) x 4 x 4\n                                           self.make_gen_block(hidden_dim*8, hidden_dim*4),                           \n                                           #(64*4) x 8 x 8\n                                           self.make_gen_block(hidden_dim*4, hidden_dim*2),                           \n                                           #(64*2) x 16 x 16\n                                           self.make_gen_block(hidden_dim*2, hidden_dim),                             \n                                           #(64) x 32 x 32\n                                           self.make_gen_block(hidden_dim, im_chan, final_layer=True))\n    \n    def make_gen_block(self, \n                       im_chan,     #image dimension\n                       op_chan,     #output dimension\n                       kernel_size=4, \n                       stride=2, \n                       padding=1, \n                       final_layer=False): \n        \n        layers = []\n        #de-convolutional layer\n        layers.append(nn.ConvTranspose2d(im_chan,     \n                                         op_chan, \n                                         kernel_size, \n                                         stride, \n                                         padding, \n                                         bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2))\n        else:\n            layers.append(nn.Tanh())\n        \n        return nn.Sequential(*layers)\n    \n    def forward(self,noise):\n        x = noise.view(-1,self.z_dim,1,1)\n        return self.generator_cnn(x)\n\n    def get_noise(n_samples, \n                  z_dim, \n                  device='cpu'):\n        return torch.randn(n_samples, \n                           z_dim, \n                           device=device)","09b58bfb":"#test Generator\nnoise = Generator.get_noise(n_samples=5,\n                            z_dim=100)\n\ng = Generator(z_dim=100,\n              im_chan=3,\n              hidden_dim=64)","b75f15ee":"print(g)","2dde376b":"class Discriminator(nn.Module):\n    def __init__(self, \n                 im_chan=3,       #image channels, 3 for red green blue\n                 conv_dim=64,     #spatial dimension of feature map\n                 image_size=64):  #spatial size of training images\n        \n        super(Discriminator, self).__init__()\n        self.image_size = image_size\n        self.conv_dim = conv_dim\n        \n        self.disc_cnn = nn.Sequential(self.make_disc_block(im_chan, conv_dim),\n                                      self.make_disc_block(conv_dim, conv_dim*2),\n                                      self.make_disc_block(conv_dim*2, conv_dim*4),\n                                      self.make_disc_block(conv_dim*4, conv_dim*8),\n                                      #no need a sigmoid here since it is included in the loss function\n                                      self.make_disc_block(conv_dim*8, 1, padding=0, final_layer=True)) \n        \n        \n    def make_disc_block(self,\n                        im_chan,\n                        op_chan,\n                        kernel_size=4,\n                        stride=2,\n                        padding=1,\n                        final_layer=False):\n        layers = []\n        layers.append(nn.Conv2d(im_chan,\n                                op_chan,\n                                kernel_size,\n                                stride,\n                                padding,\n                                bias=False))\n        \n        if not final_layer:\n            layers.append(nn.BatchNorm2d(op_chan))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n        \n        return nn.Sequential(*layers)\n    \n    #given an image tensor, returns a 1-dimension tensor representing fake\/real\n    def forward(self,image):\n        pred = self.disc_cnn(image)\n        pred = pred.view(image.size(0),-1)\n        return pred\n    \n    def _get_final_feature_dimention(self):\n        final_width_height = (self.image_size \/\/  2**len(self.disc_cnn))**2\n        final_depth = self.conv_dim * 2**(len(self.disc_cnn)-1)\n        return final_depth*final_width_height","e1eecfd6":"#test Discriminator\nd = Discriminator(im_chan=3,\n                  conv_dim=64,\n                  image_size=64)","42bc0f95":"print(d)","96269e9e":"#custom weights initialization to randomly initialize all weights\n#mean=0, stdev=0.2\ndef weights_init_normal(m):\n    \n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02) \n        \n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)","9718230c":"def real_loss(D_out,device='cpu'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    batch_size = D_out.size(0)\n    \n    #labels will be used when calculating the losses\n    #real labels = 1 and lable smoothing => 0.9\n    labels = torch.ones(batch_size, device=device)*0.9 \n    \n    loss = criterion(D_out.squeeze(), labels)\n    return loss","436fa347":"def fake_loss(D_out, device='cpu'):\n    \n    #initialize BCELoss function\n    criterion = nn.BCEWithLogitsLoss()\n    \n    #batch size\n    batch_size = D_out.size(0)\n    \n    #labels will be used when calculating the losses\n    #fake labels = 0\n    labels = torch.zeros(batch_size,\n                         device=device) \n    \n    loss = criterion(D_out.squeeze(), labels)\n    return loss","dd3b1454":"def print_tensor_images(images_tensor):\n    \n    '''\n    Function for visualizing images: Given a tensor of images, prints the images.\n    '''\n        \n    plt.rcParams['figure.figsize'] = (15, 15)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    \n    images_tensor = images_tensor.to('cpu')\n    npimgs = images_tensor.detach().numpy()\n    \n    no_plots = len(images_tensor)\n\n    for idx,image in enumerate(npimgs):\n        plt.subplot(1, 8, idx+1)\n        plt.axis('off')\n        #dnorm\n        image = image * 0.5 + 0.5\n        plt.imshow(np.transpose(image, (1, 2, 0)))\n        \n    plt.show()","ceb7ff73":"log_dir = '..\/input\/checkpoint\/ckpt_best_1.pth\/'","74e03178":"def train(D, G, \n          n_epochs,\n          dataloader,\n          d_optimizer,\n          g_optimizer,\n          z_dim,\n          print_every=50,\n          device='cpu'):\n    \n    #to keep track of the generator\u2019s learning progression, \n    #we will generate a fixed batch of latent vectors that are drawn from a Gaussian distribution   \n    sample_size=8\n    fixed_z = Generator.get_noise(n_samples=sample_size,\n                                  z_dim=z_dim,\n                                  device=device)\n    \n    # \u5982\u679c\u6709\u4fdd\u5b58\u7684\u6a21\u578b\uff0c\u5219\u52a0\u8f7d\u6a21\u578b\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u7ee7\u7eed\u8bad\u7ec3\n    if os.path.exists(log_dir):\n        path_checkpoint = log_dir  # \u65ad\u70b9\u8def\u5f84\n        checkpoint = torch.load(path_checkpoint)  # \u52a0\u8f7d\u65ad\u70b9\n\n        generator.load_state_dict(checkpoint['net'])  # \u52a0\u8f7d\u6a21\u578b\u53ef\u5b66\u4e60\u53c2\u6570\n\n        d_optimizer.load_state_dict(checkpoint['d_optimizer'])  # \u52a0\u8f7d\u4f18\u5316\u5668\u53c2\u6570\\\n        g_optimizer.load_state_dict(checkpoint['g_optimizer'])  # \u52a0\u8f7d\u4f18\u5316\u5668\u53c2\u6570\\\n        start_epoch = checkpoint['epoch']  # \u8bbe\u7f6e\u5f00\u59cb\u7684epoch \n    else:\n        start_epoch = 0\n        print('\u65e0\u4fdd\u5b58\u4e86\u7684\u6a21\u578b\uff0c\u5c06\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff01')\n    \n    for epoch in range(start_epoch,n_epochs+1):\n        #use dataloader to fetch batche\n\n        for batch_i,(real_images,_) in enumerate(dataloader):\n            batch_size = real_images.size(0)\n            real_images = real_images.to(device)\n            \n            #Part 1: Train the Discriminator ========================================================\n            #goal: to maximize the probability of correctly classifying a given input as real or fake\n            \n            #zero out the gradients before backpropagation\n            d_optimizer.zero_grad()\n            \n            ##classify all-real batch\n            d_real_op = D(real_images) #average output (across the batch) of the discriminator\n            d_real_loss = real_loss(d_real_op,\n                                    device=device)\n            \n            #train with all-fake batch\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            fake_images = G(noise)\n            \n            #classify all-fake batch\n            d_fake_op = D(fake_images) #average output (across the batch) of the generator\n            d_fake_loss = fake_loss(d_fake_op,\n                                    device=device)\n            \n            #total loss\n            d_loss = d_real_loss + d_fake_loss\n            \n            #update gradients\n            d_loss.backward()\n            #update optimizer\n            d_optimizer.step()\n            \n            #Part 2: Train the Generator ==============================================================\n            #zero out the gradients before backpropagation\n            g_optimizer.zero_grad()\n            noise = Generator.get_noise(n_samples=batch_size,\n                                        z_dim=z_dim,\n                                        device=device)\n            \n            #use discriminator to classify generator's output\n            g_out = G(noise)\n            d_out = D(g_out)\n            \n            g_loss = real_loss(d_out, \n                               device=device) \n            #update gradients\n            g_loss.backward()\n            #update optimizer\n            g_optimizer.step()\n        \n        print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch, \n                                                                               n_epochs, \n                                                                               d_loss.item(),  #keep track of loss\n                                                                               g_loss.item())) #keep track of loss\n        if (epoch % print_every == 0):\n            G.eval()\n            sample_image = G(fixed_z)\n            print_tensor_images(sample_image)\n            G.train()","ec7fd041":"#hyperparameters\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device is \", device)\n\n#incorrect hyperparameter settings lead to mode collapse\n#we will follow Goodfellow\u2019s paper\nz_dim = 100       #noise\nbeta_1 = 0.5      #as specified in the original DCGAN paper\nbeta_2 = 0.999 \nlr = 0.0001       #as specified in the original DCGAN paper\nn_epochs = 300\nbatch_size = 64\nimage_size = 64","67240077":"#initialize generator\ngenerator = Generator(z_dim, \n                      im_chan=3, \n                      hidden_dim=64).to(device)\n\n#initialize discriminator\ndiscriminator = Discriminator(im_chan=3, \n                              conv_dim=64, \n                              image_size=image_size).to(device)\n\n#setup Adam optimizers for generator\ng_optimizer = optim.Adam(generator.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup Adam optimizers for discriminator\nd_optimizer = optim.Adam(discriminator.parameters(), \n                         lr=lr, \n                         betas=(beta_1, beta_2))\n\n#setup dataloader\ndataloader = get_dataloader(batch_size, \n                            image_size, \n                            image_root)","c0d792ce":"#start training\n%time\nn_epochs = 300\ntrain(discriminator,\n          generator,\n          n_epochs,\n          dataloader,\n          d_optimizer,\n          g_optimizer,\n          z_dim,\n          print_every=50,\n          device=device)","4c0b4ddb":"#sample generation\ngenerator.to(device)\ngenerator.eval()       #eval mode\nsample_size= 8\n\nfor i in range(8):    \n    \n    #generate latent vectors\n    fixed_z = Generator.get_noise(n_samples=sample_size, \n                                  z_dim=z_dim, \n                                  device=device)    \n    \n    #generate samples\n    sample_image = generator(fixed_z)\n    \n    #display samples\n    print_tensor_images(sample_image)","ed4e85c7":"plt.figure(figsize = (15, 8))\nplt.plot(noise)\nplt.title(\"Noise\")\nplt.show()","9b723954":"generator = generator.to('cuda')\ncheckpoint = {\n        'net': generator.state_dict(),\n        'd_optimizer':d_optimizer.state_dict(),\n        'g_optimizer':g_optimizer.state_dict(),\n        'epoch': n_epochs\n    }\nif not os.path.isdir(\".\/checkpoint\"):\n    os.mkdir(\".\/checkpoint\")\ntorch.save(checkpoint, '.\/checkpoint\/ckpt_best_%s.pth')","7aa54326":"# def save_model(generator,file_name):\n#     generator = generator.to('cuda')\n#     torch.save(generator.state_dict(),\"bores_apes.pth\")\n# save_model(generator,\"kaggle\")","a122caa8":"#sample generation\ngenerator.to(device)\ngenerator.eval()       #eval mode\nsample_size= 8\n\nfor i in range(8):    \n    \n    #generate latent vectors\n    fixed_z = Generator.get_noise(n_samples=sample_size, \n                                  z_dim=100, \n                                  device=device)    \n    \n    #generate samples\n\n    sample_image = generator(fixed_z)\n    \n    #display samples\n    print_tensor_images(sample_image)","a4d9a90d":"# Training Procedure\n\n**Part 1\u200a-\u200aTrain the discriminator**\n\n* Generate fixed_z to get a fixed batch of latent vectors which we will use to keep track of the generator learning progression.\n* Train the discriminator with all-real batches of images: forward pass, calculate d_real_loss with real_loss() and calculate the gradients in a backward pass gradients in a backward pass with backward().\n* Use the generator to generate all-fake batches of images and use that to train the discriminator: forward pass, calculate d_fake_loss with fake_loss() and and calculate the gradients in a backward pass with backward()\n* Total d_loss with be the sum of d_real_loss and d_fake_loss.\n* Update d_optimizer with step().\n\n**Part 2\u200a-\u200aTrain the generator**\n\n* Use discriminator to classify generator's output g_out.\n* Computes generator's loss using real_loss().\n* Computes generator's gradients in a backward pass with backward().\n* Updates generator's optimizer with step().\n\nFor every mini-batch of data, we train the discriminator for one iteration, and then the generator for one iteration.","5b72da85":"# Build Generator\n\nDuring generator training:\n\n* The generator takes in a random input and output a single 64x64 colored image.\n* The image is upsampled: doubling in size and quadrupling in the area of activations each time it passes through the ConvTranspose2d layer.\n\nAs the result, the generator learns how to map the low dimensional image to the high dimensional image more and more effectively to fool the discriminator.","d5313409":"## Loss Functions\n\nNow we create loss functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves.","969c35c6":"# DataLoader & Preprocessing","332dff77":"# Create New CryptoPunks","bf6c0caf":"# Define Training Parameters","b71b63a1":"# Build Discriminator\n\nThe discriminator's training data comes from two sources:\n\n* real data: The discriminator uses these instances as positive examples during training. This is classified as 0.\n* fake data created by the generator: The discriminator uses these instances as negative examples during training. This is classified as 1.\n\nDuring discriminator training:\n\n* The discriminator takes in one input image and outputs a binary prediction to classify when the image is real or fake.\n* The discriminator_loss penalizes itself if it misclassifies the image. The total loss is the sum of the losses for real and fake images, discriminator_loss = real_loss + fake_loss.\n* The discriminator updates its weights through back-propagation from the discriminator loss through the discriminator network.","0b849ed7":"## Weight Initialization\n\nAll model weights will be randomly initialized from a normal distribution with mean=0, stdev=0.2 according to Goodfellow (2014).\n\n* weights_init_normal: takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization.","d10e2e23":"## Train Time!"}}