{"cell_type":{"054a398a":"code","8e4baaea":"code","b02a49f6":"code","167c024f":"code","e426b35e":"code","517c8952":"code","26365e72":"code","467464c6":"code","15d9d69a":"code","ea0a030b":"code","18fda707":"code","1b21350c":"code","fe4cb1a7":"code","e43d15ca":"code","a12f9a18":"code","9747c1cb":"code","ec9fa4f7":"code","ebedc14f":"code","5de017a5":"code","7787e0ee":"code","d29fe23e":"code","10b6d70f":"markdown","c8d73b85":"markdown","6483c614":"markdown","d6e08f19":"markdown","f498bce2":"markdown"},"source":{"054a398a":"!pip install -q efficientnet","8e4baaea":"import glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom matplotlib import pyplot as plt\nimport math, os, re, warnings, random\nfrom sklearn.utils import class_weight\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom tensorflow.keras import optimizers, applications, Sequential, losses, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler","b02a49f6":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n    \n    \nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\n","167c024f":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU@{}'.format(tpu.master()))\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    strategy = tf.distribute.get_strategy()\n    \nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync \nprint('# REPLICAS: {}'.format(REPLICAS))","e426b35e":"BATCH_SIZE = 32 * REPLICAS\nLEARNING_RATE = 3e-5 * REPLICAS\nEPOCHS = 30\nHEIGHT = 300\nWIDTH = 300\nCHANNELS = 3\nES_PATIENCE = 10\nAUG_BATCH=BATCH_SIZE\nIMAGE_SIZE=[HEIGHT,WIDTH]","517c8952":"# GCS_PATH = KaggleDatasets().get_gcs_path('leukemia-classification')\nGCS_PATH = '..\/input\/leukemia-classification\/'","26365e72":"\ntrain_dataset_0_all = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_0\/all\/*.bmp')\ntrain_dataset_0_hem = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_0\/hem\/*.bmp')\ntrain_dataset_1_all = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_1\/all\/*.bmp')\ntrain_dataset_1_hem = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_1\/hem\/*.bmp')\ntrain_dataset_2_all = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_2\/all\/*.bmp')\ntrain_dataset_2_hem = glob.glob(GCS_PATH + '\/C-NMC_Leukemia\/training_data\/fold_2\/hem\/*.bmp')\n","467464c6":"len(train_dataset_0_all)","15d9d69a":"# Include Validation data as well :-\nvalid_data=pd.read_csv(GCS_PATH + '\/C-NMC_Leukemia\/validation_data\/C-NMC_test_prelim_phase_data_labels.csv')\n\nav = valid_data[valid_data['labels'] == 1]\nhv = valid_data[valid_data['labels'] == 0]\n\nVAL_PATH = GCS_PATH + '\/C-NMC_Leukemia\/validation_data\/C-NMC_test_prelim_phase_data\/'\nAVL = [VAL_PATH +  i for i in list(av.new_names)]\nHVL = [VAL_PATH +  i for i in list(hv.new_names)]\n","ea0a030b":"# Merging happens here:-\nA=[]\nH=[]\nA.extend(train_dataset_0_all)\nA.extend(train_dataset_1_all)\nA.extend(train_dataset_2_all)\nA.extend(AVL)\nH.extend(train_dataset_0_hem)\nH.extend(train_dataset_1_hem)\nH.extend(train_dataset_2_hem)\nH.extend(HVL)\nprint(len(A))\nprint(len(H))\n\n# Create labels :-\nLabel_A = [1]*len(A)\nLabel_H = [0]*len(H)\n\n# Converting to pandas dataframe for easier access:-\nA.extend(H)\nLabel_A.extend(Label_H)\ndf = pd.DataFrame({'path':A, 'label':Label_A})\ndf = df.sample(frac=1).reset_index(drop=True)\n\nFILENAMES = df['path']\nLABELS = df['label']\n\nprint('Final Merged Data:-')\ndf","18fda707":"cw = class_weight.compute_class_weight('balanced',\n                                        np.unique(LABELS),\n                                        LABELS)\ncw = {0:cw[0], 1:cw[1]}","1b21350c":"cw","fe4cb1a7":"# Define Augmentation function:-\ndef data_augment(image, label):\n    \n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32)    \n    p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_flip_left_right(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    if p_pixel >= .2:\n        if p_pixel >= .8:\n            image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n        elif p_pixel >= .6:\n            image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n        elif p_pixel >= .4:\n            image = tf.image.random_brightness(image, max_delta=.1)\n        else:\n            image = tf.image.adjust_gamma(image, gamma=.6)\n            \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.6)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.8)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.6), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n        \n    image = tf.image.resize(image, size=[HEIGHT, WIDTH])\n    image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n    \n    return image, label","e43d15ca":"def parse_data(filename,label):\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_bmp(image)\n    image = tf.image.convert_image_dtype(image, tf.float32) \/  0.45 \n    image = tf.image.resize(image, IMAGE_SIZE)\n    return image, tf.one_hot(label,2)\n\ndef load_dataset(filenames, labels ,ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(parse_data, num_parallel_calls=AUTO)\n    return dataset\n\ndef get_dataset(FILENAMES,LABELS, ordered=False, repeated=False, augment=False):\n    dataset = load_dataset(FILENAMES, LABELS, ordered=ordered)\n    if augment:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","a12f9a18":"import math\nnp.set_printoptions(threshold=15, linewidth=80)\nCLASSES = [0,1]\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    labels = [str(i) for i in  numpy_labels]\n\n    return numpy_images, labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = label\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        # image = cv2.imdecode(image,cv2.IMREA)\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    \n    \n# Model evaluation\ndef plot_metrics(history):\n    metric_list = [m for m in list(history.keys()) if m is not 'lr']\n    size = len(metric_list)\/\/2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 4))\n    if size > 1:\n        axes = axes.flatten()\n    else:\n        axes = [axes]\n    \n    for index in range(len(metric_list)\/\/2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n        if 'loss' in metric_name:\n            axes[index].axvline(np.argmin(history[metric_name]), linestyle='dashed')\n            axes[index].axvline(np.argmin(history[val_metric_name]), linestyle='dashed', color='orange')\n        else:\n            axes[index].axvline(np.argmax(history[metric_name]), linestyle='dashed')\n            axes[index].axvline(np.argmax(history[val_metric_name]), linestyle='dashed', color='orange')\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","9747c1cb":"train_dataset = get_dataset(FILENAMES[:60],LABELS[:60], ordered=True,augment=True)\ntrain_iter = iter(train_dataset.unbatch().batch(20))\n\ndisplay_batch_of_images(next(train_iter))\ndisplay_batch_of_images(next(train_iter))","ec9fa4f7":"# Cosine Annealing:-\nLR_START = 1e-8\nLR_MIN = 1e-8\nLR_MAX = LEARNING_RATE\nLR_RAMPUP_EPOCHS = 3\nLR_SUSTAIN_EPOCHS = 0\nN_CYCLES = .5\n\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        progress = (epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) \/ (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\n        lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(math.pi * N_CYCLES * 2.0 * progress)))\n        if LR_MIN is not None:\n            lr = tf.math.maximum(LR_MIN, lr)\n            \n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nsns.set(style='whitegrid')\nfig, ax = plt.subplots(figsize=(20, 6))\nplt.plot(rng, y)\n\n# print(f'{EPOCHS} total epochs and {NUM_TRAINING_IMAGES\/\/BATCH_SIZE} steps per epoch')\nprint(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')","ebedc14f":"import tensorflow_addons as tfa","5de017a5":"# Model Architecture :-\ndef model_fn(input_shape, N_CLASSES):\n    input_image = L.Input(shape=input_shape, name='input_image')\n    base_model = efn.EfficientNetB3(input_tensor=input_image, \n                                    include_top=False, \n                                    weights='noisy-student', \n                                    pooling='avg')\n\n    for layer in base_model.layers:\n        if 'bn' in layer.name:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \n    model = tf.keras.Sequential([\n        base_model,\n        L.Dropout(.25),\n        L.Dense(N_CLASSES, activation='sigmoid', name='output')\n    ])\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss='binary_crossentropy', \n                  metrics=['accuracy',tfa.metrics.F1Score(num_classes=2, average='weighted')])\n    \n    return model","7787e0ee":"# Cross Validated training loop:-\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\noof_pred = []; oof_labels = []; history_list = []\ncv1 = cv2 = 0\n\n\nfor fold,(idxT, idxV) in enumerate(skf.split(FILENAMES,LABELS)):\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n    STEPS_PER_EPOCH = len(idxT) \/\/ BATCH_SIZE\n    \n    \n    K.clear_session()\n    with strategy.scope():\n        model = model_fn((None, None, CHANNELS), 2)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_f1_score', mode='max', \n                    patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n\n    ## TRAIN\n    history = model.fit(x=get_dataset(FILENAMES[idxT],LABELS[idxT], ordered=False, repeated=True, augment=True), \n                        validation_data=get_dataset(FILENAMES[idxV],LABELS[idxV] , ordered=True, repeated=False, augment=False), \n                        steps_per_epoch=STEPS_PER_EPOCH, \n                        callbacks=[es, LearningRateScheduler(lrfn ,verbose=0)], \n                        epochs=EPOCHS,  \n                        verbose=1,\n                        class_weight = cw).history\n      \n    model.save_weights(model_path)\n    history_list.append(history)\n    # Save last model weights\n\n    \n    ## RESULTS\n    print(f\"#### FOLD {fold+1} OOF Accuracy = {np.max(history['val_accuracy']):.3f}\")\n    cv1 += np.max(history['val_accuracy'])\n    cv2 += np.max(history['val_f1_score'])\n    del model\n    \nprint(f'### Avg. Accuracy = {cv1\/3.0} \\n ### Avg. Weighted F1 = {cv2\/3.0}')","d29fe23e":"# Plot metrics:-\nfor fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","10b6d70f":"## Step 3: Create data pipeline using tf.data.Dataset","c8d73b85":"## Step 1: Imports and necessary variables assignment","6483c614":"## Step 2: Merge all fold's data into one to later use Stratified-KFold","d6e08f19":"* v3 : adding 0.45 rescaling to images                      => 91.8\n* v4 : added rescaling by 0.8745 ---> dataset max value     => 90.9\n* v5 : using v3 rescaling with noisy-student weights","f498bce2":"## Step 4: Start training preprations"}}