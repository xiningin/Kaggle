{"cell_type":{"b9e14838":"code","a2d0d5d8":"code","b80a9981":"code","ff2035b8":"code","d10da7f7":"code","481ad0d6":"code","5316aee3":"code","2db90f67":"code","58195f0b":"code","e1abf9c4":"code","89eab47f":"code","7c42e13d":"code","24c9cc4c":"code","68e8ecc0":"code","0b618ecc":"code","95732902":"code","8e7e5be4":"code","c55dafc4":"code","c8b571b4":"code","876e92c3":"code","279d0743":"code","a33d99dc":"code","dd3bb6a6":"code","1d1aa14a":"code","cb73b918":"code","fbdef3fd":"code","243ce837":"code","1ad83565":"code","f7594de4":"code","d2166ccf":"code","edecb14e":"markdown","cdef9df0":"markdown","f4bfdef3":"markdown","c53222dc":"markdown","9432b9de":"markdown","48f5403c":"markdown","ec1fce8c":"markdown","bbe5322b":"markdown","5734d9c2":"markdown","10c84b6c":"markdown","c31627b4":"markdown","2ec56f96":"markdown","fe51b33b":"markdown","d051af99":"markdown","da71fe2a":"markdown","db73a518":"markdown","5d1bf0ac":"markdown","00901f53":"markdown"},"source":{"b9e14838":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","a2d0d5d8":"### 1-Loading meta data ###\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","b80a9981":"### 2-Fetch All of JSON File Path ###\n\n\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","ff2035b8":"### helper function to extract body text, abstract, and paper id from json files ###\n\n#file reader class\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            try:\n                self.paper_id = content['paper_id']\n            except Exception as e:\n                self.paper_id = ''\n            self.abstract = []\n            self.body_text= []\n            \n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            except Exception as e:\n                pass\n            # Body text\n            \n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            except Exception as e:\n                pass\n            \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}:{self.abstract[:200]}... {self.body_text[:200]}...'\n    \nfirst_row = FileReader(all_json[0])","d10da7f7":"## 3- Load the Data into DataFrame ##\n\n#Using the helper functions, let's read in the articles into a DataFrame that can be used easily:\n    \ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [] }\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 500) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    dict_['title'].append(meta_data['title'].values[0])\n    dict_['authors'].append(meta_data['authors'].values[0])\n\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id','abstract', 'body_text', 'authors', 'title', 'journal'])\ndf_covid.head()\n\n","481ad0d6":"## 4 -droping the duplicates ##\ndict_ = None\n\ndf_covid.info()\n\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n\ndf_covid['abstract'].describe(include='all')\ndf_covid['body_text'].describe(include='all')\n\n\n## droping the null values##\ndf_covid.dropna(inplace=True)","5316aee3":"#import sys\n#!{sys.executable} -m pip install --upgrade pip\n\n!pip install langdetect\n","2db90f67":"## 1- labelling the languages of articles in a separate column ##\n\nfrom langdetect import detect\ndf_covid['language'] = df_covid['title'].apply(detect)\n\n#checking what languages are present in the dataset\nuniqueValues = df_covid['language'].unique()\n\n## plotting top 5 highest number of published languages ##\nimport seaborn as sns\nplt.figure(figsize=(10,5))\nsns.set_style('darkgrid')\nsns.countplot(x='language',data=df_covid,order=pd.value_counts(df_covid['language']).iloc[:5].index,palette='viridis')\nplt.title('language distribution')\n\n","58195f0b":"## 2- keeping english language only ##\n\ndf_covid = df_covid.loc[df_covid['language'] == 'en']\ndf_covid.info()","e1abf9c4":"## 3- finding papers takling about covid and sars and mers, proteins, genome ##\n\n\n# a function that will work as a searching machine to find specific keywords in papers\nimport re\ndef pattern_searcher(search_str:str, search_list:str):\n\n    search_obj = re.search(search_list, search_str)\n    if search_obj :\n        return True\n    else:\n        return False\n    \n###keywords related to covid that we are looking for in literatures  \ncovid_list = ['covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'novel corona',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b',\n                    'chinese coronavirus', \n                    'sars-cov-2',\n                    'novel coronavirus', \n                    'coronavirus', \n             ]\npattern_covid = '|'.join(covid_list)\n\n#adding extra column that tags papers that are related to covid19\ndf_covid['covid_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_covid))\n\n\n###keywords related to protein and receptor surface protein that we are looking for in literatures\nprotein_list = ['protein','surface protein', 'proteome', 'proteomics', 'proteomic','proteins']\npattern_protein = '|'.join(protein_list)\n\n#adding extra column that tags papers that are related to Protein\ndf_covid['protein_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_protein))\n\n\n\n###keywords related to genomic sequences that we are looking for in literatures\ngenom_list=['genomic','genomics', 'genome', 'genome sequence', 'genome-wide']\npattern_genom = '|'.join(genom_list)\n\n#adding extra column that tags papers that are related to Genomic sequences\ndf_covid['genom_match'] = df_covid['body_text'].apply(lambda x: pattern_searcher(search_str=x, search_list=pattern_genom))\n\n","89eab47f":"df_covid.head()","7c42e13d":"### only keeping the papers that should be most related to task3 and provides good results###\n\ndf_covid = df_covid.loc[(df_covid['covid_match'] == True) ] \ndf_covid.info()\n","24c9cc4c":"df_covid.head()","68e8ecc0":"## 4-preparing the text to be ready for vectorization in the next steps ##\n\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemma = WordNetLemmatizer()\n\n\nstopword_set = set(stopwords.words('english')+['a','at','s','for','was', 'we', 'were', 'what', 'when', 'which', 'while', 'with', 'within', 'without', 'would', 'seem', 'seen','several', 'should','show', 'showed', 'shown', 'shows', 'significantly', 'since', 'so', 'some', 'such','obtained', 'of', 'often', 'on', 'our', 'overall','made', 'mainly', 'make', 'may', 'mg','might', 'ml', 'mm', 'most', 'mostly', 'must', 'each', 'either', 'enough', 'especially', 'etc','had', 'has', 'have', 'having', 'here', 'how', 'however', 'upon', 'use', 'used', 'using', 'perhaps', 'pmid','can', 'could', 'did', 'do', 'does', 'done', 'due', 'during', 'et al', 'found','study','observed','identified','fig','although','reported','group','result','include', 'figure', 'table'])\n\nstop_set=['a','at','s','for','was', 'we', 'were', 'what', 'when', 'which', 'while', 'with', 'within', 'without', 'would', 'seem', 'seen','several', 'should','show', 'showed', 'shown', 'shows', 'significantly', 'since', 'so', 'some', 'such','obtained', 'of', 'often', 'on', 'our', 'overall','made', 'mainly', 'make', 'may', 'mg','might', 'ml', 'mm', 'most', 'mostly', 'must', 'each', 'either', 'enough', 'especially', 'etc','had', 'has', 'have', 'having', 'here', 'how', 'however', 'upon', 'use', 'used', 'using', 'perhaps', 'pmid','can', 'could', 'did', 'do', 'does', 'done', 'due', 'during', 'et al', 'found','study','observed','identified','fig','although','reported','group','result','include', 'figure', 'table']\n\n#definig a function that cleans the string of full body text\ndef process(string):\n    string=' '+string+' '\n    string=' '.join([word if word not in stopword_set else '' for word in string.split()])\n    string=re.sub('\\@\\w*',' ',string)\n    string=re.sub('\\.',' ',string)\n    string=re.sub(\"[,#'-\\(\\):$;\\?%]\",' ',string)\n    string=re.sub(\"\\d\",' ',string)\n    string=re.sub(r'[^\\x00-\\x7F]+',' ', string)\n    for i in stop_set:\n        string = re.sub(' ' +i+' ', ' ', string)\n    string=\" \".join(lemma.lemmatize(word) for word in string.split())\n    string=re.sub('( [\\w]{1,2} )',' ', string)\n    string=re.sub(\"\\s+\",' ',string)\n    string=string.replace('[', '')\n    string=string.replace(']', '')\n    return string\n\n\ndf_covid['processed_text'] = df_covid['body_text'].apply(process)","0b618ecc":"## 1-Performing TF-IDF Vectorization on the data##\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_covid['processed_text'].values)\nX = vectorizer.transform(df_covid['processed_text'].values)","95732902":"## 2- Performing Principle component analysis (PCA) on the vectorized body text##\n\n# Since the TF-IDF vector was sparse PCA was performed using TruncatedSVD \nfrom sklearn.decomposition import TruncatedSVD\n\nPCA = TruncatedSVD(2)\nXpca = PCA.fit_transform(X)\n","8e7e5be4":"## Helper function for plotting the clustering results ## \nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\ndef plotting(datapoint, labels):\n    # sns settings\n    sns.set(rc={'figure.figsize':(15,15)})\n\n    # colors\n    palette = sns.color_palette(\"bright\", len(set(labels)))\n\n    # plot\n    sns.scatterplot(datapoint[:, 0], datapoint[:, 1], hue=labels, legend='full', palette=palette)\n    plt.title(\"Covid-19 Articles - Clustered(K-Means) - Using Vectorized body text\")\n\n    plt.savefig(\"\/kaggle\/working\/covid19_label_TFIDF.png\")\n    plt.show()","c55dafc4":"## 3 & 4: visualization and evaluatoin ##\n## Helper function for quantitatively comparing and measuring accuracy using a combination of clustering + KNN [5] ##\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import cross_val_score\n\n\ndef acc(data_vectors):\n    \n    # Cluster the vectors find their labels \n    kmeans_model = KMeans(n_clusters=9, init='k-means++', max_iter=100) \n    kmeans_model.fit(data_vectors)\n    labels=kmeans_model.labels_.tolist()\n    plotting(data_vectors, labels) # Plot the clusters \n    \n    # Perform KNN on using the labels from clustering\n    neigh = KNeighborsClassifier(n_neighbors=15)\n\n    # performing cross validation \n    scores = cross_val_score(neigh, data_vectors, labels, cv=5)\n    acc = np.average(scores)\n    return acc \n\nprint(acc(Xpca))","c8b571b4":"%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg=mpimg.imread('\/kaggle\/input\/assets\/Comparison of different algorithms(1).png')\nplt.figure(figsize=(12,12))\nimgplot = plt.imshow(img)\nplt.show()","876e92c3":"# Dropping irrelavant columns for further processing\ntry:\n    df_covid = df_covid.drop([ \"language\", \"processed_text\", \"protein_match\", \"sars_match\", \"mers_match\", \"covid_match\", \"genom_match\", \"paper_id\"], axis=1) # keep \"paper_id\", \"abstract\", \"authors\", \"title\", \"journal\", \"body_text\"\nexcept:\n    print(\"Items already dropped\")\n\n\n# rearranging the data columns for further visualizations \ncols = ['title', 'abstract', 'authors', 'journal', 'body_text']\ndf_covid = df_covid[cols]","279d0743":"# Helper function for plotting word clouds for various questions related to task 3\n\nfrom wordcloud import WordCloud\n\ndef word_cloud(df, filename):\n    long_string = ','.join(list(df['body_text'].values[:20])) # Create a WordCloud object using the top 20 papers \n    wordcloud = WordCloud(background_color=\"white\", max_words=50000, contour_width=5, contour_color='steelblue') # Generate a word cloud\n    wordcloud.generate(long_string)\n    wordcloud.to_image()\n    wordcloud.to_file(filename + '.png')\n    \n    \n","a33d99dc":"\n\nfrom scipy import spatial\nfrom sklearn.neighbors import NearestNeighbors\ndef get_closest_neighbours_table(question, vector_body_text, vectorizer_model, pca_model, df, filename):    \n    \n    # Vectorize the task vector using the pre-trained TF-IDF model\n    task_X = vectorizer_model.transform(question)\n    # Reduce the dimensionality of the task vector to 2 using pre-trained PCA model\n    task_out = pca_model.transform(task_X)    \n    filename = filename.replace(' ', '_')   \n    \n    # calculating the cosine similarity of the tasks between\n    df['cosine_similarity'] = [1 - spatial.distance.cosine(a, task_out) for a in vector_body_text]    # Sorting the papers based on the cosine similarity scores\n    df = df.sort_values(by=['cosine_similarity'], ascending=False)    \n    \n    # picking the top 10 papers based on highest cosine similarity\n    df = df.iloc[:10]\n    df['body_text'] = df['body_text'].apply(lambda x: x[:min(1000, len(x))])    \n    \n    # Save the dataframe to html for better visualization of data.\n    html = df.to_html()\n    #write html to file\n    text_file = open( filename +\".html\", \"w\")\n    text_file.write(html)\n    text_file.close()    \n    \n    return df\n","dd3bb6a6":"# Increasing the maximum column width to 300 for better visualization of tables \n\npd.set_option('display.max_colwidth', 300)","1d1aa14a":"# Finding the most similar documents to questions related to task 3 as suggested by Dr. Hassan Vahidnezhad\n\nquestion1 = 'How is the corona virus infecting the animals different from the covid-19 virus that is infecting humans'\nquestion_preprocessed = [process(question1)] # Performing the same pre-processing step on the question document\ndf = get_closest_neighbours_table(question_preprocessed, Xpca, vectorizer, PCA, df_covid, question1) # Finding the closest neighbours \nprint(question1)\ndf.head()\n\n","cb73b918":"# Make of word cloud using the top 20 papers.\n\nlong_string = ','.join(list(df['body_text'].values[:20])) # Create a WordCloud object using the top 20 papers \nwordcloud = WordCloud(background_color=\"white\", max_words=50000, contour_width=5, contour_color='steelblue') # Generate a word cloud\nwordcloud.generate(long_string)\nwordcloud.to_image()","fbdef3fd":"\n\nquestion2 = 'What part of the covid-19 virus genome determines the suitable host for this coronavirus'\nquestion_preprocessed = [process(question2)]\ndf = get_closest_neighbours_table(question_preprocessed, Xpca, vectorizer, PCA, df_covid, question2)\nprint(question2)\ndf.head()\n\n","243ce837":"# Make of word cloud using the top 20 papers.\n\nlong_string = ','.join(list(df['body_text'].values[:20])) # Create a WordCloud object using the top 20 papers \nwordcloud = WordCloud(background_color=\"white\", max_words=50000, contour_width=5, contour_color='steelblue') # Generate a word cloud\nwordcloud.generate(long_string)\nwordcloud.to_image()","1ad83565":"# Finding the most similar documents to complete task 3 desciption.  \n\ntask3 = 'Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time. Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged. Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over. Evidence of whether farmers are infected, and whether farmers could have played a role in the origin. Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia. Experimental infections to test host range for this pathogen. Animal host(s) and any evidence of continued spill-over to humans. Socioeconomic and behavioral risk factors for this spill-over. Sustainable risk reduction strategies.'\nquestion_preprocessed = [process(task3)]\ndf = get_closest_neighbours_table(question_preprocessed, Xpca, vectorizer, PCA, df_covid, 'General task3 description')\nprint('General task3 description')\ndf.head()\n\n","f7594de4":"# Make of word cloud using the top 20 papers.\n\nlong_string = ','.join(list(df['body_text'].values[:20])) # Create a WordCloud object using the top 20 papers \nwordcloud = WordCloud(background_color=\"white\", max_words=50000, contour_width=5, contour_color='steelblue') # Generate a word cloud\nwordcloud.generate(long_string)\nwordcloud.to_image()","d2166ccf":"%pylab inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg=mpimg.imread('\/kaggle\/input\/assets2\/Assets\/IMG_5188.JPG')\nplt.figure(figsize=(20,20))\nimgplot = plt.imshow(img)\nplt.show()","edecb14e":"**Clustering**\n\n>Since we had data featured as text, the vectors of each document are expected to be placed in space based on the similarity of their content. An interesting way of finding the patterns and visualizing it, is k-means clustering [[6](https:\/\/www.jstor.org\/stable\/2346830?seq=1)], an unsupervised learning method that takes the feature vectors and numbers of clusters as input and provides us with an output of corresponding clusters. The optimal number of cluster was found using Elbow method. \n\n**Quantitative Measure of Performance of the Vectorization Algorithm ** \n\n>The first experiment makes use of the algorithm suggested by S. Vajda et al. [[5](https:\/\/www.researchgate.net\/publication\/316550769_A_Fast_k-Nearest_Neighbor_Classifier_Using_Unsupervised_Clustering)]. To quantitatively measure the feature vectors obtained from different algorithms, the S. Vajda et al. [[5](https:\/\/www.researchgate.net\/publication\/316550769_A_Fast_k-Nearest_Neighbor_Classifier_Using_Unsupervised_Clustering)] proposes an algorithm which can be summarized in the following steps:<br\/>\n>1. Cluster all the feature vectors obtained from the feature engineering step.<br\/> \n>2. Obtain labels for all the feature vectors. This would now act as your labeled data for the following steps.<br\/>\n>3. Split the data into train and test sets.<br\/>\n>4. Perform K-Nearest Neighbour classification of the test set using the labels of the training set.<br\/>\n>5. Measure the accuracy of KNN.<br\/>\n\n>Moreover to better judge the performance of the algorithm we have performed K-fold cross-validation. The results of different algorithms have been compared in the bar graph. It was observed that TF-IDF vectorization performed better than the other algorithms.<br\/> \n\n","cdef9df0":"**Only keeping the papers that should be the most related to task 3**<br\/>\nregarding iterative discussions of final results, at the end we decided to keep the papers that have these conditions:<br\/>\n>labelled with **_covid_match_** only\n\n\n","f4bfdef3":"![](http:\/\/)","c53222dc":"\n# Goal of this Notebook is to solve task 3:\n**What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n**","9432b9de":"# Visualization and Evaluation\n","48f5403c":"*Regarding our user interview Dr. Vahinezhad, we concluded that we should label the articles based on specific keywords that are related to **'Covid 19'** , **'SARS'**, **'MERS', 'Proteins'** and **'Genomes'***\n\n>Part of the following cell is inspired by [notebook](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions) of ajrwhite","ec1fce8c":"\n# Preprocessing\n\n**After creating a dataframe we try to do some cleaning and feature engineering on our data**\n\n1. Adding extra column to our data frame which tages the language of the papers<br\/>\n2. Keeping the articles that are in english language<br\/>\n3. Based on our user interview with Prof. Dr.Vahidnezhad adding extra column to our data which labels the articles based on specifications of task 3<br\/>\n4. Keeping the articles that should be definitley considered for task 3 thus reducing the noise<br\/>\n5. Preparing the text to be ready for vectorization in the next steps<br\/>\n","bbe5322b":"# Task Description Projection and Sorting Documents based on Cosine Similarity \n\n>To find the most relevant documents we need to find the similarity between the document features vectors and the task projections. To perform this task, we projected the task description to the same feature space as the body text documents. For this, the task description was pre-processed using the same pre-processing steps. The processed task description was vectorized using the pre-trained TF-IDF model. The task vectors were projected to a lower dimension using the pre-trained PCA model.<br\/>\n\n>Now to compare documents cosine similarity was used. This reason behind this choice was that even if a document is far apart in the euclidean distance (due to the size of the document) it could still be oriented closer.<br\/>\n\n>We have also formed word clouds of the top 20 most relevant documents to see the top keywords.<br\/>\n\n\n**Supervised Qualitative Analysis of Results** \n\n>A supervised evaluation was conducted to better judge and compare the performance of the algorithms. Dr. Hassan Vahidnezhad evaluated the word clouds along with the looking at the table containing the top 10 most similar documents. Based on his expert opinion, TF-IDF performed better than Doc2Vec and Doc2Vec+TF-IDF based on the table and keywords that the doctor observed.\n\n\n","5734d9c2":"# Feature engineering (TF-IDF vectorization with PCA)\n\n **Feature engineering**\n>To perform machine learning algorithms on textual data it needs to be converted from text format to the numeric format. The method used for converting the preprocessed text data into feature vectors is called feature engineering. For this section, we have used four different methods.<br\/>\n\n> _Term Frequency Inverse Document Frequency (TF-IDF):_<br\/>\n>After pre-processing the body text, TF-IDF vectorization [[1](https:\/\/www.researchgate.net\/publication\/326425709_Text_Mining_Use_of_TF-IDF_to_Examine_the_Relevance_of_Words_to_Documents)] was utilized to vectorize the text. The major advantage of TF-IDF based vectorization was that it can be utilized to score and rank documents based on a user query. Moreover, it diminishes the weight of commonly occurring words and increases the weight of words that occur rarely. TF-IDF has also shown excellent results in search engines. <br\/>\n\n>The below code utilizes scikit learn\u2019s implementation of TF-IDF to perform the task.<br\/> \n","10c84b6c":"# References \n\n[1] Qaiser, Shahzad & Ali, Ramsha. (2018). Text Mining: Use of TF-IDF to Examine the Relevance of Words to Documents. International Journal of Computer Applications. 181. 10.5120\/ijca2018917395.\n\n[2] Le, Quoc, and Tomas Mikolov. \"Distributed representations of sentences and documents.\" In International conference on machine learning, pp. 1188-1196. 2014.\n\n[3] Wold, Svante, Kim Esbensen, and Paul Geladi. \"Principal component analysis.\" Chemometrics and intelligent laboratory systems 2, no. 1-3 (1987): 37-52.\n\n[4] Peterson, Leif E. \"K-nearest neighbor.\" Scholarpedia 4, no. 2 (2009): 1883.\n\n[5] Vajda S., Santosh K.C. (2017) A Fast k-Nearest Neighbor Classifier Using Unsupervised Clustering. In: Santosh K., Hangarge M., Bevilacqua V., Negi A. (eds) Recent Trends in Image Processing and Pattern Recognition. RTIP2R 2016. Communications in Computer and Information Science, vol 709. Springer, Singapore\n\n[6] J.  A.  Hartigan and  M.  A.  Wong,  \u201cAlgorithm as  136:  A  k-means clustering algorithm,\u201dJournal of the Royal Statistical Society. Series C (Applied Statistics), vol. 28, no. 1, pp. 100\u2013108, 1979\n","c31627b4":"# Acknowledgements\n\nWe would like to extend our gratitute toward Dr. Hassan Vahidnezhad and his team at Jefferson Institute of Molecular Medicine and the Department of Dermatology and Cutaneous Biology, Thomas Jefferson University Hospital, Philadelphia, PA for his expert medical opinions and helping us achieve the best results. He was instrumental in further helping us understand subtasks which medical experts are currently interested in understanding. We would also like to thank Dr. Prof. Dr. Vahidzadeh for his constant support and guidance. He helped us understand the intricate details and the specific areas which our text mining should focus on to show the best results. We should take to acknowledge Dr. Sara Sadr, M.D. Faculty of Medicine, Mazandaran University of Medical Sciences and Dr. Fateme Jafari, M.D. Faculty of Medicine, Tehran University of Medical Science for their valuable insights. They helped us gather relevant keywords and more specific questions to make our text mining results more accurate.","2ec56f96":"## **Preparing the text to be ready for vectorization in the next steps**\n\n\n**Data Pre-Processing Steps**\n\nEven Though the extracted data is subject-oriented, it still contains noise which is from irrelevant terms, symbols, and punctuation marks. These undesired data will reduce the performance of the classifier therefore, they should be removed from the corpus via following steps:<br\/>\n\n>1. _Stop words removal:_<br\/>\n>There always exist many unrelated terms such as \u2018is\u2019, \u2018a\u2019, \u2018this\u2019 and etc in a text which do not have any value in terms of feature generation. However if we don't omit them from the body of text, they increase the dimensionality of features, therefore, results in computational complexity of text classification model.<br\/>\n\n>2. _Character transformation:_<br\/>\n>A word may appear differently in one corpus or different corpora. Therefore, letter transformation should be used to convert all the tokens into a standard format.<br\/>\n\n>3. _Character transformation:_<br\/>\n>Usually, words in languages take several forms, that can be due to the grammatical context, which then should be considered as inflected. This matter happens mostly to verbs but also to nouns and adjectives in English language. Normalization to token reduces the inflection of words thus reducing the dimensionality of features.<br\/>\n\n","fe51b33b":"# Algorithm Overview and Discussion\n\n>In the following subsections, main stages of our analysis and technical approach are explained.\n\n>1- Dataset Analysis<br\/>\n>2- Pre-processing<br\/>\n>3- Feature engineering<br\/>\n>4- Task description projection and sorting documents based on\n>5- Clustering and KNN<br\/>\n>6- Text classification model building and its evaluation<br\/>\n\n\n\n\n> 1 - **Dataset analysis**\n>>Defining influential factors based on specific features of a defined task is crucial as data grows and the complexity increases. Hence our group decided to label the provided large dataset based on the language of the article, if the article is related to COVID-19, whether the article is related to genomic sequence of virus, and whether it has discussions on receptor surface of proteins of target virus. We believe this labeling helps us to narrow down the noise in the dataset, which means that we drop the articles that are not in this criteria, and it affects the performance of the classifier.<br\/>\n>>In this process, our group had followed a semi-supervised approach which means after reducing the number of articles, some of the literatures have been selected randomly and have been read by us to check whether it includes the criteria that we were expecting.<br\/>\n\n\n> 2 - **Data pre-processing**\n>>Even Though the extracted data is subject-oriented, it still contains noise which is from irrelevant terms, symbols, and punctuation marks. These undesired data will reduce the performance of the classifier therefore, they should be removed from the corpus via following steps:<br\/>\n\n>>2.1. **Stop words removal**:<br\/>\n>>There always exist many unrelated terms such as \u2018is\u2019, \u2018a\u2019, \u2018this\u2019 and etc in a text which do not have any value in terms of feature generation. However if we don't omit them from the body of text, they increase the dimensionality of features, therefore, results in computational complexity of text classification model.<br\/>\n\n>>2.2. **Character transformation**:<br\/>\n>>A word may appear differently in one corpus or different corpora. Therefore, letter transformation should be used to convert all the tokens into a standard format.<br\/>\n\n>>2.3. **Character transformation**:<br\/>\n>>Usually, words in languages take several forms, that can be due to the grammatical context, which then should be considered as inflected. This matter happens mostly to verbs but also to nouns and adjectives in English language. Normalization to token reduces the inflection of words thus reducing the dimensionality of features.<br\/>\n\n\n> 3 - **Feature engineering**\n>>To perform machine learning algorithms on textual data it needs to be converted from text format to the numeric format. The method used for converting the preprocessed text data into feature vectors is called feature engineering. For this section, we have used four different methods.<br\/>\n\n>>3.1. **Term Frequency Inverse Document Frequency (TF-IDF)**:<br\/>\n>>After pre-processing the body text, TF-IDF vectorization [[1](https:\/\/www.researchgate.net\/publication\/326425709_Text_Mining_Use_of_TF-IDF_to_Examine_the_Relevance_of_Words_to_Documents)] was utilized to vectorize the text. The major advantage of TF-IDF based vectorization was that it can be utilized to score and rank documents based on a user query. Moreover, it diminishes the weight of commonly occurring words and increases the weight of words that occur rarely. TF-IDF has also shown excellent results in search engines. <br\/>\n>>The below code utilizes scikit learn\u2019s implementation of TF-IDF to perform the task.<br\/> \n>>After performing TF-IDF vectorization, to perform unsupervised dimensionality reduction Principal Component Analysis [[2](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/0169743987800849)] was performed on the feature vectors. PCA was preferred since it reduces the noise in the data by reducing correlated features which don\u2019t factor in the final decision making.<br\/>\n\n>>3.2. **Doc2Vec**:<br\/>\n>>Doc2vec[[3](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf)] is used to represent a document based on its content with a feature vector. This algorithm is based on Word2Vec which is used for word embeddings. The intuition behind using Doc2Vec was that it takes the word order into account. It has also proven to generalize to longer documents and can learn from unlabelled data.<br\/>\n>>Therefore we used gensim library implementation. The algorithm is fed by preprocessed text data that is encapsulated as tagged documents. The model was trained for 10 epochs.<br\/>\n>>Parameters used for Doc2vec are as follow:<br\/>\n>>Vector-size = 100, dimensionality of the feature vector (this is one the benefits of Doc2Vec compared to Tf-Idf because it has less dimensionality).<br\/>\n>>Min count = 100, The algorithm ignores all the words which were repeated less than this parameter.<br\/>\n>>Window size = 10, we tried to allow larger numbers of words per entry so that the algorithm can generate better feature vectors.<br\/>\n\n>>3.3. **Doc2Vec and TF-IDF together**:<br\/>\n>>We performed an ensemble of Doc2Vec and TF-IDF. Both Doc2Vec and TF-IDF were separately trained on the processed body text. After which both their dimensionality was separately reduced using PCA to 2 components. The two resulting feature vectors were concatenated.<br\/>\n\n>>3.4. **Bigram phrase modeling and Doc2Vec**:<br\/>\n>>We implemented bigram phrase detection with gensim library, which detects frequently use phrases of two words and considers them together. Then we transformed our corpus into a bigram model. Afterwards we trained our doc2vec model with this newly generated data. But this time our doc2vec model was a concatenation of DBOW ( distributed bag of words) and DMM (Distributed Memory Mean). Each doc2vec model is trained separately under 30 epochs with bigram generated corpus and then the feature vectors are combined together.<br\/>\n\n\n> 4 - **Task description projection and sorting documents based on <u>cosine similarity<\/u>**\n>>To find the most relevant documents we need to find the similarity between the document features vectors and the task projections. To perform this task, we projected the task description to the same feature space as the body text documents. For this, the task description was pre-processed using the same pre-processing steps. The processed task description was vectorized using the pre-trained TF-IDF model. The task vectors were projected to a lower dimension using the pre-trained PCA model.<br\/>\n>>Now to compare documents cosine similarity was used. This reason behind this choice was that even if a document is far apart in the euclidean distance (due to the size of the document) it could still be oriented closer.<br\/>\n>>We have also formed word clouds of the top 20 most relevant documents to see the top keywords.<br\/>\n\n> 5 - **Clustering**\n>>Since we had data featured as text, the vectors of each document are expected to be placed in space based on the similarity of their content. An interesting way of finding the patterns and visualizing it, is k-means clustering [[6](https:\/\/www.jstor.org\/stable\/2346830?seq=1)], an unsupervised learning method that takes the feature vectors and numbers of clusters as input and provides us with an output of corresponding clusters. To find the optimal numbers of clusters with the elbow method, we had to reduce the dimension of feature vectors because otherwise, the graph was not showing consistent behaviour. To project a high-dimensional data onto a two-dimensional space, we used PCA on the vectors obtained with doc2vec and Tf-idf together.<br\/> \n>>As you can see we have now a very distinct cluster of vectors. All 4 vectorization methods are showing a good clustering performance.<br\/>\n>>please look at the following markdown image1.<br\/>\n\n\n> 6 - **Text classification model building and its evaluation**\n>>Two unsupervised and supervised experiments were performed to evaluate the performance of the algorithm. The algorithm was compared with other popular text mining algorithms to show the efficiency of this approach.<br\/> \n>>The first experiment makes use of the algorithm suggested by S. Vajda et al. [[5](https:\/\/www.researchgate.net\/publication\/316550769_A_Fast_k-Nearest_Neighbor_Classifier_Using_Unsupervised_Clustering)]. To quantitatively measure the feature vectors obtained from different algorithms, the S. Vajda et al. [[5](https:\/\/www.researchgate.net\/publication\/316550769_A_Fast_k-Nearest_Neighbor_Classifier_Using_Unsupervised_Clustering)] proposes an algorithm which can be summarized in the following steps:<br\/>\n\n>>6.1. Cluster all the feature vectors obtained from the feature engineering step.<br\/> \n>>6.2. Obtain labels for all the feature vectors. This would now act as your labeled data for the following steps.<br\/>\n>>6.3. Split the data into train and test sets.<br\/>\n>>6.4. Perform K-Nearest Neighbour classification of the test set using the labels of the training set.<br\/>\n>>6.5. Measure the accuracy of KNN.<br\/>\n\n>>Moreover to better judge the performance of the algorithm we have performed K-fold cross-validation. The results of different algorithms have been compared in the bar graph. It was observed that TF-IDF vectorization performed better than the other algorithms.<br\/> \n>>For the second experiment, a supervised evaluation was conducted. After sorting the documents using cosine similarity, word clouds were formed picking the top 20 papers. Dr\u2026. evaluated this word cloud along with the looking at the table containing the top 10 most similar documents. Based on his expert opinion, TF-IDF performed better than Doc2Vec and Doc2Vec+TF-IDF based on the table and keywords that the doctor observed.<br\/>\n>><br\/>\n>>**_Based on our evaluation of both these quantitative and qualitative measures, TF-IDF stood superior than the other algorithms. This made it the ideal choice to solve that text mining task and find research documents that are most relevant to task 3._**<br\/> \n\n\n\n","d051af99":"# Loading the data\n\n>1. We are going to extract 'full body texts' , 'paper id', and 'abstract' from jason files.\n>2. We load the meta data too, because some of their characteristics like author, sha and journal name will be helpful for us.\n>3. Then we merge 1 and 2 into a dataframe together\n>4. We omit the duplicated rows from the dataframe\n\n\n> This part of our notebook is inspired by a notebook from [MaksimEkin in kaggle](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering)","da71fe2a":"**Taking a look at the distribution of languages in available articles in this data set, \nit is logical to keep only the papers with english language only **","db73a518":"\n>After performing TF-IDF vectorization, to perform unsupervised dimensionality reduction Principal Component Analysis [[2](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/0169743987800849)] was performed on the feature vectors. PCA was preferred since it reduces the noise in the data by reducing correlated features which don\u2019t factor in the final decision making.<br\/>","5d1bf0ac":"# Introduction\n\n>**Our team:**\n>>We are an international team of passionate data scientist and AI engineers at Digital Product School at the Center for Business Creation and Innovation at Technical University of Munich, Germany. Afsane Asaei, Anubhav Jain, and Diana Amiri\n\n>By participating in this challenge, we hope to contribute to this global effort against Covid19 pandemic. Our aim is to provide valuable insights regarding the questions of task 3 for the doctors and researchers. We plan to do text data mining on scientific literatures. This method has been proven to be a good strategy to handle fast growing accumulation of scientific papers, which is a valid case at this moment due to the world-wide pandemic situation of Covid-19. Therefore, a sequence of steps needs to be done to find the most relative articles and snippets to the corresponding key questions. Our general approach includes feature engineering with TFIDF and PCA, topic modeling with unsupervised learning, and text classification with KNN to ease the process of finding the most relative answer.\n\n\n\n>>### Medical Genetic Insights\n>>As text data grows in scale and complexity, specific factors need to be identified that influences the appropriateness of related articles. These factors make it possible to uncover valuable insights. To identify these influential factors, we have interviewed Prof. Dr. Hassan Vahidnezhad, Ph.D. of Medical Geneticist and a faculty member at Jefferson Institute of Molecular Medicine. In addition, we have consulted Dr. M.D. Sara Sadr, and Dr. M.D. Fateme Jafari, Faculty of Medicine and University Hospital.\n\n>>The team of Medical experts helped us in having more relevant keywords that we have used for data preparation and filtering. They also added to the rich text of Task 3 with relevant variants of the concrete topics currently in focus of research and analysis on Covid19. The rich text of all variants of fundamental questions in Task 3 are clustered in 4 categories. These categories were crucial to develop a more specific concept in finding and classifying relative articles and snippets. The categories are as follow:\n\n>>> 1. Characteristics<br\/>\n>>> 2. Virulence<br\/>\n>>> 3. Origin<br\/>\n>>> 4. Role of virus genetics in infection and treatments<br\/>\n\n>>### Text Mining using Natural Language Processing Steps \n\n\n>>In data preprocessing we defined characteristic labels for articles based on specific tasks. The preprocessing helped us narrow down the number of articles that are related to these questions and provide a less noisy text data for feature engineering and text classification.\n\n>>In feature engineering many experiments have been done by our team to find the best vectorization and dimensionality reduction method that provides highest accuracy for this dataset (details of our research\/experiment can be seen in our [github link](https:\/\/github.com\/DigitalProductschool\/covid-19-kaggle)). \n\n>>On the next step, we did the clustering of articles with K-means unsupervised learning to visualize the similarity measure between the articles. The optimum number of clusters is estimated through the Elbow method. **Each cluster is given a particular label. Then, the data is sampled randomly and divided into two training (%80 of data) and testing sets (%20 of data) and used for classification**.\n\n>>We then evaluate our model with the KNN classification algorithm which indeed was successful with 98% of accuracy. This approach enabled us to compare the performance of different sets of features. We choose the best performing features for text mining. \n\n>>Finally, we sort the documents based on cosine similarity (close to 1) to each one of key questions in Task3. We would like to emphasize that the sequences of feature engineering, topic modeling, and similarity measurement methods have been examined and the final best method in accuracy and least distance has been provided. \n\n>>>1 Data preprocessing<br\/>\n>>>2 Text cleaning<br\/>\n>>>3 Feature engineering (TF-IDF with PCA)<br\/>\n>>>4 Visualisation and evaluation (with unsupervised k-means and text classification, KNN)<br\/>\n>>>5 Sorting the trained articles based on each of key questions (KNN and cosine similarity)<br\/>\n\n>>### Result of this work\n>>We provided a <u>table<\/u> and <u>word cloud<\/u> that works as literature review.<br\/>\n>>These tables are related to the key questions of task3. Therefore, the details that we present include: the **author**, **paper title**, **university** and most importantly the **snippets** of the article body text that is most relevant and insightful to the question that has been asked.<br\/>\n>>Besides, we assist the reader with a word cloud too as a very simple alternative to back up the relativity of presented articles in the table.","00901f53":"The table below shows the comparison of our algorithm which uses TF-IDF+PCA with various other popular algorithms based on the qantitative measure of performance as suggested by S. Vajda et al. [[5](https:\/\/www.researchgate.net\/publication\/316550769_A_Fast_k-Nearest_Neighbor_Classifier_Using_Unsupervised_Clustering)]. "}}