{"cell_type":{"30cd2cd2":"code","a2a499ad":"code","1428151e":"code","a2126ad5":"code","60e039b5":"code","ad2f299e":"code","3d4418e3":"code","e67dce98":"code","6229887f":"code","2e516a67":"code","cba964cc":"code","b97a0dc1":"code","d012933b":"code","77dd2bd6":"code","e25b5604":"code","c19f6340":"code","e15a988e":"code","65feea7c":"code","bc94c186":"code","e9f54844":"code","417b7fed":"code","c39b1825":"code","4690d999":"code","7909765c":"code","7092b1e6":"markdown","26a282a0":"markdown","59fc08c4":"markdown","48341b0b":"markdown","7315dbf0":"markdown","7216b8bc":"markdown","cb26de13":"markdown","5833c32d":"markdown"},"source":{"30cd2cd2":"#commonly used packages\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization & statistics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a2a499ad":"my_data = pd.read_csv(\"\/kaggle\/input\/titanic-data\/titanic.csv\")\nmy_data.head(10)","1428151e":"my_data=my_data.drop(columns=[\"class\",\"embark_town\",\"who\",\"alive\",\"alone\"])\nmy_data.isna().sum() #find out the missing values","a2126ad5":"my_data=my_data.drop(columns=[\"deck\"])\nmy_data[\"age\"]=my_data[\"age\"].fillna(my_data[\"age\"].median()) \nmy_data=my_data.drop(index=(my_data[my_data[\"embarked\"].isnull()==True].index)) #drop the missing value of \"embark\"\nmy_data.isnull().sum()","60e039b5":"sns.countplot(x='survived',data=my_data)","ad2f299e":"sns.countplot(x='embarked',data=my_data)","3d4418e3":"x=my_data[[\"survived\",\"pclass\"]].groupby([\"pclass\"],as_index=True).mean()\nx.plot(kind=\"bar\") # survival rate of each class\nplt.xticks(rotation=0)","e67dce98":"age_cut=pd.cut(my_data[\"age\"],[0,15,35,50,80],labels=[\"0\",\"1\",\"2\",\"3\"]) # Dividing age into four major categories\nmy_data[\"age\"]=age_cut","6229887f":"y=my_data[[\"age\",\"survived\"]].groupby([\"age\"]).mean()\ny.plot(kind=\"bar\")\nplt.xticks(rotation=0)","2e516a67":"# draw histogram for Sex vs. Survived\nfig = plt.figure()\nax = sns.countplot(x='sex', hue='survived', data=my_data)","cba964cc":"from sklearn.preprocessing import LabelEncoder\ntitanic_encoded = my_data.copy() # keep the original, as we ll need both in different scenarios\nfor label in ['pclass', 'embarked',\"sex\"]:\n    titanic_encoded[label] = LabelEncoder().fit_transform(titanic_encoded[label])\n    \ntitanic_encoded.head()","b97a0dc1":"sns.set(style='ticks', color_codes=True)\nplt.figure(figsize=(10, 12))\nsns.heatmap(titanic_encoded.astype(float).corr(), \n            linewidths=0.1, \n            square=True, \n            linecolor='white', \n            annot=True)\nplt.show()","d012933b":"from sklearn import preprocessing\nscaler=preprocessing.StandardScaler()\nscaler.fit(np.array(my_data[\"fare\"]).reshape(-1,1))\nx=scaler.transform(np.array(my_data[\"fare\"]).reshape(-1,1)) #standardize \"fare\"","77dd2bd6":"my_data[\"fare\"]=x\nmy_data","e25b5604":"# split into train and test data\nfrom sklearn.model_selection import train_test_split\nY=titanic_encoded['survived']\ndel titanic_encoded['survived']\nX_train, X_test, Y_train, Y_test= train_test_split(titanic_encoded, Y, test_size=0.2)","c19f6340":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","e15a988e":"round(logreg.score(X_test, Y_test) * 100, 2)","65feea7c":"from sklearn.metrics import accuracy_score\nprint('accuracy={:.2f}\\n'.format(accuracy_score(Y_test, Y_pred)*100))","bc94c186":"# SVM\nsvc  = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","e9f54844":"print('accuracy={:.2f}\\n'.format(accuracy_score(Y_test, Y_pred)*100))","417b7fed":"# knn\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","c39b1825":"# Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","4690d999":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","7909765c":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian]})\nmodels.sort_values(by='Score', ascending=False)","7092b1e6":"### In this case, use random forest to train can get the highest accuracy.","26a282a0":"### The number of these two is not very different, we can say that there's no sparsity of this data.","59fc08c4":"### The first class has a higher survival rate.","48341b0b":"### It seems that \"embarked\t\" and \"embarked_town\" are the same, and \"pcalss\" and \"class\" are the same , so we can just remove one of them.","7315dbf0":"### Men have a higher death rate.","7216b8bc":"### It's obvious that children have highest survival rate.","cb26de13":"## After filling in all values we now are going to introduce some suscriptive statistics and diagrams","5833c32d":"### The variable \"deck\"  has too much missing value, and we consider to remove it. Moreover, we have to fill up the values of other variables. Replacing null values of \"age\" with the median age seems reasonable. "}}