{"cell_type":{"a3db3a12":"code","01a823bd":"code","4e76211d":"code","1132d21f":"code","b11ceb22":"code","3b9fc7d5":"code","b1fe14ea":"code","c8b6333d":"code","9cb23074":"code","a8865283":"code","4451201d":"code","fe84b2da":"code","d7a974ee":"code","77bafcd1":"code","2fa21dbb":"code","52a14e9f":"code","48d517e4":"code","c72ffffd":"code","5379a345":"code","660d3ffe":"code","b3d92b65":"code","4125a23d":"code","cf8a3bd5":"code","6ba0988d":"code","ef6e495c":"code","7256ddf5":"markdown","0c62d7ca":"markdown","fee3d9a4":"markdown","154b7849":"markdown","2e741252":"markdown","73972c93":"markdown","e7df4637":"markdown","2cea508c":"markdown","ac1ae1dc":"markdown","3abc602d":"markdown","5d5b00cd":"markdown","3a57b51e":"markdown","c39ea46a":"markdown","97ea21cb":"markdown","a061dc6f":"markdown","f8dec2cc":"markdown","4344bd3a":"markdown","80966140":"markdown","6c7fe10e":"markdown","f462064f":"markdown","5e815713":"markdown","de9366b0":"markdown"},"source":{"a3db3a12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","01a823bd":"train_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","4e76211d":"len(train_transaction.isFraud[train_transaction.isFraud==1])\/len(train_transaction)","1132d21f":"# Helper functions\n# 1. For calculating % na values in  columns\ndef percent_na(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_df = pd.DataFrame({'column_groups': percent_missing.index,\n                                 'percent_missing': percent_missing.values})\n    return missing_value_df\n# 2. For plotting grouped histograms \ndef sephist(col):\n    yes = train_transaction[train_transaction['isFraud'] == 1][col]\n    no = train_transaction[train_transaction['isFraud'] == 0][col]\n    return yes, no","b11ceb22":"# Helper function for column value details\ndef column_value_freq(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','num_values_'+str(round(cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            num_col_99 = len(colpercount.loc[colpercount['per_count'] > (1- cum_per),])\n        else:\n            num_col_99 = len(colpercount.loc[colpercount['cum_per_count']< cum_per,] )\n        dfpercount=dfpercount.append({'col_name': col,'num_values_'+str(round(cum_per,2)): num_col_99},ignore_index = True)\n    dfpercount['unique_values'] = train_transaction[sel_col].nunique().values\n    dfpercount['unique_value_to_num_values'+str(round(cum_per,2))+'_ratio'] = 100 * (dfpercount['num_values_'+str(round(cum_per,2))]\/dfpercount.unique_values)\n    dfpercount['percent_missing'] = percent_na(train_transaction[sel_col])['percent_missing'].round(3).values\n    return dfpercount\n\ndef column_value_details(sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','values_'+str(round(cum_per,2)),'values_'+str(round(1-cum_per,2))])\n    for col in sel_col:\n        col_value = train_transaction[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            values_freq = colpercount.loc[colpercount['per_count'] > (1- cum_per),'value'].tolist()\n        else:\n            values_freq = colpercount.loc[colpercount['cum_per_count']< cum_per,'value'].tolist() \n        values_less_freq =  [item for item in colpercount['value'] if item not in values_freq]\n        dfpercount=dfpercount.append({'col_name': col,'values_'+str(round(cum_per,2)) : values_freq ,'values_'+str(round(1-cum_per,2)): values_less_freq},ignore_index = True)\n    num_values_per =[]\n    for i in range(len(dfpercount)):\n        num_values_per.append(len(dfpercount['values_'+str(round(cum_per,2))][i]))\n    dfpercount['num_values_per'] = num_values_per\n    return dfpercount","3b9fc7d5":"pd.options.display.max_colwidth =300\nVcols=train_transaction.columns[train_transaction.columns.str.startswith('V')]\ntrain_transaction_vcol_na = percent_na(train_transaction[Vcols])\ntrain_transaction_vcol_na_group= train_transaction_vcol_na.groupby('percent_missing')['column_groups'].unique().reset_index()\nnum_values_per =[]\nfor i in range(len(train_transaction_vcol_na_group)):\n    num_values_per.append(len(train_transaction_vcol_na_group['column_groups'][i]))\ntrain_transaction_vcol_na_group['num_columns_group'] = num_values_per\ntrain_transaction_vcol_na_group","b1fe14ea":"pd.options.display.max_colwidth =300\nVcols=test_transaction.columns[test_transaction.columns.str.startswith('V')]\ntest_transaction_vcol_na = percent_na(test_transaction[Vcols])\ntest_transaction_vcol_na_group= test_transaction_vcol_na.groupby('percent_missing')['column_groups'].unique().reset_index()\nnum_values_per =[]\nfor i in range(len(test_transaction_vcol_na_group)):\n    num_values_per.append(len(test_transaction_vcol_na_group['column_groups'][i]))\ntest_transaction_vcol_na_group['num_columns_group'] = num_values_per\ntest_transaction_vcol_na_group","c8b6333d":"def vcol_multiplot(col,cum_per,ax1):\n    col_freq = column_value_freq(col,cum_per)      \n    plot1=col_freq.plot(x='col_name',y=['unique_values','num_values_'+str(round(cum_per,2))],kind='bar',rot=90,ax = ax1)\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plot1.set(ylabel='Count')\n    plot1= plot1.set(title='Data Details  in each V columns with ' + str(round(col_freq.percent_missing.mean(),4)) +'% missing values')\n    \ndef vcol_plot(col,cum_per):\n    col_freq = column_value_freq(col,cum_per)      \n    plot1=col_freq.plot(x='col_name',y=['unique_values','num_values_'+str(round(cum_per,2))],kind='bar',rot=90)\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plot1.set(ylabel='Count')\n    plot1= plot1.set(title='Data Details  in each V columns with ' + str(round(col_freq.percent_missing.mean(),4)) +'% missing values')","9cb23074":"cum_per = 0.965\nfig, axs = plt.subplots(2,1, figsize=(15, 16), facecolor='w', edgecolor='k',squeeze=False)\naxs=axs.ravel()\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[0],cum_per,axs[0])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[1],cum_per,axs[1])","a8865283":"fig, axs = plt.subplots(4,2, figsize=(15,16), facecolor='w', edgecolor='k',squeeze=False)\n#fig.subplots_adjust(hspace = 0.75, wspace=.001)\naxs = axs.ravel()\nfor i in range(2,10):\n    vcol_multiplot(train_transaction_vcol_na_group.column_groups[i],cum_per,axs[i-2])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","4451201d":"fig, axs = plt.subplots(5,1, figsize=(15,16), facecolor='w', edgecolor='k',squeeze=False)\naxs=axs.ravel()\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[10],cum_per,axs[0])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[11],cum_per,axs[1])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[12],cum_per,axs[2])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[13],cum_per,axs[3])\nvcol_multiplot(train_transaction_vcol_na_group.column_groups[14],cum_per,axs[4])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","fe84b2da":"colfreq=column_value_freq(Vcols,cum_per)\ncolfreqbool = colfreq[colfreq.unique_values==2]\nif len(colfreqbool)%3 == 0:\n    nrow = len(colfreqbool)\/3\nelse:\n    nrow = len(colfreqbool) \/\/ 3 + 1 \nsns.set(rc={'figure.figsize':(14,16)})\nfor num, alpha in enumerate(colfreqbool.col_name):\n    plt.subplot(nrow, 3, num+1)\n    plot1= sns.countplot(data=train_transaction,x=alpha,hue='isFraud')\n    for p in plot1.patches[1:]:\n        h = p.get_height()\n        x = p.get_x()+p.get_width()\/2.\n        if h != 0:\n            plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\n    plt.legend(title='isFraud',loc='upper right')\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","d7a974ee":"def cum_value_count(col):\n    col_value = train_transaction[col].value_counts(normalize=True)\n    colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n    colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n    return colpercount","77bafcd1":"def V_doublecat_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_cat = Vcol_details[Vcol_details['num_values_per'] <= limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_cat)*2)})\n    x=1\n    for num, alpha in enumerate(V_cat.col_name):\n        plt.subplot(len(V_cat),2,x)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_cat),2,x+1)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make only '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","2fa21dbb":"def V_cat_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_cat = Vcol_details[Vcol_details['num_values_per'] <= limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_cat)*2)})\n    x=1\n    for num, alpha in enumerate(V_cat.col_name):\n        plt.subplot(len(V_cat),2,x)\n        sns.countplot(data=train_transaction[train_transaction[alpha].isin (V_cat['values_'+str(round(cum_per,2))][num])],y=alpha,hue='isFraud')\n        plt.legend(loc='lower right')\n        plt.title('Count of unique values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_cat),2,x+1)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_cat['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of values which make '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","52a14e9f":"def V_num_plot(cols,cum_per,limit):\n    Vcol_details=column_value_details(cols,cum_per)\n    V_num = Vcol_details[Vcol_details['num_values_per'] > limit].reset_index()\n    sns.set(rc={'figure.figsize':(14,len(V_num)*2)})\n    x=1\n    for num, alpha in enumerate(V_num.col_name):\n        plt.subplot(len(V_num),2,x)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_num['values_'+str(round(cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_num['values_'+str(round(cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of  values which make '+str(round(cum_per*100,3))+'% of data in column ' + str(alpha) )\n        plt.subplot(len(V_num),2,x+1)\n        yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (V_num['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (V_num['values_'+str(round(1-cum_per,2))][num]))][alpha]\n        plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n        plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n        plt.legend(loc='upper right')\n        plt.title('Histogram of values which make '+str(round((1-cum_per)*100,3))+'% of data in column ' + str(alpha) )\n        x= x+2\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","48d517e4":"colfreqpseudobool = colfreq[(colfreq.unique_values !=2) & (colfreq['num_values_'+str(round(cum_per,2))] <= 2)]","c72ffffd":"pseudoboolcat = colfreqpseudobool[colfreqpseudobool.unique_values <=15]['col_name'].values\nV_doublecat_plot(pseudoboolcat,cum_per,15)","5379a345":"pseudoboolnum = colfreqpseudobool[colfreqpseudobool.unique_values >15]['col_name'].values\n","660d3ffe":"V_cat_plot(pseudoboolnum,cum_per,15)","b3d92b65":"colfreqcat = colfreq[(colfreq.unique_values <=15) & (colfreq['num_values_'+str(round(cum_per,2))] > 2)]\ncolfreqcat ","4125a23d":"colfreqpseudocat = colfreq[(colfreq.unique_values >15) & (colfreq['num_values_'+str(round(cum_per,2))] <= 15) & (colfreq['num_values_'+str(round(cum_per,2))]> 2)]\n","cf8a3bd5":"V_cat_plot(colfreqpseudocat.col_name,cum_per,15)","6ba0988d":"colfreqnum = colfreq[colfreq['num_values_'+str(round(cum_per,2))]>15]\n","ef6e495c":"V_num_plot(colfreqnum.col_name,cum_per,15)","7256ddf5":"It looks like the Pseudo Boolean and Pseudo Categorical columns are important as in both tpes there is a higher proportion of fraud cases when the values fall with less than 3.5% of column data unique values","0c62d7ca":"There are 44 Columns in this category","fee3d9a4":"221 Columns starting with V have 96.5% of their values covered by one or two values\n","154b7849":"In all these columns the more frequent values are in the lower range in both cases.","2e741252":"The remaining 113 have more than 15 unique values. Let's look at data distribution in these columns.These are Pseudo Boolean numerical type columns","73972c93":"**Pseudo Booleans**","e7df4637":"**Conclusion**","2cea508c":"In some of these columns  a higher proportion of fraud cases are seen  for values which form less than 3.5% of the column data","ac1ae1dc":"Let's look at Data distribution in these columns","3abc602d":"Let's check whether similiar pattern exists in the test_transaction data.\nThe same groups exist in test data also !!!.","5d5b00cd":"There are 67 columns in this category. Let's look at how data is distributed in these columns","3a57b51e":"Of these 108 columns have only less than 15 uniques values . Let's look at data distribution in these columns.These are Pseudo Boolean categorical type columns","c39ea46a":"In all these columns almost all values is 1. However except for V305 all values even though minimal which are not 1 are from fraud transactions.","97ea21cb":"Based on the  data distribution columns can be divided into 5 types.\n\n1. **Boolean** - columns  with only two unique values\n\n2. **Pseudo- Boolean**  - columns with  96.5% data covered by  maximum two unique values. Within this there are two types.\n        \n        Pseudo-Boolean-categorical - Columns with 15 or less unique values but 96.5% data covered by  maximum two unique values\n        Pseudo-Boolean-numerical - Columns with more than 15 unique values but 96.5% data covered by  maximum two unique values\n\n4. **Pseudo-Categorical**  - Columns with  96.5% data covered by  15 or less unique values\n\n5. **Numerical** - All Other columns\n\n","a061dc6f":"**Pseudo - Categorical**","f8dec2cc":"**Boolean Columns**\n\n","4344bd3a":"Of the 394 columns in train_transaction file 339 columns start with V . In most of the models developed  Vcolumns individually have very low importance and gets removed during feature selection process.The kernel attempts to understand the similiarilties between the various V columns and the distribution of data in these columns inorder to understand various groupings that can be used to create useful features.\n\nThe first grouping is based on the percentage of missing values in the columns . The columns can be divided into 15 groups as below.","80966140":"Kernel on Unique identifer based on C & D columns  https:\/\/www.kaggle.com\/rajeshcv\/curious-case-of-c-columns\n\nKernel on Null Values  https:\/\/www.kaggle.com\/rajeshcv\/tale-of-nulls","6c7fe10e":"**Numerical**","f462064f":"It's interesting to note that in many of the columns some of the unique values which fall in the 3.5% of column data the proportion of fraudulent transactions is in the range of 25% -50%","5e815713":"The histograms of values less than 3.5% of the column data shows a higher proportion of fraud transactions.","de9366b0":"Let's explore the 15 groups of columns to understand how many unique values are there in each of the columns and how many values make 96.5% of the data in each of the columns. (96.5% is chosen as this is the percentage of transactions that is not Fraud.)"}}