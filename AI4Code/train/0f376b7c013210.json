{"cell_type":{"b3f398bf":"code","a97f2ba5":"code","982d812b":"code","2a2d955e":"code","4ec559e0":"code","b3efa636":"code","cd31006b":"code","1ff41ba0":"code","7811d29a":"code","ae8f75bd":"code","9ba0103a":"code","5efdbfb5":"code","f45d3d97":"code","206972ad":"code","0184cbe8":"code","4294fddd":"code","45a77d7e":"code","6ca2aaa9":"code","b718eaab":"code","dffce6c9":"code","e0274aa7":"code","25c0bf1d":"code","e515936e":"code","cf562273":"code","96e899aa":"code","051597be":"code","28a48124":"code","4d566bc4":"code","951c45b7":"code","0c85d2a4":"code","b9123b66":"code","662ac56a":"code","7177b9a0":"code","b263292a":"code","1c461ac4":"code","9a16c225":"code","81fcd243":"code","ebc621a9":"code","7094fc1a":"code","7c235a7f":"code","48e10261":"code","23fd6b00":"code","d7b187c3":"code","1720cbaa":"code","884eccd9":"code","6060428e":"code","1f750574":"code","540d268c":"code","f924f82d":"code","1ef177e4":"code","d7a280db":"code","8b238b18":"code","a6a117a0":"code","4a08b2e1":"markdown","3e324f17":"markdown","f3676e5c":"markdown","d06903ab":"markdown","bfea4d49":"markdown","5801f260":"markdown","d921dd2a":"markdown","ece1bbde":"markdown","6c90e6ba":"markdown","1c80ec6c":"markdown","c91e36cf":"markdown","eb27c122":"markdown","af793207":"markdown"},"source":{"b3f398bf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","a97f2ba5":"print(os.listdir(\"..\/input\/hpcc20steps\/\"))","982d812b":"!pip install tensorflow==1.14.0","2a2d955e":"variables_name = pd.read_csv(\"..\/input\/hpcc20steps\/variables_name.csv\", header=None)\nfeatures = variables_name.values[:,1]","4ec559e0":"variables_name","b3efa636":"features","cd31006b":"import json\nwith open(\"..\/input\/hpcc20steps\/X_train_HPCC_1_20.json\") as of:\n    X_train = np.array(json.load(of))\nwith open(\"..\/input\/hpcc20steps\/y_train_HPCC_1_20.json\") as of:\n    y_train = np.array(json.load(of))\nwith open(\"..\/input\/hpcc20steps\/X_test_HPCC_1_20.json\") as of:\n    X_test = np.array(json.load(of))\nwith open(\"..\/input\/hpcc20steps\/y_test_HPCC_1_20.json\") as of:\n    y_test = np.array(json.load(of))    ","1ff41ba0":"X_train.shape, y_train.shape","7811d29a":"X_test.shape, y_test.shape","ae8f75bd":"from keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.optimizers import Adam\n\n\ndef createModel(l1Nodes, l2Nodes, d1Nodes, d2Nodes, inputShape):\n    # input layer\n    lstm1 = LSTM(l1Nodes, input_shape=inputShape, return_sequences=True)\n    lstm2 = LSTM(l2Nodes, return_sequences=True)\n    flatten = Flatten()\n    dense1 = Dense(d1Nodes)\n    dense2 = Dense(d2Nodes)\n\n    # output layer\n#     outL = Dense(1, activation='relu')\n    outL = Dense(1)\n    # combine the layers\n    layers = [lstm1, lstm2, flatten,  dense1, dense2, outL]\n    # create the model\n    model = Sequential(layers)\n    opt = Adam(learning_rate=0.005)\n    model.compile(optimizer=opt, loss='mse')\n    return model","9ba0103a":"# create model\nmodel = createModel(8, 8, 8, 4, (X_train.shape[1], X_train.shape[2]))\nmodel.fit(X_train, y_train, batch_size=8, epochs=30)","5efdbfb5":"from sklearn.metrics import mean_squared_error as mse","f45d3d97":"y_pred_train = model.predict(X_train)\nmse(y_train, y_pred_train)","206972ad":"y_pred = model.predict(X_test)\nmse(y_test, y_pred)","0184cbe8":"model.summary()","4294fddd":"# Save the entire model to a HDF5 file.\n# The '.h5' extension indicates that the model shuold be saved to HDF5.\nmodel.save('HPCCv1_model.h5') ","45a77d7e":"import shap","6ca2aaa9":"import tensorflow as tf\ntf.__version__","b718eaab":"# Use the training data for deep explainer => can use fewer instances\nexplainer = shap.DeepExplainer(model, X_train)\n# explain the the testing instances (can use fewer instanaces)\n# explaining each prediction requires 2 * background dataset size runs\nshap_values = explainer.shap_values(X_test)\n# init the JS visualization code\nshap.initjs()","dffce6c9":"explainer.expected_value","e0274aa7":"len(shap_values)","25c0bf1d":"X_test.shape","e515936e":"shap_values[0].shape","cf562273":"shap_values[0][0].shape","96e899aa":"# shap.force_plot(explainer.expected_value[0], shap_values[0][0][0,:], features)\nprint(features)\nprint(len(features))","051597be":"i=0\nj=0","28a48124":"shap_values[0][i][j]","4d566bc4":"X_test[i][j].shape\n","951c45b7":"# shap.force_plot(explainer.expected_value[0], shap_values[0][0], features)\ni = 0\nj = 0\nx_test_df = pd.DataFrame(data=X_test[i][j].reshape(1,10), columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i][j], x_test_df)","0c85d2a4":"shap.__version__","b9123b66":"shap_values[0][i].shape","662ac56a":"shap_values[0][0].shape","7177b9a0":"i = 11\npred_i = model.predict(X_test[i:i+1])\nsum_shap_i = shap_values[0][i].sum() + explainer.expected_value[0]\n\npred_i, sum_shap_i","b263292a":"# Plot SHAP for ONLY one observation i\ni = 0\nshap.initjs()\n\nx_test_df = pd.DataFrame(data=X_test[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)\n## Problem:  Can not take into account many observations at the same time.\n### The pic below explain for only 1 observation of 20 time steps, each time step has 10 features.","1c461ac4":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap_average_value = np.abs(shap_values[0]).mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_value, x_average_value)","9a16c225":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider average (+ is different from -)\nshap_average_value = shap_values[0].mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(explainer.expected_value[0], shap_average_value, x_average_value)","81fcd243":"shap_values_2D = shap_values[0].reshape(-1,10)\nX_test_2D = X_test.reshape(-1,10)\n\n\nshap_values_2D.shape, X_test_2D.shape","ebc621a9":"x_test_2d = pd.DataFrame(data=X_test_2D, columns = features)","7094fc1a":"x_test_2d.corr()","7c235a7f":"shap.summary_plot(shap_values_2D, x_test_2d)","48e10261":"shap.summary_plot(shap_values_2D, x_test_2d, plot_type=\"bar\")","23fd6b00":"len_test_set = X_test_2D.shape[0]\nlen_test_set","d7b187c3":"## SHAP for each time step\nNUM_STEPS = 20\nNUM_FEATURES = 10\n\n\n# step = 0\nfor step in range(NUM_STEPS):\n    indice = [i for i in list(range(len_test_set)) if i%NUM_STEPS == step]\n    shap_values_2D_step = shap_values_2D[indice]\n    x_test_2d_step = x_test_2d.iloc[indice]\n    print(\"_______ time step {} ___________\".format(step))\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step, plot_type=\"bar\")\n    shap.summary_plot(shap_values_2D_step, x_test_2d_step)\n    print(\"\\n\")","1720cbaa":"# X_train_outlier\nwith open(\"..\/input\/hpcc20steps\/X_train_outlier.json\") as of:\n    X_train_outlier = np.array(json.load(of))\nwith open(\"..\/input\/hpcc20steps\/y_train_outlier.json\") as of:\n    y_train_outlier = np.array(json.load(of))\n\n    # X_train_normal\nwith open(\"..\/input\/hpcc20steps\/X_train_not_outlier.json\") as of:\n    X_train_not_outlier = np.array(json.load(of))\nwith open(\"..\/input\/hpcc20steps\/y_train_not_outlier.json\") as of:\n    y_train_not_outlier = np.array(json.load(of))","884eccd9":"## OUTLIERS\nshap_values = explainer.shap_values(X_train_outlier)\ni = 0\nx_test_df = pd.DataFrame(data=X_train_outlier[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)","6060428e":"## NON-OUTLIERS\nshap_values = explainer.shap_values(X_train_not_outlier)\ni = 0\nx_test_df = pd.DataFrame(data=X_train_not_outlier[i], columns = features)\nshap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_df)","1f750574":"y_train_not_outlier[0]","540d268c":"y_train_outlier[0]","f924f82d":"# Use the training data for deep explainer => can use fewer instances\nexplainer_2 = shap.GradientExplainer(model, X_train)\n# explain the the testing instances (can use fewer instanaces)\n# explaining each prediction requires 2 * background dataset size runs\nshap_values_2 = explainer_2.shap_values(X_test)\n# init the JS visualization code\nshap.initjs()","1ef177e4":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap_average_abs_value_2 = np.abs(shap_values_2[0]).mean(axis=0)\n\nx_average_value = pd.DataFrame(data=X_test.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_abs_value_2, x_average_value)","d7a280db":"################# Plot AVERAGE shap values for ALL observations  #####################\n## Consider ABSOLUTE of SHAP values ##\nshap.initjs()\nshap_values_train = explainer.shap_values(X_train)\n\nshap_average_abs_value_train = np.abs(shap_values_train[0]).mean(axis=0)\n\nx_average_value_train = pd.DataFrame(data=X_train.mean(axis=0), columns = features)\nshap.force_plot(0, shap_average_abs_value_train, x_average_value_train)","8b238b18":"shap_values_train_2D = shap_values_train[0].reshape(-1,10)\nX_train_2D = X_train.reshape(-1,10)\n\n\nshap.summary_plot(shap_values_train_2D, X_train_2D, features)","a6a117a0":"# COLOR: https:\/\/seaborn.pydata.org\/tutorial\/color_palettes.html\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i, feature in enumerate(features):\n    print(feature)\n\n    plt.figure(figsize = (8,6)) \n    tmp = shap_values_train[0][:,:,i].reshape((-1,20))\n    print(tmp.shape)\n    plot_shap = sns.heatmap(tmp, cmap=\"coolwarm\")\n    plt.show(plot_shap)\n    print(\"-----------\")","4a08b2e1":"** Some Comments **\n\n- *CPU1 Temp*: Light color at early time steps. It starts bolder from 10th to 18th steps => These steps play an important roles in prediction.\n(recall that output is the sum of 20*10 importance scores)\n- *CPU2 Temp*: time step 9th, 10th have light color; some in the end have darker color.\n- *Inlet Temp* & *Power consumption*: Early time steps have the most impact on the prediction.\n- *CPU Load*: Almost in blue => Has negative impact on the prediction\n- *Memory Usage*: As opposed to *CPU Load*\n- *Fan1* & *Fan 3*: almost blue; while *Fan2 & 4* are in orange","3e324f17":"## GradientExplainer","f3676e5c":"# Start here","d06903ab":"**Some Comments**:\n- *CPU1 Temp* is the most important feature\n- *Fan3 Speed* and *Fan2 Speed* seem to have positive corr with the output. Conversely, *Fan4* and *Fan1* have negative relationships with the output.\n- *CPU Load* doesn't have a clear linear relationship with the output (red dots at the both sides)","bfea4d49":"# Model","5801f260":"## Outliers vs Non-Outliers","d921dd2a":"## Check sum of shap values vs prediction","ece1bbde":"# Importance for each training instance with SHAP GradientExplainer ","6c90e6ba":"## DeepSHAP","1c80ec6c":"# Data","c91e36cf":"# SHAP","eb27c122":"They are the same. It looks ok","af793207":"### AVERAGE shap for ALL Obs"}}