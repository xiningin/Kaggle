{"cell_type":{"d7bb6b20":"code","78b384b9":"code","9fed6f4e":"code","83ef4da2":"code","4508e92b":"code","1148887e":"code","37b5c532":"code","5253b3ae":"code","5dd85d5f":"code","588e9b3a":"code","1846a09b":"code","139a6de7":"code","f55f8afa":"code","ae88dbd1":"code","8c99ea35":"code","98542445":"code","8e3de07b":"code","94054dde":"code","35b6526f":"code","335b74e1":"code","83d763d1":"code","90595f79":"code","924998fb":"code","de3e15b2":"code","c278a5a7":"code","b083a017":"code","261dd5ec":"code","a485e650":"code","04a926c2":"code","f290a897":"code","b9d932e5":"code","af41c7fc":"code","e4a71933":"code","9acb3f59":"code","9e3664bf":"code","a83cf39b":"code","961ed37c":"code","5afa23ae":"code","72dcf9e8":"code","67bf6e12":"code","a5367f8a":"code","c45fcfac":"code","b0ff025e":"code","397ce8be":"code","7b60e9e0":"code","a5fd8c2e":"code","db3ca31d":"code","717bafe4":"code","8bce7528":"code","59451e27":"code","e620c43d":"code","01d3a32a":"code","575b51b6":"code","79be310e":"code","afc7c1b4":"code","2253d042":"code","ae3969a3":"code","100e4110":"code","76107932":"code","3e3ae3d6":"code","c77e6b1b":"code","962991b6":"code","e6589d12":"code","9f2fb72e":"code","5d049eff":"code","7103e179":"code","eddb4954":"code","0de8c4b1":"code","e7ce68e1":"code","de45d46d":"code","27e47238":"code","e36196ed":"code","347ff2b0":"code","c05f796d":"code","62c96a28":"code","2c345a88":"code","b094ea56":"code","285a9d42":"code","182d8c32":"code","6001c7ff":"code","28ae7d77":"code","c6418095":"code","df222d30":"code","e9bfe931":"code","d8436fc6":"code","1cecf7d0":"code","2264249c":"code","79f07da2":"code","75e1385e":"code","96024eb7":"code","558aeebc":"code","5c5949d3":"code","8ef11073":"code","9ab37f82":"code","1946e332":"code","e99b080e":"code","3fcd4e39":"code","5d5bf9c6":"code","e1a29f71":"markdown","2fcf3518":"markdown","0b448143":"markdown","eddeb3d1":"markdown","2dfe35c7":"markdown","d53f0643":"markdown","4a340364":"markdown","68c0b825":"markdown","1d86e224":"markdown","d0106245":"markdown","4ad283e2":"markdown","e8f7b094":"markdown","07e60b33":"markdown","0c3ee404":"markdown","4be35927":"markdown","6c5491cf":"markdown","e5f8a905":"markdown","2bada6bd":"markdown","5fe22455":"markdown","4fe7d61d":"markdown","458ff345":"markdown","631ba37f":"markdown","773ff172":"markdown","f903a206":"markdown","d82ac5b1":"markdown","de47efdc":"markdown","33a799e1":"markdown","3898e6d7":"markdown","426190f1":"markdown","eb47997d":"markdown","9582da67":"markdown","5472928c":"markdown","564a3c5e":"markdown","ef71c258":"markdown","2de44d3b":"markdown","f0780734":"markdown","86560d69":"markdown","e69ef99c":"markdown","1b68b8dd":"markdown","7c10935c":"markdown","da7dd0c0":"markdown","c93d0f34":"markdown","e9ad8329":"markdown","d615c7ec":"markdown","322be78a":"markdown","b1bcd749":"markdown","d6d405ca":"markdown","c49025d8":"markdown","c9fae9da":"markdown","7a3baffe":"markdown","447f882c":"markdown"},"source":{"d7bb6b20":"# The information are from here:\n# https:\/\/www.kaggle.com\/sk1812\/titanic-ml-model\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# ML algorithms from scikit;\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78b384b9":"# Get train\/test data\n# Notice that train and test have same columns EXCEPT survial;\ntitanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic.head(10)","9fed6f4e":"titanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_test.head(10)","83ef4da2":"# TO DO\n# Describe the data;\ntitanic.info()\ntitanic_test.info()","4508e92b":"titanic.describe() \n","1148887e":"titanic_test.describe() ","37b5c532":"# Function to check the missing percent of a DatFrame;\ndef check_missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","5253b3ae":"# TODO\n# Check the data missing rate of titanic and titanic_test;\ncheck_missing_data(titanic)","5dd85d5f":"titanic.drop(\"PassengerId\", axis=1, inplace=True)\ntitanic.head()","588e9b3a":"# TOTO\n# Fill cabin with 0 if NaN; otherwise 1. Do this for both titanic and titanic_test;\ntitanic['Cabin'] = titanic['Cabin'].replace(np.nan, 0)\ntitanic.loc[titanic[\"Cabin\"] != 0, \"Cabin\"] = 1\n\ntitanic_test['Cabin'] = titanic_test['Cabin'].replace(np.nan, 0)\ntitanic_test.loc[titanic_test[\"Cabin\"] != 0, \"Cabin\"] = 1\n\ntitanic[\"Cabin\"] = titanic[\"Cabin\"].astype(np.int32)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].astype(np.int32)","1846a09b":"titanic.describe() ","139a6de7":"titanic[\"Age\"].mean()","f55f8afa":"# TODO\n# Fill 'Age' and 'Fare' missing data;\ntitanic[\"Age_aver\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean(),inplace = False)\ntitanic_test[\"Age_aver\"] = titanic_test[\"Age\"].fillna(titanic_test[\"Age\"].mean(),inplace = False)\n\ntitanic[\"Fare_aver\"] = titanic[\"Fare\"].fillna(titanic[\"Fare\"].mean(),inplace = False)\ntitanic_test[\"Fare_aver\"] = titanic[\"Fare\"].fillna(titanic_test[\"Fare\"].mean(),inplace = False)","ae88dbd1":"# TODO\n# Chekck again if there is missing data;\n\ncheck_missing_data(titanic)\n# check_missing_data(titanic_test)","8c99ea35":"check_missing_data(titanic_test)","98542445":"titanic[\"Embarked\"].value_counts()","8e3de07b":"titanic[\"Embarked\"].fillna(\"S\",inplace = True)","94054dde":"check_missing_data(titanic)","35b6526f":"# Function of drawing graph;\ndef draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 5,height ,ha= \"center\")","335b74e1":"# Draw survided vs. non-survived of trainign data.\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph= sns.countplot(x='Survived', hue=\"Survived\", data=titanic)\ndraw(graph)","83d763d1":"# Cabin and survived;\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Cabin\", hue =\"Survived\", data = titanic)\ndraw(graph)","90595f79":"# TODO\n# Do other plots, such as Sex vs Survived; Pclass vs Survived, and so on;","924998fb":"# Cabin and survived;\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Sex\", hue =\"Survived\", data = titanic)\ndraw(graph)","de3e15b2":"# Cabin and survived;\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Pclass\", hue =\"Survived\", data = titanic)\ndraw(graph)","c278a5a7":"# Cabin and survived;\nsns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\ngraph  = sns.countplot(x =\"Cabin\", hue =\"Survived\", data = titanic)\ndraw(graph)","b083a017":"# TODO\n# Correlation of columns;\ncorr_matrix = titanic.corr()\nsns.heatmap(corr_matrix, annot=True);","261dd5ec":"all_data = [titanic, titanic_test]","a485e650":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\ntitanic['Sex'].value_counts()","04a926c2":"# Use bin to group ages to bins;\nfor dataset in all_data:\n    dataset['Age_aver'] = dataset['Age_aver'].astype(int)\n    dataset.loc[ dataset['Age_aver'] <= 15, 'Age_aver'] = 0\n    dataset.loc[(dataset['Age_aver'] > 15) & (dataset['Age_aver'] <= 20), 'Age_aver'] = 1\n    dataset.loc[(dataset['Age_aver'] > 20) & (dataset['Age_aver'] <= 26), 'Age_aver'] = 2\n    dataset.loc[(dataset['Age_aver'] > 26) & (dataset['Age_aver'] <= 28), 'Age_aver'] = 3\n    dataset.loc[(dataset['Age_aver'] > 28) & (dataset['Age_aver'] <= 35), 'Age_aver'] = 4\n    dataset.loc[(dataset['Age_aver'] > 35) & (dataset['Age_aver'] <= 45), 'Age_aver'] = 5\n    dataset.loc[ dataset['Age_aver'] > 45, 'Age_aver'] = 6\ntitanic['Age_aver'].value_counts()","f290a897":"# Combine SibSp and Parch as new feature; \n# Combne train test first;\nall_data=[titanic,titanic_test]\n\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1\n    plt.figure(figsize = (8, 5))\nag = sns.countplot(x='Sex', hue='Family', data=titanic)","b9d932e5":"# Create categorical of fare to plot fare vs Pclass first;\nfor dataset in all_data:\n    dataset['Fare_cat'] = pd.cut(dataset['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','median_fare','Average_fare','high_fare'])\nplt.figure(figsize = (8, 5))\nag = sns.countplot(x='Pclass', hue='Fare_cat', data=titanic)","af41c7fc":"titanic.head(3)","e4a71933":"# Re-organize the data; keep the columns with useful features;\ninput_cols = ['Pclass',\"Sex\",\"Age_aver\",\"Cabin\",\"Family\"]\noutput_cols = [\"Survived\"]\n","9acb3f59":"# models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\n\n# other utils\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\nimport numpy as np\nimport statsmodels.stats.proportion as sp\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n","9e3664bf":"def algorithm_show(data, input_cols, output_cols):\n    \"\"\"\n    check the score of used algorithms\n    \n    @author: chenshuyi\n    \n    \"\"\"\n\n    temp = data.copy()\n    temp.reset_index(drop=True, inplace = True)\n\n    temp_X = temp[input_cols]\n#     temp_X = pd.get_dummies(temp_X)\n    print(temp_X.head(1))\n    X_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\n    # modeling\n    rf_model = RandomForestClassifier(n_estimators=100, max_depth=6,class_weight='balanced',random_state=20026)\n    rf_model.fit(X_train,y_train.values.ravel())\n    \n    lr_model = LogisticRegression(class_weight='balanced',random_state=20026)\n    lr_model.fit(X_train,y_train.values.ravel())\n    \n    knn_3 = KNeighborsClassifier(n_neighbors=3)\n    knn_3.fit(X_train,y_train)\n    knn_5 = KNeighborsClassifier(n_neighbors=5)\n    knn_5.fit(X_train,y_train.values.ravel())\n    \n    clf = svm.SVC()\n    clf.fit(X_train,y_train.values.ravel())\n    \n    clf_lin = svm.LinearSVC()\n    clf_lin.fit(X_train,y_train.values.ravel())\n    \n    gnb = GaussianNB()\n    gnb.fit(X_train, y_train.values.ravel())\n    \n    lgb_model = LGBMClassifier(max_depth=6,class_weight='balanced',random_state=20026)\n    lgb_model.fit(X_train,y_train.values.ravel())\n    \n    dt = tree.DecisionTreeClassifier()\n    dt.fit(X_train,y_train.values.ravel())\n    \n    n=len(y_train[y_train[output_cols]==0])\/len(y_train[y_train[output_cols]==1])\n    xgb_model = XGBClassifier(max_depth=6,scale_pos_weight=n,random_state=20026,importance_type='weight')\n    xgb_model.fit(X_train,y_train.values.ravel())\n\n    def pred_result(model,x,y):\n        ypred = model.predict(x)\n        \n        # SVM dont have prabability prediction, so no auc_score\n        try:\n            pscore = model.predict_proba(x)[:,1]\n            fpr, tpr, thresholds = metrics.roc_curve(y, pscore)\n            auc_score = metrics.auc(fpr, tpr)\n        except:\n            auc_score = \"NaN\"\n            \n        accuracy = metrics.accuracy_score(y, ypred)\n        precision = metrics.precision_score(y, ypred)\n        precision_0 = metrics.precision_score(y, ypred,pos_label=0)\n        recall = metrics.recall_score(y, ypred)\n        recall_0 = metrics.recall_score(y, ypred,pos_label=0)\n        f1score = metrics.f1_score(y, ypred)\n        f1score_0 = metrics.f1_score(y, ypred,pos_label=0)\n        accuracy_insample = model.score(X_train[input_cols],y_train)\n        return [accuracy,auc_score,precision,recall,f1score,precision_0,recall_0,f1score_0,accuracy_insample]\n\n    result = pd.DataFrame([],index=['accuracy','auc_score','precision','recall','f1score','precision_0','recall_0','f1score_0','accuracy_insample'])\n\n    result['LogisticRegression'] = pred_result(lr_model,X_test,y_test)\n    result['KNN_5'] = pred_result(knn_5,X_test,y_test)\n    result['KNN_3'] = pred_result(knn_3,X_test,y_test)\n    result['LSVM'] = pred_result(clf_lin,X_test,y_test)\n    result['SVM'] = pred_result(clf,X_test,y_test)\n    result['ngb'] = pred_result(gnb,X_test,y_test)\n    result['LightGBM'] = pred_result(lgb_model,X_test,y_test)\n    result['XGBoost'] = pred_result(xgb_model,X_test,y_test)\n    result['decisiontree'] = pred_result(dt,X_test,y_test)\n    result['RandomForest'] = pred_result(rf_model,X_test,y_test)\n\n    return result","a83cf39b":"algorithm_show(titanic, input_cols, output_cols)","961ed37c":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom xgboost import plot_importance","5afa23ae":"def change_name(dataframe):\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Miss.|Ms.|Ms|Countess.|Mlle.\"), \"Name\"] = \"Miss\"\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Mrs.|Mrs|Mme.\"), \"Name\"] = \"Mrs\"\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Master.|Master\"), \"Name\"] = \"Master\"\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Mr.|Mr\"), \"Name\"] = \"Mr\"\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Dr.|Dr|Don.|Rev.|Rev\"), \"Name\"] = \"Professional\"\n    dataframe.loc[dataframe[\"Name\"].str.contains(\"Major.|Major|Capt.|Capt|Col| Col.\"), \"Name\"] = \"Military\"\n    \n    dataframe.loc[~dataframe[\"Name\"]. \\\n           str.contains(\"Miss|Master|Mr|Professional|Military\"), \"Name\"] = \"NaN\"\n    \n    dataframe.loc[(dataframe[\"Name\"] == \"NaN\") &\n           (dataframe[\"Sex\"] == 1) &  (dataframe[\"Age\"] <= 23), \"Name\"] = \"Miss\"\n    dataframe.loc[(dataframe[\"Name\"] == \"NaN\") &\n           (dataframe[\"Sex\"] == 1), \"Name\"] = \"Mrs\"\n    dataframe.loc[(dataframe[\"Name\"] == \"NaN\") &\n           (dataframe[\"Sex\"] == 0), \"Name\"] = \"Mr\"\n    \n    print(dataframe[\"Name\"].value_counts())\n","72dcf9e8":"def my_encoder(train, test, column):\n    \n    train = train.loc[:, ~train.columns.str.startswith(column + \"_\")]\n    test = test.loc[:, ~test.columns.str.startswith(column + \"_\")]\n\n    n_encoder = OneHotEncoder(sparse=False)\n\n    # fit encoder\n    # If you run below line as \"n_encoder.fit(train_X[\"Name\"])\"\n    # with one square parantheses, you will get an error.\n    n_encoder.fit(train[[column]])\n\n    # transform Name column of train_X\n    name_one_hot_np = n_encoder.transform(train[[column]])\n    name_one_hot_np_t = n_encoder.transform(test[[column]])\n    \n    # convert datatype to int64\n    name_one_hot_np = name_one_hot_np.astype(np.int32)\n    name_one_hot_np_t = name_one_hot_np_t.astype(np.int32)\n    \n    alist = []\n    for i in range(train[column].nunique()):\n        alist.append(column + \"_\" + str(i+1))\n\n    # output of transform is a numpy array, convert it to Pandas dataframe\n    name_one_hot_df = pd.DataFrame(name_one_hot_np,\n                        columns=alist)\n    # concatenate name_one_hot_df to train_X\n    train = pd.concat([train, name_one_hot_df], axis=1)\n    \n    # drop categorical Name column\n    # dataframe.drop(column, axis=1, inplace=True)\n    train.drop(column + \"_\" + str(i+1), axis=1, inplace=True)\n    \n\n    # output of transform is a numpy array, convert it to Pandas dataframe\n    name_one_hot_df = pd.DataFrame(name_one_hot_np_t,\n                        columns=alist)\n    # concatenate name_one_hot_df to train_X\n    test = pd.concat([test, name_one_hot_df], axis=1)\n    \n    # drop categorical Name column\n    # dataframe.drop(column, axis=1, inplace=True)\n    test.drop(column + \"_\" + str(i+1), axis=1, inplace=True)\n    \n    \n    return train, test","67bf6e12":"change_name(titanic)\nchange_name(titanic_test)","a5367f8a":"titanic, titanic_test=my_encoder(titanic, titanic_test, \"Name\")\ntitanic, titanic_test=my_encoder(titanic, titanic_test, \"Embarked\")\n","c45fcfac":"# Re-organize the data; keep the columns with useful features;\ninput_cols = ['Pclass',\"Sex\",\"Age\",\"Cabin\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\"]\noutput_cols = [\"Survived\"]","b0ff025e":"def xgb_filler(train_X,test_X,column,input_cols,output_cols):\n    \n    reg_train = train_X[input_cols]\n    reg_test = test_X[input_cols]\n\n    # get only rows with non-null values\n    reg_train_X = reg_train[reg_train[column].notnull()].copy()\n    reg_train_Y = reg_train_X[[column]]\n    reg_train_X.drop(column, axis=1, inplace=True)\n    \n    reg_test_X = reg_test[reg_test[column].notnull()].copy()\n    reg_test_Y = reg_test_X[[column]]\n    reg_test_X.drop(column, axis=1, inplace=True)    \n    \n    \n    reg = xgb.XGBRegressor(colsample_bylevel=0.7,\n                       colsample_bytree=0.5,\n                       learning_rate=0.3,\n                       max_depth=5,\n                       min_child_weight=1.5,\n                       n_estimators=18,\n                       subsample=0.9)\n\n    reg.fit(reg_train_X, reg_train_Y)\n    \n    all_predicted = reg.predict(train_X[input_cols].loc[:, (train_X[input_cols].columns != column)\n                     ])\n    \n    train_X.loc[train_X[column].isnull(),\n                    column] = all_predicted[train_X[column].isnull()]\n    \n    all_predicted_t = reg.predict(test_X[input_cols].loc[:, (test_X[input_cols].columns != column)\n                     ])\n    \n    test_X.loc[test_X[column].isnull(),\n                           column] = all_predicted_t[test_X[column].isnull()]\n","397ce8be":"xgb_filler(titanic,titanic_test,'Age',input_cols,output_cols)","7b60e9e0":"xgb_filler(titanic,titanic_test,'Fare',input_cols,output_cols)","a5fd8c2e":"titanic.info()","db3ca31d":"titanic_test.info()","717bafe4":"all_data = [titanic, titanic_test]\n# Create categorical of fare to plot fare vs Pclass first;\nfor dataset in all_data:\n    dataset['Fare_cat'] = pd.cut(dataset['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','median_fare','Average_fare','high_fare'])","8bce7528":"check_missing_data(titanic)","59451e27":"# Get train\/test data\n# Notice that train and test have same columns EXCEPT survial;\ntitanic_cabin = pd.read_csv('..\/input\/titanic\/train.csv')[\"Cabin\"]\ntitanic_test_cabin = pd.read_csv('..\/input\/titanic\/test.csv')[\"Cabin\"]\ntitanic_cabin.value_counts()","e620c43d":"147\/len(titanic_cabin)","01d3a32a":"titanic['Cabin'] = titanic_cabin.str[:1]\ntitanic_test['Cabin'] = titanic_test_cabin.str[:1]\nprint(titanic[\"Cabin\"].value_counts())","575b51b6":"titanic['Cabin'] = titanic['Cabin'].replace(np.nan, \"N\")\ntitanic_test['Cabin'] = titanic_test['Cabin'].replace(np.nan, \"N\")","79be310e":"titanic,titanic_test=my_encoder(titanic, titanic_test, \"Cabin\")\ntitanic,titanic_test=my_encoder(titanic, titanic_test, \"Pclass\")","afc7c1b4":"check_missing_data(titanic)","2253d042":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\",\"Cabin_8\"]\noutput_cols = [\"Survived\"]","ae3969a3":"algorithm_show(titanic, input_cols, output_cols)","100e4110":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Parch\",\"SibSp\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\",\"Cabin_8\"]\noutput_cols = [\"Survived\"]","76107932":"algorithm_show(titanic, input_cols, output_cols)","3e3ae3d6":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\",\"Cabin_8\"]\noutput_cols = [\"Survived\"]","c77e6b1b":"temp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = train_test_split(X_train, y_train,\n                                                            test_size=0.2, random_state=20026) ","962991b6":"cls = xgb.XGBClassifier()\n\nparameters = {\n    \"colsample_bylevel\": np.arange(0.4, 1.0, 0.1),\n    \"colsample_bytree\": np.arange(0.7, 1.0, 0.1),\n    \"learning_rate\": [0.4, 0.45, 0.5, 0.55],\n    \"max_depth\": np.arange(4, 6, 1),\n    \"min_child_weight\": np.arange(1, 2, 0.5),\n    \"n_estimators\": [8, 10, 12],\n    \"subsample\": np.arange(0.6, 1.0, 0.1)\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=20026, shuffle=False)\n\ngscv = GridSearchCV(\n    estimator=cls,\n    param_grid=parameters,\n    scoring=\"roc_auc\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=True,\n    refit=True,\n)\n\ngscv.fit(train_fold_X, train_fold_Y.values.ravel());","e6589d12":"cls2 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls2.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls2.predict_proba(valid_fold_X)","9f2fb72e":"train_fold_Y.describe()","5d049eff":"# Compute ROC AUC score\nsplit_score = metrics.roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = metrics.roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls2\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = metrics.accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))\nplot_importance(cls2);","7103e179":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\",\"Cabin_8\"]\noutput_cols = [\"Survived\"]","eddb4954":"temp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = train_test_split(X_train, y_train,\n                                                            test_size=0.2, random_state=20026) \n\ncls3 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls3.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls3.predict_proba(valid_fold_X)\n\n# Compute ROC AUC score\nsplit_score = metrics.roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = metrics.roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls3\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = metrics.accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))\nplot_importance(cls3);","0de8c4b1":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\",\"Cabin_8\"]\noutput_cols = [\"Survived\"]","e7ce68e1":"temp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n","de45d46d":"def pred_result_xgb(model,x,y,opt_thr):\n    pscore = model.predict_proba(x)[:,1]\n    ypred = [1 if x > opt_thr else 0 for x in pscore]\n    fpr, tpr, thresholds = metrics.roc_curve(y, pscore)\n    auc_score = metrics.auc(fpr, tpr)\n\n    accuracy = metrics.accuracy_score(y, ypred)\n    precision = metrics.precision_score(y, ypred)\n    precision_0 = metrics.precision_score(y, ypred,pos_label=0)\n    recall = metrics.recall_score(y, ypred)\n    recall_0 = metrics.recall_score(y, ypred,pos_label=0)\n    f1score = metrics.f1_score(y, ypred)\n    f1score_0 = metrics.f1_score(y, ypred,pos_label=0)\n    accuracy_insample = model.score(X_train[input_cols],y_train)\n    return [accuracy,auc_score,precision,recall,f1score,precision_0,recall_0,f1score_0,accuracy_insample]\n\nresult = algorithm_show(titanic, input_cols, output_cols)\nresult['XGBoost_tune'] = pred_result_xgb(cls2,X_test[input_cols],y_test,0.4167)","27e47238":"result","e36196ed":"# for submission, first find optimal parameters in the entire training set\ntemp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = X_train, X_test, y_train, y_test\n\ncls = xgb.XGBClassifier()\n\nparameters = {\n    \"colsample_bylevel\": np.arange(0.4, 1.0, 0.1),\n    \"colsample_bytree\": np.arange(0.7, 1.0, 0.1),\n    \"learning_rate\": [0.4, 0.45, 0.5, 0.55],\n    \"max_depth\": np.arange(4, 6, 1),\n    \"min_child_weight\": np.arange(1, 2, 0.5),\n    \"n_estimators\": [8, 10, 12],\n    \"subsample\": np.arange(0.6, 1.0, 0.1)\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=20026, shuffle=False)\n\ngscv = GridSearchCV(\n    estimator=cls,\n    param_grid=parameters,\n    scoring=\"roc_auc\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=True,\n    refit=True,\n)\n\ngscv.fit(train_fold_X, train_fold_Y.values.ravel());\n\ncls4 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls4.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls4.predict_proba(valid_fold_X)\n\n# Compute ROC AUC score\nsplit_score = metrics.roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = metrics.roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls4\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = metrics.accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))\nplot_importance(cls4);","347ff2b0":"cls_v1 = cls4\n\ntest_predict_proba = cls_v1.predict_proba(titanic_test[input_cols])\n\n# Optimal threshold we found previously is opt_thr\ny_pred_xgb = [1 if x > opt_thr else 0 for x in test_predict_proba[:,1]]","c05f796d":"# ~~~~~~~~~~~~~~~~~ Example of Submission File ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": y_pred_xgb   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission_xgb.csv', index=False)\nsubmission = pd.read_csv('submission_xgb.csv')\nsubmission.head(20)","62c96a28":"rf_model = RandomForestClassifier(n_estimators=100, max_depth=6,class_weight='balanced',random_state=20026)\nrf_model.fit(titanic[input_cols], titanic[output_cols].values.ravel())\n\ny_pred_lr = rf_model.predict(titanic_test[input_cols])\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": y_pred_xgb   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission_rf.csv', index=False)\nsubmission = pd.read_csv('submission_rf.csv')\nsubmission.head(20)","2c345a88":"lr_model = LogisticRegression(class_weight='balanced',random_state=20026)\nlr_model.fit(titanic[input_cols], titanic[output_cols].values.ravel())\n\ny_pred_lr = lr_model.predict(titanic_test[input_cols])\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": y_pred_xgb   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission_lr.csv', index=False)\nsubmission = pd.read_csv('submission_lr.csv')\nsubmission.head(20)","b094ea56":"# Check if submit file exists\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","285a9d42":"titanic = titanic.loc[:, ~titanic.columns.str.startswith('Cabin_')]\ntitanic_test = titanic_test.loc[:, ~titanic_test.columns.str.startswith('Cabin_')]","182d8c32":"titanic.head(1)","6001c7ff":"# Get train\/test data\n# Notice that train and test have same columns EXCEPT survial;\ntitanic_cabin = pd.read_csv('..\/input\/titanic\/train.csv')[\"Cabin\"]\ntitanic_test_cabin = pd.read_csv('..\/input\/titanic\/test.csv')[\"Cabin\"]\ntitanic_cabin.value_counts()\ntitanic['Cabin'] = titanic_cabin.str[:1]\ntitanic_test['Cabin'] = titanic_test_cabin.str[:1]\nprint(titanic[\"Cabin\"].value_counts())","28ae7d77":"print(titanic_test[\"Cabin\"].value_counts())","c6418095":"titanic.head(3)","df222d30":"def knn_filler(train_X,test_X,column,input_cols,output_cols, n_neighbors = 5):\n    \n    reg_train = train_X[input_cols]\n    reg_test = test_X[input_cols]\n\n    # get only rows with non-null values\n    reg_train_X = reg_train[reg_train[column].notnull()].copy()\n    reg_train_Y = reg_train_X[[column]]\n    reg_train_X.drop(column, axis=1, inplace=True)\n    \n    reg_test_X = reg_test[reg_test[column].notnull()].copy()\n    reg_test_Y = reg_test_X[[column]]\n    reg_test_X.drop(column, axis=1, inplace=True)    \n    \n    from sklearn.neighbors import KNeighborsClassifier\n\n    #Create KNN Classifier\n    knn = KNeighborsClassifier(n_neighbors)\n\n    #Train the model using the training sets\n    knn.fit(reg_train_X, reg_train_Y)\n\n    #Predict the response for test dataset\n    all_predicted = knn.predict(train_X[input_cols].loc[:, (train_X[input_cols].columns != column)\n                     ])\n    \n    train_X.loc[train_X[column].isnull(),\n                    column] = all_predicted[train_X[column].isnull()]\n    \n    all_predicted_t = knn.predict(test_X[input_cols].loc[:, (test_X[input_cols].columns != column)\n                     ])\n    \n    test_X.loc[test_X[column].isnull(),\n                           column] = all_predicted_t[test_X[column].isnull()]\n","e9bfe931":"# Re-organize the data; keep the columns with useful features;\ninput_cols = ['Pclass',\"Sex\",\"Age\",\"Cabin\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\"]\noutput_cols = [\"Survived\"]","d8436fc6":"knn_filler(titanic,titanic_test,\"Cabin\",input_cols,output_cols)","1cecf7d0":"check_missing_data(titanic)","2264249c":"titanic,titanic_test = my_encoder(titanic,titanic_test,\"Cabin\")","79f07da2":"# Check the data missing rate of titanic and titanic_test;\ncheck_missing_data(titanic)","75e1385e":"# Check the data missing rate of titanic and titanic_test;\ncheck_missing_data(titanic_test)","96024eb7":"input_cols = ['Pclass',\"Sex\",\"Age\",\"Family\",\"Fare\",\"Name_1\",\"Name_2\",\"Name_3\",\"Name_4\",\"Embarked_1\",\"Embarked_2\",\"Cabin_1\",\"Cabin_2\",\"Cabin_3\",\"Cabin_4\",\"Cabin_5\",\"Cabin_6\",\"Cabin_7\"]\noutput_cols = [\"Survived\"]","558aeebc":"temp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = train_test_split(X_train, y_train,\n                                                            test_size=0.25, random_state=20026) ","5c5949d3":"cls = xgb.XGBClassifier()\n\nparameters = {\n    \"colsample_bylevel\": np.arange(0.4, 1.0, 0.1),\n    \"colsample_bytree\": np.arange(0.7, 1.0, 0.1),\n    \"learning_rate\": [0.4, 0.45, 0.5, 0.55],\n    \"max_depth\": np.arange(4, 6, 1),\n    \"min_child_weight\": np.arange(1, 2, 0.5),\n    \"n_estimators\": [8, 10, 12],\n    \"subsample\": np.arange(0.6, 1.0, 0.1)\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=20026, shuffle=False)\n\ngscv = GridSearchCV(\n    estimator=cls,\n    param_grid=parameters,\n    scoring=\"roc_auc\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=True,\n    refit=True,\n)\n\ngscv.fit(train_fold_X, train_fold_Y.values.ravel());","8ef11073":"cls5 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls5.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls5.predict_proba(valid_fold_X)\n\n# Compute ROC AUC score\nsplit_score = metrics.roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = metrics.roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls3\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = metrics.accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))\nplot_importance(cls5);","9ab37f82":"result = algorithm_show(titanic, input_cols, output_cols)\nresult['XGBoost_tune'] = pred_result_xgb(cls5,X_test[input_cols],y_test,opt_thr)","1946e332":"result","e99b080e":"# for submission, first find optimal parameters in the entire training set\n\ntemp = titanic.copy()\ntemp.reset_index(drop=True, inplace = True)\n\ntemp_X = temp[input_cols]\ntemp_X = pd.get_dummies(temp_X)\n# print(temp_X.head(1))\nX_train, X_test, y_train, y_test = train_test_split(temp_X, temp[output_cols], test_size=0.25, random_state=20026, stratify=temp[output_cols])\n\ntrain_fold_X, valid_fold_X, train_fold_Y, valid_fold_Y = X_train, X_test, y_train, y_test\n\ncls = xgb.XGBClassifier()\n\nparameters = {\n    \"colsample_bylevel\": np.arange(0.4, 1.0, 0.1),\n    \"colsample_bytree\": np.arange(0.7, 1.0, 0.1),\n    \"learning_rate\": [0.4, 0.45, 0.5, 0.55],\n    \"max_depth\": np.arange(4, 6, 1),\n    \"min_child_weight\": np.arange(1, 2, 0.5),\n    \"n_estimators\": [8, 10, 12],\n    \"subsample\": np.arange(0.6, 1.0, 0.1)\n}\n\nskf_cv = StratifiedKFold(n_splits=5, random_state=20026, shuffle=False)\n\ngscv = GridSearchCV(\n    estimator=cls,\n    param_grid=parameters,\n    scoring=\"roc_auc\",\n    n_jobs=-1,\n    cv=skf_cv,\n    verbose=True,\n    refit=True,\n)\n\ngscv.fit(train_fold_X, train_fold_Y.values.ravel());\n\ncls6 = xgb.XGBClassifier(**gscv.best_params_)\n\n# Fit classifier to new split\ncls6.fit(train_fold_X, train_fold_Y.values.ravel())\n\n# Predict probabilities on valid_fold_X\nsplit_pred_proba = cls6.predict_proba(valid_fold_X)\n\n# Compute ROC AUC score\nsplit_score = metrics.roc_auc_score(valid_fold_Y, split_pred_proba[:,1])\nprint(\"ROC AUC is {:.4f}\".format(split_score))\n\n# Draw ROC curve\nfpr, tpr, thr = metrics.roc_curve(valid_fold_Y, split_pred_proba[:,1])\nplt.plot(fpr, tpr, linestyle=\"-\");\nplt.title(\"ROC Curve\");\nplt.xlabel(\"False Positive Rate\");\nplt.ylabel(\"True Positive Rate\");\n\n# Compute optimal operating point and threshold\nind = np.argmin(list(map(lambda x, y: x**2 + (y - 1.0)**2, fpr,tpr)))\n\n# Optimal threshold for our classifier cls6\nopt_thr = thr[ind]\n\nprint(\"Optimal operating point on ROC curve (fpr,tpr) ({:.4f}, {:.4f})\"\n                                              \"  with threshold {:.4f}\".\n                                    format(fpr[ind], tpr[ind], opt_thr))\n\ny_pred = [1 if x > opt_thr else 0 for x in split_pred_proba[:,1]]\nacc = metrics.accuracy_score(valid_fold_Y, y_pred)\nprint(\"Accuracy is {:.4f}\".format(acc))\nplot_importance(cls6);","3fcd4e39":"# for submission, first find optimal parameters in the entire training set\ncls_v2 = cls6\n\n# # Fit classifier to new split\n# cls_v2.fit(titanic[input_cols], titanic[output_cols].values.ravel())\n\n\ntest_predict_proba = cls_v2.predict_proba(titanic_test[input_cols])\n\n# Optimal threshold we found previously is opt_thr\ny_pred_xgb = [1 if x > opt_thr else 0 for x in test_predict_proba[:,1]]\n","5d5bf9c6":"# ~~~~~~~~~~~~~~~~~ Example of Submission File ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nsubmission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": y_pred_xgb   # I have given prediction of random forest just change it to save prediction of other models here\n    })\nsubmission.to_csv('submission_xgb_v2.csv', index=False)\nsubmission = pd.read_csv('submission_xgb_v2.csv')\nsubmission.head(20)","e1a29f71":"Choose features we belive are useful (just for demo). Organize the data using selected features.","2fcf3518":"Although only 16 percent of data has value, there might be remaining info. Lets encode it.","0b448143":"Below are models. Try different models by yourself.","eddeb3d1":"Try create new features.","2dfe35c7":"# V2\n## Can we use Cabin info?","d53f0643":"## Write Result for Submit","4a340364":"So  \"Family\" is better.","68c0b825":"Also get lr for bechmarking.","1d86e224":"# V1","d0106245":"By submission, Xgb achieve score of 0.74641. In comparison, lr and rf of 0.74641.\\\nWhat can I say. Model is as good as data.","4ad283e2":"Plot survied and non-survied.","e8f7b094":"## Data Visualization","07e60b33":"First construct a classic one, with grid search for best parameters. To avoid data leaking, train best parametors on training set.","0c3ee404":"Hurrah! It results in a better submission score of 0.76315.","4be35927":"Use the models from scikit: most of models have same calling process.","6c5491cf":"Load train and test data. Check first few rows of data.","e5f8a905":"## Data Loading","2bada6bd":"Plot cabin vs. Survived.","5fe22455":"Has re-engineering improved models?","4fe7d61d":"Try use \"Parch\" and \"SibSp\" instead.","458ff345":"## Improve the model","631ba37f":"We have ['Pclass',\"Sex\",\"Age_aver\",\"Cabin\",\"Family\"]\n1. refine features:\n    - DONE: one hot encoding \"Name\" and \"Embarked\"\n    - DONE: use Fare value instead of cats.\n    - DONE: impute the regression to predict \"Age\" and \"Fare\"\n    - DONE: use Cabin values.\n    - DONE: Compare use of Parch and SibSp vs. Family.\n2. refine model - xgb:\n    - DONE: optimize the parameters and operating point of xgboost.\n    - DONE: \u5c1d\u8bd5\u4e1a\u52a1\u526a\u679d\n    - can we do similar step for other models, say random forest?","773ff172":"Now best test set accuracy is rf of 0.87. Given a small test set, this could be a probabilitisc event. Submit the best rf and tuned xgb models.","f903a206":"## Submission","d82ac5b1":"In feature engineering, we encoding the ategorical features, and create new features.","de47efdc":"Belw is a funciton of plot; use the function to draw the graph to get some feeleing of data.","33a799e1":"Load packages. Display the files in folders.","3898e6d7":"## Feature Engineering","426190f1":"Column 'Cabin' has missing data. What about filling non-empty cabin with 1, and empty cabin with 0?","eb47997d":"Think about it: is there any column has non-relavant information, i.e., has no impact on 'Survived'? If so, delete it.","9582da67":"Note traning set is not imbalanced. Use AUC is ok.","5472928c":"# **Titanic: ML Framework**","564a3c5e":"Now with optimal threshold and params, let's compare with others.","ef71c258":"## Modeling","2de44d3b":"The best test set accuracy is xgb\/randomforest of 0.84.","f0780734":"Now, to fully utilize the Cabin, replace N (unkown) Cabin with KNN multiclass prediction.","86560d69":"Think about it: why group the age data?","e69ef99c":"We can further tune the parameters of xgb. Now let's do it.","1b68b8dd":"To avoid overfit, remove the least significant feature Embarked_2.","7c10935c":"Fill missing values in 'Age' and 'Fare' with mean of non-empty values.","da7dd0c0":"Check and handling missing data. Below is a function to check the missing data rate of a dataframe. Use this function to check the missing rate of train and test data.","c93d0f34":"Everything looks great... Thes best test set score is now 0.88.","e9ad8329":"Check correlation of features","d615c7ec":"Now we are ready to compare it with others.","322be78a":"The ML Process includes:\n* Data loading\n* Data pre-processing\n* Data visualization\n* Feature engineering\n* Modeling\n* *refine the model\n* Submit","b1bcd749":"Compare AUC. Opps, it's worse now. Preferablly dont drop Embarked_2. It's because my_encoder dropped the colinearity terms already.","d6d405ca":"Check the data: describe the data information,get statistical information of each column.","c49025d8":"## Data pre-processing","c9fae9da":"The important features seem plausible...","7a3baffe":"This is a sample of ML modeling framework using Titanic data. Please fill in the code labelled as \"ToDo\". ","447f882c":"Aftering refinning features, the performance improved. "}}