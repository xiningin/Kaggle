{"cell_type":{"2fe8254a":"code","850db982":"code","cfe86e12":"code","0ca42d90":"code","c26506f2":"code","758b5fca":"code","6c0e7c47":"code","76084f6b":"code","5357693a":"code","b23eb82a":"code","ac897e32":"code","39927c5f":"code","faaa7a65":"code","c0c68469":"code","ebf0d409":"code","1068a36f":"code","ea32b760":"code","602a8f99":"code","c43e13d0":"code","6b56300b":"code","dd34febd":"code","3104b8e5":"markdown","ba3754e8":"markdown","5a8eedb6":"markdown","b4cebd89":"markdown","8e4fd8a7":"markdown","92f002fa":"markdown","9ebbe621":"markdown","b5876538":"markdown","39da6baa":"markdown","ac50e814":"markdown","b93e9ac9":"markdown","9ae4ce55":"markdown"},"source":{"2fe8254a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","850db982":"# Additional packages\nimport category_encoders as encoders\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.api.types import is_numeric_dtype\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import model_selection, metrics, naive_bayes\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","cfe86e12":"# Read the data\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')","0ca42d90":"print(df_train.info())\nprint('Size of Train data set = {}'.format(df_train.shape))","c26506f2":"# Delete columns with unique identifiers\ncol_lst = ['PassengerId', 'Name', 'Ticket', 'Cabin']\ndf_train.drop(col_lst, axis = 1, inplace=True)\nprint(df_train.info())","758b5fca":"df_train.isnull().sum()","6c0e7c47":"fare_median = df_train[(df_train['Fare']>0) & (df_train['Fare'].isnull() == False)]['Fare'].median()\nage_median = df_train[(df_train['Age']>0) & (df_train['Age'].isnull() == False)]['Age'].median()\nprint('Median = {}'.format(age_median))\nprint('No. of records with non-null Age = {}'.format(df_train[(df_train['Age']>0) & (df_train['Age'].isnull() == False)]['Age'].count()))\nprint('===={} of Median {} with {} Null Records===='.format('Age', age_median, df_train[(df_train['Age'].isnull() == True)]['Survived'].count()))\ndf_train['Age'].fillna(age_median, inplace=True)","76084f6b":"numerical = ['Age', 'SibSp', 'Parch', 'Fare']\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfor col in numerical:\n    if is_numeric_dtype(df_train[col]) == True:\n        df_train[df_train[col]>0][col].plot.hist(bins=50, grid=True, legend=None)\n        plt.title(col)\n        plt.show()","5357693a":"CATBoostENCODE = encoders.CatBoostEncoder()\ncategorical = ['Pclass', 'Sex', 'Embarked']\n\n# Cast teh Pclass from integer to string so that we can apply the categorical encoding later\ndf_train['Pclass'] = df_train['Pclass'].astype(str)\n\ndf_target = df_train['Survived'].astype(str)\n\n# Use CatBoost to encode the categorical values\nencoder_cat = CATBoostENCODE.fit_transform(df_train[categorical], df_target)\nencoded_cat = pd.DataFrame(encoder_cat)\nprint(encoded_cat.head(10))","b23eb82a":"df_model_data = df_train.copy()\ndf_model_data.drop(categorical, axis = 1, inplace=True)\ndf_model_data = pd.concat([df_model_data, encoded_cat], axis=1)\ndf_model_data.info()","ac897e32":"def get_oversample (training, testing):\n\n    smote = SMOTE()\n\n    X_train, X_test, Y_train, Y_test = model_selection.train_test_split(training, testing, test_size=0.3)\n    X_smote, Y_smote = smote.fit_resample(X_train, Y_train)\n    print(\"length of original data is \",len(training))\n    print(\"Proportion of True data in original data is \",len(Y_train[Y_train['Survived']==1])\/len(Y_train))\n    print(\"Proportion of False data in original data is \",len(Y_train[Y_train['Survived']==0])\/len(Y_train))\n    print(\"length of oversampled data is \",len(X_smote))\n    print(\"Proportion of True data in oversampled data is \",len(Y_smote[Y_smote['Survived']==1])\/len(Y_smote))\n    print(\"Proportion of False data in oversampled data is \",len(Y_smote[Y_smote['Survived']==0])\/len(Y_smote))\n   \n    return X_smote, Y_smote, X_train, X_test, Y_train, Y_test","39927c5f":"Y = df_model_data.iloc[:,0:1]\nX = df_model_data.iloc[:,1:]\nX_smote, Y_smote, X_train, X_test, Y_train, Y_test = get_oversample(X, Y)","faaa7a65":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\nfit_params={\"early_stopping_rounds\":100, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,Y_test.values.ravel())],\n            'eval_names': ['valid'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': 250,\n            'categorical_feature': 'auto'}\n\n\nparam_test ={'num_leaves': sp_randint(5, 10), \n             'min_child_samples': sp_randint(5, 10), \n             'min_child_weight': [1e-2, 1e-1, 1, 1e1, 1e2],\n             'subsample': sp_uniform(loc=0.1, scale=0.4), \n             'colsample_bytree': sp_uniform(loc=0.5, scale=0.8),\n             'max_depth' : sp_randint(7, 12),            \n             'reg_alpha': [1e-1, 1, 2, 5],\n             'reg_lambda': [1e-1, 1, 2, 5]}","c0c68469":"clf = lgb.LGBMClassifier(random_state=1234, silent=True, metric='auc', n_estimators=5000,  class_weight='balanced', n_jobs = -1)","ebf0d409":"#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 100\n\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    #scoring='auc',\n    n_jobs = -1,\n    cv=3,\n    refit=True,\n    verbose=500,\n    random_state=4563)","1068a36f":"gs.fit(X_train, Y_train.values.ravel(), **fit_params)","ea32b760":"print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","602a8f99":"#Configure from the HP optimisation\nclf_final = lgb.LGBMClassifier(**gs.best_estimator_.get_params())\n\n\nclf_final = lgb.LGBMClassifier(random_state=3453456, silent=True, metric='auc', n_estimators=10000,  class_weight='balanced', n_jobs = -1)\n\n#Train the final model with learning rate decay\nclf_final.fit(X_smote, Y_smote.values.ravel(), **fit_params )","c43e13d0":"predicted_test = pd.DataFrame(clf_final.predict(X_test))\npredicted_train = pd.DataFrame(clf_final.predict(X_train))\nprint('=============================================')\nprint('Scoring Metrics for XGBClassifier (Validation)')\nprint('=============================================')\nprint('Balanced Accuracy Score = {}'.format(metrics.balanced_accuracy_score(Y_test, predicted_test)))\nprint('Accuracy Score = {}'.format(metrics.accuracy_score(Y_test, predicted_test)))\nprint('Precision Score = {}'.format(metrics.precision_score(Y_test, predicted_test)))\nprint('F1 Score = {}'.format(metrics.f1_score(Y_test, predicted_test, labels=['0','1'])))\nprint('Recall Score = {}'.format(metrics.recall_score(Y_test, predicted_test, labels=['0','1'])))\nprint('ROC AUC Score = {}'.format(metrics.roc_auc_score(Y_test, predicted_test, labels=['0','1'])))\nprint('Confusion Matrix')\nprint('==================')\nprint(metrics.confusion_matrix(Y_test, predicted_test))\nprint('==================')\nprint(metrics.classification_report(Y_test, predicted_test, target_names=['0','1']))\nmetrics.ConfusionMatrixDisplay(metrics.confusion_matrix(Y_test, predicted_test)).plot()\n\n\nprint('=============================================')\nprint('Scoring Metrics for XGBClassifier (Training)')\nprint('=============================================')\nprint('Balanced Accuracy Score = {}'.format(metrics.balanced_accuracy_score(Y_train, predicted_train)))\nprint('Accuracy Score = {}'.format(metrics.accuracy_score(Y_train, predicted_train)))\nprint('Precision Score = {}'.format(metrics.precision_score(Y_train, predicted_train)))\nprint('F1 Score = {}'.format(metrics.f1_score(Y_train, predicted_train)))\nprint('Recall Score = {}'.format(metrics.recall_score(Y_train, predicted_train, labels=['0','1'])))\nprint('ROC AUC Score = {}'.format(metrics.roc_auc_score(Y_train, predicted_train, labels=['0','1'])))\nprint('Confusion Matrix')\nprint('==================')\nprint(metrics.confusion_matrix(Y_train, predicted_train))\nprint('==================')\nprint(metrics.classification_report(Y_train, predicted_train, target_names=['0','1']))\nmetrics.ConfusionMatrixDisplay(metrics.confusion_matrix(Y_train, predicted_train)).plot()","6b56300b":"# Read the data\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_id = df_test.iloc[:,0:1]\n\n# Delete columns with unique identifiers\ncol_lst = ['PassengerId', 'Name', 'Ticket', 'Cabin']\ndf_test.drop(col_lst, axis = 1, inplace=True)\n\n# Replace null age with median \ndf_test['Age'].fillna(age_median, inplace=True)\ndf_test['Fare'].fillna(fare_median, inplace=True)\n\n# Convert the data type of Pclass from ingeter to string value\ndf_test['Pclass'] = df_test['Pclass'].astype(str)\n\n# Categorical variable encoding\nencoder_cat = CATBoostENCODE.transform(df_test[categorical])\nencoded_cat = pd.DataFrame(encoder_cat, columns =categorical)\n\n# Prepare the dataset\ndf_test.drop(categorical, axis = 1, inplace=True)\ndf_test = pd.concat([df_test, encoded_cat], axis=1)  ","dd34febd":"# Prediction\np_model = clf_final.predict(df_test)\ndf_rst = pd.concat([df_id, pd.DataFrame(p_model, columns = ['Survived'])], axis = 1)\ndf_rst.to_csv(\"submission.csv\",index=False)\nprint('Done!')","3104b8e5":"## Check if there is any missing values","ba3754e8":"# Explortory Data Analysis","5a8eedb6":"## Distribution of numerical variables","b4cebd89":"# Introduction","8e4fd8a7":"## Prepare data for modelling","92f002fa":"# Submission","9ebbe621":"# Training and validation data","b5876538":"# Model - LightGBM","39da6baa":"# Oversampling","ac50e814":"# Data encoding","b93e9ac9":"This notebook is for submission to the Titanic. This notebook refers to my another notebook ([Titanic - Random Forest (Beginner)](https:\/\/www.kaggle.com\/bennyfung\/titanic-random-forest-beginner). The only difference these two notebooks is the classification model.  Light GBM model is applied in this notebook, and the the rest of data pipeline is almost the same. \n\n\n## Summary\nCheck if there are any null values in the features.  If do so, fill the missing value with median of the features.\nReview the distribution of variables\nUse the Catboost to encode the categorical features into numerical representation based on its distribution to the target.\nApply oversampling to equalize the size of true and false cases\nUse GridSearchCV to hyperparameterize the Random Forest\nGet the optimal hyperparameters for cross-validation and data submission\n","9ae4ce55":"## Fill the null value with Median"}}