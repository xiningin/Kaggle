{"cell_type":{"fd622f26":"code","bdfac4bf":"code","badb84e9":"code","d46fb32c":"code","dd6136d4":"code","27c51d75":"code","b62fa74a":"code","a88f97fe":"code","da4fa69a":"code","861e229c":"code","68c87f36":"code","398db09d":"code","d290bcfd":"code","98e7e1da":"markdown","a43173ae":"markdown","d1bebab1":"markdown","e6c7f8e7":"markdown","f69734e8":"markdown","7237d7e9":"markdown","f680d011":"markdown","301ce5b4":"markdown","37ca866e":"markdown"},"source":{"fd622f26":"# import libraries\n\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score ","bdfac4bf":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# Plot data that shows difference between survuved male and female\n# we see that we can use this feature for our model\nsex_pivot = train_data.pivot_table(index='Sex', values='Survived')\nsex_pivot.plot.bar()\nplt.show()\n","badb84e9":"# Plot by class\npclass_pivot = train_data.pivot_table(index='Pclass', values='Survived')\npclass_pivot.plot.bar()\nplt.show()","d46fb32c":"# Show survivors by age\nsurvived = train_data[train_data['Survived'] == 1]\ndied = train_data[train_data['Survived'] == 0]\nsurvived['Age'].plot.hist(alpha=0.5, color='red', bins=50)\ndied['Age'].plot.hist(alpha=0.5, color='blue', bins=50)\nplt.legend(['Survived', 'Died'])\nplt.show()","dd6136d4":"# Create dummy data\ndef create_dummies(df, column_name):\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n    df = pd.concat([df, dummies], axis=1)\n    return df \ntrain = create_dummies(train_data, 'Pclass')\ntest = create_dummies(test_data, 'Pclass')\ntrain.head(10)","27c51d75":"# Group all ages so later we can use it as features in model\ndef process_age(df, cut_points, label_names):\n    df['Age'] = df['Age'].fillna(-0.5)\n    df['Age_categories'] = pd.cut(df['Age'], cut_points, labels=label_names)\n    return df \n\ncut_points = [-1, 0, 5, 12, 18, 35, 60, 100]\nlabel_names = [\"Missing\", 'Infant', \"Child\", 'Teenager', \"Young Adult\", 'Adult', 'Senior']\n\ntrain_buff = process_age(train, cut_points, label_names)\n\nage_cat_pivot = train_buff.pivot_table(index=\"Age_categories\",values=\"Survived\")\nage_cat_pivot.plot.bar()\nplt.show()","b62fa74a":"# Apply new fields into data and split labels with data \nnumeric_fields = ['PassengerId','Pclass','Age', 'SibSp', 'Parch','Fare']\nx = train_data[numeric_fields]\ny = train_data['Survived']","a88f97fe":"#Remove nan from data\nx['Age'] = x['Age'].fillna(x['Age']).median()\nx['Age'].isnull().sum()","da4fa69a":"# Split training and testing\ntrainX, testX, trainY, testY = train_test_split(x,y,test_size=0.2, random_state=0)","861e229c":"random_forest = RandomForestClassifier()\nparameters = {\n    \"n_estimators\": [4,5,6,15],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n    \"max_depth\": [2, 3, 5, 10], \n    \"min_samples_split\": [2, 3, 5, 10],\n    \"min_samples_leaf\": [1, 5, 8, 10]\n}\n\ngrid_cv = GridSearchCV(random_forest, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(trainX, trainY)\n\nprint(\"Our optimized Random Forest model is:\")\ngrid_cv.best_estimator_","68c87f36":"# Fit model\nclf = RandomForestClassifier(\n    n_estimators= 5,\n    criterion= \"gini\",\n    max_features= \"sqrt\", \n    max_depth= 3, \n    min_samples_split= 3,\n    min_samples_leaf= 5)\nclf.fit(trainX, trainY)","398db09d":"# Predictions on testing data\npredictions = clf.predict(testX[numeric_fields])\naccuracy = accuracy_score(testY, predictions)","d290bcfd":"accuracy","98e7e1da":"<a name=\"Training\"><\/a>\n## 5 Training","a43173ae":"<a name=\"Conclusion\"><\/a>\n## 6 Conclusion\nI've achieved 73% accuracy, further impovements may be done to add more features and scale correctly age and numeric parameteres. Also try use another models perhaps. ","d1bebab1":"<a name=\"Introduction\"><\/a>\n## 1 Introduction\nThis is my first notbook submission. Really want to join such community. So I my goal is to use Random Forrest algorythm with GreadSearchCV for fine tunning parameters. Also aggregation and data engineering that i did in next steps. ","e6c7f8e7":"<a name=\"Preprocessing\"><\/a>\n## 3 Preprocessing","f69734e8":"<a name=\"Model\"><\/a>\n## 4 Model creation and optimization","7237d7e9":"### *Dmitry Shendryk*\n*2 December 2019*","f680d011":"[1. Introduction](#Introduction)\n\n[2. Data lookup](#DataLooking)\n\n[3. Preprocessing](#Preprocessing)\n\n[4. Model creation and optimization](#Model)\n\n[5. Training](#Training)\n\n[6. Conclusion](#Conclusion)","301ce5b4":"# Prediction with RandomForrest and GridSearchCV optimization","37ca866e":"<a name=\"DataLooking\"><\/a>\n## 2 Data lookup"}}