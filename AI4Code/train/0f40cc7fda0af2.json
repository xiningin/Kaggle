{"cell_type":{"9863de5b":"code","8648da20":"code","120c3b9d":"code","1206c33f":"code","ecf48bca":"code","72ddccb2":"code","03ca2910":"code","c973474c":"code","55bb4a27":"code","0436c20b":"code","6c224ecf":"markdown","20ff380e":"markdown","ee06b5c8":"markdown","c98d2655":"markdown","91b2d289":"markdown","20df3bb5":"markdown","791901de":"markdown","777c24bc":"markdown","21715367":"markdown"},"source":{"9863de5b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\n\n# read the dataset and print five rows\noriginal_dataset = pd.read_csv('..\/input\/creditcard.csv')\n\ndataset = original_dataset.copy()\n","8648da20":"# count how many entry there are for every class\nclasses_count = pd.value_counts(dataset['Class'])\n\nprint(\"{} Non-fraud example\\n{} Fraud examples\".format(classes_count[0], classes_count[1]))\n\n# classes_count is a Series. \nclasses_count.plot(kind = 'bar')\nplt.xlabel('Classes')\nplt.ylabel('Frequencies')\nplt.title('Fraud Class Hist')","120c3b9d":"# get only 3000 non-fraud example\ndataset_no_fraud = dataset.loc[dataset.Class == 0]\ndataset_no_fraud_index = np.random.choice(dataset_no_fraud.index, size = 3000, replace = False)\n\n# new dataset of (subset of non-fraud) + fraud example\nnew_dataset = pd.concat(\n    [ dataset.iloc[dataset_no_fraud_index], # subset of non-fraud example\n      dataset.iloc[dataset.index[dataset.Class == 1]] # all fraud example\n    ])\n\n# split into y and X\ny = new_dataset['Class'].copy()\nX = new_dataset.drop(['Time', 'Class'], axis = 1, inplace = False)\n\nprint('Dataset size: {}'.format(len(X)))","1206c33f":"# split in train and test set\nfrom sklearn.model_selection import train_test_split\nX_over_train, X_over_test, y_over_train, y_over_test = train_test_split(X, y, test_size = 0.20 , stratify = y)\n\nprint('Expected Fraud example in y_test: {}. Actual: {} '.format( len(y[y == 1]) * 0.2, len(y_over_test[y_over_test == 1]) )) \n","ecf48bca":"# scale amount\nfrom sklearn.preprocessing import StandardScaler\nam_scaler = StandardScaler().fit(X_over_train[['Amount']])\n\nX_over_train['AmountS'] = am_scaler.transform(X_over_train[['Amount']])\nX_over_test['AmountS'] = am_scaler.transform(X_over_test[['Amount']])\n\nX_over_train.drop(['Amount'], axis = 1, inplace = True)\nX_over_test.drop(['Amount'], axis = 1, inplace = True)\n\nprint(\"{} Train-set examples. Non-Fraud: {} Fraud: {}\".format( len(X_over_train), len(y_over_train[y_over_train == 0]), len(y_over_train[y_over_train ==1]) ))\nprint(\"{} Test-set examples. Non-Fraud: {} Fraud: {}\".format( len(X_over_test), len(y_over_test[y_over_test == 0]), len(y_over_test[y_over_test ==1]) ))\n","72ddccb2":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(ratio = 1.0) # with ratio = 1 we obtain the same # of examples for every class\nx_over_train_sm, y_over_train_sm = sm.fit_sample(X_over_train, y_over_train)\n\nprint(\"Now we have {} Train-set examples. Non-Fraud: {} Fraud: {}\".format( len(x_over_train_sm), len(y_over_train_sm[y_over_train_sm == 0]), len(y_over_train_sm[y_over_train_sm == 1]) ))\nprint(\"{} Test-set examples. Non-Fraud: {} Fraud: {}\".format( len(X_over_test), len(y_over_test[y_over_test == 0]), len(y_over_test[y_over_test ==1]) ))","03ca2910":"import tensorflow as tf\nimport keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import regularizers\n\n# init ann\nclassifier = Sequential()\n\n# in this case we used a rectifier activation function for the hidden layers\n# and a sigmoid function for the output layer\nclassifier.add(Dense(\n    input_dim = len(X.columns), # input neurons\n    units = 15, # first hidden layer\n    kernel_initializer = 'he_normal', \n    bias_initializer = 'zeros',\n    activation = 'relu', # activation function (rectifier)\n    kernel_regularizer=regularizers.l2(0.005)\n))\n# add a new hidden layer with the same number of neurons\nclassifier.add(Dense(\n    units = 6, # second hidden layer\n    kernel_initializer = 'he_normal',\n    bias_initializer = 'zeros',\n    activation = 'relu', # activation function (rectifier)\n    kernel_regularizer=regularizers.l2(0.005)\n))\n# add the output layer with one neuron\nclassifier.add(Dense(\n    units = 1, # output layer\n    kernel_initializer = 'random_uniform',\n    bias_initializer = 'zeros',\n    activation = 'sigmoid', # activation function (sigmoid)\n    kernel_regularizer=regularizers.l2(0.005)\n))\n\n# Compiling the ANN\nclassifier.compile(\n    optimizer = 'adam', \n    loss = 'binary_crossentropy', # cost function\n    metrics = ['accuracy']\n)\n\n\n# fit the ann to the Training set\nhistory_over = classifier.fit(\n    x_over_train_sm, y_over_train_sm,  # training set\n    validation_data = (X_over_test, y_over_test),\n    batch_size = 100,\n    epochs = 150,\n    verbose = False\n)","c973474c":"# summarize history for accuracy\nplt.plot(history_over.history['acc'])\nplt.plot(history_over.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n\n# summarize history for loss\nplt.plot(history_over.history['loss'])\nplt.plot(history_over.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","55bb4a27":"from sklearn.metrics import confusion_matrix\ny_over_test_pred = classifier.predict(X_over_test) > 0.5\ncm_over = confusion_matrix(y_over_test, y_over_test_pred)\n\nprint('Train Accuracy: {}\\nTest Accuracy:{}'.format(history_over.history['acc'][-1], history_over.history['val_acc'][-1]))\nprint(cm_over)\n","0436c20b":"plt.clf()\nplt.imshow(cm_over, interpolation='nearest', cmap=plt.cm.copper)\nclassNames = ['Negative','Positive']\nplt.title('Fraud or Not Fraud Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm_over[i][j]), color = 'white')\nplt.show()\n\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_over_test, y_over_test_pred)\n\nprint(\"F1 Score: {}\".format(f1))","6c224ecf":"Now that the dataset is splitted into train and test set, it's possible to scale tha amount value.\n\n**Must fix the warnings**","20ff380e":"And finally the confusion matrix and F1 Score","ee06b5c8":"# Neural Network for Fraud detection with SMOTE\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIn this notebook for handle the imbalanced data I have used the [SMOTE](https:\/\/jair.org\/index.php\/jair\/article\/view\/10302) technique. \n\nI have also writed another notebook where handle this situation using [undersampling](http:\/\/ https:\/\/www.kaggle.com\/davidevegliante\/nn-for-fraud-detection) technique.\n\n**These are my first two notebooks, I hope to receive comment and advice for improve the understanding of the tools I used. **\n\n\n","c98d2655":"Here we are :-) It's here that the Smote technique comes into action.","91b2d289":"Now we can plot the learning curves","20df3bb5":"Let's see how many example in our dataset we have. ","791901de":"The next step is to split our data in train and test set.\nThe stratify param allow to split also the classes of y with the same ratio. ","777c24bc":"We're ready for train our Neural Network.\n","21715367":"I have decided  don't use all the non-fraud examples but choose only 3000 random example, because I don't want to use SMOTE for generating all remaining examples."}}