{"cell_type":{"7443bec7":"code","4ad95f56":"code","082a0f0f":"code","eccf5579":"code","6f79ca7e":"code","1ca91fe9":"code","e0c6a861":"code","debd4274":"code","4edea26b":"code","766b503e":"code","6e781dbb":"code","599e2a31":"code","ed41836a":"code","ee91bcab":"code","940f3f77":"code","29511ce7":"code","66298f9f":"code","7d2571f7":"code","77ad91c8":"code","8961913f":"code","adc08108":"code","f8698bba":"code","7ff83ea5":"code","247d7264":"code","096388d2":"code","cfbd4d4a":"code","7dbc88d8":"code","0df1f0b4":"code","af0f87b7":"code","5e56185d":"code","32de3746":"code","15b77c75":"code","662dd2b6":"code","2d2749b0":"code","401c537c":"code","de768650":"code","6fcc21ce":"code","ed3c8d62":"code","b71c38d5":"code","9ef8ef6a":"code","60c990f4":"code","b153fd2f":"code","05ad9723":"code","baa8acfd":"code","acbe0493":"code","701613c2":"code","d3f68347":"code","cd440fb4":"code","68d13217":"code","bbc96bda":"code","70860216":"code","9c6bdc71":"code","30b4f2a2":"code","81db795e":"code","a8d194cf":"markdown","cba246af":"markdown","eaca61b7":"markdown","06e7e85a":"markdown","7bc6a214":"markdown","f2ca4ec6":"markdown","f3dc04db":"markdown","badba84f":"markdown","525f8a4c":"markdown","b6dbd0e1":"markdown","0d2514c3":"markdown","e93b2204":"markdown","c345ef0f":"markdown","e3f56b47":"markdown","771c60dc":"markdown","cfe5356c":"markdown","050da319":"markdown","2c423a19":"markdown","e53f0b27":"markdown","4381d64a":"markdown","90cfe7f1":"markdown","aea24020":"markdown","9bcd7b0a":"markdown","67bd6d40":"markdown","944c6dc7":"markdown","cfa3bcdc":"markdown","3588fbc5":"markdown","1d0f8628":"markdown","308c85b3":"markdown","df178ed0":"markdown","6ffeeac1":"markdown","2b88054e":"markdown","0fac1c5b":"markdown","506239ff":"markdown","1f5aff54":"markdown","84b2e42f":"markdown","98dee540":"markdown","4f5c3e93":"markdown","f59d2609":"markdown","f5b918fa":"markdown","1c1a09a5":"markdown","3813c787":"markdown","df468e74":"markdown","b4a04f24":"markdown","492d9f67":"markdown","c31bdefd":"markdown","c4e85725":"markdown","121d72f7":"markdown","53a4c522":"markdown","5889d94e":"markdown","34c59a93":"markdown","9086f8be":"markdown","3c3512ee":"markdown","bc745491":"markdown","0ba9c68b":"markdown","aaabc6ad":"markdown","1d4f903c":"markdown","aa2dd553":"markdown","4fe06a41":"markdown","078dc3b3":"markdown","584d3903":"markdown","e99b2810":"markdown","15af8c4b":"markdown","084b9904":"markdown","f2b2d273":"markdown","a35f3b42":"markdown","3e1aec1c":"markdown","8589a8fc":"markdown","d8aba491":"markdown","67faccfe":"markdown","c0fb6f24":"markdown","672fef0c":"markdown","312e4448":"markdown","1803ac8b":"markdown","ab192e3a":"markdown","e482cfa9":"markdown","74f44a63":"markdown","0f60fc49":"markdown","a39b7a9d":"markdown","c98ed4c0":"markdown","9c0d6ce5":"markdown","00183cdc":"markdown","c3c64210":"markdown","d9fb7dc3":"markdown"},"source":{"7443bec7":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import preprocessing\n%matplotlib inline\nnp.random.seed(1)","4ad95f56":"df = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\n\nprint('data shape', df.shape)\n\ndf.head()","082a0f0f":"df['DEATH_EVENT'].value_counts()","eccf5579":"df.info()","6f79ca7e":"df.describe()","1ca91fe9":"df.isna().sum()","e0c6a861":"f, ax  = plt.subplots(1,2,figsize = (18,6))\nsns.countplot(df['DEATH_EVENT'],ax=ax[0],palette = 'bright',alpha=0.7)\n\ncount = df['DEATH_EVENT'].value_counts(normalize=True)\ncount.plot.pie(autopct=\"%.2f%%\",explode = [0,0.2],ax=ax[1])\nplt.show()","debd4274":"plt.figure(1,figsize=(20,20))\nplt.subplot(321)\ndf['anaemia'].value_counts().plot.pie(title='% distribution of anaemia',explode=[0,.1],autopct=\"%1.1f\")\n\nplt.subplot(322)\ndf['diabetes'].value_counts().plot.pie(title='% distribution of diabetes',explode=[0,.1],autopct=\"%1.1f\")\n\nplt.subplot(323)\ndf['high_blood_pressure'].value_counts().plot.pie(title='% distribution of high_blood_pressure',explode=[0,.1],autopct=\"%1.1f\")\n\nplt.subplot(324)\ndf['sex'].value_counts().plot.pie(title='% distribution of sex',explode=[0,.1],autopct=\"%1.1f\")\n\nplt.subplot(325)\ndf['smoking'].value_counts().plot.pie(title='% distribution of smoking',explode=[0,.1],autopct=\"%1.1f\")\nplt.show()","4edea26b":"def createPlot(data, col:str, ttext:str, xtext:str, ytext:str) ->None:\n    \"\"\"\n    this function creates histogram and boxplot by pyplot library and also count skewness\n    \n    Arguments:\n    data -- pandas dataframe\n    col -- column name which we want to plot\n    ttext -- string which we want to write on top of the graph\n    xtext -- string which we want to write on x axis\n    ytext -- string which we want to write on y axis\n    \n    Returns:\n    None\n    \"\"\"\n    fig = make_subplots(rows=1, cols=2)\n    fig.add_trace(go.Histogram(x=data[col],marker_color='#6a6fff'),row=1,col=1)\n    fig.add_trace(go.Box(x=data[col]),row=1,col=2)\n    \n    fig.update_layout(\n        title_text=ttext,\n        xaxis_title_text=xtext,\n        yaxis_title_text=ytext, \n        bargap=0.05, \n        template = 'plotly_dark',\n        width=900, height=600\n    )\n    fig.add_annotation(dict(font=dict(color='yellow',size=15),\n                                        x=0.35,\n                                        y=1.1,\n                                        showarrow=False,\n                                        text=\"Skewness = \"+str(df[col].skew()),\n                                        textangle=0,\n                                        xanchor='left',\n                                        xref=\"paper\",\n                                        yref=\"paper\"))\n    \n    fig.show()","766b503e":"createPlot(df,\"age\",\"Age distribution\",\"Age\",\"count\")","6e781dbb":"createPlot(df,\"creatinine_phosphokinase\",\"Creatinine Phosphokinase Distribution\",\"creatinine_phosphokinase\",\"count\")","599e2a31":"createPlot(df,\"ejection_fraction\",\"Ejection Fraction Distribution\",\"Ejection fraction (%)\",\"count\")","ed41836a":"createPlot(df,\"platelets\",\"Platelets Distribution\",\"Platelets Distribution\",\"count\")","ee91bcab":"createPlot(df,\"serum_creatinine\",\"Serum Creatinine Distribution\",\"Serum Creatinine(mg\/dL)\",\"count\")","940f3f77":"createPlot(df,\"serum_sodium\",\"Serum Sodium Distribution\",\"Serum Sodium (mEq\/L)\",\"count\")","29511ce7":"createPlot(df,\"time\",\"Time Distribution\",\"Time (days)\",\"count\")","66298f9f":"matrix = df.corr()\nf, ax = plt.subplots(figsize=(12, 6))\nwith sns.axes_style(\"white\"):\n    sns.heatmap(matrix,mask=np.triu(matrix,1),annot=True,fmt=\".2f\", vmax=.8,cbar=False,cmap=\"coolwarm\");","7d2571f7":"plt.figure(figsize=(14, 14)) \nsns.pairplot(df.drop(columns=['DEATH_EVENT','anaemia','sex','time','high_blood_pressure','diabetes','smoking']), diag_kind='kde');","77ad91c8":"catVariable = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\nnumVariable = list(set(df.columns.drop(\"DEATH_EVENT\"))-set(catVariable))","8961913f":"f, ax  = plt.subplots(3,2,figsize = (15,15))\ncross = pd.crosstab(df[catVariable[0]],df['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[0][0],title =catVariable[0])\n\ncross = pd.crosstab(df[catVariable[1]],df['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[0][1],title =catVariable[1])\n\ncross = pd.crosstab(df[catVariable[2]],df['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[1][0],title =catVariable[2])\n\ncross = pd.crosstab(df[catVariable[3]],df['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[1][1],title =catVariable[3])\n\ncross = pd.crosstab(df[catVariable[4]],df['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[2][0],title =catVariable[4])\nplt.show()","adc08108":"groups = ['Low','Medium','High']\ndfForPlot = df.copy();\ndef get_categories(x):\n    if x < q1:\n        return groups[0]\n    elif x < q3:\n        return groups[1]\n    else:\n        return groups[2]    \n\nfor col in numVariable:\n    q1 = df[col].quantile(q=0.25)\n    q3 = df[col].quantile(q=0.75)\n    dfForPlot[col] = df[col].apply(get_categories)","f8698bba":"f, ax  = plt.subplots(4,2,figsize = (22,22))\ncross = pd.crosstab(dfForPlot[numVariable[0]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[0][0],title =numVariable[0])\n\ncross = pd.crosstab(dfForPlot[numVariable[1]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[0][1],title =numVariable[1])\n\ncross = pd.crosstab(dfForPlot[numVariable[2]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[1][0],title =numVariable[2])\n\ncross = pd.crosstab(dfForPlot[numVariable[3]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[1][1],title =numVariable[3])\n\ncross = pd.crosstab(dfForPlot[numVariable[4]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[2][0],title =numVariable[4])\n\ncross = pd.crosstab(dfForPlot[numVariable[5]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[2][1],title =numVariable[5])\n\ncross = pd.crosstab(dfForPlot[numVariable[6]],dfForPlot['DEATH_EVENT'],normalize=\"index\")\ncross.plot.bar(stacked=True,ax=ax[3][0],title =numVariable[6])\nplt.show()","7ff83ea5":"# def beforeAfter(data, col:str, ttext:str, xtext:str, ytext:str) ->None:\n#     \"\"\"\n#     this function creates histogram and boxplot, reduces skewness and show distribution before skewness reduction and after\n    \n#     Arguments:\n#     data -- pandas dataframe\n#     col -- column name which we want to plot\n#     ttext -- string which we want to write on top of the graph\n#     xtext -- string which we want to write on x axis\n#     ytext -- string which we want to write on y axis\n    \n#     Returns:\n#     None\n#     \"\"\"\n#     fig = make_subplots(rows=2, cols=2)\n#     fig.add_trace(go.Histogram(x=data[col],marker_color='#6a6fff'),row=1,col=1)\n#     fig.add_trace(go.Box(x=data[col]),row=1,col=2)\n    \n#     fig.add_annotation(dict(font=dict(color='yellow',size=15),\n#                                         x=0.4,\n#                                         y=1.1,\n#                                         showarrow=False,\n#                                         text=\"Skewness before= \"+str(data[col].skew()),\n#                                         textangle=0,\n#                                         xanchor='left',\n#                                         xref=\"paper\",\n#                                         yref=\"paper\"))    \n    \n#     data[col] = np.log(data[col])\n#     fig.add_trace(go.Histogram(x=data[col],marker_color='#6a6fff'),row=2,col=1)\n#     fig.add_trace(go.Box(x=data[col]),row=2,col=2)\n\n#     fig.update_layout(\n#         title_text=ttext,\n#         xaxis_title_text=xtext,\n#         yaxis_title_text=ytext, \n#         bargap=0.05, \n#         template = 'plotly_dark',\n#         width=900, height=600\n#     )\n#     fig.add_annotation(dict(font=dict(color='yellow',size=15),\n#                                        x=0.4,\n#                                        y=0.5,\n#                                        showarrow=False,\n#                                        text=\"Skewness after= \"+str(data[col].skew()),\n#                                        textangle=0,\n#                                        xanchor='left',\n#                                        xref=\"paper\",\n#                                        yref=\"paper\"))\n    \n#     fig.show()","247d7264":"# beforeAfter(df,\"creatinine_phosphokinase\",\"Creatinine Phosphokinase Distribution\",\"creatinine_phosphokinase\",\"count\")\n# df[\"serum_creatinine\"] = np.log(df[\"serum_creatinine\"])\n# df[\"age\"] = np.log(df[\"age\"])\n# df[\"ejection_fraction\"] = np.log(df[\"ejection_fraction\"])\n# df[\"platelets\"] = np.log(df[\"platelets\"])\n# df[\"serum_sodium\"] = np.log(df[\"serum_sodium\"])\n# df[\"time\"] = np.log(df[\"time\"])","096388d2":"def featureNormalization(features) ->None:\n    \"\"\"\n    this function normilize features by standartScaler library\n    \n    Arguments:\n    features -- array of strings\n    \n    Returns:\n    None\n    \"\"\"\n    scaler = preprocessing.StandardScaler()\n    scaled_df = scaler.fit_transform(df[features])\n    scaler_df = pd.DataFrame(scaled_df, columns=features)\n    df[features] = scaled_df","cfbd4d4a":"features = ['age',\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\n            \"serum_creatinine\",\"serum_sodium\",\"time\"]\nfeatureNormalization(features)","7dbc88d8":"X = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\nX_train, X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)","0df1f0b4":"sm = SMOTE()\n\nX_train_s,y_train_s =sm.fit_sample(X_train,y_train)\nX_test_s,y_test_s =sm.fit_sample(X_test,y_test)","af0f87b7":"f, ax  = plt.subplots(1,2,figsize = (18,6))\nsns.countplot(y_test,ax=ax[0],palette = 'bright',alpha=0.7,).set_title('Test data Before balancing')\n\nsns.countplot(y_test_s,ax=ax[1],palette = 'bright',alpha=0.7).set_title('Test data After balancing')\n\nplt.show()","5e56185d":"f, ax  = plt.subplots(1,2,figsize = (18,6))\nsns.countplot(y_train,ax=ax[0],palette = 'bright',alpha=0.7).set_title('Train data Before balancing')\n\nsns.countplot(y_train_s,ax=ax[1],palette = 'bright',alpha=0.7).set_title('Trin data After balancing')\n\nplt.show()\n\nX_train = X_train_s\ny_train = y_train_s\nX_test = X_test_s\ny_test = y_test_s","32de3746":"def sigmoid(z):\n    \"\"\"\n    This function counts sigmoid\n    Arguments: \n    z -- input value\n    \n    Returns: \n    the sigmoid activation value for a given input value\n    \"\"\"\n    \n    return 1 \/ (1 + np.exp(-z))","15b77c75":"def lRpredict(X,weight):\n    \"\"\"\n    The function checks the values and writes 1 and 0 depending on whether the value is greater than 0.5 or not\n    Arguments: \n    X -- The values we want to predict\n    weight -- values which we trained  \n    \n    Returns: \n    an array which contains ones and zerosdient value for a given input values\n    \"\"\"    \n    m = np.shape(X)[0]\n    X = np.concatenate((np.ones((m, 1)), X), axis=1)\n    Ypredicted = sigmoid(np.dot(X,weight.T))\n    return (Ypredicted>=0.5)*1","662dd2b6":"def lRcost(yTrue,yPredicted,m):\n    \"\"\"\n    This function compute costs for Logistical Regression\n    Arguments: \n    yTrue -- Labeled value that we had given\n    yPredicted -- The value which we have predicted \n    m -- number of samples\n    \n    Returns: \n    cost value for a given input values\n    \"\"\"\n    cost = -1\/m*(np.dot(yTrue.T,np.log(yPredicted))+np.dot((1-yTrue).T,np.log(1-yPredicted)))\n    return cost","2d2749b0":"def lRgradient(X,yTrue,yPredicted):\n    \"\"\"\n    This function compute gradient for Logistical Regression\n    Arguments: \n    X -- feature matrix\n    yTrue -- Labeled value that we had given\n    yPredicted -- The value which we have predicted \n    \n    Returns: \n    gradient value for a given input values\n    \"\"\"\n    m = X.shape[0]\n    return (1\/m)*np.dot((yPredicted - np.array(yTrue)).T,X)","401c537c":"def lRmodel(X, y, alpha=0.01, numIterations=1000):\n    \"\"\"\n    This function Implement whole Logistic Regression Model\n    Arguments:\n    X -- feature matrix\n    y -- target vector\n    alpha -- learning rate (default=0.01)\n    numIterations -- maximum number of iterations of the logistic regression algorithm (default=30)\n    \n    Returns:\n    weights, list of the cost function changing overtime\n    \"\"\"\n    m = np.shape(X)[0]  # total number of samples\n    n = np.shape(X)[1]  # total number of features\n    \n    # Add bias to our weights, so we have form n + 1 and we also need to add a row to X which will be ones.\n    np.random.seed(3)\n    X = np.concatenate((np.ones((m, 1)), X), axis=1)\n    W = np.zeros((1,n + 1))\n    \n    # stores where we keep all cost Information every 10th iteration\n    costHistory = []\n\n    # repeat this procces numIteration times\n    for current_iteration in tqdm(range(numIterations)): \n\n        # compute the dot product between our feature 'X' and weight 'W'\n        # then passed the value into our sigmoid activation function\n        yPredicted = sigmoid(X.dot(W.T))\n        # calculate the cost \n        cost = lRcost(y,yPredicted,m)\n        # compute the gradient\n        gradient = lRgradient(X,y,yPredicted)\n        # Now we have to update our weights\n        W = W - alpha * gradient\n        \n        # append out new cost value in our stores\n        costHistory.append(cost[0][0])\n    return W,costHistory","de768650":"def classification_metrics(yTrueTrain,yPredictTrain,yTrueTest,yPredictTest,heatmap=False)->None:\n    \"\"\"\n    this function prints accuracy, precision, recall, F1 scre \n    and create confusion matrix  heatmap for both train and test sets\n    \n    Arguments:\n    yTrueTrain -- the true value of prediction from the train set\n    yPredictTrain -- predicted value from the train set\n    yTrueTest -- he true value of prediction from the test set\n    yPredictTest -- predicted value from the test set\n    heatmap -- boolean which tells us if plot heatmap or not\n    \n    Returns:\n    None\n    \"\"\"\n    print(\"train data:\\t  \"+ \"\\t\"*6+ \"test data:\\t\\n\")\n    print((\"accuracy:\\t {0} \"+ \"\\t\"*6+ \"accuracy:\\t {1}\").format(accuracy_score(yTrueTrain, yPredictTrain).round(2), \n                                                                 accuracy_score(yTrueTest, yPredictTest).round(2)))\n          \n    print((\"precision:\\t {0} \"+ \"\\t\"*6+ \"precision:\\t {1}\").format(precision_score(yTrueTrain, yPredictTrain).round(2), \n                                                                   precision_score(yTrueTest, yPredictTest).round(2)))\n    \n    print((\"recall:\\t\\t {0} \"+ \"\\t\"*6+ \"recall:\\t\\t {1}\").format(recall_score(yTrueTrain, yPredictTrain).round(2), \n                                                                 recall_score(yTrueTest, yPredictTest).round(2)))\n    \n    print((\"F1:\\t\\t {0} \"+ \"\\t\"*6+ \"F1:\\t\\t {1}\").format(f1_score(yTrueTrain, yPredictTrain).round(2), \n                                                         f1_score(yTrueTest, yPredictTest).round(2)))\n    if(heatmap):\n        f, ax  = plt.subplots(1,2,figsize = (18,6))\n        sns.heatmap(confusion_matrix(yTrueTrain,yPredictTrain),cmap='coolwarm',annot=True,ax=ax[0])\n        sns.heatmap(confusion_matrix(yTrueTest,yPredictTest),cmap='coolwarm',annot=True,ax=ax[1])\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.show()","6fcc21ce":"y_train=np.array(y_train).reshape((y_train.shape[0],1))\nweight,costHistory = lRmodel(X_train, y_train, alpha=0.1,numIterations=1500)\n\ntestPredict = lRpredict(X_test,weight)\ntrainPredict = lRpredict(X_train,weight)\n\nclassification_metrics(y_train,trainPredict,y_test,testPredict,True)","ed3c8d62":"plt.title(\"Cost history\")\nplt.plot(costHistory)","b71c38d5":"mayNotImportant = [\"diabetes\",\"sex\",\"smoking\"]","9ef8ef6a":"metricHistory = []\nfor col in mayNotImportant:\n    X_trainTemp = X_train.copy().drop(columns=col)\n    X_testTemp = X_test.copy().drop(columns=col)\n    \n    weight,costHistory = lRmodel(X_trainTemp, y_train, alpha=0.5,numIterations=1500)\n\n    trainPredict = lRpredict(X_trainTemp,weight)\n    testPredict = lRpredict(X_testTemp,weight)\n    metricHistory+=[y_train,trainPredict,y_test,testPredict]","60c990f4":"classification_metrics(metricHistory[0],metricHistory[1],metricHistory[2],metricHistory[3],False)","b153fd2f":"classification_metrics(metricHistory[4],metricHistory[5],metricHistory[6],metricHistory[7],False)","05ad9723":"classification_metrics(metricHistory[8],metricHistory[9],metricHistory[10],metricHistory[11],False)","baa8acfd":"alpha = [0.01,0.1,0.5]\niteration = [500,1500,5000]","acbe0493":"metricHistory = []\nfor a in alpha:\n    X_trainTemp = X_train.copy()\n    X_testTemp = X_test.copy()\n    \n    weight,costHistory = lRmodel(X_trainTemp, y_train, alpha=a)\n\n    trainPredict = lRpredict(X_trainTemp,weight)\n    testPredict = lRpredict(X_testTemp,weight)\n    metricHistory+=[y_train,trainPredict,y_test,testPredict]","701613c2":"classification_metrics(metricHistory[0],metricHistory[1],metricHistory[2],metricHistory[3],False)","d3f68347":"classification_metrics(metricHistory[4],metricHistory[5],metricHistory[6],metricHistory[7],False)","cd440fb4":"classification_metrics(metricHistory[8],metricHistory[9],metricHistory[10],metricHistory[11],False)","68d13217":"metricHistory = []\nfor i in iteration:\n    X_trainTemp = X_train.copy()\n    X_testTemp = X_test.copy()\n    \n    weight,costHistory = lRmodel(X_trainTemp, y_train, alpha=0.01,numIterations=i)\n\n    trainPredict = lRpredict(X_trainTemp,weight)\n    testPredict = lRpredict(X_testTemp,weight)\n    metricHistory+=[y_train,trainPredict,y_test,testPredict]","bbc96bda":"classification_metrics(metricHistory[0],metricHistory[1],metricHistory[2],metricHistory[3],False)","70860216":"classification_metrics(metricHistory[4],metricHistory[5],metricHistory[6],metricHistory[7],False)","9c6bdc71":"classification_metrics(metricHistory[8],metricHistory[9],metricHistory[10],metricHistory[11],False)","30b4f2a2":"X_train_wothout_time = X_train.drop(columns=\"time\")\nX_test_wothout_time = X_test.drop(columns=\"time\")\ny_train=np.array(y_train).reshape((y_train.shape[0],1))\n\nweight,costHistory = lRmodel(X_train_wothout_time, y_train, alpha=0.01,numIterations=1000)\n\ntestPredict = lRpredict(X_test_wothout_time,weight)\ntrainPredict = lRpredict(X_train_wothout_time,weight)\n\nclassification_metrics(y_train,trainPredict,y_test,testPredict,True)","81db795e":"df[\"creatinine_phosphokinase_squared\"] = np.square(df[\"creatinine_phosphokinase\"])\ndf[\"age_squared\"] = np.square(df[\"age\"])\ndf[\"creatinine_phosphokinase_power\"] = np.power(df[\"creatinine_phosphokinase\"],3)\n\nfeatures = ['age',\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\n            \"serum_creatinine\",\"serum_sodium\",\"creatinine_phosphokinase_squared\",\n           \"age_squared\",\"creatinine_phosphokinase_power\"]\n\ndf.drop(columns=\"time\",inplace=True,errors=\"ignore\")\nfeatureNormalization(features)\n\nX = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\nX_train, X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)\n\nsm = SMOTE()\nX_train_s,y_train_s =sm.fit_sample(X_train,y_train)\nX_test_s,y_test_s =sm.fit_sample(X_test,y_test)\n                                 \nX_train = X_train_s\ny_train = y_train_s\nX_test = X_test_s\ny_test = y_test_s\n                                 \ny_train=np.array(y_train).reshape((y_train.shape[0],1))\nweight,costHistory = lRmodel(X_train, y_train, alpha=0.5,numIterations=1500)\n\ntestPredict = lRpredict(X_test,weight)\ntrainPredict = lRpredict(X_train,weight)\n\nclassification_metrics(y_train,trainPredict,y_test,testPredict,True)","a8d194cf":"as we saw above target variable is imbalance so we will use SMOTE to solve this problem\n\nSMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.","cba246af":"- platelets is left skewed (1.46).\n- Many outliers are present in the distribution.\n- Most of the applicant's Platelets Distribution is near a range of 262k.","eaca61b7":"If the weighted sum of inputs is greater than zero, the predicted class is 1 and vice-versa","06e7e85a":"#### <div align=\"center\">**model 2 results for alpha=0.1**","7bc6a214":"this feature is more complex so I separate this<br>\n\ntime captures the time of the event. That is, the time at which the patient died or were censored.<br>\nI thought thah using the time column as a feature is wrong. <br>\n\nWhen you deploy your model, no end user will be able to provide you the value of time, since they do not know at what time in the future the patient will die.\nso now lets drop this feature and check accuracy","f2ca4ec6":"- Creatinine_phosphokinase is left skewed (4.46).\n- Many outliers are present in the distribution.\n- Most of the applicant's creatinine_phosphokinase is near a range of 250.","f3dc04db":"### **DEATH_EVENT** <a  name=\"5\"><\/a>\n>lets see Deat Event distribution","badba84f":"![h.png](attachment:h.png)","525f8a4c":"![sigmoid.png](attachment:sigmoid.png)","b6dbd0e1":"### **Numerical variables** <a  name=\"7\"><\/a>\n> Lets see distribution of **age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time**<br>\n> In these visualizations I will use the plotly library because we have numerical variables and we may need to see the values on the graph","0d2514c3":">treat train data imbalance.","e93b2204":">I am trying to choose alpha value <a  name=\"18\">","c345ef0f":"# <span style=\"color:blue\"><div align=\"center\">**Load Data** <a  name=\"3\"><\/a>","e3f56b47":"As we have seen the cost function decreases and after 300 iterations it almost no longer changes<br>","771c60dc":">now I try Hyperparameter tuning to improve results","cfe5356c":"#### <div align=\"center\">**model 3 results for alpha=0.5**","050da319":"![predict.png](attachment:predict.png)","2c423a19":"#### <div align=\"center\">**model 1 results for dataframe without \"diabetes\" feature**","e53f0b27":"- time is left skewed (0.12).\n- Many outliers are present in the distribution.\n- Most of the applicant's Time Distribution is near a range of 115.","4381d64a":"# <span style=\"color:blue\"><div align=\"center\">**About This Dataset** <a  name=\"1\"><\/a>","90cfe7f1":"# <div align=\"center\">**Table of Contents**\n1. [<span style=\"color:blue\">About This Dataset](#1)<br>\n2. [<span style=\"color:blue\">Import Libraries](#2)<br>\n3. [<span style=\"color:blue\">Load Data](#3)<br>\n4. [<span style=\"color:blue\">Data analysis](#4)<br>\n    4.1 [DEATH_EVENT](#5)<br>\n    4.2 [Nominal variables](#6)<br>\n    4.3 [Numerical variables](#7)<br>\n    4.4 [Bivariate Analysis](#8)<br>\n5. [<span style=\"color:blue\">Data preparation](#9)<br>\n6. [<span style=\"color:blue\">Logistic Regression from scratch](#10)<br>\n    6.1 [Baseline model](#11)<br>\n    6.2 [Feature Engineering](#12)<br>\n    6.3 [New Features](#13)<br>\n7. [<span style=\"color:blue\">Summary](#14)<br>\n\n    \n \n    \n    ","aea24020":"we can see that model without \"time\" feature has lower accuracy but I think this model is more generic<br>\nhere I write some arguments why we don't need time feature:<br>\n- incorrect data by definition\n- it has a high correlation to our target and it is not good practice to have a feature with a very high correlation because our model may have fit only these feature","9bcd7b0a":"### **Feature Engineering** <a  name=\"12\"><\/a>","67bd6d40":"### **Baseline Model** <a  name=\"11\"><\/a>","944c6dc7":"And the loss function is given accordingly as follows","cfa3bcdc":">lets create helper function that I will use in feature normalization<br>\nWe do the normalization of the features because the value of some of them was large compared to the others<br>\nand would cause an error in which case the model would only predict 1 or only 0.<br> \nthere are several method for normalization below I wrote simple method just apply log function<br> \nand also plot different before normalization and after. also there is some library for example standarscalar<br>\nso I will use this but we can also use only log function<a  name=\"16\">","3588fbc5":"from these plots we can determine that:<br>\n- most of our patients are men<br>\n- people who don't smoke<br>\n- haven't high_blood_pressure<br>\n- haven't diabetes <br>\n- haven't anaemia<br>","1d0f8628":"from this dependept plots we can determine some key things<br>\nfor example when creatinine_phosphokinase increases serum_creatinine decreases","308c85b3":"#### <div align=\"center\">**model 1 results for iterations=500**","df178ed0":"The hypothesis for logistic regression then becomes,","6ffeeac1":"> now we need to plot relationship between numerical variable and \"DEATH_EVENT\", but before we do this, we need to transform variable to categorical. we separate them with three class low Medium and High value of number. so we write helper function get_categories which Assign variable to the appropriate class and we apply this function all variable columns.","2b88054e":"this experiment shows that when we increase number of iterations model overfits the data<br>\nbecause the train result is increases and test results decreases","0fac1c5b":"notice that we didn't get significant and better.<br>\nprobably that's why we do not have enough data and we do not notice the difference <br>\nbut in this example, we need a high recall value because we don't want to have high FalsePositive values<br>\nso in these 3 experiments the second example is the best because his recall is 0.81 so we can drop the sex feature<br>\nbut also I think that dropping data is not good practice. we can try this kind of experiment for all features","506239ff":"> check for null values in the data","1f5aff54":"as we saw this model has almost the same result<br> \nbut we were able to get higher recall and The total result left the same<br>\nwe can try many experiments like this ","84b2e42f":"- serum_creatinine is left skewed (4.45).\n- Many outliers are present in the distribution.\n- Most of the applicant's serum_creatinine is near a range of 1.1.","98dee540":"as we was [here](#8) some feature may not be correlated to \"DEATH_EVENT\" so we need to check this, I remove some features and check metrics for this dataframe, here are some features which I will remove <a  name=\"17\"> ","4f5c3e93":"![cost2.png](attachment:cost2.png)","f59d2609":"To sum up.<br> \nOur advice was to predict whether a patient would have a heart attack based on various data.<br>\n\n- First, we studied the data, distributed them one by one.[(see this)](#4)<br>\n<br>\n- We also looked at features correlations and tried to determine which one contributed to the prediction. [(see this)](#8)<br>\n<br>\n- and also looked at the connection with the target feature.[(see this)](#15)<br>\n<br>\n- We did the normalization of the features because the value of some of them was large compared to the others<br> \nand would cause an error in which case the model would only predict 1 or only 0.<br>\nWe also eliminate the imbalance in our target data to avoid certain errors. [(see this)](#16)<br>\n<br>\n- We did a logistic regression from 0 and adjusted our data.<br>\nWe tested it and evaluated its accuracy to see how the cost function was decreasing. [(see this)](#10)<br>\n<br>\n- Then we tried the algorithm in case we threw away several features<br>\nand we saw that throwing one of for example sex, gave us a not so bad result. [(see this)](#17)<br>\n<br>\n- We tried turning the parameters. We tested different values for alpha and for the number of iterations<br>\nwe assumed that alpha was the best at 0.5 while the iterations increased as the model went overfit. [(see this)](#18)<br>\n<br>\n- We also considered a separate \"time\" feature because it had the greatest impact on the prediction<br> \nand found that it was not very accurate data. [(see this)](#19)<br>\n<br>\n- We also tried a new feature consisting of the result we could not greatly improve<br>\nbut increased the recall which is very important in a similar type of examples. [(see this)](#20)<br>","f5b918fa":"#### <div align=\"center\">**model 2 results for dataframe without \"sex\" feature**","1c1a09a5":"#### <div align=\"center\">**model 1 results for alpha=0.01**","3813c787":"#### <div align=\"center\">**model 3 results for dataframe without \"smoking\" feature**","df468e74":"#### <div align=\"center\">**model 3 results for iterations=5000**","b4a04f24":"- ejection_fraction is left skewed (0.5).\n- Not many outliers.\n- Most of the applicant's ejection_fraction is near a range of 38.","492d9f67":"#### <div align=\"center\">**model 2 results for iterations=1500**","c31bdefd":"Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.<br>\n\n#### <span style=\"color:blue\">  **Columns**\n    \n>**age** <br>\n>**anaemia**  _Decrease of red blood cells or hemoglobin (boolean)_ <br> \n>**creatinine_phosphokinase** _Level of the CPK enzyme in the blood (mcg\/L)_ <br>\n>**diabetes**  _If the patient has diabetes (boolean)_ <br>\n>**ejection_fraction** _Percentage of blood leaving the heart at each contraction (percentage)_ <br>\n>**high_blood_pressure** _If the patient has hypertension (boolean)_ <br>\n>**platelets** _Platelets in the blood (kiloplatelets\/mL)_ <br>\n>**serum_creatinine** _Level of serum creatinine in the blood (mg\/dL)_ <br>\n>**serum_sodium** _Level of serum sodium in the blood (mEq\/L)_ <br>\n>**sex** _Woman or man (binary)_ <br>\n>**smoking** _If the patient smokes or not (boolean)_ <br>\n>**time** _Follow-up period (days)_ <br>\n>**DEATH_EVENT** _If the patient deceased during the follow-up period (boolean)_ <br>\n#### <span style=\"color:blue\">  **Source** https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data ","c4e85725":">It will be good if we look at the cost function and make sure that it decreases","121d72f7":"![gradient.png](attachment:gradient.png)","53a4c522":"where the sigmoid function is represented by,","5889d94e":"we notice that there is some features which is not correlated to output variable, we will check this hyphotesis below","34c59a93":"> Let us first review the general information of the data ","9086f8be":"As you can see the data types are int and float so we do not need to convert them into categorical variables","3c3512ee":"> Fit our model and test it","bc745491":">I am trying to choose number of Iterations","0ba9c68b":"### **Nominal variables** <a  name=\"6\"><\/a>\n> Lets see distribution of **anaemia, diabetes, high_blood_pressure, sex, smoking**","aaabc6ad":"In this task, I will use Logistical Regression because this task is a classification problem and I think this model will work well. We can use another classification model but Logistical Regression is simple and I think it will work fine","1d4f903c":"# <span style=\"color:blue\"><div align=\"center\">**Bivariate Analysis** <a  name=\"8\"><\/a>","aa2dd553":"As we see it, the death rate is twice as low, so we will need to treat the imbalance. which we can with two ways:\n1. preproccess data\n2. come up with appropriate metric","4fe06a41":"> lets plot the relationship between each variable regarding \"DEATH_EVENT\" <br>\nfirst consider categorical variables, so I separate data by categorical and numerical variable<a  name=\"15\">","078dc3b3":"![applySigmoid.png](attachment:applySigmoid.png)","584d3903":"There was no significant pattern observed based on the above feautres. <br> \nIt can be inferred that this features does not much impact for death","e99b2810":">lets check Creatinine Phosphokinase Distribution","15af8c4b":"A Linear Regression model can be represented by the equation.","084b9904":"# <span style=\"color:blue\"><div align=\"center\">**Logistic Regression from scratch** <a  name=\"10\"><\/a>","f2b2d273":">lets see age distribution","a35f3b42":"as we see there is some important things which we should mention\n- Aplicant age is left skewed (0.42).\n- Not many outliers.\n- Most of the applicant's age is near a range of 60.","3e1aec1c":">lets create new feature and test them<a  name=\"20\">","8589a8fc":"- serum_sodium is right skewed (1.04).\n- there are a little bit outliers in the distribution.\n- Most of the applicant's serum_sodium is near a range of 137.","d8aba491":"We can see that the results do not differ much<br>\nbut maybe the third one is the best because there we have high recall","67faccfe":"We will use gradient descent to minimize the cost function. The gradient can be given by","c0fb6f24":"# <span style=\"color:blue\"><div align=\"center\">**Data analysis** <a  name=\"4\"><\/a>","672fef0c":">firstly let's plot the correlation matrix which will give us a whole picture of the feature dependencies of our target","312e4448":"# <span style=\"color:blue\"><div align=\"center\">**Import Libraries** <a  name=\"2\"><\/a>","1803ac8b":"As we see, there are no **null** values and we don't need to drop them out or fill by mean value or something like that, this technique is called handling missing value, but we will not use it now. so we can continue","ab192e3a":"### **Time** <a  name=\"19\">","e482cfa9":"Logistic Regression cost function for our model and is: ","74f44a63":">let's divide data into train and test parts I use 0.3 test_size because we don't have a big amount of data so our test size would be a little bit more. In other cases, I would use only 0.2 or less test size","0f60fc49":"![loss.png](attachment:loss.png)","a39b7a9d":"# <span style=\"color:blue\"><div align=\"center\">**Data Preparation** <a  name=\"9\"><\/a>","c98ed4c0":"### **New Features** <a  name=\"13\"><\/a>","9c0d6ce5":"from this plots we can determine some important things unlike the previous plots. <br>\nfor example when ejection_fraction is low Most of the aplicants die <br>\nalso, time feature has a high impact on DEATH EVENT so I will consider this case below separately","00183cdc":"> helper function for plotting","c3c64210":"# <span style=\"color:blue\"><div align=\"center\">**Summary** <a  name=\"14\"><\/a>","d9fb7dc3":"> treat Test data imbalance.<br> I do this after splitting the data so that it does not happen that I only have copy files in the train and test sections"}}