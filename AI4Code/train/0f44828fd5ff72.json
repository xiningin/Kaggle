{"cell_type":{"89c75483":"code","55430a95":"code","a77a630c":"code","382fef22":"code","cb43b0e0":"code","5132f790":"code","8a603674":"code","2e8bc763":"code","48f55a20":"code","8a5e2017":"code","cc6ba3f8":"code","e449e133":"code","c078073b":"code","96f604c2":"code","be53736c":"code","1ea5c39b":"code","700f906d":"code","82680bee":"code","624eecbb":"code","5a6d151a":"code","061a803c":"code","e68040e9":"code","557f86af":"code","b1dcccde":"code","6e519601":"code","05b8a404":"code","c703b6ff":"code","095f467d":"code","fb802a9f":"code","c514758f":"code","d3f46271":"code","57d79c3c":"code","61e70c4d":"code","2ce8d194":"code","ee546fd3":"code","344bd789":"code","f2144630":"code","3f3f2610":"code","39880536":"code","ce0916cd":"code","c66b6d3c":"code","8ab39e8b":"code","ddbbbb2f":"code","cd133756":"code","9aad2553":"code","280b90b7":"code","48d8282b":"code","96f39eb1":"code","459bb7c9":"code","969923a1":"code","1732573a":"code","0346751b":"code","e52f5b09":"code","11228bfa":"code","3a846dc3":"code","360f7400":"markdown","402dda0b":"markdown","94477837":"markdown","0bce2755":"markdown","6e48ec58":"markdown","d5d23b50":"markdown","dcc3a30a":"markdown","31d33cf5":"markdown","189023ff":"markdown","07f82507":"markdown","d395f13b":"markdown","1865155d":"markdown","c53fd3b9":"markdown","0969b970":"markdown","96091a57":"markdown","8e40d7d6":"markdown","7660dbdf":"markdown","5958c1d8":"markdown","f21b1fab":"markdown","f9e15888":"markdown","ff13c6d3":"markdown","bd75708c":"markdown","72ce8731":"markdown","ad080db2":"markdown","90aaaa21":"markdown","cd90f826":"markdown","b8eabf2e":"markdown","33b815ec":"markdown","895efa56":"markdown","c48897ff":"markdown"},"source":{"89c75483":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk import ngrams\nimport string,re\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nimport warnings,os","55430a95":"plt.figure(figsize=(16,7))\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')","a77a630c":"# Locate the data directories\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","382fef22":"train=pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip',sep='\\t')\ntest=pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip',sep='\\t')","cb43b0e0":"train.shape, test.shape","5132f790":"train.head()","8a603674":"test.head()","2e8bc763":"train.info()","48f55a20":"train.isnull().sum()","8a5e2017":"train.head()","cc6ba3f8":"train['sentiment_class'] = train['Sentiment'].map({0:'negative',1:'somewhat negative',2:'neutral',3:'somewhat positive',4:'positive'})\ntrain.head()","e449e133":"def remove_punctuation(text):\n    return \"\".join([t for t in text if t not in string.punctuation])","c078073b":"train['Phrase']=train['Phrase'].apply(lambda x:remove_punctuation(x))\ntrain.head()","96f604c2":"def words_with_more_than_three_chars(text):\n    return \" \".join([t for t in text.split() if len(t)>3])","be53736c":"train['Phrase']=train['Phrase'].apply(lambda x:words_with_more_than_three_chars(x))\ntrain.head()","1ea5c39b":"stop_words=stopwords.words('english')\ntrain['Phrase']=train['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntrain.head()","700f906d":"train.groupby('Sentiment')['Sentiment'].count()","82680bee":"train.groupby('sentiment_class')['sentiment_class'].count().plot(kind='bar',title='Target class',figsize=(16,7),grid=True)","624eecbb":"((train.groupby('sentiment_class')['sentiment_class'].count()\/train.shape[0])*100).plot(kind='pie',figsize=(7,7),title='% Target class', autopct='%1.0f%%')","5a6d151a":"train['PhraseLength']=train['Phrase'].apply(lambda x: len(x))","061a803c":"train.sort_values(by='PhraseLength', ascending=False).head()","e68040e9":"plt.figure(figsize=(16,7))\nbins=np.linspace(0,200,50)\nplt.hist(train[train['sentiment_class']=='negative']['PhraseLength'],bins=bins,density=True,label='negative')\nplt.hist(train[train['sentiment_class']=='somewhat negative']['PhraseLength'],bins=bins,density=True,label='somewhat negative')\nplt.hist(train[train['sentiment_class']=='neutral']['PhraseLength'],bins=bins,density=True,label='neutral')\nplt.hist(train[train['sentiment_class']=='somewhat positive']['PhraseLength'],bins=bins,density=True,label='somewhat positive')\nplt.hist(train[train['sentiment_class']=='positive']['PhraseLength'],bins=bins,density=True,label='positive')\nplt.xlabel('Phrase length')\nplt.legend()\nplt.show()","557f86af":"# Install wordcoud library\n# !pip install wordcloud","b1dcccde":"from wordcloud import WordCloud, STOPWORDS \nstopwords = set(STOPWORDS) ","6e519601":"word_cloud_common_words=[]  \nfor index, row in train.iterrows(): \n    word_cloud_common_words.append((row['Phrase'])) \nword_cloud_common_words\n\nwordcloud = WordCloud(width = 1600, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 5).generate(''.join(word_cloud_common_words)) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (16, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","05b8a404":"text_list=[]  \nfor index, row in train.iterrows(): \n    text_list.append((row['Phrase'])) \ntext_list\n\ntotal_words=''.join(text_list)\ntotal_words=word_tokenize(total_words)","c703b6ff":"freq_words=FreqDist(total_words)\nword_frequency=FreqDist(freq_words)","095f467d":"# 10 common words\nprint(word_frequency.most_common(10))","fb802a9f":"# visualize \npd.DataFrame(word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","c514758f":"neg_text_list=[]  \nfor index, row in train[train['Sentiment']==0].iterrows(): \n    neg_text_list.append((row['Phrase'])) \nneg_text_list\n\nneg_total_words=' '.join(neg_text_list)\nneg_total_words=word_tokenize(neg_total_words)\n\nneg_freq_words=FreqDist(neg_total_words)\nneg_word_frequency=FreqDist(neg_freq_words)","d3f46271":"# visualize \npd.DataFrame(neg_word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","57d79c3c":"pos_text_list=[]  \nfor index, row in train[train['Sentiment']==4].iterrows(): \n    pos_text_list.append((row['Phrase'])) \npos_text_list\n\npos_total_words=' '.join(pos_text_list)\npos_total_words=word_tokenize(pos_total_words)\n\npos_freq_words=FreqDist(pos_total_words)\npos_word_frequency=FreqDist(pos_freq_words)","61e70c4d":"# visualize \npd.DataFrame(pos_word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","2ce8d194":"text=\"Tom and Jerry love mickey. But mickey dont love Tom and Jerry. What a love mickey is getting from these two friends\"\nbigram_frequency = FreqDist(ngrams(word_tokenize(text),3))\nbigram_frequency.most_common()[0:5]","ee546fd3":"text_list=[]  \nfor index, row in train.iterrows(): \n    text_list.append((row['Phrase'])) \ntext_list\n\ntotal_words=' '.join(text_list)\ntotal_words=word_tokenize(total_words)\n\nfreq_words=FreqDist(total_words)\nword_frequency=FreqDist(ngrams(freq_words,2))\nword_frequency.most_common()[0:5]","344bd789":"# visualize \npd.DataFrame(word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","f2144630":"train['tokenized_words']=train['Phrase'].apply(lambda x:word_tokenize(x))\ntrain.head()","3f3f2610":"count_vectorizer=CountVectorizer()\nphrase_dtm=count_vectorizer.fit_transform(train['Phrase'])","39880536":"phrase_dtm.shape","ce0916cd":"X_train,X_val,y_train,y_val=train_test_split(phrase_dtm,train['Sentiment'],test_size=0.3, random_state=38)\nX_train.shape,y_train.shape,X_val.shape,y_val.shape","c66b6d3c":"model=LogisticRegression()","8ab39e8b":"model.fit(X_train,y_train)","ddbbbb2f":"accuracy_score(model.predict(X_val),y_val)*100","cd133756":"del X_train\ndel X_val\ndel y_train\ndel y_val","9aad2553":"tfidf=TfidfVectorizer()\ntfidf_dtm=tfidf.fit_transform(train['Phrase'])","280b90b7":"X_train,X_val,y_train,y_val=train_test_split(tfidf_dtm,train['Sentiment'],test_size=0.3, random_state=38)\nX_train.shape,y_train.shape,X_val.shape,y_val.shape","48d8282b":"tfidf_model=LogisticRegression()","96f39eb1":"tfidf_model.fit(X_train,y_train)","459bb7c9":"accuracy_score(tfidf_model.predict(X_val),y_val)*100","969923a1":"print(tfidf_model.predict(X_val)[0:10])","1732573a":"def predict_new_text(text):\n    tfidf_text=tfidf.transform([text])\n    return tfidf_model.predict(tfidf_text)","0346751b":"predict_new_text(\"The movie is bad and sucks!\")","e52f5b09":"test['Phrase']=test['Phrase'].apply(lambda x:remove_punctuation(x))\ntest['Phrase']=test['Phrase'].apply(lambda x:words_with_more_than_three_chars(x))\ntest['Phrase']=test['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntest_dtm=tfidf.transform(test['Phrase'])","11228bfa":"# Predict with test data\ntest['Sentiment']=tfidf_model.predict(test_dtm)\ntest.set_index=test['PhraseId']\ntest.head()","3a846dc3":"# save results to csv file\n# test.to_csv('Submission.csv',columns=['PhraseId','Sentiment'],index=False)","360f7400":"Get percentages of each class","402dda0b":"<h1>End-to-End NLP (EDA & ML) with Sentiment Analysis<\/h1>","94477837":"Split data into training and validation sets (70:30) ratio","0bce2755":"Visualize the target variables","6e48ec58":"Distribution of phrase length on each class","d5d23b50":"Remove words with less than 2 characters","dcc3a30a":"Word Frequency","31d33cf5":"check sentiment categories","189023ff":"new data prediction function","07f82507":"Free up memory for tf-idf","d395f13b":"Prepare Training data","1865155d":"Create Bag of words with CountVectorizer","c53fd3b9":"Sentiment Description","0969b970":"Measure model performance","96091a57":"Common words with word cloud","8e40d7d6":"Prepare Test Data","7660dbdf":"<div align='center'><H1>Part 2 Machine Learning Modeling<\/H1><\/div>","5958c1d8":"Preparing data with tf-idf","f21b1fab":"<div align='center'><H1>Part 1 Exploratory Data Analysis<\/H1><\/div>","f9e15888":"Common words used for negative sentiment","ff13c6d3":"Remove stopwords","bd75708c":"Predict on test data","72ce8731":"Common words used for positive sentiment","ad080db2":"In this notebook we are going to go through on how to perform text classification using logistic regression and several text encoding techniques such as bag of words and tf-idf. Our task will be to classify text to determine it's sentiment class. Our dataset contains the movie review data with labeled sentiment class of 0,1,2,3 and 4 where 0 is negative, 1 somehow negative, 2 neutral, 3 somehow positive and 4 positive.<br><br>\nWe will start with Exploratory Data Analysis then perform machine learning modeling.","90aaaa21":"Common bigram words used for positive sentiment","cd90f826":"Load data","b8eabf2e":"Train Logistic Regression model","33b815ec":"<div align='center'><h1>Import required libraries<\/h1>","895efa56":"Adding Phrase length","c48897ff":"Remove punctuations"}}