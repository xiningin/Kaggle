{"cell_type":{"be9303b4":"code","21193904":"code","9fbee016":"code","f5446773":"code","c4e97754":"code","0caee7fb":"code","2b170b85":"code","30e2e83b":"code","d7cdbf5e":"code","cc2963de":"code","d6b4f626":"code","480e3f46":"markdown","9a2293de":"markdown","0b756caa":"markdown","010cb4da":"markdown","a3bdb596":"markdown","0e176e12":"markdown","0a8972e2":"markdown","6a4ed9a9":"markdown","8be77ed3":"markdown"},"source":{"be9303b4":"# imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 13\nnp.random.seed(seed)","21193904":"raw_data = pd.read_csv(\"..\/input\/roboBohr.csv\")\nX = raw_data.drop(['Unnamed: 0', 'pubchem_id', 'Eat'], axis = 1)\n# not sure what the last 25 features are, so I am just going to drop them for now\ny = raw_data['Eat']\nX.sample(3)","9fbee016":"from sklearn.preprocessing import StandardScaler, normalize\n\nX_standardized = StandardScaler().fit_transform(X)\nX_normalized = normalize(X)","f5446773":"sns.distplot(y)","c4e97754":"from sklearn.decomposition import PCA, KernelPCA\n## PCA\n\npca = PCA(n_components=2, random_state=seed)\n\nstart_time = time.time()\nX_reduced = pca.fit_transform(X_normalized)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nprint(\"Number of components: {}\".format(pca.components_.shape[0]))\nprint(\"Explained variance: \", pca.explained_variance_ratio_.sum())\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('PCA')\nsns.despine()\nplt.show()","0caee7fb":"start_time = time.time()\nkpca = KernelPCA(n_components=2, kernel=\"linear\")\nX_kpca = kpca.fit_transform(X)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(X_kpca, axis=0)\nexplained_variance_ratio = explained_variance \/ np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_kpca[:,0], X_kpca[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$k-PCA_1$')\nplt.ylabel(r'$k-PCA_1$')\nsns.despine()\nplt.show()","2b170b85":"start_time = time.time()\nkpca3 = KernelPCA(n_components=2, kernel=\"linear\", random_state=seed)\nX_kpca3 = kpca3.fit_transform(X_normalized)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nexplained_variance = np.var(X_kpca3, axis=0)\nexplained_variance_ratio = explained_variance \/ np.sum(explained_variance)\nprint(\"Variance Explained: \", np.sum(explained_variance_ratio))\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_kpca3[:,0], X_kpca3[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_1$')\nplt.title('Kernel PCA: Normalized')\nsns.despine()\nplt.show()","30e2e83b":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=seed)\nX_tsne = tsne.fit_transform(X_normalized)\n\nfig = plt.figure(figsize=(14,10))\nax  = fig.add_subplot(111)\n\nscatter = ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=0.5)\ncolorbar = fig.colorbar(scatter, ax=ax, label = \"E | Ry | \")\nplt.xlabel(r'$Z_1$')\nplt.ylabel(r'$Z_2$')\nplt.title('T-SNE: Perplexity = 30')\nsns.despine()\nplt.show()","d7cdbf5e":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, \n                                                    test_size=0.15, \n                                                    random_state=seed)\n\nX_train_train, X_dev, y_train_train, y_dev = train_test_split(X_train, y_train, \n                                                    test_size=0.18, \n                                                    random_state=seed)\n\n\nprint(\"X: \", X.shape[0])\nprint(\"Train: {}\".format(X_train_train.shape[0]))\nprint(\"Dev: {}\".format(X_dev.shape[0]))\nprint(\"Val: {}\".format(X_val.shape[0]))","cc2963de":"from sklearn.metrics import mean_absolute_error, r2_score\nfrom xgboost import XGBRegressor\n\n# Parameters for XGBoost model\n\nparams = {}\nparams['learning_rate'] = 0.09\nparams['max_depth'] = 8\nparams['n_estimators'] = 100\nparams['objective'] = 'reg:linear'\nparams['booster'] = 'gbtree'\nparams['gamma'] = 1e-3\nparams['subsample'] = 0.6\nparams['reg_alpha'] = 0.115\nparams['reg_lambda'] = 0.58\nparams['scale_pos_weight'] = 1\nparams['base_score'] = 0.5\nparams['random_state'] = seed\nparams['silent'] = True\nparams['num_leaves'] = 17\n\nprint('XGBoost')\nprint('--------------------------------------')\nstart_time = time.time()\n\nXGB = XGBRegressor(**params)\n#XGB.fit(X_train, y_train, verbose=True, eval_metric='rmse')\neval_set = [(X_train_train, y_train_train), (X_dev, y_dev)]\nXGB.fit(X_train_train, y_train_train, eval_metric='mae', eval_set=eval_set, verbose=False)\nY_pred_XGB = XGB.predict(X_dev)\nprint(\"Mean absolute error\", mean_absolute_error(y_dev, Y_pred_XGB))\nprint('R2 score: %0.5f'% r2_score(y_dev, Y_pred_XGB))\n\nprint(\"Took %s seconds\" % (time.time() - start_time))\nprint('--------------------------------------')\n\n# learning_rate = 0.09\n#Mean squared error 0.008758109152032869\n#R2 score: 0.","d6b4f626":"y_val_pred = XGB.predict(X_val)\n\nprint(\"Mean absoulte error: {} kcal\/mol\".format(313.495392 * mean_absolute_error(y_val, y_val_pred)))\nprint(\"R^2: \", r2_score(y_val, y_val_pred))","480e3f46":"Kernel PCA, with a linear kernel, explains 100% of the original variance of our data with only 2 components, perfect for visualization purposes! However, it is important to note that the range of the entries in the Coulomb Matrices range from 2.906146 to 388.023441, so we should normalize these values first.","9a2293de":"**Evaluate**  \n\nNow let's evaluate our final model on the validation set.","0b756caa":"**XGBoost**","010cb4da":"**PCA**","a3bdb596":"Because PCA (with 2 components) only explains less than 63% of the original variance of our data, if our goal of PCA is to visualize the data in 2 dimensions, we might want to explore something else like Kernel PCA or t-SNE.","0e176e12":"# Introduction  \n\nPlease check out my new [study](https:\/\/www.kaggle.com\/mjmurphy28\/predicting-atomization-energy-qm7), using a different but similar dataset. \n\n**Note**: atomization energies here are measured in Rydberg units, however, a more commonly used unit is kcal\/mol. So conversion is necessary:  \n\n  * **1 Ry = 313.495392 kcal\/mol**  \n  \nThe \"acceptable chemical accuracy\u201d for atomization energy is 1 kcal\/mol ([source](https:\/\/papers.nips.cc\/paper\/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf)).\n\n# Data  \n\nThe data provided was originally downloaded from PubChem, a JSON file listing every atom in the molecule with it's corresponding element type (eg. hydrogen) as well as it's Cartesian coordinates. From this, as well as atomic charges (which can be found in any basic chemistry textbook), a Coulomb Matrix was computed for every molecule according to [Rupp et al. PRL, 2012], as so:\n\n  * $C_{i,i} = 0.5 \\cdot Z^2.4$  \n  * $C_{i,j} = Z_i \\cdot \\frac{Z_j}{|(R_i\u2212R_j)|}$ \n  * $Z_i$ - nuclear charge of atom i  \n  * $R_i$ - cartesian coordinates of atom i","0a8972e2":"# Visualization  \n","6a4ed9a9":"# Regression  \n\nFirst we are going to split our data into 3 sets: train (70%), dev (15%) and test (15%). ","8be77ed3":"**t-SNE**"}}