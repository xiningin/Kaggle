{"cell_type":{"0cb9f8ab":"code","6944dde9":"code","ee6c013d":"code","8e86e1dd":"code","501fd5fc":"code","72c832d9":"code","8c24f3bd":"code","5ec6b4a7":"code","bfba2e6f":"code","5ad77496":"code","19bb787b":"code","bef4aafb":"code","9a14ad83":"code","da7cbce4":"code","d0a0c197":"code","5405d6b0":"code","2b792037":"code","f341f90b":"code","c947fd4f":"code","827cbf9c":"code","c16c4afe":"code","dca82480":"code","44231b51":"code","edd8fa25":"code","1751e061":"code","34b6b3a0":"code","5ac24d8e":"code","321e55e4":"code","a4b2a475":"code","b6f5891c":"code","7e229812":"code","797048ad":"code","a16329a7":"code","acb36eae":"code","bb2aebe7":"code","518698f1":"code","f160acbf":"code","1687d2ad":"code","bb7e43dd":"code","54fd0673":"code","2dff2dda":"code","34904fef":"code","05604a15":"code","140302c9":"code","d34c3448":"code","7cb1102b":"code","bf76a5f7":"code","2a327985":"code","b0c469fb":"code","b873c96d":"code","dc51adc2":"code","8cf58800":"code","4288a9d5":"code","927e9e1e":"code","39774172":"code","d1171f43":"code","d52cec85":"code","703fbd77":"code","4e556063":"code","dd420575":"code","351e297a":"code","69291c06":"markdown","6abc46ad":"markdown","bc39ff1f":"markdown","09250e1f":"markdown","cf6579be":"markdown","7da92eee":"markdown","6ed54a61":"markdown","99c47029":"markdown","1aa55a91":"markdown","d675b67d":"markdown","b35266eb":"markdown","c2ce84b5":"markdown","9f651c68":"markdown","bef9cc29":"markdown","23843eb9":"markdown","55eeafb0":"markdown","e05dd985":"markdown","3ca880ba":"markdown","02ec07ac":"markdown","9a5c0561":"markdown","145a5e29":"markdown"},"source":{"0cb9f8ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6944dde9":"import seaborn as sns\nimport matplotlib.pyplot as plt","ee6c013d":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","8e86e1dd":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","501fd5fc":"print(train_data.shape)\nprint(test_data.shape)","72c832d9":"train_data.info()","8c24f3bd":"train_data.isna().sum()","5ec6b4a7":"test_data.info()","bfba2e6f":"test_data.isna().sum()","5ad77496":"print(train_data['Pclass'].value_counts())\nsns.countplot(x='Pclass', data=train_data)\nplt.title('Passenger Class Count')\nplt.show()","19bb787b":"print(train_data['Survived'].value_counts())\nsns.countplot(x='Survived', data=train_data)\nplt.xticks([0,1],['No','Yes']) \nplt.title('Survived\/Non-Survived Count')\nplt.show()","bef4aafb":"print(train_data.groupby(['Pclass','Survived'])['Survived'].count())\nsns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.title(\"Class wise count of Survived\/Non-Survived Passengers\")\nplt.show()","9a14ad83":"print(train_data['Sex'].value_counts())\nsns.countplot(x='Sex', data=train_data)\nplt.title('Male\/Female Count')\nplt.show()","da7cbce4":"train_data.groupby(['Sex','Survived'])['Survived'].count()","d0a0c197":"sns.countplot(x='Survived', hue='Sex', data=train_data)\nsns.set(style=\"darkgrid\")\nplt.xticks([0,1],['No','Yes']) \nplt.title(\"Survived\/Non-Survived Passengers vs Sex\")\nplt.show()","5405d6b0":"sns.catplot(x=\"Survived\", y=\"Age\", data=train_data)\nplt.show()","2b792037":"sns.catplot(x=\"Survived\", y=\"Age\", hue=\"Pclass\", kind=\"box\", data=train_data)\nplt.xticks([0,1],['No','Yes']) \nplt.show()","f341f90b":"print(train_data.groupby(['Sex','Pclass'])['Pclass'].count())\nsns.countplot(x='Sex', hue='Pclass', data=train_data)\nplt.title(\"Class wise count of Male\/Female Passengers\")\nplt.show()","c947fd4f":"print(train_data.groupby(['Pclass','Survived','Sex'])['Survived'].count())\nsns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\",\n            markers=[\"^\", \"o\"], linestyles=[\"-\", \"--\"],\n            kind=\"point\", data=train_data)\nplt.show()","827cbf9c":"print(train_data['Embarked'].value_counts())\nsns.catplot(x=\"Embarked\", kind=\"count\", data=train_data)\nplt.show()","c16c4afe":"print(train_data.groupby(['Embarked','Pclass'])['Pclass'].count())\nsns.countplot(x='Embarked', hue='Pclass', data=train_data)\nplt.show()","dca82480":"train_data['train'] = 1\ntest_data['train'] = 0\ntest_data['Survived'] = np.NaN\ncombined_data = pd.concat([train_data,test_data])","44231b51":"combined_data.info()","edd8fa25":"combined_data.dropna(subset=['Embarked'],inplace = True)","1751e061":"# convert Sex into categorical value 0 for male and 1 for female\ncombined_data[\"Sex\"] = combined_data[\"Sex\"].map({\"male\": 0, \"female\":1}).astype(int)","34b6b3a0":"combined_data['Cabin_FE'] = combined_data['Cabin'].astype(str).apply(lambda x: x[:1])","5ac24d8e":"combined_data.Age = combined_data.Age.fillna(combined_data.Age.median())\ncombined_data.Fare = combined_data.Fare.fillna(combined_data.Fare.median())","321e55e4":"combined_data['Family_Size'] = combined_data['Parch'] + combined_data['SibSp'] + 1","a4b2a475":"combined_data['Name_Title']=0\nfor i in combined_data:\n    combined_data['Name_Title']=combined_data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","b6f5891c":"combined_data['Name_Title'].value_counts()","7e229812":"#replacing all titles with mr, mrs, miss, master\ndef replace_titles(x):\n    title=x['Name_Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', 'Sir', 'Countess', 'Lady', 'Dona', 'Dr']:\n        return 'Rare'\n    elif title in ['Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    else:\n        return title\n    \ncombined_data['Name_Title'] = combined_data.apply(replace_titles, axis=1)","797048ad":"combined_data.head()","a16329a7":"combined_data['AgeBand'] = pd.cut(combined_data['Age'], 10)\ncombined_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","acb36eae":"combined_data.loc[ combined_data['Age'] <= 8, 'Age'] = 0\ncombined_data.loc[(combined_data['Age'] >  8) & (combined_data['Age'] <= 16), 'Age'] = 1\ncombined_data.loc[(combined_data['Age'] > 16) & (combined_data['Age'] <= 24), 'Age'] = 2\ncombined_data.loc[(combined_data['Age'] > 24) & (combined_data['Age'] <= 32), 'Age'] = 3\ncombined_data.loc[(combined_data['Age'] > 32) & (combined_data['Age'] <= 40), 'Age'] = 4\ncombined_data.loc[(combined_data['Age'] > 40) & (combined_data['Age'] <= 48), 'Age'] = 5\ncombined_data.loc[(combined_data['Age'] > 48) & (combined_data['Age'] <= 56), 'Age'] = 6\ncombined_data.loc[(combined_data['Age'] > 56) & (combined_data['Age'] <= 64), 'Age'] = 7\ncombined_data.loc[(combined_data['Age'] > 64) & (combined_data['Age'] <= 72), 'Age'] = 8\ncombined_data.loc[ combined_data['Age'] > 72, 'Age'] = 9","bb2aebe7":"combined_data['FareBand'] = pd.cut(combined_data['Fare'], 10)\ncombined_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","518698f1":"combined_data.loc[ combined_data['Fare'] <= 51, 'Fare'] = 0\ncombined_data.loc[(combined_data['Fare'] >  51) & (combined_data['Fare'] <= 102), 'Fare'] = 1\ncombined_data.loc[(combined_data['Fare'] > 102) & (combined_data['Fare'] <= 153), 'Fare'] = 2\ncombined_data.loc[(combined_data['Fare'] > 153) & (combined_data['Fare'] <= 204), 'Fare'] = 3\ncombined_data.loc[(combined_data['Fare'] > 204) & (combined_data['Fare'] <= 256), 'Fare'] = 4\ncombined_data.loc[(combined_data['Fare'] > 256) & (combined_data['Fare'] <= 307), 'Fare'] = 5\ncombined_data.loc[(combined_data['Fare'] > 307) & (combined_data['Fare'] <= 358), 'Fare'] = 6\ncombined_data.loc[(combined_data['Fare'] > 358) & (combined_data['Fare'] <= 409), 'Fare'] = 7\ncombined_data.loc[(combined_data['Fare'] > 409) & (combined_data['Fare'] <= 461), 'Fare'] = 8\ncombined_data.loc[ combined_data['Fare'] > 461, 'Fare'] = 9","f160acbf":"combined_data['IsAlone'] = combined_data['Family_Size'].apply(lambda x: 1 if x == 1 else 0)","1687d2ad":"combined_data.drop(labels = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"AgeBand\", \"FareBand\"], axis = 1, inplace = True)","bb7e43dd":"combined_data = pd.get_dummies(combined_data[['Pclass', 'Survived', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked','Cabin_FE','Family_Size','Name_Title','IsAlone', 'train']])","54fd0673":"combined_data.head()","2dff2dda":"#Split to train test again\nX    = combined_data[combined_data['train'] == 1].drop(['train', 'Survived'], axis=1)\ntest = combined_data[combined_data['train'] == 0].drop(['train', 'Survived'], axis =1)\n\ny = combined_data[combined_data['train'] == 1].Survived","34904fef":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","05604a15":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split","140302c9":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)","d34c3448":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","7cb1102b":"svc = SVC(random_state=42)\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\ny_pred_test = svc.predict(test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_test.astype(int)})\noutput.to_csv('svc.csv', index=False)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='svc.csv')","bf76a5f7":"dtc = DecisionTreeClassifier(random_state=42)\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","2a327985":"rfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b0c469fb":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b873c96d":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","dc51adc2":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state=42)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","8cf58800":"import lightgbm as lgbm\nlgbmc = lgbm.LGBMClassifier(random_state=42)\nlgbmc.fit(X_train, y_train)\ny_pred = lgbmc.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","4288a9d5":"import catboost as cb\ncbc = cb.CatBoostClassifier(random_state=42, verbose=False)\ncbc.fit(X_train, y_train)\ny_pred = cbc.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","927e9e1e":"from collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","39774172":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n\n# Modeling step Test differents algorithms \nrandom_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())","d1171f43":"cv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X, y = y, scoring = \"accuracy\", cv = kfold, n_jobs=4))","d52cec85":"cv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())","703fbd77":"cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","4e556063":"### SVC classifier\nSVMC = SVC(probability=True, random_state=42)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","dd420575":"gsSVMC.best_estimator_","351e297a":"y_pred_test = SVMC_best.predict(test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_test.astype(int)})\noutput.to_csv('SVMC_best.csv', index=False)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='SVMC_best.csv')","69291c06":"From above we can say that more females were survived than men","6abc46ad":"### Drop null 'embarked' rows. Only 2 instances of this in training and 0 in test ","bc39ff1f":"### Create Age Band feature by dividing the Age in 10 different groups","09250e1f":"### Find Null Values in Training and Test Data","cf6579be":"### Class wise count of Survived passengers","7da92eee":"### Create the Name Title Feature ","6ed54a61":"### Fill the missing values of Age and Fare with median values","99c47029":"### Create New Cabin Feature which contains only the first letter of the Cabin","1aa55a91":"### Map the Male and Female values to 0 and 1","d675b67d":"## Feature Engineering","b35266eb":"From above it's clearly evident that there was low chance of survival for people who were travelling in Class 3","c2ce84b5":"## Model Building","9f651c68":"### Sex wise survival count","bef9cc29":"### Create new Family Size Feature","23843eb9":"## Data Visualization","55eeafb0":"Most females survived from Class 1 and Class 2","e05dd985":"### Class wise count of passengers","3ca880ba":"### Survival count of people on Titanic","02ec07ac":"### Create a feature if the person was travelling alone or with family","9a5c0561":"### Sex wise count ","145a5e29":"### Create Fare Band feature by dividing the Fare in 10 different groups"}}