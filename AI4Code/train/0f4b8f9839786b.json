{"cell_type":{"093d11d3":"code","e17a7285":"code","5a35a28b":"code","b9e92cd1":"code","abaa0bd3":"code","16daa2a5":"code","fbb17ecd":"code","8a824df3":"code","5cf35cd0":"code","70194931":"code","fc50bf61":"code","4513cfac":"code","b3b49ea6":"code","6f29c143":"code","aa6dbd17":"code","9eb9a6e8":"code","6e21080a":"code","a9101fbd":"code","abdccba0":"code","0acc4dea":"code","2bdafb98":"code","500adbf5":"code","bbea4758":"code","27854e6a":"code","521eb3b7":"code","80f926ad":"code","b21d8641":"code","97964974":"code","cd26c768":"code","5bf313d3":"code","7e93cd5d":"code","a32bc708":"code","4d8983f4":"code","e0ef7d80":"code","33fef042":"code","b9b90817":"code","744486b4":"code","38a3ce0f":"code","073fa0d1":"code","07af6077":"code","c5450b78":"code","ef92c1dc":"code","c1ffaaac":"code","554abb63":"code","2bce3d57":"code","4e18971f":"code","c77469e2":"code","4619dc3e":"code","7f43bcf9":"markdown","6c8d3f75":"markdown","ea90f3d6":"markdown","127ff53b":"markdown","283bc27d":"markdown","92993043":"markdown","f0d667e8":"markdown","04debfce":"markdown","cfddc4ac":"markdown","d0088be7":"markdown","5acf3b3c":"markdown","8b167426":"markdown","d63c8420":"markdown","9f0208bc":"markdown"},"source":{"093d11d3":"from sklearn.metrics import confusion_matrix ,classification_report,precision_score, recall_score ,f1_score, roc_curve, roc_auc_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","e17a7285":"train_data = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","5a35a28b":"train_data.head()","b9e92cd1":"test_data.head()","abaa0bd3":"train_data.describe()","16daa2a5":"train_data.info()","fbb17ecd":"train_data_cat = train_data[['Gender','Driving_License','Previously_Insured','Vehicle_Age','Vehicle_Damage']]\ntrain_data_num = train_data[['Age','Region_Code','Annual_Premium','Vintage','Policy_Sales_Channel']]","8a824df3":"for i in train_data_num.columns:\n    plt.hist(train_data_num[i])\n    plt.title(i)\n    plt.show()","5cf35cd0":"sns.heatmap(train_data_num.corr())","70194931":"pd.pivot_table(train_data, index='Response', values=['Age','Region_Code','Annual_Premium','Vintage','Policy_Sales_Channel'])","fc50bf61":"for i in train_data_cat.columns:\n    sns.barplot(train_data_cat[i].value_counts().index,train_data_cat[i].value_counts()).set_title(i)\n    plt.show()","4513cfac":"for i in train_data_cat:\n    print(pd.pivot_table(train_data,index='Response',columns=i, values='Age'))\n    print(\"==\"*20)","b3b49ea6":"# dealing with outliers values\nfor i in train_data_num.columns:\n    sns.boxplot(train_data_num[i])\n    plt.title(i)\n    plt.show()","6f29c143":"def outlinefree(dataCol):     \n      \n    sorted(dataCol)                          # sort column\n    Q1,Q3 = np.percentile(dataCol,[25,75])   # getting 25% and 75% percentile\n    IQR = Q3-Q1                              # getting IQR \n    LowerRange = Q1-(1.5 * IQR)              # getting Lowrange\n    UpperRange = Q3+(1.5 * IQR)              # getting Upperrange \n    \n    colname = dataCol.tolist()               # convert column into list  \n    newlist =[]                              # empty list for store new values\n    for i in range(len(colname)):\n        \n        if colname[i] > UpperRange:          # list number > Upperrange \n            colname[i] = UpperRange          # then number = Upperrange\n            newlist.append(colname[i])       # append value to empty list\n        elif colname[i] < LowerRange:        # list number < Lowrange \n            colname[i] = LowerRange          # then number = Lowrange\n            newlist.append(colname[i])       # append value to empty list \n        else:\n            colname[i]                       # list number\n            newlist.append(colname[i])       # append value to empty list\n            \n        \n\n    return newlist","aa6dbd17":"for i in range(len(train_data_num.columns)):\n    new_list =  outlinefree(train_data.loc[:,train_data_num.columns[i]]) # retrun new list\n    train_data.loc[:,train_data_num.columns[i]] = new_list ","9eb9a6e8":"train_data_num_new = train_data[['Age','Region_Code','Annual_Premium','Vintage','Policy_Sales_Channel']]","6e21080a":"# dealing with outliers values\nfor i in train_data_num_new.columns:\n    sns.boxplot(train_data_num_new[i])\n    plt.title(i)\n    plt.show()","a9101fbd":"train_data.Vehicle_Age.value_counts()","abdccba0":"train_data.replace(['Female','Male','1-2 Year','< 1 Year','> 2 Years'],[0,1,'1-2','<1Year','>2Years'],inplace=True)\ntest_data.replace(['Female','Male','1-2 Year','< 1 Year','> 2 Years'],[0,1,'1-2','<1Year','>2Years'],inplace=True)","0acc4dea":"train_data.Vehicle_Age.value_counts()","2bdafb98":"sns.pairplot(train_data)","500adbf5":"ct = pd.crosstab(train_data['Vintage'],train_data['Response'])\nfrom scipy.stats import chi2_contingency\nstat,pvalue,dof,expected_R = chi2_contingency(ct)\nprint(\"pvalue : \",pvalue)\n\nif pvalue <= 0.1:\n    print(\"Alternate Hypothesis passed. Vintage and Response have Relationship\")\nelse:\n    print(\"Null hypothesis passed. Vintage and Response doesnot have  Relationship\")","bbea4758":"train_data_1 = train_data.loc[:,['Response','Gender','Age','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Policy_Sales_Channel']]","27854e6a":"final_train_data = pd.get_dummies(train_data_1)","521eb3b7":"final_train_data.columns","80f926ad":"features = final_train_data.iloc[:,1:].values\nlabel = final_train_data.iloc[:,0].values","b21d8641":"#pip install scikit-learn  -U","97964974":"#------------------------LogisticRegression-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=70)\n\nclassimodel= LogisticRegression(solver='liblinear')  \nclassimodel.fit(X_train, y_train)\ntrainscore =  classimodel.score(X_train,y_train)\ntestscore =  classimodel.score(X_test,y_test)  \n\ny_predlogi =  classimodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predlogi),'\\n')\nprint(confusion_matrix(y_test, y_predlogi))","cd26c768":"print(' precision score: ',precision_score(y_test, y_predlogi),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predlogi),'\\n')\nprint(classification_report(y_test, y_predlogi))","5bf313d3":"#------------------------------naive bayes---------------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=94)\n\nNBmodel = GaussianNB()  \nNBmodel.fit(X_train, y_train) \n\ntrainscore =  NBmodel.score(X_train,y_train)\ntestscore =  NBmodel.score(X_test,y_test)  \n\ny_predNB =  NBmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_predNB),'\\n')\nprint(confusion_matrix(y_test, y_predNB))","7e93cd5d":"print(' precision score: ',precision_score(y_test, y_predNB),'\\n')\nprint(' recall score: ',recall_score(y_test, y_predNB),'\\n')\nprint(classification_report(y_test, y_predNB))","a32bc708":"#------------------------Decision Tree-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=34)\n\nDTmodel=  DecisionTreeClassifier(criterion = 'entropy',splitter = 'random',max_depth=4)  \nDTmodel.fit(X_train, y_train)\ntrainscore =  DTmodel.score(X_train,y_train)\ntestscore =  DTmodel.score(X_test,y_test)  \n\ny_pred =  DTmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_pred),'\\n')\nprint(confusion_matrix(y_test, y_pred))","4d8983f4":"print(' precision score: ',precision_score(y_test, y_pred),'\\n')\nprint(' recall score: ',recall_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","e0ef7d80":"#------------------------Random Forest-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=2)\n\nRFmodel=  RandomForestClassifier(criterion='entropy',max_depth=4) \nRFmodel.fit(X_train, y_train)\ntrainscore =  RFmodel.score(X_train,y_train)\ntestscore =  RFmodel.score(X_test,y_test)  \n\ny_pred =  RFmodel.predict(X_test)\nprint(' f1 score: ',f1_score(y_test, y_pred),'\\n')\nprint(confusion_matrix(y_test, y_pred))","33fef042":"print(' precision score: ',precision_score(y_test, y_pred),'\\n')\nprint(' recall score: ',recall_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","b9b90817":"#-------------------------------------- LogisticRegression -------------------------------------\nprobabilityValues = classimodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","744486b4":"#-------------------------------------- naive bayes -------------------------------------\nprobabilityValues = NBmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","38a3ce0f":"#-------------------------------------- Decision Tree -------------------------------------\nprobabilityValues = DTmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","073fa0d1":"#-------------------------------------- Random Forest -------------------------------------\nprobabilityValues = RFmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","07af6077":"finaltestdata = test_data.loc[:,['Gender','Age','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Policy_Sales_Channel']]","c5450b78":"final_test_data = pd.get_dummies(finaltestdata)","ef92c1dc":"final_test_data.columns","c1ffaaac":" Response = NBmodel.predict(final_test_data)","554abb63":"len(Response)","2bce3d57":"test_data.shape","4e18971f":"test_data['Response'] = Response","c77469e2":"submission = test_data.loc[:,['id','Response']]","4619dc3e":"submission","7f43bcf9":"## Data Description\n* **Train Data**\n\n\n1. id: Unique ID for the customer\n2. Gender: Gender of the customer\n3. Age: Age of the customer\n4. Driving_License:  **0** : Customer does not have DL, **1** : Customer already has DL\n5. Region_Code: Unique code for the region of the customer\n6. Previously_Insured: **1** : Customer already has Vehicle Insurance,**0**: Customer doesn't have Vehicle Insurance\n7. Vehicle_Age: Age of the Vehicle\n8. Vehicle_Damage: **1** : Customer got his\/her vehicle damaged in the past.**0**: Customer didn't get his\/her vehicle damaged in he past.\n9. Annual_Premium: The amount customer needs to pay as premium in the year\n10. PolicySalesChannel: Anonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n11. Vintage: Number of Days, Customer has been associated with the company\n12. Response: 1 : Customer is interested, 0 : Customer is not interested\n\n\n\n*  **Test Data**\n\n\n1. id: Unique ID for the customer\n2. Gender: Gender of the customer\n3. Age: Age of the customer\n4. Driving_License:  **0** : Customer does not have DL, **1** : Customer already has DL\n5. Region_Code: Unique code for the region of the customer\n6. Previously_Insured: **1** : Customer already has Vehicle Insurance,**0**: Customer doesn't have Vehicle Insurance\n7. Vehicle_Age: Age of the Vehicle\n8. Vehicle_Damage: **1** : Customer got his\/her vehicle damaged in the past. **0**: Customer didn't get his\/her vehicle damaged in he past.\n9. Annual_Premium: The amount customer needs to pay as premium in the year\n10. PolicySalesChannel: Anonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n11. Vintage: Number of Days, Customer has been associated with the company\n\n\n\n* **Submission**\n\n\n1. id: Unique ID for the customer\n2. Response: **1** : Customer is interested, **0** : Customer is not interested\n","6c8d3f75":"## Used Python Libraries","ea90f3d6":"## Feature Selection with hypothesis test\n1. Chi-test: It is help to figure-out relation between features and label with **\"pvalue <= 0.1\"**","127ff53b":"## Know Dataset Nature\n1. head() : It is used to get the **first 5 rows** of the dataframe.\n2. tail() : It is used to get the **last 5 rows** of the dataframe.\n3. describe() : It is used to view some **basic statistical details** like percentile, mean, std etc.\n4. info() : It is used to print a **concise summary** of a DataFrame. including the **index dtype and column dtypes, non-null values and memory usage.**","283bc27d":"## Data Normalization\n\n1. interquartile range: It is hlep us to find **outlier values** in th columns.\n2. outlinefree() : It is a customise function that help us to figureout and work on outlier values in columns. meanly, it is used to **remove outlires** values from dataset.    \n3. for-loop: with the help of for-loop, we are checking the **outlinefree()** function worked properly or not.","92993043":"## Conclusion\nI will choose a **naive bayes algorithm** for this dataset.\n\n**naive bayes score**\n\n1. **f1_score: 0.4182915181587346**\n2. **auc: 0.8214481948289034**","f0d667e8":"## Evaluation Metric\nThe evaluation metric for this hackathon is ROC_AUC score.\n\n## Public and Private split\nThe public leaderboard is based on 40% of test data, while final rank would be decided on remaining 60% of test data (which is private leaderboard)\n\n## Guidelines for Final Submission\nPlease ensure that your final submission includes the following:\n\n1. Solution file containing the predicted response of the customer (Probability of response 1)\n2. Code file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission","04debfce":"## Feature Selection\n\n1. seaborn.pairplot(): It is help to figure-out relation between features and label.","cfddc4ac":"## Receiver Operating Characteristic Score (ROC AUC)\nhere we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.\n\n1. Logistic Regression (auc: 0.8118786282323492)\n2. naive bayes (auc: 0.8214481948289034)\n3. DecisionTreeClassifier (auc: 0.829889187907877)\n4. **RandomForestClassifier (auc: 0.845225109832536)**","d0088be7":"## Context\nOur client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n\nAn insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n\nFor example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000\/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000\/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n\nJust like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called \u2018sum assured\u2019) to the customer.\n\nBuilding a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n\nNow, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.\n\n\n![](https:\/\/autonexa.com\/wp-content\/uploads\/2018\/08\/INS-1-780x405.jpg)","5acf3b3c":"# Health Insurance Cross Sell Prediction \ud83c\udfe0 \ud83c\udfe5\nPredict Health Insurance Owners' who will be interested in Vehicle Insurance\n\n\n**Best results f1_score : 0.4182915181587346 % accuracy : 0.8214481948289034**\n\n\n## Overview \n### 1) Context\n\n### 2) Describtion \n\n### 3) Used Python Libraries\n\n### 4) Exploratory Data Analysis (EDA) \n\n### 5) Data Normalization\n\n### 6) Feature Selection with Visualization\n\n### 7) Feature Selection with Hypothesis test \n\n### 8) Model Buliding \n\n### 9) Receiver Operating Characteristic Score (ROC AUC) \n\n### 10) Conclusion \n\n### 11) Applying Algorithm ","8b167426":"## Model Buliding\nhere we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.\n\n1. Logistic Regression (f1 score: 0.0 )\n2. **naive bayes (f1 score: 0.4182915181587346)**\n3. DecisionTreeClassifier (f1 score: 0.0)\n4. RandomForestClassifier (f1 score: 0.0)","d63c8420":"## Applying Algorithm\n1. we have to separet relational columns from the test dataset that will be columns assign to a new dataset.\n2.  now we are ready for applying the decision tree algorithm on the test dataset.\n3. now we have model-predicted prices and we can assign a price column to the test dataset.","9f0208bc":"## Light Data Exploration\n### 1) For numeric data \n* Made histograms to understand distributions \n* Corrplot \n* Pivot table comparing survival rate across numeric variables \n\n\n### 2) For Categorical Data \n* Made bar charts to understand balance of classes \n* Made pivot tables to understand relationship with survival "}}