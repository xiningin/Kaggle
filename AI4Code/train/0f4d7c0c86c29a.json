{"cell_type":{"8424559f":"code","b5f271b8":"code","e912cb2d":"code","60f91160":"code","d88b6c0e":"code","ad32d652":"code","40359856":"code","3c29dd0f":"code","4f520b45":"code","d1f15fc0":"code","d003a9e9":"code","db98fb4a":"code","c5e3f38b":"code","9e81124f":"code","c928b89d":"code","ce0e2465":"code","a0708210":"code","370ecc2c":"code","07d96329":"code","40c2ca2f":"code","fb39499c":"code","354d69a4":"code","5a2a6136":"code","faf0fae8":"code","2fa018ca":"code","9063963d":"code","32e70e40":"code","7fde80b6":"code","ef127bee":"code","9af0a1d8":"code","9c6de88a":"code","cdc9b4fc":"markdown","0ce304bd":"markdown","19b92c87":"markdown","9c00a1ef":"markdown","323d7693":"markdown","bea32027":"markdown","f56a0918":"markdown","9b054196":"markdown","65ee4e5a":"markdown","ca570c65":"markdown","37fa12b1":"markdown","26467c81":"markdown","060a0a70":"markdown","22fd0031":"markdown","e99193a9":"markdown","2fdf2779":"markdown","31bd9dd5":"markdown","55155c8f":"markdown","f4b5d06a":"markdown","8bd0b24f":"markdown","2461943e":"markdown","32cd2805":"markdown","308ce459":"markdown","f2890279":"markdown","8affab65":"markdown","963947f1":"markdown","0cd80102":"markdown","ffd81823":"markdown","38322728":"markdown","6ccd8fb0":"markdown","7a7cc3a8":"markdown","8f14ea7d":"markdown","2b213f3c":"markdown","a38a73c6":"markdown","713f0183":"markdown","37aa47d1":"markdown","fd8e6ea0":"markdown"},"source":{"8424559f":"#wir brauchen unsere Standardbibliotheken\nimport numpy as np #f\u00fcr vektoren und Matrizen\nimport matplotlib.pyplot as plt #f\u00fcr plots und charts\n#zeichne Grafiken direkt ins notebook:\n%matplotlib inline ","b5f271b8":"from sklearn.datasets import make_moons\nXtrain,ytrain = make_moons(n_samples=10000, noise=.4)\nXvalid,yvalid = make_moons(n_samples=10000, noise=.4)\n\n#Schauen wir uns die Trainingsdaten an (NICHT die Testdaten!)\nplt.scatter(Xtrain[ytrain==0,0],Xtrain[ytrain==0,1],c='r');\nplt.scatter(Xtrain[ytrain==1,0],Xtrain[ytrain==1,1],c='b');\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Unser Trainingsdatensatz');","e912cb2d":"from sklearn.neighbors import KNeighborsClassifier","60f91160":"clf = KNeighborsClassifier(n_neighbors=1) #\nclf.fit(Xtrain,ytrain) #Trainieren\n\nclf.score(Xtrain,ytrain)#Juhuu!...? 100% Genauigkeit!?","d88b6c0e":"clf.score(Xvalid,yvalid) # genauigkeit von 81.1%","ad32d652":"#clf.score?","40359856":"from sklearn.metrics import accuracy_score, confusion_matrix #confusion matrix: f\u00fcr x 423 sind korrekt kalssifiziert. 93 falsch. f\u00fcr y 96 falsch, 404 richtig\nclf.fit(Xtrain,ytrain)\nyhat_valid = clf.predict(Xvalid)\nprint(confusion_matrix(yvalid,yhat_valid)) #besonders n\u00fctzlich bei mehr als 2 Klassen!\ntrain_acc = accuracy_score(yvalid,yhat_valid)\nprint(\"Trainingsgenauigkeit: {0:3.0f}%\".format(100*train_acc)) #Trainingsgenauigkeit 81%","3c29dd0f":"from sklearn.metrics import classification_report\nprint(classification_report('Bitte ausf\u00fcllen','Bitte ausf\u00fcllen'))","4f520b45":"from sklearn.datasets import make_moons\n#make_moons? \nX,y = make_moons(n_samples=1000,noise=0.8)\nfrom sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier(max_depth=4) #gr\u00f6ssere tiefe gibt eine h\u00f6here Trainingsscore","d1f15fc0":"from sklearn.model_selection import learning_curve\n#Entkommentieren Sie die folgende Zeile! Lernen Sie, was diese Funktion genau macht.\n#learning_curve?","d003a9e9":"train_sizes=np.linspace(0.001,1,20)\ntrain_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=10, train_sizes=train_sizes)","db98fb4a":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\n\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.1,\n                 color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n         label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n         label=\"Cross-validation score\")\nplt.xlabel('Anzahl Trainingszeilen')\nplt.ylabel('Genauigkeit')\nplt.title('Lernkurven\\nauf einem Moons-Datensatz')\nplt.legend();","c5e3f38b":"import sklearn.datasets\niris=sklearn.datasets.load_iris()\nX = iris['data']\ny = iris['target'] \n#iris enth\u00e4lt noch weitere keys: welche?","9e81124f":"from sklearn.model_selection import train_test_split\n#Entkommentieren Sie die folgende Zeile. \n#train_test_split?","c928b89d":"Xtrain,Xvalid,ytrain,yvalid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=42)\nXtrain.shape,Xvalid.shape,ytrain.shape,yvalid.shape","ce0e2465":"#ein 1-NN Klassifikator...\nkNN1 = KNeighborsClassifier(n_neighbors=1)\n#... und ein 50-NN Klassifikator:\nkNN50 = KNeighborsClassifier(n_neighbors=50)","a0708210":"kNN50.fit(Xtrain,ytrain)\nscore50=kNN50.score(Xtrain,ytrain)\nkNN1.fit(Xtrain,ytrain)\nscore1=kNN1.score(Xtrain,ytrain)\n#print(score50,score1) # Entkommentieren Sie diese Zeile, sobald Sie die obige \u00dcbung erledigt haben.\n#Python-\u00dcbung:Erstellen Sie einen String, der eine \"S\u00e4tzliantwort\" gibt. \"Die Genauigkeit betr\u00e4gt...\"","370ecc2c":"kNN1.fit(,) #hier soll eine X- und eine y-Matrix \u00fcbergeben werden\nkNN1.score(,)\n\nkNN50.fit(,)\nkNN50.score(,)","07d96329":"Xtrain,Xvalid,ytrain,yvalid = train_test_split(X,y,train_size=0.8,test_size=0.2)\n#Besser: \n#Xtrain,Xvalid,ytrain,yvalid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=42)","40c2ca2f":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf,Xtrain,ytrain,cv=10)\nnp.around(scores,2)","fb39499c":"#Bitte entkommentieren und verbessern Sie:\n#mittlere_Genauigkeit = Ihre_Funktion(scores)\n#standardfehler = noch_eine_Funktion(scores)\n#print(\"Trainingsgenauigkeit: {0:3.0f}% +\/-{1:2.0f}%\".format(100*mittlere_Genauigkeit,100*standardfehler))","354d69a4":"from sklearn.model_selection import validation_curve\n#Entkommentieren Sie die folgende Zeile! Lernen Sie, was diese Funktion genau macht.\n#validation_curve?","5a2a6136":"kNN1 = KNeighborsClassifier(n_neighbors=5)\n\nparam_values=np.arange(1,50,2)\ntrain_scores,test_scores = validation_curve(kNN1,Xtrain[:10000,:],ytrain[:10000],\n                 'n_neighbors',param_values,cv=10)\ntrain_scores.shape,test_scores.shape,param_values.shape","faf0fae8":"plt.plot(param_values,np.mean(train_scores,axis=1),label='train');\nplt.plot(param_values,np.mean(test_scores,axis=1),label='test');\nplt.xlabel('$k$'),\nplt.ylabel('Genauigkeit')\nplt.legend();","2fa018ca":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\ntrain_scores.shape,train_scores_mean.shape","9063963d":"\nplt.grid()\nplt.fill_between(param_values, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.1,\n                 color=\"r\")\nplt.fill_between(param_values, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\nplt.plot(param_values, train_scores_mean, 'o-', color=\"r\",\n         label=\"Training score\")\nplt.plot(param_values, test_scores_mean, 'o-', color=\"g\",\n         label=\"Cross-validation score\")\nplt.xlabel('k')\nplt.ylabel('Genauigkeit')\nplt.title('Validierungskurven\\nauf einem Moons-Datensatz')\nplt.legend(); #Validierungskurve: k Anzahl Datenp\u00fcnkte \/ Trainingskurve bei 1 Datenpunkt 100% genauigkeit \/ Cross-val Score ist der Vergleich zum\"Modell\"","32e70e40":"from sklearn.datasets import load_breast_cancer\nbc = load_breast_cancer()\nXtrain,Xvalid,ytrain,yvalid = train_test_split(bc['data'],bc['target'])\nXtrain.shape,ytrain.shape,Xvalid.shape,yvalid.shape","7fde80b6":"from sklearn.tree import DecisionTreeClassifier\ndt1 = DecisionTreeClassifier(max_depth=2)\n\nparam_values=np.arange(1,25,2)\ntrain_scores,test_scores = validation_curve(dt1,Xtrain,ytrain,\n                 'max_depth',param_values,cv=10)","ef127bee":"plt.plot(param_values,np.mean(train_scores,axis=1),label='train');\nplt.plot(param_values,np.mean(test_scores,axis=1),label='test');\nplt.xlabel('max_depth'), plt.ylabel('Genauigkeit')\nplt.title('Parameterkurve f\u00fcr max_depth eines Entscheidungsbaums \\nauf dem Wisconsin Breast Cancer Datensatz')\nplt.ylim(0.8,1.05)\nplt.legend();","9af0a1d8":"from sklearn import datasets\nX, y = datasets.make_classification(n_samples=10000, n_features=20,\n                                    n_informative=2, n_redundant=10,\n                                    random_state=42)","9c6de88a":"from sklearn.model_selection import validation_curve\nparam_range=np.arange(1,40,5)\nclf= DecisionTreeClassifier()\ntrain_scores, test_scores = validation_curve(clf\n    , X, y, param_name=\"max_depth\", param_range=param_range,\n    cv=5, scoring=\"accuracy\", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.ylim(0.8,1.05)\nplt.title(\"Validation Curve\")\nplt.xlabel(r\"max_depth\")\nplt.ylabel(\"Genauigkeit\")\nlw = 2\nplt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw) #oder plt.semilogx\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\nplt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw) #oder plt.semilogx\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\nplt.legend(loc=\"best\")\n\nplt.show()","cdc9b4fc":"### Validierungs-\/Testdaten","0ce304bd":"#### Parameterkurven f\u00fcr k-NN","19b92c87":"**Schlagen Sie auch online nach, was `train_test_split` genau macht. Was retourniert z.B. `train_test_split(X,y,train_size =0.9,stratify=y)`, oder `train_test_split(X,y,X,y)`?**","9c00a1ef":"### Metriken in Scikit-Learn  \nBisher hatten wir zur Bewertung eines Klassifikators `clf` dessen Methode `clf.score(X,y)` benutzt. Mir ist dabei immer etwas mulmig zu mute, weil ich nicht weiss, was \"score\" genau ist. Nat\u00fcrlich steht's in der Dokumentation:","323d7693":"**Aufgabe:** Suchen Sie sich eine weitere Metrik (F1-Score, FPR-Rate, Recall, ...), recherchieren Sie deren Dokumentation in Scikit-Learn und werten Sie diese auf dem Validierungsdatensatz aus.","bea32027":"### 2. Regularisierung\nKlassifikatoren besitzen oft sog. **Hyperparameter**. Das sind Parameter, deren Werte von der .fit-Methode nicht gelernt werden. Sie ver\u00e4ndern den Klassifikator und m\u00fcssen von Ihnen festgelegt werden. Beim kNN-Klassifikator ist der Parameter $k$ ein Hyperparameter.  \nRelevant f\u00fcr das Overfitting sind Hyperparameter, weil sie die M\u00e4chtigkeit eines Klassifikators einschr\u00e4nken k\u00f6nnen (der Bias wird erh\u00f6ht- siehe [Bias-Variance-Tradeoff](https:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff)). Weniger m\u00e4chtige Klassifikatoren lernen einfachere Strukturen und overfitten weniger. Daher ist Regularisierung eine der wichtigsten Massnahmen gegen Overfitting. Nicht ganz klar ist jeweils zun\u00e4chst, welcher Wertebereich eines Hyperparameters st\u00e4rkere, welche eine schw\u00e4chere Regularisierung bedeuten.  \n![](http:\/\/)Daf\u00fcr werten wir zwei unterschiedliche kNN-Klassifikatoren auf den Trainings und Validierungsdaten aus.","f56a0918":"# Evaluieren von Klassifikatoren\n\nIn diesem Notebook behandeln wir die Performance-Charakterisierung von Klassifikatoren. Sie lernen, wie Sie mit [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/) an Hand von Metriken, Kreuzvalidierung und Lernkurven die Verallgemeinerungsf\u00e4higkeit von Klassifikatoren bestimmen k\u00f6nnen.  \nWichtig ist mir, dass Sie das methodisch korrekte Vorgehen kennen, mit dem Sie der Overfitting-Gefahr erkennen und sich dagegen sch\u00fctzen k\u00f6nnen.  \nBereits bekannt sein sollten folgende Themen: \n* Das Instanziieren eines Scikit-Learn Estimators (Klassifikators)\n* Wie die Methoden .fit und .predict von Estimatoren benutzt werden, um eine Vorhersage zu erstellen.\n\nZur Wiederholung, und weil es so wichtig ist, dass es nicht gen\u00fcgend oft wiederholt werden kann: Beim Machine Learning (ML) geht es um die Verallgemeinerungsf\u00e4higkeit, d.h. die Wiedererkennung von Mustern auf noch nicht gesehenen Beispielen\/Instanzen. ML-Verfahren haben eine Tendenz, sich die Trainingsdaten einfach auswendig zu lernen. Auf den Trainingsdaten k\u00f6nnen Klassifikatoren m\u00fchelos 100%-Genauigkeit erreichen, aber das ist wertlos. Man spricht von **Overfitting**, wenn die Vorhersageleistung des Klassifikators auf dem Trainingsdatensatz gr\u00f6sser ist, als auf (noch nie gesehenen) Testdaten.\n\n<span style=\"font-size:8pt;\">Der Overfitting-Begriff wird manchmal auch strenger definiert: Abu Mostafa z.B. definiert ihn basierend auf der Komplexit\u00e4t des Klassifikators (VC-Dimension) und sagt, dass Overfitting erst stattfindet, wenn folgendes passiert: Bei zunehmender Komplexit\u00e4t des Klassifikators (z.B. Tiefe eines Entscheidungsbaums) nimmt die Validierungsgenauigkeit ab. Siehe https:\/\/work.caltech.edu\/telecourse.html.<\/span>\n","9b054196":"Die folgenden Zeilen m\u00fcssen Sie nicht im Detail verstehen. Sie erstellen einen k\u00fcnstlichen 2-dimensionalen Datensatz (Features `X` und Label `y`). Aber es ist sicher n\u00fctzlich, [sklearn.datasets](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.datasets) zu kennen.","65ee4e5a":"#### Parameterkurven f\u00fcr Entscheidungsb\u00e4ume","ca570c65":"Lassen Sie sich anschliessend einen `classification_report` ausgeben und informieren Sie sich, was dieser ist.","37fa12b1":"### 5. Parameterkurven","26467c81":"### 1. Daten in Training- und Validierungsset (bzw. Testset) splitten\nBisher haben wir mit einem k\u00fcnstlich generierten Datensatz gearbeitet und so getan, als k\u00f6nnten wir beliebig viele Daten generieren. \nIn der Praxis ist dies nat\u00fcrlich anders. Typischerweise haben Sie dort nur einen Datensatz zur Verf\u00fcgung und m\u00fcssen selber entscheiden, wieviele Daten Sie f\u00fcr das Training, f\u00fcr die Validierung und f\u00fcr das Testen beiseite legen. Hier lernen Sie, wie Sie einen Datensatz aufsplitten k\u00f6nnen.  \nWir beginnen also mit nur *einem* Datensatz `X`,`y`:","060a0a70":"**Bitte korrigieren Sie den Code, so dass ein n\u00fctzlicher Report ausgegeben wird.**","22fd0031":"## 1. Metriken\nIn diesem Abschnitt wird ein (schlechtes!) Beispiel von Machine Learning gezeigt: Es wird ein Modell gelernt, welches overfittet. Welche Kennzahlen helfen uns, solche Missst\u00e4nde zu erkennen?","e99193a9":"**Aufgabe 1:** \nWelcher ist wohl auf unserem Trainingsdatensatz besser? `kNN1` oder `kNN50`? \u00dcberlegen Sie, *bevor* Sie die folgende Zelle auswerten!","2fdf2779":"### 4. Kreuzvalidierung\nWenn Sie Ihre Trainingsgenauigkeit mit `accuracy_score` berechnen, erhalten Sie eine sog. Punktsch\u00e4tzung, d.h. einen Wert. Manchmal m\u00f6chten Sie aber eine Intervallsch\u00e4tzung, d.h. einen Mittelwert und ein Konfidenzintervall (oder sonst ein Streumass, wie z.B. die Standardabweichung). Kreuzvalidierung ist ein Weg, mehrere Werte zu generieren:","31bd9dd5":"Hier Overfitten wir einen KNN-Klassifikator:","55155c8f":"Die Trainingsgenauigkeit ist deutlich h\u00f6her als die Validierungsgenauigkeit. Dieser Unterschied zeigt uns Overfitting an. Wir sollten insbesondere nicht die Trainingsgenauigkeit verwenden um zu beurteilen, wie gut unser Klassifikator ist. Wenn wir hingegen die Validierungsgenauigkeit nehmen, dann erhalten wir ein deutlich besseres Bild der Realit\u00e4t, d.h. der Genauigkeit, welche unser Klassifikator auf neuen Daten haben wird.","f4b5d06a":"Aber wir wissen ja, dass die Trainingsgenauigkeit nicht relevant ist! Wichtiger ist: Welcher Klassifikator ist auf einem *Validierungsdatensatz* besser?  \n\n**Aufgabe 2:** Fassen Sie nochmals zusammen: F\u00fchren kleine oder grosse Werte von $k$ eher zu Overfitting? Wie sieht es f\u00fcr Werte von `max_depth` bei einem Entscheidungsbaum aus? Begr\u00fcnden Sie!  \n**Aufgabe 2:** Welcher Klassifikator ist wohl auf unserem Validierungsdatensatz besser? `kNN1` oder `kNN50`? F\u00fcllen sie die je zwei Argumente der vier Zeilen in der n\u00e4chsten Zelle sinnvoll aus.  \n","8bd0b24f":"Um Overfitting erkennen zu k\u00f6nnen, ben\u00f6tigen wir Validierungs- und Testdaten. Darauf messen wir unsere Verallgemeinerungsf\u00e4higkeit, wobei die Validierungsgenauigkeit die Verallgemeinerungsf\u00e4higkeit immer noch leicht \u00fcbersch\u00e4tzen kann (wegen Data Snooping, siehe weiter unten).  \nAuch auf den Testdaten kann dies passieren (obwohl der Klassifikator diese Daten nie gesezen hat), z.B. wenn beim produktiven Einsatz des Prototypen neue Datenstrukturen auftauchen. Wie dramatisch und tragisch das sein kann, hat der [Uber-Unfall eines selbstfahrenden Autos- Achtung, nur f\u00fcr Leute mit gute Nerven!](https:\/\/www.youtube.com\/watch?v=RASBcc4yOOo) gezeigt. Die Machine Learning Ingenieure dachten, Ihr Prototyp sei bereits so gut, dass man ihn f\u00fcr Testfahrten auf die Strasse lassen kann...","2461943e":"Die folgenden Zellen zeigen, wie ein Datensatz in Training- und Testdaten aufgespalten wird.","32cd2805":"### 3. Data Snooping","308ce459":"Bitte evaluieren Sie die obigen zwei Zellen mehrmals. Sehen Sie, wie sehr die Kurven sich \u00e4ndern (warum aber beim kNN nicht)? Diese Kurven sind Zufallsgr\u00f6ssen.  \nEs ist daher auch bei diesen Kurven besser einen Mittelwert und eine Standardabweichung anzugeben anstatt eine einzelne Realisierung. Dies wird in der folgenden Zelle gezeigt:  \nDer Code stammt von [hier](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html).","f2890279":"**Aufgaben:**: \n- Warum ist die Trainingskurve fallend?\n- Warum ist die Lernkurve auf den Validierungsdaten steigend?\n- Haben Sie gen\u00fcgend Daten, um diesen Klassifikator zu trainieren? **Vergr\u00f6ssern Sie die Trainingsdatenmenge, und verfolgen Sie, wie sich die Lernkurven ver\u00e4ndern. Wie unterscheiden sich die Kurven wohl von einem schwierigeren Datensatz?\n- Woran erkennen Sie ein Overfitting? **Ergibt sich Overfitting bei grossen Datenmengen oder bei kleinen**?\n- Wenn Sie das Rauschen des Datensatzes erh\u00f6hen: Wie ver\u00e4ndern sich dann die Lernkurven?","8affab65":"## 2. Lernkurven\nMetriken sind Kennzahlen f\u00fcr einzelne Klassifikatoren. Kurven ergeben sich, wenn wir eine Schar von Klassifikatoren betrachten. Lernkurven sind ein Plot der Metrik gegen die verf\u00fcgbare Trainingsdatenmenge. Sie geben insbesondere dar\u00fcber Auskunft, ob gen\u00fcgend Trainingsdaten vorhanden sind. Aber auch Overfitting l\u00e4sst sich damit besser verstehen.","963947f1":"# Massnahmen gegen Overfitting  \nWir stellen hier verschiedene L\u00f6sungsans\u00e4tze und Strategien zum Vermeiden von Overfitting vor. Insbesondere:\n\n1. Absplitten eines Validierungs- und Testdatensatzes zur Messung der Verallgemeinerungsf\u00e4higkeit. \n2. Regularisierung\n3. Fixierung des `random_state` zur weiteren Verhinderung von *Data Snooping*\n\n4. Kreuzvalidierung f\u00fcr kleine Datens\u00e4tze\n5. Parameterkurven zur Bestimmung geeigneter Hyperparameterwerte","0cd80102":"Die Wahl von Hyperparametern wie $k$ bei $k$NN war bisher immer noch ziemlich unsystematisch. Wir h\u00e4tten gerne eine Methode, wie wir einen Hyperparameter f\u00fcr einen beliebigen Klassifikator sinnvoll w\u00e4hlen k\u00f6nnen.  \nGerne werden daf\u00fcr *Parameterkurven* benutzt: Man trainiert den Klassifikator f\u00fcr eine Reihe von Hyperparameterwerten und plottet die Validierungsgenauigkeit gegen den Parameterwert. Wichtig ist wiederum, auf die Validierungsgenauigkeit zu schauen. Die Validierungskurve zeigt uns dann einen geeigneten Wert f\u00fcr den Hyperparameter- n\u00e4mlich jene Stelle auf der $x$-Achse, an der die Kurve ihr Maximum erreicht.\n","ffd81823":"Das Problem des Overfittings ist subtiler, als dass man es durch ein einfaches train-test-splitting schon beseitigt h\u00e4tte: Indem Sie Ihre Daten **anschauen** (oder mehrfach auf den selben Validierungsdaten Hyperparameterwerte optimieren), verinnerlichen Sie m\u00f6glicherweise bereits Informationen, welche Ihnen helfen, auf dem vorliegenden Validierungsdatensatz bessere Metrik-Werte zu erhalten. Man spricht hier von [**Data-Snooping**](https:\/\/web.ma.utexas.edu\/users\/mks\/statmistakes\/datasnooping.html).   \n\nDie L\u00f6sung: ein **Testset** beiseite zu legen, sobald Sie die Daten erhalten haben. Nicht anschauen! Im Idealfall sind die Testdaten nur f\u00fcr einen Zweck da: Die Metriken des finalen f\u00fcr den praktischen Einsatz auszuliefernden Prototypen zu messen (*ohne* ihn danach nochmals zu verbessern). Der Entscheid \u00fcber das \"\u00d6ffnen\" der Testdaten ist (sinnvollerweise) ein **Management-Entscheid**, da er nicht r\u00fcckg\u00e4ngig gemacht werden kann.  \nIn der Praxis sind manchmal nicht gen\u00fcgend Daten vorhanden, um ein Testset bilden zu k\u00f6nnen. Die Overfitting- und Data Snooping-Gefahr steigt damit betr\u00e4chtlich.","38322728":"Aber vielleicht m\u00f6chten Sie ja auch mal eine andere Metrik benutzen? Scikit-Learn stell sehr viele zur Verf\u00fcgung! Z.B. die Confusion-Matrix, die Precision, oder AUC, etc. [Informieren Sie sich!](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#sklearn-metrics-metrics)","6ccd8fb0":"Der `random_state` in der Code-Zelle oben versucht, data snooping zu verhindern. Siehe weiter unten, Unterkapitel 3.","7a7cc3a8":"Aus statistischer Sicht ist das obige Vorgehen solide, und ich empfehle diese Vorgehensweise. Trotzdem sollten Sie sich bewusst sein, dass Kreuzvalidierung den Verallgemeinerungsfehler untersch\u00e4tzen kann. Eine Passage aus einem [Nature-Artikel von Tabe-Bordbar et al](https:\/\/www.nature.com\/articles\/s41598-018-24937-4):\n\n*\"CV results may depend on similarity of test and training sets. Consider first a contrived and extreme scenario where each data sample is present in many copies in the available dataset, so that in any random partition during CV (the standard way to construct training and test sets) a test sample is likely to have a copy of itself present in the training set. A supervised learning method may predict accurately on such a sample since it has already seen that sample during training. In this case, the high predictive accuracy of the model in the CV evaluation may simply be the result of this proximity between training and test points (e.g., by a successful adoption of the nearest-neighbor approach)10, and does not necessarily imply an accurate encapsulation of the input-output relationship in the trained model. (...)  Thus, for the same reason as above, the accuracy estimated by CV on random partitions \u2013 as opposed to other partitioning approaches (Fig. 1d) \u2013 may be misleading and not generalize to more dissimilar experimental conditions.\"*\n\n\u00dcberlegen Sie auch, wie aufw\u00e4ndig Kreuzvalidierung in Bezug auf die Rechenkapazit\u00e4t ist. Denken Sie dabei insbesondere auch an Deep Learning. Das sind neuronale Netze, deren Training Stunden, wenn nicht Tage dauern kann.  ","8f14ea7d":"bei k-nearest-neighbor Trainingsdatensatz wird eine Vorhersage f\u00fcr weitere Datens\u00e4tze gesucht.\nf\u00fcr k=3: sucht er die f\u00fcr jeden Punkt die 3 n\u00e4chsten Nachbarspunkte mit Hilfe von Pythagoras.\nUnd schaut welche Klasse die Nachbaren haben und passt sich deren an. (Wenn Rot dann Rot, wenn Blau dann Blau). F\u00fcr n\u00e4here Punkte k\u00f6nnte man diese st\u00e4rker Gewichten. f\u00fcr k=4, wenn die nearest Neighbors 3 blaue und 1 roter Punkt is, ist die Prediction blau. F\u00fcr 3 rote und 1 blaue ist die Prediction rot. F\u00fcr 2 und 2 ist die Prediction Random, oder dann kann die Distanz gemessen werden und so die n\u00e4heren Punkte st\u00e4rker gewichten.","2b213f3c":"**Aufgabe:** Bestimmen Sie damit nun nicht nur einen Stichprobenwert f\u00fcr die Genauigkeit, sondern einen echten Messwert, inklusive Fehlerangabe","a38a73c6":"Diskutieren Sie wiederum die Overfitting-Thematik. Pers\u00f6nlich finde ich die Tatsache, dass der Cross-validation score f\u00fcr `max_depth`>6 abnimmt, bemerkenswert. In k\u00fcnstlichen Datens\u00e4tzen habe ich dies bisher noch nie gesehen.","713f0183":"Wenn Sie die folgende Zeile ohne fixierten `random_state` \u00f6fters ausf\u00fchren, \"sehen\" sie mit der Zeit alle Ihre Daten- was zu einem Overfitting auf dem Validierungsdatensatz f\u00fchren kann: M\u00f6glicherweise erleben Sie sp\u00e4ter eine b\u00f6se \u00dcberraschung, wenn die Testdaten ge\u00f6ffnet werden. Indem mit einem fixierten random_state immer auf denselben Daten trainiert wird, kann dies teilweise verhindert werden.","37aa47d1":"Nun zeichnen wir die Lernkurven!","fd8e6ea0":"Um Overfitting erkennen zu k\u00f6nnen, ben\u00f6tigen wir Validierungs- und Testdaten. Darauf messen wir unsere Verallgemeinerungsf\u00e4higkeit, wobei (wegen Data Snooping, siehe weiter unten) die Validierungsgenauigkeit die Verallgemeinerungsf\u00e4higkeit immer noch leicht \u00fcbersch\u00e4tzen kann.  \nAuch auf den Testdaten kann dies passieren (obwohl der Klassifikator diese Daten nie gesezen hat), z.B. wenn beim produktiven Einsatz des Prototypen neue Datenstrukturen auftauchen. Wie dramatisch und tragisch das sein kann, hat der [Uber-Unfall eines selbstfahrenden Autos- Achtung, nur f\u00fcr Leute mit gute Nerven!](https:\/\/www.youtube.com\/watch?v=RASBcc4yOOo) gezeigt. Die Machine Learning Ingenieure dachten, Ihr Prototyp sei bereits so gut, dass man ihn f\u00fcr Testfahrten auf die Strasse lassen kann..."}}