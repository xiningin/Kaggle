{"cell_type":{"dd68e9b7":"code","4914f990":"code","a7274824":"code","a81054d0":"code","d1ebbe3c":"code","1835e376":"code","32dc9f24":"code","767b86a9":"code","af42dc6a":"code","3c642d6d":"code","b02e46ca":"code","54716709":"code","47525e0c":"code","c16749bf":"code","15cfb9d5":"code","bcfd891e":"code","22f4332a":"code","0f8c7932":"code","a7b0d7ee":"code","8e83ebd4":"code","e295c193":"code","9ac18ddc":"code","18856019":"code","3b462487":"code","333b5609":"code","876e2287":"code","22722180":"code","4655f56c":"code","f6361557":"code","cd693799":"code","2f176348":"code","abdbaa85":"code","40aa0805":"code","1a749984":"code","fdf19294":"code","7d5fdb0d":"code","cc571b2a":"code","eb5afa9a":"code","7a35e6e4":"code","6f45752c":"code","768442f9":"code","9516a8a1":"code","1672e8cb":"code","dbc1395b":"code","34c3624e":"code","bb4eeafe":"code","cd2fe3b6":"code","df6c06e6":"code","57c15e10":"code","a380a8c5":"code","407365a6":"code","553ad981":"code","dde6747c":"code","259e2675":"code","a1af5e61":"code","cd9cf59c":"code","262c84bf":"code","0df2d7f5":"code","217c7895":"code","277a12c7":"code","d75a7c81":"code","e47a53fb":"code","712d1ad1":"code","2c491ce7":"code","420fa04f":"code","a3d98181":"code","26d89f53":"code","9b4c6c8c":"code","a6d2ae79":"code","5315e174":"code","8d9395b8":"code","5a9bb164":"code","b33243a9":"code","ddd34438":"code","711a7e6b":"code","a2e1a8d2":"code","8670dc0e":"code","a4735bc6":"code","664828ab":"code","ac75e0f2":"code","c1a7a2dd":"code","3ad0e8d9":"code","bf5bba84":"code","8647208d":"code","515d479a":"code","02aa67fd":"code","4c56f7f2":"code","db3ea591":"code","e27ef3e4":"code","68afab5c":"code","348feec3":"code","1a952703":"code","f083876d":"code","a6ae9e71":"code","61007493":"code","74c16c22":"code","1a99e0e1":"code","49f38023":"code","7ac8f36e":"code","e0de7b6f":"code","3d32f561":"code","cbd135e6":"code","1ff6d927":"code","9e46bb37":"code","c1ee7a08":"code","02037492":"code","9019a3fd":"code","b89faef2":"code","6a688f89":"code","a726a88f":"code","e411adf3":"code","dac7c50c":"code","8dc0254b":"code","e55aa527":"code","5baef263":"code","d156a7ba":"code","6aa1f8b1":"code","e3e6d51b":"code","84f3132d":"code","5b44c694":"code","b061f1d4":"code","12c4fa7e":"code","2ec97061":"code","6390723f":"code","a763b2ab":"code","b289bb64":"code","c6b1f63c":"code","11527584":"code","2a876884":"code","d1d0cc78":"code","a715f77c":"code","e85948c0":"code","d09decd3":"code","7e5a6360":"code","cced85ee":"code","b9c07314":"code","cb95aae3":"code","80171d3a":"code","74894cab":"code","96d1548f":"code","a8154fdf":"code","5618fa48":"code","ab200989":"code","60863c55":"code","eea8d41b":"code","c618081e":"code","cf7d7cba":"code","6b97c7c8":"code","7a72e242":"code","b354e6ea":"code","65f51392":"code","173b0eb3":"code","8b41a71d":"code","738256d5":"code","aa75ab3d":"code","368a9e02":"code","9dfa8dcf":"code","60aef697":"code","8a62dedc":"code","1b5413ca":"code","8f4d8e56":"code","851941b9":"code","3a450218":"code","aea6f483":"code","03648724":"code","20d0645a":"code","0e7ee8d2":"code","3fe910fb":"code","45ae7799":"code","9ef95e44":"code","65e02a10":"code","c505a732":"code","bd77aea5":"code","30631fa9":"code","84d9ba20":"code","1078af52":"code","da5dc1a8":"code","bed21681":"code","7268d1b7":"code","46e3e7ac":"code","83d09f0e":"code","b8b14264":"code","c66f689e":"code","8cc16b3d":"code","f6d01e0c":"code","f894e728":"code","876ad7ba":"code","47a39aaf":"code","2a4fee33":"code","42601689":"code","34c1de6a":"code","336aad75":"code","6e8fda31":"code","f168af2f":"code","0e45199f":"code","b1a496fe":"code","47c1f0d0":"code","17036701":"code","d33b7b56":"code","635f7c16":"code","dd68a764":"code","eb96afa9":"code","8d807a19":"code","99d91886":"code","d58a545e":"markdown","4f38ffb4":"markdown","65ba859f":"markdown","7a78c611":"markdown","e553d0dd":"markdown","d4a3e0c4":"markdown","8ee078e0":"markdown","6fe73010":"markdown","171c605a":"markdown","ddbab485":"markdown","39abbc68":"markdown","cdf8dfc5":"markdown","7420b27b":"markdown","811bd4c6":"markdown","76cc0957":"markdown","a87c460a":"markdown","dea4db02":"markdown","af243bd4":"markdown","573b1633":"markdown","d98d0b20":"markdown","a11b993c":"markdown","cf6e35fe":"markdown","d3b25f86":"markdown","0743ab80":"markdown","969f0df7":"markdown","fd534ee0":"markdown","76ecd5e4":"markdown","e61fa190":"markdown","daab4102":"markdown","29c61648":"markdown","10822604":"markdown","ad96178a":"markdown","77e51bf0":"markdown","090b8d68":"markdown","9d4748d7":"markdown","20b79eb5":"markdown","4fa12dd8":"markdown","39fe3c16":"markdown","bfd94cc9":"markdown","f50ef557":"markdown","c4067eab":"markdown","eb60456b":"markdown","5f2c606c":"markdown","79189147":"markdown","0a324cec":"markdown","3b21a2f1":"markdown","770c3e35":"markdown","2470d5f2":"markdown","6a969ce6":"markdown","775090d1":"markdown","c5584ce3":"markdown","8cfe316f":"markdown","aa78754a":"markdown","f480b332":"markdown","e3252e2f":"markdown","293627fa":"markdown","f510cfa0":"markdown","f79cd1a8":"markdown","7f7d6995":"markdown","20836f01":"markdown","f4f775e4":"markdown","4bc998f8":"markdown","e63a5c4c":"markdown","12a14fbd":"markdown","bfc69228":"markdown","fd8bbb99":"markdown","df37db5d":"markdown","3d2ae927":"markdown","96154748":"markdown","bfa73953":"markdown","e5f342a7":"markdown","bae12716":"markdown","c4f24b01":"markdown","40fcfa25":"markdown","e2bf193d":"markdown","8a925e5e":"markdown","8ea69c62":"markdown","e82388a4":"markdown","7b3c7606":"markdown","cd568955":"markdown","dc2405c8":"markdown","f397e351":"markdown","bf0f6316":"markdown","8668c109":"markdown","3b80a7be":"markdown","0894c737":"markdown","b3704b6f":"markdown","21d55cf2":"markdown","b9b1f237":"markdown","39743a29":"markdown","e078b16a":"markdown","3cd6a2a1":"markdown","c5e99bf4":"markdown","5a7940d7":"markdown","847bbc2f":"markdown","b2254f97":"markdown","46279e2a":"markdown","9b924e2e":"markdown","905fb0c6":"markdown","ff64ed9e":"markdown","a01da0f2":"markdown","865c8508":"markdown","bdf29d63":"markdown","ff2b1516":"markdown","8f479587":"markdown","7b6bf96b":"markdown","604fcfab":"markdown","6fc7ebc2":"markdown","e87e4a17":"markdown","4940c715":"markdown","f2b8d11c":"markdown","0efbad77":"markdown","bea9717b":"markdown","c8bcb321":"markdown","16f5d000":"markdown","544c9705":"markdown","99e6291f":"markdown","8feb37ce":"markdown","7c18d1bd":"markdown","1bcd0d5e":"markdown","655d5d28":"markdown","7280ab6f":"markdown","81c754ae":"markdown","0679ca9e":"markdown","c145411b":"markdown","02e7e10e":"markdown","9a647d6c":"markdown","9c715d3f":"markdown","b115fc01":"markdown","97d66255":"markdown","6d98bfa7":"markdown","9c7189d9":"markdown","a8834b6e":"markdown","5ce5871e":"markdown","31b7716f":"markdown","67642595":"markdown","974743f0":"markdown","1432d99e":"markdown","e7396a1a":"markdown","a3e0d46c":"markdown","457db6b7":"markdown","93c24c03":"markdown","e639c515":"markdown","a10b42f5":"markdown","05e37e2b":"markdown","c7270a61":"markdown","8d8ee27b":"markdown","372d8d0e":"markdown","7c20c3b5":"markdown","fb5d266c":"markdown","7d74b6ff":"markdown","dc651aac":"markdown","e8b62271":"markdown","93c96681":"markdown","6bcc7ea8":"markdown","4660014e":"markdown","47c591e9":"markdown","3c3b3e8d":"markdown","b144ea5a":"markdown","e6851676":"markdown","8e1bd85b":"markdown","b5a5785c":"markdown","80a1f6b5":"markdown","1e882db8":"markdown","87ab1c8f":"markdown","3dd919ec":"markdown","cc9bf285":"markdown","c3953294":"markdown","7b12e5db":"markdown","f33a6b87":"markdown","7b1c69ef":"markdown","af1c80b7":"markdown","8dd8bd22":"markdown","1d98802f":"markdown","5949643e":"markdown","6525529a":"markdown","69b7b8d1":"markdown","67432ad6":"markdown","3b39c523":"markdown","173a2cc9":"markdown","211ac5a5":"markdown","ff6f183b":"markdown","cd7a5f45":"markdown","8a65e1f0":"markdown","4ed85c55":"markdown","0abb677b":"markdown","2b361598":"markdown","99ca863f":"markdown","79755b1a":"markdown","139afa8c":"markdown","76bff663":"markdown","ee2d3e23":"markdown","6102412f":"markdown","3a9a9cb1":"markdown","5fe8f7cb":"markdown","3fdbcf83":"markdown","43927f40":"markdown","b04cd938":"markdown","61608820":"markdown","4738e2b9":"markdown","8a6512ca":"markdown","17c974fa":"markdown","f6c35b1d":"markdown","e3c7e81f":"markdown","e625e6ee":"markdown","22de4e33":"markdown","a9deb598":"markdown","a6e6c1a5":"markdown","f48a17b5":"markdown","2b417ff4":"markdown","84c5bf2f":"markdown","e197ac51":"markdown","1a5eb300":"markdown","bb1734aa":"markdown","5994dae5":"markdown","d42fa712":"markdown","67f688db":"markdown","42db4a65":"markdown","a43318a6":"markdown","a4b851f6":"markdown","a8ef2589":"markdown","d6cb0deb":"markdown","793d558f":"markdown","dbdf5630":"markdown","783c29cc":"markdown","3d9b721a":"markdown","63b786f0":"markdown","40154af2":"markdown","8976c084":"markdown","494fe4c3":"markdown","2ff9469b":"markdown","5ec6b663":"markdown","e3549673":"markdown","f4dc40d8":"markdown","0134df37":"markdown","f9176d59":"markdown","2514cf2a":"markdown","3cd563f9":"markdown","8c2228b4":"markdown","ecb97900":"markdown","3b992838":"markdown","9604a9ac":"markdown","2e73233a":"markdown","5224cf7c":"markdown","524c587f":"markdown","3da53ae3":"markdown"},"source":{"dd68e9b7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nimport os\nwarnings.simplefilter(action='ignore') # suppres all warnings\n%matplotlib inline\n\npd.set_option('display.float_format', lambda x: '{:.6f}'.format(x))\nsns.set_style(\"darkgrid\")","4914f990":"train_da = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_da = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndisplay(train_da.head())","a7274824":"# look at the data structure\ndisplay(train_da.info())","a81054d0":"def get_columns(data):\n    # extract cat columns\n    s = data.dtypes == \"object\"\n    object_cols = data.columns[s].tolist()\n\n    # extract num columns\n    n = (data.dtypes == \"int64\") | (data.dtypes == \"float64\")\n    numerical_cols = data.columns[n].tolist()\n    # drop target variable from numerical cols\n\n    # find out discrete cols from numerical cols\n    discrete_cols = [num_col for num_col in numerical_cols if data[num_col].nunique()<=20]\n    \n    # get continuous cols\n    mark = pd.Series(numerical_cols).isin(discrete_cols)\n    continuous_cols = pd.Series(numerical_cols)[~mark].tolist()\n    \n    return object_cols,numerical_cols,discrete_cols,continuous_cols\n\n# drop ID \ntrain_da.drop(\"Id\",axis=1,inplace=True)\ntest_da.drop(\"Id\",axis=1,inplace=True)\nobject_cols,numerical_cols,discrete_cols,continuous_cols = get_columns(train_da)","d1ebbe3c":"# check the summary stats of numerical cols\ntrain_da[numerical_cols].describe()","1835e376":"from scipy.stats import probplot\n\ndef check_normality(data,col,label_txt,conclusion=None,color=\"deepskyblue\"):\n    fig = plt.figure(facecolor='whitesmoke',figsize=(20,5),dpi=100)\n\n    ax_left = fig.add_axes([0,0,.2,1], facecolor='whitesmoke')\n    ax_left.axis(\"off\")\n    ax_left.text(.4,.9,label_txt,color=\"crimson\",weight=\"bold\",size=25)\n    ax_left.text(.1,.8,\"Skewness {:.2f}\".format(stats.skew(data[col],bias=False),size=30))\n    ax_left.text(.1,.7,\"Kurtosis {:.2f}\".format(stats.kurtosis(data[col],bias=False),size=30))\n    #ax_left.text(.1,.4,\"Conclusion:\\n{}\".format(conclusion),size=15)\n    \n    # histogram\n    ax1 = fig.add_axes([0.25,0,0.3,0.8],facecolor=\"whitesmoke\")\n    sns.distplot(data[col],color=color,kde=True,ax=ax1)\n    ax1.set_xlabel(label_txt,fontsize=14)\n    ax1.set_ylabel(\"frequency\",fontsize=14)\n    ax1.set_title(\"Histogram\",color=\"crimson\",fontsize=16,weight=\"bold\")\n    ax1.tick_params(labelsize=10)\n    for spine in [\"top\",\"right\"]:\n        ax1.spines[spine].set_visible(False)\n    ax1.grid(False)\n\n    # qq plot\n    ax2 = fig.add_axes([0.58,0,0.3,0.8],facecolor=\"whitesmoke\")\n    probplot(data[col],plot=ax2)\n    ax2.set_xlabel(\"Theoretical Quantiles\",fontsize=14)\n    ax2.tick_params(labelsize=10)\n    ax2.set_title(\"QQ-Plot\",color=\"crimson\",fontsize=16,weight=\"bold\")\n    for spine in [\"top\",\"right\"]:\n        ax2.spines[spine].set_visible(False)\n    ax2.grid(False)\n\n    plt.show()\n    \nconclusion_txt = \"Sale Price has slight Positive \\nSkewness and greater kurtosis \\nthan that of normial distribution\"\ncheck_normality(train_da,\"SalePrice\",\"Sale Price\",conclusion_txt)","32dc9f24":"# perform log trasnformation on the target variable\ntrain_da[\"lg_SalePrice\"] = np.log(train_da[\"SalePrice\"])","767b86a9":"check_normality(train_da,\"lg_SalePrice\",\"Sale Price\",\"Quasi-Normal Distribution\")","af42dc6a":"# write function to visualize the distribution of variables\ndef check_dist(row,col,data,cols,color,isbox=True,figsize=25):\n    fig,axes = plt.subplots(row,col,figsize=(25,25),dpi=100,facecolor=\"whitesmoke\")\n    i = 0\n    if isbox:\n        for r in range(row):\n            for c in range(col):\n                try:\n                    sns.boxplot(y=data[cols[i]],ax=axes[r,c],color=color)\n                    text_title = cols[i]\n                    i += 1\n                    axes[r,c].set_facecolor(\"whitesmoke\")\n                    axes[r,c].set_title(text_title,fontsize=10,weight=\"bold\")\n                    axes[r,c].set_xlabel(\"\")\n                except IndexError:\n                    pass\n    else:\n         for r in range(row):\n            for c in range(col):\n                try:\n                    sns.distplot(data[cols[i]],ax=axes[r,c],color=color)\n                    text_title = cols[i]\n                    i += 1\n                    axes[r,c].set_facecolor(\"whitesmoke\")\n                    axes[r,c].set_title(text_title,y=1,pad=-10,fontsize=14,fontweight=\"bold\",color=\"crimson\")\n                    axes[r,c].set_xlabel(\"\")\n                except IndexError:\n                    pass       \n        \n    # delete all surplus subplots that do not include any graphs\n    if row*col > len(cols):\n        total_plot_num = row*col\n        plot_num = total_plot_num-len(cols)\n        for i in reversed(list(range(plot_num))):\n            axes[row-1,col-i-1].set_visible(False)\n            \n    plt.show()\n    \ncheck_dist(6,4,train_da,pd.Series(continuous_cols).drop(labels=0).tolist(),\"deepskyblue\",False)","3c642d6d":"nunique_num = [train_da[obj_col].nunique() for obj_col in object_cols]\nprint(\"The Number of Categories for each Categorical Variable\")\nprint(\"___________________________________________\")\nres = pd.DataFrame({\"Feature name\":object_cols,\"Unique Categories\":nunique_num})\ncm = sns.light_palette(\"deepskyblue\", as_cmap=True)\nres.sort_values(by=\"Unique Categories\",ascending=False).style.background_gradient(cm)","b02e46ca":"def plot_cat_dist(df,cat_cols,row,col,color,IsCat,unique_res=None):\n    \"\"\"Plot the distribution of categorical variables or discrete variables\"\"\"\n    \n    # create figure and multiple axes based on the number of variables\n    fig,axes = plt.subplots(row,col,figsize=(25,25),dpi=100,facecolor=\"whitesmoke\")\n    \n    # judge whether we make bar chars for categorical variables or discrete variables\n    if IsCat:\n        # filter out car with less than 15 categories\n        mark = (res[\"Unique Categories\"]<=15) & (res[\"Unique Categories\"]!=1)\n        # extract columns\n        cat_cols = res[mark][\"Feature name\"].tolist()\n\n        i = 0\n        for r in range(row):\n            for c in range(col):\n                try:\n                    temp = df[cat_cols[i]].value_counts().reset_index().rename(columns={\"index\":cat_cols[i],cat_cols[i]:\"Frequency\"})\n                    sns.barplot(data=temp,x=cat_cols[i],y=\"Frequency\",color=color,ax=axes[r,c])\n                    text_title = cat_cols[i]\n                    i += 1\n                    axes[r,c].set_facecolor(\"whitesmoke\")\n                    axes[r,c].set_title(text_title,fontsize=10,weight=\"bold\",pad=-12,y=1)\n                    axes[r,c].set_xlabel(\"\")\n                    for spine in [\"right\",\"top\"]:\n                        axes[r,c].spines[spine].set_visible(False)\n                    axes[r,c].tick_params(labelsize=10,rotation=45)\n                except IndexError:\n                    pass\n    else:\n        i = 0\n        for r in range(row):\n            for c in range(col):\n                try:\n                    temp = df[cat_cols[i]].value_counts().reset_index().rename(columns={\"index\":cat_cols[i],cat_cols[i]:\"Frequency\"})\n                    sns.barplot(data=temp,x=cat_cols[i],y=\"Frequency\",color=color,ax=axes[r,c])\n                    text_title = cat_cols[i]\n                    i += 1\n                    axes[r,c].set_facecolor(\"whitesmoke\")\n                    axes[r,c].set_title(text_title,fontsize=10,weight=\"bold\",pad=-12,y=1)\n                    axes[r,c].set_xlabel(\"\")\n                    for spine in [\"right\",\"top\"]:\n                        axes[r,c].spines[spine].set_visible(False)\n                    axes[r,c].tick_params(labelsize=10,rotation=45)\n                except IndexError:\n                    pass\n                \n    # delete all surplus subplots that do not include any graphs\n    if row*col > len(cat_cols):\n        total_plot_num = row*col\n        plot_num = total_plot_num-len(cat_cols)\n        for i in reversed(list(range(plot_num))):\n            axes[row-1,col-i-1].set_visible(False)\n            \n\n    plt.show()","54716709":"plot_cat_dist(train_da,object_cols,7,6,\"deepskyblue\",True,res)","47525e0c":"plot_cat_dist(train_da,discrete_cols,4,4,\"deepskyblue\",False)","c16749bf":"def corr_plot(data,cols):\n    \"\"\"plot the heatmap of correlation coefficients of variables\"\"\"\n    # take a quick look at the relationship between variables\n    corr_matrix = data[cols].corr()\n    # Set up the matplotlib figure\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    fig, ax = plt.subplots(figsize=(25, 20),dpi=100,facecolor=\"whitesmoke\")\n    sns.heatmap(corr_matrix,cmap=\"Blues\",annot=True,mask=mask,ax=ax,center=0,square=True,linewidths=.5,cbar_kws={\"shrink\": .5})\n    ax.set_facecolor(\"whitesmoke\")\n    ax.set_title(\"Heatmap of Correlation Coefficient\",weight=\"bold\",fontsize=14)\n    plt.show()\n    \n    \n\ncorr_plot(train_da,numerical_cols)","15cfb9d5":"# Visualize the distribution of target variable\nfig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\nsns.regplot(data=train_da,x=\"YearBuilt\",y=\"SalePrice\",color='blue',ax=ax)\nax.set_xlabel(\"Year Built\",fontsize=14)\nax.set_ylabel(\"Sale Price\",fontsize=14)\nax.set_title(\"Sale Price Against Year Built\",fontsize=16,weight=\"bold\")\nax.set_facecolor('whitesmoke')\nax.set_xticks(np.arange(train_da[\"YearBuilt\"].min(),train_da[\"YearBuilt\"].max()+1,2))\nfor spine in [\"top\",\"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.xticks(rotation=45)\nplt.show()","bcfd891e":"# filter out built around 1892 and with higher than prie of 400000\nattributes = ['OverallQual',\"SalePrice\",\"GrLivArea\",\"GarageArea\",\"TotalBsmtSF\",\"HalfBath\"]\ntrain_da.query(\"YearBuilt==1892\")[attributes]","22f4332a":"def plot_scatter(data,x,xlab,):\n    \"\"\"inspect the relationship between specified variable\n        and sales price\"\"\"\n    fig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\n    sns.regplot(data=data,x=x,y=\"SalePrice\",color='blue',ax=ax)\n    ax.set_xlabel(xlab,fontsize=14)\n    ax.set_ylabel(\"Sale Price\",fontsize=14)\n    ax.set_facecolor('whitesmoke')\n    for spine in [\"top\",\"right\"]:\n        ax.spines[spine].set_visible(False)","0f8c7932":"from sklearn.preprocessing import PolynomialFeatures\n\ndef RSS(f,x,y):\n    \"\"\"compute the residuals sum of squares\"\"\"\n    return np.sum((f(x)-y)**2)\n\n\ndef poly_plot(data,specified_var,target_var,plot_x,degree,x_label):\n    \"\"\"perform poly transformation on the specified variable,\n        compare RSS with RSS for the transformed variables,\n        and plot the specified variable against target variable\"\"\"\n    \n    x = data[specified_var]\n    y = data[target_var]\n    # get the rss for non-poly transformation\n    poly_coefs = np.polyfit(x,y,1)\n    poly_func = np.poly1d(poly_coefs)\n    rss1 = RSS(poly_func,x,y)\n    print(f\"RSS before Poly Transformation:{rss1}\")\n    # get the results of degree-3 polynomial transformation\n    poly_coefs = np.polyfit(x,y,degree) # get the coefficients of degree-3 poly function\n    poly_func = np.poly1d(poly_coefs)\n    rss2 = RSS(poly_func,x,y)\n    print(f\"RSS after Poly Transformation:{rss2}\")\n\n    # compute the result of polynomial function\n    poly_func = np.poly1d(poly_coefs)\n    fig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\n    sns.scatterplot(x,y,alpha=0.5,color=\"blueviolet\",ax=ax)\n    sns.lineplot(plot_x,poly_func(plot_x),color=\"blue\",linewidth=3)\n    ax.set_facecolor(\"whitesmoke\")\n    ax.set_xlabel(x_label,fontsize=14)\n    ax.set_ylabel(\"Sale Price\",fontsize=14)\n    for i in [\"right\",\"top\"]:\n        ax.spines[i].set_visible(False)\n    plt.show()","a7b0d7ee":"poly_plot(train_da,\"YearBuilt\",\"SalePrice\",np.arange(1872,2011,1),\n         3,\"Year Built\")","8e83ebd4":"plot_scatter(train_da,\"GarageYrBlt\",\"Garage Construction Year\")","e295c193":"plot_scatter(train_da,\"GarageYrBlt\",\"Renovation Year\")","9ac18ddc":"# compare median housing price across year\nsns.set_style('darkgrid')\nmean_house_price = train_da.SalePrice.mean()\nfig,ax = plt.subplots(1,2,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\nsns.barplot(data=train_da,x='YrSold',y=\"SalePrice\",color='lightskyblue',estimator=np.median,ax=ax[0])\nax[0].hlines(y=mean_house_price,xmin=-0.5,xmax=4.5,colors=\"black\",linewidth=1.5,linestyles=\"dashed\")\nax[0].set_xlabel(\"Year Sold\")\nax[0].set_ylabel(\"Median Sale Price\")\nax[0].set_facecolor(\"whitesmoke\")\nfor spine in [\"top\",\"right\"]:\n        ax[0].spines[spine].set_visible(False)\n\n\n# draw bar plot for median housing prices across months\nsns.barplot(data=train_da,x='MoSold',y=\"SalePrice\",color='lightskyblue',estimator=np.median,ax=ax[1])\nax[1].hlines(y=mean_house_price,xmin=-1,xmax=12,colors=\"black\",linewidth=1.5,linestyles=\"dashed\")\nax[1].set_xlabel(\"Month Sold\")\nax[1].set_ylabel(\"Median Sale Price\")\nax[1].set_facecolor(\"whitesmoke\")\nfor spine in [\"top\",\"right\"]:\n        ax[1].spines[spine].set_visible(False)\n\nplt.show()","18856019":"# Visualize the distribution of target variable\nfig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\nsns.boxplot(data=train_da,x=\"OverallQual\",y=\"SalePrice\",color='deepskyblue',ax=ax,saturation=0.5)\nax.set_xlabel(\"Overall Quality\",fontsize=14)\nax.set_ylabel(\"Sale Price\",fontsize=14)\nax.set_facecolor('whitesmoke')\nfor spine in [\"top\",\"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.show()","3b462487":"# filter out the hosue with quality of 4 and price higher than 200K\ntrain_da.query(\"OverallQual==4 and SalePrice>200000\")[attributes]","333b5609":"def plot_scatter(data,x,xlab):\n    # Visualize the distribution of target variable\n    fig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\n    sns.regplot(data=data,x=x,y=\"SalePrice\",color='blue',ax=ax)\n    ax.set_xlabel(xlab,fontsize=14)\n    ax.set_ylabel(\"Sale Price\",fontsize=14)\n    ax.set_facecolor('whitesmoke')\n    for spine in [\"top\",\"right\"]:\n        ax.spines[spine].set_visible(False)\n    plt.show()\n    \nplot_scatter(train_da,\"TotalBsmtSF\",\"Total Basement Square Footage\")","876e2287":"train_da.query(\"TotalBsmtSF>6000\")[attributes]","22722180":"plot_scatter(train_da,\"1stFlrSF\",\"First Floor Square Footage\")","4655f56c":"fig,ax = plt.subplots(1,1,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\noutliers = train_da.query(\"GrLivArea>4000\")[attributes].loc[[523,1298],:]\nsns.regplot(data=train_da,x=\"GrLivArea\",y=\"SalePrice\",color='blue',ax=ax)\nsns.scatterplot(data=outliers,x=\"GrLivArea\",y=\"SalePrice\",color='crimson',ax=ax,s=300,alpha=.6)\nax.set_xlabel(\"Above Grade Living Area\",fontsize=14)\nax.set_ylabel(\"Sale Price\",fontsize=14)\nax.set_facecolor('whitesmoke')\nax.set_xlim([0,6050])\nfor spine in [\"top\",\"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.show()","f6361557":"train_da.query(\"GrLivArea>4000 and SalePrice<=200000\")[attributes]","cd693799":"plot_scatter(train_da,\"GarageArea\",\"Size of garage in square feet\")","2f176348":"train_da.query(\"GarageArea>1200\")[attributes]","abdbaa85":" plot_scatter(train_da,\"2ndFlrSF\",\"Second floor square feet\")","40aa0805":"poly_plot(train_da,\"2ndFlrSF\",\"SalePrice\",np.arange(0,2100,1),\n         2,\"Second Floor Square Footage\")","1a749984":"plot_scatter(train_da,'PoolArea',\"Pool Area\")","fdf19294":"# create a variables HavePool\ntrain_da[\"HavePool\"] = (train_da.PoolArea>0).astype(\"int\")\ntest_da[\"HavePool\"] = (test_da.PoolArea>0).astype(\"int\")\n\n# visualize\nsns.set_style('darkgrid')\nfig,ax = plt.subplots(1,1,figsize=(5,3),dpi=100,facecolor=\"whitesmoke\")\nsns.barplot(data=train_da,x='HavePool',y=\"SalePrice\",color='lightskyblue',estimator=np.median)\nax.hlines(y=mean_house_price,xmin=-0.5,xmax=1.5,colors=\"black\",linewidth=1.5,linestyles=\"dashed\")\nax.set_xlabel(\"Have Pool\")\nax.set_ylabel(\"Median Sale Price\")\nax.set_facecolor(\"whitesmoke\")\n#ax.set_xticklabels([\"Not Completely New\",\"Completely New\"])\nfor spine in [\"top\",\"right\"]:\n        ax.spines[spine].set_visible(False)\nplt.show()","7d5fdb0d":"# have a look at the columns with Porch\nporches = train_da.columns[train_da.columns.str.contains(\"Porch\")].tolist()+[\"WoodDeckSF\"]\ntrain_da[porches].head(5)","cc571b2a":"# create the variable Total Porch Area\ntrain_da['TotalPorchSF'] = train_da.OpenPorchSF + train_da.EnclosedPorch + \\\ntrain_da[\"3SsnPorch\"] + train_da.ScreenPorch\ntest_da['TotalPorchSF'] = test_da.OpenPorchSF + test_da.EnclosedPorch + \\\ntest_da[\"3SsnPorch\"] + test_da.ScreenPorch\n\n# examine its association with sale price\ncoef = train_da[\"TotalPorchSF\"].corr(train_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(train_da,\"TotalPorchSF\",\"Total Porch Area\")","eb5afa9a":"# examine its association with sale price\ncoef = train_da[\"WoodDeckSF\"].corr(train_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(train_da,\"WoodDeckSF\",\"Wood Deck\")","7a35e6e4":"coef = train_da[\"LotArea\"].corr(train_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(train_da,\"LotArea\",\"Lot Area\")","6f45752c":"train_da['LogLotArea'] = np.log10(train_da[\"LotArea\"])\ncoef = train_da[\"LogLotArea\"].corr(train_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(train_da,\"LogLotArea\",\"Log10(Lot Area)\")","768442f9":"plot_scatter(train_da,\"MiscVal\",\"miscellaneous feature\")","9516a8a1":"train_da.drop(\"MiscVal\",axis=1,inplace=True)\ntest_da.drop(\"MiscVal\",axis=1,inplace=True)","1672e8cb":"# drop the newly added variable because I will add it in feature engineering section\ntrain_da.drop(\"LogLotArea\",axis=1,inplace=True)","dbc1395b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest,chi2\n\nX = train_da[object_cols]\ny = train_da.SalePrice\nimputer = SimpleImputer(strategy=\"most_frequent\")\nX_imputed = imputer.fit_transform(X)\nencoder = LabelEncoder()\nX_encoded = pd.DataFrame(X_imputed,columns=X.columns).apply(lambda col:encoder.fit_transform(col))\nselector = SelectKBest(chi2,20)\nX_selected = selector.fit_transform(X_encoded,y)\nX.columns[selector.get_support()]","34c3624e":"# write a function to check the relation between cat variable and target;\n# then inspect the distribution of the cat variable\ndef plot_cat_relation_cnt(data,var,xlab,box=True,**kwargs):\n    \"\"\"plot the median housing price within each cat of categorical variales\n        and compare it with median price across all houses\"\"\"\n    \n    # create one figure and two axes\n    fig,ax = plt.subplots(1,2,figsize=(15,5),dpi=100,facecolor=\"whitesmoke\")\n    # calculate the maximum length for the horizontal line\n    max_len = data[var].nunique()-0.5\n    \n    if box:\n        # visualize\n        sns.set_style('darkgrid')\n    \n        plots_x = sns.boxplot(data=data,x=var,y=\"SalePrice\",color='deepskyblue',ax=ax[0],**kwargs)\n        ax[0].hlines(y=mean_house_price,xmin=-0.5,xmax=max_len,colors=\"crimson\",linewidth=1.5,linestyles=\"dashed\")\n        ax[0].set_xlabel(xlab)\n        ax[0].set_ylabel(\"Median Sale Price\")\n        ax[0].set_facecolor(\"whitesmoke\")\n        #ax.set_xticklabels([\"Not Completely New\",\"Completely New\"])\n        for spine in [\"top\",\"right\"]:\n                ax[0].spines[spine].set_visible(False)\n        if data[var].nunique()>=10:\n            ax[0].set_xticklabels(plots_x.get_xticklabels(),rotation=45)\n    else:\n        sns.set_style('darkgrid')\n        plots_x = sns.barplot(data=data,x=var,y=\"SalePrice\",color='deepskyblue',estimator=np.median,ax=ax[0],**kwargs)\n        ax[0].hlines(y=mean_house_price,xmin=-0.5,xmax=max_len,colors=\"crimson\",linewidth=1.5,linestyles=\"dashed\")\n        ax[0].set_xlabel(xlab)\n        ax[0].set_ylabel(\"Median Sale Price\")\n        ax[0].set_facecolor(\"whitesmoke\")\n        #ax.set_xticklabels([\"Not Completely New\",\"Completely New\"])\n        for spine in [\"top\",\"right\"]:\n                ax[0].spines[spine].set_visible(False)\n        if data[var].nunique()>=10:\n            ax[0].set_xticklabels(plots_x.get_xticklabels(),rotation=45)\n    \n    # examine the distribution of the variable\n    temp = data[var].value_counts().reset_index().rename(columns={'index':var,var:\"Frequency\"})\n    plots = sns.barplot(data=temp,x=var,y=\"Frequency\",color='lightskyblue')\n    # Iterrating over the bars one-by-one\n    for bar in plots.patches:\n\n      # Using Matplotlib's annotate function and\n      # passing the coordinates where the annotation shall be done\n      # x-coordinate: bar.get_x() + bar.get_width() \/ 2\n      # y-coordinate: bar.get_height()\n      # free space to be left to make graph pleasing: (0, 8)\n      # ha and va stand for the horizontal and vertical alignment\n        plots.annotate(format(bar.get_height(), '.0f'),\n                       (bar.get_x() + bar.get_width() \/ 2,\n                        bar.get_height()), ha='center', va='center',\n                       size=12, xytext=(0, 8),\n                       textcoords='offset points')\n\n    ax[1].set_xlabel(xlab)\n    ax[1].set_ylabel(\"Frequency\")\n    ax[1].set_facecolor(\"whitesmoke\")\n    ax[1].set_facecolor(\"whitesmoke\")\n    for spine in [\"top\",\"right\"]:\n        ax[1].spines[spine].set_visible(False)\n    \n    # rotate the x ticks labels when there are more than 10 categories\n    if data[var].nunique()>=10:\n            ax[1].set_xticklabels(plots.get_xticklabels(),rotation=45)\n    plt.show()","bb4eeafe":"plot_cat_relation_cnt(train_da,\"LotShape\",\"Lot Shape\",True)","cd2fe3b6":"plot_cat_relation_cnt(train_da,\"LotConfig\",\"Lot Configuration\",saturation=0.5)","df6c06e6":"# convert the missing values into NA\ntrain_da[\"Alley\"] = np.where(train_da[\"Alley\"].isnull(),\"None\",train_da[\"Alley\"])\ntest_da[\"Alley\"] = np.where(test_da[\"Alley\"].isnull(),\"None\",test_da[\"Alley\"])\n# then examine the distribution of the alley vairable\nplot_cat_relation_cnt(train_da,\"Alley\",\"Alley\",False,errwidth=2,errcolor=\"darkblue\",capsize=.2)","57c15e10":"# first examine the difference of housing price in different neighborhoods\nplot_cat_relation_cnt(train_da,\"Neighborhood\",\"Neighborhood\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","a380a8c5":"# first examine the difference of housing price in different neighborhoods\nplot_cat_relation_cnt(train_da,\"Street\",\"Street\",False,errwidth=2,errcolor=\"darkblue\",capsize=.2)","407365a6":"plot_cat_relation_cnt(train_da,\"PavedDrive\",\"Paved Driveway\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","553ad981":"# first examine the difference of housing price in different neighborhoods\nplot_cat_relation_cnt(train_da,\"MSZoning\",\"General Zoning Classification\",True)","dde6747c":"plot_cat_relation_cnt(train_da,'MSSubClass',\"Type of Dwelling\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","259e2675":"plot_cat_relation_cnt(train_da,\"BldgType\",\"Type of Dwelling\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","a1af5e61":"plot_cat_relation_cnt(train_da,\"HouseStyle\",\"House Style\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","cd9cf59c":"plot_cat_relation_cnt(train_da,'RoofStyle','Roof Style',False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","262c84bf":"plot_cat_relation_cnt(train_da,\"ExterQual\",\"External Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","0df2d7f5":"plot_cat_relation_cnt(train_da,\"ExterCond\",\"External Condition\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","217c7895":"# look at the data\n(train_da[\"Exterior1st\"] == train_da['Exterior2nd']).head()","277a12c7":"(train_da[\"Exterior1st\"] == train_da['Exterior2nd']).value_counts()","d75a7c81":"plot_cat_relation_cnt(train_da,'Exterior1st','Exterior1st',False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","e47a53fb":"plot_cat_relation_cnt(train_da,'Exterior2nd','Exterior2nd',False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","712d1ad1":"plot_cat_relation_cnt(train_da,'LandSlope','LandSlope',False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","2c491ce7":"plot_cat_relation_cnt(train_da,'LandContour',\"Flatness of the property\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","420fa04f":"plot_cat_relation_cnt(train_da,\"Utilities\",\"Type of Utilities\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","a3d98181":"plot_cat_relation_cnt(train_da,\"Foundation\",\"Type of Foundation\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","26d89f53":"train_da[\"Fence\"] = np.where(train_da[\"Fence\"].isnull(),\"None\",train_da[\"Fence\"])\ntest_da[\"Fence\"] = np.where(test_da[\"Fence\"].isnull(),\"None\",test_da[\"Fence\"])\nplot_cat_relation_cnt(train_da,\"Fence\",\"Fence Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","9b4c6c8c":"# replace NULL with specified string character\ntrain_da[\"BsmtQual\"] = np.where(train_da[\"BsmtQual\"].isnull(),\"None\",train_da[\"BsmtQual\"])\ntest_da[\"BsmtQual\"] = np.where(test_da[\"BsmtQual\"].isnull(),\"None\",test_da[\"BsmtQual\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"BsmtQual\",\"Basement Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","a6d2ae79":"train_da[\"BsmtCond\"] = np.where(train_da[\"BsmtCond\"].isnull(),\"None\",train_da[\"BsmtCond\"])\ntest_da[\"BsmtCond\"] = np.where(test_da[\"BsmtCond\"].isnull(),\"None\",test_da[\"BsmtCond\"])\nplot_cat_relation_cnt(train_da,\"BsmtCond\",\"Basement Condition\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","5315e174":"# replace NULL with specified string character\ntrain_da[\"BsmtExposure\"] = np.where(train_da[\"BsmtExposure\"].isnull(),\"None\",train_da[\"BsmtExposure\"])\ntest_da[\"BsmtExposure\"] = np.where(test_da[\"BsmtExposure\"].isnull(),\"None\",test_da[\"BsmtExposure\"])\nplot_cat_relation_cnt(train_da,\"BsmtExposure\",\"Basement Exposure\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","8d9395b8":"# replace NULL with specified string character\ntrain_da[\"BsmtFinType1\"] = np.where(train_da[\"BsmtFinType1\"].isnull(),\"None\",train_da[\"BsmtFinType1\"])\ntest_da[\"BsmtFinType1\"] = np.where(test_da[\"BsmtFinType1\"].isnull(),\"None\",test_da[\"BsmtFinType1\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"BsmtFinType1\",\"Rating of Basement Finished Area\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","5a9bb164":"# replace NULL with specified string character\ntrain_da[\"BsmtFinType2\"] = np.where(train_da[\"BsmtFinType2\"].isnull(),\"None\",train_da[\"BsmtFinType2\"])\ntest_da[\"BsmtFinType2\"] = np.where(test_da[\"BsmtFinType2\"].isnull(),\"None\",test_da[\"BsmtFinType2\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"BsmtFinType2\",\"Rating of Basement Finished Area\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","b33243a9":"plot_cat_relation_cnt(train_da,\"BsmtFullBath\",\"Basement Full Bathrooms\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","ddd34438":"plot_cat_relation_cnt(train_da,\"BsmtHalfBath\",\"Basement Full Bathrooms\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","711a7e6b":"plot_cat_relation_cnt(train_da,\"HeatingQC\",\"Heating Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","a2e1a8d2":"plot_cat_relation_cnt(train_da,\"Heating\",\"Heating Type\",True)","8670dc0e":"def backoff_bin(data,col,threshold=15):\n    \"\"\"combine rare categories into one bin\"\"\"\n    dist_ser = data[col].value_counts()\n    mark = dist_ser<=threshold\n    target_index = dist_ser[mark].index\n    col_backoff = data[col].apply(lambda x:\"rare_cat_bin\" if x in target_index else x)\n    return col_backoff","a4735bc6":"# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"CentralAir\",\"Central Air Conditioning\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","664828ab":"# replace NULL with specified string character\ntrain_da[\"GarageType\"] = np.where(train_da[\"GarageType\"].isnull(),\"None\",train_da[\"GarageType\"])\ntest_da[\"GarageType\"] = np.where(test_da[\"GarageType\"].isnull(),\"None\",test_da[\"GarageType\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"GarageType\",\"Garage Type\")","ac75e0f2":"# replace NULL with specified string character\ntrain_da[\"GarageFinish\"] = np.where(train_da[\"GarageFinish\"].isnull(),\"None\",train_da[\"GarageFinish\"])\ntest_da[\"GarageFinish\"] = np.where(test_da[\"GarageFinish\"].isnull(),\"None\",test_da[\"GarageFinish\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"GarageFinish\",\"Interior finish of the garage\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","c1a7a2dd":"# replace NULL with specified string character\ntrain_da[\"GarageQual\"] = np.where(train_da[\"GarageQual\"].isnull(),\"None\",train_da[\"GarageQual\"])\ntest_da[\"GarageQual\"] = np.where(test_da[\"GarageQual\"].isnull(),\"None\",test_da[\"GarageQual\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"GarageQual\",\"Garage Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","3ad0e8d9":"# replace NULL with specified string character\ntrain_da[\"GarageCond\"] = np.where(train_da[\"GarageCond\"].isnull(),\"None\",train_da[\"GarageCond\"])\ntest_da[\"GarageCond\"] = np.where(test_da[\"GarageCond\"].isnull(),\"None\",test_da[\"GarageCond\"])\n# inspect association with housing price and its distribution\nplot_cat_relation_cnt(train_da,\"GarageCond\",\"Garage Condition\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","bf5bba84":"plot_cat_relation_cnt(train_da,\"GarageCars\",\"Size of Garage(in Car Capacity)\",True)","8647208d":"plot_cat_relation_cnt(train_da,\"KitchenAbvGr\",\"Kitchen Above Grade\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","515d479a":"plot_cat_relation_cnt(train_da,\"KitchenQual\",\"Kitchen Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","02aa67fd":"# replace NULL with specified string character\ntrain_da[\"FireplaceQu\"] = np.where(train_da[\"FireplaceQu\"].isnull(),\"None\",train_da[\"FireplaceQu\"])\ntest_da[\"FireplaceQu\"] = np.where(test_da[\"FireplaceQu\"].isnull(),\"None\",test_da[\"FireplaceQu\"])\nplot_cat_relation_cnt(train_da,\"FireplaceQu\",\"Fireplace Quality\",False,\n                     errwidth=2,errcolor=\"darkblue\",capsize=.2)","4c56f7f2":"plot_cat_relation_cnt(train_da,\"Fireplaces\",\"Number of Fireplaces\",False,\n                      errwidth=2,errcolor=\"darkblue\",capsize=.2)","db3ea591":"plot_cat_relation_cnt(train_da,\"Electrical\",\"Electrical System\",True)","e27ef3e4":"train_da[\"MiscFeature\"] = np.where(train_da[\"MiscFeature\"].isnull(),\"None\",train_da[\"MiscFeature\"])\ntest_da[\"MiscFeature\"] = np.where(test_da[\"MiscFeature\"].isnull(),\"None\",test_da[\"MiscFeature\"])\nplot_cat_relation_cnt(train_da,\"MiscFeature\",\"Miscellaneous feature\",True)","68afab5c":"plot_cat_relation_cnt(train_da,\"OverallCond\",\"Rating of Overall Condition of the House\",False,\n                      errwidth=2,errcolor=\"darkblue\",capsize=.2)","348feec3":"plot_cat_relation_cnt(train_da,'BedroomAbvGr',\"Bedrooms above grade\",False,\n                      errwidth=2,errcolor=\"darkblue\",capsize=.2)","1a952703":"plot_cat_relation_cnt(train_da,\"TotRmsAbvGrd\",\"Total rooms above grade\",False,\n                      errwidth=2,errcolor=\"darkblue\",capsize=.2)","f083876d":"plot_cat_relation_cnt(train_da,\"Functional\",\"Home Functional\",True)","a6ae9e71":"plot_cat_relation_cnt(train_da,\"Condition1\",\"Condition1\",True)","61007493":"plot_cat_relation_cnt(train_da,\"SaleType\",\"Sale Type\",True)","74c16c22":"plot_cat_relation_cnt(train_da,\"SaleCondition\",\"Sale Condition\",True)","1a99e0e1":"# Here I convert two boolean values into integers\ntrain_da[\"HavePool\"] = train_da.HavePool.astype(\"int\")","49f38023":"train_da.query(\"GrLivArea>4000 and SalePrice<=200000\")[attributes]","7ac8f36e":"# drop the three outliers\ntrain_da.drop(labels=[523,1298],inplace=True)","e0de7b6f":"def get_missing_pct(df):\n    \"\"\"compute the percentage for missing val\"\"\"\n    missing_df = df.apply(lambda col:col.isnull().sum()).to_frame().reset_index().rename(columns={\"index\":\"column\",0:\"missing_cnt\"})\n    IsMissing = missing_df[\"missing_cnt\"]>0\n    missing_df_ = missing_df[IsMissing]\n    missing_df_[\"missing_pct\"] = missing_df[\"missing_cnt\"] \/ df.shape[0]\n    return missing_df_","3d32f561":"# compute the missing count and missing percent for train data and test data\ntrain_missing_df = get_missing_pct(train_da)\ntest_missing_df = get_missing_pct(test_da)","cbd135e6":"cm = sns.light_palette(\"deepskyblue\", as_cmap=True)\ndisplay(train_missing_df.sort_values(by=\"missing_pct\",ascending=False).style.background_gradient(cm))\ndisplay(test_missing_df.sort_values(by=\"missing_pct\",ascending=False).style.background_gradient(cm))","1ff6d927":"# examine the observations that are not missing from both train set and test set\ntrain_da.query(\"PoolArea>0\")[['PoolArea','PoolQC']]","9e46bb37":"# check houses with pool in test set\ntest_da.query(\"PoolArea>0\")[['PoolArea','PoolQC']]","c1ee7a08":"# let's gain insights into train set\ntrain_da.query(\"PoolArea>0\")[['OverallQual','PoolArea','PoolQC']]","02037492":"display(test_da.query(\"PoolArea>0\")[['OverallQual','PoolArea','PoolQC']])","9019a3fd":"# impute the missing values of test data\ntest_da.loc[960,'PoolQC'] = \"TA\"\ntest_da.loc[1043,'PoolQC'] = \"Gd\"\ntest_da.loc[1139,'PoolQC'] = \"Fa\"","b89faef2":"train_da['PoolQC'] = np.where(train_da[\"PoolQC\"].isnull(),\"None\",train_da[\"PoolQC\"])\ntest_da['PoolQC'] = np.where(test_da[\"PoolQC\"].isnull(),\"None\",test_da[\"PoolQC\"])","6a688f89":"# examine the imputation\nprint(f\"Missing Value Count:{train_da['PoolQC'].isnull().sum()}\")\nprint(f\"Missing Value Count:{test_da['PoolQC'].isnull().sum()}\")","a726a88f":"# directly impute missing values with None\ntrain_da['Fence'] = np.where(train_da[\"Fence\"].isnull(),\"None\",train_da[\"Fence\"])\ntest_da['Fence'] = np.where(test_da[\"Fence\"].isnull(),\"None\",test_da[\"Fence\"])","e411adf3":"temp = train_da[train_da.LotFrontage.isnull()][[\"LotFrontage\",\"LotArea\",\"1stFlrSF\",\"LotShape\",\"LotConfig\"]]\nprint(f\"Distribution of Missing Values across Lot Shape:\\n{temp.LotShape.value_counts()}\")\nprint(\"_________________________________________________________________________\")\nprint(f\"Distribution of Missing Values across Lot Config:\\n{temp.LotConfig.value_counts()}\")","dac7c50c":"display(train_da.groupby(\"LotConfig\").agg({\"LotFrontage\":\"mean\"}))\ndisplay(train_da.groupby(\"LotShape\").agg({\"LotFrontage\":\"mean\"}))\ndisplay(f\"Mean Lot Frontage across all houses:{train_da.LotFrontage.mean()}\")","8dc0254b":"def impute_lotfrontage(x):\n    \"\"\"Impute Missing Values with values Corresponding category of Config\"\"\"\n    import math\n    \n    if math.isnan(x.loc[\"LotFrontage\"]) and x.loc[\"LotConfig\"] == \"Corner\":\n        x.loc[\"LotFrontage\"] = 82.895\n    elif math.isnan(x.loc[\"LotFrontage\"]) and x.loc[\"LotConfig\"] == \"CulDSac\":\n        x.loc[\"LotFrontage\"] = 59.911\n    elif math.isnan(x.loc[\"LotFrontage\"]) and x.loc[\"LotConfig\"] == \"FR2\":\n        x.loc[\"LotFrontage\"] = 63.515\n    elif math.isnan(x.loc[\"LotFrontage\"]) and x.loc[\"LotConfig\"] == \"FR3\":\n        x.loc[\"LotFrontage\"] = 70.75\n    elif math.isnan(x.loc[\"LotFrontage\"]) and x.loc[\"LotConfig\"] == \"Inside\":\n        x.loc[\"LotFrontage\"] = 67.648\n    return x","e55aa527":"train_da = train_da.apply(lambda row:impute_lotfrontage(row),axis=1)\ntest_da = test_da.apply(lambda row:impute_lotfrontage(row),axis=1)","5baef263":"# train data\nattrs = [\"GarageYrBlt\",\"GarageType\",\"GarageFinish\",\"GarageCars\",\n                                              \"GarageArea\",\"GarageQual\",\"GarageCond\"]\ntemp = train_da.query(\"GarageArea>0\")[attrs]\ntemp.isnull().sum()","d156a7ba":"# test data\ntemp = test_da.query(\"GarageArea>0\")[attrs]\ntemp.isnull().sum()","6aa1f8b1":"temp[temp.GarageYrBlt.isnull()]","e3e6d51b":"detchd_da = train_da[train_da.GarageType==\"Detchd\"]\ngarage_stats = [detchd_da.GarageYrBlt.mode().values[1],detchd_da.GarageFinish.mode().values[0],detchd_da.GarageCars.mode().values[0],\n               detchd_da.GarageArea.mean(),detchd_da.GarageQual.mode().values[0],detchd_da.GarageCond.mode().values[0]]\nres = pd.DataFrame(garage_stats,index=[\"GarageYrBlt\",\"GarageFinish\",\"GarageCars\",\"GarageArea\",\"GarageQual\",\"GarageCond\"],\n            columns=[\"Stats\"])\ndisplay(res)","84f3132d":"test_da.loc[666,[\"GarageYrBlt\",\"GarageFinish\",\"GarageQual\",\"GarageCond\"]] = \\\n[1999,\"Unf\",\"TA\",\"TA\"]\ntest_da.loc[1116,[\"GarageYrBlt\",\"GarageFinish\",\"GarageCars\",\"GarageArea\",\"GarageQual\",\"GarageCond\"]] = \\\n[1999,\"Unf\",2,426.857881,\"TA\",\"TA\"]","5b44c694":"# impute NaN with 0\ntrain_da['GarageYrBlt'] = np.where(train_da[\"GarageYrBlt\"].isnull(),0,train_da[\"GarageYrBlt\"])\ntest_da['GarageYrBlt'] = np.where(test_da[\"GarageYrBlt\"].isnull(),0,test_da[\"GarageYrBlt\"])","b061f1d4":"display(get_missing_pct(train_da))\ndisplay(get_missing_pct(test_da))","12c4fa7e":"# split the train set into labels and features\ny = train_da.SalePrice\nlg_y = train_da.lg_SalePrice\ntrain_da.drop([\"SalePrice\",\"lg_SalePrice\"],axis=1,inplace=True)","2ec97061":"# get the columns\nobject_cols,numerical_cols,discrete_cols,continuous_cols = get_columns(train_da)","6390723f":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import KNNImputer, MissingIndicator\nfrom sklearn.pipeline import FeatureUnion \n\n# preprocessing for numerical data\ncontinuous_transformer = Pipeline([\n    (\"mean imputer\",SimpleImputer(strategy=\"mean\"))\n])\n\ncats_transformer = Pipeline([\n    (\"k nearest imputer\",SimpleImputer(strategy=\"most_frequent\"))\n])\n\n\n# build the column transformer\npreprocessor = ColumnTransformer(\n  transformers = [\n    (\"continuous_num\",continuous_transformer,continuous_cols),\n      (\"cats\",cats_transformer,object_cols+discrete_cols)\n  ]\n)\n\npreprocessor_model = preprocessor.fit(train_da)\ntrain_da_imputed = pd.DataFrame(preprocessor_model.transform(train_da),\n                                columns=continuous_cols+object_cols+discrete_cols).\\\nconvert_dtypes(convert_floating=True)\ntest_da_imputed = pd.DataFrame(preprocessor_model.transform(test_da),\n                               columns=continuous_cols+object_cols+discrete_cols).\\\nconvert_dtypes(convert_floating=True)","a763b2ab":"print(train_da_imputed.duplicated().sum())\nprint(test_da_imputed.duplicated().sum())","b289bb64":"# concatenate train set and test set\nfull_data = pd.concat([train_da_imputed,test_da_imputed])","c6b1f63c":"# create the total area var\nplot_da = pd.concat([train_da.copy(),y],axis=1)\nplot_da[\"TotalLivingArea\"] = plot_da['GrLivArea'] + plot_da['TotalBsmtSF']\n# visualization\ncoef = plot_da[\"TotalLivingArea\"].corr(plot_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(plot_da,\"TotalLivingArea\",\"Total Living Area\")","11527584":"# now let's create the variable in full data\nfull_data['TotalLivArea'] = full_data['GrLivArea'] + full_data['TotalBsmtSF']\n# then drop the two variables\nfull_data.drop([\"GrLivArea\",\"TotalBsmtSF\"],axis=1,inplace=True)","2a876884":"# create variable IsRemod\nplot_da['IsRemod'] = (plot_da[\"YearBuilt\"] != plot_da[\"YearRemodAdd\"]).astype('int')\n\n# compute median price for all houses\nmean_house_price = plot_da[\"SalePrice\"].median()\n\n# visualize\nsns.set_style('darkgrid')\nfig,ax = plt.subplots(1,1,figsize=(8,5),dpi=100,facecolor=\"whitesmoke\")\nsns.barplot(data=plot_da,x='IsRemod',y=\"SalePrice\",color='lightskyblue',estimator=np.median)\nax.hlines(y=mean_house_price,xmin=-0.5,xmax=1.5,colors=\"black\",linewidth=1.5,linestyles=\"dashed\")\nax.set_xlabel(\"Is Remodeled\")\nax.set_ylabel(\"Median Sale Price\")\nax.set_facecolor(\"whitesmoke\")\nax.set_xticklabels([\"No Renovation\",'Renovation'])\nfor spine in [\"top\",\"right\"]:\n        ax.spines[spine].set_visible(False)\nplt.show()","d1d0cc78":"# Now create the variable in full data\nfull_data['Is_Remod'] = (full_data[\"YearBuilt\"] != full_data[\"YearRemodAdd\"]).astype('int')","a715f77c":"# create IsNew variable for verification of my assumption\nplot_da[\"IsNew\"] = (plot_da.YrSold == plot_da.YearBuilt).astype(\"int\")\n\n# visualize\nsns.set_style('darkgrid')\nfig,ax = plt.subplots(1,1,figsize=(8,5),dpi=100,facecolor=\"whitesmoke\")\nsns.barplot(data=plot_da,x='IsNew',y=\"SalePrice\",color='lightskyblue',estimator=np.median)\nax.hlines(y=mean_house_price,xmin=-0.5,xmax=1.5,colors=\"black\",linewidth=1.5,linestyles=\"dashed\")\nax.set_xlabel(\"Is New House\")\nax.set_ylabel(\"Median Sale Price\")\nax.set_facecolor(\"whitesmoke\")\nax.set_xticklabels([\"Not Completely New\",\"Completely New\"])\nfor spine in [\"top\",\"right\"]:\n        ax.spines[spine].set_visible(False)\nplt.show()","e85948c0":"full_data[\"IsNew\"] = (full_data.YrSold == full_data.YearBuilt).astype(\"int\")","d09decd3":"# create the age variable\nplot_da[\"house_age\"] = plot_da[\"YrSold\"] - plot_da[\"YearRemodAdd\"]\ncoef = plot_da[\"house_age\"].corr(plot_da[\"SalePrice\"])\nprint(f\"Correlation Coefficient:{round(coef,3)}\")\nplot_scatter(plot_da,\"house_age\",\"House Age\")","7e5a6360":"full_data[\"house_age\"]  =  full_data[\"YrSold\"] - full_data[\"YearRemodAdd\"]","cced85ee":"# combine bath variables\nplot_da[\"TotalBathRooms\"] = plot_da.BsmtFullBath +  plot_da.BsmtHalfBath*0.5+ \\\nplot_da.FullBath + plot_da.HalfBath*0.5","b9c07314":"# check the correlation coefficient with sale price.\nprint(f\"Correlation with Sale Price: {plot_da.TotalBathRooms.corr(y).round(4)}\")","cb95aae3":"# create the variable\nfull_data[\"TotalBathRooms\"] = full_data.BsmtFullBath +  full_data.BsmtHalfBath*0.5+ \\\nfull_data.FullBath + full_data.HalfBath*0.5\n\n# drop the four variables\nbath_vars = [\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\"]\nfull_data.drop(bath_vars,axis=1,inplace=True)","80171d3a":"from sklearn.preprocessing import OrdinalEncoder\n\ndef ordinal_encoding(df,col,cat_list):\n    \"\"\"convert text categorical variable into ordinal variable\"\"\"\n    # lot shape\n    lot_arr = np.array(df[col]).reshape(-1,1)\n    ordinal_encoder = OrdinalEncoder(categories=[cat_list])\n    ordinal_encoder.fit(lot_arr)\n    \n    return ordinal_encoder.transform(lot_arr)","74894cab":"full_data[\"LotShape\"] = ordinal_encoding(full_data,\"LotShape\",[\"Reg\",'IR1',\"IR2\",\"IR3\"])","96d1548f":"full_data[\"LandSlope\"] = ordinal_encoding(full_data,\"LandSlope\",[\"Gtl\",\"Mod\",\"Sev\"])","a8154fdf":"# street\nfull_data[\"Street\"] = ordinal_encoding(full_data,\"Street\",[\"Grvl\",\"Pave\"])\n# PavedDriveway\nfull_data[\"PavedDrive\"] = ordinal_encoding(full_data,\"PavedDrive\",[\"N\",\"P\",\"Y\"])","5618fa48":"bsmt_vars = [\"BsmtQual\",\"BsmtExposure\",\"BsmtCond\"]\nbsmt_cat_lists = [[\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],[\"None\",\"No\",\"Mn\",\"Av\",\"Gd\"],[\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"]]\n\nfor i in range(len(bsmt_vars)):\n    var = bsmt_vars[i]\n    var_list = bsmt_cat_lists[i]\n    full_data[var] = ordinal_encoding(full_data,var,var_list)","ab200989":"house_vars = ['HeatingQC',\"CentralAir\"]\nhouse_var_lists = [[\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],[\"Y\",\"N\"]]\n\nfor i in range(len(house_vars)):\n    var = house_vars[i]\n    var_list = house_var_lists[i]\n    full_data[var] = ordinal_encoding(full_data,var,var_list)","60863c55":"garage_vars = ['GarageFinish',\"GarageQual\",\"GarageCond\"]\ngarage_var_lists = [[\"None\",\"Unf\",\"RFn\",\"Fin\"],[\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],[\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"]]\n\nfor i in range(len(garage_vars)):\n    var = garage_vars[i]\n    var_list = garage_var_lists[i]\n    full_data[var] = ordinal_encoding(full_data,var,var_list)","eea8d41b":"full_data[\"PoolQC\"] = ordinal_encoding(full_data,\"PoolQC\",[\"None\",\"Fa\",\"TA\",\"Gd\",\"Ex\"])","c618081e":"full_data[\"KitchenQual\"] = ordinal_encoding(full_data,\"KitchenQual\",[\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"])","cf7d7cba":"full_data[\"FireplaceQu\"] = ordinal_encoding(full_data,\"FireplaceQu\",[\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"])","6b97c7c8":"# ordinally encode ExternalQual\nfull_data[\"ExterQual\"] = ordinal_encoding(full_data,\"ExterQual\",[\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"])\n\n# ordinally encode ExternalCond\nfull_data[\"ExterCond\"] = ordinal_encoding(full_data,\"ExterCond\",[\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"])","7a72e242":"cats = [\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"]\nfull_data[\"Functional\"] = ordinal_encoding(full_data,\"Functional\",cats)","b354e6ea":"full_data.drop([\"Utilities\",\"YrSold\"],axis=1,inplace=True)","65f51392":"# define a function to check the number of each category for all ordinal variables\ndef get_cat_dist(data,row,col,cat_cols):\n    \n    i = 0\n    fig,axes = plt.subplots(row,col,figsize=(25,25),dpi=100,facecolor=\"whitesmoke\")\n    for r in range(row):\n        for c in range(col):\n            try:\n                temp = data[cat_cols[i]].value_counts().reset_index().rename(columns={'index':cat_cols[i],cat_cols[i]:\"Frequency\"})\n                # Iterrating over the bars one-by-one\n                plots = sns.barplot(data=temp,x=cat_cols[i],y=\"Frequency\",color='lightskyblue',ax=axes[r,c])\n                for bar in plots.patches:\n                    plots.annotate(format(bar.get_height(), '.0f'),\n                                   (bar.get_x() + bar.get_width() \/ 2,\n                                    bar.get_height()), ha='center', va='center',\n                                   size=12, xytext=(0, 8),\n                                   textcoords='offset points')\n                text_title = cat_cols[i]\n                axes[r,c].set_facecolor(\"whitesmoke\")\n                axes[r,c].set_title(text_title,fontsize=10,weight=\"bold\",pad=-12,y=1)\n                axes[r,c].set_xlabel(\"\")\n                axes[r,c].set_ylabel(\"Frequency\")\n                for spine in [\"right\",\"top\"]:\n                    axes[r,c].spines[spine].set_visible(False)\n                    # rotate the x ticks labels when there are more than 10 categories\n                if data[cat_cols[i]].nunique()>=10:\n                    axes[r,c].set_xticklabels(plots.get_xticklabels(),rotation=45)\n                i += 1\n            except IndexError:\n                pass\n\n    # delete all surplus subplots that do not include any graphs\n    if row*col > len(cat_cols):\n        total_plot_num = row*col\n        plot_num = total_plot_num-len(cat_cols)\n        for i in reversed(list(range(plot_num))):\n            axes[row-1,col-i-1].set_visible(False)\n    \n    plt.show()","173b0eb3":"# get the different types of colmns\nobject_cols,numerical_cols,discrete_cols,continuous_cols = get_columns(train_da)\n\n# construct the categorical columns\n# since neighborhood has too many categories, so I will separately transform it\n# here I delete it from the list\nordinal_cols = [\"LotShape\",\"LandSlope\",\"Street\",\"PavedDrive\",\"BsmtQual\",\"BsmtExposure\",\"BsmtCond\",\n               'HeatingQC',\"CentralAir\",'GarageFinish',\"GarageQual\",\"GarageCond\",\"PoolQC\",\"KitchenQual\",\n               \"FireplaceQu\",\"ExterQual\",\"ExterCond\",\"Functional\"]\nneigh = [\"Neighborhood\",\"Utilities\"]\n# remove oridnal cosl\nonehot_cols = [col for col in object_cols if col not in ordinal_cols+neigh]\nonehot_cols = onehot_cols + [\"MSSubClass\",\"BedroomAbvGr\",\"MoSold\"]","8b41a71d":"get_cat_dist(full_data,4,6,onehot_cols)","738256d5":"# convert variables into dummy variables\nfull_data_onehot = pd.get_dummies(full_data,columns=onehot_cols)\n# filter these onehot vars \nonehot_df = full_data_onehot.iloc[:,51:]","aa75ab3d":"# examine the sample size of these variables\nfrequency_ser = onehot_df.apply(lambda col:col.sum()).sort_values(ascending=False)\nmark = frequency_ser<10\n# filter out these onehot variable with small sample\nsmall_sample_onehot = frequency_ser[mark].index\n# delete them from full data\nfull_data_onehot_drop = full_data_onehot.drop(small_sample_onehot,axis=1)","368a9e02":"print(full_data_onehot.shape)\nprint(full_data_onehot_drop.shape)","9dfa8dcf":"fig,ax = plt.subplots(1,1,figsize=(10,3),dpi=100,facecolor=\"whitesmoke\")\nNeigh_da = plot_da.groupby(\"Neighborhood\").agg({\"SalePrice\":np.median}).reset_index().sort_values(by=\"SalePrice\",ascending=True)\nsns.barplot(data=plot_da,x=\"Neighborhood\",y='SalePrice',estimator=np.median,\n            order=Neigh_da.Neighborhood,capsize=.2,color=\"lightskyblue\",errcolor=\"slategrey\",errwidth=1,ax=ax)\nax.hlines(y=mean_house_price,xmin=-0.5,xmax=len(Neigh_da),colors=\"blue\",linewidth=1.5,linestyles=\"dashed\")\nax.set_xlabel(\"Neighborhood\",fontsize=8)\nax.set_ylabel(\"Median Housing Price\",fontsize=8)\nax.set_facecolor(\"whitesmoke\")\nax.set_title(\"Housing Median Price across neighborhoods\",fontsize=14)\nfor spine in [\"top\",\"right\"]:\n    ax.spines[spine].set_visible(False)\nplt.tick_params(labelsize=8)\nplt.xticks(rotation=45)\nplt.show()","60aef697":"first_group = Neigh_da.Neighborhood.iloc[:12].tolist()\nsecond_group = Neigh_da.Neighborhood.iloc[12:22].tolist()\nthird_group = Neigh_da.Neighborhood.iloc[-3:].tolist()\n\ndef assign_numbers(x):\n    if x in first_group:\n        return 0\n    elif x in second_group:\n        return 1\n    else:\n        return 2\n\nfull_data_onehot_drop[\"Neighborhood\"] = full_data_onehot_drop.Neighborhood.apply(lambda x:assign_numbers(x))","8a62dedc":"temp = full_data_onehot_drop.iloc[:,:51]\ncontinuous_vars = [col for col in temp.columns if temp[col].nunique()>20]","1b5413ca":"check_dist(5,5,full_data_onehot_drop,continuous_vars,\"blue\",False,15)","8f4d8e56":"# defind the log_transform function\nfrom scipy import stats\n\ndef log_transform(df,col):\n    \"\"\"Perform Log Transformation dataset\"\"\"\n    \n    col_bc = np.log(df[col]+1)\n    return col_bc","851941b9":"for_plot = full_data_onehot_drop.copy()\nlog_cols = [\"LotFrontage\",\"LotArea\",\"YearBuilt\",\"YearRemodAdd\",\"MasVnrArea\",\"BsmtUnfSF\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"1stFlrSF\",\"2ndFlrSF\",\n           \"LowQualFinSF\",\"GarageArea\",\"TotalPorchSF\",\"TotalLivArea\"]\nnew_log_cols = []\nfor col in log_cols:\n    newcol = \"Log_\" + col\n    new_log_cols.append(newcol)\n    for_plot[newcol] = log_transform(for_plot,col)","3a450218":"# since boxcox only supprt for int variable, so we need to change the data type\nfor_plot[\"BsmtFinSF2\"] = for_plot[\"BsmtFinSF2\"].astype(\"int\")\nfor_plot[\"LowQualFinSF\"] = for_plot[\"LowQualFinSF\"].astype(\"int\")\nfor_plot[\"GarageArea\"] = for_plot[\"GarageArea\"].astype(\"int\")\nfor_plot[\"BsmtUnfSF\"] = for_plot[\"BsmtUnfSF\"].astype(\"int\")","aea6f483":"# conduct boxcox transformation\n# note: boxbox only support for positive number, so here I add 1\nbsmt_bc,param = stats.boxcox(for_plot[\"BsmtFinSF2\"]+1)\nfor_plot[\"bc_BsmtFinSF2\"] = bsmt_bc\n\nqual_bc,param = stats.boxcox(for_plot[\"LowQualFinSF\"].astype(\"int\")+1)\nfor_plot[\"bc_LowQualFinSF\"] = qual_bc\n\ngarage_bc,param = stats.boxcox(for_plot[\"GarageArea\"]+1)\nfor_plot[\"bc_GarageArea\"] = garage_bc\n\n# perform the same transformation on lowqualfinsf\nbsmtunf_bc,param = stats.boxcox(for_plot[\"BsmtUnfSF\"].astype(\"int\")+1)\nfor_plot[\"bc_BsmtUnfSF\"] = bsmtunf_bc","03648724":"full_data_onehot_drop[[\"BsmtFinSF2\",\"LowQualFinSF\"]].apply(lambda col:(col==0).sum())","20d0645a":"# now perform log transform in full data\nfull_data_onehot_normal = full_data_onehot_drop.copy()\nlog_cols = [\"LotFrontage\",\"LotArea\",\"MasVnrArea\",\"BsmtFinSF1\",\"1stFlrSF\",\"TotalPorchSF\",\"TotalLivArea\"]\nnew_log_cols = []\nfor col in log_cols:\n    newcol =  \"Log_\" + col\n    new_log_cols.append(newcol)\n    full_data_onehot_normal[newcol] = log_transform(full_data_onehot_normal,col)\n    full_data_onehot_normal.drop(col,axis=1,inplace=True)","0e7ee8d2":"# since boxcox only supprt for int variable, so we need to change the data type\nfull_data_onehot_normal[\"BsmtFinSF2\"] = full_data_onehot_normal[\"BsmtFinSF2\"].astype(\"int\")\nfull_data_onehot_normal[\"LowQualFinSF\"] = full_data_onehot_normal[\"LowQualFinSF\"].astype(\"int\")\nfull_data_onehot_normal[\"GarageArea\"] = full_data_onehot_normal[\"GarageArea\"].astype(\"int\")\nfull_data_onehot_normal[\"BsmtUnfSF\"] = full_data_onehot_normal[\"BsmtUnfSF\"].astype(\"int\")\n\n# conduct boxcox transformation\n# note: boxbox only support for positive number, so here I add 1\nbsmt_bc,param = stats.boxcox(full_data_onehot_normal[\"BsmtFinSF2\"].astype(\"int\")+1)\nfull_data_onehot_normal[\"bc_BsmtFinSF2\"] = bsmt_bc\n\n# perform the same transformation on lowqualfinsf\nqual_bc,param = stats.boxcox(full_data_onehot_normal[\"LowQualFinSF\"].astype(\"int\")+1)\nfull_data_onehot_normal[\"bc_LowQualFinSF\"] = qual_bc\n\ngarage_bc,param = stats.boxcox(full_data_onehot_normal[\"GarageArea\"]+1)\nfull_data_onehot_normal[\"bc_GarageArea\"] = garage_bc\n\n# perform the same transformation on lowqualfinsf\nbsmtunf_bc,param = stats.boxcox(full_data_onehot_normal[\"BsmtUnfSF\"].astype(\"int\")+1)\nfull_data_onehot_normal[\"bc_BsmtUnfSF\"] = bsmtunf_bc","3fe910fb":"# filter out the continuous columns from plot_da, which is created for plotting with Sale Price \ntemp_cols = [col for col in plot_da.columns if col in continuous_vars]\nporch_cols = [\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]\n# remove porch cols because we have TotalPorch\ntemp_cols = [col for col in temp_cols if col not in porch_cols]","45ae7799":"# write function to visualize the distribution of variables\ndef examine_linear_homo(row,col,data,cols,y,color,figsize=25):\n    fig,axes = plt.subplots(row,col,figsize=(25,25),dpi=100,facecolor=\"whitesmoke\")\n    i = 0\n\n    for r in range(row):\n        for c in range(col):\n            try:\n                sns.residplot(data=data,x=cols[i],y=y,lowess=True,ax=axes[r,c],color=color)\n                text_title = cols[i]\n                i += 1\n                axes[r,c].set_facecolor(\"whitesmoke\")\n                axes[r,c].set_title(text_title,fontsize=14,weight=\"bold\")\n            except IndexError:\n                pass\n\n    # delete all surplus subplots that do not include any graphs\n    if row*col > len(cols):\n        total_plot_num = row*col\n        plot_num = total_plot_num-len(cols)\n        for i in reversed(list(range(plot_num))):\n            axes[row-1,col-i-1].set_visible(False)\n            \n    plt.show()","9ef95e44":"# add the lg_saleprice variable\nplot_da = pd.concat([plot_da,lg_y],axis=1)","65e02a10":"examine_linear_homo(4,4,plot_da,temp_cols,\"SalePrice\",\"blue\",25)","c505a732":"temp = plot_da.copy()\ntemp_cols = [\"YearBuilt\",\"2ndFlrSF\"]\nsquared_cols = []\nfor col in temp_cols:\n    squared_col = col+\"_squared\"\n    squared_cols.append(squared_col)\n    temp[squared_col] = temp[col]**2","bd77aea5":"fig,axes = plt.subplots(1,2,figsize=(20,5),dpi=100,facecolor=\"whitesmoke\")\nfor i in range(2):\n    sns.residplot(data=temp,x=squared_cols[i],y=\"SalePrice\",lowess=True,ax=axes[i],color=\"blue\")\n    text_title = squared_cols[i]\n    axes[i].set_facecolor(\"whitesmoke\")\n    axes[i].set_title(text_title,fontsize=14,weight=\"bold\")\nplt.show()","30631fa9":"temp[\"log_price\"] = np.log10(temp[\"SalePrice\"])\ntemp_cols1 = [\"YearBuilt\",\"MasVnrArea\",\"1stFlrSF\",\"2ndFlrSF\",\"GarageArea\",\"TotalPorchSF\"]\nexamine_linear_homo(2,3,temp,temp_cols1,\"log_price\",\"blue\",5)","84d9ba20":"temp[\"lg_2ndFlrSF_squared\"] = np.log10(temp[\"2ndFlrSF_squared\"]+1)\nfig,axes = plt.subplots(1,1,figsize=(10,3),dpi=100,facecolor=\"whitesmoke\")\nfor i in range(2):\n    sns.residplot(data=temp,x=\"lg_2ndFlrSF_squared\",y=\"log_price\",lowess=True,ax=axes,color=\"blue\")\n    text_title = \"2ndFlrSF_squared\"\n    axes.set_facecolor(\"whitesmoke\")\n    axes.set_title(text_title,fontsize=14,weight=\"bold\")\nplt.show()","1078af52":"full_data_onehot_clean = full_data_onehot_normal.copy()\ncolumns_needed_trans = [\"YearBuilt\",\"2ndFlrSF\"]\nsquared_cols = []\nfor col in columns_needed_trans:\n    squared_col = col+\"_squared\"\n    squared_cols.append(squared_col)\n    full_data_onehot_clean[squared_col] = full_data_onehot_clean[col]**2\n    full_data_onehot_clean.drop(col,axis=1,inplace=True)","da5dc1a8":"def get_gmm_class(df,col,n):\n    \"\"\"Get the Gaussian Mixture Model Class\"\"\"\n    \n    from sklearn.mixture import GaussianMixture \n    \n    gmm = GaussianMixture(n_components=n,random_state=42)\n    gmm.fit(np.array(df[col]).reshape(-1, 1))\n    gmm_cluster = gmm.predict(np.array(df[col]).reshape(-1, 1))\n    \n    return gmm_cluster","bed21681":"# create gaussian mixture class\nfull_data_onehot_clean[\"gmm_MasVnrArea\"] = get_gmm_class(full_data_onehot_clean,\"Log_MasVnrArea\",2)\nfull_data_onehot_clean[\"gmm_BsmtFinSF2\"] = get_gmm_class(full_data_onehot_clean,\"bc_BsmtFinSF2\",2)\nfull_data_onehot_clean[\"gmm_GarageArea\"] = get_gmm_class(full_data_onehot_clean,\"bc_GarageArea\",2)\nfull_data_onehot_clean[\"gmm_BsmtUnfSF\"] = get_gmm_class(full_data_onehot_clean,\"bc_BsmtUnfSF\",2)\nfull_data_onehot_clean[\"gmm_BsmtFinSF1\"] = get_gmm_class(full_data_onehot_clean,\"Log_BsmtFinSF1\",2)\nfull_data_onehot_clean[\"gmm_TotalPorchSF\"] = get_gmm_class(full_data_onehot_clean,\"Log_TotalPorchSF\",2)","7268d1b7":"#let's drop the variables that are used for feature extraction\nporch_cols = [\"OpenPorchSF\",\"EnclosedPorch\",\"ScreenPorch\",\"3SsnPorch\"]\nfull_data_onehot_clean.drop(porch_cols,axis=1,inplace=True)\n#plot_da.drop(porch_cols,axis=1,inplace=True)","46e3e7ac":"# split the full data\ntrain_da_engineered = full_data_onehot_clean.iloc[:1458,:]\nlg_SalePrice = lg_y.reset_index().drop(\"index\",axis=1).lg_SalePrice\ntrain_da_engineered = pd.concat([train_da_engineered,lg_SalePrice],axis=1)\ntest_da_engineered = full_data_onehot_clean.iloc[1458:,:]","83d09f0e":"for_selection = pd.concat([train_da_engineered.iloc[:,:35],train_da_engineered[\"lg_SalePrice\"]],axis=1)","b8b14264":"# get continuous columns\ncontinuous_cols = [col for col in train_da_engineered.columns if train_da_engineered[col].nunique()>20]\ndiscrete_cols = [col for col in for_selection.columns if col not in continuous_cols]\nselection_cols = continuous_cols + [\"OverallQual\",\"OverallCond\",\"Is_Remod\",\"HavePool\",\"PoolArea\",\"TotalBathRooms\",\n                                   \"TotRmsAbvGrd\",\"IsNew\"]\n","c66f689e":"corr_plot(train_da_engineered,selection_cols)","8cc16b3d":"cols_dx = [\"YearRemodAdd\",\"LowQualFinSF\",\"bc_LowQualFinSF\",\"TotRmsAbvGrd\",\"bc_LowQualFinSF\",\"BsmtFinSF2\",\"bc_BsmtFinSF2\",\n           \"gmm_BsmtFinSF2\",\"PoolArea\",\"GarageYrBlt\",\"Log_BsmtFinSF1\",\"Fireplaces\",\"GarageCond\"]\ntrain_da_selected = train_da_engineered.drop(cols_dx,axis=1)\ntest_da_selected = test_da_engineered.drop(cols_dx,axis=1)","f6d01e0c":"print(train_da_selected.shape)\nprint(test_da_selected.shape)","f894e728":"from sklearn.model_selection import train_test_split\n\nX = train_da_selected.drop(\"lg_SalePrice\",axis=1)\ny = train_da_selected[\"lg_SalePrice\"]\n\n# split data into train set and test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n\nX_train_full = X_train.copy()\nX_test_full = X_test.copy()","876ad7ba":"unit_cols = X_train_full.dtypes[X_train_full.dtypes == \"uint8\"].index\nX_train_full[unit_cols] = X_train_full[unit_cols].astype(\"int\")\nX_test_full[unit_cols] = X_test_full[unit_cols].astype(\"int\")\ntest_da_selected[unit_cols] = test_da_selected[unit_cols].astype(\"int\")\ncols_ = [\"OverallQual\",\"OverallCond\",\"KitchenAbvGr\",\"GarageCars\",\"HavePool\",\"house_age\",\"TotalBathRooms\",\"WoodDeckSF\",\n \"Log_LotFrontage\",\"bc_GarageArea\",\"Log_MasVnrArea\",\"bc_BsmtUnfSF\",\"Log_TotalLivArea\",\"YearBuilt_squared\",\n         \"2ndFlrSF_squared\",\"Log_LotArea\",\"Log_1stFlrSF\",\"Log_TotalPorchSF\"]\nfor col in cols_:\n    X_train_full[col] = X_train_full[col].astype(\"float\")\n    X_test_full[col] = X_test_full[col].astype(\"float\")\n    test_da_selected[col] = test_da_selected[col].astype(\"float\")\n    ","47a39aaf":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.model_selection import cross_val_score,cross_val_predict, KFold\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n\nseed = 42\nlasso = make_pipeline(RobustScaler(),Lasso(normalize=False,random_state=seed,max_iter=10000))\nEnet = make_pipeline(RobustScaler(),ElasticNet(normalize=False,random_state=seed,max_iter=10000))\ngbrt = GradientBoostingRegressor(random_state=seed,max_features=\"sqrt\")\nxgb_rg = xgb.XGBRegressor(random_state=seed)\nmodels = [\n    (\"lasso\",lasso),\n    (\"ElasticNet\",Enet),\n    (\"GradientBoosting\",gbrt),\n    (\"XGBoost\",xgb_rg)\n]","2a4fee33":"def get_scores(model,x,y,scoring=\"neg_mean_squared_error\",n_folds=10):\n    \"\"\"display a set of statistics for model\"\"\"    \n    kf = KFold(n_folds, shuffle=True, random_state=seed).get_n_splits(x.values)\n    scores = -cross_val_score(model,x,y,scoring=\"neg_mean_squared_error\",cv=kf)\n    return scores","42601689":"def get_scores_frame(models,x,y,n_folds):\n    names = []\n    results = []\n    for name,model in models:\n        scores = get_scores(model,x,y,n_folds)\n        names.append(name)\n        results.append(scores)\n\n    # wrap results into dataframe\n    scores_df = pd.DataFrame(results).T\n    scores_df.columns = names\n    \n    return scores_df\n\nscores = get_scores_frame(models,X_train_full,y_train,n_folds=10)","34c1de6a":"def cross_validation_eval(model,x,y,IsTrain=True):\n    scores =cross_val_score(model,x,y,cv=10,scoring=\"neg_mean_squared_error\")\n    rmse_scores = np.sqrt(-scores)\n    if IsTrain:\n        print(\"Train Set RMSE: \")\n        print(\"----------------------------\")\n        print(\"Mean RMSE: {0}\".format(round(rmse_scores.mean(),6)))\n        print(\"STD RMSE: {0}\".format(round(rmse_scores.std(),6)))\n    else:\n        print(\"Test Set RMSE: \")\n        print(\"----------------------------\")\n        print(\"Mean RMSE: {0}\".format(round(rmse_scores.mean(),6)))\n        print(\"STD RMSE: {0}\".format(round(rmse_scores.std(),6)))","336aad75":"scores.apply(lambda x:x.mean())","6e8fda31":"# visualize the scores of each model\nfig,ax = plt.subplots(1,1,figsize=(8,5),dpi=100,facecolor=\"whitesmoke\")\nsns.boxplot(data=scores,ax=ax,palette=\"Blues\")\nax.set_xticklabels(scores.columns)\nax.set_facecolor(\"whitesmoke\")\nax.set_title(\"Performance of Each Machine Learning Algorithm\",fontsize=14,weight=\"bold\")\nplt.show()","f168af2f":"forest = RandomForestRegressor(random_state=42,n_estimators=500,max_features=\"sqrt\")\nforest.fit(X_train_full,y_train)\nfig,ax = plt.subplots(1,1,figsize=(15,35),dpi=100,facecolor=\"whitesmoke\")\nfeatureImportance_ser = pd.Series(forest.feature_importances_,index=X_train_full.columns).sort_values(ascending=True)\nfeatureImportance_ser.plot(kind=\"barh\",ax=ax,color=\"Blue\")\nax.set_xlabel(\"Featue Importance\",fontsize=14)\nax.set_facecolor(\"whitesmoke\")\nfor spine in [\"right\",\"top\"]:\n    ax.spines[spine].set_visible(False)\nplt.tick_params(labelsize=10)\nplt.show()","0e45199f":"lasso = make_pipeline(RobustScaler(),Lasso(alpha=0.00066,normalize=False,random_state=seed,max_iter=10000))\nEnet = make_pipeline(RobustScaler(),ElasticNet(alpha=0.00069,l1_ratio=0.9,normalize=False,random_state=seed,max_iter=10000))\ngb = GradientBoostingRegressor(learning_rate=0.0493,max_depth=3,max_features=\"sqrt\",min_samples_leaf=5,\n                               min_samples_split=12,n_estimators=400,random_state=42)\nXG_reg = xgb.XGBRegressor(max_depth=2,min_child_weight=4,eta=0.06,subsample=1,colsample_bytree=1,objective=\"reg:linear\",random_state=seed)","b1a496fe":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)  ","47c1f0d0":"averaged_models = AveragingModels(models = (lasso,Enet,gb))\n\ncross_validation_eval(averaged_models,X_train_full,y_train,True)\ncross_validation_eval(averaged_models,X_test_full,y_test,True)","17036701":"X_full_data = pd.concat([X_train_full,X_test_full])\ny_full_data = pd.concat([y_train,y_test])","d33b7b56":"averaged_models.fit(X_full_data,y_full_data)\ngb.fit(X_full_data,y_full_data)\nEnet.fit(X_full_data,y_full_data)\nlasso.fit(X_full_data,y_full_data)\n#XG_reg.fit(X_full_data,y_full_data)","635f7c16":"cross_validation_eval(averaged_models,X_full_data,y_full_data)\ncross_validation_eval(gb,X_full_data,y_full_data)\ncross_validation_eval(Enet,X_full_data,y_full_data)\ncross_validation_eval(lasso,X_full_data,y_full_data)","dd68a764":"aggregated_pred = np.exp(averaged_models.predict(test_da_selected))\ngb_pred = np.exp(gb.predict(test_da_selected))\nEnet_pred = np.exp(Enet.predict(test_da_selected))\nLasso_pred = np.exp(lasso.predict(test_da_selected))","eb96afa9":"final_pred = 0.7*aggregated_pred + 0.2*Enet_pred + 0.1*gb_pred","8d807a19":"prediction_df = pd.DataFrame()\nprediction_df[\"Id\"] = list(range(1461,2920))\nprediction_df[\"SalePrice\"] = final_pred\nprediction_df = prediction_df.set_index(\"Id\")\nprediction_df.to_csv(\"prediction_StackingModel_2021-12-27_version5.csv\")","99d91886":"prediction_df = pd.DataFrame()\nprediction_df[\"Id\"] = list(range(1461,2920))\nprediction_df[\"SalePrice\"] = aggregated_pred\nprediction_df = prediction_df.set_index(\"Id\")\nprediction_df.to_csv(\"prediction_StackingModel_2021-12-27_version6.csv\")","d58a545e":"##### BsmtQual\n\n- Ex: Excellent\n- Gd: Good\n- TA: Typical - slight dampness allowed\n- Fa: Fair - dampness or some cracking or settling\n- Po: Poor - Severe cracking, settling, or wetness\n- NA: No Basement\n\nThe missing observations in `BsmtQual` means no basement. So I will replace the value of missing observaitons from NULL to \"NA\". ","4f38ffb4":"Based on the insights in EDA, I will delete `Utilities` because the variable does not have sufficient variance. I will also delete `YrSold` because housing price does now show great variation across years.","65ba859f":"**Interpretation:**\n\n- Skewness: A standard normal distribition has skewness of zero, but the skewness of tagret variable is 1.88. Also, histogram and qq plot can tell us the target variable is not normally distributed with slight positive skewness because the histogram has longer tail in the right side of the distribution and data points at both tails of QQ plot lie off the straight line.\n- Kurtosis: The kurtosis of normal distribution is 3 while that of the target variable is 6.54, suggesting heavier tails than tails of normal distribution. The heavier tails mean that values in both tails are **much more extreme** than those of normal distribution.\n\n**Summary:**\n\nThe distribution of target variable is a typical patten of heavy-tailed distribution: most of sale prices are small values but a few sale prices are extremely large. We can perform *log transformation* or *boxcox transformation* on the target variable to compress the long tail in the high end of the distribution into shorter tail.","7a78c611":"##### GarageYrBlt","e553d0dd":"##### Mszoning\n\n**Identifies the general zoning classification of the sale:**\n- A: Agriculture\n- C: Commercial\n- FV: Floating Village Residential\n- I: Industrial\n- RH: Residential High Density\n- RL: Residential Low Density\n- RP: Residential Low Density Park \n- RM: Residential Medium Density","d4a3e0c4":"**Land Shape**","8ee078e0":"### Total Living Area\n\nBased on the correlation matrix, GrLivArea is the second largest determinant, indicating that people places greater importance on the activity space. Since basement is also another kind of living space, I combine the two variables into one TotalLivingArea.","6fe73010":"**Intrepretation:**\n\nWe can obtain the same insights as what we have seen in IsRemodel. Just as we expect, the new houses has much more higher median prices than houses that are not completely new.","171c605a":"##### HouseStyle\n\n- 1Story\tOne story\n- 1.5Fin\tOne and one-half story: 2nd level finished\n- 1.5Unf\tOne and one-half story: 2nd level unfinished\n- 2Story\tTwo story\n- 2.5Fin\tTwo and one-half story: 2nd level finished\n- 2.5Unf\tTwo and one-half story: 2nd level unfinished\n- SFoyer\tSplit Foyer\n- SLvl\tSplit Level","ddbab485":"##### OverallCond\n\n**Rates the overall condition of the house:**\n- 10: Very Excellent\n- 9: Excellent\n- 8: Very Good\n- 7: Good\n- 6: Above Average\t\n- 5: Average\n- 4: Below Average\t\n- 3: Fair\n- 2: Poor\n- 1: Very Poor\n\n**Hunch: Higher rating should result in more valuable houses.**\n\nLet me prove the assumption","39abbc68":"The relationship is much more stronger now than when I did not combine them. So I will include the combined variable.","cdf8dfc5":"##### Street\n\n**Type of road access to property:**\n- Grvl: Gravel\t\n- Pave: Paved","7420b27b":"### Oridnal Encoding","811bd4c6":"##### Above grade (ground) living area square feet","76cc0957":"**Interpretation:**\n\nThere is no ordinality, and some categories have very small sample size, such as 40,45, and 180.","a87c460a":"## Handle Numbers\n\nIn EDA, I found a number of variables whose values span several orders of magnitude. In order to deal with the variablew with the heavy-tailed distribution, *log transform* should be used. It compresses the longer tail of the high end of the distribution into a shorter tail, and expands low end of the distribution inot a longer head.\n\nNow to fully examine the assumptions of linear models, I will check numerical variables in terms of *Normality*, *Homoscedasticity*, *Linearity*, and *Correlation of Error Terms*.\n\n- **Normality**. Linear models assume that residuals of model should be normally distributed. Q-Q plot is an useful tool of checking this.\n- **Linearity**. The linear regression model assumes that there is a straight-line relation- ship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced. Residual plots are a useful graphical tool for identifying non-linearity.\n- **Homoscedasticity.** The assumption means that the residuals have constant variance at each level of X. If the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in the residual plot.\n- **Correlation of Error Terms.** The next assumption of linear regression is that the residuals are independent. This is mostly relevant when working with time series data. Ideally, we don\u2019t want there to be a pattern among consecutive residuals. For example, residuals shouldn\u2019t steadily grow larger as time goes on. I do not plan examine this assumption.","dea4db02":"**Interpretation:**\n\nAfter the power transformation, there is little pattern in the residuals of `YearBuilt` and `2ndFlrSF`. If we cannot spot any discerniable pattern, the residuals should have linearity.","af243bd4":"##### GarageQual\n\n- Ex: Excellent\n- Gd: Good\n- TA: Typical\/Average\n- Fa: Fair\n- Po: Poor\n- NA: No Garage","573b1633":"##### SaleType\n\n- WD: Warranty Deed - Conventional\n- CWD: Warranty Deed - Cash\n- VWD: Warranty Deed - VA Loan\n- New: Home just constructed and sold\n- COD: Court Officer Deed\/Estate\n- Con: Contract 15% Down payment regular terms\n- ConLw: Contract Low Down payment and low interest\n- ConLI: Contract Low Interest\n- ConLD: Contract Low Down\n- Oth: Other","d98d0b20":"#### Housing Conditions\n\n- OverallCond\n- BedroomAbvGr\n- TotRmsAbvGrd\n- Home functionality\n- Condition1\n- Condition2","a11b993c":"##### BsmtFullBath\n\nBasement full bathrooms","cf6e35fe":"**Interpretation:**\n\nThe same positive pattern can be discovered in the scatterplot, and I did not find any remarkable outlying observation here.","d3b25f86":"**Interpretation:**\n\n- The housing price varies from covering to covering, so the variable can be a good indicator for sale price.\n- In the right-hand table, there are only two or one observations in the last five classes. I will delete tehse categories.","0743ab80":"We can find that a house lacks the recod of `YearBlt` even if it has a garage. Let's filter it out and impute it.","969f0df7":"##### Total Basement Square Frontage","fd534ee0":"##### House Functionality\n\n- Typ: Typical Functionality\n- Min1: Minor Deductions 1\n- Min2: Minor Deductions 2\n- Mod: Moderate Deductions\n- Maj1: Major Deductions 1\n- Maj2: Major Deductions 2\n- Sev: Severely Damaged\n- Sal: Salvage only","76ecd5e4":"##### GarageCond\n\n- Ex: Excellent\n- Gd: Good\n- TA: Typical\/Average\n- Fa: Fair\n- Po: Poor\n- NA: No Garage","e61fa190":"##### Kitchen Quality\n\n- Ex: Excellent\n- Gd: Good\n- TA: Typical\/Average\n- Fa: Fair\n- Po: Poor","daab4102":"There are two observations that have infomation in certain Garage variables when the GarageYrBlt is NULL.\n\nI will compute the corresponding mode and mean for these variables given the Detchd garage type.","29c61648":"**Insights:**\n\n- There are total 81 variables and 1460 rows of observations.\n- Obviously, there are several variables including missing values.\n- Variables can be divided into three classes, object, float and int. We can write functions to extract columns with different categories so as to perform distinct transformations on these variables.","10822604":"**Interpretation:**\n\nAfter log transforming the response variable, the residuals do not get spread out when fitted values become larger, although `2ndFlrSF` displays non-linear association.\n\nNow let me perform both transformations on `2ndFlrSF`.","ad96178a":"#### Categorical Variables\n\nThrough viewing the distribution of categorical variables, I try to discover insights into which categorical variable has logical ordering so that we can conduct *ordinal encoding* on such variables. As for categorical variables without logical sequence, I will just perform *one-hot* encoding on them.","77e51bf0":"##### Pool area\n\nNot every house has pool, so I think just a small proportion of houses have pool. Let's first examine the association between poor area and sale price.","090b8d68":"### Covariation between Continuous Predictors and Target Variable\n\nSince the correlation matrix has revealed the strongl correlated variables, I first examine the relationship between these variables and sale price of houses.\n\n#### Year variables\n\nYear variables include `YearBuilt`, `YearSold`, and `YearRemodAdd`. The three variables determine the age of house. The remodelling year equals to the construction year for houses without any renovation, so the two variables are highly correlated. Also, we can know whether the house experienced any renovation in the past based on the equi-condition of the two variables. \n\n##### YearBuilt\n\nFirst, I inspect the association between construction year and sale price and spot any outliers.","9d4748d7":"##### Alley\n\n- *Grvl* Gravel\n- *Pave* Paved\n- *NA* No alley access\n\nThe missing values in Alley variable means there is no alley access. ","20b79eb5":"**Interpretation:**\n\nThis is categorical variable, so I just need to convert it to dummy variables.","4fa12dd8":"## Handle Categorical Variables\n\nIn the section, I will convert categorical variables with ordinality into ordinal variables.","39fe3c16":"**House Heating & Air**","bfd94cc9":"# Prediction","f50ef557":"Apparently, the fitted line is better than before because the log transformation expands the range of small values but compresses the range of large values. The larger x gets smaller increments of $\\log{(x)}$.","c4067eab":"**Interpretation:**\n\nIt seems like that the housing price does not increase monotomically or decrease monotomically with the rise of number of above-grade bedrooms. Therefore, the ordinal encoding does not make sense here. **I will encode them as dummy variables.**\n\nAlso, some categories have very small sample size. I will collect these categoreis in backoff bin.","eb60456b":"**Interpretation:**\n\n- As expected, the median housing price increases with rising rating of house condition. So the current ordinal encoding works well.\n- The second category of rating, which is poor, has an extremely large confidence interval, meaning huge variability. Probabily this is because the very small sample size cannot reliably estimate the housing price.  Since they are ordinal numbers instead of categories, I just leave as what they are.","5f2c606c":"##### Land Shape\n\n- *Gtl* Gentle slope\n- *Mod* Moderate Slope\t\n- *Sev* Severe Slope","79189147":"**Interpretation:**\n\nThere are a number of hosues that does not have the second floor. Also the fitted line is not a good fit of the dataset, meaning that there is a non-linear relationship between `2ndFlrSF` and `SalePrice`.\n\nNow, let raise the variable to a power of 2 and examine the decrease in RSS.","0a324cec":"##### Condition1\n\nProximity to various conditions","3b21a2f1":"## Feature Extraction\n\nFeature extraction means that we combine existing features to produce a more useful one. There are a number of variables that are describing one aspect of a house, such as open porch, so we can combine them in some ways.","770c3e35":"## Outliers in Variables\n\nOutliers are observations that are remarkably different from other observations and in most of cases are caused by logging error. If we predicted the the result of target variable using these extremly large values, the model would be pulled off course. **So we need to perform some transformations on the variables, such as quantization or binning. Sometimes, if we know the outliers are caused by logging error, we can directly delete them.**","2470d5f2":"##### First Floor Square Footage","6a969ce6":"##### YearRemodAdd","775090d1":"**Geographical Variables**","c5584ce3":"### Gaussian Mixture Modeling\n\nI find that a number of variables show bimodal or multimodal distribution. Therefore, I want to use Gaussian Mixture Model to create clusters numbers for these variables to represent their clusters. But whether the features can be good to performance depends on cross validation.","8cfe316f":"##### Over Quality of House\n\nThe rating of overall quality ranges from 1 to 10, with 1 being the wrost and 10 being very excellent.","aa78754a":"**Interpretation:**\n\n- A positive association can be found here: if a garage can accommdate more cars, it will be more valuable.\n- The low price makes sense for a small garage that cannot accommdate any car.\n- Garage size of 4 should be deleted because of its small size.","f480b332":"## Stacking Models\n\nStacking Models is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don\u2019t we train a model to perform this aggregation? Figure 7-12 shows such an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor (called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).\n\n<img src=\"https:\/\/img-blog.csdnimg.cn\/20200324132420924.png?x-oss-process=image\/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xpbmxpNTIyMzYyMjQy,size_16,color_FFFFFF,t_70\" width=\"400\">\n\n","e3252e2f":"Now define a function used for transforming categorical variables with ordinality into ordinal variables.","293627fa":"#### Housing Style Variables\n\n- MSSubClass\n- BldgType\n- HouseStyle","f510cfa0":"I decided to impute the missing observations with values of corresponding category of lotconfig.","f79cd1a8":"##### Kitchen Above Grade\n","7f7d6995":"It seems like that the first floor has the exactly same type as the second floor in terms of exterior covering.","20836f01":"Now I will perform log transform on response variable, hoping that the residulas can be constant across fitted value.","f4f775e4":"**Interpretation:**\n\nThis is an ordinal variable. So I need to ordinally encode it.","4bc998f8":"**Interpretation:**\n\nThe median housing price basically increases with the change in the level of slope of property. Also ordinality exists in the three categories, from gentle slope to severe slope. I will assign ordinal numbers to them.","e63a5c4c":"### Normality\n\nI will use the hisograms and Q-Q plot to examine the assumption.","12a14fbd":"##### BsmtCond\n\n**Evaluates the general condition of the basement:**\n- Ex: Excellent\n- Gd: Good\n- TA: Typical - slight dampness allowed\n- Fa: Fair - dampness or some cracking or settling\n- Po: Poor - Severe cracking, settling, or wetness\n- NA: No Basement","bfc69228":"**Functional**","fd8bbb99":"# Data Loading","df37db5d":"**Interpretation:**\n\nBetter paved road can increase the price of house. We should perform ordinal encoding on the variable.","3d2ae927":"**Kitchen Qual**","96154748":"**Interpretation:**\n\n- There is an obvious logical sequence, from no to yes\n- A positive correlation can be identified: a house with good air conditioning is more valuable","bfa73953":"**Intrepretation:**\n\nNo ordinality exists in the variable, and housing price does differ significantly in some categories. I will add dummy variables for these categories.","e5f342a7":"**Enseming Stacked Models, Gradient Boosting, and XGBoost**","bae12716":"###  IsRemodel\n\nNow we create a variable to indicate whether the house experienced any renovation.","c4f24b01":"## Duplication of Variables\n\nID is the primary key of the dataset and we check whethere there are duplicated rows.","40fcfa25":"**Interpretation:**\n\nOther than BsmtUnfSF,BsmtFinSF2,GarageArea,LowQualFinSF, other variables can be appropriately transformed so that they can meet with the assumption of normality. Now I will use boxcox, a more powerful transformation algorithm, to adjust the distribution of the two severely skewed variables. \n\nIn addition, the shape of Year variables does not change. So I just leave them unchanged.","e2bf193d":"**Interpretation:**\n\n- The fitted blue line in `2ndFlrSF` displays a U-shape, which provides a strong indicaton of non-linearity in the data. We can see the same pattern in `YearBuild`. **A simple solution to address this non-linear associations is to use the non-linear predictors of $\\log{X}, \\sqrt{X}, or X^2$, or we can use weighted regression.** (This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.)\n- The variance of error terms increase with the fitted value of `YearBuilt`. Graphically, the residuals display a funnel shape, which indicates heteroscedasticity. The same issue can also be identified in `MasVnrArea`, `1stFlrSF`, `2ndFlrSF`, `GarageArea`, and `TotalPorchSF`. **We can use concave function to transform the response variable, resulting in the great amount of shrinkage of large responses. Hence the heteroscedasticity can be reduced.**","8a925e5e":"#### Fireplace Variables\n\n- FireplaceQU\n- Fireplaces","8ea69c62":"#### House Heating & Air\n\n- HeatingQC\n- Heating\n- CentralAir","e82388a4":"**Interpretation:**\n\n0 bathroom seems not to be significantly different from one half bathroom in terms of sale price. Also, the sample size of 2 half bathroom is too small to be reliably estimated. **I will not include the variable in model.**","7b3c7606":"**Interpretation:**\n\n- There is no logical sequence in the categories, so I will convert it into dummy variables.\n- 2.5Unf and 2.5Fin will be collected in one category because of their small sample size.","cd568955":"# Exploratory Data Analysis\n\nWhen exploring the dataset, we always need to ask ourselves two questions:\n\n- What type of variation occurs within my variables?\n- What type of covariation occurs between my variables?\n\nVariation within variables indicate that we need to inspect the magnitude, scale, and distribution of variables by histogram or boxplot, etc, while covariation between variables mean **whether variables are correlated with each other? Whether the impact of one predictor on target variable is influenced by other variables?(interaction) and so on.**\n\n","dc2405c8":"Now let's try to use *Log Transform* to transform these variables.","f397e351":"**Interpretation:**\n\nA house with paved access road is more valuable than one with gravel road. This makes sense in real world. So I will ordinally encode the variable.","bf0f6316":"There are only two houses build in 1892 and I list some important attributes associated with high sale price. By comparing the attributes, the extremely high price of house with index 185 is seemingly reasonable because of excellent quality and large area. So I will not drop the record.\n\n***Note: it is dangerous to rashly delete any outlier we suspect***\n\nNow, I decided to perform polynomial transformation to examine whether the transformation can better fit the data.","8668c109":"**Interpretation:**\n\nI did not find ordinality here. So I will convert it into dummy variables.","3b80a7be":"##### Fireplaces\n\nNumber of fireplaces","0894c737":"**Interpretation:**\n\nTotalBsmtSF is obviously positively correlated with the sale price of houses, and so it can be a good indicator of housing price. The seeming outlier is probably the single point with higher than 6000 square footage but only priced less than 200000. We examine other attributes of the house to determine whether it is an anomaly.","b3704b6f":"**Interpretation:**\n\n- No remarkable outliers can be found\n- As expected, a house that is more valuable that is renovated more recently","21d55cf2":"##### Exterior1st & Exterior2nd","b9b1f237":"### Covariation between Categorical Predictors and Target Variable\n\nSince there are a considerable number of categorical variables, I decided to first use Chi2sq to select the feaures with high chi-square, providing me with direction of analysis. I first examine these variables and then other similar ones.","39743a29":"##### GarageType\n\n- 2Types: More than one type of garage\n- Attchd: Attached to home\n- Basment: Basement Garage\n- BuiltIn: Built-In (Garage part of house - typically has room above garage)\n- CarPort: Car Port\n- Detchd: Detached from home\n- NA: No Garage","e078b16a":"#### Basement Variables\n\n- BsmtQual: Evaluates the height of the basement \n- BsmtCond: Evaluates the general condition of the basement\n- BsmtExposure: Refers to walkout or garden level walls\n- BsmtFinType1: Rating of basement finished area\n- BsmtFinType2: Rating of basement finished area (if multiple types)\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms","3cd6a2a1":"##### GarageFinish\n\n**Interior finish of the garage:**\n- Fin: Finished\n- RFn: Rough Finished\t\n- Unf: Unfinished\n- NA: No Garage","c5e99bf4":"**Interpretation:**\n\nThere is a monotomically increasing trend of housing price with the increase in total rooms, althrough the housing price suddenly reduces after 11.\n\nSo the current ordinal numbers are reasonable here.","5a7940d7":"**Interpretation:**\n\nBasically, more irregular houses enjoy higher prices, indicating that there is a positive association between the degree of being irregular and sale prices.","847bbc2f":"#### Utilities\n\n- AllPub: All public Utilities (E,G,W,& S)\t\n- NoSewr: Electricity, Gas, and Water (Septic Tank)\n- NoSeWa: Electricity and Gas Only\n- ELO: Electricity only","b2254f97":"### OntHot Encoding\nI will perform onehot encoding algorithm on categorical variables that did not show any ordinality. Also, I will drop one hot variables that do not have enough observations.","46279e2a":"Make Predictions","9b924e2e":"#### Geographical Variables\n\n- Alley\n- Neighborhood\n- Street\n- PavedDrived\n- Mszoning","905fb0c6":"**Interpretation:**\n\nA perfect ordinality can be seen in the variable.","ff64ed9e":"Now I just impute the remaining missing values with None","a01da0f2":"#### Kitchen Variables\n\n- KitchenAbvGr\n- KitchenQual","865c8508":"**Interpretation:**\n\nPaved road is more valuable than gravel road, but hosues without alley access are surprisingly more expensive than houses with gravel road. Since there is no ordinal ordering, I will not encode them as ordinal numbers.","bdf29d63":"### Predictor Variables\n\nIn the section, we split the predictors into categorical variables and continuous variables and discrete variables. Then, we inspect the distribution of variables of the three classes.\n\n#### Continuous Variables\n\nLet's make histograms for continuous variables to examine the distribution of features. If many features are not normally distributed, we might can transform them to be more Gaussian so that some linear models perform better when using these features.","ff2b1516":"**Interpretation:**\n\n- Renovated houses are less valuaed than houses without renovation. \n- The non-overlapped error bars of the two bars indicate that there is significant difference between the sale prices of renovated houses and old houses.","8f479587":"# Feature Engineering\n\nWe need to first contenate train set and test set so that we can perform transformations on them simultaneously.","7b6bf96b":"##### Porch Area\n\n**What is a porch?**\n\nA Porch is a structure extended from the main structure to form a shelter cover that is attached to the front entrance of the building or house. \n","604fcfab":"##### BsmtHalfBath\n\nBasement half bathrooms. Just as the case of full bathrooms, the variable should be positively correlated with the sale price.","6fc7ebc2":"### Binninng\n\nNeighborhood has 25 categories. It is not appropriate to directly convert it into dummy variables. So I want to conduct binning on it.","e87e4a17":"**Interpretation:**\n\n- No ordinality can be discovered in the `Foundation`\n- In right-hand table, the last two categories have fewer than 15 observations, meaning that they should be colleted together into one backoff bin.","4940c715":"**Intrepretation:**\n\n- No ordinality can be found in the variable\n- There are four categories that have less than 15 observations. I will accummlate such categories in one special bin because the small sample size is not enough to reliable estimate the housing price for the particular category.","f2b8d11c":"**Interpretation:**\n\nHouse with a tennis court is most expensive. There is no ordinality, so I just convert the variable into  five dummy variables.\n\nIn addition, the last three classes in the histogram are too small to be reliably estimated. Even if I collect the three categories into one special bin, it still has only 5 observations. So **I will drop the three categories.**","0efbad77":"**Interpretation:**\n\nThe greater number of bathrooms can make the house more expensive. But we need to notice that there is only one observation in the category of 3 bathrooms. **I will delete the observation because of its small size.**","bea9717b":"**Additional Research**\n\nBased on information from websites, I found that the total area of a house can be splitted into several parts just as the picture shows:\n\n<img src=\"https:\/\/www.gimme-shelter.com\/wp-content\/uploads\/2011\/09\/Total-Floor-Area-of-a-House.png\" width=\"300\">\n\nAssume that this is a house with two levels\n\nTotal Floor Area(Above-grade Living Area) = Living Area + First Floor + Second Floor + Addition\n\nTotal Area = floor area + garage area + porch areax\n\n\n**Note: garage area and porch area are not counted in floor area because they are not living space, and basement area is also excluded becuase it is not above-ground area.**","c8bcb321":"**Intrepretation:**\n\n- No ordinality exists in the variable\n- The housing price varies from neighborhood to neighborhood\n- Non-overlapping error bars tell us that the difference is significant\n- Juse a few houses correspond to the categories of last three categories, which are all below 15. I will **accumulates the counts of all rare categories in a special bin.**\n- The majority of houses are the top three classes.","16f5d000":"##### Heating\n\n- Floor: Floor Furnace\n- GasA: Gas forced warm air furnace\n- GasW: Gas hot water or steam heat\n- Grav: Gravity furnace\t\n- OthW: Hot water or steam heat other than gas\n- Wall: Wall furnace","544c9705":"When checking the other attributes of the house, the remarkable high price, compared with that of similar houses of overqual 4, seems to be not reasonable. I decided to remove the observation in data cleaning section.","99e6291f":"**Conclusion:**\n\nThese important determinants semmingly all indicate that the price does not make sense for $\\color{crimson}{{\\text{index 1299 and index 524.}}}$ **I will delete the two observations in Data Cleaning Section.**","8feb37ce":"## Feature Selection\n\nSince there are a number of variables correlated with each other, which means collinearity exists, I need to exclude some less important variables.","7c18d1bd":"# Model Selection\n\nBefore data modeling, we can eaisly find that existing variables are not on the same scale. Models that are the smooth functions of the input, such as linear regression or logistic regression, are affected by the scale of the input. To change the scales of features, I will conduct **robust feature scaling.**","1bcd0d5e":"##### TotRmsAbvGrd\n\nTotal rooms above grade (does not include bathrooms)","655d5d28":"**Intrepretation:**\n\nIn the right-hand table, a severely imbalanced distribution can be identified. Only one observation is in NoSewa, meaning that the categorical variable has basically zero variance. I will keep the variable in mind and delete it in data cleaning section.","7280ab6f":"**Interpretation:**\n\nThe error bars of the bars indicate that The housing price of class 1 differs significantly from that of class 2. Furthermore, class 0 and class 3 do not have sufficiently large enough sample, so they should be deleted.","81c754ae":"- **Fence**","0679ca9e":"These attributes strongly correlated with sale price all indicate that the house should be priced higher than the current price. I will consider the single point as an outlier.","c145411b":"As expected, houses with greater age has lower price.","02e7e10e":"To avoid the issue of bimodal distribution, I will not perform log transformation on the variable.","9a647d6c":"$\\color{crimson}{\\bf{\\text{Summary:}}}$\n\nI will raise the power of 2 to `YearBuilt` and `2ndFlrSF` to get rid of non-linear associations. Also, I will log transform `SalePrice` to deal with the issue of non-constant variance. ","9c715d3f":"**Intrepretation:**\n\n- Kitchen quality shows a logical sequence\n- The higher Kitchen quality results in a more valuable house\n- Ordinal encoding should be used","b115fc01":"### Age\n\nGiven the common sense that people make purchase decision based on the newest time of renovation, I decided to use `YearRemodAdd` to determine the age of houses.","97d66255":"## Impute Missing values of Variables","6d98bfa7":"###### BsmtExposure\n\n- Gd: Good Exposure\n- Av: Average Exposure (split levels or foyers typically score average or above)\t\n- Mn: Mimimum Exposure\n- No: No Exposure\n- NA: No Basement","9c7189d9":"split the full data into train and test","a8834b6e":"#### Electrical System\n\n- SBrkr: Standard Circuit Breakers & Romex\n- FuseA: Fuse Box over 60 AMP and all Romex wiring (Average)\t\n- FuseF: 60 AMP Fuse Box and mostly Romex wiring (Fair)\n- FuseP: 60 AMP Fuse Box and mostly knob & tube wiring (poor)\n- Mix: Mixed","5ce5871e":"##### FireplaceQu\n\n**Fireplace Quality:**\n\n- Ex: Excellent - Exceptional Masonry Fireplace\n- Gd: Good - Masonry Fireplace in main level\n- TA: Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n- Fa: Fair - Prefabricated Fireplace in basement\n- Po: Poor - Ben Franklin Stove\n- NA: No Fireplace","31b7716f":"First, let's get a big picture of the relationships of continuous predictors with target variable.","67642595":"**Interpretation:**\n\n- No ordinality can be identified\n- `2Types` and `CarPort` have fewer than 15 observations, so they will be collected in backoff bin.","974743f0":"##### Neighborhood\n\n- Blmngtn\tBloomington Heights\n- Blueste\tBluestem\n- BrDale\tBriardale\n- BrkSide\tBrookside\n- ClearCr\tClear Creek\n- CollgCr\tCollege Creek\n- Crawfor\tCrawford\n- Edwards\tEdwards\n- Gilbert\tGilbert\n- IDOTRR\tIowa DOT and Rail Road\n- MeadowV\tMeadow Village\n- Mitchel\tMitchell\n- Names\tNorth Ames\n- NoRidge\tNorthridge\n- NPkVill\tNorthpark Villa\n- NridgHt\tNorthridge Heights\n- NWAmes\tNorthwest Ames\n- OldTown\tOld Town\n- SWISU\tSouth & West of Iowa State University\n- Sawyer\tSawyer\n- SawyerW\tSawyer West\n- Somerst\tSomerset\n- StoneBr\tStone Brook\n- Timber\tTimberland\n- Veenker\tVeenker\n\nThere are 25 categories of neighborhood, so we need to engineer the variable to compress the number of categories.","1432d99e":"- **GarageYrBlt**\n\nNaN means no garage. I will test whether all houses with garages have the record of GarageYrBlt, GarageQual, GarageCond.","e7396a1a":"##### ExterQual\n\n- Ex\tExcellent\n- Gd\tGood\n- TA\tAverage\/Typical\n- Fa\tFair\n- Po\tPoor","a3e0d46c":"##### MSSubClass\n\n**Identifies the type of dwelling involved in the sale:**\n- 20: 1-STORY 1946 & NEWER ALL STYLES\n- 30: 1-STORY 1945 & OLDER\n- 40: 1-STORY W\/FINISHED ATTIC ALL AGES\n- 45: 1-1\/2 STORY - UNFINISHED ALL AGES\n- 50: 1-1\/2 STORY FINISHED ALL AGES\n- 60: 2-STORY 1946 & NEWER\n- 70: 2-STORY 1945 & OLDER\n- 75: 2-1\/2 STORY ALL AGES\n- 80: SPLIT OR MULTI-LEVEL\n- 85: SPLIT FOYER\n- 90: DUPLEX - ALL STYLES AND AGES\n- 120: 1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n- 150: 1-1\/2 STORY PUD - ALL AGES\n- 160: 2-STORY PUD - 1946 & NEWER\n- 180: PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n- 190: 2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nWe should understand these numbers as **different categories**, although these types of dwelling are encoded as numerical values.","457db6b7":"**Interpretation:**\n\n- After log transformation\/boxcox transformation, some variables how approximately normal distribution\n- There are some variables with bimodal distribution. Let's use Gaussian Mixture Modeling to add the cluster number for them.","93c24c03":"#### Exterior Variables\n\n- ExterQual: Evaluates the quality of the material on the exterior \n- ExterCond: Evaluates the present condition of the material on the exterior\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n\nThe covering categories are completely same for the two types of exterial covering.","e639c515":"**Conclusion:**\n\nGreater size of garage enjoys higer price, and this makes sense in real life. I also did not find conspicuous outlying observations for the variable.","a10b42f5":"- **Pool Quality**\n\nBased on the insight gained from EDA, we know that just a small proportion of hosues have pool. Therefore, the extremely high missing percentage makses senese here. I just need to impute these missing values with None, which means no pool.","05e37e2b":"**Interpretation:**\n\nNo ordinality can be found in this variable, so just convert the variable into several dummy variables.","c7270a61":"# Prepare Data for Machine Learning\n","8d8ee27b":"The low quality and old age can explain the low price of these houses.","372d8d0e":"\n##### LotShape","7c20c3b5":"##### Lot Area\n\n>To put it simply, a floor area is the size of the indoors area (inside the building walls) and the lot area is the whole size of the lot, i.e. including the yard up to the boundaries of the lot. \n\n","fb5d266c":"**Interpretation:**\n\nThe median housing price shows no significant difference in different years, but the seasonality can be observed in the right-hand bar chart. The housing price is higher in the period between February and March or in the period between July and September. I will keep it in mind that YrSold is not a good indicator of Housing price.\n\nAnother thing I notice is that the housing price does not show a monotomically incresing trend, so the number 12, which corresponds to December, will receive a higher weight than the number 9, which refers to September and has a higher median price. The higher weight does not make sense here. **So dummy variables should be a more robust representation of  these months.**","7d74b6ff":"The result shows that there are no duplicated observations.","dc651aac":"**Interpretation:**\n\nOrdinality exists in the variable.","e8b62271":"**Interpretation:**\n\n- A strong, positive curve can be easily identified in the plot, houses of higher quality has higher prices.\n- One single point, which is rated at 4, may be an outlier. ","93c96681":"If we use the test error of XGBoost as the baseline, the test error of aggregated models decreases from 0.057413 to 0.05203, a reduction of 0.005.","6bcc7ea8":"**Interpretation:**\n\nThe house that gets more exposure is more valuable, and the logical sequence ranging from no exposure to good exposure indicates ordinality. So I will assign ordinal numbers to these categories.","4660014e":"**Pool Quality**","47c591e9":"**Interpretation:**\n\n- The saleprice of houses is strongly correlated with the size of house. For example, total square feet of basement area and First Floor square feet have the same correlation coefficient, 0.61. Similarly, the Above grade (ground) living area square feet has 0.71 corrleaiton coefficient with the sale price of houses. \n- Also, `YearBuilt` and `YearRemodAdd` is also relatively, strongly correlated with sale price of houses. The fact that critical parts of the house, like plumbing, electrical, the roof, and appliances are recently remodeled and therefore less likely to break down, can generate savings for a buyer.\n- The quality of houses, `OverallQual`, is also a good indicator for housing prices.\n- There is collinearity between predictors.\n\n$\\color{crimson}{\\bf{\\text{Summary:}}}$\n\nOverallQual, Living Area, GarageArea, and YearBuilt are strong determinants of housing price.","3c3b3e8d":"##### PavedDrive\n\n**Paved Driveway:**\n- Y\tPaved \n- P \tPartial Pavement\n- N\tDirt\/Gravel","b144ea5a":"##### BldgType\n\n- 1Fam\tSingle-family Detached\t\n- 2FmCon\tTwo-family Conversion; originally built as one-family dwelling\n- Duplx\tDuplex\n- TwnhsE\tTownhouse End Unit\n- TwnhsI\tTownhouse Inside Unit","e6851676":"**Interpretation:**\n\nThe categories are ordinal. Given the small sample size of Ex, I will drop it.","8e1bd85b":"**WoodenDeck**","b5a5785c":"First, let's address the linearity issue. We can examine the change in residuals after **raising a power of 2 to the two variables showing non-linearity.**","80a1f6b5":"**Interpretation:**\n\nThe fitted line fits the data pretty well because the assciation between living area and Sale Price seems to be a strong, positive, and linear one, although there might be some remarkable outliers that can severly impact the fit.\n\nLet's filter these outlying records out and examine whether the low price make sense.","1e882db8":"##### HearingQC\n\n- Ex: Excellent\n- Gd: Good\n- TA: Average\/Typical\n- Fa: Fair\n- Po: Poor","87ab1c8f":"Apparently, the house having a pool is much more valuable.","3dd919ec":"##### Selling Variables\n\nAlthough the correlation matrix shows insignificant correlation coefficient of YrSold, our common sense tells us that the house prices vary from year to year, meaning that the **prices might be impacted by the economic environment.** \n\nAlso, I assume that the **housing price is also impacted by seasonability.** The two assumptions are reasonable in real world. Now let me verify the validity of the two assumptions.","cc9bf285":"**Garage Variables**\n\n- GarageFinish\n- GarageQual\n- GarageCond","c3953294":"**Interpretation:**\n\nAs expected, the number of fireplaces positively impacts the housing price. ","7b12e5db":"The polynomial regression obviously fit the data points better than normal linear regression. Given that the degree-3 polynomial transformation does not lead to a remarkable reduction in RSS, so degree of 2 is a good choice.","f33a6b87":"As expected, the distritbuion of SalePrice is basically normal now.","7b1c69ef":"**Interpretation:**\n\nA huge number of categories do not have enough observations. I will write a function to drop them after converting these variables into dummy variables.","af1c80b7":"No ordinality. Just convert the variable to dummy variables.","8dd8bd22":"**FireplaceQual**","1d98802f":"**Intrepretation:**\n\n- MSZoning. Based on a research, which states that housing in higher-density zones is expected to be lower priced than housing in low-density zones because of lower land costs, I think it's reasonable to assign unique number of ordering to these categories.\n- LotShap. LotShape is categorized as regular, slightly irreugular, moderately irregular, and irregular. It is easily found that there is a logical ordering. So I decided to encode them as ordinal numbers.\n- LandSlope. LandSlope has three categories, Gentle Slope, Moderate slope, and Severe Slope. There is an obvious ordering involved in the variable. So we can encoder it as ordinal categorical variable.\n- ExternalQual & ExterCond. The variable evaluates the quality of exterior material from poor to excellent. So this is also an ordinal categorical variable.\n- BsmtQual. Ex(excellent 100+inches) > Gd(Good 90-99inches) > TA(Typical 80-89inches) > FA(Fair 70-79inches) > Po(Poor <70inches)\n- BsmCond. The variable means the condition of basement. The ordering of basement condition is Gd > Ta > Fa > Po.\n- BsmtExposure. BsmtExposure refers to walkout or garden level walls and there is a logical sequence in these categories.\n- BsmtFinType1 refers to rating of basement finished area. GLQ > ALQ > BLQ > Rec > LWQ > Unf > NA.\n- HeatingQC. Excellent > Good > Average > Fair > Good.\n- KitchenQual. The kitchen quality. Ex > Gd > Ta > Fa > Po.\n- Functional. Home functionality and the ordering is Typ >  Min1 > Min2 > Mod > Maj1 > Maj2 > Sev > Sal.\n- FireplaceQu. Fireplace Quality and the ordering is Ex > Gd > TA > Fa > Po > NA.\n- GarageQual and GarageCond have the same logical sequence.\n- PoolQC. Pool quality and the logical sequence is Ex > Gd > TA > Fa > Po > NA.\n- Fence. Fence quality is mainly correlated with the degree of privacy. Fence with higher quality has good privacy. So the logical order is GdPrv (Good Privacy)>  MnPrv(Minimum Privacy) > GdWo > MnWw > NA.\n\n**Summary:**\n\nI will conduct ordinal encoding on all of these variables, and I also notice that some categorical variables have severely skewed distribution, meaning that there are some rare categories that can cause noises. They need to be specialy treated.","5949643e":"##### Lot Configuration","6525529a":"**Interpretation:**\n\nThe relationship between TotalPorchArea and SalePrice is not very strong, even if we combine them together.","69b7b8d1":"**Interpretation:**\n\nWe can find ordinality here, and I will delete the last two categories.","67432ad6":"Through a quick look at the descriptive results, we can summarize some findings:\n\n- When checking the minimal values and maximal values of each variable, we can easily find that for some variables, such as `LotArea` and `BsmtFinSF1`, values can span several orders of magnitude. The spanning suggests that we might need to perform some transformation on these variables to get rid of **the heavy-tail distribution.**\n- Then the scale across variables are obviously different, with some variables being very small and others being very large. The different orders of magnitude can wreak havoc in leanring methods, so feature scaling might need to be implemented.","3b39c523":"##### CentralAir\n\n**Central air conditioning:**\n- N\tNo\n- Y\tYes","173a2cc9":"**Interpretation:**\n\nSince most of values of the two variables are 0, so they still have severe skewness and show bimodal distribution. I will consider deleting the two variables.","211ac5a5":"**Interpretation:**\n\nJust to my expectation, only very few houses have pool. In the case, it is not reasonable to represent the variable in the form of being continuous. Instead, I will convert the variable to a binary variable to indicate whether the house has a pool.","ff6f183b":"##### Garage Area","cd7a5f45":"**Interpretation:**\n\nThere is positive, relatively strong relationshio between wooddeck and saleprice: larger area of wood deck can increase the housing price.","8a65e1f0":"**Intrepretation:**\n\nAs expected, houses with higher quality enjoy higher sale price. The positive relation, combined with ordinality, means that ordinal encoding should be conducted on the variable.\n\n**Note: houses without basement have the lower median price. So None will be encoded as 0.**","4ed85c55":"#### MiscFeature\n\n**Miscellaneous feature not covered in other categories:**\n- Elev: Elevator\n- Gar2: 2nd Garage (if not described in garage section)\n- Othr: Other\n- Shed: Shed (over 100 SF)\n- TenC: Tennis Court\n- NA: None\n\nMissing values mean that the variable does not have these miscelleneous features, so I will convert these missing values into None.","0abb677b":"#### Lot Variables\n\n- LotShape\n- LotCongfig","2b361598":"**Intrepretation:**\n\nThe last two classes in the right-hand table have very few observations. The rare categories should be treated specially, so I will either collect them in backoff bin or drop them.","99ca863f":"A considerbale number of observations do not have miscellaneous features. There is no obvious pattern, so I will just exclude the variable.","79755b1a":"**Interpretation:**\n\n- I will convert categories whose median price is below the blue line as 0\n- `SawyerW`, `Gilbert`, `NWAmes`, `Blmngtn`, `CollgCr`, `ClearCr`, `Crawfor`, `Veenker`, `Somerst`, and `Timber`  will classified into to the same group. I will assign 1 to the group.\n- The last three categories will be collected into one group.","139afa8c":"### Impute Remaining Variables","76bff663":"## Variation within Variables\n\nIn this section, I will first explore the target variable first and then all other predictor variables.8\n\n### Target Variable\n\nFor target variable, I will examined the distribution of it by histogram and QQ-plot(quantile-quantile plot).","ee2d3e23":"Some houses with a pool do not have the quality records. I think we can impute the three values based on other relevant attributes. Let's examine other attributes of these houses that are correlated with quality.","6102412f":"#### Foundation\n\n**Type of Foundation:**\n- BrkTil: Brick & Tile\n- CBlock: Cinder Block\n- PConc: Poured Contrete\t\n- Slab: Slab\n- Stone: Stone\n- Wood: Wood","3a9a9cb1":"**Interpretation:**\n\nI did not find ordinality here. So I will convert it to dummy variables.","5fe8f7cb":"**Interpretation:**\n\nThere is a strong, positive **non-linear** relationship between the construction Year of houses and sale price. From the figure, this is more like a three-degree of polynomial association. I did not notice any obvious outlier. The point built around 1891 and valued at almost $500K may be an outlier, but maybe the price is impacted by other attributes of the house and lead to the high price. ","3fdbcf83":"#### Roof Variables\n\n- RoofStyle\n    - Flat\n    - Gable\n    - Gambrel\tGabrel (Barn)\n    - Hip\n    - Mansard\n    - Shed\n- RoofMat1\n    - ClyTile\tClay or Tile\n    - CompShg\tStandard (Composite) Shingle\n    - Membran\n    - Metal\n    - Roll\n    - Tar&Grv\tGravel & Tar\n    - WdShake\tWood Shakes\n    - WdShngl\tWood Shingles","43927f40":"**Basement Variables**\n\n- BsmtQual\n- BsmtExoposure\n- BsmtCond","b04cd938":"Each house may  have only one or more than one of the four kinds of porch. Some houses have both the porch and deck, which is a type of porch outside the hosue. We can combine them together into one variable indicating the square footage of a house. **By doing this, I want to create a more powerful variable to indicate the area of porch.**","61608820":"#### Fence\n\n- GdPrv\tGood Privacy\n- MnPrv\tMinimum Privacy\n- GdWo\tGood Wood\n- MnWw\tMinimum Wood\/Wire\n- NA\tNo Fence","4738e2b9":"#### Sale Variables\n\n- SaleType\n- SaleCondition","8a6512ca":"In the section, I will clean the dataset based on the insighst gained from EDA.\n\n\n- Missing values of variables\n- Duplication of variables\n- Outliers in variables\n","17c974fa":"**LotShape**","f6c35b1d":"**Intrepretation:**\n\nThe two types of configuration of `Cul-de-sac` and `frontage on 3 sides of property` have the above-median house prices, while the median price of other three types are very close to the median level of all houses. I will perform one-hot encoding on the the variable to include them in regression model.\n\nIn addition, FR3 does not have enough samples. It should be deleted.","e3c7e81f":"#### Size Variables\n- Total basement square footage\n- First floor square footage\n- Second floor square footage\n- Above grade living area\n- Garage area\n- Pool area\n- Porch area\n- WoodDeckSF\n\nThe size variables mainly describe the area of a house, such as the kitchen area, garage area, and living area. The value of a home is roughly estimated in price per square foot \u2014 the sales price divided by the square footage of the home. So the bigger house should be more valuable.","e625e6ee":"$$\\bf{\\text{Kaggle Project of Predicting Housing Prices}}$$\n\n**Author: Ray Sun**\n\n**Date: 28\/11 2021**\n\n**Github: [rayhezack](https:\/\/github.com\/rayhezack)**\n___\n\n**Background Info**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Framework:**\n\n- Data Loading\n- Exploratory Data Analysis\n- Data Cleaning\n- Feature Engineering\n- Model Seleciton\n- Model Evaluation\n- Prediction","22de4e33":"##### Second Floor Square Foot","a9deb598":"Let's see the feature importance of each variable in Random Forest","a6e6c1a5":"Based on insights gained from EDA, I will directly drop the outlying observations I found in EDA. Let's filter them out.","f48a17b5":"##### BedroomAbvGr\n\nBedrooms above grade (does NOT include basement bedrooms)","2b417ff4":"- **LotFrontage**\n\nFor the variable, I decided to use the median\/mean lot frontage of relevant lot categorical variables to impute the missing values.\n\nFirst, let's see the distribution of missing values across the two lot variables.","84c5bf2f":"#### Discrete Variables.","e197ac51":"## Data Modelling","1a5eb300":"**Interpretation:**\n\nI did not find ordinality here. So I will convert it into dummy variables.","bb1734aa":"#### MiscVal\n\nValue of miscellaneous feature","5994dae5":"We take the average of the four models: **Lasso, Elastic Net,Gradient Boosting, and XGBoost**\n\n**Note: the following code is quoted from another author:[Seigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling)**","d42fa712":"First, Let's see the residual plots of each predictor vursus SalePrice.","67f688db":"**Intrepretation:**\n\nTo my expectation, a house will be more valuable if it has a good fireplace. The sample size for each category is also higher than the specified threshold. So I will directly ordinally encod the variable.","42db4a65":"**Interpretation:**\n\n- No ordinality can be found in the variable\n- The last four categories in right-hand table do not have enough sample size, which is way lower than the threshold 15, so I will accummlate them in backoff bin","a43318a6":"**Data Modeling**\n\n**Note: I did not present the process of grid search, which takes time to run.**","a4b851f6":"Train Models","a8ef2589":"## Covariation between Variables\n\nNow, I begin to inspect the relationships between variables so as to find out useful variables highly correlated with target variable. And my examination mainly focuses on two aspects:\n\n- covariation between predictor and the target variable\n- covariation between predictor variables","d6cb0deb":"##### BsmtFinType1\n\n**Rating of basement finished area:**\n- GLQ: Good Living Quarters\n- ALQ: Average Living Quarters\n- BLQ: Below Average Living Quarters\t\n- Rec: Average Rec Room\n- LwQ: Low Quality\n- Unf: Unfinshed\n- NA: No Basement","793d558f":"**Interpretation:**\n\nBasically, the pool quality will be very high if the overall quality gets a high value. For instance, the pool quality is Excellent for houses that get rating between 8 and 10. So I personally assume the following mapping relation:\n\n- Pool quality will be excellent, if rating of overall quality ranges from 8 to 10\n- Pool quality will be good, if rating of overall quality ranges from 6 to 7\n- Pool quality will be average, if rating of overall quality ranges from 4 to 6\n- Pool qualitu will be 1 to 3, if rating of overall quality ranges from 1 to 3","dbdf5630":"#### Quality Variables\n\n- Total Quality of Houses\n- Quality of Basement\n- Kitchen Quality\n- Fireplace Quality\n- Fence Quality","783c29cc":"**Interpretation:**\n\nA house in a better condition is more valuable. Ordinal encoding should be performed on the varibale.\n\nAlso, `Po` has only two observations. **It should be deleted.**","3d9b721a":"**Interpretation:**\n\nThe top three important variables are all about quality, followed by Neighborhood. I decided to examine the top 20 categorical variables.","63b786f0":"**Interpretation:**\n\nPoolQC has as much as 99 percent of observations being missing from the dataset. Therefore, I will first impute the missing values of it.","40154af2":"**Conclusion:**\n\nThe polynomial transformation indeed provides a better fit based on the smaller RSS and the fitted line that follows data points closer than before.","8976c084":"To impute it, I need to know the general year of construction for Detchd Type.","494fe4c3":"### Total Bath Rooms","2ff9469b":"##### LandContour\n\n**Flatness of the property:**\n- Lvl: Near Flat\/Level\t\n- Bnk: Banked - Quick and significant rise from street grade to building\n- HLS: Hillside - Significant slope from side to side\n- Low: Depression","5ec6b663":"#### Land Variables\n\n- LandShape\n- LandContour","e3549673":"**Interpretation:**\n\n- A range of variables show binmodal or multimodel distributions, such as `YearBuilt`, `YearRemodAdd`, `2ndFirSF`, and `GarageYrBit`, and the distribution violates the assumption of linear models. I decided to perform Gaussian Mixture Modeling to transform these varaibles.\n- There are also some variables, such as `LotArea` and `MasVnrArea`, that are positively skewed distributed. The heavy-tailed distribution can be transformed by log transform.","f4dc40d8":"##### BsmtFinType2\n\n**Rating of basement finished area (if multiple types):**\n- GLQ: Good Living Quarters\n- ALQ: Average Living Quarters\n- BLQ: Below Average Living Quarters\t\n- Rec: Average Rec Room\n- LwQ: Low Quality\n- Unf: Unfinshed\n- NA: No Basement","0134df37":"The variable is better than just one GrLivArea or TotalBsmtSF. I will include the variable in model and exclude GrLivArea and TotalBsmtSF.","f9176d59":"#### Garage Variables\n\n- GarageType\n- GarageFinish\n- GarageQual\n- GarageCond\n- Garagecars","2514cf2a":"##### SaleCondition\n\n- Normal\tNormal Sale\n- Abnorml\tAbnormal Sale -  trade, foreclosure, short sale\n- AdjLand\tAdjoining Land Purchase\n- Alloca\tAllocation - two linked properties with separate deeds, typically condo with a garage unit\t\n- Family\tSale between family members\n- Partial\tHome was not completed when last assessed (associated with New Homes)","3cd563f9":"##### GarageCars\n\nSize of garage in car capacity","8c2228b4":"##### ExterCond\n\n- Ex\tExcellent\n- Gd\tGood\n- TA\tAverage\/Typical\n- Fa\tFair\n- Po\tPoor","ecb97900":"**Interpretation:**\n\nthere is a weak positive association between Lot Area and SalePrice. The association is severly impacted by outliers with extremely huge size of lot.\n\nLet's try log transformation on LotArea and inspect the association again.","3b992838":"**Interpretation:**\n\n`3SsnPorch`, which is three season porch area in square feet, has majority of values centering aound 0 but just very few observations scattering over the other categories. The same pattern can be viewed in `PoolArea`. These categories are called rare categories. One way to deal with this is through *back-off method*, a simple technique that accumulates the counts of all rare categories in a special bin. If the count is greater than a certain threshold, then the category gets its own count statistics. Otherwise, we use the statistics from the back-off bin.","9604a9ac":"##### Is New?\n\nThere are also a number of houses whose year of selling equal to the year of construction, meaning that the house is completely new when being sold to people. **My hunch is that these completely new houses should priced higher.**","2e73233a":"**Interpretation:**\n\n`LotFrontage`, `LotArea`, `MasVnrArea`, `BsmtFinSF1`, `BsmtFinSF2`, `BmstUnfSF`, `1stFlrSF`, `2ndFlrSF`, `LowQualFinSF`, `GarageArea`, `TotalPorchSF`, `TotalLivArea`, `house_age` all show skewed distribution.\n\n**Note: I do not list WoodDeckSF, OpenPorchSF, EclosedPorch, and ScreenPorch because they can be represented by TotalPorchSF.**","5224cf7c":"**External Variables**\n\n- External Quality\n- External Condition","524c587f":"### Linearity & Homoscedasticity","3da53ae3":"**Interpretation:**\n\n- `YearBuilt` and `YearRemodAdd` are relatively correlated with each other. Also `house_age` is determined by substracting the `YearRemodAdd` from `YrSold`, meaning they are perfectly correlated. In the case, I will drop `YearRemodAdd`.\n- `BsmtFinSF2` and `LowQualFinSF` are barely correlated with target variable. So I also drop them.\n- `HavePool` are derived from `PoolArea`. I will drop the latter because it is less correlated with the response variable.\n- `GarageYrBuilt` is strongly correlated with `Log_GarageArea`. I will drop `GarageYrBuilt` based on the same principle."}}