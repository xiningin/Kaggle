{"cell_type":{"719bb4d4":"code","753953cf":"code","9366c4a5":"code","9ae3e895":"code","01a43c7d":"code","4c7cf511":"code","1200d0ca":"code","ed435c63":"code","385432dd":"code","af9f838c":"code","1f48166e":"code","ebf1b094":"code","8ca71a5b":"code","97f97232":"code","5a168e10":"code","6327e82e":"code","1e2db1e3":"code","7a61921c":"code","0f8d7e84":"code","a890d7fa":"code","aa1e24d2":"code","dacba12a":"code","630a0d16":"code","27fbd870":"code","d0ec9ddc":"markdown","8becabd5":"markdown","c5314ff0":"markdown","7587296c":"markdown","5f887b9d":"markdown","55aa4c99":"markdown","564f3118":"markdown","e36896b6":"markdown","05763610":"markdown","22660f40":"markdown","51d0d707":"markdown","1ad897e5":"markdown","63c49391":"markdown","3612e49f":"markdown","d746432d":"markdown","7dd91edb":"markdown"},"source":{"719bb4d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","753953cf":"# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport pandas as pd","9366c4a5":"fashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","9ae3e895":"class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","01a43c7d":"train_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0","4c7cf511":"plt.figure(figsize=(10,10))\nfor i in range(10):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()","1200d0ca":"modeladm = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])\nmodelrms = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])\nmodeladgrad = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])\nmodelsgd = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])\nmodelsgdm = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])","ed435c63":"modeladm.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodelrms.compile(optimizer='RMSprop',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodeladgrad.compile(optimizer='adagrad',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodelsgd.compile(optimizer='SGD',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","385432dd":"opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\nmodelsgdm.compile(optimizer=opt,\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","af9f838c":"hist4=modelsgd.fit(train_images, train_labels, epochs=10)","1f48166e":"hist5=modelsgdm.fit(train_images, train_labels, epochs=10)","ebf1b094":"hist3=modeladgrad.fit(train_images, train_labels, epochs=10)","8ca71a5b":"hist2=modelrms.fit(train_images, train_labels, epochs=10)","97f97232":"hist1=modeladm.fit(train_images, train_labels, epochs=10)","5a168e10":"ep=np.arange(1,11,1)","6327e82e":"acc1=hist1.history['accuracy']\nacc2=hist2.history['accuracy']\nacc3=hist3.history['accuracy']\nacc4=hist4.history['accuracy']\nacc5=hist5.history['accuracy']\nlist_of_tuples = list(zip(ep,acc1,acc2,acc3,acc4,acc5)) ","1e2db1e3":"df = pd.DataFrame(list_of_tuples, columns = ['Epoch', 'Adam','RMS','Adagrad','SGD','SGDM']) ","7a61921c":"df.index = df['Epoch']\ndf.plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(.7, .95) # set the vertical range to [0-1]\nplt.show()","0f8d7e84":"modeladm.evaluate(test_images, test_labels)","a890d7fa":"modelrms.evaluate(test_images, test_labels)","aa1e24d2":"modeladgrad.evaluate(test_images, test_label)","dacba12a":"modelsgd.evaluate(test_images, test_labels)","630a0d16":"modeladm.evaluate(test_images, test_labels)","27fbd870":"hist1=modeladm.fit(train_images, train_labels, epochs=15)","d0ec9ddc":"# Momentum\n* It smoothens the noise gradient by taking an moving average of previous values\n* The moving average is an weighted moving average, where the average gradually decays","8becabd5":"# Plotting few sample values","c5314ff0":"# Assigning class labels","7587296c":"# Adam\n* Cobmines both RMSProp and Momentum\n* RMSProp decays the learning rate but not as fast as adagrad\n* Momentum smotthens the gradients","5f887b9d":"# SGD\n* Calculates gradients on observarions not all\n* Needs less memory and less prone to get stuck in local minima\n* Noisy gradient","55aa4c99":"# RMSProp\n* Adagrad gradually reduces the learning rate \n* RMSProp slows down this rate of decay","564f3118":"# Validating over test set","e36896b6":"# Importing Library","05763610":"# Plotting the accuracy over epochs","22660f40":"# Compiling the model with diffrent optimizers","51d0d707":"# Adgrad\n* It changes the learning rate automaticaally, so do not need to tune this\n* It automatically reduces learning rate as moves toward minima","1ad897e5":"# Creating the models","63c49391":"# Fitting all the models","3612e49f":"# Importing dataset","d746432d":"# Standarzing input","7dd91edb":"# Continuing with the best model"}}