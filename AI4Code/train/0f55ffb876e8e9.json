{"cell_type":{"4da222bf":"code","c6a5a10a":"code","95834065":"code","f350bad1":"code","ea12c93e":"code","9c7712a3":"code","b5a1075e":"code","16b1b106":"markdown","7acec70e":"markdown","9cb5d4f2":"markdown","f07739d5":"markdown","ffd669ad":"markdown","72dc2733":"markdown","5a70ae61":"markdown","6fa2957a":"markdown","c320fc5f":"markdown","b32e0f0a":"markdown"},"source":{"4da222bf":"# plotting\nimport seaborn as sns\n# punctuation\nfrom string import punctuation\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nsns.reset_defaults()\nsns.set(\n    rc={'figure.figsize':(4,3)}, \n    style=\"white\" # nicer layout\n)","c6a5a10a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","95834065":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# database.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/database.csv', delimiter=',', nrows = nRowsRead, header = None)[1].to_frame()\ndf1.dataframeName = 'database.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f350bad1":"df1.head(5)","ea12c93e":"tongue_twisters_str = open('\/kaggle\/input\/database.txt', 'rb').read().decode(encoding='utf-8')\n# Splitting it by the separator, i.e., |\ntongue_twisters = tongue_twisters_str.split('|')\nprint(\"\\n\".join(tongue_twisters[5:8]))","9c7712a3":"# We'll use counter to get the most frequently occuring words\nfrom collections import Counter \n# Join all the tongue twisters\nstri = \" \".join(tongue_twisters).lower()\n# Splitting every word\nwords = stri.split()\n# Removing words below 4 letters, just for clarity\nfor word in words:\n    if len(word) < 4 or (word in punctuation):\n        words = list(filter(lambda a: a != word, words))\n# Providing this to the Counter\nCounter = Counter(words) \n  \n# most_common() produces k frequently encountered \nmost_occur = Counter.most_common(7) \n_ = sns.barplot(y=[x[0] for x in most_occur],x=[x[1] for x in most_occur],orient='h')","b5a1075e":"least_occur = Counter.most_common()[:-7-1:-1]\n_ = sns.barplot(y=[x[0] for x in least_occur],x=[x[1] for x in least_occur],orient='h')","16b1b106":"## Exploratory Analysis\nLet's us matplotlib to do an exploratory analysis on this data, and know more about these tongue twisters","7acec70e":"This dataset is provided in three formats, \n* TXT format\n* CSV format\n* `npy` format","9cb5d4f2":"### Word Analysis\nLike with most text datasets, let's do a word analysis and find the most common words in the data.                    \nJust for convenience, let's explore this using the TXT file given...","f07739d5":"### Quite expected, wasn't it?\nWords like could, that, with seem to dominate most datasets.                      \nOkay, now let's see the *least* common","ffd669ad":"## Let's use the CSV file, `kaggle\/input\/database.csv`","72dc2733":"Let's take a quick look at what the data looks like:","5a70ae61":"Hmm... Interesting. Now let's see the most common words in the dataset, and plot them...","6fa2957a":"Well who even uses the word `yeti` in a normal conversation?? \ud83e\udd26\u200d\u2642\ufe0f","c320fc5f":"## Conclusion\nDone! We have finished our (brief) study of the Tongue Twister dataset.              \nI am genuinely hoping for someone to get brilliant insights from this. Will it be you?","b32e0f0a":"## Introduction\nHello, fellow Data Scientist! This is a notebook to get you started with the Tongue Twister dataset.                         \nThis dataset consists of around 600 sentences that are linguistically hard to pronounce.            \n##### Lets get started!"}}