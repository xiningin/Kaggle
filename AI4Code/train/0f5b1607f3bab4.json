{"cell_type":{"fb9d489a":"code","ed94b294":"code","cfde249b":"code","872a5d80":"code","d7ae0323":"code","4ff5d744":"code","78797c3f":"code","efb7612b":"code","b03b35ab":"code","4b3533dc":"code","ec22286f":"code","b0246651":"code","d2b7e797":"code","6a959d49":"code","c9fc2a52":"code","fa6d7d77":"code","6983663c":"code","af1276e0":"code","8af27e2b":"code","f12daf0e":"code","8dd01725":"code","181c7dbe":"code","70d0ad8b":"code","cc71787c":"code","a64842e8":"code","049ad1b4":"code","506782fb":"code","ac26ceb4":"code","f5db3c00":"code","9d6cea4e":"code","4b8ea283":"code","3ef69f21":"code","d764738e":"code","20c4ae35":"code","e7d0057b":"code","5bf72c79":"code","bdcdbaee":"markdown"},"source":{"fb9d489a":"#imported important libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","ed94b294":"#reading the dataset\ndf = pd.read_csv('..\/input\/Training.csv')","cfde249b":"df.head()","872a5d80":"#They are 4920 rows, 133 columns\ndf.shape","d7ae0323":"#seeing any null values are there with descending format\ndf.isnull().sum().sort_values(ascending=False)","4ff5d744":"df.columns","78797c3f":"#looking how much percent each diseases having\ndf['prognosis'].value_counts(normalize = True)","efb7612b":"#as we can see each no. diseases having the same percentage through bar chart\ndf['prognosis'].value_counts(normalize = True).plot.bar()\nplt.subplots_adjust(left = 0.9, right = 2 , top = 2, bottom = 1)","b03b35ab":"#checking if there are any other data types\ndf.dtypes.unique()","4b3533dc":"#Analyzing each symptoms\/variable\nfor x in range(df.shape[1]):\n    plt.subplot(7,22,x+1)\n    plt.subplots_adjust(left = 0.5, right = 16 , top = 10, bottom = 0.5)\n    sns.countplot(df[df.columns[x]]).set_title(df.columns[x],fontsize=23)","ec22286f":"#checking the relationship between the variables by applying the correlation \ncorr = df.corr()\nmask = np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nplt.subplots_adjust(left = 0.5, right = 16 , top = 20, bottom = 0.5)\nsns.heatmap(corr, mask=mask,vmax=.9, square=True,annot=True, cmap=\"YlGnBu\")","b0246651":"#took two high correlation variables and analysing if it is satisfying null hypothesis or alternate hypothesis\npd.crosstab(df['cold_hands_and_feets'],df['weight_gain'])","d2b7e797":"#imported the chi square contingency\nfrom scipy.stats import chi2_contingency","6a959d49":"#as p value is  0.0  which is less than 0.05 then they are actually different from each other which satisfy the alternate hypothesis \nchi2_contingency(pd.crosstab(df['cold_hands_and_feets'],df['weight_gain']))","c9fc2a52":"#seperated the independent and dependent values to repective variables \nx = df.drop(['prognosis'],axis =1)\ny = df['prognosis']","fa6d7d77":"from sklearn.model_selection import train_test_split","6983663c":"#divided into testing and training\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","af1276e0":"#imported naive_baye algorithm\nfrom sklearn.naive_bayes import MultinomialNB","8af27e2b":"#fitted the model\nmnb = MultinomialNB()\nmnb = mnb.fit(x_train, y_train)","f12daf0e":"from sklearn.metrics import accuracy_score","8dd01725":"y_pred = mnb.predict(x_test)","181c7dbe":"accuracy_score(y_pred,y_test)","70d0ad8b":"#by cross validating we got mean also 100%\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(mnb, x_test, y_test, cv=3)\nprint (scores)\nprint (scores.mean())","cc71787c":"real_diseases = y_test.values","a64842e8":"#for the cross checking purpose i want to see if predicted values and actual values are same else it gives me worng prediction \nfor i in range(0, len(real_diseases)):\n    if y_pred[i] == real_diseases[i]:\n        print ('Pred: {0} Actual:{1}'.format(y_pred[i], real_diseases[i]))\n    else:\n        print('worng prediction')\n        print ('Pred: {0} Actual:{1}'.format(y_pred[i], real_diseases[i]))","049ad1b4":"#imported Kfold\nfrom sklearn.model_selection import KFold","506782fb":"## Function to run multiple algorithms with different K values of KFold.\ndef evaluate(train_data,kmax,algo):\n    test_scores = {}\n    train_scores = {}\n    for i in range(2,kmax,2):\n        kf = KFold(n_splits = i)\n        sum_train = 0\n        sum_test = 0\n        data = df\n        for train,test in kf.split(data):\n            train_data = data.iloc[train,:]\n            test_data = data.iloc[test,:]\n            x_train = train_data.drop([\"prognosis\"],axis=1)\n            y_train = train_data['prognosis']\n            x_test = test_data.drop([\"prognosis\"],axis=1)\n            y_test = test_data[\"prognosis\"]\n            algo_model = algo.fit(x_train,y_train)\n            sum_train += algo_model.score(x_train,y_train)\n            y_pred = algo_model.predict(x_test)\n            sum_test += accuracy_score(y_test,y_pred)\n        average_test = sum_test\/i\n        average_train = sum_train\/i\n        test_scores[i] = average_test\n        train_scores[i] = average_train\n        print(\"kvalue: \",i)\n    return(train_scores,test_scores)   ","ac26ceb4":"from sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier()\nnb = MultinomialNB()\nfrom sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='entropy',)\nfrom sklearn.ensemble import RandomForestClassifier\nran = RandomForestClassifier(n_estimators = 10)","f5db3c00":"algo_dict = {'l_o_g':log,'d_t':dt,'r_a_n':ran,'N_B' : nb}\nalgo_train_scores={}\nalgo_test_scores={}","9d6cea4e":"#decision tree was found to be best fit with training score of 0.1 and testing score of 0.87 with k value of 2 in the k fold cross validation. All the other algorithm seems to be overfit.\nmax_kfold = 11\nfor algo_name in algo_dict.keys():\n    print(algo_name)\n    tr_score,tst_score = evaluate(df,max_kfold,algo_dict[algo_name])\n    algo_train_scores[algo_name] = tr_score\n    algo_test_scores[algo_name] = tst_score\nprint(algo_train_scores)\nprint(algo_test_scores)","4b8ea283":"df_test = pd.DataFrame(algo_test_scores)\ndf_train = pd.DataFrame(algo_train_scores)","3ef69f21":"df_test.plot(grid = 1)\nplt.show()\nplt.grid()","d764738e":"#building the model at k value 2 \ntest_scores={}\ntrain_scores={}\nfor i in range(2,4,2):\n    kf = KFold(n_splits = i)\n    sum_train = 0\n    sum_test = 0\n    data = df\n    for train,test in kf.split(data):\n        train_data = data.iloc[train,:]\n        test_data = data.iloc[test,:]\n        x_train = train_data.drop([\"prognosis\"],axis=1)\n        y_train = train_data['prognosis']\n        x_test = test_data.drop([\"prognosis\"],axis=1)\n        y_test = test_data[\"prognosis\"]\n        algo_model = dt.fit(x_train,y_train)\n        sum_train += dt.score(x_train,y_train)\n        y_pred = dt.predict(x_test)\n        sum_test += accuracy_score(y_test,y_pred)\n    average_test = sum_test\/i\n    average_train = sum_train\/i\n    test_scores[i] = average_test\n    train_scores[i] = average_train\n    print(\"kvalue: \",i)  ","20c4ae35":"print(train_scores)\nprint(test_scores)","e7d0057b":"#saved the model \nfrom sklearn.externals import joblib\njoblib.dump(dt,'my_model_for_healthcare')","5bf72c79":"a = list(range(2,134))\ni_name  = (input('Enter your name :'))\ni_age = (int(input('Enter your age:')))\nfor i in range(len(x.columns)):\n    print(str(i+1+1) + \":\", x.columns[i])\nchoices = input('Enter the Serial no.s which is your Symptoms are exist:  ')\nb = [int(x) for x in choices.split()]\ncount = 0\nwhile count < len(b):\n    item_to_replace =  b[count]\n    replacement_value = 1\n    indices_to_replace = [i for i,x in enumerate(a) if x==item_to_replace]\n    count += 1\n    for i in indices_to_replace:\n        a[i] = replacement_value\na = [0 if x !=1 else x for x in a]\ny_diagnosis = dt.predict([a])\ny_pred_2 = dt.predict_proba([a])\nprint(('Name of the infection = %s , confidence score of : = %s') %(y_diagnosis[0],y_pred_2.max()* 100),'%' )\nprint(('Name = %s , Age : = %s') %(i_name,i_age))","bdcdbaee":"#Need to incorporate adaptive questioning here. Please refer Webmd.com to understand the flow."}}