{"cell_type":{"f6dc51bf":"code","16c82a6d":"code","4340f63d":"code","d53c8886":"code","bd376a11":"code","4d3afd7f":"code","029bd942":"code","cd74e098":"code","64e2656a":"code","427973dc":"code","d2e67428":"code","19c85652":"code","022f084c":"code","1f92cd5d":"code","ce74d4f6":"code","ced7465f":"code","8ddf63fb":"code","527aea50":"code","88eff04e":"code","8bb6ed1f":"code","69d73aa7":"code","e74a51db":"code","6a3fe0f5":"code","801c4f32":"code","bfb4ffb4":"code","423cffd8":"code","2e6c71ef":"code","e624547e":"code","4a5bec8c":"code","8c21aa95":"code","2e48f8c8":"code","9d49ed74":"code","cda548cf":"code","f28671d8":"code","4c1d205a":"code","3b8ffc5b":"code","a85c30a0":"code","f27ff81e":"code","7dd3f852":"code","a4ad32b0":"code","27b0c079":"code","18ac7cf2":"code","8dbe0d4e":"code","3214c83c":"code","28903996":"code","78b65996":"code","9c60d784":"code","b19c6831":"code","85e9b523":"code","43f80ace":"code","63a5d00b":"code","4f0a62c7":"code","50334197":"code","09487b34":"code","83e11f52":"code","4c4489b4":"code","aa80e5d2":"code","ac748c61":"code","cd7f5b29":"code","24b69c3a":"code","919140ad":"markdown","5676bc96":"markdown","1fe9bb8d":"markdown","12dfd5b3":"markdown","f7677189":"markdown","62c90d4f":"markdown","328e89a6":"markdown","4b22897a":"markdown","8c5963de":"markdown","2ecba15a":"markdown","93e4a25b":"markdown","d937fecb":"markdown","a5757d2e":"markdown","b5e6d286":"markdown","fa62807a":"markdown"},"source":{"f6dc51bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16c82a6d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nplt.style.use('fivethirtyeight')\n# warning\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","4340f63d":"df=pd.read_csv('..\/input\/car-data\/CarPrice_Assignment.csv')\ndf.head().T","d53c8886":"object=['CarName', 'fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel','enginelocation',\n       'enginetype', 'cylindernumber', 'fuelsystem', ]","bd376a11":"numeric=['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight','enginesize',\n        'boreratio', 'stroke',\n       'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg']","4d3afd7f":"object=['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel','enginelocation',\n       'enginetype', 'cylindernumber', 'fuelsystem', ]","029bd942":"from scipy import stats\n","cd74e098":"for i in numeric:  \n    r, p=stats.pearsonr(df[i], df['price'])\n    graph=sns.jointplot(df[i], df['price'], kind=\"reg\", color=\"#ce1414\",)\n    phantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\n    graph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\n    plt.show()","64e2656a":"fueltype={'gas':1,'diesel':0}\ndf['fueltype']=[fueltype[x] for x in df['fueltype']]\n\n# aspiration \naspiration={'std':1,'turbo':0}\ndf['aspiration']=[aspiration[x] for x in df['aspiration']]\n# doornumber\ndoornumber={'four':1,'two':0}\ndf['doornumber']=[doornumber[x] for x in df['doornumber']]\n\n#drive wheel\nenginelocation={'front':1,'rear':0}\ndf['enginelocation']=[enginelocation[x] for x in df['enginelocation']]","427973dc":"numeric","d2e67428":"df.info()","19c85652":"X=df[numeric]\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","022f084c":"from sklearn import metrics\nfrom sklearn.linear_model import LinearRegression","1f92cd5d":"lr=LinearRegression()\nlr.fit(X_train, y_train)","ce74d4f6":"y_pred=lr.predict(X_train)","ced7465f":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","8ddf63fb":"y_test_pred=lr.predict(x_test)","527aea50":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"Mean Root Error:-\", (metrics.mean_absolute_error(y_test, y_test_pred))**0.5)\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","88eff04e":"from sklearn.model_selection import cross_val_score","8bb6ed1f":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(lr, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)","69d73aa7":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","e74a51db":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","6a3fe0f5":"col=['car_ID','CarName','carbody', 'drivewheel',  'enginetype',\n       'cylindernumber','fuelsystem', 'price']","801c4f32":"X=df.drop(col, axis=1)\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","bfb4ffb4":"lr=LinearRegression()\nlr.fit(X_train, y_train)","423cffd8":"y_pred=lr.predict(X_train)","2e6c71ef":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","e624547e":"y_test_pred=lr.predict(x_test)","4a5bec8c":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"Mean Root Error:-\", (metrics.mean_absolute_error(y_test, y_test_pred))**0.5)\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","8c21aa95":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import cross_val_score, train_test_split","2e48f8c8":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(lr, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)","9d49ed74":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","cda548cf":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","f28671d8":"import sklearn\nfrom sklearn.model_selection import cross_val_score, KFold","4c1d205a":"# k-fold CV (using all the 13 variables)\nscores = cross_val_score(lr, X_train, y_train, scoring='r2', cv=5).mean()\nscores ","3b8ffc5b":"# k-fold CV\n\nscores = cross_val_score(lr, X_train, y_train, scoring='r2', cv=10).mean()\nscores","a85c30a0":"from sklearn.neighbors import KNeighborsRegressor","f27ff81e":"error_rate=[]\n\nfor i in range(1,11):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(metrics.mean_absolute_error(y_test, pred))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,11), error_rate,marker='o', markersize=9)","7dd3f852":"error_rate=[]\n\nfor i in range(1,10):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(metrics.mean_absolute_error(y_test, pred))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,10), error_rate,marker='o', markersize=9)","a4ad32b0":"X=df[numeric]\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","27b0c079":"error_rate=[]\n\nfor i in range(1,10):\n    knn=KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred=knn.predict(x_test)\n    error_rate.append(metrics.mean_absolute_error(y_test, pred))\n    \nplt.figure(figsize=(15,10))\nplt.plot(range(1,10), error_rate,marker='o', markersize=9)","18ac7cf2":"knn=KNeighborsRegressor(n_neighbors=3)\nknn.fit(X_train, y_train)","8dbe0d4e":"y_pred=knn.predict(X_train)\n","3214c83c":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","28903996":"y_test_pred=knn.predict(x_test)","78b65996":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"Mean Root Error:-\", (metrics.mean_absolute_error(y_test, y_test_pred))**0.5)\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","9c60d784":"# k-fold CV \nscores = cross_val_score(knn, X_train, y_train, scoring='r2', cv=5).mean()\nscores","b19c6831":"scores = cross_val_score(knn, X_train, y_train, scoring='r2', cv=10).mean()\nscores","85e9b523":"r2scores=[]\nadjustedr2=[]\n\nfor i in range(1, 19):\n    R2 = cross_val_score(knn, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \n    r2scores.append(R2)\n    \n    n= len(X_train)\n    p = i #len(X.columns)\n    adj_R2 = 1- ((1-R2) * (n-1)\/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)\/(n-p-1)\n    # print(r2, adjustedr2)\n    adjustedr2.append(adj_R2)","43f80ace":"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\n#scoring_df['feature_names'] = train.columns\nscoring_df['features'] = range(1, 19)\nscoring_df","63a5d00b":"fig, ax = plt.subplots(figsize=(8, 6))\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\n# https:\/\/stackoverflow.com\/questions\/52308749\/how-do-i-create-a-multiline-plot-using-seaborn\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\nax.set_xlabel('No of features')\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\nax.set_title('Plot between number of features and R2\/Adj R2 scores')","4f0a62c7":"X=df.drop(col, axis=1)\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","50334197":"knn=KNeighborsRegressor(n_neighbors=3)\nknn.fit(X_train, y_train)","09487b34":"y_pred=knn.predict(X_train)\n","83e11f52":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_train, y_pred))\nprint(\"Max Error:-\", metrics.max_error(y_train, y_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_train, y_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_train, y_pred))\nprint(\"R-square Value:-\", metrics.r2_score(y_train, y_pred))","4c4489b4":"y_test_pred=knn.predict(x_test)","aa80e5d2":"print(\"Explained Variance:-\",metrics.explained_variance_score(y_test, y_test_pred))\nprint(\"Max Error:-\", metrics.max_error(y_test, y_test_pred))\nprint(\"Mean Absolute Error:-\", metrics.mean_absolute_error(y_test, y_test_pred))\nprint(\"MSE:-\", metrics.mean_squared_error(y_test, y_test_pred))\nprint(\"Mean Root Error:-\", (metrics.mean_absolute_error(y_test, y_test_pred))**0.5)\nprint(\"R-square Value:-\", metrics.r2_score(y_test, y_test_pred))","ac748c61":"# train, cv, test\nX=df.drop(col, axis=1)\ny=df['price']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)\nX_tr, X_cv, y_tr, y_cv = train_test_split(X_train, y_train, test_size=0.3)","cd7f5b29":"from sklearn.metrics import accuracy_score\nhist = {\n    'ks': [],\n    'acc_cv': [],\n    'acc_tr': []\n}\n\nfor k in range(1,50,2):\n    knn = KNeighborsRegressor(n_neighbors=k)\n    # fitting cv train\n    knn.fit(X_tr, y_tr)\n    # predict and eval  cv train\n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    mse_cv = metrics.mean_squared_error(y_cv, pred_cv) \n    mse_tr  = metrics.mean_squared_error(y_tr, pred_tr) \n    print(f\"k:{k}\\t val-mse: {mse_cv} \\ttrain-mse: {mse_tr}\")\n    \n    # log\n    hist['ks'].append(k)    \n    hist['acc_cv'].append(mse_cv)    \n    hist['acc_tr'].append(mse_tr)","24b69c3a":"plt.figure(figsize=(20, 7))\n\nplt.plot(hist['ks'], hist['acc_cv'], label='cv-MSE')\nplt.plot(hist['ks'], hist['acc_tr'], label='train-MSE')\n\nfor k, mse_cv in zip(hist['ks'], hist['acc_cv']):\n    plt.text(k, mse_cv, f'k={k}')\n\nplt.legend()\nplt.show()","919140ad":"# Categorical Variable ","5676bc96":"# K=3 is best","1fe9bb8d":"# R-square and Adjusted R-square","12dfd5b3":"# Using the Numerical variable only","f7677189":"# Cross validation ","62c90d4f":"# Conclusion \n* We can see there test error is high \n* We can see it can be overfit.","328e89a6":"# K=3 is best","4b22897a":"# KNN","8c5963de":"# Cross-Validation with Linear Regression","2ecba15a":"# Conclusion \n* Adding the categorical variable reduces the MSE and other errors\n* It is perming good but still MSE is very high \n* we need to reduce this error","93e4a25b":"# Conclusion\n* Test error is quite higher than Trainig error.\n* This is the case that model is that model is overfitting alot.\n* We need to find the cross validation error.","d937fecb":"# Taking categorical into account","a5757d2e":"# Simple Cross validation","b5e6d286":"# Calculate the Adjusted- R square","fa62807a":"# Linear Regression"}}