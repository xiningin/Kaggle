{"cell_type":{"7a06d16e":"code","b2d104cc":"code","e4811f3d":"code","a7d665fd":"code","abe52736":"code","17e7ff14":"code","609935bc":"code","54510052":"code","fe2d63f7":"code","353cb9da":"code","13876ac6":"code","8b653253":"code","1e6f77b9":"code","2dd46e09":"code","fea516f4":"code","3ad85df8":"code","a368237f":"code","856d6c07":"code","4cd67918":"code","f4bce1e2":"code","9c56ee75":"code","58832a53":"code","b0ffe6de":"code","1d49ad7a":"code","7e7389b6":"code","35130aab":"code","fc5d9ff7":"code","7e791d2b":"code","4d000eaf":"code","1b1309b5":"code","75b051c9":"code","682ca8dc":"markdown","40487ec9":"markdown","21a87dcd":"markdown","2ebee7ff":"markdown","0bf15f82":"markdown","6c20e4a0":"markdown","4dcae213":"markdown","e40472d7":"markdown","95ceb124":"markdown","037aa059":"markdown","65cd7111":"markdown","a0b2aea7":"markdown","52a571bc":"markdown","a358e86a":"markdown","6e6acb41":"markdown","80156c81":"markdown","e25e50c0":"markdown","e1b1f63e":"markdown"},"source":{"7a06d16e":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport random\nimport re\nfrom PIL import Image\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import *\nimport os\nimport sys\n\n\n\nfrom tensorflow.keras.applications.resnet50 import *\nfrom tensorflow.keras.models import *\n\n\nimport tensorflow as tf","b2d104cc":"!wget http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamSeq01\/CamSeq01.zip","e4811f3d":"cwd = os.getcwd()\ncwd","a7d665fd":"!mkdir data\n!mkdir data\/CamSeq01","abe52736":"!unzip CamSeq01.zip -d data\/CamSeq01","17e7ff14":"def _read_to_tensor(fname, output_height=224, output_width=224, normalize_data=False):\n    '''Function to read images from given image file path, and provide resized images as tensors\n        Inputs: \n            fname - image file path\n            output_height - required output image height\n            output_width - required output image width\n            normalize_data - if True, normalize data to be centered around 0 (mean 0, range 0 to 1)\n        Output: Processed image tensors\n    '''\n    \n    # Read the image as a tensor\n    img_strings = tf.io.read_file(fname)\n    imgs_decoded = tf.image.decode_jpeg(img_strings)\n    \n    # Resize the image\n    output = tf.image.resize(imgs_decoded, [output_height, output_width])\n    \n    # Normalize if required\n    if normalize_data:\n        output = (output - 128) \/ 128\n    return output","609935bc":"img_dir = 'data\/CamSeq01\/'\n\n# Required image dimensions\noutput_height = 224\noutput_width = 224","54510052":"def read_images(img_dir):\n    '''Function to get all image directories, read images and masks in separate tensors\n        Inputs: \n            img_dir - file directory\n        Outputs \n            frame_tensors, masks_tensors, frame files list, mask files list\n    '''\n    \n    # Get the file names list from provided directory\n    file_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n    \n    # Separate frame and mask files lists, exclude unnecessary files\n    frames_list = [file for file in file_list if ('_L' not in file) and ('txt' not in file)]\n    masks_list = [file for file in file_list if ('_L' in file) and ('txt' not in file)]\n    \n    frames_list.sort()\n    masks_list.sort()\n    \n    print('{} frame files found in the provided directory.'.format(len(frames_list)))\n    print('{} mask files found in the provided directory.'.format(len(masks_list)))\n    \n    # Create file paths from file names\n    frames_paths = [os.path.join(img_dir, fname) for fname in frames_list]\n    masks_paths = [os.path.join(img_dir, fname) for fname in masks_list]\n    \n    # Create dataset of tensors\n    frame_data = tf.data.Dataset.from_tensor_slices(frames_paths)\n    masks_data = tf.data.Dataset.from_tensor_slices(masks_paths)\n    \n    # Read images into the tensor dataset\n    frame_tensors = frame_data.map(_read_to_tensor)\n    masks_tensors = masks_data.map(_read_to_tensor)\n    \n    print('Completed importing {} frame images from the provided directory.'.format(len(frames_list)))\n    print('Completed importing {} mask images from the provided directory.'.format(len(masks_list)))\n    \n    return frame_tensors, masks_tensors, frames_list, masks_list\n\nframe_tensors, masks_tensors, frames_list, masks_list = read_images(img_dir)","fe2d63f7":"# Make an iterator to extract images from the tensor dataset\n\nframe_batches = tf.compat.v1.data.make_one_shot_iterator(frame_tensors)  # outside of TF Eager, we would use make_one_shot_iterator\nmask_batches = tf.compat.v1.data.make_one_shot_iterator(masks_tensors)\n\n","353cb9da":"n_images_to_show = 5\n\nfor i in range(n_images_to_show):\n    \n    # Get the next image from iterator\n    fig,ax = plt.subplots(1,figsize=(5,5))\n    frame = frame_batches.next().numpy().astype(np.uint8)\n    mask = mask_batches.next().numpy().astype(np.uint8)\n    ax.imshow(frame)\n    ax.imshow(mask,alpha = 0.4)","13876ac6":"DATA_PATH = 'data\/CamSeq01\/'\n\n# Create folders to hold images and masks\n\nfolders = ['train_frames\/train', 'train_masks\/train', 'val_frames\/val', 'val_masks\/val']\n\n\nfor folder in folders:\n    try:\n        os.makedirs(DATA_PATH + folder)\n    except Exception as e: print(e)","8b653253":"def generate_image_folder_structure(frames, masks, frames_list, masks_list):\n    '''Function to save images in the appropriate folder directories \n        Inputs: \n            frames - frame tensor dataset\n            masks - mask tensor dataset\n            frames_list - frame file paths\n            masks_list - mask file paths\n    '''\n    #Create iterators for frames and masks\n    frame_batches = tf.compat.v1.data.make_one_shot_iterator(frames)  # outside of TF Eager, we would use make_one_shot_iterator\n    mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks)\n    \n    #Iterate over the train images while saving the frames and masks in appropriate folders\n    dir_name='train'\n    for file in zip(frames_list[:-round(0.2*len(frames_list))],masks_list[:-round(0.2*len(masks_list))]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    #Iterate over the val images while saving the frames and masks in appropriate folders\n    dir_name='val'\n    for file in zip(frames_list[-round(0.2*len(frames_list)):],masks_list[-round(0.2*len(masks_list)):]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    print(\"Saved {} frames to directory {}\".format(len(frames_list),DATA_PATH))\n    print(\"Saved {} masks to directory {}\".format(len(masks_list),DATA_PATH))\n    \ngenerate_image_folder_structure(frame_tensors, masks_tensors, frames_list, masks_list)\n\n#generate_image_folder_structure(train_frames, train_masks, val_files, 'val')","1e6f77b9":"def parse_code(l):\n    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n    '''\n    if len(l.strip().split(\"\\t\")) == 2:\n        a, b = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), b\n    else:\n        a, b, c = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), c","2dd46e09":"label_codes, label_names = zip(*[parse_code(l) for l in open(img_dir+\"label_colors.txt\")])\nlabel_codes, label_names = list(label_codes), list(label_names)\nlabel_codes[:5], label_names[:5]","fea516f4":"code2id = {v:k for k,v in enumerate(label_codes)}\nid2code = {k:v for k,v in enumerate(label_codes)}","3ad85df8":"name2id = {v:k for k,v in enumerate(label_names)}\nid2name = {k:v for k,v in enumerate(label_names)}","a368237f":"def rgb_to_onehot(rgb_image, colormap = id2code):\n    '''Function to one hot encode RGB mask labels\n        Inputs: \n            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n            colormap - dictionary of color to label id\n        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n    '''\n    num_classes = len(colormap)\n    shape = rgb_image.shape[:2]+(num_classes,)\n    encoded_image = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(colormap):\n        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n    return encoded_image\n\n\ndef onehot_to_rgb(onehot, colormap = id2code):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    return np.uint8(output)","856d6c07":"from keras.preprocessing.image import ImageDataGenerator\n# Normalizing only frame images, since masks contain label info\ndata_gen_args = dict(rescale=1.\/255)\nmask_gen_args = dict()\n\ntrain_frames_datagen = ImageDataGenerator(**data_gen_args)\ntrain_masks_datagen = ImageDataGenerator(**mask_gen_args)\nval_frames_datagen = ImageDataGenerator(**data_gen_args)\nval_masks_datagen = ImageDataGenerator(**mask_gen_args)\n\n# Seed defined for aligning images and their masks\nseed = 1","4cd67918":"def TrainAugmentGenerator(seed = 1, batch_size = 5):\n    '''Train Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    train_image_generator = train_frames_datagen.flow_from_directory(\n    DATA_PATH + 'train_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n    train_mask_generator = train_masks_datagen.flow_from_directory(\n    DATA_PATH + 'train_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n    while True:\n        X1i = train_image_generator.next()\n        X2i = train_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n\ndef ValAugmentGenerator(seed = 1, batch_size = 5):\n    '''Validation Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    val_image_generator = val_frames_datagen.flow_from_directory(\n    DATA_PATH + 'val_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n\n    val_mask_generator = val_masks_datagen.flow_from_directory(\n    DATA_PATH + 'val_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n\n    while True:\n        X1i = val_image_generator.next()\n        X2i = val_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n        \n","f4bce1e2":"from tensorflow.keras.layers import Activation,Conv2D,MaxPooling2D,BatchNormalization,Input,DepthwiseConv2D,add,Dropout,AveragePooling2D,Concatenate\nfrom tensorflow.keras.models import   Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Layer, InputSpec\nfrom tensorflow.python.keras.utils import conv_utils","9c56ee75":"class BilinearUpsampling(Layer):\n\n    def __init__(self, upsampling=(2, 2), data_format=None, **kwargs):\n\n        super(BilinearUpsampling, self).__init__(**kwargs)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.upsampling = conv_utils.normalize_tuple(upsampling, 2, 'size')\n        self.input_spec = InputSpec(ndim=4)\n\n    def compute_output_shape(self, input_shape):\n        height = self.upsampling[0] * \\\n            input_shape[1] if input_shape[1] is not None else None\n        width = self.upsampling[1] * \\\n            input_shape[2] if input_shape[2] is not None else None\n        return (input_shape[0],\n                height,\n                width,\n                input_shape[3])\n\n    def call(self, inputs):\n        return tf.image.resize(inputs, (int(inputs.shape[1]*self.upsampling[0]),\n                                                   int(inputs.shape[2]*self.upsampling[1])),method = 'bilinear')\n\n    def get_config(self):\n        config = {'size': self.upsampling,\n                  'data_format': self.data_format}\n        base_config = super(BilinearUpsampling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n    \ndef xception_downsample_block(x,channels,top_relu=False):\n\t##separable conv1\n\tif top_relu:\n\t\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\t\n\t##separable conv2\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\t\n\t##separable conv3\n\tx=DepthwiseConv2D((3,3),strides=(2,2),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\treturn x\n\ndef res_xception_downsample_block(x,channels):\n\tres=Conv2D(channels,(1,1),strides=(2,2),padding=\"same\",use_bias=False)(x)\n\tres=BatchNormalization()(res)\n\tx=xception_downsample_block(x,channels)\n\tx=add([x,res])\n\treturn x\n\ndef xception_block(x,channels):\n\t##separable conv1\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\t\n\t##separable conv2\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\t\n\t##separable conv3\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(channels,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\treturn x\t\n\ndef res_xception_block(x,channels):\n\tres=x\n\tx=xception_block(x,channels)\n\tx=add([x,res])\n\treturn x\n\ndef aspp(x,input_shape,out_stride):\n\tb0=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tb0=BatchNormalization()(b0)\n\tb0=Activation(\"relu\")(b0)\n\t\n\tb1=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n\tb1=BatchNormalization()(b1)\n\tb1=Activation(\"relu\")(b1)\n\tb1=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b1)\n\tb1=BatchNormalization()(b1)\n\tb1=Activation(\"relu\")(b1)\n\t\n\tb2=DepthwiseConv2D((3,3),dilation_rate=(12,12),padding=\"same\",use_bias=False)(x)\n\tb2=BatchNormalization()(b2)\n\tb2=Activation(\"relu\")(b2)\n\tb2=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b2)\n\tb2=BatchNormalization()(b2)\n\tb2=Activation(\"relu\")(b2)\t\n\n\tb3=DepthwiseConv2D((3,3),dilation_rate=(12,12),padding=\"same\",use_bias=False)(x)\n\tb3=BatchNormalization()(b3)\n\tb3=Activation(\"relu\")(b3)\n\tb3=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b3)\n\tb3=BatchNormalization()(b3)\n\tb3=Activation(\"relu\")(b3)\n\t\n\tout_shape=int(input_shape[0]\/out_stride)\n\tb4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n\tb4=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(b4)\n\tb4=BatchNormalization()(b4)\n\tb4=Activation(\"relu\")(b4)\n\tb4=BilinearUpsampling((out_shape,out_shape))(b4)\n\t\n\tx=Concatenate()([b4,b0,b1,b2,b3])\n\treturn x\n\ndef deeplabv3_plus(input_shape=(224,224,3),out_stride=16,num_classes=32):\n\timg_input=Input(shape=input_shape)\n\tx=Conv2D(32,(3,3),strides=(2,2),padding=\"same\",use_bias=False)(img_input)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=Conv2D(64,(3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\t\n\tx=res_xception_downsample_block(x,128)\n\n\tres=Conv2D(256,(1,1),strides=(2,2),padding=\"same\",use_bias=False)(x)\n\tres=BatchNormalization()(res)\t\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tskip=BatchNormalization()(x)\n\tx=Activation(\"relu\")(skip)\n\tx=DepthwiseConv2D((3,3),strides=(2,2),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\t\n\tx=add([x,res])\n\t\n\tx=xception_downsample_block(x,728,top_relu=True)\n\t\n\tfor i in range(16):\n\t\tx=res_xception_block(x,728)\n\n\tres=Conv2D(1024,(1,1),padding=\"same\",use_bias=False)(x)\n\tres=BatchNormalization()(res)\t\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(728,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(1024,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(1024,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\t\n\tx=add([x,res])\n\t\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(1536,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(1536,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Conv2D(2048,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\t\n\tx=Activation(\"relu\")(x)\n\t\n\t#aspp\n\tx=aspp(x,input_shape,out_stride)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=Dropout(0.9)(x)\n\t\n\t##decoder \n\tx=BilinearUpsampling((4,4))(x)\n\tdec_skip=Conv2D(48,(1,1),padding=\"same\",use_bias=False)(skip)\n\tdec_skip=BatchNormalization()(dec_skip)\n\tdec_skip=Activation(\"relu\")(dec_skip)\n\tx=Concatenate()([x,dec_skip])\n\t\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\t\n\tx=DepthwiseConv2D((3,3),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\tx=Conv2D(256,(1,1),padding=\"same\",use_bias=False)(x)\n\tx=BatchNormalization()(x)\n\tx=Activation(\"relu\")(x)\n\t\n\tx=Conv2D(num_classes,(1,1),padding=\"same\")(x)\n\tx=BilinearUpsampling((4,4))(x)\n\tmodel=Model(img_input,x)\n\treturn model\n\n\n\n\nmodel=deeplabv3_plus(num_classes=32)\nmodel.summary()","58832a53":"\ndef tversky_loss(y_true, y_pred):\n    alpha = 0.5\n    beta  = 0.5\n    \n    ones = K.ones(K.shape(y_true))\n    p0 = y_pred      # proba that voxels are class i\n    p1 = ones-y_pred # proba that voxels are not class i\n    g0 = y_true\n    g1 = ones-y_true\n    \n    num = K.sum(p0*g0, (0,1,2,3))\n    den = num + alpha*K.sum(p0*g1,(0,1,2,3)) + beta*K.sum(p1*g0,(0,1,2,3))\n    \n    T = K.sum(num\/den) # when summing over classes, T has dynamic range [0 Ncl]\n    \n    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n    return Ncl-T\n\n\ndef tversky(y_true, y_pred, smooth=1):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)\/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\n\n\ndef focal_tversky_loss_r(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1-pt_1), gamma)","b0ffe6de":"@tf.function()\ndef dice_coef(y_true, y_pred):\n    mask =  tf.equal(y_true, 255)\n    mask = tf.logical_not(mask)\n    y_true = tf.boolean_mask(y_true, mask)\n    y_pred = tf.boolean_mask(y_pred, mask)\n    \n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)","1d49ad7a":"smooth = 1.","7e7389b6":"model.compile(optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.00001), loss=\"categorical_crossentropy\", metrics=[dice_coef,'accuracy'])\n","35130aab":"# tb = TensorBoard(log_dir='logs', write_graph=True)\n# mc = ModelCheckpoint(mode='max', filepath='camvid_model_vgg16_segnet_checkpoint.h5', monitor='accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n# # es = EarlyStopping(mode='min', monitor='val_loss', patience=4, verbose=1)\n# callbacks = [mc]","fc5d9ff7":"batch_size = 5\nsteps_per_epoch = np.ceil(float(len(frames_list) - round(0.2*len(frames_list))) \/ float(batch_size))\nsteps_per_epoch","7e791d2b":"validation_steps = (float((round(0.2*len(frames_list)))) \/ float(batch_size))\nvalidation_steps","4d000eaf":"num_epochs = 1","1b1309b5":"# Train model\n\nbatch_size = 5\nresult = model.fit_generator(TrainAugmentGenerator(), steps_per_epoch=steps_per_epoch ,\n                validation_data = ValAugmentGenerator(), \n                validation_steps = validation_steps, epochs=num_epochs, verbose= 1)","75b051c9":"!rm -rf .\/*","682ca8dc":"### Function to parse the file \"label_colors.txt\" which contains the class definitions","40487ec9":"### Defining data generators","21a87dcd":"### Creating folder structure common for Computer Vision problems","2ebee7ff":"### Parse and extract label names and codes","0bf15f82":"### Displaying Images in the train dataset","6c20e4a0":"## Defining dice co-efficients for model performance","4dcae213":"## Data preparation - Importing, Cleaning and Creating structured directory ","e40472d7":"### Saving frames and masks to correct directories","95ceb124":"### Reading frames and masks\n- Mask file names end in \"\\_L.png\"\n","037aa059":"### Image directory and size parameters","65cd7111":"# Creating custom Image data generators","a0b2aea7":"# implementation of DEEPLABV3 model\n\n\n","52a571bc":"with albummentation image mask augumentation  on training up to 150 epochs model can be improved very much","a358e86a":"### Custom image data generators for creating batches of frames and masks","6e6acb41":"### Define functions for one hot encoding rgb labels, and decoding encoded predictions","80156c81":"## Extract Target Class definitions","e25e50c0":"### Function to import and process frames and masks as tensors","e1b1f63e":"## Train and save the DEEPLABV3 model"}}