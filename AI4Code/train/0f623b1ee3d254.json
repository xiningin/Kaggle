{"cell_type":{"93b3f48e":"code","4c5e8723":"code","6a4a789c":"code","d9ee3d6f":"code","7e2d7568":"code","6b3731ee":"code","8926b9ce":"code","b706d473":"code","b7a1d216":"code","b41d1fde":"code","020aa300":"code","4a366b22":"code","136bfbd8":"code","6b2cb6bb":"code","15318a11":"code","a620c181":"code","ad39fff8":"code","2fd8dfe3":"code","8e726ea6":"code","a7d44652":"code","5867270b":"code","fd3bead1":"code","a45008b4":"code","88dc2527":"code","e5a464c4":"code","5b3e6722":"code","2ed930dd":"code","671d120b":"code","cd3fcc3d":"code","7b8985c9":"code","2ddc242a":"markdown","0057dfea":"markdown","e54b936e":"markdown","f3ba1a0b":"markdown","dd8c0ad8":"markdown","861b818d":"markdown","d41ef112":"markdown","a448d6b6":"markdown","230c3476":"markdown","b5c8f6dd":"markdown","ccf77efa":"markdown","1492cba0":"markdown","4299e714":"markdown","b4f29d13":"markdown","e5b98ee9":"markdown","cea55b14":"markdown","99fbcf13":"markdown","8384d171":"markdown","d7a1decc":"markdown"},"source":{"93b3f48e":"# Author: Pierre Jeanne\n# Date Created:  16 May 2021","4c5e8723":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n# display progress bar during iteration\nfrom tqdm.notebook import tqdm\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import plot_confusion_matrix\n\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# slip the data\nfrom sklearn.model_selection import train_test_split\n# scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n# classification model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n# hyperparameter tunning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# model evaluation\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","6a4a789c":"pip install dataprep","d9ee3d6f":"# LIBRARY FOR EDA\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing","7e2d7568":"header_list = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\nurl=\"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/heart-disease\/processed.cleveland.data\"\ndf_heart = pd.read_csv(url,header=None, names=header_list)\ndf_heart.head(3)","6b3731ee":"import pandas_profiling as pp\npp.ProfileReport(df_heart)","8926b9ce":"# let's remove the Nan values\nprint(\"number of row before dropping nan values: {}\".format(df_heart.shape[0]))\ndf_heart = df_heart[(df_heart['ca']!='?')&(df_heart['thal']!='?')]\nprint(\"number of row after dropping nan values: {}\".format(df_heart.shape[0]))","b706d473":"# replace 2,3 and 4 by 1 in the target variable\ndf_heart.loc[:,'num']=df_heart['num'].astype(str)\ndf_heart.loc[:,'num']=df_heart['num'].replace(['2','3','4'], ['1','1','1'])","b7a1d216":"# verify the types of the variables\ndf_heart.dtypes","b41d1fde":"# assign categorical type to the categorical variable\ncategorical_variables = ['sex','cp','fbs','restecg','exang','slope','ca','thal','num']\n\nfor col in categorical_variables:\n    df_heart[col] = df_heart[col].astype('category')","020aa300":"df_heart.dtypes","4a366b22":"# get dummies\ndf_heart = pd.get_dummies(df_heart,drop_first = True)","136bfbd8":"plot_correlation(df_heart)","6b2cb6bb":"#  plot the correlations between variables and their relations with target varibales\nfig, axes = plt.subplots(2,2,figsize=(10,10))\nfig.subplots_adjust(hspace=0.3)\nax0, ax1, ax2, ax3 = axes.flatten() \nsns.scatterplot(data=df_heart,x='thalach',y='age',hue='num_1',ax=ax0)\nsns.scatterplot(data=df_heart,x='thalach',y='oldpeak',hue='num_1',ax=ax1)\nsns.scatterplot(data=df_heart,x='trestbps',y=\"age\",hue='num_1',ax=ax2)\nsns.scatterplot(data=df_heart,x='chol',y=\"age\",hue='num_1',ax=ax3)\n\nplt.show()","15318a11":"# Displaying the count for non Deviated hole \nmajority_class = df_heart.loc[df_heart['num_1'] == 0].count()[0]\n\n# Showing the count for Deviated hole \nminority_class = df_heart.loc[df_heart['num_1'] == 1].count()[0]\n\n# Printing the classes for the deviated and non-deviated class \nprint('low risk of heart failure (num_1 = 0): {}'.format(majority_class))\nprint('high risk of heart failure (num_1 = 1) : {}'.format(minority_class))\n\n\nsns.countplot(x=\"num_1\", data=df_heart)\nplt.show()","a620c181":"from imblearn.over_sampling import SMOTE\nX = df_heart.drop('num_1',axis=1)\ny = df_heart[['num_1']].values.ravel()\n# Using SMOTE to Balance the imbalanced data \nX_resampled, y_resampled = SMOTE().fit_resample(X, y)\n\nX_resampled = pd.DataFrame(X_resampled, columns=X.columns ) ","ad39fff8":"# convert y_resampled to df\ndf_y_resampled = pd.DataFrame(y_resampled,columns=['num_1'])\n\n# showing a plot of the Balanced dataset \nmajority_class = df_y_resampled.loc[df_y_resampled['num_1'] == 0].count()[0]\n\n# Showing the count for Non Hole Deviation \nminority_class = df_y_resampled.loc[df_y_resampled['num_1'] == 1].count()[0]\n\n# Printing the classes for the deviated and non-deviated class \nprint('Non Deviated Class (num_1 = 0): {}'.format(majority_class))\nprint('Deviated Class (num_1 = 1) : {}'.format(minority_class))\n\n\nsns.countplot(x=\"num_1\", data=df_y_resampled)\nplt.show()","2fd8dfe3":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,test_size = .2, random_state=0)","8e726ea6":"scaler = MinMaxScaler()\nnumeric_col = ['age','trestbps','chol','thalach','oldpeak']\n# fit and transform \"x_train\"\nX_train.loc[:,numeric_col] = scaler.fit_transform(X_train[numeric_col])\n# transform \"x_test\"\nX_test.loc[:,numeric_col] = scaler.transform(X_test[numeric_col])","a7d44652":"list_scores = []\ndef plot_result_cv(list_score_train,list_score_test,model,pred,name):\n    print('mean scores on training set: {:2f}, and testing set: {:2f}'.format(np.mean(list_score_train),np.mean(list_score_test)))\n    fig = plt.figure(figsize=(15,5))\n    fig.subplots_adjust(hspace=0.4,wspace=0.3)\n    ax0 = fig.add_subplot(1,2,1)\n    ax0 = plt.plot(list_score_train,'go-',label='CV score on training set')\n    ax0 = plt.plot(list_score_test,'ro-',label='CV score on testing set')\n    ax0 = plt.xlabel('nb of fold cross-validation')\n    ax0 = plt.ylabel('score')\n    ax0 = plt.legend() \n    \n    ax1 = fig.add_subplot(1,2,2)\n    plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues,ax=ax1);  \n    \n    accuracy = accuracy_score(y_test,pred)\n    recall = recall_score(y_test,pred)\n    precision = precision_score(y_test,pred)\n    f1 = f1_score(y_test,pred)\n    \n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    \n    list_scores.append({'Model Name': name, 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1':f1})\n    \n    plt.show()","5867270b":"clf_lr = LogisticRegression()\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_lr,X_train, y_train,cv=12,scoring = 'accuracy')\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_lr,X_test, y_test,cv=12,scoring = 'accuracy')\n\nclf_lr_mean_train = np.mean(cv_scores_train)\nclf_lr_mean_test = np.mean(cv_scores_test)\n\nclf_lr.fit(X_train,y_train)\npred = clf_lr.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_lr,pred,'logreg')","fd3bead1":"param_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=7)\nknn_cv.fit(X_train,y_train)\nknn_cv.best_params_","a45008b4":"clf_knn = KNeighborsClassifier(n_neighbors=48)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_knn,X_train, y_train,cv=12,scoring = 'accuracy')\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_knn,X_test, y_test,cv=12,scoring = 'accuracy')\n\nclf_knn_mean_train = np.mean(cv_scores_train)\nclf_knn_mean_test = np.mean(cv_scores_test)\n\nclf_knn.fit(X_train,y_train)\npred = clf_knn.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_knn, pred,'knn_48')","88dc2527":"clf_svm = svm.SVC()\n\nparameters = { 'C':np.arange(1,5,1),'gamma':[0.001, 0.005, 0.01, 0.05, 0.09, 0.1, 0.2, 0.5,1],\n              'kernel':['rbf', 'sigmoid', 'linear', 'poly',]}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = clf_svm, param_grid = parameters,cv = 7, n_jobs = -1, verbose = 2,scoring = 'accuracy')\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","e5a464c4":"clf_svm = svm.SVC(C= 4, gamma= 0.05,kernel = 'sigmoid')\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_svm,X_train, y_train,cv=12)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_svm,X_test, y_test,cv=12)\n\nclf_svm_mean_train = np.mean(cv_scores_train)\nclf_svm_mean_test = np.mean(cv_scores_test)\n\nclf_svm.fit(X_train,y_train)\npred = clf_svm.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_svm,pred,'svc')\n","5b3e6722":"# Setup the parameters and distributions to sample from: param_dist\nparameters = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [600]}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, n_iter = 300, cv = 4, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","2ed930dd":"# view the best parameters from fitting the random search:\nrf_random.best_params_","671d120b":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [None],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [2,3],\n    'min_samples_split': [8,9,10,11,12],\n    'n_estimators': [600]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","cd3fcc3d":"clf_rf = RandomForestClassifier(bootstrap ='True',max_depth = None,max_features = 'sqrt',\n                                min_samples_leaf = 2, min_samples_split = 11, n_estimators = 600)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_rf,X_train, y_train,cv=12)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_rf,X_test, y_test,cv=12)\n\nclf_rf_mean_train = np.mean(cv_scores_train)\nclf_rf_mean_test = np.mean(cv_scores_test)\n\nclf_rf.fit(X_train,y_train)\npred = clf_rf.predict(X_test)\n# plot result cv\nplot_result_cv(cv_scores_train,cv_scores_test,clf_rf,pred,'Rand_Forest')","7b8985c9":"df_scores = pd.DataFrame(list_scores)\ndf_scores.style.highlight_max(color = 'lightgreen', axis = 0)","2ddc242a":"## <a id=\"3.1.3\"><\/a>\n**3.1.3: Classification with logistic regression**","0057dfea":"## <a id=\"3\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3- Machine learning<\/center><\/h3>","e54b936e":"# Heart Attack Analysis & Prediction Dataset\n## A dataset for heart attack classification\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQ7ZWEo96vWDovxH-6QR0jFPj1Fi1zgkBQsmA&usqp=CAU\" width=\"500px\">\n\n### Goal:\nWe should help predicting possible diameter narrowing thatcan lead to heart failure. The target variable is `num`:\n- 0 = less chance of heart attack\n- 1 = more chance of heart attack\n\n### Warning:\nIn the discussion, it is said that the dataset presents on Kaggle is a poor copy of the original dataset, and that the target values were swapped. So, here we will work with the original dataset.\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease","f3ba1a0b":"## <a id=\"2\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2: Target variable<\/center><\/h3>","dd8c0ad8":"## <a id=\"1\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1: Exploratory Data Analysis<\/center><\/h3>\n","861b818d":"**best model**","d41ef112":"<a id=\"3.2\"><\/a>\n**3.2: Conclusion**\n\nThe best classification is obtained with the Support Vector Machines model (acc. of 0.79)","a448d6b6":"[1: Exploratory Data Analysis](#1)\n- [1.1: EDA with dataprep](#1.1)\n- [1.2: Report summary main points](#1.2)\n\n[2: Target variable](#2)\n\n[3: Machine learning](#3)\n- [3.1 Preprocessing](#3.1)\n    - [3.1.1: Split the data](#3.1)\n    - [3.1.2: Scale the data](#3.1.2)\n    - [3.1.3: Classification with logistic regression](#3.1.3)\n    - [3.1.4: Classification with k-Nearest Neighbors](#3.1.4)\n    - [3.1.5: Classification with Support Vector Machines](#3.1.5)\n    - [3.1.6: Classification with random Forest classifier](#3.1.6)\n    \n- [3.2: Conclusion](#3.2)","230c3476":"<a id=\"3.1.5\"><\/a>\n**3.1.5: Classification with Support Vector Machines**","b5c8f6dd":"<a id=\"3.1.6\"><\/a>\n**3.1.6: Classification with random Forest classifier**\n**random search**","ccf77efa":"<a id=\"3.1.2\"><\/a>\n**3.1.2: Scale the data**\n\nHere, we normalize the continuous variables only, leaving the dummy variables alone. We also use the min-max scaler to give those continuous variables the same minimum of zero, max of one, range of 1. ","1492cba0":"**About this dataset**\n\n- `Age` : Age of the patient\n- `Sex` : Sex of the patient\n- `cp` : Chest Pain type chest pain type\n    - Value 0: asymptomatic\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n\n- `trtbps` : resting blood pressure (in mm Hg)\n- `chol` : cholestoral in mg\/dl fetched via BMI sensor\n- `fbs` : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n- `rest_ecg` : resting electrocardiographic results\n    - Value 0: normal\n    - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n- `thalach` : maximum heart rate achieved\n- `exang`: exercise induced angina (1 = yes; 0 = no)\n- `oldpeak`: ST depression induced by exercise relative to rest\n- `slp`: the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n- `ca`: number of major vessels (0-3)\n- `thal` - 3 = normal; 6 = fixed defect; 7 = reversable defect\n- `num` : 0= less chance of heart attack 1= more chance of heart attack","4299e714":"The target data is imbalanced \n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to use **SMOTE**.\n\nSMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or **SMOTE** for short.","b4f29d13":"## <a id=\"3.1\"><\/a>\n**3.1: Preprocessing**\n\n**3.1.1: Split the data**","e5b98ee9":"## Downloading data","cea55b14":"<a id=\"3.1.4\"><\/a>\n**3.1.4: Classification with k-Nearest Neighbors**","99fbcf13":"**Grid Search with Cross Validation**\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:","8384d171":"## <a id=\"1.1\"><\/a>\n**1.1: EDA with dataprep** ","d7a1decc":"## <a id=\"1.2\"><\/a>\n**1.2: Report summary main points**\n- there are 5 Numeric and 9 Categorical\tvariables \n- the Categorical variables have a float and object types\n- `ca` and `thal` have 4 and 2 '?' instead of missing value, respectively\n- the target variable has 5 different values: 0, 1,2,3 and 4\n\nIn the original study, the presence of heart disease in the patient is given by an integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0). So, we will replace all the value higher than 1 by 1 to obtain the desired output:\n- 0 = less chance of heart attack\n- 1 = more chance of heart attack"}}