{"cell_type":{"45336283":"code","fc03ef6b":"code","57563260":"code","f68f321d":"code","6a8e670f":"code","af41fc84":"code","763c2ae7":"code","240f0364":"code","ec121dba":"code","2b6ed49a":"code","156e46ee":"code","b083faa9":"code","3a7a937c":"code","3e3ece1e":"code","8360cf25":"code","08c19da0":"code","54f13d4c":"code","cdf005a7":"code","69f6dffe":"code","289c8ad0":"code","ab74424d":"code","39f910c8":"code","f7d79952":"code","427aa481":"code","cd2c50f5":"code","24ab5832":"code","f5c789ed":"code","67ae1af6":"markdown","efd0cdc2":"markdown","7bbe05d4":"markdown","b31b1850":"markdown","01ccd08e":"markdown","914a9d2f":"markdown","7db31dc5":"markdown","c10b2dd5":"markdown","e5d66d33":"markdown","4f038b40":"markdown","81964f6e":"markdown","2bdc9e48":"markdown","808d94fc":"markdown","051ae58e":"markdown","9ab0b683":"markdown","9993062a":"markdown","8a6ec63d":"markdown","c4ebdf43":"markdown","b69ddbdd":"markdown","f72b0cc5":"markdown","fac8ae32":"markdown","09d8f650":"markdown","90145f90":"markdown","d5dfb6f3":"markdown","3f455042":"markdown","203197e9":"markdown","8ac219ef":"markdown","e1662d69":"markdown","0622b36a":"markdown","9b0da079":"markdown","f925e3e9":"markdown","64bcaa15":"markdown"},"source":{"45336283":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc03ef6b":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')","57563260":"df=pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head(2)","f68f321d":"df = df.drop('Id',axis=1)\ndf.head(2)","6a8e670f":"df['Species'].unique()","af41fc84":"from sklearn import preprocessing \nle = preprocessing.LabelEncoder() \ndf['Species']= le.fit_transform(df['Species'])\ndf['Species'].unique()","763c2ae7":"X=df.iloc[:,0:4].values\ny=df.iloc[:,4].values\n#X","240f0364":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","ec121dba":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","2b6ed49a":"X_train.shape","156e46ee":"from sklearn.decomposition import PCA\npca=PCA(n_components=None)\nX_train=pca.fit_transform(X_train)\nX_test=pca.transform(X_test)\nexplained_variance=pca.explained_variance_ratio_","b083faa9":"explained_variance","3a7a937c":"scores = X_train \nscores_df = pd.DataFrame(scores,columns=['PC1','PC2','PC3','PC4'])\nscores_df.head(2)","3e3ece1e":"scores_df['PC1'].head(2)","8360cf25":"y_label = []\n\nfor i in y:\n    if i == 0:\n        y_label.append(\"Setosa\")\n    elif i == 1:\n        y_label.append(\"Versicolor\")\n    else:\n        y_label.append(\"Virginica\")\n        \nWines = pd.DataFrame(y_label,columns=['Species'])\n\ndf_scores = pd.concat([scores_df,Wines],axis=1)\ndf_scores.head()","08c19da0":"from sklearn.decomposition import PCA\npca=PCA(n_components=2)\nX_train=pca.fit_transform(X_train)\nX_test=pca.transform(X_test)\nexplained_variance=pca.explained_variance_ratio_\npca.explained_variance_ratio_","54f13d4c":"np.identity(X.shape[1])","cdf005a7":"components=pca.transform(np.identity(X.shape[1]))","69f6dffe":"df.columns","289c8ad0":"pd.DataFrame(components,columns=['pc_1','pc_2'],index=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])","ab74424d":"from sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","39f910c8":"y_pred=classifier.predict(X_test)","f7d79952":"from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","427aa481":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nprint(classification_report(y_test,y_pred))","cd2c50f5":"print(accuracy_score(y_test,y_pred))","24ab5832":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green','blue')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green','blue'))(i),label=j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","f5c789ed":"\nfrom matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green','blue')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green','blue'))(i),label=j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","67ae1af6":"### Vizualization of test set results","efd0cdc2":"### Labeling the Dataframe","7bbe05d4":"### Creating matrix of features","b31b1850":"The Column on Id wont have any impact on the class of the flower.So we can drop it.","01ccd08e":"We see that the the first two principal components 0.36 +0.19 = 0.55 .So we will consider the first two components.","914a9d2f":"### Shape Of Data ","7db31dc5":"### Importing Python Modules","c10b2dd5":"### Importing dataset","e5d66d33":"### Dropping Column","4f038b40":"### Splitting data into test train","81964f6e":"### Vizualizing training set results","2bdc9e48":"### Label Encoding ","808d94fc":"### Scores","051ae58e":"# 6.Visualising the Prediction","9ab0b683":"# 6.Conclusion\n\n1.In this Notebook we have demonstrated how to reduce the dimentionality of Data using Principle Component Analyis\n\n2.Out of four features we have used two features to make our final Prediction using Logistic Regression\n\n3.This is just a demonstration on how to do ensembele machine learning using PCA + Logistic Regression \n\n4.This technique will be more useful when we have very high number of features in our data set.","9993062a":"In this notebook we will discuss about dimensionality reduction.We will be using Principle Component Analysis for this.After we manage to reduce the dimensions in our dataset then we will make use of the reduced dimensions for making our predictions.In this notebook we will be covering following things.\n\n1.Data Import and Preprocessing\n\n2.Exploratory Data Analysis\n\n3.Principle Component Analysis\n\n4.Logistic Regression\n\n5.Model Evaluation\n\n6.Vizualisation of Predictions\n\n7.Conclusion","8a6ec63d":"# 1.Data Import and Preprocessing","c4ebdf43":"### Predicting the test set results","b69ddbdd":"### Accuracy Score","f72b0cc5":"### Calling DataDrame Variables","fac8ae32":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code\n","09d8f650":"### Classification Report","90145f90":"# 4.Logistic Regression","d5dfb6f3":"# TO BE CONTINUED","3f455042":"# 3.Principle Component Analysis","203197e9":"### Feature Scaling","8ac219ef":"So we have converted the Species column into numeric data.","e1662d69":"# 5.Model Evaluation","0622b36a":"# 2.Exploratory Data Analysis ","9b0da079":"### Explained Variance Ratio ","f925e3e9":"We see there are names of the Iris Species in the data set.We will use label encoding to convert the Species Data into numeric data.","64bcaa15":"### Confusion Matrix"}}