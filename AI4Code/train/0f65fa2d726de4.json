{"cell_type":{"6f7a5ce2":"code","c9ed25ad":"code","84745384":"code","c0b1e804":"code","aca6079b":"code","1657e29a":"code","507dc8c6":"code","79f8fd29":"code","1f29e055":"code","9b3a7d3b":"code","31063047":"code","9ba6d7fb":"code","a4656cc6":"code","728edd55":"markdown","a8ce5331":"markdown","3b8a2ed4":"markdown","e8b88cd6":"markdown","071eeca7":"markdown","0add1109":"markdown","c7919411":"markdown","973a3938":"markdown","cf34bb7f":"markdown","8b2296ef":"markdown","b67a8dc6":"markdown"},"source":{"6f7a5ce2":"#\u00a0Main packages\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # plot handling","c9ed25ad":"#\u00a0Tensorflow-Keras packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","84745384":"#\u00a0Define parameters values\nIMG_SIZE = (256, 256)\nVALID_SPLIT = 0.3\nBATCH_SIZE = 32\nSEED = 42\nPATH = \"..\/input\/indian-food-classification\/Food Classification\"\n\n# Get train image with generator\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    PATH,\n    validation_split=VALID_SPLIT,\n    subset=\"training\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"categorical\"\n)\n\n# Get validation image with generator\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    PATH,\n    validation_split=VALID_SPLIT,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"categorical\"\n)","c0b1e804":"#\u00a0Get class names\nclasses = train_ds.class_names","aca6079b":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(classes[np.argmax(labels[i])])\n        plt.axis(\"off\")","1657e29a":"#\u00a0Data augmentation: kind of transformations\ndata_augmentation = keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n])\n\n#\u00a0Apply transformation to train data\naugmented_train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))","507dc8c6":"augmented_train_ds = augmented_train_ds.prefetch(buffer_size=32)\nval_ds = val_ds.prefetch(buffer_size=32)","79f8fd29":"def make_model(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n    # Image augmentation block\n    x = data_augmentation(inputs)\n\n    # Entry block\n    x = layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(x)\n    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    for size in [128, 256, 512, 512]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n    \n    return keras.Model(inputs, outputs)\n\nprint(IMG_SIZE + (3,))\nmodel = make_model(input_shape=IMG_SIZE + (3,), num_classes=len(classes))","1f29e055":"#\u00a0Extend the hidden output if you want to see the model structure\nmodel.summary()","9b3a7d3b":"ok = True\n\nif ok:\n    #\u00a0Parameters\n    EPOCHS = 100\n    CALLBACK = [\n        keras.callbacks.ModelCheckpoint(\"filepath='model.{epoch:02d}-{val_loss:.2f}.h5'\"),\n        keras.callbacks.EarlyStopping(patience=15)\n    ]\n    \n    model.compile(\n        optimizer='adam',\n        loss=\"categorical_crossentropy\",\n        metrics=[\"categorical_accuracy\"],\n    )\n    \n    history = model.fit(\n        augmented_train_ds,\n        epochs=EPOCHS,\n        callbacks=CALLBACK,\n        validation_data=val_ds,\n    )\n    \nelse:\n    model = keras.models.load_model('..\/input\/indian-food-classification-keras-cnn\/save_at_50.h5')","31063047":"# List all data in history\nprint(history.history.keys())","9ba6d7fb":"# Create a plot layout\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\n\n# Summarize history for accuracy\nax[0].plot(history.history['categorical_accuracy'])\nax[0].plot(history.history['val_categorical_accuracy'])\nax[0].set_title('Accuracy vs Epoch')\nax[0].set(xlabel='epoch', ylabel='accuracy')\nax[0].legend(['train', 'val'], loc='upper left')\n\n# Summarize history for loss\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Loss vs Epoch')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nax[1].legend(['train', 'val'], loc='upper left')\n\n#\u00a0Display plots\nplt.show()","a4656cc6":"imgs = [\n    \"..\/input\/indian-food-classification-test-images\/burger.jpg\",\n    \"..\/input\/indian-food-classification-test-images\/chai.jpeg\",\n    \"..\/input\/indian-food-classification-test-images\/chapati.jpeg\",\n    \"..\/input\/indian-food-classification-test-images\/dal_makhani.jpeg\",\n    \"..\/input\/indian-food-classification-test-images\/samosa.jpg\"\n]\n\nfor img in imgs:\n    try:\n        #\u00a0Load the image\n        raw_img = keras.preprocessing.image.load_img(img, target_size=IMG_SIZE)\n    except:\n        continue\n    \n    # Conver to to numpy array\n    img_array = keras.preprocessing.image.img_to_array(raw_img)\n    \n    #\u00a0Reshaping\n    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n    \n    # Make predictions\n    predictions = model.predict(img_array)\n    series      = pd.Series(predictions[0], index=classes)\n    \n    # Get score\n    proba      = np.max(predictions)\n    pred_class = classes[np.argmax(predictions)]\n    \n    # Display results\n    fig, ax = plt.subplots(1, 2, figsize=(24, 4))\n    \n    ax[0].imshow(raw_img)\n    ax[0].title.set_text(f\"This image may be {pred_class} at {round(proba * 100,2)}%\")\n    \n    series.plot(kind='bar', ax=ax[1], color='green')\n    ax[1].title.set_text(\"Probabilities\")\n    ax[1].tick_params(labelrotation=75)\n    \n    plt.show()","728edd55":"<div class=\"alert alert-block alert-danger\">\u26a0\ufe0f If you already have a separate validation directory, then your directory setup would look like the following representation... And the code below will have to change by consequence!<\/div>\n\n```bash\n.\n\u2514\u2500\u2500 Food Classification\n \u00a0\u00a0 \u2514\u2500\u2500 train\n    \u2502    \u251c\u2500\u2500 burger\n    \u2502    \u2502     \u251c\u2500\u2500 001.jpg\n    \u2502    \u2502     \u2514\u2500\u2500 002.jpg\n    \u2502     ...\n    \u2502    \u2514\u2500\u2500 samosa\n    \u2502          \u251c\u2500\u2500 001.jpg\n    \u2502          \u2514\u2500\u2500 002.jpg\n    \u2514\u2500\u2500 valid\n          \u251c\u2500\u2500 burger\n          \u2502     \u251c\u2500\u2500 001.jpg\n          \u2502     \u2514\u2500\u2500 002.jpg\n           ...\n          \u2514\u2500\u2500 samosa\n                \u251c\u2500\u2500 001.jpg\n                \u2514\u2500\u2500 002.jpg\n```","a8ce5331":"# Run inference on data","3b8a2ed4":"# Configure the dataset for performance","e8b88cd6":"# Indian Food Classification | Tutorial Image Classification Keras\n\n<div style=\"align:center\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1147479\/1923965\/38c32cc6477b8cfb689d3cf85dea70a5\/dataset-cover.jpg?t=2021-02-09-06-46-05\">\n<\/div>\n\n<br>\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#D15817;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n    \n    <p style=\"padding: 10px;color:white;\">CAUTION: Read at your own risk... You might get hungry!<\/p>\n<\/div>\n\n<br>\n\n## Audience target\n<div style=\"display: inline-black\">\n    <img src=\"https:\/\/img.shields.io\/badge\/level-beginner-green\">\n    <img src=\"https:\/\/img.shields.io\/badge\/Made%20with-Python3.8-darkgreen?style=flat&logo=python&logoColor=white\">\n    <img src=\"https:\/\/img.shields.io\/badge\/Made%20on-Kaggle-20beff?style=flat&logo=kaggle&logoColor=white\">\n<\/div>\n\n## Data description\n\n> <ins><b>Full description<\/b> (by [@Pushkar Jain](https:\/\/www.kaggle.com\/l33tc0d3r))<\/ins>: This Dataset contains different images of food in 20 different classes. Some of the classes are of Indian food. All the images are extracted from google. Images per classes are little so Data augmentation and transfer learning will be best suited here. <br><br>\nThe first goal is to be able to automatically classify an unknown image using the dataset, but beyond this, there are several possibilities for looking at what regions\/image components are important for making classifications, build object detectors which can find similar objects in a full scene.\n\nIn short here are the main characteristics of the data used for this notebook:\n1. **Image** data -> for **classification** issue\n2. Classification issue with **20 classes**\n3. Structured dataset, optimized to used with **tensorflow-keras**\n\n## Acknowledgements\n> This notebook is inspired by *Image classification from scratch* from [keras.io](https:\/\/keras.io\/examples\/vision\/image_classification_from_scratch\/) which is a tutorial of building a image classification for dogs vs cat. The author of this tutorial is **Fran\u00e7ois CHOLLET**.","071eeca7":"# Generate datasets\nIn this section the data will be loaded by using `image_dataset_from_directory` function from the `tf.keras.preprocessing` module, it is allowed here because we have the following directory setup, where every pictures of food belonging to a class will be placed in the corresponding directory:\n```bash\n.\n\u2514\u2500\u2500 Food Classification\n \u00a0\u00a0 \u2514\u2500\u2500 burger\n    \u2502    \u251c\u2500\u2500 001.jpg\n    \u2502    \u251c\u2500\u2500 002.jpg\n    \u2502    \u2514\u2500\u2500 003.jpg\n \u00a0\u00a0 \u2514\u2500\u2500 butter_naan\n    \u2502    \u251c\u2500\u2500 001.jpg\n    \u2502    \u251c\u2500\u2500 002.jpg\n    \u2502    \u2514\u2500\u2500 003.jpg\n    ...\n    \u2514\u2500\u2500 pizza\n    \u2502    \u251c\u2500\u2500 001.jpg\n    \u2502    \u251c\u2500\u2500 002.jpg\n    \u2502    \u2514\u2500\u2500 003.jpg\n    \u2514\u2500\u2500 samosa\n          \u251c\u2500\u2500 001.jpg\n          \u251c\u2500\u2500 002.jpg\n          \u2514\u2500\u2500 003.jpg\n```","0add1109":"### Using image data augmentation\n\n<div style=\"text-align:center;\">\n    <span style=\"color:red\"><b>Data Augmentation<\/b><\/span>\n: \"<em>Data Augmentation is a regularization technique that\u2019s used to avoid overfitting when training Machine Learning models. Although the technique can be applied in a variety of domains, it\u2019s very common in Computer Vision, and this will be the focus of the tutorial. Adjustments are made to the original images in the training dataset before being used in training. Some example adjustments include translating, croping, scaling, rotating, changing brightness and contrast. We do this to reduce the dependence of the model on spurious characteristics; e.g. training data may only contain faces that fill 1\/4 of the image, so the model trainied without data augmentation might unhelpfully learn that faces can only be of this size.<\/em>\" - (Apache MXNet, Data Augmentation With Masks)\n<\/div>\n\n\n\n<div style=\"align:center\">\n    <figure>\n        <img src=\"https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/mxnet\/doc\/tutorials\/data_aug\/outputs\/with_mask\/orig_vs_aug.png\">\n        <figcaption>From <a href=\"https:\/\/mxnet.apache.org\/versions\/1.2.1\/tutorials\/python\/data_augmentation_with_masks.html\">Data Augmentation with Masks<\/a><\/figcaption>\n    <\/figure>\n<\/div>","c7919411":"# Build a model\n","973a3938":"# Visualize the data\n\nLet us take a look at what our data look like!","cf34bb7f":"<div class=\"alert alert-block alert-danger\">\u26a0\ufe0f Hey dear readers! The following code for the CNN model has been made without that much of thought, I only took the code from tutorial available on <a href=\"https:\/\/keras.io\/examples\/vision\/image_classification_from_scratch\/\">keras.io<\/a>. I can take every suggestions in order to make it better, I am still studying datascience so I apologize in advance I may offend professionals for presenting this model.<\/div>","8b2296ef":"# Setup\nIn this section the main **Python** packages and the packages for the CNN modelling will be loaded.","b67a8dc6":"# Train the model OR Load the mode\ndepending on the value of `ok`:\n- `True`  : Train\n- `False` : Load"}}