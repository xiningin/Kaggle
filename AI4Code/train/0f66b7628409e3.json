{"cell_type":{"87d5be5b":"code","4910b59b":"code","b4841986":"code","9991d243":"code","d7dfd82f":"code","780952fb":"code","32c989b4":"code","a789b5e8":"code","1bd0f9a6":"code","77b1950a":"code","0682d67e":"code","e30709d0":"code","5e8f66b1":"code","4ea5349b":"code","d060769f":"code","6888414a":"code","254d0ae6":"markdown","969e5973":"markdown","eff15d1e":"markdown","a7e8b360":"markdown","68db59f9":"markdown","7ee4cf65":"markdown","7114b4c2":"markdown","8b359f0f":"markdown","1f15fb6f":"markdown"},"source":{"87d5be5b":"import numpy as np \nimport pandas as pd \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score  \nfrom sklearn import neighbors\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\n%matplotlib inline\nimport matplotlib.pyplot as plt","4910b59b":"data = '..\/input\/EDAdataHCC.csv'\ndataHCC = pd.read_csv(data)\ndataHCC.head(5)","b4841986":"array = dataHCC.values\nX = array[:,1:38]  # feature vector\ny = array[:,40]  # class label ","9991d243":"#Counting the instances healthy and unhealthy\nprint (\"Instances:\", y.size)\nprint (\"Class 0:\", y[y==0].size)\nprint (\"Class 1:\", y[y==1].size)","d7dfd82f":"X_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.3, random_state=1)","780952fb":"print('Features - X_train: ', X_train.shape)\nprint('Class label - y_train: ', y_train.shape)\nprint('Features - X_test: ', X_test.shape)\nprint('Class label - y_test: ', y_test.shape)","32c989b4":"X_train= StandardScaler().fit_transform(X_train)\nX_test= StandardScaler().fit_transform(X_test)","a789b5e8":"pca = PCA(n_components = 5)\nX_train =pca.fit_transform(X_train)\nX_test =pca.fit_transform(X_test)","1bd0f9a6":"#PLOT\nplt.scatter(X_train[:,0],X_train[:,1],  c=y_train) ","77b1950a":"######Function plot with diferents k's\ndef plotvector(XTrain, yTrain, XTest, yTest, weights):\n    results = []\n    \n    for n in range(1, 25, 2):\n        clf = neighbors.KNeighborsClassifier(n_neighbors=n, weights=weights)\n        clf = clf.fit(XTrain, yTrain)\n        preds = clf.predict(XTest)\n        accuracy = clf.score(XTest, yTest)\n        results.append([n, accuracy])\n \n    results = np.array(results)\n    return(results)\n\n###### Plot \npltvector1 = plotvector(X_train, y_train, X_test, y_test, weights=\"uniform\")\nline1 = plt.plot(pltvector1[:,0], pltvector1[:,1], label=\"uniform\")\n\nplt.legend(loc=3)\nplt.ylim(0.5, 0.8)\nplt.title(\"Accuracy with different K's\")\nplt.grid(True)\nplt.show()","0682d67e":"clfknn = KNeighborsClassifier(algorithm='auto', metric='euclidean', n_neighbors=3, weights='uniform')\nclfknn.fit(X_train, y_train)\n#To prediction use the line below -  To prediction you need the test sample. \n#y_pred = classifier.predict(X_test)  ","e30709d0":"accuracy_knn= clfknn.score(X_train, y_train)\nprint (\"Accuracy - (train):\", accuracy_knn)","5e8f66b1":"accuracy_knn = clfknn.score(X_test, y_test)\nprint (\"Accuracy - (test):\", accuracy_knn)","4ea5349b":"# Predict using test sample\npredict_test = clfknn.predict(X_test)","d060769f":"print(\"Confusion matrix - kNN (auto, euclidean, uniform)\")\nprint(\"{0}\".format(metrics.confusion_matrix(y_test, predict_test, labels=[1, 0])))\nprint(classification_report(y_test, predict_test))  ","6888414a":"all_accuracies = cross_val_score(estimator=clfknn, X=X_train, y=y_train, cv=10)  \nprint('All accuracies:',all_accuracies)  \nprint('Mean accuracy:',all_accuracies.mean())  ","254d0ae6":"#### 4-Classification KNN <h4>\n\n4.1- Defining neighbors number. ","969e5973":"4.4-Cross validation. ","eff15d1e":"4.2- modeling and training.","a7e8b360":"To use knn, it's necessary defining the test set and the training set. For the test set, thirty percent of the data set was defined.","68db59f9":"#### 3-Pipeline <h4>\nIn this step, I used PCA and StandardScaler to improve the data. ","7ee4cf65":"#### 1-Reading data <H4>\n\nIn this work, I used the database from kernel '[HCC survival (EDA + dataset cleaning)](http:\/\/https:\/\/www.kaggle.com\/mirlei\/hcc-survival-eda-dataset-cleaning)'. \n\n","7114b4c2":"### Classification using KNN <H3>\n\nKNN is an algorithm used in machine learning unsupervised.\n","8b359f0f":"4.3-Accuracy. ","1f15fb6f":"#### 2-Separating the data<h4>\n\nIt is necessary to separate the label from the attributes. \n"}}