{"cell_type":{"5642ce2b":"code","565ea5d9":"code","cdf5c64a":"code","5c10608a":"code","67d142b2":"code","22827f2d":"code","68485c90":"code","7cafd33e":"code","b5ac09f4":"code","8053aa7d":"code","5b9d1ca6":"code","b4d82360":"code","6da17153":"code","a3cb0f63":"code","620412da":"code","ff854d88":"code","94daba16":"code","93f208f4":"code","e31c60ad":"code","2c31259d":"code","f85a9ef6":"code","a0b2668a":"code","95687038":"code","d24b58a7":"code","6a66203e":"code","61483d84":"code","63763c82":"code","83dea8a8":"code","0de74d93":"code","ccc56913":"code","fba8235c":"code","a4d976b6":"code","6eedb7db":"code","c4557267":"markdown","d282e1b4":"markdown","69fa5c3a":"markdown","061e9fc9":"markdown","75c90eaa":"markdown","5fde0873":"markdown","89b19101":"markdown","5b3e5fb9":"markdown","2e0edb1d":"markdown","daababf8":"markdown","b984d8f7":"markdown","37940223":"markdown","619ca18b":"markdown","3eb50e46":"markdown","f1741ef9":"markdown"},"source":{"5642ce2b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\n\nimport re\nimport string\nimport emoji\n\nimport warnings\nwarnings.filterwarnings('ignore')","565ea5d9":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n","cdf5c64a":"train_data.head()","5c10608a":"print(\"Total rows in train data: \",train_data.shape[0])\nprint(\"Total columns in train data: \",train_data.shape[1])\nprint(\"-\"*30)\nprint(\"Total rows in test data: \",test_data.shape[0])\nprint(\"Total columns in test data: \",test_data.shape[1])\n\n\n","67d142b2":"print(train_data.isnull().sum())\nprint(\"-\"*30)\nprint(test_data.isnull().sum())\n","22827f2d":"train_data.groupby(train_data.target).count().text","68485c90":"sns.countplot(train_data.target,data = train_data)\nplt.show()","7cafd33e":"train_data = train_data.iloc[:,3:]\ntest_data = test_data.iloc[:,3:]\n","b5ac09f4":"\n    \ndef cleanTweet(txt):\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    txt = re.sub(r'\\n','',txt)\n    # to remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    txt = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+','',txt)\n    txt = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\",\"\",txt)\n    txt = re.sub(r\"<.*?>\",\"\",txt)\n    return txt  \n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef make_Lower(text):\n    return str.lower(text)","8053aa7d":"print(\"Tweet before cleaning:: \",train_data.text[200])\nprint(\"Tweet after cleaning:: \",train_data.text.apply(cleanTweet)[200])","5b9d1ca6":"text = \"#@Hello &World\"\nprint(remove_punct(text))","b4d82360":"text = 'HELLO WORLD '\nprint(make_Lower(text))","6da17153":"train_data.text = train_data.text.apply(cleanTweet)\ntrain_data.text = train_data.text.apply(remove_punct)\ntrain_data.text = train_data.text.apply(make_Lower)\n\ntest_data.text = test_data.text.apply(cleanTweet)\ntest_data.text = test_data.text.apply(remove_punct)\ntest_data.text = test_data.text.apply(make_Lower)","a3cb0f63":"!pip install pyspellchecker","620412da":"\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"speling mistke\"\ncorrect_spellings(text)","ff854d88":"# You can use this function to correct spellings \n# train_data.text = train_data.text.apply(correct_spellings)\n# test_data.text = test_data.text.apply(correct_spellings)","94daba16":"from nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    return \" \".join(text)","93f208f4":"train_data.text = train_data.text.apply(remove_stopwords)\ntest_data.text = test_data.text.apply(remove_stopwords)\n","e31c60ad":"vocab = 20000\noov = '<OOV>'\nembedding = 32\npadding = 'post'\ntruncate = 'post'\n","2c31259d":"# import necessary libraries for text preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n","f85a9ef6":"# preparing training data\ntrain_ = train_data.drop('target',axis = 1)\ntrain_y = train_data.target","a0b2668a":"\ntrain_size = 0.8\nsize = int(train_size * train_data.shape[0])\ntrain_x_data = train_.iloc[:size,:]\ntrain_y_data =  train_y.iloc[:size]\nprint(\"Shape of X train data: \",train_x_data.shape)\nprint(\"Shape of Y train data\",train_x_data.shape)\n\nvalidation_x_data = train_.iloc[size:,:]\nvalidation_y_data =  train_y.iloc[size:]\nprint(\"Shape of X validation data: \",validation_x_data.shape)\nprint(\"Shape of Y validation data\",validation_x_data.shape)\n","95687038":"tokenizer = Tokenizer(num_words = vocab, oov_token = oov)\ntokenizer.fit_on_texts(train_x_data.text)\nword_index = tokenizer.word_index\nprint(\"Length: \",len(word_index))\n","d24b58a7":"training_x = tokenizer.texts_to_sequences(train_x_data.text)\ntraining_x_pad = pad_sequences(training_x,maxlen=25, padding=padding, truncating=truncate)\n\nvalidation_x = tokenizer.texts_to_sequences(validation_x_data.text)\nvalidation_x_pad = pad_sequences(validation_x,maxlen=25, padding=padding, truncating=truncate)\n\ntraining_y = train_y_data.values\nvalidation_y = validation_y_data.values","6a66203e":"import tensorflow as tf\nimport keras\nfrom keras import layers \n\ndef give_model():\n    model = keras.models.Sequential()\n    model.add(layers.Embedding(vocab, embedding, input_length=25))\n    model.add(layers.Bidirectional(layers.LSTM(128,return_sequences = True)))\n    model.add(layers.LSTM(64))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(units = 10,activation = 'relu'))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    \n    return model\nmodel = give_model()\nmodel.summary()","61483d84":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',patience = 2, mode = 'min', min_delta = 0.01)\nLOSS = tf.keras.losses.BinaryCrossentropy()\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate = 0.001)\nEPOCHS = 30\nVALIDATION_DATA = (validation_x_pad,validation_y)\nmodel.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = ['accuracy'])","63763c82":"history = model.fit(training_x_pad, training_y, epochs = EPOCHS, validation_data = VALIDATION_DATA,\n                   callbacks = [early_stopping])","83dea8a8":"\nplt.subplot(2,1,1)\nplt.plot( history.history['loss'], label = 'loss')\nplt.plot( history.history['val_loss'], label = 'val_loss')\nplt.legend(loc = 'best')\nplt.subplot(2,1,2)\nplt.plot( history.history['accuracy'], label = 'accuracy')\nplt.plot( history.history['val_accuracy'], label = 'val_accuracy')\nplt.legend(loc = 'best')\nplt.show()","0de74d93":"\ntokenizer = Tokenizer(num_words = vocab, oov_token = oov)\ntokenizer.fit_on_texts(train_.text)\nword_index = tokenizer.word_index\nprint(\"Length: \",len(word_index))\n\ntraining_x = tokenizer.texts_to_sequences(train_.text)\ntraining_x_pad = pad_sequences(training_x,maxlen=25, padding=padding, truncating=truncate)\n\ntesting_x_data = tokenizer.texts_to_sequences(test_data.text)\ntesting_x_pad = pad_sequences(testing_x_data,maxlen=25, padding=padding, truncating=truncate)\n\ntraining_y = train_y.values\n\n","ccc56913":"model.fit(training_x_pad, training_y, epochs = 3)","fba8235c":"prediction = model.predict(testing_x_pad)\npredicted_value = (prediction > 0.5).astype(int)","a4d976b6":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission.head()","6eedb7db":"submission.target = predicted_value\nsubmission.head()","c4557267":"#### Here, I am going to use only text column for analysis so I am dropping other columns","d282e1b4":"## For submission","69fa5c3a":"# Thank You \n## Feel free to  comment","061e9fc9":"**Here I have splitted training data to train and validation data to check the performance of model**","75c90eaa":"## Defining our model","5fde0873":"### Fitting our data ","89b19101":"#### Visualizing our model performance","5b3e5fb9":"## Training our model in all train data","2e0edb1d":"### Counting and visualizing total positive and negative target in our training data","daababf8":"### Load Data","b984d8f7":"## Cleaing tweets\nCleaning process involves removal of emojis,hyperlink,punctuations and many more.","37940223":"**Defining hyperparameters**","619ca18b":"### Import Libraries","3eb50e46":"**Checking for null data. Since keyword and location columns has null value i am going to drop them**","f1741ef9":"**Removing Stopwords from data**"}}