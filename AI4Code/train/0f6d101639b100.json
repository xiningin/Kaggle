{"cell_type":{"b2af05a0":"code","d85139d4":"code","de06cbcc":"code","76ac27d8":"code","6afbf9fb":"code","6ec1555b":"code","a6dc3d8e":"code","c8df96b1":"code","79a5ba96":"code","315e6c4a":"code","20da4729":"code","58627a52":"code","5be4c0e8":"code","2d693ffb":"code","93982fda":"code","32fb2e5b":"code","90a50531":"markdown","1906afaa":"markdown","32eaf610":"markdown","115c385c":"markdown","c8ede82a":"markdown","cc9f7b0b":"markdown","822e97c6":"markdown"},"source":{"b2af05a0":"# !pip install -U catalyst albumentations","d85139d4":"# For Colab user: download dataset and upload zip files.\n# If you use Kaggle Notebooks, you already have the dataset in a hard drive.\n\n# !gdown https:\/\/drive.google.com\/uc?id=19fBCItau0MP1ABKlBNkpj1pMxOzIZLML&export=download\n# !gdown https:\/\/drive.google.com\/uc?id=1X7TLVCvi2a57SyjAdExPppzakxfenRD0&export=download\n# !unzip train.zip -d train\n# !unzip test.zip -d test","de06cbcc":"from datetime import datetime\nimport numpy as np\nfrom pathlib import Path\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data\n\nimport catalyst\nfrom catalyst import dl\nfrom catalyst.utils import metrics, imread, set_global_seed","76ac27d8":"set_global_seed(42)","6afbf9fb":"train_image_path = Path(\"..\/input\/test-segm-comp\/train\") \/ \"images\"\ntrain_mask_path = Path(\"..\/input\/test-segm-comp\/train\") \/ \"masks\"\nALL_IMAGES = sorted(train_image_path.glob(\"*.png\"))\nALL_MASKS = sorted(train_mask_path.glob(\"*.png\"))","6ec1555b":"from torch.utils.data import Dataset\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, images=None, masks=None, transforms=None) -> None:\n        self.images = images\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self) -> int:\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -> dict:\n        image_path = self.images[idx]\n        image = imread(image_path)\n\n        result = {\"image\": image}\n\n        if self.masks is not None:\n            result[\"mask\"] = imread(self.masks[idx]).mean(2) \/\/ 255\n\n        if self.transforms is not None:\n            result = self.transforms(**result)\n            if result.get(\"mask\", None) is not None:\n                result[\"mask\"] = result[\"mask\"].unsqueeze(0)\n\n        result[\"filename\"] = image_path.name\n        result[\"image size\"] = image.shape[:2]\n\n        return result","a6dc3d8e":"import albumentations as albu\nfrom albumentations.pytorch import ToTensorV2 as ToTensor\nimport cv2\n\nIMAGE_SIZE = 256\ntrain_transform = albu.Compose([\n    albu.HorizontalFlip(p=0.5),\n    albu.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    albu.RandomResizedCrop(IMAGE_SIZE, IMAGE_SIZE, p=0.3),\n    albu.Normalize(),\n    ToTensor()\n])\n\nvalid_transform = albu.Compose([\n    albu.Resize(IMAGE_SIZE, IMAGE_SIZE),\n    albu.Normalize(),\n    ToTensor()\n])","c8df96b1":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\nbatch_size = 16\nnum_workers = 4\n\nindices = np.arange(len(ALL_IMAGES))\n\ntrain_indices, valid_indices = train_test_split(\n    indices, test_size=0.2, random_state=42, shuffle=True\n)\n\nnp_images = np.array(ALL_IMAGES)\nnp_masks = np.array(ALL_MASKS)\n\ntrain_dataset = SegmentationDataset(\n    images = np_images[train_indices].tolist(),\n    masks = np_masks[train_indices].tolist(),\n    transforms = train_transform\n)\n\nvalid_dataset = SegmentationDataset(\n    images = np_images[valid_indices].tolist(),\n    masks = np_masks[valid_indices].tolist(),\n    transforms = valid_transform\n)\n\nloaders = {\n    \"train\": DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        drop_last=True,\n    ),\n    \"valid\": DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        drop_last=True,\n    )\n}\n","79a5ba96":"class Baseline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.down_1 = self.make_down_layer_(3, 64)\n        self.down_2 = self.make_down_layer_(64, 128)\n        self.down_3 = self.make_down_layer_(128, 256)\n        self.down_4 = self.make_down_layer_(256, 512)\n\n        self.up_1 = self.make_up_layer_(512, 256)\n        self.up_2 = self.make_up_layer_(256, 128)\n        self.up_3 = self.make_up_layer_(128, 64)\n        self.up_4 = nn.Sequential(\n            nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1, stride=2, output_padding=1)\n        )\n\n    def make_down_layer_(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n    def make_up_layer_(self, in_channels, out_channels):\n        return nn.ModuleList(\n            [\n                nn.ConvTranspose2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                ),\n                nn.BatchNorm2d(2 * out_channels),\n                nn.ReLU(),\n                nn.ConvTranspose2d(\n                    2 * out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                ),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            ]\n        )\n\n    def forward(self, image):\n        x_1 = self.down_1(image)\n        x_2 = self.down_2(x_1)\n        x_3 = self.down_3(x_2)\n        x_4 = self.down_4(x_3)\n\n        u_1 = self.up_1[0](x_4)\n        u_1 = torch.cat([x_3, u_1], axis=1)\n        for m in self.up_1[1:]:\n            u_1 = m(u_1)\n        \n        u_2 = self.up_2[0](u_1)\n        u_2 = torch.cat([x_2, u_2], axis=1)\n        for m in self.up_2[1:]:\n            u_2 = m(u_2)\n\n        u_3 = self.up_3[0](u_2)\n        u_3 = torch.cat([x_1, u_3], axis=1)\n        for m in self.up_3[1:]:\n            u_3 = m(u_3)\n\n        return self.up_4(u_3)","315e6c4a":"from catalyst.contrib.nn import DiceLoss, IoULoss\nfrom catalyst.dl.runner import SupervisedRunner\nfrom torch.nn.functional import interpolate\n\n\nclass SegmentationRunner(SupervisedRunner):\n    def predict_batch(self, batch):\n        prediction = {\"filename\": batch[\"filename\"]}\n        masks = self.model(batch[self.input_key].to(self.device))\n        image_size = list(zip(*batch[\"image size\"]))\n        prediction[\"mask\"] = [\n            interpolate(mask.unsqueeze(0), image_size).squeeze(0)\n            for mask, image_size in zip(masks, image_size)\n        ]\n        return prediction\n\n# we have multiple criterions\nmodel = Baseline()\ncriterion = {\n    \"dice\": DiceLoss(),\n    \"iou\": IoULoss(),\n    \"bce\": nn.BCEWithLogitsLoss()\n}\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0003)\n\nrunner = SegmentationRunner(input_key=\"image\", input_target_key=\"mask\")","20da4729":"callbacks = [\n    dl.CriterionCallback(\n        input_key=\"mask\", prefix=\"loss_dice\", criterion_key=\"dice\"\n    ),\n    dl.CriterionCallback(\n        input_key=\"mask\", prefix=\"loss_iou\", criterion_key=\"iou\"\n    ),\n    dl.CriterionCallback(\n        input_key=\"mask\", prefix=\"loss_bce\", criterion_key=\"bce\"\n    ),\n    dl.MetricAggregationCallback(\n        prefix=\"loss\",\n        mode=\"weighted_sum\",\n        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n    ),\n    dl.DiceCallback(input_key=\"mask\"),\n    dl.IouCallback(input_key=\"mask\"),\n]","58627a52":"runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    callbacks=callbacks,\n    logdir=Path(\"logs\") \/ datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n    num_epochs=50,\n    main_metric=\"iou\", # kaggle competition metric\n    minimize_metric=False,\n    verbose=True,\n)","5be4c0e8":"def rle_encoding(x):\n    \"\"\"\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    \"\"\"\n    dots = np.where(x.T.flatten() == 1)[\n        0\n    ]  # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return \" \".join([str(i) for i in run_lengths])","2d693ffb":"from PIL import Image\nimport pandas as pd\n\nsubmission = {\"ImageId\": [], \"EncodedPixels\": []}\nthreshold = 0.5\n\ntest_image_path = Path(\"..\/input\/test-segm-comp\/test\")# \/ \"images\"\nTEST_IMAGES = sorted(test_image_path.glob(\"*.png\"))\ntest_dataset = SegmentationDataset(\n    images=TEST_IMAGES,\n    transforms=valid_transform\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n)\n\nfor prediction in runner.predict_loader(loader=test_loader):\n    submission[\"ImageId\"].extend(s[:-4] for s in prediction[\"filename\"])\n    submission[\"EncodedPixels\"].extend(\n        rle_encoding(torch.sigmoid(mask).cpu().numpy().mean(0) > threshold) for mask in prediction[\"mask\"]\n    )","93982fda":"pd.DataFrame(submission).to_csv(\"submission.csv\", index=False)","32fb2e5b":"pd.DataFrame(submission)","90a50531":"## Dataset\n\nLoad train data. Don't forget to add test data. Use test data, to compare methods\/models\/etc.","1906afaa":"## Submission\n\nTo generate submission, you'll have to write masks for images.\nUsually, in `Kaggle` segmentation competitions masks are encoded in the run length format.\nFor more information, check `Evaluation` page in `Overview`.","32eaf610":"# Segmentation task\n\nHi! It's a segmentation task baseline notebook.\nIt include a data reader, baseline model and submission generator.\n\nYou should use GPU to train your model, so we recommend using [Kaggle Notebooks](https:\/\/www.kaggle.com\/docs\/notebooks).\nTo get maximum score of the task, your model should have IoU greater than `0.8`.\n\nYou can use everything, that suits into the rules in `README.md`.","115c385c":"Our current baseline model is `U-Net`.\nYou can do anything with it: add pretrained backbone, make model wider or deeper or change a model architecture.\nYou can use `torchvision` module to create a backbone, but not a whole model.","c8ede82a":"Basic model of the second homework of the course \"Deep Learning with Catalyst\"\n[https:\/\/github.com\/catalyst-team\/dl-course\/tree\/master\/homework-2\/segmentation](http:\/\/)","cc9f7b0b":"## Augmentations\n\nTo train an accurate model for a segmentation task, you need a lot of data.\nUse data augmentations to simulate a bigger dataset.","822e97c6":"This code below will generate a submission.\nIt reads images from `test` folder and gathers prediction from the trained model.\nCheck your submission before uploading it into `Kaggle`."}}