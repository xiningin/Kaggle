{"cell_type":{"ddaeecd1":"code","225a3c3d":"code","0bf38c5b":"code","a5ae7098":"code","373c481c":"code","7e05a424":"code","2f204df8":"code","90792629":"code","10c5c2b2":"markdown","b96e81af":"markdown","137bde65":"markdown","dafa39cc":"markdown","52627074":"markdown"},"source":{"ddaeecd1":"import pandas as pd \nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import SpectralClustering \nfrom sklearn.preprocessing import StandardScaler, normalize \nfrom sklearn.decomposition import PCA \nfrom sklearn.metrics import silhouette_score ","225a3c3d":"raw_df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\nraw_df = raw_df.drop('CUST_ID', axis = 1) \nraw_df.fillna(method ='ffill', inplace = True) \nraw_df.head(2)","0bf38c5b":"# Preprocessing the data to make it visualizable \n  \n# Scaling the Data \nscaler = StandardScaler() \nX_scaled = scaler.fit_transform(raw_df) \n  \n# Normalizing the Data \nX_normalized = normalize(X_scaled) \n  \n# Converting the numpy array into a pandas DataFrame \nX_normalized = pd.DataFrame(X_normalized) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(X_normalized) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2) ","a5ae7098":"# Building the clustering model \nspectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') \n  \n# Training the model and Storing the predicted cluster labels \nlabels_rbf = spectral_model_rbf.fit_predict(X_principal)","373c481c":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","7e05a424":"# Building the clustering model \nspectral_model_nn = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') \n  \n# Training the model and Storing the predicted cluster labels \nlabels_nn = spectral_model_nn.fit_predict(X_principal)","2f204df8":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = SpectralClustering(n_clusters = 2, affinity ='nearest_neighbors') .fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","90792629":"# List of different values of affinity \naffinity = ['rbf', 'nearest-neighbours'] \n  \n# List of Silhouette Scores \ns_scores = [] \n  \n# Evaluating the performance \ns_scores.append(silhouette_score(raw_df, labels_rbf)) \ns_scores.append(silhouette_score(raw_df, labels_nn)) \n  \n# Plotting a Bar Graph to compare the models \nplt.bar(affinity, s_scores) \nplt.xlabel('Affinity') \nplt.ylabel('Silhouette Score') \nplt.title('Comparison of different Clustering Models') \nplt.show() \n\nprint(s_scores)","10c5c2b2":"<b> <u> Affinity matrix with Gaussian Kernel  <\/u><\/b>\n\n<b> \"affinity\" = \"rbf\" <\/b>","b96e81af":"# <u> Spectral Clustering <\/u>\n\nIn spectral clustering, data points are treated as nodes of a graph. Thus, spectral clustering is a graph partitioning problem. The nodes are then mapped to a low-dimensional space that can be easily segregated to form clusters. No assumption is made about the shape\/form of the clusters. The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries.\n\n### <u> Spectral Clustering vs. Kmeans <\/u>\n\n- <b><u> Compactness <\/u><\/b> \u2014 Points that lie close to each other fall in the same cluster and are compact around the cluster center. The closeness can be measured by the distance between the observations. E.g.: <b> K-Means Clustering <\/b>\n- <b><u> Connectivity <\/u><\/b> \u2014 Points that are connected or immediately next to each other are put in the same cluster. Even if the distance between 2 points is less, if they are not connected, they are not clustered together. <b> Spectral Clustering <\/b> is a technique that follows this approach.\n\n![Screen%20Shot%202019-08-30%20at%2012.30.02.png](attachment:Screen%20Shot%202019-08-30%20at%2012.30.02.png)\n\n Datasets where spectral clustering is applied for clustering:\n \n ![Screen%20Shot%202019-08-30%20at%2012.30.15.png](attachment:Screen%20Shot%202019-08-30%20at%2012.30.15.png)\n \n K-means will fail to effectively cluster these, even when the true number of clusters K is known to the algorithm. K-means, as a <i> data-clustering <\/i> algorithm, ideal for discovering globular clusters where all members of each cluster are in close proximity to each other (in Euclidean sense).\n\nSpectral clustering is more general (and powerful) because if we just use Eucledean Distance in it's similarity matrix, It will behave like k-means. The converse is not true though.\n\nIf we have \ud835\udc43 data points each with \ud835\udc41 dimensions\/features, input matrix to K-means would be \ud835\udc41 by \ud835\udc43, while input matrix to spectral clustering would be \ud835\udc43 by \ud835\udc43. Spectral clustering is indifferent to the number of features we use (Gaussian kernel which can be thought of as an infinite-dimensional feature transformation is particularly popular when using spectral clustering). We will face difficulties applying spectral clustering (at least the vanilla version) to very large datasets (large \ud835\udc43).\n\n### <u> Algorithm: <\/u>\n- Project data into $R^{n}$ matrix\n- Define an Affinity  matrix A , using a Gaussian Kernel K  or an Adjacency matrix\n- Construct the Graph Laplacian from A  (i.e. decide on a normalization)\n- Solve the Eigenvalue problem\n- Select k eigenvectors corresponding to the k lowest (or highest) eigenvalues to define a k-dimensional subspace \n- Form clusters in this subspace using k-means\n\n#### <u> Affinity Matrix: <\/u>\n\nWe first create an undirected graph G = (V, E) with vertex set V = {v1, v2, \u2026, vn} = 1, 2, \u2026, n observations in the data.\n\n- <b> <u> Epsilon-neighbourhood Graph: <\/u> <\/b> A parameter epsilon is fixed beforehand. Then each point is connected to all the points which lie in it\u2019s epsilon-radius. If all the distances between any two points are similar in scale then typically the weights of the edges ie the distance between the two points are not stored since they do not provide any additional information. Thus, in this case, the graph built is an undirected and unweighted graph.\n- <b> <u> K-Nearest Neighbours: <\/u> <\/b> A parameter k is fixed beforehand. Then, for two vertices u and v, an edge is directed from u to v only if v is among the k-nearest neighbours of u. Note that this leads to the formation of a weighted and directed graph because it is not always the case that for each u having v as one of the k-nearest neighbours, it will be the same case for v having u among its k-nearest neighbours. To make this graph undirected, one of the following approaches are followed:-\n     - Direct an edge from u to v and from v to u if either v is among the k-nearest neighbours of u <b> OR <\/b> u is among the k-nearest neighbours of v.\n     - Direct an edge from u to v and from v to u if v is among the k-nearest neighbours of u <b> AND <\/b> u is among the k-nearest neighbours of v.\n     \n- <b> <u> Fully-Connected Graph: <\/u><\/b> To build this graph, each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance.\n\n$$S(x_i, x_j) = exp(-\\frac{||x_i - x_j||^2}{2\\sigma^2})$$\n\nThus, when we create an adjacency matrix for any of these graphs, Aij ~ 1 when the points are close and Aij \u2192 0 if the points are far apart. \n\nConsider the following graph with nodes 1 to 4, weights (or similarity) wij and its adjacency matrix:\n\n![Screen%20Shot%202019-08-31%20at%2018.08.42.png](attachment:Screen%20Shot%202019-08-31%20at%2018.08.42.png)\n\n<b><u>Metric :<\/u><\/b>\n\n Affinity metric determines how close, or similar, two points our in our space. We will use a Gaussian Kernel and not the standard Euclidean metric.\n\nGiven 2 data points $x_{i},x_{j}$  (projected in $R^{n}$ ), we define an Affinity $A_{i,j}$  that is positive, symmetric, and depends on the Euclidian distance $\\Vert x_{i}-x_{j}\\Vert$  between the data points\n\n$$A_{ij} = {e}^{-\\alpha \\Vert x_{i}-x_{j}\\Vert^2 }$$\n\nWe might provide a hard cut off R , so that\n\n$A_{ij} = 0$  if $\\Vert x_{i}-x_{j}\\Vert^2 \\geq R$\n\n$A_{i,j}\\simeq 1$  when the points are close in $R^{n}$ , and $A_{i,j}\\rightarrow 0$  if the points $x_{i}$, $x_{j}$ are far apart. Close data points are in the same cluster. Data points in different clusters are far away. But data points in the same cluster may also be far away\u2013even farther away than points in different clusters. Our goal then is to transform the space so that when 2 points $x_{i}$, $x_{j}$ are close, they are always in same cluster, and when they are far apart, they are in different clusters. Generally we use the Gaussian Kernel K  directly, or we form the Graph Laplacian A.\n\n<b><u>Graph Laplacian<\/u><\/b> is just another matrix representation of a graph. It can be computed as:\n\n- Simple Laplacian $L=D-A$\n- Normalized Laplacian $L_{N}=D^{-1\/2}LD^{-1\/2}$\n- Generalized Laplacian $L_{G} = D^{-1}L$\n- Relaxed Laplacian $L_{\\rho} = L-\\rho D $\n- Ng, Jordan, & Weiss Laplacian $L_{NJW}=D^{-1\/2}AD^{-1\/2}, where A_{i,i}=0 $\n\n$L = D - A$ where A is the Adjacency matrix and D is the Degree Matrix.\n\n$$D_i = \\sum_{j|(i,j) \\in E} w_{ij}$$\n$$ L_{ij} =\n  \\begin{cases}\n    d_i           & \\quad \\text{if } i = j\\\\\n    -w_{ij}       & \\quad \\text{if } i , j \\in E \\\\\n    0             & \\quad \\text{if } i , j \\notin  E\n  \\end{cases}\n$$\n\n![Screen%20Shot%202019-08-31%20at%2018.28.20.png](attachment:Screen%20Shot%202019-08-31%20at%2018.28.20.png)\n\nThe whole purpose of computing the Graph Laplacian L was to find eigenvalues and eigenvectors for it, in order to embed the data points into a low-dimensional space.\n\n\n<b><u>The Cluster Eigenspace Problem<\/u><\/b>\n\nTo identify good clusters, Laplacian L should be approximately a block-diagonal, with each block defining a cluster. If we have 3 major clusters (C1, C2, C3), we would expect\n\n$$\\begin{matrix}\n  L_{1,1} & L_{1,2} & L_{1,3} \\\\\n  L_{2,1} & L_{2,2} & L_{2,3} \\\\\n  L_{3,1} & L_{3,2} & L_{3,3}\n \\end{matrix}\n \\sim\n \\begin{matrix}\n  L_{1,1} & 0 & 0 \\\\\n  0 & L_{2,2} & 0 \\\\\n  0 & 0 & L_{3,3}\n \\end{matrix}$$\n \n ![Screen%20Shot%202019-08-31%20at%2018.34.06.png](attachment:Screen%20Shot%202019-08-31%20at%2018.34.06.png)\n \n We also expect that the 3 lowest eigenvalues  & eigenvectors (\\lambda_{i},v_{i})  of L  each correspond to a different cluster.\n \n For K clusters, compute the first K eigen vectors. ${v_1, v_2, ...v_k}$. Stack the vectors vertically to form the matrix with eigen vecttors as columns. Represent every node as the corresponding row of this new matrix, these rows form the feature vector of the nodes. Use Kmeans to cluster these points into k clusters $C_1, C_2 ...C_k$\n \n \n### <u> Example <\/u>","137bde65":"<b> <u> Affinity matrix with Eucledean Distance  <\/u><\/b>\n\n<b>affinity = \u2018nearest_neighbors\u2019 <\/b>","dafa39cc":"<u>Building the Clustering models and Visualizing the clustering<\/u>\n\nTwo different Spectral Clustering models with different values for the parameter \u2018affinity\u2019.","52627074":"<b> Performance Evaluation <\/b>"}}