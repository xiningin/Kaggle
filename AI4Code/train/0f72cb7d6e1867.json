{"cell_type":{"a72f5738":"code","ecc9e5a8":"code","16c4f9d6":"code","faf5ac86":"code","2bc59a82":"code","6c5d5192":"code","c6477aef":"code","8b70e921":"code","416eb0c0":"code","659ce8f2":"code","00d019e4":"code","d61abb18":"code","b8b7f302":"code","229aff5a":"markdown","46a7f42b":"markdown","ac81380d":"markdown","f351e5bd":"markdown","a57075a1":"markdown","f84bd110":"markdown","e2675960":"markdown","a3d46b30":"markdown"},"source":{"a72f5738":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecc9e5a8":"# importing all the necessary libraries\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom numpy import mean\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import confusion_matrix, classification_report","16c4f9d6":"# Input data\ndata=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndata.head()","faf5ac86":"data.dtypes","2bc59a82":"data.isnull().sum()","6c5d5192":"sns.countplot(x='Class', data = data)\n\n#Counter is a container which stores the count of elements in a dictionary format where element is the key and its value corrosponds to it's count.\ncounter = Counter( data [ 'Class' ])\n#passing 'Class' feature in the Counter , it tells no. of 1s and 0s present in the dataset\nprint(counter)","c6477aef":"print(data['Amount'].ndim)\nprint(data['Amount'].shape)","8b70e921":"ss= StandardScaler()\n#Since all the features are already scaled (PCA dimesionality reduction) except Amount.\n#create a new column Norm_Amount wherein Amount is scaled by Standard Scaler\ndata['Norm_Amount'] =  ss.fit_transform(np.array(data['Amount']).reshape(-1,1))","416eb0c0":"# Since transaction time doesn't play a part in deciding fraud credit card \ndata.drop(columns=['Time' , 'Amount' ] , inplace =True)","659ce8f2":"#check correaltion between different features with Class and Norm_Amount\nplt.figure(figsize=(25,15))\nsns.heatmap(data.corr() , cmap = 'coolwarm' , annot = True)\n\n#V7 and V20 can be seen as in important feature w.r.t the amount transacted.","00d019e4":"# define y and X \ny = data['Class']\nX = data.drop(['Class'] , axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.25 , random_state=42 , shuffle=False)","d61abb18":"model = DecisionTreeClassifier()\nmodel.fit(X_train , y_train.ravel())\ny_pred=model.predict(X_test)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","b8b7f302":"# we make a list ok k values for k-nearest- neighbours and through iterating over the list of k values we find at which k we get the maximum ROC AUC score\nk_values = [1, 2, 3, 4, 5, 6, 7]\nfor k in k_values:\n    over = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n    under = RandomUnderSampler(sampling_strategy=0.5)\n    steps = [('over', over), ('under', under), ('model', model)]\n    pipeline = Pipeline(steps=steps)\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    score = mean(scores)\n    print('> k=%d, Mean ROC AUC: %.3f' % (k, score))","229aff5a":"**Since the dataset is imbalanced , we need to get it balanced to get accurate results which can be achieved by Oversampling the minority class or Undersampling the majority class . Since Undersampling looses out important information, we then tackle it by Oversampling method.**\n\n**Oversampling comprises of two methods -**\n\n1. Random OverSampling\n2. SMOTE \n\n* Random Oversampling can be understood by an example -*Suppose you have a minority class having 100 samples and a majority class with 1000 samples . Say you pick up row 50 ( can be any ) and augment it on top of the minority class samples . You do this iteratively till it matches up the length of majority class.*\n\n* Since **Random Oversampling** is just increasing the size but no variety in the train data . SMOTE on the other hand not just increases the size but also brings variety in the train data. Let's first understand what SMOTE is\n\n* **Synthetic Minority OverSampling Technique** is abbreivated as **SMOTE** - general idea to carry out this technique is to bring the minority class values ( either 0 or 1 ) to a comparable number in terms of the other class . In other words to match up the length of the other class.\n\n* But again one has to be careful on using SMOTE , when done right can be preferred over random oversampling . By right I mean - making newly synthetic samples legal. For example - the input for which legal values are animal and human ,there is nothing in between . This wont apply to an instance where input can be the weight of a human.","46a7f42b":"# **Why and where Repeated Startified KFold** ?\n\n*Repeated Startified KFold - As the name says -repeating cross validation to get better , improved and accurate estimate of our machine learning model , a single run by the cross validation might result in noisy estimate of the model performance. It reports mean results across all folds from all runs.*\n\nsplits : 10\nrepeats : 3\n","ac81380d":"* **We can see an improvement in the ROC AUC score , at k=7 -> 0.912 we get the highest ROC AUC score followed by k= 6,5,4.. and so on.**","f351e5bd":"![](http:\/\/)![77417image1.png](attachment:eaaa745f-715d-4b37-97a7-9f16bce770a4.png)","a57075a1":"**Why reshape (-1 , 1 )?**\n\n*Since fit_transform takes values in the form of ( n_samples , n_features ) and Amount feature has only 1D values (284807,) that's where reshape comes to rescue where -1 says let the rows be untouched and 1 says to add a column.*","f84bd110":"# **BORDERLINE SMOTE WITH KNN**\n\n* We use the same model same evaluation method , although use a SMOTE transformed version of the dataset. We can update the example to first oversample the minority class to have 10 percent the number of examples of the majority class (e.g. about 1,000), then use random undersampling to reduce the number of examples in the majority class to have 50 percent more than the minority class (e.g. about 2,000).\n\n* In the SMOTE function -> sampling_strategy = 0.1 \n\n* In the RandomUnderSampler -> sampling_strategy = 0.5","e2675960":"# **BEFORE SMOTE**\n* We get ROC_AUC score of 0.881 before SMOTE.* \n\n**ROC_AUC-**\n\n* *We choose ROC_AUC curve as it is the best evaluation metric for classification problems as it provides us with a degree of separability a model could harness.*  \n \n","a3d46b30":"**HOW SMOTE WORKS**\n\n1. The number of samples that need to be oversampled are set up - N.\n\n2. We have X1 , X11 , X12 , X13 and X14 - Specify the k nearest neighbours you want to consider to create artficial samples - say 4 (k = 4)\n \n* *SMOTE will find out the nearest neighbours of every point (X1 ,X11 ,...X14) . Based on number of samples you want SMOTE to create , SMOTE would first find out the lines adjoining of minority class samples to the k nearest neighbours you have considered and it will plot these instances somewhere on these lines. You can have multiple instances on a single line all depends on what you configurte and how many data points you want*\n\n3. After synthesizing the new minority instances, the imbalance that existed in the dataset would go down considerably. Ratio becomes 1:1  ( equal no.. of 0s and 1s). Wuhuu..!!\n\n **WE ARE ALL SET TO BUILD A MODEL NOW.!!!**\n\n"}}