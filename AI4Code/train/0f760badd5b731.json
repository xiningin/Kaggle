{"cell_type":{"b952537f":"code","28105940":"code","a7654cbf":"code","75f8a3dc":"code","3d7dfed5":"code","69de8447":"code","75adb26b":"code","9094fc3d":"code","3c35eaac":"code","d9342151":"code","32f4952c":"code","c0014288":"code","0375f56d":"code","87d4da88":"code","e1e5d24b":"code","7dbbbeff":"code","4e5b4d75":"code","a734e063":"code","64d28439":"markdown","04b15905":"markdown","40a0db16":"markdown","7c93d182":"markdown","825c49c2":"markdown","d8d24132":"markdown","4ae6a3cc":"markdown","9414266f":"markdown"},"source":{"b952537f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","28105940":"df = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")\ndf.head()","a7654cbf":"df.info()","75f8a3dc":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\nmissing_value_df","3d7dfed5":"df.describe()","69de8447":"cols= [\"#C2C4E2\",\"#EED4E5\"]\nsns.countplot(x= df['target'], palette = 'Blues')","75adb26b":"corr = df.corr()\nplt.figure(figsize=(15,10))\ncmap = sns.diverging_palette(260,-10,s=50, l=75, n=6, as_cmap=True)\nmask = np.zeros_like(corr, dtype= np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True,cmap = cmap)\nplt.show()","9094fc3d":"fig, axs = plt.subplots(2, 3, figsize=(15, 15))\n\nsns.histplot(data=df, x=\"age\", color=\"skyblue\", label=\"age\", kde=True, ax=axs[0, 0])\nsns.histplot(data=df, x=\"trestbps\", color=\"red\", label=\"trestbps\", kde=True, ax=axs[0, 1])\nsns.histplot(data=df, x=\"chol\", color=\"green\", label=\"chol\", kde=True, ax=axs[0, 2])\nsns.histplot(data=df, x=\"thalach\", color=\"darkblue\", label=\"thalach\", kde=True, ax=axs[1, 0])\nsns.histplot(data=df, x=\"oldpeak\", color=\"orange\", label=\"oldpeak\", kde=True, ax=axs[1, 1])\nplt.show()","3c35eaac":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.base import BaseEstimator","d9342151":"x = df.drop(columns = \"target\")\ny = df[\"target\"]\nx_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.33,random_state= 42)","32f4952c":"def initial_eval(pipeline, X_train, y_train, X_test, y_test, verbose=False):\n    \"\"\"\n    Quickly trains modeling pipeline and evaluates on test data.    \n    Returns original model, training RMSE, and testing RMSE as a tuple.\n    \"\"\"\n\n    pipeline.fit(X_train, y_train)\n    y_train_pred = pipeline.predict(X_train)\n\n    train_score = pipeline.score(X_train,y_train)\n    test_score = pipeline.score(X_test,y_test)\n\n    if verbose:\n\n        print(f\"Train score: {train_score}\")\n        print(f\"Test score: {test_score}\")\n\n    return train_score, test_score","c0014288":"classifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    LogisticRegression(),\n    SVC()\n]\n\ninitial_score = pd.DataFrame(\n    columns=['Class','Train\/Test', 'Score'])\n\nfor classifier in classifiers:\n    farm_pipeline = make_pipeline(\n        StandardScaler(),\n        classifier\n    )\n    \n    r = initial_eval(farm_pipeline, x_train, y_train, x_test, y_test)\n    initial_score = initial_score.append(pd.Series(\n        [classifier.__class__.__name__,'Train Score', r[0]],\n        index=initial_score.columns), ignore_index=True)\n    initial_score = initial_score.append(pd.Series(\n        [classifier.__class__.__name__,'Test Score', r[1]],\n        index=initial_score.columns), ignore_index=True)\n\ndisplay(initial_score)","0375f56d":"plt.figure(figsize=(9, 6))\n\nsns.barplot(y='Class', x='Score', \n            data=initial_score.sort_values(by=['Score'], ascending=False),\n            hue='Train\/Test',\n            palette='Blues').set_title('Score, $R^2$ (higher is better)');\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2);","87d4da88":"class ClfSwitcher(BaseEstimator):\n    \"\"\"\n    A Custom BaseEstimator that can switch between different Regressors.\n    \"\"\"\n\n    def __init__(\n        self,\n        estimator=LogisticRegression(),\n    ):\n\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)\n    ","e1e5d24b":"pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('clf',ClfSwitcher())\n])\n\nparameters = [\n\n    {\n        'clf__estimator': [KNeighborsClassifier()],\n        'clf__estimator__n_neighbors': [3, 5, 7,11],\n        'clf__estimator__weights': ['uniform','distance'],\n        'clf__estimator__metric': ['manhattan','minkowski'],\n    },\n    {\n        'clf__estimator': [SVC()],\n        'clf__estimator__C': [1,10,100],\n        'clf__estimator__gamma': [1,0.1,0.001],\n        'clf__estimator__kernel': ['linear'],\n    }\n]\n\ngscv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1,\n                    return_train_score=False, verbose=3,refit=True)\ngscv.fit(x_train, y_train)","7dbbbeff":"gscv.best_params_","4e5b4d75":"y_train_pred = gscv.best_estimator_.predict(x_train)\ny_test_pred = gscv.best_estimator_.predict(x_test)\nprint(gscv.best_estimator_.score(x_train,y_train))\nprint(gscv.best_estimator_.score(x_test,y_test))\nprint(classification_report(y_test,y_test_pred))","a734e063":"cm = confusion_matrix(y_test, y_test_pred)\n\ngroup_names = ['True negative','False positive','False negative','True positive']\ngroup_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cm.flatten()\/np.sum(cm)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize = (10,7))\nsns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\nplt.show()","64d28439":"Looking at above figure, we can see that our two best models are K Nearest Neighbors and SVC which have slightly higher test scores among other classifiers.\nMoving on to hyperparameter tuning, we select our two best models, K Nearest Neighbors and SVC for further investigation.\n-----\n## Hyperparameter Tuning\nFor hyperparameter tuning, we are going to use scikit-learn's `GridSearchCV()`. Since we have two models to tune, we need to make a switcher that can act as a wrapper for our regression model. It is called `ClfSwitcher()`.\n\nThen, we are going to construct a pipeline (same as before), which has scaler and classifier.\nFinally, for the grid search, we use 5-fold cross-validation.","04b15905":"## Define Train and Test Data ","40a0db16":"## Results\n\nAfter the grid search is done, we can see the best parameters and use the best estimator to predict the results.","7c93d182":"# Train and Evaluate Models\nFor This purpose, we are going to construct a scikit-learn pipeline that includes steps for data preprocessing (scaling) and classification. We are going to use a few of the well-known classification model for the initial evaluation, then choose the best performing model and tune their hyperparameters to achieve the best result.\n\n____\n## import Libraries","825c49c2":"##  initital evaluation \nFor the initial evaluation and comparing the performance of different regression model, we use a pipeline.\nFirst step in the pipeline is the scaler. This step is necessary for some models, so that all the feature have the same scale and their initial differences in the order of magnitude, will not impact the model. we use `StandardScaler()` to preserve the shape of the data.\nThe Second step in the pipeline is the regressor, for which we have 5 options.\n\n* DecisionTreeClassifier,\n* RandomForestClassifier,\n* KNeighborsClassifier,\n* LogisticRegression,\n* SVC\n\nWe will define a function (`initial_eval()`) that takes pipeline and data as input, and generates score. For the score, we will report $R^2$.","d8d24132":"### correlation between features\nas we can see the correlation between all pairs of features is less than 0.5.\nthe features which have the most correlation with target are cp, thalac and slope.","4ae6a3cc":"# EDA","9414266f":"### distribution of non-categorial features"}}