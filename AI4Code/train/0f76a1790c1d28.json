{"cell_type":{"36eebc96":"code","8017569d":"code","056f7f7c":"code","8cd6da52":"code","f0587c56":"code","105a669b":"code","cfac0db7":"code","d6984234":"code","f5763eb9":"code","a25a78fb":"code","f2a6f687":"code","0ae60162":"code","7c6ba4b2":"code","c6da20fb":"code","38b9727d":"code","80552066":"code","e6341d43":"code","fc16e58e":"code","3d1e106c":"code","84d037ad":"code","78cf25d4":"code","7851202e":"code","1aa74707":"code","34b173a0":"code","1603ec2b":"code","738d3080":"code","cbfbc617":"code","836109fc":"code","260b4ea6":"code","c71ce1eb":"code","2db9a105":"code","1c78123d":"code","c3bc9d1f":"code","b5127bbe":"code","e3e7c9dc":"code","a469d73f":"code","157b8e9c":"code","6d4eb45c":"code","8ade40d2":"code","77cba464":"code","0f30a256":"code","c6130e5f":"markdown","aaf2020a":"markdown","aa7bf787":"markdown","9cbefebd":"markdown","66afcc77":"markdown","74f1bcd6":"markdown","f6085231":"markdown","0a474050":"markdown","8be8d367":"markdown","deab9a00":"markdown","e8a052dd":"markdown","bc179da9":"markdown","267e5ecd":"markdown","7b8c3837":"markdown"},"source":{"36eebc96":"import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split ","8017569d":"X = np.genfromtxt('..\/input\/ahdd1\/csvTrainImages 60k x 784\/csvTrainImages 60k x 784.csv',delimiter=',')\nY = np.genfromtxt('..\/input\/ahdd1\/csvTrainLabel 60k x 1.csv',delimiter=',')","056f7f7c":"#Ploting some examples\nfig,axes = plt.subplots(nrows=3,ncols=3,figsize=(6,6))\n\nfor ax in axes :\n    for i in range(3):\n        n = np.random.randint(0,len(X))\n        ax[i].imshow(X[n].reshape(28,28).T)\n        ax[i].set_title(f'Label : {Y[n]}')\n        \nfig.tight_layout(pad=1.0)\n\n","8cd6da52":"#Scale to max=1\nX = X\/255.0","f0587c56":"#Encoding label\nY_code = np.zeros((len(Y),10))\nfor j in range(0,len(Y)):\n    Y_code[j,int(Y[j])] = 1","105a669b":"#Splitting Data into Train and Cross Validation sets\nX_train, X_cv, Y_train, Y_cv = train_test_split(X, Y_code, test_size=0.16,random_state=1)","cfac0db7":"print('X_train shape is :',X_train.shape)\nprint('X_cv shape is :',X_cv.shape)\nprint('Y_train shape is : ',Y_train.shape)\nprint('Y_cv shape is : ',Y_cv.shape)","d6984234":"#Relu function (activation function)\ndef relu(x) : \n    for i in x :\n        z = np.maximum(0,x)\n    return z","f5763eb9":"#Relu derivative \ndef relu_derivative(x) : \n    return np.maximum(0,x\/np.absolute(x))","a25a78fb":"#Softmax function\ndef soft_max(x):\n    x = np.exp(x)\n    x = x \/np.sum(x,axis=0)\n    return x","f2a6f687":"#Forward propagation\ndef forward_propagation(X,Y,Theta,Bias):\n    A = dict()\n    Z = dict()\n    n = len(Theta.values())\n    m = X.shape[1]\n    A['0'] = X\n    i = 1\n    for theta in Theta.values() :\n        \n        Z[f'{i}'] = np.matmul(Theta[f'{i}'], A[f'{i-1}']) + Bias[f'{i}']\n        if i < n :\n            A[f'{i}'] = relu(Z[f'{i}'])\n        else :\n            A[f'{i}'] = soft_max(Z[f'{i}'])       \n        i +=1\n        \n    cost = - np.nansum(Y*np.log(A[f'{n}']))\n    \n    return Z , A , cost\n        ","0ae60162":"#Backward propagation\ndef back_propagation(Y,Theta,Z,A,lamb=0) :\n    dZ = dict()\n    dW = dict()\n    db = dict()\n    m = Y.shape[1]\n    n = len(Theta.values())\n    for j in range(1,n+1) :\n        if j == 1 :\n            dZ[f'{n}'] = A[f'{n}']-Y\n            dW[f'{n}'] = (1\/m) *np.matmul(dZ[f'{n}'],A[f'{n-1}'].T) +lamb*Theta[f'{n}']\n            db[f'{n}'] = (1\/m) * np.sum(dZ[f'{n}'],axis=1,keepdims=True)\n        else : \n            dZ[f'{n-j+1}'] = np.matmul(Theta[f'{n-j+2}'].T,dZ[f'{n-j+2}']) * relu_derivative(Z[f'{n-j+1}'])\n            dW[f'{n-j+1}'] = (1\/m) * np.matmul(dZ[f'{n-j+1}'],A[f'{n-j}'].T)+lamb*Theta[f'{n-j+1}']\n            db[f'{n-j+1}'] = (1\/m) * np.sum(dZ[f'{n-j+1}'],axis=1,keepdims=True)            \n            \n    return dW , db      \n        \n    ","7c6ba4b2":"#Prediction function (Returns predictions + Accuracy)\ndef pred(X,Y,Theta,Bias): \n    X=X.T\n    Y=Y.T\n    Z,A,cost = forward_propagation(X,Y,Theta,Bias)\n    L = len(Theta.values())\n    A_index = np.argmax(A[f'{L}'],0) \n    Y_index = np.argmax(Y,0) \n    Accuracy = np.mean(A_index == Y_index)\n    return A_index , Accuracy","c6da20fb":"#Define a function that transforms a dictionary to a vector (This function will be used for gradient checking)\ndef dict_to_vector(dic):\n    \n    i=0\n    for l in dic.values() :\n        \n        vector = np.reshape(l,(-1,1))\n        if i ==0 :\n            VECTOR = vector\n        else :\n            VECTOR = np.concatenate((VECTOR,vector),axis=0)\n        i+=1\n    \n    return VECTOR\n        ","38b9727d":"#Define a function that transforms a vector to a dictionary (This function will be used for gradient checking)\ndef vect_to_dict(vec,dic) : \n    dic1 = dict()\n    i=1\n    for l in dic.values():\n        size = l.shape[0] * l.shape[1]\n        dic1[f'{i}'] = vec[:size].reshape(l.shape[0],l.shape[1])\n        i=i+1\n    \n    return dic1\n","80552066":"#Gradient checking for weights (Theta or W) (To make sure back propagation works well)\ndef grad_check_W(X,Y,dW,Theta,Bias,epsilon=1e-7) : \n    \n    Theta_vector = dict_to_vector(Theta)\n    num_paramaters = Theta_vector.shape[0]\n    J_minus = np.zeros((num_paramaters,0))\n    J_plus  = np.zeros((num_paramaters,0))\n    grad = dict_to_vector(dW)\n    gradapprox = np.zeros((num_paramaters,0))\n    for i in range(num_paramaters):\n    \n        Theta_plus = np.copy(Theta_vector)\n        Theta_plus[i][0] = Theta_plus[i][0] + epsilon\n        Z,A,J_plus[i] = forward_propagation(X,Y,vect_to_dict(Theta_plus,Theta),Bias)\n    \n        Theta_minus = np.copy(Theta_vector)\n        Theta_minus[i][0] = Theta_minus[i][0] - epsilon\n        Z,A,J_minus[i] = forward_propagation(X,Y,vect_to_dict(Theta_minus,Theta),Bias)\n    \n        gradapprox[i] = (J_plus[i]-J_minus[i])\/(2*epsilon)\n    \n    \n    numerator = np.linalg.norm(grad-gradapprox)                                           \n    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)                                         \n    difference = numerator\/denominator \n    \n    if difference > 2e-7:\n        print('There is a mistake in dW, difference = ',difference)\n    else:\n        print('All is good for dW')","e6341d43":"#Gradient checking for Bias (To make sure back propagation works well)\ndef grad_check_b(X,Y,db,Theta,Bias,epsilon=1e-7) : \n    \n    Bias_vector = dict_to_vector(Bias)\n    num_paramaters = Bias_vector.shape[0]\n    J_minus = np.zeros((num_paramaters,0))\n    J_plus  = np.zeros((num_paramaters,0))\n    grad = dict_to_vector(db)\n    gradapprox = np.zeros((num_paramaters,0))\n    for i in range(num_paramaters):\n    \n        Bias_plus = np.copy(Bias_vector)\n        Bias_plus[i][0] = Bias_plus[i][0] + epsilon\n        Z,A,J_plus[i] = forward_propagation(X,Y,Theta,vect_to_dict(Bias_plus,Bias))\n    \n        Bias_minus = np.copy(Bias_vector)\n        Bias_minus[i][0] = Bias_minus[i][0] - epsilon\n        Z,A,J_minus[i] = forward_propagation(X,Y,Theta,vect_to_dict(Bias_minus,Bias))\n    \n        gradapprox[i] = (J_plus[i]-J_minus[i])\/(2*epsilon)\n    \n    \n    numerator = np.linalg.norm(grad-gradapprox)                                           \n    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)                                         \n    difference = numerator\/denominator \n    \n    if difference > 2e-7:\n        print('There is a mistake in db, difference = ',difference)\n    else:\n        print('All is good for db')","fc16e58e":"#Define a function that splits training set into random batches\ndef into_batches(X,Y,batch_size=1024):\n    Batches_X = dict()\n    Batches_Y = dict()\n    m = X.shape[1]\n    num_batch = int(X.shape[1]\/batch_size)\n    ind=0\n    permutation = list(np.random.permutation(m))\n    X_shuffled = X[:,permutation]\n    Y_shuffled = Y[:,permutation]\n    for i in range(num_batch):\n        \n        Batches_X[f'{i+1}'] = X_shuffled[:,ind:ind+batch_size]\n        Batches_Y[f'{i+1}'] = Y_shuffled[:,ind:ind+batch_size]\n        ind += batch_size\n        \n    if X.shape[1]>batch_size*num_batch :\n        Batches_X[f'{num_batch+1}'] = X_shuffled[:,ind:X.shape[1]]\n        Batches_Y[f'{num_batch+1}'] = Y_shuffled[:,ind:X.shape[1]]\n        \n    return Batches_X,Batches_Y\n        \n    \n    ","3d1e106c":"#Define model using Adam optimizer (Built from scratch)\ndef Adam_learn(X,Y,nhl=3,size_hidden=[5,5,5],epochs=100 ,alpha=0.001,lamb=0,batch_size=1024,beta1=0.9,beta2=0.999,eps=1e-8,grad_check =False,display=False) :\n    \n    Theta = dict()\n    Bias = dict()\n    V_dW = dict()\n    V_db = dict()\n    S_dW = dict()\n    S_db = dict()\n    alpha_0 = alpha\n    X = X.T\n    Y = Y.T\n    n1 = X.shape[0]\n    m = X.shape[1]\n    n2 = Y.shape[0]\n    COST = []\n    \n    #Initializing parameters (Xavier Initialization)\n    for i in range (1,nhl+2) :\n        if i == 1 :\n            Theta[f'{i}'] = np.random.randn(size_hidden[i-1],n1)*np.sqrt(2\/(size_hidden[i-1]+n1))\n            V_dW[f'{i}'] = np.zeros((size_hidden[i-1],n1))*np.sqrt(2\/(size_hidden[i-1]+n1))\n            S_dW[f'{i}'] = np.zeros((size_hidden[i-1],n1))*np.sqrt(2\/(size_hidden[i-1]+n1))\n            Bias[f'{i}'] = np.ones((size_hidden[i-1],1))\n            V_db[f'{i}'] = np.zeros((size_hidden[i-1],1))\n            S_db[f'{i}'] = np.zeros((size_hidden[i-1],1))\n        elif i == nhl+1 :\n            Theta[f'{i}'] = np.random.randn(n2,size_hidden[i-2])*np.sqrt(2\/(size_hidden[i-2]+n2))\n            V_dW[f'{i}'] = np.zeros((n2,size_hidden[i-2]))\n            S_dW[f'{i}'] = np.zeros((n2,size_hidden[i-2]))\n            Bias[f'{i}'] = np.ones((n2,1))\n            V_db[f'{i}'] = np.zeros((n2,1))\n            S_db[f'{i}'] = np.zeros((n2,1))\n        else : \n            Theta[f'{i}'] = np.random.randn(size_hidden[i-1],size_hidden[i-2])*np.sqrt(2\/(size_hidden[i-2]+size_hidden[i-1]))\n            V_dW[f'{i}'] = np.zeros((size_hidden[i-1],size_hidden[i-2]))\n            S_dW[f'{i}'] = np.zeros((size_hidden[i-1],size_hidden[i-2]))\n            Bias[f'{i}'] = np.ones((size_hidden[i-1],1))\n            V_db[f'{i}'] = np.zeros((size_hidden[i-1],1))\n            S_db[f'{i}'] = np.zeros((size_hidden[i-1],1))\n                                                                                        \n    t = 1\n    #Loop over number of epochs\n    for i in range(epochs):\n        #Define a random set of batches each epoch\n        Batches_X,Batches_Y = into_batches(X,Y,batch_size=batch_size)\n        \n        #Decay learning rate after 1000th epoch\n        if i>=1001 :\n            alpha = 0.00001\n            \n        cost_cum = 0\n        \n        #Loop over batches\n        for j in range(1,len(Batches_X.values())+1) :\n            Z , A , cost = forward_propagation(Batches_X[f'{j}'],Batches_Y[f'{j}'],Theta,Bias)\n            cost_cum += cost \n            dW , db = back_propagation(Batches_Y[f'{j}'],Theta,Z,A,lamb=lamb)\n            \n            #Gradient checking (only for one gradient iteration)\n            if grad_check ==True :\n                if i==10 :\n                    grad_check_W(X,Y,dW,Theta,Bias,epsilon=1e-7) \n                    grad_check_b(X,Y,db,Theta,Bias,epsilon=1e-7) \n                \n            # Calculate momentum and RMS then update parameters\n            for p in range (1,nhl+2) : \n                \n                V_dW[f'{p}'] = (beta1*V_dW[f'{p}'] + (1-beta1)*dW[f'{p}'])\/(1-(beta1)**t)\n                V_db[f'{p}'] = (beta1*V_db[f'{p}'] + (1-beta1)*db[f'{p}'])\/(1-(beta1)**t)\n                S_dW[f'{p}'] = (beta2*S_dW[f'{p}'] + (1-beta2)*np.square(dW[f'{p}']))\/(1-(beta2)**t)\n                S_db[f'{p}'] = (beta2*S_db[f'{p}'] + (1-beta2)*np.square(db[f'{p}']))\/(1-(beta2)**t)\n                \n                Theta[f'{p}'] = Theta[f'{p}'] -alpha*V_dW[f'{p}']\/(np.sqrt(S_dW[f'{p}'])+eps)\n                Bias[f'{p}'] = Bias[f'{p}'] -alpha*V_db[f'{p}']\/(np.sqrt(S_db[f'{p}'])+eps)\n            t+=1\n        \n        #Compute cost after each epoch \n        cost_average = (1\/m)*cost_cum\n        COST.append(cost_average)\n        \n        \n    \n        if i%250 ==0 :\n            print(f'Cost after {i} is : ',cost_average)\n\n    #Plot cost \n    if display==True :\n        plt.plot(range(epochs),COST)\n        \n        \n    return Theta , Bias, COST","84d037ad":"#Learning rate and regularization factor are set using random logarithmic search and grid search \n#Gradient checking is turned off. \nTheta,Bias,COST = Adam_learn(X_train,Y_train,nhl=2,size_hidden=[15,15],epochs=3000 ,alpha=0.01,\n                             lamb=0.00097,batch_size=256,beta1=0.9,beta2=0.997,eps=1e-8,\n                             grad_check =False,display=True)","78cf25d4":"#Assessing model on train and cross validation sets\npredictions_train,Accuracy_train = pred(X_train,Y_train,Theta,Bias)\npredictions_cv,Accuracy_cv = pred(X_cv,Y_cv,Theta,Bias)\nprint('Train accuracy is equal to ' \"{:.2%}\".format(Accuracy_train))\nprint('Cross validation accuracy is equal to '\"{:.2%}\".format(Accuracy_cv))","7851202e":"# Random logarithmic search for regularization fator \n# This is the function that had been used to improve regularization factor. Due to high computational cost, it wont be executed here.\n# It uses random logarithmic search instead of grid search, this kind of search allows us to balance the number of tests between segments of the forme [10**-(n+1),10**-n]\n\n\ndef regularization_randomlog_search(X_train,Y_train,X_cv,Y_cv,min_factor=0.0001,max_factor=0.001,nb_factor=10) :\n    \n    ACCURACIES_cv = []\n    ACCURACIES_train = []\n    \n    #Create space \n    if min_factor != 0 :\n        LAMBDA =10**((np.log10(max_factor)-np.log10(min_factor))*np.random.rand(nb_factor) + np.log10(min_factor))\n    \n    #Explore space\n    for lamb in LAMBDA :\n        print('Processing for lambda = ', lamb)\n        \n        #Train model for each lambda value\n        Theta,Bias,COST = Adam_learn(X_train,Y_train,nhl=2,size_hidden=[15,15],epochs=2000 ,alpha=0.01,lamb=lamb,\n                                 batch_size=256,beta1=0.9,beta2=0.997,eps=1e-8,grad_check =False,display=False)\n        #Predict train and cross validation labels\n        predictions_cv,Accuracy_cv = pred(X_cv,Y_cv,Theta,Bias)\n        predictions_train,Accuracy_train = pred(X_train,Y_train,Theta,Bias)\n        \n        #Display results\n        print('Cross validation accuracy is equal to : ',Accuracy_cv)\n        print('\\n')\n        print('Train validation accuracy is equal to : ',Accuracy_train)\n        print('\\n')\n        \n        #Collect results\n        ACCURACIES_cv.append(Accuracy_cv)\n        ACCURACIES_train.append(Accuracy_train)\n        \n    #Select maximum value of cross validation accuracy\n    ind = ACCURACIES_cv.index(max(ACCURACIES_cv))\n        \n    #Plot results\n    fig = plt.figure(figsize=(15,6))\n    ax = fig.add_axes([0,0,1,1])\n    ax.plot(LAMBDA,ACCURACIES_cv,'ro')\n    ax.plot(LAMBDA,ACCURACIES_train,'bo')\n    ax.plot(LAMBDA[ind],ACCURACIES_cv[ind],marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')\n    ax.plot(LAMBDA[ind],ACCURACIES_train[ind],marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')","1aa74707":"#Import Test set\nX_test = np.genfromtxt('..\/input\/ahdd1\/csvTestImages 10k x 784.csv',delimiter=',')\nY_test = np.genfromtxt('..\/input\/ahdd1\/csvTestLabel 10k x 1.csv',delimiter=',')","34b173a0":"#Scale to max=1\nX_test = X_test\/255.0\n#Encoding label\nY_code_test = np.zeros((len(Y_test),10))\nfor j in range(0,len(Y_test)):\n    Y_code_test[j,int(Y_test[j])] = 1","1603ec2b":"#Making predictions\npredictions_test,Accuracy_test = pred(X_test,Y_code_test,Theta,Bias)\nprint('Test accuracy is equal to ',Accuracy_test)","738d3080":"from tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.models import Model, load_model\nfrom keras.utils import plot_model","cbfbc617":"#Reshape X (Transpose to adjust images for visualization)\nX_net = np.transpose(X.reshape(60000,28,28,1),axes=[0,2,1,3])","836109fc":"#Encode Y\nY_net = np.zeros((len(Y),10))\nfor i in range(len(Y)):\n    Y_net[i,int(Y[i])] =1","260b4ea6":"#Shapes\nprint('X_net shape is : ',X_net.shape)\nprint('Y_net shape is : ',Y_net.shape)","c71ce1eb":"#One observation...\nplt.imshow(X_net[3])\nprint(Y_net[3,:])","2db9a105":"def LeNet5(input_shape=(28, 28,1),classes = 10) :\n    \n    \n    # Input layer\n    X_input = Input(input_shape)\n    \n    #1st layer\n    X = Conv2D(20, (5, 5), strides = (1, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X_input)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((2, 2), strides=(2, 2))(X)\n    #2nd layer\n    X = Conv2D(50, (5, 5), strides = (1, 1), padding = 'same' ,name = 'conv2', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((2, 2), strides=(2, 2))(X)\n    \n    #3rd layer\n    X = Flatten()(X)\n    X = Dense(25)(X)\n    X = Activation('relu')(X)\n    \n    #Output layer\n    X = Dense(classes,activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n\n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n    \n    return model","1c78123d":"#Creating the model\nmodel = LeNet5(input_shape = (28, 28,1), classes = 10)","c3bc9d1f":"#Model visualization\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","b5127bbe":"#Compiling the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","e3e7c9dc":"#Training the model\nmodel.fit(X_net, Y_net, epochs = 50, batch_size = 256)","a469d73f":"#Reshape X_test\nX_net_test = np.transpose(X_test.reshape(10000,28,28,1),axes=[0,2,1,3])\n#Encode Y_test\nY_net_test = np.zeros((len(Y_test),10))\nfor i in range(len(Y_test)):\n    Y_net_test[i,int(Y_test[i])] =1","157b8e9c":"#Shapes\nprint('X_net_test shape is : ',X_net_test.shape)\nprint('Y_net_test shape is : ',Y_net_test.shape)","6d4eb45c":"#Evaluating the model on cross validation set\ntest_preds = model.evaluate(X_net_test, Y_net_test)\nprint (\"Loss = \" + str(test_preds[0]))\nprint (\"Test Accuracy = \" + str(test_preds[1]))","8ade40d2":"#Collecting predictions and true labels\nprediction_test = model.predict(X_net_test)\nY_pred = np.argmax(prediction_test,axis=1)\nY_true = np.argmax(Y_net_test,axis=1)","77cba464":"#Collecting milabeled observations\nerrors_img = X_net_test[Y_pred!=Y_true]\nerrors_pred = Y_pred[Y_pred!=Y_true]\nerrors_true_labels =Y_true[Y_pred!=Y_true]","0f30a256":"#Plot some mispredicted observations\nfig,axes = plt.subplots(nrows=3,ncols=3,figsize=(6,6))\n\nfor ax in axes :\n    for i in range(3):\n        n = np.random.randint(0,len(errors_img))\n        ax[i].imshow(errors_img[n])\n        ax[i].set_title(f'pred : {errors_pred[n]} true : {errors_true_labels[n]}')\n        \nfig.tight_layout(pad=1.0)","c6130e5f":"## Debugging","aaf2020a":"Since no tuning was applied no need for cross validation set","aa7bf787":"### Cross validation accuracy can be improved by finding a better regularization factor (for example, using the function defined bellow) or by trying some other regularization approaches such as dropouts...Or, first improve train accuracy by increasing the number of epochs, adding more layers then retune regularization parameters","9cbefebd":"# Import libraries and Data","66afcc77":"## Test Predictions ","74f1bcd6":"## Preprocessing Data","f6085231":"## Model ","0a474050":"Well, it looks like there is 2 types of errors : \n    \n    1\/ Observations that are labeled 0 and predicted to be 1 seems to be mislabeled because they really look like ones, same for zeros predicted as fives..\n    2\/ Model failed to predict some observations such as 3 wich looks a bit like 2.\nIn order to have significant insights on errors, more cases must be traited (at least 100) and classified in order to define subsequent improvement steps. \n    \nPotential improvements : Compared to it size the model seems to work fine. Performance might be enhanced by increasing the number of filter channels or by adding more layers, also data augmentation is likely to help (applying rotations and adding noise) for example the 7 predicted as 5 seems to be a rotation of 7 that our model failed to detect...\n    \n","8be8d367":"# Standard Neural Network","deab9a00":"## Model","e8a052dd":"# LeNet 5 on Keras ","bc179da9":"### Storing parameters this way (using dictionaries) enables more flexibility when varying hyperparameters such as the number of hidden layers. For example, incrementing nhl and adding a size component to size_hidden adds a new hidden layer to the model.","267e5ecd":"## Some prerequisite functions ","7b8c3837":"## Test prediction"}}