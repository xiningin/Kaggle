{"cell_type":{"69b6ce22":"code","f9885c7e":"code","595f9452":"code","85f21c22":"code","6fdd3dae":"code","afc9993a":"code","690311a7":"code","c33774dc":"code","0082abdc":"code","16eeb862":"code","7db49c8d":"code","888c93a8":"code","12577ec8":"code","72f19ace":"code","db033fad":"code","1c6c35be":"code","1a66ee72":"code","a846d569":"code","97d6005d":"code","b272134e":"code","efa7c109":"code","90cac335":"code","eae584d0":"code","95d91621":"code","5b6a71e5":"code","f9a7cb09":"code","14968011":"code","ae0b016a":"code","bbcc6e8e":"code","2af3f673":"code","926f2ced":"code","26d00c2c":"code","c84d24b0":"code","ae003c4e":"code","74d64b80":"code","41d66a9a":"code","eab1d1ba":"code","6cfe1e73":"code","469316f1":"code","61492b32":"code","b86d3c29":"code","c814f2dd":"code","a609f4c7":"code","f8a5c609":"code","d7dad92b":"code","2f774e80":"code","1bff66c0":"code","4de6abf8":"code","15016d42":"code","3bf0ce25":"code","3858db04":"code","9dfa07f9":"code","e98abb45":"code","24257ff2":"code","260e30e6":"code","d46d53ac":"code","56a4058d":"code","51b4ae7b":"code","8a009750":"code","1ad9583e":"code","05f741f0":"code","270373fb":"code","7d60cbdf":"code","f70c9e48":"markdown","0b672603":"markdown","c1235395":"markdown","f8149ad2":"markdown","9c29707f":"markdown","5ad78ed0":"markdown"},"source":{"69b6ce22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9885c7e":"# Load the dataset and have a peek into first rows\ndata = pd.read_csv(\"\/kaggle\/input\/ecommerce-data\/data.csv\")\ndata.head()","595f9452":"# Check the basic info of the dataset: size, variables, data types\ndata.info()","85f21c22":"# Note that we have some missing values in column \"Description\" and \"CustomerID\".\n# Since our goal is to create a customer segmentation using the RFM framework, we need \n# drop rows with missing customer ID and description.\ndata = data.dropna()\ndata.info()","6fdd3dae":"# Convert column \"InvoiceDate\" to datetime\nfrom datetime import datetime\ndata[\"InvoiceDate\"] = data[\"InvoiceDate\"].apply(lambda x: x.split(' ')[0])\ndata[\"InvoiceDate\"].head()","afc9993a":"data[\"InvoiceDate\"] = data[\"InvoiceDate\"].apply(lambda x: datetime.strptime(x, '%m\/%d\/%Y'))","690311a7":"# Check the dataset info after conversion\ndata.info()","c33774dc":"# Create a new variable OrderValue = Quanity * UnitPrice\ndata[\"OrderValue\"] = data[\"Quantity\"] * data[\"UnitPrice\"]\ndata[[\"Quantity\", \"UnitPrice\", \"OrderValue\"]].head()","0082abdc":"# Aggregate at customer level\ncustomer_data = pd.DataFrame(data.pivot_table(index=\"CustomerID\", \n                                              values=[\"InvoiceDate\", \"OrderValue\"], \n                                              aggfunc={\"InvoiceDate\": [min, max, pd.Series.nunique], \"OrderValue\": sum}))\ncustomer_data.head()","16eeb862":"customer_data.info()","7db49c8d":"# Rename columns\ncustomer_data.columns = [\"LastInvoiceDate\", \"FirstInvoiceDate\", \"Frequency\", \"MonetaryValue\"]\ncustomer_data.head()","888c93a8":"import matplotlib.pyplot as plt\n# Create column \"FirstInvoceMonth\" to look at customer intake\ncustomer_data[\"FirstInvoiceMonth\"] = customer_data[\"FirstInvoiceDate\"].apply(lambda x: x.replace(day=1))\ncustomer_data.groupby([\"FirstInvoiceMonth\"]).count()[\"FirstInvoiceDate\"].plot(kind=\"bar\")\nplt.title(\"Monthly Customer Intakes\")","12577ec8":"# Calculate recency, relative recency and relative frequency\n# Take the maximum invoice date as today\ntoday = customer_data[\"LastInvoiceDate\"].max()\ntoday","72f19ace":"# Calculate recency: the interval (days) between the last transation day and today\ncustomer_data[\"Recency\"] = (today - customer_data[\"LastInvoiceDate\"]) \/ np.timedelta64(1, 'D')\ncustomer_data[\"Recency\"].hist()","db033fad":"# Calculate customer lifetime: the interval (days) between the first transation day and today\ncustomer_data[\"Lifetime\"] = (today - customer_data[\"FirstInvoiceDate\"]) \/ np.timedelta64(1, 'D')\ncustomer_data[\"Lifetime\"].hist()","1c6c35be":"customer_data[\"Lifetime\"].describe()","1a66ee72":"# Calculate the relative recency: recency \/ customer lifetime\n# Note: this is to normalise the fact that newer customers have lower lifetime and thus lower recency\n# by nature.\ncustomer_data[\"RelRecency\"] = 1 - customer_data[\"Recency\"] \/ customer_data[\"Lifetime\"]\ncustomer_data[\"RelRecency\"].hist()","a846d569":"customer_data[\"Recency\"].describe()","97d6005d":"customer_data[\"RelRecency\"].describe()","b272134e":"sum(customer_data[\"RelRecency\"]==0) \/ len(customer_data)\n# This shows that 31% customers only had one transactions.","efa7c109":"# Calculate the relative frequency: frequency \/ customer lifetime\n# Note: this is to normalise the fact that newer customers have lower lifetime and thus lower frequency\n# by nature.\ncustomer_data[\"RelFrequency\"] = customer_data[\"Frequency\"] \/ customer_data[\"Lifetime\"]\ncustomer_data[\"RelFrequency\"].apply(lambda x: np.isinf(x)).value_counts()","90cac335":"# Found a record with inifinity value, we need to remove it\ncustomer_data[customer_data[\"RelFrequency\"].apply(lambda x: np.isinf(x))]","eae584d0":"customer_data = customer_data[customer_data[\"RelFrequency\"].apply(lambda x: np.isinf(x))==False]\ncustomer_data[\"RelFrequency\"].hist()","95d91621":"customer_data[\"RelFrequency\"].describe()","5b6a71e5":"# Calculate the relative monetary value: monetary value \/ customer lifetime\n# Note: this is to normalise the fact that newer customers have lower lifetime and thus lower frequency\n# by nature.\ncustomer_data[\"MonetaryValue\"].describe()","f9a7cb09":"customer_data[\"MonetaryValue\"].hist()\n# There are some outliers in terms of monetary value.","14968011":"np.percentile(customer_data[\"MonetaryValue\"], 99)\n# The 99.5% percentile of monetary value is 33.4K, i.e., 0.5% * 4K = 20 customers have value higher than 33.4K.\n# We should exclude these outliers from our analysis.","ae0b016a":"customer_data = customer_data[customer_data[\"MonetaryValue\"]<=np.percentile(customer_data[\"MonetaryValue\"], 99)]","bbcc6e8e":"len(customer_data)","2af3f673":"len(customer_data[customer_data[\"MonetaryValue\"]<0])\n# 43 customers have negative transation value because of the returns.","926f2ced":"customer_data[customer_data[\"MonetaryValue\"]<0].head()","26d00c2c":"data[data[\"CustomerID\"]==12454]\n# Some customers have a negative sum of order value. This is because they have returns.","c84d24b0":"customer_data[\"RelMonetaryValue\"] = customer_data[\"MonetaryValue\"] \/ customer_data[\"Lifetime\"]\ncustomer_data[\"RelMonetaryValue\"].hist()","ae003c4e":"customer_data[\"RelMonetaryValue\"].describe()","74d64b80":"# Create RFM buckets using absolute values\n# For this analysis, we take the medians and 75% quartiles of relative recency, relative frequency and relative monetary value and use them for\n# 4 bins for each variable. We label those bins as 1, 2, 3 and 4 and use them as the scores for R, F and M respectively.\n# We then create \n# By doing so we end up with 10 clusters ()","41d66a9a":"customer_data['RecencyScore'] = pd.cut(customer_data[\"Recency\"], \n                                       bins=[-1, \n                                             np.percentile(customer_data[\"Recency\"], 25), \n                                             np.percentile(customer_data[\"Recency\"], 50), \n                                             np.percentile(customer_data[\"Recency\"], 75), \n                                             customer_data[\"Recency\"].max()], \n                                       labels=[4, 3, 2, 1]).astype(\"int\")\ncustomer_data[\"RecencyScore\"].value_counts()","eab1d1ba":"customer_data[\"FrequencyScore\"] = pd.cut(customer_data[\"Frequency\"], \n                                       bins=[-1, \n                                             np.percentile(customer_data[\"Frequency\"], 25), \n                                             np.percentile(customer_data[\"Frequency\"], 50), \n                                             np.percentile(customer_data[\"Frequency\"], 75), \n                                             customer_data[\"Frequency\"].max()], \n                                       labels=[1, 2, 3, 4]).astype(\"int\")\ncustomer_data[\"FrequencyScore\"].value_counts()","6cfe1e73":"customer_data[\"MonetaryScore\"] = pd.cut(customer_data[\"MonetaryValue\"], \n                                       bins=[customer_data[\"MonetaryValue\"].min()-1, \n                                             np.percentile(customer_data[\"MonetaryValue\"], 25),\n                                             np.percentile(customer_data[\"MonetaryValue\"], 50), \n                                             np.percentile(customer_data[\"MonetaryValue\"], 75), \n                                             customer_data[\"MonetaryValue\"].max()], \n                                       labels=[1, 2, 3, 4]).astype(\"int\")\ncustomer_data[\"MonetaryScore\"].value_counts()","469316f1":"customer_data[\"RFM\"] = customer_data[\"RecencyScore\"] + customer_data[\"FrequencyScore\"] + customer_data[\"MonetaryScore\"]\ncustomer_data[\"RFM\"].value_counts()","61492b32":"rfm_abs = pd.DataFrame(customer_data.pivot_table(index=[\"RFM\"], \n                                    values=[\"Recency\", \"Frequency\", \"MonetaryValue\", \"Lifetime\"], \n                                    aggfunc={\"Recency\": [np.min, np.median, np.max], \n                                             \"Frequency\": [np.min, np.median, np.max], \n                                             \"MonetaryValue\": [np.min, np.median, np.max], \n                                             \"Lifetime\": [np.min, np.median, np.max, \"count\"]}))\nrfm_abs","b86d3c29":"# Create RFM buckets using relative values\n# For this analysis, we take the medians and 75% quartiles of relative recency, relative frequency and relative monetary value and use them for\n# 4 bins for each variable. We label those bins as 1, 2, 3 and 4 and use them as the scores for R, F and M respectively.\n# We then create \n# By doing so we end up with 10 clusters ()\ncustomer_data[\"RecencyScore\"] = pd.cut(customer_data[\"RelRecency\"], \n                                       bins=[-1, \n                                             np.percentile(customer_data[\"RelRecency\"], 25), \n                                             np.percentile(customer_data[\"RelRecency\"], 50), \n                                             np.percentile(customer_data[\"RelRecency\"], 75), \n                                             customer_data[\"RelRecency\"].max()], \n                                       labels=[1, 2, 3, 4]).astype(\"int\")\ncustomer_data[\"RecencyScore\"].value_counts()","c814f2dd":"customer_data[\"FrequencyScore\"] = pd.cut(customer_data[\"RelFrequency\"], \n                                       bins=[-1, \n                                             np.percentile(customer_data[\"RelFrequency\"], 25), \n                                             np.percentile(customer_data[\"RelFrequency\"], 50), \n                                             np.percentile(customer_data[\"RelFrequency\"], 75), \n                                             customer_data[\"RelFrequency\"].max()], \n                                       labels=[1, 2, 3, 4]).astype(\"int\")\ncustomer_data[\"FrequencyScore\"].value_counts()","a609f4c7":"customer_data[\"MonetaryScore\"] = pd.cut(customer_data[\"RelMonetaryValue\"], \n                                       bins=[customer_data[\"RelMonetaryValue\"].min()-1, \n                                             np.percentile(customer_data[\"RelMonetaryValue\"], 25),\n                                             np.percentile(customer_data[\"RelMonetaryValue\"], 50), \n                                             np.percentile(customer_data[\"RelMonetaryValue\"], 75), \n                                             customer_data[\"RelMonetaryValue\"].max()], \n                                       labels=[1, 2, 3, 4]).astype(\"int\")\ncustomer_data[\"MonetaryScore\"].value_counts()","f8a5c609":"customer_data.head()","d7dad92b":"customer_data[\"RFM\"] = customer_data[\"RecencyScore\"] + customer_data[\"FrequencyScore\"] + customer_data[\"MonetaryScore\"]\ncustomer_data[\"RFM\"].value_counts()","2f774e80":"rfm_rel = pd.DataFrame(customer_data.pivot_table(index=[\"RFM\"], \n                                    values=[\"Recency\", \"Frequency\", \"MonetaryValue\", \"Lifetime\"], \n                                    aggfunc={\"Recency\": [np.min, np.median, np.max], \n                                             \"Frequency\": [np.min, np.median, np.max], \n                                             \"MonetaryValue\": [np.min, np.median, np.max], \n                                             \"Lifetime\": [np.min, np.median, np.max, \"count\"]}))\nrfm_rel\n# Note that the median lifetime is rather constant across clusters. This is a good news - our segmentation is not biased by the lifetime\n# of the customers.","1bff66c0":"fig, axes = plt.subplots(nrows=2, ncols=1)\nfig.suptitle('Median Lifetime of RFM Segments (Absolute vs Relative)')\nrfm_abs[\"Lifetime\"][\"median\"].plot(ax=axes[0], kind=\"bar\")\nrfm_rel[\"Lifetime\"][\"median\"].plot(ax=axes[1], kind=\"bar\")\nplt.show()","4de6abf8":"# Visualise segments using 3D plot\nx = customer_data[\"RelRecency\"]\ny = customer_data[\"RelFrequency\"]\nz = (customer_data[\"RelMonetaryValue\"] - customer_data[\"RelMonetaryValue\"].min()) \/ customer_data[\"RelMonetaryValue\"].max()\nc = customer_data[\"RFM\"]\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(xs=x, ys=y, zs=z, c=c, s=30, alpha=0.5)\nax.set_title(\"RFM Visualisation\")\nax.set_xlabel(\"Relative Recency\")\nax.set_ylabel(\"Relative Frequency\")\nax.set_zlabel(\"Relative Monetary Value (with Min-Max Standardisation)\")\nplt.show()","15016d42":"fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, sharey=True, figsize=(5, 15))\nfig.suptitle('Correlation between R, F and M')\n\naxs[0].scatter(x, y, alpha=0.5)\naxs[0].set_title(\"Relative Recency (x) vs Relative Frequency (y)\")\n\naxs[1].scatter(y, z, alpha=0.5)\naxs[1].set_title(\"Relative Frequency (x) vs Relative Monetary Value (y)\")\n\naxs[2].scatter(x, z, alpha=0.5)\naxs[2].set_title(\"Relative Recency (x) vs Relative Monetary Value (y)\")\n","3bf0ce25":"x = customer_data[\"RecencyScore\"]\ny = customer_data[\"FrequencyScore\"]\nz = customer_data[\"MonetaryScore\"]\nc = customer_data[\"RFM\"]\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(xs=x, ys=y, zs=z, c=c, s=30, alpha=0.5)\nax.set_title(\"RFM Visualisation\")\nax.set_xlabel(\"Recency Score\")\nax.set_ylabel(\"Frequency Score\")\nax.set_zlabel(\"Monetary Score\")\nplt.show()","3858db04":"# From these statistics we can generate some heuristic business rules to simplify our segmentation.\n# For example, RFM score 3 and 4 can be combined, because these two groups tend to have only one transaction.\n# Create new columns with our learnings\ncustomer_data.describe()","9dfa07f9":"rfm_rel","e98abb45":"customer_data[\"last_order_within_l60d\"] = customer_data[\"Recency\"]<60 # Had transactions in the last 60 days\ncustomer_data[\"more_than_two_orders\"] = customer_data[\"Frequency\"]>2 # Logged in more than twice\ncustomer_data[\"value_higher_than_2k\"] = customer_data[\"MonetaryValue\"]>2000 # Sum of value higher than 2K\ncustomer_data.groupby([\"last_order_within_l60d\", \"more_than_two_orders\", \"value_higher_than_2k\"]).count()[\"Lifetime\"]","24257ff2":"conditions = [\n    (customer_data[\"last_order_within_l60d\"]==True)&(customer_data[\"more_than_two_orders\"]==True)&(customer_data[\"value_higher_than_2k\"]==True),\n    (customer_data[\"last_order_within_l60d\"]==True)&(customer_data[\"more_than_two_orders\"]==True)&(customer_data[\"value_higher_than_2k\"]==False),\n    (customer_data[\"last_order_within_l60d\"]==True)&(customer_data[\"more_than_two_orders\"]==False),\n    (customer_data[\"last_order_within_l60d\"]==False)&(customer_data[\"more_than_two_orders\"]==True),\n    (customer_data[\"last_order_within_l60d\"]==False)&(customer_data[\"more_than_two_orders\"]==False)\n]\nmappings = [\"01. high engagement & high value\", \n            \"02. high engagement & low value\", \n            \"03. recent and low frequency\", \n            \"04. old and high frequency\", \n            \"05. low engagement & low value\"]\ncustomer_data['FinalRFM'] = np.select(conditions, mappings, default=\"Others\")\ncustomer_data['FinalRFM'].value_counts()","260e30e6":"customer_data.pivot_table(index=[\"FinalRFM\"], \n                          values=[\"Recency\", \"Frequency\", \"MonetaryValue\", \"Lifetime\"], \n                          aggfunc={\"Recency\": [np.min, np.median, np.max], \n                                   \"Frequency\": [np.min, np.median, np.max], \n                                   \"MonetaryValue\": [np.min, np.median, np.max], \n                                   \"Lifetime\": [np.min, np.median, np.max, \"count\"]})","d46d53ac":"# Calculate the number of new users, inactive users, return users and active users in each month\n# Definition:\n# New users: those who made their first purchase in the current month\n# Active users: those who made purchases in the previous month and in the current month\n# Inactive users: those who made purchases in previous months, but not in the current month\n# Return users: those who made purchases before the previous month, not in the previous month and made purchases agian in the current month\ndata[\"InvoiceMonth\"] = data[\"InvoiceDate\"].apply(lambda x: x.replace(day=1))\ndata[\"InvoiceMonth\"] = data[\"InvoiceMonth\"].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\nuser_month_pivot = data.pivot_table(index=[\"CustomerID\"], \n                                    columns=[\"InvoiceMonth\"], \n                                    values=[\"InvoiceNo\"], \n                                    aggfunc=\"count\", \n                                    fill_value=0)\nuser_month_pivot.head()","56a4058d":"# Replace count of invoices with 1\nuser_month_pivot = user_month_pivot.applymap(lambda x: 1 if x>0 else 0)\nuser_month_pivot.head()","51b4ae7b":"# Get the number of columns\nlen((user_month_pivot).columns)","8a009750":"# Define functions to get user status\ndef user_status(data):\n    status = []\n    for i in range(13):\n    # If the user has no purchase in the current month\n        if data[i] == 0:\n            # If the user has made purchases before\n            if len(status) > 0:\n                # If the user is unregistered in the previous month\n                if status[i-1] == \"unreg\":\n                # The the user is also unregistered this month\n                    status.append(\"unreg\")\n                # Otherwise the user is an active user, i.e., he\/she already registered\n                else:\n                    status.append(\"inactive\")\n            # Otherwise the user is not registered in the current month, i.e., he\/she has never made any purchases\n            else:\n                status.append(\"unreg\")\n        else:\n            # This is the first purchase of the user\n            if len(status) == 0:\n                status.append(\"new\")\n            else:\n                if status[i-1] == \"inactive\":\n                    status.append(\"return\")\n                elif status[i-1] == \"unreg\":\n                    status.append(\"new\")\n                else:\n                    status.append(\"active\")\n    return status","1ad9583e":"user_month_status = pd.DataFrame(user_month_pivot.apply(lambda x: pd.Series(user_status(x)), axis=1))\nuser_month_status.columns = user_month_pivot.columns\nuser_month_status.head()","05f741f0":"month_status_pivot = pd.DataFrame(user_month_status.replace(\"unreg\", np.NaN).apply(lambda x: pd.value_counts(x)))\nmonth_status_pivot.head()","270373fb":"month_status_pivot = month_status_pivot.fillna(0).T\nmonth_status_pivot.reset_index(inplace=True)\nmonth_status_pivot.set_index(\"InvoiceMonth\", inplace=True)","7d60cbdf":"ax = month_status_pivot.plot.area(figsize = (12,6))\nplt.title(\"Number of Users by Status in each month\")\nplt.show()","f70c9e48":"# Intro into RFM Framework\n\nRFM framework is a method used to determine customer value by looking at the following three dimensions:\n* Recency: when is the last time the user takes an action (e.g., login, place an order)?\u00a0\n* Frequency: how many times does the user take this action?\n* Monetary value: what is the sum of monetary value from this user throughout his\/her lifetime?\n\nWhen used properly, RFM becomes a powerful tool to identify the most valuable customer (MVC) of a business. Based on the output from this model, we are able to develop customized CRM strategies for different customer segments. With this post, I want to share the Kaggle notebook from an RFM segmentation analysis plus some tips that I found useful in the application of the model output.","0b672603":"# Create RFM buckets using relative values","c1235395":"# Create RFM buckets using absolute values","f8149ad2":"As we can see, the groups have very different median days of lifetime. This suggests potential bias associated with customer sign up date.","9c29707f":"# Generate simple criteria for labelling","5ad78ed0":"# Bonus: calculate the percentage of new, active, inactive and return users in each month"}}