{"cell_type":{"6af17364":"code","c61d5819":"code","5abd6bec":"code","882dceab":"code","c38e0aae":"code","27d48d6a":"code","603110c7":"code","d0905e36":"code","eef4a720":"code","805e26d6":"code","720692d9":"code","e41c9646":"code","aa95552e":"code","bc150318":"code","f10ac915":"code","5d8b93c4":"code","dd18d597":"code","e6b7d81d":"code","82606bf3":"code","5e6c2163":"code","65c81d93":"code","3d5b902a":"code","b09282eb":"code","ab628674":"code","c3624fcf":"code","e742647e":"code","c7380c09":"code","585147f8":"code","3f0b2ab5":"code","d35bbaf6":"code","7e7db932":"code","845aa2f9":"code","fad119ce":"code","1109e48d":"code","02660a09":"code","a86e1422":"code","0383f0a7":"code","9e17d134":"code","5e1d26ba":"code","5beb7eec":"code","09d549cc":"markdown","29de5c02":"markdown","3e6bf76e":"markdown","4551b799":"markdown","5bf59884":"markdown","bc82c4ce":"markdown","91a1e424":"markdown","7ff07249":"markdown","4397063c":"markdown","7a1a9a4b":"markdown","985d2b96":"markdown","cae49b8f":"markdown","1a0328ec":"markdown","f1ff28ed":"markdown","d08b8934":"markdown","fd237be0":"markdown","051d23e7":"markdown","a2ecf873":"markdown","4b7e0302":"markdown","98ddff2b":"markdown","c60d16f8":"markdown","7d7eb52e":"markdown","74c4acc7":"markdown","839842a2":"markdown","3ffadd1f":"markdown","8ad00605":"markdown","159727cf":"markdown","36fbae09":"markdown","e912987c":"markdown","4b9fed31":"markdown","1136ed42":"markdown","9cf2259f":"markdown","7d4d80e8":"markdown","6917c20b":"markdown","9c623445":"markdown","3f58dc3a":"markdown","aab8ac3c":"markdown","c698972f":"markdown","70bd409e":"markdown","7ab8ad2a":"markdown","bd88d0d6":"markdown","294fd51d":"markdown"},"source":{"6af17364":"# the standard imports that are used for data analysis\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # for plotting our data\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline","c61d5819":"# Read the files\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","5abd6bec":"# This shows us the data structure\nprint(train.shape)\n\n# This will give us an idea of what the data looks like\ntrain.head()","882dceab":"# This will give us a summary of our data\ntrain.describe(include = 'all')","c38e0aae":"# This code will give us a summary of the missing data\nprint(pd.isnull(train).sum())","27d48d6a":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","603110c7":"# draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n# print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","d0905e36":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)","eef4a720":"# Let's check our data again\ntrain.head()","805e26d6":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#I won't be printing individual percent values for all of these.\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","720692d9":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","e41c9646":"# Remove the 'Ticket' column\ntrain = train.drop(['Ticket'],axis = 1)\ntest = test.drop(['Ticket'],axis = 1)","aa95552e":"# Remove the 'Fare' column\ntrain = train.drop(['Fare'],axis = 1)\ntest = test.drop(['Fare'],axis = 1)","bc150318":"# Check the occurance of each 'Embarked' value\nprint(\"S:\")\ns = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(s)\n\nprint(\"C:\")\nc = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(c)\n\nprint(\"Q:\")\nq = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(q)","f10ac915":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","5d8b93c4":"# map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","dd18d597":"# To create a new column to identify Cabin types\ntrain[\"CabinType\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinType\"] = (test[\"Cabin\"].notnull().astype('int'))","e6b7d81d":"# Calculate percentage of survival\nprint(\"Percentage of CabinType = 1 who survived:\", train[\"Survived\"][train[\"CabinType\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of CabinType = 0 who survived:\", train[\"Survived\"][train[\"CabinType\"] == 0].value_counts(normalize = True)[1]*100)","82606bf3":"# Drop the 'Cabin' Column\ntrain = train.drop(['Cabin'],axis = 1)\ntest = test.drop(['Cabin'],axis = 1)","5e6c2163":"#create a combined group of both datasets so it is easier to manage later\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# summary\npd.crosstab(train['Title'], train['Sex'])","65c81d93":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# check survival rate for people with different titles\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","3d5b902a":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","b09282eb":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'],axis = 1)","ab628674":"#sort the ages into logical categories\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","c3624fcf":"# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]\n\ntrain.head()","e742647e":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\n#drop the Age feature\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)\n\ntrain.head()","c7380c09":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","585147f8":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","3f0b2ab5":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","d35bbaf6":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","7e7db932":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","845aa2f9":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)\n","fad119ce":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","1109e48d":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","02660a09":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","a86e1422":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","0383f0a7":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","9e17d134":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","5e1d26ba":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","5beb7eec":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = svc.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","09d549cc":"**Name**\n\nWe are not interested in the Name themselves but rather we are interested in the titles. A title in a Name can indicate your age and social class so what we truly want from this column is the different types of social classes. We will improve this column later.","29de5c02":"**PassengerId**\n\nThis Column is a unique value and every passenger has a different passengerId. Thus this column does not contribute to our final prediction model and we should remove it. However we should keep it until we start to make predictions.","3e6bf76e":"**Splitting the Training Data**\n    We will now split our training data to prevent overfitting and to test the accuracy of our different models. In this case we take 22% as out prediction data.","4551b799":"**Age**\n\nThis column obviously will prove to be useful but there are many problems with this column. This column is actually the main challenge for us to get an accurate prediction. From the information in the missing value section the 'Age' column is actually missing 177 values which is 20% of the entire column. Whenever we deal with missing values it is sugested that if possible we should never delete any data because they are simply missing values the hard part of machine learning is that we actually need to use the best way possible to try to accurately recreate these missing values. The biggest challenge with this data is how we fill out the missing data in the 'Age' column.","5bf59884":"**Inspect the data**\n\nNow we should inspect our data and see what it looks like. The following code will print out the shape of our data. Then we use the code .head to check the first few rows of data and give us a general idea of what the data looks like.","bc82c4ce":"**Missing Values**\n\nNow that we have a better understanding of our data it is time we check how many missing values our data has. How we deal with missing values is important and will affect the accuracu of the model.","91a1e424":"We will now create a new column for the 'Cabin' column that will mark all the cabins that are recorded with 1 and all those that are not recorded with 0. This is important because it is much easier for us to operate on integers rather than strings and Machine Learning models work best with integers as well.","7ff07249":"The output shows that the female have a much higher chance of surviving. This output indicates that we need to keep this column but this column is not exactly the way we want it.\n\nMachine Learning works best with numeric data so when we have a column that we want to use but only has string values then it would be best if we turn the column into numeric data.\n\nIn this case let's create a new column that make the Males 0 and the Females 1.","4397063c":"This indicates that people with 1,2 and 3 children or parents have a higher chance of survival but those with none and with too much have a less chance of survival. We need to keep this column because it proves to be useful for our final predictions.","7a1a9a4b":"S has 644 values which is 72% of the total. \n\nNow that S is confirmed to be the most common value we can just fill the missing value in 'Embarked' with S.","985d2b96":"A bit of an anti-climax wasn't it. You were all hyped up and ready to make your own algorithmns create your own mathematical functions. The truth is Machine Learning has been around longer than you think and there are already lots of great algorithmns ready for us to use and they are hard to beat.","cae49b8f":"Now that we have filled in the missing values we need to alter the 'Embarked' column so it is easier for our model to use.","1a0328ec":"**sibsp**\n\nThis Column shows us the number of siblings or spouse that a passenger has. We are not so sure if this column is useful or not so we need to visualize it first.","f1ff28ed":"**Survived**\n\nObviously we need to keep this column because we need to predict on it but do we need to improve it? This column has numeric values to represent the two different results that we need to predict so it does not need any improvment. We leave this column as is.","d08b8934":"**Choosing the best model**\n\nNow is the most exciting part that all of you have been waiting for. The part where you finally get to build your own Machine Learning model!","fd237be0":"From the data above we can tell what type of data each column is.\n\n* 1. Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n* 1. Categorical Features: Survived, Sex, Embarked, Pclass\n* 1. Alphanumeric Features: Ticket, Cabin\n\nNumerical Features are data that contains numeric values. There are Continuous and Discrete. Continuous is where there are many different types of data and Discrete is when the amount of numbers is limited or variation is very few.\n\nCategorical Features is exactly as the name implies the column is categorized to differnt types.\n\nAlphanumeric values are strings that have combined letters and numbers. They will be less likely featured in our prediction model but sometime you can gain some information out of these variables.","051d23e7":"**Going through the Columns**\n\nWe now want to inspect the data and remove certain columns that does not contribute to our model. If we can narrow the most useful data then our model will perform better.\n\nWe also want to alternate the way of how certain columns are presented. You will learn how shortly.","a2ecf873":"**Embarked**\n\nWe need to keep this column but from the missing values above we can see that it is missing 2 values. When we encounter situations like these where the column is missing values but not a lot. It is best to fill in the missing values with the most occured value.","4b7e0302":"**Read the data**\n\nNow we will read the data and do some data processing on it.","98ddff2b":"This indicates that being in a higher class you would have a higher chance of survival so this Column is very important to us for our final predictions.","c60d16f8":"**Fare**\n\nThis is an interesting column. I believe that this column can be seen as the same as the 'Pclass' column and I think it should be removed. There are too many things that could affect the 'Fare' value but the 'Pclass' value is solid.","7d7eb52e":"**Extract Information from Name**\n\nNow let's see if we can extract some interesting information from the 'Name' column. What we really want is the titles of the people because they can someimtes indicate their age and social status.\n\nWe will now create a new column to do that.","74c4acc7":"Now using the 'Title' column that we have created we will determine what the missing ages are. We will fill the missing ages by using the mode of the age groups that have the same title as the missing value.","839842a2":"**Sex**\n\nThis is yet another obvious Column that we should include. Many may argue that no matter the sex Man and Woman have an equal chance of surviving. Let's see if that is true.","3ffadd1f":"I don't believe this is the best way to solve the missing values for 'Age' but that is the job for us. To keep on thinking and innovating and eventually come up with better ways to make the data better.","8ad00605":"This is a very interesting development. In general people with more siblings are less likely to survive because they need to find and carry all the family members which will slow them down. The interesting part is people with 1 or 2 siblings or spouse actually have a higher chance of surviving compared to those with none. This column should be kept.","159727cf":"Now we can drop the 'Name' column because we have all the information we need.","36fbae09":"Now lets calculate the percentage of those with a recorded cabin vs those without a recorded cabin","e912987c":"Now we know that people with different titles have various rates of survival. This information could also be useful when we are trying to determine what the missing ages are.","4b9fed31":"Those with a recorded Cabin have a 67% chance of surviving and those without only had a 30% chance of surviving. This can indicate that those with a recorded Cabin could have a higher social status or they have a better location for escape when accidents happen. So now the 'CabinType' is relavant to our final model but we can drop the 'Cabin' Column because it has served its purpose.","1136ed42":"Now we use the #1 model to submit our results.","9cf2259f":"Now we create a list to give us a better visualization of the accuracy","7d4d80e8":"**Fixing the Age**\n\nNow we finally get to the most difficult part of this dataset: the missing age values. Before we head on and start trying to fill those out let's first give the 'Age' column less variaton.\n\nUsing the age value we will now alter the 'Age' column and create titles for people in different age groups such as YoungAdult, Adult etc.","6917c20b":"**Cabin**\n\nNow let's look at the 'Cabin' column. From the missing data compared to the total we can see that there are 687 values missing from 891 total values in the 'Cabin' column. That means that there is 77% of values missing from this column. My initial thought was that this column is missing so much value that thi column isn't very useful for the model. After doing some research on some other Notebooks on this data others have argued that the people who have revorded values for their cabin have a higher chance of surviving. Lets see if this is true.","9c623445":"**Parch**\n\nThis column shows the number of children or parents that passengers have. Let's see if this column is useful or not.","3f58dc3a":"**Machine Learning for Beginners**\n\nThis is an introduction for Machine Learning. There are many different Machine Learning models but they almost all have the same form. This tutorial will teach you how to get started with writing Machine Learning models.\n\nThis Titanic database is not so complicated so it is suitable for beginners.\n\nThis Notebook was of great help to get me started so please check it out and support it. My version includes more explanation and I wish to improve the way of filling missing values in the 'Age' column.\n\nhttps:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner","aab8ac3c":"**Pclass**\n\nThis variable indicates which class each passenger was in. This is obviously useful but lets chekc how so.","c698972f":"We can now see that after our tranformation the 'Sex' column now only contains 0 and 1 and that is what we want.","70bd409e":"The data shows that we are missing values in 'Age', 'Cabin' and 'Embarked'.","7ab8ad2a":"**Imports**\n\nThe sections below will be some standard imports for data analysis (not model building). Just remember that numpy, pandas and matplot are the most important and common imports and you will likely use them for all your Machine Learning model analysis moving forward.","bd88d0d6":"**Ticket**\n\nThis column is alphanumerical data and this one is hard to actuallty gain any benefit from so we will simply remove this column.","294fd51d":"**Testing Different Models**\n\nThere are many algorithmns that are very powerful and are already available for us to use.\n\nGaussian Naive Bayes\nLogistic Regression\nSupport Vector Machines\nPerceptron\nDecision Tree Classifier\nRandom Forest Classifier\nKNN or k-Nearest Neighbors\nStochastic Gradient Descent\nGradient Boosting Classifier\n\nNow let's test them out."}}