{"cell_type":{"6c4db60a":"code","70180778":"code","e9e523b6":"code","762376d6":"code","a9dd7498":"code","62d78215":"code","b452c444":"code","8f8bd092":"code","8af67723":"code","50de9660":"code","c461d775":"code","b743438c":"code","d8f313cb":"code","dfdfdd0d":"code","394e9a30":"code","32a46119":"code","9600bcc3":"code","2a2724f3":"code","e33d7b58":"code","24bca5ae":"code","ae93abe0":"code","84da3634":"code","680b7686":"code","527f62d8":"code","cee13dce":"markdown","473c7e5a":"markdown","1a440e63":"markdown","04dc3877":"markdown","8e3eb70d":"markdown","40e11a0d":"markdown","a1dd5f2a":"markdown","15a128be":"markdown","977ee491":"markdown","def6deaf":"markdown","865064aa":"markdown","9503658b":"markdown","3faeb648":"markdown","a9dfdf29":"markdown","508d4cbd":"markdown","7bdfafa1":"markdown","5790568e":"markdown","ab7bca45":"markdown"},"source":{"6c4db60a":"import numpy as np\nimport pandas as pd\n\nfrom pandas.io.json import json_normalize","70180778":"specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\nspecs.head()","e9e523b6":"specs['args'].value_counts()","762376d6":"specs = specs.drop(columns='args')","a9dd7498":"specs['info'].value_counts()","62d78215":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nspecs['info'] = le.fit_transform(specs['info'])\nspecs","b452c444":"specs.index = specs.event_id\nspecs = specs.drop(columns='event_id')\nspecs","8f8bd092":"test_head = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', nrows=10000)\ntrain_head = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', nrows=10000)","8af67723":"print(f'Columns in test: {test_head.shape[1]}')\nprint(f'Columns in train: {train_head.shape[1]}')","50de9660":"test_head.sample(5)","c461d775":"train_head.sample(5)","b743438c":"train_head.info()","d8f313cb":"print(f'Unique values:')\n\nfor column in train_head.columns:\n    unique = train_head[column].value_counts().count()\n    notNA = train_head[column].value_counts().sum()\n    \n    print(f'{column} : {(unique\/notNA):.0%} ({unique} of {notNA}), type: {train_head[column].dtype}')\n    \ndel column    \ndel unique\ndel notNA\ndel train_head\ndel test_head","dfdfdd0d":"categorical_columns = [\n    'game_session', \n    'installation_id', \n    'event_code',\n    'title',\n    'type', \n    'world'\n]\n\nmerging_cols = [\n    'event_id',\n]\n\ncols_for_time_parsing = [\n    'timestamp',\n]\n\njson_cols = [\n    'event_data',\n]\n\nnumerical_cols = [\n    'event_count',\n    'game_time',\n]","394e9a30":"# event_data column\nimport json\ndef cols_in_json():\n    path = '\/kaggle\/input\/data-science-bowl-2019\/test.csv'\n    column = 'event_data'\n    size = 500000\n    \n    # read and transform data from train file\n    file_part_json = pd.read_csv(path, usecols=[column], nrows=size)\n    file_part_json = file_part_json['event_data'].apply(json.loads)\n    file_part_json = json_normalize(file_part_json)\n\n    # make a list of columns that have values in more than 30% of rows\n    cols_to_save = []\n    for col in file_part_json.columns:\n        has_value = file_part_json[col].value_counts().sum()\n\n        if (has_value\/size > 0.3):\n            print(f'{(has_value):6.0f} values - {(has_value\/size):4.0%} - in column {col}')\n            cols_to_save.append(col)\n\n\n    #print results\n    print('')\n    print(f'cols_to_save ({len(cols_to_save)} columns): {cols_to_save}')\n    \n    return cols_to_save\n\n# cols_to_save = cols_in_json() @making error on kaggle, but works at home, I'm trying to find out why \ncols_to_save = ['event_code', 'event_count', 'round', 'game_time', 'coordinates.x', 'coordinates.y', 'coordinates.stage_width', 'coordinates.stage_height', 'description', 'identifier', 'media_type', 'duration']","32a46119":"def mem_reduce(df):\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    return df","9600bcc3":"size = 100001","2a2724f3":"def file_processing(path):\n    result_set = pd.DataFrame()\n    for column in range(2):\n\n        column_name = pd.read_csv(path, usecols=[column], nrows=1).columns[0]\n        if column_name in json_cols:\n            result_col = pd.DataFrame(columns=cols_to_save)\n        else:\n            result_col = pd.DataFrame(columns=[column_name])\n\n\n        #starting a loop\n        skiprows = 0\n        file_is_over=False\n        while file_is_over == False:\n\n            # read next part, rename the columns and concat with the result df\n            file_part = pd.read_csv(path, usecols=[column], nrows=size, skiprows=skiprows)\n\n\n            # json processing for event data col\n            if column_name in json_cols: \n                file_part = file_part[file_part.columns[0]].apply(json.loads)\n                file_part = json_normalize(file_part)\n\n                for col in file_part.columns:\n                    if col not in cols_to_save:\n                        file_part = file_part.drop(columns=col)\n\n            else:\n                file_part.columns=[column_name]\n\n            #time parsing\n            if column_name in cols_for_time_parsing: \n                file_part['timestamp'] = pd.to_datetime(file_part['timestamp'], format='%Y-%m-%dT%H:%M:%S.%fZ')    \n\n\n            result_col = pd.concat([result_col, file_part], sort=False)\n\n            #iterate until the 'tail' of the file\n            file_is_over = True if len(file_part) < size else False \n            skiprows += size\n\n            result_col = mem_reduce(result_col)\n\n            print(f'Read {(skiprows):.0f} rows of the column #{column+1} ({column_name})')\n\n\n        if column_name in categorical_columns:\n            result_col[column_name] = le.fit_transform(result_col.values)\n            result_col[column_name] = result_col[column_name].astype('category') \n\n        if column_name in merging_cols:\n            result_col = pd.merge(result_col, specs, on='event_id', how='left')\n\n\n        result_set = pd.merge(result_set, result_col, how='right', left_index=True, right_index=True)\n\n    return result_set","e33d7b58":"test_set = file_processing('\/kaggle\/input\/data-science-bowl-2019\/test.csv')","24bca5ae":"test_set.head()","ae93abe0":"test_set.info()","84da3634":"train_set = file_processing('\/kaggle\/input\/data-science-bowl-2019\/train.csv')","680b7686":"train_set.head()","527f62d8":"train_set.info()","cee13dce":"## Columns analysis","473c7e5a":"Looks like `args` is a pretty useless column, lets drop it","1a440e63":"Lets look on the test and train sets","04dc3877":"# test and train","8e3eb70d":"According to dataset description useful information can be found in files:\n* specs.csv\n* train.csv\n* test.csv\n\nLets try to get all useful info from these sets","40e11a0d":"168 unique values and some of them repeated more than 10 times in a set. Lets  label the `info` column","a1dd5f2a":"Feel free to upvote! ","15a128be":"Make a closer look to the args column","977ee491":"The same procedure with the `info` column","def6deaf":"# Installs","865064aa":"### choose columns to save in json data","9503658b":"Three previous cells help us to separate columns to several groups:\n\n**merging column** (as we should use data from the `specs` file) :\n* 'event_id' (index=0)\n\n**time parsing column**\n* 'timestamp'\n\n**numerical columns** (where there are only numericals already and the number is valuabe and should not be converted to categorical):\n* 'event_count'\n* 'game_time'\n\n**json parsing column**:\n* 'event_data'\n\n**categorical columns** (other object columns that have low rate of unique values):\n* 'game_session'\n* 'installation_id',\n* 'event_code'\n* 'title'\n* 'type'\n* 'world'","3faeb648":"## Applying to sets","a9dfdf29":"The work with `specs.csv` is done for now. We will merge it with train\/test dataframe later","508d4cbd":"Looks life both sets have the same column structure. \n\nAs we need to work with a very large files, I will read both files column per column, slicing every column on a separate parts. ","7bdfafa1":"# specs.csv","5790568e":"## making functions","ab7bca45":"### processing"}}