{"cell_type":{"9e9411d1":"code","d00d321f":"code","dea61411":"code","cccff23b":"code","cd63973f":"code","528cf25a":"code","fa02a19f":"code","26971639":"code","e5dff5e4":"code","e4df505c":"code","5e014ce6":"code","291fd234":"code","fba61211":"code","731eb059":"code","7a045140":"code","aef70dcc":"code","698165d2":"markdown","9cdb1acd":"markdown","5e807fb3":"markdown","63b89a05":"markdown","6e21db78":"markdown","8c44a26e":"markdown","6bc63ce6":"markdown","44dcdffd":"markdown","6fb2295f":"markdown","e4b4b0c8":"markdown","02da24de":"markdown"},"source":{"9e9411d1":"import gc\nimport os\nimport sys\nimport time\nimport random\nimport logging\nimport datetime as dt\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torchvision as vision\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom contextlib import contextmanager\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom fastprogress import master_bar, progress_bar\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import fbeta_score\n\ntorch.multiprocessing.set_start_method(\"spawn\")","d00d321f":"@contextmanager\ndef timer(name=\"Main\", logger=None):\n    t0 = time.time()\n    yield\n    msg = f\"[{name}] done in {time.time() - t0} s\"\n    if logger is not None:\n        logger.info(msg)\n    else:\n        print(msg)\n        \n\ndef get_logger(name=\"Main\", tag=\"exp\", log_dir=\"log\/\"):\n    log_path = Path(log_dir)\n    path = log_path \/ tag\n    path.mkdir(exist_ok=True, parents=True)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n\n    fh = logging.FileHandler(\n        path \/ (dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") + \".log\"))\n    sh = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        \"%(asctime)s %(name)s %(levelname)s %(message)s\")\n\n    fh.setFormatter(formatter)\n    sh.setFormatter(formatter)\n    logger.addHandler(fh)\n    logger.addHandler(sh)\n    return logger\n\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True","dea61411":"logger = get_logger(name=\"Main\", tag=\"Pytorch-ResNet34\")","cccff23b":"!ls ..\/input\/imet-2019-fgvc6\/","cd63973f":"labels = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/labels.csv\")\ntrain = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/train.csv\")\nsample = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/sample_submission.csv\")\ntrain.head()","528cf25a":"!cp ..\/input\/pytorch-pretrained-image-models\/* .\/\n!ls","fa02a19f":"class IMetImageDataset(data.DataLoader):\n    def __init__(self, root_dir: Path, \n                 df: pd.DataFrame, \n                 mode=\"train\",\n                 device=\"cuda:0\",\n                 transforms=None):\n        self._root = root_dir\n        self.transform = transforms[mode]\n        self._img_id = (df[\"id\"] + \".png\").values\n        self.labels = df.attribute_ids.map(lambda x: x.split()).values\n        self.mode = mode\n        self.device = device\n        \n    def __len__(self):\n        return len(self._img_id)\n    \n    def __getitem__(self, idx):\n        img_id = self._img_id[idx]\n        file_name = self._root \/ img_id\n        img = Image.open(file_name)\n        \n        if self.transform:\n            img = self.transform(img)\n        if self.mode == \"train\" or self.mode == \"val\":\n            label = self.labels[idx]\n            label_tensor = torch.zeros((1, 1103))\n            for i in label:\n                label_tensor[0, int(i)] = 1\n            label_tensor = label_tensor.to(self.device)\n            return [img.to(self.device), label_tensor]\n        else:\n            return [img.to(self.device)]\n    \n    \ndata_transforms = {\n    'train': vision.transforms.Compose([\n        vision.transforms.RandomResizedCrop(224),\n        vision.transforms.RandomHorizontalFlip(),\n        vision.transforms.ToTensor(),\n        vision.transforms.Normalize(\n            [0.485, 0.456, 0.406], \n            [0.229, 0.224, 0.225])\n    ]),\n    'val': vision.transforms.Compose([\n        vision.transforms.Resize(256),\n        vision.transforms.CenterCrop(224),\n        vision.transforms.ToTensor(),\n        vision.transforms.Normalize(\n            [0.485, 0.456, 0.406], \n            [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_transforms[\"test\"] = data_transforms[\"val\"]","26971639":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.linear = nn.Linear(512, 1103)\n        self.drop = nn.Dropout(0.3)\n        \n    def forward(self, x):\n        x = self.drop(self.linear(x))\n        return torch.sigmoid(x)\n\n\nclass ResNet34(nn.Module):\n    def __init__(self, pretrained: Path):\n        super(ResNet34, self).__init__()\n        self.resnet34 = vision.models.resnet34()\n        self.resnet34.load_state_dict(torch.load(pretrained))\n        self.resnet34.fc = Classifier()\n        \n    def forward(self, x):\n        return self.resnet34(x)","e5dff5e4":"class Trainer:\n    def __init__(self, \n                 model, \n                 logger,\n                 n_splits=5,\n                 seed=42,\n                 device=\"cuda:0\",\n                 train_batch=32,\n                 valid_batch=128,\n                 kwargs={}):\n        self.model = model\n        self.logger = logger\n        self.device = device\n        self.n_splits = n_splits\n        self.seed = seed\n        self.train_batch = train_batch\n        self.valid_batch = valid_batch\n        self.kwargs = kwargs\n        \n        self.best_score = None\n        self.tag = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        self.loss_fn = nn.BCELoss(reduction=\"mean\").to(self.device)\n        \n        path = Path(f\"bin\/{self.tag}\")\n        path.mkdir(exist_ok=True, parents=True)\n        self.path = path\n        \n    def fit(self, X, n_epochs=10, kfold=False):\n        train_preds = np.zeros((len(X), 1103))\n        if kfold:\n            fold = KFold(n_splits=self.n_splits, random_state=self.seed)\n            for i, (trn_idx, val_idx) in enumerate(fold.split(X)):\n                self.fold_num = i\n                self.logger.info(f\"Fold {i + 1}\")\n                X_train, X_val = X.loc[trn_idx, :], X.loc[val_idx, :]\n\n                valid_preds = self._fit(X_train, X_val, n_epochs)\n                train_preds[val_idx] = valid_preds\n            return train_preds\n        else:\n            idx = np.arange(X.shape[0])\n            self.fold_num = 0\n            trn_idx, val_idx = train_test_split(\n                idx, test_size=0.2, random_state=self.seed)\n            X_train, X_val = X.loc[trn_idx, :], X.loc[val_idx, :]\n            valid_preds = self._fit(X_train, X_val, n_epochs)\n            train_preds = valid_preds\n            return train_preds, y_val\n    \n    def _fit(self, X_train, X_val, n_epochs):\n        seed_torch(self.seed)\n        train_dataset = IMetImageDataset(root_dir=Path(\"..\/input\/imet-2019-fgvc6\/train\/\"), \n                                         df=X_train, \n                                         mode=\"train\", \n                                         device=self.device, \n                                         transforms=data_transforms)\n        train_loader = data.DataLoader(train_dataset, \n                                       batch_size=self.train_batch,\n                                       shuffle=True)\n\n        valid_dataset = IMetImageDataset(root_dir=Path(\"..\/input\/imet-2019-fgvc6\/train\/\"), \n                                         df=X_val, \n                                         mode=\"val\", \n                                         device=self.device, \n                                         transforms=data_transforms)\n        valid_loader = data.DataLoader(valid_dataset,\n                                       batch_size=self.valid_batch,\n                                       shuffle=False)\n        \n        model = self.model(**self.kwargs)\n        model.to(self.device)\n        \n        optimizer = optim.Adam(params=model.parameters(), \n                                lr=0.0001)\n        scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs)\n        best_score = np.inf\n        mb = master_bar(range(n_epochs))\n        for epoch in mb:\n            model.train()\n            avg_loss = 0.0\n            for i_batch, y_batch in progress_bar(train_loader, parent=mb):\n                y_pred = model(i_batch)\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_loader)\n            valid_preds, avg_val_loss = self._val(valid_loader, model)\n            scheduler.step()\n\n            self.logger.info(\"=========================================\")\n            self.logger.info(f\"Epoch {epoch + 1} \/ {n_epochs}\")\n            self.logger.info(\"=========================================\")\n            self.logger.info(f\"avg_loss: {avg_loss:.8f}\")\n            self.logger.info(f\"avg_val_loss: {avg_val_loss:.8f}\")\n            \n            if best_score > avg_val_loss:\n                torch.save(model.state_dict(),\n                           self.path \/ f\"best{self.fold_num}.pth\")\n                self.logger.info(f\"Save model at Epoch {epoch + 1}\")\n                best_score = avg_val_loss\n        model.load_state_dict(torch.load(self.path \/ f\"best{self.fold_num}.pth\"))\n        valid_preds, avg_val_loss = self._val(valid_loader, model)\n        self.logger.info(f\"Best Validation Loss: {avg_val_loss:.8f}\")\n        return valid_preds\n    \n    def _val(self, loader, model):\n        model.eval()\n        valid_preds = np.zeros((len(loader.dataset), 1103))\n        avg_val_loss = 0.0\n        for i, (i_batch, y_batch) in enumerate(loader):\n            with torch.no_grad():\n                y_pred = model(i_batch).detach()\n                avg_val_loss += self.loss_fn(y_pred, y_batch).item() \/ len(loader)\n                valid_preds[i * self.valid_batch:(i + 1) * self.valid_batch] = \\\n                    y_pred.cpu().numpy()\n        return valid_preds, avg_val_loss\n    \n    def predict(self, X):\n        dataset = IMetImageDataset(root_dir=Path(\"..\/input\/imet-2019-fgvc6\/test\/\"), \n                                   df=X, \n                                   mode=\"test\", \n                                   device=self.device, \n                                   transforms=data_transforms)\n        loader = data.DataLoader(dataset, \n                                 batch_size=self.valid_batch, \n                                 shuffle=False)\n        model = self.model(**self.kwargs)\n        preds = np.zeros((X.size(0), 1103))\n        for path in self.path.iterdir():\n            with timer(f\"Using {str(path)}\", self.logger):\n                model.load_state_dict(torch.load(path))\n                model.to(self.device)\n                model.eval()\n                temp = np.zeros_like(preds)\n                for i, (i_batch, ) in enumerate(loader):\n                    with torch.no_grad():\n                        y_pred = model(i_batch).detach()\n                        temp[i * self.valid_batch:(i + 1) * self.valid_batch] = \\\n                            y_pred.cpu().numpy()\n                preds += temp \/ self.n_splits\n        return preds","e4df505c":"trainer = Trainer(ResNet34, \n                  logger, \n                  train_batch=64, \n                  kwargs={\"pretrained\": \"resnet34.pth\"})\ngc.collect()","5e014ce6":"y = train.attribute_ids.map(lambda x: x.split()).values\nvalid_preds, y_val = trainer.fit(train, n_epochs=22, kfold=False)","291fd234":"def threshold_search(y_pred, y_true):\n    score = []\n    candidates = np.arange(0, 1.0, 0.01)\n    for th in progress_bar(candidates):\n        yp = (y_pred > th).astype(int)\n        score.append(fbeta_score(y_pred=yp, y_true=y_true, beta=2, average=\"samples\"))\n    score = np.array(score)\n    pm = score.argmax()\n    best_th, best_score = candidates[pm], score[pm]\n    return best_th, best_score","fba61211":"best_threshold, best_score = threshold_search(valid_preds, y_val)\nbest_score","731eb059":"test_preds = trainer.predict(sample)","7a045140":"preds = (test_preds > best_threshold).astype(int)","aef70dcc":"prediction = []\nfor i in range(preds.shape[0]):\n    pred1 = np.argwhere(preds[i] == 1.0).reshape(-1).tolist()\n    pred_str = \" \".join(list(map(str, pred1)))\n    prediction.append(pred_str)\n    \nsample.attribute_ids = prediction\nsample.to_csv(\"submission.csv\", index=False)\nsample.head()","698165d2":"## DataLoader","9cdb1acd":"## Libraries","5e807fb3":"## Problem description\n\nIn this kernel, we are going to use Resnet34 pretrained model to fine tune with Pytorch.","63b89a05":"## Post process - threshold search -","6e21db78":"## Prediction for test data","8c44a26e":"## Train Utilities","6bc63ce6":"## Model","44dcdffd":"## Utilities","6fb2295f":"## Training","e4b4b0c8":"Since I used sigmoid for the activation, I've got the 1103 probability output for each data row.\n\nI need to decide threshold for this.There are two ways to deal with this.\n\n- Class-wise threshold search\n  - Takes some time but it's natural.\n- One threshold for all the class\n  - Low cost way.\n\n**UPDATE**\nI will use the first -> second one.","02da24de":"## Data Loading"}}