{"cell_type":{"fc210117":"code","e58197fd":"code","fa979b77":"code","44e60891":"code","3254135f":"code","bb3d81d7":"code","e18bbfbd":"code","9fd5a1ce":"markdown"},"source":{"fc210117":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n!pip install interpret >\/dev\/null\n\nimport pickle\n\nfrom interpret import show\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV,RepeatedStratifiedKFold","e58197fd":"df_ = pd.read_csv('\/kaggle\/input\/incidents\/public.csv',parse_dates=[0,], dtype={\"TIME_TYPE\": 'category', \"WORK_NO\": str, \"WORK_DEST\": str, \"FUNC_CAT\": 'category', \"TOT_BRK_TM\": float, \"EmpNo_Anon\": float, \"Incident_Number\": 'category'})","fa979b77":"train_df, test = train_test_split(df_, test_size=0.2, random_state=42)","44e60891":"wa_hols = [\n    '2009-01-01', '2009-01-26', '2009-03-02', '2009-04-10', '2009-04-11',\n    '2009-04-12', '2009-04-13', '2009-04-25', '2009-04-27', '2009-06-01',\n    '2009-09-28', '2009-12-25', '2009-12-26', '2009-12-28', '2010-01-01',\n    '2010-01-26', '2010-03-01', '2010-04-02', '2010-04-05', '2010-04-26',\n    '2010-06-07', '2010-09-27', '2010-12-25', '2010-12-26', '2010-12-27',\n    '2010-12-28', '2011-01-01', '2011-01-26', '2011-03-07', '2011-04-22',\n    '2011-04-25', '2011-04-26', '2011-06-06', '2011-10-28', '2011-12-25',\n    '2011-12-26', '2011-12-27', '2012-01-01', '2012-01-02', '2012-01-26',\n    '2012-03-05', '2012-04-06', '2012-04-09', '2012-04-25', '2012-06-04',\n    '2012-10-01', '2012-12-25', '2012-12-26', '2013-01-01', '2013-01-26',\n    '2013-03-04', '2013-03-29', '2013-04-01', '2013-04-25', '2013-06-03',\n    '2013-09-30', '2013-12-25', '2013-12-26', '2014-01-01', '2014-01-27',\n    '2014-03-03', '2014-04-18', '2014-04-19', '2014-04-21', '2014-04-25',\n    '2014-06-02', '2014-09-29', '2014-12-25', '2014-12-26', '2015-01-01',\n    '2015-01-26', '2015-03-02', '2015-04-03', '2015-04-04', '2015-04-06',\n    '2015-04-25', '2015-04-27', '2015-06-01', '2015-09-28', '2015-12-25',\n    '2016-01-01', '2016-01-26', '2016-03-07', '2016-03-25', '2016-03-28',\n    '2016-04-25', '2016-06-06', '2016-09-26', '2016-12-25', '2016-12-26',\n    '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-26', '2017-03-06',\n    '2017-04-14', '2017-04-17', '2017-04-25', '2017-06-05', '2017-09-25',\n    '2017-12-25', '2017-12-26', '2018-01-01', '2018-01-26', '2018-03-05',\n    '2018-03-30', '2018-04-02', '2018-04-25', '2018-06-04', '2018-09-24',\n    '2018-12-25', '2018-12-26', '2019-01-01', '2019-01-28', '2019-03-04',\n    '2019-04-19', '2019-04-22', '2019-04-25', '2019-06-03', '2019-09-30',\n    '2019-12-25', '2019-12-26', '2020-01-01', '2020-01-27', '2020-03-02',\n    '2020-04-10', '2020-04-13', '2020-04-25', '2020-04-27', '2020-06-01',\n    '2020-09-28', '2020-12-25', '2020-12-26', '2020-12-28', '2021-01-01',\n    '2021-01-26', '2021-03-01', '2021-04-02', '2021-04-05', '2021-04-25',\n    '2021-04-26', '2021-06-07', '2021-09-27', '2021-12-25', '2021-12-26',\n    '2021-12-27', '2021-12-28'\n]\nwa_hols = pd.to_datetime(wa_hols, format=\"%Y-%m-%d\")\n\n# We have thousands of work descriptions, lets just use the top 50, the rest will be \"Other\"\nWORK_DESC_TOP = [\n    \"Training\",\n    \"Admin\",\n    \"TCS: Pole Distribution\",\n    \"NC- Distribution Standard Jobs\",\n    \"TCS: Conductor\",\n    \"Safety\",\n    \"SEQT - Payroll Costed Hours\",\n    \"Housekeeping\",\n    \"TCS: NP   No Power\",\n    \"Data Maintenence\",\n    \"CUSTOMER SERVICE - PAYROLL COSTED HOURS\",\n    \"Meeting\/briefings\",\n    \"NC- Transmission Standard Jobs\",\n    \"Commercial - Payroll Costed Hours\",\n    \"Downtime\",\n    \"CONSTRUCTION\",\n    \"Visual Management\",\n    \"F&M - Payroll Costed Hours\",\n    \"REGION NORTH - L3 STORM DAMAGE\",\n    \"Ops Maintenance - Payroll Costed Hours\",\n    \"TCS: PB   Pole Broken\/Damaged\",\n    \"Data Management\",\n    \"Property & Fleet - Payroll Costed Hours\",\n    \"NMP&D - Payroll Costed Hours\",\n    \"BP&R - Payroll Costed Hours\",\n    \"NCIMP Continuous Improvement\",\n    \"TCS: Distribution Transformer\",\n    \"TCS: RT   Recloser Trip\",\n    \"REGION METRO - L3 STORM DAMAGE\",\n    \"Ten Hour Break\",\n    \"TCS: LV Cable Underground\",\n    \"Temporary Disconnection Only\",\n    \"TCS: Pole Distribution (T-Fixed)\",\n    \"General Training & Travel\",\n    \"HUMAN RESOURCES - PAYROLL COSTED HOURS\",\n    \"Fault Ready\",\n    \"TCS: HV Cable Underground\",\n    \"TCS: ES   Electric Shock\",\n    \"TCS: DOFT Drop Out Fuse Trip\",\n    \"Project Scoping\",\n    \"TCS: LV Cross Arm\",\n    \"Network Ops - Payroll Costed Hours\",\n    \"TCS: PP   Part Power\",\n    \"TCS: MC   Miscellaneous Non Hazard\",\n    \"CCS PAYROLL COSTED HOURS\",\n    \"Project Management\",\n    \"METER REPLACEMENT - Internal\",\n    \"TCS: MH   Miscellaneous Hazard\",\n    \"DFIS\/DFMS Data Correction Metro\",\n    \"Level 3 Event Waroona B\/fires Jan 2016\",\n]\n\ninput_cols = [\n    \"WORK_DESC_top\",\n    \"TIME_TYPE\",\n    \"FUNC_CAT\",\n    \"TOT_BRK_TM\",\n    \"hour\",\n    \"holiday\",\n    \"day_of_year\",\n    \"day_name\",\n    \"week\",\n]\n\n\ndef top_work_desc_feat(x):\n    return x if x in WORK_DESC_TOP else \"Other\"\n\n\ndef shuffle_df(df, random_seed=42):\n    return df.sample(frac=1, random_state=random_seed, replace=False)\n\n\ndef oversample(df, y_col, n=None, random_state=42):\n    \"\"\"Sample an equal amount from each class, with replacement\"\"\"\n    gs = [g for _, g in df.groupby(y_col)]\n    if n is None:\n        n = max(len(g) for g in gs)\n\n    # sample equal number of each group\n    gs = [g.sample(n, random_state=random_state, replace=True) for g in gs]\n    # concat, and shuffle\n    df = pd.concat(gs, 0)\n    df = shuffle_df(df)\n    return df\n\n\ndef preprocess(df, is_training = True):\n    # Work descriptions have high cardinality, but machine learning works best with a limited number of categories. We take the most common work descriptions and name the rest \"Other\".\n    df[\"WORK_DESC_top\"] = df[\"WORK_DESC\"].apply(top_work_desc_feat)\n\n    # Add some time features. See https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.dt.html and https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html\n    dt = df.Work_DateTime.dt\n    df[\"hour\"] = dt.hour\n    df[\"day_of_week\"] = dt.dayofweek\n    df[\"day_of_year\"] = dt.dayofyear\n    df[\"day_name\"] = dt.day_name()\n    df[\"week\"] = dt.week\n\n    # Was the day a Western Australia public holiday?\n    df[\"holiday\"] = dt.round(\"1D\").isin(wa_hols)\n\n    if is_training:\n        return df[input_cols + ['incident']]\n    else:\n        return df[input_cols]","3254135f":"target_columns = [\"incident\"]\n\ndef train(data):\n    df = preprocess(data)\n\n    # our ExplainableBoostingClassifier requires a balanced training dataset. We can acheive this by oversampling. To speed it up we train on only N samples with the positive and negative incidents, with replacement.\n    df = oversample(df, \"incident\", n=15000)\n\n    # the target variable is incident\n    y_train = df[target_columns]\n    X_train = df.drop(columns=target_columns)\n    distributions = {'interactions':[0,1,2,3], 'learning_rate':[0.01, 0.001, 0.0001, 0.0001, 0.00001, 0.000001]}\n    clf = ExplainableBoostingClassifier(random_state=42)\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    model = RandomizedSearchCV(clf, distributions, score='roc_auc', cv=cv, n_iter=500, n_jobs=-1,random_state=42)\n    \n    model.fit(X_train, y_train)\n    print(model.best_params_)\n    \n    return model","bb3d81d7":"model = train(train_df)\nTest_prep = preprocess(test, False)\npredictions = model.predict(Test_prep)","e18bbfbd":"target_columns = [\"incident\"]\ntargets = test[target_columns]\ndef scoring_fn(y, y_pred):\n    return roc_auc_score(y, y_pred)\nscoring_fn(targets, predictions)","9fd5a1ce":"1. pipeline 0.579417400238157\n2. model = ExplainableBoostingClassifier(random_state=42, interactions=1) 0.588399293221321\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.0005) 0.5963738180836489\n4. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00001)  0.6325328858802637\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00005, max_leaves=5) 0.6033818102070815\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00005, max_leaves=4) 0.601622699827788\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00005, max_leaves=3) 0.6353674698704384\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00005, max_leaves=2) 0.5350401490237442\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00005, max_leaves=3, max_interaction_bins=64)\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00001) 0.6325328858802637\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=1, learning_rate=0.00001, max_leaves=4) 0.6055227160558749\n\n\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=2, learning_rate=0.0001) 0.6193413827881906\n1. model = ExplainableBoostingClassifier(random_state=42, interactions=2, learning_rate=0.0005, max_leaves=3, max_interaction_bins=64) 0.592148835393329\n5. model = ExplainableBoostingClassifier(random_state=42, interactions=2, learning_rate=0.00001) 0.6131757146205796\n"}}