{"cell_type":{"a9a35af7":"code","9a3b32c4":"code","5e8503e0":"code","fb5e75bf":"code","7a2f73c8":"code","e5804dc5":"code","06ca05b6":"code","967d30f1":"code","21533f93":"code","797169ab":"code","8773f27f":"code","49ff3fbb":"code","ef21a7fe":"code","4aea3bcb":"code","82355814":"code","706f392e":"code","02e6f187":"code","5b0bc18c":"code","384cb7e3":"code","63e452db":"code","4ec96490":"code","c414ebfd":"code","b5640ecc":"code","31b556a7":"code","99c68958":"code","0b12943f":"code","f660a1df":"code","837ee0f9":"code","e2d1120a":"code","0150132f":"code","4bbaca41":"code","21f2151a":"markdown","86a26e60":"markdown","c7d9e0d7":"markdown","0e0cdff8":"markdown","0b9775b1":"markdown","115e72dd":"markdown","94b69329":"markdown","a05d1953":"markdown","311e1c6d":"markdown","5e26a12b":"markdown","56dbd8fc":"markdown","b67550a0":"markdown","dae86feb":"markdown","b7ce6f00":"markdown","7ad05955":"markdown","440743fc":"markdown","d1bd3d60":"markdown","261f79d9":"markdown","bc229348":"markdown","acf7a1cd":"markdown","d70a8334":"markdown","80c4ddc7":"markdown"},"source":{"a9a35af7":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfrom scipy.stats import boxcox, skew\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","9a3b32c4":"df = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\ndf.head()","5e8503e0":"df.info()","fb5e75bf":"df.isna().sum()","7a2f73c8":"df.describe()","e5804dc5":"df.dropna(inplace=True)\ndf.isna().sum()","06ca05b6":"sns.countplot(x=\"Potability\", data=df, palette=\"Set2\")\nplt.show()","967d30f1":"labels = df[\"Potability\"].unique()\nx = df[\"Potability\"].value_counts()\nexplode = (0.1) * labels\n\nfig1, ax1 = plt.subplots()\npatches, texts, autotexts = ax1.pie(\n                                    x=x, explode=explode,\n                                    labels=labels, autopct=\"%1.2f%%\",\n                                    shadow=True, startangle=45\n                                   )\nax1.axis('equal')\n\nplt.legend(patches, labels, fontsize=\"xx-large\")\nplt.title(\"Potability\", color=\"white\", size=20)\nplt.setp(texts, color='white', fontsize=15)\nplt.setp(autotexts, color=\"black\", size=12)\n\nplt.tight_layout()\nplt.show()","21533f93":"for col in df.columns[:-1]:\n    sns.catplot(x=\"Potability\", y=col, data=df, kind=\"box\")","797169ab":"cor = df.drop(\"Potability\", axis=1).corr()\nplt.figure(figsize=(10, 7))\nsns.heatmap(cor, annot=True, linewidth=.6, linecolor=\"black\")\nplt.show()","8773f27f":"sns.pairplot(data=df, hue=\"Potability\", palette=\"Set2\")\nplt.show()","49ff3fbb":"for x in df.columns[:-1]:\n    print(f\"{x} : {skew(df[x])}\")","ef21a7fe":"# from sklearn.preprocessing import FunctionTransformer\n\n# transformer = FunctionTransformer(np.log10, validate=True)\n# for x in df.columns[:-1]:\n#     df[x] = transformer.transform(df[[x]])","4aea3bcb":"for x in df.columns[:-1]:\n    df[x], _ = boxcox(df[x])","82355814":"for x in df.columns[:-1]:\n    print(f\"{x} : {skew(df[x])}\")","706f392e":"sns.pairplot(data=df, hue=\"Potability\", palette=\"Set2\")\nplt.show()","02e6f187":"X = df.drop(\"Potability\", axis=1)\ny = df[\"Potability\"]","5b0bc18c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","384cb7e3":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","63e452db":"def score(y_test=y_test, y_pred=None):\n    \"\"\"Helper function for evaluation metrics.\"\"\"\n    acc = round(accuracy_score(y_test, y_pred), 2) * 100\n    mae = round(mean_absolute_error(y_test, y_pred), 2)\n    print(f\"Accuracy: {acc:.2f}%, MAE: {mae}\")\n    \n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, linewidth=0.8, linecolor=\"black\", fmt='g')\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Truth\")\n    plt.show()\n    \n    return acc","4ec96490":"# Creating a ndarray to store model's accuracy\naccuracy_scores = np.zeros(8, dtype=\"float64\")","c414ebfd":"clf1 = LogisticRegression(solver=\"newton-cg\", random_state=42).fit(X_train, y_train)\ny_pred1 = clf1.predict(X_test)\naccuracy_scores[0] = score(y_pred=y_pred1)","b5640ecc":"clf2 = RandomForestClassifier(n_estimators=15, random_state=42).fit(X_train, y_train)\ny_pred2 = clf2.predict(X_test)\naccuracy_scores[1] = score(y_pred=y_pred2)","31b556a7":"clf3 = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_depth=6, random_state=42).fit(X_train, y_train)\ny_pred3 = clf3.predict(X_test)\naccuracy_scores[2] = score(y_pred=y_pred3)","99c68958":"clf4 = SVC(random_state=42).fit(X_train, y_train)\ny_pred4 = clf4.predict(X_test)\naccuracy_scores[3] = score(y_pred=y_pred4)","0b12943f":"clf5 = GaussianNB().fit(X_train, y_train)\ny_pred5 = clf5.predict(X_test)\naccuracy_scores[4] = score(y_pred=y_pred5)","f660a1df":"clf6 = SGDClassifier(random_state=42).fit(X_train, y_train)\ny_pred6 = clf6.predict(X_test)\naccuracy_scores[5] = score(y_pred=y_pred6)","837ee0f9":"err_rate = []\nfor i in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    err_rate.append(np.mean(y_pred != y_test))\nk_index = err_rate.index(min(err_rate))\nmin_err = min(err_rate)\nprint(f\"Minimum error of {min_err} at K = {k_index}.\")","e2d1120a":"clf7 = KNeighborsClassifier(n_neighbors=12).fit(X_train, y_train)\ny_pred7 = clf7.predict(X_test)\naccuracy_scores[6] = score(y_pred=y_pred7)","0150132f":"clf8 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=9, random_state=42).fit(X_train, y_train)\ny_pred8 = clf8.predict(X_test)\naccuracy_scores[7] = score(y_pred=y_pred8)","4bbaca41":"models = [\n          \"Logistic Regression\", \"Random Forest Classifier\", \"Decision Tree Classifier\",\n          \"Support Vector Classifier\", \"Naive Bayes\", \"Stochastic Gradient Descent\",\n          \"K-Nearest Neighbours\", \"Gradient Boosting Classifier\",\n         ]\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=models, y=accuracy_scores)\n\nplt.xlabel(\"Model Name\")\nplt.xticks(rotation = -90)\nplt.ylabel(\"Accuracy\")\n\nplt.show()","21f2151a":"## Naive Bayes","86a26e60":"# Fixing Skewed Data","c7d9e0d7":"# Conclusion","0e0cdff8":"## Logistic Regression","0b9775b1":"## Creating features (X) and label (y)\n\nFeatures are often referred to as \"independent variables\" and Label is often referred to as \"dependent variable\".\n\nHere `Potability` is our label because it depends on other features.","115e72dd":"## Stochastic Gradient Descent","94b69329":"## Decision Tree Classifier","a05d1953":"# Loading the data","311e1c6d":"## Gradient Boosting Classifier","5e26a12b":"# Dropping null values","56dbd8fc":"## Log Transformation","b67550a0":"## Random Forest Classifier","dae86feb":"By looking at the above plot, it looks like that our data is skewed.","b7ce6f00":"# Importing the libraries","7ad05955":"# Visualizing the data","440743fc":"# Exploring the data\n\nKnowledge about the data you are working on is very important for data analysis.\n\nWhat I have used:\n\n- [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html)\n\n- [pandas.DataFrame.isna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.isna.html)\n\n- [pandas.DataFrame.describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html)","d1bd3d60":"## Power Transformation","261f79d9":"## K-Nearest Neighbours","bc229348":"# Building the models","acf7a1cd":"## Splitting the data into training and testing set\n\n- Training data set is used for fitting our model to learn the patterns.\n- Testing data set is used for prediction and unbiased evaluation of our final model\n\nWe can do this by using [sklearn.model_selection.train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html).\n\nTraining data set - 80% of the total data\n\nTesting data set - 20% of the total data","d70a8334":"## Support Vector Classifier","80c4ddc7":"## Scaling the data\n\nScaling means transforming the data so that it fits within a specific scale. We can do this by using [sklearn.preprocessing.StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)\n\nStandardScaler is useful for the features that follow a __Normal Distribution__. Previously when we fixed the skewed data we got something similar to a normal distribution."}}