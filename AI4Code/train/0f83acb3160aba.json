{"cell_type":{"a545cad1":"code","c24b97ed":"code","da7c2590":"code","0fb309bf":"code","bbc74ca9":"code","b75085d1":"code","566a7039":"code","f48fcbde":"code","d4ee9094":"code","d3c1e2c3":"code","43e16964":"code","031cae9d":"code","6c88cacc":"code","508373ab":"code","0b396434":"code","348da144":"code","8efcda4a":"code","111e6038":"code","a4df2f6f":"code","5d5d08f5":"code","c8ecb4f5":"code","5e2eb672":"code","aa95d390":"code","538eedf6":"code","ca129116":"code","16985624":"code","34300fa0":"code","cd9e69d0":"code","fe01bde0":"code","1bcf7910":"code","14bc32e7":"code","26374702":"code","7b7d90e2":"code","12cec86a":"code","1c4cce14":"code","90680d2e":"code","70ebcfdd":"code","9f0fa744":"code","d440b403":"code","f7321bc9":"code","7f7bc8a2":"code","969c2bfb":"code","005f084b":"code","600a4c40":"code","34c3bc17":"code","05b81463":"code","95b62d2e":"code","7fc678d1":"code","fb7dfa60":"code","60df0739":"code","b92aac1f":"code","a948a379":"code","d04b63bc":"code","a9bd64e2":"code","7d3771cf":"code","90d36390":"code","27d903a5":"code","ee952087":"code","1aa84db3":"code","bde79d01":"code","64675a0c":"code","d350b90e":"code","cf319872":"code","694feac2":"code","ef0a2996":"code","7b78275a":"code","72345a84":"code","cc691215":"code","5658433d":"code","6bf99339":"code","48420912":"code","a2522696":"code","08d1e5fa":"code","915ecc70":"code","da7161f3":"markdown","0eddd89d":"markdown","e7987cda":"markdown","0614f4c2":"markdown","57372568":"markdown","a0237ac3":"markdown","55d6d219":"markdown","fddebe97":"markdown","0f655e10":"markdown","2d3af99a":"markdown","c6715df4":"markdown","ca27aa53":"markdown","9eda0057":"markdown","e2cb4a4a":"markdown","9534af9b":"markdown"},"source":{"a545cad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c24b97ed":"import requests\nfrom bs4 import BeautifulSoup","da7c2590":"r = requests.get(\"https:\/\/www.agriculture.com\/\")","0fb309bf":"soup = BeautifulSoup(r.content, 'html.parser')","bbc74ca9":"# Retrieve all popular news links (Fig. 1)\nlink = []\nfor i in soup.find('body').find_all('a'):\n    if (i['href'][0]) == 'h':\n        link.append(i['href'])","b75085d1":"# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\ndocuments = []\nfor i in link:\n    # Make a request to the link\n    r = requests.get(i)\n    # Initialize BeautifulSoup object to parse the content \n    soup = BeautifulSoup(r.content, 'html.parser')\n  \n    # Retrieve all paragraphs and combine it as one\n    sen = []\n    for i in soup.find_all('a'):\n        sen.append(i.get_text())\n  \n    # Add the combined paragraphs to documents\n    documents.append(' '.join(sen))","566a7039":"documents[0]","f48fcbde":"d = {'links':link,'content':documents}","d4ee9094":"documents_df = pd.DataFrame(d)","d3c1e2c3":"documents_df.head()","43e16964":"print(\"Number of documents =\",documents_df.shape[0])","031cae9d":"# Abbreviations data created manually for better cleaning\nabb_df = pd.read_csv(\"..\/input\/abbreviation\/abbreviations_data.csv\")\n# Similarly stop words data created manually\nstop_df = pd.read_excel(\"..\/input\/my-stopwords\/my_stopwords.xlsx\") ","6c88cacc":"# Converting Stopwords of type Dataframe into list type\nS = stop_df['stopwords'].to_list()\n# set is used for extracting the unqiue words (since it is created manually)\nStopWords = list(set(S))","508373ab":"# Data created for Replacing the contractions\ncontractions_dict = {\n    \"what's\":\"what is\",\n    \"what're\":\"what are\",\n    \"who's\":\"who is\",\n    \"who're\":\"who are\",\n    \"where's\":\"where is\",\n    \"where're\":\"where are\",\n    \"when's\":\"when is\",\n    \"when're\":\"when are\",\n    \"how's\":\"how is\",\n    \"how're\":\"how are\",\n\n    \"i'm\":\"i am\",\n    \"we're\":\"we are\",\n    \"you're\":\"you are\",\n    \"they're\":\"they are\",\n    \"it's\":\"it is\",\n    \"he's\":\"he is\",\n    \"she's\":\"she is\",\n    \"that's\":\"that is\",\n    \"there's\":\"there is\",\n    \"there're\":\"there are\",\n\n    \"i've\":\"i have\",\n    \"we've\":\"we have\",\n    \"you've\":\"you have\",\n    \"they've\":\"they have\",\n    \"who've\":\"who have\",\n    \"would've\":\"would have\",\n    \"not've\":\"not have\",\n\n    \"i'll\":\"i will\",\n    \"we'll\":\"we will\",\n    \"you'll\":\"you will\",\n    \"he'll\":\"he will\",\n    \"she'll\":\"she will\",\n    \"it'll\":\"it will\",\n    \"they'll\":\"they will\",\n\n    \"isn't\":\"is not\",\n    \"wasn't\":\"was not\",\n    \"aren't\":\"are not\",\n    \"weren't\":\"were not\",\n    \"can't\":\"can not\",\n    \"couldn't\":\"could not\",\n    \"don't\":\"do not\",\n    \"didn't\":\"did not\",\n    \"shouldn't\":\"should not\",\n    \"wouldn't\":\"would not\",\n    \"doesn't\":\"does not\",\n    \"haven't\":\"have not\",\n    \"hasn't\":\"has not\",\n    \"hadn't\":\"had not\",\n    \"won't\":\"will not\"\n}","0b396434":"def DataCleaning(documents_df, StopWords, contractions_dict):\n    # converting strings to lowercase\n    documents_df.content.replace(to_replace='[^a-zA-Z]', value = \" \", inplace=True, regex=True) \n    \n    # removing words having len <=2\n    documents_df.content.replace(to_replace=r'\\b\\w{1,3}\\b', value = \"\", inplace=True, regex=True) \n    \n    # Remove punctuations\n    documents_df['content'] = documents_df['content'].str.replace('[^\\w\\s]','')\n    \n    # Removing Stopwords\n    documents_df['content'] = documents_df['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in StopWords))\n    \n    # Replacing contractions \n    documents_df.content.replace(contractions_dict, regex=True,inplace=True)\n    \n    return documents_df","348da144":"documents_df = DataCleaning(documents_df, StopWords, contractions_dict)","8efcda4a":"documents_df.head()","111e6038":"from nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() ","a4df2f6f":"# Doing lemmatization only for adjectives, verbs and adverbs, not for nouns\n# it takes some time to run \ndef NextLevelCleaning(df):\n    for i in range(df.shape[0]):\n        words = []\n        # Tagging each word with their grammar meaning\n        doc = df['content'][i].split()\n        for token in doc:\n            words.append(lemmatizer.lemmatize(token))\n        df.iloc[i,1] = ' '.join(words)\n    return df","5d5d08f5":"documents_df = NextLevelCleaning(documents_df)","c8ecb4f5":"documents_df.head()","5e2eb672":"corpus = []\nfor i in range(documents_df.shape[0]):\n    for j in documents_df['content'][i].split():\n        if j.lower() not in corpus:\n            corpus.append(j.lower())","aa95d390":"len(corpus)","538eedf6":"TD = {}\n\nfor word in corpus:\n    positions = {}\n    doc = []\n    for i in range(documents_df.shape[0]):\n        if word in documents_df['content'][i].split():\n            positions[i+1] = (list(np.where(np.array(documents_df['content'][i].split()) == word)[0]))\n    doc.append(len(positions))\n    doc.append(positions)\n    TD[word] = doc","ca129116":"TD['agriculture']","16985624":"query = \"agriculture in India\"\nquery = [\" \".join(query.split())]","34300fa0":"query = pd.DataFrame(query,columns=['content'])","cd9e69d0":"query = DataCleaning(query, StopWords, contractions_dict)","fe01bde0":"print(\"Cleaned Query:\",query['content'][0])","1bcf7910":"documents_df.columns","14bc32e7":"for word in query['content'][0].split():\n    if word in TD.keys():\n        temp = []\n        for i in (sorted(TD[word][1].items(), key=lambda x: len(x[1]), reverse=True)):\n            temp.append(documents_df['links'][i[0]+1])\n    print(\"Results for\",word)\n    for l in temp:\n        print(l)","26374702":"!pip install rank_bm25 nltk","7b7d90e2":"from rank_bm25 import BM25Okapi\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nimport re","12cec86a":"documents_df['_id'] = list(range(documents_df.shape[0]))","1c4cce14":"all_words_in_doc = []\nfor i in range(documents_df.shape[0]):\n    for j in documents_df['content'][i].split():\n            all_words_in_doc.append(j.lower())","90680d2e":"SEARCH_DISPLAY_COLUMNS = list(documents_df.columns)","70ebcfdd":"class SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()","9f0fa744":"english_stopwords = list(set(stopwords.words('english')))\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens","d440b403":"class WordTokenIndex:\n    \n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        raw_search_str = self.corpus.content.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])","f7321bc9":"class RankBM25Index(WordTokenIndex):\n    \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results['orig_ind'] = ind\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score', 'orig_ind'])","7f7bc8a2":"bm25_index = RankBM25Index(documents_df)","969c2bfb":"results = None\nadded = []\nfor word in corpus:\n    word_result = bm25_index.search(word, n=10).results\n    if results is None:\n        results = word_result\n        added += [r.orig_ind for i, r in word_result.iterrows()]\n        continue\n    for i, r in word_result.iterrows():\n        if r.orig_ind not in added:\n            results = results.append(r)\n            added.append(r.orig_ind)\ndf = results.sort_values(by='Score', ascending=False)\ndf.reset_index(drop=True, inplace=True)","005f084b":"df.head()","600a4c40":"!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\nimport scispacy\nimport spacy\nimport en_core_sci_lg\nnlp = en_core_sci_lg.load()\nnlp.max_length=2000000","34c3bc17":"vector_list = []\nfor i in tqdm(df.index):\n    doc = nlp(df.iloc[i].content)\n    sents = [sent for sent in doc.sents]\n    vecs = [sent.vector for sent in sents]\n    for j in range(len(sents)):\n        vector_list.append(\n            {\"_id\": df.iloc[i]._id, \n             \"score\": df.iloc[i]['Score'],\n             \"sentence\": j, \n             \"vector\": vecs[j], \n             \"start_span\": sents[j].start_char,\n             \"end_span\": sents[j].end_char})\nvector_df = pd.DataFrame(data=vector_list)","05b81463":"queries = \"\"\"Agriculture in India \\n Crops management\"\"\"\nqueries = queries.splitlines()\nqueries_df = pd.DataFrame(data=[{\"query\":query} for query in queries])","95b62d2e":"query_vector_list = []\nfor i in tqdm(range(len(queries))):\n    doc = nlp(queries[i])\n    vec = doc.vector\n    query_vector_list.append({\"_id\": f\"query_{i}\", \"vector\": vec})\n    \nquery_vector_df = pd.DataFrame(data=query_vector_list)\nquery_vector_df.to_csv(\"query_vecs.csv\",index=False)","7fc678d1":"vector_df.head()","fb7dfa60":"from scipy.spatial import distance","60df0739":"RATIO = 0.9\ndistances = distance.cdist([value for value in query_vector_df['vector']], [value for value in vector_df['vector'].values], \"cosine\")\nw2v_searchable_df = vector_df.drop(columns=[\"vector\"])\n# Create a column with cosine distances for each query vs the sentence\nfor i in range(len(queries)):\n    w2v_searchable_df[f\"query_{i}_distance\"] = RATIO * (1 - distances[i]) + (1-RATIO) * w2v_searchable_df['score']","b92aac1f":"retrieved_content = []","a948a379":"all_content = []\nfor i in df['content']:\n    all_content.append(i)","d04b63bc":"for i in range(len(queries)):\n    columnName = f\"query_{i}_distance\"\n    context = w2v_searchable_df.sort_values(by=columnName, ascending=False)[[\"_id\",\"start_span\",\"end_span\"]][:10]\n    ix = context[\"_id\"].to_list()\n    spans1 = context[\"start_span\"].to_list()\n    spans2 = context[\"end_span\"].to_list()\n    if queries[i]:\n        print(\"Question: \" + queries[i] + \"\\n\")\n        for j in range(len(context.index)):\n            score = df[df[\"_id\"] == ix[j]].iloc[0]['Score']\n            print(\"Rank\", j+1, \"Relevance Score:\",score)\n            print(\"Content:\", (df[df[\"_id\"] == ix[j]].iloc[0]['content'])[spans1[j]:spans2[j]],\"\\n\")\n            if (df[df[\"_id\"] == ix[j]].iloc[0]['links'])[spans1[j]:spans2[j]]:\n                print(\"Link:\",(df[df[\"_id\"] == ix[j]].iloc[0]['links'])[spans1[j]:spans2[j]],\"\\n\")\n            retrieved_content.append((df[df[\"_id\"] == ix[j]].iloc[0]['content'])[spans1[j]:spans2[j]])","a9bd64e2":"doc_id = []\nlinks = []","7d3771cf":"for doc in range(len(retrieved_content)):\n    for i in range(len(all_content)):\n        count = 0\n        for w in retrieved_content[doc].split():\n            if w in all_content[i].split():\n                count = count + 1\n        if count == len(retrieved_content[doc].split()):\n            doc_id.append(i)\n            links.append(df['links'][i])\n            break","90d36390":"for query in range(len(queries)):\n    print(\"Links that are retrieved for query\",queries[query])\n    if query == 0:\n        for l in links[0:10]:\n            print(l)\n    else:\n        for l in links[10:]:\n            print(l)","27d903a5":"BM25Ranking_df = df.copy()","ee952087":"relevance_class = []","1aa84db3":"unique_doc_ids = list(set(doc_id))\ndoc_id_score = {}","bde79d01":"for doc in unique_doc_ids:\n    doc_id_score[doc] = doc_id.count(doc)","64675a0c":"for doc in df['_id']:\n    # To check whether a document is relevant\n    if doc in list(doc_id_score.keys()):\n        # To check whether a document is highly relevant\n        if doc_id_score[doc] > 5:\n            relevance_class.append(3.0)\n        # Document is relevant\n        else:\n            relevance_class.append(2.0)\n    # Document is not relevant\n    else:\n        relevance_class.append(1.0)","d350b90e":"BM25Ranking_df['Relevance_Class'] = relevance_class","cf319872":"Ideal_Ranking_df = df.copy()","694feac2":"queries_df.columns = ['content']","ef0a2996":"query = DataCleaning(queries_df, StopWords, contractions_dict)\nprint(\"Cleaned Query:\")\nprint(query)","7b78275a":"relevant_doc_ids = []\nrelevance_class = []","72345a84":"for i in range(query['content'].shape[0]):\n    for word in query['content'][i].split():\n        count = 0\n        if word in TD.keys():\n            for i in (sorted(TD[word][1].items(), key=lambda x: len(x[1]), reverse=True)):\n                relevant_doc_ids.append(i[0])","cc691215":"for doc in df['_id']:\n    # To check whether the document is relevant\n    if doc in relevant_doc_ids:\n        # To check whether the document is highly relevant\n        if relevant_doc_ids.index(doc) < 5:\n            relevance_class.append(3.0)\n        # document is relevant\n        else:\n            relevance_class.append(2.0)\n    # Document is not relevant\n    else:\n        relevance_class.append(1.0)","5658433d":"Ideal_Ranking_df['Relevance_Class'] = relevance_class","6bf99339":"relevance_id_score = {}\nfor doc in range(Ideal_Ranking_df.shape[0]):\n    relevance_id_score[Ideal_Ranking_df['_id'][doc]] = Ideal_Ranking_df['Relevance_Class'][doc]","48420912":"sorted_x = sorted(relevance_id_score.items(), key=lambda k: k[1], reverse=True)","a2522696":"import collections\n\nrelevance_id_score = collections.OrderedDict(sorted_x)","08d1e5fa":"from sklearn.metrics import ndcg_score","915ecc70":"ndcg_score(np.asarray([list(relevance_id_score.values())]), np.asarray([BM25Ranking_df['Relevance_Class'].tolist()]))","da7161f3":"<font size=\"10\">Phase 3<\/font>","0eddd89d":"Actual Ranking","e7987cda":"Data Cleaning","0614f4c2":"<font size=\"5\">Evaluation of my ranking model<\/font>","57372568":"![BM25_fromula.PNG](attachment:BM25_fromula.PNG)","a0237ac3":"- N \u2014 Size of the Collection of documents\n- ni \u2014 Number of documents in the collection containing query term ti\n- R \u2014 Relevant set size (i.e., number of documents judged relevant in collection)\n- ri \u2014 Number of judged relevant documents containing ti\n- fi \u2014 The frequency of term ti in the document\n- qfi \u2014 The frequency of term ti in the query\n- k1 \u2014 Determines how the tf(term frequency)component of the term weight changes as fi increases.\n- k2 \u2014 Determines how the tf (term frequency)component of the term weight changes as qfi increases\n- K = k1((1-b) + b. dl\/avgdl),where b is a parameter, dl is the length of the document, and avgdl is the average length of a document in the collection","55d6d219":"Precited Ranking","fddebe97":"![flow_of_work_IR.PNG](attachment:flow_of_work_IR.PNG)","0f655e10":"Ranking metric yields a high value if true labels are ranked high by predicted","2d3af99a":"Indexing with positions","c6715df4":"As, our value is close to 1, Our ranking model is better","ca27aa53":"<font size=\"10\">Phase I<\/font>","9eda0057":"<font size=\"10\">Phase 2<\/font>","e2cb4a4a":"- 1 - Not Relevant\n- 2 - Relevant\n- 3 - Highly Relevant","9534af9b":"<font size=\"5\">Conclusion<\/font>"}}