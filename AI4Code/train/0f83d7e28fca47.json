{"cell_type":{"db1a736b":"code","ef5f4b3d":"code","38a4721c":"code","1f550253":"code","3c2d5d63":"code","7164b910":"code","ba07cf39":"code","95bbc98a":"code","ea9e45b7":"code","fdeeed9b":"code","d25e6118":"code","94217fde":"code","94a751e9":"code","959ed6cd":"code","ce3a784e":"code","7da7cf3f":"code","2eb8c16e":"code","b0a6ca02":"code","3ebd3de8":"code","45d45532":"code","70873b56":"code","11d10987":"code","7ea63f0a":"code","35b210d2":"code","52aae318":"code","cc9eea58":"code","f240aa67":"code","0072558a":"code","b2e0f47b":"code","94e0a081":"code","a74686bd":"code","93bc102e":"code","14b0e1fb":"code","4091d81f":"code","e81e51e6":"code","f8be8f0c":"code","8c582e45":"code","34a052d8":"code","1ffeac5b":"code","6bd45155":"code","c313f51b":"code","39a068bb":"code","269d5934":"code","21a1fa1e":"markdown","5fee485d":"markdown","a9fbdc44":"markdown","ea656432":"markdown","0a6a9051":"markdown","75db0875":"markdown","0c331cbd":"markdown"},"source":{"db1a736b":"##### Base-model paths #####\n## Main config based on hiromu's config\nMY_CONFIG_PATH = '..\/input\/base-pre-cv072469\/base_pre_cv072469\/base_pre_cv072469_config_20200612_150232.pkl'\n\n\n## model_path & config of each models\nMODEL1_FOLDER = \"..\/input\/large-cv0726033\/RoBERTa-large-0726033 \/\"\nMODEL1_CONFIG = \"config_20200611_000411.pkl\"\n\nMODEL2_FOLDER = \"..\/input\/base-cv0722436\/RoBERTa-base-0722436\/\"\nMODEL2_CONFIG = \"config_20200610_230921.pkl\"\n\nMODEL3_FOLDER = \"..\/input\/large-pre-cv072592\/large_pre_cv072592\/\"\nMODEL3_CONFIG = \"large_pre_cv072592_config_20200612_112512.pkl\"\n\nMODEL4_FOLDER = \"..\/input\/base-pre-cv072469\/base_pre_cv072469\/\"\nMODEL4_CONFIG = \"base_pre_cv072469_config_20200612_150232.pkl\"\n\nMODEL5_FOLDER = \"..\/input\/ooka-large-seed101\/\"\nMODEL5_CONFIG = \"config_20200614_233735.pkl\"\n\nMODEL6_FOLDER = \"..\/input\/ooka-base-seed101\/ooka_base_seed101\/\"\nMODEL6_CONFIG = \"config_20200612_183046.pkl\"\n\nMODEL7_FOLDER = \"..\/input\/hiromu-large-seed101\/\"\nMODEL7_CONFIG = \"config_20200614_134559.pkl\"\n\nMODEL8_FOLDER = \"..\/input\/hiromu-base-seed101\/hiromu_base_seed101\/\"\nMODEL8_CONFIG = \"config_20200614_124718.pkl\"\n\n\n##### Reranking-model path #####\n## RoBERTa path\nROBERTA_PATH = \"..\/input\/roberta-base\"\n\n## model_path\nREMODEL0 = \"..\/input\/hiromu-reranking-cv07346\/hiromu_reranking_cv0.7346\/reranking_model_fold0.bin\"\nREMODEL1 = \"..\/input\/hiromu-reranking-cv07346\/hiromu_reranking_cv0.7346\/reranking_model_fold1.bin\"\nREMODEL2 = \"..\/input\/hiromu-reranking-cv07346\/hiromu_reranking_cv0.7346\/reranking_model_fold2.bin\"\nREMODEL3 = \"..\/input\/hiromu-reranking-cv07346\/hiromu_reranking_cv0.7346\/reranking_model_fold3.bin\"\nREMODEL4 = \"..\/input\/hiromu-reranking-cv07346\/hiromu_reranking_cv0.7346\/reranking_model_fold4.bin\"\n\nREMODEL5 = \"..\/input\/oof-bucketing-fold0-3\/reranking_model_fold0.bin\"\nREMODEL6 = \"..\/input\/oof-bucketing-fold0-3\/reranking_model_fold1.bin\"\nREMODEL7 = \"..\/input\/oof-bucketing-fold0-3\/reranking_model_fold2.bin\"\nREMODEL8 = \"..\/input\/oof-bucketing-fold0-3\/reranking_model_fold3.bin\"\nREMODEL9 = \"..\/input\/oof-bucketing-fold4\/reranking_model_fold4.bin\"","ef5f4b3d":"!pip uninstall transformers -y","38a4721c":"!pip uninstall tokenizers -y","1f550253":"!mkdir -p \/tmp\/pip\/cache-transformers\/\n!cp ..\/input\/transformers280\/transformers-2.8.0-py3-none-any.whl \/tmp\/pip\/cache-transformers\/","3c2d5d63":"!mkdir -p \/tmp\/pip\/cache-tokenizers\/\n!cp ..\/input\/transformers280\/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl \/tmp\/pip\/cache-tokenizers\/","7164b910":"!pip install --no-index --find-links \/tmp\/pip\/cache-tokenizers\/ tokenizers","ba07cf39":"!pip install --no-index --find-links \/tmp\/pip\/cache-transformers\/ transformers","95bbc98a":"import os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))+\"\/input\/tsepipeline\/TSE-kaggle\/input\/src\")\n\nfrom utils.helper import seed_everything\nfrom config.roberta_config import INPUT_DIR, OUTPUT_DIR, ORI_DATA_DIR\nfrom processing.preprocess import preprocess_df\nfrom train.roberta_train import train\n\nimport numpy as np\nimport pandas as pd\npd.set_option(\"display.max_rows\", 1000)\npd.set_option(\"display.max_colwidth\", 1000)","ea9e45b7":"SENTIMENT_COMB = {\n    'pos':['positive'],\n    'neg':['negative'],\n    'neu':['neutral'],\n    'png':['positive','negative'],\n    'pnu':['neutral','positive'],\n    'nng':['neutral','negative'],\n    'pnn':['positive','negative','neutral'],\n}","fdeeed9b":"import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n\nfrom utils.helper import seed_everything, read_pickle, Logger\nfrom train.roberta_train import DEVICE\nfrom train.roberta_train import TweetDataset, TweetCollate, RoBERTaBaseUncased\nfrom train.roberta_train import calc_jaccard, create_valid, opt_th\nfrom train.roberta_train import jaccard_apply\nfrom train.roberta_train import valid, valid_total, ensemble, valid_02, valid_ensemble, create_test_candidates, create_test_candidates_ensemble","d25e6118":"# Setting all seeds 100\nseed_everything(100)","94217fde":"# Loading main config\nMY_CONFIG = read_pickle(MY_CONFIG_PATH)","94a751e9":"# STR_NOW = '2020611_000411'\nCONFIG1 = read_pickle(MODEL1_FOLDER+MODEL1_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL1_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath1 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG1)","959ed6cd":"filepath1","ce3a784e":"# STR_NOW = '20200610_230921'\nCONFIG2 = read_pickle(MODEL2_FOLDER+MODEL2_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL2_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath2 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG2)","7da7cf3f":"filepath2","2eb8c16e":"# STR_NOW = '20200612_112512'\nCONFIG3 = read_pickle(MODEL3_FOLDER+MODEL3_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL3_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[7]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[6])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath3 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG3)","b0a6ca02":"filepath3","3ebd3de8":"# STR_NOW = '20200612_150232'\nCONFIG4 = read_pickle(MODEL4_FOLDER+MODEL4_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL4_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[7]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[6])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath4 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG4)","45d45532":"filepath4","70873b56":"# STR_NOW = '2020611_000411'\nCONFIG5 = read_pickle(MODEL5_FOLDER+MODEL5_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL5_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath5 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG5)","11d10987":"filepath5","7ea63f0a":"# STR_NOW = '2020611_000411'\nCONFIG6 = read_pickle(MODEL6_FOLDER+MODEL6_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL6_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath6 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG6)","35b210d2":"filepath6","52aae318":"# STR_NOW = '2020611_000411'\nCONFIG7 = read_pickle(MODEL7_FOLDER+MODEL7_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL7_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath7 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG7)","cc9eea58":"filepath7","f240aa67":"# STR_NOW = '2020611_000411'\nCONFIG8 = read_pickle(MODEL8_FOLDER+MODEL8_CONFIG)\ntmp = pd.DataFrame([[_p] for _p in glob.glob(MODEL8_FOLDER+'*') if _p[-4:]=='.pth'], columns=['filepath'])\ntmp['model']      = tmp['filepath'].apply(lambda x: x.split('\/')[-1])\ntmp['fold']       = tmp['model'].apply(lambda x: int(x.split('_')[4]))\ntmp['sentiment']  = tmp['model'].apply(lambda x: x.split('_')[3])\ntmp.sort_values('fold', inplace=True)\ntmp.reset_index(drop=True, inplace=True)\nfilepath8 = tmp.loc[tmp['sentiment']=='pnn', 'filepath'].tolist()\n\n# print(f'\\n\\n--- [START {STR_NOW}] {\"-\"*64}\\n')\nprint(CONFIG8)","0072558a":"filepath8","b2e0f47b":"filepath1.extend(filepath5)\nfilepath2.extend(filepath6)\nfilepath3.extend(filepath7)\nfilepath4.extend(filepath8)","94e0a081":"# Filling the gap between hiromu's config and Y.O.'s config\nfor item in set(MY_CONFIG.keys()) - set(CONFIG1.keys()):\n    CONFIG1[item] = MY_CONFIG[item]\n    CONFIG2[item] = MY_CONFIG[item]","a74686bd":"# We don't use this JAC_TH, so any value is ok\nJAC_TH  = 0.46\n\n# Loading test data\n_, test_df = preprocess_df(ORI_DATA_DIR, use_sentiment=SENTIMENT_COMB[CONFIG1['USE_SENTIMENT']], show_head=False)\n# Creating candidates from ensemble result\npred_ensemble = create_test_candidates_ensemble(test_df, filepath1, filepath2, filepath3, filepath4, CONFIG1, CONFIG2, CONFIG3, CONFIG4, JAC_TH=JAC_TH, ratio=[0.4, 0.1, 0.3, 0.2, 0.4, 0.1, 0.3, 0.2], en_ratio=[0.5, 0.5], num=2)","93bc102e":"import numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nimport re\nfrom transformers import get_linear_schedule_with_warmup\n\nseed_everything(100)\n\n\nMAX_LEN = 256\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\",\n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\ncand_nums = 5\nval_nums = 5","14b0e1fb":"class RerankingDataset:\n    def __init__(self, tweet, sentiment, selected_text, jaccard, score):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        self.jaccard = jaccard\n        self.score = score\n    \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self, item):\n        tweet            = self.tweet[item]\n        selected_text    = self.selected_text[item]\n        sentiment        = self.sentiment[item]\n        jaccard          = self.jaccard[item]\n        score            = self.score[item]\n\n        # selected_text means candidate here\n        new_tweet = str(tweet) + \" <\/s> <\/s> \" + str(selected_text)\n        tok_tweet = TOKENIZER.encode(new_tweet)\n        input_ids_orig = tok_tweet.ids\n        \n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n        \n        input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        \n        return {\n            'orig_tweet'        : tweet,\n            'sentiment'         : sentiment,\n            'orig_selected'     : selected_text,\n            'jaccard'           : jaccard,\n            'score'             : score,\n            'ids'               : input_ids,\n            'mask'              : mask,\n            'token_type_ids'    : token_type_ids\n        }\n\n# SequenceBucketing\nclass RerankingCollate:\n    def __init__(self):\n        self.CONFIG = {}\n        self.CONFIG['BUCKET'] = True\n        self.CONFIG['MAX_LEN'] = MAX_LEN\n\n    def __call__(self, batch):\n        out = {\n                'orig_tweet'        : [],\n                'sentiment'         : [],\n                'orig_selected'     : [],\n                'jaccard'           : [],\n                'score'             : [],\n                'ids'               : [], # torch.long\u306b\u578b\u5909\u63db\n                'mask'              : [], # torch.long\u306b\u578b\u5909\u63db\n                'token_type_ids'    : [], # torch.long\u306b\u578b\u5909\u63db\n            }\n\n        for i in range(len(batch)):\n            for k, v in batch[i].items():\n                out[k].append(v)\n\n        # Deciding the number of padding\n        if self.CONFIG['BUCKET']:\n            max_pad = 0\n            for p in out['ids']:\n                if len(p)>max_pad:\n                    max_pad = len(p)\n        else:\n            max_pad = self.CONFIG['MAX_LEN']\n            \n        # Padding\n        for i in range(len(batch)):\n            tokenized_text = out['ids'][i]\n            token_type_ids = out['token_type_ids'][i]\n            mask           = out['mask'][i]\n            text_len       = len(tokenized_text)\n\n            out['ids'][i]            = (tokenized_text + [1]    *(max_pad - text_len))[:max_pad]\n            out['token_type_ids'][i] = (token_type_ids + [0]    *(max_pad - text_len))[:max_pad]\n            out['mask'][i]           = (mask           + [0]    *(max_pad - text_len))[:max_pad]\n\n        # torch.float\n        out['jaccard']        = torch.tensor(out['jaccard'], dtype=torch.float)\n        out['score']          = torch.tensor(out['score'], dtype=torch.float)\n        # torch.long\n        out['ids'] = torch.tensor(out['ids'], dtype=torch.long)\n        out['mask']           = torch.tensor(out['mask'], dtype=torch.long)\n        out['token_type_ids'] = torch.tensor(out['token_type_ids'], dtype=torch.long)\n\n        return out","4091d81f":"class myModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(myModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        self.dropout = nn.Dropout(0.1)\n        self.out_proj = nn.Linear(768*2, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        o1, o2, _ = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        cat = torch.cat((apool, mpool), 1)\n\n        bo = self.dropout(cat)\n        p2 = self.out_proj(bo)\n        return p2","e81e51e6":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True\n\nmodel0 = myModel(conf=model_config)\nmodel0.to(device)\nmodel0.load_state_dict(torch.load(REMODEL0))\n\nmodel1 = myModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(REMODEL1))\n\nmodel2 = myModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(REMODEL2))\n\nmodel3 = myModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(REMODEL3))\n\nmodel4 = myModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(REMODEL4))\n\nmodel5 = myModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(REMODEL5))\n\nmodel6 = myModel(conf=model_config)\nmodel6.to(device)\nmodel6.load_state_dict(torch.load(REMODEL6))\n\nmodel7 = myModel(conf=model_config)\nmodel7.to(device)\nmodel7.load_state_dict(torch.load(REMODEL7))\n\nmodel8 = myModel(conf=model_config)\nmodel8.to(device)\nmodel8.load_state_dict(torch.load(REMODEL8))\n\nmodel9 = myModel(conf=model_config)\nmodel9.to(device)\nmodel9.load_state_dict(torch.load(REMODEL9))\nprint('model load')","f8be8f0c":"# Loading test candidates data\ndev = pd.read_csv('\/kaggle\/working\/test_candidate.csv')\ndev","8c582e45":"valid_dataset = RerankingDataset(\n    tweet=dev.tweet.values,\n    sentiment=dev.sentiment.values,\n    selected_text=dev.candidate.values,\n    jaccard=dev.jaccard.values,\n    score=dev.score.values\n)\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=val_nums,\n    collate_fn=RerankingCollate(),\n    num_workers=0,\n)","34a052d8":"def eval(valid_data_loader):\n    model0.eval()\n    model1.eval()\n    model2.eval()\n    model3.eval()\n    model4.eval()\n    model5.eval()\n    model6.eval()\n    model7.eval()\n    model8.eval()\n    model9.eval()\n    all_jaccard = 0\n    all_num = 0\n    rets = []\n    with torch.no_grad():\n        for bi, d in enumerate(valid_data_loader):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            jaccard = d[\"jaccard\"]\n            score = d[\"score\"]\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            logits0 = model0(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits1 = model1(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits2 = model2(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits3 = model3(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits4 = model4(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits5 = model5(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits6 = model6(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits7 = model7(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits8 = model8(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            logits9 = model9(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids,\n            )\n            maxind = 0\n            maxjac = (logits0[0][0] + logits1[0][0] + logits2[0][0] + logits3[0][0] + logits4[0][0] + logits5[0][0] + logits6[0][0] + logits7[0][0] + logits8[0][0] + logits9[0][0]) \/ 10 + score[0] * 0.5\n            for i in range(1, val_nums):\n                tmpjac = (logits0[i][0] + logits1[i][0] + logits2[i][0] + logits3[i][0] + logits4[i][0] + logits5[i][0] + logits6[i][0] + logits7[i][0] + logits8[i][0] + logits9[i][0]) \/ 10 + score[i] * 0.5\n                # Choosing best candidate here\n                if maxjac < tmpjac:\n                    maxind = i\n                    maxjac = tmpjac\n            rets.append(orig_selected[maxind])\n    return rets","1ffeac5b":"cand = eval(valid_data_loader)","6bd45155":"import re\ndef pp(filtered_output, real_tweet):\n    filtered_output = ' '.join(filtered_output.split())\n    if len(real_tweet.split()) < 2:\n        filtered_output = real_tweet\n    else:\n        if len(filtered_output.split()) == 1:\n            if filtered_output.endswith(\"..\"):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n                return filtered_output\n            if filtered_output.endswith('!!'):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n                return filtered_output\n\n        # Start with \" \"\n        if real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = ' '.join(real_tweet.split())\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n\n        # Not start with \" \", but contain \" {2,}\"\n        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n    return filtered_output","c313f51b":"final_output = []\ntest = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ntweet = list(test.text.values)\nfor c, t in zip(cand, tweet):\n    final_output.append(pp(c, t))","39a068bb":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample","269d5934":"sample.to_csv('submission.csv', index=False)","21a1fa1e":"# Post-processing","5fee485d":"## Loading Reranking-models","a9fbdc44":"# transformers==2.8.0 & tokenizers==0.5.2","ea656432":"# File Paths","0a6a9051":"# Inference of Base-model & Create candidates for test data","75db0875":"# Inference of Reranking-model","0c331cbd":"## Loading Base-models"}}