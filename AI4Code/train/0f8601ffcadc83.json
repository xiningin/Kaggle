{"cell_type":{"36e43d4e":"code","5c20d77d":"code","c81dcdb9":"code","20a8142a":"code","d108e55b":"code","9f773441":"code","c500ff2a":"code","109f8300":"code","a2aaf9c2":"code","2ced38cc":"code","caaad845":"code","e5e25cb5":"code","b97fe885":"code","e804a3ff":"code","12de48dd":"code","8b37ae5e":"code","748193e7":"code","05761598":"code","6ce7795f":"code","00a66d0d":"code","b5cfc108":"code","89b781e2":"code","9272defe":"code","92a834e0":"code","aabe0c38":"code","0aa55513":"code","c0680e36":"code","a25dae8a":"code","31b2ab9c":"code","c8c762cb":"code","5d4b704f":"code","30b2e36b":"code","c3c42bec":"code","bcda82e7":"code","ea587ab4":"code","8da9e943":"code","6414c373":"code","44f200f1":"code","d04780fc":"code","99031f72":"code","61274dec":"code","016f1871":"code","6f29c5b5":"code","ab77ee4d":"code","b90ba536":"code","a40370df":"code","24d35b4c":"code","485fb2cd":"code","1123753b":"code","738c018e":"code","4833ca1f":"code","7aa5d9dc":"code","bbe4fdee":"code","86c003f3":"code","41996b44":"code","efef3ff4":"code","b97c5179":"code","596f4d5c":"code","b88316c9":"code","cc754633":"code","4c45bf08":"code","5974b4a6":"code","1707c735":"code","028b4f41":"code","bb2d12a7":"code","3d596f24":"code","ce6a8bac":"code","6c53ba66":"code","22d0ba8d":"code","58785a0d":"code","f0f1ee3d":"markdown","1c9a1fac":"markdown","185df6f7":"markdown","cf1ca1bb":"markdown","57fce74d":"markdown","f8735c84":"markdown","7da1b01b":"markdown","6dd2deda":"markdown","09d33aa5":"markdown","d0265fca":"markdown","d1318083":"markdown","aeb9ae2c":"markdown","6eec5f01":"markdown","f8ceee06":"markdown","439fc0b4":"markdown","6714f40b":"markdown","d5bac748":"markdown","a23cd882":"markdown","5a20c376":"markdown","5f2cecf5":"markdown","a9081c7e":"markdown","bcfc2bdd":"markdown","49f4c893":"markdown","ad306671":"markdown","a4e1d624":"markdown","17d46e6a":"markdown","f6f4b47f":"markdown","387b06d1":"markdown","c1a5e4f3":"markdown","1552d94a":"markdown","9a39bfe5":"markdown","96b7f4ec":"markdown","1e5df6d4":"markdown","653ef95b":"markdown","d7df892b":"markdown","fd620a3b":"markdown","82caf2b4":"markdown","ae162553":"markdown","34e326fe":"markdown","017dcbcc":"markdown","20b97c3b":"markdown","4e573c69":"markdown","7cd7498c":"markdown","f1e5e8f8":"markdown","02a55e69":"markdown","970338e3":"markdown","5fd3c3c1":"markdown","69d9fa8c":"markdown","8d9b87cc":"markdown","5cee0d14":"markdown","bed5ea68":"markdown","9217abf6":"markdown","7ae9ca94":"markdown","4d2cbad7":"markdown","bd472e69":"markdown","9c981ab5":"markdown","71d5417a":"markdown","0916f379":"markdown","30fc38fb":"markdown"},"source":{"36e43d4e":"# Librer\u00edas b\u00e1sicos\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Librer\u00edas de Sklearn \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn import metrics\n\n# Librer\u00edas adicionales\nfrom boruta import BorutaPy\nimport shap","5c20d77d":"# Seteos globales de warnings\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Opci\u00f3n de Pandas para visualizar todas las columnas\npd.set_option('display.max_columns', None)","c81dcdb9":"# Librer\u00eda con funciones propias\n#import myDS as my","20a8142a":"# Evaluation function\ndef evaluation(y_true, y_pred, y_proba=[], verbose=0):\n    \n    if verbose == 1:\n        # Print Accuracy, Recall, F1 Score, and Precision metrics.\n        print('Evaluation Metrics:')\n        print('Accuracy:   ' + str(metrics.accuracy_score(y_true, y_pred)))\n        print('Recall:     ' + str(metrics.recall_score(y_true, y_pred)))\n        print('F1 Score:   ' + str(metrics.f1_score(y_true, y_pred)))\n        print('Precision:  ' + str(metrics.precision_score(y_true, y_pred)))\n        if y_proba == []:\n            print('ROC_AUC:    ' + str(metrics.roc_auc_score(y_true, y_pred)))\n        else:\n            print('ROC_AUC:    ' + str(metrics.roc_auc_score(y_true, y_proba)))\n            \n        #print('MAE = {}'.format(metrics.mean_absolute_error(y_true, y_pred)))\n        #print('RMSD = {}'.format(np.sqrt(metrics.mean_squared_error(y_true, y_pred))))\n        \n        # Print Confusion Matrix\n        print('\\nConfusion Matrix:')\n        print('   TN   FP   FN   TP')\n        conf_mat = metrics.confusion_matrix(y_true, y_pred)\n        print(conf_mat.ravel())\n        print()\n        conf_mat_df = pd.DataFrame(data=conf_mat,\n                                   index=['Actual Negative: 0', 'Actual Positive: 1'],\n                                   columns=['Predict Negative: 0', 'Predict Positive: 1'])\n        \n        sns.heatmap(conf_mat_df, annot=True, fmt='d', cmap='YlGnBu');\n                \n    Accuracy = metrics.accuracy_score(y_true, y_pred).round(3)\n    Recall = metrics.recall_score(y_true, y_pred).round(3)\n    F1_Score = metrics.f1_score(y_true, y_pred).round(3)\n    Precision = metrics.precision_score(y_true, y_pred).round(3)\n    if y_proba == []:\n        Roc_auc = metrics.roc_auc_score(y_true, y_pred).round(3)        \n    else:\n        Roc_auc = metrics.roc_auc_score(y_true, y_proba).round(3)\n        \n    #MAE = metrics.mean_absolute_error(y_true, y_pred).round(3)\n    #RMSD = metrics.mean_squared_error(y_true, y_pred).round(3)\n    \n    final = []\n    final.append(Accuracy)\n    final.append(Recall)\n    final.append(F1_Score)\n    final.append(Precision)\n    final.append(Roc_auc)\n    #final.append(MAE)\n    #final.append(RMSD)\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    for t in cm[0]:\n        final.append(t)\n    for t in cm[1]:\n        final.append(t)\n    \n    return final","d108e55b":"# Funci\u00f3n para remover outliers, datapd es el array de panda y columna es la columna de la que queremos retirar los outliers.\n# Est\u00e1 medio copiado de la clase 12, con el agregado de la posibilidad de pasarle dos valores extremos, un max y un min como\n# l\u00edmites de descarte de datos, elimina mayores que max y menores que min.\n# Devuelve array con bool de los no outliers\n# Par\u00e1metros\n# datapd: dataset de pandas \n# columna: columna del dataset\n# hab_Q3Q1: Si es False no ejecuta l\u00f3gica de Cuartiles, Si es True la ejecuta\n# max: el valor m\u00e1ximo de la columna\n# min: el valor m\u00ednimo de la columna\n# El max y el min los considera tanto si hab_Q3Q1 es True como False\n\ndef remover_outliers(datapd, columna, hab_Q3Q1=False, max=100000.0, min=0.0):\n    \n    if hab_Q3Q1 == True:\n        q1 = datapd[columna].quantile(0.25)\n        #print('q1:', q1.round(2))\n        q2 = datapd[columna].quantile(0.5)\n        #print('q2:', q2.round(2))\n        q3 = datapd[columna].quantile(0.75)\n        #print('q3:', q3.round(2))\n        iqr = (q3 - q1.round(2)) * 4 #!!! OJO CAMBIADO este valor de 1.5 !!!\n        up_threshold = q3 + iqr\n        low_threshold = q1 - iqr\n        if low_threshold < min:\n            low_threshold = min\n        if up_threshold > max:\n            up_threshold = max\n    if hab_Q3Q1 != True:\n        low_threshold = min\n        up_threshold = max\n    \n    \n    #print('up_threshold:', round(up_threshold,2))\n    #print('low_threshold:', round(low_threshold,2))\n    outlier_mask_up = datapd[columna] > up_threshold\n    outlier_mask_down = datapd[columna] < low_threshold\n    outlier_mask = np.logical_or(outlier_mask_up, outlier_mask_down)\n    not_outliers = np.logical_not(outlier_mask)\n    #data_pd_sin_outliers = datapd[not_outliers]\n    #return data_pd_sin_outliers\n    return not_outliers","9f773441":"# Funci\u00f3n para listar correlaciones > umbral\n# recibe el dataframe\n# devuelve una lista\ndef lista_correlacion(df1, umbral=0.9):\n    final = []\n    flag = 1\n    data_cor1 = df1.corr()\n    for c in data_cor1.columns:\n        for r in data_cor1.index:\n            if r != c:\n                if abs(data_cor1.loc[r, c]) > abs(umbral):\n                    flag = 0\n                    final.append([r, c, data_cor1.loc[r, c].round(3)])\n                    #print(r, '-', c, ' - Corr: ', data_cor1.loc[r, c].round(3))\n    if flag == 1:\n        print('No hay correlaciones altas')\n    return final","c500ff2a":"# Import dataset\ndf = pd.read_csv(\"..\/input\/nasa-asteroids-classification\/nasa.csv\")\n\n# Print the shape of the dataset\nprint(df.shape)\n\n# Select top of the dataset\ndf.head()","109f8300":"df.dtypes","a2aaf9c2":"df.isnull().sum()","2ced38cc":"# No se encontraron duplicados\ndf.duplicated().sum()","caaad845":"# Convierte la columna target de True\/False a 1\/0\ndf['Hazardous'] = df['Hazardous'].apply(lambda x: 1 if x == True else 0)","e5e25cb5":"# Drop de columnas por datos repetidas o indicativas\ndf.drop(columns=['Neo Reference ID',              # ID del Asteroide, \u00fanicamente indicativo\n                 'Name',                          # Nombre del Asteroide, \u00fanicamente indicativo\n                 'Est Dia in KM(min)',            # Columna repetida con otra unidad de medida\n                 'Est Dia in KM(max)',            # Columna repetida con otra unidad de medida\n                 'Est Dia in Miles(min)',         # Columna repetida con otra unidad de medida\n                 'Est Dia in Miles(min)',         # Columna repetida con otra unidad de medida\n                 'Est Dia in Miles(max)',         # Columna repetida con otra unidad de medida\n                 'Est Dia in Feet(min)',          # Columna repetida con otra unidad de medida\n                 'Est Dia in Feet(max)',          # Columna repetida con otra unidad de medida\n                 'Close Approach Date',           # Fecha de la Aproximaci\u00f3n, no es relevante\n                 'Epoch Date Close Approach',     # Fecha de la Aproximaci\u00f3n, no es relevante\n                 'Relative Velocity km per hr',   # Columna repetida con otra unidad de medida\n                 'Miles per hour',                # Columna repetida con otra unidad de medida\n                 'Miss Dist.(Astronomical)',      # Columna repetida con otra unidad de medida\n                 'Miss Dist.(lunar)',             # Columna repetida con otra unidad de medida\n                 'Miss Dist.(miles)',             # Columna repetida con otra unidad de medida\n                 'Orbiting Body',                 # Un \u00fanico valor: 'EARTH'\n                 'Orbit ID',                      # ID interno de la NASA, es s\u00f3lo una referencia\n                 'Orbit Determination Date',      # Fecha de c\u00e1lculo de la \u00f3rbita, no es relevante\n                 'Equinox'                        # Un \u00fanico valor: 'J2000'\n                ], inplace=True)","b97fe885":"df","e804a3ff":"df.info()","12de48dd":"# Esta generacion de X e Y es solo para los gr\u00e1ficos al igual que la limpieza.\nX = df.drop(columns=['Hazardous'])\nY = df.Hazardous\n\ndfp = []\nfor columns in X:\n    dfx = remover_outliers(df, columns, hab_Q3Q1=True, max=100e+12, min=0.0)\n    print('{0:30} -  Cant. de outliers = {1}'.format(columns, df.shape[0] - dfx.sum()))\n    dfp.append(dfx)\n\nlen(dfp[0])","8b37ae5e":"outlier_maskp = np.ones(len(dfp[0]))\nfor t in dfp:\n    outlier_maskp = np.logical_and(outlier_maskp, t)\n    \noutlier_maskp.sum()","748193e7":"dfx_graph = X[outlier_maskp].copy()\ndfy_graph = Y[outlier_maskp].copy()","05761598":"dfx_graph","6ce7795f":"fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(20, 28))\nfig.suptitle('Histogramas normalizados', y=0.90, fontsize=24, va='center')\nfor c, ax in zip(dfx_graph.columns[:], axes.flatten()):\n    sns.histplot(data=dfx_graph.loc[dfy_graph == 0, c].dropna(), stat='density', ax=ax, kde=False)\n    sns.histplot(data=dfx_graph.loc[dfy_graph == 1, c].dropna(), stat='density', kde=False, ax=ax, color='orange')\n    ax.legend(['No Peligroso = 0', 'Peligroso = 1'])","00a66d0d":"# Los gr\u00e1ficos muestran que 'Epoch Osculation' no suma, por lo que se dropea la columna.\ndf.drop(columns=['Epoch Osculation'], inplace=True)","b5cfc108":"# Dividimos Test Train solo para el an\u00e1lisis de features, luego m\u00e1s adelante se vuelve a dividir para el modelo.\nX = df.drop(columns=['Hazardous'])\nY = df.Hazardous\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1, test_size=.30, stratify=Y)\n\nforest = RandomForestClassifier(max_depth=5)\n\n# (Estas l\u00edneas tardan)\nfeat_selector = BorutaPy(forest, n_estimators='auto', verbose=1, random_state=1, alpha=0.05, max_iter=50)\nfeat_selector.fit(np.array(X_train), np.array(y_train))\n","89b781e2":"# Ranking de Resultados\nf = pd.DataFrame(X_train.columns.tolist(), columns=['features'])\nf['rank'] = feat_selector.ranking_\nprint(f.nsmallest(25, 'rank'))","9272defe":"# Dejamos solo las features extra\u00eddas de seg\u00fan los resultados de Boruta.\ndf.drop(columns=['Perihelion Time', 'Asc Node Longitude'], inplace=True)","92a834e0":"df.shape","aabe0c38":"df.describe()","0aa55513":"data_cor = df.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(data_cor, vmin=-1, vmax=1, center=0, cmap='YlGnBu');","c0680e36":"data_cor","a25dae8a":"#Funci\u00f3n que lista correlaci\u00f3n\naux = lista_correlacion(df, 0.8)\ndisplay(aux)","31b2ab9c":"f, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(data_cor, vmin=-1, vmax=1, annot=True, center=0, cmap='YlGnBu');","c8c762cb":"# Drop de las columnas 'Est Dia in M(min)', 'Mean Motion', 'Semi Major Axis', 'Orbital Period' por alta correlaci\u00f3n\ndf.drop(columns=['Est Dia in M(min)', 'Mean Motion', 'Semi Major Axis', 'Orbital Period'], inplace=True)","5d4b704f":"aux = lista_correlacion(df, 0.8)\ndisplay(aux)","30b2e36b":"f, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(data_cor, cmap='YlGnBu');","c3c42bec":"# drop 'Jupiter Tisserand Invariant' por alta correlaci\u00f3n\ndf.drop(columns=['Jupiter Tisserand Invariant'],inplace=True)","bcda82e7":"df","ea587ab4":"X = df.drop(columns=['Hazardous'])\nY = df.Hazardous","8da9e943":"Y.value_counts()","6414c373":"#Separamos en Train y test, verificamos la proporci\u00f3n Test train\nprint(Y.value_counts(normalize=True))\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1, test_size=.30, stratify=Y)","44f200f1":"y_train.value_counts()","d04780fc":"y_test.value_counts()","99031f72":"#Limpieza de outliers Train\ndf1 = []\nfor columns in X_train:\n    dfx = remover_outliers(X_train, columns, hab_Q3Q1=True, max=100e+12, min=0.0)\n    print('{0:30} -  Cant. de outliers = {1}'.format(columns, X_train.shape[0] - dfx.sum()))\n    df1.append(dfx)\nlen(df1[0])","61274dec":"outlier_mask = np.ones(len(df1[0]))\nfor t in df1:\n    outlier_mask = np.logical_and(outlier_mask, t)\n    \noutlier_mask.sum()","016f1871":"X_train = X_train[outlier_mask].copy()\ny_train = y_train[outlier_mask].copy()","6f29c5b5":"X_train.shape","ab77ee4d":"X_test.shape,y_test.shape","b90ba536":"y_test.sum() + y_train.sum()","a40370df":"# Utilizamos sklearn para estandarizar la matriz de Features\nscaler =  MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","24d35b4c":"models = [GaussianNB(),\n          LogisticRegression(),\n          RandomForestClassifier(),\n         ]","485fb2cd":"params = [\n             # Par\u00e1metros GaussianNB\n             {},\n    \n             # Par\u00e1metros LogisticRegression\n             {'C': [0.1, 0.2, 0.5, 1, 2, 5, 10],\n              'penalty': ['l1', 'l2', 'elasticnet'],\n              'solver': ['saga'],\n              'max_iter': [10000],\n              'class_weight': ['none', 'balanced']\n             },\n    \n             # Par\u00e1metros RandomForest\n             {'n_estimators': [10, 20, 50, 100, 200],\n              'criterion': ['gini', 'entropy'],\n              'class_weight': ['balanced', 'balanced_subsample'],\n              'bootstrap': [True, False]\n             },\n         ]","1123753b":"folds = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)","738c018e":"grids = []\nfor i in range(len(models)):\n    gs = GridSearchCV(estimator=models[i], param_grid=params[i], scoring='roc_auc', cv=folds, n_jobs=3, verbose=0)\n    print(models[i])\n    print(gs)\n    print()\n    fit = gs.fit(X_train, y_train)\n    grids.append(fit)","4833ca1f":"for i, g in enumerate(grids):\n    print('Modelo: ', models[i])\n    print('  Score: ', round(g.best_score_, 5))\n    print('  Best Params: ', g.best_params_)\n    print()","7aa5d9dc":"y_pred_GNB_test = grids[0].predict(X_test)\ny_pred_log_test = grids[1].predict(X_test)\ny_pred_RFc_test = grids[2].predict(X_test)\n\ny_pred_GNB_proba_test = grids[0].predict_proba(X_test)[:, 1]\ny_pred_log_proba_test = grids[1].predict_proba(X_test)[:, 1]\ny_pred_RFc_proba_test = grids[2].predict_proba(X_test)[:, 1]\n\ny_pred_GNB_train = grids[0].predict(X_train)\ny_pred_log_train = grids[1].predict(X_train)\ny_pred_RFc_train = grids[2].predict(X_train)\n\ny_pred_GNB_proba_train = grids[0].predict_proba(X_train)[:, 1]\ny_pred_log_proba_train = grids[1].predict_proba(X_train)[:, 1]\ny_pred_RFc_proba_train = grids[2].predict_proba(X_train)[:, 1]","bbe4fdee":"print('Resultados de Valores de TRAIN Gaussian Naive Bayes')\nprint()\nevaluation(y_train, y_pred_GNB_train, y_pred_GNB_proba_train, verbose=1);","86c003f3":"print('Resultados de Valores de TEST Gaussian Naive Bayes')\nprint()\nevaluation(y_test, y_pred_GNB_test, y_pred_GNB_proba_test, verbose=1);","41996b44":"print('Resultados de Valores de TRAIN Logistic Regression')\nprint('')\nevaluation(y_train, y_pred_log_train, y_pred_log_proba_train, verbose=1);","efef3ff4":"print('Resultados de Valores de TEST Logistic Regression')\nprint()\nevaluation(y_test, y_pred_log_test, y_pred_log_proba_test, verbose=1);","b97c5179":"print('Resultados de Valores de TRAIN Random Forest')\nprint()\nevaluation(y_train, y_pred_RFc_train, y_pred_RFc_proba_train, verbose=1);","596f4d5c":"print('Resultados de Valores de TEST Random Forest')\nprint()\nevaluation(y_test, y_pred_RFc_test, y_pred_log_proba_test, verbose=1);","b88316c9":"# Itera las opciones y genera una tabla de resumen\n\nresult = pd.DataFrame()\nresult = pd.DataFrame(columns=['Modelo', 'Data', 'Accuracy', 'Recall', 'F1_Score', 'Precision', 'ROC_AUC', 'TN', 'FP', 'FN', 'TP'])\n\nfor g in range(len(grids)):\n    \n    y_pred_test_g = grids[g].predict(X_test)\n    y_pred_train_g = grids[g].predict(X_train)\n    y_proba_test = grids[g].predict_proba(X_test)[:, 1]\n    y_proba_train = grids[g].predict_proba(X_train)[:, 1]\n        \n    eval_test = evaluation(y_test, y_pred_test_g, y_proba_test, verbose=0)\n    eval_train = evaluation(y_train, y_pred_train_g, y_proba_train, verbose=0)\n    \n    res = []\n    res.append(grids[g].estimator)\n    res.append('Test')\n    for t in  eval_test:\n        res.append(t)\n    result.loc[len(result) + 1] = res\n    \n    res = []\n    res.append(grids[g].estimator)\n    res.append('Train')\n    for t in  eval_train:\n        res.append(t)\n    result.loc[len(result) + 1] = res\n    \nresult","cc754633":"# Importancia de Features\nimportances = grids[2].best_estimator_.feature_importances_\nfeature_list = list(X.columns)\nfeature_importance = sorted(zip(importances, feature_list), reverse=True)\ndfi = pd.DataFrame(feature_importance, columns=['importance', 'feature'])\nimportance = list(dfi['importance'])\nfeature = list(dfi['feature'])\n\nprint(dfi)","4c45bf08":"# Gr\u00e1fico con la importancia de variables\nx_values = list(range(len(feature_importance)))\nplt.style.use('bmh')\nplt.figure(figsize=(12, 6))\nplt.bar(x_values, importance, orientation='vertical')\nplt.xticks(x_values, feature, rotation='vertical')\nplt.ylabel('Importancia')\nplt.xlabel('Variable')\nplt.title('Importancia de Variables');","5974b4a6":"shap.initjs()","1707c735":"rf = grids[2].best_estimator_\nprint(rf)\nexplainer1 = shap.TreeExplainer(rf)\nshap_values1 = explainer1.shap_values(X)","028b4f41":"shap.summary_plot(shap_values1, features=X, feature_names=X.columns, plot_type='bar')","bb2d12a7":"logit = grids[1].best_estimator_\nprint(logit)","3d596f24":"explainer = shap.Explainer(logit, X_train, feature_names=X.columns)\nshap_values = explainer(X_test)","ce6a8bac":"shap.summary_plot(shap_values, features=X, feature_names=X.columns, plot_type='bar')","6c53ba66":"shap.plots.beeswarm(shap_values, max_display=20)","22d0ba8d":"#ind = 1\n#shap.plots.force(shap_values[ind])","58785a0d":"logit = grids[1].best_estimator_\nbeta1 = []\nbeta = logit.coef_\nfor t in beta[0]:\n    beta1.append(t)\n    \nbeta2 = np.array(beta1)\nfeatures = np.array(X.columns)\n\ndfl = pd.DataFrame(np.array(beta2),features)\ndfl.reset_index(inplace=True)\ndfl.rename(columns={'index': 'feature', 0: 'coef'}, inplace=True)\ndfl.sort_values('coef')","f0f1ee3d":"---","1c9a1fac":"#### Logistic Regression TRAIN","185df6f7":"<div>\n    <img src=\"logo_DH.png\"\/>\n<div>\n\n## Data Science 2021 - Grupo 1 - Trabajo Pr\u00e1ctico 3 - Clasificaci\u00f3n de Asteroides\n\n<div>\n    <img src=\"cneos_banner.png\"\/>\n<div>","cf1ca1bb":"### Funci\u00f3n para evaluar resultados","57fce74d":"#### Gaussian Naive Bayes","f8735c84":"### Separamos utilizando stratify para preservar la proporci\u00f3n","7da1b01b":"### Coeficientes (Betas) de la Logistic Regression","6dd2deda":"## Dataset","09d33aa5":"#### Random Forest TRAIN","d0265fca":"<br>\n<div>\n    <h5 align='left'>Falso Negativo:<\/h5>\n    <img src=\"impacto-asteroide.jpg\" alt=\"Falso Negativo\"\/>\n<\/div>","d1318083":"## Limpieza de Outliers","aeb9ae2c":"### Funci\u00f3n Limpieza de Ouliers, devuelve un array de bool con los \"no outliers\"\n","6eec5f01":"---","f8ceee06":"### Verificamos la proporci\u00f3n","439fc0b4":"### Objetivo: predecir si un evento de aproximaci\u00f3n de asteroide es peligroso o no.\n#### Bas\u00e1ndonos en un dataset de Objetos Cercanos a la Tierra (NEOs) obtenido del Jet Propulsion Lab (JPL) de la NASA.","6714f40b":"### Prueba de selecci\u00f3n de features con la Libreria Boruta","d5bac748":"#### Random Forest TEST","a23cd882":"---","5a20c376":"## Columnas - Features","5f2cecf5":"### Carga y exploraci\u00f3n","a9081c7e":"### Features a eliminar por alta correlaci\u00f3n (segunda ronda)\n- **Jupiter Tisserand Invariant** (dejamos **Aphelion Dist** que est\u00e1 m\u00e1s alta en el ranking Boruta)","bcfc2bdd":"## Imports de Librer\u00edas","49f4c893":"### Ajuste del umbral","ad306671":"### Funci\u00f3n para listar correlaciones mayores a cierto umbral","a4e1d624":"### Boruta confirma 17 de las 19 features","17d46e6a":"### Resumen de Resultados","f6f4b47f":"### Gr\u00e1ficos de features\n#### (se limpian un poco antes para que se vean mejor, en un DataFrame aparte)","387b06d1":"#### Links:\n- https:\/\/towardsdatascience.com\/nasa-asteroid-classification-6949bda3b1da\n- https:\/\/www.kaggle.com\/shrutimehta\/nasa-asteroids-classification\n- https:\/\/cneos.jpl.nasa.gov\/","c1a5e4f3":"## Funciones","1552d94a":"#### Logistic Regression","9a39bfe5":"### Importancia de Features en Random Forest","96b7f4ec":"---","1e5df6d4":"---","653ef95b":"#### Gaussian Naive Bayes TEST","d7df892b":"## Esto es todo.\n## \u00a1MUCHAS GRACIAS!","fd620a3b":"### Nulos (no hay)","82caf2b4":"##  Gridsearch con Logistic Regression, Random Forest y Gaussian Naive Bayes","ae162553":"### Resultados de los modelos","34e326fe":"## Extras (fuera de programa)","017dcbcc":"---","20b97c3b":"---","4e573c69":"### Integrantes:\n- Ra\u00fal Bravo, raedbravo@gmail.com\n- Ricardo Climent, rickcliment@gmail.com\n- Gustavo Escandarani, gustavo@escandarani.com.ar\n- Paula Marenco, paula.marenco@gmail.com\n- Humberto Ofria, hofria@gmail.com\n- Manuel Rodr\u00edguez, manuel.f.rodriguez@gmail.com\n- Leandro Scquizzato, lscquizzato@gmail.com\n- Mart\u00edn Urbani, marurbani@hotmail.com\n\n---","7cd7498c":"---","f1e5e8f8":"### An\u00e1lisis de columnas del dataset\n\n#### Los siguientes gr\u00e1ficos fueron pensados para observar el impacto sobre la peligrosidad de cada posible feature para modelar.","02a55e69":"### Tipos de datos","970338e3":"### Campos del Dataset\n\n#### Datos del Asteroide\n- **Neo Reference ID** - ID de referencia otorgado por la NASA al asteroide o cometa cercano a la Tierra (Near Earth Object, NEO)\n- **Name** - 'Nombre' del asteroide (en este dataset viene con la misma informaci\u00f3n que el campo anterior NEO Reference ID)\n* **Absolute Magnitude** - La magnitud absoluta es una medida de la luminosidad\/brillo del asteroide bajo ciertos par\u00e1metros est\u00e1ndar de distancia y \u00e1ngulo de observaci\u00f3n\n* **Est Dia in (in KM, M, Miles, and Feet) (min)** - Di\u00e1metro estimado m\u00ednimo del asteroide (en distintas unidades de medida)\n* **Est Dia in (in KM, M, Miles, and Feet) (max)** - Di\u00e1metro estimado m\u00e1ximo del asteroide (en distintas unidades de medida)\n\n#### Datos de la aproximaci\u00f3n\n* **Close Approach Date** - Fecha de aproximaci\u00f3n a la Tierra del asteroide en la que se midieron algunos de los par\u00e1metros\n* **Epoch Date Close Approach** - Fecha de aproximaci\u00f3n a la Tierra del asteroide (en formato \"epoch\": segundos transcurridos desde cierto punto de referencia)\n* **Relative Velocity (in km per sec, km per hr, and miles per hour)** - Velocidad relativa a la Tierra (en varias unidades de medida)\n* **Miss Dist.(in Astronomical, lunar, km, and miles)** - Distancia a la que pas\u00f3 de la Tierra (en varias unidades de medida)\n* **Orbiting Body** - Los asteroides orbitan todos alrededor del Sol, en este caso este campo se indica contra qu\u00e9 cuerpo se obtuvieron los valores anteriores de velocidad y distancia (la Tierra)\n\n#### Datos de la \u00f3rbita\n* **Orbit ID** - ID de soluci\u00f3n utilizada por la NASA para determinar la \u00f3rbita del asteroide (un valor interno de la NASA)\n* **Orbit Determination Date** - Fecha y hora en la que la \u00f3rbita del asteroide fu\u00e9 determinada\/calculada\n* **Orbit Uncertainity** - Medida de \"incertidumbre\" de la \u00f3rbita calculada (de 0 a 9)\n* **Minimum Orbit Intersection** - Distancia m\u00ednima entre las \u00f3rbitas de la Tierra y del Asteroide (en Unidades Astron\u00f3micas)\n* **Jupiter Tisserand Invariant** -  Valor utilizado para diferenciar asteroides de cometas (cometas de la \"Familia de J\u00fapiter\")\n* **Epoch Osculation** - Fecha hora Tiempo (en formato \"epoch\") en la cual se midieron la posici\u00f3n y velocidad del asteroide para calcular su \u00f3rbita\n\n#### Par\u00e1metros de la \u00f3rbita\n* **Eccentricity** - Valor que especifica cu\u00e1nto se desv\u00eda la \u00f3rbita (que es siempre una elipse) de un c\u00edrculo perfecto\n* **Semi Major Axis** - Semieje mayor de la \u00f3rbita el\u00edptica (la mitad del di\u00e1metro m\u00e1ximo de la elipse)\n* **Inclination** - Inclinaci\u00f3n de la \u00f3rbita del asteroide respecto del plano de la \u00f3rbita de la Tierra                  \n* **Asc Node Longitude** - \u00c1ngulo trazado sobre el plano de la \u00f3rbita de la Tierra entre un punto de referencia y la l\u00ednea que pasa por el nodo de ascenci\u00f3n del asteroide\n* **Orbital Period** - Per\u00edodo orbital, tiempo en el que el asteroide completa una vuelta alrededor del Sol\n* **Perihelion Distance** - Distancia al Sol del punto m\u00e1s cercano de la \u00f3rbita (Perihelio)\n* **Perihelion Arg** - \u00c1ngulo trazado sobre el plano de la \u00f3rbita del asteroide, entre el nodo  de ascenci\u00f3n (cuando el asteroide cruza el plano orbital de la tierra) y el perihelio\n* **Aphelion Dist** - Distancia al Sol del punto m\u00e1s lejano de la \u00f3rbita (Afelio)\n* **Perihelion Time** - Longitud de tiempo del pasaje del asteroide por la etapa de perihelio\n* **Mean Anomaly** - Producto entre el movimiento medio (Mean Motion) y el tiempo de pasaje por el perihelio\n* **Mean Motion** - Velocidad angular requerida por el asteroide para orbitar (se deriva  del per\u00edodo orbital y de la longitud del semieje mayor)\n* **Equinox** - Sistema de coordenadas de referencia utilizado (el est\u00e1ndar actual es 'J2000')\n\n#### Variable Target\n* **Hazardous** - Asteroide peligroso (True \/ False)  ","5fd3c3c1":"---","69d9fa8c":"### Features a eliminar por alta correlaci\u00f3n\n- **Est Dia in M(min)** (dejamos **Est Dia in M(max)** que est\u00e1 m\u00e1s alta en el ranking Boruta)\n- **Mean Motion** (dejamos **Jupiter Tisserand Invariant** que est\u00e1 m\u00e1s alta en el ranking Boruta)\n- **Semi Major Axis** y **Orbital Period** (dejamos **Aphelion Dist** que est\u00e1 m\u00e1s alta en el ranking Boruta)","8d9b87cc":"### An\u00e1lisis de features con Shapley para Random Forest","5cee0d14":"### Ajustamos para sacar un 5 %  de datos extremos aprox.","bed5ea68":"#### Logistic Regression TEST","9217abf6":"## Test Train Split","7ae9ca94":"#### Gaussian Naive Bayes TRAIN","4d2cbad7":"### An\u00e1lisis de features con Shapley para Logistic Regression","bd472e69":"### Observaciones\n\n#### - La peligrosidad aumenta conforme los campos _Relative Velocity_, _Eccentricity_, _Est Dia in M_ (min y max) y _Minimum Orbit Intersection_ suben.\n#### - Tambi\u00e9n es mayor cuando hay menor incertidumbre sobre la \u00f3rbita (_Orbit Uncertainity_).\n#### - Es notable como casi todos los eventos peligrosos est\u00e1n vinculados a _Absolute Magnitude_ menor o igual a 22, a _Perihelion distance_ menor o igual a 1, y a _Minimum Orbit Intersection_ menor o igual a 1.\n\n#### - Si bien este an\u00e1lisis visual preliminar permite comprender la mayor o menor importancia de algunas features, en este proyecto utilizamos metodolog\u00edas para seleccionar y descartar features relevantes que son m\u00e1s fiables que estas obervaciones visuales.\n\n","9c981ab5":"---","71d5417a":"### Duplicados (no hay)","0916f379":"## Scaler\n### Utilizamos MixMax que es el que mejores resultados nos di\u00f3","30fc38fb":"### An\u00e1lisis de Correlaci\u00f3n"}}