{"cell_type":{"608246f5":"code","96431c6d":"code","21672743":"code","896cc4a3":"code","3557dce3":"code","322145c2":"code","b16c7c76":"code","f5a9a2e1":"code","b40a52a7":"code","9f293fb6":"code","132c0dcc":"code","6b31fbcb":"code","b9e58127":"markdown","78eeb03b":"markdown","3a18f8f6":"markdown","e4052820":"markdown","b41e2d15":"markdown","ca183d90":"markdown","78d7bd99":"markdown"},"source":{"608246f5":"import os\nimport sys\nimport collections\nimport csv\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport time\n\n# BERT files\n\nos.listdir(\"..\/input\/pretrained-bert-including-scripts\/master\/bert-master\")\nsys.path.insert(0, '..\/input\/pretrained-bert-including-scripts\/master\/bert-master')\n\nfrom run_classifier import *\nimport modeling\nimport optimization\nimport tokenization","96431c6d":"# import data\n\ntrain=pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest=pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\n# remove new lines etc.\n\ntrain['comment_text'] = train['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\ntest['comment_text'] = test['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n\n# force train into cola format, test is fine as it is\n\ntrain['dummy_1'] = 'meh'\ntrain['dummy_2'] = '*'\n\ntrain = train[['dummy_1','target','dummy_2','comment_text']]\ntrain['target'] = np.where(train['target']>=0.5,1,0)\n\ntrain = train.sample(frac=0.33)\n\n# export as tab seperated\n\ntrain.to_csv('train.tsv', sep='\\t', index=False, header=False)\ntest.to_csv('test.tsv', sep='\\t', index=False, header=True)","21672743":"task_name = 'cola'\nbert_config_file = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/bert_config.json'\nvocab_file = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/vocab.txt'\ninit_checkpoint = '..\/input\/pretrained-bert-including-scripts\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/bert_model.ckpt'\ndata_dir = '.\/'\noutput_dir = '.\/'\ndo_lower_case = True\nmax_seq_length = 72\ndo_train = True\ndo_eval = False\ndo_predict = False\ntrain_batch_size = 32\neval_batch_size = 32\npredict_batch_size = 32\nlearning_rate = 2e-5 \nnum_train_epochs = 1.0\nwarmup_proportion = 0.1\nuse_tpu = False\nmaster = None\nsave_checkpoints_steps = 99999999 # <----- don't want to save any checkpoints\niterations_per_loop = 1000\nnum_tpu_cores = 8\ntpu_cluster_resolver = None","896cc4a3":"start = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Starting training ...\")\nprint(\"--------------------------------------------------------\")","3557dce3":"bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n\nprocessor = ColaProcessor()\nlabel_list = processor.get_labels()\n\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntpu_cluster_resolver = None\nis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n\nrun_config = tf.contrib.tpu.RunConfig(\n  cluster=tpu_cluster_resolver,\n  master=master,\n  model_dir=output_dir,\n  save_checkpoints_steps=save_checkpoints_steps,\n  tpu_config=tf.contrib.tpu.TPUConfig(\n      iterations_per_loop=iterations_per_loop,\n      num_shards=num_tpu_cores,\n      per_host_input_for_training=is_per_host))\n\ntrain_examples = processor.get_train_examples(data_dir)\nnum_train_steps = int(len(train_examples) \/ train_batch_size * num_train_epochs)\nnum_warmup_steps = int(num_train_steps * warmup_proportion)\n\nmodel_fn = model_fn_builder(\n      bert_config=bert_config,\n      num_labels=len(label_list),\n      init_checkpoint=init_checkpoint,\n      learning_rate=learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=use_tpu,\n      use_one_hot_embeddings=use_tpu)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=train_batch_size)\n      \n      \ntrain_file = os.path.join(output_dir, \"train.tf_record\")\n\nfile_based_convert_examples_to_features(\n    train_examples, label_list, max_seq_length, tokenizer, train_file)\n\ntf.logging.info(\"***** Running training *****\")\ntf.logging.info(\"  Num examples = %d\", len(train_examples))\ntf.logging.info(\"  Batch size = %d\", train_batch_size)\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\n\ntrain_input_fn = file_based_input_fn_builder(\n    input_file=train_file,\n    seq_length=max_seq_length,\n    is_training=True,\n    drop_remainder=True)\n    \nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","322145c2":"end = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Training complete in \", end - start, \" seconds\")\nprint(\"--------------------------------------------------------\")","b16c7c76":"def file_based_input_fn_builder(input_file, seq_length, is_training,\n                                drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  name_to_features = {\n      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n  }\n\n  def _decode_record(record, name_to_features):\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    \n    #batch_size = params[\"batch_size\"]\n    batch_size = 64 # <----- hardcoded batch_size added here \n    \n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn","f5a9a2e1":"start = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Starting inference ...\")\nprint(\"--------------------------------------------------------\")","b40a52a7":"predict_examples = processor.get_test_examples(data_dir)\nnum_actual_predict_examples = len(predict_examples)\n\npredict_file = os.path.join(output_dir, \"predict.tf_record\")\n\nfile_based_convert_examples_to_features(predict_examples, label_list,\n                                        max_seq_length, tokenizer,\n                                        predict_file)\n\ntf.logging.info(\"***** Running prediction*****\")\ntf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n                len(predict_examples), num_actual_predict_examples,\n                len(predict_examples) - num_actual_predict_examples)\ntf.logging.info(\"  Batch size = %d\", predict_batch_size)\n\npredict_drop_remainder = True if use_tpu else False\npredict_input_fn = file_based_input_fn_builder(\n    input_file=predict_file,\n    seq_length=max_seq_length,\n    is_training=False,\n    drop_remainder=predict_drop_remainder)\n\nresult = estimator.predict(input_fn=predict_input_fn)\n\noutput_predict_file = os.path.join(output_dir, \"test_results.tsv\")\n\nwith tf.gfile.GFile(output_predict_file, \"w\") as writer:\n    num_written_lines = 0\n    tf.logging.info(\"***** Predict results *****\")\n    for (i, prediction) in enumerate(result):\n        probabilities = prediction[\"probabilities\"]\n        if i >= num_actual_predict_examples:\n            break\n        output_line = \"\\t\".join(\n            str(class_probability)\n            for class_probability in probabilities) + \"\\n\"\n        writer.write(output_line)\n        num_written_lines += 1\n","9f293fb6":"end = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Inference complete in \", end - start, \" seconds\")\nprint(\"--------------------------------------------------------\")","132c0dcc":"sample_submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\npredictions = pd.read_csv('.\/test_results.tsv', header=None, sep='\\t')\n\nsubmission = pd.concat([sample_submission.iloc[:,0], predictions.iloc[:,1]], axis=1)\nsubmission.columns = ['id','prediction']\nsubmission.to_csv('submission.csv', index=False, header=True)","6b31fbcb":"submission.head()","b9e58127":"## Fine Tuning\n\nWe'll run over the entire data set for a single epoch. Following code is just lifted from run_classifier.py - apologies for the mess :)","78eeb03b":"## Submission","3a18f8f6":"## Prepare Data\n\nTo keep things simple we adapt the competition data to the format that BERT expects for cola. ","e4052820":"## Parameters\n\nSee https:\/\/github.com\/google-research\/bert\/blob\/master\/run_classifier.py","b41e2d15":"## Introduction\n\nThis is a simple demonstration of fine tuning BERT for this competition. We make use of the fact that BERT comes with a binary classification example we can repurpose for this competition (cola). I am confident it is not possible to fine-tune BERT for a complete epoch on the entire data set in 2HR kernel, so here I've had to make some adjustments to stay within the time limits:\n\n* We only use 1\/3 of the training data\n* We use a maximum sequence length of 72\n\nI think compute might be a big factor in this competition!\n\nThanks for Jon Mischo (https:\/\/www.kaggle.com\/supertaz) for uploading BERT Models + Scripts :)","ca183d90":"## Inference\n\nFor some reason I've had issues with batch_size - I'm not quite sure where this parameter comes from. For now I just hard code it in the function below, which should work fine. As I spend more time with the code, hopefully it becomes clearer.\n\nInference should only take about 10 minutes for public test set (~100k rows).\n","78d7bd99":"## Libraries\n\nWe'll add the BERT repo to path so we can import directly."}}