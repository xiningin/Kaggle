{"cell_type":{"b727afe8":"code","7ef3d0e6":"code","6cbed874":"code","3b662132":"code","94e0b732":"code","2b6f6573":"code","bc8cf91d":"code","7f6dbab6":"code","6c8aa46a":"code","ae4fff6b":"code","9988e9d6":"code","a406e912":"code","87daf1ef":"code","0586b613":"code","ab511dad":"markdown","1484fe08":"markdown","f4144c4b":"markdown"},"source":{"b727afe8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot drawing\nfrom tensorflow import keras #import google tensorflow.keras, which we may use later\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7ef3d0e6":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n#deal with training data \nx=train.drop([\"label\"],axis=1)\nx=x.iloc[:,:].values\nx=x.reshape(42000,28,28,1)\n#deal with test data\ntest=test.iloc[:,:].values\ntest=test.reshape(28000,28,28,1)\n#normalize the data.\nx, test= x\/255.0,test\/255.0\n#Get the training labels\ny=train.label.iloc[:].values","6cbed874":"#The whole \"building\" of neuronetwork\nSequential=keras.Sequential\n#Now we choose \"who will be moved into\" each layer\n#possible Candidates are:\n#Use different filters to get different channels.\nConv2D=keras.layers.Conv2D\n#Pooling, select Maximum value from a certain pool.\nMaxPool2D=keras.layers.MaxPool2D\n#a flat ANN layer\nDense=keras.layers.Dense\n#as it suggests, flatten: \\mathbb{R}^2 \\to \\mathbb{R}\nFlatten=keras.layers.Flatten\n#Randomly select a chosen percentage of neurons and remove them temporarily\nDropout=keras.layers.Dropout\n#BarchNormalization, \"Centralize\" the batch.(randomly chose a noise on sigma is available)\nBatchNormalization=keras.layers.BatchNormalization","3b662132":"def BN_model():\n    model=Sequential()\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'same', \n                 activation ='relu'))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'same', \n                 activation ='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(0.3))\n    model.add(Dense(10, activation = \"softmax\"))\n    model.compile(keras.optimizers.Adam(0.0001), \n             loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"]\n             )\n    return model","94e0b732":"def NBN_model():\n    model=Sequential()\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n                 activation ='relu', input_shape = (28,28,1)))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'same', \n                 activation ='relu'))\n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'same', \n                 activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n    model.add(Dropout(0.3))\n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(0.3))\n    model.add(Dense(10, activation = \"softmax\"))\n    model.compile(keras.optimizers.Adam(0.0001), \n             loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"]\n             )\n    return model","2b6f6573":"#Finish building up 2 model's architecture\nBmodel=BN_model()\nNmodel=NBN_model()","bc8cf91d":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n#Considering to do data augumentation\ndatagen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)","7f6dbab6":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1)","6c8aa46a":"history_B = Bmodel.fit_generator(datagen.flow(x_train,y_train, batch_size=20),\n                                 epochs = 45, \n                                 validation_data=(x_test,y_test),\n                                 validation_steps=210,\n                                 verbose = 1, steps_per_epoch=x.shape[0] \/ 20)","ae4fff6b":"history_N = Nmodel.fit_generator(datagen.flow(x_train,y_train, batch_size=20),\n                                 epochs = 45, \n                                 validation_data=(x_test,y_test),\n                                 validation_steps=210,\n                                 verbose = 1, steps_per_epoch=x.shape[0] \/ 20)","9988e9d6":"# Plot training & validation accuracy values\nplt.plot(history_B.history['acc'])\nplt.plot(history_B.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_B.history['loss'])\nplt.plot(history_B.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","a406e912":"# Plot training & validation accuracy values\nplt.plot(history_N.history['acc'])\nplt.plot(history_N.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_N.history['loss'])\nplt.plot(history_N.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","87daf1ef":"predictionsB=Bmodel.predict_classes(test)\npredictionsN=Nmodel.predict_classes(test)","0586b613":"ids=np.arange(28000)+1\noutput=pd.DataFrame({\"ImageId\":ids, \"Label\":predictionsB})\noutput.to_csv(\"submissionB.csv\",index=False)\noutput=pd.DataFrame({\"ImageId\":ids, \"Label\":predictionsN})\noutput.to_csv(\"submissionN.csv\",index=False)","ab511dad":"Build the architecture of two models, one is a batch normalized model and a non batch normalized model","1484fe08":"# BatchNormalization\n\nThe procedure of neuro network(from my understanding, __pleas point out if I am wrong, it is really important for me__) is:\n1. Select a batch, put them togethor in to the input layer;\n2. Simultaneously play the BackPropagation through the network, and add the effect of them together to determine the next position of gradient descent.\n3. Return to 1 until we finish our epoch.","f4144c4b":"Needless to say, the first thing to do is always importing the data."}}