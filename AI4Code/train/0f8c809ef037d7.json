{"cell_type":{"4e8c8a37":"code","cf44d2ec":"code","302128e2":"code","ca424658":"code","f3ef33ab":"code","56229d1b":"code","a19adecd":"code","bf0b748b":"code","844182e4":"code","a138e549":"code","2f2fefee":"code","b437d21f":"code","70913af6":"code","43fc5190":"code","7ffc2e98":"code","0facbc70":"code","fbec7cfd":"code","dfe1463b":"code","3951ce13":"markdown","020e6f1b":"markdown","626101cf":"markdown","0ad246a5":"markdown","9705d873":"markdown","56db768d":"markdown","16180941":"markdown","c304e31a":"markdown","ebc41c4e":"markdown","b1023ef4":"markdown","bb8efb85":"markdown","a4d71c24":"markdown","051ac52d":"markdown","d70e1aa6":"markdown","9ec946a3":"markdown","205a9a7d":"markdown","ee0b1be5":"markdown"},"source":{"4e8c8a37":"# 00. Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import scale","cf44d2ec":"# A.01. Import Data\nf_train = '..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv'\ndf_train = pd.read_csv(f_train, index_col=0)","302128e2":"# A.02.01. Replace 'strange' values and convert dtype\ndf_train['Dependents'] = df_train['Dependents'].replace('3+', '3')\ndf_train['Dependents'] = pd.to_numeric(df_train['Dependents'], errors='coerce')","ca424658":"# A.02.02. Features (Categorical & Numerical)\nX = df_train.drop(columns='Loan_Status') # feature\ncategorical = []\nnumerical = []\nfor feature in list(X.columns):\n\tif X[feature].dtypes == object:\n\t\tcategorical.append(X[feature])\n\telse:\n\t\tnumerical.append(X[feature])\ncategorical = pd.concat(categorical, axis=1)\nnumerical = pd.concat(numerical, axis=1)\ncol_name_cat = categorical.columns # for later use\ncol_name_num = numerical.columns # for later use","f3ef33ab":"# A.02.03 Fill na values (imputing)\nimp_num = SimpleImputer(strategy='mean')\nimp_cat = SimpleImputer(strategy='most_frequent')\nimp_num.fit(numerical)\nimp_cat.fit(categorical)\nnumerical = pd.DataFrame(imp_num.transform(numerical), index=df_train.index,\ncolumns = col_name_num)\ncategorical = pd.DataFrame(imp_cat.transform(categorical), index=df_train.index)\nprint(categorical.isnull().sum())\nprint(numerical.isnull().sum())","56229d1b":"# A.02.04 Encode Categorical (with pd.get_dummies())\ncategorical = pd.get_dummies(categorical, drop_first=True)\ncategorical.columns = ['Male', 'Married', 'Not Graduate', 'Self-Employed', 'SemiUrban', 'Urban']","a19adecd":"data_train = pd.concat([categorical, numerical, df_train['Loan_Status']], axis=1) # for Part C","bf0b748b":"X = pd.concat([categorical, numerical], axis=1) # features after imputing and encoding\nX = scale(X) # scaled features\ny = df_train['Loan_Status'] # target\nprint(X.shape)\nprint(y.shape)","844182e4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99) # train_test_split","a138e549":"# A.03.01 K-NN Method\nparam_knn = {'n_neighbors' : np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, param_knn, cv=5)\nknn_cv.fit(X_train, y_train)\nprint('K-NN Best Parameter & Score:')\nprint(knn_cv.best_params_)\nprint(knn_cv.best_score_)\ny_pred = knn_cv.predict(X_test)\nknn_score = knn_cv.score(X_test, y_test)\nprint('\\nK-NN Accuracy Score: ', knn_score)\nprint('Classification Report: \\n')\nprint(classification_report(y_test, y_pred), '\\n')","2f2fefee":"# A.03.02 Logistic Regression\nparam_log = {'C' : np.logspace(-4, 4, 20)}\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg, param_log, cv=5)\nlogreg_cv.fit(X_train, y_train)\nprint('Log Reg Best Parameter & Score:')\nprint(logreg_cv.best_params_)\nprint(logreg_cv.best_score_)\ny_pred = logreg_cv.predict(X_test)\nlogreg_score = logreg_cv.score(X_test, y_test)\nprint('\\nLog Reg Accuracy Score: ', logreg_score)\nprint('Classification Report: \\n')\nprint(classification_report(y_test, y_pred), '\\n')","b437d21f":"# A.03.03 Random Forest Method\nparam_rf = {'n_estimators' : np.arange(100,550,100)}\nrf = RandomForestClassifier()\nrf_cv = GridSearchCV(rf, param_rf, cv=5)\nrf_cv.fit(X_train, y_train)\nprint('Random Forest Best Parameter & Score:')\nprint(rf_cv.best_params_)\nprint(rf_cv.best_score_)\ny_pred = rf_cv.predict(X_test)\nrf_score = rf_cv.score(X_test, y_test)\nprint('\\n Random Forest Accuracy Score: ', rf_score)\nprint('Classification Report: \\n')\nprint(classification_report(y_test, y_pred), '\\n')","70913af6":"f_new = '..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv'\ndf_new = pd.read_csv(f_new, index_col=0)","43fc5190":"# B.02.01. Replace 'strange' values and convert dtype\ndf_new['Dependents'] = df_new['Dependents'].replace('3+', '3')\ndf_new['Dependents'] = pd.to_numeric(df_new['Dependents'], errors='coerce')\n# B.02.02. Features (Categorical & Numerical)\nX = df_new # feature\ncategorical = []\nnumerical = []\nfor feature in list(X.columns):\n\tif X[feature].dtypes == object:\n\t\tcategorical.append(X[feature])\n\telse:\n\t\tnumerical.append(X[feature])\ncategorical = pd.concat(categorical, axis=1)\nnumerical = pd.concat(numerical, axis=1)\n# B.02.03 Fill na values (imputing)\nimp_num.fit(numerical)\nimp_cat.fit(categorical)\nnumerical = pd.DataFrame(imp_num.transform(numerical), index=df_new.index,\ncolumns = col_name_num)\ncategorical = pd.DataFrame(imp_cat.transform(categorical), index=df_new.index)\n# B.02.04 Encode Categorical (with pd.get_dummies())\ncategorical = pd.get_dummies(categorical, drop_first=True)\ncategorical.columns = ['Male', 'Married', 'Not Graduate', 'Self-Employed', 'SemiUrban', 'Urban']","7ffc2e98":"# B.03. Predictions\nX = pd.concat([categorical, numerical], axis=1) # features after imputing and encoding\nX = scale(X) # scale features\nif (knn_score > logreg_score) & (knn_score > rf_score):\n\ty_pred = knn_cv.predict(X)\n\tprint('Chosen Method : K-NN')\n\tprint('Accuracy Score: ', knn_score)\nelif (logreg_score > knn_score) & (logreg_score > rf_score):\n\ty_pred = logreg_cv.predict(X)\n\tprint('Chosen Method : LogisticRegression')\n\tprint('Accuracy Score: ', logreg_score)\nelse:\n\ty_pred = rf_cv.predict(X)\n\tprint('Chosen Method : Random Forest')\n\tprint('Accuracy Score: ', rf_score)","0facbc70":"# B.04. Save Results\ndf_result = df_new\ndf_result['Loan_Status'] = y_pred\ndf_result.to_csv('test result.csv')","fbec7cfd":"rejected = df_result[df_result['Loan_Status'] == 'N']\naccepted = df_result[df_result['Loan_Status'] == 'Y']\nprint(rejected['Credit_History'].value_counts())\nprint(accepted['Credit_History'].value_counts())","dfe1463b":"data_train.Loan_Status = data_train.Loan_Status.replace({'Y' : 1, 'N' : 0})\ncorr = data_train.corr()\n_ = sns.heatmap(corr)\nplt.show()","3951ce13":"# A.02. Cleaning Data","020e6f1b":"# C. Result Analysis\nOne of the insights that from the result is most of the **'Y' Loan_Status is given to entries with 1 in Credit_History**.","626101cf":"# A.02.04 Encode Caterogical Data\nDone with pandas' pd.get_dummies(). It is similar to sklearn's OneHotEncoder().","0ad246a5":"# A.02.02. Numerical & Categorical Data Separation","9705d873":"# A.01. Import Train Dataset","56db768d":"# A.02.01. Replace 'Strange' Data \nIn 'Dependents' column of the dataframe, we should replace '3+' with '3' and convert the column's dtype to numeric.","16180941":"# B.03. Predictions\nSet features from new data, scale, and predict based on the best classifier.","c304e31a":"# Project Summary\nThis project goal is to predict Loan_Status based on the best classifier. Algorithms tested for this project are K-NN, Logistic Regression, and Random Forest. Hyperparametric tuning for those algorithms is done by sklearn's GridSearchCV. I 'separated' the script into 3 parts:\n\n    A. ML Algorithms\n    B. Predictions\n    C. Results Analysis\n","ebc41c4e":"# A.03.02 Logistic Regression\nHyperparametric tuning is applied to 'C' parameter and also cross-validated.","b1023ef4":"# A.03. ML Algorithms\nSet features, targets, train-test split, fitting, and scoring. The features will be scaled to get a better result.","bb8efb85":"# B.04. Save Results (csv)","a4d71c24":"# A.03.01. K-NN Method Classifying\nHyperparametric tuning will be applied in 'n_neighbors' of KNN Classifier and cross-validated (5 folds).","051ac52d":"# A.02.03. Filling Null Values (Imputing)\nFor numerical values, null values will be replaced by its mean and for categorical by its most frequent value.","d70e1aa6":"This corresponds well with the training data correlation heatmap.","9ec946a3":"# B.01. Import New Data For Prediction","205a9a7d":"# A.03.02 Random Forest Method\nHyperparemtric tuning on 'n_estimators' parameter. This actually takes couple more seconds. You might lower the maximum n_estimators for the tuning.","ee0b1be5":"# B.02. Cleaning New Data\nSimilar to **Part A.02**."}}