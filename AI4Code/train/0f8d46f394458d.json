{"cell_type":{"8aac025e":"code","f84f6025":"code","e90e101e":"code","94775fbe":"code","fab4c32d":"code","19176890":"code","5c8f8dfb":"code","c8609fbe":"code","7c474adb":"code","27107659":"code","381bf458":"code","88e3c968":"code","01e0f99e":"code","0608bfe7":"code","e0263d70":"code","05752105":"code","7769486a":"code","9ebafe4d":"code","ed6fe9a8":"code","f2edc637":"code","bc22221f":"code","1ca7823c":"code","9ead9c3c":"code","7a91315e":"code","bc4a6e24":"code","d2179397":"code","4c35f983":"markdown","fb4da2b4":"markdown","6b39f746":"markdown","e6ed0ff0":"markdown","e14e8275":"markdown","53f5ec37":"markdown","4503a0cf":"markdown","bf7373a3":"markdown","a083db24":"markdown","046108f6":"markdown","6318f9a2":"markdown","d64c6849":"markdown","dc09140e":"markdown","16108411":"markdown","9222cd99":"markdown","8a4a682f":"markdown","e95d15d7":"markdown","3eb6427a":"markdown"},"source":{"8aac025e":"from __future__ import absolute_import, division, print_function\nimport gzip, pickle\nimport pathlib\nimport urllib\n\nimport random\nimport time\nimport glob\nimport shutil\nimport warnings\nimport os\nfrom tqdm import tqdm\n\nimport cv2\nimport scipy\nfrom scipy import linalg\n\nimport numpy as np\nimport pandas as pd\n\nimport xml.etree.ElementTree as ET \nimport matplotlib.pyplot as plt, zipfile \n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\nfrom sklearn.preprocessing import LabelEncoder\n \nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, Reshape, Flatten, concatenate, UpSampling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import SGD, Adam\n\nfrom PIL import Image\n","f84f6025":"%matplotlib inline\nwarnings.filterwarnings('ignore', category=FutureWarning)","e90e101e":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(43)","94775fbe":"os.listdir(path=\"..\/input\/gangangan\/GANs\/data_homework\/Annotation\/\") ","fab4c32d":"ROOT = '..\/input\/gangangan\/GANs\/data_homework\/' # \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f, \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u044e\u0442\u0441\u044f \u0444\u0430\u0439\u043b\u044b\nIMAGES = os.listdir(ROOT + 'all-dogs\/all-dogs\/')\nbreeds = os.listdir(ROOT + 'Annotation\/Annotation\/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,64,64,3))","19176890":"print(breeds)","5c8f8dfb":"ComputeLB = True # \u0444\u043b\u0430\u0433 \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\nDogsOnly = True # \u0444\u043b\u0430\u0433 \u0434\u043b\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u043e\u0431\u0430\u0447\u0435\u043a","c8609fbe":"%%time\nif DogsOnly:\n    for breed in breeds:\n        for dog in os.listdir(ROOT+'Annotation\/Annotation\/'+breed):\n            try: img = Image.open(ROOT+'all-dogs\/all-dogs\/'+dog+'.jpg') \n            except: continue           \n            tree = ET.parse(ROOT+'Annotation\/Annotation\/'+breed+'\/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                img2 = img2.resize((64,64), Image.ANTIALIAS)\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                #if idxIn%1000==0: print(idxIn)\n                namesIn.append(breed)\n                idxIn += 1\n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n    imagesIn = imagesIn[idx,:,:,:]\n    namesIn = np.array(namesIn)[idx]","7c474adb":"print(idx)","27107659":"print(idxIn)\nx = np.random.randint(0,idxIn,25)\nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n        else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()","381bf458":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043d\u0430\u0448 \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440\ndog = Input((12288,))\ndogName = Input((10000,))\nx = Dense(12288, activation='sigmoid')(dogName) \nx = Reshape((2,12288,1))(concatenate([dog,x]))\nx = Conv2D(1,(2,1),use_bias=False,name='conv')(x)\ndiscriminated = Flatten()(x)\n\n# \u0421\u043a\u043e\u043c\u043f\u0438\u043b\u0438\u0440\u0443\u0435\u043c\ndiscriminator = Model([dog,dogName], discriminated)\ndiscriminator.get_layer('conv').trainable = False\ndiscriminator.get_layer('conv').set_weights([np.array([[[[-1.0 ]]],[[[1.0]]]])])\ndiscriminator.compile(optimizer='adam', loss='binary_crossentropy')\n\n# \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443\ndiscriminator.summary()","88e3c968":"%%time\n# TRAINING DATA\nBATCH_SIZE = 256\ntrain_y = (imagesIn[:10000,:,:,:]\/255.).reshape((-1,12288))\ntrain_X = np.zeros((10000,10000))\nfor i in range(10000): train_X[i,i] = 1\nzeros = np.zeros((10000,12288))\n\n# TRAIN NETWORK\nlr = 0.75\nfor k in range(5):\n    annealer = LearningRateScheduler(lambda x: lr)\n    # This function keeps the learning rate at 0.5 for the first ten epochs\n    # and decreases it exponentially after that.\n    h = discriminator.fit([zeros,train_X], train_y, epochs = 10, batch_size=BATCH_SIZE, callbacks=[annealer], verbose=0)\n    print('Epoch',(k+1)*10,'\/50 - loss =',h.history['loss'][-1] )\n    if h.history['loss'][-1]<0.533: lr = 0.1","01e0f99e":"del train_X, train_y, imagesIn","0608bfe7":"print('Discriminator Recalls from Memory Dogs')    \nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        xx = np.zeros((10000))\n        xx[np.random.randint(10000)] = 1\n        plt.subplot(1,5,j+1)\n        img = discriminator.predict([zeros[0,:].reshape((-1,12288)),xx.reshape((-1,10000))]).reshape((-1,64,64,3))\n        img = Image.fromarray( (255*img).astype('uint8').reshape((64,64,3)))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show()","e0263d70":"#BadMemory = True\n#with tpu_strategy.scope():\nBadMemory = True\n\nif BadMemory:\n    seed = Input((10000,))\n    x = Dense(4096, activation='relu')(seed)\n    x = Reshape((8,8,64))(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x) # [bs, 8, 8, 128]\n    x = UpSampling2D((2, 2))(x) # [bs, 16, 16, 128]\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x) # [bs, 32, 32, 64]\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n    x = UpSampling2D((2, 2))(x) # [bs, 64, 64, 32]\n    x = Conv2D(3, (3, 3), activation='linear', padding='same')(x) # [bs, 64, 64, 3]\n    generated = Flatten()(x)\nelse:\n    seed = Input((10000,))\n    generated = Dense(12288, activation='linear')(seed)\n\n# COMPILE\ngenerator = Model(seed, [generated,Reshape((10000,))(seed)])\n\n# DISPLAY ARCHITECTURE\ngenerator.summary()\n","05752105":"#\u041f\u043e\u0441\u0442\u043e\u0440\u0438\u043c \u0441\u0435\u0442\u044c\ndiscriminator.trainable=False    \ngan_input = Input(shape=(10000,))\nx = generator(gan_input)\ngan_output = discriminator(x)\n\n# \u0441\u043a\u043e\u043c\u043f\u0438\u043b\u0438\u0440\u0443\u0435\u043c GAN\ngan = Model(gan_input, gan_output)\ngan.get_layer('model_1').get_layer('conv').set_weights([np.array([[[[-1 ]]],[[[255.]]]])])\ngan.compile(optimizer=Adam(5), loss='mean_squared_error')\n\n# \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443\ngan.summary()","7769486a":"%%time\n# TRAINING DATA\ntrain = np.zeros((10000,10000))\nfor i in range(10000): train[i,i] = 1\nzeros = np.zeros((10000,12288))\n\n# TRAIN NETWORKS\nep = 1; it = 10\n\nif BadMemory: lr = 0.01\nelse: lr = 5.\n    \nfor k in range(it):  \n\n    # BEGIN DISCRIMINATOR COACHES GENERATOR\n    annealer = LearningRateScheduler(lambda x: lr)\n    h = gan.fit(train, zeros, epochs = ep, batch_size=256, callbacks=[annealer], verbose=0)\n\n    # DISPLAY GENERATOR LEARNING PROGRESS \n    print('Epoch',(k+1),'\/'+str(it)+' - loss =',h.history['loss'][-1] )\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        xx = np.zeros((10000))\n        xx[np.random.randint(10000)] = 1\n        plt.subplot(1,5,j+1)\n        img = generator.predict(xx.reshape((-1,10000)))[0].reshape((-1,64,64,3))\n        img = Image.fromarray( (img).astype('uint8').reshape((64,64,3)))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show()  \n            \n    # ADJUST LEARNING RATES\n    if BadMemory:\n        ep *= 2\n        if ep>=32: lr = 0.001\n        if ep>256: ep = 256\n    else:\n        if h.history['loss'][-1] < 25: lr = 1.\n        if h.history['loss'][-1] < 1.5: lr = 0.5\n","9ebafe4d":"class DogGenerator:\n    index = 0   \n    def getDog(self,seed):\n        xx = np.zeros((10000))\n        xx[self.index] = 0.70\n        xx[np.random.randint(10000)] = 0.30\n        img = generator.predict(xx.reshape((-1,10000)))[0].reshape((64,64,3))\n        self.index = (self.index+1)%10000\n        return Image.fromarray( img.astype('uint8') ) ","ed6fe9a8":"# \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nd = DogGenerator()\nfor k in range(3):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = d.getDog(np.random.normal(0,1,100))\n        plt.axis('off')\n        plt.imshow(img)\n    plt.show() ","f2edc637":"%%time\n# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0432 \u0437\u0438\u043f \u0444\u043e\u0440\u043c\u0430\u0442\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\nz = zipfile.PyZipFile('images2.zip', mode='w')\nd = DogGenerator()\nfor k in range(10000):\n    img = d.getDog(np.random.normal(0,1,100))\n    f = str(k)+'.png'\n    img.save(f,'PNG'); z.write(f); os.remove(f)\n    #if k % 1000==0: print(k)\nz.close()","bc22221f":"class KernelEvalException(Exception):\n    pass\n\nmodel_params = {\n    'Inception': {\n        'name': 'Inception', \n        'imsize': 64,\n        'output_layer': 'Pretrained_Net\/pool_3:0', \n        'input_layer': 'Pretrained_Net\/ExpandDims:0',\n        'output_shape': 2048,\n        'cosine_distance_eps': 0.1\n        }\n}\n\ndef create_model_graph(pth):\n    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n    # Creates graph from saved graph_def.pb.\n    with tf.io.gfile.GFile( pth, 'rb') as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString( f.read())\n        _ = tf.import_graph_def( graph_def, name='Pretrained_Net')\n\ndef _get_model_layer(sess, model_name):\n    # layername = 'Pretrained_Net\/final_layer\/Mean:0'\n    layername = model_params[model_name]['output_layer']\n    layer = sess.graph.get_tensor_by_name(layername)\n    print(layer)\n#     layer.__dict__['_shape_val'] = tf.TensorShape([1, 1, 1, 2048])\n    ops = layer.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n#             print(shape._dims)\n            if shape._dims != []:\n              shape = [s if s != None else None for s in shape]\n              new_shape = []\n              for j, s in enumerate(shape):\n                if s == 1 and j == 0:\n                  new_shape.append(None)\n                else:\n                  new_shape.append(s)\n              o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n    return layer\n\ndef get_activations(images, sess, model_name, batch_size=50, verbose=False):\n    \"\"\"Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 256.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the disposable hardware.\n    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, 2048) that contains the\n       activations of the given tensor when feeding inception with the query tensor.\n    \"\"\"\n    inception_layer = _get_model_layer(sess, model_name)\n    n_images = images.shape[0]\n    if batch_size > n_images:\n        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n        batch_size = n_images\n    n_batches = n_images\/\/batch_size + 1\n    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n    for i in tqdm(range(n_batches)):\n        if verbose:\n            print(\"\\rPropagating batch %d\/%d\" % (i+1, n_batches), end=\"\", flush=True)\n        start = i*batch_size\n        if start+batch_size < n_images:\n            end = start+batch_size\n        else:\n            end = n_images\n                    \n        batch = images[start:end]\n        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n    if verbose:\n        print(\" done\")\n    return pred_arr\n\n\n# def calculate_memorization_distance(features1, features2):\n#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n#     neigh.fit(features2) \n#     d, _ = neigh.kneighbors(features1, return_distance=True)\n#     print('d.shape=',d.shape)\n#     return np.mean(d)\n\ndef normalize_rows(x: np.ndarray):\n    \"\"\"\n    function that normalizes each row of the matrix x to have unit length.\n\n    Args:\n     ``x``: A numpy matrix of shape (n, m)\n\n    Returns:\n     ``x``: The normalized (by row) numpy matrix.\n    \"\"\"\n    return np.nan_to_num(x\/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n\n\ndef cosine_distance(features1, features2):\n    # print('rows of zeros in features1 = ',sum(np.sum(features1, axis=1) == 0))\n    # print('rows of zeros in features2 = ',sum(np.sum(features2, axis=1) == 0))\n    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n    norm_f1 = normalize_rows(features1_nozero)\n    norm_f2 = normalize_rows(features2_nozero)\n\n    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n    print('d.shape=',d.shape)\n    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n    mean_min_d = np.mean(np.min(d, axis=1))\n    print('distance=',mean_min_d)\n    return mean_min_d\n\n\ndef distance_thresholding(d, eps):\n    if d < eps:\n        return d\n    else:\n        return 1\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n            \n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n             inception net ( like returned by the function 'get_predictions')\n             for generated samples.\n    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n               on an representive data set.\n    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n               generated samples.\n    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n               precalcualted on an representive data set.\n\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n\n    diff = mu1 - mu2\n\n    # product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n        warnings.warn(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        # covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n    \n    # numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(\"Imaginary component {}\".format(m))\n        covmean = covmean.real\n\n    # covmean = tf.linalg.sqrtm(tf.linalg.matmul(sigma1,sigma2))\n\n    print('covmean.shape=',covmean.shape)\n    # tr_covmean = tf.linalg.trace(covmean)\n\n    tr_covmean = np.trace(covmean)\n    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n    # return diff.dot(diff) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean\n#-------------------------------------------------------------------------------\n\n\ndef calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n    \"\"\"Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n                     must lie between 0 and 255.\n    -- sess        : current session\n    -- batch_size  : the images numpy array is split into batches with batch size\n                     batch_size. A reasonable batch size depends on the available hardware.\n    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n                     batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the incption model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the incption model.\n    \"\"\"\n    act = get_activations(images, sess, model_name, batch_size, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma, act\n    \ndef _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n    path = pathlib.Path(path)\n    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n    imsize = model_params[model_name]['imsize']\n    print(imsize)\n    # In production we don't resize input images. This is just for demo purpose. \n    x_t = [np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files]\n    print(len(x_t))\n    x = np.array(x_t)\n    #x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n    \n    m, s, features = calculate_activation_statistics(x, sess, model_name)\n    del x #clean up memory\n    return m, s, features\n\n# check for image size\ndef img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n    im = Image.open(str(filename))\n    if is_checksize and im.size != (check_imsize,check_imsize):\n        raise KernelEvalException('The images are not of size '+str(check_imsize))\n    \n    if is_check_png and im.format != 'PNG':\n        raise KernelEvalException('Only PNG images should be submitted.')\n\n    if resize_to is None:\n        return im\n    else:\n        #print(Image.ANTIALIAS)\n        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n\ndef calculate_kid_given_paths(paths, model_name, model_path, feature_path=None, mm=[], ss=[], ff=[]):\n    ''' Calculates the KID of two paths. '''\n    ops.reset_default_graph()\n    create_model_graph(str(model_path))\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = True)\n        if len(mm) != 0:\n            m2 = mm\n            s2 = ss\n            features2 = ff\n        elif feature_path is None:\n            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n        else:\n            with np.load(feature_path) as f:\n                m2, s2, features2 = f['m'], f['s'], f['features']\n\n        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n        print('starting calculating FID')\n        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n        print('done with FID, starting distance calculation')\n        distance = cosine_distance(features1, features2)        \n        return fid_value, distance, m2, s2, features2","1ca7823c":"# UNCOMPRESS OUR IMGAES\nwith zipfile.ZipFile(\"images2.zip\",\"r\") as z:\n    z.extractall(\"tmp\/images2\/\")","9ead9c3c":"# COMPUTE LB SCORE\nm2 = []; s2 =[]; f2 = []\nuser_images_unzipped_path = 'tmp\/images2\/'\nimages_path = [user_images_unzipped_path,'..\/input\/gangangan\/GANs\/data_homework\/all-dogs\/']\npublic_path = '..\/input\/gangangan\/GANs\/data_homework\/classify_image_graph_def.pb'\n\nfid_epsilon = 10e-15","7a91315e":"%%time\nfid_value_public, distance_public, m2, s2, f2 = calculate_kid_given_paths(images_path, 'Inception', public_path, mm=m2, ss=s2, ff=f2)","bc4a6e24":"distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])\nprint(\"FID_public: \", fid_value_public, \"distance_public: \", distance_public, \"multiplied_public: \",\n        fid_value_public \/(distance_public + fid_epsilon))\n","d2179397":"! rm -r ..\/tmp #\u043e\u0447\u0438\u0441\u0442\u0438\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e \u0434\u0438\u0440\u0435\u0442\u043a\u043e\u0440\u0438\u044e","4c35f983":"\u0412 \u0442\u0438\u043f\u0438\u0447\u043d\u043e\u0439 GAN \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u043d\u0435 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0437\u0430\u0440\u0430\u043d\u0435\u0435. \u0412\u043c\u0435\u0441\u0442\u043e \u044d\u0442\u043e\u0433\u043e \u043e\u043d \u0443\u0447\u0438\u0442\u0441\u044f \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0435\n\u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043e\u0442 \u043f\u043e\u0434\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0441 \u0442\u0435\u043c, \u043a\u0430\u043a \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0443\u0447\u0438\u0442\u0441\u044f \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u043f\u043e\u0434\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n\u0412 \u044d\u0442\u043e\u0439 GAN \u043c\u044b \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u043e\u0431\u0443\u0447\u0438\u043b\u0438 \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440, \u0438 \u0442\u0435\u043f\u0435\u0440\u044c \u043e\u043d \u0431\u0443\u0434\u0435\u0442 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440.","fb4da2b4":"\u0413\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0421\u0435\u0442\u044c \u0442\u0435\u043f\u0435\u0440\u044c \u0438\u0437\u0443\u0447\u0438\u043b\u0430 \u0432\u0441\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435 \u043e\u0431\u0440\u0430\u0437\u044b \u0438\u0437 \u043d\u0430\u0448\u0435\u0439 \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0421\u0435\u0442\u0438.\n\u0421 \u0435\u0433\u043e \u043f\u043b\u043e\u0445\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u044c\u044e \u043d\u0430\u0434\u0435\u0435\u043c\u0441\u044f, \u0447\u0442\u043e \u043e\u043d \u043d\u0430\u0443\u0447\u0438\u043b\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u043e\u0431\u0449\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \n\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043b\u044e\u0431\u043e\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 100-\u043c\u0435\u0440\u043d\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435. \n\u041d\u0430\u0448 \u043a\u043b\u0430\u0441\u0441 \u0432\u0435\u0440\u043d\u0435\u0442 70% \u043e\u0434\u043d\u043e\u0433\u043e \u00ab\u0437\u0430\u043f\u043e\u043c\u043d\u0435\u043d\u043d\u043e\u0433\u043e\u00bb \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0441\u043c\u0435\u0448\u0430\u043d\u043d\u043e\u0433\u043e \u0441 30% \u0434\u0440\u0443\u0433\u043e\u0433\u043e.","6b39f746":"### HomeWork\n    1) \u0417\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 kaggle(\u0435\u0441\u043b\u0438 \u0435\u0449\u0435 \u043d\u0435 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c).\n    2) \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u043d\u0430 kaggle.\n    3) \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 kaggle.\n    4) \u0417\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0441 GPU.\n    5) \u0420\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f \u0441 \u043a\u043e\u0434\u043e\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f GAN.\n    6) \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043e\u0441\u043e\u0437\u043d\u0430\u043d\u043d\u043e\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u0438\u043b\u0438 \u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438.\n       \u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u043b\u044e\u0447\u0435\u0432\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 - \u0441\u0443\u043f\u0435\u0440!\n    7) \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043a\u043e\u043c\u043c\u0438\u0442\n    8) \u0432\u044b\u0441\u043b\u0430\u0442\u044c \u043d\u043e\u0443\u0442\u0431\u0443\u043a","e6ed0ff0":"\u041f\u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0444\u043b\u0430\u0433\u0438","e14e8275":"\u0417\u0434\u0435\u0441\u044c \u043d\u0430\u043c\u0435\u0440\u0435\u043d\u043d\u043e \u0434\u0430\u0435\u0442\u0441\u044f \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0443 \u0443\u0437\u043a\u043e\u0435 \u043c\u0435\u0441\u0442\u043e \u0432 \u0435\u0433\u043e \u043f\u0430\u043c\u044f\u0442\u0438. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u043b\u043e\u0445\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438 \u0432\u044b\u043d\u0443\u0436\u0434\u0430\u0435\u0442 \n\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0443\u0447\u0438\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0438 \u043d\u0435 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e.","53f5ec37":"## \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440","4503a0cf":"\u041d\u0430\u0448 \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0437\u0430\u043f\u043e\u043c\u043d\u0438\u043b \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u0437 train. \u0422\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u0443\u0434\u0430\u043b\u0438\u043c \u0438\u0445 .\n\u041d\u0430\u0448 \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u043d\u0438\u043a\u043e\u0433\u0434\u0430 \u043d\u0435 \u0443\u0432\u0438\u0434\u0438\u0442 \u044d\u0442\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u041e\u043d \u0431\u0443\u0434\u0435\u0442 \u043e\u0431\u0443\u0447\u0435\u043d \u0442\u043e\u043b\u044c\u043a\u043e \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440\u043e\u043c.\n\u041d\u0438\u0436\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u044b \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u043b \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440.","bf7373a3":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c GAN","a083db24":"\u0412\u044b\u0432\u0435\u0434\u0435\u043c \u043e\u0431\u0440\u0435\u0437\u0430\u043d\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f","046108f6":"\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0442\u044c  train data(\u043f\u0435\u0440\u0432\u044b\u0435 10000)","6318f9a2":"\u041e\u0431\u0440\u0435\u0436\u0438\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043f\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0443 https:\/\/www.kaggle.com\/paulorzp\/show-annotations-and-breeds.\n\u0421\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u043e\u0431\u0440\u0435\u0437\u043a\u043e\u0439 \u0432\u044b\u0445\u043e\u0434\u0438\u043b\u043e \u043f\u043e\u0445\u0443\u0436\u0435","d64c6849":"\u041e\u0431\u044c\u044f\u0432\u0438\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u0434\u043b\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439","dc09140e":"\u0410\u0432\u0442\u043e\u0440 kernel https:\/\/www.kaggle.com\/cdeotte\/dog-memorizer-gan\/execution \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442 \u0432\u043c\u0435\u0441\u0442\u043e \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0430 \u0438 \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440\u0430, \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0442\u044c \u0432\u0441\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438. \u0417\u0430\u0442\u0435\u043c \u0414\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0431\u0443\u0434\u0435\u0442 \u0443\u0447\u0438\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440. \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u0443 \u0434\u0430\u0435\u0442\u0441\u044f \u043f\u043b\u043e\u0445\u0430\u044f \u043f\u0430\u043c\u044f\u0442\u044c (\u0441 \u0443\u0437\u043a\u0438\u043c \u043c\u0435\u0441\u0442\u043e\u043c) \u0432 \u043d\u0430\u0434\u0435\u0436\u0434\u0435, \u0447\u0442\u043e \u043e\u043d \u043d\u0430\u0443\u0447\u0438\u0442\u0441\u044f \u043e\u0431\u043e\u0431\u0449\u0430\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438.\n\u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u044e\u0449\u0435\u0433\u043e \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u0430 \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430. (\u041c\u044b \u043d\u0430\u0434\u0435\u0435\u043c\u0441\u044f, \u0447\u0442\u043e \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442 \u0438\u0437-\u0437\u0430 \u043f\u043b\u043e\u0445\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438).","16108411":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0441 \u043e\u0434\u043d\u043e\u0439 \u0441\u0432\u0435\u0440\u0442\u043a\u043e\u0439","9222cd99":"## \u0413\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440","8a4a682f":"https:\/\/towardsdatascience.com\/complete-guide-of-activation-functions-34076e95d044 - \u0437\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0447\u0438\u0442\u0430\u0442\u044c, \u0447\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f elu \u043e\u0442 \u0434\u0440\u0433\u0443\u0438\u0445 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439","e95d15d7":"\u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c seed","3eb6427a":"\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f.\nhttps:\/\/www.kaggle.com\/wendykan\/dog-face-generation-competition-kid-metric-input"}}