{"cell_type":{"e0fdf281":"code","7a1025d0":"code","f376f759":"code","af9ba224":"code","9565db50":"code","ab9db895":"code","cc6a2f47":"code","b41c70b5":"code","8b51af6d":"code","aaa1c9a3":"code","60749c8e":"code","4d7d7bfa":"code","15d64f43":"code","dfd7f7f1":"code","3b04b90f":"code","9384aa08":"code","56a896ad":"code","932be687":"code","251fa058":"code","d243490c":"code","8a417b7f":"code","ac889b1e":"code","daeeaf24":"code","cafd50e0":"code","6d4506ae":"code","1d673ff6":"code","1ee4af67":"code","3037706f":"code","8b9f7850":"code","a575735f":"code","c9414867":"code","826696a7":"code","a49f0767":"markdown","3a9e0e8e":"markdown","93ca7ea7":"markdown","ad24bf22":"markdown","c098c61c":"markdown","c6a871ff":"markdown","5eb8b386":"markdown","6e15b279":"markdown","8c748c68":"markdown","b34174f3":"markdown","04ce0644":"markdown","3ca8315b":"markdown","ba9eccec":"markdown","0fbded98":"markdown"},"source":{"e0fdf281":"# Please turn the 'Internet' toggle On in the Settings panel to your left, in order to make changes to this kernel.\n# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nprint(os.listdir(\"..\/input\/kernel-files\"))\n\n# Set random state for numpy\nnp.random.seed(42)","7a1025d0":"# Read in the dataset as a dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","f376f759":"train.head()","af9ba224":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","9565db50":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","ab9db895":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","cc6a2f47":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","b41c70b5":"# Fill missing values\n# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","8b51af6d":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","aaa1c9a3":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_features = handle_missing(all_features)","60749c8e":"# Let's make sure we handled all the missing values\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","4d7d7bfa":"# Fix skewed features\n# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","15d64f43":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","dfd7f7f1":"# Normalize skewed features\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","3b04b90f":"all_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['HasWoodDeck'] = (all_features['WoodDeckSF'] == 0) * 1\nall_features['HasOpenPorch'] = (all_features['OpenPorchSF'] == 0) * 1\nall_features['HasEnclosedPorch'] = (all_features['EnclosedPorch'] == 0) * 1\nall_features['Has3SsnPorch'] = (all_features['3SsnPorch'] == 0) * 1\nall_features['HasScreenPorch'] = (all_features['ScreenPorch'] == 0) * 1\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\nall_features = all_features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['GarageCars'] = all_features['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nall_features['LotFrontage'] = all_features['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\nall_features['MasVnrArea'] = all_features['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9384aa08":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\nall_features = logs(all_features, log_features)","56a896ad":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\nall_features = squares(all_features, squared_features)","932be687":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","251fa058":"all_features.head()","d243490c":"all_features.shape","8a417b7f":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]","ac889b1e":"X = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, train_labels, test_size=0.15, random_state=42)","daeeaf24":"# Please turn the 'Internet' toggle On in the Settings panel to your left, in order to make changes to this kernel.\n!pip install wandb -q","cafd50e0":"# WandB\nimport wandb\nimport keras\nfrom wandb.keras import WandbCallback\nfrom sklearn.model_selection import cross_val_score\n# Import models (add your models here)\nfrom sklearn import svm\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom xgboost import XGBRegressor","6d4506ae":"# Initialize wandb run\nwandb.init(anonymous='allow', project=\"pick-a-model\")","1d673ff6":"%%wandb\n# Initialize and fit model (add your classifier here)\nsvr = svm.SVR(C= 20, epsilon= 0.008, gamma=0.0003)\nsvr.fit(X_train, y_train)\n\n# Get CV scores\nscores = cross_val_score(svr, X_train, y_train, cv=5)\n\n# Log scores\nfor score in scores:\n    wandb.log({'cross_val_score': score})","1ee4af67":"# Initialize wandb run\nwandb.init(anonymous='allow', project=\"pick-a-model\")","3037706f":"%%wandb\n# Initialize and fit model (add your classifier here)\nxgb = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\nxgb.fit(X_train, y_train)\n\n# Get CV scores\nscores = cross_val_score(xgb, X_train, y_train, cv=3)\n\n# Log scores\nfor score in scores:\n    wandb.log({'cross_val_score': score})","8b9f7850":"# Initialize wandb run\nwandb.init(anonymous='allow', project=\"pick-a-model\")","a575735f":"%%wandb\n# Initialize and fit model (add your classifier here)\nridge = Ridge(alpha=1e-3)\nridge.fit(X_train, y_train)\n\n# Get CV scores\nscores = cross_val_score(ridge, X_train, y_train, cv=5)\n\n# Log scores\nfor score in scores:\n    wandb.log({'cross_val_score': score})","c9414867":"wandb.init(anonymous='allow', project=\"picking-a-model\", name=\"neural_network\")","826696a7":"%%wandb\n# Model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=378, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(20, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\n\n# Compile model\nmodel.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=10, verbose=0,\n        callbacks=[WandbCallback(validation_data=(X_valid, y_valid))])","a49f0767":"# Feature Engineering","3a9e0e8e":"## Model 2 - XGBoost","93ca7ea7":"## Create interesting features","ad24bf22":"## Identify the best performing model\n\nIf you go to the runs page generated by Weights & Biases above, you can find how your model performed. If you click on the name of the project, you can compare all your models' metrics together and pick the best one.\n\nThat\u2019s it now you have all the tools you need to pick the right models for your problem!","c098c61c":"## Model 1 - SVR","c6a871ff":"ML models have trouble recognizing more complex patterns (and we're staying away from neural nets for this competition), so let's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house.","5eb8b386":"## Model 4 - Neural Network","6e15b279":"# Part II - A Whirlwind Tour of Machine Learning Models\nIn Part I, we talked about the part art, part science of picking the perfect machine learning model.\n\nIn Part II, we dive deeper into the different machine learning models you can train and when you should use them!\n\nIn general tree-based models perform best in Kaggle competitions. The other models make great candidates for ensembling. For computer vision challenges, CNNs outperform everything. For natural language processing, LSTMs or GRUs are your best bet!\n\nWith that said, below is a non-exhaustive laundry list of models to try, along with some context for each model.\n\n## Regression\n### Regression \u2192 Linear Regression \u2192 Vanilla Linear Regression\n\nAdvantages\n\n- Captures linear relationships in the dataset well\n- Works well if you have a few well defined variables and need a simple predictive model\n- Fast training speed and prediction speeds\n- Does well on small datasets\n- Interpretable results, easy to explain\n- Easy to update the model when new data comes in\n- No parameter tuning required (the regularized linear models below need to tune the regularization parameter)\n- Doesn't need feature scaling (the regularized linear models below need feature scaling)\n- If dataset has redundant features, linear regression can be unstable\n\nDisadvantages\n\n- Doesn't work well for non-linear data\n- Low(er) prediction accuracy\n- Can overfit (see regularized models below to counteract this)\n- Doesn't separate signal from noise well \u2013 cull irrelevant features before use\n- Doesn't learn feature interactions in the dataset\n        \n### Regression \u2192 Linear Regression \u2192 Lasso, Ridge, Elastic-Net Regression\n\nAdvantages\n\n- These models are linear regression with regularization\n- Help counteract overfitting\n- These models are much better at generalizing because they are simpler\n- They work well when we only care about a few features\n\nDisadvantages\n\n- Need feature scaling\n- Need to tune the regularization parameter\n\n\n### Regression \u2192 Regression Trees \u2192 Decision Tree\n\nAdvantages\n\n- Fast training speed and prediction speeds\n- Captures non-linear relationships in the dataset well\n- Learns feature interactions in the dataset\n- Great when your dataset has outliers\n- Great for finding the most important features in the dataset\n- Doesn't need feature scaling\n- Decently interpretable results, easy to explain\n\nDisadvantages\n\n- Low(er) prediction accuracy\n- Requires some parameter tuning\n- Doesn't do well on small datasets\n- Doesn't separate signal from noise well\n- Not easy to update the model when new data comes in\n- Used very rarely in practice, use ensembled trees instead\n- Can overfit (see ensembled models below)\n\n\n### Regression \u2192 Regression Trees \u2192 Ensembles\n\nAdvantages\n\n- Collates predictions from multiple trees\n- High prediction accuracy - does really well in practice\n- Preferred algorithm in Kaggle competitions\n- Great when your dataset has outliers\n- Captures non-linear relationships in the dataset well\n- Great for finding the most important features in the dataset\n- Separates signal vs noise\n- Doesn't need feature scaling\n- Perform really well on high-dimensional data\n\nDisadvantages\n\n- Slower training speed\n- Fast prediction speed\n- Not easy to interpret or explain\n- Not easy to update the model when new data comes in\n- Requires some parameter tuning - Harder to tune\n- Doesn't do well on small datasets\n\n\n\n### Regression \u2192 Deep Learning\n\nAdvantages\n\n- High prediction accuracy - does really well in practice\n- Captures very complex underlying patterns in the data\n- Does really well with both big datasets and those with high-dimensional data\n- Easy to update the model when new data comes in\n- The network's hidden layers reduce the need for feature engineering remarkably\n- Is state of the art for computer vision, machine translation, sentiment analysis and speech recognition tasks\n\nDisadvantages\n\n- Very long training speed\n- Need a huge amount of computing power\n- Need feature scaling\n- Not easy to explain or interpret results\n- Need lots of training data because it learns a vast number of parameters\n- Outperformed by Boosting algorithms for non-image, non-text, non-speech tasks\n- Very flexible, come with lots of different architecture building blocks, thus require expertise to design the architecture\n\n\n\n### Regression \u2192 K Nearest Neighbors (Distance Based)\n\nAdvantages\n\n- Fast training speed\n- Doesn't need much parameter tuning\n- Interpretable results, easy to explain\n- Works well for small datasets (<100k training set)\n\nDisadvantages\n\n- Low(er) prediction accuracy\n- Doesn't do well on small datasets\n- Need to pick a suitable distance function\n- Needs feature scaling to work well\n- Prediction speed grows with size of dataset\n- Doesn't separate signal from noise well \u2013 cull irrelevant features before use\n- Is memory intensive because it saves every observation\n- Also means they don't work well with high-dimensional data\n2. Classification - Predict a class or class probabilities\n\n## Classification\n### Classification \u2192 Logistic Regression\n\nAdvantages\n\n- Classifies linearly separable data well\n- Fast training speed and prediction speeds\n- Does well on small datasets\n- Decently interpretable results, easy to explain\n- Easy to update the model when new data comes in\n- Can avoid overfitting when regularized\n- Can do both 2 class and multiclass classification\n- No parameter tuning required (except when regularized, we need to tune the regularization parameter)\n- Doesn't need feature scaling (except when regularized)\n- If dataset has redundant features, linear regression can be unstable\n\nDisadvantages\n\n- Doesn't work well for non-linearly separable data\n- Low(er) prediction accuracy\n- Can overfit (see regularized models below)\n- Doesn't separate signal from noise well \u2013 cull irrelevant features before use\n- Doesn't learn feature interactions in the dataset\n\n\n\n### Classification \u2192 Support Vector Machines (Distance based)\n\nAdvantages\n\n- High prediction accuracy\n- Doesn't overfit, even on high-dimensional datasets, so its great for when you have lots of features\n- Works well for small datasets (<100k training set)\n- Work well for text classification problems\n\nDisadvantages\n\n- Not easy to update the model when new data comes in\n- Is very memory intensive\n- Doesn't work well on large datasets\n- Not easy to update the model when new data comes in\n- Requires you choose the right kernel in order to work\n- The linear kernel models linear data and works fast\n- The non-linear kernels can model non-linear boundaries and can be slow\n- Use Boosting instead!\n\n\n\n### Classification \u2192 Naive Bayes (Probability based)\n\nAdvantages\n\n- Performs really well on text classification problems\n- Fast training speed and prediction speeds\n- Does well on small datasets\n- Separates signal from noise well\n- Performs well in practice\n- Simple, easy to implement\n- Works well for small datasets (<100k training set)\n- The naive assumption about the independence of features and their potential distribution lets it avoid overfitting\n- Also if this condition of independence holds, Naive Bayes can work on smaller datasets and can have faster training speed\n- Doesn't need feature scaling\n- Not memory intensive\n- Decently interpretable results, easy to explain\n- Scales well with the size of the dataset\n\nDisadvantages\n\n- Low(er) prediction accuracy\n\n\n\n### Classification \u2192 K Nearest Neighbors (Distance Based)\n\nAdvantages\n\n- Fast training speed\n- Doesn't need much parameter tuning\n- Interpretable results, easy to explain\n- Works well for small datasets (<100k training set)\n\nDisadvantages\n\n- Low(er) prediction accuracy\n- Doesn't do well on small datasets\n- Need to pick a suitable distance function\n- Needs feature scaling to work well\n- Prediction speed grows with size of dataset\n- Doesn't separate signal from noise well \u2013 cull irrelevant features before use\n- Is memory intensive because it saves every observation\n- Also means they don't work well with high-dimensional data\n\n\n\n### Classification \u2192 Classification Tree \u2192 Decision Tree\n\nAdvantages\n\n- Fast training speed and prediction speeds\n- Captures non-linear relationships in the dataset well\n- Learns feature interactions in the dataset\n- Great when your dataset has outliers\n- Great for finding the most important features in the dataset\n- Can do both 2 class and multiclass classification\n- Doesn't need feature scaling\n- Decently interpretable results, easy to explain\n\nDisadvantages\n\n- Low(er) prediction accuracy\n- Requires some parameter tuning\n- Doesn't do well on small datasets\n- Doesn't separate signal from noise well\n- Used very rarely in practice, use ensembled trees instead\n- Not easy to update the model when new data comes in\n- Can overfit (see ensembled models below)\n\n\n### Classification \u2192 Classification Tree \u2192 Ensembles\n\nAdvantages\n\n- Collates predictions from multiple trees\n- High prediction accuracy - does really well in practice\n- Preferred algorithm in Kaggle competitions\n- Captures non-linear relationships in the dataset well\n- Great when your dataset has outliers\n- Great for finding the most important features in the dataset\n- Separates signal vs noise\n- Doesn't need feature scaling\n- Perform really well on high-dimensional data\n\nDisadvantages\n\n- Slower training speed\n- Fast prediction speed\n- Not easy to interpret or explain\n- Not easy to update the model when new data comes in\n- Requires some parameter tuning - Harder to tune\n- Doesn't do well on small datasets\n\n\n\n### Classification \u2192 Deep Learning\n\nAdvantages\n\n- High prediction accuracy - does really well in practice\n- Captures very complex underlying patterns in the data\n- Does really well with both big datasets and those with high-dimensional data\n- Easy to update the model when new data comes in\n- The network's hidden layers reduce the need for feature engineering remarkably\n- Is state of the art for computer vision, machine translation, sentiment analysis and speech recognition tasks\n\nDisadvantages\n\n- Very long training speed\n- Not easy to explain or interpret results\n- Need a huge amount of computing power\n- Need feature scaling\n- Need lots of training data because it learns a vast number of parameters\n- Outperformed by Boosting algorithms for non-image, non-text, non-speech tasks\n- Very flexible, come with lots of different architecture building blocks, thus require expertise to design the architecture\n3. Clustering - Organize the data into groups to maximize similarity\n\n## Clustering \n### Clustering \u2192 DBSCAN\n\nAdvantages\n\n- Scalable to large datasets\n- Detects noise well\n- Don't need to know the number of clusters in advance\n- Doesn't make an assumption that the shape of the cluster is globular\n\nDisadvantages\n\n- Doesn't always work if your entire dataset is densely packed\n- Need to tune the density parameters \u2013 epsilon and min_samples to the right values to get good results\n\n\n### Clustering \u2192 KMeans\nAdvantages\n\n- Great for revealing the structure of the underlying dataset\n- Simple, easy to interpret\n- Works well if you know the number of clusters in advance\nDisadvantages\n\n- Doesn't always work if your clusters aren't globular and similar in size\n- Needs to know the number of clusters in advance - Need to tune the choice of k clusters to get good results\n- Memory intensive\n- Doesn't scale to large datasets\n\n## Misc - Models not included in this post\n- Dimensionality Reduction Algorithms\n- Clustering algorithms - Gaussian Mixture Model and Hierarchical clustering\n- Computer Vision \u2013 Convolutional Neural Networks, Image classification, Object Detection, Image segmentation\n- Natural Language Processing \u2013 RNNs (LSTM or GRUs)\n- Reinforcement Learning\n\n\n## Ensembling Your Models\n\nEnsembling models is a really powerful technique that helps reduce overfitting, and make more robust predictions by combining outputs from different models. It is especially an essential tool for winning Kaggle competitions.\nWhen picking models to ensemble together, we want to pick them from different model classes to ensure they have different strengths and weaknesses and thus capture different patterns in the dataset. This greater diversity leads to lower bias. We also want to make sure their performance is comparable in order to ensure stability of predictions generated.\nWe can see here that the blending these models actually resulted in much lower loss than any single model was able to produce alone. Part of the reason is that while all these models are pretty good at making predictions, they get different predictions right and by combining them together, we're able to combine all their different strengths into a super strong model.\n\n![](https:\/\/paper-attachments.dropbox.com\/s_4F3706A8D77436E0E5FD3135841A28E08D5140BFE82FFC2240B0ABB0C9741645_1568838886109_cv_scores.png)\n\n    # Blend models in order to make the final predictions more robust to overfitting\n    def blended_predictions(X):\n        return ((0.1 * ridge_model_full_data.predict(X)) + \\\\\n                (0.2 * svr_model_full_data.predict(X)) + \\\\\n                (0.1 * gbr_model_full_data.predict(X)) + \\\\\n                (0.1 * xgb_model_full_data.predict(X)) + \\\\\n                (0.1 * lgb_model_full_data.predict(X)) + \\\\\n                (0.05 * rf_model_full_data.predict(X)) + \\\\\n                (0.35 * stack_gen_model.predict(np.array(X))))\n\nThere are 4 types of ensembling (including blending):\n\n- **Bagging:** Train many base models with different randomly chosen subsets of data, with replacement. Let the base models vote on final predictions. Used in RandomForests.\n- **Boosting:** Iteratively train models and update the importance of getting each training example right after each iteration. Used in GradientBoosting.\n- **Blending:** Train many different types of base models and make predictions on a holdout set. Train a new model out of their predictions, to make predictions on the test set. (Stacking with a holdout set).\n- **Stacking:** Train many different types of base models and make predictions on k-folds of the dataset. Train a new model out of their predictions, to make predictions on the test set.\n\n\n---\n\n## If you like this kernel, please give it an upvote. Thank you! :)","8c748c68":"## Now that we have some context, let's get started!\n\nI encourage you to fork this kernel and play with the code!","b34174f3":"## Model 3 - Ridge Regression","04ce0644":"# Picking the right model","3ca8315b":"## Encode categorical features","ba9eccec":"# Part 1: Best Practices for Picking a Machine Learning Model\n\nThe number of shiny models out there can be overwhelming, which means a lot of times people fallback on a few they trust the most, and use them on all new problems. This can lead to sub-optimal results.\n\nToday we're going to learn how to quickly and efficiently narrow down the space of available models to find those that are most likely to perform best on your problem type. We'll also see how we can keep track of our models' performances using Weights and Biases and compare them.\n\n\n## What We'll Cover\n- Model selection in competitive data science vs real world\n- A Royal Rumble of Models\n- Comparing Models\n\nLet's get started!\n\n## If you like this kernel, please give it an upvote. Thank you! :)\n\nUnlike Lord of the Rings, in machine learning there is no one ring (model) to rule them all. Different classes of models are good at modeling the underlying patterns of different types of datasets. For instance, decision trees work well in cases where your data has a complex shape:\n\n![](https:\/\/paper-attachments.dropbox.com\/s_4F3706A8D77436E0E5FD3135841A28E08D5140BFE82FFC2240B0ABB0C9741645_1568921166295_Screenshot+2019-09-19+12.25.23.png)\n\nWhereas linear models work best where the dataset is linearly separable:\n\n![](https:\/\/paper-attachments.dropbox.com\/s_4F3706A8D77436E0E5FD3135841A28E08D5140BFE82FFC2240B0ABB0C9741645_1568921166358_Screenshot+2019-09-19+12.25.35.png)\n\n\nBefore we begin, let\u2019s dive a little deeper into the disparity between model selection in the real world vs for competitive data science.\n\n\n# Model selection in competitive data science vs real world\n\nAs William Vorhies said in his blog post \u201cThe Kaggle competitions are like formula racing for data science. Winners edge out competitors at the fourth decimal place and like Formula 1 race cars, not many of us would mistake them for daily drivers. The amount of time devoted and the sometimes extreme techniques wouldn\u2019t be appropriate in a data science production environment.\u201d\nKaggle models are indeed like racing cars, they're not built for everyday use. Real world production models are more like a Lexus - reliable but not flashy.\nKaggle competitions and the real world optimize for very different things, with some key differences being:\n\n## Problem Definition\n\nThe real world allows you to define your problem and choose the metric that encapsulates the success of your model. This allows you to optimize for a more complex utility function than just a singular metric, where Kaggle competitions come with a single pre-defined metric and don't let you define the problem efficiently.\n\n\n## Metrics\n\nIn the real world we care about inference and training speeds, resource and deployment constraints and other performance metrics, whereas in Kaggle competitions the only thing we care about is the one evaluation metric. Imagine we have a model with 0.98 accuracy that is very resource and time intensive, and another with 0.95 accuracy that is much faster and less compute intensive. In the real world, for a lot of domains we might prefer the 0.95 accuracy model because maybe we care more about the time to inference. In Kaggle competitions, it doesn't matter how long it takes to train the model or how many GPUs it requires, higher accuracy is always better.\n\n\n## Interpretability\n\nSimilarly in the real world, we prefer simpler models that are easier to explain to stakeholders, whereas in Kaggle we pay no heed to model complexity. Model interpretability is important because it allows to take concrete actions to solve the underlying problem. For example, in the real world looking at our model and being able to see a correlation between a feature (e.g. potholes on a street), and the problem (e.g. likelihood of car accident on the street), is more helpful than increasing the prediction accuracy by 0.005%.\n\n\n## Data Quality\n\nFinally in Kaggle competitions, our dataset is collected and wrangled for us. Anyone who's done data science knows that is almost never the case in real life. But being able to collect and structure our data also gives us more control over the data science process.\n\n\n## Incentives\n\nAll this incentivizes a massive amount of time spent tuning our hyperparameters to extract the last drops of performance from our model, and at times convoluted feature engineer methodologies. While Kaggle competitions are an excellent way to learn data science and feature engineering, they don't address real world concerns like model explainability, problem definition, or deployment constraints.\n\n\n# A Royal Rumble of Models\n\nIt\u2019s time to start selecting models!\n\nWhen picking our initial set of models to test, we want to be mindful of a few things:\n\n## Pick a diverse set of initial models\n\nDifferent classes of models are good at modeling different kinds of underlying patterns in data. So a good first step is to quickly test out a few different classes of models to know which ones capture the underlying structure of your dataset most efficiently! Within the realm of our problem type (regression, classification, clustering) we want to try a mixture of tree based, instance based, and kernel based models. Pick a model from each class to test out. We'll talk more about the different model types in the 'models to try' section below.\n\n\n## Try a few different parameters for each model\n\nWhile we don't want to spend too much time finding the optimal set of hyper-parameters, we do want to try a few different combinations of hyper-parameters to allow each model class to have the chance to perform well.\n\n\n## Pick the strongest contenders\n\nWe can use the best performing models from this stage to give us intuition around which class of models we want to further dive into. Your Weights and Biases dashboard will guide you to the class of models that performed best for your problem.\n\n\n## Dive deeper into models in the best performing model classes.\n\nNext we select more models belonging to the best performing classes of models we shortlisted above! For example if linear regression seemed to work best, it might be a good idea to try lasso or ridge regression as well.\n\n\n## Explore the hyper-parameter space in more detail.\n\nAt this stage, I'd encourage you to spend some time tuning the hyper-parameters for your candidate models. (The next post in this series will dive deeper into the intuition around selecting the best hyper-parameters for your models.) At the end of this stage you should have the best performing versions of all your strongest models.\n\n\n## Making the final selection - Kaggle\n\n- **Pick final submissions from diverse models**\nIdeally we want to select the best models from more than one class of models. This is because if you make your selections from just one class of models and it happens to be the wrong one, all your submissions will perform poorly. Kaggle competitions usually allow you to pick more than one entry for your final submission. I'd recommend choosing predictions made by your strongest models from different classes to build some redundancy into your submissions.\n\n- **The leaderboard is not your friend, your cross-validation scores are**\nThe most important thing to remember is that the public leaderboard is not your friend. Picking you models solely based on your public leaderboard scores will lead to overfitting the training dataset. And when the private leaderboard is revealed after the competition ends, sometimes you might see your rank dropping a lot. You can avoid this little pitfall by using cross-validation when training your models. Then pick the models with the best cross-validation scores, instead of the best leaderboard scores. By doing this you counter overfitting by measuring your model's performance against multiple validation sets instead of just the one subset of test data used by the public leaderboard.\n\n\n## Making the final selection - Real world\n\n- **Resource constraints**\nDifferent models hog different types of resources and knowing whether you\u2019re deploying the models on a IoT\/mobile device with a small hard drive and processor or a in cloud can be crucial in picking the right model.\n\n- **Training time vs Prediction time vs Accuracy**\nKnowing what metric(s) you\u2019re optimizing for is also crucial for picking the right model. For instance self driving cars need blazing fast prediction times, whereas fraud detection systems need to quickly update their models to stay up to date with the latest phishing attacks. For other cases like medical diagnosis, we care about the accuracy (or area under the ROC curve) much more than the training times.\n\n- **Complexity vs Explainability Tradeoff**\nMore complex models can use orders of magnitude more features to train and make predictions, require more compute but if trained correctly can capture really interesting patterns in the dataset. This also makes them convoluted and harder to explain though. Knowing how important it is to easily to explain the model to stakeholders vs capturing some really interesting trends while giving up explainability is key to picking a model.\n\n- **Scalability**\nKnowing how fast and how big your model needs to scale can help you narrow down your choices appropriately.\n\n- **Size of training data**\nFor really large datasets or those with many features, neural networks or boosted trees might be an excellent choice. Whereas smaller datasets might be better served by logistic regression, Naive Bayes, or KNNs.\n\n- **Number of parameters**\nModels with a lot of parameters give you lots of flexibility to extract really great performance. However there maybe cases where you don\u2019t have the time required to, for instance, train a neural network's parameters from scratch. A model that works well out of the box would be the way to go in this case!\n\n\n# Comparing Models\n\nWeights and Biases lets you track and compare the performance of you models with one line of code.\nOnce you have selected the models you\u2019d like to try, train them and simply add **wandb.log({'score': cv_score})** to log your model state. Once you\u2019re done training, you can compare your model performances in one easy dashboard!","0fbded98":"## Feature transformations\nLet's create more features by calculating the log and square transformations of our numerical features. We do this manually, because ML models won't be able to reliably tell if log(feature) or feature^2 is a predictor of the SalePrice."}}