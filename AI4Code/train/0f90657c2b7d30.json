{"cell_type":{"6ef3e525":"code","fba5272c":"code","c055b566":"code","2a8ceb01":"code","dd7f54ca":"code","e5a6c300":"code","ce8caa55":"code","426d509b":"code","4e3855e1":"code","c2d1dd48":"code","411194a4":"code","35a4aa66":"code","49f76046":"code","11a1c7b7":"code","aeb3b360":"code","f5bbc0a7":"code","a81367f6":"code","881b7944":"markdown","d8a79a0e":"markdown","7a8c2526":"markdown","1dc783ee":"markdown","e401cc0f":"markdown","8802de6f":"markdown","6edd3a71":"markdown","6eca0f2a":"markdown","5854696c":"markdown","157ce4f6":"markdown","a5b95d12":"markdown","545cb735":"markdown","250f834b":"markdown","19e45ab5":"markdown","1ddbdd0d":"markdown","e19b76ec":"markdown"},"source":{"6ef3e525":"from nltk import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import LazyCorpusLoader, CategorizedPlaintextCorpusReader, stopwords\nfrom sklearn.decomposition import PCA,TruncatedSVD,LatentDirichletAllocation\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing.label import MultiLabelBinarizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy\nimport sklearn\nimport re\nimport nltk\nimport json\nimport os\nimport seaborn as sns\n","fba5272c":"filelist = ['reuters-000.json',\n            'reuters-001.json',\n            'reuters-002.json',\n            'reuters-003.json',\n            'reuters-004.json',\n            'reuters-005.json',\n            'reuters-006.json',\n            'reuters-007.json',\n            'reuters-008.json',\n            'reuters-009.json',\n            'reuters-010.json',\n            'reuters-011.json',\n            'reuters-012.json',\n            'reuters-013.json',\n            'reuters-014.json',\n            'reuters-015.json',\n            'reuters-016.json',\n            'reuters-017.json',\n            'reuters-018.json',\n            'reuters-019.json',\n            'reuters-020.json',\n            'reuters-021.json']\ndata_set = pd.DataFrame()\nfor file in filelist:\n    fileDir = \"\/kaggle\/input\/d\/ukveteran\/reuters\/\" + file\n    df = pd.read_json(fileDir).T\n    data_set = data_set.append(df)\ndata_set = data_set.fillna(' ')","c055b566":"topics = set([x for sublist in data_set[u'topics'].values.tolist() for x in sublist])\nplaces = set([x for sublist in data_set[u'places'].values.tolist() for x in sublist])\norgs = set([x for sublist in data_set[u'organisations'].values.tolist() for x in sublist])\n\ndata_set['topic_count']=data_set[u'topics'].apply(lambda x:len([y for y in x]))\nprint(\"documents with at least one topic = \",len(data_set[data_set[u'topic_count']>0]))\nprint(\"max number of topics in one document = \",data_set[u'topic_count'].max())\nprint('topics = ',len(topics))\nprint('places = ',len(places))\nprint('organizations = ',len(orgs))\n\nplot_data = data_set[u'organisations'].apply(str).value_counts()[1:15]\nplot_data.plot(kind='bar',title=\"Document Count by Organization\")","2a8ceb01":"nltk.download(\"punkt\")\nnltk.download(\"stopwords\",\"data\")\nnltk.data.path.append('data')\nlabelBinarizer = MultiLabelBinarizer()\nstopWords = stopwords.words('english')\ncharfilter = re.compile('[a-zA-Z]+');\ndata_target = labelBinarizer.fit_transform(data_set[u'topics'])\n","dd7f54ca":"def Tokenizer(text):\n    words = map(lambda word: word.lower(), word_tokenize(text))\n    words = [word for word in words if word not in stopWords]\n    tokens = (list(map(lambda token: PorterStemmer().stem(token),words)))\n    ntokens = list(filter(lambda token:charfilter.match(token),tokens))\n    return ntokens\nvector = TfidfVectorizer(tokenizer = Tokenizer,\n                        max_features=1000,\n                        norm='l2')","e5a6c300":"some_topics = [u'cocoa',u'trade',u'money-supply',u'coffee',u'gold']\ndata_set = data_set[data_set[u'topics'].map(set(some_topics).intersection).apply( lambda x: len(x)>0 )]\ndocs = list(data_set[u'body'].values)\ndtm = vector.fit_transform(docs)\nprint(\"Number of documents with my topics = \",len(data_set))","ce8caa55":"choosen  = []\nfor i in range(1,100,5):\n    pca = PCA(n_components = i)\n    pca.fit(dtm.toarray())\n    choosen.append(pca.explained_variance_ratio_.sum())\nplt.plot(range(1,100,5),choosen,\"ro\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Proportion of Explained Variance\")","426d509b":"num_comp = 60\ncolors = np.array(sns.color_palette(\"hls\",120))\npca = PCA(n_components = i)\npca.fit(dtm.toarray())\npca_dtm = pca.transform(dtm.toarray())\ndata_target = labelBinarizer.fit_transform(data_set[u'topics'])\nplt.scatter(pca_dtm[:,0],pca_dtm[:,1],c=colors[data_target.argmax(axis=1).astype(int)])\n\nexplained_variance = pca.explained_variance_ratio_.sum()\nprint(\"Explained variance of the PCA step: {}%\".format(int(explained_variance * 100)))\n\n","4e3855e1":"palette = np.array(sns.color_palette(\"hls\", 5))\n\nmodel = KMeans(n_clusters=5,max_iter=100)\nclustered = model.fit(pca_dtm)\ncentroids = model.cluster_centers_\ny = model.predict(pca_dtm)\n\nax = plt.subplot()\nsc = ax.scatter(pca_dtm[:,0],pca_dtm[:,1],c=palette[y.astype(np.int)])","c2d1dd48":"gold_labels = data_set['topics'].map(set(some_topics).intersection).apply(lambda x: x.pop()).apply(lambda x: some_topics.index(x))\n\nax = plt.subplot()\nsc = ax.scatter(pca_dtm[:,0],pca_dtm[:,1],c=palette[gold_labels])","411194a4":"components = 60\n\npalette = np.array(sns.color_palette(\"hls\", 120))\n\nlsa = TruncatedSVD(n_components=components)\nlsa.fit(dtm)\nlsa_dtm = lsa.transform(dtm)\n\nplt.scatter(lsa_dtm[:,0],lsa_dtm[:,1],c=palette[data_target.argmax(axis=1).astype(int)])\n\nexplained_variance = lsa.explained_variance_ratio_.sum()\nprint(\"Explained variance of the SVD step: {}%\".format(\n    int(explained_variance * 100)))","35a4aa66":"palette = np.array(sns.color_palette(\"hls\", 8))\n\nmodel = KMeans(n_clusters=5,max_iter=100)\nclustered = model.fit(lsa_dtm)\ncentroids = model.cluster_centers_\ny = model.predict(lsa_dtm)\n\nax = plt.subplot()\nsc = ax.scatter(lsa_dtm[:,0],lsa_dtm[:,1],c=palette[y.astype(np.int)])","49f76046":"\ncomponents = 5\nn_top_words = 10\n\npalette = np.array(sns.color_palette(\"hls\", 120))\n    \ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n\nlda = LatentDirichletAllocation(n_components=components,max_iter=5,learning_method='online')\nlda.fit(dtm)\nlda_dtm = lda.transform(dtm)\n\nvec_feature_names = vector.get_feature_names()\nprint_top_words(lda, vec_feature_names, n_top_words)","11a1c7b7":"data_set['label'] = gold_labels\n\nX_train, X_test, y_train, y_test = train_test_split(data_set,gold_labels,test_size=0.2, random_state=10)\nprint(\"Train Set = \",len(X_train))\nprint(\"Test Set  = \",len(X_test))\n\nX_train = X_train[u'body']\nX_test = X_test[u'body']","aeb3b360":"models = [('multinomial_nb', MultinomialNB()),\n          ('log_reg', LogisticRegression()),\n          ('linear_svc', LinearSVC()),\n          ('knn', KNeighborsClassifier(n_neighbors=6)),\n          ('rf', RandomForestClassifier(n_estimators=6))]","f5bbc0a7":"for m_name, model in models:\n    pipeline = Pipeline([('vec', TfidfVectorizer(tokenizer=Tokenizer)),(m_name,model)])\n    pipeline.fit(X_train,y_train)\n    test_y = pipeline.predict(X_test)\n    print(\"model = \",model,\"\\n\")\n    print(classification_report(y_test,test_y,digits=6))","a81367f6":"vectorizer = TfidfVectorizer(tokenizer=Tokenizer)\nmodel = LinearSVC(max_iter = 2000,dual = True)\npipeline = Pipeline([('vec', vectorizer),\n                     ('model', model)])\nparameters = {'vec__ngram_range': ((1, 1), (1, 2)),\n              'vec__max_features': (500, 1000),\n              'model__loss': ('hinge', 'squared_hinge'),\n              'model__C': (1, 0.9)}\ngrid_search = GridSearchCV(pipeline, parameters,verbose=1)\ngrid_search.fit(X_train, y_train)\ntest_y = grid_search.best_estimator_.predict(X_test)\nprint(classification_report(y_test,test_y,digits=6))","881b7944":"### K-Means (K = 5)","d8a79a0e":"#### Training of Models and its Scores","7a8c2526":"## Text Normalization (Text Preprocessing)\n* transform to lower case\n* remove punctuation & numbers\n* stem verbs\n* remove stopwords\n","1dc783ee":"##\u00a0Load Dataset","e401cc0f":"## Text Clustering","8802de6f":"## EDA","6edd3a71":"## LDA ( Latent Dirichlet Allocation)","6eca0f2a":"### Split Data","5854696c":"## Libraries","157ce4f6":"### Hyperparameter Tuning","a5b95d12":"> As it can be seen above, SVM model's performance is better than others. However, we did not make hyperparameter tuning.","545cb735":"### Models","250f834b":"## Text Classification","19e45ab5":"# Topic modeling, Text Classification & Clustering (DL for NLP)","1ddbdd0d":"### Dimensionality Reduction (PCA)","e19b76ec":"## LSA (Latent Semantic Analysis )"}}