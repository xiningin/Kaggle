{"cell_type":{"9dbead22":"code","089aa17d":"code","c9dbb646":"code","99482433":"code","e888a973":"code","df86eb7a":"code","6309a187":"code","ef1d990e":"code","8d63e4ec":"code","24e0bbd8":"code","a05edc55":"code","243f58e2":"code","537f4fcf":"code","69da7271":"code","ff3aa8cd":"code","7fab362c":"code","4f932980":"code","49d31805":"code","8d74fb2d":"code","83e6e002":"code","4bfd953a":"code","8f4cbdf7":"code","989c77dc":"markdown","4cf0f114":"markdown","0e595beb":"markdown","94d0b942":"markdown","6701fbd4":"markdown"},"source":{"9dbead22":"!pip install optuna","089aa17d":"import optuna\noptuna.__version__","c9dbb646":"# coding: utf-8\nimport gc\nimport os\nimport time\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb_origin\nimport optuna.integration.lightgbm as lgb\nfrom pathlib import Path\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","99482433":"input_path = Path(\"..\/input\")\ntrain_path = input_path \/ \"train.csv\"\ntest_path = input_path \/ \"test.csv\"\nhistorical_tr_path = input_path \/ \"historical_transactions.csv\"\nnew_merchant_tr_path = input_path \/ \"new_merchant_transactions.csv\"","e888a973":"train = pd.read_csv(train_path, parse_dates=[\"first_active_month\"])\ntest = pd.read_csv(test_path, parse_dates=[\"first_active_month\"])\nnew_merchant = pd.read_csv(new_merchant_tr_path, parse_dates=['purchase_date'])\nht = pd.read_csv(historical_tr_path, parse_dates=['purchase_date'])","df86eb7a":"for df in [train,test]:\n    df['start_year'] = df['first_active_month'].dt.year\n    df['start_month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n\nytrain = train['target']\ndel train['target']","6309a187":"# binarize the categorical variables where it makes sense\nht['authorized_flag'] = ht['authorized_flag'].map({'Y':1, 'N':0})\nht['category_1'] = ht['category_1'].map({'Y':1, 'N':0})","ef1d990e":"ht['category_2x1'] = (ht['category_2'] == 1) + 0\nht['category_2x2'] = (ht['category_2'] == 2) + 0\nht['category_2x3'] = (ht['category_2'] == 3) + 0\nht['category_2x4'] = (ht['category_2'] == 4) + 0\nht['category_2x5'] = (ht['category_2'] == 5) + 0","8d63e4ec":"ht['category_3A'] = (ht['category_3'].astype(str) == 'A') + 0\nht['category_3B'] = (ht['category_3'].astype(str) == 'B') + 0\nht['category_3C'] = (ht['category_3'].astype(str) == 'C') + 0","24e0bbd8":"def aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2': ['nunique'],\n        'category_3A': ['sum'],\n        'category_3B': ['sum'],\n        'category_3C': ['sum'],\n        'category_2x1': ['sum','mean'],\n        'category_2x2': ['sum','mean'],\n        'category_2x3': ['sum','mean'],\n        'category_2x4': ['sum','mean'],\n        'category_2x5': ['sum','mean'],        \n        'city_id': ['nunique'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'merchant_category_id': ['nunique'],\n        'merchant_id': ['nunique'],\n        'month_lag': ['min', 'max'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(ht)\n\ndel ht\ngc.collect()","a05edc55":"new_merchant['authorized_flag'] = new_merchant['authorized_flag'].map({'Y':1, 'N':0})\nnew_merchant['category_1'] = new_merchant['category_1'].map({'Y':1, 'N':0})\nnew_merchant['category_3A'] = (new_merchant['category_3'].astype(str) == 'A') + 0\nnew_merchant['category_3B'] = (new_merchant['category_3'].astype(str) == 'B') + 0\nnew_merchant['category_3C'] = (new_merchant['category_3'].astype(str) == 'C') + 0\n\nnew_merchant['category_2x1'] = (new_merchant['category_2'] == 1) + 0\nnew_merchant['category_2x2'] = (new_merchant['category_2'] == 2) + 0\nnew_merchant['category_2x3'] = (new_merchant['category_2'] == 3) + 0\nnew_merchant['category_2x4'] = (new_merchant['category_2'] == 4) + 0\nnew_merchant['category_2x5'] = (new_merchant['category_2'] == 5) + 0","243f58e2":"new_merchant['purchase_date'] = pd.DatetimeIndex(new_merchant['purchase_date']).\\\n                                      astype(np.int64) * 1e-9","537f4fcf":"def aggregate_new_transactions(new_trans):    \n    \n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2': ['nunique'],\n        'category_3A': ['sum'],\n        'category_3B': ['sum'],\n        'category_3C': ['sum'],     \n        'category_2x1': ['sum','mean'],\n        'category_2x2': ['sum','mean'],\n        'category_2x3': ['sum','mean'],\n        'category_2x4': ['sum','mean'],\n        'category_2x5': ['sum','mean'],        \n\n        'city_id': ['nunique'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'merchant_category_id': ['nunique'],\n        'merchant_id': ['nunique'],\n        'month_lag': ['min', 'max'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique']        \n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_merchant)\n\ndel new_merchant","69da7271":"print(train.shape)\nprint(test.shape)\n\nxtrain = pd.merge(train, new_trans, on='card_id', how='left')\nxtest = pd.merge(test, new_trans, on='card_id', how='left')\n\ndel new_trans\n\nprint(xtrain.shape)\nprint(xtest.shape)\n\nxtrain = pd.merge(xtrain, history, on='card_id', how='left')\nxtest = pd.merge(xtest, history, on='card_id', how='left')\n\ndel history\n\nprint(xtrain.shape)\nprint(xtest.shape)\n","ff3aa8cd":"xtrain.head(3)","7fab362c":"xtrain.drop('first_active_month', axis = 1, inplace = True)\nxtest.drop('first_active_month', axis = 1, inplace = True)","4f932980":"categorical_feats = ['feature_1', 'feature_2', 'feature_3']\n\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(xtrain[col].values.astype('str')) + list(xtest[col].values.astype('str')))\n    xtrain[col] = lbl.transform(list(xtrain[col].values.astype('str')))\n    xtest[col] = lbl.transform(list(xtest[col].values.astype('str')))","49d31805":"df_all = pd.concat([xtrain, xtest])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = xtrain.shape[0]\n\nxtrain = df_all[:len_train]\nxtest = df_all[len_train:]","8d74fb2d":"# prepare for modeling\nid_train = xtrain['card_id'].copy(); xtrain.drop('card_id', axis = 1, inplace = True)\nid_test = xtest['card_id'].copy(); xtest.drop('card_id', axis = 1, inplace = True)","83e6e002":"n_splits = 5\nfolds = KFold(n_splits=n_splits, shuffle=True, random_state=15)\noof = np.zeros(len(xtrain))\npredictions = np.zeros(len(xtest))","4bfd953a":"start = time.time()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(xtrain.values, ytrain.values)):\n    print('Fold {}\/{}'.format(fold_ + 1, folds.n_splits))\n    \n    x0,y0 = xtrain.iloc[trn_idx], ytrain[trn_idx]\n    x1,y1 = xtrain.iloc[val_idx], ytrain[val_idx]\n    \n    trn_data = lgb.Dataset(x0, label= y0)\n    val_data = lgb.Dataset(x1, label= y1)\n    \n     # LightGBMTuner\n    # Reference: https:\/\/gist.github.com\/smly\/367c53e855cdaeea35736f32876b7416\n    best_params = {}\n    tuning_history = []\n\n    params = {\n                'objective': 'regression',\n                'metric': 'rmse',\n            }\n    \n    num_round = 10000\n    lgb.train(\n        params,\n        trn_data,\n        num_boost_round=num_round,\n        valid_sets=[trn_data, val_data],\n        early_stopping_rounds=100,\n        verbose_eval=500,\n        best_params=best_params,\n        tuning_history=tuning_history)\n    \n    pd.DataFrame(tuning_history).to_csv('.\/tuning_history_{}.csv'.format(fold_ + 1))\n    best_params['learning_rate'] = 0.05\n    \n    # origin LightGBM Model\n    clf = lgb_origin.train(\n                    best_params,\n                    trn_data,\n                    num_boost_round=num_round * 2,\n                    valid_names=['train', 'valid'],\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds=1000)\n    \n    oof[val_idx] = clf.predict(xtrain.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    predictions += clf.predict(xtest, num_iteration=clf.best_iteration) \/ folds.n_splits\n    print(\"Fold {} CV score: {:<8.5f}\".format(fold_ + 1, mean_squared_error(ytrain[val_idx],\n                                                                            oof[val_idx])**0.5))\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, ytrain)**0.5))","8f4cbdf7":"xsub = pd.DataFrame()\nxsub['card_id']  = id_test\nxsub['target'] = predictions\nxsub.to_csv('sub_lgb.csv', index = False)","989c77dc":"# Historical data","4cf0f114":"# Model","0e595beb":"# Combine","94d0b942":"#New data","6701fbd4":"# 1. Loading the data"}}