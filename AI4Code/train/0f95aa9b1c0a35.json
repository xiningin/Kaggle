{"cell_type":{"57f8eef5":"code","d2c1e851":"code","11e3466f":"code","c0fe5e50":"code","43e1f5e0":"code","45f8f21e":"code","055d78e0":"code","cc213785":"code","cca24eff":"code","5de7f94e":"code","319d6877":"code","e3284ebd":"code","d8670dcb":"code","6b76ec9e":"code","26943234":"code","10b4b06b":"code","e6dbf776":"code","eccc2494":"code","326504c0":"code","2c3829cb":"code","45c24a6e":"code","436b7598":"code","3122d1ae":"code","efdcf92f":"code","584aad6c":"code","83b3aca6":"code","72f27401":"code","06a0f225":"code","9a50bb4a":"code","e2dd9a94":"code","85851ac0":"code","b87c1ec7":"code","e9fe54b5":"code","85790aeb":"code","ce8d6131":"code","3c7929d5":"code","7e45e45f":"code","9e8d7273":"code","6c831f5d":"code","f2e968d3":"code","0b5d17ac":"code","ffad9e72":"markdown","e4de620e":"markdown","1dcf8209":"markdown","8aeb0719":"markdown","c02a837a":"markdown"},"source":{"57f8eef5":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\ndevice = torch.device(\"cuda\")","d2c1e851":"import pandas as pd","11e3466f":"!pip install transformers\nfrom transformers import AutoModel, AutoTokenizer\n\n\n","c0fe5e50":"!pip install openpyxl","43e1f5e0":"!pip install wandb","45f8f21e":"import wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"351cc1ebc0d966d49152a4c1937915dd4e7b4ef5\"\n\nwandb.login(key=\"351cc1ebc0d966d49152a4c1937915dd4e7b4ef5\")\n","055d78e0":"wandb.init(project = \"Sentiment Analysis Multi Label\")","cc213785":"\n\npath_dataset = \"\/kaggle\/input\/dataset1312\/Dataset_13_12.xlsx\"\ndataframe = pd.read_excel(path_dataset, sheet_name = 'Dataset')\n","cca24eff":"dataframe.head()","5de7f94e":"l_review = dataframe['Review']\nl_multilabel = []\nfor index, row in dataframe.iterrows():\n    label_ship = row[\"Label_Ship\"]\n    label_product = row[\"Label_SP\"]\n    l_multilabel.append([label_ship, label_product])","319d6877":"\n\ntrain_text, val_text, train_label, val_label= train_test_split(l_review,\n                                                                    l_multilabel,\n                                                                    random_state=2021, \n                                                                    test_size=0.1, \n                                                                    stratify=l_multilabel)\n\n\n","e3284ebd":"from torch.utils.data import Dataset, DataLoader","d8670dcb":"class SentimentDataset(Dataset):\n    def __init__(self, texts, targets):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai\/phobert-base\", use_fast=False)\n        self.max_len = 200\n        \n    def __len__(self):\n        return len(self.texts)\n\n    \n    def __getitem__(self, index):\n        target = self.targets[index]\n        text = self.texts[index]\n        \n        inputs = self.tokenizer.encode_plus(text,\n                                            None,\n                                            add_special_tokens=True,\n                                            max_length=self.max_len,\n                                            padding=\"max_length\",\n                                            truncation=True)\n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"targets\": torch.tensor(target, dtype=torch.long),\n        }","6b76ec9e":"train_dataset = SentimentDataset(train_text.to_list(), train_label)\nval_dataset = SentimentDataset(val_text.to_list(), val_label)","26943234":"dataloader_train = DataLoader(train_dataset, batch_size=16,\n                        shuffle=True, num_workers=0, pin_memory = True)\ndataloader_val = DataLoader(val_dataset, batch_size=16,\n                        shuffle=True, num_workers=0, pin_memory = True)","10b4b06b":"print(len(dataloader_val))","e6dbf776":"# for sample_batched in dataloader_train:\n#     print(sample_batched)\n#     break","eccc2494":"# print(features.pooler_output)","326504c0":"class PhoBert_Classification(torch.nn.Module):\n    def __init__(self, num_class):\n        super(PhoBert_Classification, self).__init__()\n        self.backbone = AutoModel.from_pretrained(\"vinai\/phobert-base\")\n        \n        self.dense_1 = torch.nn.Linear(in_features = 768, out_features = 128, bias=True)\n        self.dense_2 = torch.nn.Linear(in_features = 128, out_features = num_class, bias=True)\n        self.dropout1 = nn.Dropout(0.5)\n        self.relu =  nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        #softmax activation function (Log softmax)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, sent_id,mask):\n\n        #get pooler_output of ['CLS'] token from bert output\n        cls_hs= self.backbone(sent_id, attention_mask=mask).pooler_output\n        x = self.dropout1(cls_hs)\n        x = self.dense_1(x)\n\n        x = self.relu(x)\n\n        x = self.dropout2(x)\n\n        # output layer\n        x = self.dense_2(x)\n\n        x = self.sigmoid(x)\n\n        return x\n","2c3829cb":"model = PhoBert_Classification(2)\n","45c24a6e":"# seq_len = [len(i.split()) for i in train_text]\n# pd.Series(seq_len).hist(bins = 30)","436b7598":"# # tokenize and encode sequences in the training set\n# MAX_LENGTH = 200\n# tokens_train = tokenizer.batch_encode_plus(\n#     train_text.tolist(),\n#     max_length = MAX_LENGTH,\n#     pad_to_max_length=True,\n#     truncation=True\n# )\n\n# # tokenize and encode sequences in the validation set\n# tokens_val = tokenizer.batch_encode_plus(\n#     val_text.tolist(),\n#     max_length = MAX_LENGTH,\n#     pad_to_max_length=True,\n#     truncation=True\n# )\n\n# # # tokenize and encode sequences in the test set\n# # tokens_test = tokenizer.batch_encode_plus(\n# #     test_text.tolist(),\n# #     max_length = MAX_LENGTH,\n# #     pad_to_max_length=True,\n# #     truncation=True\n# # )","3122d1ae":"# train_seq = torch.tensor(tokens_train['input_ids'])\n# train_mask = torch.tensor(tokens_train['attention_mask'])\n# train_y = torch.tensor(train_labels.tolist())\n\n# val_seq = torch.tensor(tokens_val['input_ids'])\n# val_mask = torch.tensor(tokens_val['attention_mask'])\n# val_y = torch.tensor(val_labels.tolist())\n\n# # test_seq = torch.tensor(tokens_test['input_ids'])\n# # test_mask = torch.tensor(tokens_test['attention_mask'])\n# # test_y = torch.tensor(test_labels.tolist())","efdcf92f":"# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# batch_size = 32\n\n# train_data = TensorDataset(train_seq, train_mask, train_y)\n\n# train_sampler = RandomSampler(train_data)\n\n# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# val_data = TensorDataset(val_seq, val_mask, val_y)\n\n# val_sampler = SequentialSampler(val_data)\n\n# val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","584aad6c":"# test_tensordata = TensorDataset(test_seq, test_mask, test_y)\n# test_sampler =  SequentialSampler(test_tensordata)\n# test_dataloader = DataLoader(test_tensordata, sampler = test_sampler, batch_size=1)","83b3aca6":"device = torch.device(\"cuda\")\nmodel = model.to(device)","72f27401":"from transformers import AdamW\noptimizer = AdamW(model.parameters(), lr = 1e-5) ","06a0f225":"train_label_shipping = [item[0] for item in train_label]\ntrain_label_product = [item[1] for item in train_label]","9a50bb4a":"from sklearn.utils.class_weight import compute_class_weight\n#Class weight for Shipping\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_label_shipping), train_label_shipping)\n\nprint(\"Class Weights:\",class_weights)","e2dd9a94":"# from sklearn.utils.class_weight import compute_class_weight\n# #Class weight for Shipping\n# #compute the class weights\n# class_weights = compute_class_weight('balanced', np.unique(train_label_product), train_label_product)\n\n# print(\"Class Weights:\",class_weights)","85851ac0":"# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\nloss_bce  = nn.BCELoss(weight = weights) \n\n# number of training epochs\nepochs = 240","b87c1ec7":"# for step,batch in enumerate(dataloader_train):\n#     sent_id = batch[\"ids\"].to(device)\n#     mask = batch[\"mask\"].to(device)\n#     targets = batch[\"targets\"].to(device)\n#     print(targets)\n#     targets = targets.to(torch.float32)\n#     preds = model(sent_id, mask)\n#     loss = loss_bce(preds, targets)\n#     print(preds)\n#     print(loss)\n#     break","e9fe54b5":"def train(model):\n    model.train()\n    total_loss, total_accuracy = 0, 0\n    total_preds=[]\n    for step,batch in enumerate(dataloader_train):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader_train)))\n        sent_id = batch[\"ids\"].to(device)\n        mask = batch[\"mask\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        targets = targets.to(torch.float32)\n\n        model.zero_grad() \n\n        preds = model(sent_id, mask)\n\n        loss = loss_bce(preds, targets)\n        total_loss = total_loss + loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        preds=preds.detach().cpu().numpy()\n        total_preds.append(preds)\n\n    avg_loss = total_loss \/ len(dataloader_train)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","85790aeb":"from sklearn.metrics import precision_score, \\\n    recall_score, confusion_matrix, classification_report, \\\n    accuracy_score, f1_score\n\n","ce8d6131":"from sklearn import metrics","3c7929d5":"def evaluate(model, dataloader_val):\n  \n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_groundtruth = []\n    l_class = [\"Shipping\", \"Product\"]\n\n    # iterate over batches\n    for step,batch in enumerate(dataloader_val):\n        sent_id = batch[\"ids\"].to(device)\n        mask = batch[\"mask\"].to(device)\n        targets = batch[\"targets\"].to(device)\n        targets = targets.to(torch.float32)\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = loss_bce(preds,targets)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n            \n            out_labels = targets.detach().cpu().numpy()\n            total_groundtruth.append(out_labels)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(dataloader_val) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n    \n    total_preds = np.array(total_preds, dtype = np.float64)\n    total_groundtruth = np.concatenate(total_groundtruth, axis = 0)\n    total_groundtruth = np.array(total_groundtruth, dtype = np.int64)\n    roc_metrics = metrics.roc_auc_score(total_groundtruth, total_preds, average = None)\n    print(\"ROC AUC score in class Shipping: \", roc_metrics[0])\n    print(\"ROC AUC score in class Product: \", roc_metrics[1])\n    \n    average_auc_score = np.mean(roc_metrics)\n    print(\"Average AUC score: \", average_auc_score)\n    return average_auc_score, avg_loss","7e45e45f":"# set initial loss to infinite\nbest_valid_loss = float('inf')\nbest_valid_auc = 0\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\nwarmup_nepochs = 10\nfinetune_nepochs = 150\nfor param in model.backbone.parameters():\n    param.requires_grad = False\n        \n#for each epoch\nfor epoch in range(warmup_nepochs):\n    print(\"Start\")\n    print('\\n Warmup Epoch {:} \/ {:}'.format(epoch + 1, warmup_nepochs))\n    \n    #train model\n    train_loss, _ = train(model)\n    wandb.log({\"Loss train\": train_loss})\n    \n#     evaluate model\n    auc_micro, avg_loss = evaluate(model, dataloader_val)\n    wandb.log({\"Loss val\": avg_loss})\n    wandb.log({\"AUC Score\": auc_micro})\n    if auc_micro > best_valid_auc:\n        best_valid_auc = auc_micro\n        torch.save(model.state_dict(), '\/kaggle\/working\/Best_weights_AUC.pt')\n    torch.save(model.state_dict(), '\/kaggle\/working\/Lass_weights_f1.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(avg_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {avg_loss:.3f}')\n    \n\nfor param in model.backbone.parameters():\n    param.requires_grad = True\n        \nfor epoch in range(finetune_nepochs):\n    print(\"Start\")\n    print('\\n FineTune Epoch {:} \/ {:}'.format(epoch + 1, finetune_nepochs))\n\n    #train model\n    train_loss, _ = train(model)\n    wandb.log({\"Loss train\": train_loss})\n#     evaluate model\n    auc_micro, avg_loss = evaluate(model, dataloader_val)\n    wandb.log({\"Loss val\": avg_loss})\n    wandb.log({\"AUC Micro\": auc_micro})\n    #save the best model\n    if auc_micro > best_valid_auc:\n        best_valid_auc = auc_micro\n        torch.save(model.state_dict(), '\/kaggle\/working\/Best_weights_AUC.pt')\n    torch.save(model.state_dict(), '\/kaggle\/working\/Lass_weights_f1.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(avg_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {avg_loss:.3f}')","9e8d7273":"# best_model = BERT_sentiment_analysis(bert)\n# best_model.load_state_dict(torch.load(\"..\/input\/weight-model\/Best_weights_f1.pt\",map_location=device))\n# best_model = best_model.to(device)","6c831f5d":"# _ = evaluate(best_model, test_dataloader)","f2e968d3":"# def inference(model, string_input):\n#     model.eval()\n    \n#     tokens_inference = tokenizer.batch_encode_plus(\n#         [string_input],\n#         max_length = MAX_LENGTH,\n#         pad_to_max_length=True,\n#         truncation=True\n#         )\n#     inference_seq = torch.tensor(tokens_inference['input_ids'])\n#     inference_mask = torch.tensor(tokens_inference['attention_mask'])\n#     sent_id = inference_seq.to(device)\n#     mask = inference_mask.to(device)\n#     preds = model(sent_id, mask)\n#     preds = preds.detach().cpu().numpy()\n#     class_predict = np.argmax(preds, axis = 1)\n#     print(\"Class predict: \",class_predict[0])\n\n    ","0b5d17ac":"# string_input = input(\"Enter your string: \")\n# inference(best_model, string_input)","ffad9e72":"# **Build model with backbone and pretrained Bert base uncased**","e4de620e":"# **Split stratify Dataset**","1dcf8209":"# **Input and inference a sentence**","8aeb0719":"# **Training and Validation**","c02a837a":"# **Eval in test dataset**"}}