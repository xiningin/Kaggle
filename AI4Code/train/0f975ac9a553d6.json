{"cell_type":{"d8cda0c6":"code","940f000d":"code","976756ac":"code","057eef59":"code","168dbd29":"code","a892638e":"code","f12287c4":"code","9d562fcf":"code","6b7d6572":"code","5c49b8ff":"code","01ac70e4":"code","af4d4b7e":"code","1bc2b617":"code","40e6b7c3":"code","21d22e67":"code","ae5d5092":"code","15455442":"code","f2fc52c8":"code","d805c473":"code","8bdd3df4":"code","99db7013":"code","dd96e0eb":"code","74f4c0a0":"markdown","1250168d":"markdown","1c57b1fc":"markdown","e0f2b3db":"markdown","3ff3fe66":"markdown","978ea603":"markdown","0cb8e15a":"markdown","20693545":"markdown","ac4b05a1":"markdown","60ec4931":"markdown","d1b45b66":"markdown","5dfe4eec":"markdown","89dd1530":"markdown","6f7948e6":"markdown"},"source":{"d8cda0c6":"## with internet \n!pip install timm -q\n!pip install adamp -q\n!pip install bottleneck-transformer-pytorch\n!pip install torch-summary -q\n\n# ## w\/o internet \n# import os, sys\n# sys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master') # v0.4.7\n# sys.path.append('..\/input\/bottleneck-transformers-pytorch')","940f000d":"import os, sys, gc\nimport cv2\nimport copy\nimport time\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nimport torchvision\nfrom torchvision import models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda import amp\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.utils import class_weight\n\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom adamp import AdamP\nimport bottleneck_transformer_pytorch\nfrom bottleneck_transformer_pytorch import BottleStack\nfrom torchsummary import summary\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nprint('Timm version:', timm.__version__)\nprint('Torch version:', torch.__version__)","976756ac":"class CFG:\n    model_name = 'resnet18'   #    # [ resnet50, resnet101, 'seresnext50_32x4d', ... ]\n    img_size = (256, 256)            # (224, 224); (256, 819)  \n\n    scheduler = 'CosineAnnealingLR'   # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    num_epochs = 10    # 3    \n    batch_size = 128   # 64   # 12 - 16 for 512 \n\n    lr = 1e-4\n    min_lr = 1e-6\n    weight_decay = 1e-6\n    num_classes = 1\n    smoothing = 0.2\n            \n    apex=False\n    debug=False\n    train=True\n    n_fold = 4\n    trn_fold=[0]    # [0, 1, 2, 3]\n    print_freq=1500 # 100-500   ## --> reduce to see more often\n    num_workers=4\n    seed = 2020\n    T_max = 12   # CosineAnnealingLR\n    T_0 = 12     # CosineAnnealingWarmRestarts\n    factor=0.2   # ReduceLROnPlateau\n    patience=4   # ReduceLROnPlateau\n    eps=1e-6     # ReduceLROnPlateau\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    target_col='target'\n    ","057eef59":"# ====================================================\n# Utils\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\n\ndef init_logger(log_file='train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\n\n# Helpers \ndef get_score(y_true, y_pred):\n    return roc_auc_score(y_true, y_pred)\n\ndef get_result(result_df):\n    preds = result_df['preds'].values\n    labels = result_df[CFG.target_col].values\n    score = get_score(labels, preds)\n    LOGGER.info(f'AUC Score: {score:<.4f}')","168dbd29":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nset_seed(CFG.seed)\nLOGGER = init_logger()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","a892638e":"ROOT_DIR  = '..\/input\/seti-breakthrough-listen'          # \"..\/input\/cassava-leaf-disease-classification\"\nTRAIN_DIR = '..\/input\/seti-breakthrough-listen\/train'    # \"..\/input\/cassava-leaf-disease-classification\/train_images\"\nTEST_DIR  = '..\/input\/seti-breakthrough-listen\/test'     # \"..\/input\/cassava-leaf-disease-classification\/test_images\"","f12287c4":"train = pd.read_csv(f'{ROOT_DIR}\/train_labels.csv')\ntest = pd.read_csv(f'{ROOT_DIR}\/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    return \"..\/input\/seti-breakthrough-listen\/train\/{}\/{}.npy\".format(image_id[0], image_id)\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/seti-breakthrough-listen\/test\/{}\/{}.npy\".format(image_id[0], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\ntest['file_path'] = test['id'].apply(get_test_file_path)\n\n# display(train.head())\n# display(test.head())","9d562fcf":"skf = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\ndf_folds = train.copy()\nfor n, (train_index, val_index) in enumerate(skf.split(train, train[CFG.target_col])):\n    df_folds.loc[val_index, 'fold'] = int(n)\ndf_folds['fold'] = df_folds['fold'].astype(int)\n\nprint(f\"Split data into {CFG.n_fold} folds\\n\")\ndisplay(df_folds.groupby(['fold', 'target']).size())","6b7d6572":"# ====================================================\n# Dataset\n# ====================================================\nclass SetiDataset(Dataset):\n    def __init__(self, df, transform=None, ):  # n_channels=1\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df[CFG.target_col].values\n        self.transform = transform\n#         self.n_channels = n_channels\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        image = np.load(file_path)\n        image = image.astype(np.float32)  # (6, 273, 256)\n        #  image = np.vstack(image).transpose((1, 0))  \n        \n        #         # 3-channel image (only targets - A)\n        #         image = np.stack([\n        #             image[0,:,:],\n        #             image[2,:,:],\n        #             image[4,:,:] ])               # (3, 273, 256)\n        #         image = image.transpose((1,2,0))  # (273, 256, 3)\n        \n        # 1-channel image (only targets - A)\n        image = image[[0, 2, 4]]             # shape: (3, 273, 256)\n        image = np.vstack(image)             # shape: (819, 256)\n        image = image.transpose(1, 0)        # shape: (256, 819)\n        image = image.astype(\"float\")[..., np.newaxis]  # shape: (256, 819, 1)\n\n        if self.transform:\n            image = self.transform(image=image)['image']\n            image = image.transpose((2,1,0))      \n        \n        image = torch.from_numpy(image).float()\n        label = torch.tensor(self.labels[idx]).float()\n        return image, label\n    \n    \n#     def _read_cadence_array(self, path: Path):\n#         \"\"\"Read cadence file and reshape\"\"\"\n#         img = np.load(path)[[0, 2, 4]]  # shape: (3, 273, 256)\n#         img = np.vstack(img)            # shape: (819, 256)\n#         img = img.transpose(1, 0)       # shape: (256, 819)\n#         img = img.astype(\"float\")[..., np.newaxis]  # shape: (256, 819, 1)\n#         return img    \n    \n# ## WiP - select channel configuration\n# ds = SetiDataset(df_folds.sample(1000), transform=get_transforms(data='train'))\n# img, lab = ds[0]\n# img.shape, lab.shape","5c49b8ff":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            A.Resize(CFG.img_size[0], CFG.img_size[1]),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=60, p=0.5),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            A.Resize(CFG.img_size[0], CFG.img_size[1]),\n        ])\n    \n### extra aug\n# A.HueSaturationValue(\n#                 hue_shift_limit=0.2, \n#                 sat_shift_limit=0.2, \n#                 val_shift_limit=0.2, \n#                 p=0.5 ),\n# A.RandomBrightnessContrast(\n#                 brightness_limit=(-0.1,0.1), \n#                 contrast_limit=(-0.1, 0.1), \n#                 p=0.5  ),","01ac70e4":"class SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] \/ (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p][\"e_w\"] = e_w\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        p.grad.norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm","af4d4b7e":"# Helper: Converts the activation function for the entire network\n\ndef convert_act_cls(model, layer_type_old, layer_type_new):\n    conversion_count = 0\n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            # recurse\n            model._modules[name] = convert_act_cls(module, layer_type_old, layer_type_new)\n        if type(module) == layer_type_old:\n            layer_old = module\n            layer_new = layer_type_new\n            model._modules[name] = layer_new\n    return model","1bc2b617":"from bottleneck_transformer_pytorch import BottleStack\n\nbot_layer_1 = BottleStack(\n    dim = 256,      #1024,              # channels in\n    fmap_size = 16, # 14         # feature map size # need to adjust depend on image size\/depth\n    dim_out = 512, # 2048,         # channels out\n    proj_factor = 4,        # projection factor\n    downsample = True,      # downsample on first layer or not\n    heads = 4,              # number of heads\n    dim_head = 128,         # dimension per head, defaults to 128\n    rel_pos_emb = True,     # use relative positional embedding - uses absolute if False\n    activation = nn.SiLU()  # activation throughout the network\n)\n\nbot_layer_2 = BottleStack(\n    dim = 2048,              # channels in\n    fmap_size = 16,         # feature map size\n    dim_out = 2048,         # channels out\n    proj_factor = 4,        # projection factor\n    downsample = True,      # downsample on first layer or not\n    heads = 4,              # number of heads\n    dim_head = 128,         # dimension per head, defaults to 128\n    rel_pos_emb = True,     # use relative positional embedding - uses absolute if False\n    activation = nn.SiLU()  # activation throughout the network\n)\n\nbot_layer_3 = BottleStack(\n    dim = 2048,             # channels in\n    fmap_size = 8,          # feature map size\n    dim_out = 2048,         # channels out\n    proj_factor = 4,        # projection factor\n    downsample = True,      # downsample on first layer or not\n    heads = 4,              # number of heads\n    dim_head = 128,         # dimension per head, defaults to 128\n    rel_pos_emb = True,     # use relative positional embedding - uses absolute if False\n    activation = nn.SiLU()  # activation throughout the network\n)\n\n\n## stack of BoT layers with 4xheads + pos emb\n# BotStackLayer = nn.Sequential(bot_layer_1, bot_layer_2, bot_layer_3)\nBotStackLayer = nn.Sequential(bot_layer_1)\n\n# create model \ndef get_BoTModel(n_channels=1):\n    # base model \n    model = timm.create_model(CFG.model_name, pretrained=True, in_chans=n_channels)\n    num_features = model.fc.in_features  # print(num_features)\n\n    # BoT layers\n    model.layer4 = BotStackLayer\n    # head\n    model.fc = nn.Linear(num_features, CFG.num_classes)\n    # convert ReLU activation to SiLU\n    model = convert_act_cls(model, nn.ReLU, nn.SiLU()) \n    return model \n\n# ## WiP adjust fmaps for various image resolutions \n\n# # ## debug model call\n# print(CFG.model_name)\n# model = get_BoTModel(n_channels=1)\n# summary(model, (1, 256, 256))","40e6b7c3":"# ====================================================\n# LR scheduler \n# ====================================================\n\ndef get_scheduler(optimizer):\n    if CFG.scheduler=='ReduceLROnPlateau':\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n    elif CFG.scheduler=='CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n    return scheduler","21d22e67":"## Usual Torch train\/eval functions \n## adapted from here: https:\/\/www.kaggle.com\/yasufuminakama\/seti-nfnet-l0-starter-training\n\ndef train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    if CFG.apex:\n        scaler = GradScaler()\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    preds = []\n    # switch to train mode\n    model.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        if CFG.apex:\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds.view(-1), labels)\n        else:\n            y_preds = model(images)\n            loss = criterion(y_preds.view(-1), labels)\n        # record loss\n        losses.update(loss.item(), batch_size)\n        # save predictions\n        preds.append(y_preds.sigmoid().detach().to('cpu').numpy())\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        if CFG.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()     \n            \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            if CFG.apex:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                    # 'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f} ({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  #'LR: {lr:.6f}  '\n                  .format(epoch+1, step, len(train_loader), \n                        # batch_time=batch_time,data_time=data_time, \n                      loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(train_loader)),\n                   grad_norm=grad_norm,\n                   #lr=scheduler.get_lr()[0],\n                   ))\n    predictions = np.concatenate(preds)    \n    return losses.avg, predictions\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to evaluation mode\n    model.eval()\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds.view(-1), labels)\n        losses.update(loss.item(), batch_size)\n        # save predictions\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}\/{1}] '\n                    # 'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f} ({loss.avg:.4f}) '\n                  .format(\n                   step, len(valid_loader), \n                   #  batch_time=batch_time,data_time=data_time, \n                      loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                   ))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","ae5d5092":"def run_fold(model, criterion, optimizer, scheduler, device, fold=0, num_epochs=10):\n    \n    trn_idx = df_folds[df_folds['fold'] != fold].index\n    val_idx = df_folds[df_folds['fold'] == fold].index\n\n    train_folds = df_folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = df_folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds[CFG.target_col].values\n\n    train_ds = SetiDataset(train_folds, transform=get_transforms(data='train'))\n    valid_ds = SetiDataset(valid_folds, transform=get_transforms(data='valid'))\n\n    train_dl = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, \n                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n    valid_dl = DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False, \n                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    best_score = 0.\n    best_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        # train\n        avg_loss, train_preds = train_fn(train_dl, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, preds = valid_fn(valid_dl, model, criterion, device)\n\n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n            \n        # compute metrics\n        score = get_score(valid_labels, preds)\n        score_tr = get_score(train_folds[CFG.target_col].values.astype(np.float), train_preds)\n                \n        history['train_loss'].append(avg_loss)\n        history['valid_loss'].append(avg_val_loss)\n        history['train_auc'].append(score_tr)\n        history['valid_auc'].append(score)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        f'BoT_{CFG.model_name}_fold{fold}_best_score.pth')\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        f'BoT_{CFG.model_name}_fold{fold}_best_loss.pth')\n    \n    valid_folds['preds'] = torch.load(f'BoT_{CFG.model_name}_fold{fold}_best_loss.pth', map_location=torch.device('cpu'))['preds']\n\n    return valid_folds, history","15455442":"# define model\nmodel = get_BoTModel(n_channels=1)\nmodel.to(device);\n\n# Select Optim, LR, Loss \noptimizer = AdamP(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n# optimizer = SAM(model.parameters(), AdamP, lr=CFG.lr, weight_decay=CFG.weight_decay)\n# optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\ncriterion = nn.BCEWithLogitsLoss()\nscheduler = get_scheduler(optimizer)","f2fc52c8":"oof_df = pd.DataFrame()\nhist_folds = []\nfor fold_id in range(CFG.n_fold):\n    if fold_id in CFG.trn_fold:\n        \n        print(f'Start Training Fold {fold_id} with bs {CFG.batch_size}')\n        print('-'*40)\n    \n        oof_, hist_ = run_fold(model, criterion, optimizer, scheduler, device=device, fold=fold_id, num_epochs=CFG.num_epochs)\n        oof_df = pd.concat([oof_df, oof_])\n        hist_folds.append(hist_)\n        # Fold score\n        get_result(oof_)\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n# CV score\nget_result(oof_df)\noof_df.to_csv('oof_df.csv', index=False)","d805c473":"plt.style.use('fivethirtyeight')\n\nfig = plt.figure(figsize=(14,6))\nplt.plot(hist_folds[0]['train_loss'], label='train loss')\nplt.plot(hist_folds[0]['valid_loss'], label='valid loss')\nplt.legend()\nplt.title(f'Loss Curve [Fold {0}]');","8bdd3df4":"fig = plt.figure(figsize=(14,6))\nplt.plot(hist_folds[0]['train_auc'], label='train auc')\nplt.plot(hist_folds[0]['valid_auc'], label='valid auc')\nplt.legend()\nplt.title(f'AUC Curve [Fold {0}]');","99db7013":"# ====================================================\n# Helper functions\n# ====================================================\n\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        image = np.load(file_path)[[0, 2, 4]]\n        image = np.vstack(image)\n        image = image.transpose((1, 0))\n        \n        if self.transform:\n            image = self.transform(image=image)['image']\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n            # image = image.astype(\"float\")[..., np.newaxis]\n        return image\n\n\n\n\n\ndef inference(model, states, test_loader, device):\n    model.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for state in states:\n            model.load_state_dict(state)  # ['model']\n            model.eval()\n            with torch.no_grad():\n                y_preds = model(images.float())\n            avg_preds.append(y_preds.sigmoid().to('cpu').numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs","dd96e0eb":"# # ====================================================\n# # inference\n# # ====================================================\n\n# model = CustomModel() # ...\n\n# states = [torch.load(f'xxx.pth') for fold in CFG.trn_fold]\n# print('no. of checkpoints:', len(states))\n\n# test_dataset = TestDataset(test, transform=get_transforms(data='valid'))\n# test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n#                          num_workers=CFG.num_workers, pin_memory=False)\n\n# predictions = inference(model, states, test_loader, device=device)","74f4c0a0":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Install & Import Required Packages<\/span>","1250168d":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Augmentations<\/span>","1c57b1fc":"*Bottleneck Transformers for Visual Recognition*: https:\/\/arxiv.org\/abs\/2101.11605\n\n![](https:\/\/user-images.githubusercontent.com\/22078438\/106106482-f04da900-6188-11eb-8f15-820811c2f908.png)\n\n\n> We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how **ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks**. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 2.33x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.","e0f2b3db":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Run Training<\/span>","3ff3fe66":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">BottleNeck Transformer<\/span>","978ea603":"# BoTNet | Res-type bottleneck blocks + Attention \ud83d\udd25","0cb8e15a":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Dataset<\/span>","20693545":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Config<\/span>","ac4b05a1":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Train Engine<\/span>","60ec4931":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">SAM\/AdamP Optimizer<\/span>\n\n*Sharpness-Aware Minimization for Efficiently Improving Generalization* : https:\/\/arxiv.org\/abs\/2010.01412\n\n*AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*: https:\/\/arxiv.org\/abs\/2006.08217","d1b45b66":"# References: \n\n- [seresnext50-but-with-attention](https:\/\/www.kaggle.com\/debarshichanda\/seresnext50-but-with-attention) by [debarshichanda](https:\/\/www.kaggle.com\/debarshichanda) (Model + Optimizer)\n\n- [seti-nfnet-l0-starter-training](https:\/\/www.kaggle.com\/yasufuminakama\/seti-nfnet-l0-starter-training) by [yasufuminakama](https:\/\/www.kaggle.com\/yasufuminakama) (Train engine + Helpers)\n\n- [Bottleneck Transformers for Visual Recognition](https:\/\/arxiv.org\/abs\/2101.11605)\n\n- [Sharpness-Aware Minimization for Efficiently Improving Generalization](https:\/\/arxiv.org\/abs\/2010.01412)","5dfe4eec":"<span style=\"color: #0087e4; font-family: courier; font-size: 1.5em; font-weight: 300;\">Visualize Training & Metrics<\/span>","89dd1530":"<span style=\"color: #0087e4; font-family: courier ; font-size:1.5em; font-weight: 300;\">Load data & Split CV<\/span>","6f7948e6":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)"}}