{"cell_type":{"aaff7db0":"code","32154dbd":"code","f8c94878":"code","074e2218":"code","a912ad5d":"code","60e7a189":"code","5e216be9":"code","dca9e774":"code","a8b40bcf":"code","92e09581":"code","985dd178":"code","8db3528f":"code","948c9aff":"code","5885199e":"code","96f927a7":"code","1d507d03":"code","97f3b8bd":"code","0a0a787f":"code","558e8dd3":"code","96d84a8b":"code","8112d577":"code","252729c4":"code","2ad89326":"code","42ac881d":"markdown","50d46834":"markdown","d09144d5":"markdown","8f0d9876":"markdown","7a4f0323":"markdown","c4d96032":"markdown","cfa5c198":"markdown","a45e24e2":"markdown","2f40e4dd":"markdown","59b145e9":"markdown","d6334af7":"markdown","cc2dc8b1":"markdown","8737e0cf":"markdown","e15d89e4":"markdown","9079df6d":"markdown","9cb32f47":"markdown","bca3d98b":"markdown","d4f6d465":"markdown","a40b7591":"markdown","2f776145":"markdown"},"source":{"aaff7db0":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pickle\nimport time\nimport re","32154dbd":"train_set = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding=\"latin1\")\ntest_set = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv',encoding=\"latin1\")\n\ntrain_set.head()","f8c94878":"unrelevant_features = [\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"]\n\ntrain_set.drop(unrelevant_features,inplace=True,axis=1)\ntest_set.drop(unrelevant_features,inplace=True,axis=1)\ntrain_set.head()","074e2218":"train_set[\"Sentiment\"].value_counts()","a912ad5d":"positives = train_set[(train_set[\"Sentiment\"] == \"Positive\") | (train_set[\"Sentiment\"] == \"Extremely Positive\")]\npositives_test = test_set[(test_set[\"Sentiment\"] == \"Positive\") | (test_set[\"Sentiment\"] == \"Extremely Positive\")]\nprint(positives[\"Sentiment\"].value_counts())\npositives.head()\n","60e7a189":"negatives = train_set[(train_set[\"Sentiment\"] == \"Negative\") | (train_set[\"Sentiment\"] == \"Extremely Negative\")]\nnegatives_test = test_set[(test_set[\"Sentiment\"] == \"Negative\") | (test_set[\"Sentiment\"] == \"Extremely Negative\")]\nprint(negatives[\"Sentiment\"].value_counts())\nnegatives.head()","5e216be9":"neutrals = train_set[train_set[\"Sentiment\"] == \"Neutral\"]\nneutrals_test = test_set[test_set[\"Sentiment\"] == \"Neutral\"]\nprint(neutrals[\"Sentiment\"].value_counts())\nneutrals.head()","dca9e774":"import warnings as wrn\nwrn.filterwarnings('ignore')\n\nnegatives[\"Sentiment\"] = 0 \nnegatives_test[\"Sentiment\"] = 0\n\npositives[\"Sentiment\"] = 2\npositives_test[\"Sentiment\"] = 2\n\nneutrals[\"Sentiment\"] = 1\nneutrals_test[\"Sentiment\"] = 1\n\nnegatives.head()","a8b40bcf":"data = pd.concat([positives,\n                  positives_test,\n                  neutrals,\n                  neutrals_test,\n                  negatives,\n                  negatives_test\n                 ],axis=0)\n\ndata.reset_index(inplace=True)","92e09581":"data.info()","985dd178":"data.head()","8db3528f":"import random\nfor i in range(1,10):\n    random_ind = random.randint(0,len(data))\n    print(str(data[\"OriginalTweet\"][random_ind]),end=\"\\nLabel: \")\n    print(str(data[\"Sentiment\"][random_ind]),end=\"\\n\\n\")\n","948c9aff":"positiveFD = nltk.FreqDist(word for text in data[data[\"Sentiment\"] == 2][\"OriginalTweet\"] for word in text.lower().split())\nnegativeFD = nltk.FreqDist(word  for text in data[data[\"Sentiment\"] == 0][\"OriginalTweet\"] for word in text.lower().split())\nneutralDF = nltk.FreqDist(word  for text in data[data[\"Sentiment\"] == 1][\"OriginalTweet\"] for word in text.lower().split())","5885199e":"plt.subplots(figsize=(8,6))\nplt.title(\"Most Used Words in Positive Tweets\")\npositiveFD.plot(50)\nplt.show()","96f927a7":"plt.subplots(figsize=(8,6))\nplt.title(\"Most Used Words in Negative Tweets\")\nnegativeFD.plot(50)\nplt.show()","1d507d03":"plt.subplots(figsize=(8,6))\nplt.title(\"Most Used Words in Neutral Tweets\")\nneutralDF.plot(50)\nplt.show()","97f3b8bd":"cleanedData = []\n\nlemma = WordNetLemmatizer()\nswords = stopwords.words(\"english\")\nfor text in data[\"OriginalTweet\"]:\n    \n    # Cleaning links\n    text = re.sub(r'http\\S+', '', text)\n    \n    # Cleaning everything except alphabetical and numerical characters\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n    \n    # Tokenizing and lemmatizing\n    text = nltk.word_tokenize(text.lower())\n    text = [lemma.lemmatize(word) for word in text]\n    \n    # Removing stopwords\n    text = [word for word in text if word not in swords]\n    \n    # Joining\n    text = \" \".join(text)\n    \n    cleanedData.append(text)","0a0a787f":"for i in range(0,5):\n    print(cleanedData[i],end=\"\\n\\n\")","558e8dd3":"vectorizer = CountVectorizer(max_features=10000)\nBOW = vectorizer.fit_transform(cleanedData)\n","96d84a8b":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(data[\"Sentiment\"]))\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8112d577":"from sklearn.svm import SVC\nstart_time = time.time()\n\nmodel = SVC()\nmodel.fit(x_train,y_train)\n\nend_time = time.time()\nprocess_time = round(end_time-start_time,2)\nprint(\"Fitting SVC took {} seconds\".format(process_time))","252729c4":"predictions = model.predict(x_test)","2ad89326":"from sklearn.metrics import accuracy_score,confusion_matrix\n\nprint(\"Accuracy of model is {}%\".format(accuracy_score(y_test,predictions) * 100))","42ac881d":"* Finally everything is ready, we can start examining data with examining random texts.","50d46834":"* Although most of the words are same (we say them stopwords [words that not have a special meaning]) we can see characteristic words such as **panic**","d09144d5":"# Data Overview\nIn this section we will take a look at the data. And in order to do this we will use frequency distrubutions of NLTK.","8f0d9876":"* Our accuracy is 77% Not bad, but I guess we can develop a better model using deep learning.","7a4f0323":"# Support Vector Machine Classifier Modeling\nEverything is ready, now ve can fit our classifier.","c4d96032":"* Now let's convert labels into integers, I will label negatives as 0, neutrals as 1 and positives as 2.","cfa5c198":"# Data Preprocessing\nIn this section we will prepare the dataset in order to use in SVM model. Before starting I want to explain processes with 1 line explanations\n\n* Cleaning Links: We'll clean all website links using regular expressions\n* Cleaning Everything Except Alphabetical and Numerical Characters: We'll clean unrelevant digits using regular expressions\n* Tokenizing and Lemmatizing: We'll split texts into their words and convert words to their base form (dogs=>dog)\n* Removing Stopwords: We'll remove words that have no special meaning (such as **the,will,was**)\n* Joining Prepared Strings: We'll join the words\n* Bag of Words Approach: We'll create a bag of word. In bag of words approach each feature shows whether the text contains the word or not. For instance if our text contains **grocery** word, its **grocery** feature will be 1 if not it will be 0.\n\nIf you want to learn more detail about them, you can check my Naive Bayes text classification kernel. Here is the link: https:\/\/www.kaggle.com\/mehmetlaudatekman\/filtering-spam-e-mails-power-of-naive-bayes","a45e24e2":"* Now I will concatenate train and test, I will split them after processing.","2f40e4dd":"* First let's drop unrelevant features","59b145e9":"* Now we can create our bag of words, but before starting I want to give an example:\n\n        ====TEXT====           HELLO  WORLD  BRO  CLEAN  SOME  TEXT  WELCOME       \n        Hello world               1     1     0     0      0     0      0 \n        Hello bro                 1     0     1     0      0     0      0\n        Clean some text           0     0     0     1      1     1      0\n        You're welcome bro        0     0     1     0      0     0      1\n        Welcome to world          0     1     0     0      0     0      1\n        \nThis is a simple bag of words model. ","d6334af7":"* It took almost 13 minutes, I hope its worth it.","cc2dc8b1":"* And let's take a look at the frequency distributions.","8737e0cf":"* And now we can examine most used words plots.","e15d89e4":"* Now let's take a look at the accuracy and confusion matrix.","9079df6d":"* Now let's split sentences as their class, Positive,Neutral and Negative.","9cb32f47":"# Preparing the Environment\nIn this kernel, I will use **sklearn** to develop a model and vectorize the texts; **nltk** to preprocess texts and data overview.\n\nAnd I will use several libraries.","bca3d98b":"# Simply Explained: Support Vector Machine\nSupport Vector Machine (after this I will say SVM) is a machine learning approach that has a classifier and regressor, in this section I will cover classifier.\n\nThere are versions of SVM such as Linear SVM. In linear SVM, model fits **N** lines to split classes from each other (N is class count - 1 ). We will say **Decision Lines** to these lines. \n\n![image.png](attachment:image.png)\n\nAnd in order to make classifier durable, **decision line** must be in the closest position to **support points** of each class.\n\nI know, I have not explain what is support point. Support point (also support vectors) is the points that nearest to the decision line. You can see them in the graph above.\n\nAnd sometimes our dataset may be non-linear. However we can still use SVM to classify non-linear dataset. In order to do this we'll use **kernel tricks**. If we add a new dimension to data we may classify it using lines. Let's take a look at the images below: (thanks for images, here is the link of article [in Turkish])\n\n![](https:\/\/miro.medium.com\/max\/700\/1*u7ogmOy2vQDjgw7Pou0hoA.png)\n\nIn this example we've added a new dimension (Z) to data and now we can classify it using a line.\n\nEverything you need to know about basics of Support Vector Machine was this. Let's start to implement!","d4f6d465":"# Text Classification: SVM Explained\nHello people! Welcome to this kernel. In this kernel I am going to develop a sentiment analyser using **Support Vector Machine** algorithm and **Bag of Words** representation.\n\nBefore starting, I have to say that: Using deep learning approachs such as **Recurrent Neural Networks** are better to classify texts, but when we have a small dataset, traditional machine learning may be good.\n\nAnd if you interested in text classification with traditional machine learning, you might want to use a lazy algorithm to filter spam emails. If you, you can check my **Naive Bayes** kernel. In that kernel I've expalined everything (text processing and deploying a sklearn model included) detailed. Here is the link: https:\/\/www.kaggle.com\/mehmetlaudatekman\/filtering-spam-e-mails-power-of-naive-bayes\n\n# Notebook Content\n1. Preparing the Environment\n    * Importing Libraries\n    * Importing Datasets\n1. Simply Explained: Support Vector Machine\n1. Data Overview\n    * Frequency Distributions with NLTK\n1. Data Preprocessing\n    * Cleaning Links\n    * Cleaning Everything Except Alphabetical and Numerical Characters\n    * Tokenizing and Lemmatizing\n    * Removing Stopwords\n    * Joining Prepared Strings\n    * Bag of Words Approach\n1. Support Vector Machine Classifier Modeling\n    * Fitting SVMC Model with SKLearn\n    * Testing Model\n1. Conclusion\n","a40b7591":"* Everything is ready, we can split our set into train and test.","2f776145":"# Conclusion\nThanks for your attention. In this kernel I have explained what is SVM and how they work and developing a text classifier using SVMs.\n\nIf you have any question in your mind, please ask in comment section.\n\nHave a great day!"}}