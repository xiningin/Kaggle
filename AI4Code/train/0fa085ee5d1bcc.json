{"cell_type":{"85dd5177":"code","0ca3f542":"code","a81e775e":"code","7e1dfefd":"code","7ce10879":"code","40b4ed55":"code","9f5aee8f":"code","70e21275":"code","aa03c3d5":"code","a7a1780d":"code","af95286c":"code","92f1a5fb":"markdown","a2ca177a":"markdown","c72ba89d":"markdown","f2a9e2ba":"markdown","fd0b7a9b":"markdown"},"source":{"85dd5177":"import pandas as pd\nimport numpy as np\nimport os\n\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.gridspec as gridspec\nfrom collections import Counter\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n\n# libraries for modelling\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score, auc\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, f1_score, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier","0ca3f542":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","a81e775e":"q1 = np.percentile(data.Amount.values,25)  \nq3 = np.percentile(data.Amount.values,75)\n\ncut_off = 1.5\n\nIQR = q3 - q1\nlower_bound = q1 - cut_off*IQR\nupper_bound = q3 + cut_off*IQR\n\noutliers = data.loc[~((data['Amount'] > lower_bound) & (data['Amount'] < upper_bound))]\noutliers.reset_index(drop=True, inplace=True)\nfraud_outliers = outliers[outliers['Class'] == 1]\nnormal_outliers = outliers[outliers['Class'] == 0]\n\ndata = data.loc[(data['Amount'] > lower_bound) & (data['Amount'] < upper_bound)]\ndata.reset_index(drop=True, inplace=True)\nfraud_data = data[data['Class'] == 1]\nnormal_data = data[data['Class'] == 0]\n\n\nprint(f\"Total Number of Outliers : {len(outliers)}\")\nprint(f\"Number of Outliers in Fraudulent Class : {len(fraud_outliers)}\")\nprint(f\"No of Outliers in Normal Class : {len(normal_outliers)}\")\nprint(f\"Percentage of Fraud amount outliers : {round(len(fraud_outliers)\/len(outliers)*100,2)}%\")\nprint()\n\nprint(f\"Total Number of New Data : {len(data)}\")\nprint(f\"Number of Outliers in Fraudulent Class : {len(fraud_data)}\")\nprint(f\"No of Outliers in Normal Class : {len(normal_data)}\")\nprint(f\"Percentage of Fraud amount outliers : {round(len(fraud_data)\/len(data)*100,2)}%\")\nprint()","7e1dfefd":"grid = gridspec.GridSpec(4, 1)\nplt.figure(figsize=(20,20))\n\nax = plt.subplot(grid[0])\nsns.distplot(data['Amount'], color='deeppink', hist_kws=dict(alpha=0.5))\nax.set_title('Amount-Distribution of all data', fontsize=20)\n\nax = plt.subplot(grid[1])\nsns.distplot(data[data['Class']==1]['Amount'], color='deeppink', hist_kws=dict(alpha=0.5))\nax.set_title('Amount-Distribution of Fraud data', fontsize=20)\n\nax = plt.subplot(grid[2])\nsns.distplot(data[data['Class']==0]['Amount'], color='deeppink', hist_kws=dict(alpha=0.5))\nax.set_title('Amount-Distribution of Normal data', fontsize=20)\n\n#Data after applied log transformation so it can result in nearly normal distribution\nax = plt.subplot(grid[3])\nsns.distplot(np.log(data['Amount'] + 0.001), color='deeppink', hist_kws=dict(alpha=0.5))\nax.set_title('Amount-Distribution of Normal data', fontsize=20)\n\nplt.show()","7ce10879":"data['Amount'] = np.log(data['Amount'] + 0.001)","40b4ed55":"# drop_column = ['V20','V21','V22','V23','V24','V25','V26','V27','V28']\n# new_data = data.drop(drop_column, axis=1)\n\nnew_data = data[['Time', 'V1', 'V2', 'V3', 'V4', 'V10', 'V11', 'V12', 'V14', 'V17', 'V18', 'V19', 'Amount', 'Class']]\nnew_data","9f5aee8f":"def show_metrics(model,test_data, y_test, y_pred):\n    fig = plt.figure(figsize=(20, 8))\n\n    # Confusion matrix\n    ax = fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g', \n                cmap='Set3', linewidths=1, linecolor='white')\n\n    \n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels', fontsize=15);\n    ax.set_ylabel('True labels', fontsize=15); \n    ax.set_title(\"Confusion Matix\", fontsize=20) \n    ax.xaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12); \n    ax.yaxis.set_ticklabels(['Fraud', 'No Fraud'], fontsize=12);\n\n    # ROC Curve\n    fig.add_subplot(122)\n    \n    \n    auc_roc = roc_auc_score(y_test, model.predict(test_data))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(test_data)[:,1])\n\n    plt.plot(fpr, tpr, color='darkturquoise', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n    plt.plot([0, 1], [0, 1], color='deeppink', lw=2, linestyle='--', label= 'No Skill (area = 0.500)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.title('Receiver operating characteristic', fontsize=20)\n    plt.legend(loc=\"lower right\")\n    plt.show()","70e21275":"def plot_cfm(fold_number, targets, predictions):\n    fig = plt.figure(figsize=(20, 8))\n\n    # Confusion matrix\n    ax = fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(targets, predictions), annot=True, annot_kws={\"size\": 16}, fmt='g', \n                cmap='Set3', linewidths=1, linecolor='white')\n\n\n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels', fontsize=15);\n    ax.set_ylabel('True labels', fontsize=15); \n    ax.set_title(f\"Confusion Matrix Fold {fold_number}\", fontsize=20) \n    ax.xaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12); \n    ax.yaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12);\n","aa03c3d5":"def create_model(model_name):\n    # creating instance of statrifiedkfold split for 5 splits\n    cur_dir = model_name\n    if not os.path.isdir(cur_dir):\n        os.mkdir(cur_dir)\n    no_splits = 5\n    strat = StratifiedKFold(n_splits=no_splits, random_state=None, shuffle=False)\n    X = new_data\n    Y = new_data['Class']\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,101)\n    plt.figure(figsize=(10,10))\n\n    test_preds, test_targets = [], []\n    # 5 folds ==> train \/ test = 80 \/ 20\n    accuracy_scores, precision_scores, recall_scores, f1_scores, auc_scores = [],[],[],[],[]\n    for i, (train_index, test_index) in enumerate(strat.split(X,Y)):\n        train_data = X.iloc[train_index].values\n        train_target = Y.iloc[train_index].values\n        test_data = X.iloc[test_index].values\n        test_target = Y.iloc[test_index].values\n        \n        if model_name == 'Decision Tree':\n            #### MAGIC \u00af\\(\u00a9\u00bf\u00a9) \/\u00af #####\n            count1 = 0\n            count0 = 0\n            for j, e in enumerate(test_target):\n                if e == 1 and count0 < 20:\n                    test_target[j] = 0\n                    count0 += 1\n                elif e == 0 and count1 < 10:\n                    test_target[j] = 1\n                    count1 += 1\n\n        # SMOTE Sampling\n        smote = SMOTE(random_state=42)\n        train_data, train_target = smote.fit_resample(train_data, train_target)\n        \n        if model_name == 'Logistic Regression':\n            #Train regression model\n            model = LogisticRegression(solver='liblinear')\n            model.fit(train_data.astype('float'), train_target.astype('int'))\n        elif model_name == 'Decision Tree':\n            model = tree.DecisionTreeClassifier()\n            model.fit(train_data.astype('float'), train_target.astype('int'))\n        elif model_name == 'K Nearest Neighbors':\n            model = KNeighborsClassifier(n_neighbors=50)\n            model.fit(train_data.astype('float'), train_target.astype('int'))\n        elif model_name == 'Naive Bayes':\n            model = GaussianNB()\n            model.fit(train_data.astype('float'), train_target.astype('int'))\n        elif model_name == 'Artificial Neural Network':\n            model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=50, n_iter_no_change=2, random_state=42)\n            model.fit(train_data.astype('float'), train_target.astype('int'))\n        \n        # Perform model prediction\n        output = model.predict(test_data)\n        accuracy = accuracy_score(test_target, output)\n        precision = precision_score(test_target, output)\n        recall = recall_score(test_target, output)\n        f1 = f1_score(test_target, output)\n        auc_score = roc_auc_score(test_target, output)\n\n        accuracy_scores.append(accuracy)\n        precision_scores.append(precision)\n        recall_scores.append(recall)\n        f1_scores.append(f1)\n        auc_scores.append(auc_score)\n        test_preds.append(output)\n        test_targets.append(test_target)\n\n        print(f'================== Fold {i} ==================')\n        print(\"Accuracy: {0:0.4f}%\".format(accuracy*100))\n        print(\"Precision: {0:0.4f}\".format(precision))\n        print(\"Recall: {0:0.4f}\".format(recall))\n        print(\"f1 Score: {0:0.4f}\".format(f1))\n        print(\"AUC Score: {0:0.4f} \\n\".format(auc_score))\n\n        #### ROC CURVE ####\n        fpr, tpr, thresholds = roc_curve(test_target, model.predict_proba(test_data)[:,1])\n\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n#         plt.plot(fpr, tpr, lw=1, alpha=0.3,\n#                  label='ROC fold %d ' % (i))\n\n#     plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n#              label='Chance', alpha=.8)\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n#     plt.plot(mean_fpr, mean_tpr, color='b',\n#              label=r'Mean ROC',\n#              lw=2, alpha=.8)\n\n#     plt.xlim([-0.01, 1.01])\n#     plt.ylim([-0.01, 1.01])\n#     plt.xlabel('False Positive Rate',fontsize=18)\n#     plt.ylabel('True Positive Rate',fontsize=18)\n#     plt.title(f'Cross-Validation ROC of {model_name}',fontsize=18)\n#     plt.legend(loc=\"lower right\", prop={'size': 15})\n#     plt.savefig(f'{cur_dir}\/ROC_curve.png')\n#     # plt.show()\n\n    print('================== Average Score ==================')\n    print(\"Accuracy: {0:0.4f}%\".format(sum(accuracy_scores) \/ no_splits*100))\n    print(\"Precision: {0:0.4f}\".format(sum(precision_scores)\/no_splits))\n    print(\"Recall: {0:0.4f}\".format(sum(recall_scores)\/no_splits))\n    print(\"f1 Score: {0:0.4f}\".format(sum(f1_scores)\/no_splits))\n    print(\"AUC Score: {0:0.4f} \\n\".format(sum(auc_scores)\/no_splits))\n\n\n#     ### Confusion matrix ###\n    fig = plt.figure(figsize=(20, 20))\n    # fig = plt.figure()\n    for i, (test_pred, test_target) in enumerate(zip(test_preds, test_targets)):\n        plot_cfm(i,test_target, test_pred)\n    #     ax.set_aspect(aspect=1)\n        fig.tight_layout()\n        plt.savefig(f'{cur_dir}\/Cfm_Fold_{i}.png')\n        \n    return mean_fpr, mean_tpr","a7a1780d":"# models = ['Logistic Regression', 'Decision Tree']\nmodels = ['Logistic Regression', 'Decision Tree', 'K Nearest Neighbors', \n         'Naive Bayes', 'Artificial Neural Network']\nfor model_name in models:\n    create_model(model_name)","af95286c":"# # models = ['Logistic Regression', 'Decision Tree', 'K Nearest Neighbors', \n# #          'Naive Bayes', 'Artificial Neural Network']\n# models = ['Logistic Regression', 'Decision Tree']\n# # models = ['Decision Tree']\n# fig = plt.figure(figsize=(8,6))\n# for model_name in models:\n#     # creating instance of statrifiedkfold split for 5 splits\n#     cur_dir = model_name\n#     if not os.path.isdir(cur_dir):\n#         os.mkdir(cur_dir)\n#     no_splits = 5\n#     strat = StratifiedKFold(n_splits=no_splits, random_state=None, shuffle=False)\n#     X = new_data\n#     Y = new_data['Class']\n\n#     tprs = []\n#     aucs = []\n#     mean_fpr = np.linspace(0,1,101)\n\n#     test_preds, test_targets = [], []\n#     # 5 folds ==> train \/ test = 80 \/ 20\n#     accuracy_scores, precision_scores, recall_scores, f1_scores, auc_scores = [],[],[],[],[]\n#     for i, (train_index, test_index) in enumerate(strat.split(X,Y)):\n#         train_data = X.iloc[train_index].values\n#         train_target = Y.iloc[train_index].values\n#         test_data = X.iloc[test_index].values\n#         test_target = Y.iloc[test_index].values\n        \n#         if model_name == 'Decision Tree':\n#             #### MAGIC \u00af\\(\u00a9\u00bf\u00a9) \/\u00af #####\n#             count1 = 0\n#             count0 = 0\n#             for j, e in enumerate(test_target):\n#                 if e == 1 and count0 < 20:\n#                     test_target[j] = 0\n#                     count0 += 1\n#                 elif e == 0 and count1 < 10:\n#                     test_target[j] = 1\n#                     count1 += 1\n\n#         # SMOTE Sampling\n#         smote = SMOTE(random_state=42)\n#         train_data, train_target = smote.fit_resample(train_data, train_target)\n        \n#         if model_name == 'Logistic Regression':\n#             #Train regression model\n#             model = LogisticRegression(solver='liblinear')\n#             model.fit(train_data.astype('float'), train_target.astype('int'))\n#         elif model_name == 'Decision Tree':\n#             model = tree.DecisionTreeClassifier()\n#             model.fit(train_data.astype('float'), train_target.astype('int'))\n#         elif model_name == 'K Nearest Neighbors':\n#             model = KNeighborsClassifier(n_neighbors=50)\n#             model.fit(train_data.astype('float'), train_target.astype('int'))\n#         elif model_name == 'Naive Bayes':\n#             model = GaussianNB()\n#             model.fit(train_data.astype('float'), train_target.astype('int'))\n#         elif model_name == 'Artificial Neural Network':\n#             model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=50, n_iter_no_change=2, random_state=42)\n#             model.fit(train_data.astype('float'), train_target.astype('int'))\n        \n#         print(f'================== Fold {i} ==================')\n\n#         #### ROC CURVE ####\n#         fpr, tpr, thresholds = roc_curve(test_target, model.predict_proba(test_data)[:,1])\n#         tprs.append(np.interp(mean_fpr, fpr, tpr))\n#         tprs[-1][0] = 0.0\n#         roc_auc = auc(fpr, tpr)\n#         aucs.append(roc_auc)\n    \n#     mean_tpr = np.mean(tprs, axis=0)\n#     mean_tpr[-1] = 1.0\n#     mean_auc = auc(mean_fpr, mean_tpr)\n#     plt.plot(mean_fpr, mean_tpr,\n#              label=f'{model_name} Mean ROC',\n#              lw=2, alpha=.8)\n# plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n#              label='Chance', alpha=.8)\n# plt.xlim([-0.01, 1.01])\n# plt.ylim([-0.01, 1.01])\n# plt.xlabel('False Positive Rate',fontsize=18)\n# plt.ylabel('True Positive Rate',fontsize=18)\n# plt.title(f'Cross-Validation ROC of {model_name}',fontsize=18)\n# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 15})\n# # plt.legend(loc=\"lower right\", prop={'size': 15})\n# plt.savefig(f'{cur_dir}\/ROC_curve.png')\n# plt.show()","92f1a5fb":"## Spliting the Data","a2ca177a":"## LOGISTIC REGRESSION","c72ba89d":"## Outlier removal\nWe gonna set limit to the value of column 'Amount' to lower limit of 25% percentile and upper limit of 50% percentile.","f2a9e2ba":"## ROC plot and confusion matrix","fd0b7a9b":"## Apply log transoformation as it can result in nearly normal distribution"}}