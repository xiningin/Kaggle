{"cell_type":{"bf1c9e0d":"code","31de79fa":"code","3293f9d7":"code","12bfc46a":"code","e1442b54":"code","08408b3e":"code","088c2d29":"code","7856ffac":"code","ed510b6d":"code","b8701eae":"code","c664ce92":"code","97c4e371":"code","b2f162bf":"code","8d7228e9":"code","706570ca":"code","83a4b099":"code","0fe6b8fa":"code","e5d24bf3":"code","ea4211ee":"code","f632b49c":"code","ef368d3a":"code","ed63334e":"code","34d7fec9":"code","3baaf57e":"code","f826c7c3":"code","86d24b67":"code","5433553f":"code","4f1efae6":"code","d327a5e8":"markdown","e5563fac":"markdown","9fb2685e":"markdown","9cc23267":"markdown","bb316ccc":"markdown","da2905a7":"markdown","61542ff5":"markdown","9794638d":"markdown"},"source":{"bf1c9e0d":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport logging\nimport warnings\nimport requests\nimport json\nimport seaborn as sns\nimport datetime\nimport calendar\nimport datetime\nimport pickle\nimport time\nfrom datetime import timedelta\nfrom datetime import datetime, timedelta\nimport pickle\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nfrom datetime import timedelta\nimport os\n\nprint('Library import Sucessfully')\n\n#Turkey Energy exchange","31de79fa":"files=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        files.append(os.path.join(dirname, filename))\n","3293f9d7":"# Files(list) contains 4 files path\n# I am trying to make work easiest way\ndf=pd.read_csv(files[0]) #\ndf2=pd.read_csv(files[1])\ndf3=pd.read_csv(files[2])\ndf4=pd.read_csv(files[3])","12bfc46a":"# append all files in one DataFrame df\ndf=df.append(df2)\ndf=df.append(df3)\ndf=df.append(df4)","e1442b54":"df.shape","08408b3e":"df['timestamp']=df['Date']+' '+df['Hour']","088c2d29":"df.head()","7856ffac":"def convert_string_to_float(x):\n    if type(x)==str:\n        return float(x.replace(',',''))\n    else:\n        return x","ed510b6d":"df['timestamp']=pd.to_datetime(df['timestamp'])\ndf['Consumption (MWh)']=df['Consumption (MWh)'].apply(convert_string_to_float)#.replace(',',''))\ndf.drop(columns=['Date','Hour'],inplace=True)\ndf.set_index('timestamp',inplace=True)\ndf=df.resample('1D',closed='right').mean().reset_index()\ndf['month']=df['timestamp'].dt.month\ndf.sort_values(by='timestamp',inplace=True)\n","b8701eae":"df.rename(columns={'Consumption (MWh)':'P'},inplace=True)\ndf.dtypes","c664ce92":"features_df=df.copy() # I am doing this because if anything i have computed wrong so i will run my code from here :) ","97c4e371":"features_df.head(3)","b2f162bf":"features_df['P1']=features_df['P'].shift(-1) #change to -ve\nfeatures_df['P2']=features_df['P'].shift(-2)\nfeatures_df['target_column']=features_df['P1']+features_df['P2']+features_df['P'].shift(-3)+features_df['P'].shift(-4)+features_df['P'].shift(-5)+features_df['P'].shift(-6)+features_df['P'].shift(-7)\nfeatures_df.fillna(0,inplace=True)","8d7228e9":"features_df.head(3) # P1 and P2 is next day average MWH","706570ca":"features_df.sort_values(by='timestamp',inplace=True)","83a4b099":"features_df=features_df[(features_df['P1']!=0) & (features_df['P2']!=0)] # input shuold not b zero","0fe6b8fa":"scaler = MinMaxScaler()","e5d24bf3":"features_df.set_index('timestamp',inplace=True)","ea4211ee":"features_df[['P', 'P1', 'P2', 'target_column']]=scaler.fit_transform(features_df[['P', 'P1', 'P2', 'target_column']])","f632b49c":"train, test = train_test_split(features_df, test_size=0.2)","ef368d3a":"train.describe()","ed63334e":"y_target=train['target_column'].copy()\ny_test_target=test['target_column'].copy()","34d7fec9":"import xgboost as xgb\nregressor = xgb.XGBRegressor(\n    n_estimators=200,\n    reg_lambda=1,\n    gamma=0,\n    max_depth=2\n)","3baaf57e":"regressor.fit(train.drop(columns=['target_column']).values, y_target.values)\n","f826c7c3":"f=train.drop(columns=['target_column']).values\nresults=regressor.predict(f)\ndf_train_plot=pd.DataFrame(results)\ndf_train_plot['real']=train.reset_index()['target_column']\ndf_train_plot['month']=train.reset_index()['month']\ndf_train_plot.head()\n\n#real 0 is predicted values","86d24b67":"mean_squared_error(y_target.values, results)","5433553f":"f=test.drop(columns=['target_column']).values\nresults=regressor.predict(f)\ndf_train_plot=pd.DataFrame(results)\ndf_train_plot['real']=test.reset_index()['target_column']\ndf_train_plot['month']=test.reset_index()['month']\ndf_train_plot.head()\ndf_train_plot.rename(columns={0:'predicted_value'},inplace=True)\nsns.regplot(x='real', y=\"predicted_value\", data=df_train_plot);\n#real","4f1efae6":"mean_squared_error(y_test_target.values, results)","d327a5e8":"Below cell Step are\n* Convert \"timestamp\" into datetime format\n* Now we have timestamp we can extract hour, month,day from timestamp if it is required, so i am removing Date and Hour Column\n* Converting all data on day based because I want to ** predict week** based on 3 days data.","e5563fac":"Create Timestamp features with the help of  Date and Hour","9fb2685e":"Weekly Prediction for Energy Consumption dataset, based on 3 days data","9cc23267":"You can see below test result, model results is quite good :) ","bb316ccc":"*********** END Thank you you can use a model for your weekly prediction. You can use this model for monthly prediction ****","da2905a7":"Have a look MEan square error, it's means our model is good lets try on test dataset, so we willbe confident of our model results","61542ff5":"Test Dataset","9794638d":"Now Making a Window slides for 3 days and 2nd last line is for week prediction"}}