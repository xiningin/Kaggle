{"cell_type":{"d7388820":"code","1ca03830":"code","764a4650":"code","e7150e84":"code","7bc80087":"code","fda37c29":"code","a37f0001":"code","23e22b1f":"code","2f5ca5bb":"code","983e35ff":"code","9ed929f1":"code","ba6307d3":"code","d0092eef":"code","2e46a03d":"code","166b5147":"markdown","cddd0f05":"markdown","31aecf11":"markdown","afa5fca9":"markdown","30b21bb3":"markdown","77979791":"markdown","455b92cb":"markdown","3f7a2c0a":"markdown","b5468b54":"markdown"},"source":{"d7388820":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\ndtc = DecisionTreeClassifier()\nknc = KNeighborsClassifier()\nsgd = SGDClassifier()\nlr = LinearRegression()","1ca03830":"# Per tutorial, load training data into a variable\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","764a4650":"# The next step of the tutorial is to add the testing data to a variable\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","e7150e84":"# Gives us the type of each of the data fields and the amount of non-null values\nprint(train_data.info())\nprint(test_data.info())","7bc80087":"# Gives us different information, important of which is the count values for each field\nprint(train_data.describe())\nprint(test_data.describe())","fda37c29":"# The important information here is the amount of unique object in each of the fields\nprint(train_data.describe(include=\"object\"))\nprint(test_data.describe(include=\"object\"))","a37f0001":"# Gives us the amount of null values per field\nprint(train_data.isnull().sum())\nprint(test_data.isnull().sum())","23e22b1f":"# First we need to deal with NULL values for the Age, we'll do this by subbing in the average of the passengers into each one\nmean_age_train = train_data[\"Age\"].mean()\nmean_age_test = test_data[\"Age\"].mean()\n\ntrain_data[\"Age\"].replace(np.nan, mean_age_train, inplace = True)\ntest_data[\"Age\"].replace(np.nan, mean_age_test, inplace = True)","2f5ca5bb":"# We will now do the samething with the Fare field, note its only null once and only in the test datasheet\nmean_fare = test_data[\"Fare\"].mean()\n\ntest_data[\"Fare\"].replace(np.nan, mean_fare, inplace = True)","983e35ff":"# Next we'll get rid of the Cabin field, as its not present in the training and thus we can't use it to draw any conclusions \n# We will also get rid of the Name field, as it can't be used to draw any reasonable conclusions\ntrain_data.drop(columns = [\"Cabin\", \"Name\"], axis = 1, inplace = True)\ntest_data.drop(columns = [\"Cabin\", \"Name\"], axis = 1, inplace = True)\n\ntrain_data.dropna(subset=[\"Embarked\"], axis=0, inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)\n\ntest_data.dropna(subset=[\"Embarked\"], axis=0, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)","9ed929f1":"# Recheck for null values and notice there isn't any\nprint(train_data.isnull().sum())\nprint(test_data.isnull().sum())","ba6307d3":"# This cell will help us visualize some interesting correlations of the variables\n\n# First lets look at the raw number of survivors, 0 = death and 1 = survived\nsns.countplot(x = \"Survived\", data = train_data)\nplt.show()\n\n# Now lets piece in the Sex variable, notice the difference in surivors leans strongly with women\nsns.countplot(x = \"Survived\", data = train_data, hue = \"Sex\")\nplt.show()\n\n# This shows that their may be some correlation between ports and survivors, Cherbourg sees more survivors than fatalities\nsns.countplot(x = \"Survived\", data = train_data, hue = \"Embarked\")\nplt.show()\n\n# Contrary to what one might think, it seems that having multiple siblings on board meant you were less likely to survive\nsns.countplot(x = \"Survived\", data = train_data, hue = \"SibSp\" )\nplt.show()\n\n# It also seems that parents\/children present didn't have a noticable difference in survivability \nsns.countplot(x = \"Survived\", data = train_data, hue = \"Parch\" )\nplt.show()\n\n# We can see that being of a higher economic class increased the odds of survival\nsns.countplot(x = \"Survived\", data = train_data, hue = \"Pclass\" )\nplt.show()\n\nplt.hist(train_data[\"Age\"])\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()","d0092eef":"# First I need to train my data model\nfeatures = ['Sex', 'Fare', 'Age', 'Pclass'] \n\nX_train = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\ny_train = train_data['Survived']\n\n# Decision Tree Classifier\ndtc.fit(X_train, y_train)\npredictionDTC = dtc.predict(X_test)\naccuracyDTC = dtc.score(X_train, y_train)\nprint(\"The accuracy of the Decision Tree model: \", accuracyDTC)\n\n# KNeighbors Classifier\nknc.fit(X_train, y_train)\npredictionKNC = knc.predict(X_test)\naccuracyKNC = knc.score(X_train, y_train)\nprint(\"The accuracy of the KNeighbors model: \", accuracyKNC)\n\n# SGD Classifier\nsgd.fit(X_train, y_train)\npredictionSGD = sgd.predict(X_test)\naccuracySGD = sgd.score(X_train, y_train)\nprint(\"The accuracy of the SGD model: \", accuracySGD)\n\n# Linear Regression Model\nlr.fit(X_train, y_train)\npredictionLR = lr.predict(X_test)\naccuracyLR = lr.score(X_train, y_train)\nprint(\"The accuracy of the Linear Regression model: \", accuracyLR)","2e46a03d":"# For my submission I used my DTC prediciton and I largely used the code provided in the tutorial\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictionDTC})\n\noutput.to_csv('submission.csv', index = False)\nprint(\"Your submission was successfully saved!\")","166b5147":"# **Testing**","cddd0f05":"# **Needed Packages**","31aecf11":"# **In's and Out's of The Data**","afa5fca9":"# **Data Cleaning**","30b21bb3":"# **Data Loading**\n\n    kaggle competitions download -c titanic\nIs the given command to load the data.    \n\n\nHowever, I used the quick download button found under the Data tab of the Titanic project. ","77979791":"DTC, KNC, and SGD all seem to be fairly good predictors for the data. DTC does especially well, while conversely LRC performes very poorly. I'll be using DTC for my submission. \n\nMy Decision Tree model produced an accuracy of approximately 98%. My next 2 models were still quite good, each performing well above 50%. However, my Linear Regression model performed less than a random guess would have at a 38% accuracy. ","455b92cb":"# **My Submission**","3f7a2c0a":"# **Introduction**\n\n* Name: Grant Lee\n* Date: 09-24-2021\n* Assignment \\#0\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","b5468b54":"# **Exploratory Data Analysis**\n"}}