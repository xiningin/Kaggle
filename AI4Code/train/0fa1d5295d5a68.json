{"cell_type":{"86336bf9":"code","5bc543a8":"code","8bddd6a4":"code","8d442a8e":"code","b4c1c449":"code","d4b8032b":"code","0e6f7f92":"code","5f1070ab":"code","cde59df9":"code","16949f2c":"code","8ced6187":"code","e4531a31":"code","14159018":"code","933b94e2":"code","635cbbf2":"code","29454c1d":"code","0dbfdf8f":"markdown"},"source":{"86336bf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5bc543a8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, \\\n                RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nimport time\nSEED = 123","8bddd6a4":"#reading the files\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")\n\nprint(train.groupby('Cover_Type')['Id'].count())","8d442a8e":"y = train.Cover_Type\ntest_id = test['Id']\n\n#dropping Ids\ntrain = train.drop(['Id'], axis = 1)\ntest = test.drop(['Id'], axis = 1)\n\n#prepare data for training the model\nX = train.drop(['Cover_Type'], axis = 1)","b4c1c449":"print(X.columns[(X < 0).any()])","d4b8032b":"\nclf = RandomForestClassifier()\nclf = clf.fit(X,y)\n\nfeatures = pd.DataFrame({'Features': X.columns, \n                         'Importances': clf.feature_importances_})\nfeatures.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\nplt.figure(figsize=(12,4))\nsns.barplot(x='Features', y='Importances', data=features)\nplt.xticks(rotation='vertical')\nplt.show()","0e6f7f92":"def preprocess(df):\n    #horizontal and vertical distance to hydrology can be easily combined\n    cols = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\n    df['Distance_to_hydrology'] = df[cols].apply(np.linalg.norm, axis=1)\n    \n    #adding a few combinations of distance features to help enhance the classification\n    cols = ['Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points',\n            'Horizontal_Distance_To_Hydrology']\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    #taking some factors influencing the amount of radiation\n    df['Cosine_of_slope'] = np.cos(np.radians(df['Slope']) )\n    #X['Diff_azimuth_aspect_9am'] = np.cos(np.radians(123.29-X['Aspect']))\n    #X['Diff_azimuth_aspect_12noon'] = np.cos(np.radians(181.65-X['Aspect']))\n    #X['Diff_azimuth_aspect_3pm'] = np.cos(np.radians(238.56-X['Aspect']))\n\n    df['Elevation_VDH'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    return df\n\nX = preprocess(X)\ntest = preprocess(test)\nprint(X.columns)","5f1070ab":"# Plotting mode frequencies as % of data size\n#take from: https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-feature-engineering\nn_rows = X.shape[0]\nmode_frequencies = [X[col].value_counts().iat[0] for col in X.columns]\nmode_frequencies = 100.0 * np.asarray(mode_frequencies) \/ n_rows\n\nmode_df = pd.DataFrame({'Features': X.columns, \n                        'Mode_Frequency': mode_frequencies})\n\nmode_df.sort_values(by=['Mode_Frequency'], axis='index', ascending=True, inplace=True)\n\nfig = plt.figure(figsize=(14, 4))\nsns.barplot(x='Features', y='Mode_Frequency', data=mode_df)\nplt.ylabel('Mode Frequency %')\nplt.xticks(rotation='vertical')\nplt.show()","cde59df9":"def drop_unimportant(df):\n    df_ = df.copy()\n    n_rows = df_.shape[0]\n    hi_freq_cols = []\n    for col in X.columns:\n        mode_frequency = 100.0 * df_[col].value_counts().iat[0] \/ n_rows \n        if mode_frequency > 99.0:\n            hi_freq_cols.append(col)\n    df_ = df_.drop(hi_freq_cols, axis='columns')\n    return df_\n\nX = drop_unimportant(X)","16949f2c":" X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)","8ced6187":"#code from here:http:\/\/ataspinar.com\/2017\/05\/26\/classification-with-scikit-learn\/\nclf_dict = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Decision Tree\": DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Neural Net\": MLPClassifier(alpha = 1),\n    \"Naive Bayes\": GaussianNB(),\n}\n\ndef batch_classify(X_train, y_train, X_val, y_val, no_clf = 5, verbose = True):\n    dict_models = {}\n    for clf_name, clf in list(clf_dict.items())[:no_clf]:\n        t_start = time.clock()\n        clf.fit(X_train, y_train)\n        t_end = time.clock()\n        \n        t_diff = t_end - t_start\n        train_score = clf.score(X_train, y_train)\n        val_score = clf.score(X_val, y_val)\n        \n        dict_models[clf_name] = {'model': clf, 'train_score': train_score, \n                                 'val_score': val_score, 'train_time': t_diff}\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=clf_name, f=t_diff))\n    return dict_models\n\ndef display_dict_models(dict_models, sort_by='val_score'):\n    cls = [key for key in dict_models.keys()]\n    test_s = [dict_models[key]['val_score'] for key in cls]\n    training_s = [dict_models[key]['train_score'] for key in cls]\n    training_t = [dict_models[key]['train_time'] for key in cls]\n    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),4)), columns = ['classifier', 'train_score', \n                                                                     'val_score', 'train_time'])\n    for ii in range(0,len(cls)):\n        df_.loc[ii, 'classifier'] = cls[ii]\n        df_.loc[ii, 'train_score'] = training_s[ii]\n        df_.loc[ii, 'val_score'] = test_s[ii]\n        df_.loc[ii, 'train_time'] = training_t[ii]\n    print(df_.sort_values(by=sort_by, ascending=False))\n\n#dict_models = batch_classify(X_train, y_train, X_val, y_val, no_clf = 8)\n#display_dict_models(dict_models)","e4531a31":"feature_names = list(X.columns)\ntest = test[feature_names]\nprint(X.shape)\nprint(test.shape)","14159018":"clf = RandomForestClassifier(n_estimators=181, bootstrap=False, \n                               max_features='auto', random_state=SEED)\nclf1 = KNeighborsClassifier(n_neighbors=1, p=1)\nclf2 = GaussianNB()\nclf3 = DecisionTreeClassifier(max_features='auto', random_state=SEED)\nclf5 = AdaBoostClassifier(base_estimator=clf3)\nclf6 = GradientBoostingClassifier(max_features='sqrt',random_state=SEED)\nlr = LogisticRegression(multi_class='multinomial', solver='newton-cg', random_state=SEED)","933b94e2":"from mlxtend.classifier import StackingCVClassifier\nsclf = StackingCVClassifier(classifiers=[clf, clf6],meta_classifier=lr)","635cbbf2":"for clf, label in zip([clf, clf6, sclf], \n                      ['RandomForest',\n                       'GradientBoost',\n                       'StackingClassifier']):\n\n    scores = cross_val_score(clf, X.values, y.values, cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))","29454c1d":"sclf.fit(X,y)\ntest_pred = sclf.predict(test)\noutput = pd.DataFrame({'Id': test_id, 'Cover_Type': test_pred.astype(int)})\noutput.to_csv('submission.csv', index=False)","0dbfdf8f":"# Stumped - can't improve accuracy so moving on to stacking."}}