{"cell_type":{"c92c00f5":"code","6d63e044":"code","7c135045":"code","06adbe8d":"code","eb173f3f":"code","f146338a":"code","e70c9d14":"code","c2a6d693":"code","7b3f84f7":"code","797c2761":"code","a7b0fef4":"code","e656d53f":"markdown","52e222f6":"markdown","c580a73e":"markdown","018d18a5":"markdown","b849abba":"markdown","7c3ba39e":"markdown","5f43b491":"markdown","af6c2ad3":"markdown","6a099a47":"markdown","29042349":"markdown"},"source":{"c92c00f5":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.io import loadmat\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical","6d63e044":"def load_mnist_dataset():\n    \"\"\"\n    Load mnist-original dataset\n\n    Returns:\n    mnist_data -- an array of arrays in the shape of (784,)\n    mnist_label -- an array of labels\n    classes -- array of labels classes(a set of labels)\n    shape -- shape of data item\n    channels_count -- channel count of data images\n    \"\"\"\n\n    mnist = loadmat(\"..\/input\/mnist-original\/mnist-original.mat\")\n    mnist_data = mnist[\"data\"].T\n    mnist_data = mnist_data.reshape(len(mnist_data), 28, 28, 1)\n    mnist_label = mnist[\"label\"][0]\n    count = len(set(mnist_label))\n    return mnist_data, mnist_label, count, (28, 28, 1)\n\n\ndata, labels, classes_count, data_shape = load_mnist_dataset()\n\nprint(\"data shape: \" + str(data.shape))\nprint(\"labels shape: \" + str(labels.shape))\nprint(\"classes count: \" + str(classes_count))","7c135045":"mnist = loadmat(\"..\/input\/mnist-original\/mnist-original.mat\")\nlen(mnist)","06adbe8d":"def plot_images_sample(X, Y):\n    plt.figure(figsize=(10,10))\n    rand_indicies = np.random.randint(len(X), size=25)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        index = rand_indicies[i]\n        plt.imshow(np.squeeze(X[index]), cmap=plt.cm.binary)\n        # The CIFAR labels happen to be arrays, \n        # which is why you need the extra index\n        plt.xlabel(Y[index])\n    plt.show()\n\n\nplot_images_sample(data, labels)","eb173f3f":"x_train_orig, x_test_orig, y_train_orig, y_test_orig = train_test_split(data, labels, test_size=0.1, shuffle=True)\n\nX_train = x_train_orig \/ 255.\nX_test = x_test_orig \/ 255.\nY_train = to_categorical(y_train_orig)\nY_test = to_categorical(y_test_orig)","f146338a":"model = models.Sequential([\n        layers.Conv2D(8, kernel_size=(4, 4), strides=(1, 1), padding='same', activation='relu', input_shape=data_shape),\n        layers.MaxPooling2D((8, 8), padding='same'),\n        layers.Conv2D(16, kernel_size=(2, 2), strides=(1, 1), padding='same', activation='relu'),\n        layers.MaxPooling2D((4, 4), padding='same'),\n        layers.Flatten(),\n        layers.Dense(classes_count)\n    ])\n\nmodel.summary()","e70c9d14":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, Y_train, epochs=10, validation_split=0.2)","c2a6d693":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\nplt.show()","7b3f84f7":"test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint(\"test accuracy: \" + str(test_acc))","797c2761":"predictions = model.predict(X_test)\npredictions = np.argmax(predictions, axis=1)\n\nplot_images_sample(X_test, predictions)","a7b0fef4":"X_test_wrong = []\npredictions_wrong = []\nfor i in range(len(X_test)):\n    if predictions[i] != np.argmax(Y_test[i]):\n        X_test_wrong.append(X_test[i])\n        predictions_wrong.append(predictions[i])\n        \nplot_images_sample(X_test_wrong, predictions_wrong)","e656d53f":"Everything is ready to compile and train our model...","52e222f6":"Very well, now we can look at the result and see what we have done so far","c580a73e":"Import Necessary Libraries","018d18a5":"**Define the model structure**\n\nthis model includes 2 Conv layer each followed by a MaxPooling layer, and at the end after flattening, we have a fully connected layer.","b849abba":"First, load data from [MNIST](https:\/\/www.kaggle.com\/avnishnish\/mnist-original) dataset","7c3ba39e":"Well let's take a look at some of the images in dataset","5f43b491":"Before we continue, we need to prepare our data for the model\n* first, we need to splite data in 2 parts, train set and test set, so we make sure that the model does not train on test set\n* then, we need to make our input data in the range of 0 to 1 so it become easier for model to be trained\n* and finally, we in order to use our labels in the model we need to convert the to one hot encoding","af6c2ad3":"and the test accuracy is","6a099a47":"Let's look at some predictions sample","29042349":"and here we have some wrong prediction samples"}}