{"cell_type":{"260249fe":"code","e55fc619":"code","5a075ff1":"code","3bdce7a0":"code","1271ad1e":"code","ee31c218":"code","1c3bda50":"code","79297357":"code","686ed4f4":"code","452ca9ac":"code","c5e59984":"code","cfd56ef7":"code","f9db6bdd":"code","7926fde6":"code","9240734c":"code","1e2c04cf":"code","25156df9":"code","cc56231b":"code","ecf12392":"markdown","4a175163":"markdown","c5b917c7":"markdown","3d52153c":"markdown","843289fa":"markdown","5528c2e5":"markdown","a2f90cea":"markdown","c0a34ed5":"markdown","377fc89c":"markdown","58b87b27":"markdown","cfcd492c":"markdown","5973a39b":"markdown","6e6ef8fa":"markdown","21f6352d":"markdown","d200e718":"markdown","01d1ac2b":"markdown","d21a972d":"markdown"},"source":{"260249fe":"from sklearn.datasets import load_wine\nwine = load_wine()\n\ndata = wine.data\ntarget = wine.target","e55fc619":"print(data.shape)","5a075ff1":"# separando os dados em treino e teste\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n\n# separando o conjunto de treino em valida\u00e7\u00e3o tamb\u00e9m\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","3bdce7a0":"# treinando o modelo \nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# avaliando o modelo\nfrom sklearn.metrics import accuracy_score\ny_pred = knn.predict(X_val)\naccuracy_score(y_val, y_pred)","1271ad1e":"best_model = None\nbest_accuracy = 0\n\nfor k in [1,2,3,4,5]:\n\n    knn = KNeighborsClassifier(n_neighbors = k) # a cada passo, o par\u00e2metro assume um valor\n    knn.fit(X_train, y_train)\n\n    y_pred = knn.predict(X_val)\n    acc = accuracy_score(y_val, y_pred)\n    print('K:', k, '- ACC:', acc)\n    \n    if acc > best_accuracy:\n        best_model = knn\n        best_accuracy = acc\n        \ny_pred = best_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\n\nprint()\nprint('Melhor modelo:')\nprint('K:', best_model.get_params()['n_neighbors'], '- ACC:', acc * 100) #corrigir","ee31c218":"# embaralhando os dados v\u00e1rias vezes e re-executando o experimento\nimport numpy as np\nfrom sklearn.model_selection import ShuffleSplit\nss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) # 5 execu\u00e7\u00f5es diferentes com 20% dos dados para teste\n\nacc = []\nfor train_index, test_index in ss.split(data):\n    knn = KNeighborsClassifier(n_neighbors = 1)\n    knn.fit(data[train_index],target[train_index])\n    y_pred = knn.predict(data[test_index])\n    acc.append(accuracy_score(y_pred,target[test_index]))\n\nacc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar m\u00e9dia e desvio padr\u00e3o\nprint(acc)\nprint('Acur\u00e1cia - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))","1c3bda50":"# utilizando valida\u00e7\u00e3o cruzada com cross_val_score\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors = 1)\nscores = cross_val_score(knn, data, target, cv=5) # 5 execu\u00e7\u00f5es diferentes com 20% dos dados para teste\n\nprint('Acur\u00e1cia - %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))","79297357":"# utilizando valida\u00e7\u00e3o cruzada com KFold\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits = 5)\n\nacc = []\nfor train_index, test_index in kf.split(data):\n    knn = KNeighborsClassifier(n_neighbors = 1)\n    knn.fit(data[train_index],target[train_index])\n    y_pred = knn.predict(data[test_index])\n    acc.append(accuracy_score(y_pred,target[test_index]))\n\nacc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar m\u00e9dia e desvio padr\u00e3o\nprint(acc)\nprint('Acur\u00e1cia - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))","686ed4f4":"# utilizando valida\u00e7\u00e3o cruzada com KFold\nfrom sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(n_splits = 5)\n\nacc = []\nfor train_index, test_index in kf.split(data, target): # precisa passar as classes agora para que a divis\u00e3o aconte\u00e7a\n    knn = KNeighborsClassifier(n_neighbors = 1)\n    knn.fit(data[train_index],target[train_index])\n    y_pred = knn.predict(data[test_index])\n    acc.append(accuracy_score(y_pred,target[test_index]))\n\nacc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar m\u00e9dia e desvio padr\u00e3o\nprint('Acur\u00e1cia - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))","452ca9ac":"from sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\nX_r = pca.fit_transform(data)\n\ncolors = ['navy', 'turquoise', 'darkorange']\nfor color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n    plt.scatter(X_r[target == i, 0], X_r[target == i, 1], color=color, alpha=.8, label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('PCA')","c5e59984":"# verificando a escala dos atributos\nimport pandas as pd\ndf = pd.DataFrame(data)\ndf.describe()","cfd56ef7":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n\nscaler = StandardScaler()\nscaler.fit(data)\ndata_s = scaler.transform(data)\n\ndf = pd.DataFrame(data_s)\ndf.describe()","f9db6bdd":"pca = PCA(n_components=2)\nX_r = pca.fit_transform(data_s)\n\ncolors = ['navy', 'turquoise', 'darkorange']\nfor color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n    plt.scatter(X_r[target == i, 0], X_r[target == i, 1], color=color, alpha=.8, label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('PCA')","7926fde6":"from sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(n_splits = 5)\n\nacc = []\nfor train_index, test_index in kf.split(data, target): # precisa passar as classes agora para que a divis\u00e3o aconte\u00e7a\n    knn = KNeighborsClassifier(n_neighbors = 1)\n    \n    scaler = StandardScaler()\n    train = scaler.fit_transform(data[train_index]) # somente dados de treino no fit\n    test = scaler.transform(data[test_index]) # aplica-se transform no teste apenas\n    \n    knn.fit(train,target[train_index])\n    y_pred = knn.predict(test)\n    acc.append(accuracy_score(y_pred,target[test_index]))\n\nacc = np.asarray(acc) # converte pra numpy pra ficar mais simples de usar m\u00e9dia e desvio padr\u00e3o\nprint('Acur\u00e1cia - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))","9240734c":"# utilizando valida\u00e7\u00e3o cruzada com cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier(n_neighbors = 1))])\nscores = cross_val_score(pipeline, data, target, cv=5) # 5 execu\u00e7\u00f5es diferentes com 20% dos dados para teste\n\nprint('Acur\u00e1cia - %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))","1e2c04cf":"# separa-se uma parcela para encontrar os melhores par\u00e2metros (5% do original)\ndata_gs, data_cv, target_gs, target_cv = train_test_split(data, target, test_size=0.95, random_state=42, stratify=target)\n\n# uma forma autom\u00e1tica de StandardScaler + CLF\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('scaler', StandardScaler()), ('clf', KNeighborsClassifier())])\n\n# utiliza-se GridSearchCV para achar os melhores par\u00e2metros\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'clf__n_neighbors': [1,2,3,4,5], 'clf__weights' : ['uniform','distance']} # quais par\u00e2metros e quais valores ser\u00e3o testados\nclf = GridSearchCV(pipeline, parameters, cv=3, iid=False) # clf vai armazenar qual foi a melhor configura\u00e7\u00e3o\nclf.fit(data_gs, target_gs)\n\nprint(clf.best_params_)\n\n# utilizando valida\u00e7\u00e3o cruzada para avaliar o modelo\nscores = cross_val_score(clf.best_estimator_, data_cv, target_cv, cv=5)\nprint('Acur\u00e1cia - %.2f +- %.2f' % (scores.mean() * 100, scores.std() * 100))\n\nclf = clf.best_estimator_\n\nfrom sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(n_splits = 5)\n\nacc = []\nfor train_index, test_index in kf.split(data_cv, target_cv): # precisa passar as classes agora para que a divis\u00e3o aconte\u00e7a\n    \n    #scaler = StandardScaler()\n    #train = scaler.fit_transform(data_cv[train_index]) # somente dados de treino no fit\n    #test = scaler.transform(data_cv[test_index]) # aplica-se transform no teste apenas\n    \n    clf.fit(data_cv[train_index],target_cv[train_index])\n    y_pred = clf.predict(data_cv[test_index])\n    acc.append(accuracy_score(y_pred,target_cv[test_index]))\n\nacc = np.array(acc)\nprint('Acur\u00e1cia - %.2f +- %.2f' % (acc.mean() * 100, acc.std() * 100))","25156df9":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target","cc56231b":"# Implemente seu c\u00f3digo aqui","ecf12392":"Os dados est\u00e3o bastante aglomerados em algumas regi\u00f5es, dificultando o funcionamento de m\u00e9todos de aprendizagem baseados em dist\u00e2ncia e fun\u00e7\u00f5es lineares. Os dados naturalmente t\u00eam essa caracter\u00edstica, mas \u00e9 sempre importante verificar se essa aglomera\u00e7\u00e3o de informa\u00e7\u00e3o n\u00e3o \u00e9 devido a escala dos atributos. Isso, inclusive, \u00e9 extremamente prejudicial para m\u00e9todos baseados em dist\u00e2ncia.","4a175163":"O resultado melhorou ainda mais ao pesquisar qual era a melhor configura\u00e7\u00e3o de atributos, antes de avaliar a performance do modelo no conjunto de teste. O conceito e a fun\u00e7\u00e3o de Pipeline ser\u00e3o abordados com mais detalhes nos pr\u00f3ximos encontros, ent\u00e3o n\u00e3o se desespere.\n\nConclui-se aqui essa breve discuss\u00e3o e apresenta\u00e7\u00e3o sobre divis\u00e3o de dados, onde voc\u00ea pode ter aprendido sobre:\n* separa\u00e7\u00e3o dos dados em treino, valida\u00e7\u00e3o e teste;\n* normaliza\u00e7\u00e3o dos dados;\n* valida\u00e7\u00e3o cruzada; e\n* busca pelos melhores par\u00e2metros.","c5b917c7":"Para esse m\u00e9todo de aprendizagem, o conjunto de dados wine parece ser mais dif\u00edcil de classificar. Diferente da Iris, que ficava em torno de 98%, esse conjunto dificilmente passou dos 80%.\nVamos aplicar a visualiza\u00e7\u00e3o desses dados para verificar se eles s\u00e3o muito aglomerados ou linearmente separ\u00e1veis.","3d52153c":"Vamos explorar um conjunto de dados diferentes: uma base de vinhos; separar os dados e verificar como a mudan\u00e7a de par\u00e2metros afeta o resultado. H\u00e1 uma forma automatizada de fazer isso, mas isso ser\u00e1 conte\u00fado do pr\u00f3ximo encontro. Por enquanto, vamos fazer tudo manualmente para entender o processo.","843289fa":"O resultado mostra a princ\u00edpio como a redu\u00e7\u00e3o na quantidade de dados de treino impacta o desempenho do m\u00e9todo. Esse conjunto de valida\u00e7\u00e3o \u00e9 bastante importante para servir como um tipo de teste e analisar a rela\u00e7\u00e3o de melhoria ou piora com a mudan\u00e7a de par\u00e2metros no m\u00e9todo de aprendizagem.\n\nVamos supor testes com diferentes valores de *k* (n_neighbors) no m\u00e9todo k-NN. Vamos mostrar o desempenho a cada mudan\u00e7a de par\u00e2metro, e tamb\u00e9m vamos salvar o modelo sempre que um resultado maior for obtido.","5528c2e5":"A biblioteca Pandas tem uma forma f\u00e1cil de visualizar algumas informa\u00e7\u00f5es estat\u00edsticas sobre um conjunto de dados. No bloco acima, \u00e9 poss\u00edvel perceber que alguns atributos est\u00e3o em escalas completamente diferente de outros. Por exemplo, compare o atributo 0 e o atributo 12, ou 0 e 7. Escalas diferentes confundem os m\u00e9todos baseados em dist\u00e2ncia. Sabendo disso, uma estrat\u00e9gia \u00e9 colocar os dados para respeitar um padr\u00e3o ou distribui\u00e7\u00e3o. Isso pode ser feito com o StandardScaler do Scikit-Learn. Vamos verificar como fica a distribui\u00e7\u00e3o dos dados ap\u00f3s padroniz\u00e1-los e a visualiza\u00e7\u00e3o em 2D.","a2f90cea":"Aqui \u00e9 importante perceber a diferen\u00e7a da acur\u00e1cia entre essa valida\u00e7\u00e3o (repetindo 5 vezes o experimento) e o bloco de c\u00f3digo anterior. Na situa\u00e7\u00e3o anterior, o modelo de k-NN com k = 1 chegou pr\u00f3ximo a 78% de acur\u00e1cia, enquanto que ao repetir o experimento, a m\u00e9dia foi 71,11% com desvio de 8.89%, que \u00e9 bastante coisa. **Pode-se dizer que \u00e9 mais confi\u00e1vel afirmar que esse modelo tem acur\u00e1cia em torno de 71% do que 78%.**","c0a34ed5":"# Exerc\u00edcio\n\n* Com o conjunto de dados sobre *c\u00e2ncer de mama*, **tente obter o melhor desempenho de acur\u00e1cia**. \n\n* Organize e **tenha cuidado** para que seu experimento execute um *protocolo de valida\u00e7\u00e3o que fa\u00e7a sentido*.\n\nMais informa\u00e7\u00f5es sobre esse conjunto de dados poder\u00e3o ser obtidas em: \n[https:\/\/scikit-learn.org\/stable\/datasets\/index.html#breast-cancer-dataset](https:\/\/scikit-learn.org\/stable\/datasets\/index.html#breast-cancer-dataset)","377fc89c":"Agora, os dados est\u00e3o divididos em tr\u00eas blocos disjuntos: treino, valida\u00e7\u00e3o e teste. \n* O teste representa aproximadamente 33% da quantidade dados;\n* A valida\u00e7\u00e3o representa aproximadamente 22% (66% x 33%) do total de dados; e\n* O treino representa aproximadamente 44% da quantidade de dados originais;\n\nA seguir, ser\u00e1 comparado o desempenho do KNN treinado com o conjunto de treino e testado no conjunto de valida\u00e7\u00e3o.","58b87b27":"Um \u00faltimo passo \u00e9 integrar na valida\u00e7\u00e3o cruzada a pesquisa pelos melhores par\u00e2metros, algo semelhante ao que foi feito anteriormente. Novamente, o Scikit-Learn tem uma fun\u00e7\u00e3o para facilitar o processo chamada *GridSearchCV*. A dificuldade nesse ponto aparece ao perceber como ser\u00e1 formado o fluxo, afinal n\u00e3o pode-se procurar os melhores par\u00e2metros a cada execu\u00e7\u00e3o da valida\u00e7\u00e3o cruzada, uma vez que a valida\u00e7\u00e3o cruzada tem o objetivo de avaliar um modelo e a mudan\u00e7a de par\u00e2metro a cada execu\u00e7\u00e3o criaria um modelo novo. Isso criaria um loop infinito de tentativas de avaliar um modelo.\n\nA solu\u00e7\u00e3o \u00e9 separar um conjunto de dados para fazer a busca pelos melhores par\u00e2metros e formar um modelo. Assumindo que esse conjunto utilizado para esse prop\u00f3sito \u00e9 representativo, avalia-se ent\u00e3o com a valida\u00e7\u00e3o cruzada qual o desempenho desse modelo. ","cfcd492c":"Os resultados melhoraram substancionalmente e s\u00e3o confi\u00e1veis, visto que foram avaliados numa estrat\u00e9gia de valida\u00e7\u00e3o cruzada com 5 parti\u00e7\u00f5es.","5973a39b":"Os dados agora est\u00e3o todos muito pr\u00f3ximos de 0 com desvio padr\u00e3o 1, e isso melhora a disposi\u00e7\u00e3o gr\u00e1fica como pode ser visto na imagem. N\u00e3o h\u00e1 mais tanto aglomera\u00e7\u00e3o, e existe separa\u00e7\u00f5es lineares mais vis\u00edveis. Isso s\u00e3o fortes ind\u00edcios de que agora um m\u00e9todo de aprendizagem ter\u00e1 melhor desempenho. No entanto, enquanto para visualizar o StandardScaler foi aplicado em toda a base, \u00e9 importante perceber que ao testar o modelo, **o StandardScaler s\u00f3 pode ter dados de treino no fit**. Ou seja, ele n\u00e3o pode conhecer a disposi\u00e7\u00e3o dos dados de teste.","6e6ef8fa":"A valida\u00e7\u00e3o cruzada pode ser feita de v\u00e1rias formas no Scikit-Learn, mas \u00e9 importante observar que agora n\u00e3o temos mais apenas um resultado para o desempenho do modelo. Teremos um resultado para cada execu\u00e7\u00e3o, ent\u00e3o \u00e9 necess\u00e1rio olhar para essas informa\u00e7\u00f5es pela perspectiva da m\u00e9dia e desvio padr\u00e3o. As fun\u00e7\u00f5es do Scikit-Learn que auxiliar\u00e3o na valida\u00e7\u00e3o cruzada s\u00e3o: *KFold* e *cross_val_score*.","21f6352d":"## Scikit-Learn \u00e9 uma biblioteca que auxilia na valida\u00e7\u00e3o do modelo.\n\nA separa\u00e7\u00e3o de dados em treino e teste \u00e9 muito importante para ter-se condi\u00e7\u00f5es de avaliar o desempenho de um modelo em dados que ainda n\u00e3o foram vistos. Na verdade, dados ainda n\u00e3o vistos s\u00e3o o foco de aprendizado de m\u00e1quina, visto que ao desenvolver-se sistemas, o principal objetivo \u00e9 coloc\u00e1-los em produ\u00e7\u00e3o e ter performance aceit\u00e1vel (confi\u00e1vel).\n\nExiste uma subdivis\u00e3o dos dados de treino que pode ser o conjunto de valida\u00e7\u00e3o. O conjunto de valida\u00e7\u00e3o pode, por exemplo, participar de um processo de melhoria do modelo. Uma vez que o modelo est\u00e1 aperfei\u00e7oado suficiente, ent\u00e3o ele \u00e9 avaliado no conjunto de teste.","d200e718":"Os dois blocos de cima demonstram duas maneiras de aplicar a valida\u00e7\u00e3o cruzada a um modelo. Enquanto o primeiro afirma que a acur\u00e1cia do modelo \u00e9 em torno de 72%, o segundo alega 63% com desvio de 23% (que \u00e9 muito alto). O primeiro tem o objetivo de avaliar o desempenho baseado em uma m\u00e9trica, enquanto o segundo oferece quais os \u00edndices (linhas) que participar\u00e3o de cada rodada ou execu\u00e7\u00e3o, dando maior flexibilidade para o programador.\n\n**Mas por que uma diferen\u00e7a t\u00e3o grande?** A resposta \u00e9 um conceito do campo de estat\u00edstica: estratifica\u00e7\u00e3o. Ter um conjunto estratificado significa que ele \u00e9 representativo. Ou seja, se sua amostra tem 30 amostras da classe positiva e 30 da classe negativa, ao separar em um conjunto menor, \u00e9 esperado que mantenha-se essa propor\u00e7\u00e3o (50\/50). Enquanto que o cross_val_score faz a divis\u00e3o considerando a estratifica\u00e7\u00e3o, o KFold n\u00e3o o faz. Para isso, existe uma variante dessa fun\u00e7\u00e3o chamada StratifiedKFold. ","01d1ac2b":"No trecho de c\u00f3digo anterior, foi avaliado o desempenho do m\u00e9todo de aprendizagem usando 5 parametriza\u00e7\u00f5es diferentes: k = 1, 2, 3, 4 e 5. Em vez de fazer essa verifica\u00e7\u00e3o diretamente no conjunto de teste, essa verifica\u00e7\u00e3o deve ser feita no conjunto de valida\u00e7\u00e3o quando houver dados suficiente. No exemplo anterior, o melhor resultado foi obtido com k = 1. Assim, o modelo salvo como *melhor modelo* foi k = 1, e foi esse modelo o escolhido para ser utilizado na verifica\u00e7\u00e3o de performance com o conjunto de teste.","d21a972d":"Agora que o conjunto de teste est\u00e1 totalmente isolado e n\u00e3o \u00e9 usado para definir melhores par\u00e2metros, \u00e9 importante se atentar para outra quest\u00e3o: suponha que, por acaso, as amostras mais f\u00e1ceis tenham ca\u00eddo no conjunto de teste. Isso vai influenciar o resultado positivamente, gerando uma falsa impress\u00e3o de que o m\u00e9todo de aprendizagem trouxe bons resultados. Aqui duas sa\u00eddas s\u00e3o poss\u00edveis: embaralhar os dados e rodar o mesmo experimento v\u00e1rias vezes ou utilizar valida\u00e7\u00e3o cruzada.\n\n**Os experimentos feitos at\u00e9 aqui usaram a estrat\u00e9gia de valida\u00e7\u00e3o hold-out**, onde uma parcela dos dados \u00e9 isolada e utilizada para verificar o desempenho do modelo. Na primeira proposta para contornar o problema de amostras muito f\u00e1ceis ca\u00edrem no conjunto de teste, sugeriu-se usar v\u00e1rias vezes o hold-out com embaralhamento das amostras. Embora aplic\u00e1vel, estatisticamente existe um grau de cetismo da viabilidade disso. Isso porque n\u00e3o h\u00e1 garantias de que todas as amostras em algum momento estar\u00e3o no conjunto de teste. *A solu\u00e7\u00e3o de garantia \u00e9 a valida\u00e7\u00e3o cruzada ou cross-validation.*"}}