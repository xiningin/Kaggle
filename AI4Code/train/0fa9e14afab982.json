{"cell_type":{"ad256bdf":"code","6c764ed2":"code","b2ee7305":"code","467e87ba":"code","1f5219af":"code","3305d01c":"code","d8e0653c":"code","02f6cf0a":"code","71febb2f":"code","ec265c35":"code","0804e124":"code","87bd9e0d":"code","ec681308":"code","5cf440cd":"code","125fe548":"code","5518c1fd":"code","bc9afaf1":"code","de64c07a":"code","148e6d67":"code","1982579d":"code","e6526db8":"code","179438f7":"code","8a5e3ba3":"code","2450c699":"code","afbe2295":"code","82d6351b":"code","29f8e777":"code","9af61576":"code","e4b305a2":"code","a6d953a5":"markdown","f80515e3":"markdown","d579ffcc":"markdown","4cfb3ab9":"markdown"},"source":{"ad256bdf":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6c764ed2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","b2ee7305":"df_train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/test.csv')\ndf_sample = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-5\/submission.csv')","467e87ba":"df_train.columns","1f5219af":"\ndf_test.columns","3305d01c":"df_train.shape","d8e0653c":"df_test.shape","02f6cf0a":"df_train.isnull().sum()","71febb2f":"df_test.isnull().sum()","ec265c35":"# Replacing all the Province_State that are null by the Country_Region values\ndf_train.Province_State.fillna(df_train.Country_Region, inplace=True)\ndf_test.Province_State.fillna(df_test.Country_Region, inplace=True)\n\ndf_train.County.fillna(df_train.Province_State, inplace=True)\ndf_test.County.fillna(df_test.Province_State, inplace=True)\n\ndf_train.isnull().sum()\ndf_train.columns","0804e124":"# taking care of categorical values from train set\n# we can also use labelencoder for date column\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\ndf_train['Country_Region'] = labelencoder.fit_transform(df_train['Country_Region'])\ndf_train['Target'] = labelencoder.fit_transform(df_train['Target'])\n# df_train['Date'] = labelencoder.fit_transform(df_train['Date'])\n\n# taking care of categorical values from test set\n\ndf_test['Country_Region'] = labelencoder.fit_transform(df_test['Country_Region'])\ndf_test['Target'] = labelencoder.fit_transform(df_test['Target'])\n","87bd9e0d":"# taking care of the date column\ndf_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\n\ndf_train.loc[:, 'Date'] = df_train.Date.dt.strftime(\"%Y%m%d\")\ndf_train.loc[:, 'Date'] = df_train['Date'].astype(int)\n\ndf_test.loc[:, 'Date'] = df_test.Date.dt.strftime(\"%Y%m%d\")\ndf_test.loc[:, 'Date'] = df_test['Date'].astype(int)","ec681308":"ID=df_train['Id']\nFID=df_test['ForecastId']","5cf440cd":"# splitting the dataset for training and testing\n\ny_train=df_train['TargetValue']\nX_train=df_train.drop(['Id', 'County', 'Province_State','TargetValue'],axis=1)\ndf_test=df_test.drop(columns=['County','Province_State','ForecastId'])\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","125fe548":"X_train.columns","5518c1fd":"X_train","bc9afaf1":"# Fitting Random Forest Regression to the dataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs=-1)\nestimators = 100\nmodel.set_params(n_estimators=estimators)\n\nscores = []\n\npipeline = Pipeline([('scaler2' , StandardScaler()),\n                        ('RandomForestRegressor: ', model)])\npipeline.fit(X_train , y_train)\ny_pred = pipeline.predict(X_test)\n\nscores.append(pipeline.score(X_test, y_test))","de64c07a":"print(scores)\n\n\ny_pred_main = pipeline.predict(df_test)\n\n\nmain_submission = pd.DataFrame({'id':FID,'TargetValue':y_pred_main})","148e6d67":"main_submission","1982579d":"\n# cross-validation method\nfrom sklearn.model_selection import cross_val_score, KFold\n\ncv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\nprint(\"cross-validation score:\",cv_scores.mean())\n\n# Cross-validation with a k-fold method\n\nkfold = KFold(n_splits=10, shuffle=True)\nkf_cv_scores = cross_val_score(pipeline, X_train, y_train, cv=kfold)\nprint(\"K-fold CV average score:\" ,kf_cv_scores.mean())","e6526db8":"from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\nprint('explained variance_score:', explained_variance_score(y_test, y_pred))\nprint('max_error:', max_error(y_test, y_pred))\nprint('mean_absolute_error score:', mean_absolute_error(y_test, y_pred))\nprint('mean_squared_error score:', mean_squared_error(y_test, y_pred))\nprint('root mean_squared_error:', sqrt(mean_squared_error(y_test, y_pred)))","179438f7":"# Fitting xgboost Regressor to the dataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor \n\nmodel_2 = XGBRegressor(n_jobs=-1)\nestimators = 1000\nmodel_2.set_params(n_estimators=estimators)\n\nscores = []\n\npipeline_2 = Pipeline([('scaler2' , StandardScaler()),\n                        ('XGBRegressor: ', model)])\npipeline_2.fit(X_train , y_train)\ny_pred_2 = pipeline_2.predict(X_test)\n\nscores.append(pipeline_2.score(X_test, y_test))","8a5e3ba3":"print(scores)\n\n\ny_pred_main=pipeline.predict(df_test)\n\n\nmain_submission_2 = pd.DataFrame({'id':FID,'TargetValue':y_pred_main})","2450c699":"main_submission_2","afbe2295":"from sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\nprint('explained variance_score:', explained_variance_score(y_test, y_pred_2))\nprint('max_error:', max_error(y_test, y_pred_2))\nprint('mean_absolute_error score:', mean_absolute_error(y_test, y_pred_2))\nprint('mean_squared_error score:', mean_squared_error(y_test, y_pred_2))\nprint('root mean_squared_error:', sqrt(mean_squared_error(y_test, y_pred_2)))","82d6351b":"main_pred=pd.DataFrame({'id':FID,'TargetValue':y_pred_main})\nprint(main_pred)","29f8e777":"a=main_pred.groupby(['id'])['TargetValue'].quantile(q=0.05).reset_index()\nb=main_pred.groupby(['id'])['TargetValue'].quantile(q=0.5).reset_index()\nc=main_pred.groupby(['id'])['TargetValue'].quantile(q=0.95).reset_index()","9af61576":"a.columns=['Id','q0.05']\nb.columns=['Id','q0.5']\nc.columns=['Id','q0.95']\na=pd.concat([a,b['q0.5'],c['q0.95']],1)\na['q0.05']=a['q0.05']\na['q0.5']=a['q0.5']\na['q0.95']=a['q0.95']\nprint(a)","e4b305a2":"sub=pd.melt(a, id_vars=['Id'], value_vars=['q0.05','q0.5','q0.95'])\nsub['variable']=sub['variable'].str.replace(\"q\",\"\", regex=False)\nsub['ForecastId_Quantile']=sub['Id'].astype(str)+'_'+sub['variable']\nsub['TargetValue']=sub['value']\nsub=sub[['ForecastId_Quantile','TargetValue']]\nsub.reset_index(drop=True,inplace=True)\nsub.to_csv(\"submission.csv\",index=False)\nsub.head()","a6d953a5":"**making csv file for submission using random forest prediction**","f80515e3":"# **RANDOM FOREST**","d579ffcc":"# **XGBOOST**","4cfb3ab9":"**please do upvote if you like and inform if you find any mistake.**"}}