{"cell_type":{"794d19f8":"code","bccf7cc4":"code","e6c60291":"code","48fd079d":"code","5c05b983":"code","f8b7aba1":"code","56fe410c":"code","37be77ad":"code","89ce0d53":"code","b1676925":"code","074f627f":"code","09026235":"code","00996487":"code","922dee1d":"code","fc25e485":"code","3255eed9":"code","14facfda":"code","a7bf73e7":"code","c84effab":"code","4700d1c8":"code","294ebb9f":"code","f0372015":"code","4c13ee67":"code","e7278584":"code","9e77d727":"code","f89f0893":"code","832a979c":"code","665f439b":"code","d5776763":"code","f123cfad":"code","5fa47363":"code","6849fcf0":"code","3dbd8381":"code","56bf97da":"code","d63e95e6":"code","512b445e":"code","a13c7f5b":"code","a9b22ccd":"code","cea6aba1":"code","0190e773":"code","43cdb2af":"code","9a072d28":"code","1e8e621f":"code","e9663de6":"code","ddac3b0c":"markdown","e1d01dfa":"markdown","63591c81":"markdown","59127ba2":"markdown","773f7d03":"markdown","51731b86":"markdown","3bb0c1d5":"markdown","48a4c5e7":"markdown","6bafba27":"markdown","e1820a7c":"markdown","a1147309":"markdown","9830ebd6":"markdown","7938878b":"markdown","7ad4e3b9":"markdown","c78bf7da":"markdown","da057562":"markdown","f737a628":"markdown","787ee1e3":"markdown"},"source":{"794d19f8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","bccf7cc4":"df = pd.read_csv('..\/input\/breast-cancer\/Breast_cancer_data.csv')","e6c60291":"df.head()","48fd079d":"df.info()","5c05b983":"df.describe()","f8b7aba1":"df.hist()\nplt.plot()","56fe410c":"import seaborn as sns","37be77ad":"sns.pairplot(data = df, palette = 'pastel')","89ce0d53":"df.isnull().sum()","b1676925":"sns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(15,12))\nsns.boxplot(data =df, orient=\"h\")","074f627f":"def remove_outliers_using_quantiles(qu_dataset, qu_field, qu_fence):\n    a = qu_dataset[qu_field].describe()\n    \n    iqr = a[\"75%\"] - a[\"25%\"]\n    print(\"interquartile range:\", iqr)\n    \n    upper_inner_fence = a[\"75%\"] + 1.5 * iqr\n    lower_inner_fence = a[\"25%\"] - 1.5 * iqr\n    print(\"upper_inner_fence:\", upper_inner_fence)\n    print(\"lower_inner_fence:\", lower_inner_fence)\n    \n    upper_outer_fence = a[\"75%\"] + 3 * iqr\n    lower_outer_fence = a[\"25%\"] - 3 * iqr\n    print(\"upper_outer_fence:\", upper_outer_fence)\n    print(\"lower_outer_fence:\", lower_outer_fence)\n    \n    count_over_upper = len(qu_dataset[qu_dataset[qu_field]>upper_inner_fence])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field]<lower_inner_fence])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ a[\"count\"]\n    print(\"percentage of records out of inner fences: %.2f\"% (percentage))\n    \n    count_over_upper = len(qu_dataset[qu_dataset[qu_field]>upper_outer_fence])\n    count_under_lower = len(qu_dataset[qu_dataset[qu_field]<lower_outer_fence])\n    percentage = 100 * (count_under_lower + count_over_upper) \/ a[\"count\"]\n    print(\"percentage of records out of outer fences: %.2f\"% (percentage))\n    \n    if qu_fence == \"inner\":\n        output_dataset = qu_dataset[qu_dataset[qu_field]<=upper_inner_fence]\n        output_dataset = output_dataset[output_dataset[qu_field]>=lower_inner_fence]\n    elif qu_fence == \"outer\":\n        output_dataset = qu_dataset[qu_dataset[qu_field]<=upper_outer_fence]\n        output_dataset = output_dataset[output_dataset[qu_field]>=lower_outer_fence]\n    else:\n        output_dataset = qu_dataset\n    \n    print(\"length of input dataframe:\", len(qu_dataset))\n    print(\"length of new dataframe after outlier removal:\", len(output_dataset))\n    \n    return output_dataset\ndf.dropna(inplace=True)\nnew_df = remove_outliers_using_quantiles(df, \"mean_area\", \"inner\")","09026235":"new_df['mean_area']","00996487":"plt.figure(figsize=(15,12))\nsns.boxplot(x='mean_area', data = new_df, orient=\"h\")","922dee1d":"new_df.head()","fc25e485":"new_df.info()","3255eed9":"corrmat = df.corr()\nf, ax = plt.subplots(figsize =(12, 12))\nsns.heatmap(corrmat, ax = ax, annot=True, cmap =\"YlGnBu\", linewidths = 0.1)","14facfda":"### Creating Features\ny = df['diagnosis']\nfeatures = ['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness']\nX = df[features]","a7bf73e7":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=10)","c84effab":"X_train.shape","4700d1c8":"#From sklearn import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_random = RandomForestClassifier()","294ebb9f":"##Hyperparamters\nn_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]\nprint(n_estimators)","f0372015":"n_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num=8)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] ","4c13ee67":"from sklearn.model_selection import RandomizedSearchCV\nrandom_grid = {'n_estimators' : n_estimators,\n               'max_features' : max_features,\n               'max_depth' : max_depth,\n               'min_samples_split' : min_samples_split,\n               'min_samples_leaf' : min_samples_leaf\n              }\nprint(random_grid)","e7278584":"rf = RandomForestClassifier()","9e77d727":"rf_random = RandomizedSearchCV(estimator = rf,\n                               param_distributions = random_grid,\n                               scoring='neg_mean_squared_error',\n                               n_iter=10, cv=5, verbose=2, \n                               random_state = 42, n_jobs=1)","f89f0893":"rf_random.fit(X_train, y_train)","832a979c":"prediction = rf_random.predict(X_val)","665f439b":"prediction","d5776763":"from sklearn.metrics import confusion_matrix, accuracy_score","f123cfad":"cm = confusion_matrix(y_val, prediction)\nacc = accuracy_score(y_val, prediction)\nprint(cm)\nprint(acc)","5fa47363":"new_y = new_df['diagnosis']\nnew_X = new_df[features]","6849fcf0":"from sklearn.model_selection import train_test_split\nnew_X_train, new_X_val, new_y_train, new_y_val = train_test_split(new_X, new_y, random_state=10)","3dbd8381":"new_X_train.shape","56bf97da":"#From sklearn import Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_random_new = RandomForestClassifier()","d63e95e6":"n_estimators = [int(x) for x in np.linspace(start=100, stop=1200, num=12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num=8)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] ","512b445e":"from sklearn.model_selection import RandomizedSearchCV\nrandom_grid = {'n_estimators' : n_estimators,\n               'max_features' : max_features,\n               'max_depth' : max_depth,\n               'min_samples_split' : min_samples_split,\n               'min_samples_leaf' : min_samples_leaf\n              }\nprint(random_grid)","a13c7f5b":"rf_new = RandomForestClassifier()","a9b22ccd":"rf_random_new = RandomizedSearchCV(estimator = rf_new,\n                               param_distributions = random_grid,\n                               scoring='neg_mean_squared_error',\n                               n_iter=10, cv=5, verbose=2, \n                               random_state = 42, n_jobs=1)","cea6aba1":"rf_random_new.fit(new_X_train, new_y_train)","0190e773":"new_predictions = rf_random_new.predict(new_X_val)","43cdb2af":"new_predictions","9a072d28":"new_cm = confusion_matrix(new_y_val, new_predictions)\nnew_acc = accuracy_score(new_y_val, new_predictions)\nprint(new_cm)\nprint(new_acc)","1e8e621f":"import pickle","e9663de6":"file = open('random_forest_model.pkl', 'wb')\npickle.dump(rf_random, file)","ddac3b0c":"### Train Test Split\nFirstly, we will split our dataset into train and test data. As we have two dataset we will apply this techinque on both of them.\n","e1d01dfa":"From headtmap we can clearly see that many features are strongly correlated to each other like *mean_radius* and *mean_perimeter*.","63591c81":"##  Algorithm For Training Model","59127ba2":"## Model Training","773f7d03":"Using dataset without outliers we are getting 91% accuracy which is greater than accuracy with outliers. So now i will save the model and deploy it.","51731b86":"### Model without Outliers","3bb0c1d5":"Looking at the box plot, it seems that the variables *mean_area*, have outlier present in the variables. These outliers value needs to be teated","48a4c5e7":"After Using the Random Forest Classifier our model is giving 90% accuracy and its pretty good. Now Lets try this on the dataset which does not has outliers.","6bafba27":"Our dataset has no missing values which is good for our model.","e1820a7c":"As we can see, after removing the outliers our dataset has reduced I think which is not good as we already have less Data. We will train model using both the datasets and will see the results.","a1147309":"## Outliers","9830ebd6":"### Histogram\nIn the univariate analysis, we use histograms for analyzing and visualizing frequency distribution.","7938878b":"## Correlation","7ad4e3b9":"## Bivariate Analysis\nIt will show the relationship between two variables.","c78bf7da":"## Univariate Analysis","da057562":"## Missing Values","f737a628":"Well, we have visualized the data now its time to train the model.","787ee1e3":"### Saving Model"}}