{"cell_type":{"9945a959":"code","10b2c19c":"code","449ce761":"code","0f65739c":"code","59d86fd6":"code","81015d2f":"code","25056ff0":"code","cdce571c":"code","d9d1b32f":"code","d46b0a95":"code","0fc9dd55":"code","80d10842":"markdown","36b6f9a4":"markdown","457ab998":"markdown","7ac972cc":"markdown","f491228e":"markdown","cf847d4c":"markdown","70896903":"markdown","818043c9":"markdown","d534b640":"markdown"},"source":{"9945a959":"import os\nos.getcwd()","10b2c19c":"import pandas as pd\nimport numpy as np","449ce761":"'''\nIMPORT TRAIN DATA\n'''\ndata = pd.read_csv('..\/input\/titanic\/train.csv')\n\nprint('Length of train data: ', len(data))\ndata.head(5)","0f65739c":"'''\nIMPORT VALIDATION DATA - For Kaggle Submission\n'''\nval_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint('Length of validation data: ', len(val_data))","59d86fd6":"'''\nDATA EXPLORATION\n'''\n\ndata_exploration = data.copy()\n\n# distribution of passengers who survived \/ did not survive\n# 0 = did not survive, 1 = survived\ncount_not_survived = data_exploration['Survived'].value_counts()[0]\ncount_survived = data_exploration['Survived'].value_counts()[1]\n\n# distribution of passengers by gender\ncount_male = data_exploration['Sex'].value_counts()[0]\ncount_female = data_exploration['Sex'].value_counts()[1]\n\n# distribution of passengers by survived AND gender\ndf_survived_gender = data_exploration.groupby(['Survived', 'Sex']).size().reset_index(name = 'Count')\ncount_male_survived = df_survived_gender[(df_survived_gender['Survived'] == 1) & (df_survived_gender['Sex'] == 'male')]['Count'].iloc[0]\ncount_male_not_survived = df_survived_gender[(df_survived_gender['Survived'] == 0) & (df_survived_gender['Sex'] == 'male')]['Count'].iloc[0]\ncount_female_survived = df_survived_gender[(df_survived_gender['Survived'] == 1) & (df_survived_gender['Sex'] == 'female')]['Count'].iloc[0]\ncount_female_not_survived = df_survived_gender[(df_survived_gender['Survived'] == 0) & (df_survived_gender['Sex'] == 'female')]['Count'].iloc[0]\n\n# distribution of passengers by survived AND age\ndf_age_survived = pd.crosstab(pd.cut(data_exploration['Age'], bins = 10), data_exploration['Survived'])\n\n# distribution of passengers by survived AND fare\ndf_fare_survived = pd.crosstab(pd.cut(data_exploration['Fare'], bins = 10), data_exploration['Survived'])\n\n# distribution of passengers by survived AND Pclass (socio-economic class)\ndf_pclass_survived = data_exploration.groupby(['Pclass', 'Survived']).size().reset_index(name = 'Count')\n\n# distribution of passengers by survived AND Parch (# of parents\/children onboard)\ndf_parch_survived = data_exploration.groupby(['Parch', 'Survived']).size().reset_index(name = 'Count')\n\n# distribution of passengers by survived AND SibSp (# of siblings spouse on board)\ndf_sibsp_survived = data_exploration.groupby(['SibSp', 'Survived']).size().reset_index(name = 'Count')\n\nprint(\n'''\n\nSTATISTICS:\n\nNumber of rows in train data: {}\n\nNumber of NA in all columns:\n{}\n\n\n(1) BY GENDER\n\n<1>\nNumber of passengers who survived: {}\nNumber of passengers who did not survive: {}\n\n<2>\nNumber of male passengers: {}\nNumber of female passengers: {}\n\n<3>\nNumber of male passengers who survived: {}\nNumber of male passengers who did not survive: {}\nNumber of female passengers who survived: {}\nNumber of female passengers who did not survived: {}\n\n\n(2) BY AGE\n\n{}\n\n\n(3) BY PASSENGER FARE\n\n{}\n\n\n(4) BY PCLASS\n\n{}\n\n\n(5) BY # OF PARENTS\/CHILDREN\n\n{}\n\n\n(5) BY # OF SIBLINGS \/ SPOUSES\n\n{}\n\n\n'''.format(len(data_exploration), data_exploration.isna().sum(),\n           count_survived, count_not_survived, \n           count_male, count_female, \n           count_male_survived, count_male_not_survived,\n           count_female_survived, count_female_not_survived, \n           df_age_survived,\n           df_fare_survived,\n           df_pclass_survived,\n           df_parch_survived,\n           df_sibsp_survived))","81015d2f":"'''\nFEATURE ENGINEERING\n'''\n\ndef feature_engineering(df):\n    # drop passengerid, Name, ticket columns (identifiers, too many unique values, low relevance)\n    df = df.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)\n\n    # change pclass to categorical\n    df['Pclass'].replace({1: \"HighSES\", 2: \"MidSES\", 3: \"LowSES\"}, inplace = True)    \n    \n    # convert age to categorical type using classification \n    # ref: https:\/\/www.statcan.gc.ca\/en\/concepts\/definitions\/age2\n    # children (1), youth (2), adults (3), seniors (4), nans (0)\n    df['Age'] = pd.to_numeric(pd.cut(df['Age'], bins = [0, 14, 24, 64, 120], labels = [1, 2, 3, 4]))\n    df['Age'] = df['Age'].fillna(0)\n    df['Age'].replace({1: \"Children\", 2: \"Youth\", 3: \"Adults\", 4: \"Seniors\", 0: \"NoAge\"}, inplace = True)\n\n    # convert sibsp, parch to binary (whether siblings\/spouse, parent\/children is onboard (1) OR not (0))\n    df.loc[df['SibSp'] == 0, 'SibSp'] = 0\n    df.loc[df['SibSp'] >= 1, 'SibSp'] = 1\n    df.loc[df['Parch'] == 0, 'Parch'] = 0\n    df.loc[df['Parch'] >= 1, 'Parch'] = 1\n    df['SibSp'].replace({0: \"NoSibSp\", 1: \"YesSibSp\"}, inplace = True)\n    df['Parch'].replace({0: \"NoParch\", 1: \"YesParch\"}, inplace = True)\n\n    # convert sex column to binary (male (1), female (0))\n    df.loc[df['Sex'] == 'male', 'Sex'] = 'Male'\n    df.loc[df['Sex'] == 'female', 'Sex'] = 'Female'\n    \n    # convert passenger fare to first (>= $30), second (>= $13, <= 29), third (<= $12) class\n    # ref: https:\/\/www.bbc.co.uk\/bitesize\/topics\/z8mpfg8\/articles\/zng8jty\n    df['Fare'] = df['Fare'].fillna(0)\n    df.loc[df['Fare'] < 13, 'Fare'] = 3\n    df.loc[(df['Fare'] >= 13) & (df['Fare'] < 30), 'Fare'] = 2\n    df.loc[df['Fare'] >= 30, 'Fare'] = 1\n    df['Fare'].replace({1: \"FirstClsFare\", 2: \"SecondClsFare\", 3: \"ThirdClsFare\", 0: \"NoClsFare\"}, inplace = True)\n\n    # remove Fare column since there is multicollinearity issue between Fare and Pclass columns\n    df = df.drop(['Fare'], axis = 1)\n    \n    # convert embarked to numerical (S = 1, C = 2, Q = 3, nan = 0)\n#    df.loc[df['Embarked'] == 'S', 'Embarked'] = 1\n#    df.loc[df['Embarked'] == 'C', 'Embarked'] = 2\n#    df.loc[df['Embarked'] == 'Q', 'Embarked'] = 3\n\n    # fill nans with most common embarked area (S)\n    df['Embarked'] = df['Embarked'].fillna('S')\n    \n    # performance drops with column\n#    df = df.drop(['Embarked'], axis = 1)\n\n    # convert cabin to binary (value = 1, null = 0)\n    df['Cabin'] = np.where(df['Cabin'].isnull(), 0, 1)\n    \n    # performance drops with column\n    df = df.drop(['Cabin'], axis = 1)\n    \n    # combine SibSp\/Parch columns\n#    df['SibSp_Parch'] = df['SibSp'] + df['Parch']\n    \n    return df\n\ntrain_data_feature_eng = data.copy()\ntrain_data_feature_eng = feature_engineering(train_data_feature_eng)\n\nprint('Number of rows: ', len(train_data_feature_eng))\ntrain_data_feature_eng.head(20)","25056ff0":"'''\nMODEL DEVELOPMENT\n'''\nimport time\n\nfrom numpy import mean\nfrom numpy import std\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# import classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\n# ensemble method\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# CV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ntrain_data_model = train_data_feature_eng.copy()\n\n# get list of column headers without target variable\nfeature_cols = list(train_data_model.columns.values)\nfeature_cols.remove('Survived')\n\n# one-hot encoded X inputs\nencoded_X_train = pd.get_dummies(train_data_feature_eng[feature_cols])\n\nX = encoded_X_train\ny = train_data_model['Survived']\n\n# instantiate with default parameters\nlogreg = LogisticRegression()\nknn = KNeighborsClassifier()\nmlp = MLPClassifier(random_state = 0)\nsvm = SVC()\nnb = GaussianNB()\nclf = tree.DecisionTreeClassifier()\nada = AdaBoostClassifier()\nrf = RandomForestClassifier(max_depth = 5, random_state = 0)\ngbm = GradientBoostingClassifier()\nhistgbm = HistGradientBoostingClassifier()\nxgboost = XGBClassifier()\nlightgbm = LGBMClassifier()\n\n# insert new models and its name here\nall_models_dict = {logreg: 'Logistics Regression', \n                   knn: 'K-Nearest Neighbors',\n                   mlp: 'Multilayer Perceptron',\n                   svm: 'Support Vector Machines',\n                   nb: 'Naive Bayes',\n                   clf: 'Decision Trees',\n                   ada: 'ADABoost',\n                   rf: 'Random Forest',\n                   gbm: 'Gradient Boosting Classifier',\n                   histgbm: 'Histogram Gradient Boosting Classifier',\n                   xgboost: 'XGBoost',\n                   lightgbm: 'LightGBM'}\n\n# perform k-fold CV on model\ndef cross_val_evaluation(model):\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n    n_scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n    mean_n_scores = mean(n_scores)\n    std_n_scores = std(n_scores)\n    return mean_n_scores, std_n_scores\n\n# dataframe to store performance of all model\nmodel_performance = pd.DataFrame()\n\n# lists to store all means, std, names\nall_mean_scores = []\nall_std_scores = []\nall_model_names = []\n\n# iteration to run all models, and store evaluation result \/ names into list\nfor model, model_name in all_models_dict.items():\n    try:\n        start_time = time.time()\n        n_mean, n_std = cross_val_evaluation(model)\n        model.fit(X, y)\n        all_mean_scores.append(n_mean)\n        all_std_scores.append(n_std)\n        all_model_names.append(model_name)\n        end_time = time.time()\n        print(\n        '''Time to execute {}: {} seconds'''.format(model_name, end_time - start_time))\n    except:\n        print('Error with model')\n        continue\n        \n# append result \/ names into dataframe\nmodel_performance['Model Name'] = all_model_names\nmodel_performance['Accuracy (Mean)'] = all_mean_scores\nmodel_performance['Accuracy (STD)'] = all_std_scores","cdce571c":"'''\nSELECT BEST PERFORMING MODEL\n'''\n\nmodel_performance.sort_values('Accuracy (Mean)', ascending = False)","d9d1b32f":"'''\nPredict Survived for unseen data\n'''\n\nval_data_feature_eng = val_data.copy()\n\n# apply same feature engineering steps to validation data\nval_data_feature_eng = feature_engineering(val_data_feature_eng)\n\n# one-hot encoded validation X\nencoded_X_val = pd.get_dummies(val_data_feature_eng)\n\n# predicting Survived using trained model\ny_val = svm.predict(encoded_X_val)\ny_val = y_val.tolist()\n\nprint('Length of y_val: ', len(y_val))","d46b0a95":"'''\nPREPARING PREDICTIONS FOR SUBMISSION\n'''\nval_data_submission = val_data.copy()\n\nval_data_submission = val_data_submission[['PassengerId']]\nval_data_submission['Survived'] = y_val\nval_data_submission.to_csv('submission.csv', index = False)","0fc9dd55":"'''\nUNUSED CODE\n'''\n\n# MODEL EVALUATION USING TRAIN-TEST SPLIT\ndef evaluate_model(y_pred):\n    print(\n'''\nAccuracy: {}\nPrecision: {}\nRecall: {}\nF1 Score: {}\n\n'''.format(metrics.accuracy_score(y_test, y_pred),\n           metrics.precision_score(y_test, y_pred),\n           metrics.recall_score(y_test, y_pred),\n           metrics.f1_score(y_test, y_pred)))\n\n    \n# TRAIN-TEST SPLIT\n'''\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# using the models to make predictions after model.fit and train-test split\nlogreg_y_pred = logreg.predict(X_test)\nknn_y_pred = knn.predict(X_test)\nmlp_y_pred = mlp.predict(X_test)\nsvm_y_pred = svm.predict(X_test)\nnb_y_pred = nb.predict(X_test)\nclf_y_pred = clf.predict(X_test)\nada_y_pred = ada.predict(X_test)\nrf_y_pred = rf.predict(X_test)\ngbm_y_pred = gbm.predict(X_test)\n\n# model prediction accuracy on test data\nprint('LogReg')\nevaluate_model(logreg_y_pred)\nprint('KNN')\nevaluate_model(knn_y_pred)\nprint('MLP')\nevaluate_model(mlp_y_pred)\nprint('SVM')\nevaluate_model(svm_y_pred)\nprint('Naive Bayes')\nevaluate_model(nb_y_pred)\nprint('Decision Trees')\nevaluate_model(clf_y_pred)\nprint('ADA Boost')\nevaluate_model(ada_y_pred)\nprint('Random Forest')\nevaluate_model(rf_y_pred)\nprint('GBM')\nevaluate_model(gbm_y_pred)\n'''\n\n# LONG METHOD TO VALIDATE MODEL USING KFOLD\n\n'''\ndef cross_val_evaluation(model):\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n    n_scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n\n# fitting the models to the data\nmodel_name = 'Logistic Regression'\nprint(model_name)\ncross_val_evaluation(logreg)\nlogreg.fit(X_train, y_train)\n\nprint('KNN')\ncross_val_evaluation(knn)\nknn.fit(X_train, y_train)\n\nprint('MLP')\ncross_val_evaluation(mlp)\nmlp.fit(X_train, y_train)\n\nprint('SVM')\ncross_val_evaluation(svm)\nsvm.fit(X_train, y_train)\n\nprint('Naive Bayes')\ncross_val_evaluation(nb)\nnb.fit(X_train, y_train)\n\nprint('Decision Trees')\ncross_val_evaluation(clf)\nclf.fit(X_train, y_train)\n\nprint('ADA Boost')\ncross_val_evaluation(ada)\nada.fit(X_train, y_train)\n\nprint('Random Forest')\ncross_val_evaluation(rf)\nrf.fit(X_train, y_train)\n\nprint('GBM')\ncross_val_evaluation(gbm)\ngbm.fit(X_train, y_train)\n\nprint('HistGBM')\ncross_val_evaluation(histgbm)\nhistgbm.fit(X_train, y_train)\n\nprint('XGBoost')\ncross_val_evaluation(xgboost)\nxgboost.fit(X_train, y_train)\n\nprint('LightGBM')\ncross_val_evaluation(lightgbm)\nlightgbm.fit(X_train, y_train)\n'''","80d10842":"# 1. Import Train \/ Validation data","36b6f9a4":"# 7. Creating CSV file for submission\n### Competition details: https:\/\/www.kaggle.com\/c\/titanic\/overview","457ab998":"# Competition Details\n#### Platform: Kaggle\n#### Competition Title: Titanic - Machine Learning from Disaster\n#### Link: https:\/\/www.kaggle.com\/c\/titanic\/overview\n#### Author: Jonas Ng Jing Xun\n##### LinkedIn: https:\/\/sg.linkedin.com\/in\/jonasnjx\n##### Github: https:\/\/github.com\/jonasnjx\n#### Thanks for viewing!","7ac972cc":"# 2. Data Exploration","f491228e":"# 6. Predicting Survived or not for validation data","cf847d4c":"# 5. Choosing the best algorithm","70896903":"# 4. Testing out different algorithms on dataset","818043c9":"# 3. Feature Engineering (Create new columns, merge columns etc)","d534b640":"# 8. Appendix"}}