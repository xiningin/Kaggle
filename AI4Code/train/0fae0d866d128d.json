{"cell_type":{"df353fc6":"code","8f5e0935":"code","4f6572b8":"code","3ca27524":"code","578db7fe":"code","104e9a61":"code","435bc893":"code","77649209":"code","680d5787":"code","5b633ee4":"code","c90e5d75":"code","70418041":"code","b3c37b09":"code","bee5ee9e":"code","f77060d4":"code","78fee963":"code","139e4eb5":"code","3e781404":"code","fdc8a9c5":"code","11b2658f":"code","7a402d05":"code","9f0badb7":"code","c76ca72c":"code","9b80ae0a":"code","7dd037b5":"code","fc9a6b7c":"code","f593a333":"code","15ad624c":"code","5635979d":"code","7bec4054":"code","575ce3c1":"code","cab42d50":"code","32b31c3b":"code","8bd73bc3":"code","0f1afe90":"code","9f500ff2":"code","f6a151f8":"code","878ad574":"code","fda458ee":"code","399ef316":"code","ba0e35ca":"code","bb310e88":"code","cf16410f":"code","cc4a6fb2":"code","c37b672a":"code","2185c030":"code","4dd1a552":"code","b57575fc":"markdown","c14365b1":"markdown","be4376a6":"markdown","8f70bdcc":"markdown","25b842a2":"markdown","6f557a9a":"markdown","c29301d6":"markdown","f757c251":"markdown","cd42a1cb":"markdown","ace4e7eb":"markdown","e2819080":"markdown","45342d93":"markdown","f3f35f0b":"markdown","dc997746":"markdown","94f2d3c0":"markdown","958ebb99":"markdown","651151be":"markdown","7639c48d":"markdown"},"source":{"df353fc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f5e0935":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n\nimport time\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","4f6572b8":"class LabelEncoderExt(object):\n    def __init__(self):\n        \"\"\"\n        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n        \"\"\"\n        self.label_encoder = LabelEncoder()\n        # self.classes_ = self.label_encoder.classes_\n\n    def fit(self, data_list):\n        \"\"\"\n        This will fit the encoder for all the unique values and introduce unknown value\n        :param data_list: A list of string\n        :return: self\n        \"\"\"\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n\n        return self\n\n    def transform(self, data_list):\n        \"\"\"\n        This will transform the data_list to id list where the new values get assigned to Unknown class\n        :param data_list:\n        :return:\n        \"\"\"\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n\n        return self.label_encoder.transform(new_data_list)","3ca27524":"df = pd.read_csv(r'\/kaggle\/input\/black-friday-sales-prediction\/train.csv')\ndf.head()","578db7fe":"df2 = df.copy()","104e9a61":"ii = IterativeImputer(random_state=0)","435bc893":"def DataCleaning(df):\n    df['Product_Category_2'].fillna(0,inplace = True)\n    df['Product_Category_3'].fillna(0,inplace = True)\n    df['Product_Category_2'] = df['Product_Category_2'].astype(int) \n    df['Product_Category_3'] = df['Product_Category_3'].astype(int) \n\n    df['Gender'] = np.where(df['Gender']=='M', 1, 0)\n\n    df = pd.get_dummies(df, columns=['Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years',\n                                     'Product_Category_1', 'Product_Category_2', 'Product_Category_3'])\n    \n    return df","77649209":"df = DataCleaning(df)","680d5787":"df.columns","5b633ee4":"df['Product_Category_2_1'] = 0\ndf['Product_Category_2_19'] = 0\ndf['Product_Category_2_20'] = 0\n\ndf['Product_Category_3_1'] = 0\ndf['Product_Category_3_2'] = 0\ndf['Product_Category_3_7'] = 0\ndf['Product_Category_3_19'] = 0\ndf['Product_Category_3_20'] = 0\n\n\ndf = df.drop('Product_Category_2_0', axis=1)\ndf = df.drop('Product_Category_3_0', axis=1)","c90e5d75":"myL = ('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20')","70418041":"myL2 = ['Product_Category_1_1', 'Product_Category_1_2', 'Product_Category_1_3', \n        'Product_Category_1_4', 'Product_Category_1_5', 'Product_Category_1_6',\n        'Product_Category_1_7', 'Product_Category_1_8', 'Product_Category_1_9', \n        'Product_Category_1_10', 'Product_Category_1_11', 'Product_Category_1_12',\n        'Product_Category_1_13', 'Product_Category_1_14', 'Product_Category_1_15',\n        'Product_Category_1_16', 'Product_Category_1_17', 'Product_Category_1_18',\n        'Product_Category_1_19', 'Product_Category_1_20', 'Product_Category_2_1',\n        'Product_Category_2_2', 'Product_Category_2_3', 'Product_Category_2_4',\n        'Product_Category_2_5', 'Product_Category_2_6', 'Product_Category_2_7',\n        'Product_Category_2_8', 'Product_Category_2_9', 'Product_Category_2_10',\n        'Product_Category_2_11', 'Product_Category_2_12', 'Product_Category_2_13',\n        'Product_Category_2_14', 'Product_Category_2_15', 'Product_Category_2_16',\n        'Product_Category_2_17', 'Product_Category_2_18', 'Product_Category_2_19',\n        'Product_Category_2_20', 'Product_Category_3_1', 'Product_Category_3_2',\n        'Product_Category_3_3', 'Product_Category_3_4', 'Product_Category_3_5',\n        'Product_Category_3_6', 'Product_Category_3_7', 'Product_Category_3_8',\n        'Product_Category_3_9', 'Product_Category_3_10', 'Product_Category_3_11',\n        'Product_Category_3_12', 'Product_Category_3_13', 'Product_Category_3_14',\n        'Product_Category_3_15', 'Product_Category_3_16', 'Product_Category_3_17',\n        'Product_Category_3_18', 'Product_Category_3_19', 'Product_Category_3_20']","b3c37b09":"df.columns","bee5ee9e":"df4=pd.DataFrame()","f77060d4":"for i in myL:\n    df4['Product_Category_'+i] = df['Product_Category_1_'+i] + df['Product_Category_2_'+i] + df['Product_Category_3_'+i]","78fee963":"df4","139e4eb5":"df = df.drop(myL2, axis=1)\ndf","3e781404":"df = df.merge(df4, left_index=True, right_index=True)\ndf","fdc8a9c5":"df.info()","11b2658f":"df.columns","7a402d05":"label = LabelEncoderExt()  \nlabel.fit(df['Product_ID'])\ndf['Product_ID'] = label.transform(df['Product_ID'])","9f0badb7":"df.head()","c76ca72c":"plt.figure(figsize = ( 20 , 15 )) \nsns.heatmap(df.corr(), cmap='cubehelix')","9b80ae0a":"y = df.Purchase.values\n\nfeatures = ['User_ID', 'Product_ID', 'Gender', 'Marital_Status', 'Age_0-17',\n            'Age_18-25', 'Age_26-35', 'Age_36-45', 'Age_46-50', 'Age_51-55', 'Age_55+',\n            'Occupation_0', 'Occupation_1', 'Occupation_2', 'Occupation_3', 'Occupation_4',\n            'Occupation_5', 'Occupation_6', 'Occupation_7', 'Occupation_8', 'Occupation_9', 'Occupation_10',\n            'Occupation_11', 'Occupation_12', 'Occupation_13', 'Occupation_14', 'Occupation_15', \n            'Occupation_16', 'Occupation_17', 'Occupation_18', 'Occupation_19', 'Occupation_20',\n            'City_Category_A', 'City_Category_B', 'City_Category_C',\n            'Stay_In_Current_City_Years_0', 'Stay_In_Current_City_Years_1', 'Stay_In_Current_City_Years_2',\n            'Stay_In_Current_City_Years_3', 'Stay_In_Current_City_Years_4+',\n            'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Product_Category_4',\n            'Product_Category_5', 'Product_Category_6', 'Product_Category_7', 'Product_Category_8',\n            'Product_Category_9', 'Product_Category_10', 'Product_Category_11', 'Product_Category_12',\n            'Product_Category_13', 'Product_Category_14', 'Product_Category_15', 'Product_Category_16',\n            'Product_Category_17', 'Product_Category_18', 'Product_Category_19', 'Product_Category_20']\n\nX = df[features].values","7dd037b5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","fc9a6b7c":"sc = StandardScaler()\nscaler = sc.fit(X)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","f593a333":"# Number of trees in random forest\nn_estimators = [500, 800, 1300]\n\n# Number of features to consider at every split\n# max_features = ['auto', 'sqrt', 80] # auto is best\n\n# Maximum number of levels in tree\n# max_leaf_nodes = [100, 1000, 2000, 5000]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [40, 60, 100]\n\n# # Minimum number of samples required at each leaf node\nmin_samples_leaf = [8, 10, 15]\n\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf\n              }\n\nprint(random_grid)","15ad624c":"rfr = RandomForestRegressor(random_state=42, verbose=True,\n                                     max_leaf_nodes=5000, \n#                                      min_samples_leaf=4, bootstrap=True, min_samples_split=15, \n#                                      max_depth=50, max_features=90, min_samples_split=2,\n#                                      n_estimators=400, max_features='auto',\n                                     n_jobs=4)","5635979d":"start_time = time.time()\n\nCV_rfr = RandomizedSearchCV(estimator=rfr, param_distributions=random_grid, cv=2)\nCV_rfr.fit(X_train, y_train)\n\nprint(\"--- %s min ---\" % ((time.time() - start_time)\/60))","7bec4054":"print(\"best_estimator_\", CV_rfr.best_estimator_) \n# print(\"best_index_\", CV_rfr.best_index_) \nprint(\"best_params_\", CV_rfr.best_params_) \n# print(\"cv_results_\", CV_rfr.cv_results_) \nprint(\"get_params\", CV_rfr.get_params) \n# print(\"n_features_in_\", CV_rfr.n_features_in_) \n# print(\"n_splits_\", CV_rfr.n_splits_) ","575ce3c1":"rfr = CV_rfr.best_estimator_\n\n# Below is the best estimator achieved so far. \n# Feel free to throw in some comments if you think you can help us improve! \n# Would love to hear from you all.\n\n# rfr = RandomForestRegressor(max_leaf_nodes=5000, min_samples_split=60,\n#                             n_estimators=1300, n_jobs=4, random_state=42,\n#                             verbose=True)\n","cab42d50":"start_time = time.time()\nrfr.fit(X_train, y_train)\ny_pred = rfr.predict(X_test)\nprint(mean_squared_error(y_test, y_pred, squared=False))\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","32b31c3b":"test=pd.read_csv(r'..\/input\/black-friday-sales-prediction\/test.csv')\ntest.head()","8bd73bc3":"test = DataCleaning(test)","0f1afe90":"test['Product_ID'] = label.transform(test['Product_ID']) ","9f500ff2":"test.info()","f6a151f8":"test.columns","878ad574":"test['Product_Category_1_19'] = 0\ntest['Product_Category_1_20'] = 0\n\ntest['Product_Category_2_1'] = 0\ntest['Product_Category_2_19'] = 0\ntest['Product_Category_2_20'] = 0\n\ntest['Product_Category_3_1'] = 0\ntest['Product_Category_3_2'] = 0\ntest['Product_Category_3_7'] = 0\ntest['Product_Category_3_19'] = 0\ntest['Product_Category_3_20'] = 0\n\n\ntest = test.drop('Product_Category_2_0', axis=1)\ntest = test.drop('Product_Category_3_0', axis=1)","fda458ee":"df4=pd.DataFrame()","399ef316":"for i in myL:\n    df4['Product_Category_'+i] = test['Product_Category_1_'+i] + test['Product_Category_2_'+i] + test['Product_Category_3_'+i]","ba0e35ca":"test = test.drop(myL2, axis=1)\ntest","bb310e88":"test = test.merge(df4, left_index=True, right_index=True)\ntest","cf16410f":"test.info()","cc4a6fb2":"test[features]","c37b672a":"test2 = test[features].values\ntest2 = scaler.transform(test)","2185c030":"output=pd.read_csv(r'..\/input\/black-friday-sales-prediction\/test.csv',usecols=['User_ID','Product_ID'])\noutput['Purchase']=rfr.predict(test2)\noutput2 = output[['Purchase','User_ID','Product_ID']]\noutput2.to_csv('submission.csv',index=False)","4dd1a552":"os.listdir()","b57575fc":"## Scaling Down\n\nUsing sklearn.preprocessing.StandardScaler to reduce the data items to smaller numeric values which in turn helps in conducting faster calculations for huge matrices.","c14365b1":"#### Handling Product_Category_x fields","be4376a6":"## RandomizedSearchCV\n\nUsing RandomizedSearchCV to effectively tune the hyperparamets and choose the best estimator configuration.","8f70bdcc":"### Scaling down\n\nscaling down the test dataset","25b842a2":"## Data Pre-processing\n\nProcessing and cleaning the data before using it for training the model.\n\n1. Data Imputation\n2. Data Cleaning\n3. One-Hot encoding - using pd.get_dummies()\n","6f557a9a":"## Test dataset\n\nreading the Test dataset and preprocessing it in similar steps as the train set","c29301d6":"## Import","f757c251":"### Some of my Observations\n\n* If you select a specific Product_ID and look up for its other occurrences in the dataset, you will notice each of the rows will have the same values in Product_Category_1, 2, and 3.\n* Product_Category_1 will be filled first. Only if Product_Category_1 is filled and there's need for more room, Product_Category_2 is used. Same for Product_Category_3. In other words, Product_Category_3 will never be filled keeping Product_Category_2 or Product_Category_1 empty.\n* These values here, are masked and represented numerically since we do not need to know the exact values. ","cd42a1cb":"## Handling Missing Value\n\nIterativeImputer is an experimental feature in sklearn module.\nIt studies the other columns in the dataset and intelligently populates the missing values.\nThis is a smarter way to fill the missing values. Instead of filling with a single value in all the empty cells, this is a better approach to fill in considering various proportions.","ace4e7eb":"## LabelEncoder + handle unknowns\n\nThis is an extended version of sklearn.preprocessing.LabelEncoder class","e2819080":"## Handling Product_Category_x fields","45342d93":"## Train dataset\n\nThis is the train set - input for our model to be trained","f3f35f0b":"### submission.csv is ready!\n\nSo far, we've achieved 2679 error (RMSE) on test dataset.\nWe appreciate if you can share your comments and help us improve on our model.\n\n","dc997746":"### Understanding through an analogy\n\nLet's look at a scenario where there's a User Details Dataset, and in the contact details section.\n- Many users will have only 1 mobile number. The next 2 fields shall be kept null.\n- However, if required, a single user-id can have more than one mobile numbers. That's where the next 2 fields come into picture.\n- And, Mobile_Number_3 will only be used if Mobile_Number_2 field is already populated.\n","94f2d3c0":"## Predicting\n\nPredicting Purchase values for Test dataset","958ebb99":"#### Base Estimator configuration","651151be":"## Heatmap\n\n... to show correlation factor","7639c48d":"## Train-Test split"}}