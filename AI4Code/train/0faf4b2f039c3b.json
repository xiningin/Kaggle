{"cell_type":{"d2c7c24d":"code","691aa09f":"code","8fe26d98":"code","8226d0de":"code","d8178e64":"code","1a152bc5":"code","e0a38e39":"code","26404ff1":"code","bec16967":"code","643a3c98":"code","90853182":"code","bdcab0ae":"code","2a4efd3e":"code","e56ca70d":"code","3970f6c9":"code","8dbaa07e":"code","ad1c00c0":"code","fdc14cda":"code","aeeb4a37":"markdown","33f6ed80":"markdown","4e436439":"markdown","6bbe99b2":"markdown","6675b32b":"markdown","d4e0e9fc":"markdown","b8c3b363":"markdown"},"source":{"d2c7c24d":"%%capture\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, recall_score, roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport xgboost as xgb\nimport wandb","691aa09f":"heart = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\nheart.head()","8fe26d98":"heart.info()\n# heart.describe()","8226d0de":"heart.HeartDisease.value_counts()","d8178e64":"le = LabelEncoder()\n \nheart['ExerciseAngina'] = le.fit_transform(heart['ExerciseAngina'])\nheart['Sex']            = le.fit_transform(heart['Sex'])\nheart.head(2)\n","1a152bc5":"var_of_interest = \"ST_Slope\" \n# var_of_interest = \"ChestPainType\"\n# var_of_interest = \"RestingECG\"\n\nsns.countplot(data=heart, x=\"HeartDisease\", hue=var_of_interest)","e0a38e39":"# ordinal encode ST_Slope\nmapper = {'Up': 0, 'Down': 1, 'Flat': 3}\nheart[\"ST_Slope\"] = heart[\"ST_Slope\"].replace(mapper)\n\n# OHE ChestPainType & RestingECG\nheart = pd.get_dummies(heart, prefix=[\"CPT\", \"RECG\"])\n\nheart.head(2)","26404ff1":"y = heart.pop('HeartDisease')\nX = heart\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nfeature_names = X.columns\nlabels = [\"NoDisease\", \"HeartDisease\"]","bec16967":"baseline_config = {\"dataset\": \"UCI HeartDisease\", \"type\": \"baseline\"}","643a3c98":"def train_clf(clf_type):\n    # initialize model\n    if clf_type == \"LogisticRegression\":\n        clf = LogisticRegression(solver=\"liblinear\", max_iter=3000)\n    elif clf_type == \"RandomForest\":\n        clf = RandomForestClassifier(n_estimators=15)\n    elif clf_type == \"NaiveBayes\":\n        clf = GaussianNB()\n    elif clf_type == \"XGBoost\":\n        clf = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", \n                                use_label_encoder=False, nthread = -1)\n    else:\n        return\n        \n    print(f\"training {clf_type}\")\n    # train model\n    clf.fit(X_train, y_train)\n    # make preds\n    y_pred = clf.predict(X_test)\n    y_probas = clf.predict_proba(X_test)\n    \n    # log metrics to w&b\n    metrics = {'accuracy score': accuracy_score(y_test, y_pred),\n               'recall score': recall_score(y_test, y_pred),\n               'ROC': roc_curve(y_test, y_pred)}\n\n    wandb.log(metrics)\n    \n    # log plots to w&b\n    wandb.sklearn.plot_precision_recall(y_test, y_probas, labels)\n    wandb.sklearn.plot_roc(y_test, y_probas, labels)\n    \n    wandb.finish()","90853182":"model = \"LogisticRegression\"\n\nwandb.init(project=\"heart-disease-clf\", name=model, config=baseline_config)","bdcab0ae":"train_clf(clf_type=model)","2a4efd3e":"model = \"RandomForest\"\n\nwandb.init(project=\"heart-disease-clf\", name=model, config=baseline_config)\n\ntrain_clf(clf_type=model)","e56ca70d":"model = \"NaiveBayes\"\n\nwandb.init(project=\"heart-disease-clf\", name=model)\n\ntrain_clf(clf_type=model)","3970f6c9":"model = \"XGBoost\"\n\nwandb.init(project=\"heart-disease-clf\", name=model, config=baseline_config)\n\ntrain_clf(clf_type=model)","8dbaa07e":"def gridsearch_clf(clf_type, param_grid):\n    # initialize model\n    if clf_type == \"RandomForestTuned\":\n        clf = RandomForestClassifier()\n    elif clf_type == \"XGBoostTuned\":\n        clf = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", \n                                use_label_encoder=False, nthread = -1)\n    else:\n        return\n        \n    cv_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5, scoring=\"recall\") \n        \n    print(f\"training {clf_type}\")\n    # train model\n    cv_clf.fit(X_train, y_train)\n    \n    print(\"Best: %f using %s\" % (cv_clf.best_score_, cv_clf.best_params_))\n        \n    # make preds\n    y_pred = cv_clf.predict(X_test)\n    y_probas = cv_clf.predict_proba(X_test)\n    \n    # log metrics to w&b\n    wandb.log({'accuracy score': accuracy_score(y_test, y_pred),\n               'recall score': recall_score(y_test, y_pred),\n               'ROC': roc_curve(y_test, y_pred)})\n    \n    # log best params\n    wandb.config.update(cv_clf.best_params_)\n    \n    # log plots to w&b\n    wandb.sklearn.plot_precision_recall(y_test, y_probas, labels)\n    wandb.sklearn.plot_roc(y_test, y_probas, labels)\n    \n    wandb.finish()","ad1c00c0":"hp_tuned_config = {\"dataset\": \"UCI HeartDisease\", \"type\": \"hp_tuned\"}\n\nmodel = \"RandomForestTuned\"\n\nwandb.init(project=\"heart-disease-clf\", name=model, config=hp_tuned_config)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4, 6, 8],\n    'criterion' :['gini', 'entropy']\n}\n\ngridsearch_clf(clf_type=model, param_grid=param_grid)","fdc14cda":"model = \"XGBoostTuned\"\n\nwandb.init(project=\"heart-disease-clf\", name=model, config=hp_tuned_config)\n\nparam_grid = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}\n\ngridsearch_clf(clf_type=model, param_grid=param_grid)","aeeb4a37":"Balanced enough to not require any tricks.\n\n## Feature Engineering\nFirst thing I'll do is binarize `ExerciseAngina` and `Sex`","33f6ed80":"## Results\n\nLook like our highest performing model was the last one, the tuned XGBoost. \n\nWhy I'm regarding *recall* as the most important metric here:\n* If we were creating a real heart-disease-detector-model, we'd want to reduce false negatives i.e. telling someone they don't have heart disease when they really do","4e436439":"No nulls \ud83d\ude0e. Not doing any outlier checks so no real cleanup required.\n\nBefore any next steps I want to see what the class balance looks like.","6bbe99b2":"I need to get an idea of these categories, their values, and their possible correlation to heart disease. I kept this quick and dirty, deriving my intuition from the simple count plot above. `ST_Slope` is the only one IMO that displayed a defined ordinality.\n* ST_Slope = Up    -> less Heart Disease\n* ST_Slope = Down -> some Heart Disease\n* ST_Slope = Flat -> lots of Heart Disease\n\nI'll map these to values then one hot the other columns","6675b32b":"I could go ahead and dummy\/OHE all of the categorical features since there aren't that many and my feature space is still relatively small. \n\nEither way - I don't know much about what these columns actually *mean* so I'll take a quick look at them to see if there's any ordinality.","d4e0e9fc":"## Tuning\n\nNow, I could use w&b's really cool looking `sweep` functionality to do this but I simply don't have the time to learn the API for it at this time.\n\nWhat I'm going to do instead is GridSearch for optimal parameters for the same 4 models, minus NaiveBayes, and log the results to w&b.","b8c3b363":"Train & log Logistic Regression"}}