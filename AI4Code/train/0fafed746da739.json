{"cell_type":{"e56974a8":"code","07614a26":"code","870c4938":"code","d2bd4583":"code","ec710114":"code","f800f669":"code","f744ec90":"code","b1cd37df":"code","285e59bc":"code","a5edd8e9":"code","68854d34":"code","adc0558d":"code","c902d6e7":"code","532d5fe2":"code","7dd227e0":"code","66b2210b":"code","bc14bdc1":"code","1a0e9116":"code","552794a7":"code","113ea3f1":"code","b54dec37":"code","c469217e":"code","39fd171b":"code","352346a5":"code","804c6918":"code","058191ac":"code","e7e64d44":"code","9c65d705":"code","e5b56cd5":"code","07180b0e":"code","5c46f86d":"code","a6ae6d79":"code","6729f655":"code","74a9f5e9":"code","caa9902f":"code","5ba6f889":"code","60388239":"code","16d29947":"code","3d4ac51f":"code","e751ab52":"code","e408817a":"code","5ac84c12":"code","8e22ac9c":"code","7cbebefb":"code","7ee84f39":"code","8cb02603":"code","ef40144e":"code","db4cf0fe":"code","b5c2d2e8":"code","aa0af330":"code","728ab037":"code","f10696bb":"code","e359e35c":"code","8e483778":"code","f3856074":"code","70eaefb5":"code","259b9043":"code","b588b048":"code","6fe6b10c":"code","e94271a1":"code","8dff44b3":"code","bd286811":"code","92d7466e":"code","3349e0b0":"markdown","357fcdcd":"markdown","dfa9254e":"markdown","fe010a20":"markdown","3ac14dd7":"markdown","02748b70":"markdown","0a9b8e4f":"markdown","9b25b346":"markdown","cee5818c":"markdown","61f93e87":"markdown","810e2429":"markdown","368208a4":"markdown","584af41f":"markdown","41d090b6":"markdown","c124a92b":"markdown","6caf45fb":"markdown","a862d074":"markdown","c8c3c047":"markdown","e45744f2":"markdown","5c364aa7":"markdown","a3cf5927":"markdown","465729fa":"markdown","84a1617f":"markdown","865ba4de":"markdown","a928eea6":"markdown"},"source":{"e56974a8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \n%matplotlib inline \nplt.style.use('fivethirtyeight')\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics","07614a26":"pima_column_names = ['times_pregnant', 'plasma_glucose_concentration', 'diastolic_blood_pressure', 'triceps_thickness', 'serum_insulin', 'bmi', 'pedigree_function', 'age', 'onset_diabetes']\npima = pd.read_csv(r'..\/input\/pima-indians-diabetes-database\/diabetes.csv',names = pima_column_names,skiprows=1)\npima.head()\n\n","870c4938":"pima.info()","d2bd4583":"pima['onset_diabetes'].value_counts(normalize=True) ","ec710114":"col = 'plasma_glucose_concentration'\nplt.figure(figsize=(10,5))\nplt.hist(pima[pima['onset_diabetes']==0][col], 10, alpha=0.5, label='non-diabetes')\nplt.hist(pima[pima['onset_diabetes']==1][col], 10, alpha=0.5, label='diabetes')\nplt.legend(loc='upper right')\nplt.xlabel(col)\nplt.ylabel('Frequency')\nplt.title('Histogram of {}'.format(col))\nplt.show()","f800f669":"for col in ['bmi', 'diastolic_blood_pressure', 'serum_insulin','triceps_thickness', 'plasma_glucose_concentration']:\n    plt.figure(figsize=(8,4))\n    plt.hist(pima[pima['onset_diabetes']==0][col], 10, alpha=0.5, label='non-diabetes')\n    plt.hist(pima[pima['onset_diabetes']==1][col], 10, alpha=0.5, label='diabetes')\n    plt.legend(loc='upper right')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.title('Histogram of {}'.format(col))\n    plt.show()","f744ec90":"# look at the heatmap of the correlation matrix of our dataset\nplt.figure(figsize=(12,8))\ncorr=round(pima.corr(),2)\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr,mask=mask, square=True, annot=True)\nplt.xticks(rotation=90)\nplt.show()\n# plasma_glucose_concentration definitely seems to be an interesting feature here\n\n#Following is the correlation matrix of our dataset. This is showing us the correlation amongst \n#the different columns in our Pima dataset. The output is as follows:","b1cd37df":"pima.corr()['onset_diabetes'] ","285e59bc":"pima.describe()","a5edd8e9":"pima['serum_insulin'] = pima['serum_insulin'].map(lambda x:x if x != 0 else None)\n# manually replace all 0's with a None value\n\npima['serum_insulin'].isnull().sum()","68854d34":"pima.describe()","adc0558d":"columns = ['bmi', 'plasma_glucose_concentration', 'diastolic_blood_pressure', 'triceps_thickness']\n\nfor col in columns:\n    pima[col] = pima[col].map(lambda x:x if x != 0 else None)","c902d6e7":"pima.isnull().sum()","532d5fe2":"pima.info()","7dd227e0":"pima.describe()","66b2210b":"pima.head(5)","bc14bdc1":"pima['plasma_glucose_concentration'].mean(), pima['plasma_glucose_concentration'].std()\n","1a0e9116":"empty_plasma_index = pima[pima['plasma_glucose_concentration'].isnull()].index\npima.loc[empty_plasma_index]['plasma_glucose_concentration']","552794a7":"# Will try to impute the missing values from the existing v\ndef relation_with_output( column ):\n    temp = pima[pima[column].notnull()]\n    d= temp[[column,'onset_diabetes']].groupby(['onset_diabetes'])[column].apply(lambda x: x.median()).reset_index()\n    return d","113ea3f1":"#lets look relation of missing columns with onset_diabetes\nrelation_with_output('plasma_glucose_concentration')","b54dec37":"relation_with_output('diastolic_blood_pressure')","c469217e":"relation_with_output('triceps_thickness')","39fd171b":"relation_with_output('serum_insulin')","352346a5":"relation_with_output('bmi')","804c6918":"pima.isnull().sum()","058191ac":"pima.loc[(pima['onset_diabetes'] == 0 ) & (pima['serum_insulin'].isnull()), 'serum_insulin'] = 102.5\npima.loc[(pima['onset_diabetes'] == 1 ) & (pima['serum_insulin'].isnull()), 'serum_insulin'] = 169.5","e7e64d44":"pima.loc[(pima['onset_diabetes'] == 0 ) & (pima['bmi'].isnull()), 'bmi'] = 30.1\npima.loc[(pima['onset_diabetes'] == 1 ) & (pima['bmi'].isnull()), 'bmi'] = 34.3\n\n","9c65d705":"pima.loc[(pima['onset_diabetes'] == 0 ) & (pima['triceps_thickness'].isnull()), 'triceps_thickness'] = 27.0\npima.loc[(pima['onset_diabetes'] == 1 ) & (pima['triceps_thickness'].isnull()), 'triceps_thickness'] = 32.0\n\n","e5b56cd5":"pima.loc[(pima['onset_diabetes'] == 0 ) & (pima['diastolic_blood_pressure'].isnull()), 'diastolic_blood_pressure'] = 70.0\npima.loc[(pima['onset_diabetes'] == 1 ) & (pima['diastolic_blood_pressure'].isnull()), 'diastolic_blood_pressure'] = 75.0","07180b0e":"pima.loc[(pima['onset_diabetes'] == 0 ) & (pima['plasma_glucose_concentration'].isnull()), 'plasma_glucose_concentration'] = 107.0\npima.loc[(pima['onset_diabetes'] == 1 ) & (pima['plasma_glucose_concentration'].isnull()), 'plasma_glucose_concentration'] = 140.0\n","5c46f86d":"# fill the column's missing values with the mean of the rest of the column\n#pima['plasma_glucose_concentration'].fillna(pima['plasma_glucose_concentration'].mean(), inplace=True)\npima.isnull().sum()","a6ae6d79":"X = pima.loc[:,:'age']\ny = pima['onset_diabetes']","6729f655":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)","74a9f5e9":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","caa9902f":"rfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc.score(X_test, y_test)","5ba6f889":"pima.hist(figsize=(15, 15))\nplt.show()","60388239":"pima.info()\n","16d29947":"pima.hist(figsize=(15, 15), sharex=True)\nplt.show()","3d4ac51f":"print (pima['plasma_glucose_concentration'].head())","e751ab52":"# get the mean of the column\nmu = pima['plasma_glucose_concentration'].mean()\n\n# get the standard deviation of the column\nsigma = pima['plasma_glucose_concentration'].std()\n\n# calculate z scores for every value in the column.\nprint (((pima['plasma_glucose_concentration'] - mu) \/ sigma).head())","e408817a":"# mean and std before z score standardizing\npima['plasma_glucose_concentration'].mean(), pima['plasma_glucose_concentration'].std()\n\n(121.68676277850591, 30.435948867207657)\n\n\nax = pima['plasma_glucose_concentration'].hist()\nax.set_title('Distribution of plasma_glucose_concentration')","5ac84c12":"scaler = StandardScaler()\n\nglucose_z_score_standardized = scaler.fit_transform(pima[['plasma_glucose_concentration']])\nglucose_z_score_standardized.mean(), glucose_z_score_standardized.std()","8e22ac9c":"ax = pd.Series(glucose_z_score_standardized.reshape(-1,)).hist()\nax.set_title('Distribution of plasma_glucose_concentration after Z Score Scaling')","7cbebefb":"scale = StandardScaler() # instantiate a z-scaler object\n\npima_scaled = pd.DataFrame(scale.fit_transform(pima), columns=pima_column_names)\npima_scaled.hist(figsize=(15, 15), sharex=True)\nplt.show()","7ee84f39":"mean_impute_standardize = Pipeline([('imputer', SimpleImputer()), ('standardize', StandardScaler()), ('classify', knn)])\nX = pima.drop('onset_diabetes', axis=1)\ny = pima['onset_diabetes']\n\nknn_params = {'imputer__strategy':['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\n#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)\ngrid = GridSearchCV(mean_impute_standardize, knn_params)\ngrid.fit(X, y)\n\nprint (grid.best_score_, grid.best_params_)","8cb02603":"min_max = MinMaxScaler()\npima_min_maxed = pd.DataFrame(min_max.fit_transform(pima), columns=pima_column_names)\npima_min_maxed.describe()","ef40144e":"mean_impute_standardize = Pipeline([('imputer', SimpleImputer()), ('standardize', MinMaxScaler()), ('classify', knn)])\nX = pima.drop('onset_diabetes', axis=1)\ny = pima['onset_diabetes']\n\nknn_params = {'imputer__strategy': ['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\ngrid = GridSearchCV(mean_impute_standardize, knn_params)\ngrid.fit(X, y)\n\nprint (grid.best_score_, grid.best_params_)","db4cf0fe":"np.sqrt((pima**2).sum(axis=1)).mean() \n# average vector length of imputed matrix","b5c2d2e8":"normalize = Normalizer()\npima_normalized = pd.DataFrame(normalize.fit_transform(pima), columns=pima_column_names)\nnp.sqrt((pima_normalized**2).sum(axis=1)).mean()\n# average vector length of row normalized imputed matrix","aa0af330":"mean_impute_normalize = Pipeline([('imputer', SimpleImputer()), ('normalize', Normalizer()), ('classify', knn)])\nX = pima.drop('onset_diabetes', axis=1)\ny = pima['onset_diabetes']\n\nknn_params = {'imputer__strategy': ['mean', 'median'], 'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\ngrid = GridSearchCV(mean_impute_normalize, knn_params)\ngrid.fit(X, y)\n\nprint (grid.best_score_, grid.best_params_)","728ab037":"def run_model(model,hyp,X,y,cv, Scaler):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n    mean_impute_standardize = Pipeline([('imputer',SimpleImputer()),\n                                       ('standardize_values',Scaler),\n                                       ('classification',model)])\n    \n    grid = GridSearchCV(mean_impute_standardize,hyp,cv=cv)\n    grid.fit(X_train,y_train)\n    pred = grid.best_estimator_.predict(X_test)\n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    return metrics.accuracy_score(pred,y_test)","f10696bb":"hyper_parameters = {'classification__penalty':['l1','l2'],'imputer__strategy':['mean','median']}\nprint('Logistic Regression accuracy: ')\nrun_model(LogisticRegression(solver='liblinear'),hyper_parameters,\n          pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","e359e35c":"hyper_parameters = {'classification__penalty':['l1','l2'],'imputer__strategy':['mean','median']}\nprint('Logistic Regression accuracy: ')\nrun_model(LogisticRegression(solver='liblinear'),hyper_parameters,\n          pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","8e483778":"hyper_parameters = {'classification__criterion':['gini','entropy'],\n                   'classification__n_estimators':[40,50,100,150,200],\n                   'imputer__strategy':['mean','median']}\nprint('RandomForest Accuracy: ')\nrun_model(RandomForestClassifier(n_jobs=-1),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","f3856074":"hyper_parameters = {'classification__criterion':['gini','entropy'],\n                   'classification__n_estimators':[40,50,100,150,200],\n                   'imputer__strategy':['mean','median']}\nprint('RandomForest Accuracy: ')\nrun_model(RandomForestClassifier(n_jobs=-1),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","70eaefb5":"hyper_parameters = {'classification__kernel':['rbf','sigmoid','poly'],\n                   'classification__C':[0.1,0.001,0.3,1],\n                   'imputer__strategy':['mean','median']}\nprint('SupportVectorClassifier Accuracy: ')\nrun_model(SVC(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","259b9043":"hyper_parameters = {'classification__kernel':['rbf','sigmoid','poly'],\n                   'classification__C':[0.1,0.001,0.3,1],\n                   'imputer__strategy':['mean','median']}\nprint('SupportVectorClassifier Accuracy: ')\nrun_model(SVC(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","b588b048":"hyper_parameters = {'classification__p':[1.3,1.5,2],\n                   'classification__n_neighbors':[5,7,8,9],\n                   'classification__weights':['uniform','distance'],\n                    'imputer__strategy':['mean','median']}\nprint('KNeighborClassifier accuracy: ')\nrun_model(KNeighborsClassifier(n_jobs=-1),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","6fe6b10c":"hyper_parameters = {'classification__p':[1.3,1.5,2],\n                   'classification__n_neighbors':[5,7,8,9],\n                   'classification__weights':['uniform','distance'],\n                    'imputer__strategy':['mean','median']}\nprint('KNeighborClassifier accuracy: ')\nrun_model(KNeighborsClassifier(n_jobs=-1),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","e94271a1":"hyper_parameters = {'classification__learning_rate':[0.1,0.3,0.6,1],\n                   'classification__n_estimators':[30,50,80,100],\n                   'imputer__strategy':['mean','median']}\nprint('AdaBoostClassifier accuracy: ')\nrun_model(AdaBoostClassifier(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","8dff44b3":"hyper_parameters = {'classification__learning_rate':[0.1,0.3,0.6,1],\n                   'classification__n_estimators':[30,50,80,100],\n                   'imputer__strategy':['mean','median']}\nprint('AdaBoostClassifier accuracy: ')\nrun_model(AdaBoostClassifier(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","bd286811":"hyper_parameters = {'imputer__strategy':['mean','median'],\n                   'classification__learning_rate':[0.1,0.3,0.5,1],\n                    'classification__max_depth':[3,6,8],\n                    'classification__n_estimators':[30,60,100,150]\n                   }\nprint('GradientBoostingClassifier accuracy: ')\nrun_model(GradientBoostingClassifier(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,MinMaxScaler())","92d7466e":"hyper_parameters = {'imputer__strategy':['mean','median'],\n                   'classification__learning_rate':[0.1,0.3,0.5,1],\n                    'classification__max_depth':[3,6,8],\n                    'classification__n_estimators':[30,60,100,150]\n                   }\nprint('GradientBoostingClassifier accuracy: ')\nrun_model(GradientBoostingClassifier(),hyper_parameters,\n         pima.drop(labels='onset_diabetes',axis=1),pima['onset_diabetes'],3,StandardScaler())","3349e0b0":"# The row normalization method\nOur final normalization method works row-wise instead of column-wise. Instead of calculating statistics on each column, mean, min, max, and so on, the row normalization technique will ensure that each row of data has a unit norm, meaning that each row will be the same vector length. Imagine if each row of data belonged to an n-dimensional space; each one would have a vector norm, or length. Another way to put it is if we consider every row to be a vector in space:\n\n1. x = (x1, x2, ..., xn)\n\nWhere 1, 2, ..., n in the case of Pima would be 8, 1 for each feature (not including the response), the norm would be calculated as: \n\n2. ||x|| = \u221a(x12 + x22 + ... + xn2)\n\nThis is called the L-2 Norm. Other types of norms exist, but we will not get into that in this text. Instead, we are concerned with making sure that every single row has the same norm. This comes in handy, especially when working with text data or clustering algorithms.\n\nBefore doing anything, let's see the average norm of our mean-imputed matrix","357fcdcd":"We can see that after we apply our scaler to the column, mean drops to very small value and our standard deviation is one. Furthermore, if we take a look at the distribution of values across our recently scaled data","dfa9254e":"It is quite clear that our data all lives on vastly different scales. Data engineers have options on how to deal with this problem in our machine learning pipelines that are under a family of operations called normalization. Normalization operations are meant to align and transform both columns and rows to a consistent set of rules. For example, a common form of normalization is to transform all quantitative columns to be between a consistent and static range of values (for example all values must be between 0 and 1). We may also impose mathematical rules such as, all columns must have the same mean and standard deviation so that they appear nicely on the same histogram (unlike the pima histograms we computed recently). Normalization techniques are meant to level the playing field of data by ensuring that all rows and columns are treated equally under the eyes of machine learning.\n\nWe will focus on three methods of data normalization:\n1. Z-score standardization\n2. Min-max scaling\n3. Row normalization\n\nThe first two deal specifically with altering features in place, while the third option actually manipulates the rows of the data, but is still just as pertinent as the first two.","fe010a20":"Not great, but worth a try. Now that we have seen three different methods of data normalization, let's put it all together and see how we did on this dataset.\n\nThere are many learning algorithms that are affected by the scale of data. Here is a list of some popular learning algorithms that are affected by the scale of data:\n\n1. KNN- due to its reliance on the Euclidean Distance\n2. K-Means Clustering - same reasoning as KNN\n3. Logistic regression, SVM, neural networks \u2014 if you are using gradient descent to learn weights\n4. Principal component analysis \u2014 eigen vectors will be skewed towards larger columns","3ac14dd7":"Notice how the min are all zeros and the max values are all ones. Note further that the standard deviations are now all very very small, a side effect of this type of scaling. This can hurt some models as it takes away weight from outliers. Let's plug our new normalization technique into our pipeline","02748b70":"We can definitely see some differences simply by looking at just a few histograms. For example, there seems to be a large jump in plasma_glucose_concentration for those who will eventually develop diabetes. To solidify this, perhaps we can visualize a linear correlation matrix in an attempt to quantify the relationship between these variables. ","0a9b8e4f":"# Standardization and normalization\nUp until now, we have dealt with identifying the types of data as well as the ways data can be missing and finally, the ways we can fill in missing data. Now, let's talk about how we can manipulate our data (and our features) in order to enhance our machine pipelines further. So far, we have tried four different ways of manipulating our dataset, and the best cross-validated accuracy we have achieved with a KNN model is .745. If we look back at some of the EDA we have previously done, we will notice something about our features","9b25b346":"# The min-max scaling method\nMin-max scaling is similar to z-score normalization in that it will replace every value in a column with a new value using a formula. In this case, that formula is:\n\nm = (x -xmin) \/ (xmax -xmin)\n\nWhere:\n\n1. m is our new value\n2. x is the original cell value\n3. xmin is the minimum value of the column\n4. xmax is the maximum value of the column","cee5818c":"Because zero is a class for onset_diabetes and 0 is actually a viable number for times_pregnant, we may conclude that the number 0 is encoding missing values for:\n\n1. plasma_glucose_concentration\n2. diastolic_blood_pressure\n3. triceps_thickness\n4. serum_insulin\n5. bmi","61f93e87":"# The Problem","810e2429":"If our eventual goal is to exploit patterns in our data in order to predict the onset of diabetes, let us try to visualize some of the differences between those that developed diabetes and those that did not. Our hope is that the histogram will reveal some sort of pattern, or obvious difference in values between the classes of prediction","368208a4":"The scale of features is very different.\n\nBut why does this matter? Well, some machine learning models rely on learning methods that are affected greatly by the scale of the data, meaning that if we have a column such as diastolic_blood_pressure that lives between 24 and 122, and an age column between 21 and 81, then our learning algorithms will not learn optimally. To really see the differences in scales, let's invoke two optional parameters in the histogram method, sharex and sharey, so that we can see each graph on the same scale as every other graph","584af41f":"Now we can clearly see our model has already started outperforming the benchmark. That's good progress we have till now","41d090b6":"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","c124a92b":"We see that every single value in the column will be replaced, and also notice how now some of them are negative. This is because the resulting values represent a distance from the mean. So, if a value originally was below the mean of the column, the resulting z-score will be negative. Of course, in scikit-learn, we have built-in objects ","6caf45fb":"1. times_pregnant\n2. plasma_glucose_concentration\n3. diastolic_blood_pressure\n4. triceps_thickness\n5. serum_insulin\n6. bmi\n7. onset_diabetes","a862d074":"Exploratory data analysis (EDA)\nTo identify our missing values we will begin with an EDA of our dataset. We will be using some useful python packages, pandas and numpy, to store our data and make some simple calculations as well as some popular visualization tools to see what the distribution of our data looks like. Let's begin and dive into some code. First, we will do some imports:","c8c3c047":"Here, we can see the distribution of the column before doing anything. Now, let's apply a z-score scaling","e45744f2":"We will notice that our x axis is now much more constrained, while our y axis is unchanged. Also note that the shape of the data is unchanged entirely. Let's take a look at the histograms of our DataFrame after we apply a z-score transformation on every single column. When we do this, the StandardScaler will compute a mean and standard deviation for every column separately","5c364aa7":"Z-score standardization\nThe most common of the normalization techniques, z-score standardization, utilizes a very simple statistical idea of a z-score. The output of a z-score normalization are features that are re-scaled to have a mean of zero and a standard deviation of one. By doing this, by re-scaling our features to have a uniform mean and variance (square of standard deviation), then we allow models such as KNN to learn optimally and not skew towards larger scaled features. The formula is simple: for every column, we replace the cells with the following value:\n\nz = (x - \u03bc) \/ \u03c3\n\nWhere:\n1. z is our new value (z-score)\n2. x is the previous value of the cell\n3. \u03bc is the mean of the column\n4. \u03c3 is the standard deviation of the columns","a3cf5927":"This correlation matrix is showing a strong correlation between plasma_glucose_concentration and onset_diabetes. Let's take a further look at the numerical correlations for the onset_diabetes column","465729fa":"After normalizing, we see that every single row has a norm of one now. Let's see how this method fares in our pipeline","84a1617f":"This shows us quite quickly some basic stats such as mean, standard deviation, and some different percentile measurements of our data. But, notice that the minimum value of the BMI column is 0. That is medically impossible; there must be a reason for this to happen. Perhaps the number zero has been encoded as a missing value instead of the None value or a missing cell. Upon closer inspection, we see that the value 0 appears as a minimum value for the following columns:","865ba4de":"There is huge difference in the median values of the missing columns with respect of diabetes or not.\n\nWe will try to impute values according to these statistics","a928eea6":"It seems that this histogram is showing us a considerable difference between plasma_glucose_concentration between the two prediction classes. Let's show the same histogram style for multiple columns as follows"}}