{"cell_type":{"09b5b707":"code","7ae15557":"code","700cb3fe":"code","247f6278":"code","d39f12b4":"code","77a79532":"code","a2d66cfd":"code","e9c4f6b3":"code","eab38d20":"code","62b0317f":"code","52e82d96":"code","3bad2296":"code","8164f51d":"code","a574831a":"code","a74db447":"code","56004bd3":"code","619b4695":"code","9ef56aca":"code","c07c51d3":"code","aeb70e9c":"code","c890b913":"code","723a15fd":"code","b7090bc9":"code","d7cf5481":"code","f2ffaf8e":"code","b548b170":"markdown"},"source":{"09b5b707":"import os\nimport tensorflow as tf\n\n# Load compressed models from tensorflow_hub\nos.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'","7ae15557":"import IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools","700cb3fe":"def tensor_to_image(tensor):\n    tensor = tensor*255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor)>3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return PIL.Image.fromarray(tensor)","247f6278":"def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img","d39f12b4":"def imshow(image, title=None):\n    if len(image.shape) > 3:\n        image = tf.squeeze(image, axis=0)\n\n    plt.imshow(image)\n    if title:\n        plt.title(title)","77a79532":"content_path = '\/kaggle\/input\/real-faces\/ava.jpg'\nstyle_path = '\/kaggle\/input\/riga-faces\/images\/1080p\/Jauniela_25-27(3).jpg'","a2d66cfd":"content_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')","e9c4f6b3":"import tensorflow_hub as hub\nhub_model = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/2')\nstylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)","eab38d20":"x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\nx = tf.image.resize(x, (224, 224))\nvgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\nprediction_probabilities = vgg(x)\nprediction_probabilities.shape","62b0317f":"vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n\nprint()\nfor layer in vgg.layers:\n    print(layer.name)","52e82d96":"content_layers = ['block5_conv2'] \n\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\n\nnum_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)","3bad2296":"def vgg_layers(layer_names):\n    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n    # Load our model. Load pretrained VGG, trained on imagenet data\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n    vgg.trainable = False\n\n    outputs = [vgg.get_layer(name).output for name in layer_names]\n\n    model = tf.keras.Model([vgg.input], outputs)\n    return model","8164f51d":"style_extractor = vgg_layers(style_layers)\nstyle_outputs = style_extractor(style_image*255)\n\n#Look at the statistics of each layer's output\nfor name, output in zip(style_layers, style_outputs):\n    print(name)\n    print(\"  shape: \", output.numpy().shape)\n    print(\"  min: \", output.numpy().min())\n    print(\"  max: \", output.numpy().max())\n    print(\"  mean: \", output.numpy().mean())\n    print()","a574831a":"def gram_matrix(input_tensor):\n    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n    return result\/(num_locations)","a74db447":"class StyleContentModel(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(StyleContentModel, self).__init__()\n        self.vgg = vgg_layers(style_layers + content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.vgg.trainable = False\n\n    def call(self, inputs):\n        \"Expects float input in [0,1]\"\n        inputs = inputs*255.0\n        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n        outputs = self.vgg(preprocessed_input)\n        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n                                          outputs[self.num_style_layers:])\n\n        style_outputs = [gram_matrix(style_output)\n                         for style_output in style_outputs]\n\n        content_dict = {content_name: value\n                        for content_name, value\n                        in zip(self.content_layers, content_outputs)}\n\n        style_dict = {style_name: value\n                      for style_name, value\n                      in zip(self.style_layers, style_outputs)}\n\n        return {'content': content_dict, 'style': style_dict}","56004bd3":"extractor = StyleContentModel(style_layers, content_layers)\n\nresults = extractor(tf.constant(content_image))\n\nprint('Styles:')\nfor name, output in sorted(results['style'].items()):\n    print(\"  \", name)\n    print(\"    shape: \", output.numpy().shape)\n    print(\"    min: \", output.numpy().min())\n    print(\"    max: \", output.numpy().max())\n    print(\"    mean: \", output.numpy().mean())\n    print()\n\nprint(\"Contents:\")\nfor name, output in sorted(results['content'].items()):\n    print(\"  \", name)\n    print(\"    shape: \", output.numpy().shape)\n    print(\"    min: \", output.numpy().min())\n    print(\"    max: \", output.numpy().max())\n    print(\"    mean: \", output.numpy().mean())","619b4695":"style_targets = extractor(style_image)['style']\ncontent_targets = extractor(content_image)['content']","9ef56aca":"image = tf.Variable(content_image)","c07c51d3":"def clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","aeb70e9c":"opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","c890b913":"style_weight=1e-2\ncontent_weight=1e4","723a15fd":"def style_content_loss(outputs):\n    style_outputs = outputs['style']\n    content_outputs = outputs['content']\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n                           for name in style_outputs.keys()])\n    style_loss *= style_weight \/ num_style_layers\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n                             for name in content_outputs.keys()])\n    content_loss *= content_weight \/ num_content_layers\n    loss = style_loss + content_loss\n    return loss","b7090bc9":"@tf.function()\ndef train_step(image):\n    with tf.GradientTape() as tape:\n        outputs = extractor(image)\n        loss = style_content_loss(outputs)\n\n    grad = tape.gradient(loss, image)\n    opt.apply_gradients([(grad, image)])\n    image.assign(clip_0_1(image))","d7cf5481":"train_step(image)\ntrain_step(image)\ntrain_step(image)\ntensor_to_image(image)","f2ffaf8e":"import time\nstart = time.time()\n\nepochs = 10\nsteps_per_epoch = 100\n\nstep = 0\nfor n in range(epochs):\n    for m in range(steps_per_epoch):\n        step += 1\n        train_step(image)\n        print(\".\", end='')\n    display.clear_output(wait=True)\n    display.display(tensor_to_image(image))\n    print(\"Train step: {}\".format(step))\n\nend = time.time()\nprint(\"Total time: {:.1f}\".format(end-start))","b548b170":"# About\nThis is based on https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer#build_the_model"}}