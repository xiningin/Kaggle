{"cell_type":{"01f499cb":"code","2d24f828":"code","298cbd2d":"code","ce16d717":"code","1130e9e9":"code","33b3582a":"code","ced9ca17":"code","66788b51":"code","1af369b8":"code","0a9f1cb3":"code","b985c5ad":"code","4c4f3277":"code","fa1db6ec":"code","594c06df":"code","a2c21a80":"code","72ebac62":"code","0ed88057":"code","89b8fe68":"code","2e383991":"code","d7e1c62d":"code","7cce5d30":"code","0a5b0653":"code","a4932cf8":"code","b1275b58":"code","854973e6":"code","e1d8f36f":"code","d53eaf52":"markdown","5050b820":"markdown","e35ffaa7":"markdown","f1aa8026":"markdown","85970dbc":"markdown","bf765be4":"markdown","9de1c89d":"markdown","7a14aa75":"markdown","40736ca4":"markdown","182531d4":"markdown","cf152342":"markdown","c2a3ab7d":"markdown","0e86f42f":"markdown","7ad8dffb":"markdown","570dcafa":"markdown","904b3187":"markdown","18637472":"markdown"},"source":{"01f499cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d24f828":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering","298cbd2d":"df = pd.read_csv('\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndf.head()","ce16d717":"df.info()","1130e9e9":"df.columns","33b3582a":"#number of missing values\ndf.isnull().sum()","ced9ca17":"# statistical information about the dataset\ndf.describe()","66788b51":"# statistical information about the categorical variable\ndf.describe(include=['O']).T","1af369b8":"# let's change the name of the columns to make it easier for analysis\ndf.rename(columns={\"Annual Income (k$)\": \"Income\", \"Spending Score (1-100)\": \"Score\"}, inplace=True)","0a9f1cb3":"# distribution of age\nsns.displot(x='Age', data=df, kde=True)","b985c5ad":"# unique values for spending score\ndf['Score'].unique()","4c4f3277":"# distribution of spending score\nsns.displot(x='Score', data=df, kde=True)","fa1db6ec":"# distribution of annual income\nsns.displot(x='Income', data=df, kde=True)","594c06df":"# distribution of categorical variable\nsns.countplot(x='Gender', data=df)","a2c21a80":"df['Gender'].value_counts()","72ebac62":"sns.heatmap(df.corr(), annot=True)","0ed88057":"# drop CustomerID as it is not useful\ndf.drop('CustomerID', axis=1, inplace=True)","89b8fe68":"df.head()","2e383991":"df = pd.get_dummies(df).reset_index(drop=True)","d7e1c62d":"df.head()","7cce5d30":"X = df.iloc[:,].values","0a5b0653":"# Using the elbow method to find the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","a4932cf8":"# Fitting K-Means to the dataset\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(X)","b1275b58":"# Visualising the clusters\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","854973e6":"hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","e1d8f36f":"# Visualising the clusters\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","d53eaf52":"---\n### The data science methodology followed for this project has been outlined by John Rollins, IBM\n\n- Business Understanding\n- Analytical Approach\n- Data requirements\n- Data collection\n- Data Understanding\n- Data Preparation\n- Modeling","5050b820":"### Hierarchical clustering\nHierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.","e35ffaa7":"## 7. Modeling","f1aa8026":"## Correlation of the Data","85970dbc":"## 1. Business Understanding\nCustomer segmentation is the practice of dividing a company's customers into groups that reflect similarity among customers in each group. The goal of this project is to divide customers into groups based on common characteristics in order to maximize the value of each customer to the business. You own the mall and want to understand the customers like who can be easily converge [Target Customers] so that the sense can be given to marketing team and plan the strategy acc\nordingly.","bf765be4":"## 5. Data Understanding \/ Exploratory Data Analysis","9de1c89d":"### K Means Clustering\nIn K-means, objects are assigned to a cluster based on the Euclidean distance between the object and the center of the cluster, also referred to as the cluster centroid.\n\nBut we do not know in advance how many clusters there are, and we do not know what the clusters will look like. That is why we work in two steps.\n- First, we determine the optimal number of clusters, and then\n- We determine starting values for each cluster.","7a14aa75":"## Import the dataset","40736ca4":"## Libraries Used","182531d4":"## That brings us to the end of this project. \n## I regularly share contents related to Data Science on Twitter, you can connect with there [@PiyalBanik](https:\/\/twitter.com\/PiyalBanik)","cf152342":"### Feature Engineering","c2a3ab7d":"---","0e86f42f":"## Visualisation","7ad8dffb":"## 6. Data Cleaning","570dcafa":"---\nDataset Understanding:\n- There are total of 200 observations with each having 5 variables. \n- The column of the dataset include CustomerID, Gender, Age, Annual Income, Spending Score.\n- There are no missing values (Good day for us XD)\n- There is one categorical variable - Gender\n---","904b3187":"## 2. Analytical Approach \nClustering of Customers based on similar characteristics is an Unsupervised Learning as for each observation we do not have any target variable. \nFor this project I will use two Machine Learning models\n- I will use KMeans Clustering Algorithm which aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean\n- I will also use Hierarchical clustering which is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.","18637472":"## 3,4. Data Requirements and Data Collection\nWe would require a dataset which gives us information regarding customers from a market. For this project, the dataset has been provided to us on Kaggle. This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis\n\n---"}}