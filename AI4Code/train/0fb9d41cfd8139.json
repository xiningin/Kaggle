{"cell_type":{"9947d390":"code","d3bf29ea":"code","d217ec19":"code","10092431":"code","60d9a01d":"code","5a9f6b8d":"code","73b9a0a4":"code","9b6b8684":"code","d486c250":"code","656fac88":"code","975e9a33":"code","91ab81ab":"code","f5685ad8":"code","62586de4":"code","83652f33":"code","69bd2aaa":"code","f2e6dcb9":"code","558a032a":"code","c61211ea":"code","2732e4be":"code","07376608":"code","9412136a":"code","0de04528":"code","f85e44de":"code","871a5bea":"code","aed6b5b6":"code","5af69e5f":"code","0e839f48":"code","1dc711eb":"code","c1dc8146":"code","3f85d37f":"code","f1290f5c":"code","5c055c0d":"code","2b68e5d0":"code","929f4e13":"markdown","8b7096ad":"markdown","27aac9a1":"markdown","88c86ac4":"markdown","9a77f87a":"markdown","aec7d2c8":"markdown","74823d5f":"markdown","c9eb02cb":"markdown","689f976f":"markdown","d0192b1c":"markdown","4a7683ee":"markdown","aaad8fb7":"markdown","4e0a8aa0":"markdown","3ac85665":"markdown","2dcae997":"markdown","09fcd38c":"markdown","cf7fcb89":"markdown","94f6fc10":"markdown","1138b2d9":"markdown"},"source":{"9947d390":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3bf29ea":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport IPython.display as display\n\nimport shap\n","d217ec19":"def missing_count(df):\n    missing_count = df.isna().sum()\n    missing_df = (pd.concat([missing_count.rename('Missing count'),\n                     missing_count.div(len(df))\n                          .rename('Missing ratio')],axis = 1)\n             .loc[missing_count.ne(0)])\n    return missing_df.sort_values(by=\"Missing ratio\")","10092431":"SampleSubmission = \"..\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv\"\nSolutionTemplate = \"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\"\nDataDictionary = \"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\"\nUnlabeledWiDS2021 = \"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\"\nTrainingWiDS2021 = \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\"","60d9a01d":"table_train =  pd.read_csv(TrainingWiDS2021)\ntable_test = pd.read_csv(UnlabeledWiDS2021)\ndata_dictionary = pd.read_csv(DataDictionary)","5a9f6b8d":"missing_df = missing_count(table_train)\n\ndisplay.display(table_train.describe().T)\ndisplay.display(missing_df)\ntable_train[:10]","73b9a0a4":"missing_df = missing_count(table_test)\n\ndisplay.display(table_test.describe().T)\ndisplay.display(missing_df)\ntable_test[:10]","9b6b8684":"fig = go.Figure(data=[go.Table(\n    header=dict(values=list(data_dictionary.columns),\n                fill_color=\"#E1A1A9\",\n                line_color='black',\n                font_color = \"white\",\n                align='center'),\n    cells=dict(values=[data_dictionary.Category,data_dictionary['Variable Name'],data_dictionary['Unit of Measure'],data_dictionary['Data Type'],data_dictionary['Description'],data_dictionary['Example']],\n               fill_color=\"#EED8D0\",\n               font_color='black',\n               line_color='black',\n               align='center'))\n])\n\nfig.show()","d486c250":"# Droping useless columns\n# noise = ['Unnamed: 0','hospital_id', 'icu_admit_source','hospital_admit_source', 'icu_id', 'icu_stay_type', 'readmission_status' ]\nnoise = []\nh1_duplicates = [\n 'h1_diasbp_invasive_max',\n 'h1_diasbp_invasive_min',\n 'h1_diasbp_max',\n 'h1_diasbp_min',\n 'h1_diasbp_noninvasive_max',\n 'h1_diasbp_noninvasive_min',\n 'h1_heartrate_max',\n 'h1_heartrate_min',\n 'h1_mbp_invasive_max',\n 'h1_mbp_invasive_min',\n 'h1_mbp_max',\n 'h1_mbp_min',\n 'h1_mbp_noninvasive_max',\n 'h1_mbp_noninvasive_min',\n 'h1_resprate_max',\n 'h1_resprate_min',\n 'h1_spo2_max',\n 'h1_spo2_min',\n 'h1_sysbp_invasive_max',\n 'h1_sysbp_invasive_min',\n 'h1_sysbp_max',\n 'h1_sysbp_min',\n 'h1_sysbp_noninvasive_max',\n 'h1_sysbp_noninvasive_min',\n 'h1_temp_max',\n 'h1_temp_min',\n 'h1_albumin_max',\n 'h1_albumin_min',\n 'h1_bilirubin_max',\n 'h1_bilirubin_min',\n 'h1_bun_max',\n 'h1_bun_min',\n 'h1_calcium_max',\n 'h1_calcium_min',\n 'h1_creatinine_max',\n 'h1_creatinine_min',\n 'h1_glucose_max',\n 'h1_glucose_min',\n 'h1_hco3_max',\n 'h1_hco3_min',\n 'h1_hemaglobin_max',\n 'h1_hemaglobin_min',\n 'h1_hematocrit_max',\n 'h1_hematocrit_min',\n 'h1_inr_max',\n 'h1_inr_min',\n 'h1_lactate_max',\n 'h1_lactate_min',\n 'h1_platelets_max',\n 'h1_platelets_min',\n 'h1_potassium_max',\n 'h1_potassium_min',\n 'h1_sodium_max',\n 'h1_sodium_min',\n 'h1_wbc_max',\n 'h1_wbc_min',\n 'h1_arterial_pco2_max',\n 'h1_arterial_pco2_min',\n 'h1_arterial_ph_max',\n 'h1_arterial_ph_min',\n 'h1_arterial_po2_max',\n 'h1_arterial_po2_min',\n 'h1_pao2fio2ratio_max',\n 'h1_pao2fio2ratio_min']\n# col_useless = noise+h1_duplicates\ncol_useless = noise\ntable_train.drop(columns=col_useless, axis=1, inplace=True)\ntable_test.drop(columns=col_useless, axis=1, inplace=True)","656fac88":"df = table_train[[\"gender\", \"age\", \"diabetes_mellitus\"]].dropna()\nfig = px.histogram(df, x=\"age\", y=\"diabetes_mellitus\", color=\"gender\",\n                   marginal=\"box\", \n                   hover_data=df.columns)\nfig.update_layout(title_text='Diabetics by Gender - Histogram' ,\n                  barmode='overlay',\n                  template= \"xgridoff\" , \n                  xaxis = dict(title = 'Age'), \n                  yaxis = dict(title = 'Total Diabetics'))\nfig.show()","975e9a33":"temp_df = table_train[\"ethnicity\"].value_counts()\ntemp_df2 = table_train[table_train[\"diabetes_mellitus\"]==1][\"ethnicity\"].value_counts()\n\n\ntrace = go.Pie(labels=temp_df.index, \n               values=temp_df.values, \n               hoverinfo='percent+value+label', \n               textinfo='percent',\n               textposition='inside',\n               showlegend=True,\n               hole=.2,\n               pull=[0.1 for _ in range(len(temp_df.index))],\n               marker=dict(colors=plt.cm.cividis_r(np.linspace(0, 1, 28)),\n                           line=dict(color='#000000',width=0.5),\n                          )\n              )\n                \n\ntrace2 = go.Pie(labels=temp_df2.index, \n               values=temp_df2.values, \n               hoverinfo='percent+value+label', \n               textinfo='percent',\n               textposition='inside',\n               showlegend=True,\n               hole=.2,\n               pull=[0.1 for _ in range(len(temp_df2.index))],\n               marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 28)),\n                           line=dict(color='#000000',width=0.5),\n                          )\n              )\n\nlayout = go.Layout(title=\"Ethnicity Breakdown - Dataset\",template= \"simple_white\")\nfig=go.Figure(data=[trace], layout=layout)\nfig.show()\n\nlayout = go.Layout(title=\"Ethnicity Breakdown - Diabetes Positive\",template= \"simple_white\")\nfig=go.FigureWidget(data=[trace2], layout=layout)\nfig.show()","91ab81ab":"def hist_plotly(df, col,bin_size=5,color = \"black\"):\n    colors = [color]\n    fig = ff.create_distplot([df[col].dropna()], [col],bin_size=bin_size,\n                             curve_type='normal', \n                             show_rug=False,\n                             colors=colors)\n\n    # Add title\n    fig.update_layout(title_text='{} - Normal Distribution'.format(col.upper()) ,\n                  template= \"simple_white\" , \n                  xaxis = dict(title = col.upper()))\n    fig.show()","f5685ad8":"hist_plotly(table_train, \"d1_glucose_max\",bin_size=5, color= \"darkcyan\")","62586de4":"hist_plotly(table_train, \"bmi\",bin_size=1, color= \"darksalmon\")","83652f33":"bmi_cut = table_train.copy()\nbmi_cut[\"bmi\"] = pd.cut(table_train[\"bmi\"] , labels= [\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"], bins=[0, 18.5, 24.9,30, 100])","69bd2aaa":"temp_df = bmi_cut[[\"bmi\"]].value_counts().reset_index().sort_values(\"bmi\")\ntemp_df[\"bmi\"] = temp_df[\"bmi\"].astype(str)\ntemp_df.columns = [\"bmi\", \"count\"]\n\ntemp_df2 = bmi_cut[bmi_cut[\"diabetes_mellitus\"]==1][[\"bmi\"]].value_counts().reset_index().sort_values(\"bmi\")\ntemp_df2[\"bmi\"] = temp_df2[\"bmi\"].astype(str)\ntemp_df2.columns = [\"bmi\", \"count\"]\n\ntrace1 = go.Bar(\n                x = temp_df['bmi'],\n                y = temp_df['count'], name =\"Whole dataset\",\n                marker = dict(color = 'lightgray',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df['count'], textposition='outside')\n\ntrace2 = go.Bar(\n                x = temp_df2['bmi'],\n                y = -temp_df2['count'], name =\"Diabetes Positive\",\n                marker = dict(color = 'khaki',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=temp_df2['count'], textposition='outside')\n\n\n\nlayout = go.Layout(template= \"simple_white\",title = 'Breakdown by BMI Classification' , xaxis = dict(title = 'BMI classification'), yaxis = dict(title = 'Total Count'), height=650)\nfig = go.Figure(data = [trace1,trace2], layout = layout)\nfig.update_yaxes(showticklabels=False)\nfig.update_layout(barmode='overlay')\nfig.show()","f2e6dcb9":"hist_plotly(table_train, \"weight\",bin_size=1, color= \"powderblue\")","558a032a":"featu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in table_train.columns:\n    x=table_train[col].dtype\n    if x=='int64' or x == 'int32':\n        table_train[col] = table_train[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        table_train[col] = table_train[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","c61211ea":"table_train[featu_float + [\"diabetes_mellitus\"]].corr(method = \"pearson\").style.background_gradient(cmap='GnBu')","2732e4be":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n","07376608":"train_model = table_train.drop(columns=[\"diabetes_mellitus\"]).copy()\nlabel_model = table_train[[\"diabetes_mellitus\"]].copy()\ntest_model  = table_test.copy()","9412136a":"imp_simp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntrain_model[featu_obj] = imp_simp.fit_transform(train_model[featu_obj])\ntest_model[featu_obj] = imp_simp.transform(test_model[featu_obj]) \ntrain_model[:20]","0de04528":"featu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in train_model.columns:\n    x=train_model[col].dtype\n    if x=='int64' or x == 'int32':\n        train_model[col] = train_model[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        train_model[col] = train_model[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","f85e44de":"def add_new_features(data):\n    data = data.copy()\n\n    def create_features(x):\n        eth_gen = x['ethnicity']+'_'+x['gender']\n        a_bmi = x['age']*x['bmi']\n        a_glu = x['age']*x['d1_glucose_max']\n        bmi_glu = x['bmi']*x['d1_glucose_max']\n\n        return pd.Series([eth_gen,a_bmi,a_glu,bmi_glu ], index = ['ethnicity_gend','age_bmi','age_glucose','bmi_glucose' ])\n\n    return data.join(data.apply(create_features, axis=1))\n\ntrain_model_agg =  add_new_features(train_model)\ntest_model_agg  =  add_new_features(test_model)\n\ntrain_model_agg.head()","871a5bea":"featu_int=[]\nfeatu_float=[]\nfeatu_obj=[]\nfor col in train_model_agg.columns:\n    x=train_model_agg[col].dtype\n    if x=='int64' or x == 'int32':\n        train_model_agg[col] = train_model_agg[col].astype('int32')\n        featu_int.append(col)\n    elif x=='float64' or x== 'float32':\n        train_model_agg[col] = train_model_agg[col].astype('float32')\n        featu_float.append(col)\n    else:\n        featu_obj.append(col)","aed6b5b6":"import category_encoders as ce\ntarget_enc = ce.CatBoostEncoder(cols=featu_obj)\ntarget_enc.fit(train_model_agg[featu_obj], label_model)\n\n# Transform the features, rename columns with _cb suffix, and join to dataframe\ntrain_model_enc = train_model_agg.join(target_enc.transform(train_model_agg[featu_obj]).add_suffix('_cb'))\ntest_model_enc  = test_model_agg.join(target_enc.transform(test_model_agg[featu_obj]).add_suffix('_cb'))\n\ntrain_model_enc[:10]","5af69e5f":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(train_model_agg[featu_obj])\n\ndef add_onehotencoder(df, columns, encoder):\n    df = df.join(pd.DataFrame(encoder.transform(df[columns]).toarray(), columns = encoder.get_feature_names(columns) ).add_suffix('_ohe'))\n    return df.drop(columns = columns)\n    \ntrain_model_enc = add_onehotencoder(train_model_enc, featu_obj, enc)\ntest_model_enc  = add_onehotencoder(test_model_enc, featu_obj, enc)\n\ntrain_model_enc[:10]","0e839f48":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_model_enc, label_model, train_size=0.7,stratify=label_model)\n","1dc711eb":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\ndvalid = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\ndtest = xgb.DMatrix(test_model_enc)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n","c1dc8146":"params = {'min_child_weight': 5,\n            'colsample_bytree': 0.7,\n            'max_depth': 7, \n            'eta': 0.08,\n            'subsample': 0.7,\n            'lambda': 1.,\n            'nthread': -1,\n            'booster' : 'gbtree',\n            'silent': 1,\n            'objective': 'binary:logistic',\n            'eval_metric': ['auc'],\n           }\n\ntuned_parameters_xgb = [{'tree_method': ['gpu_hist'],'booster':['gbtree'], 'predictor':['gpu_predictor'], 'learning_rate':np.linspace(.05, 1, 20),'min_child_weight':np.linspace(0, 1, 20), 'n_estimators':list(range(50, 300, 20)), # 'subsample':[0.1, 0.5, 0.8,], 'n_estimators':[100,150], 'min_child_weight':[0.1, 0.5, 1.0,],\n                    'objective':['binary:logistic'],'max_depth': list(range(1,20)), 'gamma': np.linspace(0,1.,20) }]\n\nmodel = xgb.train(params, dtrain, num_boost_round=600, evals=[(dvalid, 'validate')], early_stopping_rounds=4,\n      maximize=True, verbose_eval=1)\n\nprint('Best score: %.5f'% model.best_score)\n\n","3f85d37f":"xgb.plot_importance(model, max_num_features=10, height=0.8)","f1290f5c":"y_pred     = model.predict(dtest)\nprint(y_pred)\nprint(model.best_score)","5c055c0d":"\n# y_pred = np.exp(y_pred) - 1","2b68e5d0":"Test_id = test_model_enc['encounter_id']\nsubmission = pd.concat([Test_id, pd.DataFrame(y_pred)], axis = 1)\n\nsubmission.columns = ['encounter_id','diabetes_mellitus']\nsubmission['diabetes_mellitus'] = submission.apply(lambda x : 1 if (x['diabetes_mellitus'] <= 0) else x['diabetes_mellitus'], axis = 1)\nsubmission.to_csv(\"submission.csv\", index=False)","929f4e13":"# **Correlation Matrix Exploration**\n\nIn the table below we will explore the correlation of the features and look for variables that are highly correlated between each other. We are also for the ones that are highly uncorrelated between each other.\n\n","8b7096ad":"In this part we created some extra features that are a combination of what we saw on the graphs above. We also considered high correlation and academic studies published and refered in this project to mix & match: Age, Ethnicity, Gender, Levels of Max Glucose on Day One, and BMI.\n\n\nFor that you will see our newly created features:\n\n* eth_gen => here representing gender + ethnicity\n* a_bmi => here representing age + bmi\n* a_glu => here representing age + d1_glucose_max\n* bmi_glu => here representing bmi + d1_glucose_max\n\nWe then added those new features to our dataframe:","27aac9a1":"1. # Data Visualization\n\nPloting histograms and pie graphs to have some valuable insights","88c86ac4":"# XGBoost\n\n\nFor this datathon we decided to use the XGBoost algorithm, known for its ability to handle missing values and learning ","9a77f87a":"# Women in Data Science Challenge 2021\n\nAs first timers in a data hackathon, we are glad about how far we have reached and the journey of participating in this challenge has been an upward learning curve.\n\nThis is how Dorothy team structured our project:\n\n**Some basic overview about the Dataset**\n\n**Data Visualization**\n\n**Feature Engineering**\n\n**Creation of new features**\n\n**Cat Boost**\n\n**Hot Encoding**\n\n**XG Boost**","aec7d2c8":"# Diabetes and Ethnicity\n\n\nWhen we plot the ethnicity of our population to check the incidence of diabetes, the results are alarming. There is a clear predominance of diabetes in the african american population. \n\nThis study of the Diabetes Community in the UK alerts that:\n* Type 2 diabetes is up to 6 times more likely in people of South Asian descent\n* Type 2 diabetes is up to three times more likely in African and Africa-Caribbean people\nhttps:\/\/www.diabetes.co.uk\/diabetes-and-ethnicity.html\n\nOur dataset is from the USA, a country that houses an enormously diverse ethnic mix. And according to the US department of Health and Human Services, we hear loud and clear that:\n\nAfrican American adults are 60 percent more likely than non-Hispanic white adults to have been diagnosed with diabetes by a physician.\n* In 2016, non-Hispanic blacks were 3.5 times more likely to be diagnosed with end stage renal disease as compared to non-Hispanic whites.\n* In 2016, non-Hispanic blacks were 2.3 times more likely to be hospitalized for lower limb amputations as compared to non-Hispanic whites.\n* In 2017, African Americans were twice as likely as non-Hispanic whites to die from diabetes.\nhttps:\/\/minorityhealth.hhs.gov\/omh\/browse.aspx?lvl=4&lvlid=18#:~:text=African%20American%20adults%20are%2060,compared%20to%20non%2DHispanic%20whites.\n\n\nWe found this interesting study in the US NCBI website https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3830901\/ highlighting the complications of diabetes in different ethnicities across the US. ","74823d5f":"**XGBoost parameters**\n\n\nBooster Parameter: is a parameter that powers the selected booster performance.\n \n**Parameters for Tree Booster**\n \n\nnrounds[default=100] - It controls the maximum number of iterations. For classification, it is similar to the number of trees to grow. Should be tuned using CV\n\n \neta[default=0.3][range: (0,1)] - It commands the learning rate i.e the rate at which the model learns from the data. The computation will be slow if the value of eta is small. Its value is between 0.01-0.03.\n\n \ngamma[default=0][range: (0,Inf)] - Its function is to take care of the overfitting. Its value is dependent on the data. The regularization will be high if the value of gamma is high.\n\n \nmax_depth[default=6][range: (0,Inf)] - Its function is to control the depth of the tree, if the value is high, the model would be more complex.  There is no fixed value of max_depth. The value depends upon the size of data. It should be tuned using CV.\n\n \nsubsample[default=1][range: (0,1)] - Its values lie between (0.5-0.8) and it controls the samples given to the tree.\n\n \ncolsample_bytree[default=1][range: (0,1)] - It checks about the features supplied to the tree.\n\n \nlambda[default=0] - It is used to avoid overfitting and controls L2 regularisation.\n \n\nalpha[default=1] - Enabling alpha, it results in feature selection by which it is more useful for high dimension dataset. It controls L1 regularization on weights.\n\n\n\n**Parameters for Linear Booster**\n \n\nIts computation is high as it has relatively fewer parameters to tune. \n\nnrounds[default=100] - It powers the iteration that is required by gradient descent to converge. It should be tuned using CV.\n\nlambda[default=0] - Its function is to permit Ridge Regression. \n \nalpha[default=1] -Its function is to permit  Lasso Regression. ","c9eb02cb":"# Train Test Split\n\nThe next step is to split the training set into sub-training and sub-testing sets. The reason for this is to be able to tweak model parameters to increase accuracy (i.e. increase the area under curve [AUC] value) without creating bias towards the test set. Also, and perhaps more importantly, is that we need a validation set to use with XGBoost. So the XGBoost algorithm takes three datasets: a training set, a test set, and a validation set. The validation set is used to continuously evaluate the accuracy of the training model, and finally the model is used to make predictions against the test set. So splitting out the training set into a separate train and test set gives us a test sample of which we know the outcome variables. \n\n\nAlso, we'll split the training set into a 70-30 train and test set. So from this, there are two things you can do: change the ratios by which you split the original training set (`train_model_enc`), e.g. use a 80-20 ratio and see what the results are. Also, you can include all the data points instead of just 100k. This should also have quite a siginficant effect on the outcome.\n\nHere's how we split the dataset:","689f976f":"**Getting to know the test file**\n\nand also checking how much data is missing in the test file","d0192b1c":"**Getting to know what's inside the train file**\n\nand also checking how much data is missing","4a7683ee":"**Incidence of Diabetes in Afro_American Ethnicity**\n\nIn the training dataset, we can notice that around 26.26% of the African American population has diabetes vs only 20.55% of the Caucasian population. Confirming the studies presented above.","aaad8fb7":"once we get the gender_ethnicity fixed and do the categoric boosting. we can proceed to XGBoost","4e0a8aa0":"# Hot Encoding","3ac85665":"# CatBoost\n\nCatboost is a recently created target-based categorical encoder. It is a high-performance open source library for gradient boosting on decision trees.\n\n","2dcae997":"# Glucose levels information\n\nThe graph below shows the Normal Distribution of the d1 (Day 1) Glucose Max levels","09fcd38c":"# Feature Engineering\n\nGiven the fact that we will use XGBoost, we were informed that the best approach is to leave the NaN values as is. However, we have much room to improve our current dataframe and this section will highlight the data modeling.","cf7fcb89":"# Preparing our dMatrix\n\nIn order to use XGBoost algorithm, we need to create a DMatrix","94f6fc10":"# Body Mass Index (BMI) and its influence in Diabetes\n\n\n","1138b2d9":"# Diabetics by Gender & Age\n\nIn this histogram we figure that we have about 30-40% more men diagnosed with diabetes than women. According to this article by Hannah Simmons (https:\/\/www.news-medical.net\/health\/Diabetes-in-Men-versus-Women.aspx) men are more prone to type 2 diabetes than women; which is confirmed by our training dataset. \n\nWe also clearly see that as our population ages, the chances of developing diabetes increases. This was confirmed in studies from the CDC in the US, the UK, China, Taiwan and even Bali in Indonesia. According to this article https:\/\/www.intechopen.com\/books\/glucose-tolerance\/age-is-an-important-risk-factor-for-type-2-diabetes-mellitus-and-cardiovascular-diseases the peak prevalence of diabetes can be found in the age group of 65-74 years with 15.7% in men and 10.4% in women (Shelton, 2006). More about the incidence of diabetes in older adults can be found in this paper: https:\/\/care.diabetesjournals.org\/content\/35\/12\/2650 published by the Journal of Diabetes."}}