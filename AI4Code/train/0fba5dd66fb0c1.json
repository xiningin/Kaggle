{"cell_type":{"5a258068":"code","a5e24fcb":"code","e93657da":"code","e22d820a":"code","41079a78":"code","09026607":"code","0d32f4cf":"code","4ab512a7":"code","da336a02":"code","0d527ef5":"code","7574e935":"code","90931840":"code","a0f87723":"code","8fe7dc76":"code","a1e708a2":"code","ff18b614":"code","beff5b99":"code","dda2a084":"code","21d8f074":"code","55c4822c":"code","ddce8ea3":"code","946c744a":"code","49bbd9e9":"code","01691e06":"code","6c7bb76b":"code","6471bd7a":"code","d95966b3":"code","ece3ad07":"code","9f8a9141":"code","434a15b4":"code","042ffc2a":"code","5641cf84":"code","b3a91c9e":"code","10bcf29c":"code","a742b274":"code","62c9d1f6":"code","f3e05aab":"code","3f788459":"code","fa5e2864":"code","19363cea":"code","10777077":"code","f04e93b3":"code","bb6d57ca":"code","874109a0":"code","2c656215":"code","6ab11807":"markdown","adeb349f":"markdown","e46c97e0":"markdown","523cae96":"markdown","a66c0184":"markdown","ac411426":"markdown","a41ddbf3":"markdown","fe507168":"markdown","050c1f88":"markdown","eab01df2":"markdown","c9258776":"markdown","5f073faf":"markdown","8d2c2ada":"markdown","1c5465fe":"markdown","0c00706b":"markdown","35cbb6dc":"markdown"},"source":{"5a258068":"#let's import the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')","a5e24fcb":"#let's import each datasets with the below codes\ndf_death = pd.read_excel(r\"..\/input\/breast-cancer-datasets\/death.xlsx\")\ndf_recovered = pd.read_excel(r\"..\/input\/breast-cancer-datasets\/recovered.xlsx\")\ndf_under_treatment = pd.read_excel(r\"..\/input\/breast-cancer-datasets\/under treatment.xlsx\")","e93657da":"df_death.shape, df_recovered.shape, df_under_treatment.shape","e22d820a":"#let's concat the three different excel files into one file by executing of the below codes\n\n# filenames\nexcel_names = [\"..\/input\/breast-cancer-datasets\/death.xlsx\", \"..\/input\/breast-cancer-datasets\/recovered.xlsx\", \"..\/input\/breast-cancer-datasets\/under treatment.xlsx\"]\n\n# read them in\nexcels = [pd.ExcelFile(name) for name in excel_names]\n\n# turn them into dataframes\nframes = [x.parse(x.sheet_names[0], header=0,index_col=None) for x in excels]\n\n# delete the first row for all frames except the first\n# i.e. remove the header row -- assumes it's the first\nframes[1:] = [df[1:] for df in frames[1:]]\n\n# concatenate them..\ndf = pd.concat(frames)","41079a78":"df.shape","09026607":"df.head()","0d32f4cf":"df[\"treatment_data\"]","4ab512a7":"patient_id = df['patient_id']\ndf= df.drop(df[['patient_id','treatment_data','id_healthcenter','id_treatment_region','birth_date']],axis = True)\ndf.head()\n","da336a02":"target = df['\\nBenign_malignant_cancer']\ntarget.var","0d527ef5":"df.describe()","7574e935":"df.info()","90931840":"#let's look at the columns of the datasets\ndf.columns","a0f87723":"categorical = len(df.select_dtypes(include = [\"object\"]).columns)\nnumerical = len(df.select_dtypes(include=[\"int\", \"float64\"]).columns)\n\nprint('Total Features: ', categorical, 'categorical', '+',\n      numerical, 'numerical', '=', categorical+numerical, 'features')","8fe7dc76":"# Look at numeric and categorical values separately\n\n\ndf.select_dtypes(include=[\"int\", \"float64\"]).columns","a1e708a2":"sns.pairplot(df, hue='\\nBenign_malignant_cancer', vars=['age','weight', 'thickness_tumor','blood', 'taking_heartMedicine', 'taking_gallbladder_disease_medicine',\n                                                        'smoking', 'alcohol','breast_pain','radiation_history', 'Birth_control(Contraception)',\n                                                        '\\nmenstrual_age'])\nplt.show()","ff18b614":"#correlation matrix to find out the most related features to the \"'\\nBenign_malignant_cancer'\" \ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cmap=\"coolwarm\", vmax=.8, square=True);","beff5b99":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x = '\\nmenopausal_age', y = 'age', hue = '\\nBenign_malignant_cancer', data = df)","dda2a084":"#it is too much features to look at, let's look at the top 10 features related to the Benign_malignant_cancer\n# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, '\\nBenign_malignant_cancer')['\\nBenign_malignant_cancer'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","21d8f074":"#let's look at the distribution vatiables of the target\nsns.countplot(df['\\nBenign_malignant_cancer'], label = \"Count\")","55c4822c":"#for checking the categorical variable, we would execute the below code\ndf.select_dtypes(include = [\"object\"]).columns\n","ddce8ea3":"#categorical features of the datasets\ndf_cat = df[[ 'marital_length', 'pregnency_experience', 'giving_birth',\n       'age_FirstGivingBirth  ', '\\nabortion', 'condition']]","946c744a":"#numerical variables of the datasets\ndf_num = df[['gender ', 'education','hereditary_history', 'age',\n       'weight', 'thickness_tumor', 'marital_status', 'blood',\n       'taking_heartMedicine', 'taking_blood_pressure_medicine',\n       'taking_gallbladder_disease_medicine', 'smoking', 'alcohol',\n       'breast_pain', 'radiation_history', 'Birth_control(Contraception)',\n       '\\nmenstrual_age', '\\nmenopausal_age', '\\nBenign_malignant_cancer']]","49bbd9e9":"#Distribution for all numeric variables \n#As we can see the Age Distribution is Normal and the others are not\n\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","01691e06":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts()).set_title(i)\n    plt.show()","6c7bb76b":"sns.boxplot(x=df['\\nabortion'], y=df['\\nBenign_malignant_cancer'])","6471bd7a":"most_corr = pd.DataFrame(cols)\nmost_corr.columns = ['Most Correlated Features']\nmost_corr","d95966b3":"#missing data\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)\ntrain = df.drop((missing_data[missing_data['Total'] > 81]).index,1)","ece3ad07":"df.isnull().sum()","9f8a9141":"df['\\nmenopausal_age'] = df['\\nmenopausal_age'].fillna(df['\\nmenopausal_age'].median())","434a15b4":"df.isnull().sum()","042ffc2a":"df.info()","5641cf84":"pd.get_dummies(df)","b3a91c9e":"df = df.drop(df[['marital_length', 'pregnency_experience', 'giving_birth',\n       'age_FirstGivingBirth  ', '\\nabortion', 'condition']],axis = True)","10bcf29c":"y = df['\\nBenign_malignant_cancer']\nX = df.drop('\\nBenign_malignant_cancer', axis=1)\n","a742b274":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n\n\nprint(f\"'X' shape: {X.shape}\")\nprint(f\"'y' shape: {y.shape}\")\n\npipeline = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('std_scaler', StandardScaler())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","62c9d1f6":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","f3e05aab":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC(loss='hinge', dual=True)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","3f788459":"from sklearn.svm import SVC\n\n# The hyperparameter coef0 controls how much the model is influenced by high degree ploynomials \nmodel = SVC(kernel='poly', degree=2, gamma='auto', coef0=1, C=5)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","fa5e2864":"model = SVC(kernel='rbf', gamma=0.5, C=0.1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","19363cea":"X_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","10777077":"print(\"=======================Linear Kernel SVM==========================\")\nmodel = SVC(kernel='linear')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Polynomial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='poly', degree=2, gamma='auto')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Radial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='rbf', gamma=1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","f04e93b3":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\ngrid.fit(X_train, y_train)\n\nbest_params = grid.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","bb6d57ca":"scaler = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","874109a0":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","2c656215":"plt.figure(figsize=(8,6))\nplt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap='plasma')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","6ab11807":"I want to show how a Data Scientist would address a technical problem by analyzing the breast cancer datasets from three excel files such as (Death, Recovered, Under Treatment) through conducting some technical activities such as:\n* The preparation of the breast cancer for support vector machine algorithm\n* Visualization and data exploratory of the dataset to know better the data and feature engineering steps\n* Support Vector Machine kernels (Linear, Polynomial, Radial)\n* Conducting the Support vector machine hyperparameter tuning\n* Conducting the Principal Component Analysis and how to use it to reduce the complexity of a problem\n* The goal is to correctly predict if someone weather would have Benign or malignant tumor\n\nMy overview of my implementation in this notebook would be described as below:\n* Understand the shape of the data\n* Data Cleaning\n* Data Visualization\n* Data Exploration\n* Feature Engineering\n* Data Preprocessing for Model\n* Basic Model Building (SVM)\n* Model Tuning Hyperparameters\n* PCA Analysis","adeb349f":"# 2. 3. 2. Polynomial Kernel SVM\nThis code trains a SVM classifier using 2rd degree ploynomial kernel.","e46c97e0":"Other kernels exist but are not used much more rarely. For example, some kernels are specialized for specific data structures. string kernels are sometimes used when classifying text document on DNA sequences.\n\nWith so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first, especially if the training set is very large or if it has plenty of features. If the training se is not too large, you should try the Gaussian RBF kernel as well. ","523cae96":"# 5. Summary\n\nIn this notebook you discovered the Support Vector Machine Algorithm for machine learning.  You learned about:\n- What is support vector machine?.\n- Support vector machine implementation in Python.\n- Support Vector Machine kernels (Linear, Polynomial, Radial).\n- How to prepare the data for support vector machine algorithm.\n- Support vector machine hyperparameter tuning.\n- Principal Compenent Analysis and how to use it to reduce the complexity of a problem.\n- How to calculate the Principal Component Analysis for reuse on more data in scikit-learn.","a66c0184":"# Support Vector Machine\nA Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonliner classification, regression, and even outlier detection. In this notebook, we will discover the support vector machine algorithm as well as it implementation in scikit-learn. We will also discover the Principal Component Analysis and its implementation with scikit-learn. ","ac411426":"# 1. 1. What is Support Vector Machine?\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points.\n\n![1.png](attachment:1.png)\n\n![2.png](attachment:2.png)\n\nTo separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.","a41ddbf3":"### 2. 3. 3. Radial Kernel SVM\nJust like the polynomial features method, the similarity features can be useful with any ","fe507168":"## 2. 4. Data Preparation for SVM\nThis section lists some suggestions for how to best prepare your training data when learning an SVM model.\n\n- **Numerical Inputs:** SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).\n- **Binary Classification:** Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.","050c1f88":"we do not need some features as long as they just only for indentification such as patient_id, id_healthcenter, id_treatment_region\n\nwe also do not need the treatment date because the data gathered in only for 2019\n\nwe also do not need the birth data as long as we have the age column\n","eab01df2":"## 2. 3. Support Vector Machines (Kernels)\n\n- `C parameter`: Controlls trade-off between classifying training points correctly and having a smooth decision boundary.\n    - Small C (loose) makes cost (penalty) of misclassification low (soft margin)\n    - Large C (strict) makes cost of misclassification high (hard margin), forcing the model to explain input data stricter and potentially over it.\n- `gamma parameter`: Controlls how far the influence of a single training set reaches.\n    - Large gamma: close reach (closer data points have high weight)\n    - Small gamma: far reach (more generalized solution)\n- `degree parameter` : Degree of the polynomial kernel function (`'poly'`). Ignored by all other kernels.\n\nA common approach to find the right hyperparameter values is to use grid search. It is often faster to first do a very coarse grid search, then a finer grid search around the best values found. Having a good sence of the what each hyperparameter actually does can also help you search in the right part of the hyperparameter space.","c9258776":"# 4. Principal Component Analysis\n\nPCA is:\n* Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n* Unsupervised Machine Learning\n* A transformation of your data and attempts to find out what features explain the most variance in your data. For example:","5f073faf":"# 2. 1. VISUALIZING THE DATA\n\n","8d2c2ada":"the distribution variables of the target looks fine in case of balancing data","1c5465fe":"# 3. Support Vector Machine Hyperparameter tuning","0c00706b":"# Data Understanding:\nBefore dive into the machine learning processes, let\u2019s describe what the features of the datasets stand for and how some of them are categorized. \n\nWe have three datasets with the same features and target, but for three categories such as \n* The patients who are under treatment\n* The patients who are recovered \n* The patients who died \n\n# The features could be described as bellow:\n* patient_id: the id of the patient \n* gender: the gender of the patient which \u201cFemale\u201d is 0 and \u201cMale\u201d is 1\n* education: the education level of the patient which Illiterate=0, Elementary= 1, Middle School =2 , High School =3 , Diploma = 4, Associate =5 , Bachelor =6 , Master = 7\n* treatment_date = the date(year) which the patient would receive the treatment\n* id_healthcenter:: is the id for the healthcare center\n* id_treatment_region: the region which the patient would receive the treatment\n* hereditary_history: the patient has the hereditary history of disease which 1 means \u201cYes\u201d and 0 means \u201cNo\u201d\n* birth_date: birth date (year) of the patient\n* age: the age of the patient\n* weight: the weight of the patient\n* thickness_tumor: the thickness of the tumor detected in the patient body\n* marital_status: the marital status of the patient includes : 1 means married and 0 means single\n* marital_length: the age of the martial status includes 0 means under 10 years, 1 means above 10 years\n* pregnency_experience: the patient has the pregnancy experience which 0 means has not experience and 1 means has experience\n* giving_birth: the patient has experienced giving the birth. Each number means the number of giving birth\n* age_FirstGivingBirth : in which age the patient has the first experience of giving a birth, which before age 30 equals 0 and after age 30 equals 1\n* abortion: the patient has experience of abortion which 0 means has not and 1 means has\n* blood: the type of bloods A+ =0, A- = 1, AB+ = 2, AB- = 3, B+ = 4, B- = 5, O+ = 6, O- = 7\n* taking_heartMedicine: it says if the patient takes the heart medicine or not which 0 means does not and 1 means does\n* taking_blood_pressure_medicine: it says if the patient takes the blood pressure or not which 0 means does not and 1 means does\n* taking_gallbladder_disease_medicine: it says the patient takes the gallbladder medicine or not which 0 means does not and 1 means does\n* smoking: it says if the patient smokes or not which 0 means does not and 1 means does\n* alcohol: it says if the patient drinks alcohol or not which 0 means does not and 1 means does\n* breast_pain: if the patient has pain in breast part which 0 means has not and 1 means has\n* radiation_history: if the patient has experience with radiation in breast area which 0 means has not and 1 means has\n* Birth_control(Contraception): the patient takes actions for birth control which 0 means does not and 1 means does \n* menstrual_age: at which age the patient starts natural menstrual which 0 means the patient does not start menstrual, 1 means under age 12, and 2 means above age 12\n* menopausal_age: at which age the patient starts natural menopausal which 0 means does not starts, 1 means at under age 50, and 2 means above age 50\n* tumor: if the patient has the tumor which 0 means No and 1 means 1\n* condition: the condition of the patient which categorized into different categories such as under treatment, recovered, death\n* Benign_malignant_tumor: is the target of our model which Benign is 0 and malignant is 1","35cbb6dc":"# 1. 2. Hyperplanes and Support Vectors\n\n\n![3.png](attachment:3.png)\n\nHyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n\n![5.jpg](attachment:5.jpg)\n\nSupport vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n\n# 1. 3. Large Margin Intuition\n\nIn logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. If the squashed value is greater than a threshold value(0.5) we assign it a label 1, else we assign it a label 0. In SVM, we take the output of the linear function and if that output is greater than 1, we identify it with one class and if the output is -1, we identify is with another class. Since the threshold values are changed to 1 and -1 in SVM, we obtain this reinforcement range of values([-1,1]) which acts as margin.\n\n\n# 2. SVM Implementation in Python\n\nWe will use support vector machine in Predicting if the cancer diagnosis is benign or malignant based on several observations\/features."}}