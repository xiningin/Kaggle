{"cell_type":{"e8ec1ca2":"code","1fc57ade":"code","e64c590b":"code","7b188100":"code","831a7feb":"code","a32689d7":"code","f57eb984":"code","a81ff9e8":"code","c8c7fc77":"code","a3bde090":"code","ad4edd9c":"code","e813710d":"code","b92a1e8c":"code","d49439a7":"code","5b03ec0e":"code","348d46b5":"code","aed99e12":"code","ef1cf63f":"code","d64341a2":"markdown","a5201276":"markdown","6c70ac31":"markdown","471c8553":"markdown","7b027a8e":"markdown","b23ea839":"markdown","adc23400":"markdown","fe69094c":"markdown","85aa46bc":"markdown","70f2ecaf":"markdown","889233fe":"markdown","7bc66b3f":"markdown","cf11bc86":"markdown"},"source":{"e8ec1ca2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport itertools\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\nprint(os.listdir(\"..\/input\"))\n\n# Setting Random Seed for Reproducibilty.\nseed = 66\nnp.random.seed(seed)","1fc57ade":"data_train = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ndata_test = pd.read_csv('..\/input\/fashion-mnist_test.csv')\n\ndata_train.shape","e64c590b":"data_test.shape","7b188100":"data_train.head()","831a7feb":"data_train.isnull().any().describe()","a32689d7":"data_test.isnull().any().describe()","f57eb984":"data_train.label.value_counts()","a81ff9e8":"data_test.label.value_counts()","c8c7fc77":"img = data_train.drop('label', axis=1).values[0].reshape(28,28)\nplt.imshow(img, cmap='gray')\nplt.colorbar()","a3bde090":"img_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\nX = np.array(data_train.iloc[:, 1:])\ny = to_categorical(np.array(data_train.iloc[:, 0]))\n\n# I have tried running kfold cross-validation, but the running time is longer and the performances aren't increased. \n# Therefore, I believed the dataset was large enough (especially with data augmentation) to only run a household cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)\n\n#Test data\nX_test = np.array(data_test.iloc[:, 1:])\ny_test = to_categorical(np.array(data_test.iloc[:, 0]))\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_val = X_val.astype('float32')\nX_train \/= 255\nX_test \/= 255\nX_val \/= 255","ad4edd9c":"batch_size = 256\nnum_classes = 10\nepochs = 75\n\ndata_generator = ImageDataGenerator(\n        rotation_range = 3,\n        zoom_range = 0.1,\n        shear_range = 0.3,\n        width_shift_range=0.08,\n        height_shift_range=0.08,\n        vertical_flip=False)\n\ndata_generator.fit(X_train)\n\nreduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","e813710d":"class basicConvNet():\n    @staticmethod\n    def build(input_shape, num_classes):\n        # Builds a basic ConvNet\n        # Returns Keras model object\n        model = Sequential()\n        \n        model.add(Conv2D(32, kernel_size=(3, 3),\n                         activation='relu',\n                         kernel_initializer='he_normal',\n                         input_shape=input_shape))\n        model.add(MaxPooling2D((2, 2)))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, (3, 3), activation='relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, (3, 3), activation='relu'))\n        model.add(Dropout(0.4))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(num_classes, activation='softmax'))\n        \n        return model","b92a1e8c":"class miniVGGNet():\n    @staticmethod\n    def build(input_shape, num_classes):\n        # Builds a MiniVGGNet\n        # Returns Keras model object\n        model = Sequential()\n\n        # first CONV => RELU => CONV => RELU => POOL layer set\n        model.add(Conv2D(32, (3, 3), padding=\"same\",\n            input_shape=input_shape))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n \n        # second CONV => RELU => CONV => RELU => POOL layer set\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(axis=-1))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Dropout(0.25))\n \n        # first (and only) set of FC => RELU layers\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n \n        # softmax classifier\n        model.add(Dense(num_classes))\n        model.add(Activation(\"softmax\"))\n \n        # return the constructed network architecture\n        return model","d49439a7":"# model = basicConvNet.build(input_shape, num_classes)\nmodel = miniVGGNet.build(input_shape, num_classes)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\nmodel.summary()","5b03ec0e":"history = model.fit_generator(data_generator.flow(X_train, y_train, batch_size = batch_size), \n                              epochs = epochs, \n                              validation_data = (X_val, y_val),\n                              verbose=1, \n                              steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks = [reduce_lr])","348d46b5":"%matplotlib inline\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.show()\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title(\"Model Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.show()","aed99e12":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint('Loss on test dataset:', scores[0])\nprint('Accuracy on test dataset:', scores[1])","ef1cf63f":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Oranges):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, \n            classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])","d64341a2":"The model seems to generalize well.","a5201276":"# Context\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI\/ML\/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n\nEach training and test example is assigned to one of the following labels:\n0. T-shirt\/top\n1. Trouser\n2. Pullover\n3. Dress\n4. Coat\n5. Sandal\n6. Shirt\n7. Sneaker\n8. Bag\n9. Ankle boot\n\nI will classify the different images of the fashion MNIST dataset in those 10 labels.","6c70ac31":"We can see that the class are balanced in both datasets (10% for each class). The number of different possible labels is the same.<br\/>\nTo visualize what our data represents, here is a plot of the first object.","471c8553":"The datasets are both composed of 785 columns.<br\/>\n28*28 = 784 pixels per image.<br\/>\nThe last column named \"label\" represents the class.<br\/>\n<br\/>\nNow, I verify that there isn't any null values in the datasets.","7b027a8e":"Validation loss decreases and validation accuracy increases. Model does not seem to overfit.<br\/>\nModel loss stopped decreasing and model accuracy stopped increasing. Model does not seem to underfit (dataset is also large enough).","b23ea839":"# Evaluating model\nI will now apply the model to the test set to see how well it performs","adc23400":"# Model training\nThe first model I used was a ConvNet (keras implementation of [Gabriel Preda](https:\/\/www.kaggle.com\/gpreda\/cnn-with-tensorflow-keras-for-fashion-mnist)), and the second one a miniVGGNet (keras implementation of [Adrian Rosebrock](https:\/\/www.pyimagesearch.com\/2019\/02\/11\/fashion-mnist-with-keras-and-deep-learning\/)).<br\/>\nConvNet has less parameters and trains way faster than the miniVGGNet.<br\/>\nScores were close but miniVGGNet was getting higher scores in validation, so I chose to use this one.","fe69094c":"There are 60 000 examples in the train dataset and 10 000 in the test dataset.","85aa46bc":"On the confusion matrix, it seems that the model is mostly having issues to classify shirts. A shirt being very close to a pullover or a t-shirt, this isn't very surprising.","70f2ecaf":"# Data formatting\nI will run household cross-validation with 25% of the data in the validation dataset.<br\/>\nThe dataset seems large enough (especially with data augmentation) and the data simple enough to run only a household cross-validation and not a kfold cross-validation.\n","889233fe":"# Table of Contents\n1. Data exploration\n2. Data formatting\n3. Model Training\n4. Evaluating Model","7bc66b3f":"# Data exploration\nFirst, I quickly checked the data format","cf11bc86":"Checking if the model is overfitting with loss and val_loss of the training history"}}