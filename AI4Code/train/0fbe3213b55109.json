{"cell_type":{"e88a389e":"code","56ab5c50":"code","83fea55c":"code","43683b62":"code","b5cc20f4":"code","7d438ad1":"code","a0169e07":"code","e4bf929a":"code","7516af13":"code","be82b578":"code","558cc591":"code","5fdb0b05":"code","f0e17c67":"code","e9207563":"code","33262cf9":"code","e6811843":"code","4974b46c":"code","b91da049":"code","6973bdb8":"markdown","5266aaba":"markdown","9c94468d":"markdown","5dea20d6":"markdown","5955e9c1":"markdown","5cbefc99":"markdown","a0e4cfec":"markdown","4d272678":"markdown","dc4347d3":"markdown","ffa0d739":"markdown"},"source":{"e88a389e":"import os\nimport glob\nimport numpy as np","56ab5c50":"BAND_NAME = 'MACHINE HEAD'\nBASE_PATH =  '..\/input\/large-metal-lyrics-archive-228k-songs\/metal_lyrics'","83fea55c":"lyrics_files = glob.glob(os.path.join(BASE_PATH, BAND_NAME.lower()[0], BAND_NAME,'*','*.txt'))\n\n# printing a sample\nwith open(lyrics_files[0]) as f: \n    print (f.read(200))\n    \n# generating corpus\ncorpus = []\nfor lyric in lyrics_files:\n    with open(lyric) as f: \n        corpus.append(f.read())\n\n# cleaning up the lyrics\ncorpus = '\\n'.join([x.lower() for x in corpus])       # join all songs to form on big corpus\ncorpus = corpus.split('\\n')                           # split the lines\ncorpus = [x for x in corpus if not x.startswith('[')] # removing comments starting with [\ncorpus = [x for x in corpus if x != '']               # removing empty items\ncorpus[:10]                                           # get first 25 lines","43683b62":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index = tokenizer.word_index\nreverse_word_index = dict([(v,k) for (k,v) in word_index.items()])\n\nTOTAL_WORDS = len(word_index) + 1\nprint ('Total Words :', TOTAL_WORDS) # added 1 for OOV token","b5cc20f4":"input_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_seq = token_list[:i+1]\n        input_sequences.append(n_gram_seq)","7d438ad1":"MAX_SEQ_LEN = max([len(x) for x in input_sequences])\nprint ('Max Sequence Length :', MAX_SEQ_LEN)","a0169e07":"from tensorflow.keras.preprocessing.sequence import pad_sequences\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen = MAX_SEQ_LEN, padding = 'pre'))\n\nxs = input_sequences[:,:-1]\nlabels = input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes = TOTAL_WORDS)\nxs.shape, ys.shape","e4bf929a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\nmodel = Sequential([\n    Embedding(TOTAL_WORDS, 256, input_length = MAX_SEQ_LEN - 1),\n    Bidirectional(LSTM(128, return_sequences = True)),\n    Bidirectional(LSTM(128)),\n    Dropout(0.2),\n    Dense(TOTAL_WORDS, activation = 'softmax')\n])\n\nmodel.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\nmodel.summary()\n%time history = model.fit(xs, ys, epochs = 200, verbose = 0, batch_size = 256)","7516af13":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(history.history['accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()","be82b578":"seed_text  = 'Let freedom ring with a shotgun blast'\nnext_words = 50\nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen = MAX_SEQ_LEN - 1, padding = 'pre')\n    predicted  = model.predict_classes(token_list, verbose = 0)\n    seed_text += ' ' + reverse_word_index[predicted[0]]\nprint (seed_text)","558cc591":"character_corpus = '\\n'.join(corpus)\ncharacter_corpus[:100]\nVOCAB = list(set(character_corpus))\nVOCAB_SIZE = len(VOCAB) + 1\nTOTAL_SIZE = len(character_corpus)\nword_index = dict([(x,i+1) for (i,x) in enumerate(VOCAB)]) \nreverse_word_index = dict([(v,k) for (k,v) in word_index.items()])","5fdb0b05":"print ('TOTAL SIZE :', TOTAL_SIZE)\nprint ('VOCAB SIZE :', VOCAB_SIZE)","f0e17c67":"print ([x for x in word_index.keys()])","e9207563":"xs = []; ys = []\nSEQ_LEN = 30\nfor i in range(TOTAL_SIZE - SEQ_LEN):\n    xs.append([word_index[x] for x in character_corpus[i:i+SEQ_LEN]])\n    ys.append(word_index[character_corpus[i+SEQ_LEN]])","33262cf9":"xs = np.array(xs)\nys = np.array(tf.keras.utils.to_categorical(ys, num_classes = VOCAB_SIZE))\nxs.shape, ys.shape","e6811843":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, Conv1D, MaxPooling1D\nmodel = Sequential([\n    Embedding(VOCAB_SIZE, VOCAB_SIZE, input_length = SEQ_LEN),\n    Conv1D(32, (3), activation = 'relu'),\n    Conv1D(32, (3), activation = 'relu'),\n    MaxPooling1D(2),\n    Bidirectional(LSTM(64, return_sequences = True)),\n    Bidirectional(LSTM(32)),\n    Dropout(0.2),\n    Dense(VOCAB_SIZE, activation = 'softmax')\n])\n\nmodel.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\nmodel.summary()\n%time history = model.fit(xs, ys, epochs = 200, verbose = 0, batch_size = 1024)","4974b46c":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n\nplt.plot(history.history['accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()","b91da049":"seed_text  = 'let freedom ring with a shotgun blast'\nnext_words = 500\nfor _ in range(next_words):\n    seed_reshaped = np.array([word_index[x] for x in seed_text[-SEQ_LEN:]]).reshape(1, SEQ_LEN)\n    predicted  = model.predict_classes(seed_reshaped, verbose = 0)\n    seed_text += reverse_word_index[predicted[0]]\nprint (seed_text)","6973bdb8":"# A Metal Lyrics Generator\nThis notebook shows basic application for LSTM networks using Keras and Tensorflow. We will explore a sequence generation problem which is well suited for LSTMs.\n\n### Dataset\nIn this notebook we will explore how to use LSTM to generate metal lyrics. We'll use this large metal lyrics archive, which has lyrics for almost all metal bands you can name, stored as a text file.\n\n### Approaches\nFor generating word based sequences two approaches can be followed\n\n1. Creating input sequences of words and predicting word outputs\n2. Using characters as inputs and predicting characters\n\nwe will explore both in this notebook\n\n### Methodology  \n1. First we will load the text files and combine it into a single file\n2. We will seperate file at `\\n` to create lines and then at space to create words.\n3. Clean the words that are not part of lyrics `[Music:..], [Chorus]` etc.\n4. Defile a LSTM model\n5. Predict next 50 words using seed text\n\nLet's get started...\n","5266aaba":"### Generate Lyrics","9c94468d":"From the sequences, identify the sequence with max length which will be used to pad all sequences at to this length","5dea20d6":"Printing out the vocabulary","5955e9c1":"By calling `tokenizer` method `texts_to_sequences` we can convert sequences of text to sequences of numbers. The resulting seqences can then be used to generate n_gram sequences. For example,\n\n* 1st Line - `i was born as a bastard, no father, no master,`\n* Tokenized line - `[4, 92, 324, 40, 6, 1934, 28, 846, 28, 847]`. Note that the punctuations are removed.\n* N grams sequences\n    ```\n    [4, 92]\n    [4, 92, 324]\n    [4, 92, 324, 40]\n    [4, 92, 324, 40, 6]\n    [4, 92, 324, 40, 6, 1934]\n    [4, 92, 324, 40, 6, 1934, 28]\n    [4, 92, 324, 40, 6, 1934, 28, 846]\n    [4, 92, 324, 40, 6, 1934, 28, 846, 28]\n    [4, 92, 324, 40, 6, 1934, 28, 846, 28, 847]\n    ```\n","5cbefc99":"### Loadin' & Preppin'\nThe files are organized by band name and contains sub folders for their albums. For this exercise, we will load all songs for a particular band, starting with one of my favorites - MACHINE HEAD. We will load all lyrics txt file for this band and combine them to form a corpus which we will clean to get rid of any comments","a0e4cfec":"# Approach 2 : Using character sequences\nWe will read the corpus of text as one big sentence and tokenize each character. The unique characters become part of the vocabulary, which will be much more smaller than tokenized words.\nUsing these characters as inputs, we can predict the next character, which will also include punctuations.","4d272678":"### Tokenize\nWe will use the `Tokenizer` available from keras API and pass on generated corpus. The tokenizer will split the lines into words and generate a word_index (a dictionary of words to integer mappings). For training a model, we will use all words present in the corpus","dc4347d3":"### Defining a Model","ffa0d739":"### Conclusion & Next Steps\n\nThe character based sequences prediction has advantages of following the structure, but some words don't make sense. In the word based sequence predictions, even though the words are correct the sentences don't make sense. The structure also is not adhered to. As next steps,we could try\n1. Filter out unwanted characters and words from vocabulary\n2. Tune the batch size\n3. Adding more memory units and layers\n4. Training the model in padded sentences instead of random sequences - for character based"}}