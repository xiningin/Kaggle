{"cell_type":{"ada9241b":"code","7d9b393c":"code","dcaae1bc":"code","c94e8b93":"code","bf78386c":"code","5274d8c4":"code","52452395":"code","7da234d8":"code","46c32bd5":"code","7ecd3dfd":"code","84294e7e":"code","d00c22be":"code","48586f27":"code","50300934":"code","79eef839":"code","51d876f2":"code","caae5ffa":"code","744e53dc":"code","95c70a54":"code","c7929143":"code","0a9d7f5d":"code","e151376c":"code","4928a5d8":"markdown","0d8574b9":"markdown","ff45b9b8":"markdown","3bdb4861":"markdown","877a238c":"markdown","18d99132":"markdown","51cb0f48":"markdown","5c1d8d06":"markdown","6f42f53f":"markdown","9b0962e1":"markdown","0f92b488":"markdown"},"source":{"ada9241b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport umap\nimport seaborn as sns\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None","7d9b393c":"data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")","dcaae1bc":"data.head()","c94e8b93":"x = data.iloc[:,1:76]\ny = data.iloc[:,-1]\n\nx_predict_data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nx_predict = x_predict_data.iloc[:,1:76]\n","bf78386c":"x.describe()","5274d8c4":"y.describe()","52452395":"x.head()","7da234d8":"x.isna().sum()","46c32bd5":"x.dtypes","7ecd3dfd":"pd.crosstab(x.iloc[:,0],y)\n#for i in range(0,75):\n#   print(pd.crosstab(x.iloc[:,i],y))","84294e7e":"dup = x.loc[x.duplicated(keep=False)]\ndup.shape","d00c22be":"plt.figure(figsize = (76,6));\nx1 = x.iloc[:,0:2]\nsns.boxplot(x = 'variable', y = 'value',data =x1.melt()).set_title('Original Feature Box Plots');","48586f27":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","50300934":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test  = train_test_split(x,y,train_size=0.75,random_state=42)","79eef839":"x_train = x_train.applymap(lambda p: np.log(p+1))\n\nx_test = x_test.applymap(lambda p: np.log(p+1))\n\nx_predict = x_predict.applymap(lambda p: np.log(p+1))","51d876f2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\nx_predict = sc.fit_transform(x_predict)","caae5ffa":"from sklearn.decomposition import PCA\npca = PCA().fit(x_train)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 76, step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 75, step=2)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","744e53dc":"import lightgbm as lgb\nparams = {'bagging_freq': 1, \n          'verbosity': -1,  \n          'num_threads': -1, \n          'feature_pre_filter': False,  \n          'objective': 'multiclass', \n          'metric': 'multi_logloss',  \n          'bagging_fraction': 0.5, \n          'feature_fraction': 0.8, \n          'lambda_l1': 10, \n          'lambda_l2': 10, \n          'learning_rate': 0.013959172480364537, \n          'max_depth': 6, \n          'min_child_samples': 25, \n          'num_leaves': 31}\nmodel = lgb.LGBMClassifier(**params)\nmodel.fit(x_train,y_train)","95c70a54":"#from catboost import CatBoostClassifier\n#model = CatBoostClassifier(iterations = 4000,reg_lambda=100,learning_rate = 0.01,bootstrap_type='Bernoulli',random_strength = 5,depth = 8,task_type = 'GPU',loss_function='MultiClass',silent = True)\n#model.fit(x_train,y_train)","c7929143":"y_pred_proba = model.predict_proba(x_predict)","0a9d7f5d":"output = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\noutput_df = pd.DataFrame(y_pred_proba, columns=['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9'])\noutput_df = pd.concat([x_predict_data.id,output_df],axis = 1)\noutput_df.head()","e151376c":"output_df.to_csv('.\/Output_Catb.csv', index=False)","4928a5d8":"**Considering the Max, Min values and frequency distribution of values in all the features, We can see there is a presence of skewness with majority values lying between 0-2(for example). Hence Plotting a Box plot to check if skewness exists.**","0d8574b9":"**A major difference I noticed compared to last month's TPS was the presence of Duplicate values i.e.. Feature collection having same value but giving a different classification output.**\n\n**Tried running the models with \/ without it. Got somewhat similar performance. Still have to deal with it.**\n\n**330 values from a row count of 200k**\n\n**It might just be a kind of trap in the data or the data might have been taken from a whole larger dataset \/ encoded from real world values which I was unable to figure out considering the data is numeric with no feature descriptions.**","ff45b9b8":"**Using Label Encoder to encode output classes so its easier for our models to do predictions.**","3bdb4861":"**Importing Relevant Libraries for Exploration, Plotting and use of Models through our analysis!**","877a238c":"**Tabular Playground Series is always a nice place to test new techniques and learn something new every month.**\n\nTried a different approach this month.\n\nStill on 1.7511.\n\n**Any help would be appreciated to help me reach a better score.**\n\nAlso, have done extensive EDA, will be uploading a full detailed EDA intuition and Inference Notebook pretty soon!","18d99132":"**Running some initial analysis!**\n\n1. Seeing given data has 77 columns.\n\n2. First column for Id Numbers.\n\n3. Last column for Output Classes. Total of 9 classes.\n\n4. 75 Feature columns ( Large number of features makes us think if feature engineering might be required).\n\n5. 200k rows in the dataset.\n\n6. No missing Values in dataset.\n\n7. All values are integers.\n\n8. **Max class frequency is of Class 6**\n\n9. Maximum values of features are distributed between 0-2 though there are different maximums for different features.\n\n10. Conclusion will be that this is a **Multi Class Classification problem** and we will treat it like that","51cb0f48":"**Splitting the Dataset into train and test data.**\n\nThis allows us to split the current dataset and work on it to get the most optimum solution out of it, train our model and then apply it to predict probabilities on Predition(test) data.","5c1d8d06":"**How to deal with Skewness in data?**\n\nFor skewness in data in Machine Learning problems the best way is to tackle it by standardizing data by applying Transformation, Scaling or both Transformation and Scaling.\n\n**I have used Logarithmic Transformation in combination with Standard Scaler (between 0-1) to standardize my data.**","6f42f53f":"**Now the Question arises if we need to do Principal Component Analysis (PCA)?**\n\nAs the number of features are large, it is a quick thought to reduce dimentionality of the dataset and to use only those features which might affect our outcome.\n\nI tried Plotting a graph of Cumulative variance vs No. of components\n\nThis is being done to use only the number of relevant component features which affect the outcome. Threshold being selected as 95%\n\nBut then I realise that it is a classification problem and 95% covers almost all of our features and we might miss out on important information about our output classes if we reduce any number of features.\n\nHence, i decided to skip PCA for this month's TPS solution\n\nPlease feel free to try this approach and let me know in comments if PCA gets us better outputs.\n\n","9b0962e1":"**Importing the dataset**","0f92b488":"**Applying Catboost to predict output probabilities**"}}