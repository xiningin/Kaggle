{"cell_type":{"61c15576":"code","809fd769":"code","89824cb8":"code","bb6d791a":"code","0c32fee2":"code","c11e5c02":"code","f1db4711":"code","f096983e":"code","71d9559f":"code","75175499":"code","66ff5117":"code","432b581a":"code","41e06108":"code","baf9d642":"code","8fb65561":"code","93ad68d3":"code","a7615f50":"code","4be44fe9":"code","a1170ad3":"code","51a65558":"code","7e73521e":"code","69680bb5":"markdown","a0341c86":"markdown","3eefdf25":"markdown","00a7d2c5":"markdown","9eff1b14":"markdown","ee2bc645":"markdown","bd9c35ed":"markdown","dec2a070":"markdown","74dbd419":"markdown","12aedb7a":"markdown"},"source":{"61c15576":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.metrics import plot_confusion_matrix, confusion_matrix\nimport eli5\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","809fd769":"sample_code = pd.read_csv(\"\/kaggle\/input\/sample-github-code\/sample_code.csv\", \n                          lineterminator='\\n')  \n## read CSV error : https:\/\/stackoverflow.com\/questions\/33998740\/error-in-reading-a-csv-file-in-pandascparsererror-error-tokenizing-data-c-err\n\nsample_code.head()","89824cb8":"sample_code.describe(include='all')","bb6d791a":"## Combine .C, .cpp, .cc, .h to C++ file, .cpp, say  ## Source : ## http:\/\/gcc.gnu.org\/onlinedocs\/gcc-4.4.1\/gcc\/Overall-Options.html#index-file-name-suffix-71\nsample_code.loc[sample_code['type'].isin(['C', 'cpp', 'cc', 'h']), 'type'] = 'cpp'\n## Combine .html & .htm to .html, say\nsample_code.loc[sample_code['type'].isin(['html', 'htm']), 'type'] = 'html'","0c32fee2":"type_counts = sample_code['type'].value_counts().to_frame().reset_index()\ntype_counts","c11e5c02":"## Filter irrelevant files like null, .gitignore using value counts\ntop_languages = type_counts[type_counts['type'] >= 1000]\ntop_languages","f1db4711":"sns.barplot(data=top_languages, x='index', y='type')\nplt.xticks(rotation=90)","f096983e":"## Filtering files created by IDEs, version control & data files\ntop_languages = top_languages[~top_languages['index'].isin(['sublime-snippet', 'xcworkspacedata', 'gitignore', \n                                                            'project', 'properties', 'conf', 'config', 'cfg',\n                                                            'meta', 'test', 'gradle', 'patch', 'ebuild', 'ini',\n                                                           'csv', 'json', 'txt', 'geojson', 'svg', \n                                                            'tpl', 'less', 'cmake', 'mk', 'd'])]\n\nsns.barplot(data=top_languages, x='index', y='type')\nplt.xticks(rotation=90)","71d9559f":"## Filter with top_languages\ntrain = sample_code[sample_code['type'].isin(top_languages['index'].values)]\n\nprint(train.shape)\ntrain.head()","75175499":"i=10\nprint(\"\\n\".join(train['content'][i].split(\"\\n\")[:10]))\nprint(train['type'].values[i])","66ff5117":"train_x, val_x, train_y, val_y = model_selection.train_test_split(train['content'], train['type'], test_size=0.2)\n\nprint(len(train_x), len(val_x) )","432b581a":"#encoder = preprocessing.LabelEncoder()  ## For neural networks\n#train_y = encoder.fit_transform(train_y)\n#encoder.classes_\n#val_y = encoder.transform(val_y)  ## For neural networks\n#print(val_y[0])\n#print(encoder.inverse_transform([val_y[0]]))","41e06108":"vec = TfidfVectorizer(max_df = 0.6, min_df = 0.01, max_features = 10000, analyzer = 'word', \n                      #ngram_range=(2,3),\n                      use_idf=True, token_pattern=r'\\w{1,}')\n#tfidf = TfidfTransformer()\n#clf = MultinomialNB()  ## eli5 not supported\n#clf = linear_model.LogisticRegression()  ## Too slow\nclf = linear_model.SGDClassifier(loss = 'log', max_iter=50, tol=1e-3) ## Logistic regession only\n\n#pipe = make_pipeline(vec, tfidf, clf)\npipe = make_pipeline(vec, clf)\n\npipe.fit(train_x, train_y)","baf9d642":"pred_y = pipe.predict(val_x)\nreport = metrics.classification_report(val_y, pred_y)\nprint(report)\nprint(\"accuracy: {:0.2f}\".format(metrics.accuracy_score(val_y, pred_y)))\nprint(\"F1-score (weighted): {:0.2f}\".format(metrics.f1_score(val_y, pred_y, average = 'weighted')))","8fb65561":"labels = clf.classes_.tolist()\nprint(type(labels))\nprint(list(reversed(labels)))","93ad68d3":"'''\ncm = confusion_matrix(val_y, pred_y, labels)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111) \ncax = ax.matshow(cm) \nplt.title('Confusion matrix of the classifier') \nfig.colorbar(cax) \nax.set_xticklabels([''] + labels) \nax.set_yticklabels([''] + labels) \nplt.xlabel('Predicted') \nplt.ylabel('True') \nplt.show()\n####################\nax = plt.subplot()\nsns.heatmap(cm, annot=False, ax = ax, cmap=\"Blues\"); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(list(reversed(labels)))\nplt.rcParams[\"figure.figsize\"] = (27,25)\n'''","a7615f50":"eli5.show_weights(clf, vec=vec, top=10)\n## Source : https:\/\/eli5.readthedocs.io\/en\/latest\/_notebooks\/debug-sklearn-text.html","4be44fe9":"i=25\nprint(\"\\n\".join(val_x.values[i].split(\"\\n\")[:10]))\nprint(\"Actual class : \", val_y.values[i], \"\\nPredicted class : \", pred_y[i])\neli5.show_prediction(clf, val_x.values[i], vec=vec)","a1170ad3":"val = pd.concat([val_x, val_y], axis=1)\nval['pred'] = pred_y\nprint(val.head(10))","51a65558":"misclassified_examples = val[val.type != val.pred]\nmisclassified_examples.sample(10)","7e73521e":"eli5.show_prediction(clf, misclassified_examples['content'].values[1], vec=vec)","69680bb5":"## Feature Importance","a0341c86":"## Cleaning the data","3eefdf25":"## Checking Model Performance","00a7d2c5":"In this notebook, we will look at the code in a random script picked from Github, and predict its programming language. This data has been pulled from the public dataset, available via BigQuery, of open source [Github repos](https:\/\/www.kaggle.com\/github\/github-repos). I've extracted the relevant data in [this](https:\/\/www.kaggle.com\/priteshshrivastava\/language-classifier-clustering-data-prep) notebook using Kaggle's Bigquery helper functions.","9eff1b14":"## Most common mistakes made by our classifier","ee2bc645":"## Splitting data & training the model","bd9c35ed":"## Read the data","dec2a070":"Now, let's zoom in on 1 example and see which keywords within the script and most important in making a classification","74dbd419":"We can see that a JS has a lot of False Positives !","12aedb7a":"Let's see the most important keywords for every programming language !"}}