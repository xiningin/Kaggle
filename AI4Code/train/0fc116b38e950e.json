{"cell_type":{"b5d5e53e":"code","0da4dbef":"code","f39d13ce":"code","a1ae54c4":"code","fc997230":"code","c69d05ad":"code","01e50924":"code","08be003b":"code","3cb5b9ca":"code","18ec264c":"code","6f178da7":"code","b38b52b7":"code","8c2d1ac3":"code","9c8307f6":"code","365cd601":"code","ce151098":"code","831d639d":"code","26c1eadc":"code","02ffb870":"code","f70f00c5":"code","57d92f04":"code","69762ee1":"code","4732b67b":"code","41a674fc":"code","b5f47f40":"code","fe419d38":"code","6650a142":"code","a8c0e343":"code","db97d4b4":"code","f37db65d":"code","7fa894c0":"code","0efafb50":"code","be609313":"code","c3f7b6b8":"code","0405357c":"code","572da6ed":"code","41ff77a6":"code","9022faa9":"code","612541dd":"code","f8a54a23":"code","fbb04a2f":"code","d8a78e7c":"code","6f3eae78":"code","0009abf6":"code","eab53ae8":"code","f18d8ad5":"code","384cbec3":"code","7159d758":"code","8f466e41":"code","7dcb0fa5":"code","1dd777fa":"code","0982db9e":"code","3a76e881":"code","8301f306":"code","a6e4fac7":"code","e697ccaa":"code","e8a78c68":"code","90e65f4f":"code","87b605c3":"code","c22a56cf":"code","c9375339":"code","073865d9":"code","deb28e2f":"markdown","272f019b":"markdown","97681c12":"markdown","00bd6649":"markdown","baf796c5":"markdown","f5f9357d":"markdown","87646ab8":"markdown","d3fbadfb":"markdown","93de30d9":"markdown","c3ad86d1":"markdown","14d5b4d5":"markdown","ae9c1b2e":"markdown","49cc17c1":"markdown","290c833a":"markdown","f803616c":"markdown","f71539c9":"markdown","496aedc9":"markdown","e7093b4b":"markdown","3df5cdae":"markdown","6b5047cf":"markdown","6af66994":"markdown","80d398d6":"markdown","d3258e98":"markdown","cc4cf137":"markdown","e590e81a":"markdown","2096d95f":"markdown","0e8ccedf":"markdown","991f2d01":"markdown","12d0d0e3":"markdown","ab403128":"markdown","46218c41":"markdown","ff74eeb6":"markdown","63dbe1c8":"markdown","439ce2f4":"markdown","af0884c1":"markdown","769d3799":"markdown","828274ec":"markdown","df92180a":"markdown","e2e52717":"markdown","a3dc5d8b":"markdown","1471b471":"markdown","9e7c7451":"markdown","01c7bea7":"markdown","52e85c8f":"markdown","d3ddb902":"markdown","6d0237ab":"markdown","ca3bee12":"markdown","f13792e2":"markdown","46471fdb":"markdown","e8761731":"markdown","3480f025":"markdown","59f3e4c9":"markdown","c8dd04fc":"markdown","de9801bc":"markdown","b636d7ee":"markdown","18abc85a":"markdown","2b7f793a":"markdown","087c1f39":"markdown"},"source":{"b5d5e53e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import mean_squared_error, classification_report\nfrom sklearn.preprocessing import KBinsDiscretizer, StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\n\nfrom eli5.sklearn import PermutationImportance\nfrom eli5 import show_weights\n\nimport warnings\n\nfrom IPython.display import display, Math\n\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_style('whitegrid')\nsns.set_context(\"talk\")\n\n%matplotlib inline","0da4dbef":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","f39d13ce":"all_df = pd.concat([train_df, test_df], axis=0, sort=False, ignore_index=True)","a1ae54c4":"all_df.head()","fc997230":"all_df.info()","c69d05ad":"ans = all_df.drop(\"Survived\", axis=1).isnull().sum().sort_values(ascending=False)\nplt.figure(figsize=(12,1.5))\nsns.heatmap(pd.DataFrame(data=ans[ans>0], columns=['Missing Values']).T, annot=True, cbar=False, cmap='viridis')","01e50924":"all_df['Title'] = all_df['Name'].apply(lambda name: name.split(',')[1].strip().split('.')[0])","08be003b":"newtitles={\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"}","3cb5b9ca":"all_df['Title'].update(all_df['Title'].map(newtitles))","18ec264c":"all_df['Deck'] = all_df['Cabin'].apply(lambda cabin: cabin[0] if pd.notnull(cabin) else 'N')","6f178da7":"all_df['Alone'] = (all_df['Parch'].apply(lambda value: not(value)) & all_df['SibSp'].apply(lambda value: not(value))).astype('int')","b38b52b7":"all_df['Relatives'] = all_df['SibSp'] + all_df['Parch']","8c2d1ac3":"all_df[all_df['Embarked'].isnull()]","9c8307f6":"sort_embarked_features = ['Cabin', 'Sex', 'Pclass', 'Alone', 'Fare']\n\nsorted_df = all_df.sort_values(by=sort_embarked_features)\n\nsorted_df[(sorted_df['Survived'] == 1) & \n          (sorted_df['Sex'] == 'female') & \n          (sorted_df['Pclass'] == 1) &\n          (sorted_df['Alone'] == 1)].sort_values(by=sort_embarked_features)[sort_embarked_features+['Embarked']].head()","365cd601":"all_df['Embarked'].fillna(value='C', inplace=True)","ce151098":"all_df[all_df['Fare'].isnull()]","831d639d":"sort_fare_features = ['Sex', 'Pclass', 'Title', 'Deck', 'Embarked', 'Alone', 'Age']\n\nsorted_df = all_df.sort_values(by=sort_embarked_features)\n\naux = sorted_df[(sorted_df['Alone'] == 1) & \n          (sorted_df['Sex'] == 'male') & \n          (sorted_df['Pclass'] == 1) & \n          (sorted_df['Embarked'] == 'S') & \n          (sorted_df['Deck'] == 'N') & \n          (sorted_df['Title'] == 'Mr') & \n          ((sorted_df['Age'] >= 55) & (sorted_df['Age'] <= 65))]\n\naux","26c1eadc":"all_df['Fare'].fillna(value=aux['Fare'].mean(), inplace=True)","02ffb870":"def impute_num(cols, avg, std):\n       \n    try:\n        avg_value = avg.loc[tuple(cols)][0]\n    except Exception as e:        \n        print(f'It is not possible to find an average value for this combination of features values:\\n{cols}')\n        return np.nan\n    \n    try:\n        std_value = std.loc[tuple(cols)][0]\n    except Exception as e:        \n        std_value = 0        \n    finally:\n        if pd.isnull(std_value):\n            std_value = 0\n        \n    while True:        \n        value = np.random.randint(avg_value-std_value, avg_value+std_value+1)\n        if value >= 0:\n            break\n    return round(value, 0)","f70f00c5":"group_age_features = ['Title','Relatives','Parch','SibSp','Deck','Pclass','Embarked','Sex','Alone']","57d92f04":"stat_age = all_df.pivot_table(values='Age', index=group_age_features, aggfunc=['mean','std']).round(2)","69762ee1":"ages1 = all_df[~all_df['Age'].isnull()][group_age_features].apply(impute_num, axis=1, avg=stat_age.xs(key='mean', axis=1), std=0)\nages2 = all_df[~all_df['Age'].isnull()][group_age_features].apply(impute_num, axis=1, avg=stat_age.xs(key='mean', axis=1), std=stat_age.xs(key='std', axis=1))","4732b67b":"comp = pd.DataFrame([all_df[~all_df['Age'].isnull()]['Age'], ages2, ages1]).T\ncomp.columns = ['Real Age', 'Predicted Age $(\\mu=\\mu_{Age}$, $\\sigma=\\sigma_{Age})$', 'Predicted Age $(\\mu=\\mu_{Age}$, $\\sigma=0)$']\ncomp.head(10)","41a674fc":"comp.describe()","b5f47f40":"pd.DataFrame(data=[mean_squared_error(all_df[~all_df['Age'].isnull()]['Age'], ages1), \n                   mean_squared_error(all_df[~all_df['Age'].isnull()]['Age'], ages2)],\n            index = ['$\\mu=\\mu_{Age}, \\sigma=0$', '$\\mu=\\mu_{Age}, \\sigma=\\sigma_{Age}$'],\n            columns=['Mean Square Error']).round(1)","fe419d38":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nsns.distplot(all_df[~all_df['Age'].isnull()]['Age'], bins=20, ax=ax[0])\nsns.distplot(ages1, bins=20, ax=ax[1])\nsns.distplot(ages2, bins=20, ax=ax[2])\n\nax[0].set_xlabel('Age')\nax[1].set_xlabel('Age')\nax[2].set_xlabel('Age')\n\nax[0].set_title('Real Ages')\nax[1].set_title('Predicted Ages $(\\mu=\\mu_{Age}$, $\\sigma=0)$')\nax[2].set_title('Predicted Ages $(\\mu=\\mu_{Age}$, $\\sigma=\\sigma_{Age})$')","6650a142":"group_features = ['Title','Pclass','Embarked','Sex']","a8c0e343":"stat_age = all_df.pivot_table(values='Age', index=group_features, aggfunc=['mean','std']).round(2)","db97d4b4":"ages = all_df[all_df['Age'].isnull()][group_features].apply(impute_num, axis=1, avg=stat_age.xs(key='mean', axis=1), std=stat_age.xs(key='std', axis=1))","f37db65d":"all_df['Age'].update(ages)","7fa894c0":"fig, axes = plt.subplots(nrows=3, ncols=3, sharey=True, figsize=(15,10))\nfields = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Deck', 'Alone', 'Relatives']\n\ni = 0\nfor row_ax in axes:\n    for col_ax in row_ax:\n        ans = pd.crosstab(index=[all_df['Survived'], all_df[fields[i]]], columns=all_df[fields[i]], normalize='columns').reset_index()\n        ans['%'] = 100*ans[ans.drop(['Survived', fields[i]], axis=1).columns].apply(sum, axis=1).round(2)\n        ans.drop(labels=ans.drop(['Survived', fields[i], '%'], axis=1).columns, inplace=True, axis=1)\n        ans.sort_values(by=['Survived', '%'], inplace=True)        \n        sns.barplot(x=fields[i], y='%', hue='Survived', data=ans, ax=col_ax)\n        col_ax.set_ylim((0, 100))\n        i+=1\n        \nplt.tight_layout()        ","0efafb50":"def sort_remap(crosstab_df, key):\n    alives = list(crosstab_df[crosstab_df['Survived']==1][key])\n    deads = list(crosstab_df[crosstab_df['Survived']==0][key])\n\n    alives = list(set(deads) - set(alives)) + alives\n    sorted_map = {key:value for value, key in enumerate(alives)}\n    \n    return sorted_map","be609313":"fields = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Deck', 'Alone', 'Relatives']\n\nfor feature in fields:\n    ans = pd.crosstab(index=[all_df['Survived'], all_df[feature]], columns=all_df[feature], normalize='columns').reset_index()\n    ans['%'] = 100*ans[ans.drop(['Survived', feature], axis=1).columns].apply(sum, axis=1).round(2)\n    ans.drop(labels=ans.drop(['Survived', feature, '%'], axis=1).columns, inplace=True, axis=1)\n    ans.sort_values(by=['Survived', '%'], inplace=True)\n    sorted_map = sort_remap(ans, feature)    \n    all_df[feature].update(all_df[feature].map(sorted_map))","c3f7b6b8":"fig, axes = plt.subplots(nrows=3, ncols=3, sharey=True, figsize=(15,10))\nfields = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Deck', 'Alone', 'Relatives']\n\ni = 0\nfor row_ax in axes:\n    for col_ax in row_ax:\n        ans = pd.crosstab(index=[all_df['Survived'], all_df[fields[i]]], columns=all_df[fields[i]], normalize='columns').reset_index()\n        ans['%'] = 100*ans[ans.drop(['Survived', fields[i]], axis=1).columns].apply(sum, axis=1).round(2)\n        ans.drop(labels=ans.drop(['Survived', fields[i], '%'], axis=1).columns, inplace=True, axis=1)\n        ans.sort_values(by=['Survived', '%'], inplace=True)        \n        sns.barplot(x=fields[i], y='%', hue='Survived', data=ans, ax=col_ax)\n        col_ax.set_ylim((0, 100))\n        i+=1\n        \nplt.tight_layout()        ","0405357c":"kdis = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='uniform').fit(all_df[['Age', 'Fare']])","572da6ed":"cat_age_fare = kdis.transform(all_df[['Age', 'Fare']])","41ff77a6":"all_df['Cat_Age'] = cat_age_fare[:,0].astype('int')\nall_df['Cat_Fare'] = cat_age_fare[:,1].astype('int')","9022faa9":"fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12,5))\nfields = ['Cat_Age', 'Cat_Fare']\n\ni = 0\nfor col_ax in axes:\n    ans = pd.crosstab(index=[all_df['Survived'], all_df[fields[i]]], columns=all_df[fields[i]], normalize='columns').reset_index()\n    ans['%'] = 100*ans[ans.drop(['Survived', fields[i]], axis=1).columns].apply(sum, axis=1).round(2)\n    ans.drop(labels=ans.drop(['Survived', fields[i], '%'], axis=1).columns, inplace=True, axis=1)\n    ans.sort_values(by=['Survived', '%'], inplace=True)        \n    sns.barplot(x=fields[i], y='%', hue='Survived', data=ans, ax=col_ax)\n    col_ax.set_ylim((0, 100))\n    i+=1\n        \nplt.tight_layout()        ","612541dd":"fields = ['Cat_Age', 'Cat_Fare']\n\nfor feature in fields:\n    ans = pd.crosstab(index=[all_df['Survived'], all_df[feature]], columns=all_df[feature], normalize='columns').reset_index()\n    ans['%'] = 100*ans[ans.drop(['Survived', feature], axis=1).columns].apply(sum, axis=1).round(2)\n    ans.drop(labels=ans.drop(['Survived', feature, '%'], axis=1).columns, inplace=True, axis=1)\n    ans.sort_values(by=['Survived', '%'], inplace=True)\n    sorted_map = sort_remap(ans, feature)    \n    all_df[feature].update(all_df[feature].map(sorted_map))","f8a54a23":"fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12,5))\nfields = ['Cat_Age', 'Cat_Fare']\n\ni = 0\nfor col_ax in axes:\n    ans = pd.crosstab(index=[all_df['Survived'], all_df[fields[i]]], columns=all_df[fields[i]], normalize='columns').reset_index()\n    ans['%'] = 100*ans[ans.drop(['Survived', fields[i]], axis=1).columns].apply(sum, axis=1).round(2)\n    ans.drop(labels=ans.drop(['Survived', fields[i], '%'], axis=1).columns, inplace=True, axis=1)\n    ans.sort_values(by=['Survived', '%'], inplace=True)        \n    sns.barplot(x=fields[i], y='%', hue='Survived', data=ans, ax=col_ax)\n    col_ax.set_ylim((0, 100))\n    i+=1\n        \nplt.tight_layout()        ","fbb04a2f":"features_on_off = {'PassengerId':False,\n                   'Survived':False,\n                   'Pclass':True,\n                   'Name':False,\n                   'Sex':True,\n                   'Age':False,\n                   'SibSp':True,                   \n                   'Parch':True,\n                   'Ticket':False,\n                   'Fare':False,\n                   'Cabin':False,\n                   'Embarked':True,\n                   'Title':True,\n                   'Deck':True,                   \n                   'Alone':True,\n                   'Relatives':True,\n                   'Cat_Age':True,\n                   'Cat_Fare':True}","d8a78e7c":"features_on = [key for key, status in features_on_off.items() if status]\naux = pd.DataFrame(features_on_off, index=['On\\Off'])\n\nplt.figure(figsize=(15,1.5))\nsns.heatmap(aux, cbar=False, cmap=['red', 'green'], annot=True)","6f3eae78":"poly = PolynomialFeatures(degree=2, include_bias=False).fit(all_df[features_on])\npoly_features = poly.transform(all_df[features_on])\npoly_df = pd.DataFrame(data=poly_features, columns=poly.get_feature_names(features_on))","0009abf6":"std_scaler = StandardScaler().fit(poly_df)\nscaled_features = std_scaler.transform(poly_df)\nscaled_df = pd.DataFrame(data=scaled_features, columns=poly_df.columns)","eab53ae8":"new_df = scaled_df.copy()\nnew_df['Survived'] = all_df['Survived']","f18d8ad5":"plt.figure(figsize=(12,8))\nsns.heatmap(new_df.corr()[['Survived']].sort_values('Survived', ascending=False).drop('Survived').head(15), annot=True, cbar=False, cmap='viridis')","384cbec3":"X_train = scaled_df.loc[range(train_df.shape[0])]\ny_train = all_df.loc[range(train_df.shape[0]), 'Survived']\n\nX_test = scaled_df.loc[range(train_df.shape[0], train_df.shape[0]+test_df.shape[0])]\ny_test = pd.read_csv('..\/input\/true-labels\/submission_true.csv')['Survived']","7159d758":"log_reg = LogisticRegressionCV(Cs=1000, cv=5, refit=True, random_state=101, max_iter=200).fit(X_train, y_train)","8f466e41":"print(f'Accuracy on the training set: {100*log_reg.score(X_train, y_train):.1f}%')","7dcb0fa5":"print(f'Accuracy on the test set: {100*log_reg.score(X_test, y_test):.1f}%')","1dd777fa":"p = np.linspace(0.01, 1, 50)\nerror_rate = {'train':[], 'test':[]}\n\nfor p_value in p:    \n    prob = log_reg.predict_proba(X_train)\n    y_pred = np.apply_along_axis(lambda pair: 1 if pair[1] > p_value else 0, 1, prob)\n    error_rate['train'].append(mean_squared_error(y_train, y_pred))\n    prob = log_reg.predict_proba(X_test)\n    y_pred = np.apply_along_axis(lambda pair: 1 if pair[1] > p_value else 0, 1, prob)\n    error_rate['test'].append(mean_squared_error(y_test, y_pred))","0982db9e":"best_p = p[np.array(error_rate['test']).argmin()]","3a76e881":"plt.figure(figsize=(12,5))\n\nmin_x, max_x = 0, 1\nmin_y, max_y = min(min(error_rate['test'], error_rate['train'])), max(max(error_rate['test'], error_rate['train']))\n\nplt.plot(p, error_rate['train'], label='Train', marker='o', color='red')\nplt.plot(p, error_rate['test'], label='Test', marker='o', color='blue')\n\nplt.ylabel('Mean Squared Error')\nplt.xlabel('Probability')\n\nplt.vlines(x=best_p, ymin=min_y-0.1, ymax=max_y+0.1, linestyle='--', label=f'Best $p={best_p:.2f}$')\n\nplt.ylim((min_y-0.1, max_y+0.1))\nplt.xlim(min_x-0.1, max_x+0.1)\n\nplt.legend(loc='best')","8301f306":"print(f'In this particular case, the best probability limit is around {best_p:.2}. This means that if the model provides a survival value greater than {best_p:.2}, we accept that the passenger will survive or die otherwise.')","a6e4fac7":"prob = log_reg.predict_proba(X_train)\ny_pred_train = np.apply_along_axis(lambda pair: 1 if pair[1] > best_p else 0, 1, prob)\nprint(f'Accuracy on the training set with the best choice of p: {100*np.mean(y_train == y_pred_train):.1f}%')","e697ccaa":"prob = log_reg.predict_proba(X_test)\ny_pred_test = np.apply_along_axis(lambda pair: 1 if pair[1] > best_p else 0, 1, prob)\nprint(f'Accuracy on the testing set with the best choice of p: {100*np.mean(y_pred_test == y_test):.1f}%')","e8a78c68":"print(classification_report(y_pred_train, y_train))","90e65f4f":"print(classification_report(y_pred_test, y_test))","87b605c3":"perm = PermutationImportance(log_reg).fit(X_train, y_train)\nshow_weights(perm, feature_names = list(X_train.columns))","c22a56cf":"weights = pd.DataFrame(data=log_reg.coef_[0], index=scaled_df.columns, columns=['Weights'])\nweights.loc['Intercept'] = log_reg.intercept_\nweights.sort_values('Weights', ascending=False, inplace=True)\nweights.head(15)","c9375339":"output = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred_test.astype('int')})","073865d9":"output.to_csv('submission.csv', index=False)","deb28e2f":"<a href=\"#summary\" id=\"features-engineering\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>5. Features Engineering<\/h2><\/a>\n\n<a href=\"#summary\" id=\"title-feature\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>1. Title<\/h2><\/a>","272f019b":"It looks like that the women on Cabin B35 have the same characteristics and come from Cherbourg. As I need to give a shot to fill the NaN values, I would guess that they come from Cherbourg too!","97681c12":"There are only two <a href='#missing-values'>missing values<\/a> for the **Embarked** feature. Let's see who are these passengers.","00bd6649":"For both approximations there is a high error. To choose one of them, I decided to plot the distribution (shown below). The figure to the left is the distribution of the real ages, the middle figure is the distribution of predicted ages using ($\\mu=\\mu_{AgeGroup}$, $\\sigma=0$) and the right figure is the predicted ages using ($\\mu=\\mu_{AgeGroup}$, $\\sigma=\\sigma_{AgeGroup}$). As you can see, the distribution of ($\\mu=\\mu_{AgeGroup}$, $\\sigma=\\sigma_{AgeGroup}$) gets much closer to the real ages! That's why I'll choose this approximation to fill the ages.","baf796c5":"As I did with Embarked missing values, I'll look for someone that has the same characteristics.","f5f9357d":"The heat map below shows the correlation between resources and survival. The linear correlations are weak, but this doesn't mean that these features are not good candidates for the classification model. Later we'll see how they behave together.","87646ab8":"<p id=\"alone-feature\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Alone<\/h2><\/a><\/p>","d3fbadfb":"It'll be a good idea to concatenate the train and test dataset to reduce the amount of code.","93de30d9":"The idea here is to find the average age for a group of passengers that have some characteristics in common. For example, let's say that we group the passengers using their title, gender and place of embarkation. For who has the title of Royalty, are female and embarked in Quenstown, we have an average age of 45 years old and standart deviation equal to 10 years old. For those who has the same characteristics and the age is missing, their age will be filled with some random number between\n$\\mu_{Age}-\\sigma_{Age} \\le Age \\le \\mu_{Age}+\\sigma_{Age}$.","c3ad86d1":"To test this technique I used the passengers that we already know the ages to predicted their own ages. The first one using $(\\mu=\\mu_{AgeGroup}$, $\\sigma=\\sigma_{AgeGroup})$ and second one using $(\\mu=\\mu_{AgeGroup}$, $\\sigma=0)$. The predicted ages and statiscal informations are shown in the tables below.","14d5b4d5":"<a href=\"#summary\" id=\"fare-impute\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Fare<\/h2><\/a>","ae9c1b2e":"<p id=\"impute-missing-values\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Impute Missing Values<\/h2><\/a><\/p>\n\n<a href=\"#summary\" id=\"embarked-impute\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>1. Embarked<\/h2><\/a>","49cc17c1":"The fourth feature is the number of parents, children, siblings and spouses that each passenger had on board.","290c833a":"<p id=\"output\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>10. Output<\/h2><\/a><\/p>","f803616c":"<p id=\"scaled-features\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Scaled Features<\/h2><\/a><\/p>","f71539c9":"<p id=\"deck-feature\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Deck<\/h2><\/a><\/p>","496aedc9":"We have some good and interesting information here! The two survived, belonged to the first class, were women, traveled alone in the same cabin and paid the same price for the ticket. They were not relatives, but they probably they knew each other! So we can assume they boarded the same place. To fill these NaN values, let's take a look on people that have these same characteristics.","e7093b4b":"In this step I'll split the dataset into training and testing, according to the original datasets provided by kaggle. Then I'll train the logistic classifier and see how well its perform in the test set.\n\n**I've got the true labels of the test set from some kaggle user, for learning and test purposes. So if you see my rank equal 1.0, that's not true, I've test this labels to make sure that they were correct.**","3df5cdae":"Now it's time to get some insights about the relationship between the features and the survival passengers. The following graphs show us the percentage of survived for each categorical feature.","6b5047cf":"Standardize the features is always a good practice to avoid scale effect, and it helps the minimization algorithm behind the classification model to achieve the minimum faster.","6af66994":"<p id=\"relatives-feature\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>4. Relatives<\/h2><\/a><\/p>","80d398d6":"<p id=\"corr-features\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>4. Features Correlation<\/h2><\/a><\/p>","d3258e98":"Let's check the top rows of the dataset","cc4cf137":"The third feature will tell if the person was travelling alone or not. To get it, the columns **Parch** and **SibSp** will be composed using logical operation.","e590e81a":"<p id=\"decision-boundary\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>6. Selecting the Decision Boundary<\/h2><\/a><\/p>","2096d95f":"The next lines will get the labels using the best probability limit the we found.","0e8ccedf":"When we use the prediction method provided by the logistic regression classifier, it usually returns rank based on the highest probability, but we can try to figure out the best probability boundary between alive or dead looking at the error in the test dataset for different probability limits. This is exactly what is shown by the dashed line in the chart below, which is the probability cut that corresponds to the smallest error in the test set.","991f2d01":"<p id=\"categorization\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Age and Fare Categorization<\/h2><\/a><\/p>","12d0d0e3":"<p id=\"report\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>7. Classification Reports<\/h2><\/a><\/p>","ab403128":"<a href=\"#summary\" id=\"age-impute\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Age<\/h2><\/a>","46218c41":"Most of the features are categorical, with the exception of age and fare that are a float point number. It'd be a good practice to discretizer these features. Let's do that using KBinsDiscretizer from sklearn.preprocessing with eight bins and uniform distributed.\n\nObs: I have done several tests using the model accuracy and eight bins are enough!","ff74eeb6":"The first feature can be derived from the **Name** of the passengers and it is kind of related with social status and sex.","63dbe1c8":"<p id=\"exploring\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Visualizing<\/h2><\/a><\/p>\n\n<p id=\"distributions\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>1. Distributions<\/h2><\/a><\/p>","439ce2f4":"<p id=\"model-coefficients\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>9. Model Coefficients<\/h2><\/a><\/p>","af0884c1":"<p id=\"rearrange\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Rearrange<\/h2><\/a><\/p>","769d3799":"As we can see above, all features seem to influence survival. You may notice that some features, such as SibSp, most people who survive have only one sibling or spouse on board with them. My idea here is to rearrange the categories of these features so that the chance of survival will increase or decrease as the number of categories increases or decreases.","828274ec":"<p id=\"missing-values\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>4. Missing Values<\/h2><\/a><\/p>","df92180a":"There are a few missing values in the Fare and Embarked features, and a considerable number of missing values in the Age and Cabin features. We need somehow to figure out a way to fill these missing values. Before impute those missing values, let's create a few features that can be useful to compute them.","e2e52717":"The table below shows the top 15 features coefficients that have a positive contribution to the model.","a3dc5d8b":"Now is the time to implement the classification model. The red and green figure below shows the features that will be used for the model. Feel free to play with it!","1471b471":"### Classification Report in the training set","9e7c7451":"<p id=\"preprocessing-exploring\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h1> 1. Preprocessing and exploring<\/h1><\/a><\/p>\n<p id=\"import-packages\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>1. Import Packages<\/h2><\/a><\/p>","01c7bea7":"Let's use a second degree polynomial to capture more complex relationships between the features.","52e85c8f":"<p id=\"load-dataset\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Load Dataset<\/h2><\/a><\/p>","d3ddb902":"<p id=\"introduction\"><h1>Logistic Classification Model for Titanic Survivors<\/h1><\/p>\n\n<p>Hello Kagglers!!!<\/p>\n<p>I would like to share with you my second kernel <a href=\"https:\/\/www.kaggle.com\/chavesfm\/tuning-parameters-for-k-nearest-neighbors-iris\">(if you haven't seen my first one, check out this link)<\/a>.\nIn this kernel, I'll start with a visual exploratory analysis of the data, followed by a proposal for a logistic classification model. I hope you enjoy this kernel, and if you like, please UPVOTE! If you have any suggestions or corrections, I would be happy and grateful to hear from you!\n<\/p>","6d0237ab":"I will reorganize the age and fare categories, as I did with the previous categories, so the chance of survival will increase or decrease as the number of categories grows or decreases.","ca3bee12":"### Classification Report in the testing set","f13792e2":"<p id=\"split-training-data\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>5. Train-Test-Split and Model Training<\/h2><\/a><\/p>","46471fdb":"There is only one <a href='#missing-values'>missing value<\/a> for the **Fare** feature. Let's see who is this person.","e8761731":"Great!!! We have three passengers that are very close to each other. I'll use the mean Fare to impute the missing value.","3480f025":"The second feature will be derived from the **Cabin** column. The Cabin column is usually composed by a letter (related with the deck) followed by a number (related with the number of the room). To construct this feature, we'll use only the first letter and, for null values, we'll fill with the letter N.","59f3e4c9":"Now we can check what are the most important features to our model. Let's use permutation importance to see the feature's rank.","c8dd04fc":"<p id=\"poly-features\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>2. Polynomial Features<\/h2><\/a><\/p>","de9801bc":"<p id=\"feature-importance\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>8. Feature Importance<\/h2><\/a><\/p>","b636d7ee":"<p id=\"model\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>4. Classification Model<\/h2><\/a><\/p>\n\n<p id=\"model-features\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>1. Model Features<\/h2><\/a><\/p>","18abc85a":"<p id=\"summary\"><h1>Summary<\/h1><\/p>\n\n<ol style=\"font-size:14px; line-height:1.0\">\n    <li><a href=\"#preprocessing-exploring\" style=\"padding: 14px 25px; text-decoration: none\">Preprocessing and exploring<\/a><\/li>\n        <ol>\n            <li><a href=\"#import-packages\" style=\"padding: 14px 25px; text-decoration: none\">Import Packages<\/a><\/li>\n            <li><a href=\"#load-dataset\" style=\"padding: 14px 25px; text-decoration: none\">Load Dataset<\/a><\/li>\n            <li><a href=\"#features-type\" style=\"padding: 14px 25px; text-decoration: none\">Features Type<\/a><\/li>\n            <li><a href=\"#missing-values\" style=\"padding: 14px 25px; text-decoration: none\">Missing Values<\/a><\/li>\n            <li><a href=\"#features-engineering\" style=\"padding: 14px 25px; text-decoration: none\">Features Engineering<\/a><\/li>\n                <ol>\n                    <li><a href=\"#title-feature\" style=\"padding: 14px 25px; text-decoration: none\">Title<\/a><\/li>\n                    <li><a href=\"#deck-feature\" style=\"padding: 14px 25px; text-decoration: none\">Deck<\/a><\/li>\n                    <li><a href=\"#alone-feature\" style=\"padding: 14px 25px; text-decoration: none\">Alone<\/a><\/li>\n                    <li><a href=\"#relatives-feature\" style=\"padding: 14px 25px; text-decoration: none\">Relatives<\/a><\/li>\n                <\/ol>\n        <\/ol>\n    <li><a href=\"#impute-missing-values\" style=\"padding: 14px 25px; text-decoration: none\">Impute Missing Values<\/a><\/li>\n        <ol>\n            <li><a href=\"#embarked-impute\" style=\"padding: 14px 25px; text-decoration: none\">Embarked<\/a><\/li>\n            <li><a href=\"#fare-impute\" style=\"padding: 14px 25px; text-decoration: none\">Fare<\/a><\/li>\n            <li><a href=\"#age-impute\" style=\"padding: 14px 25px; text-decoration: none\">Age<\/a><\/li>\n        <\/ol>\n    <li><a href=\"#exploring\" style=\"padding: 14px 25px; text-decoration: none\">Visualizing<\/a><\/li>\n        <ol>\n            <li><a href=\"#distributions\" style=\"padding: 14px 25px; text-decoration: none\">Distributions<\/a><\/li>\n            <li><a href=\"#rearrange\" style=\"padding: 14px 25px; text-decoration: none\">Rearrange Features<\/a><\/li>\n            <li><a href=\"#categorization\" style=\"padding: 14px 25px; text-decoration: none\">Age and Fare Categorization<\/a><\/li>\n        <\/ol>   \n    <li><a href=\"#model\" style=\"padding: 14px 25px; text-decoration: none\">Classification Model<\/a><\/li>\n        <ol>\n            <li><a href=\"#model-features\" style=\"padding: 14px 25px; text-decoration: none\">Model Features<\/a><\/li>\n            <li><a href=\"#poly-features\" style=\"padding: 14px 25px; text-decoration: none\">Polynomial Features<\/a><\/li>\n            <li><a href=\"#scaled-features\" style=\"padding: 14px 25px; text-decoration: none\">Scaled Features<\/a><\/li>\n            <li><a href=\"#corr-features\" style=\"padding: 14px 25px; text-decoration: none\">Features Correlation<\/a><\/li>\n            <li><a href=\"#split-training-data\" style=\"padding: 14px 25px; text-decoration: none\">Train-Test-Split and Model Training<\/a><\/li>\n            <li><a href=\"#decision-boundary\" style=\"padding: 14px 25px; text-decoration: none\">Selecting the Decision Boundary<\/a><\/li>\n            <li><a href=\"#report\" style=\"padding: 14px 25px; text-decoration: none\">Classification Reports<\/a><\/li>\n            <li><a href=\"#feature-importance\" style=\"padding: 14px 25px; text-decoration: none\">Feature Importance<\/a><\/li>\n            <li><a href=\"#model-coefficients\" style=\"padding: 14px 25px; text-decoration: none\">Model Coefficients<\/a><\/li>\n            <li><a href=\"#output\" style=\"padding: 14px 25px; text-decoration: none\">Output<\/a><\/li>\n        <\/ol>\n<\/ol>\n\n","2b7f793a":"There are over two hundred <a href='#missing-values'>missing values<\/a> for the **Age** feature! I'll use the same procedure to fill the NaN values used with the past features, but now I'll use the function below to help filling the values.","087c1f39":"<p id=\"features-type\"><a href=\"#summary\" style=\"padding: 14px 0px; text-decoration: none; color: black;\"><h2>3. Features Type<\/h2><\/a><\/p>"}}