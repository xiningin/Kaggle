{"cell_type":{"48e5adf9":"code","c27e0a71":"code","e4f991f4":"code","8bbb2721":"code","9c83c488":"code","ca8fed46":"code","8044bfea":"code","ba4b0d24":"code","9d31fe6f":"code","b7fbc9cb":"code","78a5c4ea":"code","9cbc871a":"code","075b22ad":"code","edc7b8f2":"code","f911a1b0":"code","bbb344b1":"code","4a62ba6b":"code","e18b169b":"code","d0a55e13":"code","6266e29a":"code","1d477d51":"code","bdfb4353":"code","d40fef29":"code","e3aa39d2":"code","9d046c79":"code","bc9625b6":"code","cfba4165":"code","dc17f094":"code","9c19595d":"code","78c12e78":"code","fec32601":"code","6fd16344":"code","d4335474":"code","4e811e11":"code","a759c997":"code","320cade0":"code","77eab8c2":"code","799c0d20":"code","34679bd2":"code","dd2e8f04":"code","4ec959ca":"code","5b316d5e":"code","b8f57417":"code","23abc6c4":"code","ba7ed2ed":"code","cc027d45":"code","35ddd0d0":"code","e30b7c09":"code","028b9073":"code","594e1868":"code","17b36b0f":"code","09f0677c":"code","77683f96":"code","44425ab1":"code","5c6dc9e1":"code","80d9729d":"code","79114018":"code","4f7fcf81":"code","8b07ed6f":"code","bab251c7":"code","65471581":"code","15a60f02":"code","6b46b5e1":"code","0a9e3bf5":"code","873d08c8":"code","183b13ea":"code","93984f73":"markdown","ccc4b4df":"markdown","dfe8702f":"markdown","11286d09":"markdown","3487ea22":"markdown","c1ca4641":"markdown","a49013fd":"markdown","743028e9":"markdown","c24f5753":"markdown","630a692c":"markdown","673143ae":"markdown","e8b8c194":"markdown","656fd288":"markdown","d4e6d0e7":"markdown","06928f88":"markdown","b31b9997":"markdown","42443d47":"markdown","c9f974fc":"markdown","ffab413c":"markdown","5015a135":"markdown","202e29e2":"markdown","df1fa28b":"markdown","fa5ba964":"markdown","d07bc0e5":"markdown","3f7da07d":"markdown","55ab6d20":"markdown","00d4ff14":"markdown","b8627605":"markdown","fb61f595":"markdown","bd7175a9":"markdown","cd8de8ab":"markdown","7e043893":"markdown","a29ad334":"markdown","482257dc":"markdown","d31329c9":"markdown","10a3f552":"markdown","338ad861":"markdown","2a75da93":"markdown","645f69b4":"markdown","713961f7":"markdown","503cf47e":"markdown","1ed431fb":"markdown","31749e14":"markdown","68bef2fb":"markdown","70d1f370":"markdown"},"source":{"48e5adf9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\npd.set_option('display.max_rows', 500)\n#df = pd.read_csv(\"application_data.csv\")\ndf = pd.read_csv(\"\/kaggle\/input\/application_data.csv\")\n#Sanity checks on the data\ndf.head()\ndf.info()\ndf.shape\n#df.dtypes","c27e0a71":"# sum it up to check how many rows have all missing values\ndf.isnull().all(axis=1).sum()","e4f991f4":"# % of the missing values (column-wise)\ncol_missing_perc = round(100*(df.isnull().sum()\/len(df.index)), 2) ","8bbb2721":"#getting cols with more than 20% missing and dropping them\ncol_missing_perc_greater_20 = []\nfor i in range(0,len(col_missing_perc)):\n    if col_missing_perc[i]>20:\n        col_missing_perc_greater_20.append(col_missing_perc.index[i])\n    \n#dropping cols with more than 20% missing\ndf.drop(col_missing_perc_greater_20, axis = 1,inplace=True)","9c83c488":"#remaining columns\ndf.shape","ca8fed46":"#subsetting the data\ndf=df[['SK_ID_CURR',\n'TARGET',\n'NAME_CONTRACT_TYPE',\n'CODE_GENDER',\n'CNT_CHILDREN',\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'NAME_INCOME_TYPE',\n'NAME_EDUCATION_TYPE',\n'NAME_FAMILY_STATUS',\n'NAME_HOUSING_TYPE',\n'DAYS_BIRTH',\n'DAYS_EMPLOYED',\n'CNT_FAM_MEMBERS',\n'ORGANIZATION_TYPE',\n'AMT_REQ_CREDIT_BUREAU_HOUR',\n'AMT_REQ_CREDIT_BUREAU_DAY',\n'AMT_REQ_CREDIT_BUREAU_WEEK',\n'AMT_REQ_CREDIT_BUREAU_MON',\n'AMT_REQ_CREDIT_BUREAU_QRT',\n'AMT_REQ_CREDIT_BUREAU_YEAR',\n'EXT_SOURCE_2'\n]]\n##final list of columns for analysis\ndf.columns","8044bfea":"#checking missing % in remaining columns\nround(100*(df.isnull().sum()\/len(df.index)), 2)","ba4b0d24":"\n#1.Handling missing values -  Categorical\n\ndf['ORGANIZATION_TYPE']=np.where(df['ORGANIZATION_TYPE'].isnull(),df['ORGANIZATION_TYPE'].mode(),df['ORGANIZATION_TYPE']) \ndf['CODE_GENDER']=np.where(df['CODE_GENDER']=='XNA',df['CODE_GENDER'].mode(),df['CODE_GENDER'])\n\n\n\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_HOUR']), ['AMT_REQ_CREDIT_BUREAU_HOUR']] = df['AMT_REQ_CREDIT_BUREAU_HOUR'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_DAY']),['AMT_REQ_CREDIT_BUREAU_DAY']]=df['AMT_REQ_CREDIT_BUREAU_DAY'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_WEEK']),['AMT_REQ_CREDIT_BUREAU_WEEK']]=df['AMT_REQ_CREDIT_BUREAU_WEEK'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_MON']),['AMT_REQ_CREDIT_BUREAU_MON']]=df['AMT_REQ_CREDIT_BUREAU_MON'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_QRT']),['AMT_REQ_CREDIT_BUREAU_QRT']]=df['AMT_REQ_CREDIT_BUREAU_QRT'].median()\ndf.loc[np.isnan(df['AMT_REQ_CREDIT_BUREAU_YEAR']),['AMT_REQ_CREDIT_BUREAU_YEAR']]=df['AMT_REQ_CREDIT_BUREAU_YEAR'].median()\n\n#1.Handling missing values -  Numerical\ndf.loc[np.isnan(df['CNT_FAM_MEMBERS']),['CNT_FAM_MEMBERS']]=df['CNT_FAM_MEMBERS'].median()\ndf.loc[np.isnan(df['AMT_ANNUITY']),['AMT_ANNUITY']]=round(df['AMT_ANNUITY'].median(),1)","9d31fe6f":"#checking missing % in remaining columns\nround(100*(df.isnull().sum()\/len(df.index)), 2)","b7fbc9cb":"df = df.dropna(axis=0, subset=['EXT_SOURCE_2'])","78a5c4ea":"round(100*(df.isnull().sum()\/len(df.index)), 2)","9cbc871a":"df.shape","075b22ad":"# Identifying and treating Outliers on columns - AMT_ANNUITY,AMT_GOODS_PRICE,AMT_CREDIT\n\n\ndf_outliers=df[['AMT_ANNUITY','AMT_GOODS_PRICE','AMT_CREDIT']]\n#df_outliers.shape (306574, 3)--before outlier removal\nQ1=df_outliers.quantile(0.25)\nQ3=df_outliers.quantile(0.75)\n\nIQR=Q3-Q1\nprint(IQR)\n\n#in case you decide to remove outliers, follow the below command.\ndf_out_final=df_outliers[~((df_outliers < (Q1-1.5*IQR)) | (df_outliers > ((Q3 + 1.5*IQR)))).any(axis=1)]\n\n\n# The mean value will be used further to impute missing value in the respective columns\ndf_out_final['AMT_GOODS_PRICE'].mean() \n","edc7b8f2":"len(df_out_final.index)\/len(df.index)","f911a1b0":"#imputing missing value of AMT_GOODS_PRICE with mean of data after removing the outlier\ndf.loc[np.isnan(df['AMT_GOODS_PRICE']),['AMT_GOODS_PRICE']]=round(df_out_final['AMT_GOODS_PRICE'].mean(),1)","bbb344b1":"## verification of the fixes\nround(100*(df.isnull().sum()\/len(df.index)), 2)","4a62ba6b":"#changing datatype of the columns\ndt_dict={'AMT_REQ_CREDIT_BUREAU_HOUR':int,\n        'CNT_FAM_MEMBERS':int,\n        'AMT_REQ_CREDIT_BUREAU_WEEK':int,\n        'AMT_REQ_CREDIT_BUREAU_MON':int,\n        'AMT_REQ_CREDIT_BUREAU_DAY':int,\n        'AMT_REQ_CREDIT_BUREAU_QRT':int,\n        'AMT_REQ_CREDIT_BUREAU_YEAR':int}\n\ndf=df.astype(dt_dict)\n\n\n# checking the datatypes\ndf.info()\n   ","e18b169b":"#removing unnecessary spaces in column names\ndf.columns=[df.columns[i].strip() for i in range(len(df.columns))]\n\n#renaming columns\ndf.rename(columns={\"EXT_SOURCE_2\": \"CREDIT_RATINGS\"},inplace=True)","d0a55e13":"#Categorising customers into following\n#Youth (<18)\n#Young Adult (18 to 35)\n#Adult (36 to 55)\n#Senior (56 and up)\n\ndf['AGE'] = abs(df['DAYS_BIRTH'])\ndf['AGE'] = round(df['AGE']\/365,1)\ndf['AGE']\n\ndf['AGE'].describe()\ndef age_group(y):\n    if y>=56:\n        return \"Senior\"\n    elif y>=36 and y<56:\n        return \"Adult\"\n    elif y>=18 and y<36:\n        return \"Young Adult\"\n    else:\n        return \"Youth\"\n    \ndf['AGE_GROUP'] = df['AGE'].apply(lambda x: age_group(x))\n\nsns.countplot(x='AGE_GROUP',hue='TARGET',data=df)","6266e29a":"df['CREDIT_RATINGS'].describe()","1d477d51":"sns.boxplot(y=df['CREDIT_RATINGS'])","bdfb4353":"credit_category_quantile = list(df['CREDIT_RATINGS'].quantile([0.20,0.5,0.80,1]))\ncredit_category_quantile","d40fef29":"def credit_group(x):\n    if x>=credit_category_quantile[2]:\n        return \"C1\"\n    elif x>=credit_category_quantile[1]:\n        return \"C2\"\n    elif x>=credit_category_quantile[0]:\n        return \"C3\"\n    else:\n        return \"C4\"\ndf[\"CREDIT_CATEGORY\"] = df['CREDIT_RATINGS'].apply(lambda x: credit_group(x))\n\nsns.countplot(x='CREDIT_CATEGORY',hue='TARGET',data=df)","e3aa39d2":"df['TARGET'].value_counts(normalize=True)","9d046c79":"#checking for unique values per column to see what all columns can be categorised\ndf.nunique().sort_values()","bc9625b6":"df0 = df[df['TARGET']==0]\ndf1 = df[df['TARGET']==1]\n","cfba4165":"# What are average values of numerical features\ndf.pivot_table(columns = 'TARGET', aggfunc = 'median')","dc17f094":"df['NAME_INCOME_TYPE'].unique()","9c19595d":"plt.figure(figsize=(20,9))\nsns.countplot(x='NAME_INCOME_TYPE',hue='TARGET',data=df)\n","78c12e78":"incomeCategories0 = pd.DataFrame(df0['NAME_INCOME_TYPE'].value_counts().rename(\"Count_0\").reset_index())\nincomeCategories0_perct = pd.DataFrame(df0['NAME_INCOME_TYPE'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\nincomeCategories0.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories0_perct.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\nincomeCategories0 = pd.merge(incomeCategories0,incomeCategories0_perct,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories0\n\nincomeCategories1 = pd.DataFrame(df1['NAME_INCOME_TYPE'].value_counts().rename(\"Count_1\").reset_index())\nincomeCategories1_perct = pd.DataFrame(df1['NAME_INCOME_TYPE'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\nincomeCategories1.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories1_perct.rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\nincomeCategories1 = pd.merge(incomeCategories1,incomeCategories1_perct,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\nincomeCategories1\n\nincomeCategories = pd.merge(incomeCategories0,incomeCategories1,how=\"inner\").rename(columns={\"index\":\"NAME_INCOME_TYPE\"})\n\ndef income_percentage_contri_0(count_0, count_1):\n    return 100*(count_0\/(count_0+count_1))\n\ndef income_percentage_contri_1(count_0, count_1):\n    return 100*(count_1\/(count_0+count_1))\n\nincomeCategories['percentage_contri_0'] = incomeCategories[['Count_0','Count_1']].apply(lambda x: income_percentage_contri_0(*x), axis=1)\nincomeCategories['percentage_contri_1'] = incomeCategories[['Count_0','Count_1']].apply(lambda x: income_percentage_contri_1(*x), axis=1)\nincomeCategories.set_index(\"NAME_INCOME_TYPE\",inplace=True)\nincomeCategories","fec32601":"fig = plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.title(\"Target = 1\")\nplt.ylabel('Percentage contribution to \"defaulters\"')\nplt.plot(incomeCategories['percentage_contri_1'])\n#ax1.set_xticklabels(labels = ax1.get_xticklabels(),rotation=30)\n","6fd16344":"plt.rcParams.update(plt.rcParamsDefault)\nincomeCategories = incomeCategories.sort_values(by='percentage_contri_1')\nincomeCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","d4335474":"df.CODE_GENDER.unique()","4e811e11":"sns.countplot(x='CODE_GENDER',hue='TARGET',data=df)","a759c997":"genderCategories0 = pd.DataFrame(df0['CODE_GENDER'].value_counts().rename(\"Count_0\").reset_index())\ngenderCategories0_perct = pd.DataFrame(df0['CODE_GENDER'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\ngenderCategories0.rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories0_perct.rename(columns={\"index\":\"CODE_GENDER\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ngenderCategories0 = pd.merge(genderCategories0,genderCategories0_perct,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories0\n\ngenderCategories1 = pd.DataFrame(df1['CODE_GENDER'].value_counts().rename(\"Count_1\").reset_index())\ngenderCategories1_perct = pd.DataFrame(df1['CODE_GENDER'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\ngenderCategories1.rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories1_perct.rename(columns={\"index\":\"CODE_GENDER\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ngenderCategories1 = pd.merge(genderCategories1,genderCategories1_perct,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\ngenderCategories1\n\ngenderCategories = pd.merge(genderCategories0,genderCategories1,how=\"inner\").rename(columns={\"index\":\"CODE_GENDER\"})\n\ndef gender_percentage_contri_0(count_0, count_1):\n    return 100*(count_0\/(count_0+count_1))\n\ndef gender_percentage_contri_1(count_0, count_1):\n    return 100*(count_1\/(count_0+count_1))\n\ngenderCategories['percentage_contri_0'] = genderCategories[['Count_0','Count_1']].apply(lambda x: gender_percentage_contri_0(*x), axis=1)\ngenderCategories['percentage_contri_1'] = genderCategories[['Count_0','Count_1']].apply(lambda x: gender_percentage_contri_1(*x), axis=1)\ngenderCategories.set_index(\"CODE_GENDER\",inplace=True)\ngenderCategories","320cade0":"plt.rcParams.update(plt.rcParamsDefault)\ngenderCategories = genderCategories.sort_values(by='percentage_contri_1')\ngenderCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","77eab8c2":"df1['CREDIT_RATINGS'].describe()","799c0d20":"df0['CREDIT_RATINGS'].describe()","34679bd2":"sns.barplot(x=\"TARGET\",y=\"CREDIT_RATINGS\",data=df)","dd2e8f04":"sns.boxplot(x=\"TARGET\", y=\"CREDIT_RATINGS\", data=df,palette='rainbow')","4ec959ca":"target =[0,1]\nfor i in target:\n    subset = df[df['TARGET'] == i]\n    sns.distplot(subset['CREDIT_RATINGS'],hist=False,kde=True,kde_kws ={'shade':True},label=i)","5b316d5e":"plt.figure(figsize=(20,9))\nplt.subplot(1,2,1)\ndf0['CREDIT_RATINGS'].hist(bins = 50)\nplt.subplot(1,2,2)\ndf1['CREDIT_RATINGS'].hist(bins = 50)","b8f57417":"df['CREDIT_CATEGORY'].value_counts()","23abc6c4":"creditCategories0 = pd.DataFrame(df0['CREDIT_CATEGORY'].value_counts().rename(\"Count_0\").reset_index())\ncreditCategories0_perct = pd.DataFrame(df0['CREDIT_CATEGORY'].value_counts(normalize=True).rename(\"Perct_0\").reset_index())\ncreditCategories0.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories0_perct.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ncreditCategories0 = pd.merge(creditCategories0,creditCategories0_perct,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories0\n\ncreditCategories1 = pd.DataFrame(df1['CREDIT_CATEGORY'].value_counts().rename(\"Count_1\").reset_index())\ncreditCategories1_perct = pd.DataFrame(df1['CREDIT_CATEGORY'].value_counts(normalize=True).rename(\"Perct_1\").reset_index())\ncreditCategories1.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories1_perct.rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\n#Merging data to get the overall view of the variable \"NAME_INCOME_TYPE\"\ncreditCategories1 = pd.merge(creditCategories1,creditCategories1_perct,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\ncreditCategories1\n\ncreditCategories = pd.merge(creditCategories0,creditCategories1,how=\"inner\").rename(columns={\"index\":\"CREDIT_CATEGORY\"})\n\ndef credit_percentage_contri_0(count_0, count_1):\n    return 100*(count_0\/(count_0+count_1))\n\ndef credit_percentage_contri_1(count_0, count_1):\n    return 100*(count_1\/(count_0+count_1))\n\ncreditCategories['percentage_contri_0'] = creditCategories[['Count_0','Count_1']].apply(lambda x: credit_percentage_contri_0(*x), axis=1)\ncreditCategories['percentage_contri_1'] = creditCategories[['Count_0','Count_1']].apply(lambda x: credit_percentage_contri_1(*x), axis=1)\ncreditCategories.set_index(\"CREDIT_CATEGORY\",inplace=True)\ncreditCategories","ba7ed2ed":"plt.plot(creditCategories['percentage_contri_1'].sort_values())","cc027d45":"creditCategories[['percentage_contri_1', 'percentage_contri_0']].plot(kind='bar', stacked=True)","35ddd0d0":"pt = df.pivot_table(columns='NAME_INCOME_TYPE',index='CREDIT_CATEGORY',values='TARGET',aggfunc='sum',fill_value = 0)\n#pt.reset_index()\n\npt","e30b7c09":"pt['Row_Total'] = pt['Businessman'] + pt['Commercial associate'] + pt['Maternity leave'] + pt['Pensioner']+pt['State servant'] +pt['Student']+pt['Unemployed']+pt['Working']","028b9073":"Column_Total = []\nfor c in pt.columns:\n    Column_Total.append(pt[c].sum())\nColumn_Total\npt.loc['Column_Total'] = Column_Total\npt","594e1868":"for i in pt.index:\n    pt.loc[i,'Total%'] = 100*(pt.loc[i,'Row_Total']\/pt.loc['Column_Total','Row_Total'])\n\nfor j in df.NAME_INCOME_TYPE.unique():\n    for i in pt.index:\n        pt.loc[i,j+'%'] = 100*(pt.loc[i,j]\/pt.loc['Column_Total',j])\npt","17b36b0f":"credit_income_type = pt.iloc[0:-1][['Working%','State servant%','Commercial associate%','Pensioner%','Unemployed%']]\ncredit_income_type\ncredit_income_type.T.plot.bar(stacked = 'TRUE')","09f0677c":"df1_corr=df[df['TARGET']==1]\ndf0_corr=df[df['TARGET']==0]\n\ndf1_corr=df1_corr[[\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'AGE',\n'DAYS_EMPLOYED']]\n\ndf0_corr=df0_corr[[\n'AMT_INCOME_TOTAL',\n'AMT_CREDIT',\n'AMT_ANNUITY',\n'AMT_GOODS_PRICE',\n'AGE',\n'DAYS_EMPLOYED']]\n\ndf1_corr_matrix=df1_corr.corr()\ndf0_corr_matrix=df1_corr.corr()\ndf1_corr_matrix","77683f96":"\n#narrowing down the data and considering less than the upper quantile AMT_INCOME_TOTAL\ndf1_corr['AMT_INCOME_TOTAL'] = df1_corr[df1_corr['AMT_INCOME_TOTAL']<df1_corr['AMT_INCOME_TOTAL'].quantile(.85)]['AMT_INCOME_TOTAL']\n#df1_corr['AMT_ANNUITY'] = df1_corr[df1_corr['AMT_GOODS_PRICE']<df1_corr['AMT_GOODS_PRICE'].quantile(.85)]['AMT_GOODS_PRICE']\n\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.scatterplot(x='AMT_INCOME_TOTAL', y='AMT_ANNUITY',data=df1_corr)","44425ab1":"df1_corr.plot.hexbin(x='AMT_INCOME_TOTAL', y='AMT_ANNUITY', gridsize=30)","5c6dc9e1":"#narrowing down the data and considering less than the upper quantile AMT_INCOME_TOTAL\ndf1_corr['AMT_CREDIT'] = df1_corr[df1_corr['AMT_CREDIT']<df1_corr['AMT_CREDIT'].quantile(.85)]['AMT_CREDIT']\n#df1_corr['AMT_ANNUITY'] = df1_corr[df1_corr['AMT_GOODS_PRICE']<df1_corr['AMT_GOODS_PRICE'].quantile(.85)]['AMT_GOODS_PRICE']","80d9729d":"df1_corr.plot.hexbin(x='AGE', y='AMT_CREDIT', gridsize=15)","79114018":"sns.boxplot(x=\"AGE_GROUP\", y=\"AMT_CREDIT\", data=df1,palette='rainbow')","4f7fcf81":"df1_corrdf = df1_corr_matrix.where(np.triu(np.ones(df1_corr_matrix.shape),k=1).astype(np.bool))\ndf0_corrdf = df0_corr_matrix.where(np.triu(np.ones(df0_corr_matrix.shape),k=1).astype(np.bool))","8b07ed6f":"df1_corrdf = df1_corrdf.unstack().reset_index()\ndf0_corrdf = df0_corrdf.unstack().reset_index()","bab251c7":"df1_corrdf.columns =['var1','var2','correlation']\ndf0_corrdf.columns=['var1','var2','correlation']","65471581":"df1_corrdf.dropna(subset=['correlation'],inplace=True)\ndf0_corrdf.dropna(subset=['correlation'],inplace=True)","15a60f02":"df1_corrdf.sort_values(by=['correlation'],ascending=False)\n#df0_corrdf.sort_values(by=['correlation'],ascending=False)","6b46b5e1":"sns.heatmap(df1_corr_matrix,annot=True,linewidth=1,annot_kws={\"size\":10},cbar=False)","0a9e3bf5":"#removing outlier for AMT_INCOME_TOTAL\ndf1_filtered = df1[df1['AMT_INCOME_TOTAL']<df['AMT_INCOME_TOTAL'].quantile(.90)]\ndf_stats_credit = df1_filtered.groupby('NAME_INCOME_TYPE').mean()[['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_GOODS_PRICE']]\n#df_stats_credit = df1_filtered.groupby('AGE_GROUP').mean()[['CREDIT_RATINGS']]\ndf_stats_credit.sort_values(by='AMT_CREDIT',ascending=False)","873d08c8":"df_stats_credit.plot.line(x_compat=True)","183b13ea":"plt.figure(figsize=(10,5))\ndf_filtered = df[df['AMT_INCOME_TOTAL']<df['AMT_INCOME_TOTAL'].quantile(.90)]\n\nsns.boxplot(x=\"NAME_CONTRACT_TYPE\", y=\"AMT_CREDIT\", data=df_filtered,palette='rainbow',hue='TARGET')\nplt.yscale(\"log\")","93984f73":"For the above table the column description is:\n    \n    Count_0 = Total number of non defaulters applicants which belong to that particular income type.\n              Ex: There are total 143550 non defaulter applicants that have \"Working\" as their income type.\n    \n    Count_1 = Total number of defaulter applicants which belong to that particular income time.\n              Ex: There are total 15224 defaulter applicants that have \"Working\" as their income type.\n    \n    Perct_0 = How much percentage the particular category contibutes to all the non defaulter applicants. Target = 0\n              Ex: Out of total applicants of Target=0, 50% of them have \"Working\" as their income type.\n    \n    Perct_1 = How much percentage the particular category contibutes to all the defaulter applicants. Target=1\n              Ex: Out of total applicants of Target=1, 61% of them have \"Working\" as their income type.\n    \n    percentage_contri_0 = Out of all the applicants with the particular category, how much percentage belong to Target=0\n              Ex: Out of total working applicants, 90% are those with Target =0 \n    \n    percentage_contri_1 = Out of all the applicants with the particular category, how much percentage belong to Target=1\n              Ex: Out of total working applicants, 9.5% are those with Target =1.\n              \n              \n#### Similar table with these columns will be used for analysis of other variables\n            ","ccc4b4df":"## 2.1 Univariate Analysis\nDone to check the impact of one independent variable on a dependent variable","dfe8702f":"## 1.4\tHandling outliers\n\nIdentification and handling of outliers has been done on the 3 columns - 'AMT_ANNUITY','AMT_GOODS_PRICE','AMT_CREDIT' for which the quantile range of 25-75% has been considered.\n\nIt is important to understand that extremely high value is not always outlier. In some cases extremely high or extremely low  value can indicate the missing information (eg: -999999 or 999999 in case of numerical data)\nHere in our case outliers have been chosen for removal in order to calculate mean value of columns. Ouliers in our case are rare occurances and may misrepresent the dataset if considered for analysis.\n","11286d09":"## 2.1.2 Univariate Analysis for Numerical Variable\n        o\tNeed to check: mean, median, mode, min, max,range, quantiles, standard deviation.\n        o\tPlots: Distribution, histogram, Box Plots\n","3487ea22":"From above we can see that there are variables above AGE can be considered as categorical variable (as unique values for them is very less compared to length of the data) and rest as contiuous variables.","c1ca4641":"Distribution of '0' is skewed. Applicants with high credit ratings, tends to repay their loan.\nApplicants with lower credit score tends to have a larger defaulter rate. ","a49013fd":"## Analysis:\nIgnoring Income type \"Unemployed\" and \"Maternity Leave\" as the data is very less.\n\nThe percentage of working people is maximum among the applicants.\n\nOut of income type \"Working\", \"Commercial associate\",\"Pensioners\" and \"state_servants\", Pensioners are highly likely to repay their loans on time.\n\nOut of all pensioners who applied for the loan, there is <b>94.6% <\/b>chance that he will repay the loan and <b>5.3% <\/b>that he will default.\n\nSimilarly out of all the working applicants, there is <b>90.4%<\/b> chance that he will repay the loan and <b>9.5%<\/b> chance that he will default.\n\n#### Therefore, the total impact of income type on the defaulters is 4.2% (9.58% being the max and 5.3% being the worst.)\n#### So if we want to consider applicants, applicants with income type as pensioners should be given the highest priority.","743028e9":"This shows that 7% of the data is having outliers for AMT_GOODS_PRICE and AMT_GOODS_PRICE. Therefore, these rows are not entirely deleted from the dataset and only missing values for AMT_GOODS_PRICE are imputed by calculating mean after removing the outliers.","c24f5753":"<b> Let us consider an another variable categorical variable CODE_GENDER - the gender of the applicant and see how the gender of a person impacts the target variable. ","630a692c":"## 1.3\tHandling missing values\n\nThe below columns have been chosen to showcase the imputation of missing values from the subset of the data selected.\n\na) ORGANIZATION_TYPE - Unordered Categorical variable.<br>\nb) CODE_GENDER - Unordered Categorical variable.<br>\nc) CNT_FAM_MEMBERS - Ordered categorical variable.<br>\n\nFor the above variables,MODE has been used to impute the missing data as the data is categorical in nature and mode represents the most common category.\n       \nFor the continuous numerical value,MEAN or MEDIAN is usually used to impute the missing data. When the data is normally distributed without any large number of outliers, taking the mean would be the best option. But sometimes when the data is not normally distributed, example, there might be few applicants with extraordinary large income. In such cases the distribution will be skewed and taking MEDIAN to impute the missing value will be the best option.","673143ae":"## 1.7\tCreating derived variables and binning the data.\n\nTwo columns have been chosen for binning,namely\n\nDAYS_BIRTH<br>\nCREDIT_RATINGS\n\nFrom the DAYS_BIRTH, AGE of the client is calculated and then AGE_GROUPS are formed based on the age.\n","e8b8c194":"<b>Finding correlations and heatmaps for all the continous variables in your data. Follow the below steps to calculate correlations between all the numerical variables in the data.\n","656fd288":"<b> There is an imbalance in the data where only 8% clients are with payment difficulties and 91% clients are all others.","d4e6d0e7":" **Analysis:\n>    From the above hexplot we can see that applicants with age 28 to 37 tend to get the larger amount of credit as compared to older applicants. We can easily confirm this with the following box plot.\n    ","06928f88":"    Analysis:\n    Of all the C1 credit rating category applicants, the maximum percentage of defaulters are 'Commmercial Associates'\n    The Percentage of applicants with income type as 'working' and with C1 credit rating is the lowest in all the defaulters.\n    ","b31b9997":"##  1. Getting the data ready\n\nThis is the first and most important step in data analysis. Usually there are large number of columns, redundant rows and missing values in a data. Its is very important to clean the data before performing any analysis.\n\n   ## 1.1 Understanding the data","42443d47":"<b>Quick Analysis: <\/b>\nThe median of credit ratings of defaulters tends to be lower than that of non defaulter applicants.\nAs we see that upper quantile of defaulters overlaps with the lower quantile of non defaulters, they still have tendency to repay but are defaulting. Therefore credit rating is not the only factor affecting the rate of defaulters. We need to consider other factors also.","c9f974fc":"Before performing analysis, you need to identify the variables as categorical variable or numerical variable. ","ffab413c":"**INTRODUCTION**\n\nExploratory data analytics requires understanding of data and how different variables are correlated. \nTo understand the data using python data visualization and pandas we will be taking a case study of bank loan applications data. \n\nThe bank applications data has a dependent variable named TARGET variable whose value is 1 when the applicant is a defaulter and have missed the replayment of loan. For other applicants this value will be 0. All other columns in the data are considered independent variables. \n\nWe will explore the data step by step, visualize the data, create count\/count percentage and summary tables and see how different variables like age, credit ratings, income etc of the applicant varies with the TARGET variable.\n\nThe following steps will be covered in the tutorial\n\n1.\tGetting the data ready<br><br>\n1.1\tUnderstanding the data<br>\n1.2\tCleaning data by dropping unwanted rows and columns<br>\n1.3\tHandling missing values<br>\n1.4\tHandling outliers<br>\n1.5\tChanging the data types of columns<br>\n1.6\tChanging column names into meaningful ones for analysis. <br>\n1.7\tCreating derived variables and binning the data. <br>\n1.8\tUnderstanding the data imbalance<br>\n\n2.\tData Analysis<br>\n2.1\tUnivariate Analysis<br>\n2.1.1\tCategorical<br>\n2.1.2\tNumerical<br>\n2.2\t  Bivariate  Analysis<br>\n2.2.1  Categorical  & Categorical<br>\n2.2.2\tNumerical & Numerical<br>\n2.2.3\tCategorical & Numerical<br>\n    \n**Note**\n* Kindly go through the code comments for details.\n* As we explore the data, I have added the Analysis and conclusion that we can make out with the plots and summary tables.\n* For the understanding of concepts, I have just taken 3 to 4 variables at a time for analysis . You can extend the analysis for other variables in the similar fashion.\n* This is just the beginning and not the exhaustive exploration. Feel free to leave suggestions in the comments section.","5015a135":"#### 0.09% of missing value of AMT_GOODS_PRICE will be handled after removal of outliers in the data.","202e29e2":"Just a quick analysis from above plot: Seniors for less likely to be a defaulter in replaying the loan.","df1fa28b":"Of the remaining columns with < 20% missing data,a detailed analysis has been done to pick the below 24 columns for further analysis.","fa5ba964":"## Analysis:\n\nOut of all the Females who have applied, 93% of them have repayed their loan and 7% of them  are defaulters.\n\nAnd if a male candidate applies, there is a 10% chance that he will default.\n\n<b> Gender can decrease the total % of \"defaulters\" by -3.24%. \n We can say that Females are likely to repay their loans on time","d07bc0e5":"<b> Quick Analysis<\/b>\n\n    There are more random peeks to be observed in above plot with target = 1 as compared to those with Target= 0. \n    The detailed analysis of categories of credit rating will be done in segmented univariate analaysis with CREDIT_CATEGORY","3f7da07d":"## 2.\tData Analysis","55ab6d20":"#### We can now confirm now that we do not have any missing values and continue further with analysis.","00d4ff14":"## 2.2.3 Categorical & Numerical\n   * Plots: Box plots, line chart\n","b8627605":"## 1.6\tChanging column names into meaningful ones for analysis.","fb61f595":"  <b>Analysis<\/b>:\n  \n    There tends to be a inverse relationship between the credit rating and the defaulters, with C1 being the highest credit rating group, there are less percentage of defaulters.\n\n    If we consider an applicant from C1 category credit rating, there is only 3.5% chance that he will be a defaulter and if we consider an applicant with C4 category, there is 15% chance that he will be a defaulter.\n\n    Therefore the credit rating can decrease the percentage of defaulters by (15%- 3.5%) 11.5%.\n    So if we want to consider applicants, applicants with higher credit ratings should be given the highest priority.","bd7175a9":"The median of AMT_CREDIT tends to highest in Adults and lowest in Young Adults. ","cd8de8ab":"## 1.2\tCleaning data by dropping unwanted rows and columns","7e043893":"**Analysis:** \n\nFrom the above box plots,we can infer that people with defaulting intentions tend to take less credit of revolving loans as revolving loans involve repayment and then reusing it.\nHence, we can deduce that more scrunity can be put in place for cash loans as against to Revolving loans.","a29ad334":"<b> Binning CREDIT_RATINGS based on quantiles <\/b>\n<br>\nThe credit rating is being categorized ino C1,C2,C3 and C4, where C1 category is the highest.\nThe categorization is done based on the quantiles.","482257dc":"## 2.2.2 Numerical & Numerical\n\n    o\tNeed to check: correlations\n    o\tPlots: heatmaps, scatter plots, hex plots\n","d31329c9":"## 1.8\tUnderstanding the data imbalance","10a3f552":"   ## 2.2.1 Categorical  & Categorical\n        o\tNeed to check: Counts\/Count% \n        o\tPlots: Bar chart, Stacked bar chart, 2-y Axis plot \u2013 line charts \n","338ad861":"**Analysis:\n> From the above hexplot we can see that large number of application lie in the region where amount annuity tends to be between 20% to 30% of the applicants income\n    ","2a75da93":"## 1.5\tChanging the data types of columns\n\nThe following columns had integer data but the datatype is float.The below code converts them to int.","645f69b4":"##### As there is huge data imbalance, converting the numbers into percentages and using them for plots and analyzing","713961f7":"## 2.2 Bivariate Analysis \nDone to check the impact of two independent variables on a dependent variable.","503cf47e":"#### Dividing the data based on the dependent variable (TARGET) for further analysis","1ed431fb":"Next step is to check the various categories of credit ratings CREDIT_CATEGORY. (We previously binned the credit ratings into categories C1,C3,C3,C4 with C1 being the highest.","31749e14":"## 2.1.1 Univariate Analysis for Categorical Variable\n\n    o\tNeed to check: Counts\/Count% \n    o\tPlots: bar-charts, stacked bar charts\n","68bef2fb":"**Analysis:**\n> From the above table we can easily see that AMT_GOODS_PRICE and  AMT_CREDIT have the highest corelation, which is quite obvious as higher the good price, higher will be the loan value.\n    ","70d1f370":"<b>We still have 0.21% of EXT_SOURCE_2 missing. As it is very less percentage, the rows with null value of EXT_SOURCE_2 has been deleted. Also EXT_SOURCE_2 is the credit rating of an applicant and is an important column for performing analysis on it is better to delete rather than imputing with an incorrect value. "}}