{"cell_type":{"df973ae7":"code","d958f766":"code","38692009":"code","ec43554b":"code","a10138f6":"code","2091f521":"code","c4de0d0c":"code","6a03b180":"code","1f778694":"code","c8784cc4":"code","308a792a":"code","33c5289a":"code","fc88772a":"code","f9923aa4":"code","5b85db2a":"code","5d911436":"code","892d2ae9":"code","1fe68a70":"markdown","516e7c7a":"markdown","c64c2426":"markdown","1be89d69":"markdown","b4c1f561":"markdown","bd7139e7":"markdown"},"source":{"df973ae7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn import model_selection\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nimport tqdm","d958f766":"train=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")","38692009":"train.head()","ec43554b":"train.isnull().sum()","a10138f6":"y = train['loss']\ntrain.drop(['id','loss'],axis=1,inplace=True)\ntest.drop(['id'],axis=1,inplace=True)","2091f521":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(train.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[train.columns.tolist()[:100][i]], color = '#34675c', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","c4de0d0c":"not_features = ['id', 'loss']\nfeatures = []\nfor feat in train.columns:\n    if feat not in not_features:\n        features.append(feat)","6a03b180":"scaler = StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","1f778694":"x = train","c8784cc4":"lda = LDA(n_components=42, solver='svd')\nX_lda = lda.fit_transform(x, y)\n\nEVR = lda.explained_variance_ratio_\nfor idx, R in enumerate(EVR):\n    print(\"Component {}: {}% var\".format(idx+1, np.round(R*100,2)))","308a792a":"def objective(trial,data=x,target=y):\n    lda = LDA(n_components=42, solver='svd')\n    \n    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25,random_state=42)\n    X_train = lda.fit_transform(X_train, y_train)\n    X_test = lda.fit_transform(X_test, y_test)\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2000),\n             'loss_function':'RMSE',\n              'task_type':\"GPU\",\n              'eval_metric':'RMSE',\n              'leaf_estimation_method':'Newton',\n              'bootstrap_type': 'Bernoulli',\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0,1),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',1,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    model = CatBoostRegressor(**params)  \n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n        \n    y_preds = model.predict(X_test)\n    loss = np.sqrt(mean_squared_error(y_test, y_preds))\n    \n    return loss","33c5289a":"OPTUNA_OPTIMIZATION = True\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","fc88772a":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_optimization_history(study))\n    display(optuna.visualization.plot_slice(study))\n    display(optuna.visualization.plot_parallel_coordinate(study))","f9923aa4":"cat_params = study.best_trial.params\ncat_params['loss_function'] = 'RMSE'\ncat_params['eval_metric'] = 'RMSE'\ncat_params['bootstrap_type']= 'Bernoulli'\ncat_params['leaf_estimation_method'] = 'Newton'\ncat_params['random_state'] = 42\ncat_params['task_type']='GPU'\ntest_preds=None\n\nprint(\"\\033[93mTraining........\")\n\nkf = StratifiedKFold(n_splits = 10 , shuffle = True , random_state = 42)\nfor fold, (tr_index , val_index) in enumerate(kf.split(x.values , y.values)):\n    \n    print(\"\u2059\" * 10)\n    print(f\"Fold {fold + 1}\")\n    \n    x_train,x_val = x.values[tr_index] , x.values[val_index]\n    y_train,y_val = y.values[tr_index] , y.values[val_index]\n        \n    eval_set = [(x_val, y_val)]\n    \n    model =CatBoostRegressor(**cat_params)\n    model.fit(x_train, y_train, eval_set = eval_set, verbose = False)\n    \n    train_preds = model.predict(x_train)    \n    val_preds = model.predict(x_val)\n    \n    print(np.sqrt(mean_squared_error(y_val, val_preds)))\n    \n    if test_preds is None:\n        test_preds = model.predict(test.values)\n    else:\n        test_preds += model.predict(test.values)\n\nprint(\"-\" * 50)\nprint(\"\\033[95mTraining Done\")\n\ntest_preds \/= 10","5b85db2a":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","5d911436":"submission['loss']=test_preds","892d2ae9":"submission.to_csv(\"subcat.csv\",index=False)","1fe68a70":"## Scaling and LDA","516e7c7a":"# **Preparing Submission**","c64c2426":"# **Model Tuning and Training**","1be89d69":"# **Exploratory Data Analysis**","b4c1f561":"## Feature Distribution","bd7139e7":"# **Credits:**\n\n* https:\/\/www.kaggle.com\/pranjalverma08\/tps-08-cb-lgbm-xgb-starter\n* https:\/\/www.kaggle.com\/dmitryuarov\/tps-aug-2021-eda-cb-vs-xgb-vs-lgbm"}}