{"cell_type":{"049cf81f":"code","8428344c":"code","1ca8164e":"code","70864809":"code","c2519c68":"code","f20f85a3":"code","e0275568":"code","4fbe45e9":"code","49774696":"code","050b98eb":"code","dc750573":"code","a929a11c":"code","9ea7e9b8":"code","a8402c8f":"code","7e4c189c":"code","fd23d6b1":"code","719bc8a0":"code","e48eea40":"code","bc4b99de":"code","66e61dbc":"code","a9f3c824":"code","7df3a6ff":"code","66f92435":"code","1487ec59":"code","bbfa6764":"code","0dce0ae6":"code","9a2c1136":"code","cd7cf957":"code","65840eda":"code","7d69700a":"code","eaf49af0":"code","58e72895":"code","964513b4":"code","4c7efa7a":"code","e82d1445":"code","2a28ef0a":"code","f571850a":"code","e958fe84":"code","7b4d42a0":"code","59a7df2a":"code","8e0e0860":"code","5c53962e":"code","3447f187":"code","76643bcb":"code","73daad02":"code","5a43285d":"code","e730e5b3":"code","f2ea9d5a":"markdown"},"source":{"049cf81f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport shap\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import cross_val_score\nimport os\nprint(os.listdir(\"..\/input\"))\n\nsns.set(color_codes=True)\n\n# Any results you write to the current directory are saved as output.","8428344c":"train = pd.read_csv('..\/input\/train_V2.csv')\ntest = pd.read_csv('..\/input\/test_V2.csv')","1ca8164e":"train.shape","70864809":"train.head()","c2519c68":"train.describe()","f20f85a3":"train.isnull().sum()","e0275568":"train[train['winPlacePerc'].isnull() == True]","4fbe45e9":"train.drop(train.index[2744604], inplace=True)","49774696":"plt.figure(figsize=(20,20))\ncorr = train.corr()\nsns.heatmap(corr, annot=True)","050b98eb":"sns.distplot(train['winPlacePerc'])","dc750573":"train[train['matchId']=='a10357fd1a4a91'].shape","a929a11c":"sns.distplot(train[train['matchId']=='a10357fd1a4a91']['winPlacePerc'], bins=5)","9ea7e9b8":"#plt.figure(figsize=(24,16))\n\nsns.violinplot(train['winPlacePerc'])","a8402c8f":"plt.title('Match Duration Distribution (s)')\nplt.figure(figsize=(16,16))\nplt.hist(train['matchDuration'])","7e4c189c":"# Feature Engineering\n\n\n# Distance and speed\ntrain['total_distance'] = train['swimDistance'] + train['walkDistance'] + train['rideDistance']\ntrain['avg_speed'] = train['total_distance'] \/ train['matchDuration']\ntrain['avg_swim_speed'] = train['swimDistance'] \/ train['matchDuration']\ntrain['avg_walk_speed'] = train['walkDistance'] \/ train['matchDuration']\ntrain['avg_ride_speed'] = train['rideDistance'] \/ train['matchDuration']\n# Kill rate feature engineering\ntrain['streak_rate'] = train['killStreaks'] \/ train['kills']\ntrain['kills_rate'] = train['kills'] \/ train['matchDuration']\ntrain['knocked_kill'] = train['DBNOs'] \/ train['kills']\ntrain['kill_per_heal'] = train['heals'] \/ train['kills']\ntrain['kills_per_place'] = train['kills'] \/ train['killPlace']\ntrain['damage_kill'] = train['damageDealt'] \/ train['kills']\ntrain['damage_rate'] = train['damageDealt'] \/ train['matchDuration']\n# Utilities items\ntrain['heals_rate'] = train['heals'] \/ train['matchDuration']\ntrain['boosts_rate'] = train['boosts'] \/ train['matchDuration']\ntrain['utility_used'] = train['boosts'] + train['heals']\ntrain['boosts_prop'] = train['boosts'] \/ train['utility_used']\ntrain['heals_prop'] = train['heals'] \/ train['utility_used']\ntrain['utility_rate'] = train['utility_used'] \/ train['matchDuration']","fd23d6b1":"train.replace([np.inf, -np.inf], np.nan, inplace=True)\ntrain.fillna(0, inplace=True)","719bc8a0":"train.describe()","e48eea40":"plt.figure(figsize=(16,16))\nplt.title('Distribution: Data for player per matches')\nplt.xlabel('Number of players per match')\nplt.hist(train['numGroups'])\n","bc4b99de":"plt.figure(figsize=(24,16))\nsns.scatterplot(train[train['matchId']=='a10357fd1a4a91']['walkDistance'], train[train['matchId']=='a10357fd1a4a91']['winPlacePerc'])\nplt.title('Walk Distance vs. Winning Percentile (Match)')","66e61dbc":"plt.figure(figsize=(24,16))\nsns.scatterplot(train['total_distance'], train['winPlacePerc'])\nplt.title('Total Distance vs. Winning Percentile')","a9f3c824":"plt.figure(figsize=(24,16))\nsns.scatterplot(train['avg_speed'], train['winPlacePerc'])\nplt.title('Average Speed vs. Winning Percentile')","7df3a6ff":"plt.figure(figsize=(24,16))\nplt.title('Kills vs Winning Percentile')\nsns.scatterplot(train['kills'], train['winPlacePerc'])\n","66f92435":"match_types = list(train['matchType'].unique())","1487ec59":"fig, axs = plt.subplots(4,4, figsize=(16,19))\nplt.subplots_adjust(hspace = 0.7)\n\nfor i,t in enumerate(match_types):\n    axs[i \/\/ 4][i % 4].hist(train[train['matchType'] == t]['winPlacePerc'].astype('float'))\n    axs[i \/\/ 4][i % 4].set_title(t+' Win Percentile')\n     \n\n","bbfa6764":"fig, axs = plt.subplots(4,4, figsize=(20,30))\nplt.subplots_adjust(hspace = 0.5)\n\nfor i,t in enumerate(match_types):\n    sns.scatterplot(train[train['matchType'] == t]['total_distance'],train[train['matchType'] == t]['winPlacePerc'], ax=axs[i \/\/ 4][i % 4])\n    axs[i \/\/ 4][i % 4].set_title(t+': Total Distance vs Win Percentile')","0dce0ae6":"fig, axs = plt.subplots(4,4, figsize=(20,30))\nplt.subplots_adjust(hspace = 0.5)\n\nfor i,t in enumerate(match_types):\n    sns.scatterplot(train[train['matchType'] == t]['weaponsAcquired'],train[train['matchType'] == t]['winPlacePerc'], ax=axs[i \/\/ 4][i % 4])\n    axs[i \/\/ 4][i % 4].set_title(t+': Weapons Acquired vs Win Percentile')","9a2c1136":"train['matchType'].unique()","cd7cf957":"# Check for fpp\n\ndef oh_matchtype(x, mode_name):\n    if len(mode_name) <= len(x):\n        if mode_name == x[:len(mode_name)]:\n            return 1\n    return 0\n    \n        \nmatch_type = ['crash','flare','duo', 'solo', 'squad' ,'normal-duo','normal-solo','normal-squad']\n\ntrain['fps_mode'] = train['matchType'].apply(lambda x: 1 if 'fpp' in x else 0)\n\nfor i in match_type:\n    train['matchtype_'+i] = train['matchType'].apply(oh_matchtype, args=(i,))","65840eda":"train.drop(['matchType'], inplace=True, axis=1)","7d69700a":"train.describe()","eaf49af0":"from sklearn.preprocessing import StandardScaler\n\n#scale_col = ['damageDealt','matchDuration','rankPoints','killPoints','winPoints','walkDistance',\n#             'rideDistance','damage_kill','total_distance','longestKill']\n#train[scale_col] = StandardScaler().fit_transform(train[scale_col])\nscaler = StandardScaler()\nscaler.fit(train.drop(['Id','groupId','matchId','winPlacePerc'],axis=1))\n","58e72895":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ntrain_red = train.sample(frac=0.1, random_state=42).reset_index(drop=True)","964513b4":"train_x, test_x, train_y, test_y = train_test_split(train_red.drop(['Id','groupId','matchId','winPlacePerc'],axis=1),\n                                                    train_red['winPlacePerc'], \n                                                    test_size=0.25)\n\nscale_train_x, scale_test_x, scale_train_y, scale_test_y = train_test_split(\n    scaler.transform(train_red.drop(['Id','groupId','matchId','winPlacePerc'],axis=1)),\n                                                    train_red['winPlacePerc'], \n                                                    test_size=0.25)","4c7efa7a":"dtrain = xgb.DMatrix(train_x, train_y)\ndtest = xgb.DMatrix(test_x, test_y)","e82d1445":"xgb_log_params = {'eta': 0.5,\n              'objective': 'reg:logistic',\n              'max_depth': 7,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'eval_metric': ['rmse','mae'],\n              'seed': 11,\n              'silent': True}","2a28ef0a":"watchlist = [(dtrain, 'train'), (dtest,'test')]","f571850a":"xgb_log_model = xgb.train(params=xgb_log_params, dtrain=dtrain, evals=watchlist)","e958fe84":"explainer = shap.TreeExplainer(xgb_log_model)\nshap_values = explainer.shap_values(train_x)\nshap.summary_plot(shap_values, train_x)","7b4d42a0":"def xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 0.8,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","59a7df2a":"xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 9), \n                                             'gamma': (0, 1),\n                                             'colsample_bytree': (0.3, 0.9)})\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\nxgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","8e0e0860":"for param in xgb_bo.max['params']:\n    xgb_log_params[param] = int(xgb_bo.max['params'][param])\n    \n","5c53962e":"xgb_log_params['eta'] = 0.09\nnum_boost_round = 10000\nearly_stopping_rounds=100","3447f187":"xgb_log_model = xgb.train(params=xgb_log_params, \n                          dtrain=dtrain, \n                          evals=[watchlist[1]],\n                         early_stopping_rounds=early_stopping_rounds,\n                         num_boost_round = num_boost_round)","76643bcb":"def crossfit(model, X, y,n_splits = 5):\n    kf = KFold(n_splits = 5, random_state=42)\n    for train_index, test_index in kf.split(X):\n        model = model\n        train_X, train_Y = X[train_index], Y[train_index]\n        test_X, test_Y = X[test_index], Y[test_index]\n        model.fit(train_X, train_Y) \n        return sqrt(mean_squared_error(model.predict(test_X), test_Y))\n","73daad02":"del dtrain\ndel dtest","5a43285d":"tuned_parameters = [{'tol': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\nclf = GridSearchCV(LinearSVR(), tuned_parameters, cv=4, scoring='neg_mean_squared_error')\nclf.fit(scale_train_x, scale_train_y)\nprint(clf.best_params_)\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"{} (+\/-{}) for {}\".format(mean, std * 2, params))","e730e5b3":"svr  = LinearSVR(max_iter=10000)\nsvr.set_params(**clf.best_params_)\nsvr_score = cross_val_score(svr, \n                scale_train_x, \n                scale_train_y, \n                cv=5,\n                scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.negative(svr_score))))","f2ea9d5a":"from sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor(max_iter=1000000, penalty='elasticnet')\n\nsgd_score = cross_val_score(sgd, \n                train_x, \n                train_y, \n                cv=5,\n                scoring='neg_mean_squared_error')\n#print(np.mean(np.sqrt(np.negative(sgd_score))))\nprint(sgd_score)"}}