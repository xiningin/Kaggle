{"cell_type":{"ae097f5a":"code","cb138eec":"code","7d51296d":"code","57d5b643":"code","64368688":"code","eb9b6c53":"code","c81b2a3b":"code","ac36a82b":"code","b1ada3c3":"code","38b21785":"code","ef230e04":"code","710230e8":"code","69803698":"code","eca9085c":"code","97835b85":"code","17d44e86":"code","acd0d257":"code","a2376425":"code","8ed12351":"code","331ed99a":"code","a83e075d":"code","7ee59167":"code","83eb6f4a":"code","b426992b":"code","a8719e81":"code","a176dd63":"code","643d6a69":"code","6750d6fc":"code","e94307b2":"code","d4a12e0a":"code","0a4d8732":"code","ce0413cd":"code","d67e5d83":"code","45ff3b3d":"code","a1f2ed41":"code","e8fb9576":"code","1792eb5a":"code","1e03ae2f":"code","e02aa7d8":"code","276d71be":"code","b84daeb5":"code","a0e63c90":"code","1f50819a":"code","86d4413a":"code","66747fcd":"code","68337d15":"code","00686f55":"code","de8603c0":"code","6aaa306b":"code","5f142546":"code","21804313":"code","eeb208d5":"code","b2ed4189":"code","29d1ff14":"code","3698af30":"markdown","2f958a2b":"markdown","7b31493d":"markdown","f88c88eb":"markdown","fbefe1d9":"markdown","21dd3403":"markdown"},"source":{"ae097f5a":"pip install featuretools[autonormalize]","cb138eec":"import os\nimport csv, json\n\nimport random\nimport numpy as np\nimport pandas as pd\n\npd.set_option('display.max_rows', 690)\npd.set_option('display.max_columns', 690)\npd.set_option('display.max_colwidth', 6969)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom itertools import combinations\n\nimport featuretools as ft","7d51296d":"import sys\n\nMIN_INT = -sys.maxsize - 1","57d5b643":"import warnings\n\nwarnings.filterwarnings('ignore', message=\"^Columns.*\")","64368688":"DF = dict()\ndatasets = ['train', 'test']\nfor dataset in datasets:\n    DF[dataset] = pd.read_csv(f'..\/input\/titanic\/{dataset}.csv')\n    display(DF[dataset].sample(n=7))\n    \nsubmission_df = DF['test']['PassengerId']","eb9b6c53":"def describe_missing_values(df):\n    miss_val = df.isnull().sum()\n    miss_val_percent = 100 * df.isnull().sum() \/ len(df)\n    miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n    miss_val_table_ren_columns = miss_val_table.rename(\n        columns = {\n            0: 'Missing Values', \n            1: '% of Total Values',\n        }\n    )\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1] != 0\n    ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    print(f\"Your selected dataframe has {df.shape[1]} columns,\")\n    print(f\"\\t{miss_val_table_ren_columns.shape[0]} columns that have missing values.\")\n\n    return miss_val_table_ren_columns","c81b2a3b":"def visualize_distribution_of_missing_values(df):\n    df_nan_check = df.isna().sum().sort_values()\n    df_nan_check = df_nan_check.to_dict()\n    df_not_nan = []\n\n    nan_cols = 0\n\n    for key, value in df_nan_check.items():\n        df_nan_check[key] = int(value\/len(df)*100)\n        if df_nan_check[key] >= 80:\n            nan_cols += 1\n        else:\n            df_not_nan.append(key)\n\n    # Visualize\n    plt.figure(figsize=(9, 6))\n    plt.suptitle('Distribution of Empty Values', fontsize=19)\n    plt.bar(df_nan_check.keys(), df_nan_check.values())\n    plt.xticks(rotation=69)\n    plt.show()","ac36a82b":"for df_name, df in DF.items():\n    print(describe_missing_values(df))\n    visualize_distribution_of_missing_values(df)","b1ada3c3":"display(DF['train'].describe(include='all'))","38b21785":"for df_name, df in DF.items():\n    display(df['Fare'].describe())\n    df['Fare'].fillna(value=df['Fare'].mean(skipna=True), inplace=True)\n    display(df['Fare'].describe())","ef230e04":"for df_name, df in DF.items():\n    display(df['Embarked'].describe())\n    df['Embarked'].fillna(value=df['Embarked'].mode()[0], inplace=True)\n    display(df['Embarked'].describe())","710230e8":"for df_name, df in DF.items():\n\n    display(df['Age'].describe())\n\n    guess_ages = np.zeros((2,3))\n    for s, s_name in enumerate(['male', 'female']):\n        for c in range(0, 3):\n            guess_df = df[\n                (df['Sex'] == s_name) & \\\n                (df['Pclass'] == c+1)\n            ]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = random.uniform(age_mean-age_std, age_mean+age_std)\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[s,c] = int(age_guess\/0.5+0.5) * 0.5\n\n    print(guess_ages)\n\n    for s, s_name in enumerate(['male', 'female']):\n        for c in range(0, 3):\n            df.loc[\n                (df.Age.isnull()) & \\\n                (df.Sex == s_name) & \\\n                (df.Pclass == c+1), 'Age'] = guess_ages[s,c]\n\n    df['Age'] = df['Age'].astype(int)\n    display(df['Age'].describe())","69803698":"for df_name, df in DF.items():\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # display(pd.crosstab(df['Title'], df['Sex']))\n\n    rare_titles = ['Lady', 'Countess','Capt', 'Col', 'Dona', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n    df['Title'].replace(to_replace=rare_titles, value='other', inplace=True)\n\n    df['Title'].replace(to_replace=['Mlle', 'Miss'], value='Ms', inplace=True)\n    df['Title'].replace(to_replace='Mme', value='Mrs', inplace=True)\n    df['Title'] = df['Title'].map({'Master': 0, 'Mr': 1, 'Mrs': 2, 'Ms': 3, 'other': 4})\n    display(pd.crosstab(df['Title'], df['Sex']))","eca9085c":"for col in DF['train'].columns:\n    print(col, DF['train'][col].nunique()\/len(DF['train']))","97835b85":"for df_name, df in DF.items():\n    df.drop(columns=['PassengerId', 'Ticket', 'Cabin', 'Name'], inplace=True)","17d44e86":"for df_name, df in DF.items():\n    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n    df['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})","acd0d257":"# for df_name, df in DF.items():\n#     df.loc[df['Age'] <= 16, 'Age'] = 0\n#     df.loc[df['Age'] > 16, 'Age'] = 1\n#     df.loc[df['Age'] > 32, 'Age'] = 2\n#     df.loc[df['Age'] > 48, 'Age'] = 3\n#     df.loc[df['Age'] > 64, 'Age'] = 4","a2376425":"display(DF['train'].sample(n=7))","8ed12351":"x_test = DF['test']\nx_test.to_csv('x_test.csv', index=False)","331ed99a":"y_train = DF['train']['Survived']\nx_train = DF['train'].drop(columns=['Survived'], inplace=False)\n\nx_train.to_csv('x_train.csv', index=False)\ny_train.to_csv('y_train.csv', index=False)","a83e075d":"x_train.dtypes","7ee59167":"EntitySets = dict()\nfor df_name in DF.keys():\n    EntitySets[df_name] = ft.EntitySet(id=\"titanic_survival\")","83eb6f4a":"from featuretools.variable_types import *\n\n\nclass Age(Variable):\n    \"\"\"Represents variables that take on an ordered discrete value\"\"\"\n    _default_pandas_dtype = float\n    _name = 'Age'\n\n    \nclass Price(Variable):\n    \"\"\"Represents variables that take on an ordered discrete value\"\"\"\n    _default_pandas_dtype = float\n    _name = 'Price'","b426992b":"import autonormalize as auto_norm\n\nfor df_name in EntitySets.keys():\n    EntitySets[df_name] = EntitySets[df_name].entity_from_dataframe(\n        entity_id='titanic_survival',\n        dataframe=x_train if df_name=='train' else x_test,\n        make_index=False, \n        index='index',\n        variable_types={\n            'Pclass': Categorical,\n            'Sex': Categorical,\n            'Age': Age,\n            'SibSp': Numeric,\n            'Parch': Numeric,\n            'Fare': Price,\n            'Embarked': Categorical,\n            'Title': Categorical,\n        }\n    )\n    \n    # Normalize ~ to create new entity and relationship from current entity\n    # EntitySets[df_name] = auto_norm.normalize_entity(EntitySets[df_name])\n    \n    # Visualize\n    display(EntitySets[df_name].plot())","a8719e81":"from featuretools.entityset import deserialize, serialize\n\n# description = serialize.entityset_to_description(EntitySets['train'])\n# description = deserialize.description_to_entityset(description)\n# description","a176dd63":"primitives = ft.list_primitives()\nprimitives.sort_values(by=['type', 'name'], inplace=True)\ndisplay(primitives)","643d6a69":"default_aggregation_primitives = [\n    \"sum\", \"max\", \"min\", \"mode\", \"mean\", \"std\", \"skew\",\n    \"count\", \"percent_true\", \"num_unique\",\n]\n\ndefault_transformation_primitives = [\n    # Numeric\n    \"greater_than_scalar\",\n    \"add_numeric\", \"subtract_numeric\", \"multiply_numeric\", \n    \n    # Datetime\n    \"day\", \"year\", \"month\", \"weekday\", \"is_weekend\", \n\n    # LatLong\n    \"haversine\", \n\n    # NaturalLanguage\n    \"num_words\", \"num_characters\", \n]","6750d6fc":"from featuretools.primitives.base import *\nfrom featuretools.utils.gen_utils import Library\nfrom featuretools.primitives.standard.binary_transform import (\n    AddNumeric, AddNumericScalar,\n    SubtractNumeric, SubtractNumericScalar, ScalarSubtractNumericFeature,\n    MultiplyNumeric, MultiplyNumericScalar,\n    DivideNumeric, DivideNumericScalar, DivideByFeature,\n    ModuloNumeric, ModuloNumericScalar, ModuloByFeature,\n    GreaterThan, GreaterThanScalar, GreaterThanEqualTo, GreaterThanEqualToScalar,\n    LessThan, LessThanScalar, LessThanEqualTo, LessThanEqualToScalar, \n    Equal, EqualScalar, NotEqual, NotEqualScalar\n)\nfrom featuretools.variable_types import *\n\n\nclass GroupOf(TransformPrimitive):\n    \n    name = 'group_of'\n    input_types = [Age]\n    return_type = Ordinal\n    \n    def __init__(self):\n        self.description_template = \"group of {}\"\n        \n    def get_function(self):\n        def assign_group(vals: pd.Series):\n            groups = vals.copy()\n            groups.loc[vals <= 18] = 0 # children & teenager\n            groups.loc[vals > 18] = 1 # the young adult\n            groups.loc[vals > 35] = 2 # the adult\n            groups.loc[vals > 55] = 3 # the retired\n            groups.loc[vals > 70] = 4 # the elderly\n            return groups\n        return assign_group\n\n\nclass Quarter(TransformPrimitive):\n    \"\"\"\n    Determines the quarter value of a datetime.\n    \n    Examples:\n        >>> from datetime import datetime\n        >>> dates = [datetime(2019, 3, 1),\n        ...          datetime(2019, 6, 17, 11, 10, 50),\n        ...          datetime(2019, 11, 30, 19, 45, 15)]\n        >>> quarter = Quarter()\n        >>> quarter(dates).tolist()\n        [1, 2, 4]\n    \"\"\"\n    name = \"quarter\"\n    input_types = [Datetime]\n    return_type = Ordinal\n    compatibility = [Library.PANDAS, Library.DASK, Library.KOALAS]\n    description_template = \"the quarter of {}\"\n\n    def get_function(self):\n        def month(vals):\n            m_vals = vals.dt.month\n            q_vals = m_vals.copy()\n            q_vals.loc[m_vals <= 3] = 1 # 1st quarter of the year\n            q_vals.loc[m_vals > 3] = 2 \n            q_vals.loc[m_vals > 6] = 3 \n            q_vals.loc[m_vals > 9] = 4 \n            return q_vals\n        return \n\n\n# Define functions to duplicate\nnumeric_functions = [\n    AddNumeric(),\n    SubtractNumeric(),\n    MultiplyNumeric(),\n    # DivideNumeric(), DivideByFeature(value=1),\n    # ModuloNumeric(), ModuloByFeature(value=1),\n    GreaterThan(), GreaterThanScalar(value=0), \n    LessThan(), LessThanScalar(value=0), \n    Equal(), EqualScalar(value=0), \n]\n\n\n# Duplicate Numeric functions for customized Variables\ndef duplicate_functions(functions: list or tuple):\n    duplicated_functions = []\n    duplicated_variables = [Age, Price]\n    for dup_func in functions:\n        for var_type in duplicated_variables:\n            # print(f\"\\nDuplicating function {dup_func.name} for variable {var_type._name}\")\n\n            # Define nearly-duplicated primitive\n            dup_func.name = dup_func.name.replace('numeric', var_type._name.lower())\n            input_types = dup_func.input_types\n            if isinstance(input_types[0], (list, tuple)):\n                n_inputs = len(input_types[0])\n                dup_func.compatibility = [Library.PANDAS]\n                dup_func.input_types = [[var_type] * n_inputs]\n            else:\n                n_inputs = len(input_types)\n                dup_func.input_types = [var_type] * n_inputs\n            \n            # Append to list\n            duplicated_functions.append(dup_func)\n    return duplicated_functions\n\n\n# Define default primitives\ndefault_aggregation_primitives = [\n    \"sum\", \"max\", \"min\", \"mode\", \n    \"mean\", \"std\", \"skew\",\n    \"count\", \n    # \"percent_true\", \"num_unique\",\n]\n\ndefault_transformation_primitives = [\n    # Datetime\n    \"day\", \"month\", \"year\", Quarter, \"weekday\", \"is_weekend\", Quarter,\n\n    # LatLong\n    \"haversine\", \n\n    # NaturalLanguage\n    \"num_words\", \"num_characters\", \n\n    # Age\n    GroupOf\n] + numeric_functions + duplicate_functions(numeric_functions)","e94307b2":"EntitySets['train']['titanic_survival'].df.sample(7)","d4a12e0a":"from featuretools.utils.gen_utils import make_tqdm_iterator\n\n\ndef encode_categorical_features(feature_matrix: pd.DataFrame, features, \n                                top_n=11, include_unknown=True,\n                                to_encode=None, inplace=False, \n                                drop_first=False, verbose=False):\n    \"\"\"\n    Encode categorical features\n\n        Args:\n            feature_matrix (pd.DataFrame): Dataframe of features.\n            features (list[PrimitiveBase]): Feature definitions in feature_matrix.\n            top_n (int or dict[string -> int]): Number of top values to include.\n                If dict[string -> int] is used, key is feature name and value is\n                the number of top values to include for that feature.\n                If a feature's name is not in dictionary, a default value of 10 is used.\n            include_unknown (pd.DataFrame): Add feature encoding an unknown class.\n                defaults to True\n            to_encode (list[str]): List of feature names to encode.\n                features not in this list are unencoded in the output matrix\n                defaults to encode all necessary features.\n            inplace (bool): Encode feature_matrix in place. Defaults to False.\n            drop_first (bool): Whether to get k-1 dummies out of k categorical\n                    levels by removing the first level.\n                    defaults to False\n            verbose (str): Print progress info.\n\n        Returns:\n            (pd.Dataframe, list) : encoded feature_matrix, encoded features\n    \"\"\"\n    if not isinstance(feature_matrix, pd.DataFrame):\n        raise TypeError(\"feature_matrix must be a Pandas DataFrame\")\n\n    X = feature_matrix if inplace else feature_matrix.copy()\n\n    old_feature_names = set()\n    for feature in features:\n        for fname in feature.get_feature_names():\n            assert fname in X.columns, (f\"Feature {fname} not found in feature matrix\")\n            old_feature_names.add(fname)\n\n    pass_through = [col for col in X.columns if col not in old_feature_names]\n\n    iterator = make_tqdm_iterator(iterable=features,\n                                  total=len(features),\n                                  desc=\"Encoding pass 1\",\n                                  unit=\"feature\") if verbose else features\n    new_feature_list = []\n    new_columns = []\n    encoded_columns = set()\n\n    for f in iterator:\n        # TODO: features with multiple columns are not encoded by this method,\n        # which can cause an \"encoded\" matrix with non-numeric vlaues\n        is_categorical = issubclass(f.variable_type, Categorical)\n        if (f.number_output_features > 1 or not is_categorical):\n            if f.number_output_features > 1:\n                print(f\"[WARNING] Feature {f} has multiple columns and will not be encoded. This may result in a matrix with non-numeric values.\")\n            new_feature_list.append(f)\n            new_columns.extend(f.get_feature_names())\n            continue\n\n        if to_encode is not None and f.get_name() not in to_encode:\n            new_feature_list.append(f)\n            new_columns.extend(f.get_feature_names())\n            continue\n\n        val_counts = X[f.get_name()].value_counts().to_frame()\n        index_name = val_counts.index.name\n        if index_name is None:\n            if 'index' in val_counts.columns:\n                index_name = 'level_0'\n            else:\n                index_name = 'index'\n        val_counts.reset_index(inplace=True)\n        val_counts = val_counts.sort_values([f.get_name(), index_name],\n                                            ascending=False)\n        val_counts.set_index(index_name, inplace=True)\n        select_n = top_n\n        if isinstance(top_n, dict):\n            select_n = top_n.get(f.get_name(), DEFAULT_TOP_N)\n        if drop_first:\n            select_n = min(len(val_counts), top_n)\n            select_n = max(select_n - 1, 1)\n        unique = val_counts.head(select_n).index.tolist()\n        for label in unique:\n            add = f == label\n            add_name = add.get_name()\n            new_feature_list.append(add)\n            new_columns.append(add_name)\n            encoded_columns.add(add_name)\n            X[add_name] = (X[f.get_name()] == label)\n\n        if include_unknown:\n            unknown = f.isin(unique).NOT().rename(f.get_name() + \" is unknown\")\n            unknown_name = unknown.get_name()\n            new_feature_list.append(unknown)\n            new_columns.append(unknown_name)\n            encoded_columns.add(unknown_name)\n            X[unknown_name] = (~X[f.get_name()].isin(unique))\n\n        X.drop(f.get_name(), axis=1, inplace=True)\n\n    new_columns.extend(pass_through)\n    new_X = X[new_columns]\n    iterator = new_X.columns\n    if verbose:\n        iterator = make_tqdm_iterator(iterable=new_X.columns,\n                                      total=len(new_X.columns),\n                                      desc=\"Encoding pass 2\",\n                                      unit=\"feature\")\n    for c in iterator:\n        if c in encoded_columns:\n            try:\n                new_X[c] = pd.to_numeric(new_X[c], errors='raise')\n            except (TypeError, ValueError):\n                pass\n\n    return new_X, new_feature_list","0a4d8732":"def auto_feature_engineering(entity_set: ft.EntitySet,\n                             table_name: str,\n                             max_depth: int=2,\n                             max_features: int=-1,\n                             seed_features: list=None,\n                             aggregation_funcs: list=default_aggregation_primitives,\n                             transformation_funcs: list=default_transformation_primitives,\n                             groupby_transform_funcs: list=None,\n                             where_clause_funcs: list=None,\n                             encoding_categorical: bool=True,\n                             verbose: bool=False) -> pd.DataFrame:\n\n    # Deep Feature Synthesis \n    #   (Categorical features are not processed, on default)\n    feature_matrix, feature_definitions = ft.dfs(\n        entityset=entity_set,\n        target_entity=table_name,\n        agg_primitives=aggregation_funcs,\n        trans_primitives=transformation_funcs,\n        max_depth=max_depth,\n        max_features=max_features,\n        verbose=verbose,\n        return_variable_types=[Numeric, Discrete, Boolean, Age, Price]\n    )\n\n    # 1-hot encoding for Categorical features, including generated ones\n    if encoding_categorical:\n        features_before = set(feature_matrix.columns.values.tolist())\n        feature_matrix, feature_definitions = encode_categorical_features(feature_matrix, feature_definitions, include_unknown=False, verbose=verbose)\n        if verbose:\n            features_after = set(feature_matrix.columns.values.tolist())\n            features_encoded = list(features_before.difference(features_after))\n            print(\"\\n\\nFeatures being encoded:\\n\\t\", \"\\n\\t\".join(features_encoded))\n\n    return feature_matrix.astype(float), feature_definitions","ce0413cd":"feature_matrix, feature_definitions = auto_feature_engineering(\n    entity_set=EntitySets['train'], \n    table_name='titanic_survival', \n    verbose=True\n)\n\ndisplay(feature_matrix.sample(n=7))","d67e5d83":"# feature_matrix, feature_definitions = ft.encode_features(feature_matrix, feature_definitions, include_unknown=False)\n# ft.save_features(feature_definitions, \"feature_definitions.json\")\n# display(feature_matrix.sample(n=7))\n\nx_train = feature_matrix","45ff3b3d":"def clean_data(df: pd.DataFrame, \n               processes: list=[('remove_null', 0.69), \n                                ('remove_single_val', False), \n                                ('remove_various_val', 0.89),\n                                ('remove_correlated', 0.89),\n                                ('remove_low_info',)]):\n    features_old = df.columns.values.tolist()\n    print('Original:\\n', features_old)\n    for process in processes:\n        print('-'*69)\n        if process[0] == 'remove_null':\n            df = ft.selection.remove_highly_null_features(df, pct_null_threshold=process[1])\n        elif process[0] == 'remove_single_val':\n            df = ft.selection.remove_single_value_features(df, count_nan_as_value=process[1])\n        elif process[0] == 'remove_correlated':\n            df = ft.selection.remove_highly_correlated_features(df, pct_corr_threshold=process[1])\n        elif process[0] == 'remove_low_info':\n            df = ft.selection.remove_low_information_features(df)\n        elif process[0] == 'remove_various_val':\n            df = remove_highly_various_features(df, threshold=process[1])\n        else:\n            continue\n        print(f'After {process[0]}:\\n', df.columns.values.tolist())\n    features_new = df.columns.values.tolist()\n    features_removed = list(set(features_old).difference(set(features_new)))\n    return df, features_removed\n\n\ndef remove_highly_various_features(df: pd.DataFrame, threshold: float=0.79) -> pd.DataFrame:\n    for col in df.columns:\n        variousness = df[col].nunique() \/ df[col].notnull().sum()\n        if variousness > threshold:\n            df.drop(columns=[col], inplace=True)\n    return df","a1f2ed41":"# x_train, features_removed = clean_data(x_train)\nfeatures_removed = []","e8fb9576":"import warnings\nimport random\n\nfrom joblib import Parallel, delayed\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator\nimport sklearn.linear_model as models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\n\ndef add_noisy_features(X: np.array, noise_ratio: float=0.5):\n    \"\"\"\n    Add noisy features to DataFrame,\n        in order to check whether the generated features are good\n    \"\"\"\n    # Verify data type\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if not isinstance(X, np.ndarray):\n        raise ValueError(f\"X must be either `np.array` or `pd.DataFrame` while being {type(X)}\")\n    n_samples, n_features = X.shape\n    noisy_feature_names = []\n    if n_samples > 50 and n_features > 1:\n        # shuffle DataFrame\n        X_shuffled = np.random.permutation(X.flatten()).reshape(X.shape)\n        random_noise = StandardScaler().fit_transform(X_shuffled)\n        X = np.hstack([X, random_noise])\n        noisy_feature_names.extend([f'noise_{n}' for n in range(n_features)])\n\n    # generate normally-distributed noise\n    n_noise = max(3, int(noise_ratio*n_features))\n    noisy_features = np.random.randn(n_samples, n_noise)\n    noisy_feature_names.extend([f'noise_{n}' for n in range(n_features, n_features+n_noise)])\n\n    return np.hstack([X, noisy_features]), noisy_feature_names\n\n\ndef noise_filtering(X: np.array, y: np.array, \n                    feature_names=None, \n                    problem_type: str=\"regression\", \n                    n_best: int=-1,\n                    n_priori_noise: int=-1):\n    \"\"\"\n    Trains a prediction model with additional noisy features and \n        selects only those of the original features \n        that have higher coefficient than any of the noisy features.\n\n    Inputs:\n        - X: n x d numpy.array with d features\n        - y: target vector corresponding to the data points in X\n        - feature_names: list of column names for the features in X\n        - problem_type: str, either \"regression\" or \"classification\" (default: \"regression\")\n    \n    Returns:\n        - good_cols: list of filtered column names\n    \"\"\"\n    # Verify data\n    n_samples, n_features = X.shape\n    if not feature_names:\n        feature_names = list(range(n_features))\n    if len(feature_names) != n_features:\n        raise ValueError(f\"Number of columns provided is different from number of features ({len(good_cols)} != {n_features})\")\n\n    # Load model\n    if problem_type == \"regression\":\n        model = models.LassoLarsCV(cv=5, eps=1e-8)\n    elif problem_type == \"classification\":\n        model = models.LogisticRegressionCV(cv=5, penalty=\"l1\", solver=\"saga\", class_weight=\"balanced\")\n    else:\n        print(f\"WARNING: Unknown problem_type ({problem_type}) - not performing noise filtering.\")\n        return feature_names\n\n    # Filter noisy features\n    X_noisy, noisy_features = add_noisy_features(X)\n    n_noise = len(noisy_features)\n    with warnings.catch_warnings():\n        warnings.simplefilter(action='ignore')\n        try:\n            model.fit(X_noisy, y)\n        except ValueError:\n            idx = np.random.permutation(n_samples)\n            model.fit(X_noisy[idx], y[idx])\n    \n    coefs = np.abs(model.coef_)\n    if problem_type == 'classification':\n        # model.coefs_ is n_classes x n_features, but we need only n_features\n        coefs = np.max(coefs, axis=0)\n    feature_coefs, noise_coefs = coefs[:n_features], coefs[n_features:]\n    weights = dict(zip(feature_names, feature_coefs))\n\n    if n_best > 0:\n        threshold = sorted(coefs, reverse=True)[min(n_best, n_features)]\n    elif n_priori_noise > 0:\n        threshold = sorted(coefs, reverse=True)[min(n_priori_noise+n_noise, n_features)]\n    else:\n        # only keep features more important than known noisy features\n        threshold = np.max(noise_coefs)\n    return [fn for fn in feature_names if weights[fn]>threshold]\n\n\ndef feature_selection(X: pd.DataFrame, y: pd.Series, \n                      problem_type: str=\"regression\", \n                      relevance_ratio: float=0.2,\n                      verbose=False) -> list:\n    \"\"\"\n    Feature Selection for NUMERIC variables\n\n    Inputs:\n        - X: pandas.DataFrame with n data points and p features; \n                to avoid overfitting, only provide data belonging to the n training data points. \n        - y: target pandas Series corresponding to the data points in X\n        - relevance_ratio: float between 0-1, \n        - problem_type: str, either \"regression\" or \"classification\" (default: \"regression\")\n        - verbose: verbosity level (boolean; default: False)\n    \n    Returns:\n        - feature_names: list of column names for X to train a prediction model\n    \"\"\"\n    features = dict()\n    features['original'] = X.columns.values.tolist()\n    n_features = len(X.columns)\n    n_samples = len(X)\n\n    # initial selection of too few but (hopefully) relevant features\n    features['relevant'] = noise_filtering(X=X.values, y=y.values, \n                                           feature_names=X.columns.values.tolist(),\n                                           problem_type=problem_type,\n                                           n_best=int(relevance_ratio*n_features))\n    features['irrelevant'] = list(\n        set(features['original']).difference(set(features['relevant']))\n    )\n    if verbose:\n        print(\"Features relevant:\\n\\t\", features['relevant'])\n        print(\"Features irrelevant:\\n\\t\", features['irrelevant'])\n\n    # Noise filtering for relevant features\n    features['relevant_denoised'] = noise_filtering(X=X[features['relevant']].values, y=y.values,\n                                                    feature_names=features['relevant'],\n                                                    problem_type=problem_type)\n    if verbose:\n        features_removed = set(features['relevant']).difference(set(features['relevant_denoised']))\n        print(\"Relevant features removed:\", list(features_removed))\n        \n    # Noise filtering for irrelevant features\n    features['irrelevant_denoised'] = []\n    if len(features['irrelevant']) > 0:\n        random.shuffle(features['irrelevant'])\n        X_noisy, noisy_names = add_noisy_features(X, noise_ratio=0.5)\n        n_relevances, n_irrelevances = len(features['relevant']), len(features['irrelevant'])\n        \n        try:\n            n_batches = int(n_irrelevances \/ max(10, 0.5*n_samples-n_relevances))\n            batch_size = int(n_irrelevances\/n_batches)\n        except ZeroDivisionError:\n            print(f\"All irrelevant features are noise\")\n            \n        for bi in range(n_batches):\n            batch_features = features['irrelevant'][bi*batch_size:(bi+1)*batch_size] + noisy_names\n            X_batch = np.hstack([X[batch_features].values, X_noisy])\n            features_relevant = noise_filtering(X=X_batch, y=y.values,\n                                                feature_names=batch_features,\n                                                problem_type=problem_type,\n                                                n_priori_noise=len(noisy_names))\n            features['irrelevant_denoised'].extend(features_relevant)\n        \n        # Remove duplicates\n        features['irrelevant_denoised'] = list(set(features['irrelevant_denoised']))\n\n        if verbose:\n            features_removed = set(features['irrelevant']).difference(set(features['irrelevant_denoised']))\n            print(\"Irrelevant features removed:\", list(features_removed))\n\n    # Noise filtering for the combination of features\n    combined_features = features['relevant'] + features['irrelevant']\n    denoised_features = noise_filtering(X=X[combined_features].values, y=y.values,\n                                        feature_names=combined_features,\n                                        problem_type=problem_type)\n    if verbose:\n        features_removed = set(combined_features).difference(set(denoised_features))\n        print(\"Combined features removed:\", list(features_removed))\n    \n    return denoised_features\n\n\ndef feature_screening(X: pd.DataFrame, y: pd.Series, \n                      n_runs: int=5,\n                      retain_features: list=[],\n                      corr_threshold: float=0.9,\n                      problem_type: str=\"regression\", \n                      n_workers: int=1,\n                      verbose=False):\n    \"\"\"\n    Select predictive NUMERIC features given the data and targets.\n\n    Inputs:\n        - X: pandas.DataFrame with n data points and p features; \n                to avoid overfitting, only provide data belonging to the n training data points.\n        - y: target pandas Series corresponding to the data points in X\n        - n_runs: number of times to perform feature selection \n                    with a random fraction of data points (int; default: 5)\n        - corr_threshold: threshold to deliminate correlated features\n        - retain_features: list of features that must be retained\n        - problem_type: str, either \"regression\" or \"classification\" (default: \"regression\")\n        - n_workers: the number of workers in parallel\n        - verbose: verbosity level (boolean; default: False)\n\n    Returns:\n        - feature_names: list of promising column names to train model\n    \"\"\"\n    # Verify data\n    n_samples, n_features = X.shape\n    if len(y) != n_samples:\n        raise ValueError(f\"Numbers of samples in X (={n_samples}) and y (={len(y)}) are mismatched !!!!\")\n    if verbose:\n        if n_runs > n_samples:\n            print(f\"[WARNING] There are fewer data samples than number of runs !!!\")\n\n    # Verify condition\n    if n_runs < 1 or problem_type not in ['regression', 'classification']:\n        print(f\"WARNING: n_runs (={n_runs}) must be larger than 1 AND problem_type (={problem_type}) must be either `regression` or `classification`\")\n        return X.columns.values.tolist()\n\n    # check that retaining columns exist \n    #       - the columns might be transformed to str !\n    retain_features = [f for f in retain_features if f in X.columns and not str(f) in X.columns] \\\n                        + [str(f) for f in retain_features if str(f) in X.columns]\n    important_features = X.columns.values.tolist()\n\n    # Scale features to uniform distribution\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        # Initialize scaler for common use\n        scaler = StandardScaler(with_mean=True, with_std=True)\n\n        # Scale features X\n        X_scaled = pd.DataFrame(columns=X.columns, dtype=np.float32)\n        for col in X.columns.values.tolist():            \n            X_scaled[col] = scaler.fit_transform(X[col].values.reshape(-1, 1)).ravel()\n\n        # Scale target y\n        y_scaled = y if problem_type=='classification' \\\n                    else scaler.fit_transform(y.values.reshape(-1, 1)).ravel()\n\n    # select good features in k runs in parallel by cross-validation\n    #       (i.e., randomly subsample data points)\n    def run_feature_screening(r):\n        if verbose:\n            print(f\"\\n\\n\\nRun {r+1} \/ {n_runs} feature screening ...\")\n        np.random.seed(r)\n        batch_size = max(10, int(0.85*n_samples))\n        indices = np.random.permutation(X_scaled.index)[:batch_size]\n        return feature_selection(X=X_scaled.iloc[indices], \n                                 y=y_scaled[indices],\n                                 problem_type=problem_type,\n                                 verbose=verbose)\n\n    if n_workers == 1 or n_runs == 1:\n        selected_features = []\n        for r in range(n_runs):\n            selected_features.extend(run_feature_screening(r))\n    else:\n        # only use parallelization code if needed\n        def merge_lists(L):\n            return [item for sublist in L for item in sublist]\n        \n        selected_features = merge_lists(\n            Parallel(n_jobs=n_workers, verbose=verbose)(\n                delayed(function=run_feature_screening)(r) for r in range(n_runs)\n            )\n        )\n    \n    if len(selected_features) > 0:\n        feature_counter = Counter(selected_features)\n\n        # sort by frequency, but down-weight long formulaes to break tie (if any)\n        selected_features = sorted(feature_counter, \n                                    key=lambda f: feature_counter[f] - 1e-11*len(str(f)))[::-1]\n        if verbose:\n            print(f\"There are {len(selected_features)} features after feature selection\")\n\n        # Add retained features\n        if len(retain_features) > 0:\n            selected_features.extend(retain_features)\n            selected_features = list(set(selected_features))\n            important_features = retain_features\n        else:\n            important_features = selected_features[:1]\n\n        # correlation filtering\n        k = len(important_features)\n        if len(selected_features) > k:\n            correlations = X_scaled[selected_features].corr()\n            for f_i, f in enumerate(selected_features[k:], k):\n                # only take features that are uncorrelated with the rest\n                if np.max(np.abs(correlations[f].ravel()[:f_i])) < corr_threshold:\n                    important_features.append(f)\n        if verbose:\n            print(f\"There are {len(selected_features)} features after correlation filtering\")\n\n    # perform noise filtering on de-correlated features\n    important_features = noise_filtering(X=X_scaled[important_features].values, y=y_scaled,\n                                         feature_names=important_features,\n                                         problem_type=problem_type)\n    if verbose:\n        print(f\"There are {len(important_features)} features after noise filtering\")\n\n    # Add retained features\n    important_features.extend(retain_features)\n    important_features = list(set(important_features))\n    if verbose:\n        print(f\"There are {len(important_features)} features after feature screening {n_runs} times\")\n    return important_features\n\n\nclass FeatureSelector(BaseEstimator):\n\n    def __init__(self, problem_type: int=\"regression\", \n                       n_runs: int=5,\n                       corr_threshold: float=0.9,\n                       retain_features: list=[],\n                       n_workers: int=1,\n                       verbose: bool=True):\n        \"\"\"\n        Multi-step cross-validated feature selection\n        \n        Inputs:\n            - problem_type: str, either \"regression\" or \"classification\" (default: \"regression\")\n            - n_runs: number of times to perform feature selection with a random fraction of data points (int; default: 5)\n            - corr_threshold: threshold to deliminate correlated features\n            - retain_features: list of features that must be retained\n            - n_jobs: how many jobs to run when selecting the features in parallel (int; default: 1)\n            - n_workers: the number of workers in parallel\n            - verbose: verbosity level (boolean; default: False)\n        \n        Attributes:\n            - important_features_: list of important features (to select via pandas.DataFrame columns)\n            - original_features_: list of original features of X when calling fit\n            - return_df_: whether to return a pandas.DataFrame; if False, return a numpy.array\n        \"\"\"\n        self.retain_features = retain_features\n        self.corr_threshold = corr_threshold\n        self.problem_type = problem_type\n        self.n_workers = n_workers\n        self.verbose = verbose\n        self.n_runs = n_runs\n\n    def fit(self, X: pd.DataFrame or np.array, y: pd.Series or np.array):\n        # Verify data types\n        if isinstance(X, pd.DataFrame):\n            self.return_df_ = True\n            self.original_features_ = list(X.columns)\n        else:\n            self.return_df_ = False\n            self.original_features_ = [f\"x_{i}\" for i in range(X.shape[1])]\n            X = pd.DataFrame(X, columns=self.original_features_)\n        if not isinstance(y, pd.Series):\n            y = pd.Series(y, name='target')\n        \n        # Perform multi-step feature selection\n        self.important_features_ = feature_screening(\n            X=X, y=y, n_runs=self.n_runs, n_workers=self.n_workers, verbose=self.verbose, \n            corr_threshold=self.corr_threshold, problem_type=self.problem_type\n        )\n        return self\n\n    def transform(self, X: pd.DataFrame or np.array):\n        # Validate attributes\n        check_is_fitted(self, [\"important_features_\"])\n        if len(self.important_features_) == 0:\n            if self.verbose:\n                print(\"WARNING: No important features found; returning data unchanged.\")\n            return X\n\n        # Validate data types\n        if isinstance(X, pd.DataFrame):\n            features = list(X.columns)\n        else:\n            features = [f\"x_{i}\" for i in range(X.shape[1])]\n        X = check_array(X, force_all_finite=\"allow-nan\")\n        if sorted(self.original_features_) != sorted(features):\n            raise ValueError(\"Features are different from calling `fit`\")\n\n        # Get selected features only\n        X = pd.DataFrame(X, columns=features)\n        X_selected = X[self.important_features_]\n        if self.return_df_:\n            return X_selected\n        return X_selected.values\n\n    def fit_transform(self, X: pd.DataFrame or np.array, y: pd.Series or np.array):\n        self.fit(X, y)\n        X_transformed = self.transform(X)\n        return X_transformed","1792eb5a":"display(x_train.sample(7))","1e03ae2f":"feature_selector = FeatureSelector(problem_type=\"classification\", \n                                   n_runs=5,\n                                   corr_threshold=0.9,\n                                   retain_features=[],\n                                   n_workers=1,\n                                   verbose=True)\n\nx_train = feature_selector.fit_transform(x_train, y_train)","e02aa7d8":"print(feature_selector.original_features_)\nprint(list(x_test.columns))","276d71be":"display(x_train.sample(7))","b84daeb5":"print(x_test.columns)\n\nx_test = ft.calculate_feature_matrix(feature_definitions, EntitySets['test'])\nx_test = feature_selector.transform(x_test)\n\nprint(x_test.columns)","a0e63c90":"pip install evalml","1f50819a":"import evalml\nfrom evalml import AutoMLSearch","86d4413a":"auto_ml = AutoMLSearch(X_train=x_train, y_train=y_train,\n                       problem_type='binary',\n                       # objective=fraud_objective,\n                       additional_objectives=['auc', 'f1', 'precision'],\n                       max_batches=1,\n                       optimize_thresholds=True)\n\nauto_ml.search(data_checks='disabled')","66747fcd":"auto_ml.rankings","68337d15":"best_pipeline = auto_ml.best_pipeline\nprint(best_pipeline)","00686f55":"auto_ml.describe_pipeline(auto_ml.rankings.iloc[0][\"id\"])","de8603c0":"best_pipeline.fit(x_train, y_train)","6aaa306b":"display(best_pipeline.feature_importance)\ndisplay(best_pipeline.graph_feature_importance())","5f142546":"# x_test = ft.calculate_feature_matrix(feature_definitions, EntitySets['test'])\n# x_test['index'] = x_test.index \ndisplay(x_test.sample(n=7))","21804313":"all_features = x_test.columns.values.tolist()\nfor col in features_removed:\n    if col not in all_features:\n        continue\n    x_test.drop(columns=[col], inplace=True)","eeb208d5":"results = best_pipeline.predict(x_test)._series","b2ed4189":"submission_df = pd.concat([submission_df, results], axis=1)\nsubmission_df.sort_values(by=['PassengerId'], inplace=True)\ndisplay(submission_df.head(11))","29d1ff14":"predictions = submission_df.values.tolist()\n\nwith open('prediction.csv', 'w', encoding='utf-8') as f:\n    writer = csv.writer(f)\n    writer.writerow(submission_df.columns)\n    writer.writerows(predictions)","3698af30":"# **Feature Engineering**","2f958a2b":"# **Model Training**","7b31493d":"# **Libraries**","f88c88eb":"# **Data Loading**","fbefe1d9":"# **Data Cleaning**","21dd3403":"# **Feature Selection**"}}