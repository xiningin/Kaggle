{"cell_type":{"851df8c8":"code","97bccc53":"code","c1ec3090":"code","63dc27a5":"code","8af25582":"code","502d2035":"code","07b7b878":"code","1991aee7":"code","e40e3b0d":"code","223dd1e0":"code","e4096ff9":"code","d83a9151":"code","4a70c7e2":"code","2143b526":"code","d562c40d":"code","2c003e2d":"code","58a15851":"code","9eb1dd9c":"code","55fc7b67":"code","2d0dbe2e":"code","97802ce2":"code","eb501877":"code","10ae6430":"code","26322416":"code","89db90a4":"code","9d448d04":"code","8f97ec56":"code","503cc7fb":"code","cef14591":"code","c62c32eb":"code","5aafb67b":"code","69a9dce0":"code","6bb07267":"code","c802e6a9":"code","6f43d08a":"code","ac2dc15e":"code","24538033":"code","f592eb8a":"code","ab0dc713":"code","7737b48d":"code","df64669e":"code","fe6af54a":"code","b516b68c":"code","1e573b4f":"code","32006735":"code","914df81e":"code","ebcefe1e":"code","1723462d":"code","e21e776e":"code","88fe5230":"code","d8bf4ab3":"code","77927d6f":"code","1688f477":"code","a17d7902":"code","cd37b638":"code","883afc48":"markdown","a8059be8":"markdown","5eb20c76":"markdown","2d2f7cc4":"markdown","4a93b53b":"markdown","399f72f8":"markdown"},"source":{"851df8c8":"\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","97bccc53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1ec3090":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","63dc27a5":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","8af25582":"woman = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_woman = sum(woman) \/ len(woman)\nprint(f\"% of women who survived: {rate_woman} \")","502d2035":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","07b7b878":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","1991aee7":"train_data.tail()","e40e3b0d":"train_data.info()","223dd1e0":"test_data.info()","e4096ff9":"train_data.describe()","d83a9151":"train_data.describe(include=['O'])","4a70c7e2":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived',ascending=False)","2143b526":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived',ascending=False)","d562c40d":"train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","2c003e2d":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","58a15851":"a = sns.FacetGrid(train_data, col='Survived')\na.map(plt.hist, 'Age', bins=20)","9eb1dd9c":"grid = sns.FacetGrid(train_data, col='Pclass', hue='Survived')\n\n#grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.0, aspect=1.6)\ngrid.map(plt.hist,'Age', alpha=0.5,bins=20)\ngrid.add_legend()","55fc7b67":"#qw = sns.FacetGrid(train_data, col='Embarked')\nqw = sns.FacetGrid(train_data, row='Embarked', size=2.0, aspect=1.6)\nqw.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\nqw.add_legend()","2d0dbe2e":"#qw = sns.FacetGrid(train_data, col=\"Embarked\")\nqw = sns.FacetGrid(train_data, row=\"Embarked\", size=2.2, aspect=1.6)\nqw.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\", palette=\"deep\")\nqw.add_legend()","97802ce2":"qw = sns.FacetGrid(train_data, col=\"Embarked\", hue=\"Survived\", palette={0:'k',1:\"w\"})\nqw.map(sns.barplot,'Sex','Fare', alpha=0.5, ci=None)\nqw.add_legend()","eb501877":"#print(\"Before\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)\n\ntrain_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\n\ncombine = [train_data, test_data]\n\nprint(\"After\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)","10ae6430":"for dataset in combine:\n    dataset[\"Title\"] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\npd.crosstab(train_data['Title'], train_data['Sex'])","26322416":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n#pd.crosstab(train_data['Title'], train_data['Sex'])\ntrain_data[['Title','Survived']].groupby(['Title'],  as_index=False).mean()","89db90a4":"pd.crosstab(train_data['Title'], train_data['Sex'])\n","9d448d04":"title_mapping = {\"Mr\":1, \"Miss\":2, \"Mrs\":3, \"Master\":4,\"Rare\":5}\nfor dataset in combine:\n    dataset[\"Title\"] = dataset[\"Title\"].map(title_mapping)\n    dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\ntrain_data.head()","8f97ec56":"train_data = train_data.drop([\"Name\",\"PassengerId\"], axis=1)\ntest_data = test_data.drop([\"Name\"], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape","503cc7fb":"for dataset in combine:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map({\"female\":1,\"male\":0}).astype(int)\n\ntrain_data.head()","cef14591":"\n# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\nqw = sns.FacetGrid(train_data, row='Pclass', col='Sex', size=2.2, aspect=1.6)\nqw.map(plt.hist, 'Age', alpha=.5, bins=20)\nqw.add_legend()","c62c32eb":"guess_ages = np.zeros((2,3))\nguess_ages ","5aafb67b":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_data.head()","69a9dce0":"train_data[\"AgeBand\"] = pd.cut(train_data[\"Age\"],5)\ntrain_data[[\"AgeBand\",\"Survived\"]].groupby([\"AgeBand\"], as_index=False).mean().sort_values(by=\"AgeBand\",ascending=True)","6bb07267":"for dataset in combine:\n    dataset.loc[dataset[\"Age\"] <= 16, 'Age'] = 0\n    dataset.loc[(dataset[\"Age\"] > 16) & (dataset[\"Age\"] <= 32), \"Age\"] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_data.head()\n\n\n\n    ","c802e6a9":"train_data = train_data.drop(['AgeBand'], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","6f43d08a":"for dataset in combine:\n    dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1 \ntrain_data[[\"FamilySize\",\"Survived\"]].groupby([\"FamilySize\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","ac2dc15e":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","24538033":"train_data = train_data.drop([\"Parch\",\"SibSp\",\"FamilySize\"], axis=1)\ntest_data = test_data.drop([\"Parch\",\"SibSp\",\"FamilySize\"],axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","f592eb8a":"for dataset in combine:\n    dataset[\"Age*Class\"] = dataset.Age * dataset.Pclass\n    \ntrain_data.loc[:, [\"Age*Class\",\"Age\",\"Pclass\"]].head(10)","ab0dc713":"freq_port = train_data.Embarked.dropna().mode()[0]\nfreq_port","7737b48d":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","df64669e":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_data.head()","fe6af54a":"test_data[\"Fare\"].fillna(test_data[\"Fare\"].dropna().median(),inplace=True)\ntest_data.head()","b516b68c":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","1e573b4f":"\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]\n    \ntrain_data.head(10)","32006735":"test_data.head(10)","914df81e":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test = test_data.drop(\"PassengerId\",axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","ebcefe1e":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100,2)\nacc_log","1723462d":"coeff_data = pd.DataFrame(train_data.columns.delete(0))\ncoeff_data.columns = [\"Feature\"]\ncoeff_data[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_data.sort_values(by=\"Correlation\", ascending=False)","e21e776e":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100,2)\n\nacc_svc","88fe5230":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100,2)\nacc_knn","d8bf4ab3":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100,2)\nacc_gaussian","77927d6f":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100,2)\nacc_decision_tree","1688f477":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train,Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100,2)\noutput = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"],\n                      \"Survived\":Y_pred})\noutput.to_csv(\"latest_submission.csv\",index=False)","a17d7902":"models = pd.DataFrame({ \n    \"Model\": [ \"Support Vector Machines\", \"KNN\", \"Logistic Regression\", \"Random Forest\", \"Naive Bayes\", \"Decision Tree\"],\n    \"Score\": [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_decision_tree]})\nmodels.sort_values(by=\"Score\", ascending=False)","cd37b638":"submission = pd.DataFrame({\n                \"PassengerId\":test_data[\"PassengerId\"], \n                \"Survived\":Y_pred\n    \n                    })\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"dene\")\n\n","883afc48":"# Random Forests","a8059be8":"# Support Vector Machines ","5eb20c76":"# Naive Bayes classifers ","2d2f7cc4":"# Logistic Regression","4a93b53b":"# Model Evalution","399f72f8":"# Decision Tree "}}