{"cell_type":{"82c33c1b":"code","70914a33":"code","f7755528":"code","2d53e1b3":"code","c3dd7060":"code","c43ed9d2":"code","f9a8bf4e":"code","54a6c49d":"code","f24adc13":"code","0e26ea68":"code","bd00ce02":"code","b63530a1":"code","5453aaac":"code","f927d848":"code","d543b854":"code","61066501":"code","6a4f8009":"code","ee40cf1c":"code","be54e5c1":"code","1a9cbe68":"code","d1264812":"code","9f1c97a3":"code","7f09289c":"code","3aaa0bc7":"code","f40e54fd":"code","f11637c2":"code","c3adc1f3":"code","d953f3b9":"code","e350d93d":"code","7ecdb90b":"code","1124b21d":"code","831bbe50":"code","d52b6dc2":"code","e524c0ed":"code","841b147f":"code","72f9161a":"code","4934f1d3":"code","b31d2c0e":"code","65c2bfb8":"code","e1789d02":"code","d1630fbf":"code","de6f1035":"code","1e7dd3ef":"markdown","40693bef":"markdown","1d030035":"markdown","cb7a89f5":"markdown","fdb324c7":"markdown","af32cb91":"markdown","0872ce58":"markdown","30e684c7":"markdown","2b53b42c":"markdown"},"source":{"82c33c1b":"ages=[10,20,35,50,28,40,55,18,16,55,30,25,43,18,30,28,14,24,16,17,32,35,26,27,65,18,43,23,21,20,19,70]","70914a33":"len(ages)","f7755528":"import numpy as np\nages_mean=np.mean(ages)\nprint(ages_mean)","2d53e1b3":"sample_size=10\nage_sample=np.random.choice(ages,sample_size)","c3dd7060":"age_sample","c43ed9d2":"from scipy.stats import ttest_1samp","f9a8bf4e":"# This will generate a p value after comparing with the mean of the sample data with the mean of the origianl data\nttest, p_value = ttest_1samp(age_sample, 30)","54a6c49d":"p_value","f24adc13":"if p_value < 0.05:    # alpha value is 0.05 or 5%\n    print(\" we are rejecting null hypothesis\")\nelse:\n    print(\"we are accepting null hypothesis\")","0e26ea68":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport math\nnp.random.seed(6)\nschool_ages=stats.poisson.rvs(loc=18,mu=35,size=1500)\nclassA_ages=stats.poisson.rvs(loc=18,mu=30,size=60)","bd00ce02":"len(school_ages)","b63530a1":"classA_ages.mean()","5453aaac":"_, p_value = ttest_1samp(a = classA_ages, popmean=school_ages.mean())","f927d848":"p_value","d543b854":"school_ages.mean()","61066501":"if p_value < 0.05:    # alpha value is 0.05 or 5%\n    print(\" we are rejecting null hypothesis\")\nelse:\n    print(\"we are accepting null hypothesis\")","6a4f8009":"np.random.seed(12)\nClassB_ages=stats.poisson.rvs(loc=18,mu=33,size=60)\nClassB_ages.mean()","ee40cf1c":"# Comparing two independent classes. Previously we compared \n_,p_value=stats.ttest_ind(a=classA_ages,b=ClassB_ages,equal_var=False)","be54e5c1":"p_value","1a9cbe68":"if p_value < 0.05:    # alpha value is 0.05 or 5%\n    print(\" we are rejecting null hypothesis\")\nelse:\n    print(\"we are accepting null hypothesis\")","d1264812":"weight1=[25,30,28,35,28,34,26,29,30,26,28,32,31,30,45]\nweight2=weight1+stats.norm.rvs(scale=5,loc=-1.25,size=15)","9f1c97a3":"print(weight1)\nprint(weight2)","7f09289c":"weight_df=pd.DataFrame({\"weight_10\":np.array(weight1),\n                         \"weight_20\":np.array(weight2),\n                       \"weight_change\":np.array(weight2)-np.array(weight1)})","3aaa0bc7":"weight_df","f40e54fd":"_, p_value = stats.ttest_rel(a = weight1, b=weight2)","f11637c2":"p_value","c3adc1f3":"if p_value < 0.05:    # alpha value is 0.05 or 5%\n    print(\" we are rejecting null hypothesis\")\nelse:\n    print(\"we are accepting null hypothesis\")","d953f3b9":"import scipy.stats as stats\nimport seaborn as sns","e350d93d":"dataset=sns.load_dataset('tips')","7ecdb90b":"dataset.head()","1124b21d":"dataset_table=pd.crosstab(dataset['sex'],dataset['smoker'])\nprint(dataset_table)","831bbe50":"dataset_table.values","d52b6dc2":"#Observed Values\nObserved_Values = dataset_table.values \nprint(\"Observed Values :-\\n\",Observed_Values)","e524c0ed":"val = stats.chi2_contingency(Observed_Values)","841b147f":"Expected_Values = val[3]","72f9161a":"no_of_rows=len(dataset_table.iloc[0:2,0])\nno_of_columns=len(dataset_table.iloc[0,0:2])\nddof=(no_of_rows-1)*(no_of_columns-1)\nprint(\"Degree of Freedom-:\",ddof)\nalpha = 0.05","4934f1d3":"from scipy.stats import chi2","b31d2c0e":"# One way to find the evidence\nchi_square=sum([(o-e)**2.\/e for o,e in zip(Observed_Values,Expected_Values)])\nchi_square_statistic=chi_square[0]+chi_square[1]","65c2bfb8":"print(\"chi square statistics:-\", chi_square_statistic)","e1789d02":"critical_value=chi2.ppf(q=1-alpha,df=ddof)\nprint('critical_value:',critical_value)","d1630fbf":"#p-value\np_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\nprint('p-value:',p_value)\nprint('Significance level: ',alpha)\nprint('Degree of Freedom: ',ddof)\nprint('p-value:',p_value)","de6f1035":"if chi_square_statistic>=critical_value:\n    print(\"Reject H0,There is a relationship between 2 categorical variables\")\nelse:\n    print(\"Retain H0,There is no relationship between 2 categorical variables\")\n    \nif p_value<=alpha:\n    print(\"Reject H0,There is a relationship between 2 categorical variables\")\nelse:\n    print(\"Retain H0,There is no relationship between 2 categorical variables\")","1e7dd3ef":"### R-squared\n\n- R-squared is a statistical measure of how close the data are to the fitted regression line.\n- R2 = 1- (SSr\/SSt), SSr = Sum of residual or error, SSt = Sum of average Total.\n- The value would be between **0 to 1**.\n- It will increase with addition of new features.\n- The more the value **closer to 1** the better the line fits the data. \n- Value can be less than 0 if the SSr is large (worst fit\/error).\n\n### Adjusted R Square\n\n- Always less than or equal to R - Squared value.\n- It takes the R-squared as the input, but has a term (N-p-1), N = No. of samples, p = Number of predictors.\n- This helps to adjust the R - squared input by dividing. 1 - (1-R2)(N-1)\/(N-p-1).\n- The adj-R-squared value will only be high when ***Independent features are not correlated with the output***.\n- The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.","40693bef":"### Two-sample T-test With Python\n\nTwo-sample T-test With Python\nThe Independent Samples t Test or 2-sample t-test compares the means of **TWO independent groups** in order to determine whether there is statistical evidence that the associated population means are significantly different. The Independent Samples t Test is a parametric test. This test is also known as: Independent t Test","1d030035":"### PDF\n- The y- axis of the PDF representation would consist of percentage of Distribution.\n- It would tell what is the percentage of distribution between certain value of the feature.\n\n### CDF\n- The y- axis of the CDF representation would consist the previous persentage of disribution plus the current distribution.\n- The value can be more than 100 percent.\n- It helps to analyse what percentage of distribution is within certain values. ","cb7a89f5":"### T Test\nT-test has 2 types : 1. one sampled t-test 2. two-sampled t-test.\n\n[Link to the video](https:\/\/www.youtube.com\/watch?v=4-rxTA_5_xA&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=39)\n\n### One-sample T-test with Python\nThe test will tell us whether means of the sample and the population are different\n![t test](t_test.jpg)","fdb324c7":"### Paired T-test With Python\nWhen you want to check how different samples from the same group are, you can go for a paired T-test","af32cb91":"### Hypothesis Test\n\n- It Evaluates two mutual exclusive statement on a **population** using a **sample data**.\n\n#### Steps-\n- Make an initial assumption(H0). **H0 = Null hypothesis**\n- Collect Data or Evidences.\n- Gather Evidence to reject or not reject Null Hypothesis. \n- If H0 is rejected then, H1 will be accepted. **H1 = Alternate Hypothesis**\n- Just as experiment that has two outcomes.\n\n      |                 |        H0       |   H1         |\n      | Do not reject   |      OK         |  Type 2 Error|      \n      | ----------------|-----------------|--------------|                \n      |                 |                 |              |\n      |  Reject         |     Type 1 Error|      OK      |\n\n#### Type 1 Error\n- H0 id to be proven correct but there is no such evidence. So we **Reject** it and consider H1\n\n#### Type 2 Error (more dangerous)\n- For example H0 = Market is going to crash, H1 = Market is not going to crash.\n- We have various evidence for H1 but in real H0 is true.\n- In this case we donot reject and consider H1.\n- This ignores the real fact of H0 and can be dangerous.","0872ce58":"[Link to the video](https:\/\/youtu.be\/w5iKu1IrTJQ)\n\n### Chi-Square Test\nThe test is applied when you have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables.","30e684c7":"[Link to the video](https:\/\/www.youtube.com\/watch?v=YrhlQB3mQFI&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=38)\n\n[Link to the article](https:\/\/www.statisticshowto.com\/probability-and-statistics\/statistics-definitions\/p-value\/)\n### P Value (Significance value)\n- A p value is used in hypothesis testing to help you **support or reject** the null hypothesis. \n- The p value is the evidence **against** a null hypothesis. \n- The smaller the p-value, the stronger the evidence that you should reject the null hypothesis. ***If the P is low, Reject the Ho !***\n\n#### One sample proportion test \n- When we have **ONE categorical features** and we perform the test on it to prove our evidence, we perform this test.\n- If that test gives a value less than the significant value (P <= signi. value) we reject the null hypothesis.\n- This works similar to every test.\n\n#### Chi Square Test\n- When we have **TWO categorical features** and we perform the test on it to prove our evidence, we perform this test.\n\n#### T Test\n- When we have **ONE continuous variable** along with the categorical features and we perform the test on it to prove our evidence, we perform this test.\n\n#### Coorelation\n- When we have **TWO continuous variable** and we perform the test on it to prove our evidence, we perform this test.\n\n####  Anova Test\n- When we have **ONE continuous variable** along with the categorical features and we perform the test on it to prove our evidence, we perform this test.\n- ***The categorical features consists of more than TWO categories***","2b53b42c":"### Some More Examples\nConsider the age of students in a college and in Class A"}}