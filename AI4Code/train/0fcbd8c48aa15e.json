{"cell_type":{"033c2bee":"code","8c84363c":"code","7ee2e150":"code","5c8a9014":"code","d2d9e612":"code","6c9bec24":"code","832fccf1":"code","76bad4b3":"code","22f864f0":"code","cb814660":"code","a8b817f2":"code","be0e871c":"code","b248eabb":"code","af71abbe":"code","bebdcdba":"code","8e8a14de":"code","702b017c":"code","2f43bf25":"code","99a14d0a":"code","8e304d66":"code","6d933f1e":"code","1b577118":"code","2684343c":"code","5b986f3c":"code","e7d13390":"code","0b555511":"code","036efe18":"code","f37f8a7c":"code","931084e5":"code","7b482c77":"code","0df233b9":"code","3e9d9f0f":"code","dcba892f":"code","361c0a98":"code","7e01a01b":"code","a530162b":"code","40b3a413":"code","1242dee9":"code","84dc7c26":"code","669e7f24":"code","c8019747":"code","b279fe3a":"code","ccbd6b0c":"code","8e06df59":"code","3a442795":"code","379d96f3":"code","4d17ac5c":"code","51a05c3a":"code","7058962e":"code","3eb980c1":"code","b8048d05":"code","235637eb":"code","7418b0c3":"code","7c02a998":"code","5e36b97a":"code","7c1b21f3":"code","3e5d81ec":"code","4aaed2f2":"code","406ae2f6":"code","3a4646a3":"code","040f3f32":"code","cdbbd16f":"code","629c2548":"code","d2c35e95":"code","a18fe0d1":"code","2e1bb508":"code","2bc821d0":"code","ed75ce86":"code","b7bd466b":"code","4aabbc9f":"code","2f2bff10":"code","77ade3fe":"code","8bf9fe14":"code","1adb1b8f":"code","d6daf1ce":"code","da2ebc7f":"code","1517b96a":"code","ae26fd7a":"code","5b3b7b15":"code","b3613d72":"code","8f40f43a":"code","5578ebd1":"code","2d4ddc16":"code","fc45099a":"code","d8dcf844":"code","b4e6d20b":"code","71603992":"code","1d52affc":"code","b46b2f86":"code","e3ea2e72":"code","155cc68d":"code","b9bfdb6b":"code","43f75f84":"code","d6410e71":"code","58fa165c":"code","cff194c3":"code","668aeef2":"code","10d23c48":"code","beeb4d8d":"code","e375b011":"code","58479205":"code","51d9e113":"code","9416a7ee":"code","a1f7ca17":"code","d5bdb41f":"code","604d9ef1":"code","5e24e73a":"code","720c5d26":"code","b0d426e3":"code","fcb1d3df":"code","8af5c92f":"code","342caee9":"code","24a0f934":"code","6fa98aa8":"code","05795738":"code","f77d16cd":"code","7ee57bb8":"code","a37ede4d":"code","c3057351":"code","e40bcd07":"code","822e29be":"code","45064257":"code","0e08cfe1":"code","a34fed03":"code","fb9ac1c1":"code","2b1807c9":"code","83c5a7a5":"code","de70f08f":"code","d4fcad68":"code","ccc70e09":"code","8f1c0228":"code","7f413b56":"code","f3bb1496":"code","018e0926":"code","024fd310":"code","e89c702a":"code","4c443c37":"code","a71af66d":"code","2e78a41a":"code","b702b59a":"code","9e4ce083":"code","16d6fa1d":"code","f6fabad9":"code","dd6396b0":"code","52df8f63":"code","2a407d89":"code","437c714b":"code","445cf123":"code","a7bca2ed":"code","61a264e2":"code","ccdddce4":"code","45f340c4":"code","a0444535":"code","1a5edd45":"code","54a47a4a":"code","b4dedfd6":"code","3fb65530":"code","759ea34a":"code","00c6fa5f":"code","480df3b2":"code","5436c48e":"code","4ccc7d16":"code","e3090ec8":"code","081fd143":"code","e1e83ee7":"code","7523b713":"code","c510ff98":"code","e8d45846":"code","1b3e639b":"code","da348539":"code","7f959cdc":"code","c83227cb":"code","9f1a2879":"code","e7a0fef5":"code","0befd04b":"code","fa691671":"code","596ebc49":"code","5c044d7e":"markdown","c0db7a44":"markdown","c9868f9e":"markdown","ec8ed266":"markdown","4be74b2d":"markdown","44f97102":"markdown","ac3fdc4a":"markdown","5403295b":"markdown","1cdf5923":"markdown","6ee5b3b1":"markdown","e6afe8f0":"markdown","176703ab":"markdown","923c5e03":"markdown","d2f0e8df":"markdown","2858ef5c":"markdown","2786821c":"markdown","00aa2f84":"markdown","e1669041":"markdown","1e00ac18":"markdown","0d94c1dd":"markdown","f4ca1c9a":"markdown","f18383d1":"markdown","60556e62":"markdown","768aa33a":"markdown","51521b22":"markdown","6f06e6d0":"markdown","e5d8dc95":"markdown"},"source":{"033c2bee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c84363c":"!pwd\n!ls \/kaggle\/input\/house-prices-advanced-regression-techniques\/\n!cp \/kaggle\/input\/house-prices-advanced-regression-techniques\/* .\n!ls\n#\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\n#\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\n#\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\n#\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv","7ee2e150":"import sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n#!conda install --yes xgboost\nimport xgboost\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#features\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n\n#stats model\nfrom sklearn import datasets, linear_model\nimport statsmodels.formula.api as smf\n\n# Model Selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\n#!conda install --yes lightgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LassoCV, LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n\n# Model Accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, accuracy_score, mean_poisson_deviance, mean_squared_error \n\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.max_rows', None)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n#warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n","5c8a9014":"df_train = pd.read_csv('train.csv')\ndf_test = pd.read_csv('test.csv')","d2d9e612":"df_train_dup = df_train.copy()\ndf_test_dup = df_test.copy()","6c9bec24":"df_train_dup.head()","832fccf1":"df_train_dup.tail()","76bad4b3":"df_train_dup.describe()","22f864f0":"df_train_dup.info()","cb814660":"print(df_train_dup.shape)\nprint(df_test_dup.shape)","a8b817f2":"#print numeric columns and counts\nnum_cols = df_train_dup._get_numeric_data().columns \ndisplay(num_cols)\nlen(num_cols)","be0e871c":"# Drop ID Column\nprint(df_train_dup.shape)\nprint(df_test_dup.shape)\ndf_train_dup = df_train.drop(labels = 'Id',axis=1)\ndf_test_dup = df_test.drop(labels = 'Id',axis=1)\nprint(df_train_dup.shape)\nprint(df_test_dup.shape)","b248eabb":"\ndf_train_dup.hist(bins=50, figsize=(30,25),color='green')\n#save_fig(\"attribute_histogram_plots\")\nplt.show()","af71abbe":"df_train_dup[\"SalePrice\"].head()","bebdcdba":"df_train_dup[\"SalePrice\"].mean()","8e8a14de":"df_train_dup[\"SalePrice\"].describe()","702b017c":"df_train_dup[\"SalePrice\"].mode()","2f43bf25":"plt.figure(figsize = (25,5))\nSalePrice = df_train_dup[\"SalePrice\"]\nplt.hist(SalePrice, bins=80)\nsns.jointplot(data=df_train, x=\"SalePrice\", y=\"Id\")","99a14d0a":"#view of outliers\nsns.boxplot(x=SalePrice,color='lightgreen')\nsns.swarmplot(x=SalePrice,color='skyblue')","8e304d66":"# Printing numeric columns and its count\nnum_cols = df_train_dup._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))\n\n# Drawing box plots to check for outliers\nplt.figure(figsize = (130,15))\ndf_train_dup.boxplot()\n# Drawing box plots to check for outliers\n#plt.figure(figsize = (50,50))\n#for i in enumerate(num_cols):\n#    plt.subplot(13,3,i[0]+1)\n#    sns.boxplot(x=df_train_dup[i[1]],color='lightgreen')\n#    sns.swarmplot(x=df_train_dup[i[1]],color='skyblue')\n#    plt.xlabel(i[1])","6d933f1e":"#looking for missing values\ndf_train_dup.isnull().sum()[df_train_dup.isnull().sum()>0]","1b577118":"#check for duplicate\nduplicate = df_train_dup.duplicated()\nprint(duplicate.sum())","2684343c":"# Printing numeric columns and its count\nnum_cols = df_train_dup._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))","5b986f3c":"# Correlation among features\nfig, ax = plt.subplots(figsize=(35,25))  \nmatrix = np.triu(df_train_dup.corr())\nsns.heatmap(df_train_dup.corr(), annot=True, mask=matrix, ax=ax, cmap=\"YlGnBu\")","e7d13390":"df_train_dup.corr()['SalePrice'][(df_train_dup.corr()['SalePrice']>0.62) & (df_train_dup.corr()['SalePrice']<1)]","0b555511":"Lm = linear_model.LinearRegression()\nLm.fit(df_train_dup[['SalePrice']], df_train_dup['OverallQual'])\ndependent = Lm.predict(df_train_dup[['SalePrice']])\nmean_squared_error(dependent, df_train_dup['OverallQual'])","036efe18":"plt.scatter(df_train_dup['SalePrice'], df_train_dup['OverallQual'],  color='lightgreen')\nplt.plot(df_train_dup['SalePrice'], dependent, color='red', linewidth=1)","f37f8a7c":"Lm = linear_model.LinearRegression()\nLm.fit(df_train_dup[['SalePrice']], df_train_dup['GrLivArea'])\ndependent = Lm.predict(df_train_dup[['SalePrice']])\nmean_squared_error(dependent, df_train_dup['GrLivArea'])","931084e5":"plt.scatter(df_train_dup['SalePrice'], df_train_dup['GrLivArea'],  color='lightgreen')\nplt.plot(df_train_dup['SalePrice'], dependent, color='red', linewidth=1)","7b482c77":"Lm = linear_model.LinearRegression()\nLm.fit(df_train_dup[['SalePrice']], df_train_dup['GarageCars'])\ndependent = Lm.predict(df_train_dup[['SalePrice']])\nmean_squared_error(dependent, df_train_dup['GarageCars'])","0df233b9":"plt.scatter(df_train_dup['SalePrice'], df_train_dup['GarageCars'],  color='lightgreen')\nplt.plot(df_train_dup['SalePrice'], dependent, color='red', linewidth=1)","3e9d9f0f":"Lm = linear_model.LinearRegression()\nLm.fit(df_train_dup[['SalePrice']], df_train_dup['GarageArea'])\ndependent = Lm.predict(df_train_dup[['SalePrice']])\nmean_squared_error(dependent, df_train_dup['GarageArea'])","dcba892f":"plt.scatter(df_train_dup['SalePrice'], df_train_dup['GarageArea'],  color='lightgreen')\nplt.plot(df_train_dup['SalePrice'], dependent, color='red', linewidth=1)","361c0a98":"#investigate potential predictors of the dependant variable (SalePrice)\n#Regression analysis #1 OverallQual\nimport statsmodels.formula.api as smf\nreg1_OverallQual = 'SalePrice~OverallQual'\nreg1_OverallQual_Output = smf.ols(reg1_OverallQual,df_train_dup).fit()\nprint(reg1_OverallQual_Output.summary())","7e01a01b":"#Regression analysis #2 GarageCars\nreg2_GarageCars = 'SalePrice~GarageCars'\nreg2_GarageCars_Output = smf.ols(reg2_GarageCars,df_train_dup).fit()\nprint(reg2_GarageCars_Output.summary())","a530162b":"#Regression analysis #3 GarageArea\nreg3_GarageArea = 'SalePrice~GarageArea'\nreg3_GarageArea_Output = smf.ols(reg3_GarageArea,df_train_dup).fit()\nprint(reg3_GarageArea_Output.summary())","40b3a413":"# Adding all finished Basements\ndf_train_dup['total_BsmtFinSF'] = df_train_dup['BsmtFinSF1'] + df_train_dup['BsmtFinSF2']","1242dee9":"# Year of Last Construction\ndf_train_dup['YearLastConstruction'] = df_train_dup['YearBuilt'] +df_train_dup['YearRemodAdd']","84dc7c26":"# Total Baths Above\ndf_train_dup['total_above_Baths'] = df_train_dup['FullBath'] + (df_train_dup['HalfBath']*.5)","669e7f24":"# Total Baths Above\/Below\ndf_train_dup['total_Baths'] = df_train_dup['total_above_Baths']+df_train_dup['BsmtFullBath']+ (df_train_dup['BsmtHalfBath']*.5)","c8019747":"# Combine total finished basement sq ft with above ground\ndf_train_dup['total_FinSqft'] = df_train_dup['total_BsmtFinSF']+df_train_dup['GrLivArea']","b279fe3a":"#Get Overall Cond and Qual score\ndf_train_dup['OverallCondQualScore'] = df_train_dup['OverallCond'] + df_train_dup['OverallQual']\/2","ccbd6b0c":"#Combining Quality with Rooms\ndf_train_dup['OverallQualityTotRooms'] = df_train_dup['OverallQual']+df_train_dup['TotRmsAbvGrd']","8e06df59":"#Combining Condition with Rooms\ndf_train_dup['OverallConditionTotRooms'] = df_train_dup['OverallCond']+df_train['TotRmsAbvGrd']","3a442795":"df_train_dup.head()","379d96f3":"# Correlation among features\nfig, ax = plt.subplots(figsize=(35,25))  \nmatrix = np.triu(df_train_dup.corr())\nsns.heatmap(df_train_dup.corr(), annot=True, mask=matrix, ax=ax, cmap=\"YlGnBu\")","4d17ac5c":"df_train_dup.corr()['SalePrice'][(df_train_dup.corr()['SalePrice']>0.7) & (df_train_dup.corr()['SalePrice']<1)]","51a05c3a":"reg = 'SalePrice~OverallQual'\nreg_output = smf.ols(reg,df_train_dup).fit()\nprint(reg_output.summary())","7058962e":"reg = 'SalePrice~GrLivArea'\nreg_output = smf.ols(reg,df_train_dup).fit()\nprint(reg_output.summary())","3eb980c1":"reg = 'SalePrice~total_FinSqft'\nreg_output = smf.ols(reg,df_train_dup).fit()\nprint(reg_output.summary())","b8048d05":"reg = 'SalePrice~OverallQualityTotRooms'\nreg_output = smf.ols(reg,df_train_dup).fit()\nprint(reg_output.summary())","235637eb":"# example of a normalization\nfrom numpy import asarray\nfrom sklearn.preprocessing import MinMaxScaler\n# define min max scaler\nscaler = MinMaxScaler()\n\ndf_train_dup_minmax = df_train_dup.copy()\n# Printing numeric columns and its count\nnum_cols = df_train_dup_minmax._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))\n# Printing numeric columns and its count\nnum_cols = df_train_dup._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))\n\n","7418b0c3":"# transform data\ndf_train_dup_minmax[num_cols] = scaler.fit_transform(df_train_dup[num_cols])\ndf_train_dup_minmax[num_cols].head()","7c02a998":"df_train_dup[num_cols].head()","5e36b97a":"plt.figure(figsize = (130,15))\ndf_train_dup.boxplot()","7c1b21f3":"plt.figure(figsize = (130,15))\ndf_train_dup_minmax.boxplot()","3e5d81ec":"# Printing numeric columns and its count\nnum_cols = df_train_dup_minmax._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))\n\n# Drawing box plots to check for outliers\n#plt.figure(figsize = (50,50))\n#for i in enumerate(num_cols):\n#    plt.subplot(13,3,i[0]+1)\n#    sns.boxplot(x=df_train_dup_minmax[i[1]],color='lightgreen')\n#    sns.swarmplot(x=df_train_dup_minmax[i[1]],color='skyblue')\n#    plt.xlabel(i[1])","4aaed2f2":"#Scales the data. Essentially returns the z-scores of every attribute\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale\n\nfrom scipy.stats import zscore\n\ndf_train_dup_stdscaler = df_train_dup.copy()\n# Printing numeric columns and its count\nnum_cols = df_train_dup_stdscaler._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))\n# Printing numeric columns and its count\nnum_cols = df_train_dup._get_numeric_data().columns \ndisplay(num_cols)\nprint(\"Count: \", len(num_cols))","406ae2f6":"for i in num_cols:\n  df_train_dup_stdscaler[i] = std_scale.fit_transform(df_train_dup_stdscaler[[i]])\nprint(df_train_dup_stdscaler.head())\n","3a4646a3":"plt.figure(figsize = (130,15))\ndf_train_dup_stdscaler.boxplot()","040f3f32":"# Drawing box plots to check outliers\n#plt.figure(figsize = (50,50))\n#for i in enumerate(num_cols):\n#    plt.subplot(13,3,i[0]+1)\n#    sns.boxplot(x=df_train_dup_stdscaler[i[1]],color='lightgreen')\n#    sns.swarmplot(x=df_train_dup_stdscaler[i[1]],color='skyblue')\n#    plt.xlabel(i[1])","cdbbd16f":"import sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n#!conda install --yes xgboost\nimport xgboost\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n#features\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n\n#stats model\nfrom sklearn import datasets, linear_model\nimport statsmodels.formula.api as smf\n\n# Model Selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\n#!conda install --yes lightgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LassoCV, LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n\n# Model Accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, accuracy_score, mean_poisson_deviance, mean_squared_error \n\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.max_rows', None)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","629c2548":"df_train = pd.read_csv('train.csv')\ndf_test = pd.read_csv('test.csv')","d2c35e95":"df_train_dup = df_train.copy()\ndf_test_dup = df_test.copy()","a18fe0d1":"#df_train_dup.head()","2e1bb508":"#df_train_dup.tail()","2bc821d0":"#df_train_dup.describe()","ed75ce86":"#df_train_dup.shape[0]","b7bd466b":"#df_train_dup['Id'].describe()\n","4aabbc9f":"# from numpy.random import seed\n# from numpy.random import randint\n\n# seed(13)\n# # generate some integers\n# df_train_dup_id_rand = randint(0,df_train_dup.shape[0]-1,df_train_dup.shape[0])\n# df_train_dup_id_rand","2f2bff10":"#df_train_dup.loc[df_train_dup_id_rand[0]]\n#df_train_dup.loc[0:3,]","77ade3fe":"\n# part_rows_count=int(df_train_dup.shape[0]\/5)\n# print(part_rows_count)\n# row_from=0\n# row_to=part_rows_count\n# df_train_dup_1 = df_train_dup.loc[df_train_dup_id_rand[row_from:row_to],]\n# df_train_dup_1.shape\n","8bf9fe14":"# row_from=part_rows_count\n# row_to=part_rows_count*2\n# df_train_dup_2 = df_train_dup.loc[df_train_dup_id_rand[row_from:row_to],]\n# df_train_dup_2.shape","1adb1b8f":"# row_from=part_rows_count*2\n# row_to=part_rows_count*3\n# df_train_dup_3 = df_train_dup.loc[df_train_dup_id_rand[row_from:row_to],]\n# df_train_dup_3.shape","d6daf1ce":"# row_from=part_rows_count*3\n# row_to=part_rows_count*4\n# df_train_dup_4 = df_train_dup.loc[df_train_dup_id_rand[row_from:row_to],]\n# df_train_dup_4.shape","da2ebc7f":"# row_from=part_rows_count*4\n# row_to=part_rows_count*5\n# df_train_dup_5 = df_train_dup.loc[df_train_dup_id_rand[row_from:row_to],]\n# df_train_dup_5.shape","1517b96a":"# df_train_dup_1.head()","ae26fd7a":"# df_train_dup_1.tail()","5b3b7b15":"# df_train_dup_2.head()","b3613d72":"# df_train_dup_2.tail()","8f40f43a":"# df_train_dup_3.head()","5578ebd1":"# df_train_dup_3.tail()","2d4ddc16":"# df_train_dup_4.head()","fc45099a":"# df_train_dup_4.tail()","d8dcf844":"# df_train_dup_5.head()","b4e6d20b":"# df_train_dup_5.tail()","71603992":"# # devided train data to 5 sub-parts and creating 4 k-fold train and 1 k-fold test data sets with 5 samples to train and test \n# df_train_dup_kfold_1 = df_train_dup_2.append(df_train_dup_3, ignore_index=True)\n# df_train_dup_kfold_1 = df_train_dup_kfold_1.append(df_train_dup_4, ignore_index=True)\n# df_train_dup_kfold_1 = df_train_dup_kfold_1.append(df_train_dup_5, ignore_index=True)\n# df_train_dup_kfold_1.shape","1d52affc":"# df_test_dup_kfold_1 = df_train_dup_1.copy()\n# df_test_dup_kfold_1.shape","b46b2f86":"# df_train_dup_kfold_2 = df_train_dup_1.append(df_train_dup_3, ignore_index=True)\n# df_train_dup_kfold_2 = df_train_dup_kfold_2.append(df_train_dup_4, ignore_index=True)\n# df_train_dup_kfold_2 = df_train_dup_kfold_2.append(df_train_dup_5, ignore_index=True)\n# df_train_dup_kfold_2.shape","e3ea2e72":"# df_test_dup_kfold_2 = df_train_dup_2.copy()\n# df_test_dup_kfold_2.shape","155cc68d":"# df_train_dup_kfold_3 = df_train_dup_1.append(df_train_dup_2, ignore_index=True)\n# df_train_dup_kfold_3 = df_train_dup_kfold_3.append(df_train_dup_4, ignore_index=True)\n# df_train_dup_kfold_3 = df_train_dup_kfold_3.append(df_train_dup_5, ignore_index=True)\n# df_train_dup_kfold_3.shape","b9bfdb6b":"# df_test_dup_kfold_3 = df_train_dup_3.copy()\n# df_test_dup_kfold_3.shape","43f75f84":"# df_train_dup_kfold_4 = df_train_dup_1.append(df_train_dup_2, ignore_index=True)\n# df_train_dup_kfold_4 = df_train_dup_kfold_4.append(df_train_dup_3, ignore_index=True)\n# df_train_dup_kfold_4 = df_train_dup_kfold_4.append(df_train_dup_5, ignore_index=True)\n# df_train_dup_kfold_4.shape","d6410e71":"# df_test_dup_kfold_4 = df_train_dup_4.copy()\n# df_test_dup_kfold_4.shape","58fa165c":"# df_train_dup_kfold_5 = df_train_dup_1.append(df_train_dup_2, ignore_index=True)\n# df_train_dup_kfold_5 = df_train_dup_kfold_5.append(df_train_dup_3, ignore_index=True)\n# df_train_dup_kfold_5 = df_train_dup_kfold_5.append(df_train_dup_4, ignore_index=True)\n# df_train_dup_kfold_5.shape","cff194c3":"# df_test_dup_kfold_5 = df_train_dup_5.copy()\n# df_test_dup_kfold_5.shape","668aeef2":"# Import librairies\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","10d23c48":"# Missing data percentage - Train\nprint(\"Train dataset Missing data: \")\ntotal = df_train_dup.isnull().sum().sort_values(ascending=False)\npercent = (df_train_dup.isnull().sum()*100\/df_train_dup.isnull().count()).sort_values(ascending=False)\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(10)","beeb4d8d":"# Missing data percentage - Test\nprint(\"Test dataset Missing data: \")\ntotal = df_test_dup.isnull().sum().sort_values(ascending=False)\npercent = (df_test_dup.isnull().sum()*100\/df_test_dup.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_test.head(10)","e375b011":"df_train_without_miss = df_train_dup.drop((missing_data_train[missing_data_train['Total'] > 100]).index,1)\ndf_test_without_miss = df_test_dup.drop((missing_data_test[missing_data_test['Total'] > 100]).index,1)","58479205":"# Missing data percentage - Train\nprint(\"Train dataset Missing data: \")\ntotal = df_train_without_miss.isnull().sum().sort_values(ascending=False)\npercent = (df_train_without_miss.isnull().sum()*100\/df_train_without_miss.isnull().count()).sort_values(ascending=False)\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(15)","51d9e113":"# Missing data percentage - Test\nprint(\"Test dataset Missing data: \")\ntotal = df_test_without_miss.isnull().sum().sort_values(ascending=False)\npercent = (df_test_without_miss.isnull().sum()*100\/df_test_without_miss.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_test.head(20)","9416a7ee":"# Check for missing value in any column\ndf_train_without_miss.isnull().sum()[df_train_without_miss.isnull().sum()>0]","a1f7ca17":"df_test_without_miss.isnull().sum()[df_test_without_miss.isnull().sum()>0]","d5bdb41f":"#List of Categorical Columns\ncate_cols = df_train_without_miss.select_dtypes(include = ['O']).columns\ncate_cols","604d9ef1":"#List of Numerical Columns\nnum_cols = df_train_without_miss._get_numeric_data().columns \nnum_cols","5e24e73a":"#Imputing all categorical variable with mode\nfor col in cate_cols:\n    df_train_without_miss[col] = df_train_without_miss[col].fillna(df_train_without_miss[col].mode()[0])","720c5d26":"#Imputing all numerical variable with mean\nfor col in num_cols:\n    df_train_without_miss[col] = df_train_without_miss[col].fillna(df_train_without_miss[col].mean())","b0d426e3":"# Check for missing value in any column\ndf_train_without_miss.isnull().sum()[df_train_without_miss.isnull().sum()>0]","fcb1d3df":"#List of Categorical Columns\ncate_cols = df_test_without_miss.select_dtypes(include = ['O']).columns\ncate_cols\n#List of Numerical Columns\nnum_cols = df_test_without_miss._get_numeric_data().columns \nnum_cols\n#Imputing all categorical variable with mode\nfor col in cate_cols:\n    df_test_without_miss[col] = df_test_without_miss[col].fillna(df_test_without_miss[col].mode()[0])\n#Imputing all numerical variable with mean\nfor col in num_cols:\n    df_test_without_miss[col] = df_test_without_miss[col].fillna(df_test_without_miss[col].mean())\n# Check for missing value in any column\ndf_test_without_miss.isnull().sum()[df_test_without_miss.isnull().sum()>0]","8af5c92f":"df_train_without_miss.shape","342caee9":"df_test_without_miss.shape","24a0f934":"#Combinig train and test data.\nntrain = df_train_without_miss.shape[0]\nntest = df_test_without_miss.shape[0]\ny_train = df_train_without_miss.SalePrice.values\nall_data = pd.concat((df_train_without_miss, df_test_without_miss)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","6fa98aa8":"all_data_encoded = pd.get_dummies(all_data)\nall_data_encoded.shape","05795738":"train_df_reg1=all_data_encoded[0:ntrain]\ntrain_df_reg1.shape","f77d16cd":"test_df_reg1=all_data_encoded[ntrain:]\ntest_df_reg1.shape","7ee57bb8":"#Checking the skewness of Dependent variable and normalising with log\nsns.histplot(x=y_train,bins = 100)\nplt.xlim([0,1000000]);","a37ede4d":"y_train_log = np.log1p(y_train)\nsns.histplot(x=y_train_log,bins = 100);","c3057351":"#Basic linear regression\n\ntrain_df_reg1[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg1, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n\n#Regression\nmodel = LinearRegression()\nmodel.fit(X_train_1, y_train_1)\npredicted_prices_regr = model.predict(X_test_1)\n\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_regr, squared=False)\nrmse\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores)  ","e40bcd07":"X_test_1=test_df_reg1\npredicted_prices_regr = model.predict(X_test_1)\nlen(predicted_prices_regr)","822e29be":"pred_sale_price=np.expm1(predicted_prices_regr)","45064257":"result = pd.DataFrame({'Id': X_test_1.Id, 'SalePrice': pred_sale_price})\nresult.to_csv('result_linear.csv', index=False)\nresult.head()","0e08cfe1":"#Polinomial regression\n\ntrain_df_reg1[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg1, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X_train_1)\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_poly, y_train_1)\npredicted_prices_pol = lin_reg_2.predict(poly_reg.transform(X_test_1))\n\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_pol, squared=False)\nrmse\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 13)\nscores = cross_val_score(lin_reg_2, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 13)\nscores = cross_val_score(lin_reg_2, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores) ","a34fed03":"from sklearn.preprocessing import OrdinalEncoder","fb9ac1c1":"df_train = pd.read_csv('train.csv')\ndf_test = pd.read_csv('test.csv')\ndf_train_dup = df_train.copy()\ndf_test_dup = df_test.copy()","2b1807c9":"df_train_dup.head()","83c5a7a5":"df_test_dup.head()","de70f08f":"# Missing data percentage - Train\nprint(\"Train dataset Missing data: \")\ntotal = df_train_dup.isnull().sum().sort_values(ascending=False)\npercent = (df_train_dup.isnull().sum()*100\/df_train_dup.isnull().count()).sort_values(ascending=False)\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(10)","d4fcad68":"# Missing data percentage - Test\nprint(\"Test dataset Missing data: \")\ntotal = df_test_dup.isnull().sum().sort_values(ascending=False)\npercent = (df_test_dup.isnull().sum()*100\/df_test_dup.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_test.head(10)","ccc70e09":"df_train_without_miss = df_train_dup.drop((missing_data_train[missing_data_train['Percent'] > 10]).index,1)\ndf_test_without_miss = df_test_dup.drop((missing_data_test[missing_data_test['Percent'] > 10]).index,1)","8f1c0228":"# Missing data percentage - Train\nprint(\"Train dataset Missing data: \")\ntotal = df_train_without_miss.isnull().sum().sort_values(ascending=False)\npercent = (df_train_without_miss.isnull().sum()*100\/df_train_without_miss.isnull().count()).sort_values(ascending=False)\nmissing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_train.head(15)","7f413b56":"# Missing data percentage - Test\nprint(\"Test dataset Missing data: \")\ntotal = df_test_without_miss.isnull().sum().sort_values(ascending=False)\npercent = (df_test_without_miss.isnull().sum()*100\/df_test_without_miss.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data_test.head(20)","f3bb1496":"# Check for missing value in any column\ndf_train_without_miss.isnull().sum()[df_train_without_miss.isnull().sum()>0]","018e0926":"df_test_without_miss.isnull().sum()[df_test_without_miss.isnull().sum()>0]","024fd310":"#List of Categorical Columns\ncate_cols = df_train_without_miss.select_dtypes(include = ['O']).columns\ncate_cols","e89c702a":"#List of Numerical Columns\nnum_cols = df_train_without_miss._get_numeric_data().columns \nnum_cols","4c443c37":"#Imputing all categorical variable with mode\nfor col in cate_cols:\n    df_train_without_miss[col] = df_train_without_miss[col].fillna(df_train_without_miss[col].mode()[0])","a71af66d":"#Imputing all numerical variable with mean\nfor col in num_cols:\n    df_train_without_miss[col] = df_train_without_miss[col].fillna(df_train_without_miss[col].median())","2e78a41a":"# Check for missing value in any column\ndf_train_without_miss.isnull().sum()[df_train_without_miss.isnull().sum()>0]","b702b59a":"df_train_without_miss.shape","9e4ce083":"df_train_without_miss.isnull().sum().any()","16d6fa1d":"# Adding all finished Basements\ndf_train_without_miss['total_BsmtFinSF'] = df_train_without_miss['BsmtFinSF1'] + df_train_without_miss['BsmtFinSF2']\n# Year of Last Construction\ndf_train_without_miss['YearLastConstruction'] = df_train_without_miss['YearBuilt'] +df_train_without_miss['YearRemodAdd']\n# Total Baths Above\ndf_train_without_miss['total_above_Baths'] = df_train_without_miss['FullBath'] + (df_train_without_miss['HalfBath']*.5)\n# Total Baths Above\/Below\ndf_train_without_miss['total_Baths'] = df_train_without_miss['total_above_Baths']+df_train_without_miss['BsmtFullBath']+ (df_train_dup['BsmtHalfBath']*.5)\n# Combine total finished basement sq ft with above ground\ndf_train_without_miss['total_FinSqft'] = df_train_without_miss['total_BsmtFinSF']+df_train_without_miss['GrLivArea']\n#Get Overall Cond and Qual score\ndf_train_without_miss['OverallCondQualScore'] = df_train_without_miss['OverallCond'] + df_train_without_miss['OverallQual']\/2\n#Combining Quality with Rooms\ndf_train_without_miss['OverallQualityTotRooms'] = df_train_without_miss['OverallQual']+df_train_without_miss['TotRmsAbvGrd']\n#Combining Condition with Rooms\ndf_train_without_miss['OverallConditionTotRooms'] = df_train_without_miss['OverallCond']+df_train_without_miss['TotRmsAbvGrd']\ndf_train_without_miss.head()","f6fabad9":"# Adding all finished Basements\ndf_test_without_miss['total_BsmtFinSF'] = df_test_without_miss['BsmtFinSF1'] + df_test_without_miss['BsmtFinSF2']\n# Year of Last Construction\ndf_test_without_miss['YearLastConstruction'] = df_test_without_miss['YearBuilt'] +df_test_without_miss['YearRemodAdd']\n# Total Baths Above\ndf_test_without_miss['total_above_Baths'] = df_test_without_miss['FullBath'] + (df_test_without_miss['HalfBath']*.5)\n# Total Baths Above\/Below\ndf_test_without_miss['total_Baths'] = df_test_without_miss['total_above_Baths']+df_test_without_miss['BsmtFullBath']+ (df_train_dup['BsmtHalfBath']*.5)\n# Combine total finished basement sq ft with above ground\ndf_test_without_miss['total_FinSqft'] = df_test_without_miss['total_BsmtFinSF']+df_test_without_miss['GrLivArea']\n#Get Overall Cond and Qual score\ndf_test_without_miss['OverallCondQualScore'] = df_test_without_miss['OverallCond'] + df_test_without_miss['OverallQual']\/2\n#Combining Quality with Rooms\ndf_test_without_miss['OverallQualityTotRooms'] = df_test_without_miss['OverallQual']+df_test_without_miss['TotRmsAbvGrd']\n#Combining Condition with Rooms\ndf_test_without_miss['OverallConditionTotRooms'] = df_test_without_miss['OverallCond']+df_test_without_miss['TotRmsAbvGrd']\ndf_test_without_miss.head()","dd6396b0":"#Null value treatment for test data\n#List of Categorical Columns\ncate_cols = df_test_without_miss.select_dtypes(include = ['O']).columns\nprint(cate_cols)\n#List of Numerical Columns\nnum_cols = df_test_without_miss._get_numeric_data().columns \nprint(num_cols)\n#Imputing all categorical variable with mode\nfor col in cate_cols:\n    df_test_without_miss[col] = df_test_without_miss[col].fillna(df_test_without_miss[col].mode()[0])\n#Imputing all numerical variable with mean\nfor col in num_cols:\n    df_test_without_miss[col] = df_test_without_miss[col].fillna(df_test_without_miss[col].median())\n# Check for missing value in any column\ndf_test_without_miss.isnull().sum()[df_test_without_miss.isnull().sum()>0]\nprint(df_test_without_miss.shape)\nprint(df_test_without_miss.isnull().sum().any())","52df8f63":"df_train_without_miss.shape","2a407d89":"df_test_without_miss.shape","437c714b":"#Combinig train and test data.\nntrain = df_train_without_miss.shape[0]\nntest = df_test_without_miss.shape[0]\ny_train = df_train_without_miss.SalePrice.values\nall_data = pd.concat((df_train_without_miss, df_test_without_miss)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data.drop(['Id'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","445cf123":"#enc = OrdinalEncoder()\n#X = all_data\n#enc.fit(X)\n#enc.categories_\n#all_data_encoded = enc.transform(all_data)","a7bca2ed":"all_data.head()","61a264e2":"#for col in cate_cols:\n#    all_data_encoded[col]=all_data[col].astype('category').cat.codes\n#all_data_encoded.head()","ccdddce4":"#Doing the label encoding to convert categorical data to numeric\nall_data_encoded=all_data.copy()\nle = preprocessing.LabelEncoder()\n#print(all_data[cate_cols[0]].describe())\n#print(all_data[cate_cols[0]].astype('category').cat.codes.describe())\nfor col in cate_cols:\n    le.fit(all_data[col])\n    all_data_encoded[col]=le.transform(all_data[col])","45f340c4":"all_data_encoded.shape","a0444535":"all_data_encoded.head()","1a5edd45":"all_data_encoded.hist(bins=100, figsize=(30,25),color='green')","54a47a4a":"skewness=all_data_encoded.skew(axis=0)\nprint(skewness.describe())\nprint(skewness.head())","b4dedfd6":"all_col=all_data_encoded.columns\nfor col in all_col:\n    skewness_col=all_data_encoded[col].skew(axis=0)\n    if ((skewness_col > skewness.describe()[6]) or (skewness_col < skewness.describe()[4])):\n        all_data_encoded[col]=np.log1p(all_data_encoded[col])","3fb65530":"skewness=all_data_encoded.skew(axis=0)\nprint(skewness.describe())\nprint(skewness.head())\nprint(skewness.describe()[4])\nprint(skewness.describe()[6])\nall_data_encoded[all_col[0]].skew(axis=0)","759ea34a":"all_data_encoded.hist(bins=100, figsize=(30,25),color='green')","00c6fa5f":"#Checking the skewness of Dependent variable and normalising with log\nsns.histplot(x=y_train,bins = 100)\nplt.xlim([0,1000000]);","480df3b2":"y_train_log = np.log1p(y_train)\nsns.histplot(x=y_train_log,bins = 100);","5436c48e":"# Checking the Correlation variables\ncorrmat = all_data_encoded.corr()\nfig, ax = plt.subplots(figsize=(25,25))  \nmatrix = np.triu(all_data_encoded.corr())\ncolor = sns.color_palette(\"YlGnBu\", as_cmap=True)\nsns.set(font_scale=1)\nsns.heatmap(corrmat[(corrmat >= 0.6) | (corrmat <= -0.4)], annot=True,vmax=.8, mask=matrix, ax=ax,cmap = color,linewidths=0.1);","4ccc7d16":"all_data_encoded.head()","e3090ec8":"all_data_encoded_minmax=all_data_encoded.copy()\nall_col=all_data_encoded.columns\nall_col\n# define min max scaler\nscaler = MinMaxScaler()\nall_data_encoded_minmax[all_col] = scaler.fit_transform(all_data_encoded[all_col])\nall_data_encoded_minmax[all_col].head()","081fd143":"skewness=all_data_encoded_minmax.skew(axis=0)\nprint(skewness.describe())\nprint(skewness.head())","e1e83ee7":"# Checking the Correlation variables\ncorrmat = all_data_encoded_minmax.corr()\nfig, ax = plt.subplots(figsize=(25,25))  \nmatrix = np.triu(all_data_encoded_minmax.corr())\ncolor = sns.color_palette(\"YlGnBu\", as_cmap=True)\nsns.set(font_scale=1)\nsns.heatmap(corrmat[(corrmat >= 0.6) | (corrmat <= -0.4)], annot=True,vmax=.8, mask=matrix, ax=ax,cmap = color,linewidths=0.1);","7523b713":"all_data_encoded_minmax.hist(bins=100, figsize=(30,25),color='green')","c510ff98":"plt.figure(figsize = (130,15))\nall_data.boxplot()","e8d45846":"plt.figure(figsize = (130,15))\nall_data_encoded.boxplot()","1b3e639b":"plt.figure(figsize = (130,15))\nall_data_encoded_minmax.boxplot()","da348539":"all_data_encoded_minmax.shape","7f959cdc":"train_df_reg=all_data_encoded_minmax[0:ntrain]\ntrain_df_reg.shape","c83227cb":"test_df_reg=all_data_encoded_minmax[ntrain:]\ntest_df_reg.shape","9f1a2879":"#Basic linear regression\ntrain_df_reg[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n#Regression\nmodel = LinearRegression()\nmodel.fit(X_train_1, y_train_1)\npredicted_prices_regr = model.predict(X_test_1)\n\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_regr, squared=False)\nrmse\n\n# create a KFold object with 10 splits \nfolds = KFold(n_splits = 10, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 10 splits \nfolds = KFold(n_splits = 10, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores)  \n# Let us explore the coefficients for each of the independent attributes\nprint(\"Coefficients=\",model.coef_)\n#checking the magnitude of coefficients\npredictors = X_train_1.columns\ncoef = pd.Series(model.coef_.flatten(), predictors).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients', figsize=(15,5));\n# R square on training data\nprint(\"R square on training data:\",model.score(X_train_1, y_train_1))\nprint(\"R square on training data:\",model.score(X_test_1, y_test_1))\nRMSE_lr_train = metrics.mean_squared_error(y_true = y_train_1, y_pred=model.predict(X_train_1),squared=False)\nRMSE_lr_test = metrics.mean_squared_error(y_true = y_test_1, y_pred=model.predict(X_test_1), squared=False)\nresultsDf = pd.DataFrame({'Train RMSE': [RMSE_lr_train],'Test RMSE': [RMSE_lr_test]},index=['LinearRegression'])\nprint(resultsDf)","e7a0fef5":"# Predicting on actual test data\nX_test_1=test_df_reg\npredicted_prices_regr = model.predict(X_test_1)\nlen(predicted_prices_regr)\npred_sale_price=np.expm1(predicted_prices_regr)\nresult = pd.DataFrame({'Id': df_test_without_miss.Id, 'SalePrice': pred_sale_price})\nresult.to_csv('result_linear.csv', index=False)\nresult.head()","0befd04b":"# Modeling with ridge regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV, LeaveOneOut\nfrom sklearn.linear_model import RidgeCV","fa691671":"#Ridge regression\ntrain_df_reg[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n#Regression\nmodel = RidgeCV(alphas=(list(np.arange(0.25,0.3,0.01))),scoring='neg_mean_squared_error',cv=LeaveOneOut())\nmodel.fit(X_train_1, y_train_1)\npredicted_prices_regr = model.predict(X_test_1)\n\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_regr, squared=False)\nrmse\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores)  \n# Let us explore the coefficients for each of the independent attributes\nprint(\"Coefficients=\",model.coef_)\n#checking the magnitude of coefficients\npredictors = X_train_1.columns\ncoef = pd.Series(model.coef_.flatten(), predictors).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients', figsize=(15,5));\n# R square on training data\nprint(\"R square on train data:\",model.score(X_train_1, y_train_1))\nprint(\"R square on test data:\",model.score(X_test_1, y_test_1))\nRMSE_lr_train = metrics.mean_squared_error(y_true = y_train_1, y_pred=model.predict(X_train_1),squared=False)\nRMSE_lr_test = metrics.mean_squared_error(y_true = y_test_1, y_pred=model.predict(X_test_1), squared=False)\nresultsDf = pd.DataFrame({'Train RMSE': [RMSE_lr_train],'Test RMSE': [RMSE_lr_test]},index=['RidgeRegression'])\nprint(resultsDf)\nprint(\"alpha=\",model.alpha_)","596ebc49":"# Predicting on actual test data\nX_test_1=test_df_reg\npredicted_prices_regr = model.predict(X_test_1)\nlen(predicted_prices_regr)\npred_sale_price=np.expm1(predicted_prices_regr)\nresult = pd.DataFrame({'Id': df_test_without_miss.Id, 'SalePrice': pred_sale_price})\nresult.to_csv('result_ridge.csv', index=False)\nresult.head()","5c044d7e":"#Lasso regression (have some issue in kaggle but running fine in jupyter)\ntrain_df_reg[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n#Finding best alpha for lasso \nlasso=Lasso()\nparameters={'alpha':[1e-10,1e-9,1e-8,1e10,1e9,1e8]}\n\nlasso_model = GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=LeaveOneOut(),n_jobs=-1)\nlasso_model.fit(X_train_1, y_train_1)\nbest_lasso_alpha=lasso_model.best_params_\nprint(\"alpha=\",best_lasso_alpha)\n#Lasso Regression\nmodel=Lasso(alpha=best_lasso_alpha['alpha'])\nmodel.fit(X_train_1, y_train_1)\npredicted_prices_regr = model.predict(X_test_1)\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_regr, squared=False)\nrmse\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores)  \n# Let us explore the coefficients for each of the independent attributes\nprint(\"Coefficients=\",model.coef_)\n#checking the magnitude of coefficients\npredictors = X_train_1.columns\ncoef = pd.Series(model.coef_.flatten(), predictors).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients', figsize=(15,5));\n# R square on training data\nprint(\"R square on train data:\",model.score(X_train_1, y_train_1))\nprint(\"R square on test data:\",model.score(X_test_1, y_test_1))\nRMSE_lr_train = metrics.mean_squared_error(y_true = y_train_1, y_pred=model.predict(X_train_1),squared=False)\nRMSE_lr_test = metrics.mean_squared_error(y_true = y_test_1, y_pred=model.predict(X_test_1), squared=False)\nresultsDf = pd.DataFrame({'Train RMSE': [RMSE_lr_train],'Test RMSE': [RMSE_lr_test]},index=['LassoRegression'])\nprint(resultsDf)","c0db7a44":"Looking for duplicates","c9868f9e":"### 3. Investigate at least three potential predictors of the dependent variable and provide appropriate graphs \/ statistics to demonstrate the relationships.","ec8ed266":"From OLS analysis also OverallQual is better predictor for saleprice as of now. ","4be74b2d":"Right skewed. The possible outliers are above $400,000","44f97102":"### 1. Provide appropriate descriptive statistics and visualizations to help understand the marginal distribution of the dependent variable.","ac3fdc4a":"Look at Distribution","5403295b":"### 2. Investigate missing data and outliers.","1cdf5923":"## week 2 improving the previous model","6ee5b3b1":"#Optimizing for alpha\nfrom sklearn.linear_model import Lasso,LassoCV","e6afe8f0":"# Predicting on actual test data\nX_test_1=test_df_reg\npredicted_prices_regr = model.predict(X_test_1)\nlen(predicted_prices_regr)\npred_sale_price=np.expm1(predicted_prices_regr)\nresult = pd.DataFrame({'Id': df_test_without_miss.Id, 'SalePrice': pred_sale_price})\nresult.to_csv('result_elastic.csv', index=False)\nresult.head()","176703ab":"Part of Data Prep","923c5e03":"Upload and Read Data","d2f0e8df":"Run Correlation now that we have added new features","2858ef5c":"#Elasticnet regression (have some issue in kaggle but running fine in jupyter)\ntrain_df_reg[\"SalesPrice\"] = y_train_log\n# train-test 80-20 split\ndf_train_reg, df_test_reg = train_test_split(train_df_reg, \n                                     train_size = 0.8, \n                                     test_size = 0.2, \n                                     random_state = 13)\n\n# Divide into X_train, y_train, X_test, y_test\ny_train_1 = df_train_reg.pop('SalesPrice')\nX_train_1 = df_train_reg\n\ny_test_1 = df_test_reg.pop('SalesPrice')\nX_test_1 = df_test_reg\n#Finding best alpha for lasso \nElasticnet=ElasticNet()\nparameters={'alpha':[1e-10,1e-9,1e-8,1e10,1e9,1e8],\n            'l1_ratio':[0.001,0.100,0.189,0.278,0.367,0.456,0.544,0.633,0.722,0.811,0.900,0.999]}\n\nelastic_model=GridSearchCV(Elasticnet,parameters,scoring='neg_mean_squared_error',cv=LeaveOneOut(),n_jobs=-1)\nelastic_model.fit(X_train_1,y_train_1)\n\nbest_elast_param=elastic_model.best_params_\nprint(\"alpha & l1_ratio=\",best_elast_param)\n#Elasticnet Regression\nmodel=ElasticNet(alpha=best_elast_param['alpha'],l1_ratio=best_elast_param['l1_ratio'])\nmodel.fit(X_train_1, y_train_1)\npredicted_prices_regr = model.predict(X_test_1)\n#RMSE\nrmse = mean_squared_error(y_test_1, predicted_prices_regr, squared=False)\nrmse\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='r2', cv=folds)\nprint(\"R^2 Scores: \",scores)\n\n# create a KFold object with 2 splits \nfolds = KFold(n_splits = 2, shuffle = True, random_state = 13)\nscores = cross_val_score(model, X_train_1, y_train_1, scoring='neg_root_mean_squared_error', cv=folds)\nprint(\"\")\nprint(\"RMSE Scores: \",scores)  \n# Let us explore the coefficients for each of the independent attributes\nprint(\"Coefficients=\",model.coef_)\n#checking the magnitude of coefficients\npredictors = X_train_1.columns\ncoef = pd.Series(model.coef_.flatten(), predictors).sort_values()\ncoef.plot(kind='bar', title='Model Coefficients', figsize=(15,5));\n# R square on training data\nprint(\"R square on train data:\",model.score(X_train_1, y_train_1))\nprint(\"R square on test data:\",model.score(X_test_1, y_test_1))\nRMSE_lr_train = metrics.mean_squared_error(y_true = y_train_1, y_pred=model.predict(X_train_1),squared=False)\nRMSE_lr_test = metrics.mean_squared_error(y_true = y_test_1, y_pred=model.predict(X_test_1), squared=False)\nresultsDf = pd.DataFrame({'Train RMSE': [RMSE_lr_train],'Test RMSE': [RMSE_lr_test]},index=['ElasticnetRegression'])\nprint(resultsDf)","2786821c":"From boxplots it seems many variables have outliers.","00aa2f84":"After Scaling in the boxplots you can see those features with biggest outliers: Mis Value (#1), followed by Lot Area,Pool Areas, and 3SsnPorch are some others with noticeable outliers","e1669041":"# Predicting on actual test data\nX_test_1=test_df_reg\npredicted_prices_regr = model.predict(X_test_1)\nlen(predicted_prices_regr)\npred_sale_price=np.expm1(predicted_prices_regr)\nresult = pd.DataFrame({'Id': df_test_without_miss.Id, 'SalePrice': pred_sale_price})\nresult.to_csv('result_lasso.csv', index=False)\nresult.head()","1e00ac18":"Comparing GarageArea, GarageCars and OverallQual. \nR-squared value for OverallQual is more close to 1 that is 0.626 is the highest among 3. ","0d94c1dd":"from sklearn.linear_model import ElasticNet","f4ca1c9a":"After doing merging of some features, OverallQual is a strong predictor with correlation 0.79 and also GrLivArea, \ntotal_FinSqft and OverallQualityTotRooms also also signifantly more than 0.7","f18383d1":"### 4. Engage in feature creation by splitting, merging, or otherwise generating a new predictor.","60556e62":"Above 'GarageCars' is a possible predictor, better predictor than 'OverallQual'","768aa33a":"Mean: $180,921.20\n","51521b22":"### 5. Using the dependent variable, perform both min-max and standard scaling in Python.","6f06e6d0":"## week 3 improving the previous model","e5d8dc95":"## week 1 starting initial submission"}}