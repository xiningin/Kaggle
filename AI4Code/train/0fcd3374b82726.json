{"cell_type":{"08cc62f8":"code","7db64d43":"code","477dbe5a":"code","c09a9613":"code","6c6c0e49":"code","004f416d":"code","8fbdda02":"code","1f491ece":"code","ec4a2cbf":"code","80839892":"code","ce66364c":"code","36da8edc":"code","0ec720e2":"code","5f554689":"code","1445b71b":"code","d97c88cc":"code","743718f7":"code","7bd90f22":"code","9a9e1c5e":"code","9007dded":"code","9c7dba6f":"code","d0eaa802":"code","05d554d8":"markdown","11a20a91":"markdown","85ff8faf":"markdown","9931d914":"markdown","96c1d814":"markdown","328bf0b6":"markdown","713ae1f0":"markdown","8cf5731e":"markdown","24fc10f6":"markdown","1ee0801c":"markdown","2045131c":"markdown"},"source":{"08cc62f8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pylab import *\nimport os\n","7db64d43":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata=pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\ndata.head(5)","477dbe5a":"#data=pd.read_csv(\"insurance.csv\")\n","c09a9613":"data.dtypes","6c6c0e49":"data[\"charges\"]=data[\"charges\"].astype(int)","004f416d":"data","8fbdda02":"x=data.iloc[:,:1].values\ny=data.iloc[:,-1].values","1f491ece":"#x=x.reshape(len(x),1)\n#y=y.reshape(len(y),1)","ec4a2cbf":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)","80839892":"x_train[:3]","ce66364c":"x_test[:3]","36da8edc":"y_train[:3]","0ec720e2":"y_test[:2]","5f554689":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n","1445b71b":"x_train[:3]\n","d97c88cc":"x_test[:3]","743718f7":"from sklearn.svm import SVC\nfrom sklearn import *\nclassifier=SVC(kernel=\"linear\", random_state=0)\nclassifier.fit(x_train, y_train)","7bd90f22":"print(classifier.predict(sc.transform([[19]])))","9a9e1c5e":"y_pred=classifier.predict(x_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test),1)),1))","9007dded":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm=confusion_matrix(y_test, y_pred)\ncm\n#accuracy_score(y_test, y_pred)","9c7dba6f":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(x_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","d0eaa802":"from matplotlib.colors import ListedColormap\nX_set, y_set = sc.inverse_transform(x_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","05d554d8":"## Importing the dataset","11a20a91":"## Visualising the Training set results","85ff8faf":"## Training the SVM model on the Training set","9931d914":"## Predicting a new result","96c1d814":"## Visualising the Test set results","328bf0b6":"## Feature Scaling","713ae1f0":"## Importing the libraries","8cf5731e":"## Making the Confusion Matrix","24fc10f6":"## Splitting the dataset into the Training set and Test set","1ee0801c":"# Support Vector Machine (SVM)","2045131c":"## Predicting the Test set results"}}