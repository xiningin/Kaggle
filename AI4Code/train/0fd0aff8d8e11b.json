{"cell_type":{"2f7df400":"code","92cba4d6":"code","99ab3b48":"code","f446aab1":"code","6786e664":"code","a0db7c8c":"code","0d35e2cd":"code","64f9a9e8":"code","f8b2a098":"code","446d4726":"code","f944c04a":"code","b4ea305a":"code","44f91440":"code","2904ccaa":"code","c366c28e":"code","518c1641":"code","2605cb04":"code","cb98ec86":"code","92ff3a81":"code","ea274e2d":"code","e931f32f":"code","996097c4":"code","8017f34b":"code","8894d3e5":"code","6d210df9":"code","d3f372d6":"code","2082e608":"code","e0fadc0a":"code","7330125f":"code","7dbc7e75":"code","d099eb79":"code","26531c23":"code","0e3724bc":"code","45b755ae":"code","ce5559ea":"code","84f390da":"code","5e3db7bb":"code","d6611604":"code","815fdfa9":"code","5148776d":"code","5dd87a4d":"code","d552820f":"code","d3e86b9c":"code","d196da59":"code","299cc7d5":"code","ceeb6a8e":"code","3971695a":"code","c9f70528":"code","f59746a4":"code","2dab8df0":"code","e1cebc4c":"code","2b3c003a":"code","ba5519d1":"code","b2469ea7":"code","f1649d09":"code","f07b6902":"code","9ddbbad0":"code","031d4786":"code","511494c4":"code","f728aaef":"code","1b7f4677":"code","5cf9008d":"code","40420d57":"code","875cda3d":"code","c4e0f595":"code","294f9990":"code","91aa9572":"code","c036143f":"code","e48cdeca":"markdown","21f612e9":"markdown","c372aed0":"markdown","e72c1e29":"markdown","d196034e":"markdown","abd7b8f3":"markdown","5d360f17":"markdown","e6e1d08e":"markdown","e5a76129":"markdown","3363191c":"markdown","e35a2916":"markdown","96f5202b":"markdown","3c70c1d3":"markdown","0ce1b79a":"markdown","fe130c47":"markdown","2a369187":"markdown","1fc219ab":"markdown"},"source":{"2f7df400":"import pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport optuna\nfrom sklearn.metrics import log_loss\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nsns.set(color_codes=True)\npal = sns.color_palette(\"viridis\", 10)\nsns.set_palette(pal)","92cba4d6":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","99ab3b48":"train.info()","f446aab1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])","6786e664":"train.isnull().sum()","a0db7c8c":"!pip install dataprep","0d35e2cd":"from dataprep.eda import plot, plot_correlation, create_report, plot_missing","64f9a9e8":"plot(train.drop(['id','target'],axis=1))","f8b2a098":"# create_report(train)","446d4726":"X = train.drop(['id','target'],axis=1)\ny = train['target']","f944c04a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test  = train_test_split(X,y,train_size=0.8,random_state=42)","b4ea305a":"from catboost import CatBoostClassifier, Pool\ntrain_pool = Pool(data=X_train, label=y_train)\ntest_pool = Pool(data=X_test, label=y_test.values) ","44f91440":"model = CatBoostClassifier(\n    loss_function='MultiClass',\n    eval_metric='MultiClass',\n    verbose=False\n)\nmodel.fit(train_pool,plot=True,eval_set=test_pool)","2904ccaa":"y_pred = model.predict_proba(X_test)\nlog_loss(y_test,y_pred)","c366c28e":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","518c1641":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (8,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')","2605cb04":"a = perm.feature_importances_\nl = []\nfor i in range(50):\n    if a[i]<0:\n        l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","cb98ec86":"train_new = train.drop(l,axis=1)\ntest_new =test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","92ff3a81":"def fun(trial,data=X_new,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'loss_function': 'MultiClass',\n        'eval_metric': 'MultiClass',\n        'learning_rate' : trial.suggest_uniform('learning_rate',1e-3,0.1),\n        \n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,30),\n        'subsample': trial.suggest_uniform('subsample',0,1),\n        'random_strength': trial.suggest_uniform('random_strength',0,1),\n        'depth': trial.suggest_int('depth',5,12),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,100),\n        'num_leaves' : trial.suggest_int('num_leaves',16,64),\n        'leaf_estimation_method' : 'Newton',\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,10),\n        'verbose':False,\n        'bootstrap_type': 'Bernoulli',\n        'random_state' : trial.suggest_categorical('random_state',[13]),\n        'task_type' : 'GPU',\n        'grow_policy' : 'Lossguide'\n        \n    }\n    model = CatBoostClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    preds = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, preds)\n    \n    return ll","ea274e2d":"study = optuna.create_study(direction='minimize')\nstudy.optimize(fun, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","e931f32f":"best_params_cb = study.best_params\nbest_params_cb['loss_function'] = 'MultiClass'\nbest_params_cb['eval_metric'] = 'MultiClass'\nbest_params_cb['verbose'] = False\nbest_params_cb['n_estimators'] = 10000\nbest_params_cb['bootstrap_type']= 'Bernoulli'\nbest_params_cb['leaf_estimation_method'] = 'Newton'\nbest_params_cb['task_type'] = 'GPU'\nbest_params_cb['grow_policy'] = 'Lossguide'","996097c4":"stacked_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class1m3', 'Class2m3','Class3m3','Class4m3','target'])","8017f34b":"columns = train_new.drop(['id','target'],axis=1).columns\ncb_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','target'])\npreds = np.zeros((test.shape[0],4))\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = CatBoostClassifier(**best_params_cb)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=500,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns=['Class1m1', 'Class2m1','Class3m1','Class4m1'])\n    df['target'] = list(y_val)\n    \n    cb_df = pd.concat([cb_df,df])\n    preds+=model.predict_proba(test_new.drop(['id'],axis=1))\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","8894d3e5":"cb_df","6d210df9":"np.mean(ll)","d3f372d6":"df_kfold = pd.DataFrame(preds,columns=['Class_1','Class_2','Class_3','Class_4'])\ndf_kfold['id']  = test['id']\ndf_kfold = df_kfold[['id','Class_1','Class_2','Class_3','Class_4']]","2082e608":"df_kfold","e0fadc0a":"output_3 = df_kfold.to_csv('submit_3.csv',index=False)","7330125f":"from lightgbm import LGBMClassifier","7dbc7e75":"model = LGBMClassifier(random_state= 13, objective= 'multiclass', metric = 'multi_logloss').fit(X_train, y_train)","d099eb79":"perm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","26531c23":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (8,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')\n","0e3724bc":"a = perm.feature_importances_\nl = []\nfor i in range(50):\n    if a[i]<0:\n        l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","45b755ae":"train_new = train.drop(l,axis=1)\ntest_new =test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","ce5559ea":"def fun2(trial, data = X_new, target=y):\n    train_x, test_x, train_y, test_y = train_test_split(data,target,train_size=0.8,random_state=42)\n    param = {\n         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 30.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 30.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n\n        'subsample': trial.suggest_uniform('subsample', 0,1),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0, 0.1 ),\n        'max_depth': trial.suggest_int('max_depth', 1,100),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100),\n        'cat_l2': trial.suggest_int('cat_l2',1,20),\n        'metric': 'multi_logloss', \n        'random_state' : trial.suggest_categorical('random_state',[13]),\n        'n_estimators': 10000,\n        'objective': 'multiclass',\n        'device_type':'gpu'\n        \n    }\n    model = LGBMClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    pred = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, pred)\n    \n    return ll","84f390da":"study_2 = optuna.create_study(direction='minimize')\nstudy_2.optimize(fun2, n_trials=100)\nprint('Number of finished trials:', len(study_2.trials))\nprint('Best trial:', study_2.best_trial.params)","5e3db7bb":"best_params_lgbm = study_2.best_params\nbest_params_lgbm['objective'] = 'multiclass'\nbest_params_lgbm['metric'] = 'multi_logloss'\nbest_params_lgbm['n_estimators'] = 10000\nbest_params_lgbm['device_type'] : 'gpu'","d6611604":"columns = train_new.drop(['id','target'],axis=1).columns\npreds_2 = np.zeros((test.shape[0],4))\nlgbm_df = pd.DataFrame(columns = ['Class1m2', 'Class2m2','Class3m2','Class4m2','target'])\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = LGBMClassifier(**best_params_lgbm)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=500,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns=['Class1m2', 'Class2m2','Class3m2','Class4m2'])\n    df['target'] = list(y_val)\n    \n    lgbm_df = pd.concat([lgbm_df,df])\n    preds_2+=model.predict_proba(test_new.drop(['id'],axis=1))\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","815fdfa9":"lgbm_df","5148776d":"np.mean(ll)","5dd87a4d":"df_kfold_lgbm = pd.DataFrame(preds_2,columns=['Class_1','Class_2','Class_3','Class_4'])\ndf_kfold_lgbm['id']  = test['id']\ndf_kfold_lgbm = df_kfold_lgbm[['id','Class_1','Class_2','Class_3','Class_4']]","d552820f":"df_kfold_lgbm","d3e86b9c":"output_5 = df_kfold_lgbm.to_csv('submit_5.csv',index=False)","d196da59":"from xgboost import XGBClassifier","299cc7d5":"model = XGBClassifier(random_State=13).fit(X_train, y_train)\nperm = PermutationImportance(model, random_state=13, scoring = 'neg_log_loss')\nperm.fit(X_test,y_test)","ceeb6a8e":"feat_importance = pd.DataFrame({'Feature':X_train.columns, 'Importance':perm.feature_importances_}).sort_values(by='Importance',ascending=False)\nplt.figure(figsize= (8,15))\nsns.barplot(data = feat_importance, y = 'Feature', x= 'Importance',orient='h')\n","3971695a":"a = perm.feature_importances_\nl = []\nfor i in range(50):\n    if a[i]<0:\n        l.append('feature_'+str(i))\n        \nprint('Dropped Features')\nprint(l)","c9f70528":"train_new = train.drop(l,axis=1)\ntest_new =test.drop(l,axis=1)\nX_new = train_new.drop(['id','target'],axis=1)","f59746a4":"def fun3(trial, data = X_new, target = y):\n    train_x, test_x, train_y, test_y = train_test_split(data,target,train_size=0.8,random_state=42)\n\n    param = {\n       'learning_rate' : trial.suggest_uniform('learning_rate',0,1),\n        'gamma' : trial.suggest_uniform('gamma',0,100),\n        'max_depth': trial.suggest_int('max_depth', 1,100),\n        'min_child_weight' : trial.suggest_uniform('min_child_weight', 0,100),\n        'max_delta_step' : trial.suggest_uniform('max_delta_step',1,10),\n        'subsample' : trial.suggest_uniform('subsample',0,1),\n        'colsample_bytree' : trial.suggest_uniform('colsample_bytree',0,1),\n        'lambda' : trial.suggest_uniform('lambda',1e-5,30),\n        'alpha' : trial.suggest_uniform('alpha',1e-5,30),\n        'tree_method' :'gpu_hist',\n        'grow_policy':'lossguide',\n        'max_leaves': trial.suggest_int('max_leaves',16,64),\n        'random_state' : trial.suggest_categorical('random_state',[13]),\n        'objective':'multi:softprob',\n        'eval_metric':'mlogloss',\n        'predictor':'gpu_predictor'\n\n        \n    }\n    model = XGBClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    pred_y = model.predict_proba(test_x)\n    \n    ll = log_loss(test_y, pred_y)\n    \n    return ll\n    ","2dab8df0":"study_3 = optuna.create_study(direction='minimize')\nstudy_3.optimize(fun3, n_trials=100)\nprint('Number of finished trials:', len(study_3.trials))\nprint('Best trial:', study_3.best_trial.params)","e1cebc4c":"best_params_xgb = study_3.best_params\nbest_params_xgb['objective'] = 'multi:softprob'\nbest_params_xgb['eval_metric'] = 'mlogloss'\nbest_params_xgb['grow_policy'] = 'lossguide'\nbest_params_xgb['n_estimators'] = 10000\nbest_params_xgb['tree_method'] ='gpu_hist'\nbest_params_xgb['predictor'] ='gpu_predictor'","2b3c003a":"columns = train_new.drop(['id','target'],axis=1).columns\npreds_3 = np.zeros((test.shape[0],4))\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nxgb_df = pd.DataFrame(columns = ['Class1m3', 'Class2m3','Class3m3','Class4m3','target'])\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train_new[columns], train_new['target']):\n    \n    X_tr, X_val = train_new[columns].iloc[tr_idx], train_new[columns].iloc[test_idx]\n    y_tr, y_val = train_new['target'].iloc[tr_idx], train_new['target'].iloc[test_idx]\n    \n    model = XGBClassifier(**best_params_xgb)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=500,verbose = False)\n    y_pred  = model.predict_proba(X_val)\n    df = pd.DataFrame(y_pred,columns=['Class1m3', 'Class2m3','Class3m3','Class4m3'])\n    df['target'] = list(y_val)\n    xgb_df = pd.concat([xgb_df,df])\n    \n    preds_3+=model.predict_proba(test_new.drop(['id'],axis=1))\/kf.n_splits\n    ll.append(log_loss(y_val, model.predict_proba(X_val)))\n    print(n+1,ll[n])\n    n+=1","ba5519d1":"xgb_df","b2469ea7":"np.mean(ll)","f1649d09":"df_kfold_xgb = pd.DataFrame(preds_3,columns=['Class_1','Class_2','Class_3','Class_4'])\ndf_kfold_xgb['id']  = test['id']\ndf_kfold_xgb = df_kfold_xgb[['id','Class_1','Class_2','Class_3','Class_4']]","f07b6902":"df_kfold_xgb","9ddbbad0":"output_6 = df_kfold_xgb.to_csv('submit_6.csv',index=False)","031d4786":"preds_combined = (preds+preds_2+preds_3)\/3\npreds_combined = np.clip(preds_combined,0.05, 0.95)\ndf_combined = pd.DataFrame(preds_combined,columns=['Class_1','Class_2','Class_3','Class_4'])\ndf_combined['id'] = test['id']\ndf_combined = df_combined[['id','Class_1','Class_2','Class_3','Class_4']]","511494c4":"df_combined","f728aaef":"final_output = df_combined.to_csv('final_submit.csv',index=False)","1b7f4677":"stacked_df['Class1m1'] = cb_df['Class1m1']\nstacked_df['Class2m1'] = cb_df['Class2m1']\nstacked_df['Class3m1'] = cb_df['Class3m1']\nstacked_df['Class4m1'] = cb_df['Class4m1']\n\nstacked_df['Class1m2'] = lgbm_df['Class1m2']\nstacked_df['Class2m2'] = lgbm_df['Class2m2']\nstacked_df['Class3m2'] = lgbm_df['Class3m2']\nstacked_df['Class4m2'] = lgbm_df['Class4m2']\n\nstacked_df['Class1m3'] = xgb_df['Class1m3']\nstacked_df['Class2m3'] = xgb_df['Class2m3']\nstacked_df['Class3m3'] = xgb_df['Class3m3']\nstacked_df['Class4m3'] = xgb_df['Class4m3']\n\nstacked_df['target'] = cb_df['target']\n\n\ntest_stacked_df = pd.DataFrame(columns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class1m3', 'Class2m3','Class3m3','Class4m3'])\ntest_stacked_df['Class1m1'] = df_kfold['Class_1']\ntest_stacked_df['Class2m1'] = df_kfold['Class_2']\ntest_stacked_df['Class3m1'] = df_kfold['Class_3']\ntest_stacked_df['Class4m1'] = df_kfold['Class_4']\n\ntest_stacked_df['Class1m2'] = df_kfold_lgbm['Class_1']\ntest_stacked_df['Class2m2'] = df_kfold_lgbm['Class_2']\ntest_stacked_df['Class3m2'] = df_kfold_lgbm['Class_3']\ntest_stacked_df['Class4m2'] = df_kfold_lgbm['Class_4']\n\ntest_stacked_df['Class1m3'] = df_kfold_xgb['Class_1']\ntest_stacked_df['Class2m3'] = df_kfold_xgb['Class_2']\ntest_stacked_df['Class3m3'] = df_kfold_xgb['Class_3']\ntest_stacked_df['Class4m3'] = df_kfold_xgb['Class_4']\n","5cf9008d":"stacked_df","40420d57":"l=[]\nfor i in stacked_df['target']:\n    l.append(int(i))\n    \nstacked_df['target'] = l","875cda3d":"preds_stacked = np.zeros((test.shape[0],4))\ncolumns = ['Class1m1', 'Class2m1','Class3m1','Class4m1','Class1m2', 'Class2m2','Class3m2','Class4m2','Class1m3', 'Class2m3','Class3m3','Class4m3']\nkf = StratifiedKFold(n_splits = 10 , random_state = 13 , shuffle = True)\nll =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(stacked_df[columns], stacked_df['target']):\n    \n    X_tr, X_val = stacked_df[columns].iloc[tr_idx], stacked_df[columns].iloc[test_idx]\n    y_tr, y_val = stacked_df['target'].iloc[tr_idx], stacked_df['target'].iloc[test_idx]\n    \n    model = LGBMClassifier(random_state= 13, objective= 'multiclass', metric = 'multi_logloss')\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=500,verbose=False)\n    y_pred  = model.predict_proba(X_val)\n    \n    preds_stacked+=model.predict_proba(test_stacked_df)\/kf.n_splits\n    ll.append(log_loss(y_val, y_pred))\n    print(n+1,ll[n])\n    n+=1","c4e0f595":"np.mean(ll)","294f9990":"df_kfold_st = pd.DataFrame(preds_stacked,columns=['Class_1','Class_2','Class_3','Class_4'])\ndf_kfold_st['id']  = test['id']\ndf_kfold_st = df_kfold_st[['id','Class_1','Class_2','Class_3','Class_4']]","91aa9572":"df_kfold_st","c036143f":"stacked_submit = df_kfold_st.to_csv('stacked_submit.csv',index=False)","e48cdeca":"# Feature Selection with Permutation Importance","21f612e9":"# Making Predictions with tuned Model","c372aed0":"# Baseline CATBoost Classifier","e72c1e29":"# Plotting + Report with Dataprep","d196034e":"# Tuning with OPTUNA","abd7b8f3":"## Insights from EDA\n> #### 1. There is no corelation between the features even with the target variable.\n> #### 2. Most of the features are skewed with 0 values even >90%, that means feature selection will be necessary.\n> #### 3. Baseline model can overfit because of skewness in data.\n> #### 4. Outlier Detection and removal will also be handy to improve score.\n> #### 5. No corelation means that there are some unnecessary features.\n> #### 6. Also we can gain some info by feature engineering by trying feature interaction or ratio and increase corelation.","5d360f17":"# Predictions on Kfold","e6e1d08e":"# Feature Selection with Permutation Importance","e5a76129":"# LGBM","3363191c":"# Optimizing Catboost Classifier with OPTUNA","e35a2916":"# Tuning with OPTUNA","96f5202b":"# XGBoost","3c70c1d3":"# LGBM Kfold Predictions","0ce1b79a":"# Voting Classifier (Catboost+LGBM+XGBoost)","fe130c47":"# Stacked Model","2a369187":"# XGBoost KFOLD Predictions ","1fc219ab":"## Thanks, and don't forget to upvote, I'm a beginner it will motivate me!!"}}