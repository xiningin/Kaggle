{"cell_type":{"deb071b7":"code","4ceb2e12":"code","8bb2e3c3":"code","5f46c177":"code","d722e3ca":"code","0aa07236":"code","d598079f":"code","b1d088e4":"code","8d7a38b6":"code","6b27789f":"code","f0aacd0f":"code","57dc2aa4":"code","d0de91d1":"code","508cd5d8":"code","99c5536c":"code","a22bbfb0":"code","83866f8a":"code","f568bbc1":"code","6f78f13d":"code","3a7910eb":"code","4c93ad93":"code","56131681":"code","95f8f32c":"code","d9f2cf4f":"code","6b51c45c":"code","20e317d5":"code","a49e86c0":"code","82940216":"code","7b534054":"code","6c20668a":"code","599c54bb":"code","2e9f7545":"markdown","305f6868":"markdown","9284360c":"markdown","3fc5b280":"markdown","d1515018":"markdown","7f6109cd":"markdown","b815f148":"markdown","3e88e25d":"markdown","5185664a":"markdown","4ebf23b6":"markdown","1ab75c45":"markdown","bca2ba95":"markdown","b34cfd02":"markdown","d35b6ade":"markdown","45fc4b44":"markdown","4d5ac3a5":"markdown","e8389685":"markdown","53da7a61":"markdown"},"source":{"deb071b7":"import numpy as np\nimport pandas as pd\n\nfrom mlens.ensemble import SuperLearner\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression, SGDClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","4ceb2e12":"# To see optuna progress you need to comment this row\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)","8bb2e3c3":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5f46c177":"train.head()","d722e3ca":"for col in train.columns:\n    print(col, str(round(100* train[col].isnull().sum() \/ len(train), 2)) + '%')","0aa07236":"train['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]\nds = pd.concat([train, test])\n\nsur = list()\ndied = list()\n\nfor index, row in ds.iterrows():\n    s = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==1)]\n    d = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==0)]\n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n    sur.append(s)\n    died.append(d)\n    \nds['FamilySurvived'] = sur\nds['FamilyDied'] = died\nds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\nds['IsAlone'] = 0\nds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\nds['Fare'] = ds['Fare'].fillna(train['Fare'].median())\nds['Embarked'] = ds['Embarked'].fillna('Q')\n\ntrain = ds[ds['Survived'].notnull()]\ntest = ds[ds['Survived'].isnull()]\ntest = test.drop(['Survived'], axis=1)\n\ntrain['rich_woman'] = 0\ntest['rich_woman'] = 0\ntrain['men_3'] = 0\ntest['men_3'] = 0\n\ntrain.loc[(train['Pclass']<=2) & (train['Sex']=='female'), 'rich_woman'] = 1\ntest.loc[(test['Pclass']<=2) & (test['Sex']=='female'), 'rich_woman'] = 1\ntrain.loc[(train['Pclass']==3) & (train['Sex']=='male'), 'men_3'] = 1\ntest.loc[(test['Pclass']==3) & (test['Sex']=='male'), 'men_3'] = 1\n\ntrain['rich_woman'] = train['rich_woman'].astype(np.int8)\ntest['rich_woman'] = test['rich_woman'].astype(np.int8)\n\ntrain[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin']])\ntest['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin']])\n\nfor cat in ['Pclass', 'Sex', 'Embarked', 'Cabin']:\n    train = pd.concat([train, pd.get_dummies(train[cat], prefix=cat)], axis=1)\n    train = train.drop([cat], axis=1)\n    test = pd.concat([test, pd.get_dummies(test[cat], prefix=cat)], axis=1)\n    test = test.drop([cat], axis=1)\n    \ntrain = train.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\ntest =  test.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\ntrain.head()","d598079f":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Age\", \n    points='all',\n    title='Age & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","b1d088e4":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Fare\", \n    points='all',\n    title='Fare & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","8d7a38b6":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilySize\", \n    points='all',\n    title='Family Size & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","6b27789f":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilyDied\", \n    points='all',\n    title='Family Died & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","f0aacd0f":"f = plt.figure(\n    figsize=(19, 15)\n)\n\nplt.matshow(\n    train.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14, \n    rotation=75\n)\n\nplt.yticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\ncb.ax.tick_params(\n    labelsize=14\n)","57dc2aa4":"train.head()","d0de91d1":"y = train['Survived']\nX = train.drop(['Survived', 'Cabin_T'], axis=1)\nX_test = test.copy()\n\nX, X_val, y, y_val = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=False)","508cd5d8":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=666)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","99c5536c":"rf = RandomForestClassifier(\n    random_state=666\n)\nrf.fit(X, y)\npreds = rf.predict(X_val)\n\nprint('Random Forest accuracy: ', accuracy_score(y_val, preds))\nprint('Random Forest f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 666\nrf_f1 = RandomForestClassifier(\n    **rf_f1_params\n)\nrf_f1.fit(X, y)\npreds = rf_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 666\nrf_acc = RandomForestClassifier(\n    **rf_acc_params\n)\nrf_acc.fit(X, y)\npreds = rf_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))","a22bbfb0":"xgb = XGBClassifier(\n    random_state=666\n)\nxgb.fit(X, y)\npreds = xgb.predict(X_val)\n\nprint('XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('XGBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        subsample=subsample,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 666\nxgb_f1 = XGBClassifier(\n    **xgb_f1_params\n)\nxgb_f1.fit(X, y)\npreds = xgb_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 666\nxgb_acc = XGBClassifier(\n    **xgb_acc_params\n)\nxgb_acc.fit(X, y)\npreds = xgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))","83866f8a":"lgb = LGBMClassifier(\n    random_state=666\n)\nlgb.fit(X, y)\npreds = lgb.predict(X_val)\n\nprint('LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('LightGBM f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    model = LGBMClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        num_leaves=num_leaves, \n        min_child_samples=min_child_samples,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nlgb_f1_params = optimizer.optimize()\nlgb_f1_params['random_state'] = 666\nlgb_f1 = LGBMClassifier(\n    **lgb_f1_params\n)\nlgb_f1.fit(X, y)\npreds = lgb_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nlgb_acc_params = optimizer.optimize()\nlgb_acc_params['random_state'] = 666\nlgb_acc = LGBMClassifier(\n    **lgb_acc_params\n)\nlgb_acc.fit(X, y)\npreds = lgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))","f568bbc1":"lr = LogisticRegression(\n    random_state=666\n)\nlr.fit(X, y)\npreds = lr.predict(X_val)\n\nprint('Logistic Regression: ', accuracy_score(y_val, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_val, preds))","6f78f13d":"dt = DecisionTreeClassifier(\n    random_state=666\n)\ndt.fit(X, y)\npreds = dt.predict(X_val)\n\nprint('Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Decision Tree f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(\n        min_samples_split=min_samples_split, \n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_depth=max_depth, \n        min_samples_leaf=min_samples_leaf, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 666\ndt_f1 = DecisionTreeClassifier(\n    **dt_f1_params\n)\ndt_f1.fit(X, y)\npreds = dt_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 666\ndt_acc = DecisionTreeClassifier(\n    **dt_acc_params\n)\ndt_acc.fit(X, y)\npreds = dt_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))","3a7910eb":"bc = BaggingClassifier(\n    random_state=666\n)\nbc.fit(X, y)\npreds = bc.predict(X_val)\n\nprint('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(\n        n_estimators=n_estimators, \n        max_samples=max_samples, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nbc_f1_params = optimizer.optimize()\nbc_f1_params['random_state'] = 666\nbc_f1 = BaggingClassifier(\n    **bc_f1_params\n)\nbc_f1.fit(X, y)\npreds = bc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nbc_acc_params = optimizer.optimize()\nbc_acc_params['random_state'] = 666\nbc_acc = BaggingClassifier(\n    **bc_acc_params\n)\nbc_acc.fit(X, y)\npreds = bc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))","4c93ad93":"knn = KNeighborsClassifier()\nknn.fit(X, y)\npreds = knn.predict(X_val)\n\nprint('KNN accuracy: ', accuracy_score(y_val, preds))\nprint('KNN f1-score: ', f1_score(y_val, preds))\n\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1')\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(\n    **knn_f1_params\n)\nknn_f1.fit(X, y)\npreds = knn_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(\n    **knn_acc_params\n)\nknn_acc.fit(X, y)\npreds = knn_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))","56131681":"abc = AdaBoostClassifier(\n    random_state=666\n)\nabc.fit(X, y)\npreds = abc.predict(X_val)\n\nprint('AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('AdaBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(\n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 666\nabc_f1 = AdaBoostClassifier(\n    **abc_f1_params\n)\nabc_f1.fit(X, y)\npreds = abc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 666\nabc_acc = AdaBoostClassifier(\n    **abc_acc_params\n)\nabc_acc.fit(X, y)\npreds = abc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))","95f8f32c":"et = ExtraTreesClassifier(\n    random_state=666\n)\net.fit(X, y)\npreds = et.predict(X_val)\n\nprint('ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    model = ExtraTreesClassifier(\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=0\n    )\n    return model\n\noptimizer = Optimizer('f1')\net_f1_params = optimizer.optimize()\net_f1_params['random_state'] = 666\net_f1 = ExtraTreesClassifier(\n    **et_f1_params\n)\net_f1.fit(X, y)\npreds = et_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\net_acc_params = optimizer.optimize()\net_acc_params['random_state'] = 666\net_acc = ExtraTreesClassifier(\n    **et_acc_params\n)\net_acc.fit(X, y)\npreds = et_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))","d9f2cf4f":"model = SuperLearner(\n    folds=5, \n    random_state=666\n)\n\nmodel.add(\n    [\n        bc, \n        lgb, \n        xgb, \n        rf, \n        dt, \n        knn\n    ]\n)\n\nmodel.add_meta(\n    LogisticRegression()\n)\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('SuperLearner f1-score: ', f1_score(y_val, preds))","6b51c45c":"mdict = {\n    'RF': RandomForestClassifier(random_state=666),\n    'XGB': XGBClassifier(random_state=666),\n    'LGBM': LGBMClassifier(random_state=666),\n    'DT': DecisionTreeClassifier(random_state=666),\n    'KNN': KNeighborsClassifier(),\n    'BC': BaggingClassifier(random_state=666),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OALGBM': LGBMClassifier(**lgb_acc_params),\n    'OFLGBM': LGBMClassifier(**lgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OABC': BaggingClassifier(**bc_acc_params),\n    'OFBC': BaggingClassifier(**bc_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OAET': ExtraTreesClassifier(**et_acc_params),\n    'OFET': ExtraTreesClassifier(**et_f1_params),\n    'LR': LogisticRegression(random_state=666),\n    'ABC': AdaBoostClassifier(random_state=666),\n    'SGD': SGDClassifier(random_state=666), \n    'ET': ExtraTreesClassifier(random_state=666),\n    'MLP': MLPClassifier(random_state=666),\n    'GB': GradientBoostingClassifier(random_state=666),\n    'RDG': RidgeClassifier(random_state=666),\n    'PCP': Perceptron(random_state=666),\n    'PAC': PassiveAggressiveClassifier(random_state=666)\n}","20e317d5":"def create_model(trial):\n    model_names = list()\n    models_list = [\n        'RF', 'XGB', 'LGBM', 'DT', \n        'KNN', 'BC', 'OARF', 'OFRF', \n        'OAXGB', 'OFXGB', 'OALGBM', \n        'OFLGBM', 'OADT', 'OFDT', \n        'OAKNN', 'OFKNN', 'OABC', \n        'OFBC', 'OAABC', 'OFABC', \n        'OAET', 'OFET', 'LR', \n        'ABC', 'SGD', 'ET', \n        'MLP', 'GB', 'RDG', \n        'PCP', 'PAC'\n    ]\n    \n    head_list = [\n        'RF', \n        'XGB', \n        'LGBM', \n        'DT', \n        'KNN', \n        'BC', \n        'LR', \n        'ABC', \n        'SGD', \n        'ET', \n        'MLP', \n        'GB', \n        'RDG', \n        'PCP', \n        'PAC'\n    ]\n    \n    n_models = trial.suggest_int(\"n_models\", 2, 6)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(\n        folds=folds, \n        random_state=666\n    )\n    \n    models = [\n        mdict[item] for item in model_names\n    ]\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(\n        mdict[head]\n    )\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, y)\n    preds = model.predict(X_val)\n    score = accuracy_score(y_val, preds)\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\", \n    sampler=sampler\n)\n\nstudy.optimize(\n    objective, \n    n_trials=50\n)","a49e86c0":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n        \nresult","82940216":"model = SuperLearner(\n    folds=folds, \n    random_state=666\n)\n\nmodels = [\n    mdict[item] for item in result\n]\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_val, preds))","7b534054":"preds = model.predict(X_test)\npreds = preds.astype(np.int16)","6c20668a":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = preds\nsubmission.to_csv('submission.csv', index=False)","599c54bb":"submission.head()","2e9f7545":"Now we will create ensemble model named SuperLearner from mlens package. For details check https:\/\/machinelearningmastery.com\/super-learner-ensemble-in-python\/","305f6868":"<a id=\"4\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>4. Final submission<center><h2>","9284360c":"As we can see we improved our best single score only in a few lines of code. Feel free to add new features and try different models inside superlearner.","3fc5b280":"We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing.","d1515018":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>3. SuperLearner training and optimization<center><h2>","7f6109cd":"Let's optimize SuperLearner","b815f148":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>1. Feature engineering<center><h2>","3e88e25d":"Lets see percent of NaNs for every column in training set","5185664a":"Lets create some separate single models and check accuracy score. We also try to optimize every single model using optuna framework. As we can see we can get some better results with it.","4ebf23b6":"Lets create train and test dataset and create holdout set for validation.","1ab75c45":"Let's do some visualization.","bca2ba95":"<h1><center>Titanic: efficient ensembling and optimization<\/center><\/h1>\n\n<center><img src=\"https:\/\/www.dlt.travel\/immagine\/33923\/magazine-titanic2.jpg\"><\/center>","b34cfd02":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:Black; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h3>\n\n* [1. Feature engineering](#1)\n* [2. Single models training and optimization](#2)\n* [3. SuperLearner training and optimization](#3)\n* [4. Final submission](#4)\n    \n    \n## Best LB score is in Version 80.\n    \n    \n#### Keras neural network for Titanic classification problem: <a href=\"https:\/\/www.kaggle.com\/isaienkov\/keras-neural-network-architecture-optimization\">Titanic: Keras Neural Network architecture optimization<\/a>\n    \n#### Hyperparameter tunning methods for Titanic classification problem: <a href=\"https:\/\/www.kaggle.com\/isaienkov\/hyperparameters-tuning-techniques\">Titanic: hyperparameters tuning techniques<\/a>","d35b6ade":"We are going to use our single models in the first layer and LogisticRegressor as metalearner.","45fc4b44":"#### In this notebook I will not focus on preprocessing and feature engineering steps, just show how to build your efficient ensemble in few lines of code. I use almost the same features as in the most of kernels in current competition.","4d5ac3a5":"Another one thing. People with family size more than 7 didn't survive.","e8389685":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>2. Single models training and optimization<center><h2>","53da7a61":"Here is some basic preprocessing to get fast training and test datasets."}}