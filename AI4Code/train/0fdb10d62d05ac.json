{"cell_type":{"797fb64b":"code","6671e290":"code","7f60aea2":"code","7fe137f0":"code","b0af8666":"code","f119f9ae":"code","eb9c3883":"code","6086e6e7":"code","11a7b50b":"code","c3104f69":"code","d7e4bbaf":"code","08e71fbc":"code","6eabbfb0":"code","04ad83ea":"code","5bf79e8e":"code","e3606e5f":"code","e171924c":"markdown","772d74c5":"markdown","1ce8c327":"markdown","a8ed8365":"markdown","14065258":"markdown","13537b50":"markdown","49749fb7":"markdown","4d1e8287":"markdown","e5446424":"markdown","8b2cec44":"markdown","133d2590":"markdown"},"source":{"797fb64b":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"\/kaggle\/input\/airline-passenger-satisfaction\/train.csv\")","6671e290":"df.head()","7f60aea2":"df.columns","7fe137f0":"y_col = [\"satisfaction\"]\nuseless_cols = [\"Unnamed: 0\" , \"id\"]\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnewdf = df.select_dtypes(include=numerics)\nnumeric_cols = [f for f in newdf.columns if f not in useless_cols]\ncat_cols = list(set(df.columns).difference(set(useless_cols).union(set(numeric_cols))))\ncat_cols = [c for c in cat_cols if c not in y_col]","b0af8666":"numeric_cols","f119f9ae":"cat_cols","eb9c3883":"encoded_cat_df = pd.get_dummies(df[cat_cols] , drop_first =True)","6086e6e7":"X = encoded_cat_df.join(df[numeric_cols])\n#X= encoded_cat_df\ny = df[y_col]","11a7b50b":"X","c3104f69":"X.loc[: , numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].mean())","d7e4bbaf":"import statsmodels.api as sm ","08e71fbc":"X = sm.add_constant(X)","6eabbfb0":"np.where(y == \"satisfied\" , 1, 0)","04ad83ea":"log_reg = sm.Logit(np.where(y == \"satisfied\" , 1, 0), X).fit_regularized() ","5bf79e8e":"log_reg.summary()","e3606e5f":"np.exp(log_reg.params)","e171924c":"There are 25 columns out of which `2` i.e `Unnamed: 0` and `id` are useless for our analysis. Lets separate out the columns into `categorical` and `numerical` so that we can pre process them separately.","772d74c5":"# Logistic Regression\n\nWe will perform logistic regression using `satisfaction` variable as dependent variable and rest of the variables as the independent variable.\n\nTo get there, we need to prepare the data.\n\n1. `one hot encode` the categorical columns\n2. `remove missing` values\n3. optionally `scale` the numerical columns\n\n## Loading the data\n\nLets read in the csv file","1ce8c327":"Lets look at the results","a8ed8365":"Now suppose we want to see how `Type of Travel` impacts the `satisfaction` level.\n\nLets look at the `exp(coeficient of type of travel)`.\nThe value is `0.0704`. Now thing to remember is that the reference category for type of travel is `Business Travel`. So the way to interpret this is that `Personal Travel` reduces the odds of being `satisfied` compared to `Business Travel`. \n\nPeople who are on personal travel are `(1-0.07)*100 = 93%` less likely to be satisfied compared to people on business travel. That was surprising right ? Duh. ","14065258":"We assign all our independent variables to `X` and the dependent variable to `y`","13537b50":"## Overall Model Significance\n\nFew of the crucial metrics to look at are\n1. `log-likelihood (ll)` - to interpret the overall significance of the model we look at the `ll` of our model vs `ll` of the `null` model\n    - The null model,contains just the intercept. So the intercept is the log-odds of \"success\", estimated without reference to any predictors.\n    - This metric gives insight into how informative our independent variables are.\n    - Since a *good* model would mean that the liklihood of seeing our data generated by the model is high (closer to 1) therefore `log(liklihood)` should be closer to 0\n    - If `ll` of our model is closer to 0 than the `ll` of the null model then we have a *good* model\n    - in this case `ll` of our model is ~ -34,000 and `ll` of the null model is ~ -71,000 with a p-val of 0.0 therefore we have a significant model.\n\n2. `Pseudo-R-sqr` - unlike in linear regression, there is no straightforward way to calculate `r^2` in logistic regression and therefore there are many other metrics that try to present similar information and have similar range (0-1). These metrics fall under the category of `pseudo-r-sqr`. There are many of these - one such list is [here](https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/)\n    -A value closer to 1 would mean that our independent variables \"explain\" most of the variation in the dependent variable. A value closer to 0 would mean that the independet variables help explain nothing about the dependent variable.\n    - In this case the value is 0.46 and therefore we don't have much explanatory power in our dependent variables and would need more (external) information about our passengers to determine what makes them `satisfied`\n   \n\n## Variable Interpretations\n\nFirst thing to do is to isolate the variables which are statistically significant. In this case (weirdly enough) most variables turn out to be statistically significant as their `p-values` are 0 or close to zero. Only `flight distance` seems to be a statistically insignificant variable at 95% confidence interval.\n\n\nNow even though a lot of variables might be statistically significant, we still need to assess their impact on the dependent variable. For that we focus on the `coef` values. But directly staring at the coef values is not that informative and therefore we first convert the coeficients into `log-odds`\n\nSince we assume that the variables are independent of each other, so we can look at each individually.\n\nRecall that logistic regression equation is \n\n![image.png](attachment:image.png)\n\nTherefore suppose we want to interpret the impact of variable `x1` on how `satisfied` it makes a person, since all variables are independent we can keep them constant and *ignore* them. \n\nThis will reduce the equation to \n\n`log(odds) = B1*x1`\n\nthen we can move the log to the other side\n\n`odds = exp(B1 * x1)`\n\nnow we can safely interpret the change in odds of someone being `satisfied` or `dissatisfied` by looking at the exponent of the coeficient of the `x1` variable.\n\nMore concretly, lets take the exponent of all coeficients we have.\n","49749fb7":"Lets run the Logit model","4d1e8287":"Finall to fill in the `missing` values we use `dataframe.fillna` method and `impute` the columns with their respective means","e5446424":"We now `one hot` encode the categorical columns. We can use `pandas.get_dummies` method for that. \n\nThis is what one hot encoding looks like\n\n\n![image.png](attachment:image.png)\n\n[source](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding)\n\nOne important thing note is that we use `drop_first` property to drop the first value of each categorical column and keep the rest. This is done to avoid `multi-collinearlity` or the `dummy variable trap`. [More here](https:\/\/www.geeksforgeeks.org\/ml-dummy-variable-trap-in-regression-models\/)\n\nSo in the above example we would drop `red` column. This also sets the dropped value as the `reference` category. Now the coeficients of `yellow` and `green` ,produced by the logit model , can be interpreted in terms of `red` , for example how more likely is `yellow` to `satisfy` the passengers compared to `red`","8b2cec44":"`Statsmodels` logit class expects that the dependent variable be `0 or 1` and therefore we use `numpy.where` to convert our `satisfied or dissatisfied` to 0 and 1. We set `satisfied` to 1.","133d2590":"One thing we have to do for our independent variables is to `manually` add a `constant` variable for which `statmodels` will find the coeficient. Statmodels provides a helper function `sm.add_constant` which adds a column to our data with a constant value of 1"}}