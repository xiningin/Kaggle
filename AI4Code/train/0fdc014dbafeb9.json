{"cell_type":{"43e1d001":"code","2fee49cf":"code","5c3fa84d":"code","9fbb70f2":"code","f64257c3":"code","8981d871":"code","c3ac4074":"code","5dc81549":"code","a5afb14d":"code","3260eeb5":"code","ac3c86c9":"code","a60b5b11":"code","6fad3767":"code","017a2627":"markdown"},"source":{"43e1d001":"## In this kernel I would like to show: \n## 1. FE creation approaches\n## 2. Sequential fe validation\n## 3. Dimension reduction\n## 4. FE validation by Permutation importance\n## 5. Mean encodings\n## 6. Parallelization for FE","2fee49cf":"import numpy as np \nimport pandas as pd \nimport os, sys, gc, warnings, psutil, random\n\nwarnings.filterwarnings('ignore')","5c3fa84d":"########################### Load data\n########################### Basic features were created here:\n########################### https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n#################################################################################\n\n# Read data\ngrid_df = pd.concat([pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_1.pkl'),\n                     pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_2.pkl').iloc[:,2:],\n                     pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_3.pkl').iloc[:,2:]],\n                     axis=1)\n\n# Subsampling\n# to make all calculations faster.\n# Keep only 5% of original ids.\nkeep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\ngrid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n\n# Let's \"inspect\" our grid DataFrame\ngrid_df.info()","9fbb70f2":"########################### Baseline model\n#################################################################################\n\n# We will need some global VARS for future\n\nSEED = 42             # Our random seed for everything\nrandom.seed(SEED)     # to make all tests \"deterministic\"\nnp.random.seed(SEED)\nN_CORES = psutil.cpu_count()     # Available CPU cores\n\nTARGET = 'sales'      # Our Target\nEND_TRAIN = 1913      # And we will use last 28 days as validation\n\n# Drop some items from \"TEST\" set part (1914...)\ngrid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n\n# Features that we want to exclude from training\nremove_features = ['id','d',TARGET]\n\n# Our baseline model serves\n# to do fast checks of\n# new features performance \n\n# We will use LightGBM for our tests\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',         # Standart boosting type\n                    'objective': 'regression',       # Standart loss for RMSE\n                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n                    'subsample': 0.8,                \n                    'subsample_freq': 1,\n                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n                    'num_leaves': 2**7-1,            # We will need model only for fast check\n                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n                    'feature_fraction': 0.8,\n                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n                    'seed': SEED,\n                    'verbose': -1,\n                } \n\n## RMSE\ndef rmse(y, y_pred):\n    return np.sqrt(np.mean(np.square(y - y_pred)))\n\n# Small function to make fast features tests\n# estimator = make_fast_test(grid_df)\n# it will return lgb booster for future analisys\ndef make_fast_test(df):\n\n    features_columns = [col for col in list(df) if col not in remove_features]\n\n    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n    \n    train_data = lgb.Dataset(tr_x, label=tr_y)\n    valid_data = lgb.Dataset(vl_x, label=v_y)\n    \n    estimator = lgb.train(\n                            lgb_params,\n                            train_data,\n                            valid_sets = [train_data,valid_data],\n                            verbose_eval = 500,\n                        )\n    \n    return estimator\n\n# Make baseline model\nbaseline_model = make_fast_test(grid_df)","f64257c3":"########################### Lets test our normal Lags (7 days)\n########################### Some more info about lags here:\n########################### https:\/\/www.kaggle.com\/kyakovlev\/m5-lags-features\n#################################################################################\n\n# Small helper to make lags creation faster\nfrom multiprocessing import Pool                # Multiprocess Runs\n\n## Multiprocessing Run.\n# :t_split - int of lags days                   # type: int\n# :func - Function to apply on each split       # type: python function\n# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n## Multiprocess Runs\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df\n\ndef make_normal_lag(lag_day):\n    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n    col_name = 'sales_lag_'+str(lag_day)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n    return lag_df[[col_name]]\n\n# Launch parallel lag creation\n# and \"append\" to our grid\nLAGS_SPLIT = [col for col in range(1,1+7)]\ngrid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n\n# Make features test\ntest_model = make_fast_test(grid_df)","8981d871":"########################### Permutation importance Test\n########################### https:\/\/www.kaggle.com\/dansbecker\/permutation-importance @dansbecker\n#################################################################################\n\n# Let's creat validation dataset and features\nfeatures_columns = [col for col in list(grid_df) if col not in remove_features]\nvalidation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n\n# Make normal prediction with our model and save score\nvalidation_df['preds'] = test_model.predict(validation_df[features_columns])\nbase_score = rmse(validation_df[TARGET], validation_df['preds'])\nprint('Standart RMSE', base_score)\n\n\n# Now we are looping over all our numerical features\nfor col in features_columns:\n    \n    # We will make validation set copy to restore\n    # features states on each run\n    temp_df = validation_df.copy()\n    \n    # Error here appears if we have \"categorical\" features and can't \n    # do np.random.permutation without disrupt categories\n    # so we need to check if feature is numerical\n    if temp_df[col].dtypes.name != 'category':\n        temp_df[col] = np.random.permutation(temp_df[col].values)\n        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n        \n        # If our current rmse score is less than base score\n        # it means that feature most probably is a bad one\n        # and our model is learning on noise\n        print(col, np.round(cur_score - base_score, 4))\n\n# Remove Temp data\ndel temp_df, validation_df\n\n# Remove test features\n# As we will compare performance with baseline model for now\nkeep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\ngrid_df = grid_df[keep_cols]\n\n\n# Results:\n## Lags with 1 days shift (nearest past) are important\n## Some other features are not important and probably just noise\n## Better make several Permutation runs to confirm useless of the feature\n## link again https:\/\/www.kaggle.com\/dansbecker\/permutation-importance @dansbecker\n\n## price_nunique -0.002 : strong negative values are most probably noise\n## price_max -0.0002 : values close to 0 need deeper investigation\n","c3ac4074":"########################### Lets test far away Lags (7 days with 56 days shift)\n########################### and check permutation importance\n#################################################################################\n\nLAGS_SPLIT = [col for col in range(56,56+7)]\ngrid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\ntest_model = make_fast_test(grid_df)\n\nfeatures_columns = [col for col in list(grid_df) if col not in remove_features]\nvalidation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\nvalidation_df['preds'] = test_model.predict(validation_df[features_columns])\nbase_score = rmse(validation_df[TARGET], validation_df['preds'])\nprint('Standart RMSE', base_score)\n\nfor col in features_columns:\n    temp_df = validation_df.copy()\n    if temp_df[col].dtypes.name != 'category':\n        temp_df[col] = np.random.permutation(temp_df[col].values)\n        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n        print(col, np.round(cur_score - base_score, 4))\n\ndel temp_df, validation_df\n        \n# Remove test features\n# As we will compare performance with baseline model for now\nkeep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\ngrid_df = grid_df[keep_cols]\n\n\n# Results:\n## Lags with 56 days shift (far away past) are not as important\n## as nearest past lags\n## and at some point will be just noise for our model","5dc81549":"########################### PCA\n#################################################################################\n\n# The main question here - can we have \n# almost same rmse boost with less features\n# less dimensionality?\n\n# Lets try PCA and make 7->3 dimensionality reduction\n\n# PCA is \"unsupervised\" learning\n# and with shifted target we can be sure\n# that we have no Target leakage\nfrom sklearn.decomposition import PCA\n\ndef make_pca(df, pca_col, n_days):\n    print('PCA:', pca_col, n_days)\n    \n    # We don't need any other columns to make pca\n    pca_df = df[[pca_col,'d',TARGET]]\n    \n    # If we are doing pca for other series \"levels\" \n    # we need to agg first\n    if pca_col != 'id':\n        merge_base = pca_df[[pca_col,'d']]\n        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n        pca_df[TARGET] = pca_df['sum']\n        del pca_df['sum']\n    \n    # Min\/Max scaling\n    pca_df[TARGET] = pca_df[TARGET]\/pca_df[TARGET].max()\n    \n    # Making \"lag\" in old way (not parallel)\n    LAG_DAYS = [col for col in range(1,n_days+1)]\n    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n    pca_df = pca_df.assign(**{\n            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n            for l in LAG_DAYS\n            for col in [TARGET]\n        })\n    \n    pca_columns = list(pca_df)[3:]\n    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n    pca = PCA(random_state=SEED)\n    \n    # You can use fit_transform here\n    pca.fit(pca_df[pca_columns])\n    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n    \n    print(pca.explained_variance_ratio_)\n    \n    # we will keep only 3 most \"valuable\" columns\/dimensions \n    keep_cols = pca_columns[:3]\n    print('Columns to keep:', keep_cols)\n    \n    # If we are doing pca for other series \"levels\"\n    # we need merge back our results to merge_base df\n    # and only than return resulted df\n    # I'll skip that step here\n    \n    return pca_df[keep_cols]\n\n\n# Make PCA\ngrid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n\n# Make features test\ntest_model = make_fast_test(grid_df)\n\n# Remove test features\n# As we will compare performance with baseline model for now\nkeep_cols = [col for col in list(grid_df) if '_pca_' not in col]\ngrid_df = grid_df[keep_cols]","a5afb14d":"########################### Mean\/std target encoding\n#################################################################################\n\n# We will use these three columns for test\n# (in combination with store_id)\nicols = ['item_id','cat_id','dept_id']\n\n# But we can use any other column or even multiple groups\n# like these ones\n#            'state_id',\n#            'store_id',\n#            'cat_id',\n#            'dept_id',\n#            ['state_id', 'cat_id'],\n#            ['state_id', 'dept_id'],\n#            ['store_id', 'cat_id'],\n#            ['store_id', 'dept_id'],\n#            'item_id',\n#            ['item_id', 'state_id'],\n#            ['item_id', 'store_id']\n\n# There are several ways to do \"mean\" encoding\n## K-fold scheme\n## LOO (leave one out)\n## Smoothed\/regularized \n## Expanding mean\n## etc \n\n# You can test as many options as you want\n# and decide what to use\n# Because of memory issues you can't \n# use many features.\n\n# We will use simple target encoding\n# by std and mean agg\nfor col in icols:\n    print('Encoding', col)\n    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n    \n    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n    joiner = '_'+col+'_encoding_'\n    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n    temp_df = temp_df.reset_index()\n    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n    del temp_df\n\n# Make features test\ntest_model = make_fast_test(grid_df)\n\n# Remove test features\nkeep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\ngrid_df = grid_df[keep_cols]\n\n# Bad thing that for some items  \n# we are using past and future values.\n# But we are looking for \"categorical\" similiarity\n# on a \"long run\". So future here is not a big problem.","3260eeb5":"########################### Last non O sale\n#################################################################################\n\ndef find_last_sale(df,n_day):\n    \n    # Limit initial df\n    ls_df = df[['id','d',TARGET]]\n    \n    # Convert target to binary\n    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n    \n    # Make lags to prevent any leakage\n    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n\n    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n    temp_df.columns = ['id','d_min','non_zero_lag']\n\n    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n\n    return ls_df[['last_sale']]\n\n\n# Find last non zero\n# Need some \"dances\" to fit in memory limit with groupers\ngrid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n\n# Make features test\ntest_model = make_fast_test(grid_df)\n\n# Remove test features\nkeep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\ngrid_df = grid_df[keep_cols]","ac3c86c9":"########################### Apply on grid_df\n#################################################################################\n# lets read grid from \n# https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe\n# to be sure that our grids are aligned by index\ngrid_df = pd.read_pickle('..\/input\/m5-simple-fe\/grid_part_1.pkl')\ngrid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\nbase_cols = list(grid_df)\n\nicols =  [\n            ['state_id'],\n            ['store_id'],\n            ['cat_id'],\n            ['dept_id'],\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            ['item_id'],\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n            ]\n\nfor col in icols:\n    print('Encoding', col)\n    col_name = '_'+'_'.join(col)+'_'\n    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n\nkeep_cols = [col for col in list(grid_df) if col not in base_cols]\ngrid_df = grid_df[['id','d']+keep_cols]","a60b5b11":"#################################################################################\nprint('Save Mean\/Std encoding')\ngrid_df.to_pickle('mean_encoding_df.pkl')","6fad3767":"########################### Final list of new features\n#################################################################################\ngrid_df.info()","017a2627":"from eli5 documentation (seems it's perfect explanation)\n\nThe idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we\u2019re interested in) decreases when a feature is not available.\n\nTo do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n\nTo avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn\u2019t work as-is, because estimators expect feature to be present. So instead of removing a feature we can **replace it with random noise** - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the **same distribution as original feature values** (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples\u2019 feature values - this is how permutation importance is computed.\n\n---\n\nIt's not good when feature remove (replaced by noise) but we have better score. Simple and easy. "}}