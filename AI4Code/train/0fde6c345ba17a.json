{"cell_type":{"271e0ef3":"code","24b3cebc":"code","e93142c8":"code","a0de4c79":"code","7f2acf95":"code","a9041322":"code","81904fed":"code","e1b2c935":"code","a5d840fb":"code","578516da":"code","2db8d860":"code","f1e5a2cc":"code","43b167f1":"code","09a26c64":"code","9859c3f4":"code","86d84df1":"code","eb5b093e":"code","0b0a7eae":"code","cdf73452":"code","60580dbb":"code","91a2b592":"code","eb710e3f":"code","84962ed6":"code","2ea79896":"code","bbd58c5a":"code","ec6bd485":"code","9410c30e":"code","231dcc15":"markdown","11b9508a":"markdown","b0e201da":"markdown","79bb2379":"markdown","bd96b38d":"markdown","f7ccac3b":"markdown","39a4f530":"markdown","1b0692ef":"markdown","6e466bcf":"markdown","318a0e07":"markdown","82961bf3":"markdown","854efd80":"markdown","a456b623":"markdown","be3849a2":"markdown"},"source":{"271e0ef3":"%%time\n\nimport numpy as np  \nimport pandas as pd  \nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport os\n\n# Install java\n! apt-get install -y openjdk-8-jdk-headless -qq > \/dev\/null\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/java-8-openjdk-amd64\"\nos.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\/bin:\" + os.environ[\"PATH\"]\n! java -version\n\n# Install pyspark\n! pip install --ignore-installed pyspark==2.4.7 spark-nlp==2.7.5\n\n#!ls '\/usr\/lib\/jvm\/java-8-openjdk-amd64'\nos.environ['JAVA_HOME'] = \"\/usr\/lib\/jvm\/java-8-openjdk-amd64\"\nos.environ['PATH'] = os.environ['JAVA_HOME'] + \"\/bin:\" + os.environ['PATH']\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\nimport sparknlp\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\nspark = sparknlp.start()\n\nfrom sklearn.metrics import classification_report\n\nprint(\"Apache Spark version\", spark.version)\nprint(\"Spark NLP version\", sparknlp.version())","24b3cebc":"train = pd.read_csv('..\/input\/atis-airlinetravelinformationsystem\/atis_intents_train.csv')\ntrain.columns = ['intent', 'snippet']\n\nprint(train.shape)\ntrain.head()","e93142c8":"train.intent.value_counts(), train.intent.value_counts(normalize=True)","a0de4c79":"test = pd.read_csv('..\/input\/atis-airlinetravelinformationsystem\/atis_intents_test.csv')\ntest.columns = ['intent', 'snippet']\n\nprint(test.shape)\ntest.head()","7f2acf95":"test.intent.value_counts(), test.intent.value_counts(normalize=True)","a9041322":"document_assembler = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n\nuse = UniversalSentenceEncoder.pretrained('tfhub_use', lang=\"en\") \\\n    .setInputCols([\"document\"])\\\n    .setOutputCol(\"sentence_embeddings\")\n\ndocument_classifier = ClassifierDLModel.pretrained('classifierdl_use_atis', 'en') \\\n  .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n  .setOutputCol(\"class\")\n\nnlpPipeline = Pipeline(stages=[document_assembler, use, document_classifier])\nlight_pipeline = LightPipeline(nlpPipeline.fit(spark.createDataFrame([['']]).toDF(\"text\")))","81904fed":"example = ['I want to fly from Albany NY to Tampa Florida.', 'what would be the cost of the flight ']\nresult = light_pipeline.annotate(example)\nresult","e1b2c935":"example = test.snippet.tolist()\nresult = light_pipeline.annotate(example)\nPreds = []\nfor j in range(len(result)):\n    Preds.append(result[j]['class'][0])\n    \nTruth = test.intent.tolist()\n\nprint(classification_report(Truth, Preds))","a5d840fb":"test = test[(test['intent'] != 'atis_aircraft') &\n                   (test['intent'] != 'atis_flight_time') &\n                   (test['intent'] != 'atis_quantity')  ] \ntest.shape","578516da":"example = test.snippet.tolist()\nresult = light_pipeline.annotate(example)\nPreds = []\nfor j in range(len(result)):\n    Preds.append(result[j]['class'][0])\n    \nTruth = test.intent.tolist()\n\nprint(classification_report(Truth, Preds))","2db8d860":"trainDataset = spark.createDataFrame(train)\ntrainDataset.show(truncate=50)","f1e5a2cc":"from pyspark.sql.functions import col\n\nprint(trainDataset.count())\n\ntrainDataset.groupBy(\"intent\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","43b167f1":"testDataset = spark.createDataFrame(test)\ntestDataset.show(truncate=50)","09a26c64":"from pyspark.sql.functions import col\n\nprint(testDataset.count())\n\ntestDataset.groupBy(\"intent\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc()) \\\n    .show()","9859c3f4":"document_assembler = DocumentAssembler() \\\n    .setInputCol(\"snippet\") \\\n    .setOutputCol(\"document\")\n    \ntokenizer = Tokenizer() \\\n  .setInputCols([\"document\"]) \\\n  .setOutputCol(\"token\")\n    \nnormalizer = Normalizer() \\\n    .setInputCols([\"token\"]) \\\n    .setOutputCol(\"normalized\")\n\nstopwords_cleaner = StopWordsCleaner()\\\n      .setInputCols(\"normalized\")\\\n      .setOutputCol(\"cleanTokens\")\\\n      .setCaseSensitive(False)\n\nlemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n    .setInputCols([\"cleanTokens\"]) \\\n    .setOutputCol(\"lemma\")","86d84df1":"glove_embeddings = WordEmbeddingsModel().pretrained() \\\n .setInputCols([\"document\",'lemma'])\\\n .setOutputCol(\"embeddings\")\\\n .setCaseSensitive(False)\n\nembeddingsSentence = SentenceEmbeddings() \\\n      .setInputCols([\"document\", \"embeddings\"]) \\\n      .setOutputCol(\"sentence_embeddings\") \\\n      .setPoolingStrategy(\"AVERAGE\")\n\nclasssifierdl = ClassifierDLApproach()\\\n  .setInputCols([\"sentence_embeddings\"])\\\n  .setOutputCol(\"class\")\\\n  .setLabelColumn(\"intent\")\\\n  .setMaxEpochs(3)\\\n  .setEnableOutputLogs(True)\n  #.setOutputLogsPath('logs')\n\nclf_pipeline = Pipeline(\n    stages=[document_assembler, \n            tokenizer,\n            normalizer,\n            stopwords_cleaner, \n            lemma, \n            glove_embeddings,\n            embeddingsSentence,\n            classsifierdl])","eb5b093e":"%%time\n\n# Train gpu = 14 secs... no gpu 16 secs\n#spark = sparknlp.start(gpu = True)\n\nclf_pipelineModel = clf_pipeline.fit(trainDataset)","0b0a7eae":"test = pd.read_csv('..\/input\/atis-airlinetravelinformationsystem\/atis_intents_test.csv')\ntest.columns = ['intent', 'snippet']\n\nprint(test.shape)\ntest.head()","cdf73452":"testDataset = spark.createDataFrame(test)\ntestDataset.show(10)","60580dbb":"# get the predictions on test Set\n\npreds = clf_pipelineModel.transform(testDataset)\npreds_df = preds.select(\"intent\",\"class.result\").toPandas()\nfor j in range(preds_df.shape[0]):\n    preds_df.result[j] = preds_df.result[j][0]\npreds_df.sample(10)","91a2b592":"\n# Definitely WORSE than the pretrained model above...this one always predicts the majority class \n\nprint(classification_report(preds_df.intent, preds_df.result))","eb710e3f":"%%time\n\ndocument_assembler = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n\ntokenizer = Tokenizer() \\\n  .setInputCols([\"document\"]) \\\n  .setOutputCol(\"token\")\n\nembeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"xx\")\\\n          .setInputCols(\"document\", \"token\") \\\n          .setOutputCol(\"embeddings\")\n\nner = NerDLModel.pretrained(\"nerdl_atis_840b_300d\", \"en\") \\\n        .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n        .setOutputCol(\"ner\")\n\nner_converter = NerConverter()\\\n    .setInputCols(['document', 'token', 'ner']) \\\n    .setOutputCol('ner_chunk')\n\npipeline = Pipeline(stages=[document_assembler, tokenizer, embeddings, ner, ner_converter])\n","84962ed6":"example = spark.createDataFrame(pd.DataFrame({'text': [\n    \"How much would cost a trip from Albany to Miami for tomorrow\"\n    ]}))\nresult = pipeline.fit(example).transform(example)\n\nresult","2ea79896":"# Visualize outputs as data frame ... NOTE pyspark df not pandas df...\n\nexploded = F.explode(F.arrays_zip('ner_chunk.result', 'ner_chunk.metadata'))\nselect_expression_0 = F.expr(\"cols['0']\").alias(\"chunk\")\nselect_expression_1 = F.expr(\"cols['1']['entity']\").alias(\"ner_label\")\nresult.select(exploded.alias(\"cols\")) \\\n    .select(select_expression_0, select_expression_1).show(truncate=False)","bbd58c5a":"resultDF = result.toPandas()\nresultDF","ec6bd485":"IOIdf = pd.DataFrame(columns = ['Result', 'Entity', 'StrBegin', 'StrEnd'])\n\n\nfor i in range(len(resultDF['ner_chunk'][0])):\n    #ResultEntity = [result['ner_chunk'][0][i]['result'] , result['ner_chunk'][0][i]['metadata']['entity']]\n    ResultEntity = pd.DataFrame()\n    ResultEntity['Result'] = [resultDF['ner_chunk'][0][i]['result']]\n    ResultEntity['Entity'] = [resultDF['ner_chunk'][0][i]['metadata']['entity']]\n    ResultEntity['StrBegin'] = [resultDF['ner_chunk'][0][i]['begin']]\n    ResultEntity['StrEnd'] = [resultDF['ner_chunk'][0][i]['end']]\n    #print(ResultEntity)\n    \n    IOIdf = IOIdf.append(ResultEntity)\n    \nIOIdf \n","9410c30e":"result","231dcc15":"### Spark NLP Refs:\n- **Home repository:** https:\/\/github.com\/JohnSnowLabs\/spark-nlp\n- **Full list of pretrained models\/pipelines:** https:\/\/github.com\/JohnSnowLabs\/spark-nlp-models\n- **All examples:** https:\/\/github.com\/JohnSnowLabs\/spark-nlp-workshop","11b9508a":"Check out my other nb w Spark NLP for Healthcare https:\/\/www.kaggle.com\/drscarlat\/extract-medical-terms-from-free-text-spark-nlp\/notebook","b0e201da":"# Data","79bb2379":"### Download the pretrained Pipeline\nhttps:\/\/nlp.johnsnowlabs.com\/2021\/01\/25\/classifierdl_use_atis_en.html\n","bd96b38d":"### Test","f7ccac3b":"# 2. Create & train a new Spark NLP ClassifierDL model\n\nBased on https:\/\/github.com\/JohnSnowLabs\/spark-nlp-workshop\/blob\/master\/tutorials\/Certification_Trainings\/Public\/5.Text_Classification_with_ClassifierDL.ipynb","39a4f530":"# 1. Intent classification with a Spark NLP pretrained model","1b0692ef":"### Train","6e466bcf":"# Install Spark NLP and dependencies","318a0e07":"# 3. NER ... going beyond the initial benchmark of 8 classes\n\n![Screen%20Shot%202021-03-21%20at%207.37.32%20PM.png](attachment:Screen%20Shot%202021-03-21%20at%207.37.32%20PM.png)\n\nBased on https:\/\/nlp.johnsnowlabs.com\/2021\/01\/25\/nerdl_atis_840b_300d_en.html","82961bf3":"### default classifierDL params:\n\nmaxEpochs -> 10,\nlr -> 5e-3f,\ndropout -> 0.5f,\nbatchSize -> 64,\nenableOutputLogs -> false,\nverbose -> Verbose.Silent.id,\nvalidationSplit -> 0.0f,\noutputLogsPath -> \"\"\n","854efd80":"### NOTE the classifierdl_use_atis model was trained on 5 classes and not 8\nhttps:\/\/nlp.johnsnowlabs.com\/2021\/01\/25\/classifierdl_use_atis_en.html ... so we remove the classes Spark has not seen during pretraining","a456b623":"### Performance on test","be3849a2":"### Goals - Classify the detailed user intent from short text snippets with Spark NLP (JSL)\n1. **Intent classification** using a Spark NLP pretrained model\n2. **Create & train** a new Spark NLP ClassifierDL model\n3. **NER** ... going beyond the initial benchmark of 8 classes, into flight.from, flight.to, flight.date, meal_desc, airport.name and etc. with a different Spark NLP pretrained model\n\nData from ATIS - Airline Travel Info System https:\/\/www.kaggle.com\/hassanamin\/atis-airlinetravelinformationsystem"}}