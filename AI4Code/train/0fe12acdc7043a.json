{"cell_type":{"fca72ac4":"code","3b04c63f":"code","11c8291d":"code","7500850c":"code","f32095d1":"code","aac82139":"code","4cdab96b":"code","221fea96":"code","7999c486":"code","a23fcccf":"code","49f0fbc9":"code","e812ff9e":"code","d8910598":"code","2de3d974":"code","9d8209a8":"code","6b9281f7":"code","077d5132":"code","5cdd6d13":"code","4352afac":"code","8a98f36b":"code","1ad59d10":"code","062d0228":"code","07020ca9":"code","94a79a67":"code","81b696bd":"markdown","0aa94be8":"markdown","deba0c43":"markdown","516fa752":"markdown"},"source":{"fca72ac4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b04c63f":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n%matplotlib inline","11c8291d":"df = pd.read_csv('..\/input\/classifieddata\/Classified Data', index_col=0) # set index_col=0 -> for using the first column as the index.","7500850c":"df.head()","f32095d1":"df.info()","aac82139":"#check target variable distribution\ndf['TARGET CLASS'].value_counts()","4cdab96b":"sns.countplot(x='TARGET CLASS', data=df)","221fea96":"#check the missing values\ndf.isnull().sum()","7999c486":"sns.pairplot(df, hue='TARGET CLASS')","a23fcccf":"scaler = StandardScaler()\n\nscaler.fit(df.drop('TARGET CLASS', axis=1))\nscaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))\n\ndf_scaled = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_scaled.head()","49f0fbc9":"X = df_scaled\ny = df['TARGET CLASS']","e812ff9e":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=101)","d8910598":"knn = KNeighborsClassifier(n_neighbors=1)","2de3d974":"knn.fit(X_train,y_train)","9d8209a8":"y_predict = knn.predict(X_test)","6b9281f7":"def draw_confusion_matrix(data):\n    groups = ['TN','FP','FN','TP']\n\n    counts = ['{0:0.0f}'.format(value) for value in data.flatten()]\n    labels = np.asarray([f'{v1}\\n{v2}' for v1, v2 in zip(groups, counts)]).reshape(2, 2)\n\n    sns.heatmap(data, annot=labels, cmap='Blues', cbar=False, fmt='')","077d5132":"draw_confusion_matrix(confusion_matrix(y_test, y_predict))","5cdd6d13":"print(classification_report(y_test, y_predict))","4352afac":"error = []\naccuracy = []\n\nfor i in range(1, 100):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))\n    accuracy.append(accuracy_score(y_test, pred_i))","8a98f36b":"df_accuracy = pd.DataFrame({'k': range(1,100), 'accuracy': accuracy, 'error': error})\ndf_accuracy.head()","1ad59d10":"fig, ax = plt.subplots(figsize=(10,6))\np1, = ax.plot(df_accuracy['k'], df_accuracy['accuracy'], color='red', marker='o', label='accuracy')\n\nax2 = ax.twinx()\np2, = ax2.plot(df_accuracy['k'], df_accuracy['error'], color='blue', label=\"error\")\n\nax.set_xlabel('K')\nax.set_ylabel('Accuracy')\nax2.set_ylabel('Error')\n\nax.legend(handles=[p1, p2], loc=0)\n\nplt.title('Accuracy, Error vs. K Value')\nplt.grid()\nplt.show()","062d0228":"knn = KNeighborsClassifier(n_neighbors=22)\n\nknn.fit(X_train, y_train)\ny_predict = knn.predict(X_test)","07020ca9":"draw_confusion_matrix(confusion_matrix(y_test, y_predict))","94a79a67":"print(classification_report(y_test, y_predict))","81b696bd":"<div style=\"justify-content: center;\">\n   <img src=\"https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2018\/08\/KNN-Classification.gif\" \/>\n<\/div>","0aa94be8":"we can see that that after arouns K>23 the error rate start to increase again. Therefore, let's select K as 22.","deba0c43":"### Standardize the Variables\n\nKNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.","516fa752":"### Select the Best K Value"}}