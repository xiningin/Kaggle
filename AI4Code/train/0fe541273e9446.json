{"cell_type":{"6593aca3":"code","ab4bda6c":"code","4bc12754":"code","5647fe90":"code","73f8e8ef":"code","052ee0c4":"code","aceecb61":"code","c8892ad0":"code","e1462078":"code","9ade13a6":"code","5b3c3967":"code","64dc4ad1":"code","ba4e15ff":"code","660ae4af":"code","bb9dc7cc":"code","c7f73697":"code","eeb30c4f":"code","7e5e7a4f":"code","3899f1f7":"code","ca97bf83":"code","5460d1ec":"code","1cf4d6b8":"code","004c6ee0":"code","2cef6955":"code","8881f557":"code","094d9858":"code","2088cfb2":"code","9d16a980":"code","013c9beb":"code","a2f96fdd":"code","06e30829":"code","d31c7a5f":"code","421ca3b5":"code","459b782c":"code","24f73703":"code","7917c1fa":"code","d3dacfcf":"code","e5c4a4e6":"markdown","977fee6b":"markdown","9fd3c615":"markdown","79e94a3f":"markdown","2ba8322a":"markdown","59255597":"markdown","bed0d0e3":"markdown","f18ef4d5":"markdown","4c3b15d5":"markdown","c18eca92":"markdown"},"source":{"6593aca3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ab4bda6c":"import string\nimport nltk\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,make_scorer\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nPUNCT_TO_REMOVE = string.punctuation\nSTOPWORDS = set(stopwords.words('english'))\nstemmer = PorterStemmer()","4bc12754":"# import training data\ndf_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","5647fe90":"df_train.head(2)","73f8e8ef":"def text_preprocessing(text):\n    '''\n    input: string to be processed\n    output: preprocssed string\n    '''\n    text = text.lower() # make everything lower case\n    text = re.compile(r'https?:\/\/\\S+|www\\.\\S+').sub(r'', text) #remove url\n    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)) #remove punctuation\n    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS]) #remove stop words\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    \n    return text","052ee0c4":"text_preprocessing('''#Flashflood causes #landslide in Gilgit #Pakistan Damage to 20 homes\n                   farmland roads and bridges #365disasters  http:\/\/t.co\/911F3IXRH0''')","aceecb61":"df_train['text_processed'] = df_train['text'].apply(text_preprocessing)\ndf_test['text_processed'] = df_test['text'].apply(text_preprocessing)","c8892ad0":"df_train.head()","e1462078":"X = df_train['text_processed']\ny = df_train['target']","9ade13a6":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,random_state=42)","5b3c3967":"vectorizer=TfidfVectorizer(ngram_range=(1,3),min_df=3,strip_accents='unicode', \n                           use_idf=1,smooth_idf=1, sublinear_tf=1,max_features=None)\nvectorizer.fit(list(df_train['text_processed'])+list(df_test['text_processed']))\nprint('vocab length',len(vectorizer.vocabulary_))","64dc4ad1":"X_train_onehot = vectorizer.transform(X_train).todense()\nX_val_onehot = vectorizer.transform(X_val).todense()","ba4e15ff":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(max_iter=150,penalty='l2',solver='lbfgs',random_state=0)\nlr_clf.fit(X_train_onehot, y_train)\nlr_pred = lr_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(lr_pred,y_val))\nprint(classification_report(y_val, lr_pred))","660ae4af":"from sklearn.metrics import log_loss\nlogloss_lr = log_loss(y_val,lr_clf.predict_proba(X_val_onehot))\nprint('logloss_lr:',logloss_lr)","bb9dc7cc":"from sklearn.naive_bayes import MultinomialNB\n\nmnb_clf = MultinomialNB()\nmnb_clf.fit(X_train_onehot, y_train)\nmnb_pred = mnb_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(mnb_pred,y_val))\nprint(classification_report(y_val, mnb_pred))","c7f73697":"logloss_mnb = log_loss(y_val,mnb_clf.predict_proba(X_val_onehot))\nprint('logloss_mnb:',logloss_mnb)","eeb30c4f":"rf_clf = RandomForestClassifier(random_state=0,n_estimators=100,\n                                max_depth=None, verbose=0,n_jobs=-1)\nrf_clf.fit(X_train_onehot, y_train)\nrf_pred = rf_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(rf_pred,y_val))\nprint(classification_report(y_val, rf_pred))","7e5e7a4f":"logloss_rf = log_loss(y_val,rf_clf.predict_proba(X_val_onehot))\nprint('logloss_rf:',logloss_rf)","3899f1f7":"# Fitting a simple xgboost on tf-idf\nxgb_clf = xgb.XGBClassifier(n_estimators=100,n_jobs=-1,max_depth=15,\n                            min_child_weight=3,objective='binary:logistic'\n                            ,colsample_bytree=0.4)\nxgb_clf.fit(X_train_onehot, y_train)\nxgb_predictions = xgb_clf.predict(X_val_onehot)\n\nprint('accuracy score: ',accuracy_score(xgb_predictions,y_val))\nprint(classification_report(y_val, xgb_predictions))","ca97bf83":"logloss_xgb = log_loss(y_val,xgb_clf.predict_proba(X_val_onehot))\nprint('logloss_rf:',logloss_xgb)","5460d1ec":"np.shape(X_train_onehot)[1]","1cf4d6b8":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import regularizers\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=np.shape(X_train_onehot)[1],\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0001, amsgrad=False)\nmodel.compile(optimizer= adam,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nprint(model.summary())\n\n# Train the model, iterating on the data in batches of 32 samples\nhist = model.fit(X_train_onehot, y_train,validation_data = (X_val_onehot,y_val), epochs=20, batch_size=16)","004c6ee0":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nhistory = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12));\nplt.plot(history[\"loss\"]);\nplt.plot(history[\"val_loss\"]);\nplt.title(\"Loss as function of epoch\");\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show();","2cef6955":"dnn_pred = model.predict_classes(X_val_onehot)","8881f557":"print('accuracy score: ',accuracy_score(dnn_pred,y_val))\nprint(classification_report(y_val, dnn_pred))","094d9858":"logloss_dnn = log_loss(y_val,model.predict_proba(X_val_onehot))\nprint('logloss_dnn:',logloss_dnn)","2088cfb2":"lr_predictions_val = lr_clf.predict_proba(X_val_onehot)\nmnb_predictions_val = mnb_clf.predict_proba(X_val_onehot)\n#rf_predictions_val = rf_clf.predict_proba(X_val_onehot)\nxgb_predictions_val = xgb_clf.predict_proba(X_val_onehot)\n#dnn_predictions_val = model.predict_proba(X_val_onehot).ravel()","9d16a980":"#predictions_val = 1\/5*lr_predictions_val[:,1]+1\/5*gnb_predictions_val[:,1] \\\n#                +1\/5*rf_predictions_val[:,1]+1\/5*xgb_predictions_val[:,1]+1\/5*dnn_predictions_val\npredictions_val = 1\/3*lr_predictions_val[:,1]+1\/3*mnb_predictions_val[:,1] \\\n                +1\/3*xgb_predictions_val[:,1]\n\npredictions_val = np.where(predictions_val>0.5, 1, 0)","013c9beb":"print('accuracy score: ',accuracy_score(predictions_val,y_val))\nprint(classification_report(y_val, predictions_val))","a2f96fdd":"df_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","06e30829":"df_test.head()","d31c7a5f":"df_test['text_processed'] = df_test['text'].apply(text_preprocessing)","421ca3b5":"X_test = df_test['text_processed']\nX_test_onehot = vectorizer.transform(X_test).todense()","459b782c":"lr_predictions = lr_clf.predict_proba(X_test_onehot)\nmnb_predictions = mnb_clf.predict_proba(X_test_onehot)\n#rf_predictions = rf_clf.predict_proba(X_test_onehot)\nxgb_predictions = xgb_clf.predict_proba(X_test_onehot)\n#dnn_predictions = model.predict_proba(X_test_onehot).ravel()","24f73703":"#predictions = 1\/5*lr_predictions[:,1]+1\/5*gnb_predictions[:,1]+1\/5*rf_predictions[:,1]+1\/5*xgb_predictions[:,1]+1\/5*dnn_predictions\npredictions = 1\/3*lr_predictions[:,1]+1\/3*mnb_predictions[:,1]+1\/3*xgb_predictions[:,1]","7917c1fa":"predictions = np.where(predictions>0.5, 1, 0)","d3dacfcf":"df_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ndf_submission['target'] = predictions\ndf_submission.to_csv('submission.csv',index=False)","e5c4a4e6":"### RandomForest","977fee6b":"## Prediction on test set","9fd3c615":"### Averaging","79e94a3f":"Next, we will try RF, XGBoost and DNN","2ba8322a":"### XGBoost","59255597":"### Train Test Split","bed0d0e3":"### Baseline Model: Logistic Regression","f18ef4d5":"### TEXT PREPROCESSING","4c3b15d5":"### DNN","c18eca92":"### Naive Bayes"}}