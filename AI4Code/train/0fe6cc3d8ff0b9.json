{"cell_type":{"5e6d9e63":"code","3c1047f2":"code","f66bb7a7":"code","da7bd46b":"code","9f8b65cc":"code","93fbfd60":"code","b961de08":"code","fa8f9020":"code","40bd9bff":"code","358c7557":"code","93003292":"code","1aed12a4":"code","0d3c253e":"code","96d6b211":"code","5eff7ed9":"code","4361ebde":"code","5cb2320d":"code","c7dc96ac":"code","eb68df5b":"markdown","a983b7d8":"markdown","8b1bcb84":"markdown","0f29b368":"markdown","db1fb135":"markdown","aca209ff":"markdown","d8f3ac65":"markdown","fc2868f8":"markdown","f6da2184":"markdown"},"source":{"5e6d9e63":"# Importar os principais pacotes\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nsns.set()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport datetime\nimport gc\n\n# Evitar que aparece os warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Seta algumas op\u00e7\u00f5es no Jupyter para exibi\u00e7\u00e3o dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False","3c1047f2":"# Importa os pacotes de algoritmos\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n\n# Importa pacotes do sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, log_loss\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA, TruncatedSVD\n","f66bb7a7":"def read_data():\n    \n    if TRAIN_OFFLINE:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('..\/dataset\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n        \n        print('Carregando arquivo dataset_teste.csv....')\n        test = pd.read_csv('..\/dataset\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n        \n    else:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n        \n        print('Carregando arquivo dataset_treino.csv....')\n        test = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n    \n    return train, test","da7bd46b":"# Leitura dos dados\ntrain, test = read_data()","9f8b65cc":"# Juntando os datasets de treino e teste para realizar todos os tratamentos nos dados\ndf = train.append(test)","93fbfd60":"# Transformando as features categorias com LabelEncoder\nle = LabelEncoder()\n\nfor i, col in enumerate(df):\n    if df[col].dtype == 'object':\n        df[col] = le.fit_transform(np.array(df[col].astype(str)).reshape((-1,)))\n        \n        \n# Realizando tratamento de missing value\nfor c in df.columns:\n    if c != 'ID' and c != 'target':\n        df[c].fillna(df[c].mean(),inplace=True)\n","b961de08":"# Criando novas features atraces do PCA \/ ICA \/ GRP e SRP\n\nn_comp = 4\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=42)\ntsvd_results_df = tsvd.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n\n# PCA\npca = PCA(n_components=n_comp, random_state=42)\npca2_results_df = pca.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=42)\nica2_results_df = ica.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=42)\ngrp_results_df = grp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=42)\nsrp_results_df = srp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n\n\n# Append dos componentes com o dataset\nfor i in range(1, n_comp+1):\n    df['pca_' + str(i)]  = pca2_results_df[:,i-1]\n    df['ica_' + str(i)]  = ica2_results_df[:,i-1]\n    df['tsvd_' + str(i)] = tsvd_results_df[:,i-1]\n    df['grp_' + str(i)]  = grp_results_df[:,i-1]\n    df['srp_' + str(i)]  = srp_results_df[:,i-1] ","fa8f9020":"# Patronizacao dos dados\nscaler = StandardScaler()\nfor c in df.columns:\n    if c != 'ID' and c != 'target':\n        df[c] = scaler.fit_transform(df[c].values.reshape(-1, 1))","40bd9bff":"df.head()","358c7557":"# Separa dataset de treino e teste depois de aplicar Feature Engineering\ntreino = df[df['target'].notnull()]\nteste = df[df['target'].isnull()]\n\n# Separando features preditoras e target\ntrain_x = treino.drop(['ID','target'], axis=1)\ntrain_y = treino['target']\n\n# Removendo ID dataset de teste\ntest_x = teste.drop(['ID','target'], axis=1)","93003292":"# Criando uma funcao para cria\u00e7\u00e3o, execu\u00e7\u00e3o e valida\u00e7\u00e3o do modelo\ndef run_model_xgb(X_tr, y_tr, useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n    \n    # Criando o modelo XGB com todas as otimiza\u00e7\u00f5es\n    modelo = XGBClassifier(learning_rate = 0.1, \n                              n_estimators = 1000, \n                              max_depth = 4,\n                              min_child_weight = 1, \n                              gamma = 0, \n                              subsample = 0.7, \n                              colsample_bytree = 0.6,\n                              reg_alpha = 0.005,\n                              objective = 'binary:logistic', \n                              n_jobs = -1,\n                              scale_pos_weight = 1, \n                              seed = 42)\n\n    # Utiliza\u00e7\u00e3o do Cross-Validation\n    if useTrainCV:\n        xgb_param = modelo.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_tr, label=y_tr)\n        \n        print ('Start cross validation')\n        cvresult = xgb.cv(xgb_param, \n                          xgtrain, \n                          num_boost_round=modelo.get_params()['n_estimators'], \n                          nfold=cv_folds,\n                          metrics=['logloss'],\n                          stratified=True,\n                          seed=42,\n                          verbose_eval=True,\n                          early_stopping_rounds=early_stopping_rounds)\n\n        modelo.set_params(n_estimators=cvresult.shape[0])\n        best_tree = cvresult.shape[0]\n        print('Best number of trees = {}'.format(best_tree))\n    \n    # Fit do modelo\n    modelo.fit(X_tr, y_tr, eval_metric='logloss')\n        \n    # Predi\u00e7\u00e3o no dataset de treino\n    train_pred = modelo.predict(X_tr)\n    train_pred_prob = modelo.predict_proba(X_tr)[:,1]\n    \n    # Exibir o relatorio do modelo\n    print(\"Log Loss (Treino): %f\" % log_loss(y_tr, train_pred_prob))\n    print(\"Log Loss (Test): %f\" % cvresult['test-logloss-mean'][best_tree-1])\n    \n    return modelo","1aed12a4":"%%time\n\nmodeloXGB = run_model_xgb(train_x, train_y)","0d3c253e":"# Configura\u00e7\u00f5es Gerais\nSTRATIFIED_KFOLD = False\nRANDOM_SEED = 737851\nNUM_THREADS = 4\nNUM_FOLDS = 10\nEARLY_STOPPING = 1000\n\nLIGHTGBM_PARAMS = {\n    'boosting_type': 'goss',#'gbdt',\n    'n_estimators': 10000,\n    'learning_rate': 0.005134,\n    'num_leaves': 54,\n    'max_depth': 10,\n    'subsample_for_bin': 240000,\n    'reg_alpha': 0.436193,\n    'reg_lambda': 0.479169,\n    'colsample_bytree': 0.508716,\n    'min_split_gain': 0.024766,\n    'subsample': 1,\n    'is_unbalance': False,\n    'silent':-1,\n    'verbose':-1\n}","96d6b211":"def run_model_lgb(data):\n    df = data[data['target'].notnull()]\n    test = data[data['target'].isnull()]\n    del_features = ['ID','target']\n    predictors = list(filter(lambda v: v not in del_features, df.columns))\n    \n    print(\"Train\/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n\n    if not STRATIFIED_KFOLD:\n        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n    else:\n        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n\n    oof_preds = np.zeros(df.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    importance_df = pd.DataFrame()\n    eval_results = dict()\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['target'])):\n        train_x, train_y = df[predictors].iloc[train_idx], df['target'].iloc[train_idx]\n        valid_x, valid_y = df[predictors].iloc[valid_idx], df['target'].iloc[valid_idx]\n\n        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n        \n        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n        \n        clf.fit(train_x, train_y, \n                eval_set=[(train_x, train_y), (valid_x, valid_y)],\n                eval_metric='logloss', \n                verbose=400, \n                early_stopping_rounds= EARLY_STOPPING)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        print('Fold %2d Log Loss : %.6f' % (n_fold + 1, log_loss(valid_y, oof_preds[valid_idx])))\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full Log Loss score %.6f' % log_loss(df['target'], oof_preds))\n        \n    return clf","5eff7ed9":"%%time\n\nmodeloLGB = run_model_lgb(df)","4361ebde":"# Submission XGB\nsubmissionXGB = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/sample_submission.csv')\nsubmissionXGB['PredictedProb'] = modeloXGB.predict_proba(test_x)[:,1]\nplt.hist(submissionXGB.PredictedProb)\nplt.show()","5cb2320d":"# Submission LGB\nsubmissionLGB = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/sample_submission.csv')\nsubmissionLGB['PredictedProb'] = modeloLGB.predict_proba(test_x)[:,1]\nplt.hist(submissionLGB.PredictedProb)\nplt.show()","c7dc96ac":"submissionEnsemble = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/sample_submission.csv')\nsubmissionEnsemble['PredictedProb'] = submissionXGB['PredictedProb'] * 0.7 + submissionLGB['PredictedProb'] * 0.3\nsubmissionEnsemble.to_csv('submission_ensemble_v1.0.6.csv', index=False)\nplt.hist(submissionEnsemble.PredictedProb)\nplt.show()","eb68df5b":"# Algoritmo LightGBM","a983b7d8":"# Algoritmo XGBoost - Extreme Gradient Boosting","8b1bcb84":"# Ensemble Submissions","0f29b368":"# Kaggle\n## Competi\u00e7\u00e3o DSA de Machine Learning - Dezembro 2019","db1fb135":"# Submissions","aca209ff":"# Feature Engineering","d8f3ac65":"# Carregando os dados de treino e teste","fc2868f8":"# Importando as bibliotecas","f6da2184":"# Criar e avaliar alguns algoritmos de Machine Learning"}}