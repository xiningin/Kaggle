{"cell_type":{"559fee92":"code","e5ba1f51":"code","ba4806fa":"code","1a00f9d4":"code","f83c485c":"code","eebac48c":"code","0ffdbc12":"code","9df2fc56":"code","16910420":"code","2c79c080":"code","3d208bd6":"code","dba168c3":"code","7a63d2dd":"code","7ab34390":"code","9cdc84e0":"code","36688a6b":"code","abe8a1cd":"code","a764ee02":"code","85e77836":"code","fcb30f50":"code","ea49de9a":"code","324d4609":"markdown","1aca676f":"markdown","a0a6b6e1":"markdown","56f96cc5":"markdown","51b0a615":"markdown","26d97af6":"markdown","c5cd17e4":"markdown"},"source":{"559fee92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport scipy.stats as stats\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5ba1f51":"#Data Importing and EDA\n\ntrain_data=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_data.info()\ntest_data.info()\n\n\n","ba4806fa":"#label and identify numerical and categorical, useful for further EDA\nlabels_num = list(train_data.select_dtypes([np.number]).columns)\nlabels_num_test=list(labels_num)\nlabels_num_test.remove(\"SalePrice\")\nlabels_cat=list(train_data.select_dtypes([object]).columns)\nlabels_cat_test=list(test_data.select_dtypes([object]).columns)\ntrain_data_num=train_data[labels_num]\ntrain_data_cat=train_data[labels_cat]\n#should include target variable\ntrain_data_cat[\"SalePrice\"]=train_data[\"SalePrice\"]\n#remove ID\ndel labels_num[0]","1a00f9d4":"#check correlations and focus on highly correlated variables to check for outliers\n\nnum_labels=(train_data_num.corr().loc[:,\"SalePrice\"].sort_values(ascending=False\n                                                                ))\nprint(num_labels.head(10))\nprint(num_labels.tail(10))\n\n#first one is the target variable.\n\nfor i in num_labels.index[1:10]:\n    fig=plt.figure()\n    sns.regplot(x=i,y=\"SalePrice\",data=train_data)\n    plt.show","f83c485c":"index=train_data[train_data[\"GrLivArea\"]>4000].index\ntrain_data.drop(index,axis=0)\n\nindex=train_data[train_data[\"SalePrice\"]>700000].index\ntrain_data.drop(index,axis=0)\n","eebac48c":"y=train_data.SalePrice\nsns.distplot(y, fit=stats.norm, hist_kws=dict(edgecolor='w',linewidth=2))\nfig=plt.figure()\nres=stats.probplot(y, plot=plt)","0ffdbc12":"y=np.log1p(y)\nsns.distplot(y, fit=stats.norm, hist_kws=dict(edgecolor='w',linewidth=2))\nfig=plt.figure()\nres=stats.probplot(y, plot=plt)","9df2fc56":"num_labels_na = (train_data_num.isnull().sum() \/ len(train_data_num)) * 100\nnum_labels_na = num_labels_na.drop(num_labels_na[num_labels_na == 0].index).sort_values(ascending=False)\n\nprint(\"Percentage of NaNs:\\n\"+str(num_labels_na))","16910420":"train_data[\"LotFrontage\"].fillna(train_data[\"LotFrontage\"].median(),inplace=True)\ntrain_data[\"GarageYrBlt\"].fillna(value=0,inplace=True)\ntrain_data[\"MasVnrArea\"].fillna(train_data[\"MasVnrArea\"],inplace=True)\n\ntest_data[\"LotFrontage\"].fillna(test_data[\"LotFrontage\"].median(),inplace=True)\ntest_data[\"GarageYrBlt\"].fillna(value=0,inplace=True)\ntest_data[\"MasVnrArea\"].fillna(test_data[\"MasVnrArea\"],inplace=True)\n\n#check for remaining NaNs\n\nprint(train_data[labels_num].isnull().sum().nlargest(10))\nprint(test_data[labels_num_test].isnull().sum().nlargest(10))","2c79c080":"#Fill the rest of the test data NAs with median\ntrain_data=train_data.fillna(train_data[labels_num].median())\nprint(train_data[labels_num].isnull().sum().nlargest(10))\n\ntest_data=test_data.fillna(test_data[labels_num_test].median())\nprint(test_data[labels_num_test].isnull().sum().nlargest(10))\n","3d208bd6":"train_data['TotalSF']=train_data['TotalBsmtSF']+train_data['GrLivArea']+train_data['1stFlrSF']+train_data['2ndFlrSF']\ntrain_data['TotalBath']=train_data['BsmtHalfBath']+train_data['BsmtFullBath']+train_data['HalfBath']+train_data['FullBath']\ntest_data['TotalSF']=test_data['TotalBsmtSF']+test_data['GrLivArea']+test_data['1stFlrSF']+test_data['2ndFlrSF']\ntest_data['TotalBath']=test_data['BsmtHalfBath']+test_data['BsmtFullBath']+test_data['HalfBath']+test_data['FullBath']\n\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.scatter(train_data.TotalSF,train_data.SalePrice)\nplt.xlabel('TotalSF')\nplt.ylabel('SalePrice')\nplt.subplot(1,2,2)\nplt.scatter(train_data.TotalBath,train_data.SalePrice)\nplt.xlabel('TotalBath')\nplt.ylabel('SalePrice')\n","dba168c3":"# categoricals EDA\n#NaN have a meaning in Categoticals, i replace them with \"NNN\"\ncat_na = (train_data_cat.isnull().sum() \/ len(train_data_cat)) * 100\ncat_na = cat_na.drop(cat_na[cat_na == 0].index).sort_values(ascending=False)\n\nprint(cat_na)\n\n\n","7a63d2dd":"#fill NA values\ntrain_data=train_data.fillna(value=\"None\")\ntest_data=test_data.fillna(value=\"None\")\n\nprint(train_data.isnull().sum())\nprint(test_data.isnull().sum())\n","7ab34390":"\nfor i in range(len(labels_cat)):\n    fig=plt.figure(figsize=[15,4])\n    g=sns.boxplot(x=labels_cat[i],y=\"SalePrice\", data=train_data)\n    g.set_xticklabels(g.get_xticklabels(),rotation=30)\n    plt.show()\n","9cdc84e0":"encoded = pd.get_dummies(train_data)\ntest_encoded = pd.get_dummies(test_data)\n\na=list(encoded.columns)\nb=list(test_encoded.columns)\n            \nlist1 = list(set(a)-set(b))\nlist1.remove(\"SalePrice\")\nprint(list1)\n\nencoded=encoded.drop(list1,axis=1)\n\nlist2=list(set(b)-set(a))\nprint(list2)\n\ntest_encoded=test_encoded.drop(list2,axis=1)\n\nprint(encoded.shape)\nprint(test_encoded.shape)\n            ","36688a6b":"#select 50% best features. Use RFE, because Both categorical and numerical data\n\nX=encoded.drop([\"SalePrice\",\"Id\"],axis=1).values\n\nrf = RandomForestRegressor(n_estimators=300,random_state=2)\n\nrf.fit(X,y)\n\n# Create a pd.Series of features importances\nimportances_rf = pd.Series(rf.feature_importances_, index = encoded.drop([\"SalePrice\",\"Id\"],axis=1).columns)\n# Sort importances_rf: 50% most informative features\nsorted_importances_rf = importances_rf.sort_values().nlargest(round(len(importances_rf)\/2))\n# Make a horizontal bar plot\nfig=plt.figure(figsize=[10,20])\nsorted_importances_rf.plot(kind='barh', color='lightgreen'); \nplt.show()","abe8a1cd":"from sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline\n\nX=encoded.loc[:,list(sorted_importances_rf.index)].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n\n#regr=Pipeline(steps=[(\"scaler\",RobustScaler()),(\"SVR\",SVR())])\n#regr=Pipeline(steps=[(\"scaler\",RobustScaler()),(\"net\",ElasticNet(max_iter=100000))])\nregr=Pipeline(steps=[(\"scaler\",StandardScaler()),(\"lasso\",Lasso(random_state=1,max_iter=100000))])\n\n#parameters = {\"net__alpha\":np.arange(0.0001,0.001, 0.0001),\"net__l1_ratio\":np.arange(0.25,1,0.05)}\nparameters = {\"lasso__alpha\":np.arange(0.0001,0.001, 0.0001)}\n#parameters = {\"SVR__C\":np.arange(0.5,5,0.5),\"SVR__gamma\":np.arange(0.001,0.01,0.001)}\n\n\ncv = GridSearchCV(regr, param_grid=parameters)\ncv.fit(X_train, y_train)\n\nbest_hyperparams = cv.best_params_\nprint('Best hyerparameters:\\n', best_hyperparams)\n\n# Extract best model from 'grid_rf'\nlasso_best_model = cv.best_estimator_\n\n# Predict the test set labels\ny_pred = lasso_best_model.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = MSE(y_test, y_pred)**(1\/2)\n# Print the test set RMSE\nprint('Test set RLMSE of Lasso: {:.3f}'.format(rmse_test))\n\n#cv = GridSearchCV(pipeline, param_grid=parameters)\n#cv.fit(X_train, y_train)\n#y_pred = cv.predict(X_test)\n\n","a764ee02":"#run random forest with best features\n\nX=encoded.loc[:,list(sorted_importances_rf.index)].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n\nparam_grid = { \n    'max_features': [\"auto\"],\n    'max_depth' : [8],\n    \"n_estimators\":[500]\n    }\n\nrf = RandomForestRegressor(random_state=2)\n\nCV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)\nCV_rf.fit(X_train, y_train)\n\nbest_hyperparams = CV_rf.best_params_\nprint('Best hyerparameters:\\n', best_hyperparams)\n\n# Extract best model from 'grid_rf'\nrf_best_model = CV_rf.best_estimator_\n# Predict the test set labels\ny_pred = rf_best_model.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = MSE(y_test, y_pred)**(1\/2)\n# Print the test set RMSE\nprint('Test set RLMSE of rf: {:.3f}'.format(rmse_test))\n","85e77836":"# let s try another algorithm, GradientBoosting\n\nX=encoded.loc[:,list(sorted_importances_rf.index)].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=1)\n\nparam_grid = { \n    'max_features': ['sqrt'],\n    'max_depth' : [2],\n    \"n_estimators\":[1000]\n    }\n\nrf = GradientBoostingRegressor(random_state=2)\n\nCV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)\nCV_rf.fit(X_train, y_train)\n\nbest_hyperparams = CV_rf.best_params_\nprint('Best hyerparameters:\\n', best_hyperparams)\n\n# Extract best model from 'grid_rf'\nGBbest_model = CV_rf.best_estimator_\n# Predict the test set labels\ny_pred = GBbest_model.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = MSE(y_test, y_pred)**(1\/2)\n# Print the test set RMSE\nprint('Test set RLMSE of GradientBoost: {:.3f}'.format(rmse_test))\n\n","fcb30f50":"fig,ax=plt.subplots(1,2,figsize=[15,5])\n\nsns.regplot(x=y_test,y=y_pred,ax=ax[0])\nsns.residplot(x=y_test,y=y_pred,ax=ax[1])","ea49de9a":"#submission\n\ntest_encoded=test_encoded[list(sorted_importances_rf.index)]\nX=test_encoded.values\n\ny_pred = GBbest_model.predict(X)\ny_pred=np.expm1(y_pred)\n\n#output\noutput = pd.DataFrame({'Id': test_data.Id,'SalePrice': y_pred})\noutput.to_csv('submission.csv', index=False)\n\nprint(y_pred)","324d4609":"We create two additional variables that we expect would be important based on basic domain knowledge:\n\nTotalSF (sum of basement, ground floor, first floor and second floor area)\nTotalBath (total number of bathrooms - sum of bathroom variables)","1aca676f":"Skewness of Target\n\nWe address the skewness of the target that we observed in the univariate analysis by using a log transformation. There are two primary sources of motivation for doing this.\n\nFirstly, having a more symmetric distribution should hopefully result in an improvement to the mean square error, with more samples on the right for the algorithm to learn better.\n\nSecondly, given that we are dealing with large values with large variation, it makes a lot of sense to work with the logarithm to aid in both interpretation and improving the fit. This is because a multiplicative model on the original target would correspond to an additive model on the log target.\n","a0a6b6e1":"Large Number of Columns, with numerical and categorical data. We will first look into numerical data, evaluate correlation, outliers, NaNs.","56f96cc5":"Next I want to check numerical variables NaNs and see how to deal with them. From the description, many of them are due to house amenities or features not present, so they do have a meaning. ","51b0a615":"Trying different Regressors. SVR gives a score of 0.20. \nLasso and Elastic Net give a better result, comparable between the two.\n","26d97af6":"From the Living Area Scatter plot we ideintify some points that we might want to remove, as they seem outliers. Same outliers seem to appear also on the other scatter plots.","c5cd17e4":"Gradient Boost best model so far!"}}