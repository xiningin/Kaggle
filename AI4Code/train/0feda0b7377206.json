{"cell_type":{"3dab0c36":"code","2246e48d":"code","382a745b":"code","9cd9cc9a":"code","785b42e0":"code","5ae88246":"code","0dc8ca01":"code","08de701b":"code","6a33cbc9":"code","401ab164":"code","acc084d0":"code","62dad8dd":"code","7aa958b0":"code","343052f4":"code","7af0e3f6":"code","9aed94d8":"code","37f871e9":"code","2f12727a":"code","b806c67a":"code","efa9dbc4":"markdown","df7cb99e":"markdown","e5e33195":"markdown","a2024f91":"markdown","5f3202c9":"markdown","c7792be3":"markdown","c849e4a6":"markdown","befecabd":"markdown","d870a0ca":"markdown","2ddeded3":"markdown","d4fcff7b":"markdown","5a32763a":"markdown","bfd78283":"markdown","ea4d7781":"markdown","c70b39e6":"markdown","61b451a5":"markdown"},"source":{"3dab0c36":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom random import choices\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import accuracy_score\n\nRS = 69420\ntf.random.set_seed(RS)\nnp.random.seed(RS)","2246e48d":"train = pd.read_hdf(\"..\/input\/janesthdf\/train.hdf5\")\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]","382a745b":"train['action'] = ((train['resp'].values) > 0).astype(int)\nfeatures = [c for c in train.columns if \"feature\" in c]\nf_mean = np.mean(train[features[1:]].values,axis=0)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","9cd9cc9a":"X = train.loc[:, train.columns.str.contains('feature')]\ny = train['action']","785b42e0":"from sklearn.model_selection import train_test_split\n# 60, 20, 20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RS)","5ae88246":"from sklearn.impute import SimpleImputer\nimpute = SimpleImputer(missing_values = np.nan)\n\nX_train = impute.fit_transform(X_train)\nX_test = impute.transform(X_test)\nX_val = impute.transform(X_val)","0dc8ca01":"callback = EarlyStopping(monitor='val_loss', patience=3, verbose=1)","08de701b":"def create_mlp(num_columns, label_smoothing):\n    \n    model = Sequential()\n    model.add(Input(shape=(num_columns,)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(1280))\n    model.add(BatchNormalization())\n    model.add(Activation(tf.keras.activations.swish))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(1))\n    model.add(Activation(\"sigmoid\"))\n    \n    model.compile(\n                    optimizer = 'adam',\n                    loss = BinaryCrossentropy(label_smoothing = label_smoothing),\n                    metrics= [AUC(), 'acc']\n                 )\n\n    return model","6a33cbc9":"label_smoothing = 1e-2\n\n# clf = create_mlp(len(features), label_smoothing)","401ab164":"# clf.summary()","acc084d0":"batch_size = 2560","62dad8dd":"# history = clf.fit(X_train, y_train, validation_data = (X_val, y_val), callbacks = callback, epochs=1000, batch_size = batch_size)","7aa958b0":"# history = pd.DataFrame(clf.history.history)\n# history[['val_loss', 'loss']].plot()","343052f4":"# y_pred = clf.predict_proba(X_test)\n# y_valpred = clf.predict(X_val)","7af0e3f6":"# If you have any recommendations to tidy this code up please comment below\n\n# th = 0.5\n# y_preds = []\n# y_valpreds = []\n\n# for i in y_pred:\n#     if i < 0.5:\n#         y_preds.append(0)\n#     else:\n#         y_preds.append(1)\n    \n# for i in y_valpred:\n#     if i < 0.5:\n#         y_valpreds.append(0)\n#     else:\n#         y_valpreds.append(1)","9aed94d8":"# mean_acc = np.mean((accuracy_score(y_test, y_preds),\n#                     accuracy_score(y_val, y_valpreds)))\n\n# print(\"The Testing Accuracy is: {}\".format(accuracy_score(y_test, y_preds)))\n# print(\"The Validation Accuracy is: {}\".format(accuracy_score(y_val, y_valpreds)))\n# print(\"The Mean Accuracy is: {}\".format(mean_acc))","37f871e9":"# clf.save('wide_boi.hdf5')","2f12727a":"from tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\nclf = tf.keras.models.load_model(\"..\/input\/wide-boi-3\/wide_boi.hdf5\")\n\nf_mean = 0.2674099\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","b806c67a":"# Submission Code Thanks to: https:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn\n\nmodels = []\n\nmodels.append(clf)\n\nth = 0.5000\n\n\nf = np.median\nmodels = models[-3:]\nimport janestreet\nenv = janestreet.make_env()\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","efa9dbc4":"# **Building the   *W I D E*   boy**","df7cb99e":"**You can see we only use values after day 85, this is due to all the excellent exploration notebooks which have shown JaneSt to recalibrate models post day 85**\n\n**I am using a HDF5 version of the training data because it loads nearly 10x faster than csv's, let me know if you would like access**","e5e33195":"**To prepare this notebook for submission first actually run and train the model (all lines above) then save and download the model. Comment out all the code above when ready to submit and then after this you will be able to re-upload the model to kaggle and load it in again below for submission**","a2024f91":"# **Submission**","5f3202c9":"*More Robust application of this model as it utilizes early stopping upon the validation set we split out*","c7792be3":"# **Train dat bih**","c849e4a6":"# **Submitting Predictions**","befecabd":"Remember that ***Accuracy is not everything*** this is a markets related competition, the markets are so dynamic there is no need for extreme accuracy, rather we want to focus on the gerneralisability of the model","d870a0ca":"**Use callbacks because we hate overfitting**","2ddeded3":"* *Tbh I dont really know why I chose to train a mega-wide network, largely its for the memes and so I can say   W I D E B O I*  \n* I would appreciate any academic rigour all you wonderful kagglers could provide!","d4fcff7b":"# **Importing Libraries**","5a32763a":"# **Results**","bfd78283":"**If you found this notebook helpful please upvote and comment, it really helps me out!**","ea4d7781":"**If you found this notebook helpful please upvote and comment, it really helps me out!**","c70b39e6":"# **Train-Test-Validation**","61b451a5":"# **Loading Data**"}}