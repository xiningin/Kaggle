{"cell_type":{"95effed6":"code","8e96c0ff":"code","50f9e8e7":"code","80d23610":"code","2fc98d52":"code","1feb8188":"code","50c68ded":"code","cdd297e6":"code","a41849c3":"code","0e2509b8":"code","ba32d787":"code","6f9725ab":"code","a7a8d397":"code","cb03d85d":"code","9eb70de3":"code","87dbd21f":"code","d790e056":"code","c21568b6":"code","0f47c805":"code","2e23663f":"code","d470ca48":"code","28481dd8":"code","1796b13a":"code","e8fd64eb":"code","d3240ca6":"code","6c755259":"code","8d82f761":"code","a115a0ad":"code","b5a6b4bf":"code","5a7f801b":"code","25d9d9de":"code","8db34769":"code","cf25913c":"code","687d4ab7":"code","805a452d":"code","bead09fd":"code","17a7dc65":"code","d7162459":"code","482d31b2":"code","8bf4b3ee":"code","5ed49ab3":"code","5197d293":"markdown","8087d6ec":"markdown","b85fb5d7":"markdown","f8e17dba":"markdown","f2c1fa44":"markdown","bb8ea495":"markdown","faf83528":"markdown","e344aa57":"markdown","d3cf052f":"markdown","82c5ec6b":"markdown","071397cb":"markdown","34d23fea":"markdown","9b83fab1":"markdown","6f582c4a":"markdown","ed162c36":"markdown","b1a947f9":"markdown","260417b4":"markdown","01048924":"markdown","c6822da6":"markdown","313ec4ad":"markdown"},"source":{"95effed6":"#from google.colab import drive\n#drive.mount('\/content\/gdrive')","8e96c0ff":"import nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","50f9e8e7":"#train  =  pd.read_csv('\/content\/gdrive\/My Drive\/Explore\/train.csv' )\n#test = pd.read_csv('\/content\/gdrive\/My Drive\/Explore\/test.csv' )\n\ntrain  =  pd.read_csv('train.csv' )\ntest = pd.read_csv('test.csv' )","80d23610":"train.head(3)","2fc98d52":"train.isnull().sum()","1feb8188":"train.shape","50c68ded":"train.iloc[1,1].split('|||')[:10]","cdd297e6":"\nprint('We have : ',len(train.iloc[1,1].split('||')), ' posts in each row.')","a41849c3":"train['type'].unique()","0e2509b8":"total = train.groupby(['type']).count()*50\ntotal #### show the total dataframe","ba32d787":"\nplt.figure(figsize = (12,6))\n\nplt.bar(np.array(total.index), height = total['posts'],)\nplt.xlabel('Personality types', size = 14)\nplt.ylabel('Number of posts available', size = 14)\nplt.title('Total posts for each personality type')","6f9725ab":"\ntrain['mind'] = train['type'].apply(lambda x: x[0] == 'E').astype('int')\ntrain['energy'] = train['type'].apply(lambda x: x[1] == 'N').astype('int')\ntrain['nature'] = train['type'].apply(lambda x: x[2] == 'T').astype('int')\ntrain['tactics'] = train['type'].apply(lambda x: x[3] == 'J').astype('int')","a7a8d397":"train.head()","cb03d85d":"messages=pd.concat([train,test],join='inner')","9eb70de3":"messages.info()","87dbd21f":"pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\nmessages['posts'] = messages['posts'].replace(to_replace = pattern_url, value = subs_url, regex = True)","d790e056":"messages.head()","c21568b6":"def remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)","0f47c805":"messages['posts'] = messages['posts'].apply(remove_punctuation)","2e23663f":"messages.head()","d470ca48":"#### let's confirm if we correctly got stopwords\nstopwords.words('english')[0:10] ","28481dd8":"sw = stopwords.words('english')","1796b13a":"def remove_stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)","e8fd64eb":"messages['posts'] = messages['posts'].apply(remove_stopwords)","d3240ca6":"messages.head(3)","6c755259":"#### let's take the words to their root source\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","8d82f761":"messages['posts'] = messages['posts'].apply(stemming)","a115a0ad":"#count_vectorizer = CountVectorizer()","b5a6b4bf":"tfid_vectorizer = TfidfVectorizer(\"english\")","5a7f801b":"\n#print(np.mean(train.mind==1))\n#print(np.mean(train.energy==1))\n#print(np.mean(train.nature==1))\n#print(np.mean(train.tactics==1))","25d9d9de":"\n\nmsg_train1, msg_test1, label_train1, label_test1 =train_test_split(messages['posts'].iloc[0:6506], train['mind'])\nmsg_train2, msg_test2, label_train2, label_test2 =train_test_split(messages['posts'].iloc[0:6506], train['energy'])\nmsg_train3, msg_test3, label_train3, label_test3 =train_test_split(messages['posts'].iloc[0:6506], train['nature'])\nmsg_train4, msg_test4, label_train4, label_test4 =train_test_split(messages['posts'].iloc[0:6506], train['tactics'])","8db34769":"#from sklearn.pipeline import Pipeline\n\n#pipeline = Pipeline([\n #   ('bow', CountVectorizer(ngram_range=(1,3),min_df=2,max_df=1.0)),#,binary=True)),  # strings to token integer counts\n  #  ('tfidf', TfidfTransformer()),\n   # ('model',LogisticRegression(solver='lbfgs',multi_class='ovr',C=5,class_weight='balanced') ),# integer counts to weighted TF-IDF scores,# train on TF-IDF vectors w\/ Naive Bayes classifier\n#])","cf25913c":"#### fit the train for the mind\n\npipeline.fit(msg_train1,label_train1)\npredictions1 = pipeline.predict(msg_test1)\ny_prob=pipeline.predict_proba(msg_test1)\ny_thresh = np.where(y_prob[:,1] > 0.77, 1, 0)\n\n","687d4ab7":"#### print the results for mind\n\nprint(accuracy_score(predictions1,label_test1)*100)\nprint(log_loss(predictions1,label_test1))\npipeline.fit(messages['posts'].iloc[0:6506],train['mind'])\npredictions1=pipeline.predict(messages['posts'].iloc[6506:])\ny_thresh1=pd.Series(predictions1)\ny_thresh1[:10]","805a452d":"#### fit for the energy trait\n\npipeline.fit(msg_train2,label_train2)\npredictions2 = pipeline.predict(msg_test2)\ny_prob=pipeline.predict_proba(msg_test2)\ny_thresh2 = np.where(y_prob[:,1] > 0.14, 1, 0)\n","bead09fd":"#### print the Results for energy\nprint(accuracy_score(predictions2,label_test2)*100)\nprint(log_loss(predictions2,label_test2))\npipeline.fit(messages['posts'].iloc[0:6506],train['energy'])\npredictions2=pipeline.predict(messages['posts'].iloc[6506:])\ny_thresh2=pd.Series(predictions2)\ny_thresh2[:10]","17a7dc65":"#### fit for the nature trait\n\npipeline.fit(msg_train3,label_train3)\npredictions3 = pipeline.predict(msg_test3)\ny_prob=pipeline.predict_proba(msg_test3)\ny_thresh3 = np.where(y_prob[:,1] > 0.54, 1, 0)","d7162459":"#### print the Results for energy\nprint(accuracy_score(predictions3,label_test3)*100)\nprint(log_loss(predictions3,label_test3))\npipeline.fit(messages['posts'].iloc[0:6506],train['nature'])\npredictions3=pipeline.predict(messages['posts'].iloc[6506:])\ny_thresh3=pd.Series(predictions3)\ny_thresh3[:10]","482d31b2":"\n#### fit for the tactics trait\n\npipeline.fit(msg_train4,label_train4)\npredictions4 = pipeline.predict(msg_test4)\ny_prob=pipeline.predict_proba(msg_test4)\ny_thresh4 = np.where(y_prob[:,1] > 0.60, 1, 0)","8bf4b3ee":"#### print the Results for nature\nprint(accuracy_score(predictions4,label_test4)*100)\nprint(log_loss(predictions4,label_test4))\npipeline.fit(messages['posts'].iloc[0:6506],train['tactics'])\npredictions4=pipeline.predict(messages['posts'].iloc[6506:])\ny_thresh4=pd.Series(predictions4)\ny_thresh4[:10]\n","5ed49ab3":"\n#### Save our results for kaggle\n\nvalues=pd.concat([y_thresh1,y_thresh2,y_thresh3,y_thresh4],axis=1)\nvalues.index=test.index\nvalues=values.rename(columns={0:'mind',1:'energy',2:'nature',3:'tactics'})\nvalues.index=test.id\nvalues.head()\nvalues.info()\nvalues.to_csv('\/content\/gdrive\/My Drive\/Explore\/Final_Output.csv')\n\n\n\n","5197d293":"- We are going to remove punctuations using string 'maketrans'\n- return the df with no punctuations","8087d6ec":"- Replace the urls with 'url-web'\n- We achieve this by using pd.replace() with a regex","b85fb5d7":"- as we can see with the distribution graph above\n- Introverted types like INFP, ITNJ, ITNP and INFJ post way more than Extroverted type\n- and this is viewed in contrast with the distribution by the Extroverted types\n- and these include ENFJ, ENFP, ENTP and ENTJ\n- Now we have explored our dataset comfortably let's create our binary columns\n- and these columns are based on the MBTI traits : mind (I\/E), enery(N\/S),nature(T\/F) and tactics(J\/P)\n\n- This we achieve by applying a lambda to create binary columns for traits and assign 0 and 1 appropriately","f8e17dba":"- now let's confirm if the length has 50 posts as promised by the competition designers","f2c1fa44":"- Now that the number of posts is confirmed let's check if we have exactly 16 personalities in the 'type' column","bb8ea495":"#### now let's view our changes ","faf83528":"- Again as promised we have only 16 personalities in alignment with MBTI personality types\n- Now let' explore the dataset further by looking at the number of posts per personality type\n- and then plot the distribution of the total number of posts per type\n- we will achieve this by grouping the train dataframe by type and count that and then multiplying by 50\n- we multiply by 50 because as we have realized each post row consist of 50 posts","e344aa57":"#### now lets plot that distribution for a better view perspective","d3cf052f":"- Now we fit and train our models for all traits\n- We then print the Results for each train\n- We use the threshold for each traint to get our y_thresh","82c5ec6b":"- Now let define a function to remove stop words and \n- and then appy that to messages 'posts' column","071397cb":"#### Now let's view this data\n- We will just view the first three columns just for saving space","34d23fea":"- Let's use snowball stemmer to take words to their root form before vectorization\n- And then create a fuunction that we will appy to messages 'posts' column to perform stemming","9b83fab1":"#### EDA let's explore the 'posts' column:","6f582c4a":"- As we were told the dataset is indeed without missing values\n- Now let's confirm the size of this dataset","ed162c36":"#### Let's create a pipeline to use with out bow, vectorizer and models\n- We Tried RandomForest, SVC, BernnouliNB, And simple Naive Bayse\n- RandomFores performs worse than all the above models \n- Our assumption is that it is because of the binary nature of our classifier\n- Random Forest performs well with multi classifiers\n\n\n- SVC,Bernnouli and simple Naive Bayes also performs bad\n- this is because they also in design are not appropriate for this type of challenge\n- our conclusion was to drop all of them except for logistic regression which seem to perform well at this\n- Now let's begin our modelling","b1a947f9":"#### Now let's do our train test split for all traits","260417b4":"- Finally we save our predictions into a dataframe and sumbit to kaggle\n- There is a lot of exploration that can still be performed with this problem \n- We did not remove any emojis, hashtags and mentions ...which we will definitely pursue\n- Shoooo!","01048924":"#### Now let's check if there are any missing values in our dataset just for a double-check","c6822da6":"\n### Declare both a CountVectorizer and TFIDF vectorizer\n- after applying TFDIFVectorizer we saw and improvement in our model and decided to stick with that\n- for our vectorization","313ec4ad":"#### Let's read in our MBTI Headset with 'read_csv'"}}