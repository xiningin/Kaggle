{"cell_type":{"e1d569c5":"code","000703c5":"code","cde1ff61":"code","a53185b1":"code","e903edee":"code","aef1f4f7":"code","febb6114":"code","088cc373":"code","95d5f599":"code","419d9a73":"code","9fbbb54d":"code","d1a8a953":"code","41e463a7":"code","8e0c058e":"code","717b35e9":"code","8ab06291":"code","f8f91f15":"code","ac531bbf":"code","f3de4633":"code","7d9451a0":"code","0e1f20fc":"code","11e9b198":"code","c70b3c8f":"code","7484c83b":"code","0b91bde8":"code","97cc7323":"code","2aa46ad7":"code","62088e96":"code","05150f39":"code","aaec6a33":"code","cf3b4b63":"code","af2c04b0":"code","7b8e533b":"code","3d325ae0":"code","bf8646de":"code","df69b68b":"code","f8ffe8ab":"code","e6ad9a7e":"code","4a9f784b":"code","fc4b1f98":"code","e85e1b2f":"code","6f18f151":"code","9c73e3e6":"code","c0b6dbde":"code","81ca6cfb":"code","b9f68812":"code","35812bb1":"code","645151b1":"code","b5d1034d":"code","e604cfa4":"code","6dd233e3":"code","9374530a":"code","858ea3cf":"code","e923e24a":"code","e5e8aa6f":"code","2ffc860f":"code","8f67672e":"code","93e46978":"code","bbdefe1c":"code","4bc6f9a8":"code","bcb0eb8d":"code","51c3f685":"code","6991b23a":"markdown","91c852de":"markdown","a0def0d2":"markdown","fb1f2fec":"markdown","bd866259":"markdown","a91f82fa":"markdown","204764d3":"markdown","e88c21fb":"markdown","f3ff9c0b":"markdown","14acbdf3":"markdown","f9b9a2b3":"markdown","f7937b07":"markdown","3fc65d20":"markdown","59834220":"markdown","eb8c0a3f":"markdown","7adb3b86":"markdown","3900cf77":"markdown","1b7a3e0d":"markdown","b931b819":"markdown","7266fc1e":"markdown","e3a58fa6":"markdown","e3b18a50":"markdown","37352652":"markdown","3ebb82f0":"markdown","cd08696d":"markdown","7e41b493":"markdown","accfa093":"markdown"},"source":{"e1d569c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","000703c5":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.gridspec as gridspec","cde1ff61":"submission=pd.read_csv(\"\/kaggle\/input\/ccpp-data\/sample_submission.csv\")\nccpp_train=pd.read_csv(\"\/kaggle\/input\/ccpp-data\/Train.csv\")\nccpp_test=pd.read_csv(\"\/kaggle\/input\/ccpp-data\/Test.csv\")","a53185b1":"ccpp_train.info()","e903edee":"ccpp_test.info()","aef1f4f7":"display(ccpp_train.head())\ndisplay(ccpp_test.head())","febb6114":"ccpp_train.describe().T","088cc373":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","95d5f599":"my_corr=ccpp_train.corr()\nplt.figure(figsize=(10,10))\nsns.set(font_scale=0.8)\nsns.heatmap(my_corr,\n            cbar=True, \n            annot=True, \n            square=True, \n            fmt='.2f', \n            annot_kws={'size':10},\n            linewidth=0.8)\nplt.show()","419d9a73":"import matplotlib.image as mpimg\nplt.figure(figsize=(8,8))\n\ng0=sns.jointplot(x = 'PE', y ='V', data = ccpp_train,color=\"m\",kind='reg')\ng1=sns.jointplot(x = 'PE', y ='RH', data = ccpp_train,color=\"m\",kind='reg')\ng2=sns.jointplot(x = 'PE', y ='AP', data = ccpp_train,color=\"m\",kind='reg')\ng3=sns.jointplot(x = 'PE', y ='AT', data = ccpp_train,color=\"m\",kind='reg')\ng0.savefig('g0.png')\nplt.close(g0.fig)\n\ng1.savefig('g1.png')\nplt.close(g1.fig)\n\ng2.savefig('g2.png')\nplt.close(g2.fig)\n\ng3.savefig('g3.png')\nplt.close(g3.fig)\n\n\nf, axarr = plt.subplots(2, 2, figsize=(8,8))\n\naxarr[0,0].imshow(mpimg.imread('g0.png'))\naxarr[0,1].imshow(mpimg.imread('g1.png'))\naxarr[1,0].imshow(mpimg.imread('g3.png'))\naxarr[1,1].imshow(mpimg.imread('g2.png'))\n\n# turn off x and y axis\n[ax.set_axis_off() for ax in axarr.ravel()]\n\nplt.tight_layout()\nplt.show()","9fbbb54d":"def multi_plotting (buy_new, feature): \n    import matplotlib.pyplot as plt \n    from scipy import stats\n    import matplotlib.gridspec as gridspec\n    from matplotlib.gridspec import GridSpec\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    fig = plt.figure(constrained_layout=True, figsize=(9,5))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, left=0.05, right=1,\n                        wspace=0.5,hspace=0.4, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(ccpp_train.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(ccpp_train.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[0:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(ccpp_train.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(ccpp_train['AT'].skew().round(3))) \n    print(\"Kurtosis: \" + str(ccpp_train['AT'].kurt().round(3)))","d1a8a953":"multi_plotting (ccpp_train,'AT')","41e463a7":"def multi_plotting (buy_new, feature): \n    import matplotlib.pyplot as plt \n    from scipy import stats\n    import matplotlib.gridspec as gridspec\n    from matplotlib.gridspec import GridSpec\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    fig = plt.figure(constrained_layout=True, figsize=(9,5))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, left=0.05, right=1,\n                        wspace=0.5,hspace=0.4, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(ccpp_train.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(ccpp_train.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[0:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(ccpp_train.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(ccpp_train['V'].skew().round(3))) \n    print(\"Kurtosis: \" + str(ccpp_train['V'].kurt().round(3)))","8e0c058e":"multi_plotting (ccpp_train,'V')","717b35e9":"def multi_plotting (buy_new, feature): \n    import matplotlib.pyplot as plt         \n    from scipy import stats\n    import matplotlib.gridspec as gridspec\n    from matplotlib.gridspec import GridSpec\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    fig = plt.figure(constrained_layout=True, figsize=(9,5))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, left=0.05, right=1,\n                        wspace=0.5,hspace=0.4, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(ccpp_train.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(ccpp_train.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[0:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(ccpp_train.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(ccpp_train['AP'].skew().round(3))) \n    print(\"Kurtosis: \" + str(ccpp_train['AP'].kurt().round(3)))","8ab06291":"multi_plotting (ccpp_train,'AP')","f8f91f15":"def multi_plotting (buy_new, feature): \n    import matplotlib.pyplot as plt \n    from scipy import stats\n    import matplotlib.gridspec as gridspec\n    from matplotlib.gridspec import GridSpec\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    fig = plt.figure(constrained_layout=True, figsize=(9,5))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, left=0.05, right=1,\n                        wspace=0.5,hspace=0.4, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(ccpp_train.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(ccpp_train.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[0:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(ccpp_train.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(ccpp_train['RH'].skew().round(3))) \n    print(\"Kurtosis: \" + str(ccpp_train['RH'].kurt().round(3)))","ac531bbf":"multi_plotting (ccpp_train,'RH')","f3de4633":"def multi_plotting (buy_new, feature): \n    import matplotlib.pyplot as plt    \n    from scipy import stats\n    import matplotlib.gridspec as gridspec\n    from matplotlib.gridspec import GridSpec\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    fig = plt.figure(constrained_layout=True, figsize=(9,5))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, left=0.05, right=1,\n                        wspace=0.5,hspace=0.4, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(ccpp_train.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(ccpp_train.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[0:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(ccpp_train.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(ccpp_train['PE'].skew().round(3))) \n    print(\"Kurtosis: \" + str(ccpp_train['PE'].kurt().round(3)))","7d9451a0":"multi_plotting (ccpp_train,'PE')","0e1f20fc":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","11e9b198":"X=ccpp_train.drop(['PE'],axis=1,inplace=False)\nY=ccpp_train.PE","c70b3c8f":"Scalar=MinMaxScaler()\nX_scale=Scalar.fit_transform(X)\nX_scale","7484c83b":"X_train,X_Val,Y_train,Y_Val=train_test_split(X,Y,test_size=0.30,random_state=123)","0b91bde8":"Lin_reg=LinearRegression(fit_intercept=True)\nlr=Lin_reg.fit(X_train,Y_train)\nprint(\"Coefficients :\", lr.coef_)\nprint(\"Intercept :\", lr.intercept_)\nprint(\"Score :\", lr.score(X_train,Y_train))","97cc7323":"Y_pred = lr.predict(X_Val)\nY_pred","2aa46ad7":"from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nmae  = mean_absolute_error(Y_Val,Y_pred)\nmse  = mean_squared_error(Y_Val,Y_pred)\nr2   = r2_score(Y_Val,Y_pred)\nrmse = mean_squared_error(Y_Val,Y_pred,squared=False)\nadjusted_r_squared = 1 - (1-r2)*(len(Y_train)-1)\/(len(Y_train)-X_train.shape[1]-1)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))\nprint('RMSE Score is {}'.format(rmse))\nprint(\"Adjusted R2 is {}\".format(adjusted_r_squared))","62088e96":"#Actual Vs Predicted Plot\nfig, ax = plt.subplots()\nax.scatter(Y_pred, Y_Val, edgecolors=(0, 0, 1))\nax.plot([Y_Val.min(), Y_Val.max()], [Y_Val.min(), Y_Val.max()], 'r--', lw=3)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual')\nplt.show()","05150f39":"#Prediction Error Plot(Actual vs Predicted Plot)\nfrom yellowbrick.regressor import PredictionError\nvisualizer = PredictionError(lr)\nvisualizer.fit(X_train, Y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_Val, Y_Val)  # Evaluate the model on the test data\nvisualizer.show()     ","aaec6a33":"# visualize the relationship between the features and the response using scatterplots\np = sns.pairplot(ccpp_train, x_vars=['AT', 'V', 'AP', 'RH'], y_vars='PE', size=5, aspect=0.5,kind='reg',plot_kws={'line_kws':{'color':'red'}})","cf3b4b63":"residual=Y_Val-Y_pred\ndata={'ei':residual}\nresidual_df=pd.DataFrame(data).reset_index(drop=False)\nresidual_df=residual_df.drop(['index'],axis=1)\nresidual_df['ei_sq']=np.square(residual_df['ei'])\nsum_of_sq_residuals=residual_df.sum()['ei_sq']\nresidual_df['ei_minus_1']=residual_df['ei'].shift()\nresidual_df=residual_df.dropna()\nresidual_df['diff_ei_eiminus1']=residual_df['ei']-residual_df['ei_minus_1']\nresidual_df['sq_diff_ei_eiminus1']=np.square(residual_df['diff_ei_eiminus1'])","af2c04b0":"sum_of_squareed_of_difference_residuals=residual_df.sum()['sq_diff_ei_eiminus1']\nsum_of_squareed_of_difference_residuals","7b8e533b":"Derbin_Watson=sum_of_squareed_of_difference_residuals\/sum_of_sq_residuals\nprint(\"Derbin_Watson Score\",Derbin_Watson)","3d325ae0":"from yellowbrick.regressor import residuals_plot\n\nplt.figure(figsize=(10,7))\nviz = residuals_plot(LinearRegression(), X_train, Y_train, X_Val, Y_Val)","bf8646de":"#Train_Test Split\nx_train,x_Val,y_train,y_Val=train_test_split(X,Y,test_size=0.30,random_state=123)\n# Importing Libraries\nfrom statsmodels.tools.tools import add_constant\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.stats.api as sms\n\nx_train = sm.add_constant(x_train)   #Add the constant term to the training data\nmy_model = sm.OLS(y_train, x_train)  #Fit the OLS model\n#Fitting Reuslts\nresult = my_model.fit()\nname = ['Lagrange multiplier statistic', 'p-value','f-value', 'f p-value']\ntest = sms.het_breuschpagan(result.resid, result.model.exog)\ndata={'Name' :['Lagrange multiplier statistic', 'p-value','f-value', 'f p-value'],\n     'Values':test}\nResult=pd.DataFrame(data)\nResult","df69b68b":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(lr,hist=False)\nvisualizer.fit(X_train,Y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_Val,Y_Val)  # Evaluate the model on the test data\nvisualizer.show() ","f8ffe8ab":"residuals = Y_Val.values-Y_pred\nmean_residuals = np.mean(residuals)\nprint(\"Mean of Residuals {}\".format(mean_residuals))","e6ad9a7e":"result=np.linalg.cond(result.model.exog)\ndisplay(\"Eigen value of Correlation is: {}\".format(result))\n#VIF\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX_vif = add_constant(x_train)\nvif = pd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)\ndisplay(vif.sort_values(ascending = False))","4a9f784b":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(lr,hist=False, qqplot=True)\nvisualizer.fit(X_train,Y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_Val,Y_Val)  # Evaluate the model on the test data\nvisualizer.show() ","fc4b1f98":"x_train = sm.add_constant(x_train)   #Add the constant term to the training data\nmy_model = sm.OLS(y_train, x_train)  #Fit the OLS model\n#Fitting Reuslts\nresult = my_model.fit()\nname = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\ntest = sms.jarque_bera(result.resid)\ndata={'Name' :['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis'],\n     'Values':test}\nResult=pd.DataFrame(data)\nResult","e85e1b2f":"p = sns.distplot(residual,kde=True)\np = plt.title('Normality of error terms\/residuals')","6f18f151":"import sklearn.model_selection as GridSearchCV\nfrom sklearn.linear_model import Ridge\nimport sklearn.model_selection as ms\nimport math\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,20)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(X_train,Y_train)\nprint(f\"The best value of Alpha is: {ridge_reg.best_params_}\")\nprint(f\"The best score achieved with Alpha=7 is: {math.sqrt(-ridge_reg.best_score_)}\")\nridge_pred=math.sqrt(-ridge_reg.best_score_)","9c73e3e6":"ridge_mod=Ridge(alpha=7)\nridge_mod=ridge_mod.fit(X_train,Y_train)\nY_pred_Val=ridge_mod.predict(X_Val)","c0b6dbde":"mae  = mean_absolute_error(Y_Val,Y_pred_Val)\nmse  = mean_squared_error(Y_Val,Y_pred_Val)\nr2   = r2_score(Y_Val,Y_pred_Val)\nrmse = mean_squared_error(Y_Val,Y_pred_Val,squared=False)\nadjusted_r_squared = 1 - (1-r2)*(len(Y_train)-1)\/(len(Y_train)-X_train.shape[1]-1)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))\nprint('RMSE Score is {}'.format(rmse))\nprint(\"Adjusted R2 is {}\".format(adjusted_r_squared))","81ca6cfb":"Ridge_CV=Ridge(alpha=7)\nMSEs=ms.cross_val_score(Ridge_CV,X_train,Y_train, scoring='neg_mean_squared_error', cv=10)\n\n#RMSE score of the 5 folds\nprint(\"RMSE scores of the 10 folds:\")\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE in Ridge: {round(math.sqrt(np.mean(-MSEs)),4)}')","b9f68812":"coefs = pd.Series(ridge_mod.coef_, index = X_train.columns)\nimp_coefs = pd.concat([coefs.sort_values().head(),coefs.sort_values().tail()])\nimp_coefs.plot(kind = \"bar\", color='yellowgreen')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","35812bb1":"from sklearn.linear_model import Lasso\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(X_train,Y_train)\nprint(f'The best value of Alpha is: {lasso_reg.best_params_}')","645151b1":"lasso_mod=Lasso(alpha=0.003)\nlasso_mod.fit(X_train,Y_train)\nY_lasso_Val=lasso_mod.predict(X_Val)","b5d1034d":"mae  = mean_absolute_error(Y_Val,Y_lasso_Val)\nmse  = mean_squared_error(Y_Val,Y_lasso_Val)\nr2   = r2_score(Y_Val,Y_lasso_Val)\nrmse = mean_squared_error(Y_Val,Y_lasso_Val,squared=False)\nadjusted_r_squared = 1 - (1-r2)*(len(Y_train)-1)\/(len(Y_train)-X_train.shape[1]-1)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))\nprint('RMSE Score is {}'.format(rmse))\nprint(\"Adjusted R2 is {}\".format(adjusted_r_squared))","e604cfa4":"Lasso_CV=Lasso(alpha=0.003)\nMSEs=ms.cross_val_score(Lasso_CV,X_train,Y_train, scoring='neg_mean_squared_error', cv=10)\n\n#RMSE score of the 5 folds\nprint(\"RMSE scores of the 10 folds:\")\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE in Ridge: {round(math.sqrt(np.mean(-MSEs)),4)}')","6dd233e3":"coefs = pd.Series(lasso_mod.coef_, index = X_train.columns)\nimp_coefs = pd.concat([coefs.sort_values().head(),coefs.sort_values().tail()])\nimp_coefs.plot(kind = \"barh\", color='yellowgreen')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","9374530a":"from sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\nalphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\nfor a in alphas:\n    model = ElasticNet(alpha=a).fit(X_train,Y_train)   \n    score = model.score(X_train,Y_train)\n    pred_Y = model.predict(X_Val)\n    mse = mean_squared_error(Y_Val, pred_Y)   \n    print(\"Alpha:{0:.4f}, R2:{1:.4f}, MSE:{2:.4f}, RMSE:{3:.5f}\".format(a, score, mse, np.sqrt(mse)))","858ea3cf":"elastic=ElasticNet(alpha=0.0001).fit(X_train, Y_train)\nY_pred = elastic.predict(X_Val)","e923e24a":"mae  = mean_absolute_error(Y_Val,Y_pred)\nmse  = mean_squared_error(Y_Val,Y_pred)\nr2   = r2_score(Y_Val,Y_pred)\nrmse = mean_squared_error(Y_Val,Y_pred,squared=False)\nadjusted_r_squared = 1 - (1-r2)*(len(Y_train)-1)\/(len(Y_train)-X_train.shape[1]-1)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))\nprint('RMSE Score is {}'.format(rmse))\nprint(\"Adjusted R2 is {}\".format(adjusted_r_squared))","e5e8aa6f":"x_ax = range(len(X_Val))\nplt.plot(x_ax, Y_pred, lw=0.8, color=\"red\", label=\"predicted\")\nplt.scatter(x_ax, Y_Val, s=5, color=\"blue\", label=\"original\")\nplt.legend()\nplt.show()","2ffc860f":"from keras.models import Sequential\nfrom keras.layers import Dense","8f67672e":"n_cols=X_train.shape[1] # number of predictors","93e46978":"# define regression model\ndef regression_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model","bbdefe1c":"model = regression_model()","4bc6f9a8":"history=model.fit(X_train,Y_train, validation_data=(X_Val,Y_Val), epochs=100, verbose=2)","bcb0eb8d":"plt.plot(history.history['loss'],label='train')\nplt.xlabel('epochs')\nplt.plot(history.history['val_loss'],label='test')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","51c3f685":"#** Documentation......To be Contd. ","6991b23a":"### Elastic Net","91c852de":"<h1 style=\"text-align:center;color:crimson;\">Analysis on Energy Output Variable<\/h1>","a0def0d2":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Prediction Error Plot(Actual vs Predicted Plot)<\/h1>","fb1f2fec":"## Regression with Keras","bd866259":"<p style=\"background-color:Tomato;\">Assumption 4. No Correlation between Independent Variables i.e. checking For multicollinearity (VIF\/Correlation Plots)<p>","a91f82fa":"## Lasso Regression","204764d3":"<h1 style=\"text-align:center;color:crimson;\">Analysis on Relative Humidity Variable<\/h1>","e88c21fb":"### Regularization Techniques","f3ff9c0b":"<p style=\"background-color:Tomato;\">Assumption 2. No Correlation between Error Terms i.e. No Autocorreation (Perform Durbin Watson Test)<\/p>","14acbdf3":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Scaling(Min-Max Scaler)<\/h1>","f9b9a2b3":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Importing Libraries & Files<\/h1>","f7937b07":"<p style=\"background-color:Tomato;\">Assumption 5. Error Terms must be Normally Distributed (Jarque-Bera\/QQ-Plot)<p>","3fc65d20":"<h1 style=\"text-align:center;color:crimson;\">Analysis on Temperature Variable<\/h1>","59834220":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Validating Model based on Assumptions<\/h1>","eb8c0a3f":"![](https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2016\/06\/durbin-watson.png)","7adb3b86":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Summary Statistics<\/h1>","3900cf77":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Data Understanding & Distribution<\/h1>","1b7a3e0d":"<p style=\"background-color:Tomato;\">Assumption 1. Linear Relationship between Dependent and Independent Variable<\/p>","b931b819":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Metrics<\/h1>","7266fc1e":"From Above Heatmap we can say No features ar highly correlated keeping Threshold of 0.85. Though Temperature and vaccum does show correlation but will keep it fr now and will drop if further analysis allows.","e3a58fa6":"### Ridge regression:<br>\n1. Minimize squared error + a term alpha that penalizes the error\n2. Find a value of alpha that minimizes the train and test error (avoid overfitting)","e3b18a50":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Actual Vs Predicted Plot<\/h1>","37352652":"![](https:\/\/i.makeagif.com\/save\/lnPuNV)","3ebb82f0":"<h1 style=\"background-color:lightgreen;color:black;text-align:center;\">Linear Regression<\/h1>","cd08696d":"<h1 style=\"text-align:center;color:crimson;\">Analysis on Ambient pressure Variable<\/h1>","7e41b493":"<p style=\"background-color:Tomato;\">Assumption3: Error Terms must have Constatnt Variance (Homoskedastic in Nature)(Breush-Pagan test\/Residual vs Fitted Plot)<p>","accfa093":"<h1 style=\"text-align:center;color:crimson;\">Analysis on Vacuum Variable<\/h1>"}}