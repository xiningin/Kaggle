{"cell_type":{"6d5da97a":"code","6fc565be":"code","d8a31736":"code","42b7decb":"code","4ad7883b":"code","443f1b82":"code","d6ad0899":"code","e4ca3c31":"code","b3becc68":"code","e43855b1":"markdown","f7574a90":"markdown","cdb4e624":"markdown","d619d028":"markdown","a8b5adf4":"markdown","e530bcca":"markdown","1aaea2bf":"markdown"},"source":{"6d5da97a":"import pandas as pd\nimport numpy as np\nimport os\nimport glob\nimport pathlib\nimport matplotlib.pyplot as plt","6fc565be":"Train_path=\"..\/input\/english-fontnumber-recognition\/Font\"","d8a31736":"#This below function will make us the dataset with filepaths and labels\n#Here in the data we don't have the label of image associated with the name of file.So look further on what we will do\n\ndef process(data):\n    path=pathlib.Path(data)#converting the string to path\n    filepaths=list(path.glob(r\"*\/*.png\"))#Going through all the subpaths \n    labels=list(map(lambda x: os.path.split(os.path.split(x)[0])[1],filepaths))#Separating the label from filepath and storing it\n    df1=pd.Series(filepaths,name='filepaths').astype(str)\n    df2=pd.Series(labels,name='labels')\n    df=pd.concat([df1,df2],axis=1)#Making the dataframe\n    return df","42b7decb":"df=process(Train_path)","4ad7883b":"df.head(5)","443f1b82":"#lets just see some random images from Sample060 class\n\ntrain_normal=Train_path+\"\/Sample060\/\"\nlen(os.listdir(train_normal))\nrandom_num=np.random.randint(0,len(os.listdir(train_normal)))\npic_path=os.listdir(train_normal)[random_num]\n\nimg_path=train_normal + pic_path\nplt.imshow(plt.imread(img_path))","d6ad0899":"#Since we don't have the label associated with sample name, we have to replace it manually\nDict={\n    \"Sample001\":\"0\",\n    \"Sample002\":\"1\",\n    \"Sample003\":\"2\",\n    \"Sample004\":\"3\",\n    \"Sample005\":\"4\",\n    \"Sample006\":\"5\",\n    \"Sample007\":\"6\",\n    \"Sample008\":\"7\",\n    \"Sample009\":\"8\",\n    \"Sample010\":\"9\",\n    \"Sample011\":\"A\",\n    \"Sample012\":\"B\",\n    \"Sample013\":\"C\",\n    \"Sample014\":\"D\",\n    \"Sample015\":\"E\",\n    \"Sample016\":\"F\",\n    \"Sample017\":\"G\",\n    \"Sample018\":\"H\",\n    \"Sample019\":\"I\",\n    \"Sample020\":\"J\",\n    \"Sample021\":\"K\",\n    \"Sample022\":\"L\",\n    \"Sample023\":\"M\",\n    \"Sample024\":\"N\",\n    \"Sample025\":\"O\",\n    \"Sample026\":\"P\",\n    \"Sample027\":\"Q\",\n    \"Sample028\":\"R\",\n    \"Sample029\":\"S\",\n    \"Sample030\":\"T\",\n    \"Sample031\":\"U\",\n    \"Sample032\":\"V\",\n    \"Sample033\":\"W\",\n    \"Sample034\":\"X\",\n    \"Sample035\":\"Y\",\n    \"Sample036\":\"Z\",\n    \"Sample037\":\"a\",\n    \"Sample038\":\"b\",\n    \"Sample039\":\"c\",\n    \"Sample040\":\"d\",\n    \"Sample041\":\"e\",\n    \"Sample042\":\"f\",\n    \"Sample043\":\"g\",\n    \"Sample044\":\"h\",\n    \"Sample045\":\"i\",\n    \"Sample046\":\"j\",\n    \"Sample047\":\"k\",\n    \"Sample048\":\"l\",\n    \"Sample049\":\"m\",\n    \"Sample050\":\"n\",\n    \"Sample051\":\"o\",\n    \"Sample052\":\"p\",\n    \"Sample053\":\"q\",\n    \"Sample054\":\"r\",\n    \"Sample055\":\"s\",\n    \"Sample056\":\"t\",\n    \"Sample057\":\"u\",\n    \"Sample058\":\"v\",\n    \"Sample059\":\"w\",\n    \"Sample060\":\"x\",\n    \"Sample061\":\"y\",\n    \"Sample062\":\"z\",\n}\n    \ndf['Font']=df.labels.map(Dict)\ndf.head()","e4ca3c31":"#Now we will dop the labels column\n\nFinal_df=df.drop(columns=['labels'])\nFinal_df.head()","b3becc68":"Final_df.to_csv(\"train.csv\",index=False)","e43855b1":"## Now you can train your model using the train.csv file directly.I will be posting a kernel in discussion for your reference on how to build a model for this data","f7574a90":"## If you love the dataset and my work then don't forget to upvote the kernel and dataset","cdb4e624":"#Exporting our dataset","d619d028":"## In this notebook i will be making the dataset from the given image data so that we can use it for Modelling ","a8b5adf4":"# Importing necessary libraries","e530bcca":"# Setting our config and Making the dataset","1aaea2bf":"# Visualizing our data"}}