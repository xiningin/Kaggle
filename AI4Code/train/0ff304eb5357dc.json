{"cell_type":{"65f0be2c":"code","112f424d":"code","d4419878":"code","76b2d31b":"code","4f67d9da":"code","bbe856a8":"code","0009966a":"code","28140e90":"code","4a6513fd":"code","6eb81cd3":"code","132f0738":"code","6d5da533":"code","76331c0a":"code","5cd6bd44":"code","2681ed16":"code","da3ceb55":"code","c671a14a":"code","79502b3a":"code","ac20deae":"code","21ed32e0":"code","10f97ed3":"code","9a0a2412":"code","6700e3f2":"code","9964058f":"code","d6c7adb6":"code","03239f3c":"code","687cff38":"code","6dd5a159":"code","b27e22c1":"code","32679eaf":"code","f8eadc11":"code","eb2c95f7":"code","3104c5ad":"code","dc80d6d4":"code","76ad6b89":"code","e791fb38":"code","4f9ab783":"code","1000dfc1":"code","58d2b50b":"code","fb45cc63":"code","9604d1fc":"code","6ac2842c":"markdown","65cfd536":"markdown","670dc6c9":"markdown","22cb4962":"markdown","b6dc55ea":"markdown","79597209":"markdown","cfd073e6":"markdown","32c4f038":"markdown","690407c3":"markdown","ca7359a5":"markdown","59d9ec85":"markdown"},"source":{"65f0be2c":"# Importando as bibliotecas necess\u00e1rias\n\n# Biblioteca para manipula\u00e7\u00e3o de Dados\nimport pandas as pd\n# Biblioteca para plot de gr\u00e1ficos\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# \nimport numpy as np\n\n# Utilizado para realizar o balanceamento da classifica\u00e7\u00e3o (TARGET) dos dados para evitar resultado tend\u00eancioso\n#!pip install imblearn\nfrom imblearn.over_sampling import SMOTE","112f424d":"import warnings\nwarnings.filterwarnings(\"ignore\")","d4419878":"# Carregando o dataset utilizando o Pandas\ndataset_train = pd.read_csv(\"\/kaggle\/input\/santander-customers\/Santander_Customers.csv\", sep = \",\")","76b2d31b":"# Visualizando os primeiros registros do dataset_train\ndataset_train.head(5)","4f67d9da":"dataset_train.info()","bbe856a8":"pd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\ndataset_train.describe()","0009966a":"def CleanData(dataset):\n    # Removendo colunas que possuem somentes valores = 0 (irrelevantes para o modelo)\n    dataset = dataset.loc[:, (dataset !=0).any(axis=0)]\n    \n    # Vari\u00e1vel var3 apresenta um valor Min muito disperso com base no describe acima podendo representar valores unknown. \n    # Realizando o ajuste da vari\u00e1vel aplicando o valor encontrado na mediana.\n    dataset = dataset.replace({'var3': -999999}, 2)\n    \n    return dataset","28140e90":"dataset_train = CleanData(dataset_train)\ndataset_train.describe()","4a6513fd":"# Verificando se existem valores NULL\ndataset_train.isnull().values.any()","6eb81cd3":"# Separando a coluna ID do dataset e controlando em outro dataset.\ndataset_train_ID = dataset_train[['ID']]\ndataset_train = dataset_train.drop(['ID'], axis=1)","132f0738":"dataset_train['TARGET'].value_counts().plot(kind = 'bar', figsize=(6,6))\nplt.title('Analisando os dados TARGET')\nplt.xlabel('Target')\nplt.ylabel('Quantidade Registros')\nplt.show()","6d5da533":"dataset_train.groupby('TARGET').size()","76331c0a":"columns = dataset_train.columns\n\nfor i in columns:\n    dataset_train[i].plot(kind = 'hist')\n    plt.title('Histograma Analisando os dados: ' + i)\n    plt.xlabel(i)\n    plt.ylabel('Distribui\u00e7\u00e3o')\n    \n    plt.show()\n    ","5cd6bd44":"columns = dataset_train.columns\n\nfor i in columns:\n    dataset_train[i].plot(kind = 'box')\n    plt.title('BoxPlots Analisando os dados: ' + i)\n    plt.xlabel(i)\n    plt.ylabel('Escala')\n    \n    plt.show()","2681ed16":"# Criando uma Matrix de Correla\u00e7\u00e3o para identificar a correla\u00e7\u00e3o entre as vari\u00e1veis\ncorrelations = dataset_train.corr()\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin = -1, vmax = 1) #Mostrar as correla\u00e7\u00f5es\nfig.colorbar(cax)\nplt.show()","da3ceb55":"dataset_train.corr()['TARGET']","c671a14a":"def NormalizingData(dataset):\n    normalized_vars = (dataset.drop(['TARGET'], axis=1) - dataset.drop(['TARGET'], axis=1).mean())\/dataset.drop(['TARGET'], axis=1).std()\n\n    df_normalized = pd.concat([ dataset[['TARGET']], normalized_vars], axis=1)\n    \n    return df_normalized","79502b3a":"dataset_train_Normalized = NormalizingData(dataset_train)\ndataset_train_Normalized.head(5)","ac20deae":"def fun_BalancingData(dataset, var_target):\n    \n    # Split da variavel target e variaveis preditoras\n    x_train = dataset.drop([var_target], axis=1)\n    y_train = dataset[var_target]\n    \n    smt = SMOTE()\n    \n    # Separando os dados em features e target\n    features_train, target_train = smt.fit_sample(x_train, y_train)\n    \n    target_train_DF = pd.DataFrame(target_train)\n    # atribuindo o nome da coluna\n    target_train_DF.columns = ['TARGET']\n    \n    features_train_DF = pd.DataFrame(features_train)\n    features_train_DF.columns = x_train.columns\n    \n    return pd.concat( [target_train_DF, features_train_DF], axis=1 )","21ed32e0":"dataset_train_Balanced = fun_BalancingData(dataset_train_Normalized, 'TARGET')","10f97ed3":"from sklearn.ensemble import ExtraTreesClassifier\n\n# Atribuindo a quantidade de vari\u00e1veis que desejamos selecionar, para reaproveitamento do valor em c\u00f3digos futuros\nn_varsImportants = 45\n\narray_dataset = dataset_train_Balanced.values\n\nfeatures_x = array_dataset[:,1:] # Seleciona todas as vari\u00e1veis preditoras\ntarget_y = array_dataset[:,0:1] # Seleciona somente a TARGET\n\nmodel = ExtraTreesClassifier()\nseletor = model.fit(features_x,target_y.ravel())\n\n# Trasnsformando em valor serial para plotar no gr\u00e1fico\nfeatures_important = pd.Series(model.feature_importances_, index= dataset_train.drop(['TARGET'], axis=1).columns.values).sort_values(ascending=False)\n\nfeatures_important[:n_varsImportants].plot(kind='bar',title='Top 45 Features Most Important by ExtraTreesClassifier',figsize=(12,8))\nplt.show()","9a0a2412":"# Fun\u00e7\u00e3o utilizada para aplicar a sele\u00e7\u00e3o das N vari\u00e1veis mais relevantes de acordo com o ExtraTreesClassifier\ndef Select_Top_Features(DF, IndexColNames):\n    DF_feature = DF.loc[:, DF.columns.isin(IndexColNames)]\n    \n    return pd.concat( [DF['TARGET'], DF_feature], axis=1 )","6700e3f2":"Columns_Feature_Select = features_important[:n_varsImportants].index\n\ndataset_train_feature_selection = Select_Top_Features(dataset_train_Balanced, Columns_Feature_Select)","9964058f":"dataset_train_feature_selection.describe()","d6c7adb6":"!pip install pyspark","03239f3c":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import Row\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Spark Session - usada quando se trabalha com Dataframes no Spark\nspSession = SparkSession.builder.master(\"local\").appName(\"Santander-Customers\").getOrCreate()\n\nsqlContext = SQLContext(spSession)\n\nDF = sqlContext.createDataFrame(dataset_train_feature_selection)","687cff38":"# Criando um LabeledPoint (target, Vector[features])\ndef transformaVar(row):\n    listvalues = []\n    # Seleciona todas as vari\u00e1veis preditoras exceto a TARGET\n    for i in row[1:]:\n        listvalues.append(i)\n\n    obj = (row[\"TARGET\"], Vectors.dense(listvalues))\n    return obj","6dd5a159":"# Transforma o Dataframe do Spark em RDD e aplica a fun\u00e7\u00e3o map\nRDDTransf = DF.rdd.map(transformaVar)","b27e22c1":"RDDTransf.collect()","32679eaf":"# Transforma o RDD de volta a um DataFrame do Spark\nDF2 = spSession.createDataFrame(RDDTransf, [\"label\", \"features\"])","f8eadc11":"type(DF2)","eb2c95f7":"DF2.select(\"features\",\"label\").show(5)","3104c5ad":"# Aplicando a redu\u00e7\u00e3o de Dimensionalidade com PCA\n# Embora algumas etapas acima eu tenha selecionado as top 45 vari\u00e1veis mais relevantes com base no resultado do ExtraTreesClassifier\n# Estou aplicando o PCA para reduzir as 45 vari\u00e1veis em componentes para aplica\u00e7\u00e3o do conceito e pr\u00e1tica operacional.\npca = PCA(k = 5, inputCol = \"features\", outputCol = \"pcaFeatures\")\n\n# Criando o modelo\npcaModel = pca.fit(DF2)\npcaResult = pcaModel.transform(DF2).select(\"label\",\"features\")\npcaResult.show(truncate = False)","dc80d6d4":"# Indexa\u00e7\u00e3o \u00e9 pr\u00e9-requisito para Decision Trees com apache Spark\n# RandomForest \u00e9 um algoritmo de Decision Tree\n\n# Utiliza a coluna \"label\" para criar uma nova coluna Indexada\nstringIndexer = StringIndexer(inputCol = \"label\", outputCol = \"indexed\")\nsi_model = stringIndexer.fit(pcaResult)\nFinalResult = si_model.transform(pcaResult)\nFinalResult.collect()","76ad6b89":"# Split dados-Treino e Dados-Teste\n(df_treino, df_teste) = FinalResult.randomSplit([0.7, 0.3]) # 70% Treino 30% Teste","e791fb38":"df_treino.count()","4f9ab783":"df_teste.count()","1000dfc1":"# Criando o modelo utilizando RandomForest\n# O par\u00e2metro \"featurescol\" espera um dado do tipo vetor, por esse motivo na etapa de pr\u00e9-processamento foi gerado um densevector.\nrfClassifer = RandomForestClassifier(labelCol = \"indexed\", featuresCol = \"features\")\nmodelo = rfClassifer.fit(df_treino)","58d2b50b":"# Previs\u00f5es com dados de teste\npredictions = modelo.transform(df_teste)\n\n# Selecionando os campos necess\u00e1rios\n# prediction: Previs\u00e3o feita pelo modelo\n# indexed e label s\u00e3o os target (dado que o label possui apenas 2 valores [0 e 1])\n# Features representa os componentes utilizados durante a cria\u00e7\u00e3o do modelo\npredictions.select(\"prediction\", \"indexed\", \"label\", \"features\").collect()","fb45cc63":"# Avaliando a acur\u00e1cia\/assertividade do modelo criado.\n\n# Informa que a Coluna \"prediction\" representa a previs\u00e3o feita pelo modelo,\n# que a coluna \"indexed\" representa a coluna target que queriamos prever,\nevaluator = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"indexed\", metricName = \"accuracy\")\n# Aplica a classifica\u00e7\u00e3o criada ao conjunto de dados predictions\nevaluator.evaluate(predictions) \n\n# Existem v\u00e1rias formas de melhorar a acur\u00e1cia do modelo criado: Ajustes de # componentes dos PCA, Incluir ou remover vari\u00e1veis... ","9604d1fc":"# Gerando uma Confusion Matrix para avaliar as classifica\u00e7\u00f5es corretas e incorretas.\npredictions.groupBy(\"indexed\", \"prediction\").count().show()","6ac2842c":"# Realizando o balanceamento dos dados de Classifica\u00e7\u00e3o utilizando SMOTE\n###### A classifica\u00e7\u00e3o dos dados est\u00e1 tend\u00eanciosa (Existem mais de uma classifica\u00e7\u00e3o que outra) . \u00c9 necess\u00e1rio aplicar o balanceamento para que o algoritmo n\u00e3o aprenda mais uma classifica\u00e7\u00e3o que a outra.\n###### Ser\u00e1 necess\u00e1rio realizar um SMOTTE sobre os dados, para nivelar o resultado para o modelo n\u00e3o se tornar Tend\u00eancioso.","65cfd536":"### Realizando a Limpeza de alguns dados identificados como \"suspeitos\" ou desnecess\u00e1rios","670dc6c9":"# Feature Selections \n###### Analisando as vari\u00e1veis mais importantes atrav\u00e9s de um gr\u00e1fico","22cb4962":"### An\u00e1lise Explorat\u00f3ria dos Dados\n###### Dataset possui muitas vari\u00e1veis, desse modo n\u00e3o ser\u00e3o gerados gr\u00e1ficos para an\u00e1lise explorat\u00f3ria de todas vari\u00e1veis.\n###### Analisando a distribui\u00e7\u00e3o da vari\u00e1vel TARGET (que queremos prever). ","b6dc55ea":"# Prevendo N\u00edvel de Satisfa\u00e7\u00e3o de Clientes Santander\n\nO Objetivo do projeto \u00e9 conseguir prever a probabilidade de cada cliente no dataset \"teste.csv\" estar ou n\u00e3o satisfeito.\n\nA coluna \"TARGET\" dataset \"train.csv\" representa a coluna que queremos prever, onde 1 representa: Cliente Insatisfeito e 0 representa: Cliente Satisfeito.\n\nO dataset pode ser obtido no Kaggle: https:\/\/www.kaggle.com\/c\/santander-customer-satisfaction","79597209":"# Resultado Final: 79.58% de acur\u00e1cia","cfd073e6":"# Pr\u00e9-Processamento dos Dados\n## Transforma o DataFrame do Pandas em um DataFrame do Spark\nA partir desse ponto em diante, irei utilizar o Spark para realizar as opera\u00e7\u00f5es visando otimizar performance.\n\nPr\u00f3ximas Etapas: \n1. Transformar o dataset em labeledPoint (Features, Target) - Exigido pelos modelos de ML utilizando Spark.\n2. Redu\u00e7\u00e3o de Dimensionalidade utilizando PCA.","32c4f038":"### Verificando a Correla\u00e7\u00e3o entre os dados com a vari\u00e1vel TARGET","690407c3":"A - Pr\u00e9-processamento\n1. OK Carregar os dados;\n2. OK Realizar an\u00e1lise explorat\u00f3ria das vari\u00e1veis (scatter plot, box plot, correla\u00e7\u00e3o entre vari\u00e1veis, matrix de correla\u00e7\u00e3o...)\n3. OK Transforma\u00e7\u00e3o de vari\u00e1veis (normaliza\u00e7\u00e3o, padroniza\u00e7\u00e3o...)\n4. OK Feature Selection (utilizar modelos de ML para identificar vari\u00e1veis + relevantes a serem utilizadas)\n5. OK Realizar o balanceamento da vari\u00e1vel target, para que o modelo n\u00e3o fique Tendencioso\n6. OK Redu\u00e7\u00e3o de Dimensionalidade (quando necess\u00e1rio, e existir muitas vari\u00e1veis...PCA \u00e9 uma poss\u00edvel solu\u00e7\u00e3o)\n\nB - Aprendizado\n1. OK Split Dados Treino\/Teste\n2. OK Sele\u00e7\u00e3o do modelo\n3. Cross-Validation\n4. M\u00e9tricas de performance (Ajuste de hiper-parametros)\n5. Otimiza\u00e7\u00e3o do modelo (ajuste de hiper-parametros)\n\nC - Avalia\u00e7\u00e3o\n1. OK Valida\u00e7\u00e3o do modelo (utilizar dados de valida\u00e7\u00e3o)\n2. Otimiza\u00e7\u00e3o\n\nD - Previs\u00e3o\n1. Passar para o modelo criado, dados que ele desconhece (dados de teste que n\u00e3o possuem a variavel TARGET)\n\n","ca7359a5":"## Padroniza\u00e7\u00e3o dos Dados\n###### Colocando todos os dados (Exceto a vari\u00e1vel TARGET) na mesma escala.","59d9ec85":"# Machine Learning\n\nPr\u00f3ximas etapas:\n1. Split do dataset em treino\/avalia\u00e7\u00e3o ap\u00f3s todo o pr\u00e9-processamento, necess\u00e1rio para a etapa de valida\u00e7\u00e3o.\n2. Escolha do Modelo a ser aplicado.\n3. Valida\u00e7\u00e3o do Modelo e ajustes.\n4. Previs\u00f5es com o dataset teste utilizando o modelo criado (Realizar a normaliza\u00e7\u00e3o dos dados de teste)."}}