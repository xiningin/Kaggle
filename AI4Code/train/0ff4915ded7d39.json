{"cell_type":{"ab376e23":"code","c0862155":"code","6ded9e04":"code","42c442fd":"code","b65855f2":"code","e46b70df":"code","25ad3a33":"code","6db850e0":"code","9dcf2014":"code","35fc3b9b":"code","28e2f5f7":"code","8b748e73":"code","9f7abc55":"code","ce62930c":"code","fc24e8f3":"code","1b181d1e":"code","a3d8f284":"code","606b54c9":"code","553aeca8":"code","cd110720":"code","752dd6b8":"code","8c38b3ac":"code","4acea568":"code","2ddddcf0":"code","680c920a":"code","1a59b1f1":"code","16aee051":"code","faa7b758":"code","81bf4777":"code","c0830f43":"code","04fcf96a":"code","35547fdb":"code","87b4655f":"code","9d88abd7":"code","9ad49a02":"code","1cf5827d":"code","9b792754":"markdown","73fd879d":"markdown","1d8b2d9e":"markdown","a645453c":"markdown","a9c11b79":"markdown"},"source":{"ab376e23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0862155":"#Importing necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6ded9e04":"train_data=pd.read_csv(\"..\/input\/housing-competition-data\/train(1).csv\")","42c442fd":"train_data.head()","b65855f2":"train_data.describe()","e46b70df":"train_data.info()","25ad3a33":"#We could straight away drop those colmns that have more than 90% data as missing values.\ntrain_data.drop(['PoolQC','Alley','MiscFeature'],axis=1,inplace=True)","6db850e0":"#Checking the remaining missing value columns for only numerical values\nmissing_data_cols=[col for col in train_data.describe().columns if train_data[col].isnull().any()]\nfor i in missing_data_cols:\n    print(i,\":\", train_data[i].isnull().sum())","9dcf2014":"train_data['MasVnrArea'].fillna(train_data['MasVnrArea'].mean(),inplace=True)\ntrain_data['GarageYrBlt'].fillna(train_data['GarageYrBlt'].median(),inplace=True)\ntrain_data[\"LotFrontage\"].fillna(train_data[\"LotFrontage\"].median(),inplace=True)","35fc3b9b":"train_data.describe().columns","28e2f5f7":"#Create a simple plot to see the distribution of data\nfor i in train_data.describe().columns:\n    plt.hist(train_data[i])\n    plt.title(i)\n    plt.show()","8b748e73":"#Check the columns which show non-normal distribution in detail\nprint(train_data['MiscVal'].value_counts())\nprint('\\n')\nprint(train_data['PoolArea'].value_counts())\nprint('\\n')\nprint(train_data['3SsnPorch'].value_counts())\nprint('\\n')\nprint(train_data['ScreenPorch'].value_counts())\nprint('\\n')\nprint(train_data['LowQualFinSF'].value_counts())\nprint('\\n')\nprint(train_data['LotArea'].value_counts())","9f7abc55":"#Drop columns having extreme non-normal distribution\ntrain_data.drop(['PoolArea','3SsnPorch','LowQualFinSF','MiscVal'],axis=1,inplace=True)","ce62930c":"#We are now done with the cleaning of numerical data coulumns. Storing the result in a list\ncleaned_num_columns=['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', 'ScreenPorch', 'MoSold', 'YrSold','NewLotShape', 'NewLandContour', 'NewMasVnrType', 'NewExterQual',\n       'NewBsmtQual', 'NewBsmtCond', 'NewBsmtExposure', 'NewCentralAir',\n       'NewKitchenQual', 'NewGarageFinish', 'NewPavedDrive', 'NewFence']","fc24e8f3":"cat_columns=train_data.select_dtypes(include='object').columns\ncat_columns","1b181d1e":"for i in cat_columns:\n    print(i,\":\",train_data[i].nunique())","a3d8f284":"#select columns having at most 3 unique values from the above \ncat_columns_shortlisted=[]\nfor i in cat_columns:\n    if (train_data[i].nunique()<=4):\n        cat_columns_shortlisted.append(i)\nprint(cat_columns_shortlisted)","606b54c9":"for i in cat_columns_shortlisted:\n    print(i,\":\",\"\\n\",train_data[i].value_counts())\n    print('\\n')","553aeca8":"#From above it is clear that we don't need the Street,Land Slope and Utilities columns as they have repeating values over a wide range\ncat_columns_final=['LotShape','LandContour','MasVnrType', 'ExterQual', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'CentralAir', 'KitchenQual', 'GarageFinish', 'PavedDrive', 'Fence']","cd110720":"#Now use label encoder to encode these values in numerical form\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in cat_columns_final:\n    train_data['New'+ i]=le.fit_transform(train_data[i])\ntrain_data.head()","752dd6b8":"#Store the label encoded columns in a list\ncleaned_cat_columns=[]\nfor i in cat_columns_final:\n    cleaned_cat_columns.append('New'+i)\ncleaned_cat_columns","8c38b3ac":"#Join the the two lists to make the features list.\nfeatures=cleaned_num_columns + cleaned_cat_columns\nfeatures","4acea568":"#X and y are defined\nX=train_data[features]\ny=train_data['SalePrice']","2ddddcf0":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#no. of trees in random forest\nn_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n#no. of features to be considered at every split\nmax_features=['auto','sqrt']\n#Max levels in tree\nmax_depth=[int(x) for  x in np.linspace(10,110,num=11)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5,10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]","680c920a":"random_grid={'n_estimators':n_estimators,\n             'max_features':max_features,\n             'max_depth':max_depth,\n             'min_samples_split':min_samples_split,\n             'min_samples_leaf':min_samples_leaf,\n             'bootstrap':bootstrap}\nprint(random_grid)","1a59b1f1":"rfr=RandomForestRegressor()\nrf_random=RandomizedSearchCV(estimator=rfr,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,random_state=42)\nrf_random.fit(X,y)","16aee051":"best_random=rf_random.best_estimator_\nprint(best_random)","faa7b758":"test_data=pd.read_csv(\"..\/input\/housing-competition-data\/test.csv\")","81bf4777":"test_data.head()","c0830f43":"test_data.info()","04fcf96a":"test_data.describe()","35547fdb":"#Same as train data\ntest_data.drop(['PoolQC','Alley','MiscFeature'],axis=1,inplace=True)\ntest_data['MasVnrArea'].fillna(test_data['MasVnrArea'].mean(),inplace=True)\ntest_data['GarageYrBlt'].fillna(test_data['GarageYrBlt'].median(),inplace=True)\ntest_data[\"LotFrontage\"].fillna(test_data[\"LotFrontage\"].median(),inplace=True)\ntest_data.drop(['PoolArea','3SsnPorch','LowQualFinSF','MiscVal'],axis=1,inplace=True)\n\n#These are some additional values which are null in test data\ntest_data['BsmtFinSF1'].fillna(test_data['BsmtFinSF1'].mean(),inplace=True)         \ntest_data['BsmtFinSF2'].fillna(test_data['BsmtFinSF2'].mean(),inplace=True)     \ntest_data['BsmtUnfSF'].fillna(test_data['BsmtUnfSF'].mean(),inplace=True)         \ntest_data['TotalBsmtSF'].fillna(test_data['TotalBsmtSF'].mean(),inplace=True)\ntest_data['BsmtFullBath'].fillna(test_data['BsmtFullBath'].mean(),inplace=True)\ntest_data['BsmtHalfBath'].fillna(test_data['BsmtHalfBath'].mean(),inplace=True)\ntest_data['GarageCars'].fillna(test_data['GarageCars'].mean(),inplace=True)\ntest_data['GarageArea'].fillna(test_data['GarageArea'].mean(),inplace=True)","87b4655f":"#Encoding categorical values\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in cat_columns_final:\n    test_data['New'+ i]=le.fit_transform(test_data[i])\ntest_data.head()","9d88abd7":"X_test=test_data[features]","9ad49a02":"prediction=best_random.predict(X_test)","1cf5827d":"#output = pd.DataFrame({'Id': test_data.Id,\n#                       'SalePrice': prediction})\n#output.to_csv('submission-8.csv', index=False)","9b792754":"# Checking categorical columns\n","73fd879d":"# Saving the output as a .csv file","1d8b2d9e":"# Cleaning the test data\nFor this dataset we can simply apply the sme cleaning code as we didi for the training set","a645453c":"# Training The Model\nI used Random Forest Regressor along with Randomized Search CV","a9c11b79":"# Focusing on missing values\n\nBased of the info columns we get an idea about which columns have missing values and the proportion of these values to the non-missing ones."}}