{"cell_type":{"d79eab90":"code","69b989ee":"code","e5803f74":"code","7880c078":"code","2bcbcd29":"code","9972bb9c":"code","27793bc0":"code","302f7ce4":"code","8525abbe":"code","7ec15453":"code","6ac5cf28":"code","e582bf28":"code","018ca590":"code","ee8ed0d5":"code","7cb88c53":"code","37a2aa26":"code","0c9c13b9":"code","343e9bf3":"code","6aeb43e0":"code","e5c1de39":"code","03877c0b":"code","8c4d94fc":"code","30598ee1":"code","6b33e688":"code","9d376fd1":"code","e6508b30":"code","78c52fb5":"code","8938dbf6":"code","444b1f2c":"code","44f48348":"code","1d311ca7":"code","2a4ad1ba":"code","63abb8e7":"code","8696d775":"code","1c78810c":"code","4e3a7286":"code","bdb87600":"code","6b71811c":"code","6c717d33":"code","f6e7dd6d":"code","d3f38eb0":"code","9bc77003":"code","d2b233dd":"code","0a0f548c":"code","3dca9911":"code","af124da0":"code","8bf1863b":"code","98acb623":"code","d298343c":"code","1df342da":"code","5cb6cefa":"code","015ec90b":"code","7f9796e3":"code","c3b73f71":"markdown"},"source":{"d79eab90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69b989ee":"tweet = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ngender_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","e5803f74":"tweet.describe","7880c078":"test.describe","2bcbcd29":"tweet.corr()","9972bb9c":"tweet.head(100)","27793bc0":"test.head(100)","302f7ce4":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","8525abbe":"tweet['target'].value_counts()\n","7ec15453":"sns.barplot(tweet['target'].value_counts().index,tweet['target'].value_counts())","6ac5cf28":"x=tweet.keyword.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","e582bf28":"tweet1 = tweet[tweet['target']==0]['keyword'].value_counts(dropna=False)\ntweet1\n","018ca590":"tweet1 = tweet[tweet['target']==1]['keyword'].value_counts(dropna=False)\ntweet1\n\n","ee8ed0d5":"x=tweet.keyword.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","7cb88c53":"x=tweet.location.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","37a2aa26":"tweet1 = tweet[tweet['target']==0]['location'].value_counts(dropna=False)\ntweet1","0c9c13b9":"tweet1 = tweet[tweet['target']==1]['location'].value_counts(dropna=False)\ntweet1","343e9bf3":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntweet_len = tweet[tweet['keyword']==tweet['keyword']]['target'].value_counts()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['keyword']!=tweet['keyword']]['target'].value_counts()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","6aeb43e0":"x=test.keyword.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","e5c1de39":"x=tweet.location.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","03877c0b":"x=test.location.value_counts(dropna=False)\n#sns.barplot(x.index,x)\n#plt.gca().set_ylabel('samples')\nprint(x)\nprint(type(x))","8c4d94fc":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nplt.ylim(0,1200)\nplt.xticks([0, 20,40,60,80,100,120,140,160])\nplt.yticks([0,200,400,600,800,1000])\ntweet_len = tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","30598ee1":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","6b33e688":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='blue')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","9d376fd1":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","e6508b30":"corpus = create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n\ntop = sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10] \n\nx,y=zip(*top)\nplt.bar(x,y)","78c52fb5":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.bar(x,y)","8938dbf6":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y)","444b1f2c":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x,y,color='green')\n","44f48348":"counter = Counter(corpus)\nmost = counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y, y=x)","1d311ca7":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","2a4ad1ba":"df = pd.concat([tweet,test])\ndf.shape","63abb8e7":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndf['text'] = df['text'].apply(lambda x : remove_URL(x))","8696d775":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndf['text'] = df['text'].apply(lambda x : remove_html(x))","1c78810c":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndf['text'] = df['text'].apply(lambda x: remove_emoji(x))","4e3a7286":"def remove_punct(text):\n    table = str.maketrans('','', string.punctuation)\n    return text.translate(table)\n\ndf['text'] = df['text'].apply(lambda x : remove_punct(x))","bdb87600":"!pip install pyspellchecker\n","6b71811c":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","6c717d33":"df['keyword'] = df['keyword'].fillna('0')\n#df['location'] = df['location'].fillna('0')","f6e7dd6d":"df['keyword'].head(100)","d3f38eb0":"df['text'] = df['text'].astype(object) + '('+ df['keyword'].astype(object) + ')'\n#df['text'] = df['text'].astype(object) + '#'+ df['keyword'].astype(object) + '$' + df['location'].astype(object)\n","9bc77003":"df['text']","d2b233dd":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus = create_corpus(df)","0a0f548c":"embedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word]  =vectors\nf.close()\n\nMAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n\nword_index = tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","3dca9911":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","af124da0":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","8bf1863b":"model.summary()","98acb623":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","d298343c":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","1df342da":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","5cb6cefa":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","015ec90b":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","7f9796e3":"sub.head()","c3b73f71":"\u7279\u5fb4\u91cf\u78ba\u8a8d"}}