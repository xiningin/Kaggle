{"cell_type":{"0043e492":"code","4a5ac3da":"code","6e3ba5b9":"code","e96056e3":"code","f00f0b4a":"code","17b8b4f5":"code","964ea15f":"code","54f50ed7":"code","d3a0a179":"code","cb1e6da3":"code","82f2024e":"code","56d6bff4":"code","fba0e4ab":"code","4261137c":"code","6be53417":"code","bdd152ff":"code","b1553ab7":"code","11dcf41d":"code","96fde2d4":"code","7a5b7366":"code","0b8757ef":"code","288f0a54":"code","1b8cf96e":"code","c8acca41":"code","7c0fb781":"code","a9b4c6d3":"code","89714f09":"code","d2180816":"code","0833e0c8":"code","3dd4048c":"code","07d2774b":"code","13e09fcc":"code","90ce49db":"code","2b46dcd9":"code","6ae2a328":"code","9ffa1429":"code","c5a433b2":"code","264a7391":"code","872e2730":"code","fa2e498c":"code","97693bd6":"code","a7c5fd2e":"code","81e5753f":"code","0d414b1a":"code","ffd5a944":"code","27bab142":"code","6496d3be":"code","9d354a1a":"code","cbb097db":"code","82cbeda8":"code","f10e6860":"code","93b69f4f":"code","598aa050":"code","cbbd4b04":"code","df2f4aea":"code","735b96e9":"code","615565cf":"code","76b553dc":"code","89ce4708":"code","684ad590":"code","3bd9c126":"code","2b994d0e":"markdown","943d2e9e":"markdown","451b6f4f":"markdown","b658f613":"markdown","adda5e3b":"markdown","61febb9e":"markdown","ec390c3b":"markdown","1435caad":"markdown","48ed2987":"markdown","cb546af9":"markdown","88f6d83f":"markdown","58025056":"markdown","596d5708":"markdown","39b571c1":"markdown","208faafc":"markdown","0e86411a":"markdown"},"source":{"0043e492":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a5ac3da":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","6e3ba5b9":"nRowRead = 3000 # specify 'None' if want to read whole file\noriginal_dataset = pd.read_csv(\"\/kaggle\/input\/prediction-of-asteroid-diameter\/Asteroid_Updated.csv\", delimiter = ',', nrows = nRowRead)\ndataset = original_dataset.copy()","e96056e3":"\ndataset.head(10)","f00f0b4a":"dataset.describe()","17b8b4f5":"dataset.info()","964ea15f":"dataset.hist(bins = 50, figsize = (20,15))","54f50ed7":"## Convert diameter To float\nconvertDict = {'diameter' : float}\ndataset = dataset.astype(convertDict) ","d3a0a179":"corr_matrix = dataset.corr()\ncorr_matrix.columns\ncorr_matrix['diameter'].sort_values(ascending = False)\n","cb1e6da3":"dataset.isnull().sum()","82f2024e":"#(extent : 10\/3000,GM : 11\/3000, 113\/3000, 'G' : 113\/3000, IR : 0 \/3000) Thse rows have maximun null value\ndropColumn = ['extent','GM','G','IR']\ndataset = dataset.drop(dropColumn, axis = 1)","56d6bff4":"dataset.info()","fba0e4ab":"dataset['diameter'].describe()","4261137c":"dataset['diameter'].median()","6be53417":"# As per Analysis of  columns diameter, we should feel this column with its mean value\n#dataset['diameter'].filna(dataset['diameter'].mean())\ndataset['diameter'].fillna(dataset['diameter'].mean(), inplace=True)","bdd152ff":"dataset['diameter'].describe()","b1553ab7":"# As per Analysis of  columns albedo, we should feel this column with its median value\ndataset['albedo'].fillna(dataset['albedo'].median(), inplace=True)\ndataset['albedo'].describe()","11dcf41d":"# As per Analysis of  columns rot_per, we should feel this column with its mean value\ndataset['rot_per'].fillna(dataset['rot_per'].mean(), inplace=True)\ndataset['rot_per'].describe()","96fde2d4":"# As per Analysis of  columns BV,UB, we should feel this column with its mean value\ndataset['BV'].fillna(dataset['BV'].mean(), inplace=True)\ndataset['UB'].fillna(dataset['UB'].mean(), inplace=True)","7a5b7366":"dataset.info()","0b8757ef":"# Looking for Coorelation\ncorr_matrix = dataset.corr()\ncorr_matrix['diameter'].sort_values(ascending = False)","288f0a54":"# dataset.plot(kind = 'scatter', x = 'rot_per',y = 'diameter', alpha = 0.6)\nimport seaborn as sns\n#dataset.info()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_data = dataset.select_dtypes(include=numerics)\n#num_data.info()\nplt.subplots(figsize=(15,12))\nsns.heatmap(num_data.corr(),annot=True,annot_kws={'size':10})\n#num_data.corr()","1b8cf96e":"# After analysing HeatMap we can element some columns which have no multicolinearity\n#e,i,w, condition_cofde, n_obs_use,albedo,not_per,ma\ndropNumColumn = ['e','i','w','condition_code','n_obs_used','rot_per','ma']\ndataset = dataset.drop(dropNumColumn, axis = 1)\n","c8acca41":"plt.subplots(figsize=(15,12))\nnum_data = dataset.select_dtypes(include=numerics)\nsns.heatmap(num_data.corr(),annot=True,annot_kws={'size':10})","7c0fb781":"#corr_matrix.columns\ndataset.head(10)","a9b4c6d3":"dataset.columns","89714f09":"categoricalData = dataset.select_dtypes(include=['object']).copy()\ncategoricalData.head(5)","d2180816":"categoricalData.isnull().sum()","0833e0c8":"#categoricalData['spec_B'].value_counts()\ncategoricalData = categoricalData.fillna(categoricalData['spec_B'].value_counts().index[0])\ncategoricalData = categoricalData.fillna(categoricalData['spec_T'].value_counts().index[0])","3dd4048c":"# Columns wise Distribution\nprint(categoricalData.isnull().sum())","07d2774b":"# As we can see that\nclass_count = categoricalData['class'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(class_count.index, class_count.values, alpha=0.9)\nplt.title('Frequency Distribution of Class')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Carrier', fontsize=12)\nplt.show()","13e09fcc":"#dataset['neo'].value_counts()\ncategoricalData['pha'].value_counts()","90ce49db":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc = LabelEncoder()\ncategoricalData['neo'] = labelEnc.fit_transform(categoricalData['neo'])\ncategoricalData['pha'] = labelEnc.fit_transform(categoricalData['pha'])","2b46dcd9":"categoricalData.head()","6ae2a328":"# Now do one hot encoder\ncategoricalData = pd.get_dummies(categoricalData, columns=['neo','pha'])\ncategoricalData.head()","9ffa1429":"from sklearn.preprocessing import LabelBinarizer\n#categoricalDataClass = categoricalDataCopy.copy()\nlb = LabelBinarizer()\nlb_results = lb.fit_transform(categoricalData['class'])\nlb_results_df = pd.DataFrame(lb_results, columns=lb.classes_)\n","c5a433b2":"categoricalData = pd.concat([categoricalData, lb_results_df], axis=1)\ncategoricalData.head()","264a7391":"categoricalData['class'].value_counts()","872e2730":"\ncategoricalData['spec_B'] = labelEnc.fit_transform(categoricalData['spec_B'])\ncategoricalData['spec_T'] = labelEnc.fit_transform(categoricalData['spec_T'])\n","fa2e498c":"categoricalData.head()","97693bd6":"# Now Drob Class column beacse it has been converted into LabelBinarizor\n# Drop name column it jus a name\ncategoricalData.drop(['name','class'], inplace = True, axis = 1)\n","a7c5fd2e":"categoricalData.head(3)","81e5753f":"num_data.head(3)","0d414b1a":"cleanDataset = pd.concat([categoricalData,num_data],axis = 1)","ffd5a944":"cleanDataset.head()","27bab142":"#Split Data into features and target\ny = cleanDataset['diameter']\nX = cleanDataset.drop(['diameter'],axis = 1)","6496d3be":"X = X.iloc[:,:].values\nX.shape","9d354a1a":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nmy_pipeline = Pipeline([\n     ('std_scaler', StandardScaler()),\n    # Add as many as you can\n])","cbb097db":"X_std = my_pipeline.fit_transform(X)","82cbeda8":"X_std.shape","f10e6860":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2, random_state = 0)","93b69f4f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n#model = LinearRegression()\n#model = DecisionTreeRegressor()\nmodel = RandomForestRegressor()","598aa050":"model.fit(X_train, y_train)","cbbd4b04":"model.predict(X_test)","df2f4aea":"from sklearn.metrics import mean_squared_error\ndiameterPrediction  = model.predict(X_test)\nlin_mse = mean_squared_error(y_test, diameterPrediction)\nlin_mse = np.sqrt(lin_mse)","735b96e9":"lin_mse","615565cf":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test,diameterPrediction)\nprint(\"R2 : \",r2)\n","76b553dc":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 10)","89ce4708":"rm_error = np.sqrt(-scores)# - because sqrt does not calculate negative value\nrm_error","684ad590":"def print_score(score):\n    print(\"Score: \", score)\n    print(\"Mean: \", score.mean())\n    print(\"Std: \", score.std())","3bd9c126":"print_score(rm_error)","2b994d0e":"## Evaluating Models","943d2e9e":"#Split Data into features and target","451b6f4f":"# R-Square\nR-squared is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r-square is 1. The closer the value of r-square to 1, the better is the model fitted.","b658f613":"## Describe Data","adda5e3b":"## Selecting a Desired Model for Our Project","61febb9e":"## Now Add Numerical and Categorical data which we hace cleaned and transformed","ec390c3b":"## We have filled Numerical data\nNow lets analyse thse data with diameter columns values","1435caad":"## Using Better Eveluation Techniques:\nHow it works:\n1 2 3 4 5 : it create  5 group(cv : fold) (example it may more)\nit trains 2 3 4 5 and test 1\nagian it trains 1 3 4 5 and test 2\n and so on , finalyy returns score","48ed2987":"## Lets Play With Categorical Data","cb546af9":"## Feature Scaling\nPrimarily, there are two types of feature scaling method:\n1. min-max scaling(Normalization)\n(values -min) \/ (max - min)  # Lies 0-1\n2. Standardization:\n(values - mean \/ std)\nfor this sklearn provide class standardScaler\n","88f6d83f":"Display Data Information","58025056":"## Represent Data","596d5708":"# Data Cleaning\n## Removing Numerical Columns\nAs We can see that Columns:\n'extent' has 10 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'GM' has 11 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'G' has 113 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'IR' has 0 non null values\n(extent : 10\/3000,GM : 11\/3000, 113\/3000, 'G' : 113\/3000,'IR' : 0 \/3000)\nHence They should be removed from Dataframe","39b571c1":"# Split Data set into Training and test set","208faafc":"## Fill Missing data in categorical","0e86411a":"# Analysys of Numerical data with Diameter"}}