{"cell_type":{"464ba937":"code","ed2d0607":"code","8e45a11e":"code","1d9d35a4":"code","6f6698b7":"code","4057736d":"code","a488aa7b":"code","60fc1f51":"code","bcae83b3":"code","e7e5a6c3":"code","03e2e4f6":"code","d37d9684":"code","96e018ed":"code","453c5a5b":"code","61eba2bc":"code","41dffa74":"code","735e7854":"code","3b84ab92":"markdown","dd95e65b":"markdown","8a584343":"markdown","5d7c31d3":"markdown","b7333ff9":"markdown","b7b2d249":"markdown","44bf9381":"markdown","63c8866f":"markdown","607fe029":"markdown","42b4460d":"markdown","43b8da3b":"markdown","77062a8f":"markdown","68179cd8":"markdown","635007c6":"markdown","57f55bb2":"markdown","678ae538":"markdown"},"source":{"464ba937":"!pip install feyn","ed2d0607":"import feyn\nimport pandas as pd\nimport sklearn.model_selection","8e45a11e":"data = '\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv'\ndf = pd.read_csv(data)\ndf","1d9d35a4":"df.isna().sum()","6f6698b7":"df.drop(columns={'time'}, axis=1, inplace=True)","4057736d":"df.DEATH_EVENT.value_counts()","a488aa7b":"train, test = sklearn.model_selection.train_test_split(df, stratify=df[\"DEATH_EVENT\"], train_size=.66, random_state=1)","60fc1f51":"test.DEATH_EVENT.value_counts()","bcae83b3":"ql = feyn.connect_qlattice()","e7e5a6c3":"ql.reset(random_seed=1)","03e2e4f6":"ql.reset(random_seed=1)\nmodels = ql.auto_run(train, output_name=\"DEATH_EVENT\", kind=\"classification\", max_complexity=10, criterion='aic')","d37d9684":"models[0]","96e018ed":"models[0].plot(train, test)","453c5a5b":"rf = feyn.reference.RandomForestClassifier(train, target=\"DEATH_EVENT\")\ngb = feyn.reference.GradientBoostingClassifier(train, target=\"DEATH_EVENT\")\nlr = feyn.reference.LogisticRegressionClassifier(train, target=\"DEATH_EVENT\", max_iter=10000)","61eba2bc":"models[0].plot_roc_curve(test, label='QLattice')\nrf.plot_roc_curve(test, label=\"Random Forest\")\ngb.plot_roc_curve(test, label=\"Gradient Boosting\")\nlr.plot_roc_curve(test, label=\"Logistic Regression\")","41dffa74":"models[0].plot_confusion_matrix(test)","735e7854":"rf.plot_confusion_matrix(test)","3b84ab92":"# Performance on train vs. test\nLet's see how the model performs on the train versus test dataset. We are looking for high accuracy and AUC, but similar values across the two datasets (we don't want overfitting!)","dd95e65b":"# What did we learn?\n1. Interestingly, with only four inputs: ejection fraction, serum creatinine, sex, and age, we can predict whether or not someone will die from heart failure relatively well. Cooler still, ejection fraction and serum creatinine have previously been identified as important features using other machine learning algorithms as cited by the authors of the original dataset. From this we can see that **the QLattice is an effective feature selection tool**.\n\n2. The QLattice also creates models that show not only which features interact to produce the outputs, but also **how** they relate to one another.\n\n3. In terms of pure AUC the QLattice model performs better than (Gradient Boost and Logistic Regression) or similar to (Random Forest) other machine learning algorithms.\n\n4. As an overall application in medecine, the QLattice model is the best option because it combines a high AUC score with a lower false negative rate when compared to its biggest competator, Random Forest.\n\n\n\n**Pretty neat!**","8a584343":"# Resetting and reproducability\nThe QLattice has the potential to store learnings between sessions to enable transfer of learning and federated learning. This is not possible with Community QLattices, since a new one gets allocated whenever we run the notebook, so it is not strictly necessary to call the reset function on our new QLattice.\n\nBut the reset function also allows us to provide a random seed, which will ensure that we get the same results every time we run this notebook","5d7c31d3":"We can see that it is slightly skewed towards survival\n# Splitting the data\nLet's split the data into train and test sets. We will stratify by `DEATH_EVENT` and take 2\/3 of the entire dataset for training","b7333ff9":"# What did we find?\n`models` is a list of graphs sorted by accuracy. Each model shows how the selected features, or inputs, interact to achieve the output. We can access the best graph by calling:","b7b2d249":"# Getting the Data\nFirst let's load the dataset and take a look","44bf9381":"# Comparaison\nLet's compare this to three other Machine Learning algorithms: Random Forest, Gradient Boost, and Logistic Regression","63c8866f":"# Search for the best model\nWe are now ready to instruct the QLattice to search for the best mathematical model to explain the data. Here we use the high-level convenience function that does everything with sensible defaults: https:\/\/docs.abzu.ai\/docs\/guides\/essentials\/auto_run.html.\n\nFor more detailed control, we could use the primitives: https:\/\/docs.abzu.ai\/docs\/guides\/primitives\/using_primitives.html\n\nNOTE: This will take a minute to complete. It invoves work done on the QLattice machine remotely as well as in the local notebook. The part that runs locally is slowing things down because of the limited CPU resources on Kaggle. Running the same on my machine locally only takes 10 seconds!","607fe029":"# Allocate a QLattice\nThe actual QLattice is a quantum simulator that runs on Abzu's hardware, but we can allocate one with a single line of code. Cool, huh?","42b4460d":"We can see here that the QLattice model has a much lower false negative rate. It predicts that 13 heart failure subjects are fine compared to the 19 predicted by Random Forest. From a medicine perspective, the QLattice model is therefore better.","43b8da3b":"# Target variable\nLet's take a look at the distribution of the target variable (how many people died versus survived)","77062a8f":"# Python imports\nIn this notebook we will use only two python modules: the `feyn` module to access the QLattice, the `pandas` module to access the data, and `sklearn` to split the data into train and test sets","68179cd8":"# First impressions:\nWe notice that:\n- The target variable is `DEATH_EVENT`\n- All data types are numerical. The QLattice works with both categorical and numerical data, but needs to be told which entries are categorical (i.e. it assumes they are numerical). Since all entries in this dataset are numerical we're all good!\n- There are no missing entries\n\nLet's remove the `time` column","635007c6":"# Explainable model for predicting heart failure using the QLattice\n\nThe QLattice is a supervised machine learning tool for symbolic regression developed by [Abzu](https:\/\/www.abzu.ai) . It is inspired by Richard Feynman's path integral formulation. That's why the python module to use it is called *Feyn*, and the *Q* in QLattice is for Quantum.\n\nAbzu provides free QLattices for non-commercial use to anyone. These free community QLattices gets allocated for you automatically if you use Feyn without an active subscription, as we will do in this notebook. Read more about how it works here: https:\/\/docs.abzu.ai\/docs\/guides\/getting_started\/community.html\n\nThe feyn Python module is not installed on Kaggle by default so we have to pip install it first. \n\n__Note__: the pip install will fail unless you enable *Internet* in the *settings* to the right--->","57f55bb2":"We can see here that the QLattice outperforms Gradient Boost and Logistic Regression. The ROC curve for the QLattice and Random Forest seem to resemble one another. So which is better?\n\n# QLattice vs. Random Forest: type of failure matters!\nLet's take a closer look at how the two models interact with the data. The performances of the two models can be quickly evaluated using accuracy or AUC, however, important insight as to which is better can be found by looking at **where the models fail**.\n\nFirst let's ask ourselves the question: Which is worse? Having the doctor tell you that you will experience heart failure and then not, or being told you are fine and dying? Clearly the second! A model predicting that you will be fine, but actually experiencing heart failure is called a false negative. In medical prediction cases such as this false negatives can be life threatening. Therfore when evaluating model performance we want to chose one that minimizes this outcome. We can see the number of false negative predictions the QLattice and Random Forest models make using a **confusion matrix**.","678ae538":"We can visualize their relative performances using their respective ROC curves"}}