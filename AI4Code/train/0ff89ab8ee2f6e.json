{"cell_type":{"a8bb9c8d":"code","4b68f90f":"code","73c4f7b1":"code","57e92edf":"code","ec786822":"code","1461af1e":"code","208deb4e":"code","f396c5e5":"code","b1ece3af":"code","a28e159a":"code","91562d2d":"code","21477759":"code","3393b7df":"code","51c78d9a":"code","3e53b047":"code","fc6b6fcd":"code","5787ecdf":"code","3cae0555":"code","6d59acef":"code","d8c2a56e":"code","5ee4323b":"code","61064147":"code","4254e417":"code","b96709d1":"code","4f3c3ab6":"code","a08c471f":"code","487d5033":"code","445f8d4f":"code","5415fd73":"code","8fde958c":"code","54423eee":"code","eb9cc43e":"code","13d81820":"code","8246290a":"code","2e24040d":"code","3a4d9a74":"code","e52c2852":"code","682db602":"code","e12df625":"code","54d0e595":"code","d47910e5":"code","6b9ac2da":"code","1ef1c059":"code","985c8cda":"code","45cea93d":"code","85978787":"code","1630ee6d":"code","5cc4b830":"code","f714559a":"code","332d65f6":"code","04628825":"markdown","b6d4305a":"markdown","c497d514":"markdown","1b5963f5":"markdown","bfbedbb5":"markdown","4268f561":"markdown","8cacdc92":"markdown","de321153":"markdown","4e32779d":"markdown","a24792d6":"markdown","a1af025f":"markdown","0fe9c165":"markdown","ef8c3df1":"markdown","53f9e145":"markdown","c5d5c997":"markdown","ca2e45e2":"markdown","ed3bd2cd":"markdown","b0e296fa":"markdown","ba9d5ff0":"markdown","8a1a838a":"markdown","768bac7f":"markdown"},"source":{"a8bb9c8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfile_path = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        file_path.append(os.path.join(dirname,filename))\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest     = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission    = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\ntrain.shape , test.shape , submission.shape\ntrain_col_name = train.columns\ntest_col_name = test.columns","4b68f90f":"train.dtypes","73c4f7b1":"categorical_variables = [\"keyword\",\"location\",\"text\"]\ncontinious_variables  = [\"target\"]","57e92edf":"dq_categorical_var = train[categorical_variables].describe()\ndq_categorical_var = dq_categorical_var.T\ndq_categorical_var[\"total_record\"] = train[\"id\"].count()\ndq_categorical_var[\"missing_value%\"] = (1- dq_categorical_var[\"count\"]\/dq_categorical_var[\"total_record\"])*100\ndq_categorical_var.T","ec786822":"dq_cont_var= train[continious_variables].describe()\ndq_cont_var = dq_cont_var.T\ndq_cont_var[\"total_record\"] = train[\"id\"].count()\ndq_cont_var[\"missing_value%\"] = (1- dq_cont_var[\"count\"]\/dq_cont_var[\"total_record\"])*100\ndq_cont_var.T","1461af1e":"col_name = train.columns\nexclude_col = [\"id\",\"text\"]\nfor c in col_name :\n    if c not in exclude_col : \n        print(train[c].value_counts())","208deb4e":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud , STOPWORDS\n\ndef word_cloud(data):\n    stopword = set(STOPWORDS)\n    comment_words = ' '\n    for val in data:\n        val = str(val)\n        tokens = val.split()\n        for i in range(len(tokens)) :\n            tokens[i] = tokens[i].lower()\n        for words in tokens:\n            comment_words = comment_words + words + ' '\n    \n    wordcloud = WordCloud(width = 800 , height = 800 , background_color = 'white', stopwords = stopword , min_font_size = 9).generate(comment_words)\n    plt.figure(figsize= (8,8), facecolor = None)\n    plt.imshow(wordcloud)","f396c5e5":"word_cloud(train[\"text\"])","b1ece3af":"import nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndef word_freq(text):\n    sent = []\n    for txt in text :\n        sent.append(txt)\n    \n    word_tokens = nltk.word_tokenize(\"\".join([s for s in sent]))\n    word_freq = nltk.FreqDist(word_tokens)\n    print(len(word_tokens))\n    sns.set(rc={'figure.figsize':(11.7,8.27)})\n    sns.set_style('darkgrid')\n    word_freq.plot(50)","a28e159a":"word_freq(train.text)","91562d2d":"from bs4 import BeautifulSoup\nimport re\nimport nltk\nimport emoji\nimport string","21477759":"# emoticons\ndef load_dict_smileys():\n    return {\n        \":\u2011)\":\"smiley\",\n        \":-]\":\"smiley\",\n        \":-3\":\"smiley\",\n        \":->\":\"smiley\",\n        \"8-)\":\"smiley\",\n        \":-}\":\"smiley\",\n        \":)\":\"smiley\",\n        \":]\":\"smiley\",\n        \":3\":\"smiley\",\n        \":>\":\"smiley\",\n        \"8)\":\"smiley\",\n        \":}\":\"smiley\",\n        \":o)\":\"smiley\",\n        \":c)\":\"smiley\",\n        \":^)\":\"smiley\",\n        \"=]\":\"smiley\",\n        \"=)\":\"smiley\",\n        \":-))\":\"smiley\",\n        \":\u2011D\":\"smiley\",\n        \"8\u2011D\":\"smiley\",\n        \"x\u2011D\":\"smiley\",\n        \"X\u2011D\":\"smiley\",\n        \":D\":\"smiley\",\n        \"8D\":\"smiley\",\n        \"xD\":\"smiley\",\n        \"XD\":\"smiley\",\n        \":\u2011(\":\"sad\",\n        \":\u2011c\":\"sad\",\n        \":\u2011<\":\"sad\",\n        \":\u2011[\":\"sad\",\n        \":(\":\"sad\",\n        \":c\":\"sad\",\n        \":<\":\"sad\",\n        \":[\":\"sad\",\n        \":-||\":\"sad\",\n        \">:[\":\"sad\",\n        \":{\":\"sad\",\n        \":@\":\"sad\",\n        \">:(\":\"sad\",\n        \":'\u2011(\":\"sad\",\n        \":'(\":\"sad\",\n        \":\u2011P\":\"playful\",\n        \"X\u2011P\":\"playful\",\n        \"x\u2011p\":\"playful\",\n        \":\u2011p\":\"playful\",\n        \":\u2011\u00de\":\"playful\",\n        \":\u2011\u00fe\":\"playful\",\n        \":\u2011b\":\"playful\",\n        \":P\":\"playful\",\n        \"XP\":\"playful\",\n        \"xp\":\"playful\",\n        \":p\":\"playful\",\n        \":\u00de\":\"playful\",\n        \":\u00fe\":\"playful\",\n        \":b\":\"playful\",\n        \"<3\":\"love\"\n        }\n\n# self defined contractions\ndef load_dict_contractions():\n    \n    return {\n        \"ain't\":\"is not\",\n        \"amn't\":\"am not\",\n        \"aren't\":\"are not\",\n        \"can't\":\"cannot\",\n        \"'cause\":\"because\",\n        \"couldn't\":\"could not\",\n        \"couldn't've\":\"could not have\",\n        \"could've\":\"could have\",\n        \"daren't\":\"dare not\",\n        \"daresn't\":\"dare not\",\n        \"dasn't\":\"dare not\",\n        \"didn't\":\"did not\",\n        \"doesn't\":\"does not\",\n        \"don't\":\"do not\",\n        \"e'er\":\"ever\",\n        \"em\":\"them\",\n        \"everyone's\":\"everyone is\",\n        \"finna\":\"fixing to\",\n        \"gimme\":\"give me\",\n        \"gonna\":\"going to\",\n        \"gon't\":\"go not\",\n        \"gotta\":\"got to\",\n        \"hadn't\":\"had not\",\n        \"hasn't\":\"has not\",\n        \"haven't\":\"have not\",\n        \"he'd\":\"he would\",\n        \"he'll\":\"he will\",\n        \"he's\":\"he is\",\n        \"he've\":\"he have\",\n        \"how'd\":\"how would\",\n        \"how'll\":\"how will\",\n        \"how're\":\"how are\",\n        \"how's\":\"how is\",\n        \"I'd\":\"I would\",\n        \"I'll\":\"I will\",\n        \"I'm\":\"I am\",\n        \"I'm'a\":\"I am about to\",\n        \"I'm'o\":\"I am going to\",\n        \"isn't\":\"is not\",\n        \"it'd\":\"it would\",\n        \"it'll\":\"it will\",\n        \"it's\":\"it is\",\n        \"I've\":\"I have\",\n        \"kinda\":\"kind of\",\n        \"let's\":\"let us\",\n        \"mayn't\":\"may not\",\n        \"may've\":\"may have\",\n        \"mightn't\":\"might not\",\n        \"might've\":\"might have\",\n        \"mustn't\":\"must not\",\n        \"mustn't've\":\"must not have\",\n        \"must've\":\"must have\",\n        \"needn't\":\"need not\",\n        \"ne'er\":\"never\",\n        \"o'\":\"of\",\n        \"o'er\":\"over\",\n        \"ol'\":\"old\",\n        \"oughtn't\":\"ought not\",\n        \"shalln't\":\"shall not\",\n        \"shan't\":\"shall not\",\n        \"she'd\":\"she would\",\n        \"she'll\":\"she will\",\n        \"she's\":\"she is\",\n        \"shouldn't\":\"should not\",\n        \"shouldn't've\":\"should not have\",\n        \"should've\":\"should have\",\n        \"somebody's\":\"somebody is\",\n        \"someone's\":\"someone is\",\n        \"something's\":\"something is\",\n        \"that'd\":\"that would\",\n        \"that'll\":\"that will\",\n        \"that're\":\"that are\",\n        \"that's\":\"that is\",\n        \"there'd\":\"there would\",\n        \"there'll\":\"there will\",\n        \"there're\":\"there are\",\n        \"there's\":\"there is\",\n        \"these're\":\"these are\",\n        \"they'd\":\"they would\",\n        \"they'll\":\"they will\",\n        \"they're\":\"they are\",\n        \"they've\":\"they have\",\n        \"this's\":\"this is\",\n        \"those're\":\"those are\",\n        \"'tis\":\"it is\",\n        \"'twas\":\"it was\",\n        \"wanna\":\"want to\",\n        \"wasn't\":\"was not\",\n        \"we'd\":\"we would\",\n        \"we'd've\":\"we would have\",\n        \"we'll\":\"we will\",\n        \"we're\":\"we are\",\n        \"weren't\":\"were not\",\n        \"we've\":\"we have\",\n        \"what'd\":\"what did\",\n        \"what'll\":\"what will\",\n        \"what're\":\"what are\",\n        \"what's\":\"what is\",\n        \"what've\":\"what have\",\n        \"when's\":\"when is\",\n        \"where'd\":\"where did\",\n        \"where're\":\"where are\",\n        \"where's\":\"where is\",\n        \"where've\":\"where have\",\n        \"which's\":\"which is\",\n        \"who'd\":\"who would\",\n        \"who'd've\":\"who would have\",\n        \"who'll\":\"who will\",\n        \"who're\":\"who are\",\n        \"who's\":\"who is\",\n        \"who've\":\"who have\",\n        \"why'd\":\"why did\",\n        \"why're\":\"why are\",\n        \"why's\":\"why is\",\n        \"won't\":\"will not\",\n        \"wouldn't\":\"would not\",\n        \"would've\":\"would have\",\n        \"y'all\":\"you all\",\n        \"you'd\":\"you would\",\n        \"you'll\":\"you will\",\n        \"you're\":\"you are\",\n        \"you've\":\"you have\",\n        \"Whatcha\":\"What are you\",\n        \"luv\":\"love\",\n        \"sux\":\"sucks\"\n        }","3393b7df":"def clean_text(txt):\n    #remove html tag\n    txt = BeautifulSoup(txt).get_text()\n    #special cases not handled before\n    txt = txt.replace('\\x92',\"''\")\n    #removal of hash tags\n    txt = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", txt).split())\n    #removal url \n    txt = ' '.join(re.sub(\"(\\w+:\\\/\\\/\\S+)\", \" \", txt).split())\n    #replace contractions\n    contractions = load_dict_contractions()\n    txt = txt.strip()\n    words = txt.split()\n    reformed = [contractions[word] if word in contractions else word for word in words]\n    txt = \" \".join(reformed)    \n    #remove special characters\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    txt = filter(None, [pattern.sub(' ', token) for token in txt])\n    txt = ''.join(txt)\n    #convert to lower case\n    txt = txt.lower()\n    #handle emoticons\n    SMILEY = load_dict_smileys()  \n    txt = txt.split()\n    reformed =  [SMILEY[word] if word in SMILEY else word for word in txt]\n    txt = \" \".join(reformed)\n    #handle emojis\n    txt = emoji.demojize(txt)\n    txt = txt.replace(\":\",\" \")\n    return txt","51c78d9a":"train = np.array(train)\ntest  = np.array(test)\n#train\nfor i in range(len(train)):\n    #print(train[i][3])\n    train[i][3] = clean_text(train[i][3])\ntrain = pd.DataFrame(train)\ntrain.columns = train_col_name\n#test\nfor i in range(len(test)):\n    #print(train[i][3])\n    test[i][3] = clean_text(test[i][3])\ntest = pd.DataFrame(test)\ntest.columns = test_col_name","3e53b047":"word_cloud(train[\"text\"])","fc6b6fcd":"word_freq(train.text)","5787ecdf":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef bow_extractor(corpus, ngram_range=(1,1)):\n    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features\n\ndef tfidf_extractor(corpus, ngram_range=(1,1)):\n    vectorizer = TfidfVectorizer(norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range ,max_features = 200,min_df = 5,stop_words='english')\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features","3cae0555":"#bow_vectorizer , bow_train_features = bow_extractor(train.text,(1,1))\n#bow_test_features = bow_vectorizer.transform(test.text)\n#tfidf_vectorizer , tfidf_train_features = tfidf_extractor(train.text, (1,1))\n#tfidf_test_vectorizer = tfidf_vectorizer.transform(test.text)","6d59acef":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pickle\n\ndef modelling_data(data, labels , split_size):\n    train_X, test_X, train_Y , test_Y = train_test_split(data,labels, test_size = split_size,random_state = 42)\n    return train_X, test_X, train_Y , test_Y\n\ndef model_accuracy(actuals , pred,model_result,flag):\n    if flag == 0 :\n        #print(\"Accuracy:\", np.round(metrics.accuracy_score(actuals, pred),2))\n        model_result[2] = np.round(metrics.accuracy_score(actuals, pred),2)\n        #print(\"Precision:\", np.round(metrics.precision_score(actuals,pred),2))\n        model_result[3] = np.round(metrics.precision_score(actuals,pred),2)\n        #print(\"Recall:\",np.round(metrics.recall_score(actuals,pred),2))\n        model_result[4] = np.round(metrics.recall_score(actuals,pred),2)\n        #print(\"F1 Score\",np.round(metrics.f1_score(actuals, pred),2))\n        model_result[5] = np.round(metrics.f1_score(actuals, pred),2)\n        #print(\"Confusion_Matrix:\", metrics.confusion_matrix(actuals,pred))\n        flag = flag  + 1\n    elif flag  == 1 :\n        #print(\"Accuracy:\", np.round(metrics.accuracy_score(actuals, pred),2))\n        model_result[6] = np.round(metrics.accuracy_score(actuals, pred),2)\n        #print(\"Precision:\", np.round(metrics.precision_score(actuals,pred),2))\n        model_result[7] = np.round(metrics.precision_score(actuals,pred),2)\n        #print(\"Recall:\",np.round(metrics.recall_score(actuals,pred),2))\n        model_result[8] = np.round(metrics.recall_score(actuals,pred),2)\n        #print(\"F1 Score\",np.round(metrics.f1_score(actuals, pred),2))\n        model_result[9] = np.round(metrics.f1_score(actuals, pred),2)\n        #print(\"Confusion_Matrix:\", metrics.confusion_matrix(actuals,pred))\n        flag = flag  + 1\n        \n    return model_result , flag\n    \ndef model_score(model ,trainX, trainY, testX, testY,model_name,embedding_type):\n    flag = 0\n    model_result = [None]*10\n    model_result[0] = model_name\n    model_result[1] = embedding_type\n    print(model)\n    model.fit(trainX, trainY)\n    train_pred = model.predict(trainX)\n    test_pred  = model.predict(testX)    \n    #print(\"Training_Model_Performance:\") \n    model_result , flag = model_accuracy(trainY,train_pred,model_result,flag)\n    #print(\"Test_Model_Performance:\") \n    model_result , flag = model_accuracy(testY,test_pred,model_result,flag)  \n    return model_result, pd.DataFrame(train_pred), pd.DataFrame(test_pred),model\n\ndef model_summary(model_output,column_names):\n    summary = pd.DataFrame(model_output)\n    summary.columns = column_names\n    return summary\n\ndef decile_analysis():\n    pass\n\ndef save_ml_model(file_name , model):\n    with open(file_name,'wb') as file_name:\n        pickle.dump(model,file_name)","d8c2a56e":"trainX, testX, trainY, testY = modelling_data(train.text, train.target , .40)\npd.DataFrame(trainX).to_csv('\/kaggle\/working\/trainX.csv')\npd.DataFrame(testX).to_csv('\/kaggle\/working\/testX.csv')\npd.DataFrame(trainY).to_csv(\"\/kaggle\/working\/trainY.csv\")\npd.DataFrame(testY).to_csv(\"\/kaggle\/working\/testY.csv\")\n\n#store model results\nmodel_output = []\ncolumns = ['Model_Name','Embeddings_Type','Train_Accuracy','Train_Precision','Train_Recall','Train_F1','Test_Accuracy','Test_Precision','Test_Recal','Test_F1']","5ee4323b":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","61064147":"bow_vectorizer , bow_train_features = bow_extractor(trainX,(1,1))\nbow_test_featurs = bow_vectorizer.transform(testX)\nsave_ml_model('bow_vectorizer',bow_vectorizer)","4254e417":"from sklearn.naive_bayes import MultinomialNB\n\nnb_model = MultinomialNB()\nmodel_result, nb_bow_train_pred, nb_bow_test_pred,trained_model = model_score(nb_model,bow_train_features,trainY.astype(\"int\"), bow_test_featurs,testY.astype(\"int\"),'nb_bow_model','bag_of words')\nmodel_output.append(model_result)\nnb_bow_train_pred.to_csv('\/kaggle\/working\/nb_bow_train_pred.csv')\nnb_bow_test_pred.to_csv('\/kaggle\/working\/nb_bow_test_pred.csv')\nsave_ml_model('nb_bow_model',trained_model)\n","b96709d1":"\nsgd_model = SGDClassifier(loss='hinge',penalty = 'l2')\nmodel_result,sgd_bow_train_pred, sgd_bow_test_pred,trained_model  = model_score(sgd_model,bow_train_features,trainY.astype(\"int\"), bow_test_featurs,testY.astype(\"int\"),'sgd_bow_model','bag_of words')\nmodel_output.append(model_result)\nsgd_bow_train_pred.to_csv('\/kaggle\/working\/sgd_bow_train_pred.csv')\nsgd_bow_test_pred.to_csv('\/kaggle\/working\/sgd_bow_test_pred.csv')\nsave_ml_model('sgd_bow',trained_model)\n","4f3c3ab6":"\nsvc_model = SVC()\nmodel_result,svm_bow_train_pred,svm_bow_test_pred,trained_model = model_score(svc_model,bow_train_features,trainY.astype(\"int\"), bow_test_featurs,testY.astype(\"int\"),'svc_bow_model','bag_of words')\nmodel_output.append(model_result)\nsvm_bow_train_pred.to_csv('\/kaggle\/working\/svm_bow_train_pred.csv')\nsvm_bow_test_pred.to_csv('\/kaggle\/working\/svm_bow_test_pred.csv')\nsave_ml_model('svm_bow',trained_model)\n","a08c471f":"#nagram = 1\ntfidf_vectorizer_1 , tfidf_train_features_1 = tfidf_extractor(trainX, (1,1))\ntfidf_test_featurs_1 = tfidf_vectorizer_1.transform(testX)\nsave_ml_model('tfidf_vectorizer',tfidf_vectorizer_1)\n\n#ngram = 2\ntfidf_vectorizer_2 , tfidf_train_features_2 = tfidf_extractor(trainX, (1,2))\ntfidf_test_featurs_2 = tfidf_vectorizer_2.transform(testX)\nsave_ml_model('tfidf_vectorizer_ngram2',tfidf_vectorizer_2)\n","487d5033":"#ngram = 1\nnb_model = MultinomialNB()\nmodel_result,nb_tfidf_ngm1_train_pred,nb_tfidf_ngm1_test_pred,trained_model = model_score(nb_model,tfidf_train_features_1,trainY.astype(\"int\"), tfidf_test_featurs_1,testY.astype(\"int\"),'nb_tfidf_model','tfidf')\nmodel_output.append(model_result)\nnb_tfidf_ngm1_train_pred.to_csv('\/kaggle\/working\/nb_tfidf_ngm1_train_pred.csv')\nnb_tfidf_ngm1_test_pred.to_csv('\/kaggle\/working\/nb_tfidf_ngm1_test_pred.csv')\nsave_ml_model('nb_tfidf_ngm1',trained_model)\n\n\n#ngram = 2\nnb_model = MultinomialNB()\nmodel_result,nb_tfidf_ngm2_train_pred,nb_tfidf_ngm2_test_pred,trained_model = model_score(nb_model,tfidf_train_features_2,trainY.astype(\"int\"), tfidf_test_featurs_2,testY.astype(\"int\"),'nb_tfidf_model','tfidf_ngram2')\nmodel_output.append(model_result)\nnb_tfidf_ngm2_train_pred.to_csv('\/kaggle\/working\/nb_tfidf_ngm2_train_pred.csv')\nnb_tfidf_ngm2_test_pred.to_csv('\/kaggle\/working\/nb_tfidf_ngm2_test_pred.csv')\nsave_ml_model('nb_tfidf_ngm2',trained_model)","445f8d4f":"\n#ngram =1\nsgd_model = SGDClassifier(loss='hinge',penalty = 'l2')\nmodel_result,sgd_tfidf_ngm1_train_pred,sgd_tfidf_ngm1_test_pred,trained_model = model_score(sgd_model,tfidf_train_features_1,trainY.astype(\"int\"), tfidf_test_featurs_1,testY.astype(\"int\"),'sgd_tfidf_model','tfidf')\nmodel_output.append(model_result)\nsgd_tfidf_ngm1_train_pred.to_csv('\/kaggle\/working\/sgd_tfidf_ngm1_train_pred.csv')\nsgd_tfidf_ngm1_test_pred.to_csv('\/kaggle\/working\/sgd_tfidf_ngm1_test_pred.csv')\nsave_ml_model('sgd_tfidf_ngm1',trained_model)\n\n#ngram= 2\nsgd_model = SGDClassifier(loss='hinge',penalty = 'l2')\nmodel_result,sgd_tfidf_ngm2_train_pred,sgd_tfidf_ngm2_test_pred,trained_model = model_score(sgd_model,tfidf_train_features_2,trainY.astype(\"int\"), tfidf_test_featurs_2,testY.astype(\"int\"),'sgd_tfidf_model','tfidf_ngram2')\nmodel_output.append(model_result)\nsgd_tfidf_ngm2_train_pred.to_csv('\/kaggle\/working\/sgd_tfidf_ngm2_train_pred.csv')\nsgd_tfidf_ngm2_test_pred.to_csv('\/kaggle\/working\/sgd_tfidf_ngm2_test_pred.csv')\nsave_ml_model('sgd_tfidf_ngm2',trained_model)\n\n\n#hyper_param tunning with ngram = 1\nparams = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n}\n\nmodel = SGDClassifier(max_iter=1000)\nsgd_opt_tfidf_clf = GridSearchCV(model, param_grid=params , cv=10, scoring='f1', verbose=0, n_jobs=-1)\nsgd_opt_tfidf_clf.fit(tfidf_train_features_1,trainY.astype(\"int\"))\nsgd_opt_tfidf_clf_best = sgd_opt_tfidf_clf.best_estimator_\nmodel_result,sgd_tfidf_opt_train_pred,sgd_tfidf_opt_test_pred,trained_model = model_score(sgd_opt_tfidf_clf_best,tfidf_train_features_1,trainY.astype(\"int\"), tfidf_test_featurs_1,testY.astype(\"int\"),'sgd_tfidf_opt_model','tfidf')\nmodel_output.append(model_result)\nsgd_tfidf_opt_train_pred.to_csv('\/kaggle\/working\/sgd_tfidf_opt_train_pred.csv')\nsgd_tfidf_opt_test_pred.to_csv('\/kaggle\/working\/sgd_tfidf_opt_test_pred.csv')\nsave_ml_model('sgd_tfidf_opt',trained_model)","5415fd73":"svc_model = SVC()\nmodel_result,svm_tfidf_train_pred,svm_tfidf_test_pred,trained_model = model_score(svc_model,tfidf_train_features_1,trainY.astype(\"int\"), tfidf_test_featurs_1,testY.astype(\"int\"),'svc_tfidf_model','tfidf')\nmodel_output.append(model_result)\nsvm_tfidf_train_pred.to_csv('\/kaggle\/working\/svm_tfidf_train_pred.csv')\nsvm_tfidf_test_pred.to_csv('\/kaggle\/working\/svm_tfidf_test_pred.csv')\nsave_ml_model('svm_tfidf',trained_model)\n\n\n#hyper_param tunning with ngram = 1\n\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n                   ]\n\nmodel = SVC()\nsvm_opt_tfidf_clf = GridSearchCV(model, param_grid=tuned_parameters , cv=10, scoring='f1', verbose=0, n_jobs=-1)\nsvm_opt_tfidf_clf.fit(tfidf_train_features_1,trainY.astype(\"int\"))\nsvm_opt_tfidf_clf_best_model = svm_opt_tfidf_clf.best_estimator_\nmodel_result,svm_tfidf_opt_train_pred,svm_tfidf_opt_test_pred,trained_model = model_score(svm_opt_tfidf_clf_best_model,tfidf_train_features_1,trainY.astype(\"int\"), tfidf_test_featurs_1,testY.astype(\"int\"),'svm_tfidf_opt_model','tfidf')\nmodel_output.append(model_result)\nsvm_tfidf_opt_train_pred.to_csv('\/kaggle\/working\/svm_tfidf_opt_train_pred.csv')\nsvm_tfidf_opt_test_pred.to_csv('\/kaggle\/working\/svm_tfidf_opt_test_pred.csv')\nsave_ml_model('svm_tfidf_opt',trained_model)","8fde958c":"summary = model_summary(model_output,columns)\nsummary.to_csv('\/kaggle\/working\/model_summary.csv')\nsummary","54423eee":"training_data_X = pd.read_csv('\/kaggle\/input\/modelling-dataset\/trainX.csv') \ntraining_data_Y = pd.read_csv('\/kaggle\/input\/modelling-dataset\/trainY.csv') \ntraining_data = pd.concat([training_data_X[['Unnamed: 0','text']], training_data_Y['target']],axis  = 1)\ntraining_data.columns = [\"Index\" ,\"text\",\"Target\"]\n\ntest_data_X     = pd.read_csv('\/kaggle\/input\/modelling-dataset\/testX.csv')\ntest_data_y     = pd.read_csv('\/kaggle\/input\/modelling-dataset\/testY.csv')\ntest_data = pd.concat([test_data_X[['Unnamed: 0','text']], test_data_y['target']],axis  = 1)\ntest_data.columns = [\"Index\" ,\"text\",\"Target\"]\norg_data      = pd.concat([training_data,test_data],axis =0)\n\n#nb model\nnb_bow_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_bow_train_pred.csv')\nnb_bow_train.index = training_data.Index\nnb_bow_test =  pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_bow_test_pred.csv')\nnb_bow_test.index = test_data.Index\nnb_bow_merge = pd.DataFrame(pd.concat([nb_bow_train['0'],nb_bow_test['0']],axis  =0))\nnb_bow_merge.columns = ['nb_bow']\n\nnb_tfidf_ngm1_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_tfidf_ngm1_train_pred.csv')\nnb_tfidf_ngm1_train.index = training_data.Index\nnb_tfidf_ngm1_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_tfidf_ngm1_test_pred.csv')\nnb_tfidf_ngm1_test.index = test_data.Index\nnb_tfidf_ngm1_merge = pd.DataFrame(pd.concat([nb_tfidf_ngm1_train['0'],nb_tfidf_ngm1_test['0']],axis  =0))\nnb_tfidf_ngm1_merge.columns = ['nb_tfidf_ngm1']\n\nnb_tfidf_ngm2_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_tfidf_ngm2_train_pred.csv')\nnb_tfidf_ngm2_train.index = training_data.Index\nnb_tfidf_ngm2_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/nb_tfidf_ngm2_test_pred.csv')\nnb_tfidf_ngm2_test.index = test_data.Index\nnb_tfidf_ngm2_merge = pd.DataFrame(pd.concat([nb_tfidf_ngm2_train['0'],nb_tfidf_ngm2_test['0']],axis  =0))\nnb_tfidf_ngm2_merge.columns = ['nb_tfidf_ngm2']\n\n#sgd model\nsgd_bow_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_bow_train_pred.csv')\nsgd_bow_train.index = training_data.Index\nsgd_bow_test =  pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_bow_test_pred.csv')\nsgd_bow_test.index = test_data.Index\nsdg_bow_merge = pd.DataFrame(pd.concat([sgd_bow_train['0'],sgd_bow_test['0']],axis  =0))\nsdg_bow_merge.columns = ['sdg_bow']\n\nsgd_tfidf_ngm1_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_ngm1_train_pred.csv')\nsgd_tfidf_ngm1_train.index= training_data.Index\nsgd_tfidf_ngm1_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_ngm1_test_pred.csv')\nsgd_tfidf_ngm1_test.index = test_data.Index\nsgd_tfidf_ngm1_merge = pd.DataFrame(pd.concat([sgd_tfidf_ngm1_train['0'],sgd_tfidf_ngm1_test['0']],axis  =0))\nsgd_tfidf_ngm1_merge.columns = ['sgd_tfidf_ngm1']\n\nsgd_tfidf_ngm2_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_ngm2_train_pred.csv')\nsgd_tfidf_ngm2_train.index= training_data.Index\nsgd_tfidf_ngm2_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_ngm2_test_pred.csv')\nsgd_tfidf_ngm2_test.index = test_data.Index\nsgd_tfidf_ngm2_merge = pd.DataFrame(pd.concat([sgd_tfidf_ngm2_train['0'],sgd_tfidf_ngm2_test['0']],axis  =0))\nsgd_tfidf_ngm2_merge.columns = ['sgd_tfidf_ngm2']\n\nsgd_tfidf_opt_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_opt_train_pred.csv')\nsgd_tfidf_opt_train.index = training_data.Index\nsgd_tfidf_opt_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/sgd_tfidf_opt_test_pred (1).csv')\nsgd_tfidf_opt_test.index = test_data.Index\nsgd_tfidf_opt_merge = pd.DataFrame(pd.concat([sgd_tfidf_opt_train['0'],sgd_tfidf_opt_test['0']],axis  =0))\nsgd_tfidf_opt_merge.columns = ['sgd_tfidf_opt']\n\n#svm model\nsvm_bow_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_bow_train_pred.csv')\nsvm_bow_train.index = training_data.Index\nsvm_bow_test =  pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_bow_test_pred.csv')\nsvm_bow_test.index= test_data.Index\nsvm_bow_merge = pd.DataFrame(pd.concat([svm_bow_train['0'],svm_bow_test['0']],axis  =0))\nsvm_bow_merge.columns = ['svm_bow']\n\nsvm_tfidf_ngm1_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_tfidf_train_pred.csv')\nsvm_tfidf_ngm1_train.index = training_data.Index\nsvm_tfidf_ngm1_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_tfidf_test_pred.csv')\nsvm_tfidf_ngm1_test.index= test_data.Index\nsvm_tfidf_ngm1_merge = pd.DataFrame(pd.concat([svm_tfidf_ngm1_train['0'],svm_tfidf_ngm1_test['0']],axis  =0))\nsvm_tfidf_ngm1_merge.columns = ['svm_tfidf_ngm1']\n\n\nsvm_tfidf_opt_train = pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_tfidf_opt_train_pred.csv')\nsvm_tfidf_opt_train.index = training_data.Index\nsvm_tfidf_opt_test = pd.read_csv('\/kaggle\/input\/modelling-dataset\/svm_tfidf_opt_test_pred.csv')\nsvm_tfidf_opt_test.index = test_data.Index\nsvm_tfidf_opt_merge = pd.DataFrame(pd.concat([svm_tfidf_opt_train['0'],svm_tfidf_opt_test['0']],axis  =0))\nsvm_tfidf_opt_merge.columns = ['svm_tfidf_opt']\n\n\n#create training dataset\nensemble_modelling_dataset = pd.concat([nb_bow_merge,nb_tfidf_ngm1_merge,nb_tfidf_ngm2_merge,sdg_bow_merge,sgd_tfidf_ngm1_merge,sgd_tfidf_ngm2_merge,sgd_tfidf_opt_merge,svm_bow_merge,svm_tfidf_ngm1_merge,svm_tfidf_opt_merge],axis = 1)\nensemble_modelling_dataset = pd.merge(ensemble_modelling_dataset, org_data[[\"Target\",\"Index\"]] , left_index = True , right_on ='Index',how ='inner')\nensemble_modelling_dataset.drop(\"Index\",axis = 1 , inplace = True)\nensemble_modelling_dataset.to_csv('\/kaggle\/working\/ensemble_modelling_dataset.csv')\nensemble_modelling_dataset.head()","eb9cc43e":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\ntrainX, testX, trainY, testY = modelling_data(ensemble_modelling_dataset[[ c for c in ensemble_modelling_dataset.columns if c != 'Target']], ensemble_modelling_dataset.Target , .40)\n","13d81820":"def model_fit(alg, trainX,trainY,testX, testY, usecv = True , cv_folds = 10 , stopping_rounds = 50):\n    if usecv == True:\n        xgb_param = alg.get_xgb_params()\n        xgb_train = xgb.DMatrix(trainX.values , label = trainY.values)\n        cvresult = xgb.cv(xgb_param,xgb_train, \n                          num_boost_round = alg.get_params()['n_estimators'],\n                          nfold = cv_folds,\n                         metrics = 'auc',\n                         early_stopping_rounds = stopping_rounds)\n        \n        alg.set_params(n_estimators = cvresult.shape[0])\n        #fit\n        alg.fit(trainX, trainY)\n        #predict\n        dtrain_pred = alg.predict(trainX)\n        dtest_pred = alg.predict(testX)\n        print(\"train>>>>>>\")        \n        model_accuracy(trainY,dtrain_pred)\n        print(\"test>>>>>>\")\n        model_accuracy(testY,dtest_pred)\n        \n        return cvresult\n \ndef model_accuracy(actuals , pred):\n    print(\"Accuracy:\", np.round(metrics.accuracy_score(actuals, pred),2))\n    print(\"Precision:\", np.round(metrics.precision_score(actuals,pred),2))\n    print(\"Recall:\",np.round(metrics.recall_score(actuals,pred),2))\n    print(\"F1 Score\",np.round(metrics.f1_score(actuals, pred),2))\n    print(\"Confusion_Matrix:\", metrics.confusion_matrix(actuals,pred))\n","8246290a":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\ncv_results = model_fit(xgb1,trainX, trainY, testX, testY)\nsave_ml_model('xgboost',xgb1)","2e24040d":"from sklearn.model_selection import GridSearchCV\n\nparam_test1 = {\n    'max_depth': range(2,7),\n    'min_child_weight' : range(2,6)\n}\ngridsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate = .1, n_esitmators = 140 ,max_depth = 12 ,min_child_weight =4,\n                                                    gamma =0,subsample = .8 , colsample_bytree = .8, objective ='binary:logistic',\n                                                    ntread =4,scale_pos_weight = 1,seed = 27), \n                          param_grid = param_test1,scoring ='f1',n_jobs =4,iid = False , cv = 10)\n\ngridsearch1.fit(trainX,trainY)\n\n","3a4d9a74":"gridsearch1.best_params_ , gridsearch1.best_index_, gridsearch1.best_score_","e52c2852":"xgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=2,\n min_child_weight=2,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\ncv_results = model_fit(xgb2,trainX, trainY, testX, testY)","682db602":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=2,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(trainX,trainY)","e12df625":"gsearch3.best_params_ , gsearch3.best_index_, gsearch3.best_score_","54d0e595":"xgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=2,\n min_child_weight=2,\n gamma=0.1,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\ncv_results = model_fit(xgb3,trainX, trainY, testX, testY)","d47910e5":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=2,\n min_child_weight=2, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='f1',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(trainX,trainY)","6b9ac2da":"gsearch4.best_params_ , gsearch4.best_index_, gsearch4.best_score_","1ef1c059":"xgb4 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=2,\n min_child_weight=2,\n gamma=0.1,\n subsample=0.8,\n colsample_bytree=0.6,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\ncv_results = model_fit(xgb4,trainX, trainY, testX, testY)","985c8cda":"param_test6 = {\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 0.05]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier(learning_rate = .1 ,n_estimators = 177 , max_depth = 2,\n                                                min_child_weight =2 , gamma = 0.1 , subsample = .8, colsample_bytree = .6,\n                                                objective = 'binary:logistic', nthread = 4 , scale_pos_weight =1 , seed =27),\n                        param_grid = param_test6, scoring = 'f1',n_jobs = 4 , iid = False , cv = 5\n                        )\ngsearch6.fit(trainX, trainY)","45cea93d":"gsearch6.best_params_ , gsearch6.best_index_, gsearch6.best_score_","85978787":"xgb5 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=5000,\n max_depth=2,\n min_child_weight=2,\n gamma=0.1,\n subsample=0.8,\n colsample_bytree=0.6,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n reg_alpha = 1,\n seed=27)\ncv_results = model_fit(xgb5,trainX, trainY, testX, testY)","1630ee6d":"import pickle\n\n#load the embeddings \ntfidf_vectorizer = pickle.load(open(\"\/kaggle\/input\/trained-model\/tfidf_vectorizer\",\"rb\"))\ntfidf_vectorizer_ngram2 = pickle.load(open(\"\/kaggle\/input\/trained-model\/tfidf_vectorizer_ngram2\",\"rb\"))\nbow_vectorizer = pickle.load(open(\"\/kaggle\/input\/trained-model\/bow_vectorizer\",\"rb\"))\n\ntest_tfidf_vectorizer =  tfidf_vectorizer.transform(test.text)\ntest_tfidf_vectorizer_ngram2 =  tfidf_vectorizer_ngram2.transform(test.text)\ntest_bow_vectorizer =  bow_vectorizer.transform(test.text)","5cc4b830":"#load models\nsvm_bow_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/svm_bow\" ,\"rb\"))\nnb_tfidf_ngm1_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/nb_tfidf_ngm1\" ,\"rb\"))\nsgd_tfidf_ngm2_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/sgd_tfidf_ngm2\" ,\"rb\"))\nsvm_tfidf_opt_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/svm_tfidf_opt\" ,\"rb\"))\nnb_bow_model_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/nb_bow_model\" ,\"rb\"))\nsgd_tfidf_ngm1_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/sgd_tfidf_ngm1\" ,\"rb\"))\nsgd_tfidf_opt_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/sgd_tfidf_opt\" ,\"rb\"))\nsvm_tfidf_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/svm_tfidf\" ,\"rb\"))\nxgboost_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/xgboost\" ,\"rb\"))\nnb_tfidf_ngm2_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/nb_tfidf_ngm2\" ,\"rb\"))\nsgd_bow_model = pickle.load(open (\"\/kaggle\/input\/trained-model\/sgd_bow\" ,\"rb\"))","f714559a":"#predictions\n#level1 prediction\ntest_model_leve1_score = pd.DataFrame()\ntest_model_leve1_score[\"nb_bow\"]  = nb_bow_model_model.predict(test_bow_vectorizer)\ntest_model_leve1_score[\"nb_tfidf_ngm1\"]  = nb_tfidf_ngm1_model.predict(test_tfidf_vectorizer)\ntest_model_leve1_score[\"nb_tfidf_ngm2\"]  = nb_tfidf_ngm2_model.predict(test_tfidf_vectorizer_ngram2)\ntest_model_leve1_score[\"sdg_bow\"]  = sgd_bow_model.predict(test_bow_vectorizer)\ntest_model_leve1_score[\"sgd_tfidf_ngm1\"]  = sgd_tfidf_ngm1_model.predict(test_tfidf_vectorizer)\ntest_model_leve1_score[\"sgd_tfidf_ngm2\"]  = sgd_tfidf_ngm2_model.predict(test_tfidf_vectorizer_ngram2)\ntest_model_leve1_score[\"sgd_tfidf_opt\"]  = sgd_tfidf_opt_model.predict(test_tfidf_vectorizer_ngram2)\ntest_model_leve1_score[\"svm_bow\"]  = svm_bow_model.predict(test_bow_vectorizer)\ntest_model_leve1_score[\"svm_tfidf_ngm1\"]  = svm_tfidf_model.predict(test_tfidf_vectorizer)\ntest_model_leve1_score[\"svm_tfidf_opt\"]  = svm_tfidf_opt_model.predict(test_tfidf_vectorizer_ngram2)\n","332d65f6":"final_prediction = xgboost_model.predict(test_model_leve1_score)\ntest[\"target\"] = final_prediction\ntest[[\"id\",\"target\"]].to_csv(\"\/kaggle\/working\/test.csv\")","04628825":"# 2.TFIDF MODEL","b6d4305a":"# # 3. SVM","c497d514":"# Preparing the data for models","1b5963f5":"# 1. Bag Of Model","bfbedbb5":"# Tune gamma","4268f561":"## Scoring","8cacdc92":"**Word Cloud**","de321153":"# # 1. NB Model","4e32779d":"# Merge the output of each model for ensembling","a24792d6":"# Modelling","a1af025f":"# Data Cleaning\n\n# 1. Remove all the hashtags as hashtags do not affect sentiments.\n# 2. Remove mentions as they also do not weigh in sentiment analyzing.\n# 3. Replace any emojis with the text they represent as emojis or emoticons plays an important role in representing a sentiment.4.\n# 4. Replace contractions with their full forms.\n# 5. Remove any URLs present in tweets as they are not significant in sentiment analysis.\n# 6. Remove punctuations.\n# 7. Fix misspelled words (very basic as this is a very time-consuming step).\n# 8. Convert everything to lowercase.\n# 9. Remove HTML tags if present.","0fe9c165":"# # 3. SVM","ef8c3df1":"# Data Analysis\n\n# 1. Data Quality\n# 2. Distribution of the target class\n# 3. Distribution of each variables with respect to target class\n# 4. Word Cloud of the text variable\n# 5. Word Frequency Analysis","53f9e145":"# # 2. SGD Model","c5d5c997":"# # 2. SGD Model","ca2e45e2":"# # 1. NB Model","ed3bd2cd":"# XGBOOT Model","b0e296fa":"**Word Embeddings **","ba9d5ff0":"# Tuning Regularization Parameters","8a1a838a":"# Tune max_depth and min_child_weight","768bac7f":"# Tune subsample and colsample_bytree"}}