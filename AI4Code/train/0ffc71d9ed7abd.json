{"cell_type":{"3ca741c3":"code","e63e7240":"code","83869b99":"code","5cafe56d":"code","fbcda13b":"code","5742d4e4":"code","257c97c8":"code","51ec06d2":"code","32011923":"code","7394db4e":"code","fee1e300":"code","e4396854":"code","29f37fa7":"code","ca854ec0":"code","da6378b3":"code","d935465d":"code","782dd13a":"code","e6b53e42":"code","1015ff8a":"code","d29742bf":"code","57dd0efd":"markdown","e57845fe":"markdown","b9a360b2":"markdown","30153a15":"markdown","b505518f":"markdown","70865d0c":"markdown","9690d1e6":"markdown","417773b8":"markdown","e1fee088":"markdown","1146aa6c":"markdown"},"source":{"3ca741c3":"# import libraries\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.externals import joblib\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom numpy.random import seed\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import regularizers","e63e7240":"# set random seed\nseed(10)\ntf.random.set_seed(10)","83869b99":"# load, average and merge sensor samples\ndata_dir = \"..\/input\/Bearing_Sensor_Data\"\nmerged_data = pd.DataFrame()\n\nfor filename in os.listdir(data_dir):\n    dataset = pd.read_csv(os.path.join(data_dir, filename), sep='\\t')\n    dataset_mean_abs = np.array(dataset.abs().mean())\n    dataset_mean_abs = pd.DataFrame(dataset_mean_abs.reshape(1,4))\n    dataset_mean_abs.index = [filename]\n    merged_data = merged_data.append(dataset_mean_abs)\n    \nmerged_data.columns = ['Bearing 1', 'Bearing 2', 'Bearing 3', 'Bearing 4']","5cafe56d":"# transform data file index to datetime and sort in chronological order\nmerged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\nmerged_data = merged_data.sort_index()\nmerged_data.to_csv('Averaged_BearingTest_Dataset.csv')\nprint(\"Dataset shape:\", merged_data.shape)\nmerged_data.head()","fbcda13b":"train = merged_data['2004-02-12 10:52:39': '2004-02-15 12:52:39']\ntest = merged_data['2004-02-15 12:52:39':]\nprint(\"Training dataset shape:\", train.shape)\nprint(\"Test dataset shape:\", test.shape)","5742d4e4":"fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\nax.plot(train['Bearing 1'], label='Bearing 1', color='blue', animated = True, linewidth=1)\nax.plot(train['Bearing 2'], label='Bearing 2', color='red', animated = True, linewidth=1)\nax.plot(train['Bearing 3'], label='Bearing 3', color='green', animated = True, linewidth=1)\nax.plot(train['Bearing 4'], label='Bearing 4', color='black', animated = True, linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Training Data', fontsize=16)\nplt.show()","257c97c8":"# transforming data from the time domain to the frequency domain using fast Fourier transform\ntrain_fft = np.fft.fft(train)\ntest_fft = np.fft.fft(test)","51ec06d2":"# frequencies of the healthy sensor signal\nfig, ax = plt.subplots(figsize=(14, 6), dpi=80)\nax.plot(train_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\nax.plot(train_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\nax.plot(train_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\nax.plot(train_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Training Frequency Data', fontsize=16)\nplt.show()","32011923":"# frequencies of the degrading sensor signal\nfig, ax = plt.subplots(figsize=(14, 6), dpi=80)\nax.plot(test_fft[:,0].real, label='Bearing 1', color='blue', animated = True, linewidth=1)\nax.plot(test_fft[:,1].imag, label='Bearing 2', color='red', animated = True, linewidth=1)\nax.plot(test_fft[:,2].real, label='Bearing 3', color='green', animated = True, linewidth=1)\nax.plot(test_fft[:,3].real, label='Bearing 4', color='black', animated = True, linewidth=1)\nplt.legend(loc='lower left')\nax.set_title('Bearing Sensor Test Frequency Data', fontsize=16)\nplt.show()","7394db4e":"# normalize the data\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(train)\nX_test = scaler.transform(test)\nscaler_filename = \"scaler_data\"\njoblib.dump(scaler, scaler_filename)","fee1e300":"# reshape inputs for LSTM [samples, timesteps, features]\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\nprint(\"Training data shape:\", X_train.shape)\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\nprint(\"Test data shape:\", X_test.shape)","e4396854":"# define the autoencoder network model\ndef autoencoder_model(X):\n    inputs = Input(shape=(X.shape[1], X.shape[2]))\n    L1 = LSTM(16, activation='relu', return_sequences=True, \n              kernel_regularizer=regularizers.l2(0.00))(inputs)\n    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\n    L3 = RepeatVector(X.shape[1])(L2)\n    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n    output = TimeDistributed(Dense(X.shape[2]))(L5)    \n    model = Model(inputs=inputs, outputs=output)\n    return model","29f37fa7":"# create the autoencoder model\nmodel = autoencoder_model(X_train)\nmodel.compile(optimizer='adam', loss='mae')\nmodel.summary()","ca854ec0":"# fit the model to the data\nnb_epochs = 100\nbatch_size = 10\nhistory = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n                    validation_split=0.05).history","da6378b3":"# plot the training losses\nfig, ax = plt.subplots(figsize=(14, 6), dpi=80)\nax.plot(history['loss'], 'b', label='Train', linewidth=2)\nax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\nax.set_title('Model loss', fontsize=16)\nax.set_ylabel('Loss (mae)')\nax.set_xlabel('Epoch')\nax.legend(loc='upper right')\nplt.show()","d935465d":"# plot the loss distribution of the training set\nX_pred = model.predict(X_train)\nX_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\nX_pred = pd.DataFrame(X_pred, columns=train.columns)\nX_pred.index = train.index\n\nscored = pd.DataFrame(index=train.index)\nXtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\nscored['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)\nplt.figure(figsize=(16,9), dpi=80)\nplt.title('Loss Distribution', fontsize=16)\nsns.distplot(scored['Loss_mae'], bins = 20, kde= True, color = 'blue');\nplt.xlim([0.0,.5])","782dd13a":"# calculate the loss on the test set\nX_pred = model.predict(X_test)\nX_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\nX_pred = pd.DataFrame(X_pred, columns=test.columns)\nX_pred.index = test.index\n\nscored = pd.DataFrame(index=test.index)\nXtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\nscored['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\nscored['Threshold'] = 0.275\nscored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\nscored.head()","e6b53e42":"# calculate the same metrics for the training set \n# and merge all data in a single dataframe for plotting\nX_pred_train = model.predict(X_train)\nX_pred_train = X_pred_train.reshape(X_pred_train.shape[0], X_pred_train.shape[2])\nX_pred_train = pd.DataFrame(X_pred_train, columns=train.columns)\nX_pred_train.index = train.index\n\nscored_train = pd.DataFrame(index=train.index)\nscored_train['Loss_mae'] = np.mean(np.abs(X_pred_train-Xtrain), axis = 1)\nscored_train['Threshold'] = 0.275\nscored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\nscored = pd.concat([scored_train, scored])","1015ff8a":"# plot bearing failure time plot\nscored.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])","d29742bf":"# save all model information, including weights, in h5 format\nmodel.save(\"bearing-sensor-anomaly-detection.h5\")\nprint(\"Model saved\")","57dd0efd":"In this workbook, we use an autoencoder neural network to identify vibrational anomalies from sensor readings in a set of bearings. The goal is to be able to predict future bearing failures before they happen. The vibrational sensor readings are from the NASA Acoustics and Vibration Database. Each data set consists of individual files that are 1-second vibration signal snapshots recorded at 10 minute intervals. Each file contains 20,480 sensor data points that were obtained by reading the bearing sensors at a sampling rate of 20 kHz.\n\nThis autoencoder neural network model is created using Long Short-Term Memory (LSTM) recurrent neural network (RNN) cells within the Keras \/ TensorFlow framework.","e57845fe":"### Blog: https:\/\/towardsdatascience.com\/lstm-autoencoder-for-anomaly-detection-e1f4f2ee7ccf\n### Github: https:\/\/github.com\/BLarzalere\/LSTM-Autoencoder-for-Anomaly-Detection","b9a360b2":"From the above loss distribution, let's try a threshold value of 0.275 for flagging an anomaly. We can then calculate the loss in the test set to check when the output crosses the anomaly threshold.","30153a15":"# Distribution of Loss Function\nBy plotting the distribution of the calculated loss in the training set, one can use this to identify a suitable threshold value for identifying an anomaly. In doing this, one can make sure that this threshold is set above the \u201cnoise level\u201d and that any flagged anomalies should be statistically significant above the background noise.","b505518f":"# Data loading and pre-processing\nAn assumption is that mechanical degradation in the bearings occurs gradually over time; therefore, we use one datapoint every 10 minutes in the analysis. Each 10 minute datapoint is aggregated by using the mean absolute value of the vibration recordings over the 20,480 datapoints in each file. We then merge together everything in a single dataframe.","70865d0c":"# Bearing Failure Anomaly Detection","9690d1e6":"Having calculated the loss distribution and the anomaly threshold, we can visualize the model output in the time leading up to the bearing failure.","417773b8":"Let\u2019s get a different perspective of the data by transforming the signal from the time domain to the frequency domain using a discrete Fourier transform.","e1fee088":"# Define train\/test data\nBefore setting up the models, we need to define train\/test data. To do this, we perform a simple split where we train on the first part of the dataset (which should represent normal operating conditions) and test on the remaining parts of the dataset leading up to the bearing failure.","1146aa6c":"This analysis approach is able to flag the upcoming bearing malfunction well in advance of the actual physical failure. It is important to define a suitable threshold value for flagging anomalies while avoiding too many false positives during normal operating conditions."}}