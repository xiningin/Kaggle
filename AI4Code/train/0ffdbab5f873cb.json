{"cell_type":{"736277b3":"code","f3437a29":"code","2181caad":"code","d8d4077c":"code","88d398db":"code","c7e5a9b2":"code","9d08c393":"code","86d06466":"code","7eb29780":"code","3a062c85":"code","b68d8a61":"code","c3df1bd9":"code","645fbc8d":"code","b7b6ca0c":"code","f3b35e29":"code","9d50f9af":"code","89b9a868":"code","6829d653":"code","0214b04c":"markdown","89e15f0e":"markdown","a43991dc":"markdown","c22cca8d":"markdown","2efc6f63":"markdown","831f220c":"markdown"},"source":{"736277b3":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\ndata = pd.read_csv('..\/input\/data.csv',index_col=0)\n\ndata['shot_id_number'] = data.index + 1  # shot id number has null value\n\nsample = pd.read_csv('..\/input\/sample_submission.csv',index_col=0)\n\ndata_train_test_combined =  data[data['is_goal'].notnull()]\ntest_file = data[data['is_goal'].isnull()] #.dropna(axis=0,subset=['shot_id_number'])","f3437a29":"data_train_test_combined.dropna(axis=0,subset=['is_goal'],inplace=True) #Not required\ny = data_train_test_combined.is_goal\nX = data_train_test_combined.drop(['is_goal'],axis =1 ) #.select_dtypes(exclude =['object'])","2181caad":"train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=0.25)\nX_test = test_file.drop(['is_goal'],axis =1)","d8d4077c":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(train_X.select_dtypes(exclude='object'))\nimputed_X_valid = my_imputer.transform(test_X.select_dtypes(exclude='object'))\nimputed_final_test = my_imputer.transform(X_test.select_dtypes(exclude='object'))","88d398db":"# pd.DataFrame(imputed_X_train,columns=train_X.select_dtypes(exclude ='object').columns) #imputer return numpy ndarray\n# pd.DataFrame(imputed_X_train)\n\n# pd.DataFrame(imputed_X_train,columns=train_X.select_dtypes(exclude ='object').columns)","c7e5a9b2":"# All categorical columns\nobject_cols = [col for col in train_X.columns if train_X[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(train_X[col]) == set(X_test[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols =  list (set(object_cols) - set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","9d08c393":"# Label Encoding \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n# Drop categorical columns that will not be encoded\nlabel_X_train = train_X.drop(bad_label_cols, axis=1)\nlabel_X_valid = test_X.drop(bad_label_cols, axis=1)\nlabel_final_X_test = X_test.drop(bad_label_cols,axis=1)\n\n# Apply label encoder \nlabel_encoder = LabelEncoder()\n\n\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(label_X_train[col].astype(str))\n    label_X_valid[col] = label_encoder.transform(label_X_valid[col].astype(str))\n    label_final_X_test[col] = label_encoder.transform(label_final_X_test[col].astype(str))","86d06466":"Numerical = pd.DataFrame(imputed_X_train,columns=train_X.select_dtypes(exclude ='object').columns)\nNumerical_V = pd.DataFrame(imputed_X_valid,columns=test_X.select_dtypes(exclude='object').columns)\nNumerical_T = pd.DataFrame(imputed_final_test,columns=X_test.select_dtypes(exclude='object').columns)\nNumerical_T.columns","7eb29780":"#pd.concat(label_X_train,pd.DataFrame(imputed_X_train,columns=train_X.select_dtypes(exclude ='object').columns))\nCategorical = label_X_train[good_label_cols]\nCategorical_V = label_X_valid[good_label_cols]\nCategorical_T = label_final_X_test[good_label_cols]\nCategorical_T.columns","3a062c85":"#model_train_x = pd.concat([label_X_train[good_label_cols],imputed_X_train],ignore_index=True,sort=False)\n# pd.concat([data1, data2], ignore_index=True, sort =False)\nModel_train_X = pd.concat([Numerical.reset_index() ,Categorical.reset_index() ],ignore_index=True,sort=False,axis=1) # Number of rows were increasing\nModel_valid_X = pd.concat([Numerical_V.reset_index(),Categorical_V.reset_index()],ignore_index=True,sort=False,axis=1) \nModel_test_X = pd.concat([Numerical_T.reset_index(),Categorical_T.reset_index()],ignore_index=True,sort=False,axis=1)","b68d8a61":"#Model_train_X.shape\n#feature engineering\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nsel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\nsel.fit(Model_train_X, train_y)\nsel.get_support()\nselected_feat= Model_train_X.columns[(sel.get_support())]\nlen(selected_feat)\nprint(selected_feat)\n\n\n","c3df1bd9":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp\n\nimport pandas as pd\n# X_train = pd.DataFrame(X)\n# y_train = pd.DataFrame(y)\n\nfrom xgboost              import XGBClassifier\nfrom sklearn.ensemble     import ExtraTreesClassifier\nfrom sklearn.tree         import ExtraTreeClassifier\nfrom sklearn.tree         import DecisionTreeClassifier\nfrom sklearn.ensemble     import GradientBoostingClassifier\nfrom sklearn.ensemble     import BaggingClassifier\nfrom sklearn.ensemble     import AdaBoostClassifier\nfrom sklearn.ensemble     import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm             import LGBMClassifier\n\n\nclfs = [XGBClassifier(),              LGBMClassifier(), \n        ExtraTreesClassifier(),       ExtraTreeClassifier(),\n        BaggingClassifier(),          DecisionTreeClassifier(),\n        GradientBoostingClassifier(), LogisticRegression(),\n        AdaBoostClassifier(),         RandomForestClassifier()]\n\nfor clf in clfs:\n    try:\n        _ = plot_feature_importances(clf, Model_train_X, train_y, top_n=Model_train_X.shape[1], title=clf.__class__.__name__)\n    except AttributeError as e:\n        print(e)\n\n","645fbc8d":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(Model_train_X,train_y)\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(my_model.predict(Model_valid_X),test_y)","b7b6ca0c":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(label_X_train[good_label_cols],train_y)\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(my_model.predict(label_X_valid[good_label_cols]),test_y)\n#ans = my_model.predict(label_final_X_test[good_label_cols])\n# ans[:5]","f3b35e29":"test_preds = my_model.predict(label_final_X_test[good_label_cols])\noutput = pd.DataFrame({'shot_id_number': X_test['shot_id_number'],\n                       'is_goal': test_preds})\noutput.loc[output['is_goal'] < 0,'is_goal'] = 0\noutput.to_csv('submission.csv',index=False)","9d50f9af":"ans = pd.read_csv('submission.csv')","89b9a868":"\n\nans[ans['is_goal'] < 0] \n","6829d653":"ans.describe()","0214b04c":"This was the first model I had built but the performance was too low so tried on the top with label encoders but again the performance had been just satisfactory uptill now. ","89e15f0e":"Understanding uptill now :\n\nWe have few rows with missing is_goal data which is our test data.\nWe have to predict the probaability for the missing rows is_goal column with our machine learnign model\n\nThe data we will use for train is the rows which are having any particular value of is_goal present.\n\nNow once we have segregated the train data and test data.\n\n\nWe will divide our data into X, y i.e input and output set, \n-->So we will put the output Y column as is_goal and for \n-->Other columns will be our input X.\n\nNow as the data we have in X constitutes of the columns with different data types , so we would require to do some exploratory data analysis before we could make use of the same in our model.\n\n\nThe first thing is that : the columns pertaining to numerical value have a lot of Null(NAN) values present in them, we tend to use filler functions to treat them first like FillNa() to fill the null values with the mean average or some by default value or we can simply drop them altogether but that might lead to loss of data.\n\nNote : We can also use SimpleImputer to transform our data .\n\nquestion for myself?\nWe are doing above steps in the hope of having better performance on our system i,e lesser mean absolute error, mean squared error or whichever evaluation metric we are using .\nor is it a necessity to implicate our Data into Ml Model that is it won't be working with NAN values or  throwing.\n\nokay so once we have treated the null values with our imputer\/fillna for numerical or i.e non object data type then we can work over our categorical features .\n\n\nAs the categorical variable data also possesss significant amout of business information it would very much apt to bring our categorical into numerical so that they can inculcated to be included into our model.\n\nWe have tried using label encoder for our data which will encode each of the term with a numerical value and i have seen mean absolute error for my model hasn't reduced much after using it.\n\n\nFor using label encoder I have filtered categorical variables and the variables with the data set similar in train , validation and test are good to be taken for label encoding.\n\n","a43991dc":"If someone has any decent suggestions over here which can lead to significant improvement in the model performance are also welcome","c22cca8d":"I had tried impelementing the model alone on the categorical variable where performance is also more or less the same.","2efc6f63":"Not used the feature engineering but for the next 2 section of code I have plotted feature engineering.","831f220c":"This dataset was part of competetion problem I had faced ,This was my first model I have built from scratch so please mind the errors. \n\n"}}