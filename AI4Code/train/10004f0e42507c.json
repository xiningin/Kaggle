{"cell_type":{"9ff9f726":"code","3a304573":"code","3023d4b7":"code","2232faef":"code","ead9f48d":"code","c4155cc0":"code","f6d3e0a7":"code","4d93b101":"code","96d01e22":"code","1ff8b955":"code","c5e23074":"code","0a1f070e":"code","a85bbe09":"code","019db554":"code","e87816c0":"code","e68f0d8b":"code","a8ebdc6d":"code","85cc55b7":"code","3eafebda":"code","4692aec5":"code","eb909315":"code","0d1b9684":"code","83ab24b0":"code","ec7bd773":"code","5d8dba62":"code","164c53ce":"code","bf8c02fb":"code","afb6c7af":"code","e07d460f":"code","73e5cb40":"code","fab8bb57":"code","f92a6e1b":"code","0273da04":"code","5594fb1b":"code","1771923c":"code","17c2b0ec":"code","dac3fac4":"code","15f67db7":"code","04e22244":"code","0699cb56":"code","e4b2cc90":"code","6ee0ae38":"markdown","8ac26663":"markdown","6873ce3d":"markdown","b6e7995a":"markdown","06b02da2":"markdown","326194ad":"markdown","84c95b2a":"markdown","9704e53c":"markdown","5747a44f":"markdown","cf9f7bd3":"markdown","23d2d4e3":"markdown","1d731a57":"markdown","86698030":"markdown","80f4bd77":"markdown","1f74571c":"markdown","16fb5827":"markdown","278b46d6":"markdown","99c84937":"markdown","cb21e6a2":"markdown","8b7ee615":"markdown","25c5a3a5":"markdown"},"source":{"9ff9f726":"# Loading packages\nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport time\nfrom datetime import datetime, timedelta,date\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import RobustScaler","3a304573":"def check_train_test_diff(train, test, col):\n    ratio_df = pd.concat([train[col].value_counts()\/train.shape[0], test[col].value_counts()\/test.shape[0]], axis=1)\n    ratio_df.columns = ['train','test']\n    ratio_df['diff'] = ratio_df['train'] - ratio_df['test']\n    return ratio_df","3023d4b7":"def category_feature_distribution(train, col, target='price'):\n    fig, ax = plt.subplots(1, 2, figsize=(16,4))\n    \n    for c in sorted(train[col].unique()):\n        sns.distplot(np.log1p(train.loc[train[col]==c, target]), ax=ax[0])\n    ax[0].legend(sorted(train[col].unique()))\n    ax[0].set_title(f'{col} {target} distribution')\n\n    sns.boxplot(x=col, y=target, data=df_train, ax=ax[1])\n    ax[1].set_title(f'{col} vs {target}')\n    \n    plt.show()","2232faef":"def continous_feature_distribution(train, test, col, target='price'):\n    fig, ax = plt.subplots(1, 2, figsize=(12,5))\n    \n    sns.distplot(train[col], ax=ax[0])\n    sns.distplot(test[col], ax=ax[0])\n    ax[0].set_title(f'{col} - train\/test distribution')\n    \n    sns.scatterplot(x=col, y=target, data=train, ax=ax[1])\n    ax[1].set_title(f'{col} - {target} scatterplot')\n    \n    plt.show()","ead9f48d":"def scatter_quantile_graph(frame, col1, col2):\n    col1_quantile = np.arange(0,1.1,0.1)\n    col2_quantile = np.arange(0,1.1,0.1)\n\n    for quantile_value in frame[col1].quantile(col1_quantile):\n        plt.axvline(quantile_value, color='red', alpha=0.3)\n    for quantile_value in frame[col2].quantile(col2_quantile):\n        plt.axhline(quantile_value, color='blue', alpha=0.3)\n        \n    sns.scatterplot(col1, col2, hue='price',data=frame)\n    \n    plt.title('{} - {}'.format(col1,col2))\n    plt.show()","c4155cc0":"def get_prefix(group_col, target_col, prefix=None):\n    if isinstance(group_col, list) is True:\n        g = '_'.join(group_col)\n    else:\n        g = group_col\n    if isinstance(target_col, list) is True:\n        t = '_'.join(target_col)\n    else:\n        t = target_col\n    if prefix is not None:\n        return prefix + '_' + g + '_' + t\n    return g + '_' + t\n    \ndef groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n    try:\n        prefix = get_prefix(group_col, target_col, prefix_param)\n        print(group_col, target_col, agg_method)\n        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n    except BaseException as e:\n        print(e)\n    return group_df.reset_index()","f6d3e0a7":"from functools import wraps\ndef time_decorator(func): \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n        start_time = time.time()\n        \n        df = func(*args, **kwargs)\n        \n        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n        print(\"TotalTime: \", time.time() - start_time)\n        return df\n        \n    return wrapper\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, params=None, **kwargs):\n        #if isinstance(SVR) is False:\n        #    params['random_state'] = kwargs.get('seed', 0)\n        self.clf = clf(**params)\n        self.is_classification_problem = True\n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        if len(np.unique(y_train)) > 30:\n            self.is_classification_problem = False\n            \n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        if self.is_classification_problem is True:\n            return self.clf.predict_proba(x)[:,1]\n        else:\n            return self.clf.predict(x)   \n    \nclass XgbWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        self.param['seed'] = kwargs.get('seed', 0)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n    \n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        need_cross_validation = True\n       \n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n            \n            print(train_round)\n            self.clf = xgb.train(self.param, dtrain, train_round)\n            del dtrain\n        else:\n            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n\n            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n                                 early_stopping_rounds=self.early_stopping,\n                                 verbose_eval=self.verbose_eval)\n            self.best_round = max(self.best_round, self.clf.best_iteration)\n\n    def predict(self, x):\n        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n\n    def get_params(self):\n        return self.param    \n    \nclass LgbmWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        self.param['seed'] = kwargs.get('seed', 0)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n        \n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        \"\"\"\n        x_cross or y_cross is None\n        -> model train limted num_rounds\n        \n        x_cross and y_cross is Not None\n        -> model train using validation set\n        \"\"\"\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n                \n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n            del dtrain   \n        else:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n                                  verbose_eval=self.verbose_eval)\n            self.best_round = max(self.best_round, self.clf.best_iteration)\n            del dtrain, dvalid\n            \n        gc.collect()\n    \n    def predict(self, x):\n        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n    \n    def plot_importance(self):\n        lgb.plot_importance(self.clf, max_num_features=50, height=0.7, figsize=(10,30))\n        plt.show()\n        \n    def get_params(self):\n        return self.param\n    \n    \n@time_decorator\ndef get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n    nfolds = kwargs.get('NFOLDS', 5)\n    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n    kfold_random_state = kwargs.get('kfold_random_state', 0)\n    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n    ntrain = x_train.shape[0]\n    ntest = x_test.shape[0]\n    \n    kf_split = None\n    if stratified_kfold_ytrain is None:\n        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train)\n    else:\n        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n        \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n\n    cv_sum = 0\n    \n    # before running model, print model param\n    # lightgbm model and xgboost model use get_params()\n    try:\n        if clf.clf is not None:\n            print(clf.clf)\n    except:\n        print(clf)\n        print(clf.get_params())\n\n    for i, (train_index, cross_index) in enumerate(kf_split):\n        x_tr, x_cr = None, None\n        y_tr, y_cr = None, None\n        if isinstance(x_train, pd.DataFrame):\n            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n        else:\n            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n\n        clf.train(x_tr, y_tr, x_cr, y_cr)\n        \n        oof_train[cross_index] = clf.predict(x_cr)\n\n        cv_score = eval_func(y_cr, oof_train[cross_index])\n        \n        print('Fold %d \/ ' % (i+1), 'CV-Score: %.6f' % cv_score)\n        cv_sum = cv_sum + cv_score\n        \n        del x_tr, x_cr, y_tr, y_cr\n        \n    gc.collect()\n    \n    score = cv_sum \/ nfolds\n    print(\"Average CV-Score: \", score)\n\n    # Using All Dataset, retrain\n    clf.train(x_train, y_train)\n    oof_test = clf.predict(x_test)\n\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score","4d93b101":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')","96d01e22":"print(\"train.csv. Shape: \",df_train.shape)\nprint(\"test.csv. Shape: \",df_test.shape)","1ff8b955":"df_train.head()","c5e23074":"default_check = pd.concat([df_train.isnull().sum(), df_train.dtypes, df_train.nunique(), df_train.describe().T], axis=1)\ndefault_check.rename(columns={0:'NULL', 1:'TYPE', 2:'UNIQUE'}, inplace=True)\ndefault_check","0a1f070e":"fig, ax = plt.subplots(1, 3, figsize=(20,4))\nsns.distplot(df_train['price'], ax=ax[0])\nsns.distplot(np.log1p(df_train['price']), ax=ax[1])\ndf_train['price'].plot(ax=ax[2])\nplt.show()","a85bbe09":"from IPython.display import display\nfor col in ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']:\n    print(col)\n    display(check_train_test_diff(df_train, df_test, col))","019db554":"for col in ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']:\n    category_feature_distribution(df_train, col)","e87816c0":"df_train['date'] = pd.to_datetime(df_train['date'])\nprint(df_train['date'].min(), df_train['date'].max())\n\ndf_test['date'] = pd.to_datetime(df_test['date'])\nprint(df_test['date'].min(), df_test['date'].max())","e68f0d8b":"print(\"Train\")\ndisplay(df_train.sort_values('date').head())\nprint(\"Test\")\ndisplay(df_test.sort_values('date').head())","a8ebdc6d":"plt.plot(df_train.sort_values('date')['price'].cumsum().values)","85cc55b7":"fig, ax = plt.subplots(1, 4, figsize=(20,5))\ndf_train.groupby('date')['price'].count().plot(ax=ax[0])\nax[0].set_title('Each date price count')\ndf_train.groupby('date')['price'].sum().plot(ax=ax[1])\nax[1].set_title('Each date price sum')\ndf_train.groupby('date')['price'].mean().plot(ax=ax[2])\nax[2].set_title('Each date price mean')\ndf_train.groupby('date')['price'].std().plot(ax=ax[3])\nax[3].set_title('Each date price std')\nplt.show()","3eafebda":"df_train.loc[df_train['date']==np.argmax(df_train.groupby('date')['price'].mean())]","4692aec5":"fig, ax = plt.subplots(1, 2, figsize=(10,5))\ndf_train.groupby('date')['id'].count().plot(ax=ax[0])\nax[0].set_title('Train each date sales count')\ndf_test.groupby('date')['id'].count().plot(ax=ax[1])\nax[1].set_title('Test each date sales count')\nplt.show()","eb909315":"df_train['yearmonth'] = df_train['date'].dt.year*100 + df_train['date'].dt.month\ncategory_feature_distribution(df_train,'yearmonth')\n","0d1b9684":"area_feature = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\nfor col in area_feature:\n    continous_feature_distribution(df_train, df_test, col)","83ab24b0":"sns.scatterplot('long','lat',hue='price',data=df_train)","ec7bd773":"scatter_quantile_graph(df_train, 'long', 'lat')","5d8dba62":"qcut_count = 10\ndf_train['qcut_long'] = pd.qcut(df_train['long'], qcut_count, labels=range(qcut_count))\ndf_train['qcut_lat'] = pd.qcut(df_train['lat'], qcut_count, labels=range(qcut_count))\ntemp = df_train.groupby(['qcut_long','qcut_lat'])['price'].mean().reset_index()\nsns.scatterplot('qcut_long','qcut_lat', hue='price', data=temp);\ndel df_train['qcut_long'], df_train['qcut_lat']","164c53ce":"df_train = df_train.loc[df_train['bedrooms']<10]","bf8c02fb":"skew_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement']\n\nfor c in skew_columns:\n    df_train[c] = np.log1p(df_train[c].values)\n    df_test[c] = np.log1p(df_test[c].values)","afb6c7af":"for df in [df_train,df_test]:\n    df['yr_renovated'] = df['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)\n    df['yr_renovated'] = df['yr_renovated'].fillna(df['yr_built'])","e07d460f":"def feature_processing(df):\n    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n    df['grade_condition'] = df['grade'] * df['condition']\n    df['sqft_total'] = df['sqft_living'] + df['sqft_lot']\n    df['sqft_total_size'] = df['sqft_living'] + df['sqft_lot'] + df['sqft_above'] + df['sqft_basement']\n   \n    df['sqft_total15'] = df['sqft_living15'] + df['sqft_lot15'] \n    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n    \n    df['roombybathroom'] = df['bedrooms'] \/ df['bathrooms']\n    df['sqft_total_by_lot'] = (df['sqft_living'] + df['sqft_above'] + df['sqft_basement'])\/df['sqft_lot']\n    \n    qcut_count = 10\n    df['qcut_long'] = pd.qcut(df['long'], qcut_count, labels=range(qcut_count))\n    df['qcut_lat'] = pd.qcut(df['lat'], qcut_count, labels=range(qcut_count))\n    df['qcut_long'] = df['qcut_long'].astype(int)\n    df['qcut_lat'] = df['qcut_lat'].astype(int)\n\n    df['date'] = pd.to_datetime(df['date'])\n    df['yearmonth'] = df['date'].dt.year*100 + df['date'].dt.month\n    df['date'] = df['date'].astype('int')\n    return df","73e5cb40":"all_df = pd.concat([df_train, df_test])\nall_df = feature_processing(all_df)","fab8bb57":"df_test = all_df.loc[all_df['price'].isnull()]\ndf_train = all_df.loc[all_df['price'].notnull()]","f92a6e1b":"group_df = groupby_helper(df_train, 'grade', 'price', ['mean'])\ndf_train = df_train.merge(group_df, on='grade', how='left')\ndf_test = df_test.merge(group_df, on='grade', how='left')\n\ngroup_df = groupby_helper(df_train, 'bedrooms', 'price', ['mean'])\ndf_train = df_train.merge(group_df, on='bedrooms', how='left')\ndf_test = df_test.merge(group_df, on='bedrooms', how='left')\n\ngroup_df = groupby_helper(df_train, 'bathrooms', 'price', ['mean'])\ndf_train = df_train.merge(group_df, on='bathrooms', how='left')\ndf_test = df_test.merge(group_df, on='bathrooms', how='left')","0273da04":"train_columns = [col for col in df_train.columns if col not in ['id','price']]\n\nx_train = df_train.copy()\ny_train = np.log1p(df_train['price'])\ndel x_train['price']\nx_train.loc[np.isinf(x_train['roombybathroom']),'roombybathroom'] = -1\n\n\nx_test = df_test.copy()\nx_test.loc[np.isinf(x_test['roombybathroom']),'roombybathroom'] = -1","5594fb1b":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))","1771923c":"lgb_param = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.015,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4950}\n\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 4950,\n    'eta': 0.0123,\n    'gamma':0,\n    'max_depth':3,\n    'reg_alpha':0.00006,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n}\n\nridge_params = {'alpha':0.0001, 'normalize':True, 'max_iter':1e7, 'random_state':0}\nlasso_params = {'alpha':0.0005, 'normalize':True, 'max_iter':1e7, 'random_state':0}\nelastic_params = {'alpha':0.001, 'normalize':True, 'max_iter':1e3, 'random_state':0, 'l1_ratio':0.8}\nsvr_param = {'C':20, 'epsilon':0.008, 'gamma':0.0003}\ngbr_param = {'n_estimators':3000, 'learning_rate':0.05, 'max_depth':4, 'max_features':'sqrt', 'min_samples_leaf':15, 'min_samples_split':10, 'loss':'huber', 'random_state':0 }","17c2b0ec":"xgb_model = XgbWrapper(params=xgb_params, num_rounds = 10000, ealry_stopping=100,\n                                   verbose_eval=100)\n\nlgb_model = LgbmWrapper(params=lgb_param, num_rounds = 10000, ealry_stopping=100,\n                                   verbose_eval=100)\n\nridge_model = SklearnWrapper(Ridge, params=ridge_params)\nlasso_model = SklearnWrapper(Lasso, params=lasso_params)\nelastic_model = SklearnWrapper(ElasticNet, params=lasso_params)\nsvr_model = SklearnWrapper(SVR, params=svr_param)\ngbr_model = SklearnWrapper(GradientBoostingRegressor, params=gbr_param)","dac3fac4":"x_train_rb = x_train.copy()\nx_test_rb = x_test.copy()\nrb = RobustScaler()\nx_train_rb[train_columns] = rb.fit_transform(x_train_rb[train_columns].fillna(-1))\nx_test_rb[train_columns] = rb.transform(x_test_rb[train_columns].fillna(-1))","15f67db7":"ridge_train, ridge_test, ridge_cv_score = get_oof(ridge_model, x_train_rb[train_columns], y_train, x_test_rb[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\nlasso_train, lasso_test, lasso_cv_score = get_oof(lasso_model, x_train_rb[train_columns], y_train, x_test_rb[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\nelastic_train, elastic_test, lasso_cv_score = get_oof(elastic_model, x_train_rb[train_columns], y_train, x_test_rb[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\nsvr_train, svr_test, lasso_cv_score = get_oof(svr_model, x_train_rb[train_columns], y_train, x_test_rb[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\ngbr_train, gbr_test, lasso_cv_score = get_oof(gbr_model, x_train[train_columns].fillna(-1), y_train, x_test[train_columns].fillna(-1), \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\nxgb_train, xgb_test, xgb_cv_score = get_oof(xgb_model, x_train[train_columns], y_train, x_test[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)\n\nlgb_train, lgb_test, lgb_cv_score = get_oof(lgb_model, x_train[train_columns], y_train, x_test[train_columns], \n                            rmse, NFOLDS=5, kfold_random_state=4950)","04e22244":"x_train_second_layer = np.concatenate((lgb_train, xgb_train, lasso_train, \n                                       ridge_train, elastic_train, svr_train, \n                                       gbr_train), axis=1)\n\nx_test_second_layer = np.concatenate((lgb_test, xgb_test, lasso_test, \n                                      ridge_test, elastic_test, svr_test, \n                                      gbr_test), axis=1)\n\nx_train = pd.concat([df_train['id'], pd.DataFrame(x_train_second_layer)], axis=1)\nx_test = pd.concat([df_test['id'], pd.DataFrame(x_test_second_layer)], axis=1)\n\nx_train.to_csv('train_oof.csv', index=False)\nx_test.to_csv('test_oof.csv', index=False)\ndel x_train['id']\ndel x_test['id']","0699cb56":"lgb_meta_param = {'num_leaves': 15,\n         'objective':'regression',\n         'max_depth': 5,\n         'learning_rate': 0.015,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4950}\n         \n#prepare fit model with cross-validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(len(x_train))\npredictions = np.zeros(len(x_test))\nfeature_importance_df = pd.DataFrame()\n\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train)):\n    trn_data = lgb.Dataset(x_train.iloc[trn_idx], label=y_train.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(x_train.iloc[val_idx], label=y_train.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(lgb_meta_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(x_train.iloc[val_idx], num_iteration=clf.best_iteration)\n   \n    #predictions\n    predictions += clf.predict(x_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n    \ncv = np.sqrt(mean_squared_error(oof, y_train))\nprint(cv)","e4b2cc90":"submission = pd.DataFrame({'id': df_test['id'], 'price': np.expm1(predictions)})\nsubmission.to_csv('submission.csv', index=False)","6ee0ae38":"train, test dataset \uc2dc\uac04\uc774 \ube44\uc2b7\ud568","8ac26663":"### \ud3c9\uac00 Metric\uc740 RMSE\n$$ \\sqrt{\\frac{1}{N}\\sum(y_t - y_pr)^2} $$\n\n### Data fields\n* ID : \uc9d1\uc744 \uad6c\ubd84\ud558\ub294 \ubc88\ud638\n* date : \uc9d1\uc744 \uad6c\ub9e4\ud55c \ub0a0\uc9dc\n* **<font color='blue'>price : \uc9d1\uc758 \uac00\uaca9(Target variable)<\/font>**\n* bedrooms : \uce68\uc2e4\uc758 \uc218\n* bathrooms : \uce68\uc2e4 \uac1c\uc218 \ub2f9 \ud654\uc7a5\uc2e4\uc758 \uc218(\ud654\uc7a5\uc2e4\uc758 \uc218 \/ \uce68\uc2e4\uc758 \uc218 )\n* sqft_living : \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n* sqft_lot : \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n* floors : \uc9d1\uc758 \uce35 \uc218\n* waterfront : \uc9d1\uc758 \uc804\ubc29\uc5d0 \uac15\uc774 \ud750\ub974\ub294\uc9c0 \uc720\ubb34 (a.k.a. \ub9ac\ubc84\ubdf0)\n* view : \uc9d1\uc774 \uc5bc\ub9c8\ub098 \uc88b\uc544 \ubcf4\uc774\ub294\uc9c0\uc758 \uc815\ub3c4\n* condition : \uc9d1\uc758 \uc804\ubc18\uc801\uc778 \uc0c1\ud0dc\n* grade : King County grading \uc2dc\uc2a4\ud15c \uae30\uc900\uc73c\ub85c \ub9e4\uae34 \uc9d1\uc758 \ub4f1\uae09\n* sqft_above : \uc9c0\ud558\uc2e4\uc744 \uc81c\uc678\ud55c \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n* sqft_basement : \uc9c0\ud558\uc2e4\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n* yr_built : \uc9c0\uc5b4\uc9c4 \ub144\ub3c4\n* yr_renovated : \uc9d1\uc744 \uc7ac\uac74\ucd95\ud55c \ub144\ub3c4\n* zipcode : \uc6b0\ud3b8\ubc88\ud638\n* lat : \uc704\ub3c4\n* long : \uacbd\ub3c4\n* sqft_living15 : 2015\ub144 \uae30\uc900 \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74, \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c)\n* sqft_lot15 : 2015\ub144 \uae30\uc900 \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74, \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c)","6873ce3d":"### \uc9d1\uc758 \ud2b9\uc131\ub4e4\n['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","b6e7995a":"#### 2.2 \uc815\uaddc\ud654\n\ucf54\ub4dc\ub294 \ub2e4\uc74c\uc758 \ub9c1\ud06c\ub97c \ud65c\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4. https:\/\/www.kaggle.com\/kcs93023\/2019-ml-month-2nd-baseline","06b02da2":"### Data load","326194ad":"### Utility Function","84c95b2a":"Stacking\uc744 \ucc98\uc74c\ubd80\ud130 \ud558\ub294 \uac83\uc740 \ucd94\ucc9c\ub4dc\ub9ac\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ucc98\uc74c\uc5d0\ub294 \ub370\uc774\ud130\uc758 \ud2b9\uc131\ubd80\ud130 \ud30c\uc545\ud569\ub2c8\ub2e4.","9704e53c":"\uc544\ub798 Kernel\uc758 \uc804\ucc98\ub9ac\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uc600\uc2b5\ub2c8\ub2e4.<br>\nhttps:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda","5747a44f":"### \uba74\uc801\ub4e4\n","cf9f7bd3":"### \uc2dc\uac04","23d2d4e3":"\uc6d0\ub798 \ub370\uc774\ud130\ub294 train, test\uac00 \ub0a0\uc9dc\ub85c \uc815\ub82c\ub41c \ud558\ub098\uc758 \ub370\uc774\ud130?","1d731a57":"\uc774\uac8c \uc77c\uc9c1\uc120\uc774 \uc544\ub2c8\uc600\ub2e4\uba74 date Leak<br>\ndate\ub85c train, test\ub97c \uc815\ub82c\ud558\uace0 \uc0c8\ub85c\uc6b4 id\ub97c \ub9e4\uaca8 dateid feature \uc0ac\uc6a9 \uac00\ub2a5\ud588\uc744 \uac83\uc73c\ub85c \ucd94\uce21","86698030":"\uac11\uc790\uae30 \ud284 price mean","80f4bd77":"\ubaa8\ub4e0 \ubcc0\uc218\ub97c \uc0b4\ud3b4\ubd10\uc57c \ud569\ub2c8\ub2e4. \uc0b4\ud3b4\ubcf4\uba74\uc11c \ud2b9\uc9d5\uc744 \ucc3e\uc2b5\ub2c8\ub2e4. <br>\n\uaf2d \ud655\uc778\ud574\uc57c \ud560 \uac83\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n* Train \/ Test \uc758 \ubd84\ud3ec \ucc28\uc774 <br>\n-> Public LB, Private LB\uc758 \ucc28\uc774\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \uc694\uc778\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* Target \uac12\uacfc\uc758 \ubd84\ud3ec\n* \uc2dc\uac04\uc5d0 \ub530\ub978 Target \ubd84\ud3ec\uc758 \ubcc0\ud654","1f74571c":"https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda","16fb5827":"\uce90\uae00\uc744 \ud558\ub2e4\ubcf4\uba74 Discussion\uc5d0\uc11c **Stacking, oof ** \uac19\uc740 \uc6a9\uc5b4\uac00 \ub9ce\uc774 \ubcf4\uc785\ub2c8\ub2e4.<br>\n\uc774 \ucee4\ub110\uc740 Stacking\uc758 \uacfc\uc815\uacfc oof Feature\ub97c \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \ub300\ud558\uc5ec \uc124\uba85\ud569\ub2c8\ub2e4.<br>\n\nStacking \uad00\ub828 \uc77d\uc5b4\ubcf4\uba74 \uc88b\uc740 \uc790\ub8cc<br>\nhttps:\/\/mlwave.com\/kaggle-ensembling-guide\/ <br>\nhttp:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/ <br>\n\n\ud55c\uad6d\uc5b4 \uc790\ub8cc(\uac10\uc0ac\ud569\ub2c8\ub2e4!)<br>\nhttps:\/\/gentlej90.tistory.com\/73 <br>\nhttps:\/\/kweonwooj.tistory.com\/36\n\n","278b46d6":"\uc77c\ubcc4 \ud310\ub9e4 \ud69f\uc218\uac00 test\uac00 \uc801\uc9c0\ub9cc, \ubd84\ud3ec\ub294 \ube44\uc2b7\ud55c \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","99c84937":"Target \uac12\uc758 \ubd84\ud3ec\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. \uc6d0\ubcf8 \uac12\uc774 \uc5bc\ub9c8\ub098 \uce58\uc6b0\uccd0\uc838 \uc788\ub294\uc9c0, log\ubcc0\ud658\ud588\uc744 \ub54c \uc5b4\ub290\uc815\ub3c4\ub85c \uc815\uaddc\uc131\uc744 \ub744\ub294\uc9c0 \ub4f1\ub4f1<br>\n\ub610\ud55c \ud639\uc2dc Leak\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. ID Leak\uc774 \uc788\ub2e4\uba74 targer\uac12\uc774 \ud06c\uae30\uac00 \uc624\ub984\ucc28\uc21c\/\ub0b4\ub9bc\ucc28\uc21c\uc758 \uc131\ud5a5\uc774\ub098 \ud2b9\uc815 \ud328\ud134\uc744 \uac00\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br>","cb21e6a2":"### Feature Preprocessing","8b7ee615":"https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda","25c5a3a5":"### \uc704\uce58"}}