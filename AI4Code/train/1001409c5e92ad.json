{"cell_type":{"729f8a75":"code","17ecc66b":"code","4695fd11":"code","4de5856a":"code","f6845ac3":"code","9ab7bc1a":"code","5cbb89b0":"code","e5402282":"code","ab13bafa":"code","db143727":"code","44f4a8c7":"code","c95a7a88":"code","ce0988f0":"code","765651d9":"code","31eb7d21":"code","78705516":"code","31302320":"code","557bca69":"code","929b445b":"code","ec6aa082":"code","8f548848":"code","a8def00d":"code","c45dfd77":"markdown"},"source":{"729f8a75":"import xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport random\nimport optuna\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score","17ecc66b":"data = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","4695fd11":"data.head()","4de5856a":"n=len(data)\nN=[]\nfor i in range(n):\n    N+=[i]\nrandom.shuffle(N)","f6845ac3":"Name=data['Species'].unique()\nprint(Name)\nM=[0,1,2]\n\nnormal_mapping=dict(zip(Name,M)) \nreverse_mapping=dict(zip(M,Name)) ","9ab7bc1a":"train = data.loc[N[0:(n\/\/4)*3]]\ntest = data.loc[N[(n\/\/4)*3:]]","5cbb89b0":"target = train['Species'].map(normal_mapping)\ntesty = test['Species'].map(normal_mapping)\ndata = train.drop('Species',axis=1)\ntest = test.drop('Species',axis=1)","e5402282":"columns=['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']","ab13bafa":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # using the GPU\n        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018,0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24,48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n    }\n    model = xgb.XGBClassifier(**param)      \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","db143727":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","44f4a8c7":"study.trials_dataframe()","c95a7a88":"# shows the scores from all trials\noptuna.visualization.plot_optimization_history(study)","ce0988f0":"# interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","765651d9":"# shows the evolution of the search\noptuna.visualization.plot_slice(study)","31eb7d21":"# parameter interactions on an interactive chart.\noptuna.visualization.plot_contour(study, params=['lambda','learning_rate'])","78705516":"# Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","31302320":"# Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","557bca69":"Best_trial= {'lambda': 0.0011376182453814178, 'alpha': 0.00399467515099432, \n             'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.016, \n             'max_depth': 11, 'random_state': 48, 'min_child_weight': 3}","929b445b":"preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\n\nfor trn_idx, test_idx in kf.split(train[columns],target):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=target.iloc[trn_idx],target.iloc[test_idx]\n    model = xgb.XGBClassifier(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits\n    rmse=mean_squared_error(y_val, model.predict(X_val),squared=False)\n    print(rmse)","ec6aa082":"PRED=[]\nfor item in preds:   \n    PRED+=[item.astype(int)]\nprint(PRED[0:10])","8f548848":"ANS=list(testy)\nANS[0:10]","a8def00d":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","c45dfd77":"# XGBoost with Optuna tuning\n* doc: \nhttps:\/\/github.com\/optuna\/optuna\n* thanks to: \nhttps:\/\/www.kaggle.com\/miklgr500\/optuna-xgbclassifier-parameters-optimize\n"}}