{"cell_type":{"d169dfb6":"code","6b4ec396":"code","e283ae2e":"code","91f4c283":"code","35a8a126":"code","b3630774":"code","27a3a697":"code","e9ba85db":"code","61d20513":"code","769a5c08":"code","89494835":"code","2dd72d3e":"code","a5534c04":"code","45d5922a":"code","aa5312e1":"code","8aba9eeb":"code","950530f3":"code","eecc5527":"code","5b0d068d":"code","5edce8a6":"code","5f73e3ca":"code","f1525bf9":"code","25169118":"code","70e164dc":"code","66cc66cc":"code","68b645d8":"code","19c392bf":"code","7b88759c":"code","82994e83":"code","5f290385":"code","f18e6da5":"code","0eac54e8":"code","587a4194":"code","781a389f":"code","38662e29":"code","8a461a9d":"code","6721be89":"code","ec101844":"code","eec44f33":"code","d865fb2f":"code","f1843c51":"code","dc930f7b":"code","685b1f62":"code","910df5ce":"code","83ea7508":"code","12ccced1":"code","f536cf65":"code","6afd5c81":"code","d9ca97f5":"code","4dbfb602":"code","a30b20a9":"code","c35b0da9":"code","018b27cb":"code","0da73f4b":"code","6f496031":"code","e9da1941":"code","75cdfff7":"code","3e5aa28e":"code","a14d912f":"code","060f8cf2":"code","89778436":"code","1d6f6a94":"code","4eb31503":"code","2793ea47":"code","26845c30":"code","043cc252":"code","ce0be6fe":"code","a0ae29d7":"code","5299d24a":"code","3313feb8":"code","da960c97":"code","3d0fe5bd":"code","dee272f9":"code","7b27e4a6":"code","4ada38fb":"code","a7303b1f":"code","e5ffff16":"code","84952092":"code","ff0925f6":"code","ee94505a":"code","1f4ec387":"code","c8064bf8":"code","1c1c2ef6":"code","013a1141":"code","0485fd5b":"code","6a583cff":"code","90c1c41a":"code","e3da8d5e":"code","07bfc5a8":"code","efcaf8e5":"code","5949cf47":"code","0de28906":"code","4efe56a8":"code","6f68ec4f":"code","eea9cab5":"code","b332c2e8":"code","a634a472":"code","8e76bb0a":"code","603da55b":"code","a8240d7d":"code","7508fa11":"code","0ec2b2ee":"code","56062e60":"code","fc76d2a5":"code","dcdee64f":"code","fddd187c":"code","789ec8b0":"code","af631786":"code","3e486b86":"code","fff2fbee":"code","5c9bb5fa":"code","89597024":"code","7360e3ae":"code","360716ed":"code","9e32108b":"code","49d24293":"code","c5306504":"code","5c757d55":"code","20764ea2":"code","c0dfe89b":"code","af0697f4":"code","f4f9d232":"code","9829c9e3":"code","ccda8d7c":"code","ade3d1cc":"code","a15f4417":"code","8a84e0ca":"code","4f5f6a34":"code","92997240":"code","56525718":"code","bd8d595c":"code","3f7f583e":"code","a87ca720":"code","4ba9754f":"code","2e83ac73":"code","d8fc85d4":"code","81b84d97":"code","f5e03266":"code","b64e36d8":"code","b900706b":"code","c651206d":"markdown","f055a6d2":"markdown","582be19f":"markdown","beda647d":"markdown","64e09816":"markdown","78a06edb":"markdown","54cf4ad5":"markdown","52a78d56":"markdown","2312bdd6":"markdown","92d336f4":"markdown","fc988b18":"markdown","60ed5a4f":"markdown","847bf7be":"markdown","7abc82bd":"markdown","54bddb83":"markdown","94f51736":"markdown","eced5a5f":"markdown","b47483c4":"markdown","ec9f9478":"markdown","c3573d1b":"markdown","4b54d609":"markdown","bbd8020a":"markdown","7448c55d":"markdown","ad87e231":"markdown","5d6286fa":"markdown","58135117":"markdown","a481faa2":"markdown","f38e5087":"markdown","81b3ea2e":"markdown","351c892f":"markdown","0ab4c511":"markdown","954e3d38":"markdown","e5423692":"markdown","bc9ff4b2":"markdown","be1babff":"markdown","d7af1e0a":"markdown","3bc74a84":"markdown","cccbaef3":"markdown","9b430d42":"markdown","b0c58717":"markdown","dc830e06":"markdown","45fc6175":"markdown","c376cab9":"markdown","7c7a9b3c":"markdown","78eb44f3":"markdown","352d743b":"markdown","334e2859":"markdown","da6c148a":"markdown","53c697f4":"markdown","4cbbe5c0":"markdown","ad189155":"markdown","0cd4e463":"markdown","87fcffe2":"markdown","fc8738ac":"markdown","9c53a0f8":"markdown","3a0c5774":"markdown","46bd0414":"markdown","559ae2da":"markdown","97b7208c":"markdown","106b363b":"markdown","e4be13f3":"markdown","1d2b52a8":"markdown","6aa89b44":"markdown","63a09651":"markdown","78048026":"markdown","b8911b4c":"markdown","85181a12":"markdown","53a04a82":"markdown","2da299cd":"markdown","bc261254":"markdown","c208f393":"markdown","2b3b2566":"markdown","f1f1dbde":"markdown","e9251180":"markdown","f0b589e6":"markdown","513dbe5a":"markdown","114ee8a7":"markdown","b7991bd0":"markdown","2019909f":"markdown","5e7d5142":"markdown","d4f361f9":"markdown","3c91a492":"markdown","d337a9d1":"markdown","ee3cc1bc":"markdown","5a99cea7":"markdown","adcf31ae":"markdown"},"source":{"d169dfb6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import  train_test_split\n\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","6b4ec396":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","e283ae2e":"df_train","91f4c283":"df_train.describe()","35a8a126":"df_test","b3630774":"test_id = df_test['Id']","27a3a697":"df_test.describe()","e9ba85db":"df = df_train","61d20513":"df","769a5c08":"df_train_pearson_correlation = df_train.corr(method='pearson')\nfig = go.Figure(data=go.Heatmap(\n                   x=df_train_pearson_correlation.columns,\n                   y=df_train_pearson_correlation.index,\n                   z=df_train_pearson_correlation.values,\n                   name='pearson',showscale=True,xgap=1,ygap=1,\n                   colorscale='Blackbody'))\nfig.update_layout(height=700, width=900, title_text=\"<b>Pearson Correlation<b>\")\nfig.show()","89494835":"df_train_corr = df_train.corr(method='pearson')[\"SalePrice\"].sort_values(ascending = False)\ndf_train_corr","2dd72d3e":"top_corr_features = df_train.corr(method='pearson').index[abs(df_train.corr(method='pearson')['SalePrice'])>0.5]\nplt.figure(figsize=(15,10))\nsns.heatmap(df[top_corr_features].corr(),annot = True,cmap='YlOrBr')\ntop_corr_features","a5534c04":"fig = px.scatter(df_train, x=\"OverallQual\", y=\"SalePrice\", marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","45d5922a":"fig = px.box(df_train, y=\"OverallQual\")\nfig.show()","aa5312e1":"fig = px.scatter(df_train, x=\"GrLivArea\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","8aba9eeb":"fig = px.box(df_train, y=\"GrLivArea\")\nfig.show()","950530f3":"fig = px.scatter(df_train, x=\"GarageCars\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","eecc5527":"fig = px.box(df_train, y=\"GarageCars\")\nfig.show()","5b0d068d":"fig = px.scatter(df_train, x=\"GarageArea\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","5edce8a6":"fig = px.box(df_train, y=\"GarageArea\")\nfig.show()","5f73e3ca":"fig = px.scatter(df_train, x=\"TotalBsmtSF\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","f1525bf9":"fig = px.box(df_train, y=\"TotalBsmtSF\")\nfig.show()","25169118":"fig = px.scatter(df_train, x=\"FullBath\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","70e164dc":"fig = px.scatter(df_train, x=\"YearBuilt\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig.show()","66cc66cc":"fig = px.box(df_train, y=\"YearBuilt\")\nfig.show()","68b645d8":"df_train['SalePrice'].describe()","19c392bf":"fig = px.histogram(df_train, x=\"SalePrice\")\nfig.show()","7b88759c":"fig = px.box(df_train, y=\"SalePrice\")\nfig.show()","82994e83":"\"\"\"\nx = df_train['SalePrice']\nfig = go.Figure(data=[go.Histogram(x=x)])\nfig.show()\n\"\"\"","5f290385":"# Skew and kurt\nprint(\"Skewness: %f\" % df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df['SalePrice'].kurt())","f18e6da5":"#log transform the target:\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","0eac54e8":"fig = px.histogram(df_train, x=\"SalePrice\")\nfig.show()","587a4194":"#df.info()","781a389f":"duplicate = df.duplicated()\nprint(duplicate.sum())","38662e29":"df_cat = df.select_dtypes(include=['object'])\ndf_num = df.select_dtypes(exclude=['object'])","8a461a9d":"y = df_train.SalePrice.values","6721be89":"df.drop(['SalePrice'], axis= 1,inplace= True)\ndf.drop(['Id'], axis= 1, inplace= True)\ndf_test.drop(['Id'],axis=1,inplace=True)","ec101844":"print(\"Train data size is : {}\".format(df.shape))\nprint(\"Test data size is : {}\".format(df_test.shape))","eec44f33":"Missing_count = df.isna().sum().sort_values(ascending = False)[:25]\nMissing_ratio = (Missing_count\/len(df))*100\nMissing_table = pd.concat([Missing_count,Missing_ratio],axis= 1, keys=['Missing_count','Missing_ratio'])\nMissing_table ","d865fb2f":"plt.figure(figsize=(15,10))\nsns.barplot(x =Missing_table.index,y = 'Missing_ratio',data=Missing_table)\nplt.xticks(rotation ='90')","f1843c51":"Missing_count1 = df_test.isna().sum().sort_values(ascending = False)[:33]\nMissing_ratio1 = (Missing_count1\/len(df))*100\nMissing_table1 = pd.concat([Missing_count1,Missing_ratio1],axis= 1, keys=['Missing_count1','Missing_ratio1'])\nMissing_table1 ","dc930f7b":"df['PoolQC'] = df['PoolQC'].fillna('None')","685b1f62":"df['MiscFeature'] = df['MiscFeature'].fillna('None')","910df5ce":"df['Alley'] = df['Alley'].fillna('None')","83ea7508":"df['Fence'] = df['Fence'].fillna('None')","12ccced1":"df['FireplaceQu'] = df['FireplaceQu'].fillna('None')","f536cf65":"df['LotFrontage']= df['LotFrontage'].fillna(df.LotFrontage.median())  ","6afd5c81":"for i in ('GarageType','GarageFinish','GarageCond','GarageQual'):\n  df[i] = df[i].fillna('None')","d9ca97f5":"df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df.GarageYrBlt.mean())","4dbfb602":"for i in ('GarageCars','GarageArea'):\n  df[i] = df[i].fillna(0)","a30b20a9":"for i in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n  df[i] = df[i].fillna(0)","c35b0da9":"for i in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n  df[i] = df[i].fillna('None')","018b27cb":"df['MasVnrArea']= df['MasVnrArea'].fillna(0)\ndf['MasVnrType']= df['MasVnrType'].fillna('None')","0da73f4b":"df['Electrical'].value_counts(), df['Electrical'].mode()","6f496031":"df['Electrical'] = df['Electrical'].fillna('SBrkr')","e9da1941":"Missing_count1 = df_test.isna().sum().sort_values(ascending = False)[:33]\nMissing_ratio1 = (Missing_count1\/len(df))*100\nMissing_table1 = pd.concat([Missing_count1,Missing_ratio1],axis= 1, keys=['Missing_count1','Missing_ratio1'])\nMissing_table1 ","75cdfff7":"df_test['PoolQC'] = df_test['PoolQC'].fillna('None')","3e5aa28e":"df_test['MiscFeature'] = df_test['MiscFeature'].fillna('None')","a14d912f":"df_test['Alley'] = df_test['Alley'].fillna('None')","060f8cf2":"df_test['Fence'] = df_test['Fence'].fillna('None')","89778436":"df_test['FireplaceQu'] = df_test['FireplaceQu'].fillna('None')","1d6f6a94":"df_test['LotFrontage']= df_test['LotFrontage'].fillna(df_test.LotFrontage.median())  ","4eb31503":"for i in ('GarageType','GarageFinish','GarageCond','GarageQual'):\n  df_test[i] = df_test[i].fillna('None')","2793ea47":"df_test['GarageYrBlt'] = df_test['GarageYrBlt'].fillna(df_test.GarageYrBlt.mean())","26845c30":"for i in ('GarageCars','GarageArea'):\n  df_test[i] = df_test[i].fillna(0)","043cc252":"for i in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n  df_test[i] = df_test[i].fillna(0)","ce0be6fe":"for i in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n  df_test[i] = df_test[i].fillna('None')","a0ae29d7":"df_test['Electrical'] = df_test['Electrical'].fillna('SBrkr')","5299d24a":"df_test['MasVnrArea']= df_test['MasVnrArea'].fillna(0)\ndf_test['MasVnrType']= df_test['MasVnrType'].fillna('None')","3313feb8":"df_test['MSZoning'].mode()","da960c97":"df_test['MSZoning'] = df_test['MSZoning'].fillna('RL')\ndf_test['MSZoning'].isna().sum()","3d0fe5bd":"df_test['Utilities'].mode()","dee272f9":"df_test['Utilities'] = df_test['Utilities'].fillna('AllPub')\ndf_test['MSZoning'].isna().sum()","7b27e4a6":"df_test['Functional'].mode()","4ada38fb":"df_test['Functional'] = df_test['Functional'].fillna('Typ')","a7303b1f":"df_test['Functional'].isna().sum()","e5ffff16":"df_test['KitchenQual'].value_counts(), df_test['KitchenQual'].mode()","84952092":"df_test['KitchenQual'] = df_test['KitchenQual'].fillna('TA')","ff0925f6":"df_test['SaleType'].value_counts(), df_test['SaleType'].mode()","ee94505a":"df_test['SaleType'] = df_test['SaleType'].fillna('SaleType')","1f4ec387":"df_test['Exterior1st'].value_counts(), df_test['Exterior1st'].mode()","c8064bf8":"df_test['Exterior1st'] = df_test['Exterior1st'].fillna('VinylSd')","1c1c2ef6":"df_test['Exterior2nd'].value_counts(), df_test['Exterior2nd'].mode()","013a1141":"df_test['Exterior2nd'] = df_test['Exterior2nd'].fillna('VinylSd')","0485fd5b":"Missing_count = df.isna().sum().sort_values(ascending = False)[:3]\nMissing_ratio = (Missing_count\/len(df))*100\nMissing_table = pd.concat([Missing_count,Missing_ratio],axis= 1, keys=['Missing_count','Missing_ratio'])\nMissing_table","6a583cff":"Missing_count1 = df_test.isna().sum().sort_values(ascending = False)[:3]\nMissing_ratio1 = (Missing_count1\/len(df))*100\nMissing_table1 = pd.concat([Missing_count1,Missing_ratio1],axis= 1, keys=['Missing_count1','Missing_ratio1'])\nMissing_table1 ","90c1c41a":"from scipy.stats import skew","e3da8d5e":"df_cat = df.select_dtypes(include=['object'])\ndf_num = df.select_dtypes(exclude=['object'])","07bfc5a8":"df_num_skewness = df_num.apply(lambda x:skew(x))","efcaf8e5":"skewness = pd.DataFrame({'Skew':df_num_skewness.sort_values(ascending=False)})\nskewness","5949cf47":"skewness = skewness[abs(df_num_skewness)>0.5]\nskewness","0de28906":"df_num[skewness.index] = np.log1p(df_num[skewness.index])","4efe56a8":"df_num","6f68ec4f":"df_cat_test = df_test.select_dtypes(include=['object'])\ndf_num_test = df_test.select_dtypes(exclude=['object'])\ndf_num__test_skewness = df_num_test.apply(lambda x:skew(x))","eea9cab5":"skewness_test = pd.DataFrame({'Skew':df_num__test_skewness.sort_values(ascending=False)})","b332c2e8":"skewness_test = skewness[abs(df_num__test_skewness)>0.5]\nskewness_test\ndf_num_test[skewness.index] = np.log1p(df_num_test[skewness.index])","a634a472":"df_num_test","8e76bb0a":"df_cat = pd.get_dummies(df_cat)","603da55b":"df_cat","a8240d7d":"df = pd.concat([df_cat,df_num],axis= 1)","7508fa11":"df","0ec2b2ee":"df_cat_test = pd.get_dummies(df_cat_test)","56062e60":"df_cat_test","fc76d2a5":"df_test = pd.concat([df_cat_test,df_num_test],axis= 1)","dcdee64f":"SC = StandardScaler()","fddd187c":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler","789ec8b0":"X = df","af631786":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44, shuffle =True)","3e486b86":"X_train = SC.fit_transform(X_train)\nX_test = SC.fit_transform(X_test)\ndf_test = SC.fit_transform(df_test)","fff2fbee":"from sklearn.model_selection import KFold\n\n# Define GridSearchCV for Lasso,Ridge\ndef fit_model(Model,X,y):\n    \n    cross_validator = KFold(n_splits=10,shuffle=True)    #K=10 Cross-validation\n    \n    regressor = Model()\n    \n    parameters = {'alpha': np.arange(0.01, 200)}\n    \n    scoring='r2'\n    \n    grid = GridSearchCV(estimator = regressor,\n                        param_grid = parameters,\n                        scoring = scoring,\n                        verbose = 0,\n                        cv = cross_validator)\n    \n    grid = grid.fit(X,y)    #GridsearchCV\n\n    print(\n        'The best parameters are {} with a R2 score of {:0.2f}.'.format(\n            grid.best_params_, grid.best_score_))\n    \n    return grid.best_estimator_    # return best model after GridSearch\n","5c9bb5fa":"def fit_ENet(X,y):\n    \n    cross_validator = KFold(n_splits=10,shuffle=True)    #K=10 Cross-validation\n    \n    regressor = ElasticNet(random_state=3)\n    \n    parameters = {'alpha': [0.001,0.01,0.1,1,10,100],\n                  'l1_ratio': np.arange(0.0, 1.0, 0.1)\n    \n    }\n    \n    scoring='r2'\n    \n    grid = GridSearchCV(estimator = regressor,\n                        param_grid = parameters,\n                        scoring = scoring,\n                        cv = cross_validator,\n                        verbose = 0\n                        )\n    \n    grid = grid.fit(X,y)    #GridsearchCV\n\n    print(\n        'The best parameters are {} with a R2 score of {:0.2f}.'.format(\n            grid.best_params_, grid.best_score_))\n    \n    return grid.best_estimator_    # return best model after GridSearch\n","89597024":"def fit_KRR(X,y):\n    \n    cross_validator = KFold(n_splits=10,shuffle=True)    #K=10 Cross-validation\n    \n    regressor = KernelRidge(kernel= 'polynomial',degree=2,coef0=2.5) \n\n    parameters = {'alpha': [0.001,0.01,0.1,1,10,100]\n              \n    }\n\n    scoring='r2'\n    \n   \n    grid = GridSearchCV(estimator = regressor,\n                        param_grid = parameters,\n                        scoring = scoring,\n                        verbose = 0,\n                        cv = cross_validator\n                        )\n    \n    grid = grid.fit(X,y)    #GridsearchCV\n\n    print(\n        'The best parameters are {} with a R2 score of {:0.2f}.'.format(\n            grid.best_params_, grid.best_score_))\n    \n    return grid.best_estimator_    # return best model after GridSearch\n","7360e3ae":"def fit_GBoost(X,y):\n    \n    cross_validator = KFold(n_splits=10,shuffle=True)    #K=10 Cross-validation\n    \n    regressor = GradientBoostingRegressor(\n                                         learning_rate = 0.05,\n                                         max_depth=4, max_features='sqrt',\n                                         min_samples_leaf=15, min_samples_split=10, \n                                         loss='huber', random_state =5)\n\n    parameters = {'n_estimators':[100,500,1000,2000,3000]}\n\n\n    grid = GridSearchCV(estimator = regressor,\n                        param_grid = parameters,\n                        scoring = 'r2',\n                        cv = cross_validator,\n                        verbose =0 \n                        )\n    \n    grid = grid.fit(X,y)    #GridsearchCV\n\n    print(\n        'The best parameters are {} with a R2 score of {:0.2f}.'.format(\n            grid.best_params_, grid.best_score_))\n    \n    return grid.best_estimator_    # return best model after GridSearch\n","360716ed":"def Evaluation(model):\n  Y_pred_model = model.predict(X_test)\n  model_RMSE = mean_squared_error(y_test , Y_pred_model,squared = False)\n  model_R2 = r2_score(y_test, Y_pred_model)\n  print(\"RMSE    : \",model_RMSE)\n  print('R2 SCORE: ',model_R2)","9e32108b":"def cross_val_score_MSE(model):\n  model_MSE = cross_val_score(model,X_train,y_train,cv = 10, scoring = 'neg_mean_squared_error')\n  return(model_MSE)\ndef cross_val_score_R2(model):\n  model_R2= cross_val_score_R2 = cross_val_score(model,X_train,y_train,cv = 10, scoring = 'r2')\n  return(model_R2)","49d24293":"Lasso_optima = fit_model(Lasso,X_train,y_train)   ","c5306504":"Evaluation(Lasso_optima)","5c757d55":"score = cross_val_score_MSE(Lasso_optima)\nprint('K=10 CV_MSE of Lasso Regression:',format(-score.mean()))\nscore = cross_val_score_R2(Lasso_optima)\nprint('K=10 CV_R2 of Lasso Regression:',score.mean())","20764ea2":"Ridge_optima = fit_model(Ridge,X_train,y_train)   ","c0dfe89b":"Evaluation(Ridge_optima)","af0697f4":"score = cross_val_score_MSE(Ridge_optima)\nprint('K=10 CV_MSE of Ridge Regression:',format(-score.mean()))\nscore = cross_val_score_R2(Ridge_optima)\nprint('K=10 CV_R2 of Ridge Regression:',score.mean())","f4f9d232":" ENet_optima = fit_ENet(X_train,y_train) \n #The best parameters are {'alpha': 0.01, 'l1_ratio': 0.5} with a R2 score of 0.90.","9829c9e3":"Evaluation(ENet_optima)","ccda8d7c":"score = cross_val_score_MSE(ENet_optima)\nprint('K=10 CV_MSE of Elastic Net Regression:',format(-score.mean()))\nscore = cross_val_score_R2(ENet_optima)\nprint('K=10 CV_R2 of Elastic Net Regression:',score.mean())","ade3d1cc":"KRR_optima = fit_KRR(X_train,y_train)","a15f4417":"Evaluation(KRR_optima)","8a84e0ca":"score = cross_val_score_MSE(KRR_optima)\nprint('K=10 CV_MSE of Kernel Ridge Regression:',format(-score.mean()))\nscore = cross_val_score_R2(KRR_optima)\nprint('K=10 CV_R2 of Kernel Ridge Regression:',score.mean())","4f5f6a34":"GBoost_optima = fit_GBoost(X_train,y_train)","92997240":"Evaluation(GBoost_optima)","56525718":"score = cross_val_score_MSE(GBoost_optima)\nprint('K=10 CV_MSE of Gradient Boosting Regression:',format(-score.mean()))\nscore = cross_val_score_R2(GBoost_optima)\nprint('K=10 CV_R2 of Gradient Boosting Regression:',score.mean())","bd8d595c":"model_XGB = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)","3f7f583e":"XGB_REG=model_XGB.fit(X_train,y_train)","a87ca720":"Evaluation(XGB_REG)","4ba9754f":"score = cross_val_score_MSE(XGB_REG)\nprint('K=10 CV_MSE of XGBoosting Regression:',format(-score.mean()))\nscore = cross_val_score_R2(XGB_REG)\nprint('K=10 CV_R2 of XGBoosting Regression:',score.mean())","2e83ac73":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","d8fc85d4":"LGB_REG = model_lgb.fit(X_train,y_train)","81b84d97":"Evaluation(LGB_REG)","f5e03266":"score = cross_val_score_MSE(LGB_REG)\nprint('K=10 CV_MSE of Light Gradient Boosting Machine Regression:',format(-score.mean()))\nscore = cross_val_score_R2(LGB_REG)\nprint('K=10 CV_R2 of Light Gradient Boosting Machine Regression:',score.mean())","b64e36d8":"model_col = ['Lasso Regression','Ridge Regression','Elastic Net Regression',\n             'Kernel Ridge Regression','Gradient Boosting Regression',\n             'XGBoost Regression','LightGBM Regression']\n\nmodel_MSE = [0.0161,0.0184,0.0154,0.0180,0.0151,0.0158,0.0158]\n\nmodel_R2 = [0.895,0.880,0.899,0.883,0.901,0.897,0.897]\n\nFinal_CV_result = pd.DataFrame(model_col,columns= ['Regressor'])\nFinal_CV_result['RMSE_mean'] = model_MSE\nFinal_CV_result['R2_mean'] = model_R2","b900706b":"Final_CV_result","c651206d":"**Categorical Train Data**","f055a6d2":"It seems that the target value has a skewness to the right, usually we want to convert it to a normal distribution, which is more conducive to the prediction of the linear model.","582be19f":"### MasVnrType & MasVnrArea\n\nMasVnrType: Masonry veneer type\n\n       BrkCmn\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       None\tNone\n       Stone\tStone\n\t\nMasVnrArea: Masonry veneer area in square feet\n\nNA most likely means there is no Masony for houses. \n\nMasVnrType has 'None' categorical value, so missing value can be filled with 'None'.\n\nMasVnrArea filled with '0'.","beda647d":"**Missing Values in Test data**","64e09816":"### KitchenQual\uff1a\n\nKitchen quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor","78a06edb":"### Define Cross-Validation","54cf4ad5":"# Libraies loading<a id = '1'><a>","52a78d56":"### SaleType\uff1a\n\n Type of sale\n\t\t\n       WD \tWarranty Deed - Conventional\n       CWD\tWarranty Deed - Cash\n       VWD\tWarranty Deed - VA Loan\n       New\tHome just constructed and sold\n       COD\tCourt Officer Deed\/Estate\n       Con\tContract 15% Down payment regular terms\n       ConLw\tContract Low Down payment and low interest\n       ConLI\tContract Low Interest\n       ConLD\tContract Low Down\n       Oth\tOther","2312bdd6":"## Reject Outlier<a id = '4.2'><a>","92d336f4":" 3 Standard Deviation \n \nThe Laida criterion is to first assume that a set of test data contains only random errors, calculate and process them to obtain the standard deviation, and determine an interval with a certain probability. It is considered that any error exceeding this interval is not a random error but a gross error. Data containing this error should be eliminated.","fc988b18":"### POOlQC: \n  \n  Pool quality\n\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       NA\tNo Pool\n\nAccording to the data description file, we learned that NA means there is no swimming pool.","60ed5a4f":"**Cross-Validation ENet regression**","847bf7be":"**Congratulations for completing the most time-consuming part\uff01\uff01\uff01**","7abc82bd":"# EDA<a id ='3'><a>","54bddb83":"### GrLivArea","94f51736":"### Define GridSearchCV for Lasso Regression,Ridge Regression","eced5a5f":"**Version 1**\n1. Reject Outlier\n3. GridSearchCV on XGBoosting and LightGBM\n4. Stacked Regression(Competition Killer)\n5. Fix missing values and submit.","b47483c4":"![123.jpg](attachment:30dee361-9e39-46d6-9840-56b05f249bf3.jpg)","ec9f9478":"### Lasso Regression","c3573d1b":"### Elastic Net Regression","4b54d609":"\nBsmtFinSF1: Type 1 finished square feet\n\nBsmtFinSF2: Type 2 finished square feet\n\nBsmtUnfSF: Unfinished square feet of basement area\n\nTotalBsmtSF: Total square feet of basement area\n\nBsmtHalfBath: Basement half bathrooms\n\n","bbd8020a":"## Deleted duplicated data <a id = '4.1'><a>","7448c55d":"## Dimensionality reduction<a id = '4.7'><a>","ad87e231":"### Functional\n\n Home functionality (Assume typical unless deductions are warranted)\n\n       Typ\tTypical Functionality\n       Min1\tMinor Deductions 1\n       Min2\tMinor Deductions 2\n       Mod\tModerate Deductions\n       Maj1\tMajor Deductions 1\n       Maj2\tMajor Deductions 2\n       Sev\tSeverely Damaged\n       Sal\tSalvage only","5d6286fa":"### Ridge Regreesion","58135117":"### Define GridSearchCV for Gradient Boosting Regression","a481faa2":"### OverallQual","f38e5087":"### LotFrontage\n\nLotFrontage: Linear feet of street connected to property.\n\n'LotFrontage' is numerical column and it also has outliers, so it can be filled with median","81b3ea2e":"###  Define GridSearchCV for Elastic Net Regression","351c892f":"### Fence:\n\nFence quality\n\t\t\n       GdPrv\tGood Privacy\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood\/Wire\n       NA\tNo Fence\n\nNA means No Fence.","0ab4c511":"### GarageYrBlt: \nYear garage was built\n","954e3d38":"## Standardization<a id ='4.8'><a>","e5423692":"### Alley:\n\nType of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n\nNA means No alley access","bc9ff4b2":"## Processing Categorical & Numerical Data<a id = '4.6'><a>","be1babff":"### Target value(Sale Price)","d7af1e0a":"## Handling the Missing Values<a id = '4.4'><a>\n\nThis work is just to reproduce the data analysis process of housing price prediction.\n*  In real busines issues, we have to consider practical problems and customer needs.\n*  In this data set, we will refer to a file named data description.txt.","3bc74a84":"The 3\u03c3 rule is:\n*  The probability that the value is distributed in (\u03bc-\u03c3,\u03bc+\u03c3) is 0.6827\n*  The probability that the value is distributed in (\u03bc-2\u03c3,\u03bc+2\u03c3) is 0.9545\n*  The probability that the value is distributed in (\u03bc-3\u03c3,\u03bc+3\u03c3) is 0.9973\n\nIt can be considered that the value of Y is almost all concentrated in the (\u03bc-3\u03c3,\u03bc+3\u03c3) interval, and the possibility of exceeding this range is only less than 0.3%.","cccbaef3":"*  SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n*  MSSubClass: The building class\n* MSZoning: The general zoning classification\n*  LotFrontage: Linear feet of street connected to property\n*  LotArea: Lot size in square feet\n*  Street: Type of road access\n*  Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale\n\n","9b430d42":"### GarageCars\n","b0c58717":"## What can be imporved","dc830e06":"### YearBuilt","45fc6175":"## Data description<a id = '2.1'><a>","c376cab9":"### Exterior1st & Exterior2nd\uff1a\n\nExterior1st: Exterior covering on house\n\n       AsbShng\tAsbestos Shingles\n       AsphShn\tAsphalt Shingles\n       BrkComm\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       CemntBd\tCement Board\n       HdBoard\tHard Board\n       ImStucc\tImitation Stucco\n       MetalSd\tMetal Siding\n       Other\tOther\n       Plywood\tPlywood\n       PreCast\tPreCast\t\n       Stone\tStone\n       Stucco\tStucco\n       VinylSd\tVinyl Siding\n       Wd Sdng\tWood Siding\n       WdShing\tWood Shingles\n\t\nExterior2nd: Exterior covering on house (if more than one material)\n\n       AsbShng\tAsbestos Shingles\n       AsphShn\tAsphalt Shingles\n       BrkComm\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       CemntBd\tCement Board\n       HdBoard\tHard Board\n       ImStucc\tImitation Stucco\n       MetalSd\tMetal Siding\n       Other\tOther\n       Plywood\tPlywood\n       PreCast\tPreCast\n       Stone\tStone\n       Stucco\tStucco\n       VinylSd\tVinyl Siding\n       Wd Sdng\tWood Siding\n       WdShing\tWood Shingles","7c7a9b3c":"### LightGBM Regression","78eb44f3":"**Skewness of Test Data**","352d743b":"### Categorical Data\n\nConvert categorical features into numerical.\n\nIn fact, there is a more detailed way here is that we should consider the order of some features in categorical data. We should perform label encoder on these categorical data with order instead of one-hot encoder or dummy variables. \n\nThis path can be used as a plan to optimize the model.\n","334e2859":"# Summary <a id = '6'><a>","da6c148a":"### Attributes related to Garage\n\n\n\nGarageFinish: Interior finish of the garage\n\n       Fin\tFinished\n       RFn\tRough Finished\t\n       Unf\tUnfinished\n       NA\tNo Garage\n\n\nGarageType: Garage location\n\t\t\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage\n\nGarageQual: Garage quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\t\t\nGarageCond: Garage condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\nNA can be defined as No Garage in the above four attributes.","53c697f4":"**Cross-validation on Ridge**","4cbbe5c0":"BsmtQual: Evaluates the height of the basement\n\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches)\n       NA\tNo Basement\n\n\nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n\nBsmtExposure: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement\n\n\nBsmtFinType1: Rating of basement finished area\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\n\n\nBsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\n\nNA in these categorical features related to Basement means that there is no basement.","ad189155":"### Kernel Ridge Regression","0cd4e463":"K=10 CV_MSE of Lasso Regression: 0.01618332240533083\nK=10 CV_R2 of Lasso Regression: 0.8951018865209956.\n\n\nK=10 CV_MSE of Ridge Regression: 0.01849955275446303\nK=10 CV_R2 of Ridge Regression: 0.8807339266288556\n\nK=10 CV_MSE of Elastic Net Regression: 0.015492391396611732\nK=10 CV_R2 of Elastic Net Regression: 0.8991384853550078\n\nK=10 CV_MSE of Kernel Ridge Regression: 0.018094334177747293\nK=10 CV_R2 of Kernel Ridge Regression: 0.8830979928007576\n\n\nK=10 CV_MSE of Gradient Boosting Regression: 0.015149853075758002\nK=10 CV_R2 of Gradient Boosting Regression: 0.9018323967806324\n\n\nK=10 CV_MSE of XGBoosting Regression: 0.015803929014916507\nK=10 CV_R2 of XGBoosting Regression: 0.8979604122359705\n\nK=10 CV_MSE of Light Gradient Boosting Machine Regression: 0.01612564604224234\nK=10 CV_R2 of Light Gradient Boosting Machine Regression: 0.8952509674644556","87fcffe2":"### Define Evaluation","fc8738ac":"**Missing Values in Train data**","9c53a0f8":"### Skew and Kurt:\n\n*   Skewness can be used to measure the asymmetry of the probability distribution\n\n\nThe range of skewness is (-\u221e, +\u221e)\n\nWhen the skewness < 0, the probability distribution graph is skewed to the left.\n\nWhen the skewness=0, it means that the data is relatively evenly distributed on both sides of the average value, not necessarily an absolute symmetrical distribution.\n\nWhen the skewness>0, the probability distribution graph is skewed to the right\n\n\n*   Kurtosis can be used to measure the steepness of the probability distribution of a random variable.\n\nThe value range of kurtosis is [1,+\u221e), and the kurtosis value of data that completely obeys the normal distribution is 3. The larger the kurtosis value, the sharper the probability distribution graph, the smaller the kurtosis value, and the chubby. .\n\n","3a0c5774":"### Attributes related of Basement\n\n\n\n\n","46bd0414":"### MiscFeature:\n\nMiscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\t  None\n\nNA means No feature.","559ae2da":"## Data visualization<a id ='3.2'><a>","97b7208c":"### FireplaceQu:\n\nFireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n\nNA means No Fireplace.","106b363b":"## Usefull Definitions<a id = '5.1'><a>","e4be13f3":"### MSZoning: \n\nIdentifies the general zoning classification of the sale.\n\t\t\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density\n\nIt seems that there is no explanation for NA here. Let's take a look at the mode of this feature.","1d2b52a8":"### TotalBsmtSF","6aa89b44":"1. The R2_mean  of all models is close to 0.9, \n2. The RMSE_mean of all models is between [0.015-0.018].\n\nBut the R2_mean of **Gradient Boosting Regressor** is slightly higher than other models.","63a09651":"> Dimensionality reduction is a very challenging task.\nIn the real world, we are often exposed to data sets with more than 1,000 or even 10,000 llion features.\nHaving so many variables is an advantage-the larger the amount of data, the more credible the results of the analysis.\nBut at the same time we may not be able to start.\n\n\n> When using a data set with a large number of features for data analysis, dimensionality reduction of the data is a way to improve the performance of the model.\n* \nHere is only my personal ideas to reduce the dimensionality of the data:\n1.  Missing value ratio: If there are too many missing values \u200b\u200bin the data set, we can use this method to reduce the number of variables.\n2.  Low variance filtering: This method can identify and delete constant variables from the data set. Variables with small variance have little effect on the target variable, so you can safely delete it.\n2.  High correlation filtering: A pair of variables with high correlation will increase the multicollinearity in the data set, so it is necessary to delete one of them in this way.\n4.  Random Forest: This is one of the most commonly used dimensionality reduction methods, it will clearly calculate the importance of each feature in the data set.\n5.  Forward feature selection and reverse feature elimination: These two methods are time-consuming and computationally expensive, so they are only suitable for data sets with fewer input variables.\n6.  Factor analysis: This method is suitable for situations where there are highly correlated variable sets in the data set.\n7.  PCA: This is one of the most widely used techniques for processing linear data.\n8.  ICA: We can use ICA to convert data into independent components and use fewer components to describe the data.\n9.  ISOMAP: Suitable for non-linear data processing.\n10.  t-SNE: It is also suitable for non-linear data processing. Compared with the previous method, the visualization of this method is more direct.\n11.  UMAP: Suitable for high-dimensional data. Compared with t-SNE, this method is faster.","78048026":"### FullBath","b8911b4c":"# Machine Learning model<a id = '5'><a>","85181a12":"# Data loading<a id ='2'><a>","53a04a82":"### Utilities: \n\nType of utilities available\n\t\t\n       AllPub\tAll public Utilities (E,G,W,& S)\t\n       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n       NoSeWa\tElectricity and Gas Only\n       ELO\tElectricity only\t","2da299cd":"## Pearson correlation cofficient<a id = '3.1'><a>","bc261254":"**Skewness of Train data**","c208f393":"We obtained these characteristics that are at least moderately relevant to SalePrice :'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF',\n'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageCars',\n'GarageArea',","2b3b2566":"### GradientBoosting Regression","f1f1dbde":"# You can consider upvoting or comment if you think it helps you! :)\n\n* This is a more detailed Kernel about housing price prediction. \n* It includes data visualization, feature engineering, building machine learning models to predict housing prices (Lasso, Ridge, Elastice Net, Kernel Ridge, Gradient Boosting,) and some personal insights.\n","e9251180":"### Numercial data \n\nFix Skewed feature","f0b589e6":"### Define GridSearchCV for Kernel Ridge Regression","513dbe5a":"### Xgboost Regression","114ee8a7":"### Electrical\nElectrical: Electrical system\n\n       SBrkr\tStandard Circuit Breakers & Romex\n       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix\tMixed","b7991bd0":"We can roughly get some features that are highly correlated with SalePrice from this heat map. Such as OverallQual,GrLivArea,GarageArea,GarageCars,TotalBsmtSF.\n\n\nBut Are these accurate? Let us explore further.","2019909f":"## Missing Values<a id = '4.3'><a>\n\nTrain Data & Test Data","5e7d5142":"### GarageArea","d4f361f9":"\nGarageCars: Size of garage in car capacity\n\nGarageArea: Size of garage in square feet","3c91a492":"### Process Missing value of Test Data","d337a9d1":"**Categorcial  Test data**","ee3cc1bc":"# Feature engineering<a id = '4'><a>","5a99cea7":"**Let's check the missing value table.**\n## Recheck the data for missing values<a id = '4.5'><a>","adcf31ae":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>\n\n* [1. Libraies Loading](#1)\n* [2. Data Loading](#2)\n    * [2.1 Data Description](#2.1)\n* [3. Exploratory Data Analysis(EDA)](#3)\n    * [3.1 Pearson Correlation & Spearman Correlation](#3.1)\n    * [3.2 Data Visualization](#3.2)\n* [4. Feature Engineering](#4)\n    * [4.1 Deleted Duplicate Data](#4.1)\n    * [4.2 Reject Outlier](#4.2)\n    * [4.3 Missing Values](#4.3)\n    * [4.4 Handling the Missing Values](#4.4)\n    * [4.5 Recheck Missing values](#4.5)\n    * [4.6 Processing Categorical & Numerical Data](#4.6)\n    * [4.7  Dimensionality reduction](#4.7)\n    * [4.8 Standardization](#4.8)\n* [5. Machine Learning model](#5)\n    * [5.1 Usefull Definitions](#5.1)\n    * [5.2 Model experiment](#5.2 )\n* [6. Summary](#6)\n        "}}