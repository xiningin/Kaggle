{"cell_type":{"02d5a11c":"code","02811fd0":"code","85a2d0be":"code","29712ed1":"code","fc3faf07":"code","6668dd26":"code","fd7f2406":"code","7575d8a9":"code","46f3dae8":"code","2a775579":"code","3d5073c8":"code","e3025227":"code","410e0a05":"code","8d4519d9":"code","44ffc15d":"code","277680ba":"code","eeba472d":"code","3f746e90":"code","13c21bda":"code","1fcdebde":"code","9f1ed925":"code","c18ef360":"code","f50dae4d":"code","404a8a6b":"code","e91d9756":"code","b2df0e81":"code","1dc1e476":"code","d581dec2":"code","e63dfd4d":"code","8db900ea":"code","f47d32b2":"code","f5b3a1a1":"code","0a560cd0":"code","18cbb369":"code","66f920f0":"code","565b8d3d":"code","9afbb086":"code","342f3ff7":"code","7ca6a528":"code","cdcf90d1":"code","46ddfd9c":"code","cd3f3864":"code","3037c3f7":"code","4330bc24":"code","4c633548":"code","8f259920":"code","74d233bb":"code","5ab7c4b2":"code","c284c20a":"code","1ee11697":"code","998e5258":"code","2430b4c2":"code","1a96b291":"code","f4050718":"code","d5970e4e":"code","ebddd1b8":"code","2073faf0":"code","f5306b09":"code","319f27f7":"code","bac17f4d":"code","d61063f0":"code","f29ff026":"code","1fdf5782":"code","a499e1cc":"code","a6dc0a1e":"code","b2ab4d4b":"code","3e33e046":"code","68c450c3":"code","0358122a":"code","a5c5ac0d":"code","a177af18":"code","f4ee9899":"code","13e6aca3":"code","270712ce":"code","a7f1eb9a":"code","c5648043":"code","001a6ab2":"code","f2e50036":"code","cf4cb506":"code","a113ca49":"code","3a069b54":"code","22a2a072":"markdown","3426d261":"markdown","5700fbb7":"markdown","afd93cda":"markdown","d855fbe8":"markdown","cb4b8e01":"markdown","2687f828":"markdown","13050d93":"markdown","0507b1fc":"markdown","7fd19a16":"markdown","0d3d947a":"markdown","40812967":"markdown","7f2deb0a":"markdown","f4d368ed":"markdown","541c4aaa":"markdown","4868be42":"markdown","3d0cb348":"markdown","3ff372dd":"markdown","c6051986":"markdown","54ad0dd1":"markdown","28eeaae4":"markdown","f5b0a87b":"markdown","0d45d5fd":"markdown","a6767b16":"markdown"},"source":{"02d5a11c":"import numpy as np\nimport pandas as pd \nfrom itertools import product\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\n\n\npd.set_option('display.max_columns', 30)","02811fd0":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.head()","85a2d0be":"items.info()","29712ed1":"items.describe()","fc3faf07":"print('Unique values of product names: {}'.format(items.item_name.unique()))\nprint()\nprint('Number of unique values of products: {}'.format(items.item_name.nunique()))","6668dd26":"def simple_hist(data, x, bins, title, xlabel, xmin, xmax):\n    plt.figure(figsize = (12, 8))\n    sns.set()\n    sns.distplot(data[x], color = 'lightcoral')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.xlim(xmin, xmax)\n    plt.show()\n\nsimple_hist(items, 'item_category_id', 10, \n            'Distribution of item categories in the item dataframe', 'item_categories_id', -2, 85)","fd7f2406":"item_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitem_categories.head()","7575d8a9":"item_categories.info()","46f3dae8":"item_categories.describe()","2a775579":"print('Unique values of product identifiers: {}'.format(item_categories.item_category_id.unique()))\nprint()\nprint('Number of unique values of product identifiers: {}'.format(item_categories.item_category_id.nunique()))","3d5073c8":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nshops.head()","e3025227":"shops.info()","410e0a05":"shops.describe()","8d4519d9":"print('Unique meanings of store names: {}'.format(shops.shop_name.unique()))\nprint()\nprint('Number of unique store values: {}'.format(shops.shop_name.nunique()))","44ffc15d":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntrain.head()","277680ba":"train.info()","eeba472d":"train.describe()","3f746e90":"simple_hist(train, 'shop_id', 10, \n            'Distribution of stores in the train dataframe', 'shop_id', -5, 65)","13c21bda":"simple_hist(train, 'item_id', 30, \n            'Distribution of items in the train dataframe', 'item_id', -1000, 25000)","1fcdebde":"simple_hist(train, 'item_price', 1000, \n            'Distribution of the price of items in the train dataframe', 'item_price', -100, 10000)","9f1ed925":"plt.figure(figsize = (12, 8))\nsns.boxplot(y=train['item_price'])\nplt.ylim(0, 10000)\nplt.grid()\nplt.title('Boxplot for the price of goods in the range from 0 to 10000 rubles')\nplt.ylabel('item_price')","c18ef360":"# the histogram is uninformative - it is better to look at the numbers\ntrain.item_cnt_day.value_counts()","f50dae4d":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest.head()","404a8a6b":"test.info()","e91d9756":"test.describe()","b2df0e81":"simple_hist(test, 'shop_id', 10, \n            'Shops distributions in the test dataframe', 'id shops', 0, 70)\n# the distributions of the training and test datasets are different in this fic","1dc1e476":"simple_hist(test, 'item_id', 30, \n            'Distribution of items in the test dataframe', 'id items', 0, 25000)\n# the distributions of the training and test datasets are similar in this feature","d581dec2":"sample_submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","e63dfd4d":"sample_submission.head()","8db900ea":"df_temp = pd.Series(list(train[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntest_iter_temp = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(df_temp.isin(test_iter_temp).sum()\/len(df_temp),2)*100)+'%')","f47d32b2":"#test_shop_ids = test['shop_id'].unique()\n#test_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\n#train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\n#train = train[train['item_id'].isin(test_item_ids)]","f5b3a1a1":"# remove outliers\ntrain = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day <= 900]","0a560cd0":"# aggregate\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# compute all shops\/items combinations\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","18cbb369":"# add sale for month\ntrain_merge = train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum'})\ntrain_merge.columns = ['item_cnt_month']\ntrain_merge.reset_index(inplace=True)","66f920f0":"# merge grid and train\ntrain_merge = pd.merge(grid, train_merge, on = index_cols, how='left').fillna(0)\ntrain_merge['item_cnt_month'] = train_merge['item_cnt_month'].clip(0, 40)","565b8d3d":"items_prepare = pd.merge(items, item_categories, on='item_category_id')\ntrain_merge = pd.merge(train_merge, items_prepare, on = ['item_id'], how = 'left')","9afbb086":"# prepare to concat with train\ntest_temp = test.copy()\ntest_temp['date_block_num'] = 34\ntest_temp.drop('ID', axis=1, inplace=True)","342f3ff7":"# merge with items and item_category\ntest_temp = test_temp.merge(items, how='left', on='item_id')\ntest_temp = test_temp.merge(item_categories, how='left', on='item_category_id')\ntest_temp.drop('item_name', axis=1, inplace=True)","7ca6a528":"# concat test and train dataframes\ntrain_merge = pd.concat([train_merge, test_temp], axis=0, ignore_index=True, keys=index_cols)\ntrain_merge.fillna(0, inplace=True)","cdcf90d1":"map_dict = {\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)': '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)' : '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            'PC ': '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b',\n            '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435': '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 '\n            }\n# extract common categories\ntrain_merge['item_category'] = train_merge['item_category_name'].apply(lambda x: x.split('-')[0])\ntrain_merge['item_category'] = train_merge['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\n# encoding common categories\ntrain_merge['item_category_common'] = LabelEncoder().fit_transform(train_merge['item_category'])","46ddfd9c":"# extract and encode cities\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\nshops.loc[shops.city == '!\u044f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u044f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n# add coordinates of cities\ncoords = dict()\ncoords['\u044f\u043a\u0443\u0442\u0441\u043a'] = (62.028098, 129.732555, 4)\ncoords['\u0430\u0434\u044b\u0433\u0435\u044f'] = (44.609764, 40.100516, 3)\ncoords['\u0431\u0430\u043b\u0430\u0448\u0438\u0445\u0430'] = (55.8094500, 37.9580600, 1)\ncoords['\u0432\u043e\u043b\u0436\u0441\u043a\u0438\u0439'] = (53.4305800, 50.1190000, 3)\ncoords['\u0432\u043e\u043b\u043e\u0433\u0434\u0430'] = (59.2239000, 39.8839800, 2)\ncoords['\u0432\u043e\u0440\u043e\u043d\u0435\u0436'] = (51.6720400, 39.1843000, 3)\ncoords['\u0432\u044b\u0435\u0437\u0434\u043d\u0430\u044f'] = (0, 0, 0)\ncoords['\u0436\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439'] = (55.5952800, 38.1202800, 1)\ncoords['\u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d'] = (0, 0, 0)\ncoords['\u043a\u0430\u0437\u0430\u043d\u044c'] = (55.7887400, 49.1221400, 4)\ncoords['\u043a\u0430\u043b\u0443\u0433\u0430'] = (54.5293000, 36.2754200, 4)\ncoords['\u043a\u043e\u043b\u043e\u043c\u043d\u0430'] = (55.0794400, 38.7783300, 4)\ncoords['\u043a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a'] = (56.0183900, 92.8671700, 4)\ncoords['\u043a\u0443\u0440\u0441\u043a'] = (51.7373300, 36.1873500, 3)\ncoords['\u043c\u043e\u0441\u043a\u0432\u0430'] = (55.7522200, 37.6155600, 1)\ncoords['\u043c\u044b\u0442\u0438\u0449\u0438'] = (55.9116300, 37.7307600, 1)\ncoords['\u043d.\u043d\u043e\u0432\u0433\u043e\u0440\u043e\u0434'] = (56.3286700, 44.0020500, 4)\ncoords['\u043d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a'] = (55.0415000, 82.9346000, 4)\ncoords['\u043e\u043c\u0441\u043a'] = (54.9924400, 73.3685900, 4)\ncoords['\u0440\u043e\u0441\u0442\u043e\u0432\u043d\u0430\u0434\u043e\u043d\u0443'] = (47.2313500, 39.7232800, 3)\ncoords['\u0441\u043f\u0431'] = (59.9386300, 30.3141300, 2)\ncoords['\u0441\u0430\u043c\u0430\u0440\u0430'] = (53.2000700, 50.1500000, 4)\ncoords['\u0441\u0435\u0440\u0433\u0438\u0435\u0432'] = (56.3000000, 38.1333300, 4)\ncoords['\u0441\u0443\u0440\u0433\u0443\u0442'] = (61.2500000, 73.4166700, 4)\ncoords['\u0442\u043e\u043c\u0441\u043a'] = (56.4977100, 84.9743700, 4)\ncoords['\u0442\u044e\u043c\u0435\u043d\u044c'] = (57.1522200, 65.5272200, 4)\ncoords['\u0443\u0444\u0430'] = (54.7430600, 55.9677900, 4)\ncoords['\u0445\u0438\u043c\u043a\u0438'] = (55.8970400, 37.4296900, 1)\ncoords['\u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0439'] = (0, 0, 0)\ncoords['\u0447\u0435\u0445\u043e\u0432'] = (55.1477000, 37.4772800, 4)\ncoords['\u044f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c'] = (57.6298700, 39.8736800, 2) \n\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]","cd3f3864":"train_merge = pd.merge(train_merge, shops, on = ['shop_id'], how='left')","3037c3f7":"train_merge.drop(['item_name', 'item_category_name', 'item_category'], axis=1, inplace=True)","4330bc24":"train_merge.head()","4c633548":"# define lag_feature\ndef lag_feature(data, lags, column):\n    temp = data[['date_block_num', 'shop_id', 'item_id', column]]\n    for lag in lags:\n        shifted = temp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', column + '_lag_' + str(lag)]\n        shifted['date_block_num'] += lag\n        data = pd.merge(data, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        data[column+'_lag_'+str(lag)] = data[column+'_lag_'+str(lag)].astype('float16')\n    return data","8f259920":"# add sales lags for last 3 months\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_cnt_month')","74d233bb":"train_merge.info()","5ab7c4b2":"# value reduction\ndef value_reduction(data):\n    for column in data.columns:\n        if data[column].dtype == 'float64':\n            data[column] = data[column].astype(np.float32)\n        if (data[column].dtype == 'int64' or data[column].dtype == 'int32') and (data[column].max() < 32767 and data[column].min() > -32768) and data[column].isnull().sum()==0:\n            data[column] = data[column].astype(np.int16)\n    return data\n\ntrain_merge = value_reduction(train_merge)","c284c20a":"# add mean encoding for items for last 3 month\nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id'])['item_cnt_month'].mean().reset_index().rename(columns={\"item_cnt_month\": \"item_target_enc\"}, errors=\"raise\")\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id'], how='left')\n\ntrain_merge['item_target_enc'] = (train_merge['item_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_target_enc')\ntrain_merge.drop(['item_target_enc'], axis=1, inplace=True)","1ee11697":"# add target encoding for item\/city for last 3 months\nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id', 'city_code'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_loc_target_enc\"}, errors=\"raise\")\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id', 'city_code'], how='left')\n\ntrain_merge['item_loc_target_enc'] = (train_merge['item_loc_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_loc_target_enc')\ntrain_merge.drop(['item_loc_target_enc'], axis=1, inplace=True)","998e5258":"# add target encoding for item\/shop for last 3 months \nitem_id_target_mean = train_merge.groupby(['date_block_num','item_id', 'shop_id'])['item_cnt_month'].mean().reset_index().rename(columns={\n    \"item_cnt_month\": \"item_shop_target_enc\"}, errors=\"raise\")\n\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_id', 'shop_id'], how='left')\n\ntrain_merge['item_shop_target_enc'] = (train_merge['item_shop_target_enc']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'item_shop_target_enc')\ntrain_merge.drop(['item_shop_target_enc'], axis=1, inplace=True)","2430b4c2":"# interaction features\nfirst_item_block = train_merge.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\nfirst_shop_item_buy_block = train_merge[train_merge['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']","1a96b291":"# merge train and new features\ntrain_merge = pd.merge(train_merge, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\ntrain_merge = pd.merge(train_merge, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')","f4050718":"# fillna and change type\ntrain_merge['first_date_block_num'].fillna(100, inplace=True)\ntrain_merge['shop_item_sold_before'] = (train_merge['first_date_block_num'] < train_merge['date_block_num']).astype('int8')\ntrain_merge.drop(['first_date_block_num'], axis=1, inplace=True)\n\ntrain_merge['item_first_interaction'].fillna(0, inplace=True)\ntrain_merge['shop_item_sold_before'].fillna(0, inplace=True)\n \ntrain_merge['item_first_interaction'] = train_merge['item_first_interaction'].astype('int8')  \ntrain_merge['shop_item_sold_before'] = train_merge['shop_item_sold_before'].astype('int8') ","d5970e4e":"# add avg category for new features\nitem_id_target_mean = train_merge[train_merge['item_first_interaction'] == 1].groupby(['date_block_num','item_category_id'])['item_cnt_month'].mean().reset_index().rename(columns={'item_cnt_month': 'new_item_cat_avg'}, errors='raise')\n\ntrain_merge = pd.merge(train_merge, item_id_target_mean, on=['date_block_num','item_category_id'], how='left')\n\ntrain_merge['new_item_cat_avg'] = (train_merge['new_item_cat_avg']\n                                .fillna(0)\n                                .astype(np.float16))\n\ntrain_merge = lag_feature(train_merge, [1, 2, 3], 'new_item_cat_avg')\ntrain_merge.drop(['new_item_cat_avg'], axis=1, inplace=True)","ebddd1b8":"train_merge.isna().sum()","2073faf0":"# fill Nan values to 0\ntrain_merge.fillna(0, inplace=True)\n# take data only after 3 since the most lag month interval is 3\ntrain_merge = train_merge[train_merge['date_block_num'] > 2]","f5306b09":"# save finished dataset to pickle\ntrain_merge.to_pickle('train_merge.pkl')","319f27f7":"# split dataset \nX_train = train_merge[train_merge.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = train_merge[train_merge.date_block_num < 33]['item_cnt_month']\nX_valid = train_merge[train_merge.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = train_merge[train_merge.date_block_num == 33]['item_cnt_month']\nX_test = train_merge[train_merge.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nprint('Shape X_train: {}'.format(X_train.shape))\nprint()\nprint('Shape y_train: {}'.format(y_train.shape))\nprint()\nprint('Shape X_valid: {}'.format(X_valid.shape))\nprint()\nprint('Shape y_valid: {}'.format(y_valid.shape))\nprint()\nprint('Shape X_test: {}'.format(X_test.shape))","bac17f4d":"cat_features = ['country_part', \n                'item_category_common',\n                'item_category_id', \n                'city_code']\n\ncatboost = CatBoostRegressor(random_state=1, \n                             iterations=2000, verbose=200, depth = 4, \n                             learning_rate=0.01, l2_leaf_reg=7,\n                             max_leaves = 2047, min_data_in_leaf = 1,\n                             subsample = 0.7,\n                             loss_function='RMSE', eval_metric='RMSE',\n                             task_type='GPU',early_stopping_rounds=30,\n                             grow_policy='Lossguide', bootstrap_type='Poisson',\n                            cat_features=cat_features)","d61063f0":"catboost.fit(X_train, y_train)","f29ff026":"# save catboost model\npickle.dump(catboost, open('catboost.sav', 'wb'))","1fdf5782":"predict_cb_train = catboost.predict(X_train)\npredict_cb_valid = catboost.predict(X_valid)\npredict_cb_test = catboost.predict(X_test)\nprint('Train rmse for Catboost:', np.sqrt(mean_squared_error(y_train, predict_cb_train)))\nprint('Validation rmse for Catboost:', np.sqrt(mean_squared_error(y_valid, predict_cb_valid)))","a499e1cc":"lr_features = ['item_target_enc_lag_1', 'item_target_enc_lag_2',\n              'item_loc_target_enc_lag_1', 'item_loc_target_enc_lag_2', 'item_loc_target_enc_lag_3', \n               'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3']\nlr_train = X_train[lr_features]\nlr_val = X_valid[lr_features]\nlr_test = X_test[lr_features]\n\n\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_valid = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)\n\nlr_level1 = LinearRegression()\nlr_level1.fit(lr_train, y_train)","a6dc0a1e":"# save linear regression model\npickle.dump(lr_level1, open('lr_level1.sav', 'wb'))","b2ab4d4b":"predict_lr_train = lr_level1.predict(lr_train)\npredict_lr_valid = lr_level1.predict(lr_valid)\npredict_lr_test = lr_level1.predict(lr_test)\nprint('Train rmse for LinearRegression:', np.sqrt(mean_squared_error(y_train, predict_lr_train)))\nprint('Validation rmse for LinearRegression:', np.sqrt(mean_squared_error(y_valid, predict_lr_valid)))","3e33e046":"# define build model function\ndef build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n                     categorical_feature=cat_features)\n    return model\n\n# define parameters\nparams = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1023,\n    'min_data_in_leaf':10,\n    'feature_fraction': 0.7,\n    'learning_rate': 0.01,\n    'num_rounds': 2000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\n# fit model\nlgb_model = build_lgb_model(params, X_train, X_valid, y_train, y_valid, cat_features)","68c450c3":"# save lightgbm model\npickle.dump(lgb_model, open('lgb_1.sav', 'wb'))","0358122a":"predict_lgb_train = lgb_model.predict(X_train)\npredict_lgb_valid = lgb_model.predict(X_valid)\npredict_lgb_test = lgb_model.predict(X_test)\nprint('Train rmse for LightGBM:', np.sqrt(mean_squared_error(y_train, predict_lgb_train)))\nprint('Validation rmse for LightGBM:', np.sqrt(mean_squared_error(y_valid, predict_lgb_valid)))","a5c5ac0d":"rf = RandomForestRegressor(random_state = 1, max_depth=10, max_features='sqrt', min_samples_leaf=7,\n                      min_samples_split=11, n_estimators=75)","a177af18":"rf.fit(X_train, y_train)","f4ee9899":"pickle.dump(rf, open('rf.sav', 'wb'))","13e6aca3":"predict_rf_train = rf.predict(X_train)\npredict_rf_valid = rf.predict(X_valid)\npredict_rf_test = rf.predict(X_test)\nprint('Train rmse for RandomForest:', np.sqrt(mean_squared_error(y_train, predict_rf_train)))\nprint('Validation rmse for RandomForest:', np.sqrt(mean_squared_error(y_valid, predict_rf_valid)))","270712ce":"# dataset that will be the train set of the ensemble model\nfirst_level = pd.DataFrame(predict_cb_valid, columns=['catboost'])\nfirst_level['lightgbm'] = predict_lgb_valid\nfirst_level['random_forest'] = predict_rf_valid\nfirst_level['linear_regression'] = predict_lr_valid\nfirst_level['label'] = y_valid.values\nfirst_level.head(5)","a7f1eb9a":"# dataset that will be the test set of the ensemble model\nfirst_level_test = pd.DataFrame(predict_cb_test, columns=['catboost'])\nfirst_level_test['lightgbm'] = predict_lgb_test\nfirst_level_test['random_forest'] = predict_rf_test\nfirst_level_test['linear_regression'] = predict_lr_test\nfirst_level_test.head()","c5648043":"meta_model = LinearRegression(n_jobs=-1)","001a6ab2":"X_first_level = first_level.drop('label', axis=1)\ny_first_level = first_level['label']","f2e50036":"# trained on validation set using the 1st level models predictions as features\nmeta_model.fit(X_first_level, y_first_level)","cf4cb506":"# make predictions on test set using the 1st level models predictions as feature\nensemble_pred_test = meta_model.predict(first_level_test).clip(0, 20)","a113ca49":"# save ensemble model\npickle.dump(meta_model, open('meta_model.sav', 'wb'))","3a069b54":"lgb_submission = pd.DataFrame({\n    'ID': test.index, \n    'item_cnt_month': lgb_model.predict(X_test).clip(0, 20)\n})\nlgb_submission.to_csv('lgb_submission.csv', index=False)\nprint(lgb_submission)\n\nensemble_submission = pd.DataFrame({\n    'ID': test.index, \n    'item_cnt_month': ensemble_pred_test\n})\nensemble_submission.to_csv('ensemble_submission.csv', index=False)\nprint(ensemble_submission)","22a2a072":"**Dataset item_categories**","3426d261":"# **Datasets research**","5700fbb7":"**Catboost**","afd93cda":"**Ensemble model**","d855fbe8":"**Prepare items features**\n\nCategorization of products.","cb4b8e01":"**LightGBM**","2687f828":"**Prepare test**","13050d93":"**Dataset items**","0507b1fc":"**Prepare shops features**\n\nExtract and encode the names of cities. Add new features - coordinates of cities and parts of the country.","7fd19a16":"**Total score**\n* LightGBM model (public score) - 0.8981\n* LightGBM model (private score) - 0.9120\n* Ensemble model (public score) - 0.9050\n* Ensemble model (private score) - 0.9070","0d3d947a":"**Dataset submission**","40812967":"Ensemble architecture:\n1st level:\n* Catboost\n* XGBM\n* Random forest\n* Linear Regression\n\n2nd level:\n* Linear Regression","7f2deb0a":"**RandomForest**","f4d368ed":"**Aggregate train data**\n\nSince the test data is generated with combination of shops and items, we have to restructure train data to match the test data generation.","541c4aaa":"**Dataset train**","4868be42":"**Data Leakage**\n\nAround 42% of training shop_id ~ item_id pairs are present in test set, but I don't use it.","3d0cb348":"# **Fit models**","3ff372dd":"**Generate lag feature and mean encoding**","c6051986":"**Merge train and item datasets**","54ad0dd1":"# **Prepare and feature engineering data**","28eeaae4":"**LinearRegression**","f5b0a87b":"**Predict and submit task**","0d45d5fd":"**Dataset test**","a6767b16":"**Dataset shops**"}}