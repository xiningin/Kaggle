{"cell_type":{"289dcd30":"code","61228302":"code","7f2d2267":"code","1cc2c10c":"code","43ba9ff6":"code","45876b86":"code","8f377b9b":"code","dacdd108":"code","fa5df078":"code","11426827":"code","b87637ab":"code","b8eeab0e":"code","1fb9ca53":"code","c4d2166d":"code","5d6ff1e5":"code","b7987321":"code","b8a07807":"code","4fb7d973":"code","f0c37109":"code","b993207a":"code","ac68c9cd":"code","9fa091d7":"code","81bf093a":"code","d04350ec":"code","86887989":"code","c27d7d51":"code","8d3b9ce5":"code","42076c8e":"code","ac6d1755":"code","63376e1f":"code","4e32b5b7":"code","ac790f11":"code","f4f65f28":"code","f2f667e0":"code","742d7b97":"code","38f01bbc":"code","75bd29a6":"code","e2f1d0a6":"code","ed123e29":"code","c3505770":"code","42b8f5c3":"code","70312270":"code","13a22233":"code","fbe40b25":"markdown","06d2ae35":"markdown","e7088480":"markdown","a5dae146":"markdown","2bd2f478":"markdown","e282a670":"markdown","a1126f2d":"markdown","8eb2e8e1":"markdown"},"source":{"289dcd30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61228302":"data = pd.read_csv(r'\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv')\ndata","7f2d2267":"import random\ndata = data.sample(frac=0.25)\ndata","1cc2c10c":"seq_len = data.text.apply(lambda x: len(x.split()))\nseq_len","43ba9ff6":"import matplotlib.pyplot as plt\nimport seaborn as sns","45876b86":"sns.set_theme(\n    context='notebook',\n    style='darkgrid',\n    palette='deep',\n    font='sans-serif',\n    font_scale=1,\n    color_codes=True,\n    rc=None,\n)\n\nplt.figure(figsize = (10,12))\nsns.distplot(seq_len) #plot distribution","8f377b9b":"seql = 400","dacdd108":"from transformers import AutoTokenizer","fa5df078":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n","11426827":"#encode_plus method\n\ntokens = tokenizer.encode_plus(\"hello world\", max_length=seql,\n                              truncation=True, padding=\"max_length\",\n                              add_special_tokens=True, return_token_type_ids=False,\n                               return_attention_mask=True, return_tensors=\"tf\")","b87637ab":"tokens","b8eeab0e":"x_ids = np.zeros((len(data),seql))\nx_mask = np.zeros((len(data),seql))\nx_ids.shape","1fb9ca53":"\nfor i, sentence in enumerate(data.text):\n    print(i,\"\\n\",sentence)\n    print(\"\\n\\n\")\n    if i==5:\n        break\n    ","c4d2166d":"for i, sentence in enumerate(data.text):\n    tokens = tokenizer.encode_plus(sentence, max_length=seql,\n                              truncation=True, padding=\"max_length\",\n                              add_special_tokens=True, return_token_type_ids=False,\n                               return_attention_mask=True, return_tensors=\"tf\")\n    x_ids[i,:],x_mask[i,:] = tokens[\"input_ids\"], tokens[\"attention_mask\"]","5d6ff1e5":"x_ids","b7987321":"x_mask","b8a07807":"data.label.unique()","4fb7d973":"label_ar = data.label.values\nlabel_ar","f0c37109":"label_ar.size","b993207a":"labels = np.zeros((label_ar.size,label_ar.max()+1))\nlabels.shape","ac68c9cd":"#ONE HOT ENCODING\nlabels[np.arange(label_ar.size),label_ar] = 1 #np.arange(label_ar.size):to create values from 0 to 10,000 which is the array size here.","9fa091d7":"labels","81bf093a":"#STORING\nwith open(\"Xids.npy\",\"wb\") as f:\n    np.save(f,x_ids)\nwith open(\"Xmask.npy\",\"wb\") as f:\n    np.save(f,x_mask)\nwith open(\"Labels.npy\",\"wb\") as f:\n    np.save(f,labels)\n    \n#DELETING\ndel x_ids,x_mask,labels\n    ","d04350ec":"labels","86887989":"#LOADING BACK\nwith open(\"Xids.npy\",\"rb\") as fp:\n    x_ids = np.load(fp)\nwith open(\"Xmask.npy\",\"rb\") as fp:\n    x_mask = np.load(fp)\nwith open(\"Labels.npy\",\"rb\") as fp:\n    labels = np.load(fp)","c27d7d51":"labels","8d3b9ce5":"import tensorflow as tf\n\"\"\"tf.config.experimental.list_physical_devices(\"GPU\")\"\"\"","42076c8e":"dataset = tf.data.Dataset.from_tensor_slices((x_ids,x_mask,labels))\n#dataset will be generated in a tuple-like format with each tuple having (x_ids array,x_mask array,label array)","ac6d1755":"#viewing an element of dataset\n\nfor i in dataset.take(1):\n    print(i)","63376e1f":"def map_func(input_id,masks,labels):\n    return {'Input_id': input_id, 'Attention_mask':masks}, labels","4e32b5b7":"dataset = dataset.map(map_func)","ac790f11":"for i in dataset.take(1):\n    print(i)","f4f65f28":"dataset = dataset.shuffle(100000).batch(64)","f2f667e0":"dataset_len = len(list(dataset))\ndataset_len","742d7b97":"split_ratio = 0.8\n\ntrain = dataset.take(int(dataset_len*split_ratio)) #opposite to take() is skip().\nvalid = dataset.skip(int(dataset_len*split_ratio)) #while take(c) takes c samples, skip(c) takes (total number of samples minus c)\n\ndel dataset","38f01bbc":"#initialize BERT\nfrom transformers import TFAutoModel","75bd29a6":"bert = TFAutoModel.from_pretrained(\"bert-base-cased\")","e2f1d0a6":"input_ids = tf.keras.layers.Input(shape=(seql,), name=\"input_ids\",dtype='int32')\nmask_id = tf.keras.layers.Input(shape=(seql,), name=\"Attention_mask\",dtype='int32')\n\nembeddings = bert(input_ids, attention_mask = mask_id)[0]   #the second tensor  called the \"pooler output\" that we ignore is basically the last hidden state run through a feed forward or linear activation function and pooled. \n                                                            #Since we intend to pool it manually, we ignore the pooler output.\n    \n#MANUAL POOLING    \nX = tf.keras.layers.GlobalMaxPool1D()(embeddings)","ed123e29":"#Normalizing the output to get better results for the trained model\nX = tf.keras.layers.BatchNormalization()(X)","c3505770":"#MODELING\nX = tf.keras.layers.Dense(128, activation= \"relu\")(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(32, activation= \"relu\")(X)\ny = tf.keras.layers.Dense(2, activation= \"softmax\",name= \"outputs\")(X)\n\nmodel = tf.keras.Model(inputs=[input_ids, mask_id], outputs=y)\n\n\n#freezing the BERT model by freezing the third layer\n#model.layers[2].trainable = False","42b8f5c3":"model.summary()","70312270":"opt = tf.keras.optimizers.Adam(0.01)\nloss = tf.keras.losses.BinaryCrossentropy()\nacc = tf.keras.metrics.BinaryAccuracy(\"accuracy\")\n\nmodel.compile(optimizer=opt, loss=loss, metrics=[acc])","13a22233":"history = model.fit(train,valid,epochs=200)","fbe40b25":"**padding = \"max_length\"** : The tokenizer must pad all the sentences till seql, the best sequence length we decided earlier.\n\n**add_special_tokens=True** : BERT comes with a few special tokens that can be added to our base tokenizer. Here this will add the start sequence tokens, the end sequence tokens and then add all our padding values.\n\nBy default, this encode_plus method gives output in the form of input_ids and token_type_ids. We just require the input_ids but the token_type_ids is a redundant information.\n\n\n\n**return_token_type_ids=False** : We set it to false to refrain the method from returning it.\n\n**return_attention_mask=True** : To obtain the attention masked tensor showing BERT what tokens to calculate attention for and which all to ignore.\n\n**return_tensors=\"tf\"** : Since we are working with tensorflow.","06d2ae35":"**UNDERSTANDING THE OUTPUT**\n\n\n\n***ARRAY1*** : *Sequence for the text*\n\n101= Start_of_sequence token used by BERT\n\n19082= Token for the word \"hello\"\n\n1362=Token for the word \"world\"\n\n102= End_of_sequence token used by BERT\n\nremaining zeros= padding tokens.\n\n\n\n***ARRAY2*** : *attention masked tensor*\n\n1 shows BERT to pay attention to the corresponding token.\n\n0 shows BERT to ignore the corresponding token.\n","e7088480":"An alternative would be \n\n[tokenizer = AutoTokenizer.from_pretrained(\"bert-base-**uncased**\")] \n\nthat converts all to lower case but here we need to pick up on uppercases that depicts customer's excitement at times and BERT is able to infer on such sentiments as well.","a5dae146":"Now for the **labels**, we use one-hot encoder.","2bd2f478":"BUILDING OUR MODEL","e282a670":"We can cut it around 1000 for retaining maximum information but that will lead to a very time-taking modeling. It looks fair to cut the best seq_len around 400.","a1126f2d":"Now we use some vizualising tools to decide how long our sentence sequence should be so that be do not loose much data and our model works effectively as well.","8eb2e8e1":"**STORING THESE DATA FOR LATER USE TO CONTINUE TRAINING**"}}