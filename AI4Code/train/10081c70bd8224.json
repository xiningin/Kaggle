{"cell_type":{"32c25656":"code","02738791":"code","06480631":"code","19a679fe":"code","a735ed40":"code","be7cac28":"code","cbd9af95":"code","a8e93e93":"code","321c91c7":"code","5fded9b1":"code","5f26df46":"code","b97dd950":"code","78d54f4a":"code","e4ac0b3d":"markdown","db42f124":"markdown","69e1bccb":"markdown","fb946871":"markdown","73a9f09c":"markdown","9a44b769":"markdown"},"source":{"32c25656":"import pandas as pd\nimport numpy as np\nimport os\n\nimport networkx as nx\n\nfrom tqdm import tqdm_notebook\nimport tqdm\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n%matplotlib inline\n\npd.options.display.float_format = '{:,.3f}'.format","02738791":"# load data\ndata_dir = \"\/kaggle\/input\/prozorro-public-procurement-dataset\/\"\ndata_competitive = \"Competitive_procurements.csv\"\ndata_comp = pd.read_csv(os.path.join(data_dir, data_competitive), index_col=0, dtype=\"str\")\n\n# change variables format\ndata_comp[[\"lot_initial_value\", \"lot_final_value\"]] = data_comp[[\"lot_initial_value\", \"lot_final_value\"]].astype(float)\ndata_comp.index = pd.to_datetime(data_comp.index)\ndata_comp.loc[:, 'lot_announce_year'] = data_comp.lot_announce_year.astype('int')\ndata_comp.loc[:, 'supplier_dummy'] = data_comp.supplier_dummy.astype('int')","06480631":"data_comp = data_copy.copy()","19a679fe":"# Keep above threshold tenders only (they are always competitive, i.e. >=2 participants per auction)\nobs_before = len(data_comp)\ndata_comp = data_comp[data_comp[\"lot_procur_type\"].isin(['Above Threshold UA', 'Above Threshold EU'])]\nprint(f\"Keep only above threshold tenders\")\nprint(f\"Discarded {obs_before - len(data_comp):,.0f} observations. Left {len(data_comp):,.0f} observations.\")\n\n# Keep only first and last \"full\" year. We will explore the difference between them\n# In 2016 Prozorro was only started to be adopted\ndisplay(data_comp.groupby(\"lot_announce_year\")['lot_id'].count())\nobs_before = len(data_comp)\ndata_comp = data_comp[data_comp[\"lot_announce_year\"].isin([2017, 2019])]\nprint(f\"Keep only first and last 'full' year\")\nprint(f\"Discarded {obs_before - len(data_comp):,.0f} observations. Left {len(data_comp):,.0f} observations.\\n\")\n\n# Keep markets with at least 10 tenders in 2017 or 2019\n# For markets with a very small number of tenders, one additional tender can cause\n# a big change in the market structure. We will look only at more 'robust' markets\n# At the same time we do not want to miss cases, when a new market emerged or stopped functioning.\n# So our criteria is 10 tenders in 2017, 2019 or in both.\nmarkets_before = data_comp[\"lot_cpv_4_digs\"].nunique()\nobs_before = len(data_comp)\n\nselection = pd.pivot_table(data=data_comp, columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"lot_id\", aggfunc=\"nunique\")\nselected_list = selection[(selection>=10).sum(axis=1)>=1].index\ndata_comp = data_comp[data_comp[\"lot_cpv_4_digs\"].isin(selected_list)]\n\nprint(f\"Keep markets with at least 10 tenders in 2017 or in 2019\")\nprint(f\"Discarded {obs_before - len(data_comp):,.0f} observations. Left {len(data_comp):,.0f} observations.\")\nprint(f\"Discarded {markets_before - data_comp['lot_cpv_4_digs'].nunique():,.0f} markets. Left {data_comp['lot_cpv_4_digs'].nunique():,.0f} markets.\")\n\nprint(f\"The shape of the DF: {data_comp.shape[0]:,.0f} rows, {data_comp.shape[1]:,.0f} columns\")\ndisplay(data_comp.head(5).T)","a735ed40":"# number of procurements per market per year\ndf_metrics = pd.pivot_table(data=data_comp, columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"lot_id\", aggfunc=\"nunique\")\ndf_metrics.columns = [\"_\".join([\"tenders_number\", str(col)]) for col in df_metrics.columns]\ndf_metrics.fillna(0, inplace=True)\n\nfor col in df_metrics:\n    df_metrics.loc[:, col] = df_metrics.loc[:, col].astype('int')\n    \n\n# sum of contracts per market per year\ndf_contracts = pd.pivot_table(data=data_comp.query(\"supplier_dummy == 1\"), columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"lot_final_value\", aggfunc=\"sum\")\ndf_contracts.columns = [\"_\".join([\"contracts_value\", str(col)]) for col in df_contracts.columns]\ndf_contracts.fillna(0, inplace=True)\n\n\n# median of contract per market per year\ndf_contracts_median = pd.pivot_table(data=data_comp.query(\"supplier_dummy == 1\"), columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"lot_final_value\", aggfunc=\"median\")\ndf_contracts_median.columns = [\"_\".join([\"contract_median\", str(col)]) for col in df_contracts_median.columns]\ndf_contracts_median.fillna(0, inplace=True)\n\n\n# unique organizers per market per year\ndf_organizers = pd.pivot_table(data=data_comp, columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"organizer_code\", aggfunc=\"nunique\")\ndf_organizers.columns = [\"_\".join([\"org_number\", str(col)]) for col in df_organizers.columns]\ndf_organizers.fillna(0, inplace=True)\n\nfor col in df_organizers:\n    df_organizers.loc[:, col] = df_organizers.loc[:, col].astype('int')\n\n\n# unique participants per market per year\ndf_participants = pd.pivot_table(data=data_comp, columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"participant_code\", aggfunc=\"nunique\")\ndf_participants.columns = [\"_\".join([\"part_number\", str(col)]) for col in df_participants.columns]\ndf_participants.fillna(0, inplace=True)\n\nfor col in df_participants:\n    df_participants.loc[:, col] = df_participants.loc[:, col].astype('int')\n    \n    \n# unique suppliers per market per year\ndf_winners = pd.pivot_table(data=data_comp.query(\"supplier_dummy == 1\"), columns=\"lot_announce_year\", index=\"lot_cpv_4_digs\", values=\"participant_code\", aggfunc=\"nunique\")\ndf_winners.columns = [\"_\".join([\"winners_number\", str(col)]) for col in df_winners.columns]\ndf_winners.fillna(0, inplace=True)\n\nfor col in df_winners:\n    df_winners.loc[:, col] = df_winners.loc[:, col].astype('int')\n    \n    \n# collect metrics together\ndf_metrics = pd.merge(df_metrics, df_contracts, left_index=True, right_index=True)\ndf_metrics = pd.merge(df_metrics, df_contracts_median, left_index=True, right_index=True)\ndf_metrics = pd.merge(df_metrics, df_organizers, left_index=True, right_index=True)\ndf_metrics = pd.merge(df_metrics, df_participants, left_index=True, right_index=True)\ndf_metrics = pd.merge(df_metrics, df_winners, left_index=True, right_index=True)","be7cac28":"def calc_hirshman_per_year(df, entity_column):\n    '''\n    Calculates Herfindahl\u2013Hirschman index per year for a selected group of entities defined by entity_column\n    '''\n    df_pivot = pd.pivot_table(data=df, columns=\"lot_announce_year\",\n                              index=entity_column, values=\"lot_final_value\", aggfunc=\"sum\")\n    df_pivot.fillna(0, inplace=True)\n\n    # market share of each entity\n    df_pivot = df_pivot \/ df_pivot.sum()\n\n    # square of market share\n    df_pivot = df_pivot * df_pivot\n    \n    # sum columns - get index\n    res = pd.DataFrame(df_pivot.sum()).T\n    res.index = [f'hirsh index {entity_column}']\n    \n    if 2017 not in df_pivot.columns:\n        res.loc[:, 2017] = None\n    \n    if 2019 not in df_pivot.columns:\n        res.loc[:, 2019] = None\n\n    return res","cbd9af95":"def analyze_market(data_sub, market, df_summary=None):\n    '''\n    Calculate market metrics that need to be calculated separately (hard to vectorize) \n    '''\n    \n    # number of participants that were active in both years\n    part_2017 = set(data_sub.query(\"lot_announce_year == 2017\").participant_code.unique())\n    part_2019 = set(data_sub.query(\"lot_announce_year == 2019\").participant_code.unique())\n    part_intersection = part_2019.intersection(part_2017)\n    part_persistent = len(part_intersection)\n\n\n    # number of organizers that were active in both years\n    org_2017 = set(data_sub.query(\"lot_announce_year == 2017\").organizer_code.unique())\n    org_2019 = set(data_sub.query(\"lot_announce_year == 2019\").organizer_code.unique())\n    org_intersection = org_2019.intersection(org_2017)\n    org_persistent = len(org_intersection)\n\n\n    # supplier with the largest number of contracts\n    max_number_supplier = pd.pivot_table(data=data_sub.query(\"supplier_dummy == 1\"), \n                                         columns=\"lot_announce_year\", index=\"participant_code\", values=\"lot_id\", \n                                         aggfunc=\"nunique\")\n\n    max_number_supplier_2017_name = max_number_supplier[2017].idxmax() if 2017 in max_number_supplier.columns else None\n    max_number_supplier_2017_value = max_number_supplier[2017].max() if 2017 in max_number_supplier.columns else None\n\n    max_number_supplier_2019_name = max_number_supplier[2019].idxmax() if 2019 in max_number_supplier.columns else None\n    max_number_supplier_2019_value = max_number_supplier[2019].max() if 2019 in max_number_supplier.columns else None\n\n\n    # supplier with the largest value of contracts\n    max_value_supplier = pd.pivot_table(data=data_sub.query(\"supplier_dummy == 1\"), \n                                        columns=\"lot_announce_year\", index=\"participant_code\", values=\"lot_final_value\", \n                                        aggfunc=\"sum\")\n\n    max_value_supplier_2017_name = max_value_supplier[2017].idxmax() if 2017 in max_value_supplier.columns else None\n    max_value_supplier_2017_value = max_value_supplier[2017].max() if 2017 in max_value_supplier.columns else None\n\n    max_value_supplier_2019_name = max_value_supplier[2019].idxmax() if 2019 in max_value_supplier.columns else None\n    max_value_supplier_2019_value = max_value_supplier[2019].max() if 2019 in max_value_supplier.columns else None\n\n\n    # organizer with the largest number of contracts\n    max_number_organizer = pd.pivot_table(data=data_sub.query(\"supplier_dummy == 1\"), \n                                         columns=\"lot_announce_year\", index=\"organizer_code\", values=\"lot_id\", \n                                         aggfunc=\"nunique\")\n\n    max_number_organizer_2017_name = max_number_organizer[2017].idxmax() if 2017 in max_number_organizer.columns else None\n    max_number_organizer_2017_value = max_number_organizer[2017].max() if 2017 in max_number_organizer.columns else None\n\n    max_number_organizer_2019_name = max_number_organizer[2019].idxmax() if 2019 in max_number_organizer.columns else None\n    max_number_organizer_2019_value = max_number_organizer[2019].max() if 2019 in max_number_organizer.columns else None\n\n\n    # organizer with the largest value of contracts\n    max_value_organizer = pd.pivot_table(data=data_sub.query(\"supplier_dummy == 1\"), \n                                        columns=\"lot_announce_year\", index=\"organizer_code\", values=\"lot_final_value\", \n                                        aggfunc=\"sum\")\n\n    max_value_organizer_2017_name = max_value_organizer[2017].idxmax() if 2017 in max_value_organizer.columns else None\n    max_value_organizer_2017_value = max_value_organizer[2017].max() if 2017 in max_value_organizer.columns else None\n\n    max_value_organizer_2019_name = max_value_organizer[2019].idxmax() if 2019 in max_value_organizer.columns else None\n    max_value_organizer_2019_value = max_value_organizer[2019].max() if 2019 in max_value_organizer.columns else None\n\n\n    # Herfindahl\u2013Hirschman index suppliers\n    hirsh_suppliers = calc_hirshman_per_year(data_sub.query(\"supplier_dummy == 1\"), \"participant_code\")\n\n\n    # Herfindahl\u2013Hirschman index organizers region\n    hirsh_org_region = calc_hirshman_per_year(data_sub.query(\"supplier_dummy == 1\"), \"organizer_region\")\n\n\n    # sum of and number of contracts wone by suppliers from region other than organizers\n    other_reg_value = data_sub.query(\"organizer_region != participant_region and supplier_dummy == 1\")['lot_final_value'].sum()\n    other_reg_number = data_sub.query(\"organizer_region != participant_region and supplier_dummy == 1\")['lot_id'].nunique()\n\n\n    # collect all metrics into dataframe\n    market_descr = pd.DataFrame([part_persistent, org_persistent, \n\n                                 max_number_supplier_2017_name, max_number_supplier_2017_value,\n                                 max_number_supplier_2019_name, max_number_supplier_2019_value, \n                                 max_value_supplier_2017_name, max_value_supplier_2017_value,\n                                 max_value_supplier_2019_name, max_value_supplier_2019_value,\n\n                                 max_number_organizer_2017_name, max_number_organizer_2017_value,\n                                 max_number_organizer_2019_name, max_number_organizer_2019_value, \n                                 max_value_organizer_2017_name, max_value_organizer_2017_value,\n                                 max_value_organizer_2019_name, max_value_organizer_2019_value,\n\n                                 hirsh_suppliers[2017][0], hirsh_suppliers[2019][0],\n                                 hirsh_org_region[2017][0], hirsh_org_region[2019][0],\n\n                                 other_reg_value, other_reg_number]).T\n\n    market_descr.columns = ['part_pers_number', 'org_pers_number',\n\n                            'supplier_max_contracts_2017_id', 'supplier_max_contracts_2017',\n                            'supplier_max_contracts_2019_id', 'supplier_max_contracts_2019',\n                            'supplier_max_value_2017_id', 'supplier_max_value_2017',\n                            'supplier_max_value_2019_id', 'supplier_max_value_2019',\n\n                            'org_max_contracts_2017_id', 'org_max_contracts_2017',\n                            'org_max_contracts_2019_id', 'org_max_contracts_2019',\n                            'org_max_value_2017_id', 'org_max_value_2017',\n                            'org_max_value_2019_id', 'org_max_value_2019',\n\n                            'hirsh_suppliers_2017', 'hirsh_suppliers_2019',\n                            'hirsh_regions_2017', 'hirsh_regionss_2019',\n\n                            'other_region_value', 'other_region_number']\n\n    market_descr.index = [market]\n    market_descr.index.name = 'lot_cpv_4_digs'\n    \n    if df_summary is not None:\n        return pd.concat([df_summary, market_descr])\n    else:\n        return market_descr","a8e93e93":"first_market = True\ncounter = 0\n\nfor market in data_comp.lot_cpv_4_digs.unique():\n    \n    print(counter, market)\n    df_sub = data_comp.query(f'lot_cpv_4_digs == \"{market}\"')\n    \n    if first_market:\n        df_summary = analyze_market(df_sub, market)\n        first_market = False\n    else:\n        df_summary = analyze_market(df_sub, market, df_summary)\n        \n    counter += 1","321c91c7":"# combine two dataframes with metrics\ndf_metrics = pd.merge(df_metrics, df_summary, left_index=True, right_index=True)\ndf_metrics.to_csv('market_metrics.csv')\n\ndisplay(df_metrics.head().T)","5fded9b1":"# Let's create the function  for creating the network\ndef making_graph_1_mode(df):\n    \"\"\"The function takes the df and creates the 1-mode network o\u0430 tender participants.\n       Node - tender participant, edge - participation in tender organized by particular public entity.\n       For example, the two companies are connected if they both particpated in tender organized by one public entity\"\"\"\n    \n    # Create the table where columns are public entities codes (organizer_code) and index - tender participants codes (participant_code)\n    df = df.pivot_table(values = \"lot_final_value\", index=\"participant_code\", columns=\"organizer_code\", aggfunc=\"count\").fillna(0)\n    # Dot product that 'connects' all the participants\n    df = df.dot(df.T)\n    # Simplification of the received matrix\n    df = df.astype(int)\n    np.fill_diagonal(df.values, 0)\n    # Create the graph from the received adjacency matrix\n    G = nx.from_pandas_adjacency(df)\n    \n    return G","5f26df46":"# df_graphs = data_comp.groupby([data_comp[\"lot_cpv_4_digs\"], data_comp[\"lot_announce_year\"]]).apply(making_graph_1_mode).reset_index()\n# df_graphs.rename(columns={0:\"graph\"}, inplace=True)\n\n# print(\"The shape of the DF:\", df_graphs.shape)\n# df_graphs.head(3)","b97dd950":"# def clustering_measures_1_mode(G):\n#     try:\n#         average_degree_centrality = np.mean(list(nx.degree_centrality(G).values()))\n#     except:\n#         average_degree_centrality = np.nan\n        \n#     try:\n#         average_betweenness_centrality = np.mean(list(nx.betweenness_centrality(G).values()))\n#     except:\n#         average_betweenness_centrality = np.nan\n    \n#     # Components\n\n#     comp_list = sorted(nx.connected_components(G), key=len, reverse=True) \n#     comp_size = [len(comp) for comp in comp_list]\n    \n#     G_largest_comp = G.subgraph(comp_list[0])\n#     edges_largest_comp = G_largest_comp.number_of_edges()\n#     centrality_largest_node_largest_comp = max(nx.degree_centrality(G_largest_comp).values())\n\n#     max_node_largest_comp = max(nx.degree_centrality(G_largest_comp))\n#     num_edges_largest_node_largest_component = G_largest_comp.degree(max_node_largest_comp)\n    \n#     return [G.number_of_nodes(), G.number_of_edges(), average_degree_centrality,\n#             nx.average_clustering(G), average_betweenness_centrality, nx.transitivity(G),\n#             nx.number_connected_components(G), comp_size[0], comp_size[0]\/G.number_of_nodes(), np.mean(comp_size), np.median(comp_size),\n#             edges_largest_comp, centrality_largest_node_largest_comp, num_edges_largest_node_largest_component]\n","78d54f4a":"# df_graph_measures_1_mode = df_graphs[\"graph\"].apply(clustering_measures_1_mode)\n# df_graph_measures_1_mode = pd.DataFrame((df_graph_measures_1_mode.tolist()), columns = [\"number_of_nodes\", \"number_of_edges\", \"average_degree_centrality\",\n#                                                                                         \"average_clustering\", \"average_betweenness_centrality\", \"transitivity\",\n#                                                                                         \"number_connected_components\", \"size_largest_component\", \"share_largest_component\",\n#                                                                                         \"average_component_size\", \"median_compomemt_size\", \n#                                                                                         \"edges_largest_comp\", \"centrality_largest_node_largest_comp\", \"num_edges_largest_node_largest_component\"])\n\n\n# print(\"The shape of the DF:\", df_graph_measures_1_mode.shape)\n# df_graph_measures_1_mode.head()","e4ac0b3d":"# Sand box (1-mode network)","db42f124":"# Load data for all competitive porcurements","69e1bccb":"# Non-network metrics","fb946871":"### Filtering","73a9f09c":"### A detailed analysis for each market","9a44b769":"# Setup"}}