{"cell_type":{"998955c0":"code","17830ea5":"code","ef6c9e90":"code","0887de81":"code","46244438":"code","ee2c4bd6":"code","35d0f065":"code","eb50545f":"code","d0bb0dc3":"code","260dd712":"code","1b29698f":"code","c9877287":"code","1711d6b4":"code","ee066e6f":"code","b1e38b04":"code","f896f234":"code","05341ca4":"code","710ca39c":"code","a522e029":"code","9f7d7b5f":"code","81154267":"code","5e7e71d3":"code","0caf8c91":"code","69565767":"code","1eda5d9c":"code","91a053d5":"code","e7debf04":"code","e4432e44":"code","501cedb0":"code","d3566bcc":"code","750651d8":"markdown","5f42ec0e":"markdown","b95c9ad8":"markdown","08dd8516":"markdown","fdda3f39":"markdown"},"source":{"998955c0":"# Feature slection using imporved Fscore ","17830ea5":"# about the data_set \n  #microarray data\n    #http:\/\/csse.szu.edu.cn\/staff\/zhuzx\/Datasets.html\n  #\"Zexuan Zhu, Y. S. Ong and M. Dash, \u201cMarkov Blanket-Embedded Genetic Algorithm for Gene Selection\u201d, Pattern Recognition, Vol. 49, No. 11, 3236-3248, 2007.\"","ef6c9e90":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0887de81":"from scipy.io import arff\nimport pandas as pd\ndata =pd.read_csv('\/kaggle\/input\/mll-data-csv\/csv_result-MLL.csv')\ndata.head()","46244438":"data.shape # there is 12583 feature present in this data set","ee2c4bd6":"data=data.dropna()\ndata=data.drop(columns=['id'])","35d0f065":"data.head()","eb50545f":"data.groupby(\"class\").size()","d0bb0dc3":"Class={\n    'ALL':    24,\n'AML' :   28,\n'MLL' :   20\n}","260dd712":"from IPython.display import Image\nImage(filename='\/kaggle\/input\/f-score\/f-score.png') # fscore ","1b29698f":"from IPython.display import Image\nImage(filename='\/kaggle\/input\/improved-fscore\/imporved  F score.png') # imporved f score","c9877287":"Activity_name=list(Class.keys())","1711d6b4":"Activity_name[0]\n","ee066e6f":"def D(col):\n    df=data[[col,'class']]\n    d_value=0.0\n    total_len=df.shape[0]\n    for ix in range(0,len(Activity_name)):\n        for ixx in range(ix+1,len(Activity_name)):\n\n            class_A=df[df['class']==Activity_name[ix]]\n            class_B=df[df['class']==Activity_name[ixx]]\n            #print(\"add\")\n            #print(((class_A.shape[0]+class_B.shape[0])*np.square(np.mean(class_A.iloc[:,0].values)-np.mean(class_B.iloc[:,0].values)))\/float(total_len))\n            #print(d_value)\n            #print(\"value\")\n            d_value=d_value+((class_A.shape[0]+class_B.shape[0])*(np.square(np.mean(class_A.iloc[:,0].values)-np.mean(class_B.iloc[:,0].values))))\/float(total_len)\n            #print(d_value)\n    return d_value","b1e38b04":"def S(col):\n    df=data[[col,'class']]\n    sx=0\n    for ix in range(0,len(Activity_name)):\n\n        in_class=df[df['class']==Activity_name[ix]]\n\n        means=(np.mean(in_class.iloc[:,0].values))\n\n        s=0\n        mins=np.square(in_class.iloc[0,0]-means) \n        maxs=np.square(in_class.iloc[0,0]-means) \n        #print(mins,maxs)\n\n        for ix in in_class.iloc[1:,0]:\n            s=s+np.square(ix-means)\n            m=np.square(ix-means)\n            #print('value of m',m)\n            if mins>m:\n                mins=m\n            if maxs<m:\n                maxs=m\n        s=(1\/float(in_class.shape[0]))*(s-mins)\/float(maxs-mins)\n        sx=sx+s\n    return sx\n","f896f234":"def Fdf(col):\n    Dx=D(col)\n    Sx=S(col)\n    return Dx\/Sx","05341ca4":"columns=data.columns[:-1]","710ca39c":"len(columns)","a522e029":"feature={}\nfor ix in columns:\n    #print(ix)\n    feature[ix]=Fdf(ix)","9f7d7b5f":"ns_df = pd.DataFrame(columns=['features','F_Scores'])\nns_df['features']=feature.keys()\nns_df['F_Scores']=feature.values()\nns_df_sorted = ns_df.sort_values(['F_Scores','features'], ascending =[False, True])","81154267":"import matplotlib.pyplot as plt\nplt.plot(list(ns_df_sorted.F_Scores[:200]),label ='F-score')\n#plt.axhline(y=, color='r', linestyle='-',label ='thershold')\n# plt.axvline(x=350, color='g', linestyle='-',label ='index no of features')\n# plt.legend()\n#plt.show()","5e7e71d3":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split","0caf8c91":"def no_feature(x):\n    fe=ns_df_sorted.iloc[:x,0]\n    X=data[fe]\n    y=data['class']\n    X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.4, random_state=42)\n    svc=svm.LinearSVC()\n    svc.fit(X_train,y_train.astype('category').cat.codes)\n    acc=svc.score(X_test,y_test.astype('category').cat.codes)\n    return acc","69565767":"features=[10,20,40,50,60,70,80,100,120,180,200,250,300,400,500,600]\n","1eda5d9c":"accuracy=[]\nfor ix in features:\n    accuracy.append(no_feature(ix))","91a053d5":"import matplotlib.pyplot as plt\nplt.plot(features,accuracy,label ='accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('no of features')\nplt.legend()\nplt.show()","e7debf04":"# we select the 50 features with 100% acc","e4432e44":"x=50","501cedb0":"fe=ns_df_sorted.iloc[:x,0]\nX=data[fe]\ny=data['class']\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.4, random_state=42)\nsvc=svm.LinearSVC()\nsvc.fit(X_train,y_train.astype('category').cat.codes)\nacc=svc.score(X_test,y_test.astype('category').cat.codes)\nprint(acc)","d3566bcc":"y_pred=svc.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred,y_test.astype('category').cat.codes))","750651d8":"# about F-score \n\n#F-score is a simple technique which measures the\ndiscrimination of two sets of real numbers. Given training\nvectors. Given training vectors x k , k=1,2,\u2026.,m, if the\nnumber of positive and negative instances are n + and n \u2013\nrespectively, then the F-score of the ith feature is defined as\n\nwhole, positive, and negative datasets, respectively. The\nnumerator indicates the discrimination between the positive\nand negative sets, and the denominator indicates the one\nwithin each of the two sets. The larger the F-score is, the\nmore likely this feature is more discriminative.\n","5f42ec0e":"improved F-Score is a technique which measures the\ndiscrimination among more than two datasets. Given training\nvectors xk , k=1,2,\u2026,m, and the number of datasets(l>=2), if\nthe number of j\nth dataset is nj, j=1,2,\u2026,l, then the improved\nF-score of the ith feature is defined as ","b95c9ad8":"# about feature slection \n'''Feature selection aims at finding the most relevant features of a problem domain. It is very helpful in\nimproving computational speed and prediction accuracy. However, identification of useful features from\nhundreds or even thousands of related features is a nontrivial task. In this paper, we introduce a hybrid\nfeature selection method which combines two feature selection methods \u2013 the filters and the wrappers.\nCandidate features are first selected from the original feature set via computationally-efficient filters. The\ncandidate feature set is further refined by more accurate wrappers. This hybrid mechanism takes advan-\ntage of both the filters and the wrappers. The mechanism is examined by two bioinformatics problems,\nnamely, protein disordered region prediction and gene selection in microarray cancer data. Experimental\nresults show that equal or better prediction accuracy can be achieved with a smaller feature set. These\nfeature subsets can be obtained in a reasonable time period'''","08dd8516":"f-score for 2 class more then 2 class we use imporved f score ","fdda3f39":"\nThe numerator indicates the discrimination between each\ndataset, and denominator indicates the one within each of\ndataset. The larger the improved F-score is, the feature is\nmore likely to be discriminative \n (Xie & Wang, 2011)."}}