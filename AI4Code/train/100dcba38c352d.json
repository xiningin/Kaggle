{"cell_type":{"6a869269":"code","d9e73a32":"code","80262fee":"code","511a1259":"code","d36dd7aa":"code","982d28e8":"code","ffe6ac1c":"code","63a58006":"code","c9412796":"code","44329a0e":"code","5c8043ff":"code","edc3128b":"code","2c088acf":"code","dd3b1e2b":"code","e7083c68":"code","3e38b421":"code","907f2aa1":"code","3cbad399":"code","2757275e":"code","2a7b2a79":"code","ef813508":"code","5b3c5777":"code","2df1676b":"code","9572bcb4":"code","1b5a9a2a":"code","35c83d91":"code","8e7aead6":"code","29f0a5fc":"code","fd44c203":"code","5803cf38":"code","2575e0f5":"code","3b2b9a48":"code","974a76d6":"code","c16fb69a":"code","a420ef2b":"code","b662cfab":"code","14a42946":"code","61738a70":"code","8456fa35":"code","72906b9f":"code","6a3c2129":"code","2e185bcc":"markdown","118425cb":"markdown","4e746e72":"markdown","d9c4ddc0":"markdown","66798c6d":"markdown","ac39acbb":"markdown","60f83429":"markdown","93513400":"markdown","7b8039f4":"markdown","4e8d88ba":"markdown","37352394":"markdown","687ef01d":"markdown","cabad269":"markdown","7dc6be97":"markdown","37af7190":"markdown","3f040203":"markdown","063a4691":"markdown","c1d125ff":"markdown","978e7ca2":"markdown","7ffb756c":"markdown"},"source":{"6a869269":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torch import autograd\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\n\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nimport os\n\nimport matplotlib.pyplot as plt","d9e73a32":"DATA_DIR = '..\/input\/dogs-cats-images\/dataset'\nTRAIN_DIR = DATA_DIR + \"\/training_set\"\nTEST_DIR = DATA_DIR + \"\/test_set\"\n","80262fee":"classes = os.listdir(TRAIN_DIR)\nprint(classes)","511a1259":"cats = os.listdir(TRAIN_DIR + \"\/cats\")\nprint('No. of training examples for cats:', len(cats))\nprint(cats[:5])","d36dd7aa":"dogs = os.listdir(TRAIN_DIR + \"\/dogs\")\nprint('No. of training examples for dogs:', len(dogs))\nprint(dogs[:5])","982d28e8":"cats = os.listdir(TEST_DIR + \"\/cats\")\nprint('No. of testing examples for cats:', len(cats))\nprint(cats[:5])","ffe6ac1c":"cats = os.listdir(TEST_DIR + \"\/dogs\")\nprint('No. of testing examples for dogs:', len(dogs))\nprint(dogs[:5])","63a58006":"from torchvision.datasets import ImageFolder","c9412796":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntransform = transforms.Compose([transforms.Resize((50,50)),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\ndataset = ImageFolder(DATA_DIR+'\/training_set', transform=transform)\ntest_dataset = ImageFolder(DATA_DIR+'\/test_set', transform=transform)\n","44329a0e":"img, label = dataset[0]\nprint(img.shape, label)\nimg","5c8043ff":"print(dataset.classes)","edc3128b":"import matplotlib.pyplot as plt\n\ndef show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","2c088acf":"show_example(*dataset[0])","dd3b1e2b":"show_example(*dataset[4800])","e7083c68":"random_seed = 42\ntorch.manual_seed(random_seed);","3e38b421":"len(dataset)","907f2aa1":"val_size = 2000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","3cbad399":"batch_size=64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True)","2757275e":"def show_batch(dl, invert=False):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(16, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        data = 1-images if invert else images\n        ax.imshow(make_grid(data, nrow=16).permute(1, 2, 0))\n        break","2a7b2a79":"show_batch(train_loader)","ef813508":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","5b3c5777":"class ImageClassificationModelBase(nn.Module):\n    def training_step(self, batch):\n        images, targets = batch \n        out = self(images)                      \n        loss = F.nll_loss(out, targets)      \n        return loss\n    \n    def validation_step(self, batch):\n        images, targets = batch \n        out = self(images)                           # Generate predictions\n        loss = F.nll_loss(out, targets)  # Calculate loss\n        acc = accuracy(out, targets)\n        return {'val_loss': loss.detach(), 'val_acc': acc }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_acc = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_acc).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch,result['train_loss'], result['val_loss'], result['val_acc']))","2df1676b":"class ImageClassificationModel(ImageClassificationModelBase):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3), #output 32 X 50 X 50 | (Receptive Field (RF) -  3 X 3\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Dropout2d(0.1),\n                        \n            nn.Conv2d(32, 64, kernel_size=3), #output 64 X 48 X 48 | (Receptive Field (RF) -  5 X 5\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(0.1),\n            \n            nn.Conv2d(64, 128, kernel_size=3), #output 64 X 46 X 46 | (Receptive Field (RF) -  7 X 7\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Dropout2d(0.1),\n            \n            \n            nn.Conv2d(128, 128, kernel_size=3), #output 128 X 44 X 44 | (Receptive Field (RF) -  11 X 11\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Dropout2d(0.1),\n        )\n        \n        #transition layer\n        self.trans1 = nn.Sequential(\n            \n            nn.MaxPool2d(2, 2), # output: 128 x 22 x 22  | RF 22 X 22\n            nn.Conv2d(128, 32, kernel_size=1), #output 32 X 20 X 20 | (Receptive Field (RF) -  24 X 24,\n            nn.ReLU(),\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3), #output 32 X 18 X 18 | (Receptive Field (RF) -  26 X 26\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(0.1),\n                        \n            nn.Conv2d(64, 64, kernel_size=3), #output 64 X 16 X 16 | (Receptive Field (RF) -  28 X 28\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(0.1),\n            \n            nn.Conv2d(64, 64, kernel_size=3), #output 64 X 18 X 18 | (Receptive Field (RF) -  30 X 30\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(0.1),\n        \n        )\n        \n        #transition layer\n        self.trans2 = nn.Sequential(\n            \n            nn.MaxPool2d(2, 2), # output: 64 x 9 x 9  | RF 60 X 60\n            nn.Conv2d(64, 32, kernel_size=1), #output 32 X 7 X 7 | (Receptive Field (RF) -  62 X 62,\n            nn.ReLU(),\n        )\n        \n            \n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=7)\n        ) # output_size = 1\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=2, kernel_size=(1, 1)),\n        )\n         \n    def forward(self, xb):\n        x = self.conv1(xb)\n        x = self.trans1(x)\n        x = self.conv2(x)\n        x = self.trans2(x)\n        x = self.gap(x)\n        x = self.conv3(x)\n\n        x = x.view(-1, 2)\n        return F.log_softmax(x, dim=-1)","9572bcb4":"#function to ensure that our code uses the GPU if available, and defaults to using the CPU if it isn't.\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \n# a function that can move data and model to a chosen device.    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n\n#Finally, we define a DeviceDataLoader class to wrap our existing data loaders and move data to the selected device, \n#as a batches are accessed. Interestingly, we don't need to extend an existing class to create a PyTorch dataloader. \n#All we need is an __iter__ method to retrieve batches of data, and an __len__ method to get the number of batches.\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","1b5a9a2a":"device = get_default_device()\ndevice","35c83d91":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","8e7aead6":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \n    \ndef fit(epochs, max_lr, model, train_loader, val_loader,weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    optimizer = opt_func(model.parameters(), max_lr,weight_decay=weight_decay)\n    \n        # Set up one-cycle learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs=[]\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)            \n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n             # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            scheduler.step()           \n            \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","29f0a5fc":"model = to_device(ImageClassificationModel(), device)\n","fd44c203":"!pip install torchsummary","5803cf38":"from torchsummary import summary\n# print the summary of the model\nsummary(model, input_size=(3, 50, 50), batch_size=-1)","2575e0f5":"history = [evaluate(model, val_loader)]\nhistory","3b2b9a48":"epochs = 20\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.SGD\n","974a76d6":"%%time\nhistory += fit(epochs, max_lr, model, train_loader, val_loader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","c16fb69a":"def plot_scores(history):\n#     scores = [x['val_score'] for x in history]\n    acc = [x['val_acc'] for x in history]\n    plt.plot(acc, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('acc')\n    plt.title('acc vs. No. of epochs');","a420ef2b":"plot_scores(history)\n","b662cfab":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","14a42946":"plot_losses(history)\n","61738a70":"!pip install jovian --upgrade --quiet","8456fa35":"import jovian","72906b9f":"project_name='image-classification-cats-vs-dogs_v2'","6a3c2129":"jovian.commit(project=project_name, environment=None)","2e185bcc":"As a internal functionality, the list of classes is stored in the .classes property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes.","118425cb":"We can now wrap our data loaders using DeviceDataLoader.","4e746e72":"We can view the image using matplotlib, but we need to change the tensor dimensions to (50,50,3). Notice, for matplotlib the channel should be specified in the third position. Let's create a helper function to display an image and its label.","d9c4ddc0":"### Training and Validation Datasets\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n- Training set - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n- Validation set - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n- Test set - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model. \nSince there's no predefined validation set, we can set aside a small portion of the training set to be used as the validation set. We'll use the random_split helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator","66798c6d":"### Loss Function\nWhile the accuracy is a great way for us (humans) to evaluate the model, it can't be used as a loss function for optimizing our model using gradient descent, for the following reasons:\n\nIt's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n\nIt doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n\nDue to these reasons, accuracy is a great evaluation metric for classification, but not a good loss function. So we will be using Negative Log Loss function to compute the loss.","ac39acbb":"### Using a GPU\nAs the sizes of our models and datasets increase, we need to use GPUs to train our models within a reasonable amount of time. GPUs contain hundreds of cores that are optimized for performing expensive matrix operations on floating point numbers in a short time, which makes them ideal for training deep neural networks with many layers.\n","60f83429":"### DataLoader\nWe can now created data loaders to help us load the data in batches. Large datasets requires loading them into memory all at once. This leads to memory outage and slowing down of programs. PyTorch offers a solution for parallelizing the data loading process with the support of automatic batching as well. This is the DataLoader class present within the torch.utils.data package.\n\nWe'll use a batch size of 64, we will load 64 samples at a time until all the 50000 images in the training set are loaded and trained to complete 1 epoch.\n\n### What is batch size?\nThe number of samples (data points) that would be passed through the network at a time.\n\n### What is epoch?\nAn epoch is one single pass of all the input data through the network.\n\n### Relation between batch_size and epoch?\nbatch_size is not equal to epoch, consider you have 1000 images. Processing all the 1000 images through the network once is considered as 1 epoch. If we set the batch size as 10, during training we will be passing 100 data points (=1000\/10) at a time until we eventually pass in all the training data to complete 1 single epoch.\n\nGenerally, larger the batch size faster the training. However, you need to have enough hardware to handle. Sometimes, even if our machine can handle heavy computation,by setting larger batch size quality of the model could degrade and could create difficulty in generalizing.","93513400":"### Evaluation Metric and Loss Function\nLet's first define out evaluation metric, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly i.e. the accuracy of the prediction","7b8039f4":"We see improvment in model performance compared to previous architecture.","4e8d88ba":"### Model","37352394":"We can look at batches of images from the dataset using the make_grid method from torchvision. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches.","687ef01d":"Before we train the model, we need to ensure that the data and the model's parameters (weights and biases) are on the same device (CPU or GPU). We can reuse the to_device function to move the model's parameters to the right device.","cabad269":"Let's look inside folders, training set and from the test set. ","7dc6be97":"### Import Libraries","37af7190":"This is in continuation to previous model development using only CNN and Maxpool layers https:\/\/www.kaggle.com\/divyakamat\/image-classification-cats-and-dogs-pytorch\n\nIn this we achieved accuracy of 79% on validation dataset, it was a highly overfit model, in this notebook, lets try to improve upon the model performance by adding regularization techniques like dropout and batch norm.","3f040203":"The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the ImageFolder class from torchvision to load the data as PyTorch tensors.","063a4691":"Since the data consists of color images with 3 channels (RGB), each image tensor has the shape of varying size.So we are resizing the image to have same shape (50,50)","c1d125ff":"Let's look at a sample element from the training dataset. Each element is a tuple, containing a image tensor and a label. Since the data consists of color images with 3 channels (RGB)","978e7ca2":"Here we are using torch.max() function, this function's default behaviour as you can guess by the name is to return maximum among the elements in the Tensor. However, this function also helps get the maximum along a particular dimension, as a Tensor, instead of a single element. To specify the dimension (axis \u2013 in numpy), there is another optional keyword argument, called dim. This represents the direction that we take for the maximum.\n\nmax_elements, max_indices = torch.max(input_tensor, dim)\n\n- dim=0, (maximum along columns).\n- dim=1 (maximum along rows).\n\nThis returns a tuple, max_elements and max_indices.\n\n- max_elements -> All the maximum elements of the Tensor.\n- max_indices -> Indices corresponding to the maximum elements.\n\nIn the above accuracy function, the == performs an element-wise comparison of two tensors with the same shape, and returns a tensor of the same shape, containing 0s for unequal elements, and 1s for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy.","7ffb756c":"### Training the Model\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our fit function:\n\n- Learning rate scheduling: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the \"One Cycle Learning Rate Policy\", which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. \n\n- Weight decay: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.\n\n- Gradient clipping: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping."}}