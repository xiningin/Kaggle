{"cell_type":{"7e6eb739":"code","4cbfc65b":"code","491dd387":"code","340808fa":"code","550f90f9":"code","af7e9e64":"code","36195be7":"code","44ec73ef":"code","49e1f952":"code","a088414a":"code","1758c44f":"code","f81cb2f1":"code","811c22a9":"code","dc14282a":"code","90ab147b":"code","d948adab":"code","e3139b71":"code","e13bc99c":"code","e7ba7cd9":"code","65b75b8d":"code","f668cf5a":"code","e9f198bc":"code","b71d0080":"code","c4ba2286":"markdown","0f1774bd":"markdown","db183d1f":"markdown","41a0f3a3":"markdown"},"source":{"7e6eb739":"import numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nimport cv2","4cbfc65b":"from cycler import cycler\nimport matplotlib as mpl\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\nraw_dark_palette = [\n    (10, 132, 255), # Blue\n    (255, 159, 10), # Orange\n    (48, 209, 88),  # Green\n    (255, 69, 58),  # Red\n    (191, 90, 242), # Purple\n    (94, 92, 230),  # Indigo\n    (255, 55, 95),  # Pink\n    (100, 210, 255),# Teal\n    (255, 214, 10)  # Yellow\n]\nraw_gray_light_palette = [\n    (142, 142, 147),# Gray\n    (174, 174, 178),# Gray (2)\n    (199, 199, 204),# Gray (3)\n    (209, 209, 214),# Gray (4)\n    (229, 229, 234),# Gray (5)\n    (242, 242, 247),# Gray (6)\n]\nraw_gray_dark_palette = [\n    (142, 142, 147),# Gray\n    (99, 99, 102),  # Gray (2)\n    (72, 72, 74),   # Gray (3)\n    (58, 58, 60),   # Gray (4)\n    (44, 44, 46),   # Gray (5)\n    (28, 28, 39),   # Gray (6)\n]\n\nlight_palette = np.array(raw_light_palette)\/255\ndark_palette = np.array(raw_dark_palette)\/255\ngray_light_palette = np.array(raw_gray_light_palette)\/255\ngray_dark_palette = np.array(raw_gray_dark_palette)\/255\n\nmpl.rcParams['axes.prop_cycle'] = cycler('color',dark_palette)\nmpl.rcParams['figure.facecolor']  = gray_dark_palette[-2]\nmpl.rcParams['figure.edgecolor']  = gray_dark_palette[-2]\nmpl.rcParams['axes.facecolor'] =  gray_dark_palette[-2]\n\nwhite_color = gray_light_palette[-2]\nmpl.rcParams['text.color'] = white_color\nmpl.rcParams['axes.labelcolor'] = white_color\nmpl.rcParams['axes.edgecolor'] = white_color\nmpl.rcParams['xtick.color'] = white_color\nmpl.rcParams['ytick.color'] = white_color\n\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","491dd387":"# Create a list with the filepaths for training and testing\ntrain_img_Path = '..\/input\/plant-pathology-2021-fgvc8\/train_images'\n\ntest_img_Path = '..\/input\/plant-pathology-2021-fgvc8\/test_images'\n\nimg_Path = '..\/input\/resized-plant2021\/img_sz_256'\n\ntrain = pd.read_csv(r'..\/input\/plant-pathology-2021-fgvc8\/train.csv')\n\nsample_submission = pd.read_csv(r'..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv')","340808fa":"train.head()","550f90f9":"print(f'Number of pictures in the training dataset: {train.shape[0]}\\n')\nprint(f'Number of different labels: {len(train.labels.unique())}\\n')\nprint(f'Labels: {train.labels.unique()}')","af7e9e64":"train['labels'].value_counts()","36195be7":"plt.figure(figsize=(14,7))\nb = sns.countplot(x='labels', data=train, order=sorted(train['labels'].unique()))\nfor item in b.get_xticklabels():\n    item.set_rotation(90)\nplt.title('Label Distribution', weight='bold')\nplt.show()","44ec73ef":"plt.figure(figsize=(20,40))\ni=1\nfor idx,s in train.head(9).iterrows():\n    img_path = os.path.join(img_Path,s['image'])\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    fig=plt.subplot(9,3,i)\n    fig.imshow(img)\n    fig.set_title(s['labels'])\n    i+=1","49e1f952":"CLASSES = train['labels'].unique().tolist()","a088414a":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Preprocessing the Training set\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.1,\n                                   horizontal_flip = True,\n                                   validation_split=0.25)\n\ntrain_data = train_datagen.flow_from_dataframe(train,\n                                              directory=img_Path,\n                                              classes=CLASSES,\n                                              x_col=\"image\",\n                                              y_col=\"labels\",\n                                              target_size=(150, 150),\n                                              subset='training')\n\nval_data = train_datagen.flow_from_dataframe(train,\n                                            directory=img_Path,\n                                            classes=CLASSES,\n                                            x_col=\"image\",\n                                            y_col=\"labels\",\n                                            target_size=(150, 150),\n                                            subset='validation')","1758c44f":"dict_classes = train_data.class_indices\ndict_classes","f81cb2f1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.applications import ResNet50V2, ResNet152V2, InceptionResNetV2\nfrom tensorflow.keras.layers import Conv2D, Dropout,MaxPooling2D,Flatten,Dense","811c22a9":"base_ResNet152V2 = ResNet152V2(include_top = False, \n                         weights = '..\/input\/keras-pretrained-models\/ResNet152V2_NoTop_ImageNet.h5', \n                         input_shape = train_data.image_shape, \n                         pooling='avg',\n                         classes = CLASSES)","dc14282a":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\nmodel_ResNet = Sequential()\nmodel_ResNet.add(base_ResNet152V2)\nmodel_ResNet.add(Dense(12, activation=('softmax')))\n\nmodel_ResNet.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel_ResNet.summary()\n\n# Training the CNN on the Train data and evaluating it on the val data\nr = model_ResNet.fit(train_data, validation_data = val_data, epochs = 15, batch_size=32)","90ab147b":"model_history = r.history\n\nplt.figure()\nplt.plot(model_history['accuracy'])\nplt.plot(model_history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.savefig('accuracy')\nplt.show()","d948adab":"plt.figure()\nplt.plot(model_history['loss'])\nplt.plot(model_history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.savefig('loss')\nplt.show()","e3139b71":"test_dir = '\/kaggle\/input\/plant-pathology-2021-fgvc8\/test_images\/'\ntest_df = pd.DataFrame()\ntest_df['image'] = os.listdir(test_dir)\n\ntest_data = train_datagen.flow_from_dataframe(dataframe=test_df,\n                                    directory=test_dir,\n                                    x_col=\"image\",\n                                    y_col=None,\n                                    batch_size=32,\n                                    seed=42,\n                                    shuffle=False,\n                                    class_mode=None,\n                                    target_size=(150, 150))","e13bc99c":"pred_resnet = model_ResNet.predict(test_data)\npred_resnet","e7ba7cd9":"pred = (pred_resnet+pred_resnet+pred_resnet).tolist()","65b75b8d":"for i in range(len(pred)):\n    pred[i] = np.argmax(pred[i])\n\n    \ndef get_key(val):\n    for key, value in dict_classes.items():\n        if val == value:\n            return key\n        \n\nfor i in range(len(pred)):\n    pred[i] = get_key(pred[i])","f668cf5a":"pred","e9f198bc":"test_df['labels'] = pred\ntest_df","b71d0080":"test_df.to_csv('submission.csv',index=False)","c4ba2286":"Apples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.\n\nAlthough computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc.","0f1774bd":"#### Defining the ResNet50V2 Convolutional Neural Net:","db183d1f":"#### Importing libraries","41a0f3a3":"## Plant Pathology 2021 - FGVC8\n#### Identify the category of foliar diseases in apple trees"}}