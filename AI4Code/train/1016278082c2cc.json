{"cell_type":{"86b0cb1a":"code","5b940c8c":"code","9f6be409":"code","77d69e73":"code","07596d70":"code","14b6f45a":"code","373d87ef":"code","7587b034":"code","6bc0e657":"code","411a2f2d":"code","f4c9c668":"code","65e25d7d":"markdown","059db972":"markdown","1b6e2bbf":"markdown","25c2c61f":"markdown","cd36a21f":"markdown","3d87a102":"markdown","edc4caf1":"markdown","b350b91e":"markdown","f56aeec4":"markdown","af353387":"markdown","a2120bd2":"markdown","19bec832":"markdown","481048d6":"markdown"},"source":{"86b0cb1a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import metrics\n\ndataset = pd.read_csv('..\/input\/seed-from-uci\/Seed_Data.csv')\ndataset.head()","5b940c8c":"dataset.describe(include = \"all\")","9f6be409":"features = dataset.iloc[:, 0:7]\ntarget = dataset.iloc[:, -1]\n\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,10))\n\nvisualizer.fit(features)    # Fit the data to the visualizer\nvisualizer.poof()    # Draw\/show\/poof the data","77d69e73":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(features)\ncluster_labels = kmeans.fit_predict(features)\n\nkmeans.cluster_centers_","07596d70":"silhouette_avg = metrics.silhouette_score(features, cluster_labels)\nprint ('silhouette coefficient for the above clutering = ', silhouette_avg)","14b6f45a":"def purity_score(y_true, y_pred):\n    # compute contingency matrix (also called confusion matrix)\n    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n    return np.sum(np.amax(contingency_matrix, axis=0)) \/ np.sum(contingency_matrix) \n\npurity = purity_score(target, cluster_labels)\nprint ('Purity for the above clutering = ', purity)","373d87ef":"!pip install pyclustering\n\nfrom pyclustering.cluster.kmedoids import kmedoids","7587b034":"# Randomly pick 3 indexs from the original sample as the mediods\ninitial_medoids = [1, 50, 170]\n\n# Create instance of K-Medoids algorithm with prepared centers.\nkmedoids_instance = kmedoids(features.values.tolist(), initial_medoids)\n\n# Run cluster analysis.\nkmedoids_instance.process()\n\n# predict function is not availble in the release branch yet.\n# cluster_labels = kmedoids_instance.predict(features.values)\n\nclusters = kmedoids_instance.get_clusters()\n\n# Prepare cluster labels\ncluster_labels = np.zeros([210], dtype=int)\nfor x in np.nditer(np.asarray(clusters[1])):\n   cluster_labels[x] = 1\nfor x in np.nditer(np.asarray(clusters[2])):\n   cluster_labels[x] = 2\n\ncluster_labels","6bc0e657":"# Mediods found in above clustering, indexes are shouwn below.\nkmedoids_instance.get_medoids()","411a2f2d":"silhouette_avg = metrics.silhouette_score(features, cluster_labels)\nprint ('silhouette coefficient for the above clutering = ', silhouette_avg)","f4c9c668":"purity = purity_score(target, cluster_labels)\nprint ('Purity for the above clutering = ', purity)","65e25d7d":"The lowest distortion, which is sum of square distances from each point to its assigned cluster, is found at k = 3 hence this clustering is optimal when 3 clusters are used. We used k-means and k-mediods with k = 3 in following clustering experiments.","059db972":"**Calculate silhouette coefficient for above clustering**","1b6e2bbf":"**Calculate Purity of the above clustering**","25c2c61f":"**Apply k-means clustering with k=3**","cd36a21f":"**Calculate Purity of the above clustering**\n\nYou can use [this](https:\/\/pml.readthedocs.io\/en\/latest\/clustering.html) library as well.","3d87a102":"**Install [pyclustering](https:\/\/pypi.org\/project\/pyclustering\/) on kernel**\n\nEnable internet setting for the kernel and run the following command to install custom python packge.\nRefer documentation for the package [here](https:\/\/codedocs.xyz\/annoviko\/pyclustering\/classpyclustering_1_1cluster_1_1kmedoids_1_1kmedoids.html#a368ecae21ba8fabc43487d5d72fcc97e).","edc4caf1":"**Apply k-mediods clustering with k=3**","b350b91e":"**Workspace setting and loading dataset**","f56aeec4":"**Calculate silhouette coefficient for above clustering**","af353387":"[](http:\/\/)**[Elbow](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/elbow.html) method to find optimal k (number of clusters)**","a2120bd2":"**Introduction**\n\n**Abstract:** Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.\n\nClasses - Kama (0), Rosa (1) and Canadian (2)\n\nLink to dataset : https:\/\/archive.ics.uci.edu\/ml\/datasets\/seeds\n\n![](https:\/\/uci-seed-dataset.s3.ap-south-1.amazonaws.com\/Dataset.PNG)\n\n> You will learn following things by reading this kernel.\n1. [Elbow](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/elbow.html) method to find optimal k (number of clusters)\n2. k-means clustering with k value found.\n3. Calculate silhouette coefficient to measure clustering quality.\n4. Calculate purity to measure clustering quality.\n5. k-mediods clutering with k value found.\n6. Conclusions on methods used.","19bec832":"**Conclusion**\n\n1. Both silhouette coefficient and purity values are very close, hence both clustering are done similarity. K-means is slightly better in both measures.\n2. K-mediods results are sensitive to the initial mediods selected.","481048d6":"o.47 would be an average value for silhouette coeffiencit. -1 is the worst and +1 is the optimal.\nRead more about silhouette coeffient at wikipedia article. And read the paper [here](https:\/\/www.sciencedirect.com\/science\/article\/pii\/0377042787901257)."}}