{"cell_type":{"1b719052":"code","6dd00b05":"code","3ba1fd72":"code","f6418b8b":"code","912f7910":"code","f50fb58d":"code","147afbe5":"code","0bc1e237":"code","d2635377":"code","27fb6895":"code","8c1b8c06":"code","4f6f6e14":"code","33ae37bb":"code","bec970d8":"code","8f94fc6e":"code","837e53e6":"code","815330b5":"code","12c82326":"code","5566fb6b":"code","5b691eeb":"code","70ddb129":"code","088fbf8d":"code","572cc034":"markdown","fbcdea84":"markdown","05c55de2":"markdown","6e647c03":"markdown","352c343c":"markdown","8d5efade":"markdown","84e377c4":"markdown","bd855a5f":"markdown"},"source":{"1b719052":"from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport glob\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nfrom tqdm import tqdm\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode","6dd00b05":"df = pd.read_csv('..\/input\/dlub-summer-school-challenge-2021\/train.csv')\ndf.head()","3ba1fd72":"df.iloc[0]","f6418b8b":"df.iloc[0]['filename']","912f7910":"%mkdir \/kaggle\/working\/train_resized\n%mkdir \/kaggle\/working\/test_resized","f50fb58d":"IMG_SIZE = 128","147afbe5":"# plot example\nfig = plt.figure(figsize=(15, 15))\nfig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05,\n                    wspace=0.05)\n\nfor i, file in enumerate(glob.glob('..\/input\/dlub-summer-school-challenge-2021\/train\/train\/*.jpg')[:9]):\n    ax = fig.add_subplot(1, 9, i + 1, xticks=[], yticks=[])\n    image = cv2.imread(file)\n    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    ax.imshow(rgb, cmap='gray')","0bc1e237":"# Training data\nfor file in tqdm(glob.glob('..\/input\/dlub-summer-school-challenge-2021\/train\/train\/*.jpg')):\n    filename = file.split('\/')[-1]\n    image = cv2.imread(file)\n    resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    cv2.imwrite(f'\/kaggle\/working\/train_resized\/{filename}', resized)\n\n# Test data\nfor file in tqdm(glob.glob('..\/input\/dlub-summer-school-challenge-2021\/test\/test\/*.jpg')):\n    filename = file.split('\/')[-1]\n    image = cv2.imread(file)\n    resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    cv2.imwrite(f'\/kaggle\/working\/test_resized\/{filename}', resized)","d2635377":"class FoodImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        filename = self.df.iloc[idx]['filename']\n        img_name = os.path.join(self.root_dir, filename)\n        image = io.imread(img_name)\n        category = int(self.df.iloc[idx]['category_id'])\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, category, filename","27fb6895":"class CNN(nn.Module):\n    def __init__(self):\n        \"\"\"CNN Builder.\"\"\"\n        super(CNN, self).__init__()\n\n        self.conv_layer = nn.Sequential(\n\n            # Conv Layer block 1\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv Layer block 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout2d(p=0.05),\n\n            # Conv Layer block 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Conv Layer block 4\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            # Conv Layer block 4\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n        )\n\n        self.fc_layer = nn.Sequential(\n            nn.Linear(4096, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(512, 25)\n        )\n\n\n    def forward(self, x):\n        \"\"\"Perform forward.\"\"\"\n        \n        # conv layers\n        x = self.conv_layer(x)\n        \n        # flatten\n        x = x.view(x.size(0), -1)\n        \n        # fc layer\n        x = self.fc_layer(x)\n\n        return x","8c1b8c06":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\ncnn = CNN().to(torch.device(\"cuda:0\"))","4f6f6e14":"!pip install torchinfo","33ae37bb":"from torchinfo import summary","bec970d8":"stats = summary(cnn, (1, 3, 128, 128), verbose=1)","8f94fc6e":"import torch.optim as optim\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn.parameters(), lr=0.0001)","837e53e6":"# more transformations \ntrain_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\ntest_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\n\ntrain_dataset = FoodImageDataset(\n    csv_file='..\/input\/dlub-summer-school-challenge-2021\/train.csv', \n    root_dir='\/kaggle\/working\/train_resized',\n    transform=train_transform)\n\ntrain_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n                                             batch_size=32, shuffle=True,\n                                             num_workers=2)\n\ntest_dataset = FoodImageDataset(\n    csv_file='..\/input\/dlub-summer-school-challenge-2021\/submission.csv', \n    root_dir='\/kaggle\/working\/test_resized',\n    transform=test_transform)\n\ntest_dataset_loader = torch.utils.data.DataLoader(test_dataset,\n                                             batch_size=32, shuffle=False,\n                                             num_workers=2)","815330b5":"EPOCH = 10\nfor epoch in range(EPOCH):\n    running_loss = 0.0\n    for step, (x, y, filenames) in enumerate(train_dataset_loader, 0):\n        x = x.cuda()\n        y = y.cuda()\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = cnn(x)\n        loss = loss_func(outputs, y)\n        loss.backward()\n\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if (step+1) % 100 == 0: \n            print('epoch:{}'.format(epoch), '| step:{}'.format(step), '| loss:{:.4f}'.format(running_loss\/100) )   \n            running_loss = 0.0\n\nprint('Finished Training ...')","12c82326":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for step, (x, y, filenames) in enumerate(train_dataset_loader, 0):\n        x = x.cuda()\n        y = y.cuda()\n        outputs = cnn(x)\n\n        predicted = torch.max(outputs.data, 1)[1]\n        total += y.size(0)\n        correct += (predicted == y).sum().item()\n        if (step+1) % 100 == 0:\n            print('Accuracy of the network : %d %%' % (100 * correct \/ total))\n            break","5566fb6b":"predictions = []\nwith torch.no_grad():\n    for x, y, filenames in test_dataset_loader:\n        x = x.cuda()\n        y = y.cuda()\n        outputs = cnn(x)\n\n        predicted = torch.max(outputs.data, 1)[1]\n        \n        for fname, y in zip(filenames, predicted.cpu().numpy()):\n            predictions.append((fname, y))","5b691eeb":"submission = pd.DataFrame(data=predictions, columns=['filename', 'category_id'])","70ddb129":"submission.head()","088fbf8d":"submission.to_csv('submission_baseline.csv', index=None)","572cc034":"### Model Parameters","fbcdea84":"### Custom Dataset ","05c55de2":"### Resizing and saving all image data 128x128 to speed up the CNN","6e647c03":"### submission","352c343c":"### Training","8d5efade":"### Model Definition","84e377c4":"### Dataset Loader","bd855a5f":"### Evaluation on the training data"}}