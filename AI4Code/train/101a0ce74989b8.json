{"cell_type":{"82f8ed6b":"code","97cf89c5":"code","6c3f0dcf":"code","e12c1d24":"code","59e59171":"code","9ad6d2cf":"code","21e1ee9d":"code","62281187":"code","1d385327":"code","d444876c":"code","1e2e391a":"code","dac75941":"code","ef0be770":"code","dc493492":"markdown","955164f4":"markdown","a40694eb":"markdown","9db341d8":"markdown","2d045f1d":"markdown","41700cdb":"markdown","a482d45c":"markdown","240c69ce":"markdown","edad4470":"markdown","b6404f4b":"markdown","55ffb9e1":"markdown","7bf6afb6":"markdown","2a17dccf":"markdown","694269a0":"markdown","63195570":"markdown","64cd49c6":"markdown","39ea4cbc":"markdown","a3f81b2d":"markdown","a4158034":"markdown","1788a753":"markdown","192d5db5":"markdown","3c4e6518":"markdown"},"source":{"82f8ed6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97cf89c5":"!pip install pydotplus","6c3f0dcf":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom IPython.display import Image\nimport pydotplus","e12c1d24":"data=pd.read_csv('\/kaggle\/input\/data.csv')\ndata.head()","59e59171":"X= data[['age','bp']]\ny=data[['diabetes']]","9ad6d2cf":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)","21e1ee9d":"model1=tree.DecisionTreeClassifier(min_samples_split=10,min_samples_leaf=6,max_depth=5)\nmodel1.fit(X_train,y_train)\ny_pred_train=model1.predict(X_train)\nprint('train:',accuracy_score(y_train,y_pred_train))\ny_pred_test=model1.predict(X_test)\nprint('test:',accuracy_score(y_test,y_pred_test))\n","62281187":"dot_data=tree.export_graphviz(model1,out_file=None,feature_names=X.columns,class_names=str(y['diabetes'].unique()))\n\ngraph=pydotplus.graph_from_dot_data(dot_data)\n\nImage(graph.create_png())","1d385327":"bp_lessthanequal_79 = X_train[X_train['bp']<=79.0].shape[0]\nbp_greaterthan_79 = X_train.shape[0]-bp_lessthanequal_79\n\nprint('Number of samples with bp<=79 :',bp_lessthanequal_79)\nprint('Number of samples with bp>79 :',bp_greaterthan_79)\nprint('number of patients having diabetes are : ',(y_train['diabetes']==1).sum())\nprint('number of patients not having diabetes are : ',(y_train['diabetes']==0).sum())","d444876c":"bp_lessthanequal_70=X_train[X_train['bp']<=70]\nbp_greaterthan_70=X_train[X_train['bp']>70]","1e2e391a":"number_lessthanequal_70=bp_lessthanequal_70.shape[0]\nnumber_bp_greaterthan_70=bp_greaterthan_70.shape[0]\n\nprint('number of patients with bp<=70 are : ',number_lessthanequal_70)\nprint('number of patients with bp>70 are : ',number_bp_greaterthan_70)\nprint('number of patients having diabetes are : ',(y_train['diabetes']==1).sum())\nprint('number of patients not having diabetes are : ',(y_train['diabetes']==0).sum())","dac75941":"positives_on_left=np.logical_and(X_train['bp']<=70,y_train['diabetes']==1).sum()\nnegatives_on_left=np.logical_and(X_train['bp']<=70,y_train['diabetes']==0).sum()\n\nprint('Number of patients with bp<=70 and have diabetes are : ',positives_on_left)\nprint('Number of patients with bp>70 and have diabetes are : ',negatives_on_left)","ef0be770":"positives_on_right=np.logical_and(X_train['bp']>70,y_train['diabetes']==1).sum()\nnegatives_on_right=np.logical_and(X_train['bp']>70,y_train['diabetes']==0).sum()\n\nprint('Number of patients with bp<=70 and have diabetes are : ',positives_on_right)\nprint('Number of patients with bp>70 and have diabetes are : ',negatives_on_right)","dc493492":"### Weighted Average\n![wa1.jpg](attachment:wa1.jpg)","955164f4":"# Observing Information gained in Case 1 and Case 2\n* We know that split will take place based on condition\/feature with maximum Information gain.\n* Just to prove this we considered two cases one with the actual split and other with random condition. \n* After calculation we can see that case 1 has higher amount of information gain as compared to case 2, That's why sklearn used case 1 for spltting.\n* Also splitting depends on other parameters as well like min_samples_split, min_samples_leaf etc.\n* Similary if we had criterion='entropy' instead of calculation gini we would have calculated entropy, and then information gain.","a40694eb":"### Information Gain\n* E(Y) is gini of root node and E(Y|X) is weighted average.\n![ig1.JPG](attachment:ig1.JPG)\n\n### So Information gain in case 1 = 0.3687","9db341d8":"## Dataframes with bp<=70 and bp>70","2d045f1d":"# Splitting data into train and test set","41700cdb":"### Weighted average and Information gain\n![ig2.JPG](attachment:ig2.JPG)\n\n### So Infomation Gain in case 2 =0.157","a482d45c":"### Positives and Negatives for patients with bp>70","240c69ce":"# Case 1\n## Claculation for the above plotted decision tree.(only for 1st split)\n* In the plot of decision tree we can see that first split is done based on condition i.e bp<=79, so lets see how is our class distributed.( zoom in the plot and have a look at root node)\n","edad4470":"### Positives and Negatives for patients with bp<=70","b6404f4b":"### Calculating Gini \n** Note: all calculations are for 1st split\n![gini1.jpg](attachment:gini1.jpg)\n\n* You may cross verify the values of calculated gini and in the plotted decision tree.","55ffb9e1":"# Let's calculate Information gain manually and try to create decsion tree.\n### First lets plot the decision tree of the model we trained.","7bf6afb6":"# Case 2\n## Let's assume we want to split root node based on condition bp<=70.\n* Instead of having condition bp<=79 lets assume that we have split our root node based on condition bp<=70\n* The code below can be skipped, its just for getting values of distribution using which we can plot Decision Tree, If want to skip directly jump to the next tree diagram ","2a17dccf":"# Loading Data","694269a0":"### If this notebook was helpful to you please upvote, it motivates me. Also suggest changes if any.","63195570":"### Tree based on above calculation\n![tree1.JPG](attachment:tree1.JPG)","64cd49c6":"# Using Decision Tree","39ea4cbc":"### Calculating gini\n![gini2.JPG](attachment:gini2.JPG)","a3f81b2d":"# Importing Libraries","a4158034":"# As split is done based on condition i.e b<=79, it means this condition gives maximum **INFORMATION GAIN**, right? Let's verify.","1788a753":"## What is gini and entropy?\n*Note: Here just formulas are mentioned but things will get more clear as you follow the notebook.\n### Gini index:-\n   * Gini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.\n   * Gini index is given as below, where P is frequentist probability of class.\n   ![gini.png](attachment:gini.png)\n        \n        \n### Entropy:-\n   * Entropy is simply a measure of disorder or uncetainty.\n   * Entropy is given as below, where P is frequentist probability of class.\n   ![entropy.png](attachment:entropy.png)\n   \n   \n### Information gain:-\n   * So we calculate Information gain using gini\/entropy and whichever feature or condition gives maximum information gain, split occurs based on that feature or condition\n   * Information gain from X on Y\n   \n   ![IG.png](attachment:IG.png)\n  \n**Note:- While calculating information gain we consider weighted values i.e if we are using entropy as our criterion then we'll use weighted entropy to calculate information gain.","192d5db5":"## Distribution ","3c4e6518":"# Thats it, right?\n### It's so simple to implement a decision tree using sklearn, but how does it actually works?\n* First parameter while creating instance of model is 'criterion'.\n* criterion parameter can take two values 'entropy' or 'gini', and has default value 'gini', have a look at screenshot of sklearn docs.\n\n![DT.JPG](attachment:DT.JPG)\n\n* Basic splits of decision tree takes place based criterion(entropy or gini whichever specified), also it depends on many other patameters like min_samples_split, max_depth etc, but basic splits depends on value of gini\/entropy."}}