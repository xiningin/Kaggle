{"cell_type":{"3a50972b":"code","210f88ca":"code","95ed6ecf":"code","d0f51171":"code","174b9f24":"code","10fb86a7":"code","06c36db4":"code","29bfc933":"code","d1f6cc38":"code","4a6db6d7":"code","e33cc7b8":"code","cb64c0ca":"code","075b3973":"code","a3b0aae8":"code","2e11e664":"code","4641f69c":"code","47980e70":"code","c109b640":"code","3be3abaa":"code","90335372":"markdown","1603660e":"markdown","465d02ec":"markdown","cef15264":"markdown"},"source":{"3a50972b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","210f88ca":"## import required libraries \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras","95ed6ecf":"from keras.datasets import cifar10\n\n(X_train,y_train),(X_test,y_test) = cifar10.load_data()\n\nprint(\"Total number of images in train data \",X_train.shape[0])\nprint(\"Total number of images in test data \",X_test.shape[0])\nprint(\"shape of input \",X_train.shape)","d0f51171":"## define labels of dataset\n\nclass_names = ['airplane','automobile','bird','cat','deer',\n               'dog','frog','horse','ship','truck']\n\nplt.figure(figsize=(16,16))\n\n## plot images of the dataset\n\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(X_train[i])\n    plt.axis(\"off\")\n    plt.title(\"{}\".format(class_names[y_train[i][0]]))\nplt.show()","174b9f24":"# Convert class vectors to binary class matrices.\n\nfrom keras.utils import to_categorical\ny_train = to_categorical(y_train,num_classes=10)\ny_test = to_categorical(y_test,num_classes=10)","10fb86a7":"## import libraries for model buildig \n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dense,Flatten,MaxPool2D\nfrom keras.layers import Activation\n\nmodel1 = Sequential()\nmodel1.add(Conv2D(64, 3, activation=\"relu\", input_shape=(X_train.shape[1:]), padding=\"same\"))\nmodel1.add(MaxPool2D(pool_size=(2,2), strides=2))\n\nmodel1.add(Conv2D(32, 3, activation=\"relu\", padding=\"same\"))\nmodel1.add(MaxPool2D(pool_size=(2,2), strides=2))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(512))\nmodel1.add(Activation('relu'))\nmodel1.add(Dense(10, activation = \"softmax\"))\n","06c36db4":"## check the summary of our model\n\nmodel1.summary()","29bfc933":"## compile our model\n\nfrom keras.optimizers import Adam\nfrom keras import metrics\n\nmodel1.compile(optimizer=Adam(lr=0.0001), loss=\"categorical_crossentropy\" , metrics=['accuracy'])","d1f6cc38":"## rescale our feature \n\nX_train = X_train.astype('float32')\nX_test = y_train.astype('float32')\n\nX_train = X_train\/255\nX_test = X_test\/255\n","4a6db6d7":"## finally run our model \n\nhistory1 = model1.fit(\n          X_train,y_train,\n          batch_size = 32,\n          epochs = 50,\n          validation_split = 0.1,\n          verbose = 2,\n          \n)","e33cc7b8":"## design our secoend model more normalized using dropout\n\nfrom keras.layers import Dropout\n\nmodel2 = Sequential()\n\nmodel2.add(Conv2D(64, 3, activation=\"relu\", input_shape=(X_train.shape[1:]), padding=\"same\"))\nmodel2.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodel2.add(Dropout(0.25))\n\nmodel2.add(Conv2D(32, 3, activation=\"relu\", padding=\"same\"))\nmodel2.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodel2.add(Dropout(0.25))\n\nmodel2.add(Flatten())\nmodel2.add(Dense(512))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.50))\nmodel2.add(Dense(10, activation = \"softmax\"))\n","cb64c0ca":"model2.compile(optimizer=Adam(lr=0.0001), loss=\"categorical_crossentropy\" , metrics=['accuracy']) ## compile our model","075b3973":"history2 = model2.fit(\n          X_train,y_train,\n          batch_size = 32,\n          epochs = 50,\n          validation_split = 0.1,\n          verbose = 2,\n          \n)  ## fit our model","a3b0aae8":"## plot our training and validation accuracy for  model 1\n\nfrom keras.callbacks import History\n\nplt.figure(figsize=(16,8))\nplt.plot(history1.history['accuracy'])\nplt.plot(history1.history['val_accuracy'])\nplt.title('Model1 Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","2e11e664":"\n\nplt.figure(figsize=(16,8))\nplt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('Model2 Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","4641f69c":"\n## create our third model introducing BatchNormalization\n\nfrom keras.layers import BatchNormalization\n\nmodel3 = Sequential()\n\nmodel3.add(Conv2D(128, 3, activation=\"relu\", input_shape=(X_train.shape[1:]), padding=\"same\"))\nmodel3.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodel3.add(BatchNormalization())\n\n\nmodel3.add(Conv2D(64, 3, activation=\"relu\", padding=\"same\"))\nmodel3.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodel3.add(Dropout(0.25))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(32, 3, activation=\"relu\", padding=\"same\"))\nmodel3.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodel3.add(Dropout(0.25))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Flatten())\nmodel3.add(Dense(512))\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(0.50))\nmodel3.add(Dense(10, activation = \"softmax\"))","47980e70":"model3.compile(optimizer=Adam(lr=0.0001), loss=\"categorical_crossentropy\" , metrics=['accuracy']) ## compile our model","c109b640":"history3 = model2.fit(\n          X_train,y_train,\n          batch_size = 32,\n          epochs = 50,\n          validation_split = 0.1,\n          verbose = 2,\n          \n)  ## fit our model","3be3abaa":"plt.figure(figsize=(16,8))\nplt.plot(history3.history['accuracy'])\nplt.plot(history3.history['val_accuracy'])\nplt.title('Model3 Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","90335372":"* Import cifar 10 dataset from keras and load that data into seperate train and test set.","1603660e":"* Using of Dropdown make the model more generalized.","465d02ec":"* I have done several hyper parameter tuning and produces different models and done above experiments to check the effect of different parameters on our model.","cef15264":"* The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n\n* The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n"}}