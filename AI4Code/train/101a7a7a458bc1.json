{"cell_type":{"9838f48c":"code","f734f153":"code","54f606bb":"code","8b196498":"code","568860e7":"code","23a6e918":"code","248c8b99":"code","7864c507":"code","9e6a4e60":"code","59b56a8f":"code","c3c407a7":"code","f9dc6e20":"code","fd92ae8e":"code","f51407e6":"code","b51ad129":"code","4b87978a":"code","9c957ccb":"code","df226b3e":"code","ba223ec1":"code","91321332":"code","6397f2f4":"code","51ea2d00":"code","f0f42430":"code","262be7ac":"code","31b9df88":"code","6978c962":"markdown","b4222f49":"markdown","c09d8961":"markdown","3a559864":"markdown","0741f4ff":"markdown","4653da54":"markdown","5fb3a208":"markdown","991a4004":"markdown","0af0adbd":"markdown","0cdfd60d":"markdown","51871ce6":"markdown","576e0bc3":"markdown","4ee24522":"markdown","8e9e1a99":"markdown"},"source":{"9838f48c":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","f734f153":"PATH = tf.keras.utils.get_file(\n  'flower_photos','https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',extract=True, untar=True)","54f606bb":"os.listdir(PATH)","8b196498":"train_dir = os.path.join(PATH)\nvalidation_dir = os.path.join(PATH)\n\ntotal_train = len(os.listdir(train_dir))\ntotal_val = len(os.listdir(validation_dir))\n\nprint(\"Total train  class:\", total_train)\nprint(\"Total validation class:\", total_val)","568860e7":"batch_size = 32\nepochs = 10\nIMG_HEIGHT = 150\nIMG_WIDTH = 150","23a6e918":"# Train data set\nimage_gen_train = ImageDataGenerator(\n                    rescale=1.\/255,\n                    rotation_range=45,\n                    width_shift_range=.15,\n                    height_shift_range=.15,\n                    horizontal_flip=True,\n                    zoom_range=0.5\n                    )\n\ntrain_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH))","248c8b99":"# Validation data set\nimage_gen_val = ImageDataGenerator(rescale=1.\/255)\n\nval_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n                                                 directory=validation_dir,\n                                                 target_size=(IMG_HEIGHT, IMG_WIDTH))","7864c507":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","9e6a4e60":"sample_training_images, _ = next(train_data_gen)\nplotImages(sample_training_images[:5])","59b56a8f":"IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","c3c407a7":"feature_batch = base_model(train_data_gen.next())\nprint(feature_batch.shape)","f9dc6e20":"base_model.trainable = False\nbase_model.summary()","fd92ae8e":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","f51407e6":"prediction_layer = tf.keras.layers.Dense(train_data_gen.num_classes, activation='softmax')\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","b51ad129":"model = tf.keras.Sequential([\n  base_model,\n  global_average_layer,\n  prediction_layer\n])","4b87978a":"result_batch = model.predict(sample_training_images)\nresult_batch.shape","9c957ccb":"base_learning_rate = 0.001\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","df226b3e":"model.summary()","ba223ec1":"history = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=train_data_gen.samples \/\/ train_data_gen.batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=val_data_gen.samples \/\/ val_data_gen.batch_size\n)","91321332":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.xlabel('epoch')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.tight_layout()\nplt.show()","6397f2f4":"base_model.trainable = True\nfor layer in base_model.layers[:100]:\n  layer.trainable =  False","51ea2d00":"model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","f0f42430":"model.summary()","262be7ac":"fine_tune_epochs = 10\ntotal_epochs =  epochs + fine_tune_epochs\n\nhistory_fine = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=train_data_gen.samples \/\/ train_data_gen.batch_size,\n    epochs=total_epochs,\n    initial_epoch = epochs,\n    validation_data=val_data_gen,\n    validation_steps=val_data_gen.samples \/\/ val_data_gen.batch_size\n)","31b9df88":"acc = acc + history_fine.history['accuracy']\nval_acc = val_acc + history_fine.history['val_accuracy']\nloss = loss + history_fine.history['loss']\nval_loss = val_loss + history_fine.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.plot([epochs-1,epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.plot([epochs-1,epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.tight_layout()\nplt.show()","6978c962":"## Fine Tuning\n\n>To increase performance even further is to fine-tune the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n\n> You should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features which generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n\nNOTE: Perform fine tuning after you have trained the top level classifier with the pretrained model.","b4222f49":"## Data Preprocessing & Augmentation","c09d8961":"## Introduction\nTransfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. Transfer learning involves the approach in which knowledge learned in one or more source tasks is transferred and used to improve the learning of a related target task.\n\nIt is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems.\n\nTraditional learning is isolated and occurs purely based on specific tasks, datasets and training separate isolated models on them. No knowledge is retained which can be transferred from one model to another. In transfer learning, you can leverage knowledge (features, weights etc) from previously trained models for training newer models and even tackle problems like having less data for the newer task!\n### In this notebook, we will try 2 ways to customize a pretrained model:\n>**Feature Extraction**: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for our dataset.\n\n>**Fine-Tuning**: Unfreezing a few of the top layers of a frozen model base and jointly training both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task.\n\n**There are different transfer learning strategies and techniques, which can be applied based on the domain, task at hand, and the availability of data.**\n\n![image](https:\/\/miro.medium.com\/max\/917\/1*mEHO0-LifV7MgwXSpY9wyQ.png)\n\n<h2> References: <\/h2>\n<ul> \n<li><\/a> https:\/\/www.tensorflow.org\/tutorials\/images\/classification <\/li>\n<li><\/a> https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning_with_hub <\/li>\n<li><\/a> https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning <\/li>\n<li><\/a> A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning  (Medium Article) <\/li>\n<\/ul>","3a559864":"### Learning Plots","0741f4ff":"## Base Model Creation\n>Create MobileNet base model from the pretrained Convnets. This is pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n>First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer is not very useful. So, we will use the very last layer called Bottleneck Layer, before the flatten operation.","4653da54":"## Train and validation data set","5fb3a208":"### Compile Model","991a4004":"## Feature Extraction\n>When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training. In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n\nNOTE: Let us freeze the convolutional base and freeze all the layers.","0af0adbd":"### Train Model","0cdfd60d":"### Classifier Layer","51871ce6":"### Fine Tune from 100 layers","576e0bc3":"## Load Data","4ee24522":"### Compile Model","8e9e1a99":"### Train Model"}}