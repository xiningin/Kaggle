{"cell_type":{"55623861":"code","226c8d64":"code","c553e953":"code","a718b16f":"code","7a682994":"code","f196fa0c":"code","342ff64c":"code","942eca1d":"code","7fa58c43":"code","46c473ac":"code","b74ef209":"code","a466d514":"code","9725843c":"code","367d268a":"code","7e3d06d9":"code","79cb3046":"code","2969bd5b":"markdown","03e41ee3":"markdown","55553ed9":"markdown","539581bf":"markdown","c2848823":"markdown","0671d6f3":"markdown"},"source":{"55623861":"#First, the necessary tools:\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))","226c8d64":"#Reading the data\ntest = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\n\n#Saving the PassengerID to the submission, since it's required...\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']","c553e953":"#inspecting the data:\ntrain.head()","a718b16f":"print(train.info())","7a682994":"#Here I'm removing things that I think that wouldn't be useful\n# (of course this is just a supposition, in fact, they are useful with proper use)\ntrain.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace = True)\ntest.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace = True)\n\n#Get_dummyes convert categorical variable into dummy\/indicator variables\ndata_train = pd.get_dummies(train)\ndata_test = pd.get_dummies(test)\n\n#\"Filnna\" fills empty values, I'm replacing they for the mean of the values.\n# (but, again, there are better ways to do this... this is just an example)\ndata_train['Age'].fillna(data_train['Age'].mean(), inplace = True)\ndata_test['Age'].fillna(data_test['Age'].mean(), inplace = True)\ndata_test['Fare'].fillna(data_test['Fare'].mean(), inplace = True)","f196fa0c":"# Calculate correlation matrix\ncorr = data_train.corr()\n\n# Plot heatmap of correlation matrix\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True)\nplt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\nplt.tight_layout()  # fits plot area to the plot, \"tightly\"\nplt.show()  # show the plot\nplt.clf()  # clear the plot area","342ff64c":"# Separating the data into variables to analyze (x) and the result (y) that we expect\nx = data_train.drop('Survived', axis=1)\ny = data_train['Survived']","942eca1d":"# Parametrizing the method and testing using cross-validation\n# You can change the parameters to get better results; ^^\nclassifier_rf = RandomForestClassifier(\n                criterion='gini',\n                max_depth=50,\n                n_estimators=100,\n                n_jobs=-1)\n    \nscores_rf = cross_val_score(classifier_rf, x, y, scoring='accuracy', cv=5)\nprint(scores_rf.mean())","7fa58c43":"#this result is for the training... we want to know for tests, so:\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)","46c473ac":"#Here we do some parameter optimization\n\nfrom sklearn.model_selection import ParameterGrid\n\n# Create a dictionary of hyperparameters to search\ngrid = {'n_estimators': [150, 100, 50], 'max_depth': [10, 25, 50], 'max_features': [4, 6, 8], 'random_state': [42], 'criterion': ['gini']}\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    classifier_rf.set_params(**g)  # ** is \"unpacking\" the dictionary\n    classifier_rf.fit(train_X, train_y)\n    test_scores.append(classifier_rf.score(test_X, test_y))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])","b74ef209":"#ok, let's fit the model with the best parameters in test\n\nclassifier_rf = RandomForestClassifier(\n                criterion='gini',\n                max_depth=10,\n                max_features=4,\n                n_estimators=50,\n                random_state=42,\n                n_jobs=-1)\n\n# Training the model with the data\nclassifier_rf.fit(x, y)\n","a466d514":"predictions = classifier_rf.predict(test_X)\nprint(confusion_matrix(test_y,predictions))\nprint(classification_report(test_y,predictions))","9725843c":"# Get feature importances from our random forest model\nimportances = classifier_rf.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\naux = range(len(importances))\n\n# Create tick labels\nlabels = x.columns.values\nplt.figure(figsize=(10,10))\nplt.bar(aux, importances[sorted_index], tick_label=labels)\n\n# Rotate tick labels to vertical\nplt.xticks(rotation=90)\nplt.show()","367d268a":"mlp = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=42,\n       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n       verbose=False, warm_start=False)\n\nmlp.fit(x,y)","7e3d06d9":"predictions = mlp.predict(test_X)\nprint(confusion_matrix(test_y,predictions))\nprint(classification_report(test_y,predictions))","79cb3046":"# Creating a submission\nsubmission['Survived'] = classifier_rf.predict(data_test)\nsubmission.to_csv('submission.csv', index=False)","2969bd5b":"Now just go there and post your submission! ^^\n\n**Thanks for reading and have a great day!**\n","03e41ee3":"# 95% that's impressive for a simple model.. :D","55553ed9":"### Well, we have some unnecessary columns (like having two sex variables, where we only need one...)\n### We should do some feature enginery, for sure","539581bf":"# 80%, not so good (but remember that we skipped the parameter tuning)","c2848823":"## Before our submission, let's try another model:","0671d6f3":"**Hello!**\n\nThis is just a simple example, using **Random Forest**, **MPL** and a little of **parameter tunning**  in the **Titanic dataset**.\nHope you enjoy it..."}}