{"cell_type":{"f3a55364":"code","d855df3c":"code","e94aa5f8":"code","60d32bca":"code","7cced83e":"code","a576c7b5":"code","3bd37095":"code","b53b07c4":"code","44db7dd5":"code","75420337":"code","efc03cdf":"code","375ffe1f":"code","8be7f6f1":"code","2ebc857a":"code","f3e9afef":"code","37a6e912":"code","9e3a6a98":"code","f40f1c3f":"code","4ada7cd8":"code","d20fec63":"code","032be0ed":"code","e1b8dd6a":"code","b64a0495":"code","6d4b9bdc":"code","8d792f65":"code","f4dd4c3b":"code","bd6e0f1b":"code","97869320":"code","c7381f3b":"code","c60b757a":"code","4be0f52d":"code","f3372dfb":"code","6d96f391":"code","95bfda1d":"code","929dc0ec":"code","92332c3e":"code","f760dd28":"code","68ce627c":"code","3ddb4f51":"code","646e74cb":"code","970d4e6e":"code","3172ca28":"code","539647fa":"code","c1590ee9":"code","3eed05b3":"code","1c7745dc":"code","d8d0b995":"code","b9004cf3":"code","22a8ffa6":"code","3e630427":"markdown","044ee4ce":"markdown","48513f3d":"markdown","bed1bc08":"markdown","831e1f82":"markdown","7f782c5e":"markdown","fcf3994b":"markdown","064e07ed":"markdown","1463aac4":"markdown","3dc1d9a5":"markdown","76366922":"markdown","a959a6b0":"markdown","eae82daa":"markdown","ce509f80":"markdown","3c055925":"markdown","f5e848af":"markdown","8b414a56":"markdown"},"source":{"f3a55364":"!pip install tensorflow-text","d855df3c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport nltk\nimport spacy\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e94aa5f8":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","60d32bca":"df.head()","7cced83e":"df.info()","a576c7b5":"def fillna(df, column, fillwith):\n  df[column] = df[column].fillna(fillwith)\n  return df","3bd37095":"for i in ['keyword','location']:\n  df = fillna(df,i,'a')","b53b07c4":"df.info()","44db7dd5":"df.head()","75420337":"df['space'] = ' '\ndf['full_text'] = df['keyword'] + df['space'] + df['location']+ df['space'] + df['text']\ndf.drop('keyword', axis=1, inplace=True)\ndf.drop('location', axis=1, inplace=True)\ndf.drop('text', axis=1, inplace=True)\ndf.drop('space', axis=1, inplace=True)\ndf.drop('id', axis=1, inplace=True)","efc03cdf":"df.head()","375ffe1f":"import string\n\n\ndef remove_punct(text):\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(table)\ndf[\"full_text\"] = df.full_text.map(lambda x: remove_punct(x))\n\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words(\"english\"))\n\n\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return \" \".join(text)\n\ndf[\"full_text\"] = df[\"full_text\"].map(remove_stopwords)","8be7f6f1":"df.head()","2ebc857a":"df_dis = df[df['target']==1]\ndf_no_dis = df[df['target']==0]","f3e9afef":"#Func to represent randomly\nnlp = spacy.load('en_core_web_sm')\ndef namedRandom(df):\n  random = [np.random.randint(0,len(df)-1) for i in range(0,5)]\n  for index in random:\n    text = df.full_text.iloc[index]\n    doc = nlp(text)\n    spacy.displacy.render(doc, style=\"ent\", jupyter=True)","37a6e912":"namedRandom(df_dis)","9e3a6a98":"namedRandom(df_no_dis)","f40f1c3f":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ndef cloudRandom(df, type):\n  random = [np.random.randint(0,len(df)-1) for i in range(0,5)]\n  text = ''\n  for index in random:\n    text = text + df.full_text.iloc[index]\n  wc = WordCloud(background_color=\"white\", \n               max_words=350, \n               width=1000, \n               height=600, \n               random_state=1).generate(text)\n  \n  plt.figure(figsize=(15,15))\n  plt.imshow(wc)\n  plt.axis(\"off\")\n  plt.title('Word Cloud for '+ (type) +' tweets')","4ada7cd8":"cloudRandom(df_dis, 'disaster')","d20fec63":"cloudRandom(df_no_dis, 'non - disaster')","032be0ed":"def getLemmText(text):\n tokens=word_tokenize(text)\n lemmatizer = WordNetLemmatizer()\n tokens=[lemmatizer.lemmatize(word) for word in tokens]\n return ' '.join(tokens)\ndf['full_text'] = list(map(getLemmText,df['full_text']))\n\ndef getStemmText(text):\n tokens=word_tokenize(text)\n ps = PorterStemmer()\n tokens=[ps.stem(word) for word in tokens]\n return ' '.join(tokens)\ndf['full_text'] = list(map(getStemmText,df['full_text']))","e1b8dd6a":"#defining visualisation params\nfig_dims = (10, 8)","b64a0495":"#Looking at class distribution to see if it is balanced\nclasses = df['target']\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.countplot(x = classes, ax=ax, palette='Oranges')\nplt.title('Class Distribution')\nplt.xlabel('0: No disaster tweet           1: Disaster tweet')\nplt.ylabel('Count of tweets')\nplt.show()","6d4b9bdc":"# Removing # and @ from the tweets.\ndef removetags(text):\n  tags = ['@','#']\n  tokens = word_tokenize(text)\n  tokens = [word for word in tokens if word not in tags]\n  return ' '.join(tokens)\ndf['full_text'] = list(map(removetags,df['full_text']))","8d792f65":"#Logic to find big numbers\nfor i in df.full_text[3].split():\n  print(i)","f4dd4c3b":"#Function to check bigNumbers Count\ndef bigNumCount(df):\n  bigNum_Count = []\n  truth_val = []\n  for text in df.full_text:\n    for token in text.split():\n      if token.isnumeric():\n        try:\n          if int(token)>10000:\n            bigNum_Count.append(text)\n            break\n        except:\n          pass\n  return bigNum_Count","bd6e0f1b":"#Looking at the Big number analysis\n#looking at BigNumbers in entire dataset\ndf_entire_BigNum = bigNumCount(df)\n# If there are more than 300 bigNum tweets we'll consider big numbers as features\nprint('There are '+ str(len(df_entire_BigNum)) +' big Numbers in the entire dataset.')","97869320":"#Looking at the Big number analysis\n#looking at BigNumbers in disaster dataset\ndf_dis = df[df['target']==1]\ndf_dis_BigNum = bigNumCount(df_dis)\n# If there are more than 100 bigNum tweets we'll consider big numbers as features\nprint('There are '+ str(len(df_dis_BigNum)) +' big Numbers in the disaster dataset.')","c7381f3b":"#Defining a function to replace \ndef replaceNumbers(text):\n  tokens = word_tokenize(text)\n  tokens = [word if word.isalpha() else 'bignumber' for word in tokens]\n  return ' '.join(tokens)\nprint('Example:')\nprint('text : 13000 one guy')\nprint(replaceNumbers('result : 13000 one guy'))\ndf['full_text'] = list(map(replaceNumbers,df['full_text']))","c60b757a":"## Code to remove numbers If you decide otherwise.\n# def removeNumbers(text):\n#   tokens = word_tokenize(text)\n#   tokens = [word for word in tokens if word.isalpha()]\n#   return ' '.join(tokens)\n\n# df['full_text'] = list(map(removeNumbers,df['full_text']))","4be0f52d":"df.head()","f3372dfb":"labels = df.target","6d96f391":"xtrain, xtest, ytrain, ytest = train_test_split(df.full_text.values, labels, \n random_state=42, test_size=0.3, shuffle=True, stratify = labels)","95bfda1d":"bert_model_name = 'small_bert\/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-768_A-12\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/google\/electra_small\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/google\/electra_base\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/pubmed\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/wiki_books\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/3',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/3',\n    'electra_small':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'electra_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","929dc0ec":"bert_preprocess = hub.KerasLayer(tfhub_handle_preprocess)\nbert_encoder = hub.KerasLayer(tfhub_handle_encoder)","92332c3e":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","f760dd28":"# with tpu_strategy.scope(): creating the model in the TPUStrategy scope means we will train the model on the TPU\n# model = create_model()","68ce627c":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","3ddb4f51":"model.fit(xtrain, ytrain, epochs=10, validation_data=(xtest, ytest))","646e74cb":"def graph_plots(history, string):\n plt.plot(history.history[string])\n plt.plot(history.history['val_'+string])\n plt.xlabel('Epochs')\n plt.ylabel(string)\n plt.legend([string, 'val_'+string])\n plt.show()\n \ngraph_plots(model.history, 'accuracy')\ngraph_plots(model.history, 'loss')","970d4e6e":"from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\npredicted = model.predict(xtest)","3172ca28":"np.unique(predicted)","539647fa":"results = []\nfor i in predicted:\n  if i>0.5:\n    results.append(1)\n  else:\n    results.append(0)","c1590ee9":"real = np.array(ytest)\npredicted = results","3eed05b3":"print(classification_report(real,predicted))","1c7745dc":"subtest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubtest.head()","d8d0b995":"for i in ['keyword','location']:\n  subtest = fillna(subtest,i,'a')\n\nsubtest['space'] = ' '\nsubtest['full_text'] = subtest['keyword'] + subtest['space'] + subtest['location']+ subtest['space'] + subtest['text']\nsubtest.drop('keyword', axis=1, inplace=True)\nsubtest.drop('location', axis=1, inplace=True)\nsubtest.drop('text', axis=1, inplace=True)\nsubtest.drop('space', axis=1, inplace=True)\nsubtest.drop('id', axis=1, inplace=True)\n\nsubtest[\"full_text\"] = subtest.full_text.map(lambda x: remove_punct(x))\nsubtest[\"full_text\"] = subtest[\"full_text\"].map(remove_stopwords)\n\nsubtest['full_text'] = list(map(getLemmText,subtest['full_text']))\nsubtest['full_text'] = list(map(getStemmText,subtest['full_text']))\n\nsubtest['full_text'] = list(map(removetags,subtest['full_text']))\nsubtest['full_text'] = list(map(replaceNumbers,subtest['full_text']))","b9004cf3":"subtest.head()","22a8ffa6":"test_pred = model.predict(subtest)\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('sub.csv', index=False)","3e630427":"The problem\n--------------------------------\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\nThe objective\nPredicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","044ee4ce":"# Submission","48513f3d":"Choosing BERT model for training","bed1bc08":"# Cleaning the data","831e1f82":"**LOOKING AT THE WORD CLOUD**","7f782c5e":"Visualising Named Entity\n\n---\nVisualising these can help us understand the features data might reply upon the most\n","fcf3994b":"Checking for Big Numbers if they can be considered as features or not.","064e07ed":"Lemmetizing and Stemming the text","1463aac4":"# Loading Libraries and Data","3dc1d9a5":"**LOADING THE DATA**","76366922":"Filling Nan Values","a959a6b0":"Looks well balanced","eae82daa":"Data Pre-processing on Test data","ce509f80":"# Training The model using BERT","3c055925":"Removing punctuation and stopwords.","f5e848af":"# Exploratory data analysis after cleaning","8b414a56":"# Result Evaluation"}}