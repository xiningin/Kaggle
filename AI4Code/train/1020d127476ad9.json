{"cell_type":{"bccfc598":"code","8b8402e8":"code","788f9a5f":"code","906b658a":"code","3098bc00":"code","c98cf603":"code","01f5637a":"code","830c4a9e":"code","14affe5a":"code","e2b31c77":"code","96a1474a":"code","8bc24081":"code","796b6099":"code","75f87640":"code","07405910":"code","14aaf7bf":"code","caed34b4":"markdown","90c1c639":"markdown","3c30950d":"markdown","00af135c":"markdown","75eb383e":"markdown","d56e7402":"markdown","00e4b4af":"markdown","9e91ddf9":"markdown","624b9bdc":"markdown","aeb41106":"markdown","125d6228":"markdown","9dcea1cc":"markdown","1823f183":"markdown","f0a92b6c":"markdown","48a42975":"markdown","b78f0ce8":"markdown","f4fae271":"markdown","b2b471c8":"markdown","2b957d99":"markdown","171a44e2":"markdown","dc014c02":"markdown"},"source":{"bccfc598":"import torch\nimport numpy as np","8b8402e8":"if not torch.cuda.is_available():\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\n","788f9a5f":"from torchvision import datasets\nimport torchvision.transforms as transforms\n\nnum_workers = 0\nbatch_size = 20\nvalid_size = 0.2","906b658a":"transform= transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\ntrain_data= datasets.CIFAR10('data',train=True,transform=transform,download=True)\ntest_data= datasets.CIFAR10('data',train=False,transform=transform,download=True)","3098bc00":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n     num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)","c98cf603":"images,target=next(iter(train_loader))\nimages.shape","01f5637a":"classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck']\ndef imshow(img):\n    img = img \/ 2 + 0.5 \n    plt.imshow(np.transpose(img, (1, 2, 0))) ","830c4a9e":"import matplotlib.pyplot as plt\ndataiter=next(iter(train_loader))\nimages,labels= dataiter\nimages=images.numpy()\nimshow(images[2])","14affe5a":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # convolutional layer (sees 32x32x3 image tensor)\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        # convolutional layer (sees 16x16x16 tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # convolutional layer (sees 8x8x32 tensor)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # max pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # linear layer (64 * 4 * 4 -> 500)\n        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n        # linear layer (500 -> 10)\n        self.fc2 = nn.Linear(500, 10)\n        # dropout layer (p=0.25)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        # flatten image input\n        x = x.view(-1, 64 * 4 * 4)\n        # add dropout layer\n        x = self.dropout(x)\n        # add 1st hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add 2nd hidden layer, with relu activation function\n        x = self.fc2(x)\n        return x\n\n# create a complete CNN\nmodel = Net()\nmodel","e2b31c77":"if torch.cuda.is_available():\n    model.cuda()","96a1474a":"import torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=0.001)","8bc24081":"epochs= 15\nvalid_loss_min = np.Inf\ntrain_epoch=[]\ntrain_loss_vals=[]\ntrain_acc_vals=[]\nvalid_epoch=[]\nvalid_loss_vals=[]\nvalid_acc_vals=[]\ntest_loss_val=[]\ntest_epoch=[]\nfor i in range(epochs):\n    model.train()\n    train_acc=0\n    valid_acc=0\n    total=0\n    for data,target in train_loader:\n        if torch.cuda.is_available():\n            data,target = data.cuda(), target.cuda()\n        optimizer.zero_grad()\n        output=model(data)\n        _, predicted = torch.max(output.data, 1)\n        train_acc+=((predicted==target).sum().item())\n        total += target.size(0)\n        loss = criterion(output, target)\n        loss.backward()\n        train_epoch.append(loss.item())\n        optimizer.step()\n    train_loss_vals.append(sum(train_epoch)\/len(train_epoch))\n    train_acc_vals.append(100 * train_acc\/ total)\n    model.eval()\n    total=0\n    for data,target in valid_loader:\n        if torch.cuda.is_available():\n            data,target= data.cuda(),target.cuda()\n        output=model(data)\n        _, predicted = torch.max(output.data, 1)\n        valid_acc+=((predicted==target).sum().item())\n        total += target.size(0)\n        loss= criterion(output,target)\n        valid_epoch.append(loss.item())\n    valid_loss_vals.append(sum(valid_epoch)\/len(valid_epoch))\n    valid_acc_vals.append(100 * valid_acc\/ total)\n    \n    print(\"epoch:{}\\t  training_loss:{}\\t  validation_loss:{}\\t  train_accuracy:{}\\t  validation_accuracy:{}\"\n          .format(i,train_loss_vals[i],valid_loss_vals[i],train_acc_vals[i],valid_acc_vals[i]))\n    if valid_loss_vals[i] <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss_vals[i]))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss_vals[i]","796b6099":"model.load_state_dict(torch.load('model_cifar.pt'))","75f87640":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_acc_vals,label='train_accuracy')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_acc_vals,label='valid_accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.title('Accuracy curve')","07405910":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_loss_vals,label='train_loss')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_loss_vals,label='valid_loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.title('loss functions')","14aaf7bf":"test_loss = 0.0\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\n\nmodel.eval()\nfor data, target in test_loader:\n    if torch.cuda.is_available():\n        data, target = data.cuda(), target.cuda()\n    output = model(data)\n    loss = criterion(output, target)\n    test_loss += loss.item()*data.size(0)\n    _, pred = torch.max(output, 1)    \n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n    for i in range(batch_size):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\ntest_loss = test_loss\/len(test_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","caed34b4":"# We come to the end of this notebook.\n## PLease upvote if you found this notebook useful.","90c1c639":"Test your trained model on previously unseen data! ","3c30950d":"## 5. Defining the architecture of the Neural Network.","00af135c":"## 6. Defining the loss function and the optimizer.","75eb383e":"We import the required dataset from the torchvision package and we also import transforms. We preprocess the image before we actually start working on it.\nWe convert the given images in the dataset into tensors and then normalize the images to ensure that all the image pixels have the same data range representation.","d56e7402":"A Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. \n\nThe CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.You can find a link to the dataset here https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html","00e4b4af":"## 9. Plotting the training and validation loss curves.","9e91ddf9":"Defining the classes present in the dataset. We have 10 different classes in this dataset.","624b9bdc":"## 2. Training on either CPU Or GPU as required.","aeb41106":"## 4. Dividing the dataset into train, test and validation sets.","125d6228":"We will be training the above defined network on 5 epochs.\nWe move our labels and the images to the GPU if we are training on a GPU or else CPU.\nWe intialize the gradient to zero and we calculate the output. We compare the output with that of the original label to find the loss.\nThen we perform gradient descent to minimize this loss to the minimum possible.\nWe print out the training loss,validation loss,training accuracy as well as validation accuracy at end of each epoch. We save the model only when the validation loss decreases from one epoch to the next.","9dcea1cc":"# Using Convolutional Neural Network for classifying CIFAR-10 images. ","1823f183":"## 7. Training the network.","f0a92b6c":"Displaying one of the original images.","48a42975":"We define the function \"imshow\" which performs the denormalization as well as converts it to numpy form to display the original image.","b78f0ce8":"## 10. Test set accuracy and loss.","f4fae271":"This CrossEntropy criterion combines LogSoftmax and NLLLoss in one single class.","b2b471c8":"we start of with the input image being of dimensions (32,32,3). We define a Convolutional layer and then the output of that layer is passed through the ReLU activation function. We also perform maxpooling after each layer to reduce the dimensions of the image and also reduce the number of weights that the network has to learn.\nWe use 3 Convolutional nets and the output of this network is passed through the fully connected layers whose output gives us the probability that the image belongs to a particular class.\nWe also use the technique of dropout inbetween layers so that overfitting of the model is avoided.","2b957d99":"## 1. Importing necessary libraries.","171a44e2":"##3. Importing the required CIFAR-10 dataset from Torchvision package.","dc014c02":"## 8. Plotting the training and validation accuracy curves."}}