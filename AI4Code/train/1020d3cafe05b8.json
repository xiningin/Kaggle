{"cell_type":{"0aaddf7f":"code","4037d38d":"code","95ea7b7a":"code","19feadcf":"code","5ebb09d2":"code","c49e7ecb":"code","4d34e85b":"code","60b8a23c":"code","d7be5d40":"code","2d3ab4d1":"code","6899f60f":"code","a8ca7a22":"code","70e96944":"code","70cda08c":"code","167f0147":"code","01fd99d9":"code","a80c2767":"code","05434347":"code","2c05e0b7":"code","0b534193":"code","e7aafabc":"markdown","e7344197":"markdown","1c65210d":"markdown","c9c668ea":"markdown","5e04cce0":"markdown","86bee714":"markdown","60a678ac":"markdown","83f2469d":"markdown","e54303d3":"markdown","e30af270":"markdown","9d9e8180":"markdown","b596f17e":"markdown","5bbc34d8":"markdown","0f5bae71":"markdown","ac40422c":"markdown","9b977548":"markdown","d8ed8aeb":"markdown","f9ca61a6":"markdown","c92e1b03":"markdown"},"source":{"0aaddf7f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.display import display, Markdown, Latex","4037d38d":"train_data = pd.read_csv(\"..\/input\/Dataset\/Train.csv\", index_col=\"Employee_ID\")\nX_test_raw = pd.read_csv(\"..\/input\/Dataset\/Test.csv\", index_col=\"Employee_ID\")","95ea7b7a":"y_train_raw = train_data.Attrition_rate\nX_train_raw = train_data.drop(\"Attrition_rate\", axis=1)","19feadcf":"sns.distplot(y_train_raw)\nplt.title(\"Distribution of Attrition Rate\")","5ebb09d2":"def fill_missing(data):\n    df = pd.DataFrame.copy(data)\n    df['Pay_Scale'] = df['Pay_Scale'].fillna(df.groupby(['Age', 'Education_Level', 'Time_of_service'])['Pay_Scale'].transform('median'))\n    df['Time_of_service'] = df['Time_of_service'].fillna(df.groupby(['Age', 'Education_Level', 'Pay_Scale'])['Time_of_service'].transform('median'))\n    df['Age'] = df['Age'].fillna(df.groupby(['Education_Level', 'Relationship_Status', 'Time_of_service'])['Age'].transform('median'))\n    df = df.fillna(df.median())\n    return df","c49e7ecb":"X_train_no_na = fill_missing(X_train_raw)\nX_test_no_na = fill_missing(X_test_raw)","4d34e85b":"numerical_cols = list(X_train_no_na.describe().columns)\nplt.subplots(4,4, figsize=(20,20))\ni = 1\nfor col in numerical_cols:\n    plt.subplot(4, 4, i)\n    try:\n        plt.hist(X_train_no_na[col])\n        plt.title(col)\n    finally:\n        i += 1","60b8a23c":"numeric_categoricals = [\n 'Education_Level',\n 'Time_since_promotion',\n 'Travel_Rate',\n 'Post_Level',\n 'Work_Life_balance',\n 'VAR1',\n 'VAR2',\n 'VAR3',\n 'VAR4',\n 'VAR5',\n 'VAR6',\n 'VAR7']\n\nnon_numeric_categoricals = [x for x in X_train_no_na.columns if x not in numerical_cols]","d7be5d40":"def convert_categoricals(data):\n    df = pd.DataFrame.copy(data)\n    for col in numeric_categoricals:\n        df[col] = pd.Categorical(df[col])\n    for col in non_numeric_categoricals:\n        df[col] = pd.Categorical(df[col])\n    return df","2d3ab4d1":"X_train_cats = convert_categoricals(X_train_no_na)\nX_test_cats = convert_categoricals(X_test_no_na)","6899f60f":"\noutput = \"| Categorial Column | Train Categories | Test Categories | Equal |\"\noutput += \"\\n|:--|:--|:--|:--|\"\nfor col in numeric_categoricals + non_numeric_categoricals:\n    output += \"\\n|\" \n    output += col \n    output += \"|\"\n    output += str(sorted(X_train_cats[col].unique()))\n    output += \"|\"\n    output += str(sorted(X_test_cats[col].unique()))\n    output += \"|\"\n    output += [\"No\",\"Yes\"][sorted(X_train_cats[col].unique())==sorted(X_test_cats[col].unique())]\n    output += \"|\"\n\ndisplay(Markdown(output))","a8ca7a22":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnumeric_features = list(X_train_cats.describe().columns)\n\nX_train_normalized = pd.DataFrame.copy(X_train_cats)\nX_train_normalized[numeric_features] = scaler.fit_transform(X_train_cats[numeric_features])\n\nX_test_normalized = pd.DataFrame.copy(X_test_cats)\nX_test_normalized[numeric_features] = scaler.transform(X_test_cats[numeric_features])","70e96944":"X_train_normalized.describe()","70cda08c":"X_train_one_hot = pd.get_dummies(X_train_normalized)\nX_test = pd.get_dummies(X_test_normalized)","167f0147":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_one_hot, y_train_raw, test_size=0.1, random_state=42)","01fd99d9":"model_type = 'boost'\nif model_type == 'boost':\n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n    from sklearn.model_selection import GridSearchCV\n\n    cls = AdaBoostRegressor(random_state=42, n_estimators=100)\n\n    params = {\n        'learning_rate': [0.001, 0.01, 0.1, 1, 10,100],\n        'loss': ['linear', 'square', 'exponential']\n    }\n\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n    grid_search = GridSearchCV(cls, param_grid=params, scoring=scorer, verbose=3)\n    grid_search.fit(X_train, y_train)\n\n    best_cls = grid_search.best_estimator_\nelse:\n    from sklearn.svm import SVR\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n    from sklearn.model_selection import GridSearchCV\n\n    cls = SVR()\n\n    params = {\n        'kernel': ['linear', 'poly', 'rbf'],\n        'degree': [1,2,3,4,5],\n        'gamma': [10,20,30,50,100],\n        'C': [0.1, 0.5, 1, 5, 10]\n    }\n\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n    grid_search = GridSearchCV(cls, param_grid=params, scoring=scorer, verbose=3)\n    grid_search.fit(X_train, y_train)\n\n    best_cls = grid_search.best_estimator_","a80c2767":"pred_train = best_cls.predict(X_train)\nprint(\"[Training]Mean Squared Error:\", mean_squared_error(y_train, pred_train))\nprint(\"[Training]Mean Absolute Error:\", mean_absolute_error(y_train, pred_train))\n\npred_val = best_cls.predict(X_val)\nprint(\"[Validation]Mean Squared Error:\", mean_squared_error(y_val, pred_val))\nprint(\"[Validation]Mean Absolute Error:\", mean_absolute_error(y_val, pred_val))","05434347":"RMSE = mean_squared_error(y_val, pred_val)**0.5\nprint(\"RMSE:\", RMSE)\nscore = 100 * max(0, 1-RMSE)\nprint(\"Score:\", score)","2c05e0b7":"pred_test = pd.DataFrame(best_cls.predict(X_test), columns=['Attrition_rate'], index=X_test.index)\npred_test","0b534193":"pred_test.to_csv(\"Submission.csv\")","e7aafabc":"## Train-Validation Sets","e7344197":"### One Hot Encoding","1c65210d":"## Prediction","c9c668ea":"# Introduction\n\nThis kernel for solving the hackerearth challenge, [Hackerearth Machine Learning Challenge Predict Employee Attrition Rate](https:\/\/www.hackerearth.com\/challenges\/competitive\/hackerearth-machine-learning-challenge-predict-employee-attrition-rate\/)\n\n## Problem statement\n\nEmployees are the most important part of an organization. Successful employees meet deadlines, make sales, and build the brand through positive customer interactions.\n\nEmployee attrition is a major cost to an organization and predicting such attritions is the most important requirement of the Human Resources department in many organizations. In this problem, your task is to predict the attrition rate of employees of an organization.\n\n## Data Variable Description\n|Column Name |\tDescription|\n|:-|:-|\n|Employee_ID| \tUnique ID of each employee|\n|Age| \tAge of each employee|\n|Unit| \tDepartment under which the employee work|\n|Education| \tRating of Qualification of an employee (1-5)|\n|Gender| \tMale-0 or Female-1|\n|Decision_skill_possess| \tDecision skill that an employee possesses|\n|Post_Level| \tLevel of the post in an organization (1-5)|\n|Relationship_Status| \tCategorical Married or Single |\n|Pay_Scale| \tRate in between 1 to 10|\n|Time_of_service| \tYears in the organization|\n|growth_rate| \tGrowth rate in percentage of an employee|\n|Time_since_promotion| \tTime in years since the last promotion|\n|Work_Life_balance| \tRating for work-life balance given by an employee.|\n|Travel_Rate| \tRating based on travel history(1-3)|\n|Hometown| \tName of the city|\n|Compensation_and_Benefits| \tCategorical Variabe|\n|VAR1 - VAR5| \tAnominised variables|\n|Attrition_rate(TARGET VARIABLE)| \tAttrition rate of each employee|","5e04cce0":"Clearly from the above table, we have made a right decision by making these columns categorical as the test data and train data have same values of categories. Also this can be confirmed from the data table given at the Introduction part of this notebook.\n\n**Note**: In some scenerio where test data may get new categories. If test data and train data have same categories, these can be turned to one hot encoding, or else we have to go with categorical values.","86bee714":"# Model","60a678ac":"## Training","83f2469d":"## Validation","e54303d3":"## Data Preprocessing","e30af270":"### Distribution of Training labels","9d9e8180":"For our categorical features, which do not have numerical values, cannot be processed by our model. So we can either map those values to numeric equalent or convert them to one hot encoding\n\nIn our case, we will convert all the categorical features to one hot encoding","b596f17e":"# Datasets","5bbc34d8":"### Convert Categorical Features to Non-Numeric","0f5bae71":"#### Score according to Hackerearth:\nscore = 100 * max(0, 1 - RMSE(actual_values, predicted_values))","ac40422c":"### Missing Values","9b977548":"### Distribution of Features","d8ed8aeb":"From the histograms, its clear that only continous features are **Age**, **Time_of_service**, **growth_rate** and **Pay_Scale**. All other features are actually categorical but are having categories that are defined as numbers. So we convert all the categorical values to have non_numeric type","f9ca61a6":"As it can be seen that most of the employees fall in attrition rate of 0-20%. i.e., employees are less likely to leave the company.","c92e1b03":"### Normalize the Numeric Features"}}