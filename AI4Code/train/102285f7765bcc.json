{"cell_type":{"69f3cd4c":"code","31abac0f":"code","217d3572":"code","d5ee3884":"code","60ac2539":"code","c07c1cec":"code","c5e4c0f6":"code","1350a063":"code","5b5577ba":"code","14680736":"code","0350e19c":"code","ee3987b0":"code","49c9b7a7":"code","f125a710":"code","0a4588a6":"code","cb3172d5":"code","7ca34a58":"code","17668bd4":"code","49c991bc":"code","b3bb5f56":"code","7c6d27cf":"markdown","23406f32":"markdown","ddfc3eec":"markdown","553707ed":"markdown","dc9de496":"markdown","2ee8cf38":"markdown","d14fd000":"markdown","75d46441":"markdown","6211a345":"markdown","07fdb491":"markdown","e51abbf8":"markdown","59604d80":"markdown","16668222":"markdown","d2990279":"markdown","5b5fb4e7":"markdown","3f87e289":"markdown","3bc10ff7":"markdown","9f77af65":"markdown","a52ffb92":"markdown","f50a8b2d":"markdown","3dac30a5":"markdown","425ebaa5":"markdown","40b4fad3":"markdown","9ccd7463":"markdown"},"source":{"69f3cd4c":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score","31abac0f":"# Input path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","217d3572":"train = pd.read_csv('\/kaggle\/input\/instant-gratification\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/instant-gratification\/test.csv')","d5ee3884":"train.head()","60ac2539":"train.info()","c07c1cec":"train.describe()","c5e4c0f6":"sns.countplot(train.target)","1350a063":"def plot_feature_distplot(df, features):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        sns.distplot(df[feature])\n        plt.xlabel(feature, fontsize=9)\n    plt.show();","5b5577ba":"cols = [col for col in train.columns if col not in [\"id\", \"target\"]]\n\n# distribution plot for first 16 variables\nplot_feature_distplot(train, cols[0:16])","14680736":"plt.figure(figsize=[16,9])\nsns.heatmap(train[cols].corr())","0350e19c":"train.info()","ee3987b0":"train.dtypes[train.dtypes == np.int64]","49c9b7a7":"train['wheezy-copper-turtle-magic'].value_counts()","f125a710":"train['wheezy-copper-turtle-magic'].nunique()","0a4588a6":"print('Train set')\nprint('Minimum value of wheezy-copper-turtle-magic:',train['wheezy-copper-turtle-magic'].min())\nprint('Maximum value of wheezy-copper-turtle-magic:',train['wheezy-copper-turtle-magic'].max())","cb3172d5":"cols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\n\n# Stratified K-fold\nskf = StratifiedKFold(n_splits=5)\n \nfor train_idx, val_idx in skf.split(train[cols], train['target']):\n    \n    # LR model\n    clf = LogisticRegression()\n    clf.fit(train.loc[train_idx][cols], train.loc[train_idx]['target'])\n    oof[val_idx] = clf.predict_proba(train.loc[val_idx][cols])[:,1]\n\nauc = roc_auc_score(train['target'],oof)\nprint('LR CV score w\/o Magic feature =',round(auc,4))","7ca34a58":"cols = [c for c in train.columns if c not in ['id', 'target']]\n\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\n\n# Build 512 models\nfor i in range(512):\n    \n    # train the data for each value of 'wheezy-copper-turtle-magic'\n    train1 = train[train['wheezy-copper-turtle-magic']==i]     \n    \n    idx1 = train1.index\n    train1.reset_index(drop = True, inplace = True)\n    \n    # Stratified K-fold\n    skf = StratifiedKFold(n_splits = 5)     \n    \n    for train_idx, val_idx in skf.split(train1[cols], train1['target']):\n        \n        # LR model \n        clf = LogisticRegression(solver = 'liblinear', penalty = 'l1', C = 0.05)\n        clf.fit(train1.loc[train_idx][cols], train1.loc[train_idx]['target'])\n        oof[idx1[val_idx]] = clf.predict_proba(train1.loc[val_idx][cols])[:,1]\n    \nauc = roc_auc_score(train['target'],oof)       ","17668bd4":"print('LR CV score with Magic feature =',round(auc,4)) ","49c991bc":"cols = [c for c in train.columns if c not in ['id', 'target']]\n\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\n\n# Build 512 models\nfor i in range(512):\n    \n    # train the data for each value of 'wheezy-copper-turtle-magic'\n    train1 = train[train['wheezy-copper-turtle-magic']==i]     \n    \n    idx1 = train1.index\n    train1.reset_index(drop = True, inplace = True)\n    \n    # Dropping low-variance features (fit and transform)\n    sel = VarianceThreshold(threshold = 1.5).fit(train1[cols])\n    train2 = sel.transform(train1[cols])\n    \n    # Stratified K-fold\n    skf = StratifiedKFold(n_splits = 5)     \n    \n    for train_idx, val_idx in skf.split(train2, train1['target']):\n        \n        # QDA model \n        clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        clf.fit(train2[train_idx,:], train1.loc[train_idx]['target'])\n        oof[idx1[val_idx]] = clf.predict_proba(train2[val_idx,:])[:,1]\n    \nauc = roc_auc_score(train['target'],oof)  ","b3bb5f56":"print('QDA, CV score =',round(auc,4))","7c6d27cf":"# <a id='0'>Content<\/a>\n\n- <a href='#1'>1. Read the data<\/a>\n- <a href='#2'>2. Data Understanding<\/a>\n- <a href='#3'>3. Data Exploration<\/a>\n - <a href='#7'>3.1 Distribution of Y variable<\/a>\n - <a href='#8'>3.2 Distribution of X variables<\/a>\n - <a href='#9'>3.3 Correlation<\/a>\n- <a href='#4'>4. Magic Feature<\/a>\n- <a href='#5'>5. Model (LR)<\/a>\n - <a href='#10'>5.1 Model w\/o Magic feature<\/a>\n - <a href='#11'>5.2 Model with Magic feature<\/a>\n- <a href='#6'>6. Model (QDA)<\/a>","23406f32":"### Here, we are gong to build the model by considering both options\n1. Treat the column 'wheezy-copper-turtle-magic' as numeric\n2. Treat the column 'wheezy-copper-turtle-magic' as category","ddfc3eec":"### <a id='8'>3.2 Distribution of X variables<\/a>","553707ed":"### Conclusion: \nWe can see huge difference in the CV score with Magic feature\n1. LR, CV score without Magic feature: 0.53\n2. LR, CV score with Magic feature:    0.79","dc9de496":"### Conclusion: \nTarget variable seems to be equally distributed","2ee8cf38":"## <a id='4'>4. Magic Feature<\/a>","d14fd000":"### Conclusion: \nFrom heatmap, it seems to be there is no relation between 'X' variables.","75d46441":"## <a id='2'>2. Data Understanding<\/a>","6211a345":"## <a id='6'>6. Model (QDA)<\/a>","07fdb491":"## <a id='1'>1. Read the data<\/a>","e51abbf8":"### <a id='10'>5.1 Model w\/o Magic Feature<\/a>","59604d80":"## <a id='5'>5. Model (LR)<\/a>","16668222":"### <a id='11'>5.2 Model with Magic Feature<\/a>","d2990279":"### <a id='7'>3.1 Distribution of Y variable<\/a>","5b5fb4e7":"### <a id='9'>3.3 Correlation<\/a>","3f87e289":"### Conclusion: \nAll 'X' variables seems to be normally distributed.","3bc10ff7":"<img src = \"https:\/\/storage.googleapis.com\/kaggle-forum-message-attachments\/543450\/13399\/Untitled.jpg\" width = \"400\"><\/img>","9f77af65":"### There are two 'int' type columns in train dataset\n1. wheezy-copper-turtle-magic\n2. target","a52ffb92":"#### Column 'wheezy-copper-turtle-magic' can be treated as numeric or category.","f50a8b2d":"# Introduction","3dac30a5":"### Here, we consider the column 'wheezy-copper-turtle-magic' as category and build 512 models for each value","425ebaa5":"### Summary\n\nQDA outperforms LR and other models as well.\n\nThe dataset most likely was produced by sklearn.datasets make_classification. This method generates clusters of gaussians with non-diagonal covariance matrix and assigns them classes. QDA works exactly with this structure of data, it learns normal distributions with n-dimentional covariance matrix.\n\nQDA works by finding the multivariate Gaussian distribution of target=1 and finding the multivariate Gaussian distribution of target=0. A multivariate Gaussian distribution is an hyper-ellipsoid in p dimensional space where p is the number of variables.\n\nFor more information, please refer:\nhttps:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/93843","40b4fad3":"### Here, we consider the column 'wheezy-copper-turtle-magic' as numeric and build the model","9ccd7463":"## <a id='3'>3. Data Exploration<\/a>"}}