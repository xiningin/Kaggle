{"cell_type":{"2b13a49b":"code","c0120aaa":"code","39cefa4a":"code","a48a977e":"code","c15202e1":"code","88b9e333":"code","fcbb8775":"code","9e8e4c7c":"code","01a0d586":"code","1da5fd8c":"code","44459506":"code","77362330":"code","d54ce4c6":"code","1eb68365":"code","69a99102":"code","e461af0c":"code","cb6d4c5f":"code","6e13468a":"code","8a725888":"code","647703c1":"code","9eef9515":"code","b1b67f19":"code","f084eed4":"code","0eb16a27":"code","8e2643ea":"code","fce6c465":"code","146a2280":"code","32f04f3a":"code","8ed55b25":"code","26bbdfab":"code","9d7b9c97":"code","026ebfac":"code","7fe2a3d5":"code","b5c81fac":"code","fba1b6f8":"code","93f1c821":"code","2b4e79d7":"code","cad84da8":"code","c88f8df1":"code","838f3a5e":"code","53f90251":"code","93c462db":"code","8362cf73":"code","bc8bc656":"code","a4b619dd":"code","725c80f0":"code","25b5ffad":"code","1e54c6bf":"code","51117c84":"code","f9b0f927":"code","eac75337":"code","2e9bc27d":"code","af3843dd":"code","fe72bd48":"code","298d1c5d":"code","2163b1ee":"code","20bb8817":"code","89256223":"code","e557b1b7":"code","feff123e":"code","a2e50169":"code","ed194c33":"code","99267afb":"code","7d2e632a":"code","9a2f9de9":"code","4cb0f664":"code","4d2c9b49":"code","6956edff":"code","b1601cbd":"code","eabd4dd7":"code","ea7d6544":"code","c08ee641":"code","2d4891ce":"markdown","a6a71728":"markdown","4f75f530":"markdown","f2051b08":"markdown","fabfb9b6":"markdown","22a40895":"markdown","1a451d79":"markdown","4c88e93e":"markdown","38d511c0":"markdown","c4eb39d6":"markdown","d15040c3":"markdown","9de6f6c6":"markdown","508531f0":"markdown","fdc3bfbe":"markdown","22647890":"markdown","2cb62a10":"markdown","918081bf":"markdown","78add3d5":"markdown","b6b51f13":"markdown","bafe5df3":"markdown","aa33b90a":"markdown","42f3e5e9":"markdown","7417599c":"markdown","67d88442":"markdown","3db1b839":"markdown","6b922e07":"markdown","c420fb93":"markdown","41d9acd4":"markdown","05ebd554":"markdown","4997aa61":"markdown","67294c3f":"markdown","8c52bdb2":"markdown","8a304c92":"markdown","934e05f7":"markdown","8db7d5c3":"markdown","cc07ca57":"markdown","cd05b93a":"markdown","b99b6a60":"markdown","4436c3c4":"markdown","f338b945":"markdown","af622bcd":"markdown","758182ae":"markdown","db7ea0e5":"markdown","8a583d38":"markdown","2c83136f":"markdown","0cc137ea":"markdown","92fb0b1e":"markdown","6817675f":"markdown","b9298696":"markdown","101f9072":"markdown","9e59767f":"markdown","11ef8f6c":"markdown","efb237f8":"markdown"},"source":{"2b13a49b":"import numpy as np\nimport pandas as pd\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# stat on data\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n# # import library for machine learning\nfrom sklearn import preprocessing, model_selection, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","c0120aaa":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train = df_train.drop(columns=['Id'],axis=1)\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test = df_test.drop(columns=['Id'],axis=1)","39cefa4a":"df_train['SalePrice']","a48a977e":"print('Shape of the file')\nprint('-'*30)\nprint(df_train.shape)","c15202e1":"def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, f'Columns with less than {thresh} missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","88b9e333":"msv1(df_train, 20, color=sns.color_palette('Reds',15))","fcbb8775":"df_train = df_train.dropna(thresh=len(df_train)*0.8, axis=1)","9e8e4c7c":"msv1(df_test, 20, color=sns.color_palette('Reds',15))","01a0d586":"df_test = df_test.dropna(thresh=len(df_test)*0.8, axis=1)","1da5fd8c":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ntarget = df_train[['SalePrice']]\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","44459506":"# create list with columns having NaN values\ncol_with_NaN_value = all_data.columns[all_data.isnull().any()]\n# create df with only columns having NaN value \ndf_nan = all_data[col_with_NaN_value]\n# count number of NaN value per columns\nprint('Number of NaN values')\nprint('-'*30)\ndf_nan.isnull().sum()","77362330":"def replace_nan_value(df):\n    # LotFrontage: replace Nan By mean\n    df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())\n    df['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(df['BsmtFinSF1'].mean())\n    df['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(df['BsmtFinSF2'].mean())\n    df['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(df['BsmtUnfSF'].mean())\n    df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(df['TotalBsmtSF'].mean())\n    \n    # Alley,MasVnrType,Bsmtcond,BsmtExposure,BsmtFinType1,Garage : replace nan by none\n    df['MasVnrType'] = df['MasVnrType'].replace(np.nan, 'none')\n    df['BsmtCond'] = df['BsmtCond'].replace(np.nan, 'none')\n    df['BsmtExposure'] = df['BsmtExposure'].replace(np.nan, 'none')\n    df['BsmtFinType1'] = df['BsmtFinType1'].replace(np.nan, 'none')\n    df['GarageType'] = df['GarageType'].replace(np.nan, 'none')\n    df['GarageFinish'] = df['GarageFinish'].replace(np.nan, 'none')\n    df['GarageQual'] = df['GarageQual'].replace(np.nan, 'none')\n    df['GarageCond'] = df['GarageCond'].replace(np.nan, 'none')\n    # MasVnrArea, BsmtQual : replace nan by '0',(because MasVnrType = none)\n    df['MasVnrArea'] = df['MasVnrArea'].fillna(0)\n    df['BsmtQual'] = df['BsmtQual'].fillna(0)\n    df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\n    df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)    # BsmtFinType2: replave nan by '0' if BsmtFinType1 = none else by average\n    df.loc[df['BsmtFinType1'] == 'none', 'BsmtFinType2'] = 'none'\n    return df","d54ce4c6":"replace_nan_value(all_data)","1eb68365":"all_data['BsmtFinType2'] = all_data['BsmtFinType2'].replace(np.nan,'Unf')\n# Electrical: replace by standard\nall_data['Electrical'] = all_data['Electrical'].replace(np.nan,'SBrkr')\nall_data['Utilities'] = all_data['Utilities'].replace(np.nan, 'AllPub')\nall_data['Exterior1st'] = all_data['Exterior1st'].replace(np.nan, 'VinylSd')\nall_data['Exterior2nd'] = all_data['Exterior2nd'].replace(np.nan, 'VinylSd')\nall_data['MSZoning'] = all_data['MSZoning'].replace(np.nan, 'RL')\nall_data['KitchenQual'] = all_data['KitchenQual'].replace(np.nan, 'TA')\nall_data['Functional'] = all_data['Functional'].replace(np.nan, 'Typ')\nall_data['GarageCars'] = all_data['GarageCars'].fillna(2)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(519.042857)\nall_data['SaleType'] = all_data['SaleType'].replace(np.nan, 'WD')","69a99102":"# verify if still NaN values:\ncol_with_NaN_value_all_data = all_data.columns[all_data.isnull().any()]\nprint(len(col_with_NaN_value_all_data))","e461af0c":"# To replace the nan values in the remaining columns, we used countplot to see the dominant category\nsns.countplot(x='Electrical',data=all_data)","cb6d4c5f":"# df_train.dtypes[df_train.dtypes == 'int64']\n# df_train.dtypes[df_train.dtypes == 'float64']\n# list_var_cat = df_train.dtypes[df_train.dtypes != 'int64'][df_train.dtypes != 'float64']","6e13468a":"def new_feature(df):\n    df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] +df['GarageArea']\n    df['Bathrooms'] = df['FullBath'] + df['HalfBath']*0.5 \n    df['Year average']= (df['YearRemodAdd']+df['YearBuilt'])\/2\n    return df","8a725888":"new_feature(all_data)\nnew_feature(df_train)","647703c1":"# get stat:\nall_data['GarageYrBlt'].describe()","9eef9515":"def garage_year_to_cat(df):\n    # assign 0 if no garage:\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(1)\n    # Using cut() - create category ranges and names: is year = 1 then 'never'\n    ranges = [0,1899,1925,1950,1975,2000,2020]\n    group_garage_year_built = ['never','inf 1925', '1925-1950', '1950-1975','1975-2000','2000-2020']\n    # Create income group column\n    df['GarageYrBlt_group'] = pd.cut(df['GarageYrBlt'], bins=ranges,labels=group_garage_year_built)\n    # drop column GarageYrBlt':\n    df = df.drop(['GarageYrBlt'], axis=1)\n\ngarage_year_to_cat(all_data)\ngarage_year_to_cat(df_train)","b1b67f19":"# Print unique values for categorical variables:\nprint('MasVnrType: ', all_data['MasVnrType'].unique(), \"\\n\")\nprint('LotShape', all_data['LotShape'].unique(), \"\\n\")\nprint('LandContour', all_data['LandContour'].unique(), \"\\n\")\nprint('Utilities', all_data['Utilities'].unique(), \"\\n\")            ## two Utilies do not appear in the test set\nprint('LotConfig', all_data['LotConfig'].unique(), \"\\n\")\nprint('LandSlope', all_data['LandSlope'].unique(), \"\\n\")\nprint('Neighborhood', all_data['Neighborhood'].unique(), \"\\n\")  \nprint('Condition1', all_data['Condition1'].unique(), \"\\n\")\nprint('Condition2', all_data['Condition1'].unique(), \"\\n\")\nprint('RoofStyle', all_data['RoofStyle'].unique(), \"\\n\")\nprint('Exterior2nd', all_data['Exterior2nd'].unique(), \"\\n\")","f084eed4":"def collapsing_cat(df):\n    # merge IR1= 'Slightly irregular' with IR2 = 'Moderately Irregular'\n    df.loc[df['LotShape'] == 'IR2', 'LotShape'] = 'IR1'\n\n    # simplity category to: 200ft of an RR and adjacent to a RR \n    df.loc[(df['Condition1'] == 'RRNn')|(df['Condition1'] == 'RRNe'), 'Condition1'] = '200ft_RR'\n    df.loc[(df['Condition1'] == 'RRAn')|(df['Condition1'] == 'RRAe'), 'Condition1'] = 'ADJ_RR'\n\n    df.loc[(df['Condition2'] == 'RRNn')|(df['Condition2'] == 'RRNe'), 'Condition2'] = '200ft_RR'\n    df.loc[(df['Condition2'] == 'RRAn')|(df['Condition2'] == 'RRAe'), 'Condition2'] = 'ADJ_RR'\n\n    # simplify RoofMatl to: wood\n    df.loc[(df['RoofMatl'] == 'WdShake')|(df['RoofMatl'] == 'WdShngl'), 'RoofMatl'] = 'wood'\n\n    # simplify Exterior1s: base on appearance stone.concret,wood, stucco etc..\n    df.loc[(df['Exterior1st'] == 'AsbShng')|(df['Exterior1st'] == 'AsphShn')|(df['Exterior1st'] == 'WdShing'), 'Exterior1st'] = 'shingles'\n    df.loc[(df['Exterior1st'] == 'BrkComm')|(df['Exterior1st'] == 'BrkFace'), 'Exterior1st'] = 'brick'\n    df.loc[(df['Exterior1st'] == 'CBlock')|(df['Exterior1st'] == 'CemntBd')|(df['Exterior1st'] == 'Stone'), 'Exterior1st'] = 'concret'\n    df.loc[(df['Exterior1st'] == 'CBlock')|(df['Exterior1st'] == 'CemntBd'), 'Exterior1st'] = 'concret'\n    df.loc[(df['Exterior1st'] == 'HdBoard')|(df['Exterior1st'] == 'Plywood')|(df['Exterior1st'] == 'VinylSd')|(df['Exterior1st'] == 'Wd Sdng'), 'Exterior1st'] = 'wood'\n    df.loc[(df['Exterior1st'] == 'ImStucc')|(df['Exterior1st'] == 'Stucco'), 'Exterior1st'] = 'Stucco'\n    # simplify Exterior2s: base on appearance stone.concret,wood, stucco etc..\n    df.loc[(df['Exterior2nd'] == 'AsbShng')|(df['Exterior2nd'] == 'AsphShn')|(df['Exterior2nd'] == 'WdShing'), 'Exterior2nd'] = 'shingles'\n    df.loc[(df['Exterior2nd'] == 'BrkComm')|(df['Exterior2nd'] == 'BrkFace'), 'Exterior2nd'] = 'brick'\n    df.loc[(df['Exterior2nd'] == 'CBlock')|(df['Exterior2nd'] == 'CemntBd')|(df['Exterior2nd'] == 'Stone'), 'Exterior2nd'] = 'concret'\n    df.loc[(df['Exterior2nd'] == 'CBlock')|(df['Exterior2nd'] == 'CemntBd'), 'Exterior2nd'] = 'concret'\n    df.loc[(df['Exterior2nd'] == 'HdBoard')|(df['Exterior2nd'] == 'Plywood')|(df['Exterior2nd'] == 'VinylSd')|(df['Exterior2nd'] == 'Wd Sdng'), 'Exterior2nd'] = 'wood'\n    df.loc[(df['Exterior2nd'] == 'ImStucc')|(df['Exterior2nd'] == 'Stucco'), 'Exterior2nd'] = 'Stucco'\n\n    # simplify MasVnrType: \n    df.loc[(df['MasVnrType'] == 'BrkCmn')|(df['MasVnrType'] == 'BrkFace'), 'MasVnrType'] = 'brick'\n\n    # simplify heating: \n    df.loc[(df['Heating'] == 'Floor')|(df['Heating'] == 'Wall')|(df['Heating'] == 'Grav'), 'Heating'] = 'furnace'\n    df.loc[(df['Heating'] == 'GasA')|(df['Heating'] == 'GasW'), 'Heating'] = 'Gas'\n\n    # simplify Functional: \n    df.loc[(df['Functional'] == 'Min1')|(df['Functional'] == 'Min2'), 'Functional'] = 'Min'\n    df.loc[(df['Functional'] == 'Maj1')|(df['Functional'] == 'Maj2'), 'Functional'] = 'Maj'\n    return df\n","0eb16a27":"collapsing_cat(all_data)\ncollapsing_cat(df_train)","8e2643ea":"datatype = all_data.dtypes\nlist_col_object  = datatype[(datatype == 'object')].index.tolist()\n# convert object dtypes to category\nfor col in list_col_object:\n    all_data[col] = all_data[col].astype('category')\n\n# convert int dtypes to category    \ndef convert_int_to_cat(df):\n    df['MSSubClass'] = df['MSSubClass'].astype('category')\n    df['OverallQual'] = df['OverallQual'].astype('category')\n    df['OverallCond'] = df['OverallCond'].astype('category')\n    df['YrSold'] = df['YrSold'].astype('category')\n    df['MoSold'] = df['MoSold'].astype('category')\n    return df\n\nconvert_int_to_cat(all_data)\nconvert_int_to_cat(df_train)\n","fce6c465":"#descriptive statistics summary\ntarget['SalePrice'].describe()","146a2280":"sns.distplot(target['SalePrice'], fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(target['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(target['SalePrice'], plot=plt)\nplt.show()","32f04f3a":"#skewness and kurtosis\nprint(\"Skewness: %f\" % target['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % target['SalePrice'].kurt())","8ed55b25":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\ntarget['SalePrice'] = np.log1p(target['SalePrice'])\n#Check the new distribution \nsns.distplot(target['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(target['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(target['SalePrice'], plot=plt)\nplt.show()","26bbdfab":"#skewness and kurtosis\nprint(\"Skewness: %f\" % target['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % target['SalePrice'].kurt())","9d7b9c97":"list_col_numeric = all_data.dtypes[all_data.dtypes != \"category\"].index\n# Check the skew of all numerical features\nskewed_feats = all_data[list_col_numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(5)","026ebfac":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","7fe2a3d5":"# Check the skew of all numerical features\nskewed_feats = all_data[list_col_numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(5)","b5c81fac":"# update list with categorical variables\nupdated_datatype = all_data.dtypes\nlist_col_numeric  = updated_datatype[(updated_datatype != 'category')].index.tolist()\nlist_col_category = updated_datatype[(updated_datatype == 'category')].index.tolist()","fba1b6f8":"corr = df_train.corr(method='pearson')\n\nfig, axes = plt.subplots(1,figsize=(18,18))\nax0 = plt.subplot(1,1,1)\nsns.heatmap(corr,annot=True,linewidths=.5, annot_kws={\"size\": 10},vmin=-1.0, vmax=1.0,square=True,cbar=True)\n# bottom, top = ax0.get_ylim()\n# ax0.set_ylim(bottom + 0.5, top - 0.5)\nax0.set_title('correlations between numerical variables',size=18,y=1.05)\nax0.set_yticklabels(ax0.get_yticklabels(), rotation=0,size=14) \nax0.set_xticklabels(ax0.get_xticklabels(), rotation=90,size=14) \nplt.show()","93f1c821":"def plot_correlation(df,var1,var2,color):\n    \"\"\"\n    plot data and a linear regression model fit. \n    Parameters\n    ----------\n    df : dataframe\n    var1: 'column_name' of the variable 1 in df\n    var2: 'column_name' of the variable 2 in df\n    color : color scatter points\n    Returns\n    -------\n    Figure  \n    \"\"\"\n    # transform var1 and var2 into numpy array:\n    xm = np.array(df[var1])\n    ym = np.array(df[var2])\n    # get regression line properties:\n    slope, intercept, r_value, p_value, std_err = stats.linregress(xm, ym)\n    # Plot linear regression with 95% confidence interval and the regression coefficient\n    sns.regplot(var1,var2,data=df,fit_reg=True,color = color,\n                line_kws={'label':\"R={:.2f}\".format(r_value),\"color\": \"black\"}) \n    # axes and title properties\n    plt.xlabel(var1,fontsize=15)\n    plt.ylabel(var2,fontsize=15)\n    # plot legend\n    plt.legend(prop={'size': 15})\n    \ndef plot_residual(df,var1,var2,color):\n    \"\"\"\n    plot the residual plot for independent variable GrLivArea and our target variable SalePrice .the residual plot for independent variable GrLivArea and our target variable SalePrice .\n    Parameters\n    ----------\n    df : dataframe\n    var1: 'column_name' of the variable 1 in df\n    var2: 'column_name' of the variable 2 in df\n    color : color scatter points\n    Returns\n    -------\n    Figure  \n    \"\"\"\n    sns.residplot(var1,var2,data=df,color = color)\n    # axes and title properties\n    plt.xlabel(var1,fontsize=15)\n    plt.ylabel(var2,fontsize=15)\n    # plot legend\n    plt.legend(prop={'size': 15})\n    ","2b4e79d7":"# define figure size\nfig = plt.figure(figsize=(18,15))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between numerical variables and sale prices', fontsize=18,y=0.9)\n\n# subplot\nax1 = fig.add_subplot(6,2,1)\nax1 = plot_correlation(df_train,'YearBuilt','SalePrice','blue')\nax2 = fig.add_subplot(6,2,2)\nax2 = plot_correlation(df_train,'YearRemodAdd','SalePrice','green')\nax3 = fig.add_subplot(6,2,3)\nax3 = plot_correlation(df_train,'TotalBsmtSF','SalePrice','red')\nax4 = fig.add_subplot(6,2,4)\nax4 = plot_correlation(df_train,'1stFlrSF','SalePrice','black')\nax5 = fig.add_subplot(6,2,5)\nax5 = plot_correlation(df_train,'GrLivArea','SalePrice','c')\nax6 = fig.add_subplot(6,2,6)\nax6 = plot_correlation(df_train,'FullBath','SalePrice','m')\nax7 = fig.add_subplot(6,2,7)\nax7 = plot_correlation(df_train,'TotRmsAbvGrd','SalePrice','olive')\nax8 = fig.add_subplot(6,2,8)\nax8 = plot_correlation(df_train,'GarageCars','SalePrice','gold')\nax9 = fig.add_subplot(6,2,9)\nax9 = plot_correlation(df_train,'GarageArea','SalePrice','grey')\nax10 = fig.add_subplot(6,2,10)\nax10 = plot_correlation(df_train,'TotalArea','SalePrice','darkblue')\nax9 = fig.add_subplot(6,2,11)\nax9 = plot_correlation(df_train,'Bathrooms','SalePrice','lime')\nax10 = fig.add_subplot(6,2,12)\nax10 = plot_correlation(df_train,'Year average','SalePrice','purple')\n\nplt.show()","cad84da8":"# define figure size\nfig = plt.figure(figsize=(18,15))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between numerical variables and sale prices', fontsize=18,y=0.9)\n\n# subplot\nax1 = fig.add_subplot(6,2,1)\nax1 = plot_residual(df_train,'YearBuilt','SalePrice','blue')\nax2 = fig.add_subplot(6,2,2)\nax2 = plot_residual(df_train,'YearRemodAdd','SalePrice','green')\nax3 = fig.add_subplot(6,2,3)\nax3 = plot_residual(df_train,'TotalBsmtSF','SalePrice','red')\nax4 = fig.add_subplot(6,2,4)\nax4 = plot_residual(df_train,'1stFlrSF','SalePrice','black')\nax5 = fig.add_subplot(6,2,5)\nax5 = plot_residual(df_train,'GrLivArea','SalePrice','c')\nax6 = fig.add_subplot(6,2,6)\nax6 = plot_residual(df_train,'FullBath','SalePrice','m')\nax7 = fig.add_subplot(6,2,7)\nax7 = plot_residual(df_train,'TotRmsAbvGrd','SalePrice','olive')\nax8 = fig.add_subplot(6,2,8)\nax8 = plot_residual(df_train,'GarageCars','SalePrice','gold')\nax9 = fig.add_subplot(6,2,9)\nax9 = plot_residual(df_train,'GarageArea','SalePrice','grey')\nax10 = fig.add_subplot(6,2,10)\nax10 = plot_residual(df_train,'TotalArea','SalePrice','darkblue')\nax9 = fig.add_subplot(6,2,11)\nax9 = plot_residual(df_train,'Bathrooms','SalePrice','lime')\nax10 = fig.add_subplot(6,2,12)\nax10 = plot_residual(df_train,'Year average','SalePrice','purple')\n\nplt.show()","c88f8df1":"print(df_train['TotalBsmtSF'].sort_values(ascending=False).head(1))\nprint(df_train['GrLivArea'].sort_values(ascending=False).head(2))\nprint(df_train['1stFlrSF'].sort_values(ascending=False).head(1))\n# print(df_train['TotalArea'].sort_values(ascending=False).head(2))","838f3a5e":"positon_outliers = [1298,523]\ndf_train = df_train.drop(df_train.index[positon_outliers])","53f90251":"# define figure size\nfig = plt.figure(figsize=(18,8))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('Highest correlations found between numerical variables and sale prices', fontsize=18)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_correlation(df_train,'TotalBsmtSF','SalePrice','red')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_correlation(df_train,'1stFlrSF','SalePrice','black')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_correlation(df_train,'GrLivArea','SalePrice','c')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_correlation(df_train,'TotalArea','SalePrice','darkblue')","93c462db":"# define figure size\nfig = plt.figure(figsize=(18,8))\nfig.subplots_adjust(hspace=0.4,wspace=0.3)\n\n# figure title\nfig.suptitle('residual', fontsize=18)\n\n# subplot\nax1 = fig.add_subplot(2,2,1)\nax1 = plot_residual(df_train,'TotalBsmtSF','SalePrice','red')\nax2 = fig.add_subplot(2,2,2)\nax2 = plot_residual(df_train,'1stFlrSF','SalePrice','black')\nax3 = fig.add_subplot(2,2,3)\nax3 = plot_residual(df_train,'GrLivArea','SalePrice','c')\nax4 = fig.add_subplot(2,2,4)\nax4 = plot_residual(df_train,'TotalArea','SalePrice','darkblue')","8362cf73":"print(len(list_col_category))","bc8bc656":"fig = plt.figure(figsize=(15,40))\nfig.subplots_adjust(hspace = 0.4, wspace=0.3)\n\nlist_col_category = df_train.dtypes[df_train.dtypes == \"object\"].index\n# Create subplot based of target\nfor i,col in zip (range(1,50),list_col_category):\n    # sort boxplot by median value\n    grouped = df_train.loc[:,[col, \"SalePrice\"]].groupby([col]).median().sort_values(by=\"SalePrice\")\n    ax = fig.add_subplot(16,3,i)\n    ax = sns.boxplot(x=col, y=\"SalePrice\", data=df_train,order = grouped.index)\n    \nplt.show()\n","a4b619dd":"# get dummies\ndf_all_data = pd.get_dummies(all_data,drop_first = True)","725c80f0":"df_train2 = df_all_data[:ntrain]\ndf_test2 = df_all_data[ntrain:]","25b5ffad":"print(df_train2.shape)\nprint(df_test2.shape)","1e54c6bf":"# remove outliers:\npositon_outliers = [1298,523]\ndf_train2 = df_train2.drop(df_train2.index[positon_outliers]).values\ntarget = target.drop(target.index[positon_outliers]).values","51117c84":"from sklearn.model_selection import train_test_split\nX = df_train2\nX_train, X_test, y_train, y_test = train_test_split(X, target,test_size = .3, random_state=0)","f9b0f927":"from sklearn.preprocessing import RobustScaler\nscaler= RobustScaler()\n# transform \"x_train\"\nX_train = scaler.fit_transform(X_train)\n# transform \"x_test\"\nX_test = scaler.transform(X_test)\n\n#Transform the test set\nX_df_test= scaler.fit_transform(df_test2)\n","eac75337":"import math\ndef score(y_pred):\n    return str(math.sqrt(mean_squared_error(y_test, y_pred)))\n\ndef get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score\n\nfrom statsmodels.graphics.api import abline_plot\ndef model_evaluation(prediction):\n    print(\"R2 (explained variance):\", round(metrics.r2_score(y_test, prediction), 2))\n    print(\"Mean Absolute Perc Error (\u03a3(|y-pred|\/y)\/n):\", np.mean(np.abs((y_test-prediction)\/prediction)))\n    print(\"Mean Absolute Error (\u03a3|y-pred|\/n):\", \"{:,f}\".format(metrics.mean_absolute_error(y_test, prediction)))\n    print(\"Root Mean Squared Error (sqrt(\u03a3(y-pred)^2\/n)):\", \"{:,f}\".format(np.sqrt(metrics.mean_squared_error(y_test, prediction))))\n    ## residuals\n    prediction = prediction.reshape(len(prediction),1)\n    residuals = y_test - prediction\n    if abs(max(residuals)) > abs(min(residuals)):\n        max_error = max(residuals)  \n    else:\n        max_error = min(residuals) \n    max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))\n    max_true = y_test[max_idx]\n    max_pred = prediction[max_idx]\n    print(\"Max Error:\", \"{}\".format(max_error))\n    \n    ## Plot predicted vs true\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n    ax[0].scatter(prediction, y_test, color=\"black\")\n    abline_plot(intercept=0, slope=1, color=\"red\", ax=ax[0])\n    ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[0].grid(True)\n    ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")\n    ax[0].legend()\n\n    ## Plot predicted vs residuals\n    ax[1].scatter(prediction, residuals, color=\"red\")\n    ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label=\"max error\")\n    ax[1].grid(True)\n    ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n    ax[1].hlines(y=0, xmin=np.min(prediction), xmax=np.max(prediction))\n    ax[1].legend()\n    plt.show()\n","2e9bc27d":"# linreg = LinearRegression()\n# parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n# grid_linear = GridSearchCV(linreg, parameters, cv=12, verbose=1 , scoring = 'neg_mean_squared_error')\n# grid_linear.fit(X_train, y_train)\n\n# sc_linear = get_best_score(grid_linear)","af3843dd":"## call model 1 with all the variables\nmodel_LinReg = linear_model.LinearRegression(copy_X= True, fit_intercept= True, normalize= False)\nmodel_LinReg.fit(X_train,y_train)\nprediction = model_LinReg.predict(X_test)\nmodel_evaluation(prediction)","fe72bd48":"# ## Importing Ridge. \n# from sklearn.linear_model import Ridge\n# ridge = Ridge()\n# parameters = {'alpha':[1e-6,1e-5,1e-4,1e-3,1e-2,0.1,0.5,1], \n#               'tol':[1e-5,1e-4,1e-3]}\n# grid_ridge = GridSearchCV(ridge, parameters, cv=12, verbose=1, scoring = 'neg_mean_squared_error')\n# grid_ridge.fit(X_train, y_train)\n\n# sc_ridge = get_best_score(grid_ridge)","298d1c5d":"## best fit: \nridge_bf = Ridge(alpha= 1, normalize=False,tol = 1e-05)\n## fit the model. \nridge_bf.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = ridge_bf.predict(X_test)\n\nmodel_evaluation(y_pred)","2163b1ee":"# from sklearn.linear_model import Lasso\n\n# lasso = Lasso()\n# parameters = {'alpha': [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40],\n#               'tol':[1e-8,1e-7,1e-06,1e-05,5e-05]\n#              }\n# grid_lasso = GridSearchCV(lasso, parameters, cv=12, verbose=1, scoring ='neg_mean_squared_error')\n# grid_lasso.fit(X_train, y_train)\n\n# sc_lasso = get_best_score(grid_lasso)","20bb8817":"## best fit: \nlasso_bf = Lasso(alpha=0.0001, tol=1e-08)\n## fit the model. \nlasso_bf.fit(X_train, y_train)\n## Predicting the target value based on \"X_test\"\ny_pred = lasso_bf.predict(X_test)\n\nmodel_evaluation(y_pred)","89256223":"# from sklearn.linear_model import ElasticNet\n# elastic_reg = ElasticNet(random_state=3)   \n\n# # perform the search\n# parameters = {\n#     'alpha': [1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0],\n#     'l1_ratio': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n# }\n\n# grid_elas = GridSearchCV(elastic_reg, parameters, cv=12, verbose=1, scoring ='neg_mean_squared_error')\n\n# # Fit randomized_mse to the data\n# grid_elas.fit(X_train,y_train)\n\n# # # Print the best parameters and lowest RMSE\n# sc_elas = get_best_score(grid_elas)","e557b1b7":"## ElasticNet best fit\nelastic_reg = ElasticNet(alpha=0.001, l1_ratio=0.3, random_state=3) \n## fit the model. \nelastic_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = elastic_reg.predict(X_test)\n\nmodel_evaluation(y_pred)","feff123e":"# from sklearn.linear_model import SGDRegressor\n\n# sgd = SGDRegressor()\n# parameters = {'loss': ['squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'],\n#               'max_iter' :[10000],\n#               'penalty':['l2','l1','elasticnet'],\n#               'alpha':[1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3,1e4],\n#               'l1_ratio':[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n#               'epsilon':[1e-05,1e-04,1e-03,1e-02,1e-01,0],\n#               'fit_intercept' : [False],\n#               'learning_rate' : ['optimal','invscaling']\n#              }\n# grid_sgd = RandomizedSearchCV(estimator=sgd,\n#                                     param_distributions=parameters,\n#                                     scoring=\"neg_mean_squared_error\",\n#                                     n_iter=300,cv=5, verbose=1)\n# grid_sgd.fit(X_train, y_train)\n\n# sc_sgd = get_best_score(grid_sgd)","a2e50169":"## SGDRegressor best fit\nsgd_reg = SGDRegressor(epsilon=0.01, fit_intercept=False, l1_ratio=1,\n             loss='epsilon_insensitive', max_iter=10000, penalty='l1') \n## fit the model. \nsgd_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = sgd_reg.predict(X_test)\n\nmodel_evaluation(y_pred)","ed194c33":"# from sklearn.tree import DecisionTreeRegressor\n\n# parameters = { 'max_depth' : range(3,10,1),\n#               'max_features' : range(5,15,1)\n#              }\n            \n# dtree_reg = GridSearchCV(DecisionTreeRegressor(), parameters, cv=12, refit=True, verbose=1,\n#                          scoring = 'neg_mean_squared_error')\n# dtree_reg.fit(X_train, y_train)\n\n# sc_dtree_reg = get_best_score(dtree_reg)","99267afb":"## DecisionTreeRegressor\ndtree_reg = DecisionTreeRegressor(max_depth=9, max_features=12)\n## fit the model. \ndtree_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = dtree_reg.predict(X_test)\n\nmodel_evaluation(y_pred)","7d2e632a":"# from sklearn.ensemble import RandomForestRegressor\n\n# param_grid = {'min_samples_split' : range(2,11,1),\n#               'n_estimators' : range(5,100,5),\n#               'random_state': [5] }\n# grid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=12, refit=True,\n#                        verbose=1, scoring = 'neg_mean_squared_error')\n# grid_rf.fit(X_train, y_train)\n\n# sc_rf = get_best_score(grid_rf)","9a2f9de9":"## DecisionTreeRegressor\nrf_reg = RandomForestRegressor(min_samples_split=2, n_estimators=95, random_state=5)\n## fit the model. \nrf_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = rf_reg.predict(X_test)\n\nmodel_evaluation(y_pred)","4cb0f664":"# # Import xgboostxgboost as xgb\n# import xgboost as xgb\n\n# # Create the DMatrix: housing_dmatrix\n# train_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n\n# # Instantiate the regressor: gbm\n# gbm = xgb.XGBRegressor(objective='reg:linear',n_estimators=1500)\n\n# gbm_param_grid = {\n#     'colsample_bytree': [0.1,0.2,0.3,0.4,0.6,0.8],\n#     'max_depth': range(3,10,1),   \n#     'eta' : [0.01,0.02,0.03,0.04,0.05,0.1,0.15,0.2,0.3,0.4],\n#     'lambda': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n#     'alpha': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n#     'subsample': [0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n# }\n\n# randomized_mse = RandomizedSearchCV(estimator=gbm,\n#                                     param_distributions=gbm_param_grid,\n#                                     scoring=\"neg_mean_squared_error\",\n#                                     n_iter=200,cv=4, verbose=1)\n\n# # Fit randomized_mse to the data\n# grid_xgb =  randomized_mse.fit(X_train,y_train)\n\n# # Print the best parameters and lowest RMSE\n# sc_xgb = get_best_score(grid_xgb)","4d2c9b49":"# Instantiate the XGBRegressor: xg_reg\nimport xgboost as xgb\n\n# Create the DMatrix: housing_dmatrix\ntrain_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n\nxgb  = xgb.XGBRegressor(objective='reg:linear',n_estimators = 1500,subsample = 0.7,\n                        max_depth = 4, eta = 0.03,colsample_bytree = 0.2,alpha = 0)\n                        \n# Fit the regressor to the training set\nxgb.fit(X_train,y_train)\n\n## Predicting the target value based on \"Test_x\"\ny_pred = xgb.predict(X_test)\n\nmodel_evaluation(y_pred)","6956edff":"list_scores = [sc_ridge, sc_lasso, sc_elas,sc_sgd, sc_dtree_reg, sc_rf, 0.12249424662755407]\nlist_regressors = ['Ridge','Lasso','ElaNet','SGD','DTr','RF','xgboost']\n\ndic_score = {'model': list_regressors,\n            'score':list_scores}\n\ndic_score = pd.DataFrame(dic_score)\ndic_score","b1601cbd":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(15,5))\nax = plt.subplot(1,1,1)\n\nax = sns.pointplot(x = \"model\", \n              y = \"score\", \n              data = dic_score) \n\nax.set_ylabel('Score (RMSE)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n# ax.set_xticklabels(ax.get_xticklabels(), size=14) \n# ax.text(i, dic_score[0] + 0.002, '{:.6f}'.format(dic_score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n     ax.text(ind,dic_score['score'][ind]+0.002,'{:.5f}'.format(dic_score['score'][ind]),\n             horizontalalignment='left', size='medium', color='black', weight='semibold',fontsize=12)\n\n\n        \nplt.title('Scores of Models', size=20)\n\nplt.show()","eabd4dd7":"# the best model is ElasticNet:\n# use np.exp to re-scale the data\ny_pred = np.expm1(elastic_reg.predict(X_df_test))","ea7d6544":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nmy_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice':y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","c08ee641":"my_submission","2d4891ce":"### Skewed features:","a6a71728":"## <a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2: EDA<\/center><\/h3>","4f75f530":"## 2.2: Numerical values\n### 2.2.1: Correlations between variables: heatmap","f2051b08":"## 2.1.2: Check the asymmetry of the probability distribution","fabfb9b6":"### 3.5.2: run DecisionTreeRegressor with best score","22a40895":"## 1.6: Features engineering ","1a451d79":"- Cross Validation: Using 12-fold cross-validation\n- Models: On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)","4c88e93e":"Now, the skew seems corrected and the data appears more normally distributed.","38d511c0":"Here, we see that the charts on the right have Homoscedasticity(almost an equal amount of variance across the zero lines). It is because we have trainsformed the target variable using numpy.log1p. Overwise, the residual plot may if seems like there is a linear relationship between the response variable (increases in variance associated with the increase of the target variables = Heteroscedasticity).","c4eb39d6":"### 2.1.4: Box Cox Transformation of (highly) skewed features\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n\n","d15040c3":"### 3.3.2: Lasso regression\n#### 3.3.2.1: Gridsearch()","9de6f6c6":"# House Prices: Advanced Regression Techniques\n### Predict sales prices and practice feature engineering, RFs, and gradient boosting\n\n### Goal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n","508531f0":"### 3.2.2: Best fit ","fdc3bfbe":"One columns corresponding to the year the garage was built, here NaN because no garage. columns will be transformed as categorical variables ","22647890":"## 3.6: RandomForestRegressor\n### 3.6.1: Gridsearch","2cb62a10":"### 1.7.1: GarageYrBlt: int to categorical","918081bf":"### 3.6.2: run RandomForestRegressor with best score","78add3d5":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1: LOAD AND CLEAN THE DATA<\/center><\/h3>","b6b51f13":"### 2.2.2: plot correlations and residuals","bafe5df3":"## 1.1: Load and check the data","aa33b90a":"## 1.5: Verify dtypes:","42f3e5e9":"## 4: SUMMARY","7417599c":"### 2.1.3: Log transform skewed numeric features:\n**We want our skewness value to be around 0 and kurtosis less than 3.** \n\nHere are two examples of skewed features: Ground living area and 1st floor SF. We will apply np.log1p to the skewed variables.","67d88442":"3.3.2.2: run Lasso model with best score","3db1b839":"We can see:\n* <b>Deviate from the normal distribution.<\/b> \n* <b>Have appreciable positive skewness.<\/b> (means that the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.)\n* <b>High Kurtosis.<\/b> (means that the data tends to have heavy tails, or outliers)","6b922e07":"## 3.5: DecisionTreeRegressor\n### 3.5.1: Gridsearch","c420fb93":"## 3.7: xgboost regression\n### 3.7.1: Gridsearch()","41d9acd4":"### 3.3.3: ElasticNet regression\n#### 3.3.3.1: Gridsearch()","05ebd554":"## 1.3: merge test and train data","4997aa61":"## 3.1 Preprocessing\n### 3.1.1: Split the data","67294c3f":"### **Observation**\n\nAs we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let the models(e.x. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists.\n\nThere is 0.83 or 83% correlation between GarageYrBlt and YearBuilt.\n83% correlation between TotRmsAbvGrd and GrLivArea.\n89% correlation between GarageCars and GarageArea.\nSimilarly many other features such asBsmtUnfSF, FullBath have good correlation with other independent feature.\nIf I were using only multiple linear regression, I would be deleting these features from the dataset to fit better multiple linear regression algorithms. However, we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible outcome. Therefore, we will keep all the features for now.","8c52bdb2":"## 1.4: replace nan values","8a304c92":"## <a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3: MODEL<\/center><\/h3>","934e05f7":"## 1.7: Collapsing data into categories","8db7d5c3":"###  2.3: Categorical variables","cc07ca57":"### 2.2.3: Outliers detection:","cd05b93a":"## 2.1: Analysing target variable: 'SalePrice'\n### 2.1.1: basic stat","b99b6a60":"**SECTION 1: LOAD AND CLEAN THE DATA**\n\n1.1: Load and check the data\n\n1.2: Verify NaN value:\n\n1.3: Merge test and train data\n\n1.4: Replace nan values\n\n1.5: Verify dtypes\n\n1.6: Features engineering\n\n1.7: Collapsing data into categories\n    \n    - 1.7.1: GarageYrBlt: int to categorical\n    - 1.7.2: Verify Value consistency\n    - 1.7.3: Collapsing too many categories to few\n    - 1.7.4: Making sure all the relevant data are categorical variables.\n\n**SECTION 2: EDA**\n\n2.1: Analysing target variable: 'SalePrice'\n\n    - 2.1.1: basic stat\n    - 2.1.2: Check the asymmetry of the probability distribution\n    - 2.1.3: Log transform skewed numeric features\n    - 2.1.4: Box Cox Transformation of (highly) skewed features\n\n2.2: Numerical values\n\n    - 2.2.1: Correlations between variables: heatmap\n    - 2.2.2: plot correlations and residuals\n    - 2.2.3: Outliers detection\n\n2.3: Categorical variables\n\n**SECTION 3: MODEL**\n\n3.1 Preprocessing\n\n    - 3.1.1: Split the data\n    - 3.1.2: Scale the data\n    \n3.2 Linear regression\n\n    - 3.2.1: GridSearchCV\n    - 3.2.2: Best fit\n    \n3.3 Regularization Models\n\n    - 3.3.1: Ridge\n        - 3.3.1.1: Gridsearch()\n        - 3.2.1.2: run ridge model with best score\n    - 3.3.2: Lasso regression\n        - 3.3.2.1: Gridsearch()\n        - 3.3.2.2: run Lasso model with best score\n    - 3.3.3: ElasticNet regression\n        - 3.3.3.1: Gridsearch()\n        - 3.3.3.2: run ElasticNet regression with best score\n        \n3.4: Stochastic Gradient Descent\n\n    - 3.4.1: Gridsearch()\n    - 3.4.2: run SGDRegressor regression with best score\n3.5: DecisionTreeRegressor\n\n    - 3.5.1: Gridsearch\n    - 3.5.2: run DecisionTreeRegressor with best score\n    \n3.6: RandomForestRegressor\n\n    - 3.6.1: Gridsearch\n    - 3.6.2: run RandomForestRegressor with best score\n    \n3.7: xgboost regression\n\n    - 3.7.1: Gridsearch\n    - 3.7.2: run xgboost regression with best score\n\n**SECTION 4: SUBMISSION**","4436c3c4":"### 1.7.4: Making sure all the relevant data are categorical variables.","f338b945":"### split the df with all data into the training and testing set\n","af622bcd":"### 1.7.3: Collapsing too many categories to few","758182ae":"### 3.4.2: run SGDRegressor regression with best score","db7ea0e5":"### 3.1.2: Scale the data\nWe use RobustScaler to scale the data because it's powerful against outliers, we already detected some but there must be some other outliers out there.","8a583d38":"#### 3.2.1.2: run ridge model with best score","2c83136f":"## 1.2: Verify NaN value:","0cc137ea":"## 3.3 Regularization Models\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.\n\nThere are three types of regularizations.\n\n- Ridge\n- Lasso\n- Elastic Net\n\n### 3.3.1: Ridge\n#### 3.3.1.1: Gridsearch()","92fb0b1e":"## 3.4: Stochastic Gradient Descent\n### 3.4.1: Gridsearch()","6817675f":"First thing to do is get rid of the features with more than 80% missing values (figure below). For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 80%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 80% missing values.\n\nPS: In this version, I lower the threshold to 20% to drop more columns","b9298696":"### 1.7.2: Verify Value consistency","101f9072":"## <a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>4: SUBMISSION<\/center><\/h3>","9e59767f":"#### 3.3.3.2: run ElasticNet regression with best score","11ef8f6c":"## 3.2 Linear regression\n### 3.2.1:  GridSearchCV()","efb237f8":"### 3.7.2: run xgboost with best score"}}