{"cell_type":{"f184e6c0":"code","0347b53b":"code","3f80cfb8":"code","99598899":"code","d3062638":"code","66e4791b":"code","a066755b":"code","9e13a2e2":"code","83308ed8":"code","17437a72":"code","a86e15e0":"code","045e6698":"code","a91e81d4":"code","039330e5":"code","c2b04996":"code","ea6278de":"code","f3c4890e":"code","09033f1b":"code","e82c1006":"code","3607a6c6":"code","d80efc9d":"code","e9a56de5":"code","df39f05d":"code","7094c8ee":"code","ea410b37":"code","6d14e71e":"code","d86c4240":"code","af9994df":"code","e0e075fa":"code","008a3cb2":"code","3910c580":"code","72dd3b4b":"code","5d4dbc6d":"code","48a89b02":"code","899c9c94":"code","3d7f33bb":"code","7e8c2a6a":"code","0ff00226":"code","07ad592c":"code","9693d97d":"code","91db2d6a":"code","a845dc8a":"code","dcb4ab92":"code","7f2a9c70":"code","36a3e2bd":"code","f75041cc":"code","54b4b4c6":"code","0ca3085e":"code","9445d734":"code","822d02d5":"code","cf32638c":"code","f50e9a6a":"code","f3bc6954":"code","69066861":"code","5552c2a9":"code","f12a7f83":"code","db8f7f18":"code","2492df23":"markdown","5b90e2e8":"markdown","92763db5":"markdown","da52e87c":"markdown","a0927cb6":"markdown","4955ab3d":"markdown","ae1e9bc6":"markdown","ac11e130":"markdown","b6be7e20":"markdown","4f9e4c53":"markdown","bd5d6274":"markdown","f35e3773":"markdown","c4988a01":"markdown","f6ed344b":"markdown","b6ef2f17":"markdown","4b2e77dc":"markdown","ca1db963":"markdown","f30aee7b":"markdown"},"source":{"f184e6c0":"import numpy as np\nimport pandas as pd\nimport warnings\n\nwarnings.filterwarnings(action='ignore') # Ignore warning message\n\n# date path\ndata_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\n\nsales_train = pd.read_csv(data_path + 'sales_train.csv')\nshops = pd.read_csv(data_path + 'shops.csv')\nitems = pd.read_csv(data_path + 'items.csv')\nitem_categories = pd.read_csv(data_path + 'item_categories.csv')\ntest = pd.read_csv(data_path + 'test.csv')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv')","0347b53b":"def downcast(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        dtype_name = df[col].dtype.name\n        if dtype_name == 'object':\n            pass\n        elif dtype_name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype_name.startswith('int') or (df[col].round() == df[col]).all():\n            df[col] = pd.to_numeric(df[col], downcast='integer')\n        else:\n            df[col] = pd.to_numeric(df[col], downcast='float')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('{:.1f}% compressed'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\nall_df = [sales_train, shops, items, item_categories, test]\nfor df in all_df:\n    df = downcast(df)","3f80cfb8":"# Extract data with a item_price greater than 0\nsales_train = sales_train[sales_train['item_price'] > 0]\n# Extract data with a item_priceof less than 50,000\nsales_train = sales_train[sales_train['item_price'] < 50000]\n# Extract data with item_cnt_day greater than 0\nsales_train = sales_train[sales_train['item_cnt_day'] > 0]\n# Extract data with item_cnt_day less than 1,000\nsales_train = sales_train[sales_train['item_cnt_day'] < 1000]","99598899":"print(shops['shop_name'][0], '||', shops['shop_name'][57])\nprint(shops['shop_name'][1], '||', shops['shop_name'][58])\nprint(shops['shop_name'][10], '||', shops['shop_name'][11])\nprint(shops['shop_name'][39], '||', shops['shop_name'][40])","d3062638":"#  Modify shop_id in sales_train data\nsales_train.loc[sales_train['shop_id'] == 0, 'shop_id'] = 57\nsales_train.loc[sales_train['shop_id'] == 1, 'shop_id'] = 58\nsales_train.loc[sales_train['shop_id'] == 10, 'shop_id'] = 11\nsales_train.loc[sales_train['shop_id'] == 39, 'shop_id'] = 40\n\n#  Modify shop_id in test data\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 39, 'shop_id'] = 40","66e4791b":"# Leaking to imporve performance\nunique_test_shop_id = test['shop_id'].unique()\nsales_train = sales_train[sales_train['shop_id'].isin(unique_test_shop_id)]","a066755b":"shops['city'] = shops['shop_name'].apply(lambda x: x.split()[0])","9e13a2e2":"shops['city'].unique()","83308ed8":"shops.loc[shops['city'] =='!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'","17437a72":"from sklearn.preprocessing import LabelEncoder\n\n# Create Label Encoder\nlabel_encoder = LabelEncoder()\n# City Feature Label Encoding \nshops['city'] = label_encoder.fit_transform(shops['city'])","a86e15e0":"# Remove shop_name feature\nshops = shops.drop('shop_name', axis=1)\n\nshops.head()","045e6698":"# Remove item_name feature\nitems = items.drop(['item_name'], axis=1)","a91e81d4":"# Create the date the product was first sold as a feature\nitems['first_sale_date'] = sales_train.groupby('item_id').agg({'date_block_num': 'min'})['date_block_num']\n\nitems.head()","039330e5":"items[items['first_sale_date'].isna()]","c2b04996":"# Replace NaN of first_sale_date with 34\nitems['first_sale_date'] = items['first_sale_date'].fillna(34)","ea6278de":"# Extract the first word of the item_categories_name into category\nitem_categories['category'] = item_categories['item_category_name'].apply(lambda x: x.split()[0])  ","f3c4890e":"item_categories['category'].value_counts()","09033f1b":"def make_etc(x):\n    if len(item_categories[item_categories['category']==x]) >= 5:\n        return x\n    else:\n        return 'etc'\n\n# Replace with 'etc' if category count is less than 5\nitem_categories['category'] = item_categories['category'].apply(make_etc)","e82c1006":"item_categories.head()","3607a6c6":"# Create Label Encoder\nlabel_encoder = LabelEncoder()\n# Category Feature Label Encoding \nitem_categories['category'] = label_encoder.fit_transform(item_categories['category'])\n\n# Remove item_category_name feature\nitem_categories = item_categories.drop('item_category_name', axis=1)","d80efc9d":"from itertools import product\n\ntrain = []\n# Create date_block_num, sop_id, item_id combination\nfor i in sales_train['date_block_num'].unique():\n    all_shop = sales_train.loc[sales_train['date_block_num']==i, 'shop_id'].unique()\n    all_item = sales_train.loc[sales_train['date_block_num']==i, 'item_id'].unique()\n    train.append(np.array(list(product([i], all_shop, all_item))))\n\nidx_features = ['date_block_num', 'shop_id', 'item_id'] # base features\ntrain = pd.DataFrame(np.vstack(train), columns=idx_features)","e9a56de5":"group = sales_train.groupby(idx_features).agg({'item_cnt_day': 'sum',\n                                               'item_price': 'mean'})\ngroup = group.reset_index()\ngroup = group.rename(columns={'item_cnt_day': 'item_cnt_month', 'item_price': 'item_price_mean'})\n\ntrain = train.merge(group, on=idx_features, how='left')\n\ntrain.head()","df39f05d":"import gc\n\n# group variable garbage collection\ndel group\ngc.collect();","7094c8ee":"# Add a feature for the number of items sold\ngroup = sales_train.groupby(idx_features).agg({'item_cnt_day': 'count'})\ngroup = group.reset_index()\ngroup = group.rename(columns={'item_cnt_day': 'item_count'})\n\ntrain = train.merge(group, on=idx_features, how='left')\n\n# Garbage collection\ndel group, sales_train\ngc.collect()\n\ntrain.head()","ea410b37":"# Set test data date_block_num to 34\ntest['date_block_num'] = 34\n\n# Concatenate train and test\nall_data = pd.concat([train, test.drop('ID', axis=1)],\n                     ignore_index=True,\n                     keys=idx_features)\n# Replace NaN with 0\nall_data = all_data.fillna(0)\n\nall_data.head()","6d14e71e":"# Merge other data\nall_data = all_data.merge(shops, on='shop_id', how='left')\nall_data = all_data.merge(items, on='item_id', how='left')\nall_data = all_data.merge(item_categories, on='item_category_id', how='left')\n\n# Data downcasting\nall_data = downcast(all_data)\n\n# Garbage collection\ndel shops, items, item_categories\ngc.collect();","d86c4240":"def resumetable(df):\n    print(f'Data Shape: {df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['Dtypes'])\n    summary['Null'] = df.isnull().sum().values\n    summary['Uniques'] = df.nunique().values\n    summary['First_values'] = df.loc[0].values\n    summary['Second_values'] = df.loc[1].values\n    summary['Third_values'] = df.loc[2].values\n    \n    return summary","af9994df":"resumetable(all_data)","e0e075fa":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmpl.rc('font', size=13)\nfigure, ax = plt.subplots() \nfigure.set_size_inches(11, 5)\n\n# total montly item sales\ngroup_month_sum = all_data.groupby('date_block_num').agg({'item_cnt_month': 'sum'})\ngroup_month_sum = group_month_sum.reset_index()\n\nsns.barplot(x='date_block_num', y='item_cnt_month', data=group_month_sum)\nax.set(title='Distribution of monthly item counts by date block number',\n       xlabel='Date block number', \n       ylabel='Monthly item counts');","008a3cb2":"figure, ax= plt.subplots() \nfigure.set_size_inches(11, 5)\n\n# Total item sales by item_category_id\ngroup_cat_sum = all_data.groupby('item_category_id').agg({'item_cnt_month': 'sum'})\ngroup_cat_sum = group_cat_sum.reset_index()\n\n# Extract only item categories with total sales > 10,000\ngroup_cat_sum = group_cat_sum[group_cat_sum['item_cnt_month'] > 10000]\n\nsns.barplot(x='item_category_id', y='item_cnt_month', data=group_cat_sum)\nax.set(title='Distribution of total item counts by item category id',\n       xlabel='Item category ID', \n       ylabel='Total item counts')\nax.tick_params(axis='x', labelrotation=90) # Rotate X label","3910c580":"figure, ax= plt.subplots() \nfigure.set_size_inches(11, 5)\n\n# Total item sales by shop_id\ngroup_shop_sum = all_data.groupby('shop_id').agg({'item_cnt_month': 'sum'})\ngroup_shop_sum = group_shop_sum.reset_index()\n\ngroup_shop_sum = group_shop_sum[group_shop_sum['item_cnt_month'] > 10000]\n\nsns.barplot(x='shop_id', y='item_cnt_month', data=group_shop_sum)\nax.set(title='Distribution of total item counts by shop id',\n       xlabel='Shop ID', \n       ylabel='Total item counts')\nax.tick_params(axis='x', labelrotation=90)","72dd3b4b":"def add_mean_features(df, mean_features, idx_features):\n    # Check base features\n    assert (idx_features[0] == 'date_block_num') and \\\n           len(idx_features) in [2, 3]\n    \n    # Set derived feature name \n    if len(idx_features) == 2:\n        feature_name = idx_features[1] + '_mean_sales'\n    else:\n        feature_name = idx_features[1] + '_' + idx_features[2] + '_mean_sales'\n    \n    # Get average monthly sales by grouping based on base features\n    group = df.groupby(idx_features).agg({'item_cnt_month': 'mean'})\n    group = group.reset_index()\n    group = group.rename(columns={'item_cnt_month': feature_name})\n    \n    # Merge df with group based on idx_features\n    df = df.merge(group, on=idx_features, how='left')\n    # Date downcasting\n    df = downcast(df, False)\n    # Append newly created mean_feature_name features to the mean_features list\n    mean_features.append(feature_name)\n    \n    # Garbage collection\n    del group\n    gc.collect()\n    \n    return df, mean_features","5d4dbc6d":"# List of derived features containing 'item_id' in the grouping base features\nitem_mean_features = []\n\n\n# Create monthly average sales derived features grouped by ['date_block_num', 'item_id']\nall_data, item_mean_features = add_mean_features(df=all_data,\n                                                 mean_features=item_mean_features,\n                                                 idx_features=['date_block_num', 'item_id'])\n\n# Create monthly average sales derived features grouped by ['date_block_num', 'item_id', 'city']\nall_data, item_mean_features = add_mean_features(df=all_data,\n                                                 mean_features=item_mean_features,\n                                                 idx_features=['date_block_num', 'item_id', 'city'])","48a89b02":"item_mean_features","899c9c94":"# List of derived features containing 'shop_id' in the grouping base features\nshop_mean_features = []\n\n# Create monthly average sales derived features grouped by ['date_block_num', 'shop_id', 'item_category_id']\nall_data, shop_mean_features = add_mean_features(df=all_data, \n                                                 mean_features=shop_mean_features,\n                                                 idx_features=['date_block_num', 'shop_id', 'item_category_id'])","3d7f33bb":"shop_mean_features","7e8c2a6a":"def add_lag_features(df, lag_features_to_clip, idx_features, \n                     lag_feature, nlags=3, clip=False):\n    # Copy only the part of the DataFrame needed to create the lag features\n    df_temp = df[idx_features + [lag_feature]].copy() \n\n    # Create lag features\n    for i in range(1, nlags+1):\n        # Lag featrue name\n        lag_feature_name = lag_feature +'_lag' + str(i)\n        # Set df_temp column name\n        df_temp.columns = idx_features + [lag_feature_name]\n        # Add 1 to date_block_num feature in df_temp\n        df_temp['date_block_num'] += i\n        # Merge df with df_temp based on idx_feature\n        df = df.merge(df_temp.drop_duplicates(), \n                      on=idx_features, \n                      how='left')\n        # Replace NaN with 0\n        df[lag_feature_name] = df[lag_feature_name].fillna(0)\n        # Add lag features to lag_features_to_clip to clip between 0 and 20\n        if clip: \n            lag_features_to_clip.append(lag_feature_name)\n    \n    # Date downcasting\n    df = downcast(df, False)\n    # Garbage collection\n    del df_temp\n    gc.collect()\n    \n    return df, lag_features_to_clip","0ff00226":"lag_features_to_clip = [] # list of lag features to be clipped to between 0 to 20 \nidx_features = ['date_block_num', 'shop_id', 'item_id'] # base features\n\n# Create 3 month lag features of item_cnt_month based on idx_features\nall_data, lag_features_to_clip = add_lag_features(df=all_data, \n                                                  lag_features_to_clip=lag_features_to_clip,\n                                                  idx_features=idx_features,\n                                                  lag_feature='item_cnt_month', \n                                                  nlags=3,\n                                                  clip=True)","07ad592c":"all_data.head().T","9693d97d":"lag_features_to_clip","91db2d6a":"# Create 3 month lag features of item_count feature based on idx_features\nall_data, lag_features_to_clip = add_lag_features(df=all_data, \n                                                  lag_features_to_clip=lag_features_to_clip,\n                                                  idx_features=idx_features,\n                                                  lag_feature='item_count', \n                                                  nlags=3)\n\n# Create 3 month lag features of item_price_mean feature based on idx_features\nall_data, lag_features_to_clip = add_lag_features(df=all_data, \n                                                  lag_features_to_clip=lag_features_to_clip,\n                                                  idx_features=idx_features,\n                                                  lag_feature='item_price_mean', \n                                                  nlags=3)","a845dc8a":"X_test_temp = all_data[all_data['date_block_num'] == 34]\nX_test_temp[item_mean_features].sum()","dcb4ab92":"# Create lag features by item_mean_features element based on dx_features\nfor item_mean_feature in item_mean_features:\n    all_data, lag_features_to_clip = add_lag_features(df=all_data, \n                                                      lag_features_to_clip=lag_features_to_clip, \n                                                      idx_features=idx_features, \n                                                      lag_feature=item_mean_feature, \n                                                      nlags=3)\n# Remove features in item_mean_features\nall_data = all_data.drop(item_mean_features, axis=1)","7f2a9c70":"shop_mean_features","36a3e2bd":"# Create lag features by shop_mean_features element based on ['date_block_num', 'shop_id', 'item_category_id']\nfor shop_mean_feature in shop_mean_features:\n    all_data, lag_features_to_clip = add_lag_features(df=all_data,\n                                                      lag_features_to_clip=lag_features_to_clip, \n                                                      idx_features=['date_block_num', 'shop_id', 'item_category_id'], \n                                                      lag_feature=shop_mean_feature, \n                                                      nlags=3)\n# Remove features in shop_mean_features\nall_data = all_data.drop(shop_mean_features, axis=1)","f75041cc":"# Remove data less than date ID 3\nall_data = all_data.drop(all_data[all_data['date_block_num'] < 3].index)","54b4b4c6":"all_data['item_cnt_month_lag_mean'] = all_data[['item_cnt_month_lag1',\n                                         'item_cnt_month_lag2', \n                                         'item_cnt_month_lag3']].mean(axis=1)","0ca3085e":"# Clip 0~20\nall_data[lag_features_to_clip + ['item_cnt_month', 'item_cnt_month_lag_mean']] = all_data[lag_features_to_clip +['item_cnt_month', 'item_cnt_month_lag_mean']].clip(0, 20)","9445d734":"all_data['lag_grad1'] = all_data['item_cnt_month_lag1']\/all_data['item_cnt_month_lag2']\nall_data['lag_grad1'] = all_data['lag_grad1'].replace([np.inf, -np.inf], \n                                                        np.nan).fillna(0)\n\nall_data['lag_grad2'] = all_data['item_cnt_month_lag2']\/all_data['item_cnt_month_lag3']\nall_data['lag_grad2'] = all_data['lag_grad2'].replace([np.inf, -np.inf], \n                                                        np.nan).fillna(0)","822d02d5":"all_data['brand_new'] = all_data['first_sale_date'] == all_data['date_block_num']","cf32638c":"all_data['duration_after_first_sale'] = all_data['date_block_num'] - all_data['first_sale_date']\nall_data = all_data.drop('first_sale_date', axis=1)","f50e9a6a":"all_data['month'] = all_data['date_block_num']%12","f3bc6954":"# Remove item_price_mean, item_count features\nall_data = all_data.drop(['item_price_mean', 'item_count'], axis=1)\nall_data = downcast(all_data, False) # Data downcasting\nall_data.info()","69066861":"# Train data (Features)\nX_train = all_data[all_data['date_block_num'] < 33]\nX_train = X_train.drop(['item_cnt_month'], axis=1)\n# Valid data (Features)\nX_valid = all_data[all_data['date_block_num'] == 33]\nX_valid = X_valid.drop(['item_cnt_month'], axis=1)\n# Test data (Features)\nX_test = all_data[all_data['date_block_num'] == 34]\nX_test = X_test.drop(['item_cnt_month'], axis=1)\n\n# Train data (Target values)\ny_train = all_data[all_data['date_block_num'] < 33]['item_cnt_month']\n# Valid data (Target values)\ny_valid = all_data[all_data['date_block_num'] == 33]['item_cnt_month']\n\n# Garbage collection\ndel all_data\ngc.collect();","5552c2a9":"import lightgbm as lgb\n\n# lgb hyper-parameters\nparams = {'metric': 'rmse',\n          'num_leaves': 255,\n          'learning_rate': 0.005,\n          'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'bagging_freq': 5,\n          'force_col_wise' : True,\n          'random_state': 10}\n\ncat_features = ['shop_id', 'city', 'item_category_id', 'category', 'month']\n\n# lgb train and valid dataset\ndtrain = lgb.Dataset(X_train, y_train)\ndvalid = lgb.Dataset(X_valid, y_valid)\n \n# Train LightGBM model\nlgb_model = lgb.train(params=params,\n                      train_set=dtrain,\n                      num_boost_round=1500,\n                      valid_sets=(dtrain, dvalid),\n                      early_stopping_rounds=150,\n                      categorical_feature=cat_features,\n                      verbose_eval=100)      ","f12a7f83":"preds = lgb_model.predict(X_test).clip(0,20)\n\nsubmission['item_cnt_month'] = preds\nsubmission.to_csv('submission.csv', index=False)","db8f7f18":"del X_train, y_train, X_valid, y_valid, X_test, lgb_model, dtrain, dvalid\ngc.collect();","2492df23":"### itmes: Create derived features","5b90e2e8":"## Visualization","92763db5":"# Predict Future Sales Competition\n## Top 3.5% Solution with Feature Engineering\n\n- [Competition Link](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales)\n- [Reference modeling link](https:\/\/www.kaggle.com\/dkomyagin\/predict-future-sales-lightgbm-framework)","da52e87c":"### Feature summary","a0927cb6":"### Concatenate test data, Merge remaining data","4955ab3d":"# Thank you \ud83d\ude42 Upvote is free \ud83d\udc4d","ae1e9bc6":"### Shops: create derived features and encode","ac11e130":"### Create Average Monthly Sales Derived Feature by Base Feature","b6be7e20":"## Feature Engineering II - Create Lag features ","4f9e4c53":"### Create Lag Features","bd5d6274":"## This is Top 3.5% modeling code with feature engineering. I made a total of 30 featrues. If it was helpful, please upvote my code!! \ud83d\udc40","f35e3773":"### sales_train: remove outliers and preprocess","c4988a01":"### Create item_categories derived feature and encode","f6ed344b":"### Generate data combinations and derived features","b6ef2f17":"## Train model and Submit","4b2e77dc":"### Other Features Engineering","ca1db963":"#### Data Downcasting","f30aee7b":"## Feature Engineering I - handling sales_train, shops, items, item_categories"}}