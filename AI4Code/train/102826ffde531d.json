{"cell_type":{"0969c643":"code","dcc6175c":"code","7bc26c05":"code","a34c742f":"code","94daa793":"code","db053edd":"code","975a51dc":"code","d032e7ba":"code","520440af":"code","4c3cfd1b":"code","93528ccc":"code","931af0f7":"code","86207754":"code","3b0458fe":"code","88ffc309":"code","b0fe9041":"code","40cc09e2":"code","f730c6cc":"code","0a01977d":"code","ad61ba6f":"code","e219c3ad":"code","e7d32301":"code","71a9ccf5":"markdown","187aa896":"markdown","5eec68f9":"markdown","675296f2":"markdown","d5a01dce":"markdown","11067fdb":"markdown","452da5a8":"markdown","3b69d326":"markdown","c86218d7":"markdown","8bbf6bb6":"markdown","7a2f33b9":"markdown","4541c6b2":"markdown"},"source":{"0969c643":"import pandas as pd\nimport numpy as np\nimport json\nimport io\nimport re","dcc6175c":"train_data = list()\nwith io.open('train_data.json','r',encoding='utf8') as f:\n    for line in f.readlines():\n        d = json.loads(line)\n        train_data.append(d)\n\ntest_data = list()\nwith io.open('test_data.json','r',encoding='utf8') as f:\n    for line in f.readlines():\n        d = json.loads(line)\n        test_data.append(d)","7bc26c05":"good_ends = [u'!', u'\u2026', u'.', u'?', u'']\nabbrs = [u'\u0442.\u0435.', u' \u0438\u043c.' u'\u0442.\u043a.', u' \u0433.', u' \u043c.', u' \u0443\u043b.'] \nlabels = np.zeros((26476,1))","a34c742f":"\u0421\u043f\u043e\u0441\u043e\u0431\u044b \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u044d\u043c\u043f\u0435\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 ","94daa793":"def check(pos, nxt_pos, txt):\n    if (len(txt) > pos + 2 and \n            txt[pos+1] is ' ' and \n            corr_pref(txt[pos + 1: nxt_pos]) and\n            not_abbr(txt, pos) and\n            not txt[pos+1].isnumeric() and\n            not bracket_balance(txt[0: pos])):\n        return True\n    return False\ndef bracket_balance(text):\n    return text.count(u'(')!= text.count(u')') \n\ndef trashhold(txt, pos):\n    nxt_pos=pos+1\n    if len(txt)-1==pos:\n        return True\n    while txt[nxt_pos].islower(): \n            if  nxt_pos<len(txt):\n                    nxt_pos += 1\n    if check(pos, nxt_pos, txt):\n        return True\n\ndef corr_pref(text):\n    for l in text:\n        if l.isalpha() or l in good_ends:\n            return False\n    return True\n\ndef not_abbr(txt, pos):\n    look_thrw=txt[max(0, pos - 3): min(len(txt),pos + 3)]\n    for a in abbrs:\n        if txt.find(a) != -1: \n            return False\n    return True\n","db053edd":"count_m=(lambda x: x['Paragraph'].count(u'.')+x['Paragraph'].count(u'\u2026')+x['Paragraph'].count(u'!')+x['Paragraph'].count(u'?')+x['Paragraph'].count(u'\u00bb')+x['Paragraph'].count(u'\"'))\nglobal_card=(lambda x: [sublist for lst in list(map(get_card, x['Sentences'])) for sublist in lst])\nget_card=(lambda x: [0]*(x.count(u'.')+x.count(u'\u2026')+x.count(u'!')+x.count(u'?')+x.count(u'\u00bb')+x.count(u'\"')-1)+[1])\nglobal_ends=(lambda x: [sublist for lst in list(map(get_ends, x['Sentences'])) for sublist in lst])\nget_ends=(lambda x: re.findall(r'\\s\\w*[.\u2026!?\u00bb\"]', x))\nget_marks_size=(lambda x: len(x['Marks']))","975a51dc":"x_ends=np.array([sublist for lst in list(map(global_ends,train_data)) for sublist in lst])","d032e7ba":"def notate_paragraph(dic, Index, trained_list=[]):\n    bad_ends=[u'!', u'\u2026', u'.', u'?', u'\u00bb', u'\"', '\"']\n    opening_bracket_list=['(', '{', '[', '\u201c', '\u201d']\n    closing_bracket_list=[')', '}', ']', '\u201d', '\u201c']\n    bad_ends+=list(trained_list)\n    paragraph=dic['Paragraph']\n    pos=0\n    tab=0\n    Closed=0\n    Marks=[]\n    while pos<len(paragraph):\n        if paragraph[pos]==u' ':\n            tab=pos\n        if paragraph[pos] in bad_ends:\n            for i, each in enumerate(opening_bracket_list):\n                if paragraph[:pos].count(each)==paragraph[:pos].count(closing_bracket_list[i]):\n                    Closed=0\n                else:\n                    Closed=1\n                    break\n            Index+=1\n            Marks+=[{'Index':Index, 'Pos':pos,'Mark':paragraph[pos], 'Word':paragraph[tab:pos+1], 'Closed':Closed}]\n            tab=pos\n        pos+=1\n    return Marks, Index\n\ndef create_notation(train_data, trained_list=[]):\n    notated_data=[]\n    Index=0\n    for each in train_data:\n        Marks, Index=notate_paragraph(each, Index, trained_list)\n        notated_data+=[{'Paragraph':each['Paragraph'],'Marks':Marks}]\n    return notated_data\nprint(create_notation(test_data[:1]))\nprint(test_data[:1])\n        ","520440af":"import sklearn.model_selection as md\nfrom sklearn.metrics import accuracy_score as accuracy","4c3cfd1b":"def parser(data, train=True, trained_list=[]):\n    notated_train_data=create_notation(data) #, trained_list)\n    size=sum(list(map(get_marks_size, notated_train_data)))\n    y_real=None\n    if train:\n        y_real=np.array([sublist for lst in list(map(global_card,data)) for sublist in lst])\n    y_hat=np.zeros(size)\n    if not train:\n        y_hat=np.zeros(size)\n    for p in notated_train_data:\n        par = p['Paragraph']\n        for cand in p['Marks']:\n            if cand['Mark'] in good_ends:\n                if trashhold(par, cand['Pos']):\n                    y_hat[cand['Index']-1]=1\n    x_words=[]\n    Closed=[]\n    for each in notated_train_data:\n        for every in each['Marks']:\n            x_words+=[every['Word']]\n            Closed+=[every['Closed']]\n    d={'Y_real':y_real, 'Predict':y_hat, 'Words': x_words, 'Closed':Closed}\n    data=pd.DataFrame(data=d)\n    return data","93528ccc":"for each in d:\n    print(len(d[each]), each)","931af0f7":"Data_frame= parser(train_data, True)\ntrained_list=Data_frame[Data_frame.Predict!=Data_frame.Y_real].Words.apply(lambda x: x[1:-1]).value_counts().index[:35]\ndef feature_generator(Data_frame, trained_list=[]):\n    Data_frame['Len']=Data_frame.Words.apply(lambda x: len(x))\n    Data_frame['Litera']=Data_frame.Words.apply(lambda x: x[-1])\n    Data_frame['Last1']=Data_frame.Words.apply(lambda x: x[-3:-1])\n    Data_frame=pd.concat((Data_frame, pd.get_dummies(Data_frame.Litera)), axis=1)\n    Data_frame=pd.concat((Data_frame, pd.get_dummies(Data_frame.Last1)), axis=1)\n    for each in trained_list:\n        Data_frame[each]=Data_frame.Words.apply(lambda x: 1 if x[1:-1] in trained_list else 0)\n    return Data_frame\nData_frame=feature_generator(Data_frame=Data_frame, trained_list=trained_list)","86207754":"Data_frame= parser(train_data, True)\nData_frame=feature_generator(Data_frame=Data_frame, trained_list=trained_list)","3b0458fe":"Test_frame=parser(test_data, False, trained_list=trained_list)\nTest_frame=feature_generator(Test_frame, trained_list)","88ffc309":"Data_frame=Data_frame[list(set(Test_frame.columns).intersection(set(Data_frame.columns)))]\nTest_frame=Test_frame[list(set(Test_frame.columns).intersection(set(Data_frame.columns)))]","b0fe9041":"X=Data_frame.drop([\"Y_real\", 'Words', 'Litera', 'Last1'], axis=1)\ny=Data_frame['Y_real']","40cc09e2":"X_train, X_test, y_train, y_test=md.train_test_split(X, y, test_size=0.3)","f730c6cc":"from sklearn.linear_model import LogisticRegressionCV","0a01977d":"labels=model.predict(Test_frame.drop(['Last1', 'Litera', 'Y_real', 'Words'], axis=1))\n\ndf = pd.DataFrame(labels, columns=['Mark'], index=range(1, len(labels)+1))\ndf.index.name = 'Id'\ndf.to_csv(\"sampleSubmission0.csv\")","ad61ba6f":"import lightgbm as lgb","e219c3ad":"X_train = X_train.drop(0, axis=1).values\nX_test = X_test.drop(0, axis=1).values\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'auc'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\nprint('Start training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)","e7d32301":"labels=gbm.predict(Test_frame.drop(['Last1', 'Litera', 'Y_real', 'Words'], axis=1).drop(0, axis=1).values)\n\ndf = pd.DataFrame(labels, columns=['Mark'], index=range(1, len(labels)+1))\ndf.index.name = 'Id'\ndf.to_csv(\"sampleSubmission0.csv\")","71a9ccf5":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","187aa896":"\u041d\u0443 \u0438 \u0442\u0435\u043f\u0435\u0440\u044c \u043d\u0430\u0434\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u0432\u0441\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c.","5eec68f9":"\u0421\u043b\u043e\u0432\u0430, \u043f\u0440\u0435\u0434\u0448\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c\u0443 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0443.","675296f2":"\u0412\u0441\u044f\u043a\u0438\u0435 \u043c\u043e\u0438 \u043b\u044e\u0431\u0438\u043c\u044b\u0435 \u043b\u044f\u043c\u0431\u0434\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u043a\u043e\u0432\u044b\u0440\u0438\u0432\u0430\u044e\u0442 \u0444\u0438\u0447\u0438 \u0438\u0437 \u0441\u043f\u0438\u0441\u043a\u043e\u0432. \u0421\u043f\u0430\u0441\u0438\u0431\u043e \u043f\u0430\u0440\u0430\u043c \u043f\u043e Lisp \u0432 \u044d\u0442\u043e\u043c \u0441\u0435\u043c\u0435\u0441\u0442\u0440\u0435, \u0438\u0437-\u0437\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u043d\u0435 \u043b\u0435\u0433\u0447\u0435 \u043f\u0438\u0441\u0430\u0442\u044c \u0442\u0430\u043a\u0438\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f!","d5a01dce":"\u042f \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043f\u0440\u043e\u0441\u0442\u043e\u0439: \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0438 \u044d\u043c\u043f\u0435\u0440\u0438\u043a\u0430 \u043d\u0435 \u0432\u0437\u043b\u0435\u0442\u0430\u044e\u0442- \u043f\u0440\u043e\u0431\u0443\u044e \u0438\u043c\u043f\u043e\u0440\u0447\u0443 LGBM","11067fdb":"\u0420\u0430\u0437\u0431\u043e\u0440 \u043f\u0430\u0440\u0430\u0433\u0440\u0430\u0444\u0430 \u043f\u043e\u0434 \u043e\u0431\u0449\u0435\u043f\u0440\u0438\u043d\u044f\u0442\u044b\u0439 \u0432\u0438\u0434 \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u0444\u0438\u0447\u0438 \u0438 \u0438\u0437 \u0442\u0435\u0441\u0442\u0430 \u0438 \u0438\u0437 \u0442\u0440\u044d\u0439\u043d\u0430 (\u043e\u043d\u0438 \u0440\u0430\u0437\u043d\u043e\u0440\u043e\u0434\u043d\u044b\u0435)","452da5a8":"**\u0412\u0440\u0435\u043c\u044f \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c LightGbm \u043f\u043e\u0432\u0435\u0440\u0445 \u044d\u043c\u043f\u0435\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043f\u0440\u0430\u0432\u0438\u043b.**","3b69d326":"\u041c\u043e\u0436\u0435\u043c \u043d\u0430\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u0444\u0438\u0447, \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435 \u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u043c\u0435\u0436\u0434\u0443 \u0442\u0440\u044d\u0439\u043d\u043e\u043c \u0438 \u0442\u0435\u0441\u0442\u043e\u043c.","c86218d7":"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0444\u0438\u0447 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442. ","8bbf6bb6":"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0444\u0438\u0447 \u0432 \u0442\u0440\u044d\u0439\u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442.  \u041d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0445\u0430\u043a- \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043e\u0448\u0438\u0431\u0430\u0435\u0442\u0441\u044f \u044d\u043c\u043f\u0435\u0440\u0438\u043a\u0430 \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0438\u0445 \u0432 \u0441\u0435\u0442 \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438. \u042d\u0442\u0430\u043a\u043e\u0435 \u0440\u0443\u0447\u043d\u043e\u0435 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043a\u0438.","7a2f33b9":"God-function \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u043d\u0443 \u043f\u043e\u0447\u0442\u0438 \u0433\u043e\u0442\u043e\u0432\u0438\u0442).","4541c6b2":"\u042d\u043c\u043f\u0435\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0440\u0430\u0437\u0431\u043e\u0440\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f \u0441\u043c\u043e\u0433 \u043f\u0440\u0438\u0434\u0443\u043c\u0430\u0442\u044c."}}