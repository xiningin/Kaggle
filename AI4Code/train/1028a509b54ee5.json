{"cell_type":{"4f08d3d6":"code","e0a9a07b":"code","72c44728":"code","d1805c53":"code","5a8cafd9":"code","36c444e6":"code","c15937a1":"code","05883237":"code","63d7d32f":"code","4304b536":"code","5644f6c8":"code","452e2c45":"code","946ad050":"code","8324257a":"code","1f2d7eb9":"code","6b01bae3":"code","c77440c0":"code","279ee0ce":"code","c5d4815d":"code","c58c14a7":"code","4717ac9d":"code","b0c8416c":"code","3b257998":"code","e1d834c3":"code","6bdb05c9":"markdown"},"source":{"4f08d3d6":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport os\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom keras import optimizers\n","e0a9a07b":"print(\"Version \", tf.__version__)\nprint(\"Eager mode:\", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\",\"available\" if tf.config.list_physical_devices('GPU') else\"Not Available\")","72c44728":"data_dir = '..\/input\/plant-disease\/dataset'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')","d1805c53":"import time\nimport os\nfrom os.path import exists\n\ndef count(dir, counter=0):\n    \"returns number of files in dir and subdirs\"\n    for pack in os.walk(dir):\n        for f in pack[2]:\n            counter += 1\n    return dir + \" : \" + str(counter) + \"files\"","5a8cafd9":"print('total images for training :', count(train_dir))\nprint('total images for test :', count(test_dir))","36c444e6":"IMAGE_SHAPE = (224, 224)\n\nBATCH_SIZE = 64","c15937a1":"# Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning.\n\nvalidation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)\nvalidation_generator = validation_datagen.flow_from_directory(\n    test_dir, \n    shuffle=False, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)\n\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n  rescale = 1.\/255,\n  rotation_range=40,\n  horizontal_flip=True,\n  width_shift_range=0.2, \n  height_shift_range=0.2,\n  shear_range=0.2, \n  zoom_range=0.2,\n  fill_mode='nearest' )\n  \ntrain_generator = train_datagen.flow_from_directory(\n    train_dir, \n    subset=\"training\", \n    shuffle=True, \n    seed=42,\n    color_mode=\"rgb\", \n    class_mode=\"categorical\",\n    target_size=IMAGE_SHAPE,\n    batch_size=BATCH_SIZE)","05883237":"classes = {j: i for i, j in train_generator.class_indices.items()}","63d7d32f":"print('Number of classes:',len(classes))","4304b536":"  print (classes)","5644f6c8":"model = tf.keras.Sequential([\n  hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/4\", \n                 output_shape=[1280],\n                 trainable=False),\n  tf.keras.layers.Dropout(0.4),\n  tf.keras.layers.Dense(512, activation='relu'),\n  tf.keras.layers.Dropout(rate=0.2),\n  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n])","452e2c45":"#Compile model specifying the optimizer learning rate\n\nLEARNING_RATE = 0.001\n\nmodel.compile(\n   optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE), \n   loss='categorical_crossentropy',\n   metrics=['accuracy'])","946ad050":"EPOCHS=16\n\nhistory = model.fit(\n        train_generator,\n        steps_per_epoch=train_generator.samples\/\/train_generator.batch_size,\n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        validation_steps=validation_generator.samples\/\/validation_generator.batch_size)","8324257a":"import matplotlib.pylab as plt\nimport numpy as np\nimport cv2\nimport seaborn as sns\nimport random\nimport pandas as pd\nfrom sklearn.metrics import classification_report,confusion_matrix","1f2d7eb9":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","6b01bae3":"predictions = np.argmax(model.predict(validation_generator), axis=-1)","c77440c0":"class_names = [\"Class \" + classes[i] for i in range(len(classes))]\nprint(classification_report(validation_generator.classes, predictions, target_names = class_names))","279ee0ce":"cm = confusion_matrix(validation_generator.classes,predictions)\ncm = pd.DataFrame(cm , index = [i for i in range(len(classes))] , columns = [i for i in range(len(classes))])\nplt.figure(figsize = (30,30))\nsns.heatmap(cm,cmap= \"Greens\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","c5d4815d":"def load_image(filename):\n    img = cv2.imread(os.path.join(test_dir, filename))\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]) )\n    img = img \/255\n    \n    return img\n\n\ndef predict(image):\n    probabilities = model.predict(np.asarray([img]))[0]\n    class_idx = np.argmax(probabilities)\n    \n    return {classes[class_idx]: probabilities[class_idx]}","c58c14a7":"for idx, filename in enumerate(random.sample(validation_generator.filenames, 4)):\n    print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n    \n    img = load_image(filename)\n    prediction = predict(img)\n    print(\"PREDICTED: class: %s, confidence: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n    plt.imshow(img)\n    plt.figure(idx)    \n    plt.show()","4717ac9d":"import time\nt = time.time()\n\nexport_path = \"\/kaggle\/working\/saved_models\/{}.h5\".format(int(t))\nmodel.save(export_path)\n\nexport_path","b0c8416c":"# Now confirm that we can reload it, and it still gives the same results\nreloaded = tf.keras.models.load_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer})","3b257998":"def predict_reload(image):\n    probabilities = reloaded.predict(np.asarray([img]))[0]\n    class_idx = np.argmax(probabilities)\n    \n    return {classes[class_idx]: probabilities[class_idx]}","e1d834c3":"for idx, filename in enumerate(random.sample(validation_generator.filenames, 4)):\n    print(\"SOURCE: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n    \n    img = load_image(filename)\n    prediction = predict_reload(img)\n    print(\"PREDICTED: class: %s, confidence: %f\" % (list(prediction.keys())[0], list(prediction.values())[0]))\n    plt.imshow(img)\n    plt.figure(idx)    \n    plt.show()","6bdb05c9":"Output Analysis"}}