{"cell_type":{"37adac5d":"code","de278751":"code","7f69dc23":"code","fa74d08d":"code","933e6ce1":"code","26a9c8c6":"code","97daddab":"code","e1932aa5":"code","c031ddd3":"code","5822b2b5":"code","4745ae99":"code","da1857ee":"code","1bfd7f52":"code","ecf45ae4":"code","a12ada1e":"code","0f8d1708":"code","c8ca32b6":"code","fe0adc69":"code","a305c444":"code","919acb86":"code","95ae7ad4":"code","389e03f2":"code","6ff88767":"code","41944fbf":"code","bf2e03d7":"code","cffdbab6":"code","ce0c74d2":"code","1d717954":"code","d29d1f11":"code","c2251611":"code","8ec074c0":"code","6c7a1bb7":"code","540bed1d":"code","6dba39a8":"code","a93dc1a2":"code","d2712f20":"code","7aad19c8":"code","ddb29426":"code","e59691ac":"code","ef3bcf20":"code","4d6dc8b2":"code","9d2ba96f":"code","0b55f3b5":"code","57acd865":"code","0b449566":"code","c198b6fb":"code","b8b5a896":"markdown","dc714d78":"markdown","fb221963":"markdown","8104f2fd":"markdown","78e5725c":"markdown","486bf88a":"markdown","1296c8e8":"markdown","b8320b1f":"markdown","3e7f74e6":"markdown","5aa14875":"markdown","9a851a52":"markdown","0599bb41":"markdown","5f6fde41":"markdown","b9ea750a":"markdown","0a4aafb8":"markdown","739c6a68":"markdown"},"source":{"37adac5d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","de278751":"data= pd.read_csv('..\/input\/train.csv')\ndata2 = pd.read_csv('..\/input\/test.csv')","7f69dc23":"data.head()","fa74d08d":"data2.head()","933e6ce1":"data.shape","26a9c8c6":"data2.shape","97daddab":"data.describe()","e1932aa5":"data.isnull().sum()","c031ddd3":"data2.isnull().sum()","5822b2b5":"del data['Cabin']\ndel data2['Cabin']","4745ae99":"del data['PassengerId']\ndel data2['PassengerId']","da1857ee":"del data['Name']\ndel data2['Name']","1bfd7f52":"del data['Ticket']\ndel data2['Ticket']","ecf45ae4":"sns.boxplot('Pclass','Age',data=data)","a12ada1e":"sns.boxplot('Embarked','Age',data=data)","0f8d1708":"data['Age'].fillna(0,inplace=True)\ndata2['Age'].fillna(0,inplace=True)","c8ca32b6":"for i in range(891):\n    if data['Pclass'][i]==1 and data['Age'][i]==0:\n        data['Age'][i]=data['Pclass'].median()\n    elif data['Pclass'][i]==2 and data['Age'][i]==0:\n        data['Age'][i]=data['Pclass'].median()\n    elif data['Pclass'][i]==3 and data['Age'][i]==0:\n        data['Age'][i]=data['Pclass'].median()","fe0adc69":"for i in range(418):\n    if data2['Pclass'][i]==1 and data2['Age'][i]==0:\n        data2['Age'][i]=data2['Pclass'].median()\n    elif data2['Pclass'][i]==2 and data2['Age'][i]==0:\n        data2['Age'][i]=data2['Pclass'].median()\n    elif data2['Pclass'][i]==3 and data2['Age'][i]==0:\n        data2['Age'][i]=data2['Pclass'].median()","a305c444":"data.isnull().sum()","919acb86":"data2.isnull().sum()","95ae7ad4":"sns.countplot(data['Embarked'])","389e03f2":"data['Embarked'].fillna('S',inplace=True)","6ff88767":"data.isnull().sum()","41944fbf":"plt.figure(figsize=(7,7))\nsns.heatmap(data.corr(),annot=True,cmap='magma')","bf2e03d7":"sns.factorplot('SibSp','Survived',data=data,kind='bar',palette='muted')","cffdbab6":"sns.countplot('Pclass',data=data,hue='Survived')","ce0c74d2":"del data['Fare']\ndel data2['Fare']","1d717954":"sns.countplot('Sex',hue='Survived',data=data)","d29d1f11":"data.head()","c2251611":"data2.head()","8ec074c0":"X= data.iloc[:,1:].values\ny= data.iloc[:,0].values\nX_test= data2.iloc[:,:].values","6c7a1bb7":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nle_X_1= LabelEncoder()\nX[:,1]=le_X_1.fit_transform(X[:,1])\n# data2\nle_Xtest_1= LabelEncoder()\nX_test[:,1]=le_Xtest_1.fit_transform(X_test[:,1])","540bed1d":"le_X_2= LabelEncoder()\nX[:,5]=le_X_2.fit_transform(X[:,5])\nohe= OneHotEncoder(categorical_features=[5])\nX= ohe.fit_transform(X).toarray()\nX= X[:,1:]\n# data2\nle_Xtest_2= LabelEncoder()\nX_test[:,5]=le_Xtest_2.fit_transform(X_test[:,5])\nohe= OneHotEncoder(categorical_features=[5])\nX_test= ohe.fit_transform(X_test).toarray()\nX_test= X_test[:,1:]","6dba39a8":"from sklearn.model_selection import GridSearchCV,StratifiedKFold","a93dc1a2":"kfold = StratifiedKFold(n_splits=10)","d2712f20":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nrfc = RandomForestClassifier()\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [3,4,5,6],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(rfc,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X,y)\n\nRFC_best=gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","7aad19c8":"'''rfc=RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=10, min_samples_split=3,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nrfc.fit(X,y)\ny_pred=rfc.predict(X_test)'''","ddb29426":"'''from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nparams = {\n     'learning_rate': [0.05],\n     'n_estimators': [1000,1100],\n     'max_depth':[7,8],\n     'reg_alpha':[0.3,0.4,0.5]\n    }\n \n# Initializing the XGBoost Regressor\nxgb_model = XGBClassifier()\n \n# Gridsearch initializaation\ngsearch = GridSearchCV(xgb_model, params,\n                    verbose=True,\n                    cv=5,\n                    n_jobs=-1)\ngsearch.fit(X,y) \n#Printing the best chosen params\nXGB_best=gsearch.best_params_'''","e59691ac":"'''xgb_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n       max_depth=7, min_child_weight=1, missing=None, n_estimators=1000,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0.4, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\nprint(XGBClassifier())\n \n# Cross validation scores\nf1_scores = cross_val_score(xgb_model, X_train, y_train, cv=10, scoring='f1')\nprint(\"F1-score = \",f1_scores,\" Mean F1 score = \",np.mean(f1_scores))\n \n# Training the models\nxgb_model.fit(X_train,y_train)\ny_pred=xgb_model.predict(X_test)'''","ef3bcf20":"# Adaboost\n\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X,y)\n\nada_best=gsadaDTC.best_estimator_","4d6dc8b2":"'''from sklearn.tree import DecisionTreeClassifier\nadc=AdaBoostClassifier(algorithm='SAMME.R',\n                   base_estimator=DecisionTreeClassifier(class_weight=None,\n                                                         criterion='entropy',\n                                                         max_depth=None,\n                                                         max_features=None,\n                                                         max_leaf_nodes=None,\n                                                         min_impurity_decrease=0.0,\n                                                         min_impurity_split=None,\n                                                         min_samples_leaf=1,\n                                                         min_samples_split=2,\n                                                         min_weight_fraction_leaf=0.0,\n                                                         presort=False,\n                                                         random_state=None,\n                                                         splitter='best'),\n                   learning_rate=1.5, n_estimators=2, random_state=7)\nadc.fit(X,y)\ny_pred=adc.predict(X_test)'''","9d2ba96f":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X,y)\n\nGBC_best=gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_\n","0b55f3b5":"from sklearn.svm import SVC\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X,y)\n\nSVMC_best=gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","57acd865":"votingC = VotingClassifier(estimators=[('rfc', RFC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X, y)\ny_pred = votingC.predict(X_test)","0b449566":"data3=pd.read_csv('..\/input\/test.csv')","c198b6fb":"submission = pd.DataFrame({'PassengerId': data3.PassengerId, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nsubmission.to_csv('FirstCompetition_self.csv', index=False)","b8b5a896":"**'Embarked' column has much more 'S' category, thus we can replace Null values by S**","dc714d78":"**Pclass**","fb221963":"**Columns- PassengerId, Name, Ticket, are of no use in the prediction, as they are just values indicating nothing**","8104f2fd":"Person having 0 or 1 sibling\/spouse have more chances to survive.","78e5725c":"Now, lets Encode the 'Sex' and 'Embarked' column","486bf88a":"**We can replace 'Age' null values, with 'Pclass' each category median values as indicated by the Boxplot**<br>\n'Embarked' is not the right variable for 'Age' null values, as all median are nearly same.","1296c8e8":"**Adaboost**","b8320b1f":"Since, Higher the class, more will be the Fair. Thus, we can remove the 'Fare' column from the dataset","3e7f74e6":"**We can delete 'Cabin' as there too many null values**","5aa14875":"**Male is less Survived as compared to Females...**","9a851a52":"**SVC**","0599bb41":"**RandomForest**","5f6fde41":"Clearly, we can see that higher the class of passenger, more the probabilty that he\/she will be saved","b9ea750a":"**Gradient Boosting Classifier**","0a4aafb8":"**XGBOOST**","739c6a68":"**SibSp**"}}