{"cell_type":{"542e30bc":"code","d5365912":"code","a9a30ce6":"code","fc45ff1d":"code","1e849453":"code","d5a336fb":"code","02b10f56":"code","b64dd232":"code","2cbea6ad":"code","38f06582":"code","7e1a57bf":"code","2c302ca2":"code","260577ce":"code","fb25053c":"code","da93ff00":"code","153bd2fb":"code","ac5c6109":"code","9e60d35f":"code","a95b13e5":"code","d7881914":"code","ecec4820":"code","fd419755":"code","13ccffa6":"code","7d7e08c6":"code","3cc00cf3":"code","83746dc8":"code","0abcf0ea":"code","38ca51de":"code","da73a964":"code","faf79c03":"code","002d3ecb":"code","a0e8015a":"code","d391359e":"code","8d56357c":"markdown","a70ea1dd":"markdown","008abfdc":"markdown","7bb5df58":"markdown","04e47d06":"markdown","ac9133bd":"markdown","c64dcd21":"markdown","6b76efdb":"markdown","5b801ae6":"markdown","38cda0f6":"markdown"},"source":{"542e30bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5365912":"import torch\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport time\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","a9a30ce6":"df = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv',nrows=300000)\ndf.head()","fc45ff1d":"df=df.sample(frac=1).reset_index(drop=True)\ndf.head()","1e849453":"df.isna().sum()\ndf=df.dropna()\n    ","d5a336fb":"df.isna().sum()","02b10f56":"#Time is in UTC which is 4 hrs ahead of US Time\ndf['pickup_datetime']=pd.to_datetime(df['pickup_datetime']) - pd.Timedelta(hours=4)\ndf['Hour']=df['pickup_datetime'].dt.hour\ndf['Weekday']=df['pickup_datetime'].dt.strftime(\"%a\")\n\ndf.head()","b64dd232":"def distance(df,lat,long,lat1,long1):\n    radius=6371\n    latradians=np.radians(df[lat])\n    latradians1=np.radians(df[lat1])\n    diflatradians=np.radians(df[lat1]-df[lat])\n    diflongradians=np.radians(df[long1]-df[long])\n    a=np.sin(diflatradians\/2)**2 + np.cos(latradians)* np.sin(diflongradians\/2)**2 * np.cos(latradians1)\n    distance = radius * (2*np.arctan2(np.sqrt(a),np.sqrt(1-a)))\n    return distance","2cbea6ad":"df['distance']=distance(df,'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude')","38f06582":"df['distance'].describe()","7e1a57bf":"df['Am-Pm']='am'\ndf['Am-Pm'][df['Hour']>=12]='pm'\ndf.head()","2c302ca2":"df1=df[(df['fare_amount']>=0)  ]\ndf=df[(df['fare_amount']>=0)  ]\ndf1.shape\n# Some instances where fare_amount is less than 0","260577ce":"plt.hist(df['fare_amount'],bins=20)\nplt.show()\n","fb25053c":"df['fare_amount'].describe()","da93ff00":"col=['Hour','Weekday','Am-Pm','fare_amount']\ncol1=col[:-1]\ndf1=df1[col]\ndf1.head()\nprint(col1)","153bd2fb":"for i in col1:\n    ls1=[]\n    ls2=[]\n    for j in sorted(df1[i].unique()):\n        a=df1[df1[i]==j][\"fare_amount\"].mean(axis=0)\n        ls1.append(round(a,2))\n        ls2.append(j)\n    ls=pd.DataFrame(data = ls1, \n                  index = ls2, \n                  columns = [i+\"_avg_fare_amount\"])\n    ls.plot()\n    print(ls)","ac5c6109":"cat_col=[\"Hour\",\"Weekday\",\"Am-Pm\"]\ncon_col=['pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'distance','passenger_count' ]\ndf_cat=df[cat_col]\ndf_con=df[con_col]\ny_col = ['fare_amount']","9e60d35f":"#to convert the type to category\nfor cat in df_cat.columns:\n    df_cat[cat] = df_cat[cat].astype('category')\n\ndf_cat.info()","a95b13e5":"cat=np.stack([df_cat[i].cat.codes.values for i in df_cat.columns ],1)\ncat[:5]\ncat = torch.tensor(cat, dtype=torch.int64)","d7881914":"con=np.stack([df_con[i].values for i in df_con.columns],1)\ncon[:5]\ncon = torch.Tensor(con)\nprint(con.dtype)","ecec4820":"y=torch.Tensor(np.stack([df[y_col].values],1)).reshape(-1,1)\nprint(y.dtype)\n","fd419755":"#Embeddings for Categorical Variables\nembcat=[(df_cat[i].nunique(),(df_cat[i].nunique()+1)\/\/2) for i in df_cat.columns]\nprint(embcat)","13ccffa6":"#Dropout is used so as to to reduce over fitting\nclass Tabular1(nn.Module):\n    def __init__(self,con_n,out_sz,layers,embcat,p=0.5):\n        super().__init__()\n        layerlist=[]\n        self.drop=nn.Dropout(p)\n        self.emb=nn.ModuleList([nn.Embedding(i,o) for (i,o) in embcat])\n        self.cont=nn.BatchNorm1d(con_n)\n        total_cat=sum([o for (i,o) in embcat])\n        totaln=total_cat+con_n\n        for i in layers:\n            layerlist.append(nn.Linear(totaln,i))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            totaln=i\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        self.layers=nn.Sequential(*layerlist)\n    def forward(self,xcat,xcon):\n        embe=[]\n        for i, e in enumerate(self.emb):\n            embe.append(e(xcat[:,i]))\n        xcat=torch.cat(embe,1)\n        xcat=self.drop(xcat)\n        xcon=self.cont(xcon)\n        x=torch.cat([xcat,xcon],1)\n        x=self.layers(x)\n        return x","7d7e08c6":"model=Tabular1(con.shape[1],1,[200,100],embcat,0.4)","3cc00cf3":"model","83746dc8":"#number of parameters\ntotal=0\nfor param in model.parameters():\n    print(param.numel())\n    total+=param.numel()\nprint(f'Total: {total}')    ","0abcf0ea":"criteria=nn.MSELoss()\noptimizer=torch.optim.Adam(model.parameters(), lr=0.005)","38ca51de":"#to get batches\ncat_con=torch.cat([cat,con],1)\nprint(cat_con.shape)\ntrain_size=int(cat_con.shape[0]*0.8)\ncat_con_tr=cat_con[:train_size]\ncat_con_te=cat_con[train_size:]\nX_cat_tr=cat_con_tr[:,:len(df_cat.columns)]\nX_cat_tr=X_cat_tr.type(torch.long)\nX_cat_te=cat_con_te[:,:len(df_cat.columns)]\nX_con_tr=cat_con_tr[:,len(df_cat.columns):]\nX_cat_te=X_cat_te.type(torch.long)\nX_con_te=cat_con_te[:,len(df_cat.columns):]\nytr=y[:train_size]\nyte=y[train_size:]\nprint(cat_con_te.shape)\nprint(X_cat_tr.dtype)\nprint(ytr.shape)\nprint(X_cat_tr.shape)","da73a964":"train=TensorDataset(cat_con_tr,ytr)\nprint(train[0])\n#creating batches for training \ntrain_data=DataLoader(train,batch_size=20000,shuffle=True)","faf79c03":"import time\nstart=time.time()\nepochs=50\nlosses1=[]\nval_losses=[]\nfor i in range (epochs):\n    i=i+1\n    for b,(X_train,y_train) in enumerate(train_data):\n        X_train_cat=X_train[:,:len(df_cat.columns)]\n        X_train_cat = X_train_cat.type(torch.LongTensor)\n        X_train_con=X_train[:,len(df_cat.columns):]\n        y_pred = model(X_train_cat, X_train_con)\n        loss = torch.sqrt(criteria(y_pred, y_train))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    y_pred = model(X_cat_tr,X_con_tr)\n    loss = torch.sqrt(criteria(y_pred, ytr))\n    if i%10==0:\n        print(f'Epoch:{i} Loss:{loss.item():10.4f}')\n    losses1.append(loss.item()) \n    with torch.no_grad():\n        y_val = model(X_cat_te,X_con_te)\n        loss = torch.sqrt(criteria(y_val,yte))\n        val_losses.append(loss.item()) \nprint(f'Time in mins: {((time.time()-start)\/60):10.4f}')        ","002d3ecb":"plt.plot(losses1, label='training loss')\nplt.plot(val_losses, label='validation loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","a0e8015a":"with torch.no_grad():\n    y_val = model(X_cat_te,X_con_te)\n    loss = torch.sqrt(criteria(y_val,yte))\nprint(f'RMSE: {loss:.8f}')","d391359e":"for i in range (0,100):\n    print (f'Predicted: {y_val[i].item():6.2f}   Actual: {yte[i].item():6.2f}   Difference: {abs(y_val[i].item()-yte[i].item()):6.2f}')","8d56357c":"# Training","a70ea1dd":"# Creating Batches","008abfdc":"# Data Understanding","7bb5df58":"# Shuffling","04e47d06":"# NN Model with Flexible Layers","ac9133bd":"# Data Loading","c64dcd21":"# Categorical Variables Identified","6b76efdb":"# Distance Between Coordinates","5b801ae6":"Training for more Epochs and for more Layers can be looked into. Feel free to make changes to the code and get better models.\nPlease leave a like if you learnt something. Would motivate me alot.","38cda0f6":"# Output of NN"}}