{"cell_type":{"aedc5eeb":"code","f0780614":"code","12cdbc51":"code","a0d82fd3":"code","df2e7bff":"code","1c92bb90":"code","8a9ce3e7":"code","93ac3e8c":"code","968fc8ce":"code","9f963944":"code","a3989842":"code","932a3523":"code","074b2108":"code","28a8fe1a":"code","f550fca8":"code","66a8a66f":"code","1030af72":"code","b76f107f":"code","9fbc707c":"code","b110eddc":"code","acedb364":"code","ac8a4520":"code","9e091af7":"code","182bd6b7":"code","d94e62f7":"code","b2b7ca68":"code","25b02ccd":"markdown","532035ba":"markdown","5cdd773d":"markdown","6218e497":"markdown","fe6c3e1a":"markdown","b9796f94":"markdown","05ffc51b":"markdown","4ec5ef1d":"markdown","5cad6d40":"markdown","919497df":"markdown","6976e846":"markdown","bbb52b5e":"markdown","de7aac69":"markdown","f7df4fad":"markdown","0e8f8e6c":"markdown","0b4caf1d":"markdown","0c07a897":"markdown","31282fb9":"markdown","c5602287":"markdown","c101bb08":"markdown","10b78358":"markdown","53342d77":"markdown","d87dcdac":"markdown","c05a8cf2":"markdown","8d5823ed":"markdown","2fa1ed35":"markdown","21074e18":"markdown","4c01d53e":"markdown"},"source":{"aedc5eeb":"import os\n#for file management\nimport json\n#for standardized data storage\nimport glob\n#for precise file selection(low verbosity)\nimport random\n#random amount generator\nimport collections\n#has premade datastructure objects that can be implemented\nimport time\n#return the time in seconds since the epoch as a floating point number. A good way to watch your training progress quantitatively\nimport re\n#import regular expression matching operations. alows for facilitated use of string comparison. Docs:https:\/\/docs.python.org\/3\/library\/re.html\nimport math\n#provides mathematical functions established within the c-standard. gives ceil, factorial, etc. Docs:https:\/\/docs.python.org\/3\/library\/math.html\nimport numpy as np\n#linear algebra\nimport pandas as pd\n#succinct array and data handling\nimport cv2\n#image data handler\n\nimport matplotlib.pyplot as plt\n#data visualization library\nimport seaborn as sns\n#data visualization library\n\nimport pydicom\n#c based dicom modulation tool\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n#pixel handler that uses voi_lut to grab pixel data from within the frame of a window\n\nfrom random import shuffle\n#random library allows for generation of pseudo-random number and ways to use os to get as random as feasible. shuffle is method provided that will use a random function to effectively randomize the order of an array\nfrom sklearn import model_selection as sk_model_selection\n#the sklearn library provides the model_slection library and we set the name for this notebook to be 'sk_model_selection'.  it wraps the input validation data(X and true for training purposes ) and splits it according \n    #to a random function that can thereby be set to have a seed that will assure reproducible results accross training sessions. Stratification(approximately even proportions of 0 and 1 data) is the default\n    #Docs: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n\nfrom tensorflow import keras\n#the notorious tf library provises the keras framework that can be used to interface with artifical neural networks via methods and pre-built objects from the libary\n    #Docs: https:\/\/faroit.com\/keras-docs\/1.2.0\/\nfrom tensorflow.keras import layers\n#from the keras library import layers: \"A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\". source: https:\/\/www.tensorflow.org\/guide\/keras\/custom_layers_and_models\nimport tensorflow as tf\n#importing the tensor flow library as tf","f0780614":"data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\n#set data_directory variable to file location of the main dir\npytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\n#here we are loading a pretrained model from the EfficientNet-PyTorch-3D library","12cdbc51":"# import torch\n# from torch import nn\n# from torch.nn import functional as F\n\n# from .utils import (\n#     round_filters,\n#     round_repeats,\n#     drop_connect,\n#     get_same_padding_conv3d,\n#     get_model_params,\n#     efficientnet_params,\n#     Swish,\n#     MemoryEfficientSwish,\n# )\n\n# class MBConvBlock3D(nn.Module):\n#     \"\"\"\n#     Mobile Inverted Residual Bottleneck Block\n\n#     Args:\n#         block_args (namedtuple): BlockArgs, see above\n#         global_params (namedtuple): GlobalParam, see above\n\n#     Attributes:\n#         has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n#     \"\"\"\n\n#     def __init__(self, block_args, global_params):\n#         super().__init__()\n#         self._block_args = block_args\n#         self._bn_mom = 1 - global_params.batch_norm_momentum\n#         self._bn_eps = global_params.batch_norm_epsilon\n#         self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n#         self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n#         # Get static or dynamic convolution depending on image size\n#         Conv3d = get_same_padding_conv3d(image_size=global_params.image_size)\n\n#         # Expansion phase\n#         inp = self._block_args.input_filters  # number of input channels\n#         oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n#         if self._block_args.expand_ratio != 1:\n#             self._expand_conv = Conv3d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n#             self._bn0 = nn.BatchNorm3d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n#         # Depthwise convolution phase\n#         k = self._block_args.kernel_size\n#         s = self._block_args.stride\n#         self._depthwise_conv = Conv3d(\n#             in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n#             kernel_size=k, stride=s, bias=False)\n#         self._bn1 = nn.BatchNorm3d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n#         # Squeeze and Excitation layer, if desired\n#         if self.has_se:\n#             num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n#             self._se_reduce = Conv3d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n#             self._se_expand = Conv3d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n#         # Output phase\n#         final_oup = self._block_args.output_filters\n#         self._project_conv = Conv3d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n#         self._bn2 = nn.BatchNorm3d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n#         self._swish = MemoryEfficientSwish()\n\n#     def forward(self, inputs, drop_connect_rate=None):\n#         \"\"\"\n#         :param inputs: input tensor\n#         :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n#         :return: output of block\n#         \"\"\"\n\n#         # Expansion and Depthwise Convolution\n#         x = inputs\n#         if self._block_args.expand_ratio != 1:\n#             x = self._swish(self._bn0(self._expand_conv(inputs)))\n#         x = self._swish(self._bn1(self._depthwise_conv(x)))\n\n#         # Squeeze and Excitation\n#         if self.has_se:\n#             x_squeezed = F.adaptive_avg_pool3d(x, 1)\n#             x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n#             x = torch.sigmoid(x_squeezed) * x\n\n#         x = self._bn2(self._project_conv(x))\n\n#         # Skip connection and drop connect\n#         input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n#         if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n#             if drop_connect_rate:\n#                 x = drop_connect(x, p=drop_connect_rate, training=self.training)\n#             x = x + inputs  # skip connection\n#         return x\n\n#     def set_swish(self, memory_efficient=True):\n#         \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n#         self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n\n\n# class EfficientNet3D(nn.Module):\n#     \"\"\"\n#     An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n\n#     Args:\n#         blocks_args (list): A list of BlockArgs to construct blocks\n#         global_params (namedtuple): A set of GlobalParams shared between blocks\n\n#     Example:\n#         model = EfficientNet3D.from_pretrained('efficientnet-b0')\n\n#     \"\"\"\n\n#     def __init__(self, blocks_args=None, global_params=None, in_channels=3):\n#         super().__init__()\n#         assert isinstance(blocks_args, list), 'blocks_args should be a list'\n#         assert len(blocks_args) > 0, 'block args must be greater than 0'\n#         self._global_params = global_params\n#         self._blocks_args = blocks_args\n\n#         # Get static or dynamic convolution depending on image size\n#         Conv3d = get_same_padding_conv3d(image_size=global_params.image_size)\n\n#         # Batch norm parameters\n#         bn_mom = 1 - self._global_params.batch_norm_momentum\n#         bn_eps = self._global_params.batch_norm_epsilon\n\n#         # Stem\n#         out_channels = round_filters(32, self._global_params)  # number of output channels\n#         self._conv_stem = Conv3d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n#         self._bn0 = nn.BatchNorm3d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n#         # Build blocks\n#         self._blocks = nn.ModuleList([])\n#         for block_args in self._blocks_args:\n\n#             # Update block input and output filters based on depth multiplier.\n#             block_args = block_args._replace(\n#                 input_filters=round_filters(block_args.input_filters, self._global_params),\n#                 output_filters=round_filters(block_args.output_filters, self._global_params),\n#                 num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n#             )\n\n#             # The first block needs to take care of stride and filter size increase.\n#             self._blocks.append(MBConvBlock3D(block_args, self._global_params))\n#             if block_args.num_repeat > 1:\n#                 block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n#             for _ in range(block_args.num_repeat - 1):\n#                 self._blocks.append(MBConvBlock3D(block_args, self._global_params))\n\n#         # Head\n#         in_channels = block_args.output_filters  # output of final block\n#         out_channels = round_filters(1280, self._global_params)\n#         self._conv_head = Conv3d(in_channels, out_channels, kernel_size=1, bias=False)\n#         self._bn1 = nn.BatchNorm3d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n#         # Final linear layer\n#         self._avg_pooling = nn.AdaptiveAvgPool3d(1)\n#         self._dropout = nn.Dropout(self._global_params.dropout_rate)\n#         self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n#         self._swish = MemoryEfficientSwish()\n\n#     def set_swish(self, memory_efficient=True):\n#         \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n#         self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n#         for block in self._blocks:\n#             block.set_swish(memory_efficient)\n\n\n#     def extract_features(self, inputs):\n#         \"\"\" Returns output of the final convolution layer \"\"\"\n\n#         # Stem\n#         x = self._swish(self._bn0(self._conv_stem(inputs)))\n\n#         # Blocks\n#         for idx, block in enumerate(self._blocks):\n#             drop_connect_rate = self._global_params.drop_connect_rate\n#             if drop_connect_rate:\n#                 drop_connect_rate *= float(idx) \/ len(self._blocks)\n#             x = block(x, drop_connect_rate=drop_connect_rate)\n\n#         # Head\n#         x = self._swish(self._bn1(self._conv_head(x)))\n\n#         return x\n\n#     def forward(self, inputs):\n#         \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n#         bs = inputs.size(0)\n#         # Convolution layers\n#         x = self.extract_features(inputs)\n\n#         # Pooling and final linear layer\n#         x = self._avg_pooling(x)\n#         x = x.view(bs, -1)\n#         x = self._dropout(x)\n#         x = self._fc(x)\n#         return x\n\n#     @classmethod\n#     def from_name(cls, model_name, override_params=None, in_channels=3):\n#         cls._check_model_name_is_valid(model_name)\n#         blocks_args, global_params = get_model_params(model_name, override_params)\n#         return cls(blocks_args, global_params, in_channels)\n\n#     @classmethod\n#     def get_image_size(cls, model_name):\n#         cls._check_model_name_is_valid(model_name)\n#         _, _, res, _ = efficientnet_params(model_name)\n#         return res\n\n#     @classmethod\n#     def _check_model_name_is_valid(cls, model_name):\n#         \"\"\" Validates model name. \"\"\" \n#         valid_models = ['efficientnet-b'+str(i) for i in range(9)]\n#         if model_name not in valid_models:\n#             raise ValueError('model_name should be one of: ' + ', '.join(valid_models))","a0d82fd3":"mri_types = ['FLAIR','T1w','T1wCE','T2w']\n#setting the mri_types variable to the 4 image types we have in our folders \nSIZE = 256\n#setting a SIZE to be the magic number 256\nNUM_IMAGES = 64\n#setting the a magic number variable NUM_IMAGES to 64","df2e7bff":"train_df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\n#creating a dataframe from our train_labels.csv\n\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID]\n#we are creating a column BraTS21ID5 that represents the 5 digit long ID and uses regular expressions in order to reformat the string contents that are read from the BraTS21ID column \n    #side note: this was clever","1c92bb90":"x = train_df.BraTS21ID[25]\nprint(str(x) + \"  versus  \" + format(x, '05d'))\n# format(x, '05d') is effectively in words: format the d(data) in x such that it 5 characters where the empty slots are zero","8a9ce3e7":"train_df.head(5)\n#prints the first 5 rows of the dataframe","93ac3e8c":"data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\npytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\n \nmri_types = ['FLAIR','T1w','T1wCE','T2w']\nIMAGE_SIZE = 256\nNUM_IMAGES = 64\nBATCH_SIZE= 4\ntrain_df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID]\ntrain_df.head(3)","968fc8ce":"data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\n#set data_directory variable to file location of the main dir\npytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\n#here we are loading a pretrained model from the EfficientNet-PyTorch-3D library\n \nmri_types = ['FLAIR','T1w','T1wCE','T2w']\n#setting the mri_types variable to the 4 image types we have in our folders \nSIZE = 256\n#setting a SIZE to be the magic number 256\nNUM_IMAGES = 64\n#setting the a magic number variable NUM_IMAGES to 64\n\ntrain_df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\n#creating a dataframe from our train_labels.csv\n\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID]\n#we are creating a column BraTS21ID5 that represents the 5 digit long ID and uses regular expressions in order to reformat the string contents that are read from the BraTS21ID column ","9f963944":"def load_dicom_image(path, img_size=SIZE, voi_lut=True, rotate=0):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        data = cv2.rotate(data, rot_choices[rotate])\n        \n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\", rotate=0):\n\n    files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        \n    if np.min(img3d) < np.max(img3d):\n        img3d = img3d - np.min(img3d)\n        img3d = img3d \/ np.max(img3d)\n            \n    return np.expand_dims(img3d,0)","a3989842":"def load_dicom_image(path, img_size=SIZE, voi_lut=True, rotate=0):\n    #load_dicom_image function provide from the pydicom library. pass path to point at to grab and load the dicom data to pixel data. \n        #voi lut is true so we remove area in the image within the given frame of reference, rotate=0 is the degree angle relative to the positive X axis we want to rotate the image, so 0 degrees \n    dicom = pydicom.read_file(path)\n    #define the dicom variable that is the dcm object from the file within the given path\n    data = dicom.pixel_array\n    #define the data variable to the pixel_array data of the dicom image(see metadata for available attributes)\n    if voi_lut: #if voi_lut is True (which it should be, as we just set it to be)\n        #if we can isolate the data to more valuable information by removing window size-do it with apply_voi_lut\n        data = apply_voi_lut(dicom.pixel_array, dicom) #\" Value Of Interest LUT to eliminate area's that are not\n                                                       #clinically significant (think Window width\/level)\"-source: https:\/\/groups.google.com\/g\/comp.protocols.dicom\/c\/0YAVTRd3BZ0\n    else:\n        #if somehow the always True we setup in the if fails just give us the pixel_array of the loaded dicom and set the variable data to its value\n        data = dicom.pixel_array\n        \n        #if somehow rotate is note what we set it to use the cv2 library's image handling capability to rotate the pixel_data on its axis by 0 degrees clockwise, so dont rotate it lol\n        #a good resource for this: https:\/\/www.geeksforgeeks.org\/python-opencv-cv2-rotate-method\/\n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        #data = pixel_data in the same place it started because we rotate by 0 degrees clockwise\n        data = cv2.rotate(data, rot_choices[rotate])\n        \n        #resize the data using our presets from earlier\n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\n\n#constructing 3d dicom images with our given meta data, image num(depth), img size(height and width), scan type, split category(train) and with rotate set to 0 or rather, 'no rotation while processing'\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\", rotate=0):\n\n       files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n                  #we set files equal to a sorted function\n                      #glob.glob function parses an f string using our input parameters\n                      \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n                  #we set our key value that will dictate our sorting to a lambda function that utilizes an regexpression\n                        #logic annotated in expanded form example below\n    \n    #lambda function in expanded form\n    '''\n                    --define lambda function--\ndef lambda(var):\n                    ---for x in the contents of the 1d array generated by parsing each individual character grouping via the regex expression and re.findall expression---\n    for x in re.findall((r'[^0-9]|[0-9]+', var)):\n                    ---if x is a digit, return the value as an integer in an array and using the sorted value use that mapped array that was generated at the given-----\n                    ---values to return a files variable the is in numerical ascending order based on Image-#--------\n            if x.isdigit():\n                return int(x)\n            else:\n                return x\n                \n    '''               \n    \n    middle = len(files)\/\/2\n    #set variable middle to the index at the halfway point of the files array\n    \n    num_imgs2 = num_imgs\/\/2\n    #set num_imgs2 to half of the number of images present\n    \n    p1 = max(0, middle - num_imgs2)\n    #sets p1 to the value of the middle array - run_imgs2 unless num_imgs > than halfway indice in which case it is 0\n    \n    p2 = min(len(files), middle + num_imgs2)\n    #sets p2 to the value of the either the length of the files array or the  value of half of the len of + 32(half of 64 from earlier)\n        #im assuming the goal here is to create indices by which to iterate through while generating 3D arrays for each respective folder without compromising too much in terms of standardization(someone please correct me if I am wrong)\n    \n    img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T #uses a for loop in a \n    #set img3d to the transposition(were flipping the result array) of the stacked array generated by stacking the each dicom image per file between the p1 amount and p2 amount, including p2 value\n        #better conceptual explanation:\n        '''\n        #we are in effect grabbing images that are more likely to have pixel data as they aren't going to be the first few empty frames due to our p1 and p2 setup and\n        #then from there we are iterating through the p1 and p2 amounts and loading the dicom data from each respective image, transposing it, and stacking it on the previous image(this is how we get that 3d image)\n        i am curious however, why transpose? are the new dimensions more favorable and why?\n        '''\n    if img3d.shape[-1] < num_imgs:\n        #conditional to check for the number of layers the 3d image has and if that is less than the number of images in the folder we used \n        \n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        #returns a 3d array of the same dimensions as the generated 3d image that accounts for the missing layers we filtered out earlier\n        \n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        #we add the 3d image with the blank array along the last axis of the array and remaining dimensions\n        \n    if np.min(img3d) < np.max(img3d):\n        #conditional that normalizes the checks to see if the minimum value is less than the max value in the array(which it should be)\n        \n        #in these two statements we are normalizing the data contained within the 3d array to values between 0 and 1 via subtracting the im3d element amounts by the mean array element and then dividing the im3d array elements by the max element value in the array\n        img3d = img3d - np.min(img3d)\n        img3d = img3d \/ np.max(img3d)\n     \n    #returns what I believe is a 4d array via expanding the dimensions here, but that dim will remain as 1\n    return np.expand_dims(img3d,0)\n\n#where a is an example 3d image object we made at id 00019\na = load_dicom_images_3d(\"00019\")\n\n#(1, 256, 256, 64)\nprint(a.shape)\n\n#0.0 1.0 0.07237920020074569 0.0\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))\n\nimage = a[0]\n#set image to the slice(2d image) at a[0]\n\nprint(\"Dimension of the CT scan is:\", image.shape)\n#Dimension of the CT scan is: (256, 256, 64)\n\nplt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\n'''comment from the author:\na = load_dicom_images_3d(\"00000\")\nprint(a.shape)--> the output is (1, 256, 256, 64) (channel,width,height,depth) \/\/ channel=1 because we have gray images.\nimage = a[0] \/\/ I selected first channel already I have one channel\n\nEach three-dimensional region is called a voxel. A typical input is 256x256x64 (width,height,depth) voxels, where 64 is the number of slices and 256x256 is the resolution of each image.\nwhen I selected \"image[:, :, 31]\" to get slice number 31 that is in the middle voxel, you can change the number to get another slice.\n\nGood Luck.\n\n'''\n\n#use the np.squeeze when accessing the image pixel data at a depth of the 30th pixel(?) and remove the unnecessary \n#dimension(we grabbed a single slice from a 3d image so it makes sense we still have 256,256,_ where the _ is the z or depth at which you took that slice)","932a3523":"path = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/00000\/FLAIR\/Image-1.dcm'\n\ndicom = pydicom.read_file(path)\ntype(dicom) #should return FileDataset which holds the metadata for a given dcm file","074b2108":"df_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=12, \n    stratify=train_df[\"MGMT_value\"],\n)\n\nlen(df_train)","28a8fe1a":"#use the sklearn split mechanism and stratify train data\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=12, \n    stratify=train_df[\"MGMT_value\"],\n)\n\nlen(df_train)\n#pretty sure this was done just to double check his df was correct before proceeding","f550fca8":"from keras.utils import Sequence\nclass Dataset(Sequence):\n    def __init__(self,df,is_train=True,batch_size=1,shuffle=True):\n        self.idx = df[\"BraTS21ID\"].values\n        self.paths = df[\"BraTS21ID5\"].values\n        self.y =  df[\"MGMT_value\"].values\n        self.is_train = is_train\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n    def __len__(self):\n        return math.ceil(len(self.idx)\/self.batch_size)\n   \n    def __getitem__(self,ids):\n        id_path= self.paths[ids]\n        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n        \n        if self.y is not None:\n            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n            \n        list_x =  load_dicom_images_3d(id_path)#str(scan_id).zfill(5)\n        #list_x =  [load_dicom_images_3d(x) for x in batch_paths]\n        batch_X = np.stack(list_x)\n        if self.is_train:\n            return batch_X,batch_y\n        else:\n            return batch_X\n    \n    def on_epoch_end(self):\n        if self.shuffle and self.is_train:\n            ids_y = list(zip(self.idx, self.y))\n            shuffle(ids_y)\n            self.idx, self.y = list(zip(*ids_y))","66a8a66f":"from keras.utils import Sequence\n#Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators. src: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/Sequence\n\nclass Dataset(Sequence):\n   # override and add to parent class sequence that returns getiter, iter, and len magic functions\n    def __init__(self,df,is_train=True,batch_size=BATCH_SIZE,shuffle=True):\n        #set idx to the array of id values\n        self.idx = df[\"BraTS21ID\"].values\n        #set the paths values to the values inside the ID5 array we created earlier\n        self.paths = df[\"BraTS21ID5\"].values\n        #set the y values to the values of the true mgmt values \n        self.y =  df[\"MGMT_value\"].values\n        #set is_train to True\n        self.is_train = is_train\n        #set batch_size to the parameter passed for batchsize\n        self.batch_size = batch_size\n        #set shuffle to True\n        self.shuffle = shuffle\n        \n    def __len__(self):\n        return math.ceil(len(self.idx)\/self.batch_size)\n        #find the ratio of idx to batch size and round up to find the declared len of this dataset object\n   \n    def __getitem__(self,ids):\n        id_path= self.paths[ids]\n        #grabs current id path from paths array at ids\n        \n        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n        #creates and sets batch_paths to the range between the batch size at current ids*batch_size and next ids*batch_size\n        \n        if self.y is not None:\n            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n            #initializing matching y values for corresonding ids\n            \n        #author comment: list_x =  load_dicom_images_3d(id_path)  #str(scan_id).zfill(5)\n        list_x =  [load_dicom_images_3d(x) for x in batch_paths]\n        #create a list of 3d dicom images for the number of batch paths(double check needed)\n        batch_X = np.stack(list_x, axis=4)\n        #create an array on a new axis, 4\n        if self.is_train:\n            return batch_X,batch_y\n            #return the appropriately setup batch for training\n        else:\n            return batch_X\n    \n    def on_epoch_end(self):\n        if self.shuffle and self.is_train:#which will be true\n            ids_y = list(zip(self.idx, self.y))\n            #create and set ids_y to a list of pairings via dictionary zipping of the idx and corresponding y\n            shuffle(ids_y) #shuffle the order of the pairings within the array\n            self.idx, self.y = list(zip(*ids_y))\n            #unpacks the pairs and sets the idx and y to be the respective arrays for each with corresponding indexing","1030af72":"train_dataset = Dataset(df_train)\nvalid_dataset = Dataset(df_valid)\n#test_dataset = Dataset(test,is_train=False)","b76f107f":"for i in range(1):\n    images, label = train_dataset[i]\n    print(\"Dimension of the CT scan is:\", images.shape)\n    print(\"label=\",label)\n    plt.imshow(images[0,:,:,32,0], cmap=\"gray\")\n    plt.show()","9fbc707c":"#iinitialize datasets for train and valid dfs\ntrain_dataset = Dataset(df_train)\nvalid_dataset = Dataset(df_valid)\n#test_dataset = Dataset(test,is_train=False)\nlen(train_dataset)","b110eddc":"#test for that newly generated dataset is accessible and representative of goal dimensions\nfor i in range(1):\n    images, label = train_dataset[i]\n    #grabs a single image and its label\n    print(\"Dimension of the CT scan is:\", images.shape)\n    print(\"label=\",label)\n    plt.imshow(images[0,:,:,32,0], cmap=\"gray\")\n    #our image dimensionality at this point is a bit odd but all that matters it that we are grabbing the image at i in our data set where our axis=4 is at the value 0\n    plt.show()","acedb364":"def get_model(width=256, height=256, depth=64):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = keras.Input((width, height, depth, 1))\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n\n    return model\n\n\n# Build model.\nmodel = get_model(width=256, height=256, depth=64)\nmodel.summary()","ac8a4520":"def get_model(width=IMAGE_SIZE, height=IMAGE_SIZE, depth=64):\n    #grab voxel data from our data\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = keras.Input((width, height, depth, 1))\n    #setting the shape of our keras object to be our voxel values with the inclusion of 1 for indicating the channel is gray scale(3 would have been RGB)\n        #we are effectively stating that we have a depth number of inputs of widthxheight images per input src:https:\/\/stackoverflow.com\/questions\/55572325\/how-can-i-use-neutral-network-with-gray-scale-images-in-keras\n        #keras docs:https:\/\/keras.io\/api\/layers\/core_layers\/input\/\n        \n    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)#input specifies what values are being used\n    #  filters ; Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution)\n        #the abstracted frame our model will continue to use to find features\n    #  kernel_size ; An integer or tuple\/list of 3 integers, specifying the depth, height and width of the 3D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n        #indicates the dimensionality of the 3D data we are passing\n    #activation='relu' parses input and returns a max value based on threshold(we used default)\n        #\"this returns the standard ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\"-https:\/\/keras.io\/api\/layers\/activations\/\n\n    x = layers.MaxPool3D(pool_size=2)(x)\n    #Downsamples the input along its spatial dimensions (depth, height, and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. \"-https:\/\/keras.io\/api\/layers\/pooling_layers\/max_pooling3d\/\n        #pool_size=2 specifies (2,2,2) implicitly thereby acting as the strides along each dimension that they are shifted by\n        '''\n        x=30\n        y=30\n        z=30\n        color=1\n        pool_size=3\n        \n        results in:\n        x=10\n        y=10\n        z=10\n        color=1\n        \n        '''\n    x = layers.BatchNormalization()(x)\n    #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.-https:\/\/keras.io\/api\/layers\/normalization_layers\/batch_normalization\/\n        #batch normalization is different for training than inference, here it normalizes output using the mean and standard deviation versus in inference it will be calculated using a moving avg and std\n    \n    x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.01)(x)\n    #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1\/(1 - rate) suchh\n    #that the sum over all inputs is unchanged.-https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/\n\n    #repeat layers to increase predictive resoloution. note the dropout scaling to the change in size after previous dropout. filters continually expand and pool size changes\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.01)(x)\n    \n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.02)(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.03)(x)\n\n    x = layers.Conv3D(filters=512, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.04)(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    #default shape input : (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n    #default shape output :  (batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)\n        #reconfigures datastructure(?)\n\n    x = layers.Dense(units=1024, activation=\"relu\")(x)\n    x = layers.Dropout(0.08)(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n    #defining\/storing keras model object with its learned parameters\n\n    return model\n\n# Build model.\nmodel = get_model(width=IMAGE_SIZE, height=IMAGE_SIZE, depth=64)#generating model\nmodel.summary()#will print the type of all layers in a model, output shape for each layer, number of weight parameter of each layer, model general topology(pretty sure this is loss and similar components)\n                #, and the total number of trainable and nontrainable params","9e091af7":"# Compile model.\ninitial_learning_rate = 0.0005\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    metrics=[\"acc\"],\n)\n\n# Define callbacks.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    \"Brain_3d_classification.h5\", save_best_only=True\n)\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# Train the model, doing validation at the end of each epoch\nepochs = 50\nmodel.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n)","182bd6b7":"# Compile model.\ninitial_learning_rate = 0.0001 #sets the step size to 0.0001 for optimizer\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True#the following is true of these params;\n    #initial_learning_rate * decay_rate ^ (step \/ decay_steps) is what generates the step size where the exponent exists if we set staircase=True which we have\n    #When fitting a Keras model, decay every 100000 steps with a base of 96-learning_rate_schedules\n        #we are effectively generating a decaying step learning rate as we get closer and closer to the minimum for a better chance at getting a min\n)\nmodel.compile( #generates model object with ;\n    loss=\"binary_crossentropy\", #binary_crossentropy loss function\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), #optimizer Adam with learning_rate prescribed the object lr_schedule \n    metrics=[AUC(name='auc'),\"acc\"],# defines AUC metric\n    '''quick AUC def:\n    \"The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between \n    classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of\n    the model at distinguishing between the positive and negative classes\"-https:\/\/www.analyticsvidhya.com\/blog\/2020\/06\/auc-roc-curve-machine-learning\/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.'''\n)\n# Define callbacks.\nmodel_save = ModelCheckpoint('Brain_3d_cls_FLAIR.h5', #prevents overly random training and leverages ability to specify a successful model to iterate more effectively \n                             save_best_only = True, \n                             monitor = 'val_auc', \n                             mode = 'max', verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_auc', \n                           patience = 15, mode = 'max', verbose = 1,\n                           restore_best_weights = True)\n\n# Train the model, doing validation at the end of each epoch\nepochs = 100\nmodel.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=1,#prints training values per epoch if set=1\n    callbacks = [model_save, early_stop],\n)","d94e62f7":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"acc\", \"loss\"]):\n    ax[i].plot(model.history.history[metric])\n    ax[i].plot(model.history.history[\"val_\" + metric])\n    ax[i].set_title(\"Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])","b2b7ca68":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n#create two figures with width 20 by 7 on one row\n\nax = ax.ravel()\n#create a flattened, 1d array\n\n#iterate through metric values then parse and wrap values for graph beautification\nfor i, metric in enumerate([\"acc\", \"loss\"]):\n    ax[i].plot(model.history.history[metric])\n    ax[i].plot(model.history.history[\"val_\" + metric])\n    ax[i].set_title(\"Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])","25b02ccd":"#### **<center>Unannotated: Model<\/center>**\n","532035ba":"#### **<center>Unannotated: Training<\/center>**\n","5cdd773d":"#### **<center>Unannotated: Loading Data<\/center>**","6218e497":"**reviewing the dicom variable's data type**","fe6c3e1a":"#### **<center>Annotated: Loading Data<\/center>**","b9796f94":"## [Quick Demonstration] Printing some regular expressions","05ffc51b":"#### **<center>Annotated: Custom Data Generator<\/center>**\n","4ec5ef1d":"**I went ahead and grabbed the model details and will leave it in the next code block for those interested:**\n> I can, and likely will have to come back and review this in more detail and annotate some more when I get to my model collection period for ensembling this competition(will get back with this)","5cad6d40":"#### **<center>Annotated<\/center>**","919497df":"#### **<center>Unannotated: Splitting Data<\/center>**","6976e846":"#### **<center>Unannotated: Custom Data Generator<\/center>**","bbb52b5e":"#  \"why 256 and 64?\"* \n[source](https:\/\/www.researchgate.net\/post\/Which_Image_resolution_should_I_use_for_training_for_deep_neural_network)\n******\n\n**The answer lies in the object we will be declaring later that has been provided to us via this pytorch library, time constraints, and opinion.**\n\n**256x256 is 'known-good' for image classification problems and we will be training our data in batches as we have quite a bit and the choice was 64 here.**\n\n**later on we will be generating a 3D object from our 2d slices, we will make the depth of said objects 64 pixels**\n\n**32-64 is also allegedly ideal for batches of image data but ultimately it depends on if you have a GPU cluster at your finger tips or not as to whether or not you want to do larger batch sizes with more epochs(not me sadly, taking donations;) )**","de7aac69":"#### **<center>Unannotated: initializing custom dataset<\/center>**\n","f7df4fad":"## images from original source:\n    https:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-training\/output","0e8f8e6c":"#### **<center>Unannotated: Visualizing model performance<\/center>**\n","0b4caf1d":"#### **<center>Annotated: initializing custom dataset<\/center>**","0c07a897":"#### **<center>Unannotated: Functions to load images<\/center>**","31282fb9":"#### **<center>Annotated: Training<\/center>**\n","c5602287":"> see author's model feedback(currently working with a lot of limitations time wise)\nhttps:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-training?scriptVersionId=72507803&cellId=20","c101bb08":"#### **<center>Annotated: Splitting Data<\/center>**\n","10b78358":"#### Data setup","53342d77":"#### **<center>Annotated: Model<\/center>**\n","d87dcdac":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 5px;\n              color:white;\">\n                   <center> Author: @ammarnassanalhajali<\/center> <br>\n                   <center> Original Notebook: <a href=\"https:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-training\" style=\"color:gold\" >Notebook<\/a> <\/center> <br>\n<\/p>               \n<\/div>","c05a8cf2":"#### **<center>Annotated: Functions to load images<\/center>**\n","8d5823ed":"#### **<center>Annotated: Visualizing model performance<\/center>**\n","2fa1ed35":"#### **<center>Imports<\/center>**","21074e18":"![image.png](attachment:f629e45b-769b-4086-a106-fa3c195e3304.png)![image.png](attachment:6828f8c7-7180-4741-a14b-9ff71f09b6ef.png)","4c01d53e":"<center><h1>In this Notebook we will be going through and annotating @ammarnassanalhajali 's \"\ud83e\udde0Brain Tumor 3D [Training]\" Notebook<br> ( <i>please see the original work<\/i> )<\/center>\n\n### The Goal: provide clear analysis as to what the logic in each function is doing such that anyone new to ml or python can replicate a given concept with their own logic after some critical thought\n\n#### Last Couple of Remarks before the Annotations: I am currently working through annotating the model section of this notebook as well as a few more preprocessing notebooks. Thanks for Reading!\n\n#### (8\/25 update) I apologize for the late annotations on this, school started up and things got busy\n\n#### (8\/25 update) see author's continuation of this nb w\/ inference @ https:\/\/www.kaggle.com\/ammarnassanalhajali\/brain-tumor-3d-inference\/data"}}