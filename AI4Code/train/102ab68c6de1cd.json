{"cell_type":{"ca3cdced":"code","a4484131":"code","c92c271b":"code","a694627f":"code","c958cd67":"code","c97eb4b4":"code","15ad5b13":"code","9e94e693":"code","e9e090a4":"code","ef810431":"code","12fcb7bd":"code","0fd598ff":"code","45374b0e":"code","80815b15":"code","7bd34fea":"code","32410367":"code","afa5b18b":"code","892f8110":"code","6f41c19d":"code","137a7be5":"code","823e84df":"code","30fae381":"code","0339dc8e":"code","3cb4059a":"markdown","51ab4660":"markdown","dc73567d":"markdown","f53daa08":"markdown","607a76cf":"markdown","199eb525":"markdown","b48b4996":"markdown","1a83188d":"markdown","a6becf05":"markdown","9b5a8cd7":"markdown","8bb82288":"markdown","6b050f82":"markdown","85b4053e":"markdown","93544667":"markdown"},"source":{"ca3cdced":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","a4484131":"df = pd.read_csv('..\/input\/train.csv',nrows=5123456, usecols=[1,2,3,4,5,6,7])","c92c271b":"df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 16)\ndf['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=False, format='%Y-%m-%d %H:%M')","a694627f":"df.head()","c958cd67":"df.dtypes","c97eb4b4":"test = pd.read_csv('..\/input\/test.csv').set_index('key')\ntest['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=False, format='%Y-%m-%d %H:%M')\ntest.head()","15ad5b13":"test.describe(include=\"all\")","9e94e693":"# Remove the few observations with missing values\ndf.dropna(how='any', axis='rows', inplace=True)\n\n# Removing observations with erroneous values\nmask = df['pickup_longitude'].between(-75, -72.8)\nmask &= df['dropoff_longitude'].between(-75, -72.8)\nmask &= df['pickup_latitude'].between(40, 42)\nmask &= df['dropoff_latitude'].between(40, 42)\nmask &= df['passenger_count'].between(0, 7)\nmask &= df['fare_amount'].between(0, 250)\n\ndf = df[mask]","e9e090a4":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    \n    return distance","ef810431":"def transform(data):\n    # Extract date attributes and then drop the pickup_datetime column\n    data['hour'] = data['pickup_datetime'].dt.hour\n    data['day'] = data['pickup_datetime'].dt.day\n#     data['month'] = data['pickup_datetime'].dt.month\n    data['year'] = data['pickup_datetime'].dt.year\n    \n\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    data['distance_to_center'] = dist(nyc[1], nyc[0],\n                                      data['pickup_latitude'], data['pickup_longitude'])\n    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                         data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n#     data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], \n#                                           data['pickup_latitude'], data['pickup_longitude'])\n#     data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],\n#                                            data['dropoff_latitude'], data['dropoff_longitude'])\n    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                          data['pickup_latitude'], data['pickup_longitude'])\n    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],\n                                           data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    data['dist'] = dist(data['pickup_latitude'], data['pickup_longitude'],\n                        data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    return data","12fcb7bd":"df = transform(df)\ntest = transform(test)","0fd598ff":"df.to_csv(\"taxiFare5M_train.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"taxiFare5M_test.csv.gz\",index=False,compression=\"gzip\")","45374b0e":"df.drop('pickup_datetime', axis=1,inplace=True)\n# test.drop('pickup_datetime', axis=1,inplace=True)","80815b15":"import xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","7bd34fea":"X_train, X_test, y_train, y_test = train_test_split(df.drop('fare_amount', axis=1),\n                                                    df['fare_amount'], test_size=0.25)\ndel(df)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndel(X_train)\ndtest = xgb.DMatrix(X_test)\ndel(X_test)","32410367":"def xgb_evaluate(max_depth, gamma, colsample_bytree):\n    params = {'eval_metric': 'rmse',\n              'max_depth': int(max_depth),\n              'subsample': 0.8,\n              'eta': 0.1,\n              'gamma': gamma,\n              'colsample_bytree': colsample_bytree}\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=60, nfold=2)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","afa5b18b":"xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), \n                                             'gamma': (0, 1),\n                                             'colsample_bytree': (0.3, 0.9)})\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\nxgb_bo.maximize(init_points=5, n_iter=6, acq='ei')","892f8110":"params = xgb_bo.res['max']['max_params']\nparams['max_depth'] = int(params['max_depth'])\nprint(params)","6f41c19d":"# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)\n\n# Report testing and training RMSE\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(np.sqrt(mean_squared_error(y_train, y_train_pred)))","137a7be5":"import matplotlib.pyplot as plt\nfscores = pd.DataFrame({'X': list(model2.get_fscore().keys()), 'Y': list(model2.get_fscore().values())})\nfscores.sort_values(by='Y').plot.bar(x='X')","823e84df":"# Predict on holdout set\ntest = transform(test)\ntest.drop('pickup_datetime', axis=1,inplace=True)","30fae381":"dtest = xgb.DMatrix(test)\ny_pred_test = model2.predict(dtest)","0339dc8e":"holdout = pd.DataFrame({'key': test.index, 'fare_amount': y_pred_test})\nholdout.to_csv('submission.csv', index=False)","3cb4059a":"## Training\nOptimizing hyperparameters with bayesian optimization. I've tried to limit the scope of the search as much\nas possible since the search space grows exponentially when considering aditional hyperparameters.\n\nGPU acceleration with a few pre tuned hyperparameters speeds up the search a lot.","51ab4660":"## Feature Engineering\nManhattan distance provides a better approximation of actual travelled distance than haversine for most trips.","dc73567d":"## Train\/Test split","f53daa08":"## Clean","607a76cf":"* ## Predict on (external) Test Set","199eb525":"## Testing","b48b4996":"Being careful about memory management, which is critical when running the entire dataset.","1a83188d":"See __[NYC Taxi Fare - Data Exploration](https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration)__ for an excellent EDA on this dataset and the intuition for including airports.","a6becf05":"# Bayesian Optimization with XGBoost","9b5a8cd7":"Slicing off unecessary components of the datetime and specifying the date format results in a MUCH more efficiecnt conversion to a datetime object.","8bb82288":"## Submit predictions","6b050f82":"## Feature Importance","85b4053e":"## Read Data\nUse all data for a slightly better score. \nThe data appears to be randomized, so reading in the beginning rows is acceptable.\n\nUsing the entire dataset will use ~ 32gb of memory","93544667":"Extract the parameters of the best model."}}