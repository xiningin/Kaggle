{"cell_type":{"91793e11":"code","bc537e0f":"code","57256269":"code","98dba868":"code","106655d8":"code","cd35f371":"code","0a772f1b":"code","6404b78a":"code","c0f5eb98":"code","7e14f7ae":"code","a5246fbc":"code","78153891":"code","3500fea0":"code","d9177272":"code","8aa0aedc":"code","f8227d3f":"code","bc6891ce":"code","9a0ebf76":"code","eedf16fd":"code","d450a989":"code","585db126":"code","44889061":"code","18755afd":"code","715eae08":"code","e018149e":"code","13ad8e34":"code","f45e7e64":"markdown","46648382":"markdown","4eb9de33":"markdown","0ff5c9e1":"markdown","8c8b646d":"markdown","41429649":"markdown","19849340":"markdown","c48862a5":"markdown","965f43e2":"markdown","4a098147":"markdown","b256d236":"markdown","8d494d50":"markdown"},"source":{"91793e11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc537e0f":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","57256269":"test.info(memory_usage=\"deep\")","98dba868":"train.head(10)","106655d8":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","cd35f371":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","0a772f1b":"# Statistical description of the train dataset\ntrain.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T","6404b78a":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), test.isna().sum().sum()","c0f5eb98":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","7e14f7ae":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","a5246fbc":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","78153891":"# Combined dataframe containing numerical features only\ndf = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","3500fea0":"# Combined dataframe containing categorical features only\ndf = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"teal\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n\n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","d9177272":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"darkorange\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"steelblue\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","8aa0aedc":"# Checking if test data doesn't contain categories that are not present in the train dataset\nfor col in cat_features:\n    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","f8227d3f":"# Plot dataframe\ndf = train.drop(\"id\", axis=1)\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","bc6891ce":"columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n# plt.suptitle(\"Features vs target\", y=0.99)\nplt.show();","9a0ebf76":"# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    train[col] = encoder.fit_transform(np.array(train[col]).reshape(-1, 1))\n    test[col] = encoder.transform(np.array(test[col]).reshape(-1, 1))","eedf16fd":"train[cat_features].head()","d450a989":"test[cat_features].head()","585db126":"X = train.drop([\"id\", \"target\"], axis=1)\nX_test = test.drop([\"id\"], axis=1)\ny = train[\"target\"]","44889061":"# Model hyperparameters\nxgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.25,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': 4}","18755afd":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = KFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params, tree_method = 'gpu_hist')\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict(X_test) \/ splits\n    \n    # Getting mean feature importances (i.e. devided by number of splits)\n    model_fi += model.feature_importances_ \/ splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    oof_preds[valid_idx] = model.predict(X_valid)\n    \n    # Getting score for a fold model\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_rmse += fold_rmse \/ splits\n    \nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")","715eae08":"# Creating a dataframe to be used for plotting\ndf = pd.DataFrame()\ndf[\"Feature\"] = X.columns\n# Extracting feature importances from the trained model\ndf[\"Importance\"] = model_fi \/ model_fi.sum()\n# Sorting the dataframe by feature importance\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","e018149e":"fig, ax = plt.subplots(figsize=(13, 10))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n               color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","13ad8e34":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","f45e7e64":"As you can see, target column is very weakly correlated with all features.\n\nLet's visualize each feature vs target.","46648382":"# **Data preprocessing**","4eb9de33":"I recently learnt that it is against Kaggle\u2019s rules to publish public notebook forks with non-meaningful edits, since it is a form of plagiarism. Since I have no idea if making the model GPU compatible and tuning hyperparameters to increase model score is called \"non-meaningful edits\", I'll make it clear that this is a forked version. All credit (except those for the tuned hyperparameters and GPU compatibility) goes to the author of the original version.","0ff5c9e1":"# **Predictions submission**","8c8b646d":"There are no missing value in the both datasets.\n\nLet's check target distribution.","41429649":"The dataset contains categorical and numerical values. Let's see values distribution for these categories.","19849340":"# **Data import**","c48862a5":"# **Model training**","965f43e2":"# **Feature importances**","4a098147":"# **EDA**","b256d236":"Let's check if the datasets have different amount of categories in categorical features.","8d494d50":"So the datasets are pretty well balanced. Let's look at feature correlation."}}