{"cell_type":{"5003a376":"code","86b544b9":"code","09a8c7d8":"code","835e8f3e":"code","c2e3acd4":"code","40577202":"code","813887f0":"code","69f7b84a":"code","19f70e05":"code","ef44706e":"code","3a5ccf55":"code","ca400ed8":"code","97ebf3fb":"markdown","9481d149":"markdown","8ffd15a5":"markdown","6b34db88":"markdown","622a737a":"markdown","39fb27ba":"markdown","51807f32":"markdown","56375cdb":"markdown","2cb4224f":"markdown"},"source":{"5003a376":"!pip install pytorch_transformers pytreebank","86b544b9":"import os\nimport numpy as np\nimport pytreebank\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom pytorch_transformers import BertTokenizer,BertConfig, BertForSequenceClassification\nfrom tqdm import tqdm","09a8c7d8":"os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\nMODEL_OUT_DIR = '\/kaggle\/working'\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","835e8f3e":"def rpad(array, n=70):\n    \"\"\"Right padding.\"\"\"\n    current_len = len(array)\n    if current_len > n:\n        return array[: n - 1]\n    extra = n - current_len\n    return array + ([0] * extra)\n\ndef get_binary_label(label):\n    \"\"\"Convert fine-grained label to binary label.\"\"\"\n    if label < 2:\n        return 0\n    if label > 2:\n        return 1\n    raise ValueError(\"Invalid label\")","c2e3acd4":"class SSTDataset(Dataset):\n    ## Configurable SST Dataset.\n\n    def __init__(self, split=\"train\", root=True, binary=True):\n        \"\"\"Initializes the dataset with given configuration.\n\n        Args:\n            split: str\n                Dataset split, one of [train, val, test]\n            root: bool\n                If true, only use root nodes. Else, use all nodes.\n            binary: bool\n                If true, use binary labels. Else, use fine-grained.\n        \"\"\"\n        self.sst = sst[split]\n        \n        if root and binary:\n            self.data = [\n                (\n                    rpad(\n                        tokenizer.encode(\"[CLS] \" + tree.to_lines()[0] + \" [SEP]\"), n=66\n                    ),\n                    get_binary_label(tree.label),\n                )\n                for tree in self.sst\n                if tree.label != 2\n            ]\n        elif root and not binary:\n            self.data = [\n                (\n                    rpad(tokenizer.encode(\"[CLS] \" + tree.to_lines()[0] + \" [SEP]\"), n=66),\n                    tree.label,\n                )\n                for tree in self.sst\n            ]\n        elif not root and not binary:\n            self.data = [\n                (rpad(tokenizer.encode(\"[CLS] \" + line + \" [SEP]\"), n=66), label)\n                for tree in self.sst\n                for label, line in tree.to_labeled_lines()\n            ]\n        else:\n            self.data = [\n                (\n                    rpad(tokenizer.encode(\"[CLS] \" + line + \" [SEP]\"), n=66),\n                    get_binary_label(label),\n                )\n                for tree in self.sst\n                for label, line in tree.to_labeled_lines()\n                if label != 2\n            ]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        X, y = self.data[index]\n        X = torch.tensor(X)\n        return X, y\n","40577202":"def train_one_epoch(model, lossfn, optimizer, dataset, batch_size=32):\n    generator = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=True\n    )\n    model.train()\n    train_loss, train_acc = 0.0, 0.0\n    for batch, labels in tqdm(generator, position=0, leave=True):\n        batch, labels = batch.to(device), labels.to(device)\n        optimizer.zero_grad()\n        loss, logits = model(batch, labels=labels)\n        err = lossfn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        pred_labels = torch.argmax(logits, axis=1)\n        train_acc += (pred_labels == labels).sum().item()\n    train_loss \/= len(dataset)\n    train_acc \/= len(dataset)\n    return train_loss, train_acc","813887f0":"def evaluate_one_epoch(model, lossfn, optimizer, dataset, batch_size=32):\n    generator = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=True\n    )\n    model.eval()\n    loss, acc = 0.0, 0.0\n    with torch.no_grad():\n        for batch, labels in tqdm(generator, position=0, leave=True):\n            batch, labels = batch.to(device), labels.to(device)\n            logits = model(batch)[0]\n            error = lossfn(logits, labels)\n            loss += error.item()\n            pred_labels = torch.argmax(logits, axis=1)\n            acc += (pred_labels == labels).sum().item()\n    loss \/= len(dataset)\n    acc \/= len(dataset)\n    return loss, acc","69f7b84a":"def train(\n    root=True,\n    binary=False,\n    bert=\"bert-large-uncased\",\n    epochs=30,\n    batch_size=32,\n):\n    trainset = SSTDataset(\"train\", root=root, binary=binary)\n    devset = SSTDataset(\"dev\", root=root, binary=binary)\n    testset = SSTDataset(\"test\", root=root, binary=binary)\n    \n    best_val_loss = np.Inf\n    \n    config = BertConfig.from_pretrained(bert)\n    if not binary:\n        config.num_labels = 5\n    model = BertForSequenceClassification.from_pretrained(bert, config=config)\n\n    model = model.to(device)\n    lossfn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n    for epoch in range(1, epochs):\n        train_loss, train_acc = train_one_epoch(\n            model, lossfn, optimizer, trainset, batch_size=batch_size\n        )\n        val_loss, val_acc = evaluate_one_epoch(\n            model, lossfn, optimizer, devset, batch_size=batch_size\n        )\n        test_loss, test_acc = evaluate_one_epoch(\n            model, lossfn, optimizer, testset, batch_size=batch_size\n        )\n        print(f\"epoch={epoch}\")\n        print(\n            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, test_loss={test_loss:.4f}\"\n        )\n        print(\n            f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}, test_acc={test_acc:.3f}\"\n        )\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            label = \"binary\" if binary else \"fine\"\n            nodes = \"root\" if root else \"all\"\n#             torch.save(model, f\"{bert}__{nodes}__{label}.pickle\")\n            model.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            config.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            tokenizer.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n\n    print(\"Done!\")\n","19f70e05":"## Loading Tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n## loading SST Dataset (Stanford Tree Bank Dataset)\nsst = pytreebank.load_sst()\n\n## Configuration Values\nbinary = False \nroot = True\nsave = False\nbert_config = \"bert-large-uncased\"\ntrain(binary=binary, root=root, bert=bert_config)\n","ef44706e":"import torch.nn.functional as F\nbert=\"bert-large-uncased\"\nconfig = BertConfig.from_pretrained(bert)\ntokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\nconfig.num_labels = 5\nmodel = BertForSequenceClassification.from_pretrained(bert, config=config)\n","3a5ccf55":"def classify_sentiment(sentence):\n    model.eval()\n    with torch.no_grad():\n        data = torch.tensor(rpad(tokenizer.encode(\"[CLS] \" + sentence + \" [SEP]\"), n=66)).unsqueeze(dim=0)  # Sometimes Unsqueeze \n        logits = model(data)[0]\n        prob = F.softmax(logits, dim=1)\n        print(prob)\n        pred_label = torch.argmax(prob, axis=1)\n        print(pred_label)\n        ","ca400ed8":"sentence = \"great great love\"\nclassify_sentiment(sentence)","97ebf3fb":"Sentiment classification is an important process in understanding people's perception towards a product, service, or topic. Many natural language processing models have been proposed to solve the sentiment classification problem. However, most of them have focused on binary sentiment classification. In this paper, we use a promising deep learning model called BERT to solve the fine-grained sentiment classification task. Experiments show that our model outperforms other popular models for this task without sophisticated architecture. We also demonstrate the effectiveness of transfer learning in natural language processing in the process.\n\n\n@Source => https:\/\/arxiv.org\/abs\/1910.03474\n@Source => https:\/\/github.com\/munikarmanish\/bert-sentiment","9481d149":"## Utils","8ffd15a5":"## Sentiment Classification on 5 Categories-State of Art Approach using BERT LARGE on SST Fine grained Dataset.","6b34db88":"## Importing Libraries","622a737a":"## Training","39fb27ba":"## Validation Function","51807f32":"## Config","56375cdb":"## Inference","2cb4224f":"## Train Function"}}