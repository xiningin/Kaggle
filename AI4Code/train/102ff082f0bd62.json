{"cell_type":{"5cc27bc7":"code","f9d2becb":"code","d0fd139b":"code","4c7a86ee":"code","0d7c78c2":"code","721144c6":"code","1f0ffe40":"code","56070e67":"code","eb1116b3":"code","46648e5d":"code","43362cb2":"code","33c30832":"code","51320cca":"code","9243dfb6":"code","f0027339":"code","903b16aa":"code","11af8373":"code","0a4b2dd6":"code","6bddf945":"code","3c4b5aec":"code","c93cb89e":"code","ec330f73":"code","a84376ca":"code","75a2a354":"code","4f7ab1c4":"markdown","2e16b6b5":"markdown","0cc73c14":"markdown","5f5a3c69":"markdown","8324e9de":"markdown","308f3fb9":"markdown","710f3109":"markdown","b7961525":"markdown","56b5b3c5":"markdown","f20e3369":"markdown","7bba29cc":"markdown","6327001c":"markdown"},"source":{"5cc27bc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9d2becb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split","d0fd139b":"(x_train, y_train),  (x_test, y_test) = keras.datasets.cifar10.load_data()","4c7a86ee":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","0d7c78c2":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')","721144c6":"# normalize \nmean = np.mean(x_train)\nstd = np.std(x_train)\nx_train = (x_train-mean)\/(std+1e-7)\nx_test = (x_test-mean)\/(std+1e-7)","1f0ffe40":"x_test.shape","56070e67":"y_train = keras.utils.to_categorical(y_train, num_classes=10)\ny_test = keras.utils.to_categorical(y_test, num_classes = 10)","eb1116b3":"x_test.shape , y_test.shape","46648e5d":"model = keras.models.Sequential()\n\nmodel.add(keras.layers.Convolution2D(32, (3,3), input_shape = x_train.shape[1:], activation ='selu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.20))\n\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(512, activation = 'selu'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(10, activation ='softmax'))","43362cb2":"model.compile(loss = 'categorical_crossentropy',\n             optimizer = tf.keras.optimizers.RMSprop(),\n             metrics = ['accuracy'])","33c30832":"history = model.fit(x_train, y_train, epochs = 20, \n         validation_split=0.2)","51320cca":"pd.DataFrame(history.history).plot()","9243dfb6":"score = model.evaluate(x_test, y_test)","f0027339":"model = keras.models.Sequential()\nmodel.add(keras.layers.Convolution2D(32, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Convolution2D(32, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Convolution2D(64, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Convolution2D(64, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.25))\n\n\nmodel.add(keras.layers.Convolution2D(128, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Convolution2D(128, (3,3), activation = 'relu', input_shape = x_train.shape[1:], padding = 'same'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation = 'softmax'))\nmodel.summary()","903b16aa":"model.compile(loss = 'categorical_crossentropy',\n             optimizer = tf.keras.optimizers.RMSprop(),\n             metrics = ['accuracy'])","11af8373":"history = model.fit(x_train, y_train, epochs = 20, validation_split = 0.2)","0a4b2dd6":"pd.DataFrame(history.history).plot()","6bddf945":"model.evaluate(x_test, y_test)","3c4b5aec":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n rotation_range=30,\n width_shift_range=0.2,\n height_shift_range=0.2,\n horizontal_flip=True,\n )\ndatagen.fit(x_train)","c93cb89e":"batch_size = 64\nhistory = model.fit(datagen.flow(x_train, y_train, \n batch_size=batch_size),\n epochs=40,\n verbose=1,validation_data=(x_test,y_test))","ec330f73":"model_json = model.to_json()\n\nwith open(r'\/kaggle\/working\/model.json', 'w') as json_file :\n    json_file.write(model_json)\n\nmodel.save_weights(r'\/kaggle\/working\/model.h5')","a84376ca":"pd.DataFrame(history.history).plot()","75a2a354":"model.evaluate(x_test, y_test)","4f7ab1c4":"### With ImageDataGenerator Accuracy 84%","2e16b6b5":"## Second Model accuracy 82%","0cc73c14":"We also need to convert our label dataset to categorical structure, It is similar to the one hot encoding ","5f5a3c69":"But though our data is getting good accuracy on training it is not perorming well with test data, We got only 67% accuracy","8324e9de":"As you can see the datatype of our dataset is integer, So we will convert that to float","308f3fb9":"## Third Model","710f3109":"We will try to get more training data using IMAGEDATAFENERATOR","b7961525":"As we can seee from the figure,  \nabout Training,  Training accuracy increasing with descresed in training loss  \nabout Validation, Validation increasing but loss is fluctuacting for a while and the increased","56b5b3c5":"## Load Data","f20e3369":"## First, Simple Model","7bba29cc":"## Preprocess Data","6327001c":"* Let's understand our code line by line  \n     1) We have created sequential model\n     2) We are adding 32 convolution filter of size (3,3) having same input and output shape.  \n     3) Then we are adding maxpooling layer which will reduce the x and y dimention by 2.    \n     4) We will add dropout layer, Which will ensure that multiple neuron will learn how to identify part of image.  \n     5) We are flattening our image to one dimentional array by using flatten.  .  \n     6) We are using dense layer with 512 neuron, With activation function selu. Which is maximum of alpha(exp(z) -1 ) or z.    \n     7) We will again use dropout to drop 50% of neuron.  \n     8) Now we will use softmax to get probablities of each class.  \n    "}}