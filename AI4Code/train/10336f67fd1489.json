{"cell_type":{"8111df46":"code","b9cbf69a":"code","40d59a6e":"markdown"},"source":{"8111df46":"!pip install xgboost==1.5.0","b9cbf69a":"import csv\nfrom typing import List\nfrom tqdm import tqdm\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\n\ncache = \"\/kaggle\/working\/hf_model\"\n\nclass TransformerExtractor:\n    def __init__(self, name=\"distilbert-base-uncased\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(name, cache_dir=cache)\n        self.model = AutoModel.from_pretrained(name, cache_dir=cache)\n\n    def extract(self, texts: List[str]) -> np.array:\n        feats = np.zeros((len(texts), 768), dtype=np.float16)\n        for itt, text in enumerate(tqdm(texts)):\n            tokenized_text = self.tokenizer(text, return_tensors=\"pt\")\n            model_output = self.model(**tokenized_text)[0].detach().cpu()\n            feats[itt, :] = model_output.numpy().mean(axis=1)\n        return feats\n\n\ndef read_csvfile_to_rows(filename: str) -> List[List[str]]:\n    with open(filename) as f:\n        csvreader = csv.reader(f, delimiter=\",\", quotechar='\"')\n        return [row for row in csvreader]\n\n\ndef write_results_file(ids, preds, filename) -> None:\n    with open(filename, \"w\") as to_file:\n        csvwriter = csv.writer(to_file, delimiter=\",\", quotechar='\"')\n        csvwriter.writerow([\"PassengerId\", \"Survived\"])\n        for id, prediction in zip(ids, preds):\n            csvwriter.writerow([id, prediction])\n\n\ndef titanic_using_transformers():\n    extractor = TransformerExtractor()\n    scaler = StandardScaler()\n    classifier = XGBClassifier(use_label_encoder=False)\n\n    rows = read_csvfile_to_rows(\"\/kaggle\/input\/titanic\/train.csv\")\n    train_labels = [int(row[1]) for row in rows[1:]]\n    texts = [\", \".join(row[2:]) for row in rows[1:]]\n    train_features = scaler.fit_transform(extractor.extract(texts))\n    classifier.fit(train_features, train_labels)\n\n    rows = read_csvfile_to_rows(\"\/kaggle\/input\/titanic\/test.csv\")\n    ids = [row[0] for row in rows[1:]]\n    texts = [\", \".join(row[1:]) for row in rows[1:]]\n    preds = classifier.predict(scaler.transform(extractor.extract(texts)))\n    write_results_file(ids, preds, \"submission.csv\")\n\n\ntitanic_using_transformers()","40d59a6e":"# Using Transformers on the Titanic dataset\n\nIn this notebook we show how to use transformers to model the titanic dataset. A blog-post with some more details is provided here:\nhttps:\/\/www.nyckel.com\/blog\/titanic-vs-transformers\/"}}