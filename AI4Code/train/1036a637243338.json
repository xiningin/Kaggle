{"cell_type":{"3b886898":"code","f01c1724":"code","7b32d24a":"code","7798bf61":"code","3ebee1fe":"code","f30d2403":"code","2f9e13a3":"code","ce1b514a":"code","1c8fe5f0":"code","1e661f46":"code","5ae9f89b":"code","08bd5f78":"code","398b646a":"code","7f711102":"code","536cff33":"code","098dcc53":"code","c18b0191":"code","11ea35bc":"code","301cc48e":"code","4940c8d2":"code","f19efe57":"code","d3248b2d":"code","d8ac283a":"code","30783687":"code","6d1c81e4":"code","8eeea9a9":"code","74ad24eb":"code","0b961a93":"code","7a8bd1e7":"code","060b43fb":"code","fe655a0e":"code","95198bfc":"code","6c55a368":"code","6d3dd479":"code","c87f56af":"code","ca4f7fc0":"code","c17b0a5a":"code","c0ebabc9":"code","47b3c0f4":"code","9406d7bf":"code","daafe2fa":"code","799b46eb":"code","150acfe0":"code","2abae06a":"code","7cbf100a":"code","c9dd3e7f":"code","3ec4a402":"code","2f130d45":"code","814c331e":"code","8b126e16":"code","1006e8c0":"code","87b50916":"code","d3773e28":"code","c6547797":"code","c74fb65e":"code","59a76828":"code","abcbd0a5":"code","34db45b0":"code","37a47028":"code","36d1d20a":"code","8e8172d5":"code","86d5f3c1":"code","60d42e46":"code","ae1e1e92":"code","e0b5d02c":"code","57635844":"markdown","734100f9":"markdown","18ae823f":"markdown","b404cfb9":"markdown","6ff7d5ef":"markdown","e5ff0cbd":"markdown","d5a3ef41":"markdown","5ae10ef8":"markdown","d3a67a49":"markdown","9f5dbf06":"markdown","9ca87cf0":"markdown","20a031c1":"markdown","74f848a6":"markdown","64371100":"markdown","65f730c6":"markdown","1ae304b2":"markdown","97e48a4b":"markdown","46916bcd":"markdown","39f7e5ea":"markdown","b7f04af1":"markdown","11debb36":"markdown"},"source":{"3b886898":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f01c1724":"import warnings\nwarnings.simplefilter(action = \"ignore\")","7b32d24a":"#Reading data set and the first 5 observation\ndata=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","7798bf61":"#Data set consists of 768 observation units and 9 variables.\ndata.shape","3ebee1fe":"#Descriptive statistics of the data set accessed.\ndata.describe([0.10,0.25,0.50,0.75,0.90,0.99]).T","f30d2403":"#The distribution of the Outcome variable\ndata[\"Outcome\"].value_counts()*100\/len(data)","2f9e13a3":"#The classes of the Outcome variable\nsns.countplot(x = 'Outcome', data = data);\ndata.Outcome.value_counts()","ce1b514a":"#The histogram of the Age variable\ndata[\"Age\"].hist(edgecolor = \"red\");","1c8fe5f0":"#Pregnancy and age averages by Outcome classes\nprint(data.groupby(\"Outcome\").agg({\"Pregnancies\":\"mean\"}))\nprint(data.groupby(\"Outcome\").agg({\"Age\":\"mean\"}))\nprint(data.groupby(\"Outcome\").agg({\"Insulin\": \"mean\"}))\nprint(data.groupby(\"Outcome\").agg({\"Glucose\": \"mean\"}))\nprint(data.groupby(\"Outcome\").agg({\"BMI\": \"mean\"}))","1e661f46":"print(\"Maximum Age: \" + str(data[\"Age\"].max()))\nprint(\"Minimum Age: \" + str(data[\"Age\"].min()))","5ae9f89b":"#Histogram and density graphs of all variables\nfig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.BMI, bins = 20, ax=ax[3,1])","08bd5f78":"my_colors = ['lightblue','lightsteelblue','silver']\nax = data[\"Outcome\"]\nax.value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True,colors=my_colors)\nplt.show()","398b646a":"#Access to the correlation of the data set was provided. What kind of relationship is examined between the variables. \n#If the correlation value is> 0, there is a positive correlation. While the value of one variable increases, the value of the other variable also increases.\n#Correlation = 0 means no correlation.\n#If the correlation is <0, there is a negative correlation. While one variable increases, the other variable decreases. \n#When the correlations are examined, there are 2 variables that act as a positive correlation to the Salary dependent variable.\n#These variables are Glucose. As these increase, Outcome variable increases.\nf, ax = plt.subplots(figsize= [20,15])\nsns.heatmap(data.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","7f711102":"#We fill cells with a value of 0 as NAN\ndata[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","536cff33":"#Now, we can look at where are missing values\ndata.isnull().sum()","098dcc53":"#Have been visualized using the missingno library for the visualization of missing observations\nimport missingno as msno\nmsno.bar(data);","c18b0191":"#The missing values will be filled with the median values of each variable\ndef median_target(variable):   \n    temp = data[data[variable].notnull()]\n    temp = temp[[variable, 'Outcome']].groupby(['Outcome'])[[variable]].median().reset_index()\n    return temp","11ea35bc":"#The values to be given for incomplete observations are given the median value of people who are not sick and the median values of people who are sick\ncolumns = data.columns\ncolumns = columns.drop(\"Outcome\")\nfor i in columns:\n    median_target(i)\n    data.loc[(data['Outcome'] == 0 ) & (data[i].isnull()), i] = median_target(i)[i][0]\n    data.loc[(data['Outcome'] == 1 ) & (data[i].isnull()), i] = median_target(i)[i][1]","301cc48e":"#Missing values were filled\ndata.isnull().sum()","4940c8d2":"#In the data set, there were asked whether there were any outlier observations compared to the 15% and 85% quarters.\n\nfor feature in data:\n    Q1 = data[feature].quantile(0.15)\n    Q3 = data[feature].quantile(0.85)\n    IQR = Q3-Q1\n    lower = Q1- 1.5*IQR\n    upper = Q3 + 1.5*IQR\n    \n    if data[(data[feature] > upper) | (data[feature] < lower)].any(axis=None):\n        print(feature,\"\u2192 YES\")\n        print(data[(data[feature] > upper) | (data[feature] < lower)].shape[0])\n    else:\n        print(feature,\"\u2192 NO\")","f19efe57":"#The process of visualizing the Insulin variable with boxplot method was done. We find the outlier observations on the chart.\nsns.boxplot(x = data[\"Insulin\"]);","d3248b2d":"#We suppress contradictory values\nQ1 = data.Insulin.quantile(0.20)\nQ3 = data.Insulin.quantile(0.80)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndata.loc[data[\"Insulin\"] > upper,\"Insulin\"] = upper\n\nsns.boxplot(x = data[\"Insulin\"]);","d8ac283a":"Q1 = data.DiabetesPedigreeFunction.quantile(0.25)\nQ3 = data.DiabetesPedigreeFunction.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndata.loc[data[\"DiabetesPedigreeFunction\"] > upper,\"DiabetesPedigreeFunction\"] = upper\n\nsns.boxplot(x = data[\"DiabetesPedigreeFunction\"]);","30783687":"#We determine outliers between all variables with the LOF method\nfrom sklearn.neighbors import LocalOutlierFactor\nlof=LocalOutlierFactor(n_neighbors= 10)\nlof.fit_predict(data)","6d1c81e4":"data_scores = lof.negative_outlier_factor_\nnp.sort(data_scores)[0:30]","8eeea9a9":"#We choose the threshold value according to lof scores\nthreshold = np.sort(data_scores)[5]\nthreshold","74ad24eb":"#We delete those that are higher than the threshold\noutlier = data_scores > threshold\ndata = data[outlier]\ndata.shape","0b961a93":"#According to BMI, some ranges were determined and categorical variables were assigned.\nNew_BMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\ndata[\"New_BMI\"] = New_BMI\ndata.loc[data[\"BMI\"] < 18.5, \"New_BMI\"] = New_BMI[0]\ndata.loc[(data[\"BMI\"] > 18.5) & (data[\"BMI\"] <= 24.9), \"New_BMI\"] = New_BMI[1]\ndata.loc[(data[\"BMI\"] > 24.9) & (data[\"BMI\"] <= 29.9), \"New_BMI\"] = New_BMI[2]\ndata.loc[(data[\"BMI\"] > 29.9) & (data[\"BMI\"] <= 34.9), \"New_BMI\"] = New_BMI[3]\ndata.loc[(data[\"BMI\"] > 34.9) & (data[\"BMI\"] <= 39.9), \"New_BMI\"] = New_BMI[4]\ndata.loc[data[\"BMI\"] > 39.9 ,\"New_BMI\"] = New_BMI[5]","7a8bd1e7":"data.head()","060b43fb":"#A categorical variable creation process is performed according to the insulin value.\ndef set_insulin(row): \n    if row[\"Insulin\"] >= 100 and row[\"Insulin\"] <= 126:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","fe655a0e":"#The operation performed was added to the dataframe.\ndata = data.assign(New_Insulin_Score=data.apply(set_insulin, axis=1))\ndata.head()","95198bfc":"#Some intervals were determined according to the glucose variable and these were assigned categorical variables.\nNew_Glucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\ndata[\"New_Glucose\"] = New_Glucose\ndata.loc[data[\"Glucose\"] <= 70, \"New_Glucose\"] = New_Glucose[0]\ndata.loc[(data[\"Glucose\"] > 70) & (data[\"Glucose\"] <= 99), \"New_Glucose\"] = New_Glucose[1]\ndata.loc[(data[\"Glucose\"] > 99) & (data[\"Glucose\"] <= 126), \"New_Glucose\"] = New_Glucose[2]\ndata.loc[data[\"Glucose\"] > 126 ,\"New_Glucose\"] = New_Glucose[3]\ndata.head()","6c55a368":"#Let's look at the breaking of newly created variables.\ndata.groupby([\"New_BMI\",\"New_Insulin_Score\", \"New_Glucose\"]).agg({\"Outcome\": \"count\"})","6d3dd479":"#The new \"New_diabetes\" variable defines whether the probability of having diabetes is high or normal or low.\ndata.loc[data[\"New_BMI\"] == \"Underweight\", \"New_Diabet\"] = \"Dusuk\"\ndata.head()","c87f56af":"data[data[\"New_BMI\"] == \"Underweight\"]\ndata.loc[data[\"New_Glucose\"] == \"Low\", \"New_Diabet\"] = \"Dusuk\"\ndata[data[\"New_Glucose\"] == \"Low\"]\ndata.loc[data[\"New_Glucose\"] == \"High\", \"New_Diabet\"] = \"Yuksek\"\ndata[data[\"New_Glucose\"] == \"High\"]\ndata.loc[data[\"New_Glucose\"] == \"Secret\", \"New_Diabet\"] = \"Yuksek\"\ndata[data[\"New_Glucose\"] == \"Secret\"]\ndata.loc[(data[\"New_BMI\"] == \"Obesity 3\") & (data[\"New_Insulin_Score\"] == \"Normal\") & (data[\"New_Glucose\"] == \"Secret\"), \"New_Diabet\"] = \"Dusuk\"\ndata[(data[\"New_BMI\"] == \"Obesity 3\") & (data[\"New_Insulin_Score\"] == \"Normal\") & (data[\"New_Glucose\"] == \"Secret\")]\ndata[\"New_Diabet\"].fillna(\"Normal\", inplace = True)\ndata[\"New_Diabet\"].isnull().sum()","ca4f7fc0":"data.head()","c17b0a5a":"#Here, by making One Hot Encoding transformation, categorical variables were converted into numerical values. It is also protected from the Dummy variable trap.\ndata = pd.get_dummies(data, columns =[\"New_BMI\",\"New_Insulin_Score\", \"New_Glucose\", \"New_Diabet\"], drop_first = True)\ndata.head()","c0ebabc9":"categorical_data = data[['New_BMI_Obesity 1','New_BMI_Obesity 2', 'New_BMI_Obesity 3', 'New_BMI_Overweight','New_BMI_Underweight',\n                     'New_Insulin_Score_Normal','New_Glucose_Low','New_Glucose_Normal', 'New_Glucose_Overweight', 'New_Glucose_Secret','New_Diabet_Normal','New_Diabet_Yuksek']]\ncategorical_data.head()","47b3c0f4":"y = data[\"Outcome\"]\nX = data.drop([\"Outcome\",'New_BMI_Obesity 1','New_BMI_Obesity 2', 'New_BMI_Obesity 3', 'New_BMI_Overweight','New_BMI_Underweight',\n                     'New_Insulin_Score_Normal','New_Glucose_Low','New_Glucose_Normal', 'New_Glucose_Overweight', 'New_Glucose_Secret','New_Diabet_Normal','New_Diabet_Yuksek'], axis = 1)\ncols = X.columns\nindex = X.index","9406d7bf":"X.head()","daafe2fa":"#The variables in the data set are an effective factor in increasing the performance of the models by standardization.  \n#There are multiple standardization methods. These are methods such as\" Normalize\",\" MinMax\",\" Robust\" and \"Scale\".\nfrom sklearn import preprocessing\nnormalizer = preprocessing.Normalizer().fit(X)\nX = normalizer.transform(X) \nX = pd.DataFrame(X, columns = cols, index = index)","799b46eb":"X = pd.concat([X,categorical_data], axis = 1)\nX.head()","150acfe0":"y.head()","2abae06a":"#Validation scores of all base models\n\nmodels = []\nmodels.append(('LR', LogisticRegression(random_state = 123456)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier(random_state = 123456)))\nmodels.append(('RF', RandomForestClassifier(random_state = 123456)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 123456)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 123456)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 123456)))\n\n#Evaluate each model in turn\nresults = []\nnames = []","7cbf100a":"for name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 1234)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c9dd3e7f":"rf_params = {\"n_estimators\" :[100,200,500,1000], \n             \"max_features\": [3,5,7], \n             \"min_samples_split\": [2,5,10,30],\n            \"max_depth\": [3,5,8,None]}","3ec4a402":"rf_model = RandomForestClassifier(random_state = 123456)","2f130d45":"gs_cv = GridSearchCV(rf_model, \n                    rf_params,\n                    cv = 15,\n                    n_jobs = -1,\n                    verbose = 2).fit(X, y)","814c331e":"gs_cv.best_params_","8b126e16":"rf_tuned = RandomForestClassifier(**gs_cv.best_params_)\nrf_tuned = rf_tuned.fit(X,y)\ncross_val_score(rf_tuned, X, y, cv = 15).mean()","1006e8c0":"feature_imp = pd.Series(rf_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","87b50916":"lgbm = LGBMClassifier(random_state = 123456)","d3773e28":"lgbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.5],\n              \"n_estimators\": [500, 1000, 1500],\n              \"max_depth\":[3,5,8]}","c6547797":"gs_cv = GridSearchCV(lgbm, \n                     lgbm_params, \n                     cv = 15, \n                     n_jobs = -1, \n                     verbose = 2).fit(X, y)","c74fb65e":"gs_cv.best_params_","59a76828":"lgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X,y)\ncross_val_score(lgbm_tuned, X, y, cv = 15).mean()","abcbd0a5":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","34db45b0":"xgb = GradientBoostingClassifier(random_state = 123456)","37a47028":"xgb_params = {\n    \"learning_rate\": [0.01, 0.1, 0.2, 1],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n    \"max_depth\":[3,5,8],\n    \"subsample\":[0.5, 0.9, 1.0],\n    \"n_estimators\": [100,1000]}","36d1d20a":"xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 15, n_jobs = -1, verbose = 2).fit(X, y)","8e8172d5":"xgb_cv_model.best_params_","86d5f3c1":"xgb_tuned = GradientBoostingClassifier(**xgb_cv_model.best_params_).fit(X,y)\ncross_val_score(xgb_tuned, X, y, cv = 15).mean()","60d42e46":"feature_imp = pd.Series(xgb_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Significance Score Of Variables')\nplt.ylabel('Variables')\nplt.title(\"Variable Severity Levels\")\nplt.show()","ae1e1e92":"models = []\n\nmodels.append(('RF', RandomForestClassifier(random_state = 123456, max_depth = 8, max_features = 3, min_samples_split = 2, n_estimators = 500)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 123456, learning_rate = 0.01,  max_depth = 5, n_estimators = 1500)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 123456, learning_rate = 1, max_depth = 5, min_samples_split = 0.2777777777777778, n_estimators = 1000, subsample = 0.9)))\n\n#Evaluate each model in turn\nresults = []\nnames = []","e0b5d02c":"for name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 123456)\n        cv_results = cross_val_score(model, X, y, cv = 15, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","57635844":"### **Final Model Installation**","734100f9":"### **XGBoost Tuning**","18ae823f":"### **II.II) Outlier Observation Analysis**","b404cfb9":"## **VII)Comparison of Final Models**","6ff7d5ef":"## **III) Feature Engineering**","e5ff0cbd":"### **II.III) Local Outlier Factor(LOF)**","d5a3ef41":"The aim of this study was to create classification models for the diabetes data set and to predict whether a person is sick by establishing models and to obtain maximum validation scores in the established models. The work done is as follows:\n\n1) Diabetes Data Set read.\n\n2) With Exploratory Data Analysis; The data set's structural data were checked. The types of variables in the dataset were examined. Size information of the dataset was accessed. The 0 values in the data set are missing values. Primarily these 0 values were replaced with NaN values. Descriptive statistics of the data set were examined.\n\n3) Data Preprocessing section; df for: The NaN values missing observations were filled with the median values of whether each variable was sick or not. The outliers were determined by LOF and dropped. The X variables were standardized with the rubost method..\n\n4) During Model Building; Logistic Regression, KNN, SVM, CART, Random Forests, XGBoost, LightGBM like using machine learning models Cross Validation Score were calculated. Later Random Forests, XGBoost, LightGBM hyperparameter optimizations optimized to increase Cross Validation value.\n\n5) Result; The model created as a result of Random Forest hyperparameter optimization became the model with the lowest Cross Validation Score value. (0.88)","5ae10ef8":"### **LightGBM Tuning**","d3a67a49":"### **Final Model Installation**","9f5dbf06":"### **Random Forests Tuning**","9ca87cf0":"## **II) Data Preprocessing**","20a031c1":"### **II.I) Missing Observation Analysis**","74f848a6":"# **Diabetes Classification using Machine Learning**\n\nDiabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period. Symptoms of high blood sugar include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, and damage to the eyes.\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n## **Objective**\n\nWe will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n**Details about the dataset:**\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n- *Pregnancies:* Number of times pregnant\n- *Glucose:* Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- *BloodPressure:* Diastolic blood pressure (mm Hg)\n- *SkinThickness:* Triceps skin fold thickness (mm)\n- *Insulin:* 2-Hour serum insulin (mu U\/ml)\n- *BMI:* Body mass index (weight in kg\/(height in m)^2)\n- *DiabetesPedigreeFunction:* Diabetes pedigree function\n- *Age:* Age (years)\n- *Outcome:* Class variable (0 or 1)\n\n**Number of Observation Units:** 768\n\n**Variable Number:** 9","64371100":"## **V)Base Models**","65f730c6":"## **I) Data Analysis**","1ae304b2":"## **VIII)Reporting**","97e48a4b":"## **VI)Model Tuning**","46916bcd":"Creating new variables is important for models. But you need to create a logical new variable. For this data set, some new variables were created according to BMI, Insulin and glucose variables.","39f7e5ea":"## **IV)One Hot Encoding**","b7f04af1":"### **Final Model Installation**","11debb36":"Categorical variables in the data set should be converted into numerical values. For this reason, these transformation processes are performed with Label Encoding and One Hot Encoding method."}}