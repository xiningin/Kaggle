{"cell_type":{"deb8b29b":"code","d780c87c":"code","5ec2a462":"code","500cefdd":"code","0a4631f8":"code","499f0940":"code","f1c65cf8":"code","e89e7508":"code","c43a7740":"code","041403e4":"code","a8e89535":"code","de9329c9":"code","1ac183dd":"code","92c7bc3e":"code","0f852f67":"code","9d1478c3":"code","d1f8f0d7":"code","dec07858":"code","975834a6":"code","b41db97f":"code","594222d9":"code","5b08bfac":"code","1fa5d6b5":"code","32715be9":"code","8c2ae91e":"code","5ff93908":"markdown","0df11dc7":"markdown","15c64741":"markdown","44027d78":"markdown","8125714e":"markdown","96db5b90":"markdown","40167078":"markdown","49dac482":"markdown"},"source":{"deb8b29b":"!pip install torchsummary\n!pip install mlencoders","d780c87c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport tqdm\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom mlencoders.target_encoder import TargetEncoder\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchsummary import summary\n\nwarnings.filterwarnings(\"ignore\")","5ec2a462":"raw_data = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv', sep=',', verbose=True)","500cefdd":"raw_data.head(2)","0a4631f8":"def weird_division(n, d):\n    if d:\n        return n \/ d\n    else:\n        return 0\n\nraw_data['CPM'] = raw_data.apply(lambda x: weird_division(((x['total_revenue'] * 100)), x['measurable_impressions']) * 1000, axis=1)","499f0940":"def features_for_drop(df, cols):\n    n = []\n    f = []\n    for col in cols:\n        if len(df[col].unique()) < 2: \n            n.append(len(df[col].unique()))\n            f.append(col)\n    return dict(zip(f, n))\n\nfeatures_for_drop(raw_data, raw_data.columns)","f1c65cf8":"clean_data = raw_data.drop(['revenue_share_percent', 'integration_type_id', 'total_revenue', 'measurable_impressions'], axis=1)","e89e7508":"X_train = clean_data[clean_data['date'].between('2019-06-01 00:00:00', '2019-06-21 00:00:00')]\nX_test = clean_data[clean_data['date'].between('2019-06-22 00:00:00', '2019-06-30 00:00:00')]\n\nX_train_95_per = X_train['CPM'].quantile(0.95)\nX_test_95_per = X_test['CPM'].quantile(0.95)\n\nX_test = X_test[(X_test['CPM'] >= 0) & (X_test['CPM'] < X_test_95_per)]\n\nX_valid = X_train.sample(frac=0.1, random_state=1234, replace=False)\nX_train = X_train.loc[list(set(X_train.index) - set(X_valid.index)), :]\n\nX_train = X_train[X_train['CPM'] < X_train_95_per]\nX_valid = X_valid[X_valid['CPM'] < X_train_95_per]","c43a7740":"enc = TargetEncoder(cols=['site_id',\n                          'ad_type_id',\n                          'geo_id',\n                          'device_category_id',\n                          'advertiser_id',\n                          'order_id',\n                          'line_item_type_id',\n                          'os_id',\n                          'monetization_channel_id',\n                          'ad_unit_id',\n                          'total_impressions',\n                          'viewable_impressions',\n                         ],\n                   )","041403e4":"X_train_enc = enc.fit_transform(X_train, X_train['CPM'])\nX_valid_enc = enc.transform(X_valid)\nX_test_enc = enc.transform(X_test)","a8e89535":"X_train_enc.head(2)","de9329c9":"y_train = X_train_enc['CPM'].values\nX_train = X_train_enc.drop(['CPM', 'date'], axis=1).values\n\ny_val = X_valid_enc['CPM'].values\nX_val = X_valid_enc.drop(['CPM', 'date'], axis=1).values\n\ny_test = X_test_enc['CPM'].values\nX_test = X_test_enc.drop(['CPM', 'date'], axis=1).values","1ac183dd":"scaler = MinMaxScaler()\n\nX_train_sc = scaler.fit_transform(X_train)\nX_val_sc = scaler.transform(X_val)\nX_test_sc = scaler.transform(X_test)","92c7bc3e":"device = torch.device('cpu')\ndevice","0f852f67":"class RegressionDataset(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)\n\n    \ntrain_dataset = RegressionDataset(torch.from_numpy(X_train_sc).float(), torch.from_numpy(y_train).float())\nval_dataset = RegressionDataset(torch.from_numpy(X_val_sc).float(), torch.from_numpy(y_val).float())\ntest_dataset = RegressionDataset(torch.from_numpy(X_test_sc).float(), torch.from_numpy(y_test).float())","9d1478c3":"EPOCHS = 30\nBATCH_SIZE = 1024\nLEARNING_RATE = 0.001\n\nNUM_FEATURES = X_train_sc.shape[1]","d1f8f0d7":"train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1)","dec07858":"class MultipleRegression(nn.Module):\n    def __init__(self, num_features):\n        super(MultipleRegression, self).__init__()\n        \n        self.layer_1 = nn.Linear(num_features, 30)\n        self.bn1 = nn.BatchNorm1d(num_features=30)\n        self.layer_2 = nn.Linear(30, 15)\n        self.bn2 = nn.BatchNorm1d(num_features=15)\n        self.layer_3 = nn.Linear(15, 10)\n        self.bn3 = nn.BatchNorm1d(num_features=10)\n        self.layer_out = nn.Linear(10, 1)       \n        self.relu = nn.ReLU()\n    \n    def forward(self, inputs):\n        x = self.relu(self.bn1(self.layer_1(inputs)))\n        x = self.relu(self.bn2(self.layer_2(x)))\n        x = self.relu(self.bn3(self.layer_3(x)))\n        x = self.layer_out(x)\n        return x","975834a6":"model = MultipleRegression(NUM_FEATURES)\nmodel.to(device)","b41db97f":"criterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)","594222d9":"summary(model=model, input_size=(NUM_FEATURES, ), device='cpu')","5b08bfac":"loss_stats = {'train': [], \"val\": [], }","1fa5d6b5":"print(\"Start training\")\n\nfor e in tqdm.tqdm_notebook(range(1, EPOCHS+1)):\n    \n    # TRAINING\n    train_epoch_loss = 0\n    model.train()\n    for X_train_batch, y_train_batch in train_loader:\n        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n        optimizer.zero_grad()\n        y_train_pred = model(X_train_batch)\n        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n        train_loss.backward()\n        optimizer.step()\n        train_epoch_loss += train_loss.item()\n              \n    # VALIDATION    \n    with torch.no_grad():\n        val_epoch_loss = 0\n        model.eval()\n        for X_val_batch, y_val_batch in val_loader:\n            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n            y_val_pred = model(X_val_batch)            \n            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n            val_epoch_loss += val_loss.item()\n            \n    loss_stats['train'].append(train_epoch_loss \/ len(train_loader))\n    loss_stats['val'].append(val_epoch_loss \/ len(val_loader))                              \n    \n    print(f'Epoch {e+0:03}: | Train MSE: {train_epoch_loss \/ len(train_loader):.5f} | Val MSE: {val_epoch_loss \/ len(val_loader):.5f}')\n    \nprint(\"Finish training\")","32715be9":"y_pred_list = []\n\nwith torch.no_grad():\n    model.eval()\n    for X_batch, _ in test_loader:\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch)\n        y_pred_list.append(y_test_pred.cpu().numpy())\n        \ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]","8c2ae91e":"print('MSE on test data = {}'.format(mean_squared_error(y_test, y_pred_list)))","5ff93908":"# \u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 ","0df11dc7":"# \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043d\u0443\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","15c64741":"# \u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","44027d78":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435","8125714e":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (\u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u0430\u044f \u0441\u0435\u0442\u044c)","96db5b90":"# \u0414\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e","40167078":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","49dac482":"# \u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f "}}