{"cell_type":{"e20be4bd":"code","c9230947":"code","1ad37539":"code","331cff3b":"code","061300c9":"code","090bf716":"code","61104781":"code","f9bcd892":"code","f5802b8d":"code","ad16b88a":"code","fb9b4c5c":"markdown","2504c9d4":"markdown","4df4c769":"markdown","5fbbcee6":"markdown","43459fd0":"markdown","43a63624":"markdown","465621c2":"markdown","9060f23f":"markdown"},"source":{"e20be4bd":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_regression\n\nfrom sklearn.metrics import r2_score","c9230947":"class Regression:\n    def __init__(self, learning_rate, iteration, regularization):\n        \"\"\"\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.m = None\n        self.n = None\n        self.w = None\n        self.b = None\n        self.regularization = regularization # will be the l1\/l2 regularization class according to the regression model.\n        self.lr = learning_rate\n        self.it = iteration\n\n    def cost_function(self, y, y_pred):\n        \"\"\"\n        :param y: Original target value.\n        :param y_pred: predicted target value.\n        \"\"\"\n        return (1 \/ (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n    \n    def hypothesis(self, weights, bias, X):\n        \"\"\"\n        :param weights: parameter value weight.\n        :param X: Training samples.\n        \"\"\"\n        return np.dot(X, weights) \n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        X = np.insert(X, 0, 1, axis=1)\n\n        # Target value should be in the shape of (n, 1) not (n, ).\n        # So, this will check that and change the shape to (n, 1), if not.\n        try:\n            y.shape[1]\n        except IndexError as e:\n            # we need to change it to the 1 D array, not a list.\n            print(\"ERROR: Target array should be a one dimentional array not a list\"\n                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n            return \n        \n        # m is the number of training samples.\n        self.m = X.shape[0]\n        # n is the number of features.\n        self.n = X.shape[1]\n\n        # Set the initial weight.\n        self.w = np.zeros((self.n , 1))\n\n        # bias.\n        self.b = 0\n\n        for it in range(1, self.it+1):\n            # 1. Find the predicted value through the hypothesis.\n            # 2. Find the Cost function value.\n            # 3. Find the derivation of weights.\n            # 4. Apply Gradient Decent.\n            y_pred = self.hypothesis(self.w, self.b, X)\n            #print(\"iteration\",it)\n            #print(\"y predict value\",y_pred)\n            cost = self.cost_function(y, y_pred)\n            #print(\"Cost function\",cost)\n            # fin the derivative.\n            dw = (1\/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n            #print(\"weights derivation\",dw)\n            #db = -(2 \/ self.m) * np.sum((y_pred - y))\n\n            # change the weight parameter.\n            self.w = self.w - self.lr * dw\n            #print(\"updated weights\",self.w)\n            #self.b = self.b - self.lr * db\n\n\n            if it % 10 == 0:\n                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n    def predict(self, test_X):\n        \"\"\"\n        :param test_X: feature values to predict.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        test_X = np.insert(test_X, 0, 1, axis=1)\n\n        y_pred = self.hypothesis(self.w, self.b, test_X)\n        return y_pred\n    ","1ad37539":"class l1_l2_regularization:\n    def __init__(self, lamda = 0.1, l_ratio = 0.5):\n        self.lamda = lamda \n        self.l_ratio = l_ratio\n\n    def __call__(self, weights):\n        l1_contribution = self.l_ratio * self.lamda * np.sum(np.abs(weights))\n        l2_contribution = (1 - self.l_ratio) * self.lamda * 0.5 * np.sum(np.square(weights))\n        return (l1_contribution + l2_contribution)\n\n    def derivation(self, weights):\n        l1_derivation = self.lamda * self.l_ratio * np.sign(weights)\n        l2_derivation = self.lamda * (1 - self.l_ratio) * weights\n        return (l1_derivation + l2_derivation)","331cff3b":"# Define the traning data.\nX, y = make_regression(n_samples=50000, n_features=8)\n\n# Chnage the shape of the target to 1 dimentional array.\ny = y[:, np.newaxis]\n\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\nprint(\"Shape of the target value ----------> {}\".format(y.shape))","061300c9":"# display the data.\ndata = pd.DataFrame(X)\ndata.head()","090bf716":"# display the data.\ndata_y = pd.DataFrame(y)\ndata_y.head()","61104781":"class ElasticNetRegression(Regression):\n    \"\"\"\n    Elastic Regression class\n    \"\"\"\n    def __init__(self, lamda, l_ratio, learning_rate, iteration):\n        \"\"\"\n        Define the hyperparameters we are going to use in this model.\n        :param lamda: Regularization factor.\n        :param l_ratio: The ratio between lasso and ridge regression--> default is 0.5.\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.regularization = l1_l2_regularization(lamda,l_ratio)\n        super(ElasticNetRegression, self).__init__(learning_rate, iteration, self.regularization)\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        return super(ElasticNetRegression, self).train(X, y)\n    def predict(self, test_X):\n        \"\"\"\n        parma test_X: Value need to be predicted.\n        \"\"\"\n        return super(ElasticNetRegression, self).predict(test_X)","f9bcd892":"#define the parameters\nparam = {\n    \"l_ratio\" : 0.5,\n    \"lamda\" : 0.1,\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 100\n}\nprint(\"=\"*100)\nelastic_net_reg = ElasticNetRegression(**param)\n\n# Train the model.\nelastic_net_reg.train(X, y) \n\n# Predict the values.\ny_pred = elastic_net_reg.predict(X)\n\n#Root mean square error.\nscore = r2_score(y, y_pred)\nprint(\"The r2_score of the trained model\", score)","f5802b8d":"from sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import r2_score\n\n# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\n","ad16b88a":"elastic_net_sklearn = ElasticNet()\nelastic_net_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = elastic_net_sklearn.predict(X)\nscore = r2_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"R2 score of the model is {}\".format(score))","fb9b4c5c":"# Conclution\nOur model perofrming well with the default parameters. We can tune the parameter littele bit to get some more good results without the tension of pverfitting :)","2504c9d4":"# Supervised Machine Learning models scratch series....\nyou can also check....\n\n- 1) Linear Regression         ---> https:\/\/www.kaggle.com\/ninjaac\/linear-regression-from-scratch\n- 2) Lasso Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch \n- 3) Ridge Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch\n- 4) ElasticNet Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/elasticnet-regression-from-scratch (Same Notebook you are looking now)\n- 5) Polynomail Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch \n- 5) PolynomailRidge Regression---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch\n- 6) KNN Classifier            ---> https:\/\/www.kaggle.com\/ninjaac\/knnclassifier-from-scratch ","4df4c769":"Elastica Net is the combination of Lasso and Ridge regression. It has the advanage of both Lasso and Ridge Regression.\n- **Lasso** --> Will completely eliminates the correlated\/unwanted features from model. So, the model has less complexity and becomes easy to learn.\n- **Ridge** --> Will shrink the parameter close to zero , but never eliminate the variables.\n\nUsage:\n- We can use the **Lasso** model --> when we have a dataset with more correlated\/unwanted features.\n- We can use the **Ridge** model --> When we have a dataset with more useful features.\n\n### We can use the Elastic Net when we don't know the correlation between the features and having lots of (1000s<) features.","5fbbcee6":"# Data Creation","43459fd0":"# Common Regression class","43a63624":"# Elastic Net using scikit-learn for comparition","465621c2":"# Regularization classes","9060f23f":"# Elastic Net Regression From Scratch"}}