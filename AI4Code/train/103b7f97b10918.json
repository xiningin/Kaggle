{"cell_type":{"083c5ea4":"code","44c310b8":"code","037338a5":"code","df82726f":"code","2d32c211":"code","a5d856b1":"code","74970b32":"code","fce06a54":"code","ebb150eb":"code","a58fb599":"code","d65023cd":"code","eca75f54":"code","7115274f":"code","9b72d144":"code","c80c2bbe":"code","e0934fa4":"code","3882cd55":"code","4d9ce4ad":"code","350206a1":"code","c108b01b":"code","2d79191d":"code","56ca344a":"code","9855e5f0":"code","f5d5d1d6":"code","03a6672e":"code","d8fd88be":"code","25edc8d5":"code","dfd4fec1":"code","c8e26200":"code","c673cc24":"code","87e59ea1":"code","cca3b7f3":"code","7e700c5f":"code","7b1e5c88":"code","bcdb2688":"code","af44840b":"code","715488c4":"markdown","67b42bd7":"markdown","265f191c":"markdown","6e97ce98":"markdown","d5456a28":"markdown","b325f7a0":"markdown","40de49b8":"markdown","e45069d7":"markdown","27edf33e":"markdown"},"source":{"083c5ea4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import RobustScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport xgboost as xgb\n#import xgboost as XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import TimeSeriesSplit\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","44c310b8":"train_transaction=pd.read_csv('..\/input\/train_transaction.csv')\ntrain_identity=pd.read_csv('..\/input\/train_identity.csv')\ntest_transaction=pd.read_csv('..\/input\/test_transaction.csv')\ntest_identity=pd.read_csv('..\/input\/test_identity.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\nprint(\"Data is loaded\")","037338a5":"print (\"% of train_transaction data missing = \",(train_transaction[train_transaction.columns].isnull().sum().sum()\/np.product(train_transaction.shape)) * 100)\nprint (\"% of train_identity data missing = \",(train_identity[train_identity.columns].isnull().sum().sum()\/np.product(train_identity.shape)) * 100)\nprint (\"% of test_transaction data missing = \",(test_transaction[test_transaction.columns].isnull().sum().sum()\/np.product(test_transaction.shape)) * 100)\nprint (\"% of test_identity data missing = \",(test_identity[test_identity.columns].isnull().sum().sum()\/np.product(test_identity.shape)) * 100)","df82726f":"print(train_transaction.shape)\nprint(train_identity.shape)\nprint(test_transaction.shape)\nprint(test_identity.shape)","2d32c211":"train = train_transaction.merge(train_identity, left_on='TransactionID', right_on='TransactionID', how='left')\ndel train_identity, train_transaction\ntrain = train.set_index('TransactionID', drop = 'True')\n\ntest = test_transaction.merge(test_identity, left_on='TransactionID', right_on='TransactionID', how='left')\ndel test_identity, test_transaction\ntest = test.set_index('TransactionID', drop = 'True')","a5d856b1":"def datatype_conversion(df):\n\n    card_cols = [c for c in df.columns if 'card' in c]\n    for col in card_cols:\n        df[col] = df[col].astype('object')\n\n    addres_cols = [c for c in df.columns if 'addr' in c]\n    for col in addres_cols:\n            df[col] = df[col].astype('object')\n\n    id_cols = [c for c in df.columns if 'id' in c]\n    for col in id_cols:\n            df[col] = df[col].astype('object')\n\n    M_cols = [c for c in df.columns if 'M' in c]\n    for col in M_cols:\n            df[col] = df[col].astype('object')\n\n    df['ProductCD'] = df['ProductCD'].astype('object')\n    df['P_emaildomain'] = df['P_emaildomain'].astype('object')\n    df['R_emaildomain'] = df['R_emaildomain'].astype('object')    \n    df['DeviceType'] = df['DeviceType'].astype('object')\n    df['DeviceInfo'] = df['DeviceInfo'].astype('object')\n    \n    return df","74970b32":"train = datatype_conversion(train)\ntest = datatype_conversion(test)","fce06a54":"print(set(list(test.dtypes)))\nprint(set(list(train.dtypes)))","ebb150eb":"print(test.shape)\nprint(train.shape)","a58fb599":"def variable_inspection(train, column):\n    for col in column:\n        print(train[col].value_counts(dropna = False))#.head(10))","d65023cd":"C_column = [c for c in train.columns if 'C' in c and 'ProductCD' not in c] \nD_column = [c for c in train.columns if 'D' in c and 'TransactionID' not in c\n            and 'TransactionDT' not in c and 'DeviceType' not in c \n            and 'DeviceInfo' not in c and 'ProductCD' not in c] \nV_column = [c for c in train.columns if 'V' in c]\nM_column = [c for c in train.columns if 'M' in c]\ncard_column = [c for c in train.columns if 'card' in c]\nid_column = [c for c in train.columns if 'id' in c]","eca75f54":"variable_inspection(train, C_column)","7115274f":"variable_inspection(train, D_column)","9b72d144":"variable_inspection(train, V_column)","c80c2bbe":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nax = sns.distplot(train['dist1'], bins=10, hist=False, ax=ax1,label='train')\nax = sns.distplot(test['dist1'], bins=10, hist=False, ax=ax1, label='test')\nax1.set_title('dist1 Distribution', fontsize=14)\n\nax = sns.distplot(train['dist2'], bins=10, hist=False, ax=ax2, label='train')\nax = sns.distplot(test['dist2'], bins=10, hist=False, ax=ax2, label='test')\nax2.set_title('dist2 Distribution', fontsize=14)\n\nax = sns.distplot(train['TransactionAmt'], bins=2, hist=False, ax=ax3, label='train')\nax = sns.distplot(test['TransactionAmt'], bins=2, hist=False, ax=ax3, label='test')\nax3.set_title('TransactionAmt Distribution', fontsize=14)","e0934fa4":"# New feature - decimal part of the transaction amount\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24","3882cd55":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4d9ce4ad":"col = [c for c in train.columns if c  not in ['dist1', 'dist2', 'TransactionAmt','isFraud', 'TransactionDT']]\n\n#x = train[col].copy()\nfor col in train[col].columns:\n    print(col)\n    value_counts = train[col].value_counts()\n    labels = value_counts.index[value_counts < 2500]\n    train[col][train[col].isin(labels)] = '-1'\n    print(train[col].value_counts())","350206a1":"%%time\ntrain = reduce_mem_usage2(train)","c108b01b":"col = [c for c in test.columns if c  not in ['dist1', 'dist2', 'TransactionAmt','isFraud', 'TransactionDT']]\n\n#x = train[col].copy()\nfor col in test[col].columns:\n    print(col)\n    value_counts = test[col].value_counts()\n    labels = value_counts.index[value_counts < 2500]\n    test[col][test[col].isin(labels)] = '-1'\n    print(test[col].value_counts())","2d79191d":"%%time\ntest = reduce_mem_usage2(test)","56ca344a":"def missing_value_statistics(df):\n    missing_values_count = df[df.columns].isnull().sum()\n    print (missing_values_count.head())\n    total_cells = np.product(df.shape)\n    total_missing = missing_values_count.sum()\n    \n    print (\"% of missing data = \",(total_missing\/total_cells) * 100)","9855e5f0":"#imputation of dist1, dist2 and TransactionAmt with the mean of the respective columns\nsubset_col_1 = [c for c in train.columns if c in ['dist1', 'dist2', 'TransactionAmt']]\n\nimputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimputer_train = imputer.fit(train[subset_col_1])  \nimputer_test = imputer.fit(train[subset_col_1]) \n\ntrain[subset_col_1] = imputer_train.transform(train[subset_col_1])\ntest[subset_col_1] = imputer_test.transform(test[subset_col_1])\n\nmissing_value_statistics(train[subset_col_1])\nmissing_value_statistics(test[subset_col_1])\n\n#imputation of features other than dist1, dist2 and TransactionAmt with -999\nsubset_col_2 = [c for c in train.columns if c  not in ['dist1', 'dist2', 'TransactionAmt','isFraud', 'TransactionDT']]\n\nfor col in subset_col_2:\n    train[col] = train[col].astype('object')\n    \nfor col in subset_col_2:\n    test[col] = test[col].astype('object')\n    \ntrain[subset_col_2] = train[subset_col_2].fillna(-999)\ntest[subset_col_2] = test[subset_col_2].fillna(-999)\n\nmissing_value_statistics(train[subset_col_2])\nmissing_value_statistics(test[subset_col_2])","f5d5d1d6":"train_clean = train\ntest_clean = test\ndel train, test","03a6672e":"train_clean['TransactionAmt'] = np.log(train_clean['TransactionAmt']+1)\ntrain_clean['dist1'] = np.log(train_clean['dist1']+1)\ntrain_clean['dist2'] = np.log(train_clean['dist2']+1)\n\ntest_clean['TransactionAmt'] = np.log(test_clean['TransactionAmt']+1)\ntest_clean['dist1'] = np.log(test_clean['dist1']+1)\ntest_clean['dist2'] = np.log(test_clean['dist2']+1)","d8fd88be":"def Robust_Scaler(df):\n        cols_1 = [c for c in df.columns if c in ['dist1','dist2','TransactionAmt']]\n        cols_2 = [c for c in df.columns if c not in cols_1]\n\n        # RobustScaler is less prone to outliers.\n        rob_scaler = RobustScaler(with_scaling=True, with_centering=False)\n        train_clean_rob = pd.DataFrame(data=rob_scaler.fit_transform(df[cols_1]), columns=['dist1','dist2','TransactionAmt'])\n\n        # Set the index of the scaled dataset. It is the same as the original dataset\n        s=df.index\n        train_clean_rob = train_clean_rob.set_index([s])\n\n        #Merge the scaled dataset with the categorical features and the [\"isFraud\", \"TransactionDT\"] columns to get back the cleaned \n        #dataset but with scaled numerical columns\n        train_clean_rob = pd.merge(train_clean_rob, df[cols_2],left_index=True, right_index=True)\n\n        #Just a check of the dimensions.\n        print(df.shape)\n        print(train_clean_rob.shape)\n    \n        return train_clean_rob","25edc8d5":"train_clean_rob = Robust_Scaler(train_clean)\ntest_clean_rob = Robust_Scaler(test_clean)","dfd4fec1":"f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\nax = sns.distplot(train_clean_rob['dist1'], bins=10, hist=False, ax=ax1, label='train')\nax = sns.distplot(test_clean_rob['dist1'], bins=10, hist=False, ax=ax1, label='test')\nax1.set_title('dist1 Distribution After Scaling', fontsize=14)\n\nax = sns.distplot(train_clean_rob['dist2'], bins=10, hist=False, ax=ax2, label='train')\nax = sns.distplot(test_clean_rob['dist2'], bins=10, hist=False, ax=ax2, label='test')\nax2.set_title('dist2 Distribution After Scaling', fontsize=14)\n\nax = sns.distplot(train_clean_rob['TransactionAmt'], bins=2, hist=False, ax=ax3, label='train')\nax = sns.distplot(test_clean_rob['TransactionAmt'], bins=2, hist=False, ax=ax3, label='test')\nax3.set_title('TransactionAmt Distribution After Scaling', fontsize=14)","c8e26200":"train_clean_rob.to_csv('train_clean_RobustScaler.csv',index=True)\ntest_clean_rob.to_csv('test_clean_RobustScaler.csv',index=True)","c673cc24":"# Label Encoding\nfor f in train_clean_rob.columns:\n    #print(f)\n    if train_clean_rob[f].dtype=='object' and test_clean_rob[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_clean_rob[f].values) + list(test_clean_rob[f].values))\n        train_clean_rob[f] = lbl.transform(list(train_clean_rob[f].values))\n        test_clean_rob[f] = lbl.transform(list(test_clean_rob[f].values))","87e59ea1":"#del X_train, y_train, X_val, y_val, dtrain, dvalidation, y_val_preds, y_val_preds_2\n#del X, y, cols, tscv\n\ntrain_col = train_clean_rob.columns#.tolist()\ntest_col = test_clean_rob.columns#.tolist()\n\ncols = [c for c in train_clean_rob if c not in ['isFraud', 'TransactionID', 'TransactionDT']]\ny=np.array(train_clean_rob['isFraud'])\nX=np.array(train_clean_rob[cols])\n\n# %% [code]\ntscv = TimeSeriesSplit(n_splits=2)\nfor train_index, val_index in tscv.split(train_clean_rob):\n    print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n\ndel train_clean_rob","cca3b7f3":"dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=cols)\ndvalidation = xgb.DMatrix(X_val, feature_names=cols)\n\nparams =  {'bagging_fraction': 0.8993155305338455, \n               'colsample_bytree': 0.7463058454739352,\n               'feature_fraction': 0.7989765808988153, \n               'gamma': 0.6665437467229817, \n               'learning_rate': 0.013887824598276186,\n               'max_depth': 16, \n               'min_child_samples': 170, \n               'num_leaves': 220, \n               'reg_alpha': 0.39871702770778467, \n               'reg_lambda': 0.24309304355829786, \n               'subsample': 0.7,\n               'objective': 'binary:logistic'}\n\nnum_rounds=1000\n\ntrain_labels = dtrain.get_label()\nratio= float(np.sum(train_labels == 0)) \/ np.sum(train_labels == 1)\nparams['scale_pos_weight'] = ratio\n\n# %% [code]\nbst = xgb.train(params, dtrain, num_rounds)\ny_val_preds = (bst.predict(dvalidation) > 0.5).astype('int')\ny_val_preds_2 = bst.predict(dvalidation)\n\npd.crosstab(\n    pd.Series(y_val, name='Actual'),\n    pd.Series(y_val_preds, name='Predicted'),\n    margins=True\n)\n\n# %% [code]\nprint('Accuracy: {0:.2f}'.format(accuracy_score(y_val, y_val_preds)))\nprint('Precision: {0:.2f}'.format(precision_score(y_val, y_val_preds)))\nprint('Recall: {0:.2f}'.format(recall_score(y_val, y_val_preds)))\nprint('AUC ROC: {0:.2f}'.format(roc_auc_score(y_val, y_val_preds_2)))","7e700c5f":"def feature_importance_2(measure):\n    feature_importances = bst.get_score(importance_type=measure)\n    \n    keys = list(feature_importances.keys())\n    values = list(feature_importances.values())\n    \n    data = pd.DataFrame(data=values, index=keys, columns=[measure]).sort_values(by = measure, ascending=False)\n    \n    data[measure+'_rank'] = data[measure].rank(method = 'first', ascending = False)\n    \n    data.index.name='features'\n    \n    data = pd.DataFrame(data[measure+'_rank'])\n    \n    return data\n    ","7b1e5c88":"df = pd.DataFrame(index = cols)\ndf.index.name='features'\n\nmeasures = ['gain',  'total_gain', 'weight', 'cover', 'total_cover']\nfor measure in measures:\n    data = feature_importance_2(measure)\n    df = df.merge(data, left_on='features', right_on='features', how='left')","bcdb2688":"del X_train, y_train, X_val, y_val, dtrain, dvalidation, y_val_preds, y_val_preds_2, X, y, tscv","af44840b":"X_test = test.drop(['TransactionDT'], axis=1)\ndel test\ndtest = X_test.as_matrix()\ndtest = xgb.DMatrix(dtest, feature_names=cols)\nsample_submission['isFraud'] = bst.predict(dtest)\nsample_submission.to_csv('Xgboost.csv')\ndf.to_csv('feature_importance.csv')","715488c4":"### [Feature Engineering](#3)<a id=\"3\"><\/a> <br>\n\n* Day of the week and hour are engineerd from TransactionDT.\n* The decimal part of the TransactionAmt is engineerd as an separate feature. \n","67b42bd7":"### [Load Packages and Data](#1)<a id=\"1\"><\/a> <br>","265f191c":"Get feature importance of each feature. Importance type can be defined as:\n\n\u2018weight\u2019: the number of times a feature is used to split the data across all trees.\n\n\u2018gain\u2019: the average gain across all splits the feature is used in.\n\n\u2018cover\u2019: the average coverage across all splits the feature is used in.\n\n\u2018total_gain\u2019: the total gain across all splits the feature is used in.\n\n\u2018total_cover\u2019: the total coverage across all splits the feature is used in.","6e97ce98":"The purpose of this notbook is to perform the following tasks:\n \n 1. [Load Packages and Data](#1)\n 2. [Exploratory Data Analysis](#2)\n 3. [Feature Engineering](#3)\n 4. [Binning of variables and Imputation of missing values](#4)\n 5. [Outlier Analysis and Feature Scaling](#5)\n 6. [Modeling: xgboost](#6)\n \nMain Findings:\n \n * The feature set can be divided into 3 data types: \n \n  1. Categorical Variables, \n  2. Numerical Variables and \n  3. Numeric Encoding Variables {C_X, D_X and V_X} \n  \n  \n * 41% of the train_transaction is missing data.\n * 36% of the test_transaction is missing data.\n * 35% of train_identity is missing data.\n * 36% of test_identity is missing data.\n \nAt the end of the task the training and test datasets are imputed for missing values, scaled and fixed for outliers and features are binned to prevent overfitting. The results are saved as outputs of this notebook. The user can use the outputs of this notebook for model building.","d5456a28":"### [Outlier Analysis and Feature Scaling](#5)<a id=\"5\"><\/a> <br>\n \n  * I have taken these two subjects under one section because I am using **RobustScaler()** to scale dist1, dist2 and TransactionAmt. **RobustScaler()** applies Inter Quartile Range(IQR) to scale the features when the argument **with_scaling=True**.\n  * Outlier Analysis is contentious territory. I have opted for Inter Quartile Range (IQR) to detect outliers since it does not assume any distribution for the features.\n  *  There are different ways to perform feature scaling. Here it has been opted for **RobustScaler()** because it is relatively more robust outliers. [Here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html) you can find more information regarding this scaler. \n  *  It is applied only to dist1, dist2 and TransactionAmt features.","b325f7a0":"![fraud%20detection.jpeg](attachment:fraud%20detection.jpeg)","40de49b8":"### [Exploratory Data Analysis](#2)<a id=\"2\"><\/a> <br>\n \n * Variables C_X and D_X have only integers as values and most of V_X as well. \n * I strongly believe that C_X, D_X and V_X are numeric encoding. This is important to know when imputing the missing values, performing outlier analysis and modelling.\n * Varibles dist1, dist2 and TransactionAmt have a long tail distribution. Agian this is also important to realize when performing outlier analysis.\n * 41% of the train_transaction is missing.\n * 36% of the test_transaction is missing.\n * 35% of train_identity is missing data.\n * 36% of test_identity is missing data.","e45069d7":"### [Modeling: Xgboost](#6)<a id=\"6\"><\/a> <br>\n\n* The parameters are not optimised.\n* The xgboost model is based on [this](copied from https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s) great kernel. \n* I am using scale_pos_weight to account for the imbalance in the target variable","27edf33e":"### [Binning of variables and Imputation of missing values](#4)<a id=\"4\"><\/a> <br>\n \n  * Before we go into the missing value imputation I have explicitly changed the dataype of some of the categorical features into type boolean beacuse some of the categorical features are loaded as numerical.\n  * For numerical features dist1, dist2 and TransactionAmt the missing values are replaced by the mean of the column. The missing value of other features are replaced by -999\n  * Variables C_X and D_X have only integers as values and most of V_X as well. There missing values are replaced by -999.\n  * The rare values of all features other than dist1, dist2, TransactionAmt and TransactionDT are binned together. This is a very important step to perform to prevent overfitting when performing modelling.\n  * To reduce the size of the datasets the kernel published by [MJ Bahamani](https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee) is applied to the datasets. It is a very helpfull function."}}