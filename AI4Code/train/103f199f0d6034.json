{"cell_type":{"6ce2e30a":"code","6e9fd9d7":"code","bbf3c44a":"code","cff6dffe":"code","d701a015":"code","84a3ef23":"code","ef724e5e":"code","98ba651f":"code","cc3ac94b":"code","4908c5e6":"code","cbf60171":"code","7e6be06c":"code","8a1d270c":"code","34d16c06":"code","a988030c":"code","d557542c":"code","0fe2ed51":"code","34a8a6dd":"code","ca8ab935":"code","1162d64e":"code","89ea9f78":"code","904015b8":"code","7a0d322b":"code","5416e3ea":"code","f165a290":"code","42d9ff8e":"code","737f61fa":"code","b669e1a7":"code","cfb9407c":"code","1dce9c89":"code","5cf1ff89":"code","2c4899be":"code","a65831e4":"code","2644d504":"code","a1c539f6":"code","95d69f53":"code","e5ee44ef":"code","4261007f":"code","bd356535":"code","dcbd61d7":"code","59d9384e":"code","6920b20a":"code","4e17de5a":"code","7a2da416":"code","becd694f":"code","46967174":"code","0c7cd536":"code","94761c63":"code","e54a4047":"code","5bcff8b4":"code","0d3fe8e1":"code","aa688398":"code","f0636b05":"code","265277a0":"code","3d4ba02e":"markdown","fc8e84a8":"markdown","02c53801":"markdown","ab13cde6":"markdown","211dd9a2":"markdown","08f5ba0d":"markdown","a88d9cdd":"markdown","22a99646":"markdown","7fcb36b3":"markdown","2c7a7076":"markdown","93855858":"markdown","ceb1abf1":"markdown","01c8c935":"markdown","868ebadb":"markdown","b2bba46d":"markdown","f591dd28":"markdown","5567e46f":"markdown","864e745a":"markdown","5648ef06":"markdown","cd19967d":"markdown","51a8a993":"markdown","c1a6e8c1":"markdown","f8910b2e":"markdown","20f853ab":"markdown"},"source":{"6ce2e30a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6e9fd9d7":"question_1 = pd.read_csv('\/kaggle\/input\/edit-technical-questions\/technical_data\/1_c_d.csv')\nquestion_2 = pd.read_csv('\/kaggle\/input\/edit-technical-questions\/technical_data\/2_a.csv')","bbf3c44a":"question_1","cff6dffe":"question_1.set_index('Unnamed: 0', inplace = True)","d701a015":"question_1","84a3ef23":"from statistics import mean, stdev\ndef statistics(gene):\n    index = [\"Mean\", \"Standard Deviation\", \"Number of Elements\"]\n    column = [\"Descriptive Statistics\"]\n    return pd.DataFrame([mean(gene), stdev(gene), len(gene)], index = index, columns = column)\n\n","ef724e5e":"import scipy.stats as stats\nfrom math import sqrt","98ba651f":"statistics(question_1.iloc[0])","cc3ac94b":"#1 sample z test\n\narray = statistics(question_1.iloc[0]).to_numpy().ravel()\nx_bar = array[0] # sample mean \nn = array[2] # number of elements\nsigma = array[1] # sd of population\nmu = 4 # Population mean aka threshold\n\nz = (x_bar - mu)\/(sigma\/sqrt(n))\n#z = (x_bar - mu)\/(sigma)\nz\n","4908c5e6":"import matplotlib.pyplot as plt\nplt.fill_between(x=np.arange(-4,z,0.01),\n                 y1= stats.norm.pdf(np.arange(-4,z,0.01)) ,\n                 facecolor='red',\n                 alpha=0.5,\n\n                 label= 'Area below z-statistic'\n                 )\n\nplt.fill_between(x=np.arange(z,4,0.01), \n                 y1= stats.norm.pdf(np.arange(z,4,0.01)) ,\n                 facecolor='blue',\n                 alpha=0.5,\n                 label= 'Area above z-statistic')\nplt.legend()\nplt.title (f'z-statistic = {z}');","cbf60171":"p = 1 - stats.norm.cdf(z)\np","7e6be06c":"from statistics import mean, stdev\nclass GeneTest:\n    \"\"\"Class to interpret gene data\"\"\"\n   \n    def __init__(self,filename):\n        \"\"\"Reads in a csv and creates a dataframe object\"\"\"\n        \n        data = pd.read_csv(filename)\n        data.set_index('Unnamed: 0', inplace = True)\n        self.data = data\n        \n    def statistics(self, genenum):\n        \"\"\"Outputs a dataframe with Mean,\n        Standard Deviation, and number of Samples.\"\"\"\n        \n        row = self.data.iloc[genenum]\n        index = [\"Mean\", \"Standard Deviation\", \"Number of Elements\"]\n        column = [\"Descriptive Statistics\"]\n        df = pd.DataFrame([mean(row), stdev(row), len(row)], index = index, columns = column)\n        return df\n    \n    def zTest(self, genenum, threshold):\n        \"\"\"Performs a z test on a gene, given a threshold\"\"\"\n        \n        sample = self.statistics(genenum).to_numpy().ravel()\n        x_bar = sample[0] # sample mean \n        n = sample[2] # number of elements\n        sigma = sample[1] # sd of population\n        mu = threshold # Population mean aka threshold\n        z = (x_bar - mu)\/(sigma\/sqrt(n))\n        p = 1 - stats.norm.cdf(z)\n        return p\n        \n    def runTest(self, threshold, alpha):\n        \"\"\"Performs a z test on a gene, given a threshold\"\"\"\n\n        pvalue = []\n        genestatus = []\n        for i in range(len(self.data)):\n            samplep = self.zTest(i, threshold)\n            pvalue.append(samplep)\n            if(samplep < alpha):\n                genestatus.append(True)\n            else:\n                genestatus.append(False)\n        return list(zip(pvalue, genestatus))\n    \n    def scatter(self, genenum1, genenum2):\n        \"\"\"runs a z test and outputs a list containing\n        the p value given a certain threshold and if\n        there is sufficent evidence to reject the null \n        hypothesis.\"\"\"\n\n        gene1 = self.data.iloc[genenum1]\n        gene2 = self.data.iloc[genenum2]\n        f, ax = plt.subplots()\n        \n        ax.plot(gene1, gene2, 'o', color='black')\n        return ax\n\n            \n\n","8a1d270c":"ex = GeneTest('\/kaggle\/input\/edit-technical-questions\/technical_data\/1_c_d.csv')\nex.statistics(0)","34d16c06":"ex.runTest(4, 5.05)","a988030c":"ax1= ex.scatter(5, 7)\nax1.set_title('Expressions of Gene 5 and Gene 7')\nax1.set_xlabel('Gene 5')\nax1.set_ylabel('Gene 7')\n\nax2= ex.scatter(2, 4)\nax2.set_title('Expressions of Gene 2 and Gene 4')\nax2.set_xlabel('Gene 2')\nax2.set_ylabel('Gene 4')\n","d557542c":"question_2.head()","0fe2ed51":"# creating new dataframe, taking the column names and making it into a row\nreformat = np.vstack((question_2.columns, question_2.to_numpy()))\nquestion_2_new = pd.DataFrame(reformat)\nquestion_2_new.head(2)","34a8a6dd":"#loop through and get rid of the labels\nfor i in range(len(question_2_new[0])):\n    if \"ALL\" in question_2_new[0][i]:\n        question_2_new[0][i] = \"ALL\"\n    elif \"AML\" in question_2_new[0][i]:\n        question_2_new[0][i] = \"AML\"\n","ca8ab935":"question_2_new.head()","1162d64e":"question_2_new[0].value_counts().plot(kind = 'bar')\nplt.xlabel(\"Type of Gene Expression\", labelpad=14)\nplt.ylabel(\"Counts\", labelpad=14)\nplt.title(\"Counts of ALL vs AML\", y=1.02);","89ea9f78":"X = question_2_new.drop(question_2_new.columns[0], axis=1)\ny = question_2_new[0]","904015b8":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","7a0d322b":"X = StandardScaler().fit_transform(X)# subtracts means and divides by std\nPCA = PCA(n_components=2)\nprincipalComponents = PCA.fit_transform(X)\nPCDF = pd.DataFrame(principalComponents, columns = ['principal component 1', 'principal component 2'])","5416e3ea":"PCDF['target'] = y","f165a290":"fig = plt.figure()\nax = fig.add_subplot(1,1,1) \ntargets = ['ALL', 'AML']\ncolors = ['b', 'r']\nfor target, color in zip(targets,colors):\n    correct = PCDF['target'] == target\n    ax.scatter(PCDF.loc[correct, 'principal component 1'],\n               PCDF.loc[correct, 'principal component 2'],\n               c = color)\n\nax.set_xlabel('1st Principal Component', fontsize = 15)\nax.set_ylabel('2nd Principal Component', fontsize = 15)\nax.set_title('PCA', fontsize = 25)\nax.legend(targets)\nax.grid()","42d9ff8e":"PCA.explained_variance_ratio_\n\n#the first two principal components contain 20.349% of the information.\n#First component contains 13.32% of the variance\n#Second component contains 7.025% of the variance","737f61fa":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics","b669e1a7":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state = 42)\nX_train.shape,y_train.shape","cfb9407c":"knn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)\n\nscores = cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy')\nprint(scores)\nprint(\"Kfold on KNeighborsClassifier: %0.4f (std = %0.4f)\" % (scores.mean(), scores.std()))\nknn.score(X_test, y_test)","1dce9c89":"svm = SVC()\nsvm.fit(X_train,y_train)\n\nscores = cross_val_score(svm, X_train, y_train, cv=3, scoring='accuracy')\nprint(scores)\nprint(\"Kfold on SVM: %0.4f (std = %0.4f)\" % (scores.mean(), scores.std()))\nsvm.score(X_test, y_test)","5cf1ff89":"xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',eval_metric = \"logloss\",\n                      nthread=1)\nxgb.fit(X_train, y_train)\nscores = cross_val_score(xgb, X_train, y_train, cv=3, scoring='accuracy')\nprint(scores)\nprint(\"Kfold on XGBOOST: %0.4f (std = %0.4f)\" % (scores.mean(), scores.std()))","2c4899be":"\n'''xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',eval_metric = \"logloss\",\n                      nthread=1)\nrandom_search = RandomizedSearchCV(xgb, param_distributions= {\n        'min_child_weight': [1, 5, 10, 20],\n        'gamma': [0.1,0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0, 2],\n        'colsample_bytree': [0.6, 0.8, 1.0, 2],\n        'max_depth': [3, 4, 5, 10,]\n        }, n_iter=5, scoring='roc_auc', n_jobs=4, cv=3, verbose=3, random_state=42 )\n\nrandom_search.fit(X_train, y_train)\nxgb_params = random_search.best_params_\n\nprint(xgb_params)'''","a65831e4":"mnist_test = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')\nmnist_train = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')","2644d504":"mnist_train","a1c539f6":"labels = {0 : \"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}","95d69f53":"def plot_label_per_class(data):\n    f, ax = plt.subplots(1,1, figsize=(12,4))\n    g = sns.countplot(data.label, order = data[\"label\"].value_counts().index)\n    g.set_title(\"Counts for each label\")\n\n    for p, label in zip(g.patches, data[\"label\"].value_counts().index):\n        g.annotate(labels[label], (p.get_x(), p.get_height()+0.1))\n    plt.show()  \n    \nplot_label_per_class(mnist_train)\nplot_label_per_class(mnist_test)\n","e5ee44ef":"mnist_train.isnull().any().sum(), mnist_test.isnull().any().sum()\n\n","4261007f":"fig, axes = plt.subplots(4, 4, figsize = (8,8))\nfor row in axes:\n    for ax in row:\n        index = np.random.randint(60000)\n        img = mnist_train.drop('label', axis=1).values[index].reshape(28,28)\n        cloths = mnist_train['label'][index]\n        ax.imshow(img)\n        ax.set_title(labels[cloths])\n        ax.set_axis_off()\n","bd356535":"mnist_train.shape, mnist_test.shape\n\n# 6:1 aplit\n\ndata = mnist_train[list(mnist_train.columns)[1:]].to_numpy()\nlabel = mnist_train['label'].values\n\n## normalize\ndata = data \/ 255\n\n## create train and validation datasets\ntrain_X, test_X, train_y, test_y = train_test_split(data, label, test_size=0.2)\n","dcbd61d7":"train_X.shape, test_X.shape\n","59d9384e":"principalComponents2 = PCA.fit_transform(train_X)\nPCDF2 = pd.DataFrame(principalComponents2, columns = ['principal component 1', 'principal component 2'])\nPCDF2['target'] = train_y","6920b20a":"PCDF2.head()","4e17de5a":"fig = plt.figure(figsize = (20,20))\nax = fig.add_subplot(1,1,1) \ntargets = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\ncolors = ['r', 'g', 'b' ,'c','m','y','k', 'k','k','k']\n#colors = plt.get_cmap('jet', 10)\nfor target, color in zip(targets, colors):\n    correct = PCDF2['target'] == target\n    ax.scatter(PCDF2.loc[correct, 'principal component 1'],\n               PCDF2.loc[correct, 'principal component 2']\n               ,cmap = plt.get_cmap('jet', 10) )\n\nax.set_xlabel('1st Principal Component', fontsize = 30)\nax.set_ylabel('2nd Principal Component', fontsize = 30)\nax.set_title('PCA', fontsize = 40)\nax.legend(targets, prop={'size': 20})\nax.grid()","7a2da416":"import tensorflow as tf","becd694f":"#train_X = train_X.reshape(( 48000, 28, 28, 1))\ndata = data.reshape(( 60000, 28, 28, 1))\n","46967174":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') >= 0.997):\n            print(\"\\nReached 99.7% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\nearly = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, mode='max',\n                                        restore_best_weights=True)","0c7cd536":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, MaxPooling2D\nfrom keras.optimizers import Adam, RMSprop\ndef NN_model():\n\n    model = Sequential()\n\n    model.add(Flatten())\n    model.add(Dense(128, activation = 'relu'))\n    model.add(Dense(128, activation = 'relu'))\n    model.add(Dense(10, activation = 'softmax'))\n\n    optimizer = Adam(lr=0.001)\n\n    model.compile(optimizer = optimizer,\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics  = 'accuracy')\n    return model\n","94761c63":"def CNN_model():\n\n    model = Sequential()\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                     data_format='channels_last', input_shape=(28,28,1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                     data_format='channels_last'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                     data_format='channels_last'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    optimizer = Adam(lr=0.001)\n\n    model.compile(optimizer = optimizer,\n                  loss = 'sparse_categorical_crossentropy',\n                  metrics  = ['accuracy'])\n    return model","e54a4047":"baseline = NN_model()\n\nbaseline = baseline.fit(data, label, epochs=100, batch_size =64,validation_split = 0.2 ,callbacks=[early])\n","5bcff8b4":"CNN = CNN_model()\n\nCNN = CNN.fit(data, label, epochs=100,batch_size =64, validation_split = 0.2 ,callbacks=[early])\n","0d3fe8e1":"plt.plot(CNN.history['loss'])\nplt.plot(CNN.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('CNN Loss')\n\nplt.show()\n\nplt.plot(CNN.history['accuracy'])\nplt.plot(CNN.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('CNN Accuracy')\n\nplt.show()","aa688398":"plt.plot(baseline.history['loss'])\nplt.plot(baseline.history['val_loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('NN Loss')\n\nplt.show()\n\nplt.plot(baseline.history['accuracy'])\nplt.plot(baseline.history['val_accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('NN Accuracy')\n\nplt.show()","f0636b05":"'''\ndef TF_model():\n    model = Sequential()\n    model.add(keras.applications.inception_resnet_v2.InceptionResNetV2(weights='imagenet',\n                                                                      include_top=False,\n                                                                      input_shape=(28, 28, 3)))\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(33, activation='softmax'))\n\n    model.compile(optimizer=keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n    return  model'''\n","265277a0":"#T#F = TF_model()\n\n#TF_history = TF.fit(data, label, epochs=100, batch_size =64,validation_split = 0.2 ,callbacks=[early])\n","3d4ba02e":"# Question 1: Programming Statistics","fc8e84a8":"Finding the p value","02c53801":"KNN","ab13cde6":"# Part C: Fashion MNIST","211dd9a2":"CNN","08f5ba0d":"# Data Cleaning","a88d9cdd":"Load Data","22a99646":"CallBacks","7fcb36b3":"**Part A**","2c7a7076":"SVM","93855858":"**Part C**","ceb1abf1":"# Modeling","01c8c935":"# Question 2: Machine Learning","868ebadb":"# Imports","b2bba46d":"Baseline Model","f591dd28":"Our p value is 0.0188 which is smaller than our alpha value of 0.05.\nWe can say that there is enough evidence to reject the null hypothesis which states that the sample mean is less than or equal to the threshold value of 4.\n\n\n\n\n\n","5567e46f":"# Modeling","864e745a":"# PCA","5648ef06":"Reformatting the DataFrame","cd19967d":"XGBOOST","51a8a993":"# Part A","c1a6e8c1":"**Part B**","f8910b2e":"NN","20f853ab":"# PCA"}}