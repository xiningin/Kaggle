{"cell_type":{"8b9463e1":"code","c8b3a4ea":"code","5c825d29":"code","544e7c2b":"code","536ace6d":"code","83f6b79a":"code","bcbcb97c":"code","b178a3ba":"code","f858f53c":"code","8aff18fa":"code","77be1910":"code","ba3d7b65":"code","b7528d20":"code","e3b56338":"code","687aec03":"code","014c5e5e":"code","2d25824f":"code","b32f7930":"code","656b235a":"code","2c872d9f":"code","c670ca58":"code","29f85ab4":"code","e3a334e5":"code","fd36f586":"code","08bc1e3e":"code","82defa50":"code","40b0c549":"code","4233e785":"code","1a3602b0":"code","a5cc0bc7":"markdown","25b62463":"markdown","dd0adc6a":"markdown","f7d1e2f5":"markdown","9e7bc1a4":"markdown","7cc14f84":"markdown","546aef9d":"markdown"},"source":{"8b9463e1":"import os\nimport copy\nimport math\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nimport matplotlib.pyplot as plt\nimport random\nimport csv\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport transformers\nfrom transformers import (XLMRobertaTokenizer, XLMRobertaModel,\n                          DistilBertTokenizer, DistilBertModel)","c8b3a4ea":"train=pd.read_csv(\"..\/input\/amazon-ml-challenge-2021-hackerearth\/train.csv\", escapechar = \"\\\\\", quoting = csv.QUOTE_NONE)\ntrain.head()","5c825d29":"def set_seed(seed=42):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","544e7c2b":"set_seed()","536ace6d":"class CFG:\n    DistilBERT = False # if set to False, BERT model will be used\n    bert_hidden_size = 768\n    num_classes=9919\n    batch_size = 192\n    epochs = 4\n    num_workers = 2\n    learning_rate = 1e-5 #3e-5\n    scheduler = \"ReduceLROnPlateau\"\n    step = 'epoch'\n    patience = 2\n    factor = 0.8\n    dropout = 0.5\n    model_path = \"\/kaggle\/working\"\n    max_length = 64\n    model_save_name = \"model.pt\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')","83f6b79a":"if CFG.DistilBERT:\n    model_name='distilbert-base-uncased'\n    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n    bert_model = DistilBertModel.from_pretrained(model_name)\nelse:\n    model_name='xlm-roberta-base'\n    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n    bert_model = XLMRobertaModel.from_pretrained(model_name)","bcbcb97c":"text = train['TITLE'].values[np.random.randint(0, len(train) - 1, 1)[0]]\nprint(f\"Text of the title: {text}\")\nencoded_input = tokenizer(text, return_tensors='pt')\nprint(f\"Input tokens: {encoded_input['input_ids']}\")\ndecoded_input = tokenizer.decode(encoded_input['input_ids'][0])\nprint(f\"Decoded tokens: {decoded_input}\")\noutput = bert_model(**encoded_input)\nprint(f\"last layer's output shape: {output.last_hidden_state.shape}\")","b178a3ba":"id2lbl={lbl: idx for idx,lbl in enumerate(list(train[\"BROWSE_NODE_ID\"].unique()))}\nlbl2id={lbl:idx for idx,lbl in id2lbl.items()}","f858f53c":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    data = data.sample(frac=1).reset_index(drop=True)\n    y=data[\"BROWSE_NODE_ID\"]\n    kf = StratifiedKFold(n_splits=num_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=y)):\n        data.loc[v_, 'kfold'] = f\n    return data\n","8aff18fa":"train=create_folds(train, 5)","77be1910":"train=train.loc[train.kfold.isin([1,2,3])]\ntrain=train.reset_index(drop=True)\ntrain.head()","ba3d7b65":"temp=train.dropna(subset=['TITLE'])\ntemp=temp.reset_index(drop=True)","b7528d20":"temp.head()","e3b56338":"temp[\"BROWSE_NODE_ID\"]=temp[\"BROWSE_NODE_ID\"].map(id2lbl)","687aec03":"class TextDataset(Dataset):\n  def __init__(self,data,tokenizer,mode=\"train\", max_length=None):\n    super(TextDataset, self).__init__()\n    self.sentence=data[\"TITLE\"]\n    if mode != \"test\":\n        self.label=data[\"BROWSE_NODE_ID\"]\n    self.tokenizer=tokenizer\n    self.max_length=max_length\n    self.mode=mode\n\n  def __len__(self):\n    return len(self.sentence)\n  \n  def __getitem__(self,idx):\n    inp_tokens=self.tokenizer.encode_plus(self.sentence[idx], \n                                          padding=\"max_length\", \n                                          add_special_tokens=True,\n                                          max_length=self.max_length,\n                                          truncation=True)\n    item={\n        \"input_ids\":torch.tensor(inp_tokens.input_ids,dtype=torch.long),\n        \"attention_mask\":torch.tensor(inp_tokens.attention_mask,dtype=torch.long)\n    }\n    if self.mode != \"test\":\n        item['labels'] = torch.tensor(self.label[idx], dtype=torch.long)\n\n    return item","014c5e5e":"dataset = TextDataset(temp, tokenizer, max_length=CFG.max_length)\ndataloader = DataLoader(dataset, \n                         batch_size=CFG.batch_size, \n                         num_workers=CFG.num_workers, \n                         shuffle=True)","2d25824f":"len(dataset)","b32f7930":"next(iter(dataloader))","656b235a":"# code from https:\/\/github.com\/ronghuaiyang\/arcface-pytorch\/blob\/47ace80b128042cd8d2efd408f55c5a3e156b032\/models\/metrics.py#L10\n\nclass ArcMarginProduct(nn.Module):\n    r\"\"\"Implement of large margin arc distance: :\n        Args:\n            in_features: size of each input sample\n            out_features: size of each output sample\n            s: norm of input feature\n            m: margin\n            cos(theta + m)\n        \"\"\"\n    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        # print(output)\n\n        return output","2c872d9f":"class Model(nn.Module):\n    def __init__(self, \n                 bert_model, \n                 num_classes=CFG.num_classes, \n                 last_hidden_size=CFG.bert_hidden_size):\n        \n        super().__init__()\n        self.bert_model = bert_model\n        self.arc_margin = ArcMarginProduct(last_hidden_size, \n                                           num_classes, \n                                           s=30.0, \n                                           m=0.50, \n                                           easy_margin=False)\n    \n    def get_bert_features(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state\n    \n    def forward(self, batch):\n        CLS_hidden_state = self.get_bert_features(batch)\n        output = self.arc_margin(CLS_hidden_state, batch['labels'])\n        return output","c670ca58":"class AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n    \n    def reset(self):\n        self.avg, self.sum, self.count = [0]*3\n    \n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum \/ self.count\n    \n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef one_epoch(model, \n              criterion, \n              loader,\n              optimizer=None, \n              lr_scheduler=None, \n              mode=\"train\", \n              step=\"batch\"):\n    \n    loss_meter = AvgMeter()\n    acc_meter = AvgMeter()\n    \n    tqdm_object = tqdm(loader, total=len(loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        preds = model(batch)\n        loss = criterion(preds, batch['labels'])\n        if mode == \"train\":\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if step == \"batch\":\n                lr_scheduler.step()\n                \n        count = batch['input_ids'].size(0)\n        loss_meter.update(loss.item(), count)\n        \n        accuracy = get_accuracy(preds.detach(), batch['labels'])\n        acc_meter.update(accuracy.item(), count)\n        if mode == \"train\":\n            tqdm_object.set_postfix(train_loss=loss_meter.avg, accuracy=acc_meter.avg, lr=get_lr(optimizer))\n        else:\n            tqdm_object.set_postfix(valid_loss=loss_meter.avg, accuracy=acc_meter.avg)\n    \n    return loss_meter, acc_meter\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]\n\ndef get_accuracy(preds, targets):\n    \"\"\"\n    preds shape: (batch_size, num_labels)\n    targets shape: (batch_size)\n    \"\"\"\n    preds = preds.argmax(dim=1)\n    acc = (preds == targets).float().mean()\n    return acc","29f85ab4":"def train_eval(epochs, model, train_loader, valid_loader, \n               criterion, optimizer, lr_scheduler=None):\n    \n    best_loss = float('inf')\n    best_model_weights = copy.deepcopy(model.state_dict())\n    \n    for epoch in range(epochs):\n        print(\"*\" * 30)\n        print(f\"Epoch {epoch + 1}\")\n        current_lr = get_lr(optimizer)\n        \n        model.train()\n        train_loss, train_acc = one_epoch(model, \n                                          criterion, \n                                          train_loader, \n                                          optimizer=optimizer,\n                                          lr_scheduler=lr_scheduler,\n                                          mode=\"train\",\n                                          step=CFG.step)                     \n        model.eval()\n        with torch.no_grad():\n            valid_loss, valid_acc = one_epoch(model, \n                                              criterion, \n                                              valid_loader, \n                                              optimizer=None,\n                                              lr_scheduler=None,\n                                              mode=\"valid\")\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            best_model_weights = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), f'{CFG.model_path}\/{CFG.model_save_name}')\n            print(\"Saved best model!\")\n        \n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            lr_scheduler.step(valid_loss.avg)\n            if current_lr != get_lr(optimizer):\n                print(\"Loading best model weights!\")\n                model.load_state_dict(torch.load(f'{CFG.model_path}\/{CFG.model_save_name}', \n                                                 map_location=CFG.device))\n        \n        print(\"*\" * 30)","e3a334e5":"len(temp['TITLE'])","fd36f586":"len(temp['BROWSE_NODE_ID'])","08bc1e3e":"train_df, valid_df = train_test_split(temp, \n                                      test_size=0.33, \n                                      shuffle=True, \n                                      random_state=42)\ntrain_df=train_df.reset_index(drop=True)\nvalid_df=valid_df.reset_index(drop=True)\n\ntrain_dataset = TextDataset(train_df, tokenizer, max_length=CFG.max_length)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=True)\n\nvalid_dataset = TextDataset(valid_df, tokenizer, max_length=CFG.max_length)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, \n                                           batch_size=CFG.batch_size, \n                                           num_workers=CFG.num_workers, \n                                           shuffle=False)","82defa50":"model = Model(bert_model).to(CFG.device)\nprint(model)","40b0c549":"\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\nif CFG.scheduler == \"ReduceLROnPlateau\":\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                              mode=\"min\", \n                                                              factor=CFG.factor, \n                                                              patience=CFG.patience)\n\ntrain_eval(CFG.epochs, model, train_loader, valid_loader,\n           criterion, optimizer, lr_scheduler=lr_scheduler)","4233e785":"!mkdir tokenizer\ntokenizer.save_pretrained(\".\/tokenizer\")\ntorch.save(model.state_dict(), \"final.pt\")","1a3602b0":"torch.save(model,'RoBERTArcFace.pth')","a5cc0bc7":"See an example","25b62463":"Encoding label_group coulmn to numeric labels so we can feed them to the model and loss function.","dd0adc6a":"## DistilBERT finetuning with ArcMargin","f7d1e2f5":"max_length is set to 30 according to the histogram. But you can safely change it.","9e7bc1a4":"## Dataset","7cc14f84":"The following histogram gives us an idea that roughly how many words are there in each title. It is not a precise count of the tokens fed to the model because DistilBERT tokenizer does a more sophisticated function than simply splitting the sentence from its white spaces.","546aef9d":"Loading the model and its tokenizer from amazing HuggingFace model hub. As mentioned before, this model has been pre-trained on indonesian wikipedia."}}