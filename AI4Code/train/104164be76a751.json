{"cell_type":{"634c7002":"code","aa455feb":"code","531cc300":"code","d049c245":"code","07d39606":"code","c1e6b46e":"code","208fdedc":"code","226c2f1b":"code","6b0cb971":"code","201b8627":"code","b7da9fc7":"code","3d570724":"code","d1768fdb":"code","33c77c07":"code","e1e35c65":"code","a75166e6":"code","5411257c":"code","45c726cb":"code","7976efbb":"code","88c16415":"code","6bac16b2":"code","cf3ff20d":"code","355af1da":"code","005b1d64":"code","b4eaaeb9":"code","6dd7804c":"code","ea0aa794":"code","a9e54cb9":"code","744fa3f8":"code","48b7eb3d":"code","a568ec74":"code","7b9c1994":"code","0a13d4e1":"code","25d94a9c":"code","41d17cdd":"code","b4daa391":"code","4870ca95":"code","c651af5d":"code","67f77155":"code","e94a89a6":"code","5be506a4":"code","039496d5":"code","67973c76":"code","f1dd67b1":"code","031b2269":"code","4188a957":"code","191ed729":"code","9bd28de7":"code","e9a5529d":"code","ec158ff5":"code","aa33a2d6":"code","873025cc":"code","631d11eb":"code","e0e4c5db":"code","7b50b5a5":"code","d8cba6bb":"code","0ec86c6d":"code","269b8eb3":"code","a340c7ba":"code","d64cbf7a":"code","fd8e285c":"code","cc8ebb5c":"code","337a7e0b":"code","b12745da":"code","cfea2015":"code","04d22625":"code","0938ef04":"code","5dc6bb61":"code","f851e95a":"code","d6f414ed":"code","bd0121f8":"code","baaa3110":"code","1b9fb94a":"code","0159f44b":"code","0f84ff21":"code","8635f2f3":"code","1ce501f5":"code","6ca1d884":"code","d59f303b":"code","5a229542":"code","e3e4be2f":"code","e607a9d1":"code","4a599746":"code","cce7c99f":"code","613d8d46":"code","07724fb0":"code","72ba7863":"code","e79a2d62":"code","8d566cf0":"code","7a6c8a6c":"code","73345426":"code","254de58b":"code","a211f91d":"markdown","444d729a":"markdown","e9d2e968":"markdown","2b5e051a":"markdown","5057cfb9":"markdown","51a0db5f":"markdown","1183ab77":"markdown","fd73f37d":"markdown","ebeaa902":"markdown","b0192599":"markdown","42cf042e":"markdown","9107a323":"markdown","49a5b348":"markdown","a66ff069":"markdown","5966abeb":"markdown","e219a8e8":"markdown","915402ae":"markdown","762a6ed5":"markdown","9e14b071":"markdown","3f721225":"markdown","f0408d1e":"markdown","0d3a48c2":"markdown","c21f1185":"markdown","fdf3a777":"markdown","ffbb4111":"markdown","04ced1b7":"markdown","f5255a54":"markdown","fc9380c5":"markdown","bcfb2893":"markdown","85a7e16d":"markdown","f86100f8":"markdown","61201f51":"markdown","bf9ae2ce":"markdown","59112769":"markdown","357091d0":"markdown","3f1e915c":"markdown","cc25c596":"markdown","b531ea11":"markdown","0e865f6b":"markdown","f1676125":"markdown","b267316f":"markdown","2ccd2aa4":"markdown","0e78eb11":"markdown","d80abc30":"markdown","3e2a458a":"markdown","2a27f30b":"markdown","74f1270b":"markdown","f1fb40ce":"markdown","1cd4e73a":"markdown","d952166f":"markdown","9dd00ad4":"markdown","48f8b1a5":"markdown","f83f47d6":"markdown","137fe466":"markdown","9b9165ae":"markdown","b8566650":"markdown","312b0733":"markdown","8c104df3":"markdown","6eac4359":"markdown","1d30fa91":"markdown","c727c790":"markdown","1cb93191":"markdown","8030e51d":"markdown","a67ae09b":"markdown","d3410a8a":"markdown","b6ee8977":"markdown","33e68b91":"markdown","46dd7fdd":"markdown","b04312a8":"markdown","e547f4ba":"markdown","62b20e81":"markdown","d246b385":"markdown","0dccab14":"markdown","5c0d44d0":"markdown","2aa8017d":"markdown","2418d1ab":"markdown","f0b977e8":"markdown","2473b13e":"markdown","f66c4da4":"markdown","9385b2ea":"markdown","9ada3fe2":"markdown","055244e0":"markdown","f2536cab":"markdown","2a744324":"markdown","dd7e286d":"markdown","55dae5a9":"markdown","5b8b49fb":"markdown","fa6f8aff":"markdown","6b0d315e":"markdown","d4bb29a7":"markdown","66310ceb":"markdown","e69e184d":"markdown","94313139":"markdown","d6681101":"markdown","c65b8bc0":"markdown","5b50ed17":"markdown","0963869c":"markdown","efb1d4d5":"markdown","7f8429b8":"markdown","d976bb8e":"markdown","d07c110f":"markdown","79fb4ebc":"markdown","062ac77b":"markdown","9c53ef59":"markdown","41f03f60":"markdown","82884195":"markdown","e7242644":"markdown","3e273a57":"markdown","d6bb9f74":"markdown","d3aa4c28":"markdown","226e61b1":"markdown","480dd754":"markdown","b40c447a":"markdown","ffeb42e4":"markdown","5e51a5c7":"markdown","3118f858":"markdown"},"source":{"634c7002":"# Importing the necessary libraries\nimport math\nimport numpy                            as np                        # importing numpy library\nimport pandas                           as pd                        # importing pandas library\nimport seaborn                          as sns                       # For Data Visualization \nimport matplotlib.pyplot                as plt                       # Necessary module for plotting purpose\nimport warnings                                                      # importing warning library\n\n# add graphs into jupiter notebook\n%matplotlib inline                             \nwarnings.filterwarnings('ignore')                                    # for ignoring warnings in notebook\n\nimport statsmodels.api                  as sm                        # importing statsmodel api\nfrom sklearn.preprocessing              import MinMaxScaler          # importing MinMaxScaler for data scalling\nfrom sklearn.model_selection            import train_test_split      # For train-test split\n# getting methods for confusion matrix, F1 score, Accuracy Score\nfrom sklearn.metrics                    import confusion_matrix,f1_score,accuracy_score,classification_report,roc_curve,auc\nfrom sklearn.linear_model               import LogisticRegression    # For logistic Regression\nfrom sklearn.naive_bayes                import GaussianNB            # For Naive Bayes classifier\nfrom sklearn.neighbors                  import KNeighborsClassifier  # For K-NN Classifier\nfrom sklearn.svm                        import SVC                   # For support vector machine based classifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa455feb":"# loading \"Bank_Personal_Loan_Modelling.csv\" data into loadDataOrg df using pandas read_csv function\nloanDataOrg = pd.read_csv(\"..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv\")\n\n# printing top 5 rows of the loanDataOrg df\nloanDataOrg.head()","531cc300":"'''\nTo use columns of loanDataOrg df more conveniently following are some changes I have done\n   a. pushing target column i.e 'Personal Loan' to last column\n   b. converting all column names in lower case\n   c. replacing spaces in the all column names with '_'\n'''\n\nloanData = loanDataOrg.copy()                                               # creating a copy of loanDataOrg into loanData\n\ntargetCol = 'Personal Loan'                                                 # defining target column\ntargetColDf = loanData.pop(targetCol)                                       # popping target column from loanData df\nloanData.insert(len(loanData.columns),targetCol, targetColDf)               # inserting target column to last column\n\n# deleting variables that were used for changing column position of target column\ndel targetCol \ndel targetColDf\n\n# converting column names into lower case and replacing spaces in column names with '_'\nloanData.columns = [c.lower().replace(' ', '_') for c in loanData.columns]\n\n# to check the above printing top 5 rows\nloanData.head()","d049c245":"# Printing shape of the data i.e Rows and columns in the given dataset\nprint(\"\\033[1mThere are {0} Rows and {1} Columns in the given Dataset.\\033[0m\".format(loanData.shape[0],loanData.shape[1]))","07d39606":"# setting id column as index column\nloanData.set_index('id',inplace=True)","c1e6b46e":"# after setting column 'id' as index now we have less columns to confirm that printing number of rows and column once again\nprint(\"\\033[1mAfter setting 'id' column as index of the Dataset,\\033[0m now there are \\033[1m{0}\\033[0m Rows and \\033[1m{1}\\033[0m Columns in the given Dataset.\".format(loanData.shape[0],loanData.shape[1]))","208fdedc":"# printing top 5 rows once again to check\nloanData.head()","226c2f1b":"# printing datatypes of each columns of the dataset\n\nprint(\"\\033[1m*\"*100)\nprint(\"a.\\nColumn_Names        Data_Types\")\nprint(\"*\"*30)\nprint(\"\\033[0m{0}\\033[1m\".format(loanData.dtypes))\nprint(\"*\"*30)\nprint()\n\n# printing No of Columns having different Types of Datatype\n\nprint(\"*\"*100)\nprint(\"b.\\nNumber of Columns with each DataTypes as follows :\")\nprint(\"*\"*50)\nprint(\"Column_Names     No_of_Columns\\033[0m\")\nprint(\"*\"*30)\nprint(loanData.dtypes.value_counts())\nprint(\"\\033[1m*\"*30)\nprint(\"\\033[0m\")\n\n# printing Different Column Names of the dataset\n\nprint(\"\\033[1m*\"*100)\nprint(\"c.\\nEach Column Names of the dataset\")\nprint(\"*\"*80)\nprint(\"\\033[0m{0}\\033[1m\".format(loanData.columns))\nprint(\"*\"*80)\nprint(\"\\033[0m\")","6b0cb971":"# checking missing values in dataset for each attributes \/ columns \n\nprint(\"\\033[1m*\"*100)\nprint(\"Column_Name       No_of_Missing_Values\")\nprint(\"*\"*50)\nprint(\"\\033[0m{0}\".format(loanData.isnull().sum()))\nprint(\"\\033[1m*\"*50)\nprint()\n\n# checking if any duplicate rows available in the dataset\n\nprint(\"*\"*100)\nprint(\"Showing Duplicate rows if any in the dataset: \")\nprint(\"*\"*50)\nprint(\"\\033[0m{0}\".format(loanData[loanData.duplicated()]))\nprint(\"\\033[1m*\"*100)\nprint(\"\\033[0m\")","201b8627":"# Five point summary of each attribute\nloanData.describe().transpose()","b7da9fc7":"# 5 point summary of age column\nloanData.age.describe()","3d570724":"plt.figure(figsize=(10,5))                     # setting figure size with width = 10 and height = 5\nsns.histplot(loanData.age, kde=True)           # seaborn histplot to examine distribution of the age\nplt.title(\"Distribution of column : 'age'\")    # setting title of the figure","d1768fdb":"# plotting bar graph to see mean age with personal loan status i.e 0 = Not taken Loan and 1 = Taken Loan\nloanData.groupby('personal_loan').age.mean().plot(kind='bar')\nplt.title(\"Mean Age w.r.t Loan Status\")          # setting title of the figure","33c77c07":"plt.figure(figsize=(12,8))                         # setting figure size with width = 12 and height = 8\n# plotting histogram of age column where customers not opted for loan\nsns.histplot(loanData[loanData.personal_loan == 0].age,kde=False, bins=5, color='b', label='Personal Loan = 0 (No)')\n# plotting histogram of age column where customers opted for loan\nsns.histplot(loanData[loanData.personal_loan == 1].age,kde=False, bins=5, color='r', label='Personal Loan = 1 (Yes)')\nplt.legend()                                       # plotting legend on the figure\nplt.title(\"Distribution of column : 'age'\")        # setting title of the figure","e1e35c65":"bins = [20,30,40,50,60,70]                                         # defining age bins\n# defining labels of age groups as per bins defined as above\nageGroup = ['Age : 20-30', 'Age : 30-40', 'Age : 40-50', 'Age : 50-60', 'Age : 60-70']\nloanDataAgeBin = pd.cut(loanData.age,bins,labels=ageGroup)         # segmenting data as per bins defined\n\n# putting into pandas crosstab and applying lambda function to take percentage and assigning to ageGroupCol variable\nageGroupCol = pd.crosstab(loanDataAgeBin,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(ageGroupCol)                                                 # printing above crosstab\n\n# plotting a stacked bar chart to show loan status for different age group\nageGroupCol.div(ageGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Age group\")                  # setting title of the figure","a75166e6":"loanData.experience.describe()","5411257c":"plt.figure(figsize=(12,10))\nsns.heatmap(loanData.corr(),annot=True)","45c726cb":"# looping through distinct negative values of 'experience' column\nfor oddExp in loanData[loanData.experience <0].experience.unique():\n    # listing all ages of the negative values of 'experience' column\n    ageForOddExp = loanData[loanData.experience == oddExp].age.value_counts().index.tolist()\n    # looping through locations where negative values exist on 'experience' column\n    for i in loanData[loanData.experience == oddExp].experience.index.tolist():\n        # replacing mean experience of similar 'age' group for negative values in 'experience' column\n        loanData.loc[i,'experience'] = loanData[(loanData.age.isin(ageForOddExp)) & (loanData.experience > 0)].experience.mean()\n    print(\"{0} values in experience column is replaced with mean of expirance of same age group.\\n\".format(oddExp))","7976efbb":"# checking for any negative value in 'experience' column\nloanData[loanData.experience < 0].experience.value_counts()","88c16415":"loanData.experience.describe()","6bac16b2":"plt.figure(figsize=(10,5))                         # setting figure size with width = 10 and height = 5\nsns.histplot(loanData.experience, kde=True)        # seaborn histplot to examine distribution of the experience\nplt.title(\"Distribution of column : 'experience'\") # setting title of the figure","cf3ff20d":"# plotting bar graph to see mean experience with personal loan status i.e 0 = Not taken Loan and 1 = Taken Loan\nloanData.groupby('personal_loan').experience.mean().plot(kind='bar')\nplt.title(\"Mean Experience w.r.t Loan Status\")      # setting title of the figure","355af1da":"plt.figure(figsize=(12,8))                                # setting figure size with width = 12 and height = 8\n# plotting histogram of experience column where customers have not opted for loan\nsns.histplot(loanData[loanData.personal_loan == 0].experience,kde=False, bins=5, color='b', label='Personal Loan = 0 (No)')\n# plotting histogram of experience column where customers opted for loan\nsns.histplot(loanData[loanData.personal_loan == 1].experience,kde=False, bins=5, color='r', label='Personal Loan = 1 (Yes)')\nplt.legend()                                              # plotting legend on the figure\nplt.title(\"Distribution of column : 'experience'\")        # setting title of the figure","005b1d64":"bins = [0,10,20,30,40,50]                                          # defining experience bins,\n# defining labels of experience groups as per bins defined as above\nexpGroup = ['Experience : 0-10', 'Experience : 10-20', 'Experience : 20-30', 'Experience : 30-40', 'Experience : 40-50']\nloanDataExpBin = pd.cut(loanData.experience,bins,labels=expGroup)  # segmenting data as per bins defined\n\n# putting into pandas crosstab and applying lambda function to take percentage and assigning to expGroupCol variable\nexpGroupCol = pd.crosstab(loanDataExpBin,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(expGroupCol)                                                 # printing above crosstab\n\n# ploting a stacked bar chart to show loan status for different experience group\nexpGroupCol.div(expGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Experience group\")           # setting title of the figure","b4eaaeb9":"loanData.income.describe()","6dd7804c":"Q1 = loanData.income.quantile(0.25)        # evaluating lower \/ first quartile\nQ3 = loanData.income.quantile(0.75)        # evaluating upper \/ third quartile\nIQR = Q3 - Q1                              # evaluating Inter Quartile Range i.e IQR\n'''\nfinding outliers which are mild outliers (Lower quartile - 1.5 times IQR) or\nextreme outliers (Upper quartile + 1.5 times IQR)\n'''\noutliers = loanData[((loanData.income < (Q1 - 1.5 * IQR)) |(loanData.income > (Q3 + 1.5 * IQR)))].income\nplt.figure(figsize=(15,3))                 # setting figure size with width = 15 and height = 3\nprint(\"*\"*30)\nprint(\"\\033[1mBoxplot of income column : \\033[0m\")\nprint(\"*\"*30)\nax = sns.boxplot(x=loanData.income)        # seaborn boxplot to examine outliers of the feature\n# printing mean, median and IQR for the feature\nprint(\"\\033[1mFeature {0} : Mean = {1}, Median = {2} and Inter-Quartile-Range (IQR) = {3}\\033[0m\"\n      .format('income',round(np.mean(loanData.income),3),round(np.median(loanData.income),3),round(IQR,3))\n     )\nif(outliers.shape[0] == 0):                # comparing if number of outlier is zero\n    print(\"There are \\033[1mno outliers\\033[0m in \\033[1m'income'\\033[0m feature.\")\nelse:                                      # if the above condition is false i.e number of outlier is not zero\n    # printing No of outliers, percentage of the data points are outliers and the values of the outliers\n    print(\"There are \\033[1m{0} outliers\\033[0m ({1} % of the data points) in \\033[1m{2}\\033[0m feature and the values are \\033[1m{3}\\033[0m\"\n    .format(outliers.shape[0],round(((outliers.shape[0]\/loanData.income.shape[0])*100),3),'income',outliers.tolist()))\nprint(\"*\"*125)","ea0aa794":"plt.figure(figsize=(10,5))                     # setting figure size with width = 10 and height = 5\nsns.histplot(loanData.income, kde=True)        # seaborn histplot to examine distribution of the income\nplt.title(\"Distribution of column : 'income'\") # setting title of the figure","a9e54cb9":"loanData.income.skew()","744fa3f8":"# plotting bar graph to see mean experience with personal loan status i.e 0 = Not taken Loan and 1 = Taken Loan\nloanData.groupby('personal_loan').income.mean().plot(kind='bar')\nplt.title(\"Mean Income w.r.t Loan Status\")        # setting title of the figure","48b7eb3d":"plt.figure(figsize=(12,8))                                # setting figure size with width = 12 and height = 8\n# plotting histogram of income column where customers not opted for loan\nsns.histplot(loanData[loanData.personal_loan == 0].income,kde=False, bins=5, color='b', label='Personal Loan = 0 (No)')\n# plotting histogram of income column where customers opted for loan\nsns.histplot(loanData[loanData.personal_loan == 1].income,kde=False, bins=5, color='r', label='Personal Loan = 1 (Yes)')\nplt.legend()                                              # plotting legend on the figure\nplt.title(\"Distribution of column : 'income'\")            # setting title of the figure","a568ec74":"bins = [0,50,100,150,200,250]                                         # defining income bins,\n# defining labels of income groups as per bins defined as above\nincGroup = ['Income : 0-50', 'Income : 50-100', 'Income : 100-150', 'Income : 150-200', 'Income : 200-250']\nloanDataIncBin = pd.cut(loanData.income,bins,labels=incGroup)         # segmenting data as per bins defined\n\n# putting into pandas crosstab and applying lambda function to take percentage and assigning to incGroupCol variable\nincGroupCol = pd.crosstab(loanDataIncBin,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(incGroupCol)                                                    # printing above crosstab\n\n# ploting a stacked bar chart to show loan status for different income group\nincGroupCol.div(incGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Income group\")                  # setting title of the figure","7b9c1994":"loanData.zip_code.value_counts()","0a13d4e1":"pd.crosstab(loanData.zip_code,loanData.personal_loan, values=loanData.personal_loan, aggfunc='count').sort_values([1],ascending=False)","25d94a9c":"print(\"*\"*40)\nprint(\"loanData dataframe before droping zip_code :\")\nprint(\"*\"*80)\nprint(loanData.head())\nprint(\"*\"*80)\nprint()\nloanData.drop('zip_code',axis=1,inplace=True)\nprint(\"*\"*40)\nprint(\"loanData dataframe after droping zip_code :\")\nprint(\"*\"*80)\nprint(loanData.head())\nprint(\"*\"*80)","41d17cdd":"plt.figure(figsize=(10,5))                                  # setting figure size with width = 10 and height = 5\nax = sns.catplot(x='family', kind=\"count\", data=loanData)   # seaborn count catplot to examine distribution of the family\nplt.title(\"Distribution of column : 'family'\")              # setting title of the figure","b4daa391":"plt.figure(figsize=(10,5))                        # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the family\nax = sns.catplot(x='family',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'family'\")    # setting title of the figure","4870ca95":"print(\"*\"*70)\nprint(\"\\033[1mNo of Customers taken loan or not w.r.t Family member :\\033[0m\")\nprint(\"*\"*70)\nfamGroupCol = pd.crosstab(loanData.family,loanData.personal_loan)\nprint(famGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Customers taken loan or not w.r.t Family member :\\033[0m\")\nprint(\"*\"*70)\nfamGroupPer = pd.crosstab(loanData.family,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(famGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# plotting a stacked bar chart to show loan status for different no of family member group\nfamGroupCol.div(famGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Family Member\")              # setting title of the figure","c651af5d":"loanData.ccavg.describe()","67f77155":"Q1 = loanData.ccavg.quantile(0.25)        # evaluating lower \/ first quartile\nQ3 = loanData.ccavg.quantile(0.75)        # evaluating upper \/ third quartile\nIQR = Q3 - Q1                             # evaluating Inter Quartile Range i.e IQR\n'''\nfinding outliers which are mild outliers (Lower quartile - 1.5 times IQR) or\nextreme outliers (Upper quartile + 1.5 times IQR)\n'''\noutliers = loanData[((loanData.ccavg < (Q1 - 1.5 * IQR)) |(loanData.ccavg > (Q3 + 1.5 * IQR)))].ccavg\nplt.figure(figsize=(15,3))                # setting figure size with width = 15 and height = 3\nprint(\"*\"*30)\nprint(\"\\033[1mBoxplot of ccavg column : \\033[0m\")\nprint(\"*\"*30)\nax = sns.boxplot(x=loanData.ccavg)        # seaborn boxplot to examine outliers of the feature\n# printing mean, median and IQR for the feature\nprint(\"\\033[1mFeature {0} : Mean = {1}, Median = {2} and Inter-Quartile-Range (IQR) = {3}\\033[0m\"\n      .format('ccavg',round(np.mean(loanData.ccavg),3),round(np.median(loanData.ccavg),3),round(IQR,3))\n     )\nif(outliers.shape[0] == 0):                # comparing if number of outlier is zero\n    print(\"There are \\033[1mno outliers\\033[0m in \\033[1m'ccavg'\\033[0m feature.\")\nelse:                                      # if the above condition is false i.e number of outlier is not zero\n    # printing No of outliers, percentage of the data points are outliers and the values of the outliers\n    print(\"There are \\033[1m{0} outliers\\033[0m ({1} % of the data points) in \\033[1m{2}\\033[0m feature and the values are \\033[1m{3}\\033[0m\"\n    .format(outliers.shape[0],round(((outliers.shape[0]\/loanData.ccavg.shape[0])*100),3),'income',outliers.tolist()))\nprint(\"*\"*125)","e94a89a6":"plt.figure(figsize=(10,5))                     # setting figure size with width = 10 and height = 5\nsns.histplot(loanData.ccavg, kde=True)         # seaborn histplot to examine distribution of the income\nplt.title(\"Distribution of column : 'ccavg'\")  # setting title of the figure","5be506a4":"loanData.ccavg.skew()","039496d5":"# plotting bar graph to see mean ccavg with personal loan status i.e 0 = Not taken Loan and 1 = Taken Loan\nloanData.groupby('personal_loan').ccavg.mean().plot(kind='bar')\nplt.title(\"Mean Credit Card Spending (annual) w.r.t Loan Status\")  # setting title of the figure","67973c76":"plt.figure(figsize=(12,8))                                # setting figure size with width = 12 and height = 8\n# plotting histogram of ccavg column where customers not opted for loan\nsns.histplot(loanData[loanData.personal_loan == 0].ccavg,kde=False, bins=5, color='b', label='Personal Loan = 0 (No)')\n# plotting histogram of ccavg column where customers opted for loan\nsns.histplot(loanData[loanData.personal_loan == 1].ccavg,kde=False, bins=5, color='r', label='Personal Loan = 1 (Yes)')\nplt.legend()                                              # plotting legend on the figure\nplt.title(\"Distribution of column : 'ccavg'\")             # setting title of the figure","f1dd67b1":"bins = [0,2,4,6,8,10]                                              # defining ccavg bins,\n# defining labels of ccavg groups as per bins defined as above\nccavgGroup = ['CcAvg : 0-2', 'CcAvg : 2-4', 'CcAvg : 4-6', 'CcAvg : 6-8', 'CcAvg : 8-10']\nloanDataCcavgBin = pd.cut(loanData.ccavg,bins,labels=ccavgGroup)   # segmenting data as per bins defined\n\n# putting into pandas crosstab and applying lambda function to take percentage and assigning to ccavgGroupCol variable\nccavgGroupCol = pd.crosstab(loanDataCcavgBin,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(ccavgGroupCol)                                               # printing above crosstab\n\n# plotting a stacked bar chart to show loan status for different ccavg group\nccavgGroupCol.div(ccavgGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different CcAvg group\")                # setting title of the figure","031b2269":"plt.figure(figsize=(10,5))                                     # setting figure size with width = 10 and height = 5\nax = sns.catplot(x='education', kind=\"count\", data=loanData)   # seaborn count catplot to examine distribution of the family\nplt.title(\"Distribution of column : 'education'\")              # setting title of the figure","4188a957":"plt.figure(figsize=(10,5))                                 # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the education\nax = sns.catplot(x='education',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'eduation'\")           # setting title of the figure","191ed729":"print(\"*\"*70)\nprint(\"\\033[1mNo of Cutomers taken loan or not w.r.t Education :\\033[0m\")\nprint(\"*\"*70)\neduGroupCol = pd.crosstab(loanData.education,loanData.personal_loan)\nprint(eduGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Cutomers taken loan or not w.r.t Education :\\033[0m\")\nprint(\"*\"*70)\neduGroupPer = pd.crosstab(loanData.education,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(eduGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# ploting a stacked bar chart to show loan status for different Educational background\neduGroupCol.div(eduGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Educational Background\")     # setting title of the figure","9bd28de7":"loanData.mortgage.describe()","e9a5529d":"Q1 = loanData.mortgage.quantile(0.25)        # evaluating lower \/ first quartile\nQ3 = loanData.mortgage.quantile(0.75)        # evaluating upper \/ third quartile\nIQR = Q3 - Q1                                # evaluating Inter Quartile Range i.e IQR\n'''\nfinding outliers which are mild outliers (Lower quartile - 1.5 times IQR) or\nextreme outliers (Upper quartile + 1.5 times IQR)\n'''\noutliers = loanData[((loanData.mortgage < (Q1 - 1.5 * IQR)) |(loanData.mortgage > (Q3 + 1.5 * IQR)))].mortgage\nplt.figure(figsize=(15,3))                   # setting figure size with width = 15 and height = 3\nprint(\"*\"*30)\nprint(\"\\033[1mBoxplot of mortgage column : \\033[0m\")\nprint(\"*\"*30)\nax = sns.boxplot(x=loanData.mortgage)        # seaborn boxplot to examine outliers of the feature\n# printing mean, median and IQR for the feature\nprint(\"\\033[1mFeature {0} : Mean = {1}, Median = {2} and Inter-Quartile-Range (IQR) = {3}\\033[0m\"\n      .format('mortgage',round(np.mean(loanData.mortgage),3),round(np.median(loanData.mortgage),3),round(IQR,3))\n     )\nif(outliers.shape[0] == 0):                  # comparing if number of outlier is zero\n    print(\"There are \\033[1mno outliers\\033[0m in \\033[1m'mortgage'\\033[0m feature.\")\nelse:                                        # if the above condition is false i.e number of outlier is not zero\n    # printing No of outliers, percentage of the data points are outliers and the values of the outliers\n    print(\"There are \\033[1m{0} outliers\\033[0m ({1} % of the data points) in \\033[1m{2}\\033[0m feature and the values are \\033[1m{3}\\033[0m\"\n    .format(outliers.shape[0],round(((outliers.shape[0]\/loanData.mortgage.shape[0])*100),3),'mortgage',outliers.tolist()))\nprint(\"*\"*125)","ec158ff5":"plt.figure(figsize=(10,5))                       # setting figure size with width = 10 and height = 5\nsns.histplot(loanData.mortgage, kde=True)        # seaborn histplot to examine distribution of the mortgage\nplt.title(\"Distribution of column : 'mortgage'\") # setting title of the figure","aa33a2d6":"loanData.mortgage.skew()","873025cc":"# plotting bar graph to see mean mortgage with personal loan status i.e 0 = Not taken Loan and 1 = Taken Loan\nloanData.groupby('personal_loan').mortgage.mean().plot(kind='bar')\nplt.title(\"Mean of mortgage w.r.t Loan Status\")     # setting title of the figure","631d11eb":"plt.figure(figsize=(12,8))                                # setting figure size with width = 12 and height = 8\n# plotting histogram of mortgage column where customers not opted for loan\nsns.histplot(loanData[loanData.personal_loan == 0].mortgage,kde=False, bins=5, color='b', label='Personal Loan = 0 (No)')\n# plotting histogram of mortgage column where customers opted for loan\nsns.histplot(loanData[loanData.personal_loan == 1].mortgage,kde=False, bins=5, color='r', label='Personal Loan = 1 (Yes)')\nplt.legend()                                              # plotting legend on the figure\nplt.title(\"Distribution of column : 'ccavg'\")             # setting title of the figure","e0e4c5db":"bins = [0,100,200,300,400,500,600]                                         # defining ccavg bins,\n# defining labels of mortgage groups as per bins defined as above\nmortgageGroup = ['Mortgage : 0-100', 'Mortgage : 100-200', 'Mortgage : 200-300', 'Mortgage : 300-400', 'Mortgage : 400-500', 'Mortgage : 500-600']\nloanDataMortgageBin = pd.cut(loanData.mortgage,bins,labels=mortgageGroup)  # segmenting data as per bins defined\n\n# putting into pandas crosstab and applying lambda function to take percentage and assigning to mortGroupCol variable\nmortgageGroupCol = pd.crosstab(loanDataMortgageBin,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(mortgageGroupCol)                                                    # printing above crosstab\n\n# ploting a stacked bar chart to show loan status for different mortgage group\nmortgageGroupCol.div(mortgageGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different Mortgage group\")                     # setting title of the figure","7b50b5a5":"plt.figure(figsize=(10,5))                                   # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the securities_account\nax = sns.catplot(x='securities_account', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'securities_account'\")   # setting title of the figure","d8cba6bb":"plt.figure(figsize=(10,5))                                   # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the securities_account\nax = sns.catplot(x='securities_account',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'securities_account'\")   # setting title of the figure","0ec86c6d":"print(\"*\"*70)\nprint(\"\\033[1mNo of Cutomers taken loan or not w.r.t securities_account :\\033[0m\")\nprint(\"*\"*70)\nsecGroupCol = pd.crosstab(loanData.securities_account,loanData.personal_loan)\nprint(secGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Cutomers taken loan or not w.r.t securities_account :\\033[0m\")\nprint(\"*\"*70)\nsecGroupPer = pd.crosstab(loanData.securities_account,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(secGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# ploting a stacked bar chart to show loan status w.r.t securities_account\nsecGroupCol.div(secGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different securities_account\")         # setting title of the figure","269b8eb3":"plt.figure(figsize=(10,5))                           # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the cd_account\nax = sns.catplot(x='cd_account', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'cd_account'\")   # setting title of the figure","a340c7ba":"plt.figure(figsize=(10,5))                             # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the cd_account\nax = sns.catplot(x='cd_account',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'cd_account'\")     # setting title of the figure","d64cbf7a":"print(\"*\"*70)\nprint(\"\\033[1mNo of Customers taken loan or not w.r.t cd_account :\\033[0m\")\nprint(\"*\"*70)\ncdGroupCol = pd.crosstab(loanData.cd_account,loanData.personal_loan)\nprint(cdGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Customers taken loan or not w.r.t cd_account :\\033[0m\")\nprint(\"*\"*70)\ncdGroupPer = pd.crosstab(loanData.cd_account,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(cdGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# ploting a stacked bar chart to show loan status w.r.t cd_account\ncdGroupCol.div(cdGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different cd_account\")                # setting title of the figure","fd8e285c":"plt.figure(figsize=(10,5))                           # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the online\nax = sns.catplot(x='online', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'online'\")       # setting title of the figure","cc8ebb5c":"plt.figure(figsize=(10,5))                            # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the online\nax = sns.catplot(x='online',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'online'\")        # setting title of the figure","337a7e0b":"print(\"*\"*70)\nprint(\"\\033[1mNo of Cutomers taken loan or not w.r.t online :\\033[0m\")\nprint(\"*\"*70)\nibGroupCol = pd.crosstab(loanData.online,loanData.personal_loan)\nprint(ibGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Cutomers taken loan or not w.r.t online :\\033[0m\")\nprint(\"*\"*70)\nibGroupPer = pd.crosstab(loanData.online,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(ibGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# ploting a stacked bar chart to show loan status w.r.t online\nibGroupCol.div(ibGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different online\")                    # setting title of the figure","b12745da":"plt.figure(figsize=(10,5))                            # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the creditcard\nax = sns.catplot(x='creditcard', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'creditcard'\")    # setting title of the figure","cfea2015":"plt.figure(figsize=(10,5))                             # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the creditcard\nax = sns.catplot(x='creditcard',hue='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'creditcard'\")     # setting title of the figure","04d22625":"print(\"*\"*70)\nprint(\"\\033[1mNo of Cutomers taken loan or not w.r.t creditcard :\\033[0m\")\nprint(\"*\"*70)\nccGroupCol = pd.crosstab(loanData.creditcard,loanData.personal_loan)\nprint(ccGroupCol)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint()\nprint(\"*\"*70)\nprint(\"\\033[1mPercentage of Cutomers taken loan or not w.r.t creditcard :\\033[0m\")\nprint(\"*\"*70)\nccGroupPer = pd.crosstab(loanData.creditcard,loanData.personal_loan).apply(lambda r: r\/r.sum()*100, axis=1)\nprint(ccGroupPer)                                                 # printing above crosstab\nprint(\"*\"*40)\nprint(\"*\"*100)\n# ploting a stacked bar chart to show loan status w.r.t creditcard\nccGroupCol.div(ccGroupCol.sum(1).astype(float), axis=0).plot(kind='bar',stacked=True)\nplt.title(\"Loan Status with different creditcard\")                # setting title of the figure","0938ef04":"plt.figure(figsize=(10,5))                                 # setting figure size with width = 10 and height = 5\n# seaborn count catplot to examine distribution of the personal_loan\nax = sns.catplot(x='personal_loan', kind=\"count\", data=loanData)\nplt.title(\"Distribution of column : 'personal_loan'\")      # setting title of the figure\ny = []                                                     # creating a null or empty array\nfor val in range(loanData.personal_loan.nunique()):        # looping for number of unique values in the personal_loan\n    # appending count of each unique values from personal_loan to array y\n    y.append(loanData.groupby(loanData.personal_loan,sort=False)['personal_loan'].count()[val])\nfor i, v in enumerate(y):                                  # looping count of each unique value in the personal_loan\n    # including count of each unique values in the plot \n    plt.annotate(str(v), xy=(i,float(v)), xytext=(i-0.1, v+40), color='black', fontweight='bold')","5dc6bb61":"plt.figure(figsize=(5,5))                               # setting figure size with width = 10 and height = 5\n# seaborn pie chart to examine distribution of the personal loan\nloanData.groupby(['personal_loan']).personal_loan.count().plot(kind='pie',labels=['No Personal Loan : 0','Personal Loan : 1'],\n                                                               startangle=90, autopct='%1.1f%%')\nplt.title(\"Distribution of column : 'personal_loan'\")   # setting title of the figure","f851e95a":"sns.pairplot(loanData,hue='personal_loan',diag_kind='hist')","d6f414ed":"# checking correlation of independent variable with dependent variable i.e personal_loan\nloanData.corr().personal_loan.sort_values(ascending=False)","bd0121f8":"# taking the columns for deriving signifiance to predict target attribute\nsigniLoanData = loanData.loc[:,['personal_loan', 'income', 'ccavg', 'cd_account', 'mortgage', 'education', 'family', 'securities_account', 'age','experience']]\nsigniLoanData['intercept'] = 1               # setting intercept to 1\n\n# Significance test for numerical columns\nlogit = sm.Logit(signiLoanData['personal_loan'], signiLoanData[['intercept', 'income', 'ccavg', 'mortgage', 'age']]).fit()\nlogit.summary()","baaa3110":"# Let see the statistical significance of ordinal categorical variables Family and Education\nlogit = sm.Logit(signiLoanData['personal_loan'], signiLoanData[['intercept','family', 'education']]).fit()\nlogit.summary()","1b9fb94a":"logit = sm.Logit(signiLoanData['personal_loan'], signiLoanData[['intercept','cd_account', 'securities_account']]).fit()\nlogit.summary()","0159f44b":"'''\nWe will drop age from the dataset as 'age' has no significance effect on dependent variable i.e personal_loan\nand also 'age' and 'experience' both are highly correlated to each other. \nNote : id column is already shifted to index.\n'''\nX = loanData.iloc[:,1:-1]\ny = loanData.personal_loan\n\n# splitting the data into training and test set in the ratio of 70:30 respectively\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=7)","0f84ff21":"# Let us scale train as well as test data using MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","8635f2f3":"logRegModel = LogisticRegression(solver='liblinear')          # creating Logistic Regression model using constructor\nlogRegModel.fit(X_train,y_train)                              # fit the model to training set\ny_pred = logRegModel.predict(X_test)                          # predict the test data to get y_pred\n\nlrAccScore = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the Logistic Regression model is {} %\".format(lrAccScore*100))\nprint(\"-\"*70)\nprint()\nlrF1Score = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the Logistic Regression the model is {} %\".format(lrF1Score*100))\nprint(\"-\"*70)\nprint()\nlrConMatrix = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the Logistic Regression is: \\n\",lrConMatrix)\nprint(\"-\"*70)\nprint()\nlrClassReport = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for logistic regression is: \\n\",lrClassReport)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nlrProb = logRegModel.predict_proba(X_test)\nlrProbFpr, lrProbTpr, lrProbThreshold = roc_curve(y_test,lrProb[:,1])\nlrProbAUC = auc(lrProbFpr,lrProbTpr)\nprint(\"Area under the ROC curve for Logistic Regression is :{}\".format(round(lrProbAUC,3)))\nprint(\"-\"*70)","1ce501f5":"logRegModelScaled = LogisticRegression()                            # creating Logistic Regression model using constructor\nlogRegModelScaled.fit(X_train_scaled,y_train)                       # fit the model to training set\ny_pred = logRegModelScaled.predict(X_test_scaled)                   # predict the test data to get y_pred\n\nlrAccScoreScaled = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the Logistic Regression model is {} %\".format(lrAccScoreScaled*100))\nprint(\"-\"*70)\nprint()\nlrF1ScoreScaled = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the Logistic Regression the model is {} %\".format(lrF1ScoreScaled*100))\nprint(\"-\"*70)\nprint()\nlrConMatrixScaled = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the Logistic Regression is: \\n\",lrConMatrixScaled)\nprint(\"-\"*70)\nprint()\nlrClassReportScaled = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for logistic regression is: \\n\",lrClassReportScaled)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nlrProbScaled = logRegModelScaled.predict_proba(X_test_scaled)\nlrProbFprScaled, lrProbTprScaled, lrProbThresholdScaled = roc_curve(y_test,lrProbScaled[:,1])\nlrProbAUCScaled = auc(lrProbFprScaled,lrProbTprScaled)\nprint(\"Area under the ROC curve for Logistic Regression is :{}\".format(round(lrProbAUCScaled,3)))\nprint(\"-\"*70)","6ca1d884":"nbModel = GaussianNB()                                        # creating Gaussian Naive Bayes model using constructor\nnbModel.fit(X_train,y_train)                                  # fit the model to training set\ny_pred = nbModel.predict(X_test)                              # predict the test data to get y_pred\n\nnbAccScore = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the Naive Bayes Classifier model is {} %\".format(nbAccScore*100))\nprint(\"-\"*70)\nprint()\nnbF1Score = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the Naive Bayes Classifier the model is {} %\".format(nbF1Score*100))\nprint(\"-\"*70)\nprint()\nnbConMatrix = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the Naive Bayes Classifier is: \\n\",nbConMatrix)\nprint(\"-\"*70)\nprint()\nnbClassReport = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for Naive Bayes Classifier is: \\n\",nbClassReport)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nnbProb = nbModel.predict_proba(X_test)\nnbProbFpr, nbProbTpr, nbProbThreshold = roc_curve(y_test,nbProb[:,1])\nnbProbAUC = auc(nbProbFpr,nbProbTpr)\nprint(\"Area under the ROC curve for Naive Bayes Classifier is :{}\".format(round(nbProbAUC,3)))\nprint(\"-\"*70)","d59f303b":"nbModelScaled = GaussianNB()                                        # creating Gaussian Naive Bayes model using constructor\nnbModelScaled.fit(X_train_scaled,y_train)                           # fit the model to training set\ny_pred = nbModelScaled.predict(X_test_scaled)                       # predict the test data to get y_pred\n\nnbAccScoreScaled = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the Naive Bayes Classifier model is {} %\".format(nbAccScoreScaled*100))\nprint(\"-\"*70)\nprint()\nnbF1ScoreScaled = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the Naive Bayes Classifier the model is {} %\".format(nbF1ScoreScaled*100))\nprint(\"-\"*70)\nprint()\nnbConMatrixScaled = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the Naive Bayes Classifier is: \\n\",nbConMatrixScaled)\nprint(\"-\"*70)\nprint()\nnbClassReportScaled = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for Naive Bayes Classifier is: \\n\",nbClassReportScaled)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nnbProbScaled = nbModelScaled.predict_proba(X_test_scaled)\nnbProbFprScaled, nbProbTprScaled, nbProbThresholdScaled = roc_curve(y_test,nbProbScaled[:,1])\nnbProbAUCScaled = auc(nbProbFprScaled,nbProbTprScaled)\nprint(\"Area under the ROC curve for Logistic Regression is :{}\".format(round(nbProbAUCScaled,3)))\nprint(\"-\"*70)","5a229542":"knnModel = KNeighborsClassifier()                              # creating K-NN model using constructor\nknnModel.fit(X_train,y_train)                                  # fit the model to training set\ny_pred = knnModel.predict(X_test)                              # predict the test data to get y_pred\n\nknnAccScore = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the K-NN Classifier model is {} %\".format(knnAccScore*100))\nprint(\"-\"*70)\nprint()\nknnF1Score = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the K-NN Classifier the model is {} %\".format(knnF1Score*100))\nprint(\"-\"*70)\nprint()\nknnConMatrix = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the K-NN Classifier is: \\n\",knnConMatrix)\nprint(\"-\"*70)\nprint()\nknnClassReport = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for K-NN Classifier is: \\n\",knnClassReport)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nknnProb = knnModel.predict_proba(X_test)\nknnProbFpr, knnProbTpr, knnProbThreshold = roc_curve(y_test,knnProb[:,1])\nknnProbAUC = auc(knnProbFpr,knnProbTpr)\nprint(\"Area under the ROC curve for K-NN Classifier is :{}\".format(round(knnProbAUC,3)))\nprint(\"-\"*70)","e3e4be2f":"# KNN Accuracy for neighbors = 1,3,...99\nknnAcc=[]\nknnF1 = []\nfor i in range(1,100,2):\n    print(\"Calculating the K-NN classifier accuracy for {} neighbors.\".format(i))\n    knnModel = KNeighborsClassifier(n_neighbors=i)              # Calling default constructor\n    knnModel.fit(X_train,y_train)                               # fit the model to training set\n    y_pred = knnModel.predict(X_test)                           # Predict the test data to get y_pred\n    knnAccScore = accuracy_score(y_test,y_pred)                 # get accuracy of model\n    knnAcc.append(knnAccScore*100)\n    # get F1-score of model\n    knnF1Score = f1_score(y_test,y_pred) \n    knnF1.append(knnF1Score*100)\ndfKnn = pd.DataFrame({'n_neighbors':list(range(1,100,2)), 'Accuracy':knnAcc,'f1-score':knnF1}) \ndfKnn","e607a9d1":"knnModelScaled = KNeighborsClassifier()                              # creating K-NN model using constructor\nknnModelScaled.fit(X_train_scaled,y_train)                           # fit the model to training set\ny_pred = knnModelScaled.predict(X_test_scaled)                       # predict the test data to get y_pred\n\nknnAccScoreScaled = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the K-NN Classifier model is {} %\".format(knnAccScoreScaled*100))\nprint(\"-\"*70)\nprint()\nknnF1ScoreScaled = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the K-NN Classifier the model is {} %\".format(knnF1ScoreScaled*100))\nprint(\"-\"*70)\nprint()\nknnConMatrixScaled = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the K-NN Classifier is: \\n\",knnConMatrixScaled)\nprint(\"-\"*70)\nprint()\nknnClassReportScaled = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for K-NN Classifier is: \\n\",knnClassReportScaled)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nknnProbScaled = knnModelScaled.predict_proba(X_test_scaled)\nknnProbFprScaled, knnProbTprScaled, knnProbThresholdScaled = roc_curve(y_test,knnProbScaled[:,1])\nknnProbAUCScaled = auc(knnProbFprScaled,knnProbTprScaled)\nprint(\"Area under the ROC curve for K-NN Classifier is :{}\".format(round(knnProbAUCScaled,3)))\nprint(\"-\"*70)","4a599746":"# KNN Accuracy for neighbors = 1,3,...99\nknnAcc=[]\nknnF1 = []\nfor i in range(1,100,2):\n    print(\"Calculating the K-NN classifier accuracy for {} neighbors.\".format(i))\n    knnModel = KNeighborsClassifier(n_neighbors=i)              # Calling default constructor\n    knnModel.fit(X_train_scaled,y_train)                        # fit the model to training set\n    y_pred = knnModel.predict(X_test_scaled)                    # Predict the test data to get y_pred\n    knnAccScore = accuracy_score(y_test,y_pred)                 # get accuracy of model\n    knnAcc.append(knnAccScore*100)\n    # get F1-score of model\n    knnF1Score = f1_score(y_test,y_pred) \n    knnF1.append(knnF1Score*100)\ndfKnn = pd.DataFrame({'n_neighbors':list(range(1,100,2)), 'Accuracy':knnAcc,'f1-score':knnF1}) \ndfKnn","cce7c99f":"dfComp = pd.DataFrame({'Classification Algorithm':['Logistic Regression', 'Naive Bayes', 'K-Nearest Neighbor'],\n                       'Accuracy (%) on Unscaled':[lrAccScore*100, nbAccScore*100, knnAccScore*100],\n                       'f1-score (%) on Unscaled':[lrF1Score*100, nbF1Score*100, knnF1Score*100],\n                       'Area under ROC on Unscaled':[lrProbAUC, nbProbAUC, knnProbAUC],\n                       'Accuracy (%) on Scaled':[lrAccScoreScaled*100, nbAccScoreScaled*100, knnAccScoreScaled*100],\n                       'f1-score (%) on Scaled':[lrF1ScoreScaled*100, nbF1ScoreScaled*100, knnF1ScoreScaled*100],\n                       'Area under ROC on Scaled':[lrProbAUCScaled, nbProbAUCScaled, knnProbAUCScaled] \n                      })\n\nprint(\"Following table shows comparison of the classification algorithms: \")\ndfComp","613d8d46":"# Logistic Regression confusion matrix on unscaled dataset\ndfConMat = pd.DataFrame(lrConMatrix, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","07724fb0":"# Logistic Regression confusion matrix on scaled dataset\ndfConMat = pd.DataFrame(lrConMatrixScaled, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","72ba7863":"# Naive Bayes Classifier confusion matrix on unscaled dataset\ndfConMat = pd.DataFrame(nbConMatrix, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","e79a2d62":"# Naive Bayes Classifier confusion matrix on scaled dataset\ndfConMat = pd.DataFrame(nbConMatrixScaled, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","8d566cf0":"# K-NN Classifier confusion matrix on unscaled dataset\ndfConMat = pd.DataFrame(knnConMatrix, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","7a6c8a6c":"# K-NN Classifier confusion matrix on scaled dataset\ndfConMat = pd.DataFrame(knnConMatrixScaled, index = [i for i in [\"No\",\"Yes\"]],columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(dfConMat, annot=True ,fmt='g')","73345426":"svmModel = SVC()                                               # creating K-NN model using constructor\nsvmModel.fit(X_train_scaled,y_train)                           # fit the model to training set\ny_pred = svmModel.predict(X_test_scaled)                       # predict the test data to get y_pred\n\nsvmAccScore = accuracy_score(y_test,y_pred)                    # get accuracy of model\nprint(\"-\"*70)\nprint(\"Accuracy of the SVM model is {} %\".format(svmAccScore*100))\nprint(\"-\"*70)\nprint()\nsvmF1Score = f1_score(y_test,y_pred)                           # get F1-score of model\nprint(\"-\"*70)\nprint(\"f1-score of the SVM the model is {} %\".format(svmF1Score*100))\nprint(\"-\"*70)\nprint()\nsvmConMatrix = confusion_matrix(y_test,y_pred)                 # get the confusion matrix\nprint(\"-\"*70)\nprint(\"Confusion matrix for of the SVM is: \\n\",svmConMatrix)\nprint(\"-\"*70)\nprint()\nsvmClassReport = classification_report(y_test,y_pred)          # get the classification report\nprint(\"-\"*70)\nprint(\"Detailed classification report for SVM is: \\n\",svmClassReport)\nprint(\"-\"*70)\nprint()\nprint(\"-\"*70)\nlrProbScaled = logRegModel.predict_proba(X_test_scaled)\nlrProbFprScaled, lrProbTprScaled, lrProbThresholdScaled = roc_curve(y_test,lrProbScaled[:,1])\nlrProbAUCScaled = auc(lrProbFprScaled,lrProbTprScaled)\nprint(\"Area under the ROC curve for Logistic Regression is :{}\".format(round(lrProbAUCScaled,3)))\nprint(\"-\"*70)","254de58b":"svmAcc=[]\nsvmF1 = []\nfor i in range(1,1000,100):\n    print(\"Calculating the SVM classifier accuracy for C : {} \".format(i))\n    svmModel = SVC(C=i)              # Calling default constructor\n    svmModel.fit(X_train_scaled,y_train)                               # fit the model to training set\n    y_pred = svmModel.predict(X_test_scaled)                           # Predict the test data to get y_pred\n    svmAccScore = accuracy_score(y_test,y_pred)                 # get accuracy of model\n    svmAcc.append(svmAccScore*100)\n    # get F1-score of model\n    svmF1Score = f1_score(y_test,y_pred) \n    svmF1.append(svmF1Score*100)\ndfSvm = pd.DataFrame({'C':list(range(1,1000,100)), 'Accuracy':svmAcc,'f1-score':svmF1})\ndfSvm","a211f91d":"- We can find out the following from the above crosstab:\n    * **Age group between 20-30 and 60-70 having maximum number of loan conversion with percentage of 10.577 and 10.786 respectively.**\n    * **Age group between 50-60 having minimum number of conversion of loans with percentage of 8.692..**\n    * **Age group between 40-50 and 50-60 having loan conversion percentage of 9.547 and 9.606 respectively.**\n- Normaly we think chances of customers aged between 30-40 tend to take loan compared to other age groups, **but our analysis as above contradict the same.**","444d729a":"* **Naive Bayes Classifier confusion matrix on unscaled dataset**","e9d2e968":"As above we can conclude, customers with no cd account with bank are more tend to take personal loan.\n* Again lets see the numbers and percentage of loan conversion :","2b5e051a":"As above we can see, customers with 1 member in family having highest count followed by family member of 2, 4 and 3.\n* Lets check family column w.r.t loan status:","5057cfb9":"1. **On unscaled data**","51a0db5f":"**For each model We will first evaluate the model performances on unscaled data and then will evalute the model performance on scaled data. For this lets scale the dataset using MinMaxScaler and save it to variables for future use.**","1183ab77":"* **In comparison with the logistic regression with unscaled data, in this case the algorithm accuracy is less than the former but the f1-score is increased. We can note that recall value is 69% for class-1.**\n* **Recall value of 69% means that, out of all the customers who would actually buy the loan, 69% were correctly predicted to be positive (would buy the personal loan).**\n* **Area under ROC Curve is 0.966 , which almost equal with earlier model of logistic regression with unscaled data.**","fd73f37d":"- **We can see that,**\n    * **$p$-values for Income, CCAvg are less than $\\alpha = 0.05$. Hence with 95% confidence, we can say that they are significant for predicting the target attribute class.**\n    * **$p$-values for mortgage is less than $\\alpha = 0.10$. Hence with 90% confidence, we can say that mortgage attribute is significant for predicting the target attribute class.**\n    * **Age attribute seems to be insignificant to predict target attribute at $\\alpha = 0.10$**","ebeaa902":"**As above we can see there are 96 outliers (1.92 % of the datapoints) out of 5000 datapoints in 'income' column.**\n* Lets check distribution of the 'income' column :","b0192599":"**We can see that K-NN with 5 neighbor performs the best in terms of accuracy (97%) as well as f1-score(81.30%).**","42cf042e":"From 5-point summary we can find 'mortgage' column having mean of 56.499 with Inter Quartile Range (IQR) of 101.0 (Q3 - Q1 = 101.0 - 0.0 = 101.0), which shows **there are many outliers present in the 'mortgage' column.**<br>\n* Lets check what are the outliers present in 'mortgage' column","9107a323":"### 7. Analyzing 'education' column : ","49a5b348":"### K-Nearest Neighbour Classifier","a66ff069":"- We can find out the following from the above crosstab:\n    * **Mortgage group between 500-600 having maximum number of loan conversion with percentage of 66.667 .**\n    * **Mortgage group between 400-500 having second higest conversion of loans with percentage of 41.667, followed by Mortgage group between 300-400 having conversion of loans with percentage of 31.250 .**\n    * **Mortgage group between 200-300 having low conversion of loan with percentage of 13.468**\n    * **Mortgage group between 100-200 and 0-100 having conversion of loans percentage of only 5.145 and 4.610 respectively.**","5966abeb":"# For Support Vector Classifier with C = 201, we get accuracy of 97.400% and f1-score around 86.598% which is the best result for the given problem in comparison with the Logistic Regression, Naive Bayes Classifier and K-NN Classifiers used earlier.","e219a8e8":"- **After observing the dataset and column description given we can conclude the followings:**\n    * **Columns having only two datatypes, int64, float64.**\n    * **Column 'ccavg' is only having float64 datatype, remaining all columns datatype is int64.** \n    * **Columns 'age', 'experience', 'income', 'mortgage' and 'ccavg' are Numeric column.**\n    * **Columns 'zip_code', 'securities_account', 'cd_account', 'online', 'creditcard' and 'personal_loan' are basicaly Nominal Categorical column.**\n    * **Columns 'family' and 'education' are Ordinal Categorical column.**","915402ae":"- We can find out the following from the above crosstab:\n    * **Customers with security account is having loan conversion with percentage of 11.494 .**\n    * **Customers without security account is having loan conversion with percentage of 9.379 .**\n    * Customer with Securities account have slightly higher percentage of taking the personal loan than the customers with no Securities account","762a6ed5":"### 3. Get the target column distribution. Your comments (5 marks) ","9e14b071":"* Lets check the percentage and plot a pie chart to show :","3f721225":"* **We can observe that K-NN algorithm with different values for neighbors doesn't provide much improvement in accuracy as well as f1-score over best performing Naive Bayes Classifier algorithm on unscaled data.**\n2. **On Scaled Data:**","f0408d1e":"**$p$-values of Family and Education are less than $\\alpha = 0.05$, we can say with 90% confidence that both the attributes are significant for predicting the target attribute class.**","0d3a48c2":"### 6. Print the confusion matrix for all the above models (5 marks)","c21f1185":"- We can find out the following from the above crosstab:\n    * **Customers with online is having loan conversion with percentage of 9.752 .**\n    * **Customers without online is having loan conversion with percentage of 9.375 .**\n    * Customer with online have slightly higher percentage of taking the personal loan than the customers without online","fdf3a777":"As above we can conclude, customers with no security account with bank are more tend to take personal loan.\n* Again lets see the numbers and percentage of loan conversion :","ffbb4111":"* **setting id as index of the dataset as 'id' column does not have any significance towards a customer opted for personal loan (target variable - 'personal_loan')**","04ced1b7":"- **Following are the findings after looking into the above 5-point summary of the given dataset(loadData df)**\n    * Numerical column 'age' and 'experience' column have no outliers.\n    * Numerical column 'income', 'ccavg' and 'mortgage' column have outliers present in the dataset.\n    * Nominal categorical columns 'zip_code', 'securities_account', 'cd_account', 'online', 'creditcard' and 'personal_loan' have no outliers and no data cleaning required.\n    * Ordinal Categorical column 'family' and 'education' also have no outliers and are also clean.\n    * **Intersetingly we can see the minimum value of -3.0 (negetive three) exist in the 'experience' column, which needs to be rectified or droped.**","f5255a54":"* **On unscaled dataset :** If we consider the algorithm comparison with unscaled data, Logistic Regression works best which has accuracy of 95.333% and f1-score of 70.833%. The reason it performs better is that Logistic Regression assumes that **(a)** the target \/ dependent variable is binary, it fits one of two clear cut categories. And **(b)** independent variables are indpendent of each other, means no multicollinearity between the independent variables. In this particular case both the assumsions are true. It can be seen in below mentioned correlation heatmap. **Please note earlier we had dropped 'age' attribute which had correlation of $ \\rho = 0.99$ with 'experience' attribute.**\n* **On scaled dataset :** With scaled dataset we can see that performance of K-NN classifier works best which has accuracy of 96.933% and f1-score of 81.300%. The reason it performs better is that it is a distance-based algorithm and as we had treated the outliers through data scalling MinMaxScalar method,K-NN classifier performance dramatically increased.","fc9380c5":"As above we can see, customers with Credit Card  are less than customers without Credit Card\n* Lets check creditcard column w.r.t loan status:","bcfb2893":"#### * **accuracy and f1 score for KNN model with different values for neighbors on unscaled dataset.**","85a7e16d":"Earlier we saw customers with high ccavg tend to buy personal loan, but here we can see that **customers who took personal loan are almost equally segregated in all values of ccavg.** Also we can conclude that percentage of customers who have taken loan is high in higher ccavg bins.\n* **Checking the percentage of customers under each ccavg group who bought personal Loans :**<br>\nSimilar to what we have done in 'age', 'experience' and 'income' column we will use pandas.cut(), pandas.crosstab() & DataFrame.div() function to plot the stacked bar graph which can be shown as:","f86100f8":"**We have taken mean experience in y-axis and grouped them with target variable. As we can see there is not much of a difference between mean experience of customer who has taken loan or not.**\n* **Lets see how experience is distributed between customer who has taken loan and not taken loan :**","61201f51":"## Logistic Regression","bf9ae2ce":"Above we can see **'experience' column is highly correlated with 'age' column ( $ \\rho = 0.99$)**.<br> \nI'll use 'age' column to rectify 'experience' columns where negetive values exist.","59112769":"- We can find out the following from the above crosstab:\n    * **Customers with family member of 3 are having maximum number of loan conversion with percentage of 13.168 .**\n    * **Customers with family member of 4 are having second highest conversion of loans with percentage of 10.966, followed by Customers with family member of 2 and 1 having conversion of loans with percentage of 8.179 and 7.269 respectively .**","357091d0":"### 5. Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans (15 marks)","3f1e915c":"As above we can see, customers with online (Internet Banking) are more than customers without online\n* Lets check online column w.r.t loan status:","cc25c596":"As above we can see, customers with educational qualification of 1: Undergrad having highest count followed by educational background of 3: Advanced\/Professional , 2: Graduate.\n* Lets check education column w.r.t loan status:","b531ea11":"From the above 5-point summary we can confirm negative values of 'experience' column are replaced with suitable values\n* **Now we will check the distribution of 'experience' column**\n","0e865f6b":"Earlier we saw customers with high mortgage having more customers who tend to buy personal loan, but here we can see that lower range of mortgage customers are having higher converson rate in terms of percentage of customer to take personal loan.\n* **Checking the percentage of customers under each mortgage group who bought personal Loans :**<br>\nSimilar to what we have done in 'age', 'experience','income' and 'ccavg' column we will use pandas.cut(), pandas.crosstab() & DataFrame.div() function to plot the stacked bar graph which can be shown as:","f1676125":"Surprisingly, customers with family members of 3 and 4 have highest no of personal loan.\n* Again lets see the numbers and percentage of loan conversion :","b267316f":"- **From the above we can conclude the following:**\n    * **'income', 'ccavg' and 'cd_account' are the most important features to decide whether a customer will take personal loan or not.**\n    * **Other features such as 'cd_account', 'mortgage', 'education', 'family', 'securities_account', 'online', 'creditcard' are also of some importance as per their mentioned order.**\n    * **Since, Age and Experience have very low correlation with target attribute, they seem to be ineffective for predicting whether customer will take a personal loan or not.**\n- **Lets check the significance statisticaly in predicting the target attribute :**","2ccd2aa4":"- We can find out the following from the above crosstab:\n    * **Experience group between 40-50 having maximum number of loan conversion with percentage of 12.963 .**\n    * **Experience group between 0-10 having second highest conversion of loans with percentage of 10.303 .**\n    * **Experience group between 10-20, 20-30 and 30-40 having loan conversion percentage of 9.417, 9.147 and 9.338 respectively.**","0e78eb11":"**We have taken mean mortgage in y-axis and grouped them with target variable. As we can see customers with high mortgage are more tend to take personal loan.**\n* **Lets see how mortgage is distributed between customers taken loan and not taken loan :**","d80abc30":"As above we can see some zip codes are having more customers taking personal loan, but no of customers are also more for those zip codes. So we can think that banking outlets are more accesible to those zip codes than other zip code where no of customers are low.<br>\n* So, we can conclude that zip_code has not played any role in customers taking personal loan for this dataset, and will procced to drop this column.","3e2a458a":"* **Logistic Regression confusion matrix on unscaled dataset**","2a27f30b":"- We can find out the following from the above crosstab:\n    * **CcAvg group between 4-6 having maximum number of loan conversion with percentage of 46.926 .**\n    * **CcAvg group between 8-10 having second higest conversion of loans with percentage of 35.897, followed by CcAvg group between 6-8 having conversion of loans with percentage of 30.693 .**\n    * **CcAvg group between 2-4 having low conversion of loan with percentage of 13.549 and CcAvg group between 0-2 having conversion of loans percentage of only 3.025 .**","74f1270b":"### 10. Analyzing 'cd_account' column : ","f1fb40ce":"### 1. 'age' column : ","1cd4e73a":"As zip_code is numbers of series we might drop, but before dropping let us see if any demographic advantage is there or not for taking personal loan.<br>\n* Lets quickly check maximum time unique zip code appears in the dataset.","d952166f":"- **We can see that accuracy is decreased slightly than logistic regression on unscaled data, as well as f1-score and Recall value in case of class 1 is also decreased. And area under the ROC curve for Naive Bayes Classifier is :0.926 .**\n2. **On scaled data:**","9dd00ad4":"### 7. Give your reasoning on which is the best model in this case and why it performs better? (5 marks)","48f8b1a5":"From 5-point summary we can find 'income' column having mean of 73.774 with Inter Quartile Range (IQR) of 59 (Q3 - Q1 = 98 - 39 = 59), which shows **there are many outliers present in the 'income' column.**<br>\n* Lets check what are the outliers present in 'income' column","f83f47d6":"* **Naive Bayes Classifier confusion matrix on scaled dataset**","137fe466":"1. **On unscaled data**","9b9165ae":"- **From above we can conclude :**\n    * **age has a positive linear relationship with experience.**\n    * **income, ccavg, mortgage histograms are not normally distributed.**","b8566650":"**As shown in the above, <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a.) There are no missing values<br>and (b.) No duplicate row in the given dataset**\n","312b0733":"**Scaling actually decreases the performance of Naive Bayes classifier, although no significant change in accuracy however the f1-score is merely 3% which is a below par performance. And area under the ROC curve for Logistic Regression is :0.925 .**","8c104df3":"**As above we can see there are 291 outliers (5.82 % of the datapoints) out of 5000 datapoints in 'mortgage' column.**\n* Lets check distribution of the 'mortgage' column :","6eac4359":"As above we can see, customers with cd account (Certificate of Deposit) are very less than customers without cd account\n* Lets check cd account column w.r.t loan status:","1d30fa91":"**From above we can see out of 5000 customers, 480 customers taken personal loan, 9.6 percentage of customers taken personal loan, which we can say is a very health conversion rate.**","c727c790":"* **Evidently K-NN (5-neighbor) on scaled data performs very well..The accuracy is 97% and f1-score and recall is also very good.**\n* **Area under the ROC curve for K-NN Classifier is :0.951 .**\n#### * **accuracy and f1 score for KNN model with different values for neighbors on scaled dataset.**","1cb93191":"**For different C value of SVC constructor**","8030e51d":"As above we can conclude, customers with high educational background are more tend to take personal loan.\n* Again lets see the numbers and percentage of loan conversion :","a67ae09b":"From 5-point summary we can find 'experience' column having mean of 20.105 with Inter Quartile Range (IQR) of 20 (Q3 - Q1 = 30 - 10 = 20), which shows there are no outliers in the 'experience' column. **But we can see the minimum value of -3.0 (negative three) exist in the 'experience' column, which needs to be rectified or can be dropped.**\n\nI will go with rectification on negetive values of 'experience' column, for that I'll check if any correlation exist with any other column:","d3410a8a":"### 9. Analyzing 'securities_account' column : ","b6ee8977":"### 11. Analyzing 'online' column : ","33e68b91":"- **We can see that accuracy is less than logistic regression, f1-score is lowest among the compared algorithms. Recall value is merely .33 in case of class 1 i.e., out of all the customers who actually took the loan, only 33% were correctly predicted to be positive.**\n* **Area under the ROC curve for K-NN Classifier is :0.885 .**","46dd7fdd":"Clearly 'mortgage' column is highly right-skewed \/ positively skewed. If customers have no house mortgage, will have 0 values in it, but if customers have any house mortgage will have higher values. That's why this column is very highly skewed.<br>\n* Skewness of 'mortgage' column as below:","b04312a8":"### 4. Analyzing 'zip_code' column : ","e547f4ba":"- **We can see from the above that accuracy with Logistic Regression on unscaled dataset is 95%, which is good. Precision is good at 83%, shows that out of predicted customers 83% are predicted to be correct but f1 score and recall values are little bit less for class 1 at 71% and 62% respectively. Also area under ROC curve is 0.963 .**\n2. **Lets check on scaled data :**","62b20e81":"As above we can see some of the zip codes appear to be more then others. There are 467 unique zip codes available in the dataset. <br>\n* Lets find out conversion of loans w.r.t zip code :","d246b385":"As above, we can clearly conclude that 'income' column is slightly right-skewed \/ positively skewed.<br>\n* Skewness of 'income' column as below:","0dccab14":"**Since the $p$-values are less than 0.05, both the attributes are useful for target class prediction**","5c0d44d0":"**In the context of independent categorical variables with binary response, we observe their correlation with target attribute that only CD Account shows better correlation whereas Securities Account is slightly correlated. Let us see**","2aa8017d":"### 5. Analyzing 'family' column : ","2418d1ab":"**In the above we have taken mean age in y-axis and grouped them with target variable. As we can see there is not much of a difference between mean age of person taken loan or not.**\n* **Lets see how age is distributed between person taken loan and not taken loan :**","f0b977e8":"- We can find out the following from the above crosstab:\n    * **Income group between 150-200 having maximum number of loan conversion with percentage of 50.470 .**\n    * **Income group between 100-150 having second highest conversion of loans with percentage of 28.571, followed by Income group between 200-250 having conversion of loans with percentage of 18.750 .**\n    * **Income group between 0-50 have no loan conversion and Income group between 50-100 having conversion of loans percentage of only 2.241 .**","2473b13e":"**As above we can see there are 324 outliers (6.48 % of the datapoints) out of 5000 datapoints in 'ccavg' column.**\n* Lets check distribution of the 'ccavg' column :","f66c4da4":"1. **On unscaled data**","9385b2ea":"**mortgage is highly right \/ positive skewed with value of 2.104 .**\n* **Lets check mean mortgage w.r.t Loan Status (personal_loan column)**","9ada3fe2":"## 1. Read the column description and ensure you understand each attribute well\n\n**There are total 14 attributes in the dataset and in the context of the given problem, the target (or dependent) attribute is \"Personal Loan\" whereas the remaining are independent attributes.**\n\n**Attribute Information:**\n* ID : Customer ID, is unique for ach customer which is assigned by the Bank.\n* Age : Age of a particular Customer's in completed years.\n* Experience : professional experience in terms of years.\n* Income : Annual income of the customer. (\\$000)\n* ZIP Code : Home Address ZIP code of the customer.\n* Family : Size of the customer's Family.\n* CCAvg : Average spending on credit cards per month by customers. (\\$000)\n* Education : Education Level of the customers. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional.\n* Mortgage : If any customer have any house mortgage then value of house mortgage. (\\$000)\n* Securities Account : Does the customer have a securities account with the bank?\n* CD Account : Does the customer have a certificate of deposit (CD) account with the bank?\n* Online : Does the customer use internet banking facilities?\n* Credit card : Does the customer use a credit card issued by UniversalBank?\n* Personal Loan : Did this customer accept the personal loan offered in the last campaign? (**Target Attribute**)","055244e0":"### 3. Analyzing 'income' column : ","f2536cab":"As above we can see, customers with securities account are very less than customers without securities account\n* Lets check securities account column w.r.t loan status:","2a744324":"- We can find out the following from the above crosstab:\n    * **Customers with Credit Card is having loan conversion with percentage of 9.728 .**\n    * **Customers without Credit Card is having loan conversion with percentage of 9.547 .**\n    * Customer with Credit Card have slightly higher percentage of taking the personal loan than the customers without Credit Card.","dd7e286d":"**We have taken mean income in y-axis and grouped them with target variable. As we can see customers with high annual income are more inclined to take personal loan.** Without this insight we can think customers with low annual income are more inclined to opt for personal loan to fullfil their needs.\n* **Lets see how income is distributed between customers taken loan and not taken loan :**","55dae5a9":"**Algorithm for above code :**\n1. find unique negative experience values\n2. for each distinct negative values<br>\n    a. find list of ages for negative values<br>\n    b. loop through all location of negatve experience values\n        a. find mean experience of same age (same age w.r.t negative experience value) where experience is greater than zero\n        b. replace mean experiace with negative values at each location","5b8b49fb":"**Experience seems to be distributed quite uniformly.**\n* **Lets check mean experience w.r.t Loan Status (personal_loan column)**","fa6f8aff":"**We have taken mean ccavg (Annual Credit Card spending) in y-axis and grouped them with target variable. As we can see customers with high annual ccavg are more tend to buy personal loan.**\n* **Lets see how ccavg is distributed between customers taken loan and not taken loan :**","6b0d315e":"* **Logistic Regression confusion matrix on scaled dataset**","d4bb29a7":"As above, we can clearly conclude that 'income' column is highly right-skewed \/ positively skewed.<br>\n* Skewness of 'ccavg' column as below:","66310ceb":"* **Before moving further, let us plot the pairplot using all attributes :**","e69e184d":"### 4. Split the data into training and test set in the ratio of 70:30 respectively (5 marks)","94313139":"- We can find out the following from the above crosstab:\n    * **Customers with education of 3: Advanced\/Professional is having maximum number of loan conversion with percentage of 13.658 .**\n    * **Customers with education of 2: Graduate is having second higest conversion of loans with percentage of 12.972, followed by Customers with education of 1: Undergrad having lowest conversion of loans with percentage of 4.437 .**","d6681101":"## Supervised Learning - Project\n\n### Steps and tasks:\n1. Read the column description and ensure you understand each attribute well\n2. Study the data distribution in each attribute, share your findings (15 marks)\n3. Get the target column distribution. Your comments (5 marks)\n4. Split the data into training and test set in the ratio of 70:30 respectively (5 marks)\n5. Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans (15 marks)\n6. Print the confusion matrix for all the above models (5 marks)\n7. Give your reasoning on which is the best model in this case and why it performs better? (5 marks)\n","c65b8bc0":"'age' column having mean of 45.338 with Inter Quartile Range (IQR) of 20 (Q3 - Q1 = 55 - 35 = 20), which shows there is no outliers in the 'age' column.\n- Let's Check some facts: ","5b50ed17":"**ccavg is highly right \/ positive skewed with value of 1.598 .**\n* **Lets check mean ccavg w.r.t Loan Status (personal_loan column)**","0963869c":"* **K-NN Classifier confusion matrix on scaled dataset**","efb1d4d5":"**From the above distribution, we can say almost every Age group of the customers have bought Personal Loans.**<br>\n* **Checking the percentage of customers under each age group who bought personal Loans :**<br>\nFor this we will use pandas.cut() which is used to segment data values into different categorical bins. And we will use pandas.crosstab() & DataFrame.div() function to plot the stacked bar graph which can be shown as:","7f8429b8":"### Naive Bayes Classifier","d976bb8e":"### \\*\\*\\* Feature Analysis :\\*\\*\\*","d07c110f":"As above we can conclude, customers with Internet Banking with bank are more tend to take personal loan.\n* Again lets see the numbers and percentage of loan conversion :","79fb4ebc":"### Let us check with SVM (Support Vector Machine) performance on the scaled dataset:","062ac77b":"* **Lets check mean income w.r.t Loan Status (personal_loan column)**","9c53ef59":"### 12. Analyzing 'creditcard' column : ","41f03f60":"**From the above distribution, we can say almost every experience group of the customers have opted Personal Loans.**<br>\n* **Checking the percentage of customers under each experience group who opted personal Loans :**<br>\nSimilar to what we have done in 'age' column we will use pandas.cut(), pandas.crosstab() & DataFrame.div() function to plot the stacked bar graph which can be shown as:","82884195":"As above we can conclude same as before that customers with high annual income are more tend to opt for personal loan.<br>\n**One interesting thing we can see from above is customers with annual income between 0-60 (approx) have not taken any personal loan.**\n* **Checking the percentage of customers under each income group who bought personal Loans :**<br>\nSimilar to what we have done in 'age' and 'experience' column we will use pandas.cut(), pandas.crosstab() & DataFrame.div() function to plot the stacked bar graph which can be shown as:","e7242644":"* **K-NN Classifier confusion matrix on unscaled dataset**","3e273a57":"- **Accuracy, f1-score and Area under ROC for different Classification Algorithms used on unscaled and scaled dataset:**","d6bb9f74":"**Age seems to be distributed quite uniformly.**\n* **Lets check mean age w.r.t Loan Status (personal_loan column)**","d3aa4c28":"### 2. Study the data distribution in each attribute, share your findings (15 marks)","226e61b1":"### 2. 'experience' column : ","480dd754":"### 6. Analyzing 'ccavg' column : ","b40c447a":"- We can find out the following from the above crosstab:\n    * **Customers with cd account is having loan conversion with percentage of 46.358 .**\n    * **Customers without cd account is having loan conversion with percentage of 7.237 .**\n    * Customer with cd account have very higher percentage of taking the personal loan than the customers with no cd account","ffeb42e4":"As above we can conclude, customers without Credit Card with bank are more tend to take personal loan.\n* Again lets see the numbers and percentage of loan conversion :","5e51a5c7":"### 8. Analyzing 'mortgage' column : ","3118f858":"From 5-point summary we can find 'ccavg' column having mean of 1.938 with Inter Quartile Range (IQR) of 1.8 (Q3 - Q1 = 2.5 - 0.7 = 1.8), which shows **there are some outliers present in the 'ccavg' column.**<br>\n* Lets check what are the outliers present in 'ccavg' column"}}