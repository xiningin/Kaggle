{"cell_type":{"64781dea":"code","d7110db2":"code","393b8f81":"code","d37fce50":"code","18de4e77":"code","7163d664":"code","a8ab0a41":"code","8377f33a":"code","04aa6f59":"code","8c1570b8":"code","543c0cb0":"code","8bec66ee":"code","9de1a0c1":"code","deb2b0f8":"code","d73cfbac":"code","84007046":"code","6a3a815c":"code","29b8150e":"code","e32efd5e":"code","26a7c095":"code","6f379aa5":"code","183853a9":"code","a00748e3":"code","e57f440a":"code","b353f3cf":"code","57667354":"code","26f2bd15":"code","781126e5":"code","e54c5d41":"code","d8d2b57c":"code","fe78fe4e":"code","ca33980c":"code","e0661f8b":"code","c2ea6482":"code","91f72023":"markdown","4ac3c9cf":"markdown","4e8a1283":"markdown","f8a558e9":"markdown","44b1a70d":"markdown","5b05ecd8":"markdown","e5e2d3fa":"markdown","da51d8d4":"markdown","6ce3af21":"markdown","6a758eba":"markdown","d4665bc2":"markdown","1ee5899a":"markdown","123b4577":"markdown","93e0d6df":"markdown","20cd8653":"markdown","d90ccac9":"markdown","642df320":"markdown","1b5c4e64":"markdown","a1157c5c":"markdown","201b859e":"markdown","43919de9":"markdown","f81a8a26":"markdown","8e65ad10":"markdown","923b7d63":"markdown","30202f15":"markdown","a0e47e5a":"markdown","98e3c69e":"markdown","9742f708":"markdown","d435ae74":"markdown","65367207":"markdown","c50c8388":"markdown","49cda0ed":"markdown","b6cc9782":"markdown","33c5f388":"markdown"},"source":{"64781dea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\npyo.init_notebook_mode()\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\n\nplt.rc('figure',figsize=(17,13))\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\n","d7110db2":"!pip install vaderSentiment\n!pip install twython","393b8f81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d37fce50":"data=pd.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\ndata.head()","18de4e77":"def clean(text):\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('<.*?>+', '', text)\n    return text\n    \n\n\ndata['text'] = data['text'].apply(lambda x:clean(x))\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\nscores=[]\nfor i in range(len(data['text'])):\n    \n    score = analyser.polarity_scores(data['text'][i])\n    score=score['compound']\n    scores.append(score)\nsentiment=[]\nfor i in scores:\n    if i>=0.05:\n        sentiment.append('Positive')\n    elif i<=(-0.05):\n        sentiment.append('Negative')\n    else:\n        sentiment.append('Neutral')\ndata['sentiment']=pd.Series(np.array(sentiment))","7163d664":"data.head()","a8ab0a41":"from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n\ntweet_All = \" \".join(review for review in data.text)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (10,10))\n# Create and generate a word cloud image:\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(tweet_All)\n\n# Display the generated image:\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off')\n","8377f33a":"def clean_text(text):\n    \n    text = str(text).lower()\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    return text\ndata['text'] = data['text'].apply(lambda x:clean_text(x))","04aa6f59":"data['text']","8c1570b8":"df=pd.DataFrame()\ndf['text']=data['text']\ndef tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n    \ndf['No_stopwords'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n\nps = nltk.PorterStemmer()\n\ndef stemming1(text):\n    text = [ps.stem(word) for word in text]\n    return text\n\ndf['stemmed_porter'] = df['No_stopwords'].apply(lambda x: stemming1(x))\n\nfrom nltk.stem.snowball import SnowballStemmer\ns_stemmer = SnowballStemmer(language='english')\ndef stemming2(text):\n    text = [s_stemmer.stem(word) for word in text]\n    return text\ndf['stemmed_snowball'] = df['No_stopwords'].apply(lambda x: stemming2(x))\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\n\ndf['lemmatized'] = df['No_stopwords'].apply(lambda x: lemmatizer(x))","543c0cb0":"df.head()","8bec66ee":"data['text']=df['lemmatized']\ndata.head()","9de1a0c1":"temp = data.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","deb2b0f8":"\nplt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=data)\nfig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","d73cfbac":" \ngenre_difference_metric = [data['sentiment'].value_counts().index,data.groupby(['sentiment']).sum()['favorites'].sort_values(ascending=False).index,data.groupby(['sentiment']).sum()['retweets'].sort_values(ascending=False).index]\n\n#Dataframe to be used for plotting.\ngenre_evolution_df = pd.DataFrame(columns=['genre', 'rank_type', 'rank'])\n\n#Populate the dataframe\nfor metric in range(3):\n    for genre in range(len(genre_difference_metric[metric])):\n        genre_evolution_df = genre_evolution_df.append({'genre':genre_difference_metric[metric][genre], 'rank_type': metric, 'rank':genre},\n                                   ignore_index=True)\nplt.style.use('seaborn-bright')       \nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111)\n\n    \n\nax.set_xlim([-2,4])\nxs = [0.0, 1.0, 2.0]\nx_labels = ['sentiment count', 'sum of favorites', 'sum of retweets']\nplt.xticks(range(len(xs)), x_labels)\nplt.xticks(xs, x_labels, rotation='vertical')\nsns.pointplot(x=genre_evolution_df.rank_type,\n              y=3-genre_evolution_df['rank'], \n              hue=genre_evolution_df.genre)\nys = range(1,5)\ny_labels = ['3rd', '2nd', '1st']\nplt.yticks(ys, y_labels)\nax.set_ylabel('Sentiment Category Rank')\n\nplt.legend(bbox_to_anchor=(0.7, 0., 0.5, 0.5),loc='best',ncol=1)\nplt.show();  ","84007046":"\nsns.catplot(data=data, x='user_verified', kind= 'count',height=5,aspect=2)\n","6a3a815c":"sns.barplot(x=\"user_verified\", y=\"favorites\", hue=\"sentiment\", data=data)\n","29b8150e":"sns.barplot(x=\"user_verified\", y=\"retweets\", hue=\"sentiment\", data=data)","e32efd5e":"all_words=[]\nfor i in range(len(data['text'])):\n    a=data['text'][i]\n    for i in a:\n        all_words.append(i)\nall_words=pd.Series(np.array(all_words))\n\ncommon_words=all_words.value_counts()[:30].rename_axis('Common Words').reset_index(name='count')\n\nfig = px.treemap(common_words, path=['Common Words'], values='count',title='30 Most Common Words In Tweets')\nfig.show()","26a7c095":"data['hashtags']=data['hashtags'].fillna('[]')\nall_hashtags=[]\nfor i in range(len(data['hashtags'])):\n    a=data['hashtags'][i].strip('][').split(', ') \n    for i in a:\n        all_hashtags.append(i)\nall_hashtags=['No Hashtag' if x=='' else x for x in all_hashtags]       \n\nall_hashtags=pd.Series(np.array(all_hashtags))\nprint('There are {} instances of tweets in which No Hashtags were used'.format(all_hashtags.value_counts()[1]))\n\ncommon_hashtags=all_hashtags.value_counts().drop(labels='No Hashtag')[:30].rename_axis('Common Hashtags').reset_index(name='count')\nfig = px.treemap(common_hashtags, path=['Common Hashtags'], values='count',title='30 Most Common Hashtags')\nfig.show()","6f379aa5":"Positive_tweet = data[data['sentiment']=='Positive'].reset_index()\nNegative_tweet = data[data['sentiment']=='Negative'].reset_index()\nNeutral_tweet = data[data['sentiment']=='Neutral'].reset_index()","183853a9":"all_positive_words=[]\nall_positive_hashtags=[]\nfor i in range(len(Positive_tweet['text'])):\n    a=Positive_tweet['text'][i]\n    b=Positive_tweet['hashtags'][i].strip('][').split(', ')\n    for i in a:\n        all_positive_words.append(i)\n    for i in b:\n        all_positive_hashtags.append(i)\nall_positive_words=pd.Series(np.array(all_positive_words))\nall_positive_hashtags=pd.Series(np.array(all_positive_hashtags))\ncommon_words=all_positive_words.value_counts().drop(labels='')[:70].rename_axis('Common Positive Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Positive Words'], values='count',title='70 Most Common Words In Positive Tweets')\nfig.show()\ncommon_hashtags=all_positive_hashtags.value_counts()[:70].drop(labels='').rename_axis('Common Positive Hashtags').reset_index(name='count')\nfig = px.treemap(common_hashtags, path=['Common Positive Hashtags'], values='count',title='70 Most Common Hashtags In Positive Tweets')\nfig.show()","a00748e3":"all_negative_words=[]\nall_negative_hashtags=[]\nfor i in range(len(Negative_tweet['text'])):\n    a=Negative_tweet['text'][i]\n    b=Negative_tweet['hashtags'][i].strip('][').split(', ')\n    for i in a:\n        all_negative_words.append(i)\n    for i in b:\n        all_negative_hashtags.append(i)\nall_negative_words=pd.Series(np.array(all_negative_words))\nall_negative_hashtags=pd.Series(np.array(all_negative_hashtags))\ncommon_words=all_negative_words.value_counts().drop(labels='')[:70].rename_axis('Common Negative Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Negative Words'], values='count',title='70 Most Common Words In Negative Tweets')\nfig.show()\ncommon_hashtags=all_negative_hashtags.value_counts()[:70].drop(labels='').rename_axis('Common Negative Hashtags').reset_index(name='count')\nfig = px.treemap(common_hashtags, path=['Common Negative Hashtags'], values='count',title='70 Most Common Hashtags In Negative Tweets')\nfig.show()","e57f440a":"all_neutral_words=[]\nall_neutral_hashtags=[]\nfor i in range(len(Neutral_tweet['text'])):\n    a=Neutral_tweet['text'][i]\n    b=Neutral_tweet['hashtags'][i].strip('][').split(', ')\n    for i in a:\n        all_neutral_words.append(i)\n    for i in b:\n        all_neutral_hashtags.append(i)\nall_neutral_words=pd.Series(np.array(all_neutral_words))\nall_neutral_hashtags=pd.Series(np.array(all_neutral_hashtags))\ncommon_words=all_neutral_words.value_counts().drop(labels='')[:70].rename_axis('Common Neutral Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Neutral Words'], values='count',title='70 Most Common Words In Neutral Tweets')\nfig.show()\ncommon_hashtags=all_neutral_hashtags.value_counts()[:70].drop(labels='').rename_axis('Common Neutral Hashtags').reset_index(name='count')\nfig = px.treemap(common_hashtags, path=['Common Neutral Hashtags'], values='count',title='70 Most Common Hashtags In Neutral Tweets')\nfig.show()","b353f3cf":"common=set(all_positive_words).intersection(set(all_negative_words)).intersection(set(all_neutral_words))\ncommon_list=list(common)\n\ncommon_words=all_negative_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Negative Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Negative Words'], values='count',title='Top 30 Unique Words In Negative Tweets')\nfig.show()\ncommon_words=all_positive_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Positive Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Positive Words'], values='count',title='Top 30 Unique Words In Positive Tweets')\nfig.show()\ncommon_words=all_neutral_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Neutral Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Neutral Words'], values='count',title='Top 30 Unique Words In Neutral Tweets')\nfig.show()","57667354":"data_ = data['source'].value_counts().reset_index()\n\ntrace1 = go.Bar(\n                x = ['Twitter for Android', 'Twitter Web App', 'Twitter for iPhone',\n       'TweetDeck', 'Buffer', 'Twitter for iPad', 'Twitter Media Studio',\n       'ThreadReaderApp', 'Instagram', 'SocialFlow', 'Hootsuite Inc.',\n       'LinkedIn', 'Twitter for Mac', '24liveblog', 'Publer ', 'IFTTT',\n       'Socialbakers', 'Falcon Social Media Management ', 'Echobox',\n       'Microsoft Power Platform', 'Nonli', 'Sendible',\n       'Tweetbot for Mac', 'EastMojo',\n       'Twitter Media Studio - LiveCut'], #temp_df['index'],\n                y = data_['source'],\n                marker = dict(color = 'rgb(250,13,92)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=data_['source'], textposition='outside')\nlayout = go.Layout(template= \"plotly_dark\",title = 'SOURCE DISTRIBUTION OF TWEETS' , xaxis = dict(title = 'SOURCE'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()","26f2bd15":"data_verified=data[(data['user_verified']==True)].reset_index()\ndata_not_verified=data[(data['user_verified']==False)].reset_index()","781126e5":"\ndata_ = data_verified['source'].value_counts().reset_index()\n\ntrace1 = go.Bar(\n                x = ['Twitter Web App', 'Twitter for iPhone', 'TweetDeck', 'Buffer',\n       'SocialFlow', 'Hootsuite Inc.', 'Twitter for iPad',\n       'Twitter for Android', 'Socialbakers', 'Echobox',\n       'Twitter Media Studio', 'EastMojo',\n       'Twitter Media Studio - LiveCut', 'GT_Backend'], #temp_df['index'],\n                y = data_['source'],\n                marker = dict(color = 'rgb(250,13,92)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=data_['source'], textposition='outside')\nlayout = go.Layout(template= \"plotly_dark\",title = 'SOURCE DISTRIBUTION OF TWEETS FROM VERIFIED ACCOUNTS' , xaxis = dict(title = 'SOURCE'), yaxis = dict(title = 'Count'), height=650)\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()","e54c5d41":"all_hashtags=[]\nfor i in range(len(data_verified['hashtags'])):\n    a=data_verified['hashtags'][i].strip('][').split(', ') \n    for i in a:\n        all_hashtags.append(i)\nall_hashtags=['No Hashtag' if x=='' else x for x in all_hashtags]       \n\nall_hashtags=pd.Series(np.array(all_hashtags))\ncommon_hashtags=all_hashtags.value_counts()[:30].rename_axis('Common Hashtags').reset_index(name='count')\nfig = px.treemap(common_hashtags, path=['Common Hashtags'], values='count',title='30 Most Common Hashtags by VERIFIED ACCOUNTS')\nfig.show()","d8d2b57c":"\n\nPositive_tweet = data_verified[data_verified['sentiment']=='Positive'].reset_index()\nNegative_tweet = data_verified[data_verified['sentiment']=='Negative'].reset_index()\nNeutral_tweet = data_verified[data_verified['sentiment']=='Neutral'].reset_index()\nall_positive_words=[]\n\nfor i in range(len(Positive_tweet['text'])):\n    a=Positive_tweet['text'][i]\n    \n    for i in a:\n        all_positive_words.append(i)\n    \nall_positive_words=pd.Series(np.array(all_positive_words))\nall_neutral_words=[]\n\nfor i in range(len(Neutral_tweet['text'])):\n    a=Neutral_tweet['text'][i]\n    \n    for i in a:\n        all_neutral_words.append(i)\n    \nall_neutral_words=pd.Series(np.array(all_neutral_words))\nall_negative_words=[]\n\nfor i in range(len(Negative_tweet['text'])):\n    a=Negative_tweet['text'][i]\n   \n    for i in a:\n        all_negative_words.append(i)\n    \nall_negative_words=pd.Series(np.array(all_negative_words))\ncommon=set(all_positive_words).intersection(set(all_negative_words)).intersection(set(all_neutral_words))\ncommon_list=list(common)\n\ncommon_words=all_negative_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Negative Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Negative Words'], values='count',title='30 Most Common Unique Negative Words by VERIFIED ACCOUNTS')\nfig.show()\ncommon_words=all_positive_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Positive Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Positive Words'], values='count',title='30 Most Common Unique Positive Words by VERIFIED ACCOUNTS')\nfig.show()\ncommon_words=all_neutral_words.value_counts().drop(labels=common_list)[:30].rename_axis('Common Neutral Words').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Neutral Words'], values='count',title='30 Most Common Unique Neutral Words by VERIFIED ACCOUNTS')\nfig.show()","fe78fe4e":"data['user_location'] = data['user_location'].fillna('NaN')\nPositive_tweet = data[data['sentiment']=='Positive'].reset_index()\nNegative_tweet = data[data['sentiment']=='Negative'].reset_index()\nNeutral_tweet = data[data['sentiment']=='Neutral'].reset_index()\npos_location=Positive_tweet['user_location']\nneg_location=Negative_tweet['user_location']\nneu_location=Neutral_tweet['user_location']\n\ncommon=set(pos_location).intersection(set(neg_location)).intersection(set(neu_location))\ncommon_list=list(common)\n\ncommon_words=neg_location.value_counts().drop(labels=common_list)[:10].rename_axis('Common Negative Locations').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Negative Locations'], values='count',title='10 Top Unique Negative Tweets Locations')\nfig.show()\ncommon_words=pos_location.value_counts().drop(labels=common_list)[:10].rename_axis('Common Positive Locations').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Positive Locations'], values='count',title='10 Top Unique Positive Tweets Locations')\nfig.show()\ncommon_words=neu_location.value_counts().drop(labels=common_list)[:10].rename_axis('Common Neutral Locations').reset_index(name='count')\nfig = px.treemap(common_words, path=['Common Neutral Locations'], values='count',title='10 Top Unique Neutral Tweets Locations')\nfig.show()","ca33980c":"from matplotlib import rcParams\n","e0661f8b":"fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(10, 16))\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax1, palette=[\"b\"],\n           data=data[(data.sentiment== \"Positive\")]\\\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax1.set_title('Top 10 Accounts with Highest Followers who tweet Positive')\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax2, palette=[\"g\"],\n           data=data[(data.sentiment == \"Neutral\")]\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax2.set_title('Top 10 Accounts with Highest Followers who tweet Neutral')\nsns.barplot(x=\"user_followers\", y=\"user_name\", orient=\"h\", ax=ax3, palette=[\"r\"],\n           data=data[(data.sentiment == \"Negative\")]\n           .drop_duplicates(subset=[\"user_name\"])\\\n           .sort_values(by=[\"user_followers\"], ascending=False)[[\"user_name\", \"user_followers\"]][:10])\nax3.set_title('Top 10 Accounts with Highest Followers who tweet Negative')\n\nfig.show()","c2ea6482":"data[\"date\"] = pd.to_datetime(data.date) \ntimeline = data.resample('D', on='date')[\"sentiment\"].value_counts().unstack(1)\n\ntimeline.reset_index(inplace=True)\n\ntimeline = timeline.melt(\"date\", var_name='sentiment',  value_name='vals')\n\nsns.set_style(\"whitegrid\")\nsns.lineplot(x=\"date\", y=\"vals\", hue=\"sentiment\", data=timeline, palette=[\"r\", \"g\",\"b\"])\nplt.figure(figsize=(40,10))","91f72023":"# DISTRIBUTION OF THE FAVORITES RECIEVED BASED ON TWEET SENTIMENT AND WHETHER THE ACCOUNT IS VERIFIED\/NOT","4ac3c9cf":" **REMOVING PUNCTUATIONS AND MAKING TEXT LOWERCASE**","4e8a1283":"\n* A sudden fall in the positive tweets can be seen around 22-25 Dec.\n* A sudden spike in negative tweets can be seen around 30 Dec 2020.","f8a558e9":"# **SENTIMENT WISE ANALYSIS OF HASHTAGS AND WORDS:**","44b1a70d":"# ****Pfizer and BioNTech Vaccine Tweets Analysis****","5b05ecd8":"IMPORTING MODULES\n","e5e2d3fa":"# GLIMPSE AT VERIFIED ACCOUNTS\n","da51d8d4":"**THANKYOU FOR READING MY FIRST NOTEBOOK.\nDO UPVOTE AND GIVE FEEDBACK IF YOU FOUND IT USEFUL!**","6ce3af21":"# **THE TOP 30 MOST FREQUENTLY USED HASHTAGS IN THE TWEET TEXT DATA**","6a758eba":"# NEGATIVE TWEETS","d4665bc2":"# SENTIMENT COUNTS","1ee5899a":"# NEUTRAL TWEETS","123b4577":"It can be seen that 'PfizerBioNTech','COVID19','vaccine',etc are common in all the sentiments and are not giving a clear distinction in the words used exclusively for a particular sentiment.\nSo,the following plots show the distribution of words that are unique to each sentiment.\n","93e0d6df":"# **WORDCLOUD ANALYSIS OF TWEET TEXT**","20cd8653":"# POSITIVE TWEETS","d90ccac9":"# TIME BASED ANALYSIS OF TWEET SENTIMENT:","642df320":"# SOURCE DISTRIBUTION OF TWEETS","1b5c4e64":"#  **STEMMING AND LEMMATIZATION**","a1157c5c":"My first notebook, comments, suggestions  and upvotes are all welcome :)","201b859e":"# **SOURCE DISTRIBUTION**","43919de9":"# ACCOUNT:VERIFIED\/NON-VERIFIED","f81a8a26":"# FUNNEL CHART FOR BETTER VISUALIZATION OF SENTIMENT DISTRIBUTION\n","8e65ad10":"# DISTRIBUTION OF THE RETWEETS BASED ON TWEET SENTIMENT AND WHETHER THE ACCOUNT IS VERIFIED\/NOT","923b7d63":"# **SENTIMENT WISE WORD FREQUENCY**","30202f15":"This gave a clearer and better analysis!","a0e47e5a":"# **THE TOP 30 MOST FREQUENTLY OCCURING WORDS IN THE TWEET TEXT DATA**\n","98e3c69e":"# 'ACCOUNTS WITH HIGHEST FOLLOWERS' ANALYSIS BASED ON THE TWEET SENTIMENT","9742f708":"#  TWEET SENTIMENT BASED ANALYSIS OF LOCATION","d435ae74":"IT LOOKS LIKE BOTH THE STEMMING METHODS ARE REDUCING THE TEXT WORDS TO UNUSUAL STEMS WHILE LEMMATIZATION SEEMS TO WORK FINE \n","65367207":"* df shows the text at each step of the preprocessing\n* From the dataframe df we can decide which one of the stemming(Porter or Snowball)\/lemmatization or both is suitable for our data.\n","c50c8388":"# **TEXT PREPROCESSING FOR VADER SENTIMENT ANALYSIS**\n* Here I didn't touch punctuations and case for now as VADER SENTIMENT ANALYSIS SCORES are affected by factors like punctuations,capitalization,preceeding-trigrams,degree modifiers,conjunctions etc.\n* The Sentiments have been classified based on scores according to the VADER convention.\n* A new column 'Sentiment'is thus, added giving sentiment corresponding to each Tweet text.","49cda0ed":"# **EDA PART:**","b6cc9782":"# RANKING THE SENTIMENTS BASED ON THREE METRICS:\n1. THE FIRST METRIC IS THE SPECIFIC SENTIMENT'S COUNT THROUGHOUT THE TWEET DATA.NEUTRAL TWEETS ARE AT TOP HERE.\n2. THE SECOND METRIC IS THE SUM OF FAVORITES ON A PARTICULAR SENTIMENT'S TWEET.IT CAN BE SEEN THAT THE POSITIVE TWEETS GET THE HIGHEST FAVORITES EVEN THOUGH NEUTRAL TWEETS HAVE THE HIGHEST COUNT.ATLEAST PEOPLE WANT TO BE POSITIVE WHEN IT'S A VACCINE BEING TALKED ABOUT.\n3. THE THIRD METRIC IS THE SUM OF RETWEETS ON A PARTICULAR SENTIMENT'S TWEET.IT CAN BE SEEN THAT THE NEUTRAL TWEETS GET THE HIGHEST RETWEETS \n\nNEGATIVE TWEETS IS AT THE LAST POSITION IN ALL THE THREE METRICS","33c5f388":"# **HASHTAGS**"}}