{"cell_type":{"a5f3f8a3":"code","9ab411c2":"code","8e203876":"code","5d6c4849":"code","2155e36a":"code","71afef5b":"code","3fee5e9d":"code","a1d9db0a":"code","8cc229b5":"code","c9375090":"code","512a8c56":"code","bcc92d75":"code","0bc40d8f":"code","e2e7bd07":"code","745967fc":"code","d2e15005":"code","d5422343":"code","e9dc5ce5":"code","57688874":"code","5a98c32e":"code","7a92ea2b":"code","c8b6190e":"code","7affaf38":"code","4b13134b":"code","369f3b84":"code","6b2fb6a8":"code","90c26695":"code","516ea92e":"code","0b6a3673":"code","026bc5f7":"code","f4c79b14":"code","25278f5b":"markdown","68ef5dc8":"markdown","a3af9f23":"markdown","a12dea5e":"markdown","495593e1":"markdown","854b9f61":"markdown","2918f59a":"markdown","bbe473d8":"markdown","ef4fe8be":"markdown","1d9e810c":"markdown","79d0bde3":"markdown","64a4cc07":"markdown","022b1425":"markdown","c00517cd":"markdown","1c865f99":"markdown","49b69723":"markdown","ec8b2d2d":"markdown","7229c19d":"markdown","f230d57b":"markdown","9bd14956":"markdown","806e8fe2":"markdown","47313978":"markdown","a6663de7":"markdown","08ed4285":"markdown","c1f8852a":"markdown"},"source":{"a5f3f8a3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('darkgrid')","9ab411c2":"columns = ['CustomerID', 'Gender', 'Age', 'Income', 'Score']\ndf = pd.read_csv('..\/input\/Mall_Customers.csv', index_col='CustomerID', names=columns, header=0)\ndf = df[['Age', 'Income', 'Score', 'Gender']] # Putting Gender (target variable) at the end\ndf.head()","8e203876":"df.shape","5d6c4849":"df.describe()","2155e36a":"# % share of gender in dataset\ndf.Gender.value_counts(normalize=True)","71afef5b":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(9,4))\n\nfemale = df[df.Gender == 'Female']\nmale = df[df.Gender == 'Male']\n\nsns.distplot(female.Age, bins=12 ,ax=ax1)\nsns.distplot(male.Age, bins=12, ax=ax2)\n\nax1.set_title('Age distr among females')\nax2.set_title('Age distr among males')","3fee5e9d":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(9,4))\n\nfemale = df[df.Gender == 'Female']\nmale = df[df.Gender == 'Male']\n\nsns.distplot(female.Income, bins=12 ,ax=ax1)\nsns.distplot(male.Income, bins=12, ax=ax2)\n\nax1.set_title('Income distr among females')\nax2.set_title('Income distr among males')","a1d9db0a":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(9,4))\n\nfemale = df[df.Gender == 'Female']\nmale = df[df.Gender == 'Male']\n\nsns.distplot(female.Score, bins=12 ,ax=ax1)\nsns.distplot(male.Score, bins=12, ax=ax2)\n\nax1.set_title('Score distr among females')\nax2.set_title('Score distr among males')","8cc229b5":"# Map Gender to 1 for female and 0 for male\n\nmapping = {'Female': 1, 'Male': 0}\ndf.Gender.replace(mapping, inplace=True)\ndf.head()","c9375090":"# Comparing pairwise correlations between variables\nsns.pairplot(df[['Age', 'Income', 'Score']])","512a8c56":"sns.lmplot('Score', 'Income', hue='Gender', data=df, fit_reg=False)","bcc92d75":"# Standardize the data to all be the same unit\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(df.drop('Gender', axis=1))\n\n# Transforming the data\nscaled_features = scaler.transform(df.drop('Gender', axis=1))\nscaled_features","0bc40d8f":"# Use the scaler to create scaler dataframe\n# This gives us a standardized version of our data\n\ndf_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\ndf_feat.head()","e2e7bd07":"from sklearn.model_selection import train_test_split, GridSearchCV\n\nX = df_feat\ny = df['Gender']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","745967fc":"# Training and Predictions\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5) # k=5\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\npred","d2e15005":"# Evaluating the algorithm\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint (confusion_matrix(y_test, pred))\nprint (classification_report(y_test, pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","d5422343":"error_rate = []\n\nfor i in range(1,40): # Checking every possible k value between 1-40\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nerror_rate","e9dc5ce5":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40), error_rate, color='grey', marker='o', markerfacecolor='red')\nplt.title('Error rate vs K value')\nplt.xlabel('K value')\nplt.ylabel('Mean error rate')","57688874":"knn = KNeighborsClassifier(n_neighbors=17)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","5a98c32e":"# Training the algorithm\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100, random_state=101)\nforest.fit(X_train, y_train)\ny_pred = forest.predict(X_test)","7a92ea2b":"# Evaluating the algorithm\n\nprint (confusion_matrix(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","c8b6190e":"# Grid search\n\ngrid_param = {  \n    'n_estimators': [50, 80, 100, 120],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False],\n    'max_depth': [10,30,50],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_split': [3,9,20],\n    'min_samples_leaf': [1, 2, 4]\n    }\n\ngs = GridSearchCV(estimator=forest,  \n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=-1)\n\ngs.fit(X_train, y_train)","7affaf38":"print(gs.best_params_)","4b13134b":"# Training the tuned algorithm\n\nforest_tuned = RandomForestClassifier(n_estimators=100,\n                                      criterion= 'gini',\n                                      bootstrap= False,\n                                      max_depth= 10,\n                                      max_features= 'auto',\n                                      min_samples_split= 20,\n                                      min_samples_leaf= 1,\n                                      random_state=101)\nforest_tuned.fit(X_train, y_train)\ny_pred = forest_tuned.predict(X_test)","369f3b84":"# Evaluating the tuned algorithm\n\nprint (confusion_matrix(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","6b2fb6a8":"# Training the algorithm\n\nfrom sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(X_train, y_train)\ny_pred = svm.predict(X_test)","90c26695":"# Evaluating the algorithm\n\nprint (confusion_matrix(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","516ea92e":"# Grid search\n\n# \"C\" controls the cost of misclassification on the training data. \n# A large C-value gives you low bias and high variance. Low bias causes you penalize the cost of misclassification a lot.\n\n# Small \"gamma\" means a Gaussian of a large variance. Large gamma leads to high bias and low variance in the model. \n\nparam_grid = {\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': [1, 0.1, 0.01, 0.001, 0.0001]\n}\n\ngs = GridSearchCV(SVC(), param_grid, verbose=3)\ngs.fit(X_train, y_train)","0b6a3673":"print(gs.best_params_)","026bc5f7":"# Training the tuned algorithm\n\nsvm_tuned = SVC(C = 10, gamma = 0.1)\nsvm_tuned.fit(X_train, y_train)\ny_pred = svm_tuned.predict(X_test)","f4c79b14":"# Evaluating the algorithm\n\nprint (confusion_matrix(y_test, y_pred))\nprint (classification_report(y_test, y_pred))\nprint ('Accuracy Score: ' + str(accuracy_score(y_test, y_pred)))","25278f5b":"Interestingly, females' scores are clustered relatively symetrically around the midpoint. Males, however, peak at the very bottom, very top, and the middle.","68ef5dc8":"We were able to classify a couple of more points correctly, but in general, an accuracy score of 0.65 is not good. It looks like we'd need more data (more features or larger dataset) to build a more robust model.","a3af9f23":"We have data on 200 customers. This is not very much. ","a12dea5e":"### **Feature scaling - standardizing the data**","495593e1":"### **KNN**","854b9f61":"This dataset is probably best suited for unsupervised ML techniques, but I was curious to see whether there are attributes that can help predict whether a customer is male or female. The attributes in question are Age, Income and Score. It's a small dataset (both in terms of features and number of customers), however I think this notebook gives a useful introduction to applying ML algorithms such as Random Forest, SVM and KNN.","2918f59a":"## **Data Preprocessing**","bbe473d8":"### **SVM**","ef4fe8be":"## **Exploratory Data Analysis**\n\nLet's look into the data. We start off by isolating the attributes and look at differences between the genders.","1d9e810c":"### **Train test split**","79d0bde3":"## **Machine Learning**\n\nCan we predict the gender of a customer (target variable) based on attributes such as age, income and score? We try training 3 different algorithms for this task - KNN, Random Forest, and SVM.","64a4cc07":"Not great, let's search for the best parameters using grid search.","022b1425":"Ouch, this ain't good. Nothing better than a random draw. Let's instead use grid search to find the best parameter values. Parameter tuning is the process to selecting the values for a model\u2019s parameters that maximize the accuracy of the model.","c00517cd":"0.35 is a very high error rate, but it is the best we're able to find. Let's now run the model again with k=17 again instead of k=1.","1c865f99":"Let's train the algorithm again, using the information from the grid search.","49b69723":"Age ranges between 18-70, income ranges between 15K-137K, and score ranges between 1-99.","ec8b2d2d":"### **Random Forest**\n\nLet's try the Random Forest algorithm instead. We have already scaled data and split into train and test sets.","7229c19d":"Slightly better than previously, but still not noticably different from a random draw.","f230d57b":"No significant correlations between the variables.","9bd14956":"## **Conclusion**\n\nNeither KNN, Random Forest nor the SVM algorithm are very useful in terms of predicting the gender of the customer based on the features Age, Income and Score. This indicates that the data does not have prediction capability. This doesn't come as a huge surprise, as we could already see in the EDA that there was little that suggested any major differences between the two genders when it came to these variables. However, the sample used is very small (n=200), and having more data might have given us higher accuracy scores.\n\nA high error rate indicates that the model is underfitting and has high bias. The model is not sufficiently complex, so it's simply not capable of representing the relationship between y and the input features. To combat this we could try increasing the number of input features.","806e8fe2":"The error rate is what we want to minimize, so we want to know the k that gives the smallest error rate. Let's create a visual representation to make life easier. ","47313978":"Let's train the algorithm again, using the information from the grid search.","a6663de7":"Slightly better than previously, but the model still has poor predictive ability.","08ed4285":"The algorithm doesn't do a better prediction than a random draw would. Let's find the k with the lowest error rate through iterations.","c1f8852a":"We see a pattern, although this is not a linear one. Interestingly there seems to be a cluster where most people who have income in the 40k-60k range have a score between 40-60. Anyone with income above or below this range seems to be at the extremes of the Score range - either very high or very low."}}