{"cell_type":{"c29eeff1":"code","2cfa293b":"code","6efffc4b":"code","9c45dd38":"code","cb4c17fb":"code","d7531331":"code","a9c399f5":"code","242f7501":"code","96ead35b":"code","8c04dcc3":"code","287fdd2c":"code","97d2be83":"code","e9749518":"code","ca00e326":"code","25869f58":"code","e26ad1f3":"code","8b372b2e":"code","1f20b057":"code","fb022834":"code","2f3e8f3a":"code","df27c789":"code","6fe7ea43":"code","1ed62e9d":"code","fee7e1da":"markdown","c1a6cc0b":"markdown","b4738d2b":"markdown","010cab9d":"markdown","24d79ef2":"markdown","61c755e7":"markdown","5d8e062e":"markdown","d6a18aa7":"markdown","13d7f03b":"markdown","928c1c79":"markdown","b7a596a6":"markdown","a6ffb492":"markdown","970a9385":"markdown","33645db7":"markdown","5f7054bc":"markdown","811dd249":"markdown","06024b9c":"markdown","d2d0ae7f":"markdown","ca4a292a":"markdown","ae18c564":"markdown"},"source":{"c29eeff1":"# Importing Data \n# Visualization and comments\n# Feature Engineering -- Encoding catorigical column ,genrating new features and Feature Selection \n# Modelling\n# Ensambling \n","2cfa293b":"!pip install autoviz","6efffc4b":"\nimport pandas as pd \nimport numpy as np \n\n## for Plottng and Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas_profiling\nfrom autoviz.AutoViz_Class import AutoViz_Class\nimport graphviz  # to visualse the decesion tree\nfrom yellowbrick.contrib.classifier import DecisionViz\nfrom mlxtend.plotting import plot_decision_regions\n\n## To apply NN\nfrom keras import  models\nfrom keras.layers import Dense\n\n# For encoding  categorical data\nimport category_encoders as ce\n\n\n## scikit Library for models and Feature Engineering\nfrom sklearn.model_selection import train_test_split\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom  sklearn.preprocessing import StandardScaler \nfrom sklearn.feature_selection import SelectFromModel\n\n\n","9c45dd38":"data=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('..\/input\/titanic\/test.csv')\nX_train, X_valid, y_train, y_valid = train_test_split( data.drop(columns=\"Survived\"), data.Survived, test_size=0.1, random_state=42)\nprint(\"This is how our data looks like\")\nX_train.head()\n# SibSp is the number of Siblings\/Spouses on board\n# Parch is the number of parents\/children on board\n# Pclass is the 1=1st , 2nd=2 and 3rd=3","cb4c17fb":"report = pandas_profiling.ProfileReport(X_train)\ndisplay(report)","d7531331":"# More Visualization \nAV = AutoViz_Class()\n \n# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz('..\/input\/titanic\/train.csv')","a9c399f5":"# encode sex column into 0 for female and 1 for male \nsex_enc=preprocessing.LabelEncoder()\nX_train[\"Sex_enc\"]=sex_enc.fit_transform(X_train[\"Sex\"])\nX_valid[\"Sex_enc\"]= sex_enc.transform(X_valid[\"Sex\"])\ntest_data[\"Sex_enc\"]=sex_enc.transform(test_data[\"Sex\"])\n\n\n#to avoid the model of creating a bais, standard scale Fare cus it has large scale \nFare_std_scaler=StandardScaler()\nX_train[\"std_Fare\"]=Fare_std_scaler.fit_transform(np.array(X_train.Fare).reshape(-1,1))\nX_valid[\"std_Fare\"]=Fare_std_scaler.transform(np.array(X_valid.Fare).reshape(-1,1))\ntest_data[\"std_Fare\"]=Fare_std_scaler.transform(np.array(test_data.Fare).reshape(-1,1))\n\n# Generate a family col and we will see if it is uselful\nX_train[\"Family\"]=X_train[\"SibSp\"]+X_train[\"Parch\"]\nX_valid[\"Family\"]=X_valid[\"SibSp\"]+X_valid[\"Parch\"]\ntest_data[\"Family\"]=test_data[\"SibSp\"]+test_data[\"Parch\"]\n\n## encode_Family\nFamily_enc_tar=ce.TargetEncoder(X_train[\"Embarked\"])\nX_train[\"Family_enc_tar\"]=Family_enc_tar.fit_transform(X_train[\"Family\"],y_train)\nX_valid[\"Family_enc_tar\"]= Family_enc_tar.transform(X_valid[\"Family\"])\ntest_data[\"Family_enc_tar\"]=Family_enc_tar.transform(test_data[\"Family\"])\n\n# encode the ticket using targetencoding to see if the holders of the same ticket number have any relationship helped them to survive together\n\nTicket_enc =ce.TargetEncoder(X_train[\"Ticket\"])\nX_train[\"Ticket_enc\"]=Ticket_enc.fit_transform(X_train[\"Ticket\"],y_train)\nX_valid[\"Ticket_enc\"]=Ticket_enc.transform(X_valid[\"Ticket\"])\ntest_data[\"Ticket_enc\"]=Ticket_enc.transform(test_data[\"Ticket\"])\n\n# fill nan values with the most major category('S') then Encode Embarked col\nX_train[\"Embarked\"].fillna(value='S',inplace=True)\nEmbar_enc=preprocessing.LabelEncoder()\nX_train[\"Embarked_enc\"]=Embar_enc.fit_transform(X_train[\"Embarked\"])\nX_valid[\"Embarked_enc\"]= Embar_enc.transform(X_valid[\"Embarked\"])\ntest_data[\"Embarked_enc\"]=Embar_enc.transform(test_data[\"Embarked\"])\n\n# try to encode Embarked col with Target encoder to see directly if there is a relationship btween where you embarkd and your survival \nEmbar_enc_tar=ce.TargetEncoder(X_train[\"Embarked\"])\nX_train[\"Embarked_enc_tar\"]=Embar_enc_tar.fit_transform(X_train[\"Embarked\"],y_train)\nX_valid[\"Embarked_enc_tar\"]= Embar_enc_tar.transform(X_valid[\"Embarked\"])\ntest_data[\"Embarked_enc_tar\"]=Embar_enc_tar.transform(test_data[\"Embarked\"])\n\n# Extract Capin Letter and fill nan values based on class \n#X_train[\"Cabin_letter\"]=X_train[\"Cabin\"].str.extract(pat = '([A-Z])')\n#print (X_train.groupby(\"Cabin_letter\").Pclass.describe() )\n\n# Create new title column to help us infer age and standardze the values\nX_train[\"Title\"]=X_train[\"Name\"].str.split(',',expand=True)[1].str.split('.',expand=True)[0]\nX_valid[\"Title\"]=X_valid[\"Name\"].str.split(',',expand=True)[1].str.split('.',expand=True)[0]\ntest_data[\"Title\"]=test_data[\"Name\"].str.split(',',expand=True)[1].str.split('.',expand=True)[0]\n\n## Target encode the title column\nTitle_enc_tar=ce.TargetEncoder(X_train[\"Title\"])\nX_train[\"Title_enc_tar\"]=Title_enc_tar.fit_transform(X_train[\"Title\"],y_train)\nX_valid[\"Title_enc_tar\"]= Title_enc_tar.transform(X_valid[\"Title\"])\ntest_data[\"Title_enc_tar\"]=Title_enc_tar.transform(test_data[\"Title\"])\n\n\n## fill Nan values of Age\navg_age_per_title=X_train.groupby(\"Title\").Age.mean()\nintermediate_df_train=X_train[X_train.Age.isnull()]\nintermediate_df_train.Age=avg_age_per_title[intermediate_df_train.Title].values.astype(int)\nX_train.Age.fillna(intermediate_df_train.Age,inplace=True)\n\nintermediate_df_valid=X_valid[X_valid.Age.isnull()]\nintermediate_df_valid.Age=avg_age_per_title[intermediate_df_valid.Title].values.astype(int)\nX_valid.Age.fillna(intermediate_df_valid.Age,inplace=True)\n\nintermediate_df_test=test_data[test_data.Age.isnull()]\nintermediate_df_test.Age=avg_age_per_title[intermediate_df_test.Title].values.astype(int)\ntest_data.Age.fillna(intermediate_df_test.Age,inplace=True)\n\n\n\nAge_std_scaler=StandardScaler()\nX_train[\"std_Age\"]=Age_std_scaler.fit_transform(np.array(X_train.Age).reshape(-1,1))\nX_valid[\"std_Age\"]=Age_std_scaler.transform(np.array(X_valid.Age).reshape(-1,1))\ntest_data[\"std_Age\"]=Age_std_scaler.transform(np.array(test_data.Age).reshape(-1,1))\n\n\n## deopping unneeded cols.\nX_train1=X_train.drop(columns=['Name','Cabin','Ticket','Sex','Age','Ticket','Fare','Embarked','Title','Family'])\nX_valid1=X_valid.drop(columns=['Name','Cabin','Ticket','Sex','Age','Ticket','Fare','Embarked','Title','Family'])\ntest_data1=test_data.drop(columns=['Name','Cabin','Ticket','Sex','Age','Ticket','Fare','Embarked','Title','Family'])","242f7501":"log_reg = LogisticRegression(penalty='l1',solver='liblinear',C=0.05).fit(X_train1, y_train)\nselector = SelectFromModel(log_reg, prefit=True)\nX_new = selector.transform(X_train1)\nX_train_selected_features=pd.DataFrame(selector.inverse_transform(X_new),columns=X_train1.columns)\nX_valid_selected_features=X_valid1.copy()\ntest_data_selected_features=test_data1.copy()\nfor i in X_train_selected_features.columns:\n    if X_train_selected_features[i].mean()==0:\n        X_train_selected_features.drop(columns=i,inplace=True)\n        X_valid_selected_features.drop(columns=i,inplace=True)\n        test_data_selected_features.drop(columns=i,inplace=True)\n        \n        \nprint(\"So here is the set of the selected Features ..... \\n \\n\",X_train_selected_features.columns)","96ead35b":"# helping Function\ndef to_np_arr(arr):\n    return np.array(arr).reshape(-1,1)\n\n","8c04dcc3":"\nmodel_gender=tree.DecisionTreeClassifier(max_depth=5)\nmodel_gender.fit(to_np_arr(X_train_selected_features.Sex_enc),to_np_arr(y_train))\nmodel_gender_score= model_gender.score(to_np_arr(X_train_selected_features.Sex_enc),to_np_arr(y_train))\nvalid_gender=model_gender.score(to_np_arr(X_valid_selected_features.Sex_enc),to_np_arr(y_valid))\n\nmodel_age=tree.DecisionTreeClassifier(max_depth=5)\nmodel_age.fit(to_np_arr(X_train_selected_features.std_Age),to_np_arr(y_train))\n#tree.plot_tree(model_gender)\nmodel_age_score= model_gender.score(to_np_arr(X_train_selected_features.std_Age),to_np_arr(y_train))\nvalid_age=model_gender.score(to_np_arr(X_valid_selected_features.std_Age),to_np_arr(y_valid))\n\n\nmodel_class=tree.DecisionTreeClassifier(max_depth=5)\nmodel_class.fit(to_np_arr(X_train_selected_features.Pclass),to_np_arr(y_train))\nmodel_class_score= model_class.score(to_np_arr(X_train_selected_features.Pclass),to_np_arr(y_train))\nvalid_class=model_gender.score(to_np_arr(X_valid_selected_features.Pclass),to_np_arr(y_valid))\n\nprint(\"Training Accurcy for gender only model is \\n \\n Training Acc:\",model_gender_score,\"\\n Valid Acc:\",valid_class,\" \\n \\n Training Acc for Age only model is \\n\",\"\\n Training Acc\",model_age_score,\"\\n Valid Acc:\",valid_age,\" \\n \\nTraining Acc for PClass only model is \\n \",\"\\n Training Acc\",model_class_score,\"\\n valid acc :\",valid_class)","287fdd2c":"fn=['Sex_enc']  \ncn=['NOT_Survived',\"Survived\"]  #sorted ascending numerically so notsurvived=0 first \nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=300)\ntree.plot_tree(model_gender,\n               feature_names = fn, \n               class_names=cn,\n               filled = True);\nfig.savefig('imagename.png')","97d2be83":"gender_class=[\"Pclass\",\"Sex_enc\"]\nmodel2=tree.DecisionTreeClassifier(max_depth=6)\nmodel2.fit(X_train_selected_features[gender_class],to_np_arr(y_train))\nmodel2_score= model2.score(X_train_selected_features[gender_class],to_np_arr(y_train))\nvalid_model2=model2.score(X_valid_selected_features[gender_class],to_np_arr(y_valid))\n\nprint(\"Training Acc for the two-Features model is \",model2_score,\"\\nValidation Acc: \",valid_model2 ,\"\\nOk..This is the best till now!!\")\n","e9749518":"# This plot shows the  decision boundary for the social class and age  for tree of depth=10\nX_=X_train_selected_features[gender_class].to_numpy()\nplot_decision_regions(X_,to_np_arr(y_train).flatten(), clf=model2, legend=2)\n\n# Adding axes annotations\nplt.xlabel('P_Class')\nplt.ylabel('Gender')\nplt.title('model')\nplt.show()","ca00e326":"parameters = {\n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_desicion_tree = tree.DecisionTreeClassifier(\n    random_state=1,\n    class_weight='balanced',\n)\n\nmodel_desicion_tree = GridSearchCV(\n    model_desicion_tree, \n    parameters, \n    cv=30,\n    scoring='accuracy',\n)\nmodel_desicion_tree.fit(X_train_selected_features,to_np_arr(y_train))\nprint(\"chosen param is \",model_desicion_tree.best_params_,\"Training Acc when applying chosen params\",model_desicion_tree.best_score_)\nprint(\"Validation acc : \", model_desicion_tree.score(X_valid_selected_features,to_np_arr(y_valid)))","25869f58":"parameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25], \n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\nrf_model=GridSearchCV(RandomForestClassifier( random_state=0),parameters,cv=30,scoring='accuracy')\nrf_model.fit(X_train_selected_features,to_np_arr(y_train))\nprint(\"chosen params are \",rf_model.best_params_,\"Training Acc when applying chosen params\",rf_model.best_score_)\nprint(\"Validation acc : \", rf_model.score(X_valid_selected_features,to_np_arr(y_valid)))","e26ad1f3":"model_NN = models.Sequential()\nmodel_NN.add(Dense(60, activation='relu'))\nmodel_NN.add(Dense(40, activation='relu'))\nmodel_NN.add(Dense(20, activation='relu'))\nmodel_NN.add(Dense(10, activation='relu'))\nmodel_NN.add(Dense(1, activation='sigmoid'))\nmodel_NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory=model_NN.fit(X_train_selected_features,to_np_arr(y_train),epochs=150, batch_size=10) ","8b372b2e":"plt.plot(history.history[\"accuracy\"])\nplt.xlabel(\"number of Iterations\")\nplt.ylabel(\"accuracy\")\nplt.title(\"accuracy vs iterations \")\n\n\n# evaluate the keras model\n_, accuracy = model_NN.evaluate(X_valid_selected_features,to_np_arr(y_valid))\nprint('Accuracy on Validation Set: %.2f' % (accuracy*100))","1f20b057":"preprocessor=preprocessing.PolynomialFeatures(degree=1)\nfs_poly_train=preprocessor.fit_transform(X_train_selected_features)\nfs_poly_valid=preprocessor.transform(X_valid_selected_features)\n\nlog_model = LogisticRegression(random_state=0 ,penalty='l2',C=0.1).fit(fs_poly_train, to_np_arr(y_train))\nprint(\"Training acc of logistic regression model with first degree \",log_model.score(fs_poly_train,to_np_arr(y_train)))\nprint(\"\\nValidation Acc:\",log_model.score(fs_poly_valid, to_np_arr(y_valid)))","fb022834":"parameters={\n    \"C\":[0.1 ,1, 10 ],\n    \"degree\":[1 ,2 ,3 ],\n       \n    \n}\n\n#model_SVC_pred=GridSearchCV(SVC(kernel='linear'),parameters,cv=5,scoring='accuracy')\nmodel_SVC_pred=SVC(C= 1, degree= 3, gamma=1, kernel= 'linear')\nmodel_SVC_pred.fit(X_train_selected_features,to_np_arr(y_train))\nmodel_SVC_pred_score=model_SVC_pred.score(X_train_selected_features,to_np_arr(y_train))","2f3e8f3a":"valid_acc_svc=model_SVC_pred.score(X_valid_selected_features,to_np_arr(y_valid))\n#print(\"chosen params are \",model_SVC_pred.best_params_,\"Training Acc when applying chosen params\",model_SVC_pred.best_score_)\nprint(\"Training Acc:\",model_SVC_pred_score,\"\\n Validation Acc:\" ,valid_acc_svc)\n","df27c789":"tree_pred_train = model_desicion_tree.predict(X_train_selected_features)\nrf_train_pred = rf_model.predict(X_train_selected_features)\nnn_pred_train = model_NN.predict_classes(X_train_selected_features)\nlog_train_pred = log_model.predict(preprocessor.transform(X_train_selected_features))\nsvc_train_pred=model_SVC_pred.predict(X_train_selected_features)\n\nall_models_train=pd.DataFrame({\"tree_pred_train\":tree_pred_train,\"rf_train_pred\":rf_train_pred,\"nn_pred_train\":nn_pred_train.flatten(),\"log_train_pred\":log_train_pred,'svc':svc_train_pred},index=X_train_selected_features.index)\n\n\ntree_pred_valid = model_desicion_tree.predict(X_valid_selected_features)\nrf_valid_pred = rf_model.predict(X_valid_selected_features)\nnn_pred_valid = model_NN.predict_classes(X_valid_selected_features)\nlog_valid_pred = log_model.predict(preprocessor.transform(X_valid_selected_features))\nsvc_valid_pred=model_SVC_pred.predict(X_valid_selected_features)\n\nall_models_valid=pd.DataFrame({\"tree_pred_valid\":tree_pred_valid,\"rf_valid_pred\":rf_valid_pred,\"nn_pred_valid\":nn_pred_valid.flatten(),\"log_valid_pred\":log_valid_pred,'svc':svc_valid_pred})\n#all_models_pred.join(pd.Series({'nn_pred_train':nn_pred_train})[0])\n#mean_train_pred = np.round((rf_train_pred + SVC_train_pred + tree_pred_train + log_train_pred ) \/ 4)\n## Creating a data frame of the prev predictions \n\n\nparameters={\"degree\":[1,2,3,4,5],\"gamma\":[1, 0.1, 0.001, 0.0001, 'auto'],\"kernel\":['linear', 'poly', 'rbf']}\n\n#Ensamling_pred=GridSearchCV(SVC(C=0.01),parameters,cv=5,scoring='accuracy')\nEnsamling_pred=SVC(C= 0.01,kernel='linear',gamma=1)\nEnsamling_pred.fit(all_models_train, to_np_arr(y_train))\nEnsamling_pred_score=Ensamling_pred.score(all_models_train,to_np_arr(y_train))\n#print(\"chosen params are \",Ensamling_pred.best_params_,\"Training Acc when applying chosen params\",Ensamling_pred.best_score_)\nvalid_score_Ensambling=Ensamling_pred.score(all_models_valid,to_np_arr(y_valid))\nprint(\"\\n Training Score\",Ensamling_pred_score,\"\\nValidation Acc:\",valid_score_Ensambling)","6fe7ea43":"# Let's look at the coeff of the last model \n\nweighted_avg=Ensamling_pred.coef_\nprint(\"coeff of Decision Tree Model\",weighted_avg[0][0])\nprint(\"coeff of Random Forest Model\",weighted_avg[0][1])\nprint(\"coeff of NN  Model\",weighted_avg[0][2])\nprint(\"coeff of Logestic Regression Model\",weighted_avg[0][3])\nprint(\"coeff of SVC Model\",weighted_avg[0][4])\n\n","1ed62e9d":"tree_pred_test= model_desicion_tree.predict(test_data_selected_features)\nrf_test_pred = rf_model.predict(test_data_selected_features)\nnn_pred_test = model_NN.predict_classes(test_data_selected_features)\nlog_test_pred = log_model.predict(preprocessor.transform(test_data_selected_features))\nsvc_test_pred=model_SVC_pred.predict(test_data_selected_features)\n\nall_models_test=pd.DataFrame({\"tree_pred_valid\":tree_pred_test,\"rf_valid_pred\":rf_test_pred,\"nn_pred_valid\":nn_pred_test.flatten(),\"log_valid_pred\":log_test_pred,'svc':svc_test_pred})\nypred_test=Ensamling_pred.predict(all_models_test)\nsub_file=pd.DataFrame({\"PassengerId\":test_data.PassengerId,\"Survived\":ypred_test},dtype=np.int64)\n\nsub_file.to_csv(\"sub_file.csv\",index=False)","fee7e1da":"## Learning our models\n##### First, I will start learning using one  or two features just to have get some insight","c1a6cc0b":"### III.NN","b4738d2b":"\n1. #### From the previous coeffs, the biggest contribution belongs to  Random Forest","010cab9d":"##### II.Feature Selection  using L1 penality ","24d79ef2":"### IV. Logistic Regression","61c755e7":"### Achieved 0.78225 on Test Data ","5d8e062e":"### II.RandomForestClassifier","d6a18aa7":"#### Better *Dancing and praying it will do the same on the test set","13d7f03b":"# Ensembling Classifier\n(learn a weighted average of each one of them , get them working together, get higher Acc , Hopfully)\n","928c1c79":"# All Features Models\n### I.DecisionTreeClassifier","b7a596a6":"##### Looks like as we predicted Passeneger Gender has a lot to do with their Survival!  \n###### Lets see that more ","a6ffb492":"#### Two-Feature model","970a9385":"#### knowing that Female is encoded withh 0 and male with 1 ,  The previous fig shows that the model predicts high survival for females ","33645db7":"* ## Aloooot Better whoooohooo ","5f7054bc":"# Feature Engineering\n##### I. Encoding Categorical Columns ","811dd249":"### Let's finish with \n##### V. Support Support Vector Classification ","06024b9c":"# Output","d2d0ae7f":"The above visualization of the decision boundary shows that survival favors female of class 1","ca4a292a":"# Data Visualization","ae18c564":"### Some Commets after exploring our data :\n\n\n##### Caterogrocal Columns: Embarked, Capin, Sex, Name and Ticket\n##### Ticket coloumn  has a high cardinality but not all distinct so there are more than one person with the same ticket and they may have any kind of a relationship helped in their survival..let's see\n##### Fare column has a wide range from 0 to 512 with high variance \n##### From the corrolation Matrix , there is a negative corrolation between class and survival \n##### We can make a use of the capin col as I noticed for example that passnegers of  class 3 were in F,G and E capins\n##### There is a strong corrolation between Class and Fare which makes sense\n##### There are aloooot of missing values from age col but we can infer it from the title in the name col \n##### There are also many missing values in Fare col , we can fill it using the negative relationhip between Fare and Class \n##### 68% of survivors were females despite that 64% of passengers were males\n\n\n"}}