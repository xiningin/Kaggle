{"cell_type":{"b3a2ff0c":"code","367a69f0":"code","b943c631":"code","c9947808":"code","138b420b":"code","8dc36a56":"code","5b67f173":"code","884696bd":"code","a3462e11":"code","ace194e8":"code","f929ddf9":"code","805fca8b":"code","763eec87":"code","07dede75":"code","ad03e777":"code","e4244326":"code","1629dbb4":"code","f12b74cc":"code","acb86a9c":"code","152e0913":"code","fd7d6d2a":"code","a80cf5cd":"code","4f7ec20f":"code","9394b150":"code","4fe3a684":"code","87866838":"code","e2ff44d0":"code","c24580df":"code","70a26ed1":"code","01bc1c6f":"code","456e7868":"code","edc3756e":"code","0d9ed8d7":"code","29d596ac":"code","845b2a07":"code","a7b1b1c6":"code","afd86eb7":"code","eda487c5":"code","0f7362da":"code","cd47b6a3":"code","3efeba44":"code","b0051a75":"code","f3b283d8":"code","ad2e44ac":"code","16283e3a":"code","4a1ab6a4":"code","25796e02":"code","aa562fc1":"code","079e2d18":"code","3ff7a533":"code","efe9db7c":"code","ed3ab69c":"code","95516b87":"code","bb4e3cff":"code","cd824740":"code","14e184f2":"code","d2a2ccf1":"code","0f595b2d":"code","135c6e4b":"code","8ebcf5d3":"code","d81ac578":"code","3fd12ffa":"code","423b1da3":"code","6e9d44e7":"code","b0c248e3":"code","720775fb":"code","66a549f0":"code","e87a2471":"code","7d16ab3b":"code","9d58f73f":"code","2fd0577a":"code","97c40161":"code","99488500":"code","6ae03de5":"code","8987efcb":"code","cd16715e":"code","0d23240e":"markdown","ad2324c1":"markdown","b7c0c81f":"markdown","554002d3":"markdown","6ea636e1":"markdown","d167d20d":"markdown","d4e48cbf":"markdown","f9c5e9b8":"markdown","c34cac3f":"markdown","58256e7b":"markdown","ebe8f205":"markdown","57892e91":"markdown","c9d9237d":"markdown","b38652f5":"markdown","0afd8574":"markdown","7c4fb435":"markdown","4c9df748":"markdown","499e4b57":"markdown","ddb0dca2":"markdown","44ad0a9b":"markdown","fe739e8d":"markdown","41e07ce7":"markdown","4224ce84":"markdown","5c6889e2":"markdown","a47ae880":"markdown","96705ad9":"markdown","26aa912f":"markdown","4e2b8096":"markdown","17e89bd7":"markdown","e121c80d":"markdown","e6ae8e5e":"markdown","823ef589":"markdown","ad81baaf":"markdown","85d391a7":"markdown","22cf25f5":"markdown","88dd12fa":"markdown","a237f61a":"markdown","d505defe":"markdown","6cc14bda":"markdown","ed47815f":"markdown","2d52ee6b":"markdown"},"source":{"b3a2ff0c":"import nltk\nimport pandas as pd \nimport  numpy as np\nfrom collections import Counter\nfrom ds_exam import *\nfrom update_time import *\nfrom bag_words import *","367a69f0":"france = pd.read_csv(\"\/kaggle\/input\/coronavirus-france-dataset\/patient.csv\")\ntunisia = pd.read_csv(\"..\/input\/coronavirus-tunisia\/Coronavirus_Tunisia.csv\")\njapan = pd.read_csv(\"\/kaggle\/input\/close-contact-status-of-corona-in-japan\/COVID-19_Japan_Mar_07th_2020.csv\")\nindonesia = pd.read_csv(\"\/kaggle\/input\/indonesia-coronavirus-cases\/patient.csv\")\nkorea = pd.read_csv(\"\/kaggle\/input\/coronavirusdataset\/PatientInfo.csv\")\nHubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/Hubei.csv\")\noutside_Hubei = pd.read_csv(\"\/kaggle\/input\/covid19official\/outside_Hubei.csv\")\nworld_a = pd.read_csv(\"\/kaggle\/input\/covid19-outbreak-realtime-case-information\/latestdata.csv\")\n","b943c631":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ndatasets_name = [\"france\", \"tunisia\", \"japan\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\", \"world\"]\n\ngarbge = [print(\"\\n\"+datasets_name[i], [i for i in datasets[i].columns]) for i in range(len(datasets_name))]","c9947808":"p = ['age', 'sex', 'city', 'province', 'country',\"age\", 'date_confirmation', 'date_onset_symptoms',\n     'date_admission_hospital', 'symptoms', 'source']\nl = ['ID', 'age', 'sex', 'city', 'province', 'country', 'wuhan(0)_not_wuhan(1)', 'geo_resolution',\n     'date_onset_symptoms', 'date_admission_hospital', 'date_confirmation', 'symptoms',\n     'lives_in_Wuhan', 'travel_history_dates', \n      'chronic_disease_binary']\n\n\ndf1 = Hubei.loc[:, l]\n\ndf2 =  outside_Hubei.loc[:, l]\n\ndf_diff = pd.concat([df2,df1]).drop_duplicates()\n\nwww =  df_diff\n\n\ndf2 =  world_a.loc[:, l]\n\ndf_diff = pd.concat([df2,www]).drop_duplicates()\n\nmm = df_diff\n\n","138b420b":"e = []\nfor ind in range(len(mm['date_confirmation'])):\n    if type(mm['date_confirmation'][ind]) == pd.core.series.Series:\n        e.append(ind)","8dc36a56":"mm['date_confirmation'][(mm['date_confirmation'][21164].notnull()) == ]","5b67f173":"# ls_i = [i  for i in mm.date_confirmation[mm['date_confirmation'].notnull()]]\n\n# indexs = mm.index[mm['date_confirmation'].notnull()]\n# dataset =mm\n# input_col = 'date_confirmation'\n# output_col ='date_confirmation'\n# date_character= \".\"\n# character_separator =  \"-\"\n\n# d = []\n\n# for i in ls_i:\n\n#     p = i.split(character_separator)\n#     p = UpdateTime.del_str_equal_x_from_ls(p, \"\")\n#     boo = UpdateTime.redundant_numbers_date(p, date_character)\n#     if len(p) == 1:\n#             d.append(p)\n            \n# mm['date_confirmation']            \n# # mm['date_confirmation'].max()","884696bd":"o = []\nfor i in range(len(datasets)):\n    print(datasets_name[i],datasets[i].shape)\n    o.append(datasets[i].shape[0])\nprint(\"\\nnum of i \" + str(sum(o)))","a3462e11":"france.rename(columns={\"health\":\"severity_illness\",\"status\":\"treatment\",\"infection_reason\":\"infection_place\"}\n              , inplace = True)\n\ntunisia.rename(columns={\"date\":\"confirmed_date\", \"gender\":\"sex\", \"situation\":\"severity_illness\", \n                        \"return_from\":\"infection_place\", \"health\":\"background_diseases\"}, inplace = True)\n\njapan.rename(columns={\"No.\":\"id\", \"Fixed date\":\"confirmed_date\",\"Age\":\"age\", \"residence\":\"region\",\n                      \"Surrounding patients *\":\"infected_by\"}, inplace = True)\n\nindonesia.rename(columns={\"patient_id\":\"id\",\"gender\": \"sex\", \"province\":\"region\", \"hospital\":\"hospital_name\",\n                          \"contacted_with\":\"infected_by\", \"current_state\":\"severity_illness\"}, inplace = True)\n\nkorea.rename(columns={\"patient_id\":\"id\", \"disease\":\"background_diseases_binary\", \"state\":\"severity_illness\",\n                      \"province\":\"region\", \"infection_case\" :\"infection_place\",\n                      \"symptom_onset_date\":\"date_onset_symptoms\"}, inplace = True)\n\nHubei = Hubei.rename(columns={ \"province\":\"region\",\"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\"})\n\noutside_Hubei = outside_Hubei.rename(columns={ \"province\":\"region\", \"date_confirmation\": \"confirmed_date\",\n                              \"chronic_disease_binary\":\"background_diseases_binary\", \n                              \"chronic_disease\":\"background_diseases\", \"outcome\":\"severity_illness\" })\n","ace194e8":"def format_datatime(dataset, input_col, output_col, indexs, date_character, character_separator, earliest=False):\n\n    drop = []\n    indexs_error = []\n    for indx in indexs:\n        i = dataset.loc[indx, input_col]\n        print(i)\n#         ls = i.split(character_separator)\n#         ls = UpdateTime.del_str_equal_x_from_ls(ls, \"\")\n#         boo = UpdateTime.redundant_numbers_date(ls, date_character)\n#         if boo == False:\n#                 indexs_error.append(indx)\n#         else:\n#             if len(ls) > 1:\n#                 ls = UpdateTime.make_ls_of_str_datatime(ls)\n#                 value = UpdateTime.time_range_extremity(ls, earliest)\n#                 dataset.loc[indx, output_col] = value\n#                 drop.append(indx)\n#     indexs = indexs.drop(drop)\n#      return indexs, indexs_error\n\nformat_datatime(mm,'date_confirmation','date_confirmation' ,indexs ,\".\",[ \"-\"])","f929ddf9":"# 'travel_history_dates'\n\n# world['date_confirmation'] = world['date_confirmation'].apply(pd.to_datetime)\nindexs = world.index[world['date_confirmation'].notnull()]\nindexs_ , error = UpdateTime.updte_time(world,'date_confirmation','date_confirmation' ,[1],\".\",[ \"-\"])\n","805fca8b":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)","763eec87":"def full_common(exam_df):\n    \"\"\"\n    Returns columns that all DATASETS have\n    \"\"\"\n    full_common = []\n    for j in exam_df.columns:\n        boolyan = exam_df[j].all()\n        if boolyan == True:\n            full_common.append(j)\n    return full_common","07dede75":"common = []\nunique = []\nblank = []\nfor i in exam_df.columns:\n    if exam_df[i].value_counts()[True]>1:\n        common.append(i)\n    elif exam_df[i].value_counts()[True]==1:\n        unique.append(i)\n    else:\n        blank.append(i)\n        \n        \nprint(common)\nprint(unique)\nprint(blank)   ","ad03e777":"for x in [outside_Hubei, Hubei]:\n    l = x.index[x.country_new.notnull() == True]\n    p = []\n    for i in l:\n        if x.country_new[i] == x.country[i]:\n            p.append(i)\n\n    print(\"country_new == country\",len(p))\n    print(\"country_new.notnull\",len(l),\"\\n\")","e4244326":"for x in [outside_Hubei, Hubei]:\n    m =[]\n    for i in range(len(x)):\n        if x.ID[i] != str(i+1):\n            m.append(i)\n    print(m[0],x.ID[m[0]])","1629dbb4":"france = france.drop([\"departement\",\"source\",\"comments\",\"contact_number\"],axis=1)\n\n\nindonesia = indonesia.drop(['nationality'],axis=1)\n\nkorea = korea.drop([\"age\",\"contact_number\"],axis=1)\n\nHubei = Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                    'geo_resolution','admin_id', \"country_new\",\"source\",\"additional_information\",\"geo_resolution\"\n                    ,\"notes_for_discussion\"],axis=1)\n\noutside_Hubei = outside_Hubei.drop([\"ID\",'location', 'admin3', 'admin2', \"admin1\" ,'latitude', 'longitude',\n                                    'geo_resolution', 'admin_id', \"country_new\", \"data_moderator_initials\",\n                                    \"source\",\"additional_information\",\"geo_resolution\",\n                                    \"notes_for_discussion\"],axis=1)","f12b74cc":"def examining_values_by_col (datasets, datasets_name, col):\n    \"\"\"\n    Prints values of each DF per column\n    \"\"\"\n    counter = 0\n    \n    for i in datasets:\n        if col in i.columns:\n            print(\"\\n\" + datasets_name[counter])\n            print(i[col].value_counts())\n        counter =counter + 1","acb86a9c":"datasets = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei, world]\ncolumns_name = Exam.build_columns_name_ls(datasets)\nexam_df = Exam.df_exam_columns_dfs(datasets,datasets_name,columns_name)\n\nfor j in exam_df.columns[1:len(exam_df.columns)]:\n    print(j)\n    examining_values_by_col (datasets , datasets_name , j) ","152e0913":"l1= tunisia.index[tunisia[\"return_date\"] == \"Local\"]\nl2 = tunisia.index[ tunisia[\"return_date\"].notnull()]\n\nindex = l2.drop(l1)\n\nfor indx in index:\n    tunisia.loc[indx,\"return_date\"] = pd.to_datetime(tunisia.loc[indx,\"return_date\"])","fd7d6d2a":"# cols = [\"confirmed_date\",\"released_date\", \"deceased_date\"]\n\n# france[cols] = france[cols].apply(pd.to_datetime)\n# indonesia[cols] = france[cols].apply(pd.to_datetime)\n# japan[cols] = france[cols].apply(pd.to_datetime)\n# korea[cols] = korea[cols].apply(pd.to_datetime)\n\n# #### different#####\n\n# # korea\n# korea_col = [\"date_onset_symptoms\"]\n# korea[korea_col] = korea[korea_col].apply(pd.to_datetime)\n\n# #  tunisia\n# tunisia_col = [\"confirmed_date\"]\n# tunisia[tunisia_col] = tunisia[tunisia_col].apply(pd.to_datetime)\n\n# # Hubei\n# Hubei_col = [\"confirmed_date\", \"date_death_or_discharge\", \"date_onset_symptoms\"]\n# Hubei[Hubei_col] = Hubei[Hubei_col].apply(pd.to_datetime)\n\n# # outside_Hubei\n# outside_Hubei_col = [\"date_death_or_discharge\"]\n# outside_Hubei[outside_Hubei_col] = outside_Hubei[outside_Hubei_col].apply(pd.to_datetime)\n\n# # 'travel_history_dates'\n# for j in [\"confirmed_date\", \"date_onset_symptoms\"]:\n#     indexs = outside_Hubei.index[outside_Hubei[j].notnull()]\n#     indexs_ , error = UpdateTime.updte_time(outside_Hubei, j, j, indexs,\".\",[ \"-\", ','])\n#     print(j , error)","a80cf5cd":"# error\noutside_Hubei.index[outside_Hubei[\"date_onset_symptoms\"] == \"- 25.02.2020\"]","4f7ec20f":"examining_values_by_col (datasets , datasets_name , \"date_onset_symptoms\") ","9394b150":"tunisia_sex = {\"F\":\"female\", \"M\":\"male\",np.nan:np.nan}\ntunisia.sex = [tunisia_sex[item] for item in  tunisia.sex] \n\njapan_sex = {\"Woman\":\"female\", \"Man\":\"male\",np.nan:np.nan, \"Checking\":np.nan, \"investigating\":np.nan}\njapan.sex = [japan_sex[item] for item in  japan.sex] \n\nfrance_sex = {\"female\":\"female\", \"male\":\"male\",\"Female\":\"female\", \"Male\":\"male\", \"male\\xa0?\":\"male\", \n              np.nan:np.nan }\nfrance.sex = [france_sex[item] for item in  france.sex] \n","4fe3a684":"examining_values_by_col(datasets, datasets_name, \"sex\")","87866838":"# indexs = Hubei.index[Hubei.city.notnull()]\n# for indx in indexs:\n#     i = Hubei.loc[indx, \"city\"]\n#     i = i.split(\" \")\n#     print(i)\n#     if len(i)> 1:\n#         ls_value = del_str_equal_x_from_ls(i, \"City\")\n#         for i in ls_value:\n#         Hubei.loc[indx, \"city\"] = value","e2ff44d0":"Hubei.loc[5, \"city\"]","c24580df":"examining_values_by_col(datasets, datasets_name, \"city\")","70a26ed1":"def update_index(dataset, col, indexs, data):\n    \"\"\"\n    Value change according index\n    \n    dataset: df\n    \n    col : str\n        name of col you want to change\n        \n    indexs: pd.index\n    \n    data: int\/ str\/ float\n        data you want to into\n    \n    \"\"\"\n    for indx in indexs:\n        dataset.loc[indx,col] = data","01bc1c6f":"indexs = korea.index[korea.background_diseases_binary == True]\nupdate_index(korea, \"background_diseases_binary\", indexs, 1.0)","456e7868":"tunisia[\"background_diseases_binary\"] = np.nan\n\nfor dataset in [tunisia, Hubei, outside_Hubei]:\n    indexs = dataset.index[dataset.background_diseases.notnull()]\n    update_index(dataset,\"background_diseases_binary\",indexs,1.0) ","edc3756e":"examining_values_by_col (datasets, datasets_name, \"background_diseases_binary\")","0d9ed8d7":"examining_values_by_col (datasets, datasets_name, \"background_diseases\")","29d596ac":"# ps = nltk.stem.SnowballStemmer('english')\n\n# for i in outside_Hubei.background_diseases[outside_Hubei.background_diseases.notnull()]:\n#     print(\"\\n\"+i)\n#     i = BagWords.clean_str(i)\n    \n#     print([ps.stem(x) for x in i ])\n    \n# # chronic obstructive pulmonary disease  'obstruct', 'pulmonari', 'diseas'","845b2a07":"# a_bag_words= {\"hypertension\":[\"hypertens\",  ],\n            \n#             \"coronary heart disease\":[\"stenocardia\" ],\n            \n#             \"diabetes\":['diabet', \"mellitus\"],\n               \n#             \"tuberculosis\": [\"tuberculosi\"],\n            \n#             \"parkinson\": [\"parkinso\", \"madopar\"],\n           \n#             \"hypertriglyceridemia\": ['hypertriglyceridemia',],\n            \n#             \"obesity\": ['obes',],\n             \n#             \"Chronic obstructive pulmonary\": ['copd',],\n            \n#             \"hiv\": [\"hiv\",],\n#            \"asthma\": [\"asthma\",],}\n\n\n\n# a_sentences_bag = {\"hypertension\":[['high', 'blood', 'pressur'],],\n\n#                 \"coronary heart disease\": [['coronari', 'heart'], ['coronari', 'stent']],\n\n#                 \"chronic bronchitis\":[['chronic', 'bronchiti'],],\n\n#                 \"chronic renal insufficiency\":[[ 'chronic', 'renal', \"insuffici\"],],\n\n#                 \"hemorrhage of digestive tract\":[['hemorrhag', 'of', 'digest', \"tra\"],],\n\n#                 \"colon cancer\":[['colon', 'cancer'],],\n                \n#                  \"lung cancer\":[['lung', 'cancer'],],\n\n#                 \"prostate hypertrophy\": [['prostat', 'hypertrophi'],],\n                 \n#                 \"hip replacement\": [['hip', \"replac\"],],\n\n#                 \"cerebral infarction\":[['hypertens', 'cerebr', 'infarct'],],\n                \n#                  \"hepatitis B\": [[\"hepat\", 'b'],]}\n                                 \n# # 'encephalomalacia'  encephalomalacia   coronary bypass                               \n# # time = {['20', 'year'], ['four', 'year'], ['five', 'year']9 years}","a7b1b1c6":"# datasets2 = [tunisia, Hubei, outside_Hubei]\n# datasets_name1 = [ \"tunisia\", \"Hubei\", \"outside_Hubei\"]\n\n# for ind in range(len(datasets_name1)):\n#     dataset = datasets2[ind]\n#     print(ind)\n#     dataset[\"guess\"]= [np.nan for i in range(len(dataset.background_diseases)) ]\n#     indexs = dataset.index[dataset.background_diseases.notnull()]\n#     no_guess,multi_guess = BagWords.guess_category(dataset, \"background_diseases\", \"guess\",indexs, ps, a_bag_words, a_sentences_bag)\n    \n#     print(datasets_name1[ind])\n#     print(no_guess)\n#     print(multi_guess)","afd86eb7":"# indexs = Hubei.index[Hubei.background_diseases.notnull()]\n\n# indexs","eda487c5":"# bag_words= {\"pneumonia\":[\"pneumonia\",\"pneumon\"],\n#             \"fever\":[\"fever\"], \n#             \"cough\": [\"cough\" ],\n#             \"fatigue\":[\"fatigu\"],\n#             \"discomfort\": [\"discomfort\"],\n#            \"weakness\": [\"weak\", ['lack', 'of', 'energi]],\n#            \"dizziness\": [\"dizzi\"],\n#            \"rhinorrhoea\": [\"rhinorrhoea\",['runni', \"nose\"]],\n#            \"sneezing\": [\"sneez\"],\n#            \"diarrhea\": [\"diarrhea\"],\n#             \"expectoration\": [\"expector\"],\n#             \"headache\": [\"headach\"],\n#             \"diarrhea\": [\"diarrhea\"],\n#             \"chills\": [\"chill\"],\n#             \"dyspnea\": [\"dyspnea\"],\n#             \"rigor\": [\" rigor\"],\n#                                  \"pharyngalgia\": [\"pharyngalgia\",],\n#                                  \"no_symptom\": [\"asymptomat\"],\n#                                  \"nausea\": [\"nausea\"],\n                                 \n            \n#            }\n\n# sentences_bag = {\"nasal congestion\":[['nasal', 'congest'],['in', 'progress']],\n#                 \"sore throat\":[['sore', \"throat\"],], \n#                 \"pleuritic chest pain\": [ [ 'pleurit', 'chest', 'pain']],\n#                 \"muscular soreness\":[['muscular',\"sore\"]],\n#                  \"chest distress\":[[' chest', 'distress']],\n#                  \"muscular stiffness\":[['muscular', 'stiff']],\n#                  \"muscular soreness\":[['muscular',\"sore\"],[ \"muscl\", 'ach'],['muscl', \"pain\"], [\"myalgia\"]],\n#                  \"joint pain\":[['muscular', 'stiff']],\n#                  \"sore limbs\":[['muscular', 'stiff']],\n                 \n                 \n#                }\n                 \n# # pleuritic chest pain , ","0f7362da":"def getKeysByValue(dictOfElements, valueToFind):\n    listOfKeys = list()\n    listOfItems = dictOfElements.items()\n    for item  in listOfItems:\n        if item[1] == valueToFind:\n            listOfKeys.append(item[0])\n    return  listOfKeys\n\ndef remove(dict_a, keys_remove ):\n    for key in keys_remove:\n        if key in dict_a.keys():\n            dict_a.pop(key)\n","cd47b6a3":"ps = nltk.stem.SnowballStemmer('english')\nr = []\no = []\nfor ind in outside_Hubei.index[outside_Hubei.symptoms.notnull()]:\n    i = outside_Hubei.loc[ind, \"symptoms\"]\n\n    i = BagWords.clean_str(i)\n    l = [ps.stem(x) for x in i]\n\n    for x in l:\n        if x.isalpha():\n            r.append(x)\n            \nkeys_remove = ['to', 'a','like',  'no', 'and',  'yes', 'then','complaint',\"great\", \"even\", \n         \"for\", \"the\", \"non\",  'of' , \"this\",  'on' ,'with', \"was\", 'c',\n         \"cannot\", \"recommend\", \"as\", \"a\", \"i\", \"did\", \"not\", \"want\", \"to\", \"have\", \"to\", \"do\", \"this\"]\n\n\n            \n# r = columns_name = list(dict.fromkeys(o))\ntest_dict = dict(Counter(r))\nremove(test_dict, keys_remove )\nprint(test_dict)\n\n\n","3efeba44":"for key in test_dict.keys():\n    \n    o = test_dict[key]\n    print(key, o)\n    if o < 9:\n        test_dict.pop(key)\n        \nprint(test_dict)\n\n","b0051a75":"# def remover(my_string =\"\"):\n#     values = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \")\n#     for item in my_string:\n#         if item not in values:\n#             my_string = my_string.replace(item,\"\")\n#     return my_string\n\n\n\n\n# ps = nltk.stem.SnowballStemmer('english')\n\n# for i in Hubei.severity_illness[Hubei.severity_illness.notnull()]:\n#     i = remover(i)\n#     i = i.split(\" \")\n#     print(i)\n# #     print([ps.stem(x) for x in i ])\n    ","f3b283d8":"bag_words= {\"good\":[\"good\",\"stabl\", \"follow\"],\n            \"critical\":[\"critic\", \"intens\", \"sever\"], \n            \"deceased\": [\"death\",\"dead\", \"die\", \"deceas\" ],\n            \"cured\":[\"discharg\", \"releas\", \"cure\", \"recov\", 'health'],\n            np.nan: [\"isol\"]}\n\nsentences_bag = {\"good\":[['not', 'hospit'],['in', 'progress']],\n                \"critical\":[], \n                \"deceased\": [ ],\n                \"cured\":[]}\n\n\n\ndatasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\n\nfor ind in range(len(datasets_name1)):\n    dataset = datasets2[ind]\n    indexs = dataset.index[dataset.severity_illness.notnull()]\n    no_guess,multi_guess = BagWords.guess_category(dataset, \"severity_illness\", \"severity_illness\",indexs, ps, bag_words, sentences_bag)\n    \n    print(datasets_name1[ind])\n    print(no_guess)\n    print(multi_guess)\n\nfor x in [indonesia, france]:  \n    indexs = x.index[x.deceased_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"deceased\") \n\n    indexs = x.index[x.released_date.notnull()]\n    update_index(x,\"severity_illness\",indexs,\"cured\") ","ad2e44ac":"datasets2 = [france, tunisia, indonesia, korea, Hubei, outside_Hubei]\ndatasets_name1 = [\"france\", \"tunisia\", \"indonesia\", \"korea\", \"Hubei\", \"outside_Hubei\"]\nexamining_values_by_col(datasets2, datasets_name1,  \"severity_illness\")","16283e3a":"categories = [\"deceased\", \"cured\"]\ncols = [\"deceased_date\",\"released_date\"]\nfor indx_j in range(len(cols)) :\n    j  = cols[indx_j]\n    category = categories[indx_j]\n    \n    for x in [outside_Hubei, Hubei]:\n        x[j] = np.nan \n        indexs = x.index[x[\"severity_illness\"] == category]\n\n        for i in indexs:\n            x.loc[i, j]= pd.to_datetime(x.loc[ i, \"date_death_or_discharge\"])\n","4a1ab6a4":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"released_date\")","25796e02":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\nexamining_values_by_col(datasets2, datasets_name, \"deceased_date\")","aa562fc1":"for x in [outside_Hubei, Hubei]:\n    l = x.date_death_or_discharge.notnull().sum()\n    y = x.severity_illness.notnull().sum()\n    p = x.released_date.notnull().sum() +x.deceased_date.notnull().sum()\n    print(l,y,p)","079e2d18":"for x in [outside_Hubei]:\n    complete_features =list(x.index[x.released_date.notnull()]) + list(x.index[x.deceased_date.notnull()])\n    date_death_or_discharge = list(x.index[x.date_death_or_discharge.notnull()])\n    severity_illness = list(x.index[x.severity_illness.notnull()])\n\n    if complete_features == date_death_or_discharge:\n        print(\"==\")\n    else:\n        print(\"not ==\")\n    \n    for i in severity_illness:\n        if i not in complete_features:\n            print(i)","3ff7a533":"indexs =  outside_Hubei.index[outside_Hubei.age.notnull()]\n\ndef int_num(dataset, col, indexs):\n    to_float = []\n\n    for i in indexs:\n        if dataset.loc[i, col].isdigit() == True:\n            to_float.append(i)\n\n        \n        if \".\" in i:\n            to_float.append(i)\n\n#     for indx in  to_float:\n#         dataset.loc[indx, col] = int(dataset.loc[indx, col])\n    \n    return to_float\n        \n\nto_float= int_num(outside_Hubei, \"age\",indexs)\nindexs = indexs.drop(to_float)\n\nprint(indexs)","efe9db7c":"float_indx = []\n\nfor indx in indexs:\n    \n   print(type(outside_Hubei.loc[indx, \"age\"]))\n\n\n    \n    ","ed3ab69c":"for i in indexs:\n    ls = i.split(\"-\")\n    if len(ls)>1:\n        y = ls[0]-ls[1]\n        print(y)\n    print(outside_Hubei.loc[i, \"age\"], i)","95516b87":"\noutside_Hubei.head(11714)","bb4e3cff":"\n# outside_Hubei_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30-39\":\"30s\", \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# outside_Hubei.age = [outside_Hubei_age[item] for item in outside_Hubei.age] ","cd824740":"# Hubei.age.value_counts()","14e184f2":"# Hubei_age = {\"15-88\":np.nan, \"25-89\":np.nan, \"21-39\":np.nan, \"40-49\":\"40s\", \"50-59\":\"50s\", \"60-69\":\"60s\",\n#              \"70-82\":\"70s\" }\n# Hubei.age = [Hubei_age[item] for item in Hubei.age] ","d2a2ccf1":"# japan.age.value_counts()","0f595b2d":"# japan_age = {\"investigating\":np.nan, \"Checking\":np.nan, \"Under 10\":\"0s\", \"Under teens\":\"0s\",\"305\":\"30s\",\n#             \"10s\":\"10s\",\"20s\":\"20s\", \"30s\":\"30s\", \"40s\":\"40s\", \"50s\":\"50s\", \"60s\":\"60s\", \"70s\":\"70s\" ,\n#              \"80s\":\"80s\",\"90s\":\"90s\" }\n# japan.age = [japan_age[item] for item in japan.age] ","135c6e4b":"def birth_year_to_age(data):\n    age_ls = []\n\n    for i in range(len(data)):\n        age_ls.append(data.confirmed_date[i].year - data.birth_year[i])\n    return age_ls\n\nkorea[\"age\"] = birth_year_to_age(korea)\nfrance[\"age\"] = birth_year_to_age(france)","8ebcf5d3":"tunisia[\"country\"] = [\"tunisia\" for i in range(len(tunisia))]\njapan[\"country\"] = [\"japan\" for i in range(len(japan))]\nindonesia[\"country\"] = [\"indonesia\" for i in range(len(indonesia))]","d81ac578":"print(len(korea))\nprint(outside_Hubei.country.value_counts()[\"South Korea\"])\nprint()\n\nprint(len(france))\nprint(outside_Hubei.country.value_counts()[\"France\"])","3fd12ffa":"print(Hubei[\"wuhan(0)_not_wuhan(1)\"].value_counts())\nprint(outside_Hubei[\"travel_history_location\"].value_counts())","423b1da3":"# TODO","6e9d44e7":"france = france.drop([\"birth_year\", \"treatment\",\"group\"],axis=1)\ntunisia = tunisia.drop([\"hospital_place\"],axis=1)\njapan = japan.drop([\"Close contact situation\"],axis=1)\nkorea = korea.drop([\"birth_year\",\"global_num\"],axis=1)\nHubei = Hubei.drop([\"date_death_or_discharge\"],axis=1)","b0c248e3":"france = france.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)\ntunisia = tunisia.drop([\"hospital_name\"],axis=1)\nindonesia = indonesia.drop([\"infected_by\",\"hospital_name\"],axis=1)\nkorea = korea.drop([\"infection_place\", \"infected_by\",\"infection_order\"],axis=1)","720775fb":"datasets2 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets2)\nexam_df = Exam.df_exam_columns_dfs(datasets2, datasets_name, columns_name)\nprint(columns_name)\nexam_df.infection_place","66a549f0":"datasets3 = [france, tunisia, japan, indonesia, korea, Hubei, outside_Hubei]\ncolumns_name = Exam.build_columns_name_ls(datasets3)\nexam_df2 = Exam.df_exam_columns_dfs(datasets3,datasets_name,columns_name)","e87a2471":"for i in exam_df2.columns:\n    print(\"\\n\"+i)\n    examining_values_by_col (datasets, datasets_name, i)","7d16ab3b":"exam_df.sex","9d58f73f":"for i in dfs:\n    print(i[col].isnull().sum())","2fd0577a":"for i in datasets:\n    print(i.sex.isnull().sum())","97c40161":"datasets_final = [france, tunisia, japan, indonesia, korea]\nfinal_DS = pd.concat(datasets_final, axis=0)","99488500":"final_DS.status.value_counts()","6ae03de5":"final_DS.index = range(len(final_DS))","8987efcb":"final_DS.to_csv(r'\/kaggle\/working\/Characteristics_Corona_patients1.csv', index = False)","cd16715e":"final_DS.to_csv()","0d23240e":"age","ad2324c1":"country","b7c0c81f":"outside_Hubei data VS country data","554002d3":"city","6ea636e1":"index","d167d20d":"severity_illness","d4e48cbf":"deceased_date exam","f9c5e9b8":"symptoms","c34cac3f":"# format col","58256e7b":"ID","ebe8f205":"unipue","57892e91":"test","c9d9237d":"exam ","b38652f5":"Tests for columns' usefulness before drop","0afd8574":"# drop Non-baked features","7c4fb435":"background_diseases","4c9df748":"_____________________________________________________lab","499e4b57":"# exam_df = columns vs dfs","ddb0dca2":"infected by","44ad0a9b":"# change name of col","fe739e8d":"background_diseases_binary","41e07ce7":"# Examining values - v1 ","4224ce84":"infection_place","5c6889e2":"sex","a47ae880":">datetime","96705ad9":"# drop feature","26aa912f":"#  Garbage drop \n- Features that have only one dataset or  built with Engineered another feature with them","4e2b8096":"released_date \/ deceased_date","17e89bd7":"country_new","e121c80d":"# drop","e6ae8e5e":"The column has no new information to give","823ef589":"#                                                     Complete features","ad81baaf":"released_date exam","85d391a7":"**> Feature sum**","22cf25f5":"# orgnaze DS","88dd12fa":"**The purpose of this notebook is to create a DATASET that includes**\n\n** Characteristics of patients like - **\n\n*age\n\n*sex\n\n*Country\n\nand so\n\n\n\n\n**The condition of the patients and their characteristics - **\n\n* Disease time (from diagnosis date)\n\n* Have been cured\n \n* Deaths\n\n\nThe database is designed to allow easy exploration of the data\n\nAnyone interested can use and donate","a237f61a":"infection_case\n\n= Community \\abroad \\ Nan","d505defe":"# build final DS","6cc14bda":"As you can see the ID is not arranged in any numerical order.\nand because there is no column that needs another row identifier\nI drop ID","ed47815f":"common feature","2d52ee6b":"# datasets.shape"}}