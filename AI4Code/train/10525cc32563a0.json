{"cell_type":{"dbb19eb6":"code","d7469f80":"code","cf1685a6":"code","87c48ae0":"code","0201a12f":"code","16bb838c":"code","f7d0c1d3":"code","b413504b":"code","e0448d73":"code","81a74722":"code","04ea9345":"code","e4e2ff8a":"code","f91ea48b":"code","2b01693d":"code","5aa5b407":"markdown"},"source":{"dbb19eb6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","d7469f80":"# prefix = 'd:\\\\AI_datasets\\\\mnist\\\\'\nprefix = '..\/input\/'\ntrain_data = pd.read_csv(prefix + 'train.csv')\ntest_data = pd.read_csv(prefix + 'test.csv')","cf1685a6":"x_train_data = train_data.loc[:, 'pixel0':].values\ny_train_data = train_data.loc[:, 'label'].values.reshape(-1,1)\ny_train_data = OneHotEncoder(categories='auto').fit_transform(y_train_data).toarray()\nprint('x_train_shape:',x_train_data.shape)\nprint('y_train_shape:',y_train_data.shape)\n\nx_train, x_test, y_train, y_test = train_test_split(x_train_data, y_train_data, test_size=0.01)","87c48ae0":"# tensorboard \u53c2\u6570\u603b\u7ed3\u663e\u793a\ndef var_summary(var):\n        mean = tf.reduce_mean(var)\n        stddev = tf.reduce_mean(tf.square(var - mean))\n        tf.summary.scalar('mean',mean) # \u5e73\u5747\u503c\n        tf.summary.scalar('stddev',stddev) #\u6807\u51c6\u5dee\n        tf.summary.scalar('max',tf.reduce_max(var)) # \u6700\u5927\u503c\n        tf.summary.scalar('min', tf.reduce_min(var)) # \u6700\u5c0f\u503c\n        # tf.summary.histogram('histogram',var) # \u76f4\u65b9\u56fe","0201a12f":"# \u5b8f\u89c2\u53c2\u6570\nn_input = 784\nn_output = 10\n\nn_conv1 = (3, 64)\nn_conv2 = (3, 128)\n\nn_fc1 = 1024\n\nkeep_prob = 0.6\nlearning_rate = 0.01\ntf.reset_default_graph()\n\nwith tf.name_scope('parameters_of_C1'):\n    # \u6807\u51c6\u5dee\u8bbe\u7f6e\u5f88\u91cd\u8981\uff0c\u9ed8\u8ba4\u6807\u51c6\u5dee\u4e3a1\uff0c\u6d4b\u8bd5\u4e2d\u751f\u6210\u7684\u6700\u5927\u6743\u91cd\u53ef\u80fd\u4e3a2\uff0c\u7ecf\u591a\u5c42\u7f51\u7edc\u4f20\u64ad\u540e\uff0c\u6700\u540e\u7684\u8f93\u51fa\u5c42\u67091e19\u4e4b\u5927\uff0c\u76f4\u63a5\u5bfc\u81f4loss\u503c\u4e3anan\uff01\uff01\uff01\n    wc1 = tf.Variable(tf.truncated_normal([n_conv1[0], n_conv1[0], 1, n_conv1[1]],stddev=0.1), name='weights')\n    bc1 = tf.Variable(tf.truncated_normal([n_conv1[1]]), name='biases')\n    \n    var_summary(wc1)\n    var_summary(bc1)\n\n    \nwith tf.name_scope('parameters_of_C2'):\n    wc2 = tf.Variable(tf.truncated_normal([n_conv2[0], n_conv2[0], n_conv1[1], n_conv2[1]],stddev=0.01), name='weights')\n    bc2 = tf.Variable(tf.truncated_normal([n_conv2[1]]), name='biases')\n        \n    var_summary(wc2)\n    var_summary(bc2)\n\nwith tf.name_scope('parameters_of_FC1'):\n    # 7 * 7 \u7ecf\u8fc7\u4e24\u4e2apooling\u5c42\u540e\u56fe\u50cf\u753128*28->7*7\n    # 128 \u4e0a\u4e00\u5c42\u6709128\u4e2a\u7279\u5f81\u56fe\uff0c\u9700\u8981\u8f6c\u4e3a\u4e00\u7ef4\u5411\u91cf\n    wfc1 = tf.Variable(tf.truncated_normal([7 * 7 * n_conv2[1], n_fc1],stddev=0.01), name='weights')\n    bfc1 = tf.Variable(tf.truncated_normal([n_fc1]), name='baises')\n    \n        \n    var_summary(wfc1)\n    var_summary(bfc1)\n    \n    \nwith tf.name_scope('parameters_of_FC2'):\n    wfc2 = tf.Variable(tf.truncated_normal([n_fc1, n_output],stddev=0.01), name='weights')\n    bfc2 = tf.Variable(tf.truncated_normal([n_output]), name='baises')\n    var_summary(wfc2)\n\nweights = {\n    'wc1' : wc1,\n    'wc2' : wc2,\n    'wfc1': wfc1,\n    'wfc2' : wfc2\n}\n\nbaises = {\n    'bc1' : bc1,\n    'bc2' : bc2,\n    'bfc1': bfc1,\n    'bfc2': bfc2\n}","16bb838c":"def forward_propagation(_input,w ,b, keepratio):\n    # preprocess intput to tf format\n    #[n, h, w, c]\n    input_r = tf.reshape(_input, shape=[-1, 28, 28, 1])\n    \n    #CONV LAYER 1\n    with tf.name_scope('CONVOLUTION_LAYER_1'):\n        conv1 = tf.nn.conv2d(input_r, w['wc1'], [1, 1, 1, 1], padding='SAME',name = 'conv1_convolution')\n        # tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu=True,data_format='NHWC',dilations=[1, 1, 1, 1],name=None\n        \n        conv1 = tf.nn.relu(tf.add(conv1,b['bc1'], name='conv1_bais_add'), name='conv1_activation')\n        \n    # POOL LAYER 1\n    with tf.name_scope('POOL_LAYER_1'):\n        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',name='pool1_pooling')\n        pool1_dr = tf.nn.dropout(pool1, keepratio, name='pool1_dropout')\n        \n    #CONV LAYER 2\n    with tf.name_scope('CONVOLUTION_LAYER_2'):\n        conv2 = tf.nn.conv2d(pool1_dr, w['wc2'], [1, 1, 1, 1], padding='SAME',name = 'conv2_convolution')\n        # tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu=True,data_format='NHWC',dilations=[1, 1, 1, 1],name=None\n        \n        conv2 = tf.nn.relu(tf.add(conv2,b['bc2'], name='conv2_bais_add'), name='conv2_activation')\n        \n    # POOL LAYER 2\n    with tf.name_scope('POOL_LAYER_2'):\n        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',name='pool2_pooling')\n        pool2_dr = tf.nn.dropout(pool2, keepratio, name='pool1_dropout')  \n        \n        \n    #FULLY CONNECTED LAYER 1\n    with tf.name_scope('FULLY_CONNECTED_LAYER_1'):\n         # change shape of result of pool layer 2 to satisify fully connected layer\n        fc1_input = tf.reshape(pool2_dr, [-1, w['wfc1'].get_shape().as_list()[0]], name='fc1_reshape')\n        \n        fc1_z = tf.add(tf.matmul(fc1_input, w['wfc1']),b['bfc1'], name='fc1_full_connection')\n        fc1_a = tf.nn.relu(fc1_z, name='fc1_activation')\n        \n\n    #FULLY CONNECTED LAYER 2\n    with tf.name_scope('FULLY_CONNECTED_LAYER_2'):\n        fc2_z = tf.add(tf.matmul(fc1_a, w['wfc2']),b['bfc2'], name='fc2_full_connection')\n        \n    out = {\n        'conv1': conv1,\n        'pool1': pool1,\n        'conv2': conv2,\n        'fc1_input': fc1_input,\n        'fc1_a': fc1_a,\n        'fc2_z': fc2_z\n    }    \n    return out","f7d0c1d3":"# INPUT\nwith tf.name_scope('INPUT'):\n    x = tf.placeholder(tf.float32, [None, n_input], name='input_x')\n    y = tf.placeholder(tf.float32, [None, n_output], name='input_y')\n\n# FORWARD_PROPAGATION\nwith tf.name_scope('FORWARD_PROPAGATION'):\n    pred = forward_propagation(x, weights, baises, keep_prob)['fc2_z']\n    prediction = tf.argmax(pred, 1, output_type=tf.int32)\n\nout = forward_propagation(x, weights, baises, keep_prob)    \n# LOSS FUNCTION\nwith tf.name_scope('LOSS_FUNCTION'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred), name='average_loss')\n    tf.summary.scalar('loss', loss)\n# OPTIMIZE\nwith tf.name_scope('OPTIMIZER'):\n    optim = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# ACCURACY\nwith tf.name_scope('ACCURACY'):\n    same_matrix = tf.equal(tf.argmax(y, 1), tf.argmax(pred, 1))\n    accuracy = tf.reduce_mean(tf.cast(same_matrix, tf.float32))\n    sum_acc = tf.summary.scalar('accuracy', accuracy)\n\n\ninit = tf.global_variables_initializer()\nmerged = tf.summary.merge_all()\nsaver =tf.train.Saver()","b413504b":"n_epoch = 30\n\nprint_step = 2\n\nbatch_size = 100\nn_batch = len(x_train) \/\/ batch_size\n\n\ndo_train = 1","e0448d73":"%%time\nif do_train:\n    with tf.Session() as sess:\n        sess.run(init)\n        writer = tf.summary.FileWriter('.',sess.graph)\n\n        for epoch in range(n_epoch):\n            ave_loss         =      0\n            ave_train_acc    =      0\n            ave_test_acc     =      0\n            for batch in range(n_batch):\n                start = batch * batch_size\n                batch_xs, batch_ys = x_train[start: start + batch_size], y_train[start: start + batch_size]\n                summary, _, p_acc, p_loss = sess.run([merged, optim,accuracy,loss],feed_dict={x:batch_xs, y:batch_ys})\n                \n                ave_train_acc += (p_acc \/ n_batch)\n                ave_loss += (p_loss \/ n_batch)\n                \n                if batch % (n_batch \/\/ 10) == 0:\n                    print('#', end='')\n            print('\\n',end='')\n            \n            writer.add_summary(summary, epoch)\n            if epoch % print_step == 0:\n                ave_test_acc = sess.run(accuracy, feed_dict={x:x_test, y:y_test})\n                print(\"Epoch: %d\/%d, Loss: %.4f, train accuracy: %.4f, test accuracy: %.4f\" % \n                      (epoch, n_epoch, ave_loss, ave_train_acc, ave_test_acc))\n        saver.save(sess, 'cnn_mnist.ckpt')\n        do_train = 0","81a74722":"if not do_train:\n    n_batch = len(test_data) \/\/ batch_size\n    res = np.array([])\n    with tf.Session() as sess:\n        sess.run(init)\n        saver.restore(sess, 'cnn_mnist.ckpt')\n        for batch in range(n_batch):\n            start = batch * batch_size\n            batch_xs = test_data[start: start + batch_size]\n            res = np.hstack((res, sess.run(prediction,feed_dict={x:batch_xs.values})) ).astype(int) \n        sub = pd.DataFrame({\n        'ImageId': list(range(1,len(test_data) + 1)),\n        'Label':res\n                 })\n        sub.to_csv('submission.csv',index=0)","04ea9345":"# init = tf.global_variables_initializer()\n# with tf.Session() as sess:\n#     sess.run(init)\n#     print(sess.run(tf.reduce_max(wc1)))\n#     print(sess.run(tf.reduce_min(wc1)))\n#     print(sess.run(tf.reduce_max(wc2)))\n#     print(sess.run(tf.reduce_min(wc2)))\n#     loss = (sess.run((accuracy), feed_dict={x:x_train[:1000], y: y_train[:1000]}))","e4e2ff8a":"# plt.figure(dpi=300)\n# num_total = 50\n# for i in range(num_total):\n#     plt.subplot(num_total \/\/ 5 ,5,i+1)\n#     plt.axis('off')\n#     plt.imshow( res['conv2'][2,:,:,i], cmap='bone')","f91ea48b":"sub = pd.read_csv('submission.csv')\nsub.head(10)","2b01693d":"sub.to_csv('submission.csv',index=0)","5aa5b407":"# # \u6807\u51c6\u5dee\u8bbe\u7f6e\u5f88\u91cd\u8981\uff0c\u9ed8\u8ba4\u6807\u51c6\u5dee\u4e3a1\uff0c\u6d4b\u8bd5\u4e2d\u751f\u6210\u7684\u6700\u5927\u6743\u91cd\u53ef\u80fd\u4e3a2\uff0c\u7ecf\u591a\u5c42\u7f51\u7edc\u4f20\u64ad\u540e\uff0c\u6700\u540e\u7684\u8f93\u51fa\u5c42\u67091e19\u4e4b\u5927\uff0c\u76f4\u63a5\u5bfc\u81f4loss\u503c\u4e3anan\uff01\uff01\uff01"}}