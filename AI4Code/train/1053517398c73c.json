{"cell_type":{"d49aea2d":"code","ee383200":"code","4483f296":"code","29cce896":"code","0a424a84":"code","7bb49116":"code","2ce59ef0":"code","ef13de47":"code","0675e06b":"code","e4cd6315":"code","2f718069":"code","2566dd5d":"code","c30f537d":"code","ecbc5b7c":"code","4e6bccf7":"code","8f68cd50":"code","d889cba8":"code","3fba8801":"code","82db1ea3":"code","fdede25a":"code","42eda165":"code","d3616da2":"code","69647c31":"code","bba93ca4":"code","d25496f6":"code","16cdfc1f":"code","d09f4c12":"code","c8dfbaba":"markdown","f4de6487":"markdown","ea997377":"markdown","666fc43f":"markdown","e6a8d934":"markdown","9b9025dc":"markdown","9926c4a9":"markdown","626341da":"markdown","0c0b728d":"markdown","c0ce3ff9":"markdown","39b8693a":"markdown","a0a7cd36":"markdown","ddc6dc2c":"markdown","45d9a865":"markdown","a4361655":"markdown","082154cc":"markdown","80d93c1b":"markdown","09a3df8d":"markdown","0ac4afa0":"markdown","bc9d51a9":"markdown","753dca93":"markdown","d1ae5115":"markdown","287b2582":"markdown","35ebba8a":"markdown","b7024d92":"markdown","062e7dfb":"markdown","1ba8c3d1":"markdown","fe232738":"markdown","abca15c5":"markdown","9b452cfb":"markdown","2077f564":"markdown","692c0105":"markdown","19ec6691":"markdown","3ead348e":"markdown","bceaf25c":"markdown","cb9cc315":"markdown","c6c64b4c":"markdown","2f797764":"markdown","683ebbea":"markdown","2b9c3e14":"markdown","fdfee68e":"markdown","4c286cf3":"markdown","1ea0d91b":"markdown"},"source":{"d49aea2d":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","ee383200":"VERBOSE=1","4483f296":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","29cce896":"lr_clf = LogisticRegression(verbose=VERBOSE,\n                            random_state=RANDOM_STATE,\n                            n_jobs=N_JOBS)","0a424a84":"parameters = {\n    'solver': ['newton-cg', 'sag', 'lbfgs'],\n    'penalty': ['none', 'l2']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","7bb49116":"parameters = {\n    'solver': ['liblinear'],\n    'penalty': ['l1', 'l2']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","2ce59ef0":"parameters = {\n    'solver': ['saga'],\n    'l1_ratio': [x \/ 10 for x in range(0, 11)],\n    'penalty': ['none', 'elasticnet']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ef13de47":"parameters = {\n    'solver': ['liblinear'],\n    'penalty': ['l2'],\n    'dual': [True, False]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","0675e06b":"parameters = {\n    'tol': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","e4cd6315":"parameters = {\n    'C': [(0.9 + x \/ 50) for x in range(0, 10)]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","2f718069":"parameters = {\n    'fit_intercept': [True, False]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","2566dd5d":"parameters = {\n    'solver' : ['liblinear'],\n    'fit_intercept': [True],\n    'intercept_scaling': [1, 2, 3, 5, 8, 13, 21, 34]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","c30f537d":"parameters = {\n    'class_weight' : [None, 'balanced']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\ntable_grid_search(clf, all_ranks=True)","ecbc5b7c":"parameters = {\n    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","4e6bccf7":"parameters = {\n    'max_iter': range(50, 250, 50)\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","8f68cd50":"parameters = {\n    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n    'multi_class': ['ovr', 'multinomial']\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d889cba8":"parameters = {\n    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","3fba8801":"parameters = {\n    'solver': ['liblinear'],\n    'penalty': ['l1', 'l2'],\n    'C': [0.98, 1.00, 1.02],\n    'tol': [1e-7, 1e-8, 1e-9],\n    'max_iter': range(100, 250, 50)\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","82db1ea3":"with open('clf_liblinear.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","fdede25a":"parameters = {\n    'solver': ['saga'],\n    'max_iter': range(100, 250, 50)\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","42eda165":"with open('clf_saga.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","d3616da2":"parameters = {\n    'solver': ['sag'],\n    'max_iter': range(100, 250, 50),\n    'multi_class': ['ovr', 'multinomial'],\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","69647c31":"with open('clf_sag.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","bba93ca4":"parameters = {\n    'solver': ['lbfgs'],\n    'penalty': ['none', 'l2'],\n    'C': [0.98, 1.00, 1.02],\n    'fit_intercept': [True, False],\n    'max_iter': range(100, 250, 50),\n    'multi_class': ['ovr', 'multinomial'],\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d25496f6":"with open('clf_lbfgs.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","16cdfc1f":"parameters = {\n    'solver': ['newton-cg'],\n    'penalty': ['none', 'l2'],\n    'C': [0.98, 1.00, 1.02],\n    'fit_intercept': [True, False],\n    'max_iter': range(100, 250, 50),\n    'multi_class': ['ovr', 'multinomial'],\n}\nclf = GridSearchCV(lr_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d09f4c12":"with open('clf_newton-cg.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","c8dfbaba":"As all the categories has the same number of samples,\n`balanced` and `None` options has the same result.","f4de6487":"From `1e-08` the score already has the maximum value. The minimum fit time for the maximum score if also for `1e-08`.","ea997377":"# l1_ratio\n##### : float or None, optional (default=None)\n\nThe Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\nused if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\nto using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\nto using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\ncombination of L1 and L2.","666fc43f":"The parameter `fit_intercept` with `True` value has a slightly better score.","e6a8d934":"All classes in train are equal, then 'balanced' value has no sense.\n\nThe weight of the classes in the test set can be estimated from the results of a good prediction.\nThe LightGBM model has a 0.77311 of public score.\nThis is a good approximation.\nThe real weighting will differ.","9b9025dc":"# Search over parameters","9926c4a9":"The penalty `elasticnet` has no effect on score for whatever value of `l1_ratio`.\nFit time is better for no penalty.","626341da":"# dual\n##### : bool, optional (default=False)\n\nDual or primal formulation. Dual formulation is only implemented for\nl2 penalty with liblinear solver. Prefer dual=False when\nn_samples > n_features.","0c0b728d":"## newton-cg","c0ce3ff9":"The solver `liblinear` scores more with `l1` penalty.","39b8693a":"## lbfgs","a0a7cd36":"Each solver is itself an own model with its own particularities.\nEach parameter affect in a different way each solver.\nThus, all solvers has analized separately.\n\nImproving each solver separately make them better than at the beginning,\nbut each solver has the same rank in the maximum score.\n\nThe best scoring solvers are newton-cg and liblinear.","ddc6dc2c":"Multinomial is better for `newton-cg`, `sag` and `saga`.\nWhere as ovr is better for `lbfgs`.","45d9a865":"# warm_start\n##### : bool, optional (default=False)\n\nWhen set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nUseless for liblinear solver.","a4361655":"`newton-cg` has clearly the greatest score, and also the greatest fit time.","082154cc":"### Export grid search results","80d93c1b":"Warm_start seems to have no effect.","09a3df8d":"#     max_iter\n##### : int, optional (default=100)\n\n        Maximum number of iterations taken for the solvers to converge.","0ac4afa0":"# Prepare data","bc9d51a9":"# class_weight\n##### : dict or 'balanced', optional (default=None)\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples \/ (n_classes * np.bincount(y))``.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.","753dca93":"## saga","d1ae5115":"# penalty\n##### : str, 'l1', 'l2', 'elasticnet' or 'none', optional (default='l2')\n\nUsed to specify the norm used in the penalization*. The 'newt*on-cg',\n'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\nonly supported by the 'saga' solver. If 'none' (not supported by the\nliblinear solver), no regularization is applied.","287b2582":"There are two values with the same score: 1 and 1.08.\nIt presents a chaotic behaviour.","35ebba8a":"#     solver\n##### : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, optional (default='liblinear').\n\nAlgorithm to use in the optimization problem.\n\n- For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n  'saga' are faster for large ones.\n- For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n  handle multinomial loss; 'liblinear' is limited to one-versus-rest\n  schemes.\n- 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n- 'liblinear' and 'saga' also handle L1 penalty\n- 'saga' also supports 'elasticnet' penalty\n- 'liblinear' does not handle no penalty\n\nNote that 'sag' and 'saga' fast convergence is only guaranteed on\nfeatures with approximately the same scale. You can\npreprocess the data with a scaler from sklearn.preprocessing.","b7024d92":"## sag","062e7dfb":"Intercept_scaling presents a chaotic behaviour.\nThe value 13 has the greatest score.","1ba8c3d1":"# tol\n##### : float, optional (default=1e-4)\n\nTolerance for stopping criteria.","fe232738":"### Export grid search results","abca15c5":"# Conclusions","9b452cfb":"# intercept_scaling\n##### : float, optional (default=1)\n\nUseful only when the solver 'liblinear' is used\nand self.fit_intercept is set to True. In this case, x becomes\n[x, self.intercept_scaling],\ni.e. a \"synthetic\" feature with constant value equal to\nintercept_scaling is appended to the instance vector.\nThe intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\nNote! the synthetic feature weight is subject to l1\/l2 regularization\nas all other features.\nTo lessen the effect of regularization on synthetic feature weight\n(and therefore on the intercept) intercept_scaling has to be increased.","2077f564":"Used in combination with penalty='elasticnet'.","692c0105":"### Export grid search results","19ec6691":"The solver `newton-cg` scores more with no penalty.\nWhereas `lbfgs` scores more with `l2` penalty.\nThe solver `sag` has the same score.","3ead348e":"Eash solver has some compatible values at some parameters.\nThus, a complete grid search could not be done.\n\nThe strategy is to do a grid search for each cluster of compatible solvers.","bceaf25c":"# fit_intercept\n##### : bool, optional (default=True)\n\nSpecifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.","cb9cc315":"### Export grid search results","c6c64b4c":"# Introduction\n\nThe aim of this notebook is to optimize the Logistic Regression model.\n\nFirst, all [LogisticRegression model](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","2f797764":"### Export grid search results","683ebbea":"## liblinear","2b9c3e14":"Default value False is the best option.","fdfee68e":"#    C\n###### : float, optional (default=1.0)\n\nInverse of regularization strength; must be a positive float.\nLike in support vector machines, smaller values specify stronger\nregularization.","4c286cf3":"# Exhaustive search","1ea0d91b":"# multi_class\n##### : str, {'ovr', 'multinomial', 'auto'}, optional (default='ovr')\n\nIf the option chosen is 'ovr', then a binary problem is fit for each\nlabel. For 'multinomial' the loss minimised is the multinomial loss fit\nacross the entire probability distribution, *even when the data is\nbinary*. 'multinomial' is unavailable when solver='liblinear'.\n'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\nand otherwise selects 'multinomial'."}}