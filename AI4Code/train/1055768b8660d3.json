{"cell_type":{"d1dea5a2":"code","629b414c":"code","27e58da4":"code","9b032a9c":"code","2a0c669d":"code","a526fb57":"code","ede49652":"code","85d102bf":"code","7182ec66":"code","c160a103":"code","7f50ab81":"code","9004043d":"code","d16537c5":"code","ddb37a70":"code","02e59e2f":"code","da07c0dc":"code","21fc9d20":"code","fa5829b6":"code","f887646b":"code","173c8d53":"code","416515fb":"code","fa2b1bdc":"code","4dd0225d":"code","6efbd491":"code","2050dbc8":"code","bc17d46f":"code","51bd47be":"code","6c27d28a":"code","ac53195b":"code","497c931c":"code","9ddbc227":"code","476e4178":"code","ccaff3d3":"code","b63f6fec":"code","abdf688f":"code","055a8ab0":"code","7cd05af0":"code","1517a09f":"code","f45581f9":"code","8d5ab439":"code","03f84980":"code","036252b3":"code","f9a55f01":"code","f62a7a3e":"code","896d4d56":"code","abe6b133":"code","001e6dbd":"code","f7631412":"code","1440de64":"code","c8efdfe9":"code","56b25da7":"code","81e9bd03":"code","1c0672eb":"code","50d06e75":"code","96c3d387":"code","748d4454":"markdown","52ddb90a":"markdown","8e9ed6e8":"markdown","0e2f411e":"markdown","690af764":"markdown","9aff5b72":"markdown","f4021e0d":"markdown","2f496c40":"markdown","c81349f4":"markdown","93e642ea":"markdown","ffe45e9d":"markdown","15b59d57":"markdown","87698e86":"markdown","24e46549":"markdown","ca6d5aed":"markdown","6f9aecfd":"markdown","9e96e892":"markdown","b2aad586":"markdown","a1e4fd3f":"markdown","b90bd704":"markdown","08ceba7e":"markdown","b2cf4572":"markdown","3170d19c":"markdown","d2af0c46":"markdown","b8439aca":"markdown","4f2c09d1":"markdown","3dd7005e":"markdown","3e88bfe6":"markdown","55be3d20":"markdown","9c99c2b3":"markdown","e6bcecb8":"markdown"},"source":{"d1dea5a2":"# Importing Required Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","629b414c":"store_data = pd.read_csv('..\/input\/c\/rossmann-store-sales\/store.csv')\nstore_data.head(5)","27e58da4":"train_data = pd.read_csv('..\/input\/c\/rossmann-store-sales\/train.csv')\ntrain_data.head(5)","9b032a9c":"combined_data = pd.merge(store_data,train_data,on='Store')\ncombined_data.head(5)","2a0c669d":"combined_data.shape","a526fb57":"# Checking for null values\ncombined_data.isnull().mean()*100","ede49652":"# Unique values\ncolumns = list(combined_data.columns)\ncolumns.remove('Date')\ncolumns.remove('CompetitionDistance')\nfor i in columns:\n    print('Unique values in column :',combined_data[i].unique())","85d102bf":"# extracting year and month from Date \ncombined_data['year'] = combined_data['Date'].apply(lambda x : int(str(x)[0:4]))\ncombined_data['month'] = combined_data['Date'].apply(lambda x : int(str(x)[5:7]))\n\n# Sales with respect to year \nsns.barplot(x='year', y='Sales', data=combined_data).set(title='Year vs Sales')\n# sns.barplot(x='month',y='Sales', data=combined_data).set(title='Month vs Sales')\n\nplt.show()","7182ec66":"sns.barplot(x='DayOfWeek',y='Sales',data=combined_data).set(title='Sales vs Day of Week')","c160a103":"# Lets see how promo is impacting sales\nsns.barplot(x='Promo',y='Sales',data=combined_data).set(title='Sales on Promo')","7f50ab81":"# StateHoliday column has values 0 & \"0\", So, we need to change values with 0 to \"0\"\n\ncombined_data['StateHoliday'].loc[combined_data['StateHoliday'] == 0] = '0'\n\n# Sales with respect to State Holiday\nsns.barplot(x='StateHoliday', y='Sales', data=combined_data).set(title='State Holiday vs Sales')\nplt.show()","9004043d":"# Sales with respect to School Holiday\nsns.barplot(x='SchoolHoliday', y='Sales', data=combined_data).set(title='School Holiday vs Sales')","d16537c5":"# Sales with respect to Storetype\nsns.barplot(x='StoreType', y='Sales', data=combined_data).set(title='StoreType vs Sales')","ddb37a70":"# Sales with respect to Assortment\nsns.barplot(x='Assortment', y='Sales', data=combined_data).set(title='Assortment vs Sales')","02e59e2f":"store_data.isnull().sum()","da07c0dc":"train_data.isnull().sum()","21fc9d20":"# Filling Promo2SinceWeek, Promo2SinceYear, PromoInterval with 0\nstore_data.update(store_data['Promo2SinceWeek'].fillna(value=0,inplace=True))\nstore_data.update(store_data['Promo2SinceYear'].fillna(value=0,inplace=True))\nstore_data.update(store_data['PromoInterval'].fillna(value=0,inplace=True))","fa5829b6":"# Filling CompetitionDistance with mean distance\nmean_CompetitionDistance = store_data['CompetitionDistance'].mean()\nstore_data.update(store_data['CompetitionDistance'].fillna(value=mean_CompetitionDistance,inplace=True))","f887646b":"# Filling CompetitionOpenSinceMonth, CompetitionOpenSinceYear with most occuring month and year respectively\nmode_CompetitionOpenSinceMonth = store_data['CompetitionOpenSinceMonth'].mode()[0]\nmode_CompetitionOpenSinceYear = store_data['CompetitionOpenSinceYear'].mode()[0]\n\nstore_data.update(store_data['CompetitionOpenSinceMonth'].fillna(value=mode_CompetitionOpenSinceMonth,inplace=True))\nstore_data.update(store_data['CompetitionOpenSinceYear'].fillna(value=mode_CompetitionOpenSinceYear,inplace=True))\n\nstore_data.isnull().sum()","173c8d53":"combined_data = pd.merge(store_data,train_data,on='Store')\nprint(combined_data.shape)\ncombined_data.head(5)","416515fb":"combined_data.isnull().mean()*100","fa2b1bdc":"combined_data.plot(x='CompetitionDistance',y='Sales',kind='scatter',figsize =(10,6))","4dd0225d":"sns.displot(combined_data,x='Sales',bins=60)","6efbd491":"mean_sales = np.mean(combined_data['Sales'])\nstd_sales = np.std(combined_data['Sales'])\n\nthreshold = 3\n\noutliers = []\nfor i in combined_data['Sales']:\n    z_score = (i-mean_sales)\/std_sales\n    if z_score > threshold:\n        outliers.append(i)\n        \nprint('Total No.of outliers in dataset: ', len(outliers))\n\nsns.displot(x=outliers,bins=20).set(title='Outliers Distribution')","2050dbc8":"# Percentage of Outliers \nzero_sales = combined_data.loc[combined_data['Sales']==0]\n\nsales_greater_than_25k = combined_data.loc[combined_data['Sales'] > 25000]\n\nprint('Length of the dataset:', len(combined_data))\nprint('Percentage of Zeros in dataset: %.3f%%' %((len(zero_sales)\/len(combined_data))*100))\nprint('Percentage of sales greater than 25k in dataset: %.3f%% ' %((len(sales_greater_than_25k)\/len(combined_data))*100))","bc17d46f":"combined_data.drop(combined_data.loc[combined_data['Sales'] > 25000].index,inplace=True)","51bd47be":"combined_data.shape","6c27d28a":"no_sales = combined_data.loc[(combined_data['Sales']==0) & (combined_data['Open'] == 1) & (combined_data['StateHoliday'] == 0) \n                               & (combined_data['SchoolHoliday'] == 0)]\nprint(no_sales.shape)\nno_sales.head()","ac53195b":"combined_data.drop(combined_data.loc[(combined_data['Sales']==0) & (combined_data['Open'] == 1)\n                                     & (combined_data['StateHoliday'] == 0) & \n                                     (combined_data['SchoolHoliday'] == 0)].index,inplace=True)\nprint(combined_data.shape)","497c931c":"combined_data.head()","9ddbc227":"combined_data['Year'] = combined_data['Date'].apply(lambda x: int(str(x)[0:4]))\ncombined_data['Month'] = combined_data['Date'].apply(lambda x: int(str(x)[5:7]))\ncombined_data.drop(['Date'],axis=1,inplace=True)\n\ncombined_data.head(5)","476e4178":"combined_data.dtypes","ccaff3d3":"# StateHoliday column has values 0 & \"0\", So, we need to change values with 0 to \"0\"\n\ncombined_data['StateHoliday'].loc[combined_data['StateHoliday'] == 0] = '0'","b63f6fec":"# PromoInterval column has values 0 & \"0\", So, we need to change values with 0 to \"0\"\n\ncombined_data['PromoInterval'].loc[combined_data['PromoInterval'] == 0] = '0'","abdf688f":"combined_data['PromoInterval'].head()","055a8ab0":"# encoding all categorical varibale to numeric values\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ncombined_data['StoreType'] = label_encoder.fit_transform(combined_data['StoreType'])\ncombined_data['Assortment'] = label_encoder.fit_transform(combined_data['Assortment'])\ncombined_data['StateHoliday'] = label_encoder.fit_transform(combined_data['StateHoliday'])\ncombined_data['PromoInterval'] = label_encoder.fit_transform(combined_data['PromoInterval'])\n\ncombined_data.head()","7cd05af0":"# Correlation\ncorrelation = combined_data.corr()\ncorrelation","1517a09f":"# Heat Map\nplt.figure(figsize=(18,10))\nsns.heatmap(correlation, annot=True, linewidths=0.2, cmap='BrBG')","f45581f9":"combined_data_open = combined_data[combined_data['Open']==1]\ncombined_data_closed = combined_data[combined_data['Open']==0]","8d5ab439":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\nimport math\n\nX_train, X_test, y_train, y_test_open = train_test_split(combined_data_open.drop(['Sales','Customers','Open'],axis=1),\n                                                        combined_data_open['Sales'], test_size=0.2, random_state=23)","03f84980":"X_train.columns","036252b3":"y_train.head()","f9a55f01":"y_test_closed = np.zeros(combined_data_closed.shape[0])\ny_test = np.append(y_test_open, y_test_closed)","f62a7a3e":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)","896d4d56":"prediction_open = model.predict(X_test)\nprediction_closed = np.zeros(combined_data_closed.shape[0])\n\ny_predict = np.append(prediction_open,prediction_closed)","abe6b133":"# Performance of the model\n\nprint('r2_score:',r2_score(y_test,y_predict))\nprint('Mean absolute error: %.2f' % mean_absolute_error(y_test,y_predict))\nprint('Root mean squared error: ', math.sqrt(mean_squared_error(y_test,y_predict)))","001e6dbd":"plt.figure(figsize=(8,8))\nplt.scatter(y_test,y_predict)\n\np1 = max(max(y_predict),max(y_test))\np2 = min(min(y_predict),min(y_test))\nplt.plot([p1,p2],[p1,p2],c='r')\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')","f7631412":"from sklearn.linear_model import SGDRegressor\n\nmodel = SGDRegressor()\nmodel.fit(X_train,y_train)\n\nprediction_open = model.predict(X_test)\nprediction_closed = np.zeros(combined_data_closed.shape[0])\n\ny_predict = np.append(prediction_open,prediction_closed)\n\n# Performance of the model\n\nprint('r2_score:',r2_score(y_test,y_predict))\nprint('Mean absolute error: %.2f' % mean_absolute_error(y_test,y_predict))\nprint('Root mean squared error: ', math.sqrt(mean_squared_error(y_test,y_predict)))\n\nplt.figure(figsize=(8,8))\nplt.scatter(y_test,y_predict)\n\np1 = max(max(y_predict),max(y_test))\np2 = min(min(y_predict),min(y_test))\nplt.plot([p1,p2],[p1,p2],c='r')\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')","1440de64":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train,y_train)\n\nprediction_open = model.predict(X_test)\nprediction_closed = np.zeros(combined_data_closed.shape[0])\n\ny_predict = np.append(prediction_open,prediction_closed)\n\n# Performance of the model\n\nprint('r2_score:',r2_score(y_test,y_predict))\nprint('Mean absolute error: %.2f' % mean_absolute_error(y_test,y_predict))\nprint('Root mean squared error: ', math.sqrt(mean_squared_error(y_test,y_predict)))\n\nplt.figure(figsize=(8,8))\nplt.scatter(y_test,y_predict)\n\np1 = max(max(y_predict),max(y_test))\np2 = min(min(y_predict),min(y_test))\nplt.plot([p1,p2],[p1,p2],c='r')\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')","c8efdfe9":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest_model = RandomForestRegressor(n_estimators=100)\nrandom_forest_model.fit(X_train,y_train)\n\nprediction_open = random_forest_model.predict(X_test)\nprediction_closed = np.zeros(combined_data_closed.shape[0])\n\ny_predict = np.append(prediction_open,prediction_closed)\n\n# Performance of the model\n\nprint('r2_score:',r2_score(y_test,y_predict))\nprint('Mean absolute error: %.2f' % mean_absolute_error(y_test,y_predict))\nprint('Root mean squared error: ', math.sqrt(mean_squared_error(y_test,y_predict)))\n\nplt.figure(figsize=(8,8))\nplt.scatter(y_test,y_predict)\n\np1 = max(max(y_predict),max(y_test))\np2 = min(min(y_predict),min(y_test))\nplt.plot([p1,p2],[p1,p2],c='r')\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')","56b25da7":"# getting weights of all the features used in the data\nfeature_importance = random_forest_model.feature_importances_\nfeature_importance","81e9bd03":"# features used\ncolumns = list(X_train.columns)\ncolumns","1c0672eb":"# Lets make a dataframe consists of features and values\nfeature_importance_df = pd.DataFrame({'Features':columns, 'Values':feature_importance})\nfeature_importance_df","50d06e75":"feature_importance_df.sort_values(by=[\"Values\"], inplace=True, ascending=False)\nfeature_importance_df","96c3d387":"# Feature Importance\nplt.figure(figsize=(15,6))\n\nsns.barplot(x=feature_importance_df['Features'], y=feature_importance_df['Values'],\n            data = feature_importance_df ).set(title='Feature Importance')\n\nplt.xticks(rotation=90)\nplt.show()","748d4454":"## Data Visualization","52ddb90a":"#### Observation:\nSales have been increasing year to year","8e9ed6e8":"#### Observation:\nCustomers are definately attracted by Promo codes thus sales are higher when there is a Promo code in a Store","0e2f411e":"#### Observation:\nOf all a,b,c,d are store models b type stores have the highest sales","690af764":"# Rossmann Store Sales\n                                                         Code written by: Dasari Mohan\n\n\n## PROJECT OBJECTIVE :\n\nForecast sales using store, promotion, and competitor data\n\n## CONTEXT :\n\nRossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. We are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column.","9aff5b72":"#### Here we want our ML model to predict sales only when they are open and we know that there will be no sales if the store is closed","f4021e0d":"#### Observation:\nThere are total 12 dates where there is no record of sales even without any holidays. We can remove these data points too as they are an exceptional case","2f496c40":"#### Obervation:\nWe can drop the sales datapoints which are greater than 25k as they are very less percentage of the dataset and are probably outliers","c81349f4":"#### Observation:\n\nMost stores are closed on State Holidays that's why we can see that there are very less sales in a,b,c \n\nwhere\n\na = Public Holiday,\nb = Easter Holiday,\nc = Chirstmas,\n0 = No Holiday, Working day","93e642ea":"### Converting Categorical Variable to Numeric","ffe45e9d":"### Some exceptional cases\nLooking for a scenerio where the Stores are open and yet there is no sales on that day","15b59d57":"# Conclusion:\n\n1. Closer competiton distance make stores more competitive and thus by using Promo codes can help them to boost their sales.\n\n2. Store Type affects the sales - Of all a,b,c,d store models 'b' type stores have the highest sales.\n\n3. Promo code can help increase in the competition and lead to more sales.\n\n4. Sales on 1 (Monday) and 5 (Friday) are the highest.\n\n5. Assortment level 'b' have the highest sales.\n\n6. Customers are definately attracted by Promo codes thus sales are higher when there is a Promo code in a Store\n\n7. Since most of the stores are closed on Holidays, the feature state holidays has no effect on sales","87698e86":"# My Approch:\n\n1.\tImporting the required libraries and reading the dataset. \n\n    a.\t Merging of the two datasets \n    \n    b.\t Understanding the dataset\n\n2.\tExploratory Data Analysis (EDA) \u2013 \n\n    a.\t Data Visualizatiom\n\n3.\tFeature Engineering \n\n    a.\t Dropping of unwanted columns and values (closed stores)\n    \n    b.\t Filling Missing Values with Imputation\n    \n    c.   Outliers Detection and removal\n\n4.  Further Exploratory Data Analysis to find out a few exceptional cases.\n\n5.\tLabel Encoding (Converting categorical variables to numerical values)\n\n6.\tModel Building \n\n    a.\t Performing train test split \n    \n    b.\t Linear Regression Model \n    \n    c.\t SGD Regression Model \n    \n    d.\t Decision Tree Regression Model \n    \n    e.\t Random Forest Regression Model\n\n7.\tModel Validation \n\n    a.\t r2 score \n    \n    b.\t Mean absolute error \n    \n    c.\t Root mean squared error\n\n8.\tCreating the final right model and making predictions\n\n9.\tFeature Importance Analysis\n\n10.\tConclusion","24e46549":"## Linear Regression Algorithm","ca6d5aed":"#### Observation:\nFrom the above plot we can see that Linear regression model is performing badly as its not making any predictions more than 10000 even for 25000 sales.\n\n## SGD Regression Algorithm","6f9aecfd":"## Buliding a Regression Model","9e96e892":"#### Observation:\nAssortment level 'b' have the highest sales","b2aad586":"#### Observation\nSales on 1 (Monday) and 5 (Friday) are the highest","a1e4fd3f":"#### Observation:\nFrom the above plot we can say that more nearer the compitetor store are the more sales in Rossman stores.\n\n## Finding Outliers","b90bd704":"### Filling Missing Values and Removing Outliers\nFew columns have high number of missing values, so we need to fill them with appropriate method for better result.\n\n##### Approach\n1: The null values in Column Promo2SinceWeek, Promo2SinceYear, PromoInterval is due to Promo2 is 0 for those stores. So we would fill all the null values in these columns with 0.\n\n2: Since Competition Distance for 3 stores isn't given so we could fill it with mean of the distance given for all other stores\n\n3: CompetitionOpenSinceMonth, CompetitionOpenSinceYear can be filled using the most occuring month and year respectively.","08ceba7e":"#### Observation:\nRandom Forest regressor had the lowest error as compared to other models, which means it is better at predicting sales than other models.","b2cf4572":"#### Observation:\nOn School Holidays there are more sales","3170d19c":"#### Observation:\nCorrelation map shows\n\nSales is highly correlated with Customers, Open and Promo code and minorly correlated to school holidays\n","d2af0c46":"#### Observation:\nThe decision tree regressor performing well compared to Linear and SGD regressors\n\n## Random Forest Regressor","b8439aca":"#### Observation:\nHere we can clearly see that only store data has null values in it and we need to fill missing values in store data","4f2c09d1":"#### Observation:\nThe SGD regressor is performing worse than Linear Regression as its giving negative r2 score, lets see other regression models\n\n## Decision Tree Regressor","3dd7005e":"Data fields\n\n1.\tId - an Id that represents a (Store, Date) duple within the test set\n2.\tStore - a unique Id for each store\n3.\tSales - the turnover for any given day (this is what you are predicting)\n4.\tCustomers - the number of customers on a given day\n5.\tOpen - an indicator for whether the store was open: 0 = closed, 1 = open\n6.\tStateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n7.\tSchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n8.\tStoreType - differentiates between 4 different store models: a, b, c, d\n9.\tAssortment - describes an assortment level: a = basic, b = extra, c = extended\n10.\tCompetitionDistance - distance in meters to the nearest competitor store\n11.\tCompetitionOpenSince[Month\/Year] - gives the approximate year and month of the time the nearest competitor was opened\n12.\tPromo - indicates whether a store is running a promo on that day\n13.\tPromo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n14.\tPromo2Since[Year\/Week] - describes the year and calendar week when the store started participating in Promo2\n15.\tPromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store\n","3e88bfe6":"## Exploring Data Analysis","55be3d20":"### Understanding the important features","9c99c2b3":"#### Great ! We don't have any null values, we can proceed further","e6bcecb8":"#### Observation:\nAs we can see in the distribution plot Sales greater than 25k are very less,therefore they might be the outliers.\n\n### Z-Score: If the Z-Score of any datapoint is greater than 3(threshold) then that can be considered as an Outlier"}}