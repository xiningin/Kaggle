{"cell_type":{"cc198483":"code","20aad79f":"code","bfee89ca":"code","3499b1fa":"code","da61f004":"code","3ce0b569":"code","d2117b16":"code","f3505413":"code","c9a9d6e7":"code","2636eb72":"code","d4a9f885":"code","10874317":"code","0e456de3":"code","d8bdd706":"code","d681f98a":"code","9de20d17":"code","071d94fc":"code","cc1d6ca6":"code","e62dc76d":"code","dc36b13c":"code","49cc2829":"code","3d721cfa":"code","7b5bb0b4":"code","979dae03":"code","42868957":"code","cca371f6":"code","35a662d5":"code","b6514e29":"code","215e4684":"code","c75cb5a0":"code","b572d29c":"code","b4655f99":"code","d706ed7d":"code","38cd5048":"code","6514f291":"code","96400409":"code","4385dbc3":"code","ac67ef0d":"code","1bdb57de":"code","489e5538":"code","5f7b2e49":"code","818bdc0e":"code","3c0a50cf":"code","fd8e0d74":"code","16e50a23":"code","96e8b6a1":"markdown","02c680e8":"markdown","9f7da75a":"markdown","d7ffb0a2":"markdown","21af515f":"markdown","70fda99e":"markdown","ef8339e9":"markdown","b677c88a":"markdown","a1e84af2":"markdown","d53859b7":"markdown","9dae0635":"markdown","0420b53f":"markdown","3c0d112c":"markdown","845bb465":"markdown","b07f633f":"markdown","9b9ef42d":"markdown","85ecb690":"markdown","74ffadd8":"markdown","4e1b1165":"markdown","d8a446ac":"markdown","313d7bb0":"markdown","a3b68373":"markdown","8d2b8989":"markdown","67668f80":"markdown","2072c720":"markdown","0a9c14a0":"markdown","b1b62087":"markdown","a3c681ff":"markdown","48638f8c":"markdown","30050e2c":"markdown"},"source":{"cc198483":"# Install package for PEP8 verification\n!pip install pycodestyle\n!pip install --index-url https:\/\/test.pypi.org\/simple\/ nbpep8","20aad79f":"# Install Beautifulsoup4\n!pip install beautifulsoup4","bfee89ca":"# Install langdetect\n!pip install langdetect","3499b1fa":"# Import Python libraries\nimport os\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom langdetect import detect\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.snowball import EnglishStemmer\nimport spacy\nfrom spacy import displacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# Library for PEP8 standard\nfrom nbpep8.nbpep8 import pep8","da61f004":"plt.style.use('seaborn-whitegrid')\nsns.set_style(\"whitegrid\")","3ce0b569":"nltk.download('popular')","d2117b16":"# Define path to data\npath = '..\/input\/stackoverflow-questions-filtered-2011-2021\/'\n\n# Concat all CSV datasets in one Pandas DataFrame\ndf_columns = pd.read_csv(path+'StackOverflow_questions_2009.csv').columns\ndata = pd.DataFrame(columns=df_columns)\nfor f in os.listdir(path):\n    if(\"cleaned\" not in f):\n        temp = pd.read_csv(path+f)\n        data = pd.concat([data, temp], \n                         axis=0,\n                         ignore_index=True)\ndata.head(3)","f3505413":"# Print full dataset infos\ndata.info()","c9a9d6e7":"# Describe data\ndata.describe()","2636eb72":"data.set_index('Id', inplace=True)","d4a9f885":"# Convert CreationDate to datetime format\ndata['CreationDate'] = pd.to_datetime(data['CreationDate'])\n\n# Grouper with 1 year delta\npost_year = data.groupby(pd.Grouper(key='CreationDate',\n                                    freq='1Y')).agg({'Title': 'count'})\n\n# Plot evolution\nfig = plt.figure(figsize=(15,6))\nsns.lineplot(data=post_year, x=post_year.index, y='Title')\nplt.axhline(post_year.Title.mean(), \n            color=\"r\", linestyle='--',\n            label=\"Mean of question per year : {:04d}\"\\\n                   .format(int(post_year.Title.mean())))\nplt.xlabel(\"Date of questions\")\nplt.ylabel(\"Number of questions\")\nplt.title(\"Number of questions evolution from 2009 to 2020\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","10874317":"fig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.Title.str.len())\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 5))\nplt.axvline(data.Title.str.len().median() - data.Title.str.len().min(),\n            color=\"r\", linestyle='--',\n            label=\"Title Lenght median : \"+str(data.Title.str.len().median()))\nax.set_xlabel(\"Lenght of title\")\nplt.title(\"Title lenght of Stackoverflow questions\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","0e456de3":"# Discretizer for Body characters lenght\nX = pd.DataFrame(data.Body.str.len())\n\n# Sklearn discretizer with 200 bins\ndiscretizer = KBinsDiscretizer(n_bins=200,\n                               encode='ordinal',\n                               strategy='uniform')\nbody_lenght = discretizer.fit_transform(X)\nbody_lenght = discretizer.inverse_transform(body_lenght)\nbody_lenght = pd.Series(body_lenght.reshape(-1))","d8bdd706":"fig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=body_lenght)\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 25))\nax.set_xlabel(\"Lenght of Body (after discretization)\")\nplt.title(\"Body lenght of Stackoverflow questions\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","d681f98a":"# Filter data on body lenght\ndata = data[data.Body.str.len() < 4000]\ndata.shape","9de20d17":"data['Tags'].head(3)","071d94fc":"# Replace open and close balise between tags\ndata['Tags'] = data['Tags'].str.translate(str.maketrans({'<': '', '>': ','}))\n\n# Delete last \",\" for each row\ndata['Tags'] = data['Tags'].str[:-1]\ndata['Tags'].head(3)","cc1d6ca6":"def count_split_tags(df, column, separator):\n    \"\"\"This function allows you to split the different words contained\n    in a Pandas Series cell and to inject them separately into a list.\n    This makes it possible, for example, to count the occurrences of words.\n\n    Parameters\n    ----------------------------------------\n    df : Pandas Dataframe\n        Dataframe to use.\n    column : string\n        Column of the dataframe to use\n    separator : string\n        Separator character for str.split.\n    ----------------------------------------\n    \"\"\"\n    list_words = []\n    for word in df[column].str.split(separator):\n        list_words.extend(word)\n    df_list_words = pd.DataFrame(list_words, columns=[\"Tag\"])\n    df_list_words = df_list_words.groupby(\"Tag\")\\\n        .agg(tag_count=pd.NamedAgg(column=\"Tag\", aggfunc=\"count\"))\n    df_list_words.sort_values(\"tag_count\", ascending=False, inplace=True)\n    return df_list_words","e62dc76d":"tags_list = count_split_tags(df=data, column='Tags', separator=',')\nprint(\"Le jeu de donn\u00e9es compte {} tags.\".format(tags_list.shape[0]))","dc36b13c":"# Plot the results of splits\nfig = plt.figure(figsize=(15, 8))\nsns.barplot(data=tags_list.iloc[0:40, :],\n            x=tags_list.iloc[0:40, :].index,\n            y=\"tag_count\", color=\"#f48023\")\nplt.xticks(rotation=90)\nplt.title(\"40 most popular tags in Stackoverflow (2009 - 2020)\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","49cc2829":"# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(tags_list.to_dict()['tag_count'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 best Tags on StackOverflow (2009 - 2020)\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","3d721cfa":"# Subplots parameters\nyears = {0: 2009, 1: 2012, 2: 2016, 3: 2020}\ncolors = {0: \"#f48023\", 1: \"#d16e1e\",\n          2: \"#b25d19\", 3: \"#904b14\"}\nsubplots = 4\ncols = 2\nrows = subplots \/\/ cols\nrows += subplots % cols\nposition = range(1, subplots + 1)\n\n# Plot popular tags for each year\nfig = plt.figure(1, figsize=(20, 16))\nfor k in range(subplots):\n    subset = data[data[\"CreationDate\"].dt.year == years[k]]\n    temp_list = count_split_tags(df=subset, column='Tags', separator=',')\n    ax = fig.add_subplot(rows, cols, position[k])\n    sns.barplot(data=temp_list.iloc[0:20, :],\n            x=temp_list.iloc[0:20, :].index,\n            y=\"tag_count\", color=colors[k])\n    plt.xticks(rotation=90)\n    ax.set_title(\"20 most popular tags for {}\".format(years[k]),\n                 fontsize=18, color=\"#641E16\")\n\nfig.tight_layout()\nplt.show()","7b5bb0b4":"# Create a list of Tags and count the number\ndata['Tags_list'] = data['Tags'].str.split(',')\ndata['Tags_count'] = data['Tags_list'].apply(lambda x: len(x))\n\n# Plot the result\nfig = plt.figure(figsize=(12, 8))\nax = sns.countplot(x=data.Tags_count, color=\"#f48023\")\nax.set_xlabel(\"Tags\")\nplt.title(\"Number of tags used per question\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","979dae03":"def filter_tag(x, top_list):\n    \"\"\"Comparison of the elements of 2 lists to \n    check if all the tags are found in a list of top tags.\n\n    Parameters\n    ----------------------------------------\n    x : list\n        List of tags to test.\n    ----------------------------------------\n    \"\"\"\n    temp_list = []\n    for item in x:\n        if (item in top_list):\n            #x.remove(item)\n            temp_list.append(item)\n    return temp_list","42868957":"top_tags = list(tags_list.iloc[0:50].index)\ndata['Tags_list'] = data['Tags_list']\\\n                    .apply(lambda x: filter_tag(x, top_tags))\ndata['number_of_tags'] = data['Tags_list'].apply(lambda x : len(x))\ndata = data[data.number_of_tags > 0]\nprint(\"New size of dataset : {} questions.\".format(data.shape[0]))","cca371f6":"def remove_code(x):\n    \"\"\"Function based on the Beautifulsoup library intended to replace \n    the content of all the <code> <\/code> tags of a text specified as a parameter.\n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    soup = BeautifulSoup(x,\"lxml\")\n    code_to_remove = soup.findAll(\"code\")\n    for code in code_to_remove:\n        code.replace_with(\" \")\n    return str(soup)","35a662d5":"start_time = time.time()\n# Delete <code> in Body text\ndata['Body'] = data['Body'].apply(remove_code)\n# Delete all html tags\ndata['Body'] = [BeautifulSoup(text,\"lxml\").get_text() for text in data['Body']]\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body'].head(3))","b6514e29":"# Create feature \"lang\" with langdetect library\ndef detect_lang(x):\n    try:\n        return detect(x)\n    except:\n        pass\n\nstart_time = time.time()\ndata['short_body'] = data['Body'].apply(lambda x: x[0:100])\ndata['lang'] = data.short_body.apply(detect_lang)\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)","215e4684":"# Count titles for each language\npd.DataFrame(data.lang.value_counts())","c75cb5a0":"# Deletion of data that is not in the English language\ndata = data[data['lang']=='en']","b572d29c":"def remove_pos(nlp, x, pos_list):\n    doc = nlp(x)\n    list_text_row = []\n    for token in doc:\n        if(token.pos_ in pos_list):\n            list_text_row.append(token.text)\n    join_text_row = \" \".join(list_text_row)\n    join_text_row = join_text_row.lower().replace(\"c #\", \"c#\")\n    return join_text_row","b4655f99":"def text_cleaner(x, nlp, pos_list):\n    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n        It allows you to remove extra spaces, unicode characters, \n        English contractions, links, punctuation and numbers.\n        \n        The re library for using regular expressions must be loaded beforehand.\n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    # Remove POS not in \"NOUN\", \"PROPN\"\n    x = remove_pos(nlp, x, pos_list)\n    # Case normalization\n    x = x.lower()\n    # Remove unicode characters\n    x = x.encode(\"ascii\", \"ignore\").decode()\n    # Remove English contractions\n    x = re.sub(\"\\'\\w+\", '', x)\n    # Remove ponctuation but not # (for C# for example)\n    x = re.sub('[^\\\\w\\\\s#]', '', x)\n    # Remove links\n    x = re.sub(r'http*\\S+', '', x)\n    # Remove numbers\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    # Remove extra spaces\n    x = re.sub('\\s+', ' ', x)\n    \n    # Return cleaned text\n    return x","d706ed7d":"# Apply cleaner on Body\n# Spacy features\nnlp = spacy.load('en', exclude=['tok2vec', 'ner', 'parser', \n                                'attribute_ruler', 'lemmatizer'])\npos_list = [\"NOUN\",\"PROPN\"]\n\nstart_time = time.time()\nprint('-' * 50)\nprint(\"Start Body cleaning ...\")\nprint('-' * 50)\n\ntqdm.pandas()\ndata['Body_cleaned'] = data.Body.progress_apply(lambda x : text_cleaner(x, nlp, pos_list))\n\nexec_time = time.time() - start_time\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","38cd5048":"start_time = time.time()\n# Tockenization\ndata['Body_cleaned'] = data.Body_cleaned.apply(nltk.tokenize.word_tokenize)\n\n# List of stop words in \"EN\" from NLTK\nstop_words = stopwords.words(\"english\")\n\n# Remove stop words\ndata['Body_cleaned'] = data.Body_cleaned\\\n    .apply(lambda x : [word for word in x\n                       if word not in stop_words\n                       and len(word)>2])\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","6514f291":"# Apply lemmatizer on Body\nstart_time = time.time()\nwn = WordNetLemmatizer()\ndata['Body_cleaned'] = data.Body_cleaned\\\n    .apply(lambda x : [wn.lemmatize(word) for word in x])\nexec_time = time.time() - start_time\nprint('-' * 50)\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Body_cleaned'].head(3))","96400409":"# Calculate lenght of each list in Body\ndata['body_tokens_count'] = [len(_) for _ in data.Body_cleaned]\n\n# Countplot of body lenght\nfig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.body_tokens_count)\nstart, end = ax.get_xlim()\nax.xaxis.set_ticks(np.arange(0, end, 25))\nplot_median = data.body_tokens_count.median()\nplt.axvline(plot_median - data.body_tokens_count.min(),\n            color=\"r\", linestyle='--',\n            label=\"Body tokens Lenght median : \"+str(plot_median))\nax.set_xlabel(\"Lenght of body tokens\")\nplt.title(\"Body tokens lenght of Stackoverflow questions after cleaning\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","4385dbc3":"# Create a list of all tokens for Body\nfull_corpus = []\nfor i in data['Body_cleaned']:\n    full_corpus.extend(i)","ac67ef0d":"# Calculate distribition of words in Body token list\nbody_dist = nltk.FreqDist(full_corpus)\nbody_dist = pd.DataFrame(body_dist.most_common(2000),\n                         columns=['Word', 'Frequency'])\nbody_dist.describe()","1bdb57de":"# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(body_dist.set_index('Word').to_dict()['Frequency'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 most popular words on Body feature\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","489e5538":"def text_cleaner(x, nlp, pos_list, lang=\"english\"):\n    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n        It allows you to remove extra spaces, unicode characters, \n        English contractions, links, punctuation and numbers.\n        \n        The re library for using regular expressions must be loaded beforehand.\n        The SpaCy and NLTK librairies must be loaded too. \n\n    Parameters\n    ----------------------------------------\n    x : string\n        Sequence of characters to modify.\n    ----------------------------------------\n    \"\"\"\n    # Remove POS not in \"NOUN\", \"PROPN\"\n    x = remove_pos(nlp, x, pos_list)\n    # Case normalization\n    x = x.lower()\n    # Remove unicode characters\n    x = x.encode(\"ascii\", \"ignore\").decode()\n    # Remove English contractions\n    x = re.sub(\"\\'\\w+\", '', x)\n    # Remove ponctuation but not # (for C# for example)\n    x = re.sub('[^\\\\w\\\\s#]', '', x)\n    # Remove links\n    x = re.sub(r'http*\\S+', '', x)\n    # Remove numbers\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    # Remove extra spaces\n    x = re.sub('\\s+', ' ', x)\n        \n    # Tokenization\n    x = nltk.tokenize.word_tokenize(x)\n    # List of stop words in select language from NLTK\n    stop_words = stopwords.words(lang)\n    # Remove stop words\n    x = [word for word in x if word not in stop_words \n         and len(word)>2]\n    # Lemmatizer\n    wn = nltk.WordNetLemmatizer()\n    x = [wn.lemmatize(word) for word in x]\n    \n    # Return cleaned text\n    return x","5f7b2e49":"# Spacy features\nnlp = spacy.load('en', exclude=['tok2vec', 'ner', 'parser', \n                                'attribute_ruler', 'lemmatizer'])\npos_list = [\"NOUN\",\"PROPN\"]\n# Apply full cleaner on Title\nprint('-' * 50)\nprint(\"Start Title cleaning ...\")\nprint('-' * 50)\nstart_time = time.time()\ndata['Title_cleaned'] = data.Title\\\n                            .progress_apply(lambda x: \n                                            text_cleaner(x,\n                                                         nlp,\n                                                         pos_list,\n                                                         \"english\"))\nexec_time = time.time() - start_time\nprint(\"Execution time : {:.2f}s\".format(exec_time))\nprint('-' * 50)\nprint(data['Title_cleaned'].head(3))","818bdc0e":"# Calculate lenght of each list in Body\ndata['Title_tokens_count'] = [len(_) for _ in data.Title_cleaned]\n\n# Countplot of body lenght\nfig = plt.figure(figsize=(20, 12))\nax = sns.countplot(x=data.Title_tokens_count)\nmedian_plot = data.Title_tokens_count.median()\nplt.axvline(median_plot - data.Title_tokens_count.min(),\n            color=\"r\", linestyle='--',\n            label=\"Title tokens Lenght median : \"+str(median_plot))\nax.set_xlabel(\"Lenght of body tokens\")\nplt.title(\"Title tokens lenght of Stackoverflow questions after cleaning\",\n          fontsize=18, color=\"#641E16\")\nplt.legend()\nplt.show()","3c0a50cf":"# Create a list of all tokens for Title\nfull_corpus_t = []\nfor i in data['Title_cleaned']:\n    full_corpus_t.extend(i)\n\n# Calculate distribition of words in Title token list\ntitle_dist = nltk.FreqDist(full_corpus_t)\ntitle_dist = pd.DataFrame(title_dist.most_common(500),\n                          columns=['Word', 'Frequency'])\n\n# Plot word cloud with tags_list (frequencies)\nfig = plt.figure(1, figsize=(17, 12))\nax = fig.add_subplot(1, 1, 1)\nwordcloud = WordCloud(width=900, height=500,\n                      background_color=\"black\",\n                      max_words=500, relative_scaling=1,\n                      normalize_plurals=False)\\\n    .generate_from_frequencies(title_dist.set_index('Word').to_dict()['Frequency'])\n\nax.imshow(wordcloud, interpolation='bilinear')\nax.axis(\"off\")\nplt.title(\"Word Cloud of 500 most popular words on Title feature\\n\",\n          fontsize=18, color=\"#641E16\")\nplt.show()","fd8e0d74":"# Delete items with number of Body tokens < 5\ndata = data[(data.body_tokens_count >= 5) & (data.Title_tokens_count > 0)]\n# Remove calculated features\ndata = data[['Title_cleaned',\n             'Body_cleaned',\n             'Score',\n             'Tags_list']]\n# Rename columns\ndata = data.rename(columns={'Title_cleaned': 'Title',\n                            'Body_cleaned': 'Body',\n                            'Tags_list': 'Tags'})\ndata.head(3)","16e50a23":"# Export to CSV\ndata.to_csv(\"StackOverflow_questions_2009_2020_cleaned.csv\", sep=\";\")","96e8b6a1":"A pr\u00e9sent, nous avons des listes de mots d\u00e9barrass\u00e9es des mots courants (stop words), de la ponctuation, des liens et des nombres. Une derni\u00e8re \u00e9tape que nous pouvons effectuer est la **Lemmatisation**. Ce proc\u00e9d\u00e9 consiste \u00e0 prend le mot \u00e0 sa forme racine appel\u00e9e Lemme. Cela nous permet d'amener les mots \u00e0 leur forme \"dictionnaire\". Nous allons pour cela utiliser \u00e0 nouveau la librairie `NLTK`.","02c680e8":"## <span style=\"color: #641E16\" id=\"section_3\">Nettoyage des questions<\/span>\n\nAfin de traiter au mieux les donn\u00e9es textuelles du `Body`, il est n\u00e9cessaire de r\u00e9aliser plusieurs t\u00e2ches de data cleaning. Par exemple, le texte stock\u00e9 dans cette variable est au format HTML. Ces balises vont polluer notre analyse. Nous allons donc **supprimer toutes les balises HTML** avec la librairie `BeautifulSoup` pour ne conserver que le texte brut.\n\nMais avant cette op\u00e9ration,nous allons supprimer tout le contenu plac\u00e9 entre 2 balises html `<code><\/code>`, cela nous permettra de supprimer tout le code brut souvent copi\u00e9 dans les questions Stackoverflow et qui pourrait avoir un fort impact pour la suite.","9f7da75a":"A pr\u00e9sent, nous devons **v\u00e9rifier si les textes des questions sont r\u00e9dig\u00e9s en diverses langues**. Cela nous permettra de d\u00e9finir la liste des stop words \u00e0 \u00e9liminer :","d7ffb0a2":"Nous allons modifier les s\u00e9parateurs de Tags pour favoriser les extractions :","21af515f":"## <span style=\"color: #641E16\" id=\"section_2\">Data exploration<\/span>\n\nDans un premier temps, nous allons regarder l'**\u00e9volution du nombre de questions par ann\u00e9e** dans notre jeu de donn\u00e9es.","70fda99e":"Les tags contenus dans la variable `Tags` sont ensuite split\u00e9s et ajout\u00e9s dans une liste pour ensuite les classer :","ef8339e9":"Dans les 40 tags les plus populaires sur StackOverflow, les tags **C++**, **C#** et **java** sont sans surprise dans le top 3. Le dataset compte **plus de 16 800 tags** diff\u00e9rents pour la p\u00e9riode 2009 - 2020. \n\nNous pouvons \u00e9galement **visualiser les 500 premi\u00e8res cat\u00e9gories dans un nuage de mots** :","b677c88a":"<div style=\"text-align:center;\"><img src=\"http:\/\/www.mf-data-science.fr\/images\/projects\/intro.jpg\" style='width:100%; margin-left: auto; margin-right: auto; display: block;' \/><\/div>","a1e84af2":"### Analyse des tags\nNous allons faire une rapide analyse exploratoire sur les tags du jeu de donn\u00e9es.","d53859b7":"Nous allons \u00e9galement ploter la r\u00e9partition des **longueurs de la variable** `Body` *(les corps de texte des questions)*. L'\u00e9tendue \u00e9tant tr\u00e8s importante, nous allons dans un premier temps **discr\u00e9tiser ces longueur** pour ne pas surcharger les temps de calculs de projection graphique :","9dae0635":"Pour la majorit\u00e9 des questions StackOverflow analys\u00e9es, 3 tags sont utilis\u00e9s. Cela nous donne d\u00e9j\u00e0 une indication sur le type de mod\u00e9lisation \u00e0 mettre en oeuvre.\n\n### Filtrage du jeu de donn\u00e9es avec les meilleurs Tags : \nLes process de NLP sont des algorithmes assez lents compte tenu de la quantit\u00e9 de donn\u00e9es \u00e0 traiter. Pour filtrer notre jeu de donn\u00e9es, nous allons **s\u00e9lectionner toutes les questions qui comportent au minimum un des 50 meilleurs tags et supprimer les autres tags** :","0420b53f":"On remarque que 75% des 2000 mots les plus communs apparaissent 69 fois *(3\u00e8me quartile)*. Or, le maximum se situe \u00e0 plus de 5700.","3c0d112c":"## <span style=\"color: #641E16\" id=\"section_4\">Nettoyage des titres<\/span>\nNous avons pr\u00e9alablement d\u00e9fini une fonction pour notre cleaning des Body. Nous allons la modifier pour y int\u00e9grer la tokenisation, les stop words et la lemmanisation afin d'obtenir un processus complet \u00e0 appliquer aux titres des posts.","845bb465":"Enfin, afin d'avoir suffisement de \"mati\u00e8re\" pour alimenter nos algorithmes de pr\u00e9diction de Tags, nous allons **conserver uniquement les questions qui comptent au minimum 5 tokens dans la variable Body**.","b07f633f":"## <span style=\"color: #641E16\" id=\"section_5\">Export du dataset nettoy\u00e9<\/span>\nNous pouvons maintenant supprimer les variables cr\u00e9\u00e9es pour l'analyse exploratoire, qui ne nous seront plus utiles, et exporter le dataset pour nos mod\u00e9lisations supervis\u00e9es et non supervis\u00e9es disponible dans le Notebook [Stackoverflow questions - tag generator](https:\/\/www.kaggle.com\/michaelfumery\/stackoverflow-questions-tag-generator)","9b9ef42d":"# <span style=\"color:#641E16\">Sommaire<\/span>\n1. [Importation et description des donn\u00e9es](#section_1)\n2. [Data exploration](#section_2)\n3. [Nettoyage des questions](#section_3)\n4. [Nettoyage des titres](#section_4)\n5. [Export du dataset nettoy\u00e9](#section_5)","85ecb690":"Le body \u00e9tant \u00e0 pr\u00e9sant clean\u00e9, nous allons regarder la r\u00e9partition de la **taille des corpus** dans le jeu de donn\u00e9 nettoy\u00e9 :","74ffadd8":"Nous allons \u00e0 pr\u00e9sent r\u00e9aliser plusieurs op\u00e9rations de Text cleaning pour que nos donn\u00e9es soient exploitables par les algorithmes de NLP :\n- Suppression de tous les mots autres que les noms\n- Mettre tout le texte en **minuscules**\n- Supprimer les **caract\u00e8res Unicode** (comme les Emojis par exemple)\n- Suppression des **espaces suppl\u00e9mentaires**\n- Suppression de la **ponctuation**\n- Suppression des **liens**\n- Supprimer les **nombres**","4e1b1165":"On remarque en effet que les centres d'int\u00e9r\u00eat \u00e9voluent en fonction des ann\u00e9es. Cependant, on retrouve les principaux languages et framework informatiques dans les premi\u00e8res places.     \n\nNous allons a pr\u00e9sent regarder le **nombre de Tags par question** :","d8a446ac":"Maintenant que nous avons un texte brut d\u00e9barass\u00e9 de ses balises HTML et du code, nous allons utiliser `nltk.pos_tag` pour **identifier la nature de chaque mot du corpus afin de pouvoir ensuite conserver uniquement les noms**. Nous allons ici cr\u00e9er une function qui sera appliqu\u00e9e ensuite dans un cleaner plus complet *(tout aurait pu \u00eatre r\u00e9alis\u00e9 avec SpaCy, mais pour l'exercice et la m\u00e9thode, les autres \u00e9tapes sont d\u00e9taill\u00e9es)*.","313d7bb0":"## <span style=\"color: #641E16\" id=\"section_1\">Importation et description des donn\u00e9es<\/span>","a3b68373":"Nous pouvons \u00e0 pr\u00e9sent projeter la distribution de la taille des tokens Title et le nuage de mots correspondant aux 500 meilleurs apparitions : ","8d2b8989":"On remarque que la majeur partie des questions compte moins de 4000 caract\u00e8res *(balises HTML compris)* mais certains posts d\u00e9passent les 31 000 caract\u00e8res. Nous allons **filtrer notre jeu de donn\u00e9es pour conserver uniquement les questions de moins de 4 000 caract\u00e8res** afin de ne pas compliquer le NLP plus que n\u00e9cessaire.","67668f80":"Il peut \u00eatre int\u00e9ressant de regarder si ces tags populaires ont \u00e9volu\u00e9s au fil du temps. Prenons par exemple les ann\u00e9es 2009, 2012, 2016 et 2020 pour v\u00e9rifier.","2072c720":"La langue Anglaise est tr\u00e8s majoritairement repr\u00e9sent\u00e9e dans notre dataset. Nous allons donc **supprimer de notre jeu de donn\u00e9es tous les post dans une autre langue que l'anglais**.","0a9c14a0":"# <span style=\"color: #641E16\">Contexte<\/span>\nNous allons ici d\u00e9velopper un algorithme de Machine Learning destin\u00e9 \u00e0 assigner automatiquement plusieurs tags pertinents \u00e0 une question pos\u00e9e sur le c\u00e9l\u00e9bre site Stack overflow.     \nCe programme s'adresse principalement aux nouveaux utilisateurs, afin de leur sugg\u00e9rer quelques tags relatifs \u00e0 la question qu'ils souhaitent poser.\n\n### Les donn\u00e9es sources\nLes donn\u00e9es ont \u00e9t\u00e9 capt\u00e9es via l'outil d\u2019export de donn\u00e9es ***stackexchange explorer***, qui recense un grand nombre de donn\u00e9es authentiques de la plateforme d\u2019entraide.     \nElles portent sur la p\u00e9riode 2009 \/ 2020 et **uniquement sur les posts \"de qualit\u00e9\"** ayant au minimum 1 r\u00e9ponse, 5 commentaires, 20 vues et un score sup\u00e9rieur \u00e0 5.\n\n### Objectif de ce Notebook\nDans ce Notebook, nous allons traiter la partie **data cleaning et exploration des donn\u00e9es**. Un second notebook traitera ensuite les approches supervis\u00e9es et non supervis\u00e9es pour traiter la cr\u00e9ation de Tags \u00e0 partir des donn\u00e9es textuelles.     \n\nTous les Notebooks du projet seront **versionn\u00e9s dans Kaggle mais \u00e9galement dans un repo GitHub** disponible \u00e0 l'adresse https:\/\/github.com\/MikaData57\/Analyses-donnees-textuelles-Stackoverflow","b1b62087":"Nous pouvons \u00e0 pr\u00e9sent **supprimer tous les stop words en langue Anglaise** gr\u00e2ce \u00e0 la librairie `NLTK`. Avant cette \u00e9tape, nous allons r\u00e9aliser une **tockenisation** c'est \u00e0 dire d\u00e9couper les phrase en mots et cr\u00e9ation d'une liste *(chaque phrase est une liste de mots)*","a3c681ff":"Nous allons \u00e9galement regarder les **fr\u00e9quences de chaque mots de la variable Body** pour visualiser les plus repr\u00e9sent\u00e9s :","48638f8c":"Le jeu de donn\u00e9es ne compte pas de valeurs nulles. La variable Id ne compte que des valeurs uniques, nous pouvons donc l'utiliser en index :","30050e2c":"On remarque ici que sur nos crit\u00e8res de s\u00e9lection, le nombre de questions pos\u00e9es a tendance \u00e0 diminuer de mani\u00e8re constante depuis 2014.\n\nNous allons \u00e0 pr\u00e9sent v\u00e9rifier la **longeur des diff\u00e9rents titres** de la base :"}}