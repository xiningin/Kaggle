{"cell_type":{"590746bd":"code","58d19c51":"code","08c14dd0":"code","d7c4ed54":"code","f6d704cc":"code","b75d6a81":"code","5b4ec8d1":"code","9804f490":"code","58ee814e":"code","d2fa1a49":"code","05ee1cad":"code","5c062c66":"code","ec71afe2":"code","c9d496ae":"code","df0ea1eb":"code","aa282777":"code","80009d2e":"code","f383fc38":"code","15d4df66":"code","bae66028":"code","6b2e093b":"code","425de61f":"code","d02f9c24":"code","2b0dee60":"code","0b198e7b":"code","d4b844aa":"code","6965e54a":"code","1cac0bb1":"code","dc75543e":"code","4e7faecb":"code","3ae3ceff":"code","c615ce47":"code","8f2c4a09":"code","6b074584":"code","95ec9689":"code","8ca1f8cd":"code","169ec879":"code","924e24a4":"code","ec847ada":"code","d2417df4":"code","5ba9bc4e":"code","fc7fc3fd":"code","0f41fa73":"code","c0db9128":"code","9a7c5b19":"code","0496ec32":"code","c12a0123":"code","1a53a815":"code","44ec68a2":"markdown","cfd3d24a":"markdown","2cab22d8":"markdown","d4da4309":"markdown","581cf79d":"markdown","56cb5ec1":"markdown","72d0d14e":"markdown","aa4e5aa6":"markdown","0990cf45":"markdown","6c9a232e":"markdown","d8dc0093":"markdown","e0c7196e":"markdown","8ab76e6b":"markdown","f063b07a":"markdown","30bc51fe":"markdown","137cd8a6":"markdown","b03a0a23":"markdown","972c5a4e":"markdown","d1ecb67c":"markdown","467509d6":"markdown","afba1c9f":"markdown","79ec8866":"markdown","3f28ec39":"markdown","7a9ae3a9":"markdown","f49f58c1":"markdown","59202d04":"markdown","334123c6":"markdown","9a7129e6":"markdown","158400d8":"markdown","c2135368":"markdown","776d3b95":"markdown"},"source":{"590746bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","58d19c51":"import pandas as pd\nimport numpy as np\nimport re\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom xgboost import XGBClassifier\n\n# from imblearn.over_sampling import SMOTE\n# from imblearn.under_sampling import RandomUnderSampler","08c14dd0":"# set plot rc parameters\n\n# jtplot.style(grid=False)\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = '#232323'\n#plt.rcParams['axes.edgecolor'] = '#FFFFFF'\nplt.rcParams['figure.figsize'] = 10, 7\nplt.rcParams['legend.loc'] = 'best'\nplt.rcParams['legend.framealpha'] = 0.2\nplt.rcParams['text.color'] = '#666666'\nplt.rcParams['axes.labelcolor'] = '#666666'\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['xtick.color'] = '#666666'\nplt.rcParams['xtick.labelsize'] = 14\nplt.rcParams['ytick.color'] = '#666666'\nplt.rcParams['ytick.labelsize'] = 14\n\n# plt.rcParams['font.size'] = 16\n\nsns.color_palette('dark')\n%matplotlib inline","d7c4ed54":"# load data and print first five data points\n\ndftrain = pd.read_csv('..\/input\/titanic\/train.csv')\ndftest = pd.read_csv('..\/input\/titanic\/test.csv')\ndftrain.head()","f6d704cc":"# get dimentions of data table\n\ndftrain.shape","b75d6a81":"# meta data\n\ndftrain.info()","5b4ec8d1":"# meta data\n\ndftest.info()","9804f490":"# let's look at unique values for each attribute\n\nfor col in dftrain.columns:\n    print('Unique values in {}: {}'.format(col, len(dftrain[col].value_counts())))","58ee814e":"# let's look at unique values for each attribute\n\nfor col in dftest.columns:\n    print('Unique values in {}: {}'.format(col, len(dftest[col].value_counts())))","d2fa1a49":"dftrain.describe()","05ee1cad":"dftest.describe()","5c062c66":"# Distribution of categorical features\n\nfig, axs = plt.subplots(2,2, figsize=(15,12))\naxs = axs.flatten()\ncols = ['Survived', 'Sex', 'Pclass', 'Embarked']\nfor idx, ax in enumerate(axs):\n    sns.countplot(data=dftrain, x=cols[idx], ax=ax)","ec71afe2":"# Survival w.r.p.t gender and passenger class\n\nfig, axs = plt.subplots(1,2, figsize=(15,8))\naxs = axs.flatten()\nsns.countplot(data=dftrain, x='Sex', hue='Survived', ax=axs[0])\nsns.countplot(data=dftrain, x='Pclass', hue='Survived', ax=axs[1])\n\nplt.show()","c9d496ae":"# Distribution of age and fare w.r.p.t survival \n\nfig, axs = plt.subplots(1,2, figsize= (15, 8))\naxs = axs.flatten()\nsns.violinplot(data=dftrain, x='Sex', y='Age', hue='Survived', ax=axs[0], split=True)\nsns.violinplot(data=dftrain, x='Sex', y='Fare', hue='Survived', ax=axs[1], split=True)\n\nplt.show()","df0ea1eb":"# Distribution of age and fare w.r.p.t survival \n\nfig, axs = plt.subplots(1,2, figsize= (15, 8))\naxs = axs.flatten()\nsns.violinplot(data=dftrain, x='Pclass', y='Age', hue='Sex', ax=axs[0], split=True)\nsns.violinplot(data=dftrain, x='Pclass', y='Fare', hue='Sex', ax=axs[1], split=True)\n\nplt.show()","aa282777":"# impute age\ndftrain['Age'] = dftrain['Age'].fillna(dftrain['Age'].median())\ndftest['Age'] = dftest['Age'].fillna(dftrain['Age'].median())\n\n# impute embarked\ndftrain['Embarked'] = dftrain['Embarked'].fillna('S')\n\n# impute fare\ndftest['Fare'] = dftest['Fare'].fillna(dftrain['Fare'].median())","80009d2e":"dftrain.info(), dftest.info()","f383fc38":"# one hot encode train data\ndftrain = pd.concat([dftrain,\n                    pd.get_dummies(dftrain['Sex']),\n                    pd.get_dummies(dftrain['Embarked'])], axis=1)\n# one hot encode test data\ndftest = pd.concat([dftest,\n                    pd.get_dummies(dftest['Sex']),\n                    pd.get_dummies(dftest['Embarked'])], axis=1)","15d4df66":"dftrain.head()","bae66028":"dftrain.shape, dftest.shape","6b2e093b":"# passenger had cabin or not\ndftrain['Cabin_bool'] = dftrain['Cabin'].isna().astype(int)\ndftest['Cabin_bool'] = dftest['Cabin'].isna().astype(int)","425de61f":"# drop non numeric columns\ndropcols = ['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\ntrainid = dftrain['PassengerId']\ntestid = dftest['PassengerId']\ndftrain.drop(dropcols, inplace=True, axis=1)\ndftest.drop(dropcols, inplace=True, axis=1)\ndftrain.head()","d02f9c24":"dftest.head()","2b0dee60":"# x and y for training\nX = dftrain.drop('Survived', axis=1)\nY = dftrain['Survived']","0b198e7b":"cols = X.columns","d4b844aa":"# scaler instances\ntrain_scaler = StandardScaler()\ntest_scaler = StandardScaler()\n# scale data\nX = train_scaler.fit_transform(X)\ndftest = test_scaler.fit_transform(dftest)\n# add columns\nX = pd.DataFrame(X, columns=cols)\ndftest = pd.DataFrame(dftest, columns=cols)","6965e54a":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.25, random_state=12)","1cac0bb1":"Xcorr = X.corr()\n\n# plot correlatation matrix\nplt.figure(figsize=(16, 16))\ng = sns.heatmap(Xcorr,\n            cbar = True,\n            square = True,\n            linewidth=0.3,\n            fmt='.2f')\nplt.title('Feature correlation',\n          color='#666666',\n          fontdict={'fontsize': 22})\nplt.show()","dc75543e":"# PCA for visualization\npca = PCA(n_components = 2)\npca.fit(Xtrain)\nX_pca = pca.transform(Xtrain)\n\n# plot PCA components\nfig, axs = plt.subplots(figsize=[10,9])\nsns.scatterplot(x=X_pca[:,0],\n                y=X_pca[:,1],\n                hue=Ytrain,\n                palette=[sns.xkcd_rgb['red pink'],\n                         sns.xkcd_rgb['greenish cyan']],\n                edgecolor=None,\n                ax=axs)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Scatter plot based on survival')\nplt.show()","4e7faecb":"# Function to print model performance summary statistics\n\ndef performance_summary(model, Xtrain, Xtest, Ytrain, Ytest):\n    \n    Ytrain_pred = model.predict(Xtrain)\n    Ytest_pred = model.predict(Xtest)\n\n    # model performance\n    # accuracy score\n    print('Training Accuracy:\\n', accuracy_score(Ytrain, Ytrain_pred))\n    print('\\n')\n    print('Test Accuracy:\\n', accuracy_score(Ytest, Ytest_pred))\n    print('\\n')\n    # classification report\n    print('Classification Report training:\\n', classification_report(Ytrain,Ytrain_pred))\n    print('\\n')\n    print('Classification Report test:\\n', classification_report(Ytest,Ytest_pred))\n    \n    return","3ae3ceff":"# Function to plot Confusion matrix\n\ndef plot_confusion_matrix(model, Xtrain, Xtest, Ytrain, Ytest):\n    \n    Ytrain_pred = model.predict(Xtrain)\n    Ytest_pred = model.predict(Xtest)\n\n    # confusion matrix\n    fig, axs = plt.subplots(1,2,\n                            figsize=[15,5])\n    axs = axs.flatten()\n    \n    axs[0].set_title('Training data')\n    # axs[0].set_xlabel('Predicted label')\n    # axs[0].set_ylabel('True label')\n    axs[1].set_title('Test data')\n    # axs[1].set_xlabel('Predicted label')\n    # axs[1].set_ylabel('True label')\n    \n    fig.text(0.27, 0.04, 'Predicted label', ha='center')\n    fig.text(0.70, 0.04, 'Predicted label', ha='center')\n    fig.text(0.04, 0.5, 'True label', va='center', rotation='vertical')\n    fig.text(0.5, 0.5, 'True label', va='center', rotation='vertical')\n    \n    sns.heatmap(confusion_matrix(Ytrain,Ytrain_pred),\n                    annot=True,\n                    xticklabels=['dpd < 30', 'dpd > 30'],\n                    yticklabels=['dpd < 30', 'dpd > 30'],\n                    fmt=\"d\",\n                    ax=axs[0])\n    \n    sns.heatmap(confusion_matrix(Ytest,Ytest_pred),\n                    annot=True,\n                    xticklabels=['dpd < 30', 'dpd > 30'],\n                    yticklabels=['dpd < 30', 'dpd > 30'],\n                    fmt=\"d\",\n                    ax=axs[1])\n    plt.show()\n    \n    return","c615ce47":"# Function to plot ROC\n\ndef plot_roc(model, Xtrain, Xtest, Ytrain, Ytest):\n    # ROC curve and area under ROC curve\n\n    # get FPR and TPR for training and test data\n    Ytrain_pred_proba = model.predict_proba(Xtrain)\n    fpr_train, tpr_train, thresholds_train = roc_curve(Ytrain, Ytrain_pred_proba[:,1])\n    # tpr fpr are swapped \n    roc_auc_train = auc(fpr_train, tpr_train)\n    Ytest_pred_proba = model.predict_proba(Xtest)\n    fpr_test, tpr_test, thresholds_test = roc_curve(Ytest, Ytest_pred_proba[:,1])\n    # tpr fpr are swapped\n    roc_auc_test = auc(fpr_test, tpr_test)\n\n    # print area under roc curve\n    print ('AUC_ROC train:\\t', roc_auc_train)\n    print ('AUC_ROC test:\\t', roc_auc_test)\n\n    # plot auc roc\n    fig, axs = plt.subplots(1,2,\n                            figsize=[15,5],\n                            sharex=False,\n                            sharey=False)\n    \n    # training data\n    axs[0].set_title('Receiver Operating Characteristic trainning')\n    axs[0].plot(fpr_train,\n                tpr_train,\n                sns.xkcd_rgb['greenish cyan'],\n                label='AUC = %0.2f'% roc_auc_train)\n    axs[0].legend(loc='lower right')\n    \n    axs[0].plot([0,1],[0,1],\n                ls='--',\n                c=sns.xkcd_rgb['red pink'])\n    \n    axs[0].set_xlim([-0.01,1.01])\n    axs[0].set_ylim([-0.01,1.01])\n    axs[0].set_ylabel('True Positive Rate')\n    axs[0].set_xlabel('False Positive Rate')\n    \n    # test data\n    axs[1].set_title('Receiver Operating Characteristic testing')\n    axs[1].plot(fpr_test,\n                tpr_test,\n                sns.xkcd_rgb['greenish cyan'],\n                label='AUC = %0.2f'% roc_auc_test)\n    axs[1].legend(loc='lower right')\n    \n    axs[1].plot([0,1],[0,1],\n                ls='--',\n                c=sns.xkcd_rgb['red pink'])\n    \n    axs[1].set_xlim([0.0,1.0])\n    axs[1].set_ylim([0.0,1.0])\n    axs[1].set_ylabel('True Positive Rate')\n    axs[1].set_xlabel('False Positive Rate')\n\n    plt.show()\n    \n    return","8f2c4a09":"# hyper parameter tuning\nlr_clf = GridSearchCV(LogisticRegression(penalty='l1', solver='saga', l1_ratio=1),\n                      cv=5,\n                      param_grid={'C': [0.001,0.01,0.1,1,10,100,1000]},\n                      scoring='accuracy')\n\n# fit to data\nlr_clf.fit(Xtrain, Ytrain)","6b074584":"lr_clf.best_params_, lr_clf.best_score_","95ec9689":"# Train model Logistic Regression\nlogreg_model = LogisticRegression(penalty='l1',solver='saga', C=0.1)\nlogreg_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(logreg_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(logreg_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(logreg_model, Xtrain, Xtest, Ytrain, Ytest)","8ca1f8cd":"svm_params = {'kernel': ('linear', 'rbf'),\n             'C': [0.001,0.01,0.1,1,10,100,1000]}\n\nsvm_clf = GridSearchCV(estimator=SVC(),\n                      cv=5,\n                      param_grid=svm_params,\n                      scoring='recall',\n                      n_jobs=-1)\n\nsvm_clf.fit(Xtrain,Ytrain)","169ec879":"svm_clf.best_params_, svm_clf.best_score_","924e24a4":"svm_model = SVC(kernel='rbf',\n                C=1000,\n                probability=True)\nsvm_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(svm_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(svm_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(svm_model, Xtrain, Xtest, Ytrain, Ytest)","ec847ada":"# training model\nxgb_model = XGBClassifier()\nxgb_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(xgb_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(xgb_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(xgb_model, Xtrain, Xtest, Ytrain, Ytest)","d2417df4":"# Grid search xgboost\n\ngbm_params = {'subsample':[0.6,0.7,0.8],\n             'colsample_bytree':[0.6,0.7,0.8],\n             'learning_rate': [0.1,1,10],\n             'n_estimators': [500,1000,1500],\n             'reg_alpha': [1,3,5,9],\n             'max_depth': [2,3,4,5],\n             'gamma': [0.01,0.1,1,10],\n             'min_child_weight': [0.01,0.5,1,3,5]}\n\nclf_params = {'silent':False,\n              'objective':'binary:logistic'}\n\ngbm_clf = RandomizedSearchCV(XGBClassifier(**clf_params),\n                             param_distributions=gbm_params,\n                             scoring = 'accuracy',\n                             cv = 5,\n                             n_jobs = -1,\n                             n_iter=100)\n\ngbm_clf.fit(Xtrain, Ytrain)","5ba9bc4e":"gbm_clf.best_score_, gbm_clf.best_params_","fc7fc3fd":"# model params\nparam_xgb = {'n_estimators': 1000,\n                'colsample_bytree': 0.8,\n                'objective': 'binary:logistic',\n                'max_depth': 5,\n                'min_child_weight': 0.01,\n                'learning_rate': 0.1,\n                'subsample': 0.7,\n                'gamma': 1,\n                'reg_alpha': 1}\n\n# training model\nxgb_model = XGBClassifier(**param_xgb)\nxgb_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(xgb_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(xgb_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(xgb_model, Xtrain, Xtest, Ytrain, Ytest)","0f41fa73":"# training model\nrf_model = RandomForestClassifier()\nrf_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(rf_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(rf_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(rf_model, Xtrain, Xtest, Ytrain, Ytest)","c0db9128":"rf_param = {'bootstrap': [True, False],\n            'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n            'max_features': ['auto', 'sqrt'],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10],\n            'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800]}\n\nrf_clf = RandomizedSearchCV(estimator=RandomForestClassifier(),\n                           cv=5,\n                           param_distributions=rf_param,\n                           scoring='recall',\n                           n_jobs=-1,\n                           n_iter=100)\n\nrf_clf.fit(Xtrain,Ytrain)","9a7c5b19":"rf_clf.best_params_, rf_clf.best_score_","0496ec32":"# model params\nparam_rf = {'n_estimators': 1400,\n           'bootstrap':False,\n           'max_depth':40,\n           'max_features':'sqrt',\n           'min_samples_leaf':2,\n           'min_samples_split':2}\n\n# training model\nrf_model = RandomForestClassifier(**param_rf)\nrf_model.fit(Xtrain, Ytrain)\n\n# performance summary\nperformance_summary(rf_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# confusion matrix\nplot_confusion_matrix(rf_model, Xtrain, Xtest, Ytrain, Ytest)\n\n# ROC plot\nplot_roc(rf_model, Xtrain, Xtest, Ytrain, Ytest)","c12a0123":"# Train model Logistic Regression\nmodel = LogisticRegression(penalty='l1',solver='saga', C=0.1)\nmodel.fit(X, Y)\n\n# prediction on test data\nYpred = model.predict(dftest)","1a53a815":"my_submission = pd.DataFrame([testid, Ypred], columns=['PassengerId', 'Survived'])\nmy_submission.to_csv('submission.csv',index=False)","44ec68a2":"### split data","cfd3d24a":"## Train models","2cab22d8":"Training data has 7 unique values for Parch attribute while test data have 8, we need to fix that","d4da4309":"## Random Forest","581cf79d":"### drop non numeric columns","56cb5ec1":"### PCA Visualization","72d0d14e":"### Train SVM","aa4e5aa6":"Since train and test has similar distribution of all the attributes that means we can impute values in them saperately","0990cf45":"### Train Logistic Regression","6c9a232e":"## Load data","d8dc0093":"### Feature correlation","e0c7196e":"### Impute missing values","8ab76e6b":"## Logistic Regression","f063b07a":"## XGBoost","30bc51fe":"__Observation:__\n* Age has 177 missing values in training data and 86 missing values in test data\n* Cabin has 687 missing values in training data and 326 missing values in test data (people without cabin must be having missing value for cabin attribute)\n* Embarked has two missing values in training data\n\nBefore imputing missing values let's explore data a little more","137cd8a6":"## Import libraries","b03a0a23":"# Titanic Survival Prediction","972c5a4e":"### one hot encode","d1ecb67c":"## Data pre-processing","467509d6":"## SVM","afba1c9f":"### Hyper-parameter tuning","79ec8866":"## Discriptive Statistics","3f28ec39":"### EDA","7a9ae3a9":"### column stadardization","f49f58c1":"### Feature engineering","59202d04":"## Submission","334123c6":"### Helper functions","9a7129e6":"### Hyper-parameter tuning","158400d8":"### Hyper-parameter tuning","c2135368":"*  XGBoost and Logistic Regression gave similar performance","776d3b95":"### Hyper-parameter tuning for SVM"}}