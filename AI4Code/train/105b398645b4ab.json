{"cell_type":{"3c6ea1c1":"code","c4538c89":"code","6526559a":"code","787918a0":"code","e89370b1":"code","f88e472a":"code","7855f596":"code","a6ec2154":"code","818a368f":"code","3f2097c3":"code","3b687665":"code","c12602dd":"code","8186a4e1":"code","829299be":"code","aaefc272":"code","bf51feb7":"code","9ed0eaef":"code","6ecc6649":"code","fa3d1b40":"code","2a1235dc":"code","225d8d89":"code","84700c5a":"code","20e0cf5f":"code","b3c1034a":"markdown","bb80afef":"markdown"},"source":{"3c6ea1c1":"import os\nos.listdir('\/kaggle\/input\/cat-and-dog\/training_set\/training_set')","c4538c89":"dog_dir='\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/dogs'\ncat_dir='\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/cats'","6526559a":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator","787918a0":"x=[]\ny=[]\ni=0\nimg_size=(227,227)\ndef make_list(directory,name):\n    try:\n        \n        \n        for img in os.listdir(directory):\n            path=os.path.join(directory,img)\n            img_array=cv2.imread(path,cv2.IMREAD_COLOR)\n            img_array=cv2.resize(img_array,img_size)\n            x.append(np.array(img_array))\n            y.append(str(name))\n    except:\n        None\n    print('Images of {} are inserted'.format(name))","e89370b1":"make_list(dog_dir,'Dog')\nmake_list(cat_dir,'cat')","f88e472a":"#ONe hot encoding\nle=LabelEncoder()\ny=le.fit_transform(y)\ny=to_categorical(y,2)","7855f596":"from sklearn.model_selection import train_test_split\nx=np.array(x)\nx=x\/255.0\ntrain_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=42)","a6ec2154":"from keras.callbacks import EarlyStopping\nearlystop=EarlyStopping(patience=10)\n","818a368f":"learning_rate_reduction=ReduceLROnPlateau(monitor='val_acc',patience=2,verbose=1,factor=0.5,min_lr=0.001)","3f2097c3":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization","3b687665":"np.random.seed(1000)","c12602dd":"#Instantiation\nAlexNet = Sequential()\n\n#1st Convolutional Layer\nAlexNet.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#2nd Convolutional Layer\nAlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#3rd Convolutional Layer\nAlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n\n#4th Convolutional Layer\nAlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n\n#5th Convolutional Layer\nAlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n\n#Passing it to a Fully Connected layer\nAlexNet.add(Flatten())\n# 1st Fully Connected Layer\nAlexNet.add(Dense(4096, input_shape=(32,32,3,)))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nAlexNet.add(Dropout(0.4))\n\n#2nd Fully Connected Layer\nAlexNet.add(Dense(4096))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n#Add Dropout\nAlexNet.add(Dropout(0.4))\n\n#3rd Fully Connected Layer\nAlexNet.add(Dense(4096))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n#Add Dropout\nAlexNet.add(Dropout(0.4))\n\n#Output Layer\nAlexNet.add(Dense(2))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('softmax'))","8186a4e1":"AlexNet.summary()","829299be":"# Compiling the model\nAlexNet.compile(loss = \"binary_crossentropy\", optimizer= 'adam', metrics=['accuracy'])","aaefc272":"history=AlexNet.fit(train_x,train_y,epochs=25,validation_split=0.30,batch_size=50)","bf51feb7":"#lets take a look on the loss on both the training set and test set\nplt.figure(figsize=(12,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('mean_absolute_error')\nplt.title('epochs-vs-loss')\nplt.legend(['train_set','test_set'])\nplt.show()","9ed0eaef":"#lets analyse the accuracy score on both train and test sets\nplt.figure(figsize=(12,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epochs')\nplt.ylabel('accuracy score')\nplt.title('epochs-vs-accuracy score')\nplt.legend(['train_set','test_set'])\nplt.show()","6ecc6649":"\ntrain_dir='\/kaggle\/input\/cat-and-dog\/training_set\/training_set'\ntest_dir='\/kaggle\/input\/cat-and-dog\/test_set\/test_set'\ntrain_datagen=ImageDataGenerator(rescale=1.\/255,\n                                rotation_range=40,\n                                shear_range=0.2,\n                                width_shift_range=0.2,\n                                height_shift_range=0.2,\n                                horizontal_flip=True,\n                                zoom_range=0.2,\n                                fill_mode='nearest')\ntest_datagen=ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator=train_datagen.flow_from_directory(train_dir,\n                                                 target_size=(227,227),\n                                                 batch_size=50,\n                                                 class_mode='binary')\ntest_generator=test_datagen.flow_from_directory(test_dir,\n                                               target_size=(227,227),\n                                               batch_size=50,\n                                               class_mode='binary')","fa3d1b40":"#Instantiation\nAlexNet_1 = Sequential()\n\n#1st Convolutional Layer\nAlexNet_1.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11), strides=(4,4), padding='same'))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\nAlexNet_1.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#2nd Convolutional Layer\nAlexNet_1.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\nAlexNet_1.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#3rd Convolutional Layer\nAlexNet_1.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\n\n#4th Convolutional Layer\nAlexNet_1.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\n\n#5th Convolutional Layer\nAlexNet_1.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\nAlexNet_1.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n#Passing it to a Fully Connected layer\nAlexNet_1.add(Flatten())\n# 1st Fully Connected Layer\nAlexNet_1.add(Dense(4096, input_shape=(32,32,3,)))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nAlexNet_1.add(Dropout(0.4))\n\n#2nd Fully Connected Layer\nAlexNet_1.add(Dense(4096))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\n#Add Dropout\nAlexNet_1.add(Dropout(0.4))\n\n#3rd Fully Connected Layer\nAlexNet_1.add(Dense(4096))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('relu'))\n#Add Dropout\nAlexNet_1.add(Dropout(0.4))\n\n#Output Layer\nAlexNet_1.add(Dense(2))\nAlexNet_1.add(BatchNormalization())\nAlexNet_1.add(Activation('softmax'))","2a1235dc":"AlexNet_1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","225d8d89":"# epochs=25\n\n# fits the model on batches with real-time data augmentation:\nhistory_1= AlexNet_1.fit(train_generator,validation_data=test_generator, epochs=30,steps_per_epoch=125)","84700c5a":"#lets analyse the accuracy score on both train and test sets\nplt.figure(figsize=(12,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.xlabel('epochs')\nplt.ylabel('accuracy score')\nplt.title('epochs-vs-accuracy score')\nplt.legend(['train_set','test_set'])\nplt.show()","20e0cf5f":"#lets take a look on the loss on both the training set and test set\nplt.figure(figsize=(12,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('mean_absolute_error')\nplt.title('epochs-vs-loss')\nplt.legend(['train_set','test_set'])\nplt.show()","b3c1034a":"**AlexNet Implementation Using DATA AUGMENTATION**","bb80afef":"**ALEXNET ARCHITECTURE**"}}