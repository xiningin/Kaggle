{"cell_type":{"ec08ca55":"code","e8f19165":"code","690cdf2a":"code","74fb103a":"code","77a8e735":"code","51e5d93f":"code","7866f433":"code","d74778ba":"code","d00556c9":"code","3e0affcb":"code","85863291":"code","afd5bc66":"code","3854d5a1":"code","ed35cf31":"code","17fb1ba7":"code","7ef22d79":"code","c473c022":"code","f030cca3":"code","964ea4df":"code","a8a346a6":"code","317a11ee":"code","2c55622f":"code","23c1019e":"code","49d39d16":"code","7822a97f":"code","3fc4f71a":"code","e21f90ae":"code","c2d31e60":"code","ca715ac4":"code","99147fcb":"code","500d2504":"markdown","d5a72ce1":"markdown","0c2f368e":"markdown","28cc6159":"markdown","a8b2a49e":"markdown","19555d40":"markdown","ca56fd59":"markdown","fbfe3216":"markdown","fc2881b8":"markdown","81d98dd1":"markdown","6e376ddc":"markdown","563ae2ec":"markdown","8c1d9974":"markdown","24114e18":"markdown","52aa9526":"markdown","30bdd67e":"markdown","41e823a1":"markdown","8e3f5583":"markdown","9751fca7":"markdown","59aed427":"markdown","5a2efc11":"markdown"},"source":{"ec08ca55":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\"\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport gc\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.math import softplus, tanh\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Dense, Input, Activation, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow_addons.optimizers import RectifiedAdam\n\nimport transformers\nfrom transformers import AutoTokenizer, TFXLMRobertaModel","e8f19165":"sns.set_style(\"darkgrid\")","690cdf2a":"SEED = 42\nEPOCHS = 10\nMAX_LEN = 96\nNUM_SPLITS = 3\nLR = 3e-5\nBATCH_SIZE = 16","74fb103a":"os.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","77a8e735":"path = Path(\"..\/input\/contradictory-my-dear-watson-translated-dataset\/\")","51e5d93f":"train = pd.read_csv(path\/\"train_augmented.csv\", index_col=[\"id\"])\ntest = pd.read_csv(path\/\"test_augmented.csv\", index_col=[\"id\"])","7866f433":"df = pd.concat([train, test])\ndf.loc[df[\"label\"]!=-1, \"type\"] = \"train\"\ndf.loc[df[\"label\"]==-1, \"type\"] = \"test\"","d74778ba":"plt.figure(figsize=(12, 9))\nx = sns.countplot(x=\"language\", hue=\"type\", data=df)\n_ = plt.title(\"Language Distribution\")\n_ = x.set_xticklabels(x.get_xticklabels(), rotation='45')","d00556c9":"plt.figure(figsize=(6, 4))\n_ = sns.countplot(x=\"label\", data=train)\n_ = plt.title(\"Label Distribution\")","3e0affcb":"del df\ngc.collect()","85863291":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)","afd5bc66":"# instantiate a distribution strategy\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)","3854d5a1":"BATCH_SIZE = BATCH_SIZE*strategy.num_replicas_in_sync","ed35cf31":"MODEL = 'jplu\/tf-xlm-roberta-large'\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL)","17fb1ba7":"def fast_encode(df):\n    text = df[['premise', 'hypothesis']].values.tolist()\n    encoded = TOKENIZER.batch_encode_plus(\n        text,\n        pad_to_max_length=True,\n        max_length=MAX_LEN\n    )\n    return np.array(encoded[\"input_ids\"])","7ef22d79":"test_encoded = fast_encode(test)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded)\n    .batch(BATCH_SIZE)\n)","c473c022":"# softplus - log(exp(x)+1)\ndef mish(x):\n    return x*tanh(softplus(x))\nget_custom_objects()[\"mish\"] = Activation(mish)","f030cca3":"def create_model(transformer):\n    \"\"\"\n    Fine-Tuning XLM-Roberta by adding a couple of dense layers & dropout.\n    Adds a dense layer at the end for 3 labels\n    \"\"\"\n    input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32)\n    sequence_output = transformer(input_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    cls_token = Dropout(0.3)(cls_token)\n    cls_token = Dense(32, activation='mish')(cls_token)\n    cls_token = Dense(16, activation='mish')(cls_token)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    optimizer = RectifiedAdam(lr=LR)\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        optimizer, \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    \n    return model","964ea4df":"kfold = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=SEED)","a8a346a6":"# learning rate scheduler\ndef build_lrfn(lr_start=0.00001, lr_max=0.00004, \n               lr_min=0.000001, lr_rampup_epochs=3, \n               lr_sustain_epochs=0, lr_exp_decay=.5):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn","317a11ee":"lrfn = build_lrfn()\nlrs = LearningRateScheduler(lrfn, verbose=1)","2c55622f":"plt.figure(figsize=(9, 6))\nepochs = list(range(EPOCHS))\nlearning_rates = [lrfn(i) for i in epochs]\nplt.plot(epochs, learning_rates)\n_ = plt.title(\"Learning Rate Schedule\")\n_ = plt.xlabel(\"# Epcohs\")\n_ = plt.ylabel(\"Learning Rate\")","23c1019e":"# storing labels only\noof_preds = np.zeros((len(train)))\n# storing probabilities per class\ntest_preds = np.zeros((len(test), 3))","49d39d16":"for fold, (train_index, valid_index) in enumerate(kfold.split(train, train['label'])):\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    print(\"*\"*60)\n    print(\"*\"+\" \"*26+f\"FOLD {fold+1}\"+\" \"*26+\"*\")\n    print(\"*\"*60, end=\"\\n\\n\")\n    \n    X_train = train.iloc[train_index, :].reset_index(drop=True)\n    X_valid = train.iloc[valid_index, :].reset_index(drop=True)\n    \n    y_train = X_train['label'].values\n    y_valid = X_valid['label'].values\n    \n    train_encoded = fast_encode(X_train)\n    valid_encoded = fast_encode(X_valid)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_encoded, y_train))\n    train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_encoded, y_valid))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    num_steps = len(X_train)\/\/BATCH_SIZE\n    \n    # instantiating the model in the strategy scope creates the model on the TPU\n    with strategy.scope():\n        transformer_layer = TFXLMRobertaModel.from_pretrained(MODEL)\n        model = create_model(transformer_layer)\n    \n    history = model.fit(\n        train_dataset,\n        steps_per_epoch=num_steps,\n        validation_data=valid_dataset,\n        epochs=EPOCHS,\n        callbacks=[lrs]\n    ) \n    \n    # stores validation data prediction at respective indices\n    valid_preds = model.predict(valid_dataset)\n    oof_preds[valid_index] = valid_preds.argmax(axis=1)\n    \n    # adds up test prediction per fold\n    preds = model.predict(test_dataset)\n    test_preds += preds\/NUM_SPLITS","7822a97f":"print(f\"Accuracy: {accuracy_score(train['label'], oof_preds)}\")","3fc4f71a":"print(f\"Prediction Shape: {test_preds.shape}\")\nprint(f\"Predictions:\\n{test_preds[:5]}\")","e21f90ae":"test[\"prediction\"] = test_preds.tolist()\n# groupby \"id\" original & translated version\nsubmission = test.groupby(by='id')['prediction'].apply(lambda x: [sum(y) for y in zip(*x)]).reset_index()","c2d31e60":"# assigning index from sample_submission file\nsample_submission = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\")\nsubmission = submission.set_index(\"id\")\nsubmission = submission.reindex(index=sample_submission[\"id\"])\nsubmission = submission.reset_index()","ca715ac4":"submission['prediction'] = submission[\"prediction\"].apply(lambda x: np.argmax(x))\nsubmission.to_csv(\"submission.csv\", index=False)","99147fcb":"submission[\"prediction\"].value_counts()","500d2504":"## Training\n\n* Make Stratified 5-Fold split\n* Train the model per fold\n* Predict & store test predictions per model\n* Combine them in the end by dividing it by number of folds & taking the maximum index label","d5a72ce1":"## Approach\n\nI'll be using the augmented data created by translating training data. I've made the dataset public & you can read about it [here](https:\/\/www.kaggle.com\/aditya08\/contradictory-my-dear-watson-translated-dataset). Essentially, it increases the training dataset by translating training data as explained in [this](https:\/\/www.kaggle.com\/jpmiller\/augmenting-data-with-translations) kernel.\n\n* Augment train & test data\n* Make K-Fold train & validation split\n* Train XLM-Roberta large model from Huggingface per fold\n* Test Time Augmentation (TTA) \n    * Make prediction on test data per fold & average the result\n    * Groupby test data by \"ID\" to average the result of original & translated test data ","0c2f368e":"## Constants","28cc6159":"## Encoding","a8b2a49e":"## About the competition\n\nThe challenge is a Natural Language Inference (NLI) competition where given two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated.\n\nYour task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. Also, the hypothesis and premise are in multiple languages. ","19555d40":"## Importing Libraries","ca56fd59":"*Mish* is a smooth activation function. It calculates the following -\n\n$x * tanh(softplus(x))$","fbfe3216":"Loads the tokenzier *(SentencePiece tokenizer)* used in `XLM-Roberta`.","fc2881b8":"To use K-Fold on TPUs without causing memory issues (refer discussion [here](https:\/\/www.kaggle.com\/c\/flower-classification-with-tpus\/discussion\/131045)) - \n* Call `tf.tpu.experimental.initialize_tpu_system(tpu)` between each model training to completely reinitialize the TPU\n* We can use `drop_remainder=True` while training to increase speed (TPUs run very slowly on final partial batch)","81d98dd1":"## Setting up TPU","6e376ddc":"## Model","563ae2ec":"Kindly consider upvoting the kernel, if you find it useful!","8c1d9974":"* Majority of the training & testing samples are of English language and the rest are in minority.\n* Number of samples per language in training data appear to be similar\n* Langauges in training and test dataset appear in similar ratios ","24114e18":"## Submission","52aa9526":"Setting seed for reproducibility","30bdd67e":"To avoid data starvation to TPUs & achieve peak performance, we need an efficient input pipeline that delivers data for the next step before the current step has finished. We can apply the following transformation on training dataset `(tf.data.Datset)`. Refer [here](https:\/\/www.tensorflow.org\/guide\/data_performance) for more details\n\n* `prefetch()`\n* `batch()`\n* `caching()`","41e823a1":"### Scaling batch size\nA general idea is to scale the batch size by the number of TPU cores. For Kaggles, TPU v3-8, the core count is 8. So, we essentially increase the batch size 8-fold.","8e3f5583":"## Basic Exploration","9751fca7":"### Detection & Initialization\n**Detection** - The TPUClusterResolver instance helps locate the TPUs.\n\n**Initialization** - Initialization process takes place when we switch to TPU accelerator, as we connect to remote clusters and initialize TPUs.","59aed427":"All the 3 classes are evenly distributed","5a2efc11":"### Distribution Strategy\n\nTensorflow provides apis to distribute training across multiple GPUs, multiple machines or TPUs. Tensorflow provides  multiple distribution strategies as follows -\n* MirroredStrategy\n* TPUStrategy\n* MultiWorkerMirroredStrategy\n* CentralStorageStrategy\n* ParameterServerStrategy\n\nIn Tensorflow 2.2 however, only **MirroredStrategy** & **TPUStrategy** are compatible to be used with Keras API & Custom training loops. You can find more detail in this table [here](https:\/\/www.tensorflow.org\/guide\/distributed_training#types_of_strategies)\n\n**MirroredStrategy** - Supports distributed training on multiple GPUs on one machine. It creates one replica per GPU device.\n\n**TPUStrategy** - TPUStrategy lets us train tensorflow models on TPUs. Similar to *MirroredStrategy*, all the variables are copied to each core. All-reduce algorithms communicate the updates of these variables & aggregates the result from all the devices."}}