{"cell_type":{"3126cbee":"code","77c4b103":"code","a22dd715":"code","693a8638":"code","75aa2ab1":"code","6fa09145":"code","b1bafc54":"code","a4c2585e":"code","e1c37550":"code","24282a89":"markdown","a56d8fbc":"markdown","0147c13e":"markdown","a5f662bf":"markdown","75ad5ec4":"markdown"},"source":{"3126cbee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77c4b103":"!pip install python-mnist","a22dd715":"import numpy as np\nfrom mnist.loader import MNIST","693a8638":"\"\"\"Activation Functions\"\"\"\n\ndef signum(x):\n    x[x > 0] = 1\n    x[x <= 0] = -1\n\n    return x\n\n\n# --------------------------------------------------------\n\n\"\"\"Predicts the labels by choosing the label of the classifier with highest confidence(probability)\"\"\"\ndef predict(all_weights, images, labels, size):\n    images = np.hstack((np.ones((size, 1)), images))\n\n    predicted_labels = np.dot(all_weights, images.T)\n\n    # signum activation function\n    predicted_labels = signum(predicted_labels)\n\n    predicted_labels = np.argmax(predicted_labels, axis=0).T\n\n    corr = 0\n    label_tot = {}\n    label_corr = {}\n    label_acc = {}\n    L = np.unique(labels)\n    for i in L:\n        label_tot[i] = 0\n        label_corr[i] = 0\n        label_acc[i] = 0\n\n    for i in range(len(labels)):\n        label_tot[labels[i]] += 1\n        if predicted_labels[i] == labels[i]:\n            corr += 1\n            label_corr[labels[i]] += 1\n\n    for i in L:\n        label_acc[i] = label_corr[i]\/label_tot[i] * 100\n\n    return corr\/len(labels)*100, label_acc\n\n\n# --------------------------------------------------------\ndef learning(train_images, train_labels, weights):\n    epochs_values = []\n    error_values = []\n\n    for k in range(epochs):\n        missclassified = 0\n\n        for t, l in zip(train_images, train_labels):\n            h = np.dot(t, weights)\n\n            h = signum(h)\n\n            if h[0] != l[0]:\n                missclassified += 1\n\n            gradient = t * (h - l)\n\n            # reshape gradient\n            gradient = gradient.reshape(gradient.shape[0], 1)\n\n            weights = weights - (gradient * alpha)\n\n        error_values.append(missclassified \/ training_size)\n        epochs_values.append(k)\n\n    return weights\n\n\n\"\"\"Find optimal weights for each logistic binary classifier\"\"\"\n\n\ndef train(train_images, train_labels):\n    # add 1's as x0\n    train_images = np.hstack((np.ones((training_size, 1)), train_images))\n\n    # add w0 as 0 initially\n    all_weights = np.zeros((labels, train_images.shape[1]))\n\n    train_labels = train_labels.reshape((training_size, 1))\n\n    train_labels_copy = np.copy(train_labels)\n\n    for j in range(labels):\n\n        print(\"Training Classifier: \", j+1)\n\n        train_labels = np.copy(train_labels_copy)\n\n        # initialize all weights to zero\n        weights = np.zeros((train_images.shape[1], 1))\n\n        for k in range(training_size):\n            if train_labels[k, 0] == j:\n                train_labels[k, 0] = 1\n            else:\n                train_labels[k, 0] = -1\n\n        weights = learning(train_images, train_labels, weights)\n\n        all_weights[j, :] = weights.T\n\n    return all_weights\n\n","75aa2ab1":"# Global Variables\ntraining_size = 60000\ntesting_size = 10000\n\n\nalpha = 0.01\niterations = 2000  # epochs for batch mode gradient descent\n\nepochs = 15\n\nlabels = 10\n\n","6fa09145":"# load data\ndata = MNIST('\/kaggle\/input\/mnist-samples\/samples\/')\n\ntrain_images, train_labels = data.load_training()\ntest_images, test_labels = data.load_testing()\n\ntrain_images = np.array(train_images[:training_size])\ntrain_labels = np.array(train_labels[:training_size], dtype=np.int32)\n\ntest_images = np.array(test_images[:testing_size])\ntest_labels = np.array(test_labels[:testing_size], dtype=np.int32)\n\n\"\"\"Rescaling Data\"\"\"\ntrain_images = train_images \/ 255\ntest_images = test_images \/ 255","b1bafc54":"\"\"\" learning a perceptron with perceptron learning rule \"\"\"\nprint(\"Running Experiment using Perceptron Learning Rule for Thresholded Unit\")\n\nprint(\"Training\")\nall_weights = train(train_images, train_labels)\nprint(\"Weights Learned!\")","a4c2585e":"train_acc = predict(all_weights, train_images, train_labels, training_size)\nprint('Training Accuracy:', train_acc[0], '%\\n')\ntrain_best = max(train_acc[1], key=train_acc[1].get)\ntrain_worst = min(train_acc[1], key=train_acc[1].get)\nprint('Best Digit in Train', train_best,', with Accuracy:', train_acc[1][train_best], '%')\nprint('Worst Digit in Train', train_worst,', with Accuracy:', train_acc[1][train_worst], '%')","e1c37550":"test_acc = predict(all_weights, test_images, test_labels, testing_size)\nprint('Test Accuracy:', test_acc[0], '%\\n')\ntest_best = max(test_acc[1], key=test_acc[1].get)\ntest_worst = min(test_acc[1], key=test_acc[1].get)\nprint('Best Digit in Train', test_best,', with Accuracy:', test_acc[1][test_best], '%')\nprint('Worst Digit in Train', test_worst,', with Accuracy:', test_acc[1][test_worst], '%')\n","24282a89":"# Feed-Forward Neural Nets\n\n## Perceptrons\n\nStandard introduction: use of neural networks (NN) in computer vision:\n\n\u2022 this area has been revolutionized by NNs\n\n\u2022 its basic starting point, light intensity, is represented naturally by real numbers,\nwhich are what NNs manipulate\n\n#### Consider the problem of identifying hand-written digits (zero to nine):\n\u2022 build camera\n\n\u2022 light sensors\n\n\u2022 discretize image\n\nMnist gives us the discretized images, 28*28 array of integers.\n\n\u2022 pixel values \u2208 [0 .. 255] represent shades of gray\n\nSimple-minded approach: heuristics (rules that often work, but may not always).\n\nMachine learning approach: how can the computer learn from examples.\n\n\u2022 fully supervised learning problem (there is also semi-supervised and unsupervised).\nIn essence: classification problem: given a set of inputs (features), identify (classify)\nthe entity which gives rise to those inputs (or has those features) as one of a finite\nnumber of alternatives.\n\n    xs = [x\u2081, x\u2082, ..., x] 7\u2192 a \u2208 {0, ..., 9}\n\nIn general, x \u2208 R (in the pixel case here x \u2208 Z).\n","a56d8fbc":"# Author\n\nA Pakistani Machine Learning Engineer","0147c13e":"\n# Notebook\n\nImplement a Multiclass Perceptron for the digit recognition task. \n\n### Document listing the main results:\n\u2022 accuracy achieved on the training set\n\n\u2022 accuracy achieved on the test set\n\n\u2022 which digit is best\/worst recognized\n","a5f662bf":"\n# Multiclass decision problems\nA multiclass perceptron\n\nEach perceptron is trained to recognize one class. E.g., for the MNIST task we need ten perceptrons, one for each digit.\n\nIdeally, only one perceptron will return 1 for a given input. This presupposes nonoverlapping classes which are linearly separable, not a very realistic assumption.\n\n","75ad5ec4":"# Perceptrons\n\nSimpler problem: binary classification problem: is an image zero or not zero?\n\nPerceptrons: simple computational models of neurons\n\nA perceptron consists of a vector of weights ws = [w\u2081, ..., w] and a distinguished weight\nb called bias.\nIn general, let \u03a6 be a list of parameters, with \u03d5i \u2208 \u03a6 the $i$-th parameter. For the\nperceptron \u03a6 = ws \u222a{b}\nThe perceptron computes the function\n\n    f\u03a6(xs) = {1, if b + \u03a3i(xi \u00d7 wi) > 0\n                0, otherwise\n\nDot product: \u03a3i(xi \u00d7 wi) = xs \u00b7 ws\n    \n    f\u03a6(xs) = {1, if b + xs \u00b7 ws > 0\n                0, otherwise\n\n\nElements that compute xs \u00b7 ws are called linear units.\n\u201dTrick\u201d: add a constant input (or feature) 1; thus the bias can be treated as just another\nweight.\n\n\n## Perceptron algorithm\n\n\u2013 Machine learning can also be characterized as a function approximation\nproblem. From this point of view, a single unit perceptron defines a parametrized\nclass of functions. Learning the weights means picking out the member of\nthe class that best approximates the solution function.\n\n\u2022 Sets of problem examples: ideally we have at least two and preferably three\nsets:\n\n1. the training set: used to adjust the parameters of the model;\n\n2. the development set (also called held-out set, or validation set): used to test the model as we try to improve it;\n\n3. the test set: used to evaluate the model\n\nMnist: 60000 training data, 10000 development\/test.\n\nProperty of the perceptron algorithm: if there exists a \u03a6 that enables the perceptron to correctly find the data, the algorithm is guaranteed to find it.\\\n\nFor most real-world problems there is not such \u03a6. In these cases, the perceptron canat most classify a high percentage of examples correctly.\n\nThe algorithm iterates over the training set several times (a complete pass is called an epoch), adjusting the parameters to increase the number of correct answers. If we go\nthrough the training set without any parameters needing to change, we know we have the correct parameters and stop. However, if there is no correct parameter set, then we cycle forever. To prevent this, we cut off training after N iterations.\n"}}