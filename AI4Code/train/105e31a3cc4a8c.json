{"cell_type":{"ac54c878":"code","21fab129":"code","d539ded7":"code","419bcb6f":"code","2dc40005":"code","3205fe10":"code","47a96091":"code","8a682c62":"code","25c6468d":"code","e8f39a83":"code","d9d2803f":"code","08a7c43d":"code","ecadeb76":"code","217e0696":"code","9ddeb009":"code","8db910cd":"code","3caf5cdb":"code","d67e1fa6":"code","d2b112c7":"code","9d1043db":"code","822e03e0":"code","ead85f8c":"code","c6da15dd":"code","056d4699":"code","7d88ecf5":"code","c5a304db":"code","f97b2c7a":"code","3a678aa9":"code","3dc196a0":"code","42e0c753":"code","0e3eb410":"code","8ba4c319":"code","e0ca3370":"code","6422fd2a":"code","c9971328":"code","5c94b45c":"code","850f9366":"code","5ca7671c":"code","79d32d6f":"code","f1f3dc60":"code","35ec1156":"code","ea9e8a7b":"markdown","17330be7":"markdown","9e3e8809":"markdown","007ce37e":"markdown","a92a434b":"markdown","eb216dbe":"markdown","31abd1cc":"markdown","634ea4c6":"markdown","cc6c5a6e":"markdown","b76191a6":"markdown","4e30c78d":"markdown","b34ce249":"markdown","9abf97a5":"markdown","c9eecac8":"markdown","b698eefe":"markdown","9c07383a":"markdown","4c351cc4":"markdown","b9a06c9d":"markdown","42b49353":"markdown","087171bc":"markdown","4c05885a":"markdown","07fb689b":"markdown","8abe5308":"markdown","5301c9a4":"markdown","8b84f00e":"markdown","7f54018e":"markdown","bca5b72b":"markdown","d18981df":"markdown","0b2ef4b0":"markdown"},"source":{"ac54c878":"# Importing essential tools\n# Regular EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocessor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score,cross_validate,KFold,ShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score, recall_score, precision_score, plot_roc_curve,roc_curve,roc_auc_score\n# Feature selection\nfrom sklearn.feature_selection import SelectFromModel\n#Pipeline\nfrom sklearn.pipeline import make_pipeline,Pipeline\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n","21fab129":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\",sep=\",\")","d539ded7":"df.head()","419bcb6f":"df.shape #(rows,columns)","2dc40005":"df.head()","3205fe10":"df.target.value_counts() # balanced dataset","47a96091":"df.target.value_counts().plot.bar();","8a682c62":"df.info() # No-Null values and all numerical features","25c6468d":"df.isna().sum() # another way of checking missing values","e8f39a83":"df.describe()","d9d2803f":"df.sex.value_counts() # 1=male , 0-female","08a7c43d":"# comparing target `column` with `sex` column\npd.crosstab(df.sex,df.target)","ecadeb76":"fig, ax = plt.subplots(figsize=(10, 6))\n\npd.crosstab(df.sex, df.target).plot(kind=\"bar\",\n                                    color=[\"salmon\", 'lightblue'],\n                                    figsize=(10, 6),\n                                    ax=ax);\n\nax.set(xlabel=\"Sex (Female-0 , Male-1)\",\n       ylabel=\"Heart Disease Frequeny\",\n       title=\"Heart disease frequency for sex\");\n\nplt.xticks(rotation=0);\n\nax.legend(['Negative','Positive'],title =\"Target\");","217e0696":"fig, ax = plt.subplots(figsize=(10, 6))\n\nscatter = ax.scatter(x=df.age,\n           y= df.thalach,\n           c=df.target,\n               cmap='winter');\n\nax.set(xlabel=\"Age\",ylabel=\"Max Heaer Rate Achieved\",title=\"Heart Disease in function of Age and Max_Heart_Rate \")\nax.legend(*scatter.legend_elements(),title=\"Target\");\nplt.xticks(rotation=0);","9ddeb009":"df.age.hist(bins= 15); # Helps in checking the ouliers","8db910cd":"pd.crosstab(df.ca,df.target,)","3caf5cdb":"fig, ax = plt.subplots(figsize=(10, 6))\n\npd.crosstab(df.cp,df.target,).plot.bar(color=[\"salmon\",\"lightblue\"],ax=ax)\n\nax.set(xlabel=\"Chest Pain type\",\n       ylabel=\"Heart Disease Frequeny\",\n       title=\"Heart Disease frequency per chest pain type\");\n\nplt.xticks(rotation=0);\n\nax.legend(['Negative','Positive'],title =\"Heart Disease\");\n ","d67e1fa6":"corr_matrix = df.corr()","d2b112c7":"corr_matrix","9d1043db":"fig,ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")","822e03e0":"X,y = df.drop(\"target\",axis=1),df.target\nnp.random.seed(24)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","ead85f8c":"models = {\n    \"Random Forest Classifier    \" : RandomForestClassifier(),\n    \"Logistic Regression\" : LogisticRegression(),\n    \"Knn\" : KNeighborsClassifier()\n}\nscore_dict = {}\nfor i in models:\n    models[i].fit(X_train,y_train)\n    score_dict[i] = cross_val_score(models[i],X,y).mean();\nscore_dict","c6da15dd":"score_df = pd.DataFrame(score_dict,index=['cross_val_score']).T\nscore_df.plot(kind='bar',figsize=(10,6))\nplt.xticks(rotation=0);","056d4699":"tuned_score = {} ","7d88ecf5":"\ndef evaluate(model,a,b):\n    metrics=['accuracy','f1','recall','precision','roc_auc']\n    \n    # cross-validatins the model to calculate scores\n    cv = ShuffleSplit(n_splits=5, test_size=0.2,random_state=99)\n    score = cross_validate(model,a,b,scoring=metrics,cv= cv)\n    \n    # rounding off the scores to 2 decimal places\n    for i in score:\n        score[i] = round(np.array(score[i]).mean(),ndigits=2)\n    \n    #plotting the cross-validates scores\n    score_df = pd.DataFrame(score,index=[\"score\"]).iloc[:,2:].T\n    score_df.plot.bar(figsize=(6,5),color=['salmon'],width=0.2)\n    plt.xticks(rotation=0) \n    \n    return (score) # returning the dictionery of crossvalidation scores for all metrics","c5a304db":"# splitting data into features and target\nX,y = df.drop(\"target\",axis=1),df.target\n\n# converting the categorical features into dummies and dropping 1 column out of each of them\ncat_features = ['cp','ca','slope','thal','sex']\nX = pd.get_dummies(X,columns=cat_features)\nX = X.drop(['cp_0','ca_0','slope_0','thal_0','sex_0'],axis=1)\n\n# splitting dataset into train and test set\nnp.random.seed(24)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n\n# scaling the features usng Standard Scaler\nscaler = StandardScaler()\nX_train_encoded_scaled = scaler.fit_transform(X_train);\nX_test_encoded_scaled = scaler.transform(X_test);","f97b2c7a":"# variables for storing train and test score\ntrain_score = np.array([])\ntest_score = np.array([])\n\nneighbors = np.arange(1,21)\nfor i in neighbors :\n    # initializing model variable with diffrent values of n_neighbors parameter\n    knn = KNeighborsClassifier(n_neighbors=i)\n    \n    # fitting training set data\n    knn.fit(X_train_encoded_scaled,y_train)\n    \n    # appending scores in the respective arrays\n    train_score =  np.append(train_score , [knn.score(X_train_encoded_scaled,y_train)])\n    test_score =  np.append(test_score , [knn.score(X_test_encoded_scaled,y_test)])\n    \n# plotting scores \nplt.plot(neighbors,train_score,'b',label=\"train\")\nplt.plot(neighbors,test_score,'g',label=\"test\")\nplt.legend()\nplt.xticks(np.arange(1,21))\nplt.title(f'max_score is {test_score.max()}   at  n_neighbor ={np.argmax(test_score)+1}');","3a678aa9":"# list of parameters and their possible values\nparameters = {\n    \"kneighborsclassifier__n_neighbors\" : np.arange(1,15),\n    \"kneighborsclassifier__leaf_size\" : np.arange(1,5),\n    \"kneighborsclassifier__weights\" : [\"uniform\",\"distance\"]\n}\n\n# initializing model variable and implementing GridSearchCV\nknn = KNeighborsClassifier()\n\n# creating a pipeline with sacler and estimator \nknn_scaling_pipeline = make_pipeline(StandardScaler(),KNeighborsClassifier())\n# implementing GridSearchCV on the pipeline\nknn_cv = GridSearchCV(knn_scaling_pipeline,param_grid=parameters,n_jobs=-1,cv=5)\n\n# fitting the training data to the \nknn_cv.fit(X_train,y_train)\nbest_estimator =  knn_cv.best_estimator_\n\n# creating pipeline with scaler and best_estimator so as to calculate the cross-validation scores in the \"evaluate\" function\nknn_scaling_pipeline = make_pipeline(StandardScaler(), best_estimator)\n\nsc = evaluate(knn_scaling_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['k-NN'] = sc\nsc","3dc196a0":"# Creating features and label set\nX,y = df.drop(\"target\",axis=1),df.target\n\n# splitting dataset into train and test set\nnp.random.seed(24)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","42e0c753":"# list of possible values of the parameters\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2],\n    'min_samples_leaf': [5],\n    'min_samples_split': [ 12],\n    'n_estimators': np.arange(0,300,50)\n}\n# initializing model \nrfc =RandomForestClassifier(random_state=42)\n\n# implimenting GridSeachCV and fitting training set to it\nrfc_cv = GridSearchCV(rfc,n_jobs=-1,param_grid=param_grid,cv=5)\nrfc_cv.fit(X_train,y_train)\nbest_estimator = rfc_cv.best_estimator_\n\n#evaluating model\nsc = evaluate(best_estimator,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['RFC'] = sc\nsc","0e3eb410":"rfc_cv.best_estimator_.feature_importances_","8ba4c319":"# initializing the transformer \nsfm = SelectFromModel(best_estimator,threshold=0.03,prefit=True)\n# since estimator is pre-fitted therfore no need to fit it again with the training set data , just make prefit = True \n \n# transforming the train and test dataset , features with importance value above threshold will be selected\nX_important = sfm.transform(X)\nX_train_important = sfm.transform(X_train)\nX_test_important = sfm.transform(X_test)\n\n# fittting the GridSeachCV model with important features\nrfc_cv.fit(X_train_important,y_train)\n\n# evaluating model\nsc = evaluate(rfc_cv.best_estimator_,X_important,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['RFC_imp_features'] = sc\nsc","e0ca3370":"X,y = df.drop(\"target\",axis=1),df.target\nnp.random.seed(24)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n\nscaler = StandardScaler()\nX_train__scaled = scaler.fit_transform(X_train);\nX_test__scaled = scaler.transform(X_test);","6422fd2a":"# initializing model and evaluating results on the the scaled data\nlr = LogisticRegression()\nlr.fit(X_train__scaled,y_train)\nprint(\"test-score = \",lr.score(X_test__scaled,y_test),\"\\n\")\n\n# pipeline with scaler and estimator \nlr_scaling_pipeline = make_pipeline(StandardScaler(), lr)\nsc = evaluate(lr_scaling_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['LR-scaled'] = sc\nsc","c9971328":"param_grid = {'logisticregression__C': np.logspace(-4,4,30),'logisticregression__solver':['liblinear'],'logisticregression__penalty':['l1','l2']}\n\n\n# implementing GridSearchCV\nlr = LogisticRegression()\n\n# pipeline with scaler and estimator to prevent data leakage while scaling\nlr_pipeline = make_pipeline(StandardScaler(), lr)\nlr_cv = GridSearchCV(lr_pipeline,cv=5,n_jobs=-1,param_grid=param_grid)\n\n# fitting the training data to lr_cv\nlr_cv.fit(X_train,y_train)\n\n# evaluating model using pipleine\nlr_cv_pipeline = make_pipeline(StandardScaler(),lr_cv.best_estimator_)\nsc = evaluate(lr_cv_pipeline,X,y)\n\n# inserting the scores into tuned_scores dict\ntuned_score['LR-Tuned'] =sc\nsc","5c94b45c":"# variable for storing the 2-dimensional array\nscore_array=np.ndarray((5,5))\n\n# converting the scores in the tuned)score dict into 2-dimensional np.array\nfor i,j in enumerate(tuned_score):\n    score_array[i,:] = list(tuned_score[j].values())[2:]\n\n# initializing scaler object\nscaler = MinMaxScaler(feature_range=(0,1))\n\n# MinMax Scaled cross-validation scores\nscaled_score_array = np.transpose(scaler.fit_transform(score_array))\n\n# cross-validation scores ( columns - models)(index - metrics)\nscore_array = np.transpose(score_array)\nprint(score_array)","850f9366":"# Dataframe with cross-validation scores\nscore_df = pd.DataFrame(score_array,columns=list(tuned_score.keys()),index= ['accuracy','f1','recall','precision','roc_auc'])\n\n# Dataframe with MinMax scaled cross-validation scores\nscaled_score_df = pd.DataFrame(scaled_score_array,columns=list(tuned_score.keys()),index= ['accuracy','f1','recall','precision','roc_auc'])","5ca7671c":"score_df","79d32d6f":"scaled_score_df","f1f3dc60":"# Plotting scores\nscore_df.plot.bar(figsize=(15,4));\nplt.yticks(np.linspace(0,1,11,endpoint=True));\nplt.xticks(rotation=0,fontsize=12)\nplt.xlabel(\"Metrics\",fontsize=15)\nplt.ylabel(\"CV-Score\",fontsize=15)\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n          ncol=3, fancybox=True, shadow=True);","35ec1156":"scaled_score_df.plot.bar(figsize=(15,4));\nplt.yticks(np.linspace(0,1,11,endpoint=True));\nplt.xticks(rotation=0,fontsize=12)\nplt.xlabel(\"Metrics\",fontsize=15)\nplt.ylabel(\"Relative CV-Score\",fontsize=15)\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n          ncol=3, fancybox=True, shadow=True);","ea9e8a7b":"# Conclusion\nknn and RFC are the best peforming models with comaparable scores","17330be7":"# Plotting the final report","9e3e8809":"# Modeling","007ce37e":"## Heart disease frequency per chest pain type ","a92a434b":"# Features \n* age = age in years\n* sex = (1 = male; 0 = female)\n* cp = chest pain type\n* trestbps =  = resting blood pressure (in mm Hg on admission to the hospital)\n* chol = serum cholestoral in mg\/dl\n* fbs = (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg =  resting electrocardiographic results\n* thalach = maximum heart rate achieved\n* exang = exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* slope = the slope of the peak exercise ST segment\n* ca = number of major vessels (0-3) colored by flourosopy\n* thal = 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target = 1 or 0","eb216dbe":"## RandomForestClassifier (RFC)","31abd1cc":"## Relative Cross-validation scores\nMinMax scaled scores - easy to comapre performance","634ea4c6":" `tuned_score` to store the cross-validation scores of various models","cc6c5a6e":"**LogisticRegression has best baseline score**","b76191a6":"# Testing various models","4e30c78d":"### k-NN - Tuning Hyperparameters using GridSearchCV","b34ce249":"# Load Data","9abf97a5":"## Prepare data for machine learning model","c9eecac8":"# Predictiong heart disease using various ML models\nThis notebook looks into using various Python-based machine learning machine learning and adata science libraries in an attempt to build a machine learning model capable of predictiong whether or notsomeone has haert disease based on their medical attributes.\n\n<Font color=\"salmon\">I hope you find this kernel helpful and some UPVOTES would be very much appreciated<\/Font>","b698eefe":"### LR - With scaled features","9c07383a":"# Tuning and Evaluating model","4c351cc4":"## Age vs Max Heart rate for Heart Disease","b9a06c9d":"## Correlation Matrix","42b49353":"### RFC - Tuning hyperparameters of RandomForestClassifier using GridSearchCV","087171bc":"# Data Exploration (Exploring data analysis or EDA)","4c05885a":"### RFC - Selecting important features","07fb689b":"Testing various models to check which model works best","8abe5308":"### k-NN - Tuning Hyperparameters by hand","5301c9a4":"## Cross validation Scores","8b84f00e":"## Function to comapre models based upon their cross-validation-scores","7f54018e":"## Heart Disease frequency according to sex","bca5b72b":"## k-Nearest Neighbors","d18981df":"## Logistic Regression (LR)","0b2ef4b0":"### LR - Tuning hyperparametrs using GridSearchCV"}}