{"cell_type":{"e6eecf99":"code","806ee1a9":"code","28779ab0":"code","b9e710fb":"code","abbc5330":"code","ad5d6477":"code","f56f55f7":"code","77bdac1b":"code","ad6afc3a":"code","32559e24":"code","1c820ab8":"code","095eb763":"code","6fbb8e7b":"code","79a47202":"code","59273e1f":"code","96526073":"code","adbe0fac":"code","25ddeaef":"code","0c651117":"code","ed0139f6":"code","a7c970c6":"code","7b8309c8":"code","ad138049":"code","757ffd08":"code","5fcfa2e9":"code","7140c072":"code","8685accc":"code","3764cb6e":"code","a353a76b":"code","a01137c8":"code","0a0e1959":"code","22bf8268":"code","978d4b94":"code","1aa7ec03":"code","da40de79":"code","83c37c36":"code","0e3a91ad":"code","5d78c185":"code","aa8dc5c3":"code","1d30747c":"markdown","afe6afba":"markdown","8bdba7aa":"markdown","6e104c61":"markdown","61d04faf":"markdown","ece285b7":"markdown","30106331":"markdown","b1098219":"markdown","fa6dbf10":"markdown","1152b478":"markdown","be948d6d":"markdown","3203294d":"markdown","487eb733":"markdown","c2d33035":"markdown","18b849b5":"markdown","9194ca07":"markdown","6ffdbf84":"markdown","7aba39a0":"markdown","27bc74d5":"markdown","21b2c024":"markdown"},"source":{"e6eecf99":"import pandas as pd\nimport sklearn\nimport numpy as np\nimport os\nimport sys\n\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","806ee1a9":"## Build 3D dataset:\n\nnp.random.seed(4)\nm = 60\nw1, w2 = 0.1, 0.3\nnoise = 0.1\n\nangles = np.random.rand(m) * 3 * np.pi \/ 2 - 0.5\nX = np.empty((m, 3))\nX[:, 0] = np.cos(angles) + np.sin(angles)\/2 + noise * np.random.randn(m) \/ 2\nX[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) \/ 2\nX[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)","28779ab0":"\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered) #s NumPy\u2019s svd() function to obtain all the principalcomponents of the training set\n                                     #, then extracts the first two PCs\nc1 = Vt.T[:, 0]\nc2 = Vt.T[:, 1]","b9e710fb":"m, n = X.shape\n\nS = np.zeros(X_centered.shape)\nS[:n, :n] = np.diag(s)","abbc5330":"np.allclose(X_centered, U.dot(S).dot(Vt))","ad5d6477":"# projects the training set onto the plane defined by the first two principal components\nW2 = Vt.T[:, :2]\nX2D = X_centered.dot(W2)","f56f55f7":"X2D_using_svd = X2D","77bdac1b":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2) # reduces the dimensionality to 2 dimension\nX2D = pca.fit_transform(X)","ad6afc3a":"X2D[:5]","32559e24":"X2D_using_svd[:5]","1c820ab8":"# Both method give same solution except scikit - learn gives fliped axis.\nnp.allclose(X2D, -X2D_using_svd) ","095eb763":"pca.components_.T[:,0]","6fbb8e7b":"X3D_inv = pca.inverse_transform(X2D)","79a47202":"np.allclose(X3D_inv, X)","59273e1f":"np.mean(np.sum(np.square(X3D_inv - X), axis=1))","96526073":"##  Principal components\npca.components_","adbe0fac":"pca.explained_variance_ratio_","25ddeaef":"from sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.uint8)","0c651117":"from sklearn.model_selection import train_test_split\n\nX = mnist[\"data\"]\ny = mnist[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","ed0139f6":"X_train.shape# consists of 784 features","a7c970c6":"pca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_) # cumulative sum along the given axis\nd = np.argmax(cumsum >= 0.95) + 1","7b8309c8":"d ","ad138049":"pca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)","757ffd08":"pca.n_components_ # also gives the same number of bimension as before","5fcfa2e9":"# Plot\nplt.figure(figsize=(6,4))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 400, 0, 1])\nplt.xlabel(\"Dimensions\")\nplt.ylabel(\"Explained Variance\")\nplt.plot([d, d], [0, 0.95], \"k:\")\nplt.plot([0, d], [0.95, 0.95], \"k:\")\nplt.plot(d, 0.95, \"ko\")\nplt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\nplt.grid(True)\n# save_fig(\"explained_variance_plot\")\nplt.show()","7140c072":"pca = PCA(n_components = 154)\nX_reduced = pca.fit_transform(X_train)\nX_recovered = pca.inverse_transform(X_reduced)","8685accc":"rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state = 42)\nX_reduced = rnd_pca.fit_transform(X_train)","3764cb6e":"from sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_train, n_batches): #array_split() splits an array into batches of size n_batches\n     inc_pca.partial_fit(X_batch)                  # use partial_fit() not fit() due to mini-batches\nX_reduced = inc_pca.transform(X_train)","a353a76b":"import time\n\nfor n_components in (2, 10, 154):\n    print(\"n_components =\", n_components)\n    regular_pca = PCA(n_components=n_components)\n    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)\n    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver=\"randomized\")\n\n    for pca in (regular_pca, inc_pca, rnd_pca):\n        t1 = time.time()\n        pca.fit(X_train)\n        t2 = time.time()\n        print(\"    {}: {:.1f} seconds\".format(pca.__class__.__name__, t2 - t1))","a01137c8":"times_rpca = []\ntimes_pca = []\nsizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]\nfor n_samples in sizes:\n    X = np.random.randn(n_samples, 5)\n    pca = PCA(n_components = 2, svd_solver=\"randomized\", random_state=42)\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_rpca.append(t2 - t1)\n    pca = PCA(n_components = 2)\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_pca.append(t2 - t1)","0a0e1959":"# Plot\nplt.plot(sizes, times_rpca, \"b-o\", label=\"RPCA\")\nplt.plot(sizes, times_pca, \"r-s\", label=\"PCA\")\nplt.xlabel(\"n_samples\")\nplt.ylabel(\"Training time\")\nplt.legend(loc=\"upper left\")\nplt.title(\"PCA and Randomized PCA time complexity \")","22bf8268":"times_rpca = []\ntimes_pca = []\nsizes = [1000, 2000, 3000, 4000, 5000, 6000]\nfor n_features in sizes:\n    X = np.random.randn(2000, n_features)\n    pca = PCA(n_components = 2, random_state=42, svd_solver=\"randomized\")\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_rpca.append(t2 - t1)\n    pca = PCA(n_components = 2)\n    t1 = time.time()\n    pca.fit(X)\n    t2 = time.time()\n    times_pca.append(t2 - t1)","978d4b94":"# Plot\nplt.plot(sizes, times_rpca, \"b-o\", label=\"RPCA\")\nplt.plot(sizes, times_pca, \"r-s\", label=\"PCA\")\nplt.xlabel(\"n_features\")\nplt.ylabel(\"Training time\")\nplt.legend(loc=\"upper left\")\nplt.title(\"PCA and Randomized PCA time complexity \")","1aa7ec03":"from sklearn.datasets import make_swiss_roll","da40de79":"X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)","83c37c36":"from sklearn.decomposition import KernelPCA\n\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04) # Using RBF kernels\nX_reduced = rbf_pca.fit_transform(X)","0e3a91ad":"#Plot\n\nplt.figure(figsize=(11, 4))\nsubplot, pca, title  = 131, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"\nX_reduced = pca.fit_transform(X)\nif subplot == 132:\n    X_reduced_rbf = X_reduced\n\nplt.subplot(subplot)\n#plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"gs\")\n#plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"y^\")\nplt.title(title, fontsize=14)\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.grid(True)","5d78c185":"# This code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, \n# then applying Logistic Regression for classification. Then it uses Grid SearchCV to find the best \n#kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nclf = Pipeline([\n (\"kpca\", KernelPCA(n_components=2)),\n (\"log_reg\", LogisticRegression())\n ])\nparam_grid = [{\n \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n }]\n\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\ngrid_search.fit(X, y)","aa8dc5c3":"# Best params are\nprint(grid_search.best_params_)","1d30747c":"#### Selecting a Kernel and Tuning Hyperparameters","afe6afba":"### Time complexity (Depends on computational capability)\n#### Number of principal components\n- **Regular PCA vs Incremental PCA vs Randomized PCA**","8bdba7aa":"#### Incremental PCA\n- Split the training set into mini-batches and feed an IPCA algorithm **one mini-batch at a time**.\n- Usefull to apply **PCA Online or On the FLy**.","6e104c61":"### PCA\n- First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n\n#### Preserving the Variance \n- The line that is selected along the data is called as a Principal Component.\n- The datapoints are projected along the lines.\n- The best selected line is the one which gives the least variance amongst all.\n- It will most likely lose less information than the other projections.\n\n#### Principal Components\n- PCA identifies the axis that accounts for the largest amount of variance in the training set.\n- It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance.\n- If it were a higher-dimensional dataset, PCA would also find as many axes as the number of dimensions in the dataset.\n\n#### PCA using SVD(Singular Value decomposition)","61d04faf":"#### Number of instances (rows)\n- **PCA VS Randomized PCA **","ece285b7":"#### PCA using Scikit-Learn","30106331":"Another way is to plot the explained variance as a function of the number of\ndimensions **The Elbow Method**","b1098219":"Another way is to set n_components = 0.95, indicating the ratio of variance you wish to preserve.","fa6dbf10":"Computes PCA without reducing dimensionality, then computes\nthe minimum number of dimensions required to preserve 95% of the training set\u2019s\nvariance","1152b478":"#### 2,000 instances with various numbers of features (columns)\n- **PCA VS Randomized PCA **","be948d6d":"We can compute the **reconstruction error**:\nThe mean squared distance between the original data and the reconstructed data\n(compressed and then decompressed) is called the **reconstruction error.**","3203294d":"### Main Approaches for Dimensionality Reduction\n#### Projection\n- All the points in the space are projected on a selected plane. \n- But, some condition where our dataset may be twist and turn, such as in the famous Swiss roll toy dataset.","487eb733":"### Kernel PCA\n- **Kernel Tricks** - Kernel functions help to map low dimensional function to high dimension. Kernel trick allows us to operate in the **original feature space without computing the coordinates** of the data in a higher dimensional space.","c2d33035":"## Dimensionality Reduction","18b849b5":"#### PCA for Compression\n- As there is a loss of 20% of the original size the reduced images would look slightly compressed.\n- We can use inverse_transform() to recover the images but it **won't** be as original, it will have about 5% loss. ","9194ca07":"Recover the **3D points projected on the plane**","6ffdbf84":"#### Explained Variance Ratio\n- It indicates the proportion of the dataset\u2019s variance that lies along the axis of each principal component.\n\nThis tells you that 84.2% of the dataset\u2019s variance lies along the first axis, and 14.6%\nlies along the second axis. This leaves less than 1.2% for the **third axis**, so it is reasonable to assume that it probably **carries little information.**","7aba39a0":"#### Randomized PCA\n- When svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a sto\u2010chastic algorithm called **Randomized PCA** that quickly finds an approximation of the first d principal components.\n- Scikit-Learn automatically uses the randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m or n","27bc74d5":"#### Choosing the Right Number of Dimensions","21b2c024":"Of course, there was some loss of information during the projection step, so the recovered 3D points are not exactly equal to the original 3D points:"}}