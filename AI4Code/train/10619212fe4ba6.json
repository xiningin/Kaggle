{"cell_type":{"04c36dc6":"code","aab9bf84":"code","45451819":"code","c96ec33b":"code","70cbd787":"code","b331cc2f":"code","acfeb7a1":"code","ec0b6b32":"code","58091cd4":"code","d0496564":"code","47c83f78":"code","332ce387":"code","81c495e9":"code","891ca61e":"markdown","615e2875":"markdown","374714a7":"markdown","7ef52a28":"markdown","3e4a8a7f":"markdown","b31b5127":"markdown","15b4ead6":"markdown","7e97a954":"markdown","b3cafc12":"markdown","f4e23619":"markdown","9723bddb":"markdown","2c1d54a1":"markdown","e577660a":"markdown","9ddaa58b":"markdown"},"source":{"04c36dc6":"import math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\n%matplotlib inline","aab9bf84":"train = pd.read_csv(\"..\/input\/train.csv\")\nx_test = pd.read_csv(\"..\/input\/test.csv\")\n\nx_train = train.drop(labels=[\"label\"], axis=1) \ny_train = to_categorical(train[\"label\"], num_classes = 10)\ndel train\n\nx_train = x_train \/ 255.0\nx_test = x_test \/ 255.0\n\nx_train = x_train.values.reshape(-1,28,28,1)\nx_test = x_test.values.reshape(-1,28,28,1)\n\nprint('Train Shape:', x_train.shape)\nprint('Test Shape:', x_test.shape)","45451819":"def sample_img_from_each_class():\n    img_indices = [None]*10\n    img_found = 0\n    while img_found < 10:\n        img_idx = np.random.randint(0, len(y_train))\n        img_class = np.argmax(y_train[img_idx])\n        if img_indices[img_class] is None:\n            img_indices[img_class] = img_idx\n            img_found += 1\n    return img_indices\n\n\nsample_image_indices = sample_img_from_each_class()\n\n\ndef display_sample_imgs(images):\n    num_imgs = len(sample_image_indices)\n    fig = plt.figure(figsize=(2*num_imgs, 2),facecolor='w', edgecolor='k')\n    for i,img_idx in enumerate(sample_image_indices):\n        img = images[img_idx]\n        fig.add_subplot(1,num_imgs,i+1)\n        plt.imshow(img[:,:,0], cmap='Greys',  interpolation='nearest')\n        plt.axis('off')\n\n        \ndisplay_sample_imgs(x_train)","c96ec33b":"from skimage.morphology import skeletonize_3d\n\nx_train_skeleton = np.array([skeletonize_3d(img) for img in x_train])\nx_test_skeleton = np.array([skeletonize_3d(img) for img in x_test])\n\ndisplay_sample_imgs(x_train_skeleton)","70cbd787":"def is_visited(coord,visited_mat):\n    return visited_mat[coord[0],coord[1]] > 0\n\n\ndef angle_between(c1,c2):\n    return math.atan2(c1[0]-c2[0], c1[1]-c2[1])\n\n\ndef angle_diff(a1,a2):\n    return np.pi - abs(abs(a1 - a2) - np.pi)\n\n\ndef same_corner(c1,c2):\n    return (c1[0] == c2[0] and abs(c1[1] - c2[1]) == 1) or (c1[1] == c2[1] and abs(c1[0] - c2[0]) == 1)\n\n\ndef coord_in_bounds(coord, bounds): # assuming bounds is shape (starting at 0)\n    return coord[0] < bounds[0] and coord[1] < bounds[1] and coord[0] >= 0 and coord[1] >= 0\n\n\ndef line_present_at(coord,mat,thresh=1):\n    return coord_in_bounds(coord,mat.shape) and mat[coord[0],coord[1]] >= thresh\n\n\ndef corners_from(coord):\n    return [(coord[0]-1,coord[1]-1),(coord[0]+1,coord[1]-1),(coord[0]-1,coord[1]+1),(coord[0]+1,coord[1]+1)] \n\n\ndef next_door_from(coord):\n    return [(coord[0]-1,coord[1]),(coord[0]+1,coord[1]),(coord[0],coord[1]-1),(coord[0],coord[1]+1)] \n\n\ndef all_neighbors_from(coord):\n    return next_door_from(coord) + corners_from(coord)\n\n\ndef all_present_neighbors_from(coord,mat):\n    present_neighbors = []\n    for nd in all_neighbors_from(coord):\n        if line_present_at(nd,mat):\n            present_neighbors.append(nd)\n    return present_neighbors\n\n\ndef angle_avg(last,new):\n    weight_last = 1.0\n    weight_new = 1.0\n    return math.atan2((math.sin(last)*weight_last + math.sin(new)*weight_new) , (math.cos(last)*weight_last + math.cos(new)*weight_new))\n\n\ndef stroke_starting_points(stroke_pts):\n    starting_coords = []\n    first_coord = None\n    last_coord = None\n    for x in range(stroke_pts.shape[1]):\n        for y in range(stroke_pts.shape[0]):\n            if stroke_pts[y,x] <= 0: \n                continue\n            if first_coord is None:\n                first_coord = (y,x)\n            last_coord = (y,x)\n            nb_coords = all_present_neighbors_from(last_coord,stroke_pts)\n\n            if len(nb_coords) == 1:\n                starting_coords.append((y,x))\n            elif len(nb_coords) == 2 and same_corner(nb_coords[0],nb_coords[1]):\n                starting_coords.append((y,x))\n\n    # when no good starting points are found, try bottom-left and top-right pixels\n    if len(starting_coords) == 0:\n        starting_coords = [first_coord, last_coord]\n\n    return starting_coords\n\n\ndef split_stroke_at(stroke,split_coord):\n    res = [[],[]]\n    index = 0\n    for c in stroke:\n        if c == split_coord:\n            index += 1\n            continue\n        res[index].append(c)\n    return res\n\n\ndef best_fit_angle(Y, X):\n    if len(set(Y)) == 1: # horizontal line\n        slope = 0 \n    elif len(set(X)) == 1: # vertial line\n        slope = 10e12 \n    else:\n        xbar = sum(X)\/len(X)\n        ybar = sum(Y)\/len(Y)\n        n = len(X) \n\n        numer = sum([xi*yi for xi,yi in zip(X, Y)]) - n * xbar * ybar\n        denum = sum([xi**2 for xi in X]) - n * xbar**2\n\n        slope = numer \/ (denum + (0.000000001))\n\n    return angle_between((1.0,slope),(0,0))\n\n\nPRE_POST = 1\nPRE_STROKE = 2\nPOST_STROKE = 3\ndef most_similar(pre,post,stroke):\n    if min(len(pre),len(post),len(stroke)) <= 3:\n        return PRE_POST \n    depth = -7\n    pre_angle = best_fit_angle(*list(zip(*pre[depth:])))\n    post_angle = best_fit_angle(*list(zip(*post[depth:])))\n    stroke_angle = best_fit_angle(*list(zip(*stroke[depth:])))\n    pre_post_diff = angle_diff(pre_angle,post_angle)\n    pre_stroke_diff = angle_diff(pre_angle,stroke_angle)\n    post_stroke_diff = angle_diff(post_angle, stroke_angle)\n    min_diff = min(pre_post_diff, pre_stroke_diff, post_stroke_diff)\n    if min_diff == pre_post_diff:\n        return PRE_POST\n    if min_diff == pre_stroke_diff:\n        return PRE_STROKE\n    return POST_STROKE\n\n\ndef get_strokes(stroke_pts):\n    res_img = stroke_pts\n    remaining_px = res_img.copy()\n    visited = np.zeros(res_img.shape)\n    strokes = []\n    last_start_coords = []\n    while len(remaining_px[np.where(remaining_px > 0)]) > 2:\n        start_coords = stroke_starting_points(remaining_px)\n        if last_start_coords == start_coords:\n            break\n        last_start_coords = start_coords\n        for i,sc in enumerate(start_coords):\n            if visited[sc] > 0:\n                continue # aleady been here!\n            sc_strokes = []\n            stroke_id = len(strokes) + 1\n            for sc_neighbor in all_present_neighbors_from(sc,res_img):\n                cur = sc\n                angle = angle_between(cur,sc_neighbor)\n                stroke = []\n                while True:\n                    best_angle_diff = np.pi \/ 2\n                    visited[cur] = stroke_id\n                    remaining_px[cur] = 0\n                    stroke.append(cur)\n                    nxt_coord = None\n                    nxt_angle = None\n                    for nb in all_present_neighbors_from(cur,res_img):\n                        test_angle = angle_between(cur,nb)\n                        test_angle_diff = angle_diff(angle,test_angle)\n                        if test_angle_diff <= best_angle_diff:\n                            best_angle_diff = test_angle_diff\n                            nxt_coord = nb\n                            nxt_angle = test_angle\n                    if nxt_coord is None: \n                        break\n                    if visited[nxt_coord] > 0:\n                        stroke_overlapping = int(visited[nxt_coord]) # keep track of which stroke has the contested coord\n                        # CONFLICT RESOLUTION:\n                        if stroke_id != stroke_overlapping:\n                            stroke_index = stroke_overlapping-1\n                            pre,post = split_stroke_at(strokes[stroke_index],nxt_coord)\n                            post = post[::-1]\n                            combine = most_similar(pre,post,stroke)\n                            if combine == POST_STROKE:\n                                strokes[stroke_index] = pre \n                                stroke = stroke + [nxt_coord] + post[::-1]\n                            elif combine == PRE_STROKE: \n                                strokes[stroke_index] = post \n                                stroke = stroke + [nxt_coord] + pre[::-1]\n                            # do nothing for PRE_POST\n                        break\n                    cur = nxt_coord\n                    angle = angle_avg(angle,nxt_angle)\n                         \n                if len(stroke) > 2:\n                    sc_strokes.append(stroke)\n                else:\n                    for coord in stroke:\n                        visited[coord] = 0\n\n            if len(sc_strokes) == 1:\n                strokes.append(sc_strokes[0])\n            elif len(sc_strokes) == 2:\n                combined_stroke = sc_strokes[1][::-1][:-1] + sc_strokes[0]\n                strokes.append(combined_stroke)\n            elif len(sc_strokes) > 2:\n                print('UNEXPECTED NUMBER OF STARTING POINTS FROM SINGLE STARTING POINT')\n                exit()\n\n    return strokes\n\n\ndef log_coords(coords, base_img):\n    num_imgs = 1 + len(coords)\n    fig = plt.figure(figsize=(2*num_imgs, 2),facecolor='w', edgecolor='k')\n    \n    o = fig.add_subplot(1,num_imgs,1)\n    o.set_title('Original')\n    plt.imshow(base_img[:,:,0], cmap='Greys',  interpolation='nearest')\n    plt.xticks([])\n    plt.yticks([])\n\n    for i,st in enumerate(coords):\n        coord_img = base_img * 0\n        for s_coord in st:\n            coord_img[s_coord[0],s_coord[1]] = 255\n        s = fig.add_subplot(1,num_imgs,i+2)\n        s.set_title('Stroke '+str(i+1))\n        plt.imshow(coord_img[:,:,0], cmap='Greys',  interpolation='nearest')\n        plt.xticks([])\n        plt.yticks([])","b331cc2f":"x_train_strokes = [get_strokes(img_skeleton) for img_skeleton in x_train_skeleton]\nx_test_strokes = [get_strokes(img_skeleton) for img_skeleton in x_test_skeleton]","acfeb7a1":"for i,img_idx in enumerate(sample_image_indices):\n    log_coords(x_train_strokes[img_idx],x_train[img_idx])","ec0b6b32":"from rdp import rdp\n\ndef simplify_strokes(strokes,epsilon):\n    simplified = []\n    for stroke in strokes:\n        if len(stroke) > 1:\n            simplified.append(rdp(stroke,epsilon=epsilon))\n    return simplified\n\nx_train_simple_strokes = [simplify_strokes(img_strokes, 0.15) for img_strokes in x_train_strokes]\nx_test_simple_strokes = [simplify_strokes(img_strokes, 0.15) for img_strokes in x_test_strokes]","58091cd4":"for i,img_idx in enumerate(sample_image_indices):\n    log_coords(x_train_simple_strokes[img_idx],x_train[img_idx])","d0496564":"def flat_relative_paths(strokes):\n    res = []\n    pos = (0,0)\n    for stroke in strokes:\n        is_new_stroke = 1\n        for coord in stroke:\n            delta_x = coord[0] - pos[0]\n            delta_y = coord[1] - pos[1]\n            res.append([is_new_stroke, delta_x, delta_y])\n            pos = coord\n            is_new_stroke = 0\n    return np.array(res)\n            \n    \nx_train_flat_rel = [flat_relative_paths(simple_strokes) for simple_strokes in x_train_simple_strokes]\nx_test_flat_rel = [flat_relative_paths(simple_strokes) for simple_strokes in x_test_simple_strokes]\n\nprint('Representation of  our sample \"8\"')\nprint('Absolute Stroke Coords:\\n',x_train_simple_strokes[sample_image_indices[7]])\nprint('\\n','-'*30,'\\n')\nprint('Flat Relative Stroke Coords:\\n',x_train_flat_rel[sample_image_indices[7]])","47c83f78":"from keras.preprocessing import sequence\n\nmax_len = max([len(seq) for seq in x_train_flat_rel])\nx_train_padded = sequence.pad_sequences(x_train_flat_rel, maxlen=max_len)\n\nprint('Longest Sequence Length:', max_len)\nprint('Original Image Representation:', 28*28, 'integers')\nprint('Stroke Path Representation:   ', 3*max_len, 'integers')","332ce387":"from sklearn.cross_validation import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Conv1D, Dropout\nfrom keras.callbacks import LearningRateScheduler\n\n\nmodel = Sequential()\nmodel.add(Conv1D(48, kernel_size=(5), padding='same', input_shape=(max_len,3)))\nmodel.add(Conv1D(64, kernel_size=(4), padding='same'))\nmodel.add(Conv1D(96, kernel_size=(3), padding='same'))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\ndef step_decay(epoch):\n    lrate = 0.001\n    if epoch > 16:\n        lrate = 0.0005\n    if epoch > 32:\n        lrate = 0.0001\n    if epoch > 45:\n        lrate = 0.00005\n    return lrate\n\nlrate = LearningRateScheduler(step_decay)\n\n\nUSE_VALIDATION = False\nEPOCHS = 16 # Reduced for Kaggle, for best results use ~50 epochs\nif USE_VALIDATION:\n    x_train_fin, x_val, y_train_fin, y_val = train_test_split(x_train_padded, y_train, test_size=0.2, random_state=42)\n    model.fit(x_train_fin, y_train_fin, batch_size=256, epochs=EPOCHS, validation_data=(x_val, y_val), callbacks=[lrate])\nelse:\n    model.fit(x_train_padded, y_train, batch_size=256, epochs=EPOCHS, callbacks=[lrate])","81c495e9":"def generate_prediction_file(model,test_data):\n    pred = model.predict(test_data)\n    pred = np.argmax(pred, axis=1)\n    pred_series = pd.Series(pred,name=\"Label\")\n    submission = pd.concat([pd.Series(range(1,28001), name=\"ImageId\"), pred_series], axis=1)\n    submission.to_csv(\"lstm_submission.csv\", index=False)\n\npadded_test_data = sequence.pad_sequences(x_test_flat_rel, maxlen=max_len)\ngenerate_prediction_file(model, padded_test_data)","891ca61e":"## Skeletonization:\n\nNotice in the images above that most of the strokes are >1 pixel wide. We eventually want to follow a path from one pixel to the next and this is going to be difficult with strokes wider than 1px. (Where is the next pixel in the path when all the surrounding pixels are black?) We need to thin the digits out somehow. Fortunately, SciKit's skimage module has a function that does exactly what we want:","615e2875":"Nice. That's a pretty significant reduction in the size of our inputs.\n\n## RNN Model & Training\nWe're *finally* ready to train a model. The setup below is the result of some trial, error and hyperparameter tuning. ","374714a7":"## Path Tracing\nGreat. Next, we want to turn our skeletonized digit images into sequences of coordinates representing the path(s) of the writer's pen. Here's the basic idea:\n\n1. Find black pixels with only 1 black \"neighbor\" pixel and use them as starting points.\n2. From each starting point, record the pixel's location, mark it 'visited' and then move to an unvisited, black, neighbor pixel. \n    * At stroke intersections, where there's >1 unvisited black neighbor pixel, use the previous few pixels to approximate the direction\/angle that the stroke is moving in and choose the neighbor pixel that continues in that direction.\n    * When there are no more neighbors to visit, consider it the end of the stroke.\n3. Repeat step 2 with the next unvisited starting point (if any).\n4. Ignoring all visited pixels, repeat steps 1-3 until you've visited every black pixel in the image.\n\nThere are a some additional things going on in the (hidden) code block below. I hope it isn't too hard to folllow. Let me know in the comments if you have any questions about it.","7ef52a28":"Pretty good. Our skeletonization and path extraction isn't perfect, but hopefully the same sort of mistakes show up in the same classes. \ud83e\udd1e\n\n## Simplifying Strokes with RDP\nWe don't actually need every pixel in the path to know what the stroke looks like. For example, for a simple '1' we might only need the top and bottom pixel. If we know those two pixels are connectd by a line, we can ignore all the pixels in between. For curved digits like '8' we could remove a lot of pixels by making the curves sharper and boxier. One potential advantage of this is that we can provide our calssifier with more signal and less noise. Additionally, it limits the length of our inputs which should speed up training.\n\nThere's a neat algorithm called  \"[Ramer\u2013Douglas\u2013Peucker](https:\/\/en.wikipedia.org\/wiki\/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm)\" that does exactly this sort of simplification:\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/30\/Douglas-Peucker_animated.gif\/440px-Douglas-Peucker_animated.gif)\nAnd lucky for us, [Fabian Hirschmann](https:\/\/www.kaggle.com\/fabianhirschmann1) was nice enough to wrap the algorightm in a python module called [rdp](https:\/\/github.com\/fhirschmann\/rdp).\n\nThe algorithm take a parameter (epsilon) that controls how aggressive the simplification is. Larger values result in fewer points and more jagged curves.\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/69\/RDP%2C_varying_epsilon.gif\/440px-RDP%2C_varying_epsilon.gif)\nThrough some trial and error I found that smaller  epsilon values lead to more accurate classifications but there's not much advantage in setting it below 0.2.\n","3e4a8a7f":"Now we'll run the path tracing code above to convert every image in both datasets into sequences of \"strokes\" which are composed of sequences of coordinates (pixels):","b31b5127":"We'll quick load the train and test datasets and make a few minor adjustments:","15b4ead6":"Let's see what our test images look like:","7e97a954":"Now we can generate a submision file based on the test data and the model we trained above:","b3cafc12":"Let's see what our sample number paths look like after applying RDP:","f4e23619":"## Results\nValidation set and public leaderboard **accuracy tends to be around 95%** for this method. It's a solid score, but considering all the work we put into this and that it's pretty easy to score above 99% with a simple CNN setup, our results are a little dissapointing. You're definitely better off just using a CNN, but I thought this was at least an interesting approach worth sharing. \n\n### Ideas for Improvement\n* Scale up the images before skeletonization.\n* Data Augmentation: randomly rotate and sheer and scale the images to increase the size of the dataset.\n* Use a larger model and\/or train for longer\n\n\n### Thanks for Reading!\nLet me know if you have any questions or suggestions in the comments below. \ud83d\ude0a","9723bddb":"## Sample Images\nLet's pick 1 random image from each class to preview our stroke-path-extraction code on. We'll add a few quick helper methods and then take a peek at our chosen images before we make any changes to them:","2c1d54a1":"## Contents\n1. [Motivation](#Motivation)\n2. [Sample Images](#Sample-Images)\n3. [Skeletonization](#Skeletonization)\n4. [Path Tracing](#Path-Tracing)\n5. [Simplifying Strokes with RDP](#Simplifying-Strokes-with-RDP)\n6. [Flat, Relative Paths](#Flat,-Relative-Paths)\n8. [RNN Model & Training](#RNN-Model-&-Training)\n9. [Results](#Results)\n\n## Motivation\nThere are already a lot of great kernels using different CNN architectures. So I wanted to try a different approach. I thought it'd be interesting to try to extract the path of the writer's pen from an image, and then feed that path (represented by a sequence of coordinates) into a recurrent neural network (RNN).\n\nLet's get started!","e577660a":"Finally, we want each image to be the same length, so we'll zero pad up to the lenght of the longest sequence. ","9ddaa58b":"Looks good, kind of like those connect-the-dots puzzles for kids.\n\n## Flat, Relative Paths\nCurrently, our paths are represented by absolute coordinates, (positions on a 28x28 grid, ex (0,0) represents the top left corner). So if the same image was shifted by a few pixels, it's path would be totally different! Minor shifts like this could hurt our classifier's performance. To fix the issue we can use relative paths. Every pixel's position is represented by its **x\/y distance from the previous pixel**. So now if an image shifts in any direction, only 1 coordinate needs to change instead of all of them. \n\nAdditionally, every image's stroke path has 2 dimensions of varying length (# of strokes, # of coordinates in a stroke). If we want to feed these into an RNN we need to flatten it to a single dimension. One simple way to handle this is to get rid of the stroke dimension and add a flag to every coordinate that indicates it's the beginning of a new stroke, ie there's no connection between it and the previous pixel."}}