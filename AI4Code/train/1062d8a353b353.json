{"cell_type":{"f7fd384e":"code","1c1a25e3":"code","0beb27c7":"code","e31c1d54":"code","1b6ef359":"code","92be5fa1":"code","63e1b36e":"code","8bb829f4":"code","0d8a2f2f":"code","61d0959d":"code","76b60753":"code","267e74c7":"code","fd864713":"code","60490d07":"code","347fe043":"code","41d0e5e1":"code","b0106cb5":"code","d540d3c0":"code","108f2ec5":"code","d4da74a3":"code","627f080f":"code","6a8d3ae1":"markdown","b05b5410":"markdown","0267b7fc":"markdown","4cc28993":"markdown","a6c86039":"markdown","d771e258":"markdown","3b5d5e7f":"markdown","0d1bf653":"markdown","98ff379b":"markdown","2a372467":"markdown"},"source":{"f7fd384e":"# \u8cc7\u6599\u8655\u7406\u5957\u4ef6\nimport os\nimport gc\nimport cv2\nimport time\nimport random\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1c1a25e3":"# \u8a2d\u5b9a\u986f\u793a\u4e2d\u6587\u5b57\u9ad4\nfrom matplotlib.font_manager import FontProperties\nplt.rcParams['font.sans-serif'] = ['Microsoft JhengHei'] # \u7528\u4f86\u6b63\u5e38\u986f\u793a\u4e2d\u6587\u6a19\u7c64\nplt.rcParams['font.family'] = 'AR PL UMing CN'\nplt.rcParams['axes.unicode_minus'] = False # \u7528\u4f86\u6b63\u5e38\u986f\u793a\u8ca0\u865f","0beb27c7":"# pytorch\u6df1\u5ea6\u5b78\u7fd2\u6a21\u7d44\u5957\u4ef6\nimport torch\nimport torchvision\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import functional as F\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","e31c1d54":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE","1b6ef359":"# \u67e5\u770bpytorch\u7248\u672c\nprint(torch.__version__)","92be5fa1":"'''\u57f7\u884c\u74b0\u5883\u53c3\u6578\u8a2d\u5b9a'''\n\n# (Boolean)\u662f\u5426\u70ba\u672c\u6a5f\nLOCAL = False\n\n# (Boolean)\u662f\u5426\u70ba Colab\nCOLAB = False\n\n\n'''\u6a94\u6848\u8def\u5f91\u53c3\u6578\u8a2d\u5b9a'''\n\n# (String)Root\u8def\u5f91\nif LOCAL:\n    PATH = r'..\/'\nelif COLAB:\n    PATH = r'\/content\/drive\/My Drive\/Colab Notebooks\/'\nelse:\n    PATH = r'..\/input\/'\n    \n# (String)\u8cc7\u6599\u6839\u8def\u5f91\nDATA_ROOT_PATH = PATH+r'sartorius-cell-instance-segmentation\/' \n\n# (String)CSV\u6839\u8def\u5f91\nCSV_ROOT_PATH = PATH+r'sartorius-cell-instance-segmentation\/'\n\n# (String)\u6e2c\u8a66\u8cc7\u6599\u8def\u5f91\nTEST_DATA_PATH = DATA_ROOT_PATH+r'test\/'\n\n# (String)\u6e2c\u8a66CSV\u8def\u5f91\nTEST_CSV_PATH = CSV_ROOT_PATH+r'sample_submission.csv'\n\n# (Boolean)\u662f\u5426\u8981\u532f\u5165Library\nIMPORT_PYTORCH_LIBRARY = False\n\n# (String)Library\u7684\u8def\u5f91\nPYTORCH_LIBRARY_PATH = PATH + \"Util\/\"\n\n# (String)\u8b80\u53d6\u9810\u8a13\u7df4\u6a21\u578b\/\u6b0a\u91cd\u7684\u540d\u7a31\uff0c\u7576fold model\u6642\uff0c\u5f8c\u9762\u6703\u81ea\u52d5\u52a0_NUMBER\nLOAD_MODEL_NAME = ['maskrcnn_resnet50_fpn']\n\n# (String)\u8b80\u53d6\u9810\u8a13\u7df4\u6a21\u578b\/\u6b0a\u91cd\u7684\u5132\u5b58\u8def\u5f91\nLOAD_MODEL_PATH = [PATH + r'test1\/']","63e1b36e":"# Override pythorch checkpoint with an \"offline\" version of the file\n!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/pytorch-pretrained-models\/resnet50-19c8e357.pth \/root\/.cache\/torch\/hub\/checkpoints\/resnet50-0676ba61.pth","8bb829f4":"'''\u5ba2\u88fd\u53c3\u6578\u8a2d\u5b9a'''\n\n\n'''\u8cc7\u6599\u53c3\u6578\u8a2d\u5b9a'''\n\n# (Dict)\u6a19\u7c64\u5b57\u5178\nLABEL_DICT = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n\n# (Int)\u96c6\u6210\u6a21\u578b\u6578\u91cf\nENSEMBLE_MODEL_COUNT = 1\n\n# (Int List)\u6709CSV\u6a94\u8a72\u53c3\u6578\u624d\u6709\u7528\uff0c1\u5247\u70ba\u4e0d\u505a\u4ea4\u53c9\u9a57\u8b49\nFOLD = [1]*ENSEMBLE_MODEL_COUNT\n\n# (Int)\u5716\u7247\u5c3a\u5bf8\u5bec\nIMAGE_SIZE_W = [704]*ENSEMBLE_MODEL_COUNT\n\n# (Int)\u5716\u7247\u5c3a\u5bf8\u9ad8\nIMAGE_SIZE_H = [520]*ENSEMBLE_MODEL_COUNT\n\n# (String)CSV\u5716\u7247\u6a94\u540d\u6b04\u4f4d\nIMAGE_NAME = \"id\"\n\n# (String)CSV\u6a19\u6ce8\u9810\u6e2c\u6b04\u4f4d\nANNOTATION_NAME = \"predicted\"\n\n# (Boolean)CSV\u5716\u7247\u6a94\u540d\u6b04\u4f4d\u662f\u5426\u5305\u542b\u526f\u6a94\u540d\nIMAGE_NAME_HAVE_EXTENSION = False\n\n# (String)\u5716\u7247\u526f\u6a94\u540d\nIMAGE_NAME_EXTENSION = '.png'\n\n# (Flag)\u5716\u50cf\u8b80\u53d6\u7684\u683c\u5f0f\nIMREAD_FLAGS = cv2.IMREAD_COLOR\n\n#  (Boolean)\u5716\u50cf\u662f\u5426\u8981\u8f49\u63db\nCOLOR_CONVERT = False\nif COLOR_CONVERT:\n    #  (Boolean)\u5716\u50cf\u8f49\u63db\u901a\u9053\n    COLOR_CONVERT_CHANNEL = cv2.COLOR_BGR2RGB\n\n# (Int)\u4e0d\u540c\u7684\u7a2e\u5b50\u6703\u7522\u751f\u4e0d\u540c\u7684Random\u6216\u5206\u5c64K-FOLD\u5206\u88c2, 42\u5247\u662f\u9810\u8a2d\u56fa\u5b9a\u7a2e\u5b50\nSEED = 42\n\n# (Boolean)\u5982\u70baTrue\u6bcf\u6b21\u8fd4\u56de\u7684\u5377\u7a4d\u7b97\u6cd5\u5c07\u662f\u78ba\u5b9a\u7684\uff0c\u5373\u9ed8\u8a8d\u7b97\u6cd5\nCUDNN_DETERMINISTIC = True\n\n# (Boolean)PyTorch \u4e2d\u5c0d\u6a21\u578b\u88e1\u7684\u5377\u7a4d\u5c64\u9032\u884c\u9810\u5148\u7684\u512a\u5316\uff0c\u4e5f\u5c31\u662f\u5728\u6bcf\u4e00\u500b\u5377\u7a4d\u5c64\u4e2d\u6e2c\u8a66 cuDNN \u63d0\u4f9b\u7684\u6240\u6709\u5377\u7a4d\u5be6\u73fe\u7b97\u6cd5\uff0c\n# \u7136\u5f8c\u9078\u64c7\u6700\u5feb\u7684\u90a3\u500b\u3002\u9019\u6a23\u5728\u6a21\u578b\u555f\u52d5\u7684\u6642\u5019\uff0c\u53ea\u8981\u984d\u5916\u591a\u82b1\u4e00\u9ede\u9ede\u9810\u8655\u7406\u6642\u9593\uff0c\u5c31\u53ef\u4ee5\u8f03\u5927\u5e45\u5ea6\u5730\u6e1b\u5c11\u8a13\u7df4\u6642\u9593\nCUDNN_BENCHMARK = True\n\n\n'''\u8cc7\u6599\u64f4\u589e\u53c3\u6578\u8a2d\u5b9a'''\n\n# (Boolean)\u662f\u5426\u5716\u5f62\u6b78\u4e00\u5316\nIS_NORMALIZE = False\n\n# (Tuple Float)\u6b63\u898f\u5316\u7684\u5e73\u5747\u503c((0,1)\u7684\u53c3\u8003\u5e73\u5747\u503c:(0.485, 0.456, 0.406), (-1,1)\u7684\u53c3\u8003\u5e73\u5747\u503c:(0.5, 0.5, 0.5)\nNORMALIZE_MEAN = (0.485, 0.456, 0.406)\n\n# (Tuple Float)\u6b63\u898f\u5316\u7684\u6a19\u6e96\u5dee((0,1)\u7684\u53c3\u8003\u6a19\u6e96\u5dee(0.229, 0.224, 0.225), (-1,1)\u7684\u53c3\u8003\u6a19\u6e96\u5dee(0.5, 0.5, 0.5)\nNORMALIZE_STD = (0.229, 0.224, 0.225)\n\n# (Float)\u6c34\u5e73\u7ffb\u8f49\u7684\u555f\u7528(0:\u4e0d\u555f\u7528,1.0:\u4e00\u5f8b\u555f\u7528,\u5c0f\u6578\u9ede:\u6a5f\u7387\u555f\u7528)\nP_HORIZONTALFLIP = 0.5\n\n# (Float)\u5782\u76f4\u7ffb\u8f49\u7684\u555f\u7528(0:\u4e0d\u555f\u7528,1.0:\u4e00\u5f8b\u555f\u7528,\u5c0f\u6578\u9ede:\u6a5f\u7387\u555f\u7528)\nP_VERTICALFLIP = 0.5\n\n\n''''\u6a21\u578b\u53c3\u6578\u8a2d\u5b9a'''\n\n# (String List)\u6a21\u578b\u8f09\u5165\u65b9\u5f0f - 1 MODEL;2 WEIGHT_OF_CUSTOM_MODEL;\n# 3 WEIGHT_OF_BASE_MODEL\nMODEL_LIST = [3] * ENSEMBLE_MODEL_COUNT\n\nif 2 in MODEL_LIST:\n    # (Model List)\u6a21\u578b\u8f09\u5165\u65b9\u5f0f\u6709CUSTOM_MODEL\uff0c\u4f9d\u7167index\u4f4d\u7f6e\u586b\u5165\n    CUSTOM_MODEL = [None] * ENSEMBLE_MODEL_COUNT\n\nif 3 in MODEL_LIST:\n    # (Model List)\u6a21\u578b\u8f09\u5165\u65b9\u5f0f\u6709BASE_MODEL\uff0c\u4f9d\u7167index\u4f4d\u7f6e\u586b\u5165\n    BASE_MODEL = [torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained = False, box_detections_per_img = 540)] * ENSEMBLE_MODEL_COUNT\n\n    # (Int)\u6a21\u578b\u96b1\u85cf\u5c64\n    HIDDEN_LAYER = 256\n\n    # (String)\u6a21\u578bBox\u9810\u6e2c\n    MODEL_BOX_PREDICTOR = FastRCNNPredictor\n\n    # (String)\u6a21\u578bMask\u9810\u6e2c\n    MODEL_MASK_PREDICTOR = MaskRCNNPredictor\n\n# (Boolean)\u662f\u5426\u5370\u51fa\u5b8c\u6574\u6a21\u578b\nMODEL_PRINT = False\n\n\n''''\u63a8\u8ad6\u53c3\u6578\u8a2d\u5b9a'''\n\n# (Int List)\u6bcf\u6279\u63a8\u8ad6\u7684\u5c3a\u5bf8\nBATCH_SIZE = [2]*ENSEMBLE_MODEL_COUNT\n\n# (Int)\u6307\u5b9a\u5217\u5370\u9032\u5ea6\u689d\u7684\u4f4d\u7f6e\uff08\u5f9e0\u958b\u59cb\uff09\nTQDM_POSITION = 0\n\n# (Boolean)\u4fdd\u7559\u8fed\u4ee3\u7d50\u675f\u6642\u9032\u5ea6\u689d\u7684\u6240\u6709\u75d5\u8de1\u3002\u5982\u679c\u662fNone\uff0c\u53ea\u6703\u5728position\u662f0\u6642\u96e2\u958b\nTQDM_LEAVE = True\n\n\n''''\u8a55\u50f9\u6307\u6a19\u53c3\u6578\u8a2d\u5b9a'''\n\n# (Float dist)\u6a19\u7c64\u5206\u985e\u7684\u6700\u5c0f\u5141\u8a31\u5206\u6578\uff0c\u5426\u5247\u4e0d\u8a55\u50f9\nMIN_SCORE_DICT = {1: 0.55, 2: 0.75, 3: 0.5}\n\n# (Float dist)Mask\u7684\u6700\u5c0f\u5141\u8a31\u95be\u503c\uff0c\u5426\u5247\u4e0d\u8a55\u50f9\nMASK_THRESHOLD_DICT = {1: 0.55, 2: 0.75, 3:  0.6}","0d8a2f2f":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = CUDNN_DETERMINISTIC\n    torch.backends.cudnn.benchmark = CUDNN_BENCHMARK\n\nseed_everything(SEED)","61d0959d":"print('Reading data...')\n\n# \u8b80\u53d6\u8a13\u7df4\u8cc7\u6599\u96c6CSV\u6a94\nif os.path.isfile(TEST_CSV_PATH):\n    test_csv = pd.read_csv(TEST_CSV_PATH,encoding=\"utf8\")\nelse:\n    test_data_directory_list = os.listdir(TEST_CSV_PATH)\n    test_csv = pd.DataFrame(test_data_directory_list, columns=[IMAGE_NAME])\n    del test_data_directory_list\n    gc.collect()\n\nprint('Reading data completed')","76b60753":"# \u986f\u793a\u6e2c\u8a66\u8cc7\u6599\u96c6CSV\u6a94\ntest_csv.head()","267e74c7":"print(\"Shape of train_data :\", test_csv.shape)","fd864713":"def build_model(count, model_path):\n    if MODEL_LIST[count] == 1:\n        # \u8f09\u5165\u9810\u8a13\u7df4\u6a21\u578b\n        model = torch.load(model_path)\n    else:\n        if MODEL_LIST[count] == 2:\n            # \u8f09\u5165\u6a21\u578b\u67b6\u69cb\n            model = CUSTOM_MODEL[count]\n        elif MODEL_LIST[count] == 3:\n            model = BASE_MODEL[count]\n\n            # get the number of input features for the classifier\n            in_features = model.roi_heads.box_predictor.cls_score.in_features\n            # replace the pre-trained head with a new one\n            model.roi_heads.box_predictor = MODEL_BOX_PREDICTOR(in_features, len(LABEL_DICT)+1)\n\n            # now get the number of input features for the mask classifier\n            in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n            # and replace the mask predictor with a new one\n            model.roi_heads.mask_predictor = MODEL_MASK_PREDICTOR(in_features_mask, HIDDEN_LAYER, len(LABEL_DICT)+1)\n        \n    if MODEL_LIST[count] != 1:\n        # \u8f09\u5165\u9810\u8a13\u7df4\u6b0a\u91cd\n        model.load_state_dict(torch.load(model_path))\n        \n    for param in model.parameters():\n        param.requires_grad = False\n        \n    return model","60490d07":"def rle_encoding(x: np.ndarray):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","347fe043":"class MyDataset(Dataset):\n    def __init__(self, transforms = None):\n        self.transforms = transforms\n        self.image_ids = [f[:-4]for f in os.listdir(TEST_DATA_PATH)]\n            \n    def __len__(self):\n        return len(self.image_ids)\n            \n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        image_path = os.path.join(TEST_DATA_PATH, image_id + IMAGE_NAME_EXTENSION)\n        image = cv2.imread(image_path, IMREAD_FLAGS)\n        image = cv2.resize(image, (IMAGE_SIZE_W[0], IMAGE_SIZE_H[0]))\n        \n        if COLOR_CONVERT:\n            image = cv2.cvtColor(image, COLOR_CONVERT_CHANNEL)\n\n        if self.transforms:\n            image, _ = self.transforms(image = image, target = None)\n            \n        return {'image': image, 'image_id': image_id}","41d0e5e1":"# These are slight redefinitions of torch.transformation classes\n# The difference is that they handle the target and the mask\n# Copied from Abishek, added new ones\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, NORMALIZE_MEAN, NORMALIZE_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\ndef get_transforms():\n    transforms = [ToTensor()]\n    if IS_NORMALIZE:\n        transforms.append(Normalize())\n\n    return Compose(transforms)","b0106cb5":"def inference_one_epoch(model, test_dataloader, \n                        min_score_dict: dict={1: 0.55, 2: 0.75, 3: 0.5}, \n                        mask_threshold_dict: dict={1: 0.55, 2: 0.75, 3:  0.6}):\n    model.eval()\n    pbar = tqdm(enumerate(test_dataloader), total=len(test_dataloader), \n                position = TQDM_POSITION, leave = TQDM_LEAVE)\n    for batch_idx, (sample) in pbar:\n        image = sample['image']\n        image_id = sample['image_id']\n        with torch.no_grad():\n            result = model([image.to(DEVICE)])[0]\n\n        previous_masks = []\n        for i, mask in enumerate(result[\"masks\"]):\n\n            # Filter-out low-scoring results.\n            score = result[\"scores\"][i].cpu().item()\n            label = result[\"labels\"][i].cpu().item()\n            if score > min_score_dict[label]:\n                mask = mask.cpu().numpy()\n                # Keep only highly likely pixels\n                binary_mask = mask > mask_threshold_dict[label]\n                binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n                previous_masks.append(binary_mask)\n                rle = rle_encoding(binary_mask)\n                submission.append((image_id, rle))\n                \n        # Add empty prediction if no RLE was generated for this image\n        all_images_ids = [image_id for image_id, rle in submission]\n        if image_id not in all_images_ids:\n            submission.append((image_id, \"\"))","d540d3c0":"def inference_process(count, fold, kf, submission):\n    if kf:\n        print('Model %i : Fold %i - image size W:%i H:%i with %s and batch size %i'%(count+1, fold, IMAGE_SIZE_W[count], IMAGE_SIZE_H[count], LOAD_MODEL_NAME[count].upper(), BATCH_SIZE[count]))\n    else:\n        print('Model %i : Image size W:%i H:%i with %s and batch_size %i'%(count+1, IMAGE_SIZE_W[count], IMAGE_SIZE_H[count], LOAD_MODEL_NAME[count].upper(), BATCH_SIZE[count]))\n    \n    test_dataset = MyDataset(transforms = get_transforms())\n\n    if kf:\n        model_path = LOAD_MODEL_PATH[count] + LOAD_MODEL_NAME[count] + '_' + str(fold) + '.pth'\n    else:\n        model_path = LOAD_MODEL_PATH[count] + LOAD_MODEL_NAME[count] + '.pth'\n\n    model = build_model(count, model_path)\n    model = model.to(DEVICE)\n    \n    inference_one_epoch(model, test_dataset, MIN_SCORE_DICT, MASK_THRESHOLD_DICT)\n    \n    df_sub = pd.DataFrame(submission, columns=[IMAGE_NAME, ANNOTATION_NAME])\n    if kf:\n        df_sub.to_csv(\"submission_model\" + str(count+1) + \"_fold\" + str(fold) + \".csv\", index=False)\n    elif ENSEMBLE_MODEL_COUNT != 1 and not kf:\n        df_sub.to_csv(\"submission_model\" + str(count+1) + \".csv\", index=False)\n    else:\n        df_sub.to_csv(\"submission.csv\", index=False)\n    print(df_sub.head())\n    \n    del test_dataset, model\n    gc.collect()\n    torch.cuda.empty_cache()","108f2ec5":"submission = []","d4da74a3":"def main():\n    try:\n        print('Inference start')\n        since = time.time()\n        for count in range(len(LOAD_MODEL_NAME)):\n            if FOLD[count] > 1:\n                for fold in enumerate(np.arange(FOLD[count]), 1):\n                    inference_process(count, fold = fold, kf = True, submission = submission)\n            else:\n                inference_process(count, fold = 0, kf = False, submission = submission)\n        time_elapsed = time.time() - since\n        print('Inference complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    except Exception as exception:\n        print(exception)\n        raise","627f080f":"if __name__ == '__main__':\n    main()","6a8d3ae1":"# 3. \u958b\u767c\u53c3\u6578\u8a2d\u5b9a<a class=\"anchor\" id=\"3\"><\/a>\n[Back to Table of Contents](#0)","b05b5410":"# 5. \u5b9a\u7fa9\u6a21\u578b\u65b9\u6cd5<a class=\"anchor\" id=\"5\"><\/a>\n[Back to Table of Contents](#0)","0267b7fc":"# 4. \u8cc7\u6599\u8655\u7406<a class=\"anchor\" id=\"4\"><\/a>\n[Back to Table of Contents](#0)","4cc28993":"# 2. \u74b0\u5883\u6aa2\u6e2c\u8207\u8a2d\u5b9a<a class=\"anchor\" id=\"2\"><\/a>\n[Back to Table of Contents](#0)","a6c86039":"## 4.1 \u8f09\u5165CSV\u6a94 <a class=\"anchor\" id=\"4.1\"><\/a>\n[Back to Table of Contents](#0)","d771e258":"<a class=\"anchor\" id=\"0\"><\/a>\n# Table of Contents\n\n1. [\u5957\u4ef6\u5b89\u88dd\u8207\u8f09\u5165](#1)\n1. [\u74b0\u5883\u6aa2\u6e2c\u8207\u8a2d\u5b9a](#2)\n1. [\u958b\u767c\u53c3\u6578\u8a2d\u5b9a](#3)\n1. [\u8cc7\u6599\u8655\u7406](#4)\n    -  [\u8f09\u5165CSV\u6a94](#4.1)\n1. [\u5b9a\u7fa9\u6a21\u578b\u65b9\u6cd5](#5)\n1. [\u88fd\u4f5c\u8cc7\u6599\u96c6\uff06\u8cc7\u6599\u64f4\u589e\uff06\u63a8\u8ad6\u6a21\u578b](#6)\n1. [\u5f85\u8fa6\u4e8b\u9805](#7)","3b5d5e7f":"# Instance_Segmentation_Pytorch (Inference)","0d1bf653":"# 7. \u5f85\u8fa6\u4e8b\u9805<a class=\"anchor\" id=\"7\"><\/a>\n[Back to Table of Contents](#0)","98ff379b":"# 1. \u5957\u4ef6\u5b89\u88dd\u8207\u8f09\u5165<a class=\"anchor\" id=\"1\"><\/a>\n[Back to Table of Contents](#0)","2a372467":"# 6. \u88fd\u4f5c\u8cc7\u6599\u96c6\uff06\u8cc7\u6599\u64f4\u589e\uff06\u63a8\u8ad6\u6a21\u578b<a class=\"anchor\" id=\"6\"><\/a>\n[Back to Table of Contents](#0)"}}