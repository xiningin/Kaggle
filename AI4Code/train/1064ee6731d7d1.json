{"cell_type":{"890e977f":"code","ccd666eb":"code","e40d797f":"code","52499b02":"code","2e21ed52":"code","fc94ca89":"code","56cc309d":"code","ac9ebcdf":"code","76292edc":"code","b6f82b05":"code","77c9644c":"code","39a2373a":"code","57974ea5":"code","f7ee6294":"code","7665f7c3":"code","65343f03":"code","450106e1":"code","830ccff3":"code","dbf80726":"markdown","0726caf4":"markdown","0cb7d5d0":"markdown","35505328":"markdown","980bc534":"markdown","f5f0e285":"markdown","572b1844":"markdown","f6294178":"markdown","81822c94":"markdown","36004bdf":"markdown","21b9f25d":"markdown","5c3aed45":"markdown","2a646ffc":"markdown","765b88f3":"markdown","58b54929":"markdown","20040a2f":"markdown","53d4b028":"markdown","e0c1b383":"markdown","f6a13652":"markdown"},"source":{"890e977f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccd666eb":"# Fundamentals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import NLTK to use its functionalities on texts\n\"\"\"DO NOT forget to download followings if you do not have\n# nltk.download('punkt')\n#nltk.download('wordnet')\n\"\"\"\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# We will visualize the messages with a word cloud\nfrom wordcloud import WordCloud\n\n# Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Import Tf-idf Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Import the Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Import the train test split\nfrom sklearn.model_selection import train_test_split\n\n# To evaluate our model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\n\n# I will keep the resulting plots\n%matplotlib inline\n\n# Enable Jupyter Notebook's intellisense\n%config IPCompleter.greedy=True","e40d797f":"# Load the data\ndata = pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')","52499b02":"# Display firt five rows\ndisplay(data.head())\n\n# Display the summary statistics\ndisplay(data.describe())\n\n# Print the info\nprint(data.info())","2e21ed52":"# Print the counts of each category\nprint(data['Category'].value_counts())\n\nprint()\n\n# Print the proportions of each category\nprint(data['Category'].value_counts(normalize=True))\n\n# Visualize the Categories\nsns.countplot(data['Category'])\nplt.title(\"Category Counts\")\nplt.show()","fc94ca89":"# Initialize the Label Encoder.\nle = LabelEncoder()\n\n# Encode the categories\ndata['Category_enc'] = le.fit_transform(data['Category'])\n\n# Display the first five rows again to see the result\ndisplay(data.head())\n\n# Print the datatypes\nprint(data.dtypes)","56cc309d":"# Store the number of words in each messages\ndata['word_count'] = data['Message'].str.split().str.len()\n\n# Print the average number of words in each category\nprint(data.groupby('Category')['word_count'].mean())\n\n# Visualize the distribution of word counts in each category\nsns.distplot(data[data['Category']=='spam']['word_count'], label='Spam')\nsns.distplot(data[data['Category']=='ham']['word_count'], label='Ham'),\nplt.legend()\nplt.show()","ac9ebcdf":"# Make the letters lower case and tokenize the words\ntokenized_messages = data['Message'].str.lower().apply(word_tokenize)\n\n# Print the tokens to see how it looks like\nprint(tokenized_messages)","76292edc":"# Define a function to returns only alphanumeric tokens\ndef alpha(tokens):\n    \"\"\"This function removes all non-alphanumeric characters\"\"\"\n    alpha = []\n    for token in tokens:\n        if str.isalpha(token) or token in ['n\\'t','won\\'t']:\n            if token=='n\\'t':\n                alpha.append('not')\n                continue\n            elif token == 'won\\'t':\n                alpha.append('wont')\n                continue\n            alpha.append(token)\n    return alpha\n\n# Apply our function to tokens\ntokenized_messages = tokenized_messages.apply(alpha)\n\nprint(tokenized_messages)","b6f82b05":"# Define a function to remove stop words\ndef remove_stop_words(tokens):\n    \"\"\"This function removes all stop words in terms of nltk stopwords\"\"\"\n    no_stop = []\n    for token in tokens:\n        if token not in stopwords.words('english'):\n            no_stop.append(token)\n    return no_stop\n\n# Apply our function to tokens\ntokenized_messages = tokenized_messages.apply(remove_stop_words)\n\nprint(tokenized_messages)","77c9644c":"# Define a function to lemmatization\ndef lemmatize(tokens):\n    \"\"\"This function lemmatize the messages\"\"\"\n    # Initialize the WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # Create the lemmatized list\n    lemmatized = []\n    for token in tokens:\n            # Lemmatize and append\n            lemmatized.append(lemmatizer.lemmatize(token))\n    return \" \".join(lemmatized)\n\n# Apply our function to tokens\ntokenized_messages = tokenized_messages.apply(lemmatize)\n\nprint(tokenized_messages)","39a2373a":"# Replace the columns with tokenized messages\ndata['Message'] = tokenized_messages\n\n# Display the first five rows\ndisplay(data.head())","57974ea5":"# Get the spam messages\nspam = data[data['Category']=='spam']['Message'].str.cat(sep=', ')\n\n# Get the ham messages\nham = data[data['Category']=='ham']['Message'].str.cat(sep=', ')\n\n# Initialize the word cloud\nwc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color ='white')\n\n# Generate the world clouds for each type of message\nspam_wc = wc.generate(spam)\n\n# plot the world cloud for spam                     \nplt.figure(figsize = (5, 5), facecolor = None) \nplt.imshow(spam_wc) \nplt.axis(\"off\") \nplt.title(\"Common words in spam messages\")\nplt.tight_layout(pad = 0) \nplt.show() \nham_wc = wc.generate(ham)\n\n# plot the world cloud for spam                       \nplt.figure(figsize = (5, 5), facecolor = None) \nplt.imshow(ham_wc) \nplt.axis(\"off\")\nplt.title(\"Common words in ham messages\")\nplt.tight_layout(pad = 0) \nplt.show() ","f7ee6294":"# Select the features and the target\nX = data['Message']\ny = data['Category_enc']","7665f7c3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)","65343f03":"# Create the tf-idf vectorizer\nvectorizer = TfidfVectorizer(strip_accents='ascii')\n\n# First fit the vectorizer with our training set\ntfidf_train = vectorizer.fit_transform(X_train)\n\n# Now we can fit our test data with the same vectorizer\ntfidf_test = vectorizer.transform(X_test)","450106e1":"# Initialize the Multinomial Naive Bayes classifier\nnb = MultinomialNB()\n\n# Fit the model\nnb.fit(tfidf_train, y_train)\n\n# Print the accuracy score\nprint(\"Accuracy:\",nb.score(tfidf_test, y_test))","830ccff3":"# Predict the labels\ny_pred = nb.predict(tfidf_test)\n\n# Print the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix\\n\")\nprint(cm)\n\n# Print the Classification Report\ncr = classification_report(y_test, y_pred)\nprint(\"\\n\\nClassification Report\\n\")\nprint(cr)\n\n\n# Print the Receiver operating characteristic Auc score\nauc_score = roc_auc_score(y_test, y_pred)\nprint(\"\\nROC AUC Score:\",auc_score)\n\n# Get probabilities.\ny_pred_proba = nb.predict(tfidf_test)\n\n# Get False Positive rate, True Positive rate and the threshold\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\n# Visualize the ROC curve.\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FP Rate')\nplt.ylabel('TP Rate')\nplt.title('ROC')\nplt.show()","dbf80726":"It seems that ham messages contain lot's of abbreviation and informal words. Spams tend to contain mostly *free, mobile, cole, text*\n\n## 3. Build the model\n\nFirst select the our features and the target.","0726caf4":"It gives us approximately 96% accuracy. \n\n## 4. Evaluate the model\n\nThe accuracy score is not enought to say our model's performance great. We need to do more calculation to be make sure.","0cb7d5d0":"# Naive Bayes for Text Classification\n\n## Introduction\n\nThe dataset contains SMS messages labeled as *ham* (non-spam) or *spam*. We'll use scikit-learn's Naive Bayes Classifier to predict these labels.\n\nOriginally published here: [link](https:\/\/github.com\/Bhasfe\/ml-algorithms\/tree\/master\/Naive%20Bayes)","35505328":"It's time to deal with the stop words. We've already imported stopwords from nltk.","980bc534":"We need to split our data into train and test sets. We'll use **stratify** parameter of train_test_split since our data is unbalanced","f5f0e285":"Visualize the words mostly used in each type of messages.","572b1844":"As we can see above, the dataset is unbalanced. We have to consider this when we build our model. As well as, we need to encode the labels to use our machine learning model. To achieve this we'll use the Label Encoder from scikit-learn","f6294178":"There some non-alphanumeric characters ( * ' \" - ) and stop words like a, and, the etc. Let's discard them","81822c94":"Our dataset has **Message** and **Category** columns which consist of *object* data type. There are 5572 messages. We have to check whether the category data is *balanced or not*.","36004bdf":"Area under the ROC curve gave us lower but it still looks fine. According to confusion matrix and classification report our model is perfect to detect spams but not hams\n\nThis is the and of the notebook. I hope it can be helpful to understand using *Naive Bayes* classifier in Python.","21b9f25d":"Now we can look at main features of the dataset","5c3aed45":"Let's replace Message column with tokenized_messages","2a646ffc":"Apparently, spams tend to contain more number of words. What about the most common words in each ?","765b88f3":"## 1. Exploratory Data Analysis\n\nLet's start with importing the data into a pandas DataFrame called data","58b54929":"Now, we can get the tf-idf by using scikit-learn's TfidfVectorizer. Tf-idf stands for *term frequency - inverse document frequency*. It is commonly used in Natural Language Processing to determine the most important words in the document.","20040a2f":"We got the encoded categories. Now, *0 = ham* and *1 = spam*.\n\n## 2. Feature Engineering\n\nBefore build the classification model, let's explore the data. First we'll compare the word counts of messages in each category","53d4b028":"### How does it work ?\n\nNaive Bayes is an algorithm which is commonly used in natural language processing (NLP) tasks such as spam filtering, sentiment analysis, classification, recommendation. It is based on *Bayes' Theorem* as shown below. \n\n<img src=\"https:\/\/github.com\/Bhasfe\/ml-algorithms\/blob\/master\/Naive%20Bayes\/bayesian.png?raw=true\" width=\"300px\" height=\"300px\" align=\"left\" \/><br><br><br><br><br>\n\n*P(A|B)* = probability of A if B occurs <br>\n*P(B|A)* = probability of B if A occurs <br>\n*P(A)* = probability of A <br>\n*P(B)* = probability of A <br>\n\nA Naive Bayes classifier gives us the conditional probabilities of events occur related to each other by using Bayes' Theorem. If you would like to deep into, please visit [here](https:\/\/www.kaggle.com\/prashant111\/naive-bayes-classifier-tutorial).<br>\n<br>\nIn scikit-learn, there are 3 types naive bayes algorithms.\n\n**Gaussian Naive Bayes:** It works with continuous attributes, it assumes the data normally distributed (Gaussian Distribution) <br><br>\n**Multinomial Naive Bayes:** It works with frequencies of features.It is used text classification or <br><br>\n**Bernoulli Naive Bayes:** It works with multinomial binary variables, usually used for text classification like multinomial<br><br>\n\nWe'll use **Multinomal** in this notebook.\n\n### Basics of Natural Language Processing\n\nSince we'll use *Naive Bayes* in a *text classification* task, I would like to explain briefly some concepts that are we going to use.<br>\n\n**Lemmatization:** It is a process of make the same words in their stem. For example run, ran, running are different in terms of Python. We lemmatiza the word to get run.<br><br>\n**Stop words:** The words that are not important in terms of the context<br><br>\n**Tokenization:** The process of extracting words in a sentence by spaces and punctuations. In this project we'll use nltk's [word_tokenize](https:\/\/www.nltk.org\/api\/nltk.tokenize.html)<br><br>\n**Bag Of Words:** BoW is the representation of text data in a numerical way that machine learning algoritms can work with. <br><br>\n**Tf-idf (Term Frequency - Inverse Term Frequency):** It is a statistical concept to be used to get importance of words in corpus. We'll use scikit-learn's *TfidfVectorizer*. The vectorizer will calculate the weight of each word in corpus and will return a tf-idf matrix. You can find further information [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)<br><br>\n<img src=\"https:\/\/github.com\/Bhasfe\/ml-algorithms\/blob\/master\/Naive%20Bayes\/tfidf.png?raw=true\" width=\"300px\" height=\"300px\" align=\"left\" \/><br><br><br><br><br>\n*td* = Term frequency (number of occurance each i in j)<br>\n*df* = Document frequency <br>\n*N* = Number of documents<br>\n*w* = tf-idf weight\nfor each *i* and *j* (document).\n\n### Requirements\n\nBefore start the project. We have to install the necessary libraries via following commands.\n\n`pip install -U scikit-learn`\n\n`pip install wordcloud`\n\n`pip install --user -U nltk`","e0c1b383":"Finally, we can build the our machine learning model.","f6a13652":"Moreover, we need to lemmatize the words. We've already imported WordNetLemmatizer from nltk."}}