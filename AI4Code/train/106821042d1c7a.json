{"cell_type":{"5fe28950":"code","51542d01":"code","4d0fbcf3":"code","077fdc49":"code","0f18c891":"code","1ec4b3b0":"code","2270fa2f":"code","baddbc65":"code","ef7e3864":"code","f1b213b6":"code","8d93be38":"code","4513658c":"code","081fd560":"code","6786927d":"markdown","51bbaf1a":"markdown"},"source":{"5fe28950":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51542d01":"import pandas as pd\nimport numpy as np\n\nfrom xgboost import XGBClassifier as XGB\nimport seaborn as sns","4d0fbcf3":"import lightgbm as lgb","077fdc49":"data_all = pd.read_csv(\"..\/input\/210718-cae-resultcsv\/210718_cae_result.csv\")\ndata_all","0f18c891":"data_all.describe()","1ec4b3b0":"train = data_all[:80]\ntest = data_all[80:]","2270fa2f":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\ntrain_x = train[[\"fai\",\"L\",\"d\",\"x\"]]\ntrain_y = train[[\"P\"]]\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\ntest_x = test[[\"fai\",\"L\",\"d\",\"x\"]]\ntest_y = test[[\"P\"]]","baddbc65":"%%time\nparams = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\",\n    \"num_leaves\" : 40,\n    \"learning_rate\" : 0.01,\n    \"bagging_fraction\" : 0.8,\n    \"feature_fraction\" : 0.4,\n    \"bagging_frequency\" : 6,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1,\n    \"seed\": 42\n}\n\ntrain_lgb = lgb.Dataset(train_x, label=train_y)\ntest_lgb = lgb.Dataset(test_x, label=test_y)\nevals_result = {}\nmodel_lgb = lgb.train(params, train_lgb, 10000, \n                  valid_sets=[train_lgb, test_lgb], \n                  early_stopping_rounds=200, \n                  verbose_eval=1000, \n                  evals_result=evals_result)","ef7e3864":"pred_lgb = model_lgb.predict(test_x)\npred_lgb","f1b213b6":"columns = [\"pred_y\"]\npred_d = pd.DataFrame(data=pred_lgb, columns = columns)\ntest_y_reindex = test_y.reset_index(drop=True)\n\nsns.scatterplot(x=test_y_reindex['P'], y=pred_d['pred_y'])\nsns.scatterplot(x=test_y_reindex['P'], y=test_y_reindex['P']) #perfect fitting line","8d93be38":"# \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u7b97\u51fa (\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3067\u53d6\u5f97)\n\n# \u7279\u5fb4\u91cf\u540d\u306e\u30ea\u30b9\u30c8(\u76ee\u7684\u5909\u6570CRIM\u4ee5\u5916)\n# \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u7b97\u51fa\u65b9\u6cd5 'gain'(\u63a8\u5968) : \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306e\u640d\u5931\u306e\u6e1b\u5c11\u91cf\u3092\u8a55\u4fa1\ncols = list(train_x.columns)\ncols_df = pd.DataFrame(cols)\n\n# \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u306e\u7b97\u51fa \/\/\nf_importance = np.array(model_lgb.feature_importance(importance_type='gain'))\n# \u6b63\u898f\u5316(\u5fc5\u8981\u306a\u3044\u5834\u5408\u306f\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8)\nf_importance = f_importance \/ np.sum(f_importance)\nf_importance_df = pd.DataFrame(f_importance)\ndf_importance = cols_df.join(f_importance_df,lsuffix='_features', rsuffix='_importance')\n# \u964d\u9806\u30bd\u30fc\u30c8\ndf_importance = df_importance.sort_values('0_importance', ascending=False)\ndf_importance","4513658c":"# \u4e88\u6e2c\u3057\u305f\u3044\u5024\u3092\u5165\u308c\u308b\nlist_new = [[103,510,24,12],[101,505,22,11]]\ntest_new = pd.DataFrame(list_new)\ntest_new.columns = [\"fai\",\"L\",\"d\",\"x\"] \ntest_new","081fd560":"pred_lgb_new = model_lgb.predict(test_new)\ntest_new[\"pred\"] = pred_lgb_new\ntest_new","6786927d":"## LightGBM\u3067CAE\u306e\u30b5\u30ed\u30b2\u30fc\u30c8\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\n[\u3010\u5b9f\u8df5python\u3011AI\u306b\u3088\u308bCAE\u30b5\u30ed\u30b2\u30fc\u30c8\u30e2\u30c7\u30eb\u306e\u4f5c\u308a\u65b9](https:\/\/hasimoto-soken.com\/%e3%80%90%e5%ae%9f%e8%b7%b5python%e3%80%91ai%e3%81%ab%e3%82%88%e3%82%8bcae%e3%82%b5%e3%83%ad%e3%82%b2%e3%83%bc%e3%83%88%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e4%bd%9c%e3%82%8a%e6%96%b9\/)\u306b\u3066\u5229\u7528\u3057\u305f\u30b3\u30fc\u30c9\u3067\u3059\u3002<br>\n\u306a\u308b\u3079\u304f\u30b7\u30f3\u30d7\u30eb\u306bLightGBM\u3092\u5b9f\u88c5\u3057\u305f\u3064\u3082\u308a\u3067\u3059\u304c\u3001\u6539\u5584\u3067\u304d\u308b\u500b\u6240\u306a\u3069\u3042\u308c\u3070\u30b3\u30e1\u30f3\u30c8\u9802\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002<br>","51bbaf1a":"This is the code used in[\u3010python\u3011How to make a CAE surrogate model by AI](https:\/\/hasimoto-soken.com\/%e3%80%90%e5%ae%9f%e8%b7%b5python%e3%80%91ai%e3%81%ab%e3%82%88%e3%82%8bcae%e3%82%b5%e3%83%ad%e3%82%b2%e3%83%bc%e3%83%88%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e4%bd%9c%e3%82%8a%e6%96%b9\/)<br>\nI intend to implement LightGBM as simply as possible, but I would be grateful if you could comment if there are any points that can be improved.<br>"}}