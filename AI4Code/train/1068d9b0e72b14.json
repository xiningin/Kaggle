{"cell_type":{"79c7b8ab":"code","4748df1c":"code","ad8205d8":"code","634b555c":"code","42c667b7":"code","2efb6e9d":"code","18016fc7":"code","10e39700":"code","576a609a":"code","08a958a7":"code","d7d25b1b":"code","a9466008":"code","05c6618a":"code","3550332f":"code","1cb2724e":"code","6347ddfa":"code","bfb67ed1":"code","ec68e936":"code","08de9dce":"code","f9bdcf47":"code","11c9806e":"code","7f066ee5":"code","80bd065d":"code","8fcc3057":"code","fcd76606":"code","792e4cc7":"code","cf0d5baa":"code","a9e60cd6":"code","231adab8":"code","41a45025":"code","c9512024":"code","16b3bad1":"code","2705fa74":"code","0aff2ff4":"code","c9ba343d":"code","b2f9d0aa":"code","2725ba5b":"code","7b123dc2":"code","0f4c9adb":"code","739aba94":"code","a314bd11":"code","4c02c2ef":"code","a41879e0":"code","46dfbdf6":"code","ad27b943":"code","338750e9":"code","2ce72c65":"code","16aa3083":"code","7301d0d8":"code","7ff5b8a5":"code","712147c2":"code","eebbf651":"code","2331b2ad":"code","dff2640a":"code","f151a355":"code","e4d4209c":"code","2833b37e":"code","fb4015c8":"code","01a8733c":"code","0a4c4519":"code","409d0c05":"code","c92ea4b3":"code","72b1e522":"code","9758956f":"code","50ab81ba":"code","3d6d54b1":"code","ee1d2316":"code","6c4ed5ed":"code","0b616bac":"code","08e51874":"code","005cb742":"code","2c45f882":"code","7789e411":"code","c95de77c":"code","bd1a2bc1":"code","23be0339":"code","6789a6e1":"code","7083cf17":"markdown","0644ebd7":"markdown","360fcc04":"markdown","5ead9ea3":"markdown","45e9673a":"markdown","e4165bf9":"markdown","94896982":"markdown","1458c2ee":"markdown","1975c647":"markdown","2708b49d":"markdown","75ae021b":"markdown","24d8d5b4":"markdown","228ea16a":"markdown","b5ef8c90":"markdown","54dad3a6":"markdown","0b36de61":"markdown"},"source":{"79c7b8ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4748df1c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score","ad8205d8":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_train.head()","634b555c":"df_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_test.head()","42c667b7":"df_train.info()","2efb6e9d":"df_test.info()","18016fc7":"df_train.shape","10e39700":"df_train.isnull().sum()","576a609a":"df_train.Age.isnull().sum()\/df_train.Age.isnull().count()","08a958a7":"df_train.Cabin.isnull().sum()\/df_train.Cabin.isnull().count()","d7d25b1b":"# Replacing the null values in age column with median score and dropping the cabin column as null values are greater than 50%\n\ndf_train.Age = df_train.Age.fillna(df_train.Age.median())\ndf_train.Embarked = df_train.Embarked.fillna(df_train.Embarked.mode()[0])\n\ndf_train.drop(\"Cabin\" , axis=1 , inplace=True)","a9466008":"df_train.isnull().sum()","05c6618a":"#Performing the above steps on test data\n\ndf_test.Age = df_test.Age.fillna(df_test.Age.median())\ndf_test.Fare = df_test.Fare.fillna(df_test.Fare.median())\n\ndf_test.drop(\"Cabin\" , axis=1 , inplace=True)","3550332f":"df_test.isnull().sum()","1cb2724e":"#Creating a new feature varibale\n\ndf_train[\"Family\"] = np.where(df_train[\"SibSp\"]+df_train[\"Parch\"] > 0 , 1 , 0)\ndf_train.drop(\"SibSp\" , axis=1 , inplace=True)\ndf_train.drop(\"Parch\" , axis=1 , inplace=True)\n\n#Renaming the fields \n\ndf_train.Pclass = df_train.Pclass.map({3 : \"Lower\" , 2 : \"Middle\" , 1 : \"Upper\"})\ndf_train.Age = pd.cut(df_train.Age , [0 , 16 , 24 , 55 , 75] , labels = [ \"Minor\" , \"Youths\" , \"Adults\" , \"Senior Citizen\"])","6347ddfa":"#Performing the above steps on test data\n\ndf_test[\"Family\"] = np.where(df_test[\"SibSp\"]+df_test[\"Parch\"] > 0 , 1 , 0)\ndf_test.drop(\"SibSp\" , axis=1 , inplace=True)\ndf_test.drop(\"Parch\" , axis=1 , inplace=True)\n\n\ndf_test.Pclass = df_test.Pclass.map({3 : \"Lower\" , 2 : \"Middle\" , 1 : \"Upper\"})\ndf_test.Age = pd.cut(df_test.Age , [0 , 16 , 24 , 55 , 75] , labels = [ \"Minor\" , \"Youths\" , \"Adults\" , \"Senior Citizen\"])","bfb67ed1":"plt.figure(figsize =(15,10))\nplt.subplot(3,3,1)\nsns.countplot(data= df_train , x = \"Survived\")\nplt.xticks([0,1] , [\"Drowned\" , \"Survived\"])\nplt.subplot(3,3,2)\nsns.countplot(data= df_train , x = \"Pclass\")\nplt.subplot(3,3,3)\nsns.countplot(data= df_train , x = \"Sex\")\nplt.subplot(3,3,4)\nsns.countplot(data= df_train , x = \"Age\")\nplt.subplot(3,3,5)\nsns.countplot(data= df_train , x = \"Embarked\")\nplt.subplot(3,3,6)\nsns.countplot(data= df_train , x = \"Family\")\nplt.xticks([0,1] , [\"No\" , \"Yes\"])\nplt.show()","ec68e936":"plt.figure(figsize = [15,10])\nplt.subplot(2,2,1)\nsns.countplot(x = \"Embarked\" , hue=\"Survived\" , data = df_train )\nplt.title(\"Port of Embarkation\")\n\nplt.subplot(2,2,2)\nsns.countplot(x=\"Sex\" , hue = \"Survived\" , data=df_train)\nplt.title(\"Sex of Passenger\")\n\nplt.subplot(2,2,4)\nsns.countplot(x = \"Pclass\" , hue=\"Survived\" , data=df_train)\nplt.title(\"Passenger class\")\n\nplt.subplot(2,2,3)\nsns.countplot(x=\"Family\" , hue=\"Survived\" , data=df_train)\nplt.xticks([0,1] , [\"No\" , \"Yes\"])\nplt.title(\"Presence of family\")\nplt.show()","08de9dce":"df_train.groupby(\"Age\")[\"Survived\"].value_counts(normalize =True).unstack().plot(kind = \"bar\" , stacked = True)\nplt.title(\"Age-wise Distribution\")\nplt.ylabel(\"Number of passengers\")\nplt.show()","f9bdcf47":"sns.catplot(x =\"Age\" , y =\"Fare\" , data=df_train , kind=\"box\" )\nplt.show()","11c9806e":"sns.catplot(x =\"Pclass\" , y =\"Fare\" , data=df_train , kind=\"box\" )\nplt.show()","7f066ee5":"sns.catplot(x =\"Embarked\" , y =\"Fare\" , data=df_train , kind=\"box\" )\nplt.show()","80bd065d":"#creating dummies for categorical variables with more than 2 categories\n\ndata_train = pd.get_dummies(df_train , columns = [\"Pclass\" , \"Embarked\" , \"Sex\" , \"Age\"] , drop_first =True )\nfinal_data_train= data_train\n\n#Dropping irrelavent columns\n\nfinal_data_train.drop(\"Name\" , axis = 1 , inplace=True)\nfinal_data_train.drop(\"Ticket\" , axis = 1 , inplace=True)\nfinal_data_train.drop(\"PassengerId\" , axis = 1 , inplace=True)","8fcc3057":"final_data_train.head()","fcd76606":"#Performing the above steps on test data \ndata_test = pd.get_dummies(df_test , columns = [\"Pclass\" , \"Embarked\" , \"Sex\" , \"Age\"] , drop_first = True)\n\nfinal_data_test= data_test\n\nfinal_data_test.drop(\"Name\" , axis = 1 , inplace=True)\nfinal_data_test.drop(\"Ticket\" , axis = 1 , inplace=True)\nfinal_data_test.drop(\"PassengerId\" , axis = 1 , inplace=True)","792e4cc7":"final_data_test.head()","cf0d5baa":"y = final_data_train[\"Survived\"]\nX = final_data_train[final_data_train.columns[1:]]","a9e60cd6":"#Spliting the initial training data into test and train data set for modelling\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , train_size = 0.7 , test_size = 0.3 ,random_state = 100)","231adab8":"#Scaling the numerical column so that the values are comparable \n#fit and transform is performed on train data set \n\nscaler = MinMaxScaler()\nX_train[[\"Fare\"]] = scaler.fit_transform(X_train[[\"Fare\"]])\n\n#for test data set only tranform is performed\nX_test[[\"Fare\"]] = scaler.transform(X_test[[\"Fare\"]])","41a45025":"plt.figure(figsize = (15,10))\nsns.heatmap(final_data_train.corr() , cmap = \"Greens\" ,annot=True)\nplt.show()","c9512024":"#Model 1 \n\nmod1 = sm.GLM(y_train , (sm.add_constant(X_train)) , family = sm.families.Binomial())\nmod1.fit().summary()","16b3bad1":"col = X_train.columns\n\n#Calculating the VIF score for model 1\n\nvif = pd.DataFrame()\nvif[\"Features\"] = X_train[col].columns\nvif[\"VIF\"] = [variance_inflation_factor(X_train[col].values , i) for i in range(X_train[col].shape[1])]\nvif[\"VIF\"] = round(vif[\"VIF\"] ,2)\nvif = vif.sort_values(by = \"VIF\" , ascending = False )\nvif","2705fa74":"col1 = col.drop(\"Fare\" , 1)","0aff2ff4":"#Model 2 \n\nmod2 = sm.GLM(y_train , sm.add_constant(X_train[col1]) , family = sm.families.Binomial())\nmod2.fit().summary()","c9ba343d":"vif = pd.DataFrame()\nvif[\"Features\"] = X_train[col1].columns\nvif[\"VIF\"] = [variance_inflation_factor(X_train[col1].values , i) for i in range(X_train[col1].shape[1])]\nvif[\"VIF\"] = round(vif[\"VIF\"] ,2)\nvif = vif.sort_values(by = \"VIF\" , ascending = False )\nvif","b2f9d0aa":"col2 = col1.drop(\"Family\" , 1)","2725ba5b":"#Model 3\n\nmod3 = sm.GLM(y_train , sm.add_constant(X_train[col2]) , family = sm.families.Binomial())\nmod3.fit().summary()","7b123dc2":"vif = pd.DataFrame()\nvif[\"Features\"] = X_train[col2].columns\nvif[\"VIF\"] = [variance_inflation_factor(X_train[col2].values , i) for i in range(X_train[col2].shape[1])]\nvif[\"VIF\"] = round(vif[\"VIF\"] ,2)\nvif = vif.sort_values(by = \"VIF\" , ascending = False )\nvif","0f4c9adb":"col3 = col2.drop(\"Embarked_Q\" , 1)","739aba94":"#Model 4\n\nmod4 = sm.GLM(y_train , sm.add_constant(X_train[col3]) , family = sm.families.Binomial())\nmod4.fit().summary()","a314bd11":"vif = pd.DataFrame()\nvif[\"Features\"] = X_train[col3].columns\nvif[\"VIF\"] = [variance_inflation_factor(X_train[col3].values , i) for i in range(X_train[col3].shape[1])]\nvif[\"VIF\"] = round(vif[\"VIF\"] ,2)\nvif = vif.sort_values(by = \"VIF\" , ascending = False )\nvif","4c02c2ef":"res = mod4.fit()","a41879e0":"#predicting the survival proability\n\nX_train_sm = sm.add_constant(X_train[col3])\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)","46dfbdf6":"y_train_pred_final = pd.DataFrame({\"Survived_actual\" : y_train.values , \"Survival_Prob\" : y_train_pred })\ny_train_pred_final[\"Passenger ID\"] = y_train.index\ny_train_pred_final.head()","ad27b943":"# Drawing the ROC curve to find if the model is good or  not\n\n\ndef draw_roc(actual , probs):\n    fpr , tpr , thresholds = metrics.roc_curve(actual , probs, drop_intermediate=False)\n    auc_score = metrics.roc_auc_score(actual,probs)\n    plt.figure(figsize = [10,5])\n    plt.plot(fpr , tpr , label='ROC curve (area = %0.2f)' % auc_score)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0 , 1.0])\n    plt.ylim([0.0 , 1.05])\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    \n    return None","338750e9":"fpr , tpr , thresholds = metrics.roc_curve(y_train_pred_final.Survived_actual , y_train_pred_final.Survival_Prob , drop_intermediate = False)\nprint(\"False positive rate: \" , fpr)\nprint(\"True Positive rate: \" , tpr)\nprint(\"Threshold value :\" , thresholds)","2ce72c65":"metrics.roc_auc_score(y_train_pred_final.Survived_actual , y_train_pred_final.Survival_Prob)","16aa3083":"#Plot of TPR vs FPR\n\ndraw_roc(y_train_pred_final.Survived_actual , y_train_pred_final.Survival_Prob)","7301d0d8":"# Predicting for different possible threshold values \n\nnumbers = [float(x\/10) for x in range(10)]\nfor i in numbers :\n    y_train_pred_final[i] = y_train_pred_final.Survival_Prob.map(lambda x : 1 if x > i else 0)\ny_train_pred_final.head()","7ff5b8a5":"cutoff_df = pd.DataFrame(columns = [\"prob\" , \"accuracy\" , \"sensitivity\" , \"specificity\"])\n\nfor i in numbers:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived_actual , y_train_pred_final[i])\n    accuracy = (cm1[0 ,0]+cm1[1,1])\/sum(sum(cm1))\n    sensitivity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    specificity = cm1[0,0]\/(cm1[0,1] +cm1[0,0])\n    cutoff_df.loc[i] = [i , accuracy, sensitivity , specificity]\n    \ncutoff_df.head()","712147c2":"#the point where the 3 values meet is taken as the optimal and best threshold value\n\ncutoff_df.plot.line(x=\"prob\" , y = [\"accuracy\" , \"sensitivity\" , \"specificity\"])\nplt.show()","eebbf651":"#As shown in the graph 0.4 is chosen as the optimal threshold value.\n\ny_train_pred_final[\"predicted\"] = y_train_pred_final.Survival_Prob.map(lambda x: 1 if x>0.3 else 0)\ny_train_pred_final.head()","2331b2ad":"metrics.accuracy_score(y_train_pred_final.Survived_actual , y_train_pred_final.predicted)","dff2640a":"conf_matrix = metrics.confusion_matrix(y_train_pred_final.Survived_actual , y_train_pred_final.predicted)\n\nTN = conf_matrix[0,0]\nFP = conf_matrix[0,1]\nFN = conf_matrix[1,0]\nTP = conf_matrix[1,1]","f151a355":"sensitivity = TP\/float(TP+FN)\nsensitivity","e4d4209c":"specificity = TN\/float(FP+TN)\nspecificity","2833b37e":"precision = TP\/float(TN+TP)\nprecision","fb4015c8":"recall = TP\/float(TP+FN)\nrecall","01a8733c":"X_test_sm = sm.add_constant(X_test[col3])\nX_test_sm.head()","0a4c4519":"y_test_pred = res.predict(X_test_sm)\ny_test_data_pred = pd.DataFrame(y_test_pred)\ny_test_data_pred[\"Passenger_id\"] = df_train.PassengerId\ny_test_data_pred.rename(columns = { 0 :\"survival_prob\"} , inplace =True)\ny_test_data_pred.head()","409d0c05":"y_test_data_pred[\"pred_test\"] = y_test_data_pred.survival_prob.map(lambda x: 1 if x>0.3 else 0 )\ny_test_data_pred[\"original\"] = df_train.Survived\ny_test_data_pred.head()","c92ea4b3":"metrics.accuracy_score(y_test_data_pred.original , y_test_data_pred[\"pred_test\"])","72b1e522":"conf_matrix = metrics.confusion_matrix(y_test_data_pred.original , y_test_data_pred[\"pred_test\"])\n\nTN = conf_matrix[0,0]\nFP = conf_matrix[0,1]\nFN = conf_matrix[1,0]\nTP = conf_matrix[1,1]","9758956f":"sensitivity = TP\/float(TP+FN)\nsensitivity","50ab81ba":"specificity = TN\/float(TN+FP)\nspecificity","3d6d54b1":"precision_score(y_test_data_pred.original , y_test_data_pred[\"pred_test\"])","ee1d2316":"recall_score(y_test_data_pred.original , y_test_data_pred[\"pred_test\"])","6c4ed5ed":"f1_score(y_test_data_pred.original , y_test_data_pred[\"pred_test\"])","0b616bac":"#first we transform the numerical variable \nfinal_data_test[[\"Fare\"]] = scaler.transform(final_data_test[[\"Fare\"]])","08e51874":"gender = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ngender.head()","005cb742":"final_test_sm = sm.add_constant(final_data_test[col3])\nfinal_test_pred = res.predict(final_test_sm)\nfinal_result = pd.DataFrame(final_test_pred)\n\nfinal_result[\"Passenger_id\"] = gender.PassengerId\nfinal_result.head()","2c45f882":"final_result.rename(columns = { 0 :\"survival_prob\"} , inplace =True)\nfinal_result[\"pred_test\"] = final_result.survival_prob.map(lambda x: 1 if x>0.4 else 0 )\nfinal_result[\"original\"] = gender.Survived\nfinal_result.head()","7789e411":"metrics.accuracy_score(final_result.original , final_result[\"pred_test\"])","c95de77c":"f1_score(final_result.original , final_result[\"pred_test\"])","bd1a2bc1":"#Final Result\n\nresult = final_result\nresult.drop([\"survival_prob\" , \"original\"] ,axis =1 , inplace =True)\nresult.rename(columns = {\"Passenger_id\" : \"PassengerId\" , \"pred_test\" : \"Survived\"} , inplace =True)\nresult.head()","23be0339":"result.to_csv('.\/submission.csv' , index = False , header =True)","6789a6e1":"submission = pd.read_csv(\"submission.csv\")\nsubmission.head()","7083cf17":"1. Here we can see that the passengers who have bought tickets of lower class have higher chances of not surviving\n2. The passenger with embarkation port \"S\" can see large number non-survived passengers","0644ebd7":"Implimenting the model on the unseen test data ie: test.csv","360fcc04":"The age group of people who are senior citizen are the highly affected one","5ead9ea3":"Family column has a high p-value hence dropping it","45e9673a":"# **Model Building using Logistic Regression**","e4165bf9":"Before we move further lets look at some of the model evalulation parameters to get a better understanding.\n\n1. True Positive(TP) - Passengers who survived and also predicted as being survived \n2. True Negative(TN) - Passengers who did not survive and also predicted as not being survived\n3. False Positive(FP) - Passengers who actually survived but were predicted as not being survived \n4. False Negative(FN) - Passengers who actually were not survived but were predicted as survived\n\n\nBased on these we can calulate two important metrics\n\n1. Sensitivity = TP\/(TP+FN)\n     It tells of all the actual poistives how many were correctly predicted\n     \n2. Specificity = TN\/(TN+FP)\n     It tells of all the actual negatives how many were correctly predicted\n     \nThe ROC(Reciever Operator characteristic can help us identify the best threshold value. It is done by pplotting the graph between TPR and FPR. We choose a threshold such that TPR is high and FPR is low.\n\n1. TPR(True Positive rate) = TP\/(TP+FN) \n   - It is similar to that of sensitivity and also kown as recall. Tells us what proportion of positive class was   correctly predicted \n    \n2. FPR(False Positive Rate) = FP\/(FP+TN) \n   - It tells us what proportion of negative class was incorrectly predicted. It is also written as (1-Specificity)\n   ","94896982":"As we can see model4 looks fine will all p-values in range and accpetable VIF values as well ","1458c2ee":"Age_Adults has high VIF but less p value, hence we choose to drop the variable with high p-value that is greater than 0.05. Dropping \"Fare\" column","1975c647":"# **Data Preprocessing**","2708b49d":"The Embarked_Q column has a large p-value hence dropping it and rebuiling the model","75ae021b":"Here is a small attempt in the very famous titanic case study. Do post your reviews and commets below and upvote if you liked it!! \nThanks in advance..","24d8d5b4":"The fare amount says that the passengers with \"C\" embarked port have had a higher compared to passengers from other port. Likewise\nthe upper class ticket were comparitively higher\n","228ea16a":"# **Testing model on test data set**","b5ef8c90":"1. Here we can see that the number of drowned people are more than that who survived\n2. We can see large number of people with lower class ticket\n3. Most of the passengers who travelled did not have a family\n4. Male passengers were high in number as compared to that of females","54dad3a6":"# **Data Preparation**","0b36de61":"# **Exploratory data analysis**"}}