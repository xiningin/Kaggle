{"cell_type":{"afe7f6e9":"code","7c5b6a4f":"code","c6d06d03":"code","b5fbe4d9":"code","6fc249b7":"code","ad5e2f84":"code","3bf8a6d6":"code","3605f56f":"code","33f9358d":"code","f593fb31":"code","f9494800":"code","bdcd92a7":"code","1c7d574c":"code","3f5d2f51":"code","f288a622":"code","e0d6f443":"code","f25e809a":"code","6593a1d2":"code","cdab45a4":"code","c0ade816":"code","0a4ea4cd":"code","3eade10b":"code","0bbdd631":"code","b7879a23":"code","57bf0795":"code","351d1ac2":"code","99562a7c":"code","c92907e6":"code","8ca7b692":"code","f2bb55e4":"code","e97c37aa":"code","6c15d03b":"code","70d78d85":"code","f0a8fc3e":"code","f111ea4b":"code","c70cccc2":"code","daf83643":"code","127c6afb":"code","cfcdc350":"code","93a8e0d5":"markdown","f6cad175":"markdown","678f3952":"markdown","1987a7fd":"markdown","c367c5fc":"markdown","cd078344":"markdown","afe27ebc":"markdown","d735c363":"markdown","969218ff":"markdown","1681f929":"markdown","3dbaf8f5":"markdown","3603c1bd":"markdown","5e58ef89":"markdown","bd855319":"markdown","46b78a12":"markdown","b06dfd5a":"markdown","ae83a464":"markdown","bae2531e":"markdown","95f58738":"markdown","bc25e55f":"markdown","947073bf":"markdown","92fff2e8":"markdown","63230a6f":"markdown"},"source":{"afe7f6e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c5b6a4f":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize']=[16,6]\nimport seaborn as sns","c6d06d03":"df=pd.read_csv('..\/input\/insurance\/insurance.csv')","b5fbe4d9":"df.head(2)","6fc249b7":"df.shape","ad5e2f84":"# percentile list\nperc =[.20, .40, .60, .80]\n# list of dtypes to include\ninclude =['object', 'float', 'int']\ndf.describe(percentiles=perc,include=include)","3bf8a6d6":"df.isna().sum()","3605f56f":"#by default drop_first=True\ndf = pd.get_dummies(df, drop_first=True)# Get dummies will return OHE columns\ndisplay(df.shape)\ndf.head()","33f9358d":"y=df.pop('smoker_yes')\nX=df","f593fb31":"from sklearn.ensemble import ExtraTreesRegressor","f9494800":"fea_model=ExtraTreesRegressor()\nfea_model.fit(X,y)","bdcd92a7":"plt.figure(figsize=(18,8))\nplt.xticks(rotation=45)\nsns.barplot(x=df.columns, y=fea_model.feature_importances_);","1c7d574c":"X1=X # for future use\nX=X[['age','bmi','children','charges']]\nX.head()","3f5d2f51":"from sklearn.model_selection import train_test_split","f288a622":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e0d6f443":"display(X_train.shape, X_test.shape)\nX_train.head(2)","f25e809a":"display(y_train.shape, y_test.shape)\ny_train.head(2)","6593a1d2":"from sklearn.linear_model import LinearRegression","cdab45a4":"model=LinearRegression()","c0ade816":"#Fit the regressor to the training data\nmodel.fit(X_train, y_train)","0a4ea4cd":"display(model.coef_)\ndisplay(model.intercept_)","3eade10b":"y_predict=model.predict(X_test)","0bbdd631":"from sklearn.metrics import mean_squared_error\nimport math","b7879a23":"score1 = model.score(X_test, y_test)\ncorrelation1= math.sqrt(model.score(X_test, y_test))\nrmse1 = np.sqrt(mean_squared_error(y_test, y_predict))\n# Compute and print RMSE\nprint(\"Linear regression score : \",score1)\nprint('Correlation: ',correlation1)\nprint(\"Root Mean Squared Error: {}\".format(rmse1))","57bf0795":"from sklearn import model_selection","351d1ac2":"kfold = model_selection.KFold(n_splits=10)\nresults = model_selection.cross_val_score(model, X,y, cv=kfold)\nprint(results.mean())","99562a7c":"sns.boxplot(x=y_test,y=y_predict,hue=y_test)\nplt.title('Compare y-test data and y-predicted by linear Regression Important Feature',fontsize=24)\nplt.xlabel('y-test data',fontsize=24)\nplt.ylabel('y-predict data',fontsize=24)","c92907e6":"X=X1\nX.head()","8ca7b692":"# We are following same previous step but now included all feature for x_train \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel=LinearRegression()\nmodel.fit(X_train, y_train)\ny_predict=model.predict(X_test)\n\n###\nscore2 = model.score(X_test, y_test)\ncorrelation2 = math.sqrt(model.score(X_test, y_test))\nrmse2 = np.sqrt(mean_squared_error(y_test, y_predict))\n# Compute and print RMSE\nprint(\"Linear regression score : \",score2)\nprint('Correlation: ',correlation2)\nprint(\"Root Mean Squared Error: {}\".format(rmse2))\n\nkfold = model_selection.KFold(n_splits=10)\nresults = model_selection.cross_val_score(model, X,y, cv=kfold)\nprint('Model Selection',results.mean())\n\n###\nsns.boxplot(x=y_test,y=y_predict,hue=y_test)\nplt.title('Compare y-test data and y-predicted by linear Regression All Features',fontsize=24)\nplt.xlabel('y-test data',fontsize=24)\nplt.ylabel('y-predict data',fontsize=24)\n","f2bb55e4":"print (score2- score1) # \"Linear regression score\nprint(correlation2-correlation1) #'Correlation\nprint(rmse2-rmse1) #Root Mean Squared Error","e97c37aa":"# #sample example\n# arr = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n# print(arr)\n# print(arr[0:2, 1:4])\n# print(arr[:, :-1])\n# print(arr[:, :-1][:1])\n##\n# x = np.array([48,34,0,1])\n# print(x.argsort())  # return sorted number index in sort order","6c15d03b":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=100, max_depth=10, max_features=0.3, n_jobs=-1, random_state=0)","70d78d85":"X.head(2)","f0a8fc3e":"model.fit(X,y)","f111ea4b":"#plot the importances #\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],\n             axis=0)\nindex = np.argsort(importances)","c70cccc2":"display('Feature count',model.n_features_in_)\ndisplay('importances ',model.feature_importances_)\ndisplay('model.estimators_',model.estimators_[:3])\ndisplay('std',std)\ndisplay('indices',index)","daf83643":"plt.figure(figsize=(16,6))\nplt.title(\"Feature importances\")\nplt.bar(index, importances[index],color=\"b\", yerr=std[index], align=\"center\")\nplt.show()","127c6afb":"from sklearn import model_selection","cfcdc350":"kfold = model_selection.KFold(n_splits=10)\nresults = model_selection.cross_val_score(model, X,y, cv=kfold)\nprint(results.mean())","93a8e0d5":"# 1. Selecting important Features only","f6cad175":"# Root Mean Squared Error","678f3952":"# Model Building: Linear Regression - from sklearn.linear_model import LinearRegression","1987a7fd":"# Extra Trees Regressor - 2.0\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesRegressor.html","c367c5fc":"# Load","cd078344":"# Reference\n\nhttps:\/\/www.kaggle.com\/rahulvks\/extratreesregressor-python","afe27ebc":"# Train-Test split - from sklearn.model_selection import train_test_split","d735c363":"# 2. Selecting All Features ","969218ff":"# One Hot Encoding categorical columns","1681f929":"# Model Selection - from sklearn import model_selection","3dbaf8f5":"# What is the difference between extra tree and random forest?\n\nRandom Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization.","3603c1bd":"# Setting Smoker as Target","5e58ef89":"n_estimators int, default=100  The number of trees in the forest.\n\nmax_depth int, default=None If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\nmax_features{\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, int or float, default=\u201dauto\u201d The number of features to consider when looking for the best split\n\nn_jobs int, default=None The number of jobs to run in parallel. \n","bd855319":"# Import ","46b78a12":"we think sex and region as not important feature in First Process \n\nbut above solution prove it is not true they also need for better prediction","b06dfd5a":"# what is ExtraTreesRegressor?\n\nExtraTreesClassifier is an ensemble learning method fundamentally based on decision trees. ExtraTreesClassifier, like RandomForest, randomizes certain decisions and subsets of data to minimize over-learning from the data and overfitting.\n","ae83a464":"As we seen already same result we are getting here also 3 feature charges have highest performance","bae2531e":"# Checking Feature importance - from sklearn.ensemble import ExtraTreesRegressor","95f58738":"# Initial View","bc25e55f":"# How does extra tree classifier work?\n\nThe Extra Trees algorithm works by creating a large number of unpruned decision trees from the training dataset. Predictions are made by averaging the prediction of the decision trees in the case of regression or using majority voting in the case of classification.\n","947073bf":"# Clean","92fff2e8":"# 1 & 2 Comparing - Selected Feature Vs All Feature ","63230a6f":"From above we get that Charges , age ,bmi ,children  are the main key feature "}}