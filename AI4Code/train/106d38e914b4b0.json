{"cell_type":{"cfaae052":"code","36a03e0f":"code","860bc4d3":"code","c0ce6785":"code","759a3693":"code","8b2730ec":"code","f9b2f8b6":"code","7a0a0877":"code","d920ac8a":"code","c9bc1a10":"code","e134733b":"code","a74b94eb":"code","26cef5b4":"code","0d4f94eb":"code","d64f3e0c":"code","f4c5dc27":"code","bc2e605c":"code","d4dfe0a1":"code","b9218fcb":"code","0c6ff7a1":"code","7ec13349":"code","c57bfff3":"code","a09fbf8e":"code","656ac134":"code","5221784d":"code","bf1fdc95":"code","6d3e58e8":"code","34a7f903":"code","40a9c26b":"code","f4087f9a":"code","5711af43":"code","e3e52e87":"code","9a7a1e6e":"code","15cd0a18":"code","47b082b2":"code","0c7ba2da":"markdown","b8c8fc1a":"markdown","0083c9d5":"markdown","18def266":"markdown","85bd4b90":"markdown","40a266c8":"markdown","79d16fd9":"markdown","e7fe8900":"markdown","536f3b82":"markdown","c06fd5a1":"markdown","6329de79":"markdown"},"source":{"cfaae052":"import pandas as pd \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport gensim\nimport gensim.corpora as corpora\nimport re\nimport numpy as np\nfrom pprint import pprint\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)  ","36a03e0f":"papers = pd.read_csv('..\/input\/201812_CL_Github.csv')","860bc4d3":"papers.head()","c0ce6785":"papers.shape\n#Total 106 papers given","759a3693":"papers.info()","8b2730ec":"#Removing symbols from Abstracts\n\npapers['Abstract_Cleaned'] = papers.apply(lambda row: (re.sub(\"[^A-Za-z0-9' ]+\", ' ', row['Abstract'])),axis=1)","f9b2f8b6":"# Tokenization\n\npapers['Abstract_Cleaned'] = papers.apply(lambda row: (word_tokenize(row['Abstract_Cleaned'])), axis = 1)","7a0a0877":"# Removing Stopwords\n\nstop_words = set(stopwords.words('english'))\npapers['Abstract_Cleaned'] = papers.apply(lambda row: ([w for w in row['Abstract_Cleaned'] if w not in stop_words]),axis=1)","d920ac8a":"# Lemmatization\n\nlmtzr = WordNetLemmatizer()\npapers['Abstract_Cleaned'] = papers.apply(lambda row: ([lmtzr.lemmatize(w) for w in row['Abstract_Cleaned']]), axis=1)","c9bc1a10":"papers['Abstract_Cleaned'][1][:20]","e134733b":"# Creating Dictionary and Corpus\n\ndictionary = corpora.Dictionary(papers['Abstract_Cleaned'])\ntexts = papers['Abstract_Cleaned']\ncorpus = [dictionary.doc2bow(text) for text in papers['Abstract_Cleaned']]","a74b94eb":"# Building LDA Model\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=10, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","26cef5b4":"# Printing the Keywords in topics\n\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","0d4f94eb":"# Visualizing topics using pyLDAvis\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\nvis","d64f3e0c":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\nabstracts = papers['Abstract'].values\n\ncount_vectorizer = CountVectorizer()\ncounts = count_vectorizer.fit_transform(abstracts)\ntfidf_vectorizer = TfidfTransformer().fit(counts)\ntfidf_abstracts = tfidf_vectorizer.transform(counts)","f4c5dc27":"tfidf_abstracts.shape","bc2e605c":"#Testing affinity propogation on abstracts tfidf(experimental)\n\nfrom sklearn.cluster import AffinityPropagation\n\nX = tfidf_abstracts\nclustering = AffinityPropagation().fit(X)\nclustering ","d4dfe0a1":"abstract_affinity_clusters = list(clustering.labels_)\nabstract_affinity_clusters","b9218fcb":"len(set(abstract_affinity_clusters))","0c6ff7a1":"# Building LDA Model\n\nlda_model_17 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=17, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","7ec13349":"# Visualizing topics using pyLDAvis\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model_17, corpus, dictionary)\nvis","c57bfff3":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda_model = LatentDirichletAllocation(n_topics=9, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf_abstracts)\nlda_W = lda_model.transform(tfidf_abstracts)\nlda_H = lda_model.components_\n#Not sure how to find lda_W & lda_H using gensim lda model","a09fbf8e":"def display_topics(H, W, feature_names, title_list, no_top_words, no_top_documents):\n    for topic_idx, topic in enumerate(H):\n        print('\\n',\"Topic %d:\" % (topic_idx))\n        print(\"Top Words: \",\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n        for doc_index in top_doc_indices:\n            print(title_list[doc_index])\n            \nno_top_words = 15\nno_top_documents = 4     \ntitle_list = papers['Title'].tolist()\ntf_feature_names = count_vectorizer.get_feature_names()\ndisplay_topics(lda_H, lda_W, tf_feature_names, title_list, no_top_words, no_top_documents)\n","656ac134":"from sklearn.decomposition import NMF\n\nnmf_model = NMF(n_components=9, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf_abstracts)\nnmf_W = nmf_model.transform(tfidf_abstracts)\nnmf_H = nmf_model.components_","5221784d":"display_topics(nmf_H, nmf_W, tf_feature_names, title_list, no_top_words, no_top_documents)\n","bf1fdc95":"from sklearn.metrics.pairwise import cosine_similarity, paired_cosine_distances","6d3e58e8":"tfidf_abstracts_sim = []\n\nfor i in range(0,tfidf_abstracts.shape[0]):\n    sim_array = cosine_similarity(tfidf_abstracts,tfidf_abstracts[i])\n    sim_array = sim_array.flatten()\n    tfidf_abstracts_sim.append(sim_array)\n    \ntfidf_abstracts_sim[0]","34a7f903":"tfidf_abstracts_sim_matrix =  np.array(tfidf_abstracts_sim)\ntfidf_abstracts_sim_matrix.shape","40a9c26b":"import seaborn as sns\n\nsns.set(rc={'figure.figsize':(18.7,15.27)})\nsns.heatmap(tfidf_abstracts_sim_matrix)","f4087f9a":"!pip install corextopic","5711af43":"from corextopic import corextopic as ct","e3e52e87":"anchors = []\nmodel = ct.Corex(n_hidden=5, seed=36)\nmodel = model.fit(\n    tfidf_abstracts,\n    words=tf_feature_names\n)","9a7a1e6e":"for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))","15cd0a18":"# Anchors designed to nudge the model towards measuring specific genres\nanchors = [\n    [\"adversarial\",\"anomaly\"],\n    [\"entity extraction\",\"ner\"],\n    [\"fake\",\"news\"],\n    [\"bias\",\"dimensionality\"],\n    [\"toxic\",\"attack\"]\n]\nanchors = [\n    [a for a in topic if a in tf_feature_names]\n    for topic in anchors\n]\n\nmodel = ct.Corex(n_hidden=5, seed=40)\nmodel = model.fit(\n    tfidf_abstracts,\n    words=tf_feature_names,\n    anchors=anchors, # Pass the anchors in here\n    anchor_strength=3 # Tell the model how much it should rely on the anchors\n)","47b082b2":"for i, topic_ngrams in enumerate(model.get_topics(n_words=10)):\n    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))","0c7ba2da":"### Introduction\nIn this notebook I am exploring few NLP papers dataset. I am doing text analysis on paper Abstracts & Titles using nltk for text preprocessing, Topic modeling using LDA & NMF, Paper similarity analysis using TF-IDF vectors. \n\nPlease share your valuable feedback & upvote if you learn something new today from this analysis. \n","b8c8fc1a":"#### Data Preprocessing","0083c9d5":"#### Mapping Top Topics & Abstracts","18def266":"But above visualization shows 9 dominant topic clusters","85bd4b90":"#### Affinity Propogation","40a266c8":"#### Abstracts TF-IDF\n","79d16fd9":"Lets check paper titles whose abstracts have maximum correspondence with top topic words, took below ideas from https:\/\/towardsdatascience.com\/improving-the-interpretation-of-topic-models-87fd2ee3847d","e7fe8900":"Lets try topic modeling again with number of topics equal to clusters found by affinity propogation algorithm","536f3b82":"#### Visualizing Similarity among Abstracts","c06fd5a1":"Testing a new semi-supervised learning approach to overcome limitations of Topic Models. This approach was published on medium blog: https:\/\/medium.com\/pew-research-center-decoded\/overcoming-the-limitations-of-topic-models-with-a-semi-supervised-approach-b947374e0455\n\nThis approach is based on ideas from paper \"Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge\" which were implemented in open source project \"corextopic\". ","6329de79":"#### Building LDA Topic Model"}}