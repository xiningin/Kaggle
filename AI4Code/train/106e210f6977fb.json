{"cell_type":{"d1452d5a":"code","d3f603f8":"code","5bcde0a9":"code","7cdeb68b":"code","dc206971":"code","c9c81db4":"code","152c9b48":"code","14936b12":"code","f78cb518":"code","1e77f3f2":"code","083b8306":"code","f0988f51":"code","c0638e5b":"code","d96a5414":"code","1eecbe22":"code","e7274d28":"code","376bcd8a":"code","eb824a6d":"code","e085c984":"code","03d38b66":"code","7aaee02c":"code","3a05c1f1":"code","08b20ae8":"code","93f5b2d7":"code","9903f77b":"code","57f83a91":"code","4d567ec7":"code","20bf467b":"code","c954b294":"code","c75c090a":"code","a3ee1b85":"code","4bfc7792":"code","fb865d30":"code","8a05ba8f":"code","56560f9b":"code","d6686a4e":"code","a356cece":"code","ac749da7":"code","0af5e7e2":"markdown","37177a8b":"markdown","3f6ca9e2":"markdown","054f74dd":"markdown","5e49b4e8":"markdown","36e342ad":"markdown","c6e8c4b4":"markdown","cb0e4a35":"markdown","9e950e38":"markdown","78e5cdc9":"markdown","1971b1bd":"markdown","35c7217b":"markdown","a97db036":"markdown","5bdf0cd3":"markdown","e3eff6d1":"markdown","af002a28":"markdown","b9fb67a5":"markdown","63714690":"markdown","c793654a":"markdown","7a0e6529":"markdown","c7fede2e":"markdown","928012c7":"markdown","5c7f095f":"markdown","13732f46":"markdown","6e8f6881":"markdown","7056fa38":"markdown","1ea50e4b":"markdown","160697f2":"markdown","0e5f5f28":"markdown","a73ca96e":"markdown","9a40c43d":"markdown","0fc0b88f":"markdown","7b30a58f":"markdown","fe5edcf8":"markdown","8d11c84a":"markdown","6c9dd2c7":"markdown","f90922ec":"markdown","2bffe63b":"markdown","c09f67ab":"markdown","ce7530e8":"markdown","d7f84e79":"markdown","4ebbf4c1":"markdown","a1ee1a7f":"markdown","8cd4fd03":"markdown","9f2e92c4":"markdown","8ca70db9":"markdown","c7f47cfd":"markdown"},"source":{"d1452d5a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML, display\nfrom IPython.core import display as ICD\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nimport numpy as np\nimport statsmodels.api as sm\nimport pylab\nimport scipy as sp\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import neighbors\nfrom sklearn import linear_model\n\ninit_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')","d3f603f8":"PATH = '..\/input\/'\nfilename = 'winequality-white.csv'\nwhite_data = pd.read_csv(PATH + filename)\n\ndata_head = white_data.head()\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')\ndf_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10,11]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')","5bcde0a9":"value_counts = white_data.quality.value_counts()\ntarget_counts = pd.DataFrame({'quality': list(value_counts.index), 'value_count': value_counts})","7cdeb68b":"plt.figure(figsize=(10,4))\ng = sns.barplot(x='quality', y='value_count', data=target_counts, capsize=0.3, palette='spring')\ng.set_title(\"Frequency of target class\", fontsize=15)\ng.set_xlabel(\"Quality\", fontsize=13)\ng.set_ylabel(\"Frequency\", fontsize=13)\ng.set_yticks([0, 500, 1000, 1500, 2000, 2500])\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()\/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n                textcoords='offset points', fontsize=14, color='black')","dc206971":"plt.figure(figsize=(10,3))\nsns.boxplot(data=white_data['quality'], orient='horizontal', palette='husl')\nplt.title(\"Distribution of target variable\")","c9c81db4":"white_data.describe().drop(columns=['quality'])\n\n# data_head = white_data.describe().drop(columns=['quality'])\n# data_head.columns = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n#        'chlorides', 'free_SO2', 'total_SO2', 'density',\n#        'pH', 'sulphates', 'alcohol']\n# colorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\n# df_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5,6,7,8,9,10]], 3), colorscale=colorscale)\n# py.iplot(df_table, filename='wine_quality')\n# df_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10]], 3), colorscale=colorscale)\n# py.iplot(df_table, filename='wine_quality')","152c9b48":"plt.figure(figsize=(10,10))\nsns.boxplot(data=white_data.drop(columns=['quality']), orient='horizontal', palette='husl')","14936b12":"y = white_data['quality']\nwhite_data = white_data.loc[:, ~white_data.columns.isin(['quality'])]\n\nscaler = MinMaxScaler()\nscaled_values = scaler.fit_transform(white_data)\nwhite_data.loc[:,:] = scaled_values\n\nwhite_data['quality'] = y","f78cb518":"data_head = white_data.head()\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5]], 3), colorscale=colorscale, )\npy.iplot(df_table, filename='wine_quality')\ndf_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10,11]], 3), colorscale=colorscale, )\npy.iplot(df_table, filename='wine_quality')","1e77f3f2":"columns = list(white_data.columns)\nnew_column_names = []\nfor col in columns:\n    new_column_names.append(col.replace(' ', '_'))\nwhite_data.columns = new_column_names","083b8306":"plt.figure(figsize=(10,10))\nsns.boxplot(data=white_data.drop(columns=['quality']), orient='horizontal', palette='husl')","f0988f51":"corr_matrix = white_data.corr().abs()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')","c0638e5b":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nf, axes = plt.subplots(4, 3, figsize=(15, 10), sharex=True)\nsns.distplot(features[\"fixed_acidity\"], rug=False, color=\"skyblue\", ax=axes[0, 0])\nsns.distplot(features[\"volatile_acidity\"], rug=False, color=\"olive\", ax=axes[0, 1])\nsns.distplot(features[\"citric_acid\"], rug=False, color=\"gold\", ax=axes[0, 2])\nsns.distplot(features[\"residual_sugar\"], rug=False, color=\"teal\", ax=axes[1, 0])\nsns.distplot(features[\"chlorides\"], rug=False, ax=axes[1, 1])\nsns.distplot(features[\"free_sulfur_dioxide\"], rug=False, color=\"red\", ax=axes[1, 2])\nsns.distplot(features[\"total_sulfur_dioxide\"], rug=False, color=\"skyblue\", ax=axes[2, 0])\nsns.distplot(features[\"density\"], rug=False, color=\"olive\", ax=axes[2, 1])\nsns.distplot(features[\"pH\"], rug=False, color=\"gold\", ax=axes[2, 2])\nsns.distplot(features[\"sulphates\"], rug=False, color=\"teal\", ax=axes[3, 0])\nsns.distplot(features[\"alcohol\"], rug=False, ax=axes[3, 1])","d96a5414":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.5)]","1eecbe22":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nsns.pairplot(features, diag_kind='kde', palette='husl', hue='quality')","e7274d28":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nsns.pairplot(features, vars=to_drop, diag_kind='kde', palette='husl', hue='quality')","376bcd8a":"model_reg = LinearRegression().fit(white_data.drop(columns=['quality']), y)\ny_true = white_data.quality\ny_pred = model_reg.predict(white_data.drop(columns=['quality']))","eb824a6d":"column_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nregression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': model_reg.coef_}, columns=['Feature', 'Coefficient'])","e085c984":"column_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\n\nplt.figure(figsize=(15,5))\ng = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')\ng.set_title(\"Contribution of features towards target variable\", fontsize=15)\ng.set_xlabel(\"Feature\", fontsize=13)\ng.set_ylabel(\"Degree of Coefficient\", fontsize=13)\ng.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])\ng.set_xticklabels(column_names)\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()\/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n               textcoords='offset points', fontsize=14, color='black')","03d38b66":"model_ols = ols(\"\"\"quality ~ fixed_acidity \n                        + volatile_acidity \n                        + citric_acid\n                        + residual_sugar \n                        + chlorides \n                        + free_sulfur_dioxide\n                        + total_sulfur_dioxide \n                        + density \n                        + pH \n                        + sulphates \n                        + alcohol\"\"\", data=white_data).fit()","7aaee02c":"model_summary = model_ols.summary()\nHTML(\n(model_ols.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: <\/th>')\n    .replace('<th>Model:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Model: <\/th>')\n    .replace('<th>Method:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Method: <\/th>')\n    .replace('<th>No. Observations:<\/th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: <\/th>')\n    .replace('<th>  R-squared:         <\/th>', '<th style=\"background-color:#aec7e8;\"> R-squared: <\/th>')\n    .replace('<th>  Adj. R-squared:    <\/th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: <\/th>')\n    .replace('<th>coef<\/th>', '<th style=\"background-color:#ffbb78;\">coef<\/th>')\n    .replace('<th>std err<\/th>', '<th style=\"background-color:#c7e9c0;\">std err<\/th>')\n    .replace('<th>P>|t|<\/th>', '<th style=\"background-color:#bcbddc;\">P>|t|<\/th>')\n    .replace('<th>[0.025<\/th>    <th>0.975]<\/th>', '<th style=\"background-color:#ff9896;\">[0.025<\/th>    <th style=\"background-color:#ff9896;\">0.975]<\/th>'))\n)","3a05c1f1":"model_ols = ols(\"\"\"quality ~ fixed_acidity \n                        + volatile_acidity \n                        + residual_sugar \n                        + free_sulfur_dioxide\n                        + total_sulfur_dioxide \n                        + density \n                        + pH \n                        + sulphates \n                        + alcohol\"\"\", data=white_data).fit()","08b20ae8":"model_summary = model_ols.summary()\nHTML(\n(model_ols.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: <\/th>')\n    .replace('<th>Model:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Model: <\/th>')\n    .replace('<th>Method:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Method: <\/th>')\n    .replace('<th>No. Observations:<\/th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: <\/th>')\n    .replace('<th>  R-squared:         <\/th>', '<th style=\"background-color:#aec7e8;\"> R-squared: <\/th>')\n    .replace('<th>  Adj. R-squared:    <\/th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: <\/th>')\n    .replace('<th>coef<\/th>', '<th style=\"background-color:#ffbb78;\">coef<\/th>')\n    .replace('<th>std err<\/th>', '<th style=\"background-color:#c7e9c0;\">std err<\/th>')\n    .replace('<th>P>|t|<\/th>', '<th style=\"background-color:#bcbddc;\">P>|t|<\/th>')\n    .replace('<th>[0.025<\/th>    <th>0.975]<\/th>', '<th style=\"background-color:#ff9896;\">[0.025<\/th>    <th style=\"background-color:#ff9896;\">0.975]<\/th>'))\n)","93f5b2d7":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef goodness(y_true, y_pred):\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    r_squared = r2_score(y_true, y_pred)\n    return mape, mse, r_squared","9903f77b":"model = LinearRegression().fit(white_data.drop(columns=['quality', 'citric_acid', 'chlorides']), y)\ny_true = white_data.quality\ny_pred = model.predict(white_data.drop(columns=['quality', 'citric_acid', 'chlorides']))","57f83a91":"column_names = ['fixed_acidity', 'volatile_acidity', 'residual_sugar',\n       'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nregression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': model.coef_}, columns=['Feature', 'Coefficient'])","4d567ec7":"column_names = ['fixed_acidity', 'volatile_acidity', 'residual_sugar',\n       'free_SO2', 'total_SO2', 'density',\n       'pH', 'sulphates', 'alcohol']\n\nplt.figure(figsize=(15,5))\ng = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')\ng.set_title(\"Contribution of features towards target variable\", fontsize=15)\ng.set_xlabel(\"Feature\", fontsize=13)\ng.set_ylabel(\"Degree of Coefficient\", fontsize=13)\ng.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])\ng.set_xticklabels(column_names)\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()\/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n               textcoords='offset points', fontsize=14, color='black')","20bf467b":"error = y_true - y_pred\nerror_info = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'error': error}, columns=['y_true', 'y_pred', 'error'])","c954b294":"fig = plt.figure(figsize=(10,12))\nfig = sm.graphics.plot_partregress_grid(model_ols, fig=fig)","c75c090a":"plt.figure(figsize=(8,5))\ng = sns.regplot(x=\"y_pred\", y=\"error\", data=error_info, color='blue')\ng.set_title('Check Homoskedasticity', fontsize=15)\ng.set_xlabel(\"predicted values\", fontsize=13)\ng.set_ylabel(\"Residual\", fontsize=13)","a3ee1b85":"fig, ax = plt.subplots(figsize=(8,5))\nax = error_info.error.plot()\nax.set_title('Uncorrelated errors', fontsize=15)\nax.set_xlabel(\"Data\", fontsize=13)\nax.set_ylabel(\"Residual\", fontsize=13)","4bfc7792":"fig, ax = plt.subplots(figsize=(6,4))\n_ = sp.stats.probplot(error_info.error, plot=ax, fit=True)\nax.set_title('Probability plot', fontsize=15)\nax.set_xlabel(\"Theoritical Qunatiles\", fontsize=13)\nax.set_ylabel(\"Ordered Values\", fontsize=13)","fb865d30":"ax = sm.qqplot(error_info.error, line='45')\nplt.show()","8a05ba8f":"pca = PCA()\ntransform_X = pca.fit_transform(white_data.drop(columns=['quality']), white_data.quality)\n\ncolumns = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7',\n            'feature_8', 'feature_9', 'feature_10', 'feature_11']\ntransform_df = pd.DataFrame.from_records(transform_X)\ntransform_df.columns = columns\ntransform_df['quality'] = white_data.quality","56560f9b":"model_ols_new = ols(\"\"\"quality ~ feature_1 \n                        + feature_2 \n                        + feature_3\n                        + feature_4 \n                        + feature_5 \n                        + feature_6 \n                        + feature_7 \n                        + feature_8 \n                        + feature_9 \n                        + feature_10 \n                        + feature_11\"\"\", data=transform_df).fit()","d6686a4e":"model_summary = model_ols_new.summary()\nHTML(\n(model_ols_new.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: <\/th>')\n    .replace('<th>Model:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Model: <\/th>')\n    .replace('<th>Method:<\/th>', '<th style=\"background-color:#c7e9c0;\"> Method: <\/th>')\n    .replace('<th>No. Observations:<\/th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: <\/th>')\n    .replace('<th>  R-squared:         <\/th>', '<th style=\"background-color:#aec7e8;\"> R-squared: <\/th>')\n    .replace('<th>  Adj. R-squared:    <\/th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: <\/th>')\n    .replace('<th>coef<\/th>', '<th style=\"background-color:#ffbb78;\">coef<\/th>')\n    .replace('<th>std err<\/th>', '<th style=\"background-color:#c7e9c0;\">std err<\/th>')\n    .replace('<th>P>|t|<\/th>', '<th style=\"background-color:#bcbddc;\">P>|t|<\/th>')\n    .replace('<th>[0.025<\/th>    <th>0.975]<\/th>', '<th style=\"background-color:#ff9896;\">[0.025<\/th>    <th style=\"background-color:#ff9896;\">0.975]<\/th>'))\n)","a356cece":"r2_linear_regression = model_ols_new.rsquared\n\nmodel_ridge=linear_model.Ridge()\nmodel_ridge.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_ridge = model_ridge.predict(white_data.drop(columns=['quality']))\nr2_ridge = r2_score(y_true, y_predict_ridge)\n\nmodel_lasso=linear_model.Lasso()\nmodel_lasso.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_lasso = model_lasso.predict(white_data.drop(columns=['quality']))\nr2_score(y_true, y_predict_lasso)\n\nn_neighbors=5\nknn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\nknn.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_knn=knn.predict(white_data.drop(columns=['quality']))\nr2_knn = r2_score(y_true, y_predict_knn)\n\nreg = linear_model.BayesianRidge()\nreg.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_pred_reg=reg.predict(white_data.drop(columns=['quality']))\nr2_bayesian = r2_score(y_true, y_pred_reg)\n\ndec = tree.DecisionTreeRegressor(max_depth=6)\ndec.fit(white_data.drop(columns=['quality']),white_data.quality)\ny1_dec=dec.predict(white_data.drop(columns=['quality']))\nr2_dt = r2_score(y_true, y1_dec)\n\nsvm_reg=svm.SVR()\nsvm_reg.fit(white_data.drop(columns=['quality']),white_data.quality)\ny1_svm=svm_reg.predict(white_data.drop(columns=['quality']))\nr2_svm = r2_score(y_true, y1_svm)","ac749da7":"r2_list = [r2_linear_regression, r2_ridge, r2_knn, r2_dt, r2_bayesian, r2_svm]\nr2_names = ['Linear Regression', 'Ridge Regression', 'KNN', 'Decision Tree', 'Bayesian Regression', 'SVM']\n\ncol = {'R-squared':r2_list, 'Method':r2_names}\ndf = pd.DataFrame(data=col, columns=['Method', 'R-squared'])\n\ndata_head = df\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')","0af5e7e2":"#### Analysis:\n- If we observe the distribution of all features, they follow a Normal distribution. \n- There is some fluctuation in the features \u201csulphates\u201d and \u201calcohol\u201d. ","37177a8b":"## Discussion\n- We can do alot of changes to improve the accuracy of the model. Some of the conditions above are violated. If we can transform the variables accordingly, we can achieve good results. If we observe the R-squared score, it is 0.282. It is able to explain only 28% of the variance, which is poor. So, there is a scope to apply different methods to get better models. \n- I have applied different popular Regression methods on the data to compare the results we got. The below table shows the comparison of the R-squared of different methods.","3f6ca9e2":"#### Analysis:\n- From this plot, we can see how different features are correlated with each other. \n- In the above plot, the features that are plotted on the x-axis and y-axis are in the given order itself. ","054f74dd":"### Distribution of Target Attribute","5e49b4e8":"#### Probability-Probability plot","36e342ad":"#### Analysis:\n- If we observe the p-values of transformed features, all the p-values are less than 0.05, which shows that multicollinearity problem is solved. ","c6e8c4b4":"- Report can be found [here](https:\/\/github.com\/Abhishekmamidi123\/Regression-Analysis).","cb0e4a35":"### Normality of error terms\n- This can be checked by plotting probability probability plot(p-p plot) or Quantile-Quantile plot(Q-Q plot). ","9e950e38":"## Visualize data\n### Correlation between features ","78e5cdc9":"### Correlation of errors\n- If there is no correlation between errors, then the model is good.","1971b1bd":"# <center> Introduction to Regression - Complete Analysis on Wine data <\/center>","35c7217b":"## Methodology \n1. Description of data \n2. Preprocess data \n3. Visualize data \n4. Build a Regression model \n5. Check Regression Assumptions \n6. Goodness of fit \n7. Compare different Regression methods","a97db036":"#### Analysis:\n- If we observe the above boxplot, the range of features is different from each other. \n- We can normalize the data. All the variables range from 0 to 1 after normalization and don\u2019t lose any information. ","5bdf0cd3":"### Homoskedasticity \n- To check homoskedasticity, we plot the residuals vs predicted values\/fitted values. \n- If we see any kind of funnel shape, we can say that there is heteroskedasticity. ","e3eff6d1":"### Pair plot between features\n- This is to understand the relation between features. ","af002a28":"#### Analysis:\n- The quality of wine ranges from 3 to 9 \n- The data is not balanced. The number of data points having quality 6 is very high and quality 3 and 9 are very low. \n- This may affect the model.","b9fb67a5":"## Conclusion  \n- We have visualized wine dataset in all possible ways and they are shown in the form of plots.  \n- A Linear Regression model is built to predict the target variable. Some improvements have been done on the model by removing some features that are not contributing and the data is transformed using Principal Component Analysis(PCA). \n- The test of assumptions for Linear Regression is also checked and they are analyzed properly. \n- In the end, Linear Regression is compared with different other popular Regression methods, in which KNN performed well owhen compared to others. \n- There is alot of scope to increase the performance of Linear Regression model. \n- We can increase the samples to build a robust model. Also, we can add some more features that contribute to the wine quality. ","63714690":"### Linearity \n- Plot partial regression plots to check linearity.","c793654a":"- There is no change in the values of R-squared and there is an increase of 0.001 of Adjusted R-squared. So, there is no harm in removing those features from the data. Also, the contribution of these features to predict the quality of the wine is very less as shown before. Now, we are left with 9 features.","7a0e6529":"### Linear Regression Assumption: Multicollinearity\n- If the independent variables are independent of each other, then we say there is no multicollinearity. \n- This can be tested in different ways: \n1. **Correlation plot**: If we observe the plot, there is multicollinearity between variables. \n2. **Variation Inflation Factor**: With VIF > 10 there is an indication that multicollinearity may be present. With VIF > 100 there is certainly multicollinearity among the variables. \n- We can conclude that multicollinearity among variables exists. \n- If multicollinearity is found in the data, centring the data, that is deducting the mean score might help to solve the problem. Other alternatives to tackle the problems is conducting a **factor analysis**\/**Principal Component Analysis(PCA)** and rotating the factors to ensure the independence of the factors in the linear regression analysis. \n- We can do the same analysis after applying PCA on the data. We can see some improvements in the model as there won\u2019t be any multicollinearity. \n- The results of OLS Regression are shown below after transforming the feature variables using **PCA**. ","c7fede2e":"### Distribution of features - After normalization \n- This looks good than before and very easy to understand the distribution of data.","928012c7":"#### Analysis:\n- As we have seen above in the correlation plot, there is a high correlation(>0.5) in between some of the features. \n- Here, we can visualize how these features are correlated. \n- If we observe carefully, we cannot separate the data points of different quality easily, because all the data points of various quality are overlapped.","5c7f095f":"- Let\u2019s check each condition using the predicted values and the errors\/residuals. \n- Residuals are the difference between \u201ctrue value\u201d and the \u201cpredicted value\u201d. \n\n**Note**: The two features \u201cchlorides\u201d and \u201ccitric acid\u201d are removed from the data.","13732f46":"## Abstract \n- The main goal of this report is to extract maximum knowledge from the Wine data in different ways. The data is analyzed and the plots are shown. A regression model is built to predict the quality of wine using the features provided. The assumptions of regression are also checked.","6e8f6881":"### Distribution of target attribute - Box plot","7056fa38":"#### Analysis:\n- The points are not random. Also, we can see the shape of a funnel to the right, which confirms that there is heteroskedasticity. \n- It means that the variance of Y across all X is not the same. \n- We can conclude that, Homoskedasticity condition doesn\u2019t hold in this case. ","1ea50e4b":"#### Analysis:\n- The correlation between \u201cdensity\u201d and \u201cresidual sugar\u201d is 0.84. \n- The correlation between \u201calcohol\u201d and \u201cdensity\u201d is 0.78. \n- The correlation between \u201ctotal sulfur dioxide\u201d and \u201cfree sulfur dioxide\u201d \nis 0.62. \n- These are the three pairs of features having a high correlation(>0.5).","160697f2":"### Features \n1. Fixed acidity \n2. Volatile acidity \n3. Citric acid \n4. Residual sugar \n5. Chlorides \n6. Free sulfur dioxide \n7. Total sulfur dioxide \n8. Density \n9. pH \n10. Sulphates \n11. Alcohol\n\n### Target Attribute \n- Quality of wine","0e5f5f28":"### Pair plot between correlated features","a73ca96e":"### Data","9a40c43d":"- If we compare R-square, KNN outperformed on all the Regression methods. Also, all the methods performed better than LinearRegression. So, we can conclude there is alot of scope to improve the Linear Regression model. ","0fc0b88f":"### Contribution of features after removing two features ","7b30a58f":"#### Analysis:\n- If we observe the above plots, we can conclude that the errors are following a Normal distribution, because the plot shows the fluctuation around the line and there is not much deviation. \n- The graph is linear.","fe5edcf8":"- The coefficients of features have changed a little bit. ","8d11c84a":"### Thank you for reading till the end.\n### Do Upvote if you find it useful. Feedback is always welcome. Please let me know in the comment section below.","6c9dd2c7":"### Ordinary Least Squares(OLS)\n- In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. \n- The OLS method corresponds to minimizing the sum of squared differences between the observed and predicted values. This minimization leads to the estimators of the parameters of the model. \n- The results of OLS Regression are shown below: ","f90922ec":"## Description of data \n1. **Name of the data**: Wine data from UCI Machine learning repository \n2. **Number of data points**: 4898 \n3. **Number of features**: 11 \n4. **Target attribute**: Quality of wine \n5. **Range of target attribute**: 3 to 9","2bffe63b":"### Describe data","c09f67ab":"### Quantile-Quantile plot","ce7530e8":"### Distribution of each feature","d7f84e79":"## Problem Definition\n-  A wine dataset is provided. The task is to analyze data and build a regression model to predict the quality of the wine.","4ebbf4c1":"### Analysis:\n- If we observe carefully, all the partial residual plots between the independent variable and dependent variable are linear.  \n- Linearity condition is satisfied. ","a1ee1a7f":"## Check Regression Assumptions \n1. Linearity \n2. Homoscedasticity \n3. Correlation of errors \n4. Normality of errors. ","8cd4fd03":"#### Analysis:\n- The R-squared is 0.282 and Adjusted R-squared is 0.280. \n- If p-value > 0.05, we fail to reject the null hypothesis, otherwise we reject the null hypothesis.\n- The p-values of the features \u201ccitric acid\u201d and \u201cchlorides\u201d, is greater than 0.05. Also, the contribution of these features is very less. \n- So, we can remove the remove the features from the data. \n- Let\u2019s fit the model again and see if there would be any change. The results of OLS Regression are shown after the removal of these two features from the data. ","9f2e92c4":"#### Analysis:\n- If we observe, there is no correlation\/pattern between errors. It is purely random. \n- We can also check this condition using the Durbin-Watson test: \n    - If DW = 2, then there is no correlation. \n    - If DW < 2, then the errors are positively correlated. \n    - If DW > 2, then the errors are negatively correlated. \n- If we perform Durbin-Watson test, the value of DW is 1.621. \n- According to the test, we can say that the errors are positively correlated.  \n- However, this is a point estimate for perfect uncorrelation of errors(DW=2). So, we won\u2019t get DW as 2 on real data. If it around 2, then we can conclude that the errors are uncorrelated. ","8ca70db9":"## Preprocess data","c7f47cfd":"## Build a Regression model\n### Linear Regression using Gradient Descent\n- The method of Linear Regression that finds the coefficients of different features using Gradient Descent optimization, is fit to the data to see how independent variables are contributing to the dependent variable. \n- The below plot shows the coefficients of features(contribution). "}}