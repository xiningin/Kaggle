{"cell_type":{"4a75904a":"code","e4544fdc":"code","9d9b2064":"code","9dc7ff9a":"code","0c09f149":"code","ae515e3e":"code","e731d4fd":"code","d6013c4d":"code","98f43d32":"code","210bdc44":"code","290c57bf":"code","640fcf65":"code","afd14d21":"code","2176915e":"code","c31bf2ca":"code","077ca0e2":"code","a3782aec":"code","1e2b7966":"code","fcd82ca7":"code","e7ce940e":"code","c27b661c":"code","1e8c07e8":"code","131cdde5":"code","630283b1":"code","0dafbbb6":"code","9c97357f":"code","66a861f6":"code","f57e3661":"code","1a9cdff4":"code","01d97e48":"code","b722125f":"code","2eef8c9b":"code","80e90e41":"code","f1664298":"code","b04e5dd8":"code","f1343911":"code","86dbd337":"code","9549059f":"code","5a3f66c9":"code","cd88904d":"code","47e384a8":"code","3bfabdcb":"code","26b84e80":"code","40867f79":"code","0013de99":"code","e4de5e2b":"code","9af2810a":"code","cc80d1a2":"code","e45d02ef":"code","c7f85492":"code","de220138":"code","819d540f":"code","642e0285":"code","e5f7eaa1":"code","12a09bef":"code","5cfa637c":"code","55b30dec":"code","63ba7d79":"code","ff37873d":"code","2b94a6ac":"code","54fdcb2e":"code","85489d31":"code","ada9df86":"code","1137d97e":"code","178fd0a5":"code","a58c2060":"code","2ec4035b":"code","710e95a4":"markdown","0013d4c0":"markdown","bfb4f56f":"markdown","245f161c":"markdown","c672f81b":"markdown","ab23b043":"markdown","452accc8":"markdown","8cae2a77":"markdown","f79616ee":"markdown","f29904a8":"markdown","111a566a":"markdown","10fdc519":"markdown","b526bf4c":"markdown","69317d74":"markdown","e19477b8":"markdown","5e54f586":"markdown","53fffee2":"markdown","6ca25536":"markdown","709facc8":"markdown","563df65f":"markdown","3f05dcb6":"markdown","6f057ad1":"markdown","00d790ec":"markdown","0afe46ca":"markdown","c579ec42":"markdown","14ad0af5":"markdown","f651c11f":"markdown","093ac871":"markdown","da1661da":"markdown","88d1f67b":"markdown"},"source":{"4a75904a":"!python -m spacy download en_core_web_lg","e4544fdc":"import json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom collections import Counter \nimport statsmodels.api as sm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom numpy import array\nfrom sklearn.decomposition import TruncatedSVD","9d9b2064":"data = {\"review_id\":[],\"business_id\":[],\"text\":[],\"stars\":[]}\n\nwith open('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json') as f:\n    for line in tqdm(f):\n        review = json.loads(line)\n        data['review_id'].append(review['review_id'])\n        data['business_id'].append(review['business_id'])\n        data['text'].append(review['text'])\n        data['stars'].append(review['stars'])","9dc7ff9a":"df = pd.DataFrame(data)\nprint(df.shape)\ndf.head()","0c09f149":"data1 = {\"name\":[],\"business_id\":[],\"city\":[],\"categories\":[]}\n\nwith open('..\/input\/yelp-dataset\/yelp_academic_dataset_business.json') as f:\n    for line in tqdm(f):\n        business = json.loads(line)\n        data1['name'].append(business['name'])\n        data1['business_id'].append(business['business_id'])\n        data1['city'].append(business['city'])\n        data1['categories'].append(business['categories'])","ae515e3e":"df1 = pd.DataFrame(data1)\nprint(df1.shape)\ndf1.head()","e731d4fd":"df.dropna(inplace=True)\ndf1.dropna(inplace=True)","d6013c4d":"restaurants_business = df1[df1.categories.str.contains(\"Restaurants\")]","98f43d32":"restaurants_business['city'].value_counts()","210bdc44":"Toronto_rest = restaurants_business[restaurants_business.city == \"Toronto\"]","290c57bf":"finaldf = Toronto_rest.merge(df,on=\"business_id\",how=\"inner\")\nfinaldf.head()","640fcf65":"finaldf['name'].value_counts()","afd14d21":"final_reviews = finaldf[finaldf.name==\"Pai Northern Thai Kitchen\"]","2176915e":"final_reviews.head()","c31bf2ca":"final_reviews.shape","077ca0e2":"review = final_reviews.reset_index().text","a3782aec":"data = pd.DataFrame(review) ","1e2b7966":"data.head()","fcd82ca7":"nltk.download('punkt')","e7ce940e":"def tokenize(sentence):\n    sentence = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+|\\n+\", \"\", sentence.lower())\n    return sentence","c27b661c":"result = [tokenize(x) for x in data.text]","1e8c07e8":"nltk.download('vader_lexicon')\nnlp = spacy.load(\"en_core_web_lg\")\nsid = SentimentIntensityAnalyzer()","131cdde5":"def find_sentiment(doc):\n    # find roots of all entities in the text\n    ner_heads = {ent.root.idx: ent for ent in doc.ents}\n    rule3_pairs = []\n    for token in doc:\n        children = token.children\n        A = \"999999\"\n        M = \"999999\"\n        add_neg_pfx = False\n        for child in children:\n            if(child.dep_ == \"nsubj\" and not child.is_stop): # nsubj is nominal subject\n                if child.idx in ner_heads:\n                    A = ner_heads[child.idx].text\n                else:\n                    A = child.text\n            if(child.dep_ == \"acomp\" and not child.is_stop): # acomp is adjectival complement\n                M = child.text\n            # example - 'this could have been better' -> (this, not better)\n            if(child.dep_ == \"aux\" and child.tag_ == \"MD\"): # MD is modal auxiliary\n                neg_prefix = \"not\"\n                add_neg_pfx = True\n            if(child.dep_ == \"neg\"): # neg is negation\n                neg_prefix = child.text\n                add_neg_pfx = True\n        if (add_neg_pfx and M != \"999999\"):\n            M = neg_prefix + \" \" + M\n        if(A != \"999999\" and M != \"999999\"):\n            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound']))\n    return rule3_pairs","630283b1":"print(find_sentiment(nlp(\"Food was good but customer service was bad. The ambience was great although\")))","0dafbbb6":"aspect_opinion = [find_sentiment(nlp(x)) for x in result]","9c97357f":"aspect_opinion[:5]","66a861f6":"l1=[]\nfor i in range(0,len(aspect_opinion)):\n    for j in aspect_opinion[i]:\n        a1=list(j)\n        a1.append(final_reviews[\"stars\"].iloc[i])\n        l1.append(a1)","f57e3661":"asp=[]\npol=[]\nstars=[]\nfor i in l1:\n    asp.append(i[0])\n    pol.append(i[2])\n    stars.append(i[3])\naspectdf = pd.DataFrame()\naspectdf[\"aspect\"]=asp\naspectdf[\"polarity\"]=pol\naspectdf[\"stars\"]=stars\naspectdf.head() #all aspects with their respective polarity and stars","1a9cdff4":"aspectdf01 = aspectdf[aspectdf['polarity'] != 0]","01d97e48":"aspectdf01.shape","b722125f":"mydict=dict(aspectdf01[\"aspect\"].value_counts())\nfreq_asp = pd.DataFrame(list(mydict.items()),columns = ['aspect','frequency'])\nfreq_asp.head()","2eef8c9b":"#list of frequent occuring aspects\nfreqasp=[]\nz=list(freq_asp[\"frequency\"])\nz=np.percentile(z,90)\nfreq_asp01 = freq_asp[freq_asp['frequency'] >= z]\nfor i in freq_asp01[\"aspect\"]:\n    freqasp.append(i)\nlen(freqasp)","80e90e41":"d1 = pd.DataFrame(index=range(len(aspect_opinion)),columns=freqasp)","f1664298":"for i in aspect_opinion:\n    for j in i:\n        if j[0] in freqasp:\n            d1[j[0]].iloc[aspect_opinion.index(i)]=j[2]","b04e5dd8":"d1=d1.fillna(0)","f1343911":"d1[\"Stars\"]=list(final_reviews[\"stars\"])","86dbd337":"d1.head(10) #dataframe that we will use for regression and clustering","9549059f":"d0=d1.copy()","5a3f66c9":"d1[\"zeroes\"]=np.nan\nfor i in range (0, d1.shape[0]):\n    d1[\"zeroes\"].iloc[i]=list(d1.iloc[i]).count(0)\nd1 = d1[d1['zeroes'] != len(freqasp)]\nd1=d1.drop(\"zeroes\",axis=1)","cd88904d":"d1.head()","47e384a8":"X = d1.drop([\"Stars\"],axis=1)\nx = X.values\nY = d1[\"Stars\"]\ny = Y.values","3bfabdcb":"# Program to find the optimal number of components for Truncated SVD\nn_comp = range(0,len(freqasp)) # list containing different values of components\nexplained = [] # explained variance ratio for each component of Truncated SVD\nfor n in n_comp:\n    svd = TruncatedSVD(n_components=n)\n    svd.fit(x)\n    explained.append(svd.explained_variance_ratio_.sum())\ndsvd = pd.DataFrame()\ndsvd[\"Components\"]=n_comp\ndsvd[\"Explained Variance\"]=explained\nplt.plot(n_comp, explained)\nplt.xlabel('Number of components')\nplt.ylabel(\"Explained Variance\")\nplt.title(\"Plot of Number of components v\/s explained variance\")\nplt.show()","26b84e80":"z1=[]\nfor i in range(0,dsvd.shape[0]):\n    if dsvd[\"Explained Variance\"].iloc[i]>=0.7:\n        z1.append(dsvd[\"Components\"].iloc[i])\nzfinal=min(z1)+1","40867f79":"svd = TruncatedSVD(n_components=zfinal)\nsvd.fit(x)\nresult = svd.transform(x)","0013de99":"d2 = pd.DataFrame(index=range(len(svd.components_)),columns=freqasp)","e4de5e2b":"for i in range(0,d2.shape[0]):\n    d2.iloc[i]=list(svd.components_)[i]","9af2810a":"d2","cc80d1a2":"l2=[]\nb1=list(d2.columns)\nfor i in range(0,d2.shape[0]):\n    s1=\"\"\n    for j in b1:\n        a2=list(d2.iloc[i])\n        if d2[j].iloc[i]>np.percentile(a2,98):\n            s1=s1+\" & \"+j\n    l2.append(s1[3:])","e45d02ef":"l2","c7f85492":"#model with SVD\n#result=sm.add_constant(result)\nmodel = sm.OLS(y,result).fit()\naspectdf_regression = pd.DataFrame()\naspectdf_regression[\"Latent Factors\"]=range(0, d2.shape[0])\naspectdf_regression[\"coeff\"]=list(model.params)\naspectdf_regression[\"prob\"]=list(model.pvalues)\naspectdf_regression","de220138":"model.summary()","819d540f":"aspectdf01[\"freq\"]=np.nan\nfor i in range(0,aspectdf01.shape[0]):\n    if aspectdf01[\"aspect\"].iloc[i] in freqasp:\n        aspectdf01[\"freq\"].iloc[i]=1\naspectdf01=aspectdf01.dropna()\naspectdf_final=aspectdf01.drop(['freq'], axis=1)\naspectdf_final.head()","642e0285":"grouped = aspectdf_final.groupby('aspect')\naspectdf_final01=grouped['polarity'].agg([np.mean])\naspectdf_final01.shape","e5f7eaa1":"aspectdf_final01=aspectdf_final01.reset_index()","12a09bef":"aspectdf_regression = aspectdf_regression[aspectdf_regression['prob'] < 0.05]\naspectdf_regression.shape","5cfa637c":"aspectdf_regression=aspectdf_regression.sort_values(by=['coeff'])","55b30dec":"aspectdf_regression","63ba7d79":"aspectdf_regression_pos=aspectdf_regression[aspectdf_regression[\"coeff\"]>0]\naspectdf_regression_neg=aspectdf_regression[aspectdf_regression[\"coeff\"]<0]","ff37873d":"a3=aspectdf_regression_pos.shape[0]\nif int(a3\/2)<5:\n    if int(a3\/2)==0:\n        a4=1\n    else:\n        a4=int(a3\/2)\nelse:\n    a4=5\nd3=aspectdf_regression_pos.tail(a4)\nl3=list(d3[\"Latent Factors\"])","2b94a6ac":"for i in range(0,a4):\n    print(l2[l3[i]])","54fdcb2e":"d4=aspectdf_regression_pos.head(a4)\nl4=list(d4[\"Latent Factors\"])","85489d31":"for i in range(0,a4):\n    print(l2[l4[i]])","ada9df86":"final_reviews[\"stars\"].mean() #average user rating of the restaurant","1137d97e":"d0[\"UserID\"]=range(1,d0.shape[0]+1)\nd6 = d0[d0['Stars'] == 5.0]\nd6[\"zeros\"]=np.nan\nfor i in range(0,d6.shape[0]):\n    d6[\"zeros\"].iloc[i]=list(d6.iloc[i]).count(0.0)\nd6=d6.sort_values(by=['zeros'])","178fd0a5":"d6[\"neg\"]=np.nan\nfor i in range(0,d6.shape[0]):\n    numbers=list(d6.iloc[i])\n    z3=sum(1 for number in numbers if number < 0)\n    d6[\"neg\"]=z3","a58c2060":"d6=d6[d6[\"neg\"]==0]","2ec4035b":"#top 5 reviews for restaurant owners to use in their websites\nprint(\"Review 1:\")\nprint(data[\"text\"].iloc[d6[\"UserID\"].iloc[0]-1])\nprint(\"Review 2:\")\nprint(data[\"text\"].iloc[d6[\"UserID\"].iloc[1]-1])\nprint(\"Review 3:\")\nprint(data[\"text\"].iloc[d6[\"UserID\"].iloc[2]-1])\nprint(\"Review 4:\")\nprint(data[\"text\"].iloc[d6[\"UserID\"].iloc[3]-1])\nprint(\"Review 5:\")\nprint(data[\"text\"].iloc[d6[\"UserID\"].iloc[4]-1])","710e95a4":"Checking the frequency of the aspects ","0013d4c0":"Creating a dataframe with the components as rows and the freq aspects as columns and their respective proportions","bfb4f56f":"## **Output 1:**\n### **Aspects that together if are good, positively affect the restaurant ratings**","245f161c":"Extracting the only rows that has restaurant business","c672f81b":"Extracting only the reviews from a particular restaurant that has the maximum reviews","ab23b043":"Extracting the only rows among the restaurant business rows that is in Toronto because maximum reviews are from there","452accc8":"## Data collection and pre processing","8cae2a77":"Removing the rows which has none of the aspects","f79616ee":"## Spacy Dependency Parsing","f29904a8":"Getting All aspects in a dataframe with their respective polarity and their respective review's overall rating","111a566a":"## Importing libraries","10fdc519":"Mapping the Toronto restaurant business dataset with the one that has the reviews and ratings","b526bf4c":"Taking only the aspects that occur frequently","69317d74":"Separating the negative coeff and positive coeff factors","e19477b8":"Reviews pre processing","5e54f586":"Removing the aspects with 0 polarity because few of them were not correct","53fffee2":"Taking only those components that are significant","6ca25536":"Getting just the review column","709facc8":"## Modelling","563df65f":"Extracting certain columns from the yelp review dataset","3f05dcb6":"## **Output 3:**\n### **Overall user rating of the customers**","6f057ad1":"Extracting aspects, opinions, polarity from the set of reviews","00d790ec":"Extracting the mean polarity for all the freq aspects (in case we need it)","0afe46ca":"Crreating a dataframe with the freq aspect as columns and stars and each review as one row","c579ec42":"Naming the components based on their aspect proportions","14ad0af5":"## **Output 4:**\n### **Top 5 reviews for the restaurant so that they can use these in their website**","f651c11f":"Dimensionality reduction using SVD","093ac871":"Extracting certain columns from the yelp business datset","da1661da":"OLS","88d1f67b":"## **Output 2:**\n### **Aspects that together does not matter that much to the ratings**"}}