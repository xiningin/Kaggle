{"cell_type":{"1c589cd4":"code","113fd102":"code","ae4a12c2":"code","59980949":"code","fc889df9":"code","4bb37007":"code","81cb90e5":"code","3e6128de":"code","18e2338c":"code","91771e56":"code","db415827":"code","994e2acf":"code","89611788":"code","d03f12ee":"code","94ad5214":"code","4f1ec1f6":"code","db5de741":"code","f9f7ea7f":"code","0b16b2ba":"code","0282a6bc":"code","47797872":"code","ef5abcb1":"code","54e39411":"code","bd7996f2":"code","7150b8c8":"code","28810e1b":"code","8f469a40":"code","88e65a7e":"code","45d0602e":"code","324f4363":"code","aec989cf":"code","43844be9":"code","0f6cb11d":"code","4ec1121a":"code","6938facf":"code","98dda6da":"code","b358fc62":"code","5d57766e":"code","dacc8cd1":"code","64ff15b9":"code","2d365f8b":"code","2cbe2d33":"code","c21a6ebb":"markdown","a6ed1280":"markdown","1b760b2e":"markdown","f6103fdb":"markdown","50c0f4af":"markdown","e3598a94":"markdown","adbbd008":"markdown","ed0bf5dc":"markdown","b0447afc":"markdown","7d58b4c0":"markdown","d309c8c6":"markdown","a2d0ce80":"markdown","b8776a38":"markdown","b95b8ec8":"markdown","a57fadda":"markdown","29d00d33":"markdown","24b38f15":"markdown","38368f65":"markdown","64942864":"markdown"},"source":{"1c589cd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","113fd102":"import warnings\n\nwarnings.filterwarnings('ignore')","ae4a12c2":"# Import all the tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n# we want our plots to appear inside the notebook\n%matplotlib inline\n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics","59980949":"iris = pd.read_csv(\"\/kaggle\/input\/iris-flower-dataset\/IRIS.csv\")\niris.shape","fc889df9":"iris.head()","4bb37007":"# Let's find out how many flowers of the species there are\niris[\"species\"].value_counts()","81cb90e5":"iris.info()","3e6128de":"iris.describe()","18e2338c":"# Check if there are any missing values\niris.isna().sum()","91771e56":"# Checking the relation of different attributes to species using histogram\n\nfig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(ncols=2,\n                                             nrows=2,\n                                             figsize=(15, 10))\n\nax0.hist(pd.crosstab(iris[\"sepal_length\"], iris[\"species\"]), bins=10)\nax0.legend(iris[\"species\"].unique(), title=\"species\")\n\nax0.set(title=\"Sepal_length and Species\",\n        xlabel=\"sepal_length\",\n        ylabel=\"Frequency\",\n        xlim=[0, 8])\n\nax1.hist(pd.crosstab(iris[\"sepal_width\"], iris[\"species\"]), bins=10)\nax1.legend(iris[\"species\"].unique(), title=\"species\")\n\nax1.set(title=\"Sepal_Width and Species\",\n        xlabel=\"sepal_width\",\n        ylabel=\"Frequency\",\n        xlim=[0, 12])\n\nax2.hist(pd.crosstab(iris[\"petal_length\"], iris[\"species\"]), bins=10)\nax2.legend(iris[\"species\"].unique(), title=\"species\")\n\nax2.set(title=\"Petal_length and Species\",\n        xlabel=\"petal_length\",\n        ylabel=\"Frequency\",\n        xlim=[0, 13])\n\nax3.hist(pd.crosstab(iris[\"petal_width\"], iris[\"species\"]), bins=10)\nax3.legend(iris[\"species\"].unique(), title=\"species\")\n\nax3.set(title=\"Petal_width and Species\",\n        xlabel=\"petal_width\",\n        ylabel=\"Frequency\",\n        xlim=[0, 50])\n\nfig.suptitle(\n    \"Checking the relation of different attributes to species using histogram\",\n    fontsize=16,\n    fontweight=\"bold\")\nplt.show()\n\n","db415827":"iris[\"species\"].value_counts().plot(\n    kind=\"bar\", color=[\"salmon\", \"lightblue\", \"lightgreen\"])\nplt.xticks(rotation=0)\n\nplt.style.use(\"default\")\nplt.style.use(\"seaborn-whitegrid\")","994e2acf":"# Checking the Disribution of flowers according to different attributes\n\niris.plot(kind='hist', subplots=True, figsize=(6, 8))\n\nplt.tight_layout()\nplt.show()","89611788":"# Transforming species into numeric category\n\ncategorical_features = [\"species\"]\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([(\"one_hot\", one_hot, categorical_features)],\n                                remainder=\"passthrough\")\ntransformed_iris = transformer.fit_transform(iris)\ntransformed_iris","d03f12ee":"transformed_iris = pd.DataFrame(transformed_iris,\n                                columns=[\n                                    'Iris-setosa', 'Iris-versicolor',\n                                    'iris-virginica', 'sepal_length',\n                                    'sepal_width', 'petal_lenght',\n                                    'petal_width'\n                                ])","94ad5214":"transformed_iris","4f1ec1f6":"# Making a correlation matrix\ntransformed_iris.corr()","db5de741":"# Visualizing the correlation matrix\nax = sns.heatmap(transformed_iris.corr(), cbar=False, annot=True)\n\nplt.style.use(\"default\")\nplt.style.use(\"seaborn-whitegrid\")","f9f7ea7f":"X = iris.drop(\"species\", axis=1)\ny = iris[\"species\"]\n\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","0b16b2ba":"X_train","0282a6bc":"# Put models in a dictionary\n\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(),\n    \"Random Forest\": RandomForestClassifier()\n}\n\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through Models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","47797872":"model_scores = fit_and_score(models, X_train, X_test, y_train, y_test)\n\nmodel_scores","ef5abcb1":"grid_knn = { 'n_neighbors' : [5,7,9,11,13,15],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}\ngrid_clf = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}","54e39411":"# Tune KNN\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for KNN\ngs_knn = GridSearchCV(KNeighborsClassifier(),\n                      param_grid=grid_knn,\n                      cv=5,\n                      verbose=True)\n\n# Fit random hyperparameter search model for KNN\ngs_knn.fit(X_train, y_train)","bd7996f2":"gs_knn.best_params_","7150b8c8":"gs_knn.score(X_test, y_test)","28810e1b":"# Eliminating some hyperparameters before using GridSearchCV\n\n# Setup Random Seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=grid_clf,\n                           n_iter=20,\n                           cv=5,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","8f469a40":"rs_rf.best_params_","88e65a7e":"# Setting new hyperparameter dictionary\n\ngrid_clf_2 = {\n    'bootstrap': [True],\n    'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3],\n    'min_samples_split': [10, 12],\n    'n_estimators': [200, 300]\n}","45d0602e":"# Using GridSearchCV\n\n# Setup Random Seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\ngs_rf = GridSearchCV(RandomForestClassifier(),\n                     param_grid=grid_clf_2,\n                     cv=5,\n                     verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\ngs_rf.fit(X_train, y_train)","324f4363":"gs_rf.best_params_","aec989cf":"gs_rf.score(X_test,y_test)","43844be9":"model_scores","0f6cb11d":"clf= RandomForestClassifier(bootstrap=True,\n max_depth= 80,\n max_features=3,\n min_samples_leaf= 3,\n min_samples_split= 10,\n n_estimators= 200)\n\nclf.fit(X_train,y_train)","4ec1121a":"# get a prediction\ny_preds = clf.predict(X_test)","6938facf":"print(confusion_matrix(y_test, y_preds))","98dda6da":"print(classification_report(y_test, y_preds))","b358fc62":"# Visualizing Confusion matrix\n\nsns.set(font_scale=1.5)\n\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(5, 5))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cbar=False)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    \n    fig.canvas.draw()\n    labels = ['Setosa',\"Versicolor\",\"Virginica\"]\n\n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n\nplot_conf_mat(y_test, y_preds)","5d57766e":"scores = cross_val_score(clf, X, y, cv=5)","dacc8cd1":"print('Model accuracy: ', np.mean(scores))","64ff15b9":"clf.feature_importances_","2d365f8b":"# Match features to columns\nfeature_dict = dict(zip(iris.columns,list(clf.feature_importances_))) \nfeature_dict","2cbe2d33":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\", legend=False);","c21a6ebb":"**We weren't able to improve KNN, so we will try to improve RandomForest Classifier**","a6ed1280":"## Classification of Iris flower\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting which type of iris flower it is.\n\n<img src = \"https:\/\/miro.medium.com\/max\/1000\/1*lFC_U5j_Y8IXF4Ga87KNVg.png\" with =600\/>","1b760b2e":"## Conclusion\n\nTherefore, we see that Random Forest Classifier is the best model.\n\nIf you found this helpful, do consider upvoting \ud83d\ude4c.","f6103fdb":"## Encoding the data","50c0f4af":"## Data Preprocessing","e3598a94":"### Train the model","adbbd008":"### Train test split","ed0bf5dc":"## Evaluating our tuned machine learning classifier, beyond accuracy\n* Confusion matrix\n* Classification report","b0447afc":"## Preparing the tools\nWe're going to use pandas, Matplotlib and Numpy for data analysis and manipulation","7d58b4c0":"## Hyperparameter tuning with GridSearchCV","d309c8c6":"Now we've got hyperparameter grids setup for each of our models, let's tune them using GridSearchCV...","a2d0ce80":"## Features\n\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class:\n  * -- Iris Setosa\n  * -- Iris Versicolour\n  * -- Iris Virginica\n\n","b8776a38":"## Visualizing the data","b95b8ec8":"**We see that Logistic Regression is a perfect model but it might be a case of overfitting so we will go forward with KNN and Random Forest Classifier**","a57fadda":"## Modelling","29d00d33":"## Feature Importance","24b38f15":"**Since RandomForestClassifier has the highest accuracy, we will proceed with it**","38368f65":"***We noticed that Iris-setosa had the largest sepal and petal length while having less sepal and petal width***","64942864":"**We see that petal_length is the most distinguished feature**"}}