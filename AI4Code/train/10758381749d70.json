{"cell_type":{"f02fae81":"code","5b27d974":"code","80160b7a":"code","9ee97ea7":"code","0ca3332f":"code","f3996eb9":"code","d5a3e376":"code","8deade03":"code","7cb68015":"code","403aba79":"code","cafd43c8":"code","daeb6ed1":"code","9b02213f":"code","51db4676":"code","2a9a4a21":"code","d40cfbbe":"code","81aecc60":"code","d8e5998b":"code","242e9ec6":"code","ff17e286":"code","fe94dcde":"code","50de96e4":"code","2c95b66b":"code","59b3f531":"code","b894e1cd":"code","b612e7cd":"code","be961f16":"code","5c3297a4":"code","1be8ea7d":"code","b3f1403f":"code","71b716ac":"code","65724988":"code","a7c451a7":"code","53b15aad":"code","6fb6a6db":"code","3d3352c3":"code","4ae6b2bf":"code","c3a3fb02":"code","73baa2d1":"code","beb8bcb6":"code","cff0367a":"code","d03d71c2":"code","60899025":"code","e46bba70":"code","9b679854":"code","842d8a31":"code","1872fd18":"code","4ce68672":"code","f350caee":"code","608796bd":"code","8668ad93":"code","e01e5fbd":"code","d815b483":"code","0f1527ab":"code","0eb20b8a":"code","6c6411db":"code","7e780c5f":"code","9a3a41ca":"code","8803d9d1":"code","22f53e4c":"markdown","ccf93d67":"markdown","8b3d4d59":"markdown","e102a860":"markdown","ead88839":"markdown","a4c99cf2":"markdown","bef6e5ed":"markdown","f68c2cb9":"markdown"},"source":{"f02fae81":"import numpy as np, pandas as pd\nimport json\nimport ast \nfrom textblob import TextBlob\nimport nltk\nimport torch\nimport pickle\nfrom scipy import spatial\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom nltk import Tree\nen_nlp = spacy.load('en')\nfrom nltk.stem.lancaster import LancasterStemmer\nst = LancasterStemmer()\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer","5b27d974":"# !conda update pandas --y","80160b7a":"train = pd.read_csv(\"..\/input\/sentence-embedding\/train.csv\",encoding=\"latin-1\")","9ee97ea7":"train.shape","0ca3332f":"with open(\"..\/input\/sent-emb-result\/dict_embeddings1.pickle\", \"rb\") as f:\n    d1 = pickle.load(f)","f3996eb9":"with open(\"..\/input\/sent-emb-result\/dict_embeddings2.pickle\", \"rb\") as f:\n    d2 = pickle.load(f)","d5a3e376":"dict_emb = dict(d1)\ndict_emb.update(d2)","8deade03":"len(dict_emb)","7cb68015":"del d1, d2","403aba79":"def get_target(x):\n    idx = -1\n    for i in range(len(x[\"sentences\"])):\n        if x[\"text\"] in x[\"sentences\"][i]: idx = i\n    return idx","cafd43c8":"train.head(3)","daeb6ed1":"train.shape","9b02213f":"train.dropna(inplace=True)","51db4676":"train.shape","2a9a4a21":"def process_data(train):\n    \n    print(\"step 1\")\n    train['sentences'] = train['context'].apply(lambda x: [item.raw for item in TextBlob(x).sentences])\n    \n    print(\"step 2\")\n    train[\"target\"] = train.apply(get_target, axis = 1)\n    \n    print(\"step 3\")\n    train['sent_emb'] = train['sentences'].apply(lambda x: [dict_emb[item][0] if item in\\\n                                                           dict_emb else np.zeros(4096) for item in x])\n    print(\"step 4\")\n    train['quest_emb'] = train['question'].apply(lambda x: dict_emb[x] if x in dict_emb else np.zeros(4096) )\n        \n    return train   ","d40cfbbe":"train.shape","81aecc60":"t=train[50000:]","d8e5998b":"train = process_data(t)","242e9ec6":"train.head(3)","ff17e286":"def cosine_sim(x):\n    li = []\n    for item in x[\"sent_emb\"]:\n        li.append(spatial.distance.cosine(item,x[\"quest_emb\"][0]))\n    return li   ","fe94dcde":"def pred_idx(distances):\n    return np.argmin(distances)   ","50de96e4":"def predictions(train):\n    \n    train[\"cosine_sim\"] = train.apply(cosine_sim, axis = 1)\n    train[\"diff\"] = (train[\"quest_emb\"] - train[\"sent_emb\"])**2\n    train[\"euclidean_dis\"] = train[\"diff\"].apply(lambda x: list(np.sum(x, axis = 1)))\n    del train[\"diff\"]\n    \n    print(\"cosine start\")\n    \n    train[\"pred_idx_cos\"] = train[\"cosine_sim\"].apply(lambda x: pred_idx(x))\n    train[\"pred_idx_euc\"] = train[\"euclidean_dis\"].apply(lambda x: pred_idx(x))\n    \n    return train\n    ","2c95b66b":"train.columns","59b3f531":"predicted = predictions(train)","b894e1cd":"predicted.head(3)","b612e7cd":"predicted[\"cosine_sim\"][50000]","be961f16":"predicted[\"euclidean_dis\"][50000]","5c3297a4":"def accuracy(target, predicted):\n    \n    acc = (target==predicted).sum()\/len(target)\n    \n    return acc","1be8ea7d":"print(accuracy(predicted[\"target\"], predicted[\"pred_idx_euc\"]))","b3f1403f":"print(accuracy(predicted[\"target\"], predicted[\"pred_idx_cos\"]))","71b716ac":"predicted.to_csv(\"train_detect_sent.csv\", index=None)","65724988":"#predicted.iloc[75207,:]","a7c451a7":"ct,k = 0,0\nfor i in range(predicted.shape[0]):\n    if predicted.iloc[i,10] != predicted.iloc[i,5]:\n        k += 1\n        if predicted.iloc[i,11] == predicted.iloc[i,5]:\n            ct += 1","53b15aad":"ct, k","6fb6a6db":"label = []\nfor i in range(predicted.shape[0]):\n    if predicted.iloc[i,10] == predicted.iloc[i,11]:\n        label.append(predicted.iloc[i,10])\n    else:\n        label.append((predicted.iloc[i,10],predicted.iloc[i,10]))","3d3352c3":"\"\"\"ct = 0\nfor i in range(75206):\n    item = predicted[\"target\"][i]\n    try:\n        if label[i] == predicted[\"target\"][i]: ct +=1\n    except:\n        if item in label[i]: ct +=1\n   \"\"\"        ","4ae6b2bf":"#ct\/75206","c3a3fb02":"predicted = pd.read_csv(\"train_detect_sent.csv\").reset_index(drop=True)","73baa2d1":"doc = en_nlp(predicted.iloc[0,1])","beb8bcb6":"predicted.iloc[0,1]","cff0367a":"predicted.iloc[0,2]","d03d71c2":"def to_nltk_tree(node):\n    if node.n_lefts + node.n_rights > 0:\n        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n    else:\n        return node.orth_","60899025":"[to_nltk_tree(sent.root).pretty_print()  for sent in en_nlp(predicted.iloc[0,2]).sents]","e46bba70":"[to_nltk_tree(sent.root) .pretty_print() for sent in doc.sents][50005]","9b679854":"for sent in doc.sents:\n    roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n    print(roots)","842d8a31":"def match_roots(x):\n    question = x[\"question\"].lower()\n    sentences = en_nlp(x[\"context\"].lower()).sents\n    \n    question_root = st.stem(str([sent.root for sent in en_nlp(question).sents][0]))\n    \n    li = []\n    for i,sent in enumerate(sentences):\n        roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n\n        if question_root in roots: \n            for k,item in enumerate(ast.literal_eval(x[\"sentences\"])):\n                if str(sent) in item.lower(): \n                    li.append(k)\n    return li","1872fd18":"predicted[\"question\"][21493]","4ce68672":"predicted[\"context\"][21493]","f350caee":"predicted[\"root_match_idx\"] = predicted.apply(match_roots, axis = 1)","608796bd":"predicted[\"root_match_idx_first\"]= predicted[\"root_match_idx\"].apply(lambda x: x[0] if len(x)>0 else 0)","8668ad93":"(predicted[\"root_match_idx_first\"]==predicted[\"target\"]).sum()\/predicted.shape[0]","e01e5fbd":"predicted.to_csv(\"train_detect_sent.csv\", index=None)","d815b483":"#predicted[(predicted[\"sentences\"].apply(lambda x: len(ast.literal_eval(x)))<11) &  (predicted[\"root_match_idx_first\"]>10)]       \n\n","0f1527ab":"#len(ast.literal_eval(predicted.iloc[21493,4]))","0eb20b8a":"\"\"\"question = predicted[\"question\"][21493].lower()\nsentences = en_nlp(predicted[\"context\"][21493].lower()).sents\n    \nquestion_root = st.stem(str([sent.root for sent in en_nlp(question).sents][0]))\n    \nli = []\nfor i,sent in enumerate(sentences):\n    roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n    print(roots)\n\n    if question_root in roots: li.append(i)\n\"\"\"    ","6c6411db":"#ast.literal_eval(predicted[\"sentences\"][21493])","7e780c5f":"#predicted[\"context\"][21493]","9a3a41ca":"#en_nlp = spacy.load('en')\n#sentences = en_nlp(predicted[\"context\"][2].lower()).sents #21493","8803d9d1":"#for item in sentences:\n#    print(item)","22f53e4c":"## Predicted Cosine & Euclidean Index","ccf93d67":"### Accuracy for  euclidean Distance","8b3d4d59":"## Accuracy","e102a860":"### Accuracy for Cosine Similarity","ead88839":"### Loading Embedding dictionary","a4c99cf2":"### Root Match","bef6e5ed":"### Combining Accuracy","f68c2cb9":"## Data Processing"}}