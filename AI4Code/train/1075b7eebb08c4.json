{"cell_type":{"3aa118d8":"code","7f07daed":"code","f8cda39a":"code","3db5630b":"code","5ee7de71":"code","b988554d":"code","be38bfb3":"code","b7652c43":"code","7ab72d50":"code","1d27d61d":"code","8d54337a":"code","f97115f1":"code","354a108e":"code","636af62a":"code","71cf1e5a":"code","bdb820af":"code","ca5c935d":"code","f3921385":"code","bb7e5dad":"code","8f8583fa":"code","e055cb62":"code","729860ad":"code","989e08e9":"code","6ab3f46d":"code","2bfc176d":"code","b2914871":"code","6ab39eca":"code","caafaafe":"code","7adf6303":"code","d0ab20fb":"code","e35da325":"code","31a66404":"code","c620b8c0":"code","7ea23599":"code","bc49e48e":"code","cab44bc4":"markdown","8f979094":"markdown","f54d60d7":"markdown","16217f98":"markdown","81d2dd82":"markdown","3e01152c":"markdown","318169e3":"markdown","ccdfd8f9":"markdown","22391970":"markdown","ae43f469":"markdown","60646814":"markdown","dda2f113":"markdown","526c70a9":"markdown","71beb060":"markdown","95c2aa78":"markdown","285b7d95":"markdown","dd92d309":"markdown","985650af":"markdown"},"source":{"3aa118d8":"import pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","7f07daed":"test = pd.read_csv(\"..\/input\/sf-crime\/test.csv.zip\",parse_dates = ['Dates'],index_col = 'Id')\ntrain = pd.read_csv(\"..\/input\/sf-crime\/train.csv.zip\",parse_dates = ['Dates'])","f8cda39a":"test.head()","3db5630b":"train.head()","5ee7de71":"train.info()","b988554d":"test.info()","be38bfb3":"#Function for printing null_values and related info\ndef description(data):\n    no_rows=data.shape[0]\n    types=data.dtypes\n    col_null = data.columns[data.isna().any()].to_list()\n    counts=data.apply(lambda x: x.count())\n    uniques=data.apply(lambda x: x.unique())\n    nulls=data.apply(lambda x: x.isnull().sum())\n    distincts=data.apply(lambda x: x.unique().shape[0])\n    nan_percent=(data.isnull().sum()\/no_rows)*100\n    cols={'dtypes':types, 'counts':counts, 'distincts':distincts, 'nulls':nulls,  \n          'missing_percent':nan_percent, 'uniques':uniques}\n    table=pd.DataFrame(data=cols)\n    return table","b7652c43":"#Checking Null Values In Train\ndetails_tr = description(train)\ndetails_tr.reset_index(level=[0],inplace =True)\ndetails_tr.sort_values(by='missing_percent', ascending=False)","7ab72d50":"#Checking Null Values In Test\ndetails_test = description(test)\ndetails_test.reset_index(level=[0],inplace =True)\ndetails_test.sort_values(by='missing_percent', ascending=False)","1d27d61d":"train.duplicated().sum()","8d54337a":"#Checking the outliers\nfigure, axs = plt.subplots(1,2,figsize = (15,5))\nsns.boxplot(data = train[[\"X\"]],ax=axs[0])\nsns.boxplot(data = train[[\"Y\"]],ax=axs[1])\n\n","f97115f1":"\ntrain.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\ntest.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n    test.loc[test['PdDistrict'] == district, ['X', 'Y']] = imp.transform(\n        test.loc[test['PdDistrict'] == district, ['X', 'Y']])","354a108e":"figure, axs = plt.subplots(1,2,figsize = (15,5))\nsns.boxplot(data = train[[\"X\"]],ax=axs[0])\nsns.boxplot(data = train[[\"Y\"]],ax=axs[1])","636af62a":"train = train[train[\"Y\"] < 80]\nsns.displot(train[[\"X\"]],kde=True)\nplt.show()","71cf1e5a":"data = train.groupby('Category').count()\ndata = data['Dates'].sort_values(ascending=False)\n\nplt.figure(figsize=(20, 12))\nax = sns.barplot(data.values ,data.index,palette=cm.ScalarMappable(cmap='magma').to_rgba(data.values))\n\nplt.title('Count by Category', fontdict={'fontsize': 24})\nplt.xlabel('Count')\nplt.grid()","bdb820af":"train['DayOfWeek'] = train['Dates'].dt.weekday\ntrain['Month'] = train['Dates'].dt.month\ntrain['Year'] = train['Dates'].dt.year\ntrain['Hour'] = train['Dates'].dt.hour\n\nyear = train.groupby('Year').count().iloc[:,0]\nmonth = train.groupby('Month').count().iloc[:,0]\nhour = train.groupby('Hour').count().iloc[:,0]\ndayofweek = train.groupby('DayOfWeek').count().iloc[:, 0]\n\nfigure, axs = plt.subplots(2,2, figsize = (15,10))\n\nsns.barplot(x=year.index, y= year,ax = axs[0][0],palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\nsns.barplot(x=month.index, y= month,ax = axs[0][1],palette=cm.ScalarMappable(cmap='viridis').to_rgba(data.values))\nsns.barplot(x=hour.index, y= hour,ax = axs[1][0],palette=cm.ScalarMappable(cmap='Blues').to_rgba(data.values))\nsns.barplot(x=dayofweek.index, y= dayofweek,ax = axs[1][1],palette=cm.ScalarMappable(cmap='cool').to_rgba(data.values))\nplt.show()","ca5c935d":"figure, axs = plt.subplots(figsize = (10,5))\nsns.countplot(x = train[\"PdDistrict\"])\nplt.show()","f3921385":"df_cr=pd.DataFrame(train['Category'].value_counts())\ndf_cr.tail()\nplt.figure(figsize=(16,10))\nax1 =  plt.subplot2grid((1,2),(0,0))\nax1.set_title('Top 10', size=16)\nsns.barplot(x=df_cr.head(10).index, y='Category', data=df_cr.head(10))\nax1.set_xticklabels(ax1.xaxis.get_ticklabels(), rotation=90)\nax2 =  plt.subplot2grid((1,2),(0,1))\nax2.set_title('Bottom 10', size=16)\nsns.barplot(x=df_cr.tail(10).index, y='Category', data=df_cr.tail(10))\nax2.set_xticklabels(ax2.xaxis.get_ticklabels(), rotation=90)\nplt.show()","bb7e5dad":"top10cc=pd.Series(df_cr.head(10).index)\ntop10=train[train['Category'].isin(top10cc)]\ntmp=pd.DataFrame(top10.groupby(['PdDistrict','Category']).size(), columns=['count'])\ntmp.reset_index(inplace=True)\ntmp=tmp.pivot(index='PdDistrict',columns='Category',values='count')\nfig, axes = plt.subplots(1,1,figsize=(15,15))\ntmp.plot(ax=axes,kind='bar', stacked=True)","8f8583fa":"def feature_engineering(data):\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n    data['n_days'] = (data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n    data[\"X-Y\"] = data[\"X\"] - data[\"Y\"]\n    data[\"XY\"] = data[\"X\"] + data[\"Y\"]\n    data.drop(columns=['Dates','Date','Address'], inplace=True)\n    return data\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)","e055cb62":"le1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\ntest['PdDistrict'] = le1.transform(test['PdDistrict'])\n\nle2 = LabelEncoder()\nX = train.drop(columns=['Category'])\ny= le2.fit_transform(train['Category'])","729860ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)","989e08e9":"#Fitting data in Decision Tree\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","6ab3f46d":"#Predicting the results\npredictions = dtree.predict(X_test)","2bfc176d":"print (classification_report(y_test,predictions))","b2914871":"#Fitting In RandomForest Ensemble\nrfc = RandomForestClassifier(n_estimators=40,min_samples_split=100 )\nrfc.fit(X_train, y_train)","6ab39eca":"#Predicting The Final Results\nrfc_pred = rfc.predict(X_test)\nprint (\"Train Accuracy: \", accuracy_score(y_train, rfc.predict(X_train)))\nprint (\"Test Accuracy: \", accuracy_score(y_test, rfc_pred))\n","caafaafe":"print (classification_report(y_test,rfc_pred))","7adf6303":"cm = confusion_matrix(y_test,predictions)\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.heatmap(cm, annot=False, ax = ax); \nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix');","d0ab20fb":"n_features = X.shape[1]\nplt.barh(range(n_features),rfc.feature_importances_)\nplt.yticks(np.arange(n_features),train.columns[1:])\nplt.show()","e35da325":"keys = le2.classes_\nvalues = le2.transform(le2.classes_)\nkeys","31a66404":"dictionary = dict(zip(keys, values))\nprint(dictionary)","c620b8c0":"y_pred_proba = rfc.predict_proba(test)\ny_pred_proba","7ea23599":"result = pd.DataFrame(y_pred_proba, columns=keys)\nresult.head()","bc49e48e":"result.to_csv(path_or_buf=\"Random_forest.csv\",index=True, index_label = 'Id')","cab44bc4":"# Checking For Outliers","8f979094":"* **We will check if the dataset contains any duplicate values**","f54d60d7":"# Data Visualization and Preprocessing","16217f98":"# Computing Null Values and Duplicates","81d2dd82":"# EDA","3e01152c":"# Training the Model","318169e3":"* **Box Plot after removing outliers**","ccdfd8f9":"> **Plotting feature importance in prediction*","22391970":"# Feature Engineering","ae43f469":"* **We can see there are outliers in X at -120.5 and Y at 90.0 so we will remove them in the next step**","60646814":"> **Loading Datasets**","dda2f113":"*   **District Wise Crime Count**","526c70a9":"*  **Plot of crimes count in the districts**","71beb060":"* **Top 10 and the least 10 occuring crimes**","95c2aa78":"# Encoding","285b7d95":"# Submission","dd92d309":"# Importing Essential Libraries","985650af":"> **We can see that there are no NAN values in both the datasets**"}}