{"cell_type":{"e9bf8405":"code","e4f6111e":"code","5a1a2820":"code","0ee3cecb":"code","18017bbb":"code","cb3fc7ab":"code","474d676b":"code","ce2cc164":"code","fdc2d8a0":"code","5652e184":"code","7adda861":"code","2948d6a7":"code","f1292414":"code","814b5302":"code","3511fc8e":"code","6419d26d":"code","3e14aafa":"code","f33ab4b7":"code","67a732f0":"code","91fc861c":"code","66b8303a":"code","116e6caa":"code","44bc88ee":"code","4a69a3a4":"code","3bea5f2d":"code","4451183b":"code","4f40746d":"code","6d22a817":"code","28339901":"code","c9e66d8a":"code","3a1e79b5":"code","a1973a3d":"code","4b88beb7":"code","7c8664c5":"code","dfd15776":"code","cc72ebd2":"code","7b8db507":"code","a8fdf0d1":"code","4f113cf2":"code","21f0c5cd":"code","1926364f":"code","82140a34":"code","4344deb2":"code","6207ba54":"code","e1c408f3":"code","57b8e7fc":"code","07a53720":"code","bfc02a59":"code","73795c78":"code","0e67fec0":"code","30815261":"code","140ddf2f":"code","76edebe4":"code","4c5055f1":"code","43efaa73":"code","63131406":"code","71395b11":"code","15ba9f46":"code","72f8077b":"code","db45a7fe":"code","31f94d84":"code","2b7974a8":"code","0e72a58d":"code","5dc3bb03":"code","9202d024":"code","88b136b9":"code","bc89f843":"code","e66651ba":"code","822fc94c":"markdown","bd8d9b6e":"markdown","2b6db53c":"markdown","1ca34f58":"markdown","05a03e9e":"markdown","3ac548e6":"markdown","8852e4ab":"markdown","ec0531de":"markdown","75cfca4f":"markdown","aa7fbda8":"markdown","c46e07be":"markdown","422ee90a":"markdown","b4d67f0a":"markdown","f5baef4a":"markdown","0f964d30":"markdown","14dedbf9":"markdown","9d30b78f":"markdown","06356506":"markdown","76a11fff":"markdown","0e91b86d":"markdown","f1cc0ec5":"markdown","f0faf1c4":"markdown","71bae42e":"markdown","babdfd82":"markdown","ede923f4":"markdown","6933b17e":"markdown","1764bc8a":"markdown","f118f791":"markdown","40b518ed":"markdown","3c1efdfb":"markdown","3bf88def":"markdown","6037c8a5":"markdown","4519f590":"markdown","f182180c":"markdown","a877758e":"markdown","b856861c":"markdown","c16c1afe":"markdown","129c7982":"markdown","a02704e8":"markdown","692a2ac2":"markdown"},"source":{"e9bf8405":"import numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\n\nfrom scipy import stats \nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report, f1_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\n\n#from colorsetup import colors, palette","e4f6111e":"gcr_data = pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\", index_col=0)\n#gcr_data = pd.read_csv(\"gcr_data_imputed.csv\")\n#gcr_data = pd.read_csv(\"gcr_processed.csv\")","5a1a2820":"gcr_data.head()","0ee3cecb":"gcr_data.nunique()","18017bbb":"gcr_data.info()","cb3fc7ab":"gcr_data.isnull().sum()","474d676b":"gcr_data['Checking account'].value_counts()","ce2cc164":"feature_cols = [x for x in gcr_data.columns if x!='Risk']\nX = gcr_data[feature_cols]\ny = gcr_data['Risk']","fdc2d8a0":"X.head()","5652e184":"sns.heatmap(gcr_data.corr(), annot=True)","7adda861":"sns.barplot(x=\"Job\", y=\"Credit amount\", hue=\"Sex\", data=gcr_data);","2948d6a7":"sns.barplot(x=\"Sex\", y=\"Credit amount\", hue=\"Risk\", data=gcr_data);","f1292414":"sns.pointplot(x=\"Housing\", y=\"Duration\", hue=\"Sex\", data=gcr_data,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);","814b5302":"y_tar = (gcr_data['Risk']=='good').astype(int)\ncorrelations = gcr_data[list(gcr_data.columns[:-1])].corrwith(y_tar)\ncorrelations.sort_values(inplace=True)\ncorrelations","3511fc8e":"ax = correlations.plot(kind='bar')\nax.set(ylim=[-1, 1], ylabel='pearson correlation');","6419d26d":"cat_cols = gcr_data.columns[gcr_data.dtypes == 'O']\nnum_cols = gcr_data.columns[gcr_data.dtypes == 'int']\n#ordinal_cols = [ 'Housing', 'Saving accounts', 'Checking account'] \n\n#nominal_cols = ['Purpose']\n","3e14aafa":"print(cat_cols)","f33ab4b7":"replace_map = {'Housing': {'free': 1, 'rent': 2, 'own': 3}}\ngcr_data.replace(replace_map, inplace=True)","67a732f0":"replace_map = {'Saving accounts': {'little': 1, 'moderate': 2, 'quite rich': 3, 'rich': 4}}\ngcr_data.replace(replace_map, inplace=True)","91fc861c":"replace_map = {'Checking account': {'little': 1, 'moderate': 2, 'rich': 3}}\ngcr_data.replace(replace_map, inplace=True)","66b8303a":"gcr_data.head()","116e6caa":"binary_cols = ['Sex', 'Risk']\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor column in binary_cols:\n    gcr_data[column] = le.fit_transform(gcr_data[column])","44bc88ee":"gcr_data.head()","4a69a3a4":"nominal_cols = ['Housing', 'Saving accounts', 'Checking account', 'Purpose']\n\ngcr_data = pd.get_dummies(gcr_data, columns=nominal_cols, drop_first=False)","3bea5f2d":"gcr_data.head()","4451183b":"gcr_data.nunique()","4f40746d":"feature_cols = [x for x in gcr_data.columns if x!='Risk']\nX = gcr_data[feature_cols]\ny = gcr_data['Risk']","6d22a817":"X.head()","28339901":"# define imputer\nimputer = IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')","c9e66d8a":"# fit on the dataset\nimputer.fit(X)","3a1e79b5":"# transform the dataset\nXtrans = imputer.transform(X)","a1973a3d":"X = pd.DataFrame(Xtrans, columns=X.columns)","4b88beb7":"X.isnull().sum()","7c8664c5":"X.dtypes","dfd15776":"#num_cols = X.columns[X.dtypes == 'float']\nnum_cols = X.columns\nnum_cols","cc72ebd2":"skew_vals = X[num_cols].skew()\n\nskew_limit = 0.75\nskew_cols = (skew_vals.\n            sort_values(ascending=False)\n            .to_frame()\n            .rename(columns={0:'Skew'})\n            .query('abs(Skew) > {}'.format(skew_limit)))\nskew_cols","7b8db507":"for col in skew_cols.index.values:\n    X[col] =X[col].apply(np.log1p)","a8fdf0d1":"X.head()","4f113cf2":"X.isnull().sum()","21f0c5cd":"gcr_data_imputed = pd.concat([X, y], axis=1)","1926364f":"outputfile = 'gcr_data_imputed2.csv'\ngcr_data_imputed.to_csv(outputfile, index=False)","82140a34":"import numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\n\nfrom scipy import stats \nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report, f1_score, precision_score, recall_score\nfrom sklearn.metrics import roc_curve, precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\n\n#from colorsetup import colors, palette","4344deb2":"gcr_data = pd.read_csv(\"gcr_data_imputed2.csv\")\n#gcr_data = pd.read_csv(\"gcr_data_imputed.csv\")\n#gcr_data = pd.read_csv(\"gcr_processed.csv\")","6207ba54":"gcr_data.head()","e1c408f3":"gcr_data['Credit_amount\/duration'] = gcr_data['Credit amount']\/gcr_data['Duration']","57b8e7fc":"gcr_data.head()","07a53720":"#X = gcr_data.drop(['Risk','Credit_amount\/duration'], axis=1)\n#X = gcr_data.drop(['Risk', 'Credit amount'], axis=1)\nX = gcr_data.drop(['Risk'], axis=1)\n\ny = gcr_data['Risk']","bfc02a59":"X.shape","73795c78":"#apply SelectKBest class to extract top 5 best features\nbestfeatures = SelectKBest(score_func=chi2, k=6)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns, dfscores], axis=1)\nfeatureScores.columns = ['Specs', 'Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10, 'Score'))  #print best features","0e67fec0":"#X = gcr_data[featureScores.nlargest(23, 'Score')['Specs'].values]","30815261":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_fi = ExtraTreesClassifier()\nmodel_fi.fit(X,y)\n#print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model_fi.feature_importances_, index=X.columns)\nfeat_importances.sort_values(ascending=True).nlargest(14).plot(kind='barh')\nplt.show()","140ddf2f":"# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\nmodel_eval = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n\nresults = list()\nfor i in range(1,X.shape[1]+1):\n        scores = evaluate_model(model_eval, X[feat_importances.nlargest(i).index], y)\n        results.append(scores)\n        print('> %s) %.3f (%.3f)' % (i, mean(scores), std(scores)))","76edebe4":"no_of_features = [str(i) for i in range(1,X.shape[1]+1)]\n# plot model performance for comparison\nplt.figure(figsize=(8,6))\nplt.boxplot(results, labels=no_of_features, showmeans=True)\nplt.xticks(rotation=75)\nplt.title('No. of features vs. Average Accuracy')\nplt.show()","4c5055f1":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    # lr\n    rfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\n    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n    models['lr'] = Pipeline(steps=[('s',rfe),('m',model)])\n    \n    # perceptron\n    rfe = RFE(estimator=Perceptron(), n_features_to_select=5)\n    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n    models['per'] = Pipeline(steps=[('s',rfe),('m',model)])\n    \n    # cart\n    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n    models['cart'] = Pipeline(steps=[('s',rfe),('m',model)])\n    \n    # rf\n    rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)\n    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n    models['rf'] = Pipeline(steps=[('s',rfe),('m',model)])\n    \n    # gbm\n    rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=5)\n    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n    models['gbm'] = Pipeline(steps=[('s',rfe),('m',model)])\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores","43efaa73":"# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s) %.3f (%.3f)' % (name, mean(scores), std(scores)))","63131406":"# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.title('RFE Estimator vs. Average Accuracy')\nplt.show()","71395b11":"# get a list of models to evaluate\ndef best_estimator():\n    models = dict()\n    for i in range(2, X.shape[1]+1):\n        rfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=i)\n        model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n        models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n    return models\n\n# evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n# get the models to evaluate\nmodels = best_estimator()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('> %s) %.3f (%.3f)' % (name, mean(scores), std(scores)))","15ba9f46":"# plot model performance for comparison\nplt.figure(figsize=(8,6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.xticks(rotation=75)\nplt.title('No. of features vs. Average Accuracy')\nplt.show()","72f8077b":"# define RFE\nrfe = RFE(estimator=GradientBoostingClassifier(), n_features_to_select=8)\n# fit RFE\nrfe.fit(X, y)\n# summarize all features\nfor i in range(X.shape[1]):\n    print('Column: %d, Selected %s, Rank: %.3f' % (i, rfe.support_[i], rfe.ranking_[i]))","db45a7fe":"#X = gcr_data[feat_importances.nlargest(12).index]\nX = gcr_data[X.columns[rfe.support_]]\n\nX.head()","31f94d84":"from sklearn.model_selection import StratifiedShuffleSplit\n\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(X, gcr_data.Risk))\n\n# Create the dataframes\nX_train = X.loc[train_idx, X.columns]\ny_train = gcr_data.loc[train_idx, 'Risk']\n\nX_test  = X.loc[test_idx, X.columns]\ny_test  = gcr_data.loc[test_idx, 'Risk']","2b7974a8":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","0e72a58d":"# Suppress warnings about too few trees from the early models\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)","5dc3bb03":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n#ABC = AdaBoostClassifier(n_estimators=100, base_estimator= None,learning_rate=1, random_state = 1)\n\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.001]}\n\nABC_GCV = GridSearchCV(ABC,\n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\nABC_GCV = ABC_GCV.fit(X_train, y_train)\n\n# The best model\nprint(ABC_GCV.best_estimator_)\n\nABC_GCV = AdaBoostClassifier(n_estimators=100, base_estimator= DecisionTreeClassifier(max_depth=1),learning_rate=0.01)\nABC_GCV = ABC_GCV.fit(X_train, y_train)","9202d024":"y_pred = list()\ny_prob = list()\n\nlabels = ['ABC_GCV']\nmodels = [ABC_GCV]\n\nfor lab,mod in zip(labels, models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\nmetrics = list()\ncm = dict()\n\nfor lab in labels:\n\n    # Precision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1]),\n              label_binarize(y_pred[lab], classes=[0,1]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)\n\nmetrics","88b136b9":"sns.set_context('talk')\n\nfig, axList = plt.subplots(nrows=1, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(10, 4)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList, labels):\n    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()","bc89f843":"from sklearn.metrics import classification_report, f1_score\n\nprint('#'*60)\n\ny_pred_gb = ABC_GCV.predict(X_test)\nprint('AdaBoostClassifier')\nprint(classification_report(y_test, y_pred_gb))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred_gb), 3))\nprint('F1 Score: ', round(f1_score(y_test, y_pred_gb), 3))\n\nprint('\\n')\nprint('#'*60)","e66651ba":"sns.set_context('talk')\n\nfig, axList = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(10, 5)\n\n# Plot the ROC-AUC curve\n\nax = axList[0]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob[lab])\nax.plot(fpr, tpr, linewidth=5)\n\n# It is customary to draw a diagonal dotted line in ROC plots.\n# This is to indicate completely random prediction. Deviation from this\n# dotted line towards the upper left corner signifies the power of the model.\nax.plot([0, 1], [0, 1], ls='--', color='black', lw=.3)\nax.set(xlabel='FPR',\n       ylabel='TPR',\n       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n       title='ROC curve: {}'.format(lab))\nax.grid(True)\n\n# Plot the precision-recall curve\n\nax = axList[1]\nprecision, recall, _ = precision_recall_curve(y_test, y_prob[lab])\nax.plot(recall, precision, linewidth=5)\nax.set(xlabel='Recall', ylabel='Precision',\n       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n       title='Precision-Recall curve: {}'.format(lab))\nax.grid(True)\n\nplt.tight_layout()","822fc94c":"from sklearn.impute import SimpleImputer\n\nvalues = gcr_data.values \nimputer = SimpleImputer(missing_values= np.nan, strategy='constant', fill_value='missing') \ntransformed_values = imputer.fit_transform(values) ","bd8d9b6e":"<a id=\"conmat_abc\"><\/a>\n<h2>5.4 Confusion Matrix and Classification Report<\/h2>","2b6db53c":"<a id=\"metrics_abc\"><\/a>\n<h2>5.2 Metrics<\/h2>","1ca34f58":"### 2) IterativeImputer","05a03e9e":"<a id=\"modelling_abc\"><\/a> <br>\n# **5. Models**","3ac548e6":"### Read the Data","8852e4ab":"### 3. Correlation Matrix with Heatmap","ec0531de":"### Label encoding categorical variables","75cfca4f":"## 1.) Imports","aa7fbda8":"### 1) SimpleImputer","c46e07be":"## 4).Feature Engineering","422ee90a":"outputfile = 'gcr_data_imputed.csv'\ngcr_data_imputed.to_csv(outputfile, index=False)","b4d67f0a":"<h1>**AdaBoost Classifier on German Credit Risk data set**<\/h1>","f5baef4a":"<h2>5.1 AdaBoostClassifier with GridSearchCV<\/h2>","0f964d30":"<a id=\"summary_abc\"><\/a> <br>\n# **6. Conclusion and Benefits of the model**\n\n- The AdaBoostClassifier with GridSearchCV gives the FPR of 20% and 71.0% accuracy.\n- Changing the number of important features does not affect the metrics.","14dedbf9":"<a id=\"preprocessing_abc\"><\/a> <br>\n# **4. Preprocess**","9d30b78f":"## 2.) Read the data","06356506":"<h2>Categorical Missing Values Imputation<\/h2>\n","76a11fff":"### 4. RFE","0e91b86d":"## 3.) Visualize the data","f1cc0ec5":"<a id=\"intro_abc\"><\/a> <br>\n# **1. Introduction:** \n<h2>Context<\/h2>\nThe original dataset contains 1000 entries with 9 feature variables. In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes.\n\n<h2>Content<\/h2>\nI have cleaned and preprocessed the data already and also I have created a more relevant feature from two of the most important feature in the data set i.e. Credit amount and Duration. The preprocessed data set is already saved in a CSV file and we are going to use that file for our model training and testing purposes. The selected variables from the orginal data set are:\n\n<b>Age <\/b>(numeric)<br>\n<b>Sex <\/b>(text: male, female)<br>\n<b>Job <\/b>(numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)<br>\n<b>Housing<\/b> (text: own, rent, or free)<br>\n<b>Saving accounts<\/b> (text - little, moderate, quite rich, rich)<br>\n<b>Checking account <\/b>(numeric, in DM - Deutsch Mark)<br>\n<b>Credit amount<\/b> (numeric, in DM)<br>\n<b>Duration<\/b> (numeric, in month)<br>\n<b>Purpose<\/b>(text: car, furniture\/equipment, radio\/TV, domestic appliances, repairs, education, business, vacation\/others<br>\n<b>Risk <\/b> (Value target - Good or Bad Risk)<br>","f0faf1c4":"<a id=\"libraries_abc\"><\/a> <br>\n# **3. Libraries**","71bae42e":"<h2>5.5 ROC curve and Precision-Recall curve<\/h2>","babdfd82":"### Imports","ede923f4":"# Tables of Content:\n\n**1. [Introduction](#intro_abc)** <br>\n    - Information about the data set <br>\n**2. [Reason for using this model](#reasons_abc)** <br>\n    - The purpose of this specific model <br>\n**3. [Libraries](#libraries_abc)** <br>\n    - Importing Libraries <br>\n    - Importing Dataset <br>\n**4. [Preprocess](#preprocessing_abc)** <br>\n    - 4.1 Separating feature and target variables <br>\n    - 4.2 [Feature Selection](#feature_selection_abc)<br>\n    - 4.3 [Spliting the X and Y in train and test](#split_abc)<br>\n**5. [Models](#modelling_abc)**<br>\n    - 5.1 AdaBoostClassifier with GridSearchCV<br>\n    - 5.2 [Metrics](#metrics_abc)<br>\n    - 5.3 [Confusion Matrix and Classification Report](#conmat_abc)<br>\n    - 5.4 ROC curve and Precision Recall curve<br>\n**6. [Conclusion and Benefits of the model](#summary_abc)** <br>\n    The summary of the model implementation","6933b17e":"<a id=\"feature_selection_abc\"><\/a>\n<h2>4.2 Feature Selection<\/h2>","1764bc8a":"### 1. Univariate Selection","f118f791":"### 2. Feature Importance","40b518ed":"<h1>**Preprocessing on German Credit Risk data set**<\/h1>","3c1efdfb":"sns.set_context('talk')\n#sns.set_palette(palette)\nsns.set_style('white')\n\nsns.pairplot(gcr_data, hue='Risk')","3bf88def":"#get correlations of each features in dataset\ncorrmat = gcr_data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n\n#plot heat map\ng=sns.heatmap(gcr_data[top_corr_features].corr(), annot=False, cmap=\"RdYlGn\")","6037c8a5":"gcr_data_imputed = pd.DataFrame(transformed_values, columns=gcr_data.columns)","4519f590":"<h2>4.1 Separating feature and target<\/h2>","f182180c":"### Log transforming skew variables","a877758e":"gcr_data_imputed[\"Credit amount\"] = gcr_data_imputed[\"Credit amount\"].astype(int)\ngcr_data_imputed[\"Duration\"] = gcr_data_imputed[\"Duration\"].astype(int)\ngcr_data_imputed[\"Job\"] = gcr_data_imputed[\"Job\"].astype(int)\ngcr_data_imputed[\"Age\"] = gcr_data_imputed[\"Age\"].astype(int)","b856861c":"#### Suppressing any warnings","c16c1afe":"<a id=\"resons_abc\"><\/a> <br>\n# **2. Reason for using this model**\n<h2>Our goal is to: <\/h2>\n\n- Implement AdaBoostClassifier with GridSearchCV.\n- Moreover, we are going to assess various metrics for the model and plot area-under-curve and precision-recall curve.\n- We are going to estimate the best estimator i.e. the best hyperparameters for our model.\n- False Positive Rate are calculated using confusion matrix to better understand the potentiality of losses which will incur due to giving loans to the person who will default.","129c7982":"########################################################################################################################################","a02704e8":"Credit Amount means the maximum amount that Lender is committed to lend","692a2ac2":"<a id=\"split_abc\"><\/a>\n<h2>4.3 StratifiedShuffleSplit<\/h2>"}}