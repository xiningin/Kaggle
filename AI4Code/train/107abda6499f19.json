{"cell_type":{"50162636":"code","ed161e49":"code","0429ec9c":"code","02ff9ac7":"code","7dd25b8a":"code","836c16a2":"code","4fe80f1a":"code","396350ca":"code","81315b44":"code","a971c8a8":"code","a693ff50":"code","db384b4e":"code","c3f3bdec":"code","d24f43ad":"code","ed584b3e":"code","3707427c":"markdown","f31dd88d":"markdown","1eace31b":"markdown","6c97d891":"markdown","62ea2234":"markdown","0cd8cd27":"markdown"},"source":{"50162636":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ed161e49":"import pandas as pd\npdf = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","0429ec9c":"pdf.head()","02ff9ac7":"pdf.columns","7dd25b8a":"features = pdf.columns.drop(['diagnosis','id','Unnamed: 32']) ","836c16a2":"features.size","4fe80f1a":"for i in range(30):\n    a = sns.FacetGrid( pdf, hue = 'diagnosis', aspect=4 )\n    a.map(sns.kdeplot, features[i], shade= True )\n    #a.set(xlim=(0 , pdf['radius_mean'].max()))\n    a.add_legend()","396350ca":"X = pdf.drop(['diagnosis','id','Unnamed: 32'], axis = 1)","81315b44":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()#instantiate\nscaler.fit(X) # compute the mean and standard which will be used in the next command\nX_scaled=scaler.transform(X)# fit and transform can be applied together and I leave that for simple exercise\n# we can check the minimum and maximum of the scaled features which we expect to be 0 and 1","a971c8a8":"X_scaled","a693ff50":"from sklearn.decomposition import PCA\npca=PCA(n_components=3) \npca.fit(X_scaled) \nX_pca=pca.transform(X_scaled) \n#let's check the shape of X_pca array\nX_pca.shape","db384b4e":"type(X_pca)","c3f3bdec":"pdf = pd.concat([pdf, pd.DataFrame(X_pca)], axis=1)","d24f43ad":"pdf.head()","ed584b3e":"pal = dict(M=\"seagreen\", B=\"gray\")\ng = sns.FacetGrid(pdf, hue=\"diagnosis\", palette=pal, height=5)\ng.map(plt.scatter, 0, 1, s=50, alpha=.7, linewidth=.5, edgecolor=\"white\")\ng.add_legend()","3707427c":"Now, since the PCA components are orthogonal to each other and they are not correlated, we can expect to see malignant and benign classes as distinct. Let\u2019s plot the malignant and benign classes based on the first two principal components","f31dd88d":"PCA is essentially a method that reduces the dimension of the feature space in such a way that new variables are orthogonal to each other (i.e. they are independent or not correlated). \n\nBefore applying PCA, we scale our data such that each feature has unit variance. This is necessary because fitting algorithms highly depend on the scaling of the features. Here we use the StandardScalermodule for scaling the features individually. StandardScalersubtracts the mean from each features and then scale to unit variance.","1eace31b":"Now from these histograms we see that features like- mean fractal dimension has very little role to play in discerning malignant from benign, but worst concave points or worst perimeter are useful features that can give us strong hint about the classes of cancer data-set. ","6c97d891":"Explanation:\n\nBasic Approach and formule:\n* https:\/\/www.youtube.com\/watch?v=ZUbogbEFSkg\n* https:\/\/www.youtube.com\/watch?v=hhz4eigZQr8\n* https:\/\/www.youtube.com\/watch?v=b0DHvcdIStE\n\nGeometrical Intution:\n* https:\/\/www.youtube.com\/watch?v=PFDu9oVAE-g&t=406s\n\nReferences:\n* https:\/\/towardsdatascience.com\/dive-into-pca-principal-component-analysis-with-python-43ded13ead21\n* https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n\nMore Mathy References and SVD:\n* https:\/\/medium.com\/@jonathan_hui\/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491\n* https:\/\/medium.com\/@jonathan_hui\/machine-learning-linear-algebra-a5b1658f0151\n","62ea2234":"You could see how the individual features have contributed to the 3 PCs.\n\nhttps:\/\/towardsdatascience.com\/dive-into-pca-principal-component-analysis-with-python-43ded13ead21","0cd8cd27":"The two classes are well separated with the first 2 principal components as new features. As good as it seems like even a linear classifier could do very well to identify a class from the test set."}}