{"cell_type":{"9ba0baef":"code","aaad0bdc":"code","8ba374fc":"code","e6a7b91c":"code","03d8a8ad":"code","f5b04159":"code","dee45a66":"code","56ae3b24":"code","843bd1de":"code","6034c0aa":"code","253962df":"code","29260c42":"code","2e7d90d5":"code","f3270104":"code","c2b33e31":"code","f5b9e0ea":"code","ea38b59b":"code","8cd38643":"code","77396241":"code","7906b920":"code","62b2ab23":"code","0cb5fe6d":"code","0071db01":"code","584ee40d":"code","e0d9238e":"code","2095dacb":"code","21be033b":"code","0880be43":"code","68c6f806":"code","beed4b8a":"code","3d549724":"code","3b718a24":"code","44169a58":"code","455d4492":"code","409d54ca":"code","7a9507f9":"code","655d5949":"code","6afd60e3":"code","5e897355":"code","51883d42":"code","5aab8098":"code","733830e6":"code","a50e81bb":"code","71309a04":"code","58098141":"code","256789c0":"code","8f8c59ae":"code","ccc1c26e":"code","8fff34b2":"code","f6554f5e":"code","4652d7ea":"code","a60b5be3":"code","100a0434":"code","5e44dcc9":"code","e0ca2803":"code","5b50a050":"code","0d766ad2":"code","3c8cb296":"code","5d1fbe0b":"code","4a5bab4d":"code","05751e29":"code","dc5af0b2":"code","ed7d4429":"markdown","f005d597":"markdown","dfe8300e":"markdown","6a627082":"markdown","f93f7e83":"markdown","60a5e1c1":"markdown","3cfe9084":"markdown","27c17dff":"markdown","c4378c12":"markdown","efda0846":"markdown","405262ca":"markdown","5666c886":"markdown","fcaf1a82":"markdown","3bee9e11":"markdown","4b2b2214":"markdown","108081cc":"markdown","8235b724":"markdown","01789155":"markdown","a1a3462b":"markdown","1a58b403":"markdown","be20929a":"markdown","cd8b2878":"markdown","d9b10c1d":"markdown","dae8cbca":"markdown","4d9ffe40":"markdown","45cd6085":"markdown","31479706":"markdown","8b0fade8":"markdown","85464a24":"markdown","59a015f0":"markdown","71bbf7ef":"markdown","e91638f0":"markdown","0d1826d8":"markdown","57c86059":"markdown","6bc7116a":"markdown","d4941e23":"markdown","c8d85993":"markdown","4f5ced95":"markdown","937be679":"markdown","6a632957":"markdown","b6e28339":"markdown","41f90e14":"markdown","09b45134":"markdown","6da862b2":"markdown","b6016fd1":"markdown","ac73b03b":"markdown","3ce72e34":"markdown","4548ba05":"markdown","8298d0a2":"markdown","0cf87347":"markdown","8b10bdd3":"markdown","8ec0711f":"markdown","79395976":"markdown","c6cc356f":"markdown","cfb758a1":"markdown","1f8868e3":"markdown","88b562ee":"markdown","cc043a53":"markdown","70f01d22":"markdown"},"source":{"9ba0baef":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import ceil\n\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.manifold import TSNE\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual,VBox,HBox,Layout\nimport ipywidgets as widgets","aaad0bdc":"data= pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv',header=0,encoding=\"ISO-8859-1\",engine='c')\nprint('Number of rows={0:.0f} and columns={1:.0f} \\n'.format(data.shape[0],data.shape[1]))\nprint(data.info())","8ba374fc":"data.head()","e6a7b91c":"data['InvoiceDate']=pd.to_datetime(data['InvoiceDate'])\ndata['Sales'] = data.Quantity*data.UnitPrice\ndata['Year']=data.InvoiceDate.dt.year\ndata['Month']=data.InvoiceDate.dt.month\ndata['Week']=data.InvoiceDate.dt.isocalendar().week\ndata['Year_Month']=data.InvoiceDate.dt.to_period('M')\ndata['Hour']=data.InvoiceDate.dt.hour\ndata['Day']=data.InvoiceDate.dt.day\ndata['is_cancelled']=data.InvoiceNo.apply(lambda x: 'Yes' if x[0]=='C' else 'No')\ndata['weekday'] = data.InvoiceDate.dt.day_name()\ndata['Quarter'] = data.Month.apply(lambda m:'Q'+str(ceil(m\/4)))\ndata['Date']=pd.to_datetime(data[['Year','Month','Day']])\ndata.head()","03d8a8ad":"missing_data = data.isnull().sum().reset_index()\nmissing_data.columns=['Variables','missing_count']\n#missing_data['Missing_perc']\nmissing_data['missing_perc']=missing_data.missing_count\/data.shape[0]\nprint(missing_data)\ndel missing_data","f5b04159":"\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(go.Box(y=data.UnitPrice),row=1, col=1)\nfig.add_trace(go.Box(y=data.Quantity),row=1, col=2)\nfig.add_trace(go.Box(y=data.Sales),row=1, col=3)\n\nfig.update_xaxes(title_text=\"Unit Price\", row=1, col=1)\nfig.update_xaxes(title_text=\"Quantity\", row=1, col=2)\nfig.update_xaxes(title_text=\"Sales\", row=1, col=3)\n\nfig.update_layout(height=500, width=700)\nfig.show()","dee45a66":"df = px.data.gapminder()\ndf = df [['country','iso_alpha']]\ndata = pd.merge(data,df[['country','iso_alpha']],left_on='Country',right_on='country',how='left').drop(columns=['country'])\ndel df","56ae3b24":"grp_data = data.groupby(by='Country')['Sales'].sum().sort_values(ascending=False).reset_index()\n\nfig = go.Figure(data=go.Choropleth(\n    locations = grp_data['Country'],\n    z = grp_data['Sales'],\n    text = grp_data['Country'],\n    colorscale = 'earth',\n    locationmode = 'country names',\n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = 'Sales',\n))\n\n\nfig.update_layout(\n    title_text='Sales by country',\n    geo=dict(showframe=False,showcoastlines=False,projection_type='equirectangular'),\n    annotations = [dict(x=0.55,y=0.1,xref='paper',yref='paper',showarrow = False)])\n\nfig.show()\n\ndel grp_data","843bd1de":"data_=data[data.is_cancelled=='No']\ndel data","6034c0aa":"quarter_sales_country = data_.pivot_table(index='Country', columns='Quarter', values='Sales',aggfunc='sum').fillna(0)\nrank_data = quarter_sales_country.apply(lambda x: x.rank(method='dense').astype(int)).reset_index()\nrank_data['idx']=rank_data.index\nrank_data = rank_data.melt(id_vars=['Country','idx'])\n\nfig = px.scatter(rank_data , y=\"Country\", x='Quarter', color=\"Country\", hover_data=['Country']\n                 ,size='value')\n#fig.data[0].update(mode='markers+lines')\nfig.update_layout(height=1500,width=700)\nfig.show()\n\ndel [rank_data,quarter_sales_country]","253962df":"sales_by_date = data_.groupby(by='Date')['Sales'].sum().reset_index()\nfig = go.Figure(data=go.Scatter(x=sales_by_date.Date,y=sales_by_date.Sales\n                                ,line = dict(color='black', width=1.5)))\nfig.update_layout(xaxis_title=\"Date\",yaxis_title=\"Sales\",title='Daily Sales',template='ggplot2')\nfig.show()","29260c42":"sales_by_hour = data_.groupby(by='Hour')['Sales'].sum().reset_index()\nsales_by_weekday = data_.groupby(by='weekday')['Sales'].sum().reset_index()\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Total Hourly Sales\", \"Total Sales by Weekday\"))\nfig.add_trace(go.Bar(y=sales_by_hour.Hour, x=sales_by_hour.Sales,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(x=sales_by_weekday.weekday, y=sales_by_weekday.Sales),row=1, col=2)\nfig.update_layout(height=700, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Sales\", row=1, col=1)\nfig.update_xaxes(title_text=\"Weekday\", row=1, col=2)\nfig.update_yaxes(title_text=\"Hours\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales\", row=1, col=2)\nfig.show()\n\n\ndel [sales_by_hour,sales_by_weekday]","2e7d90d5":"customer_by_month1 = data_.groupby('CustomerID')['Date'].min().reset_index()\ncustomer_by_month1['days'] = pd.TimedeltaIndex(customer_by_month1.Date.dt.day,unit=\"D\")\ncustomer_by_month1['Month'] = customer_by_month1.Date- customer_by_month1.days+pd.DateOffset(days=1)\ncustomer_by_month1['Quarter_acquisition'] = customer_by_month1['Month'].dt.quarter.apply(lambda x:'Q'+str(x))\ncustomer_by_month1['Year_acquisition'] = customer_by_month1['Month'].dt.year\ncustomer_by_month = data_.groupby(by = customer_by_month1.Month)['CustomerID'].size().reset_index()\ncustomer_by_month.sort_values(by ='Month',ascending=True,inplace=True)\ncustomer_by_month['cum_customer'] = np.cumsum(customer_by_month.CustomerID)\ncustomer_by_month['Month_1'] = customer_by_month['Month'].dt.strftime('%b-%y')\n","f3270104":"\nplt.style.use('ggplot')\nplt.figure(figsize=(20,5))\nplt.plot(customer_by_month.Month_1,customer_by_month.cum_customer,'bo-',color='black')\n\n# zip joins x and y coordinates in pairs\nfor d,c in zip(customer_by_month['Month_1'],customer_by_month['cum_customer']):\n\n    label = \"{:.0f}\".format(c)\n\n    plt.annotate(label, \n                 (d,c), \n                 textcoords=\"offset points\"\n                 , bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\")\n                 #,arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"angle,angleA=0,angleB=10,rad=90\")\n                 ,xytext=(0,10),\n                 ha='center') \nplt.show()\n\ndel customer_by_month","c2b33e31":"customer_sales = data_.groupby(by = ['CustomerID','Year','Quarter'])['Sales'].sum().reset_index()\ncustomer_sales = customer_sales.merge(customer_by_month1[['CustomerID','Quarter_acquisition','Year_acquisition']]\n                                      ,on ='CustomerID',how='inner')\ncustomer_sales_acquisition = customer_sales.groupby(by=['Year','Quarter','Year_acquisition','Quarter_acquisition'])['Sales'].sum().reset_index()\ncustomer_sales_acquisition['Sales_Year_quarter'] =customer_sales_acquisition[['Year','Quarter']].apply(lambda row:str(row.Year)+'-'+row.Quarter,axis=1)\ncustomer_sales_acquisition['Acquisition_Year_quarter'] =customer_sales_acquisition[['Year_acquisition','Quarter_acquisition']].apply(lambda row:str(row.Year_acquisition)+'-'+row.Quarter_acquisition,axis=1)\ncustomer_sales_acquisition.drop(columns =['Year','Quarter','Year_acquisition','Quarter_acquisition'],inplace=True)\ndf = customer_sales_acquisition.pivot(index='Sales_Year_quarter',columns =['Acquisition_Year_quarter']).fillna(0).reset_index()\n\n\nfig = go.Figure(data=[\n    go.Bar(name='First Order 2010-Q4', x=df.Sales_Year_quarter, y=df.iloc[:,1],marker_color='lightslategrey'),\n    go.Bar(name='First Order 2011-Q1', x=df.Sales_Year_quarter, y=df.iloc[:,2],marker_color='lightblue'),\n    go.Bar(name='First Order 2011-Q2', x=df.Sales_Year_quarter, y=df.iloc[:,3],marker_color='seagreen'),\n    go.Bar(name='First Order 2011-Q3', x=df.Sales_Year_quarter, y=df.iloc[:,4],marker_color='orange')\n])\nfig.update_layout(barmode='stack',template='ggplot2')\nfig.show()\n\ndel [customer_sales_acquisition, df,customer_sales]","f5b9e0ea":"cust_date =data_.loc[~data_.CustomerID.isna(),['CustomerID','Date']].drop_duplicates()\ncust_date.sort_values(by=['CustomerID','Date'],inplace=True)\ncust_date['rnk'] = cust_date.groupby(by='CustomerID')['Date'].transform('rank', method='dense')\ncust_date = cust_date[cust_date.rnk<=2]\ncust_date['Purchase'] = cust_date.rnk.map({1:'First Purchase',2:'Second Purchase'})\n\ncust_purchase= cust_date.pivot(index='CustomerID',columns=['Purchase'],values='Date').reset_index()\n\ncust_purchase['gap']=(cust_purchase['Second Purchase']- cust_purchase['First Purchase'])\/ np.timedelta64(30, 'D')\ncust_purchase['gap'] = cust_purchase['gap'].fillna(0).apply(lambda x:ceil(x))\n\ndays = pd.TimedeltaIndex(cust_purchase['First Purchase'].dt.day,unit=\"D\")\n\ncust_purchase['First_Purchase_month']=cust_purchase['First Purchase']- days+pd.DateOffset(days=1)\ncust_purchase_grid = cust_purchase.pivot_table(index=['First_Purchase_month'],columns =['gap'],values='CustomerID',aggfunc='count').fillna(0)\ncust_purchase_grid = cust_purchase_grid.sort_index( ascending=False)\n\nfig = go.Figure(data=go.Heatmap(z=cust_purchase_grid\n                                ,y=cust_purchase_grid.index,\n                   x=cust_purchase_grid.columns.tolist(),\n                   hoverongaps = True,colorscale='balance'))\nfig.update_xaxes(side=\"top\",showticklabels = True,ticktext= cust_purchase_grid.columns.tolist())\nfig.update_layout(height=600,width=800,xaxis_title=\"Future Month of next Purchase after making first purchase\"\n                  ,yaxis_title=\"Month of 1st Purchase\",\n           yaxis=dict(autorange='reversed',showticklabels = True)\n                  ,autosize=False,template='ggplot2')\n\nfig.show()\n\ndel [cust_date,cust_purchase,days,cust_purchase_grid]","ea38b59b":"sales_by_hour = data_.groupby(by='Hour')['Sales'].mean().reset_index()\nsales_by_weekday = data_.groupby(by='weekday')['Sales'].mean().reset_index()\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Avg Hourly Sales\", \"Avg Sales by Weekday\"))\nfig.add_trace(go.Bar(y=sales_by_hour.Hour, x=sales_by_hour.Sales,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(x=sales_by_weekday.weekday, y=sales_by_weekday.Sales),row=1, col=2)\nfig.update_layout(height=700, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Sales\", row=1, col=1)\nfig.update_xaxes(title_text=\"Weekday\", row=1, col=2)\nfig.update_yaxes(title_text=\"Hours\", row=1, col=1)\nfig.update_yaxes(title_text=\"Sales\", row=1, col=2)\nfig.show()\n\ndel [sales_by_hour,sales_by_weekday]","8cd38643":"from statsmodels.formula.api import ols\nimport statsmodels.api as sm\nmodel = ols('Sales ~ C(Hour)',data=data_).fit()\n#ols.summary()\ntable = sm.stats.anova_lm(model, typ=2)\nprint(table)\ndel [table,model]","77396241":"model = ols('Sales ~ C(weekday)',data=data_).fit()\n#moore_lm.summary()\ntable = sm.stats.anova_lm(model, typ=2)\nprint(table)\n\ndel [table,model]\n","7906b920":"LRFM = data_.groupby('CustomerID').agg(Frequency=pd.NamedAgg(column=\"InvoiceNo\", aggfunc=\"nunique\")\n                                        ,Monetary=pd.NamedAgg(column=\"Sales\", aggfunc=\"sum\")).reset_index()\n\nlength = data_.groupby('CustomerID')['Date'].max() - data_.groupby('CustomerID')['Date'].min()\nlength =  (length\/np.timedelta64(1, 'D')).reset_index()\nlength.columns = ['CustomerID','Length_of_stay']\nLRFM = LRFM.merge(length,on='CustomerID',how='inner')\ndel length\n\nLRFM.head()","62b2ab23":"fig = go.Figure(data=[go.Scatter3d(x=LRFM.Monetary,y=LRFM.Length_of_stay,z=LRFM.Frequency,mode='markers'\n                                   ,marker=dict(size=4,color='coral',colorscale='Viridis',opacity=0.8))])\nfig.update_layout(margin=dict(l=1, r=2, b=1, t=1)\n                  ,scene=dict(xaxis=dict(title='Sales')\n                              ,yaxis=dict(title='Lenght of Stay')\n                              ,zaxis=dict(title='Frequency')),width=800,height=500)\nfig.show()","0cb5fe6d":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.manifold import TSNE","0071db01":"X=LRFM.drop(columns = 'CustomerID')\nerror = []\nsilhouette = []\nnp.random.seed(12)\nrng = range(2,20)\nfor i in rng:\n    km = KMeans(n_clusters=i, init='random',n_init=20, max_iter=200,tol=.0001, random_state=12)\n    km.fit(X)\n    error.append(km.inertia_)\n    lbls = km.fit_predict(X)\n    silhouette.append(silhouette_score(X, lbls))\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Number of Cluster vs Error\", \"Number of Cluster vs Silhouette\"))\nfig.add_trace(go.Scatter(x=list(rng), y=error),row=1, col=1)\nfig.add_trace(go.Scatter(x=list(rng), y=silhouette),row=1, col=2)\nfig.update_layout(height=500, width=800,template='ggplot2')\nfig.update_xaxes(title_text=\"Number of Clusters\", row=1, col=1)\nfig.update_xaxes(title_text=\"Number of Clusters\", row=1, col=2)\nfig.update_yaxes(title_text=\"Errors\", row=1, col=1)\nfig.update_yaxes(title_text=\"Silhouette Distance\", row=1, col=2)\nfig.show()\n\ndel [error,silhouette,rng,km,lbls,X]","584ee40d":"X=LRFM.drop(columns = 'CustomerID')\ncluster_lbls = KMeans(n_clusters=10, random_state=12).fit_predict(X)\nX['cluster'] = cluster_lbls\nX['sample_silhouette_values'] = silhouette_samples(X, cluster_lbls)\nX['txt']=X.cluster.apply(lambda x:'Cluster '+str(x))","e0d9238e":"\nfig = go.Figure(data=go.Scatter(x=X.Monetary,y=X.sample_silhouette_values\n                                , mode='markers',marker_color=X.cluster,text = X.txt))\n\nfig.update_layout(xaxis_title=\"Sales\",yaxis_title=\"Silhouette\"\n                  ,title='Sample Size vs Silhoutte Values')\n\nfig.show()\n","2095dacb":"df = X.groupby('cluster').agg({'cluster':'size', 'Monetary':'mean','Frequency':'mean','Length_of_stay':'mean'}) \\\n       .rename(columns={'cluster':'Size','Monetary':'Avg Sales','Frequency':'Avg Recency','Length_of_stay':'Avg Lenght of Stay'}) \\\n       .reset_index().sort_values(by = 'Avg Sales')\n\ncluster_map ={'Cluster 4':'lightskyblue','Cluster 0':'lightskyblue','Cluster 8':'lightskyblue'\n              ,'Cluster 6':'lightskyblue','Cluster 2':'lightskyblue',\n             'Cluster 3':'orange','Cluster 7':'orange','Cluster 9':'orange'\n              ,'Cluster 1':'olive','Cluster 5':'olive'}\n\ntxt =['Size = {0:.0f}'.format(i) for i in df.Size]\ndf['cluster']=df.cluster.apply(lambda x:'Cluster '+str(x))\ndf['Group']=df.cluster.map(cluster_map)\n\nfig = make_subplots(rows=1, cols=3,subplot_titles=(\"Avg Sales\", \"Avg Recency\",'Avg Lenght of Stay'))\n\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Sales'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=1)\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Recency'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=2)\nfig.add_trace(go.Bar(y=df.cluster, x=df['Avg Lenght of Stay'],hovertext=txt\n                        ,text=txt,textposition='auto',marker_color=df.Group,orientation='h'),row=1, col=3)\n\nfig.update_traces(marker_line_color='rgb(8,48,107)', marker_line_width=1.5, opacity=0.8)\nfig.update_layout(title_text='Cluster Size',width = 800,height=600,template='ggplot2'\n                  ,font=dict(family=\"Courier New, monospace\",size=10,color=\"RebeccaPurple\"))\n\nfig.show()\n","21be033b":"fig = go.Figure(data=[go.Scatter3d(x=X.Monetary,y=X.Length_of_stay,z=X.Frequency,mode='markers'\n                                   ,marker=dict(size=4,color=X.cluster\n                                                ,colorscale='Viridis',opacity=0.8))])\n\n# tight layout\nfig.update_layout(margin=dict(l=1, r=2, b=1, t=1)\n                  ,scene=dict(xaxis=dict(title='Sales')\n                              ,yaxis=dict(title='Lenght of Stay')\n                              ,zaxis=dict(title='Recency')),width=700,height=500)\nfig.show()","0880be43":"fig = go.Figure(data=go.Scatter(x=X.Monetary,y=X.Length_of_stay\n                                , mode='markers',marker_color=X.cluster,text=X.txt))\nfig.update_layout(xaxis_title=\"Sales\",yaxis_title=\"Length of Stay\",title='Sales vs Length of Stay'\n                  ,width=800,height=500)\nfig.show()","68c6f806":"fig = go.Figure(data=go.Scatter(x=X.Monetary,y=X.Frequency\n                                , mode='markers',marker_color=X.cluster,text=X.txt))\nfig.update_layout(xaxis_title=\"Sales\",yaxis_title=\"Frequency\",title='Sales vs Frequency'\n                  ,width=800,height=500)\nfig.show()","beed4b8a":"prpxlt = [10,15,30,40]\nfig = make_subplots(rows=2, cols=2,subplot_titles=['perplexity = %s'%i for i in prpxlt])\n\nfor i, prpxlt_ in enumerate(prpxlt):\n    X_tsne = TSNE(n_components=2, init='random',\n                         random_state=0, perplexity=prpxlt_).fit_transform(X[['Frequency', 'Monetary', 'Length_of_stay']])\n    if i%2==0:\n        c=2\n    else:\n        c=1\n    r = (i)\/\/2+1\n    fig.add_trace(go.Scatter(x=X_tsne[:,0],y=X_tsne[:,1]\n                                    , mode='markers',marker=dict(size=4,color=X.cluster\n                                                ,colorscale='Viridis',opacity=0.8),text=X.txt),row=r, col=c)\n\n    fig.update_xaxes(title_text=\"Component 1\", row=r, col=c)\n    fig.update_yaxes(title_text=\"Component 2\", row=r, col=c)\n    \nfig.update_layout(height=600, width=800)\nfig.show()\n\ndel [X_tsne,fig]","3d549724":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_pacf,plot_acf","3b718a24":"ds_weekly = data_.groupby(by=['Year','Week'])['Sales'].sum().reset_index()\nds_daily = data_.groupby(by=['Date'])['Sales'].sum().reset_index()","44169a58":"def plot_(t_train,t_test,x_train,x_test,x_train_pred,x_test_pred,forecast,title='Weekly'):\n    xt = (max(t_test)+np.arange(len(forecast)))+1\n    fig_train=go.Scatter(name='Train : Actual ',x=t_train,y=x_train,showlegend=True)\n    fig_trian_pred=go.Scatter(name='Train : Predict',x=t_train,y=x_train_pred,showlegend=True)\n    fig_test=go.Scatter(name='Test : Actual',x=t_test,y=x_test,showlegend=True)\n    fig_test_pred=go.Scatter(name='Test : Predict',x=t_test,y=x_test_pred,showlegend=True)\n    fig_forecast=go.Scatter(name='Forecast',x=xt,y=forecast,showlegend=True)\n\n    fig = go.Figure([fig_train,fig_trian_pred,fig_test,fig_test_pred,fig_forecast])\n    fig.update_layout(xaxis_title=title,yaxis_title=\"Sales\",title=title +' Trend'\n                      ,height=400,hovermode=\"x\",template='ggplot2')\n    fig.show()","455d4492":"fig = go.Figure(data=[go.Scatter(x=ds_weekly.index,y=ds_weekly.Sales)])\nfig.update_layout(xaxis_title=\"Week\",yaxis_title=\"Sales\",title='Weekly Trend',height=400,template='ggplot2')\nfig.show()","409d54ca":"output = adfuller(ds_weekly.Sales)\nprint('***************************Week*********************************')\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is extreemly high which indicates that we are fail to reject null hypothesis \" \\\n      \"and can conclude that series is not stationary \")\n","7a9507f9":"d=1\nprint('***************************Week*********************************')\n# series = ds.Sales\nseries = ds_weekly.Sales.diff(d)# - ds.Sales.rolling(window=12).mean()\nseries = series.dropna()\noutput = adfuller(series)\n\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is close to zero which is less than .05 hence we reject null hypothesis and conclude \" \\\n      \" that series is stationary with rolling mean difference at lag of 12 \")","655d5949":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nplot_acf(series, ax=ax[0])\nplot_pacf(series, ax=ax[1])\nplt.show()\n","6afd60e3":"series=ds_weekly.Sales\nsplit_time = 45\ntime=np.arange(len(ds_weekly))\nxtrain=series[:split_time]\nxtest=series[split_time:]\ntimeTrain = time[:split_time]\ntimeTest = time[split_time:]\nprint('Full Set Size ',series.shape)\nprint('Training Set Size ',xtrain.shape)\nprint('Testing Set Size ',xtest.shape)","5e897355":"from statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(xtrain, order=(1,1,2))\nmodel_fit = model.fit()\nprint(model_fit.summary())","51883d42":"ytrain_pred = model_fit.predict()\nytest_pred = model_fit.predict(start=min(timeTest),end=max(timeTest),dynamic=True)\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest)**2)))\nforecast = model_fit.forecast(20, alpha=0.05)\nplot_(t_train = timeTrain,t_test = timeTest,x_train = xtrain,x_test = xtest,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Weekly')","5aab8098":"from statsmodels.tsa.statespace.sarimax import SARIMAX\ns_model = SARIMAX(endog=xtrain , order=(1, 1, 2), seasonal_order=(1, 1, 1, 3), trend='c')\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","733830e6":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest),end=max(timeTest),dynamic=True)\nprint('RMSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain)**2)))\nprint('RMSE Test :',np.sqrt(np.mean((ytest_pred - xtest)**2)))\nforecast = s_model_fit.forecast(20, alpha=0.05)\nplot_(t_train = timeTrain,t_test = timeTest,x_train = xtrain,x_test = xtest,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Weekly')","a50e81bb":"fig = go.Figure(data=[go.Scatter(x=ds_daily.Date,y=ds_daily.Sales)])\nfig.update_layout(xaxis_title=\"Date\",yaxis_title=\"Sales\",title='Daily Trend',height=400,template='ggplot2')\nfig.show()","71309a04":"print('\\n***************************Daily*********************************')\noutput = adfuller(ds_daily.Sales)\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is extreemly high which indicates that we are fail to reject null hypothesis \" \\\n      \"and can conclude that series is not stationary \")","58098141":"print('\\n***************************Daily*********************************')\nseries_date = ds_daily.Sales.diff(d)\nseries_date = series_date.dropna()\noutput = adfuller(series_date)\nprint('ADF Statistic: {0:.2f} and P value:{1:.5f}'.format(*output))\nprint(\"As we can see the p value is close to zero which is less than .05 hence we reject null hypothesis and conclude \" \\\n      \" that series is stationary with rolling mean difference at lag of 12 \")","256789c0":"\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nplot_acf(series_date, ax=ax[0])\nplot_pacf(series_date, ax=ax[1])\nplt.show()\n","8f8c59ae":"series_date=ds_daily.Sales\nsplit_time = 250\ntime_d=np.arange(len(ds_daily))\nxtrain_d=series_date[:split_time]\nxtest_d=series_date[split_time:]\ntimeTrain_d = time_d[:split_time]\ntimeTest_d = time_d[split_time:]\nprint('Full Set Size ',series_date.shape)\nprint('Training Set Size ',xtrain_d.shape)\nprint('Testing Set Size ',xtest_d.shape)","ccc1c26e":"s_model = ARIMA(endog=xtrain_d , order=(1, 1, 1))\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","8fff34b2":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest_d),end=max(timeTest_d),dynamic=True)\n\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain_d)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest_d)**2)))\nforecast = s_model_fit.forecast(20, alpha=0.05)\n\nplot_(t_train = timeTrain_d,t_test = timeTest_d,x_train = xtrain_d,x_test = xtest_d,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Daily')","f6554f5e":"from statsmodels.tsa.statespace.sarimax import SARIMAX\ns_model = SARIMAX(endog=xtrain_d , order=(1, 1, 1), seasonal_order=(1, 1, 2, 12), trend='t')\ns_model_fit=s_model.fit()\nprint(s_model_fit.summary())","4652d7ea":"ytrain_pred = s_model_fit.predict()\nytest_pred = s_model_fit.predict(start=min(timeTest_d),end=max(timeTest_d),dynamic=True)\n\nprint('MSE Train :',np.sqrt(np.mean((ytrain_pred - xtrain_d)**2)))\nprint('MSE Test :',np.sqrt(np.mean((ytest_pred - xtest_d)**2)))\nforecast = s_model_fit.forecast(30, alpha=0.05)\nxt = max(timeTest_d)+np.arange(len(forecast))\n\nplot_(t_train = timeTrain_d,t_test = timeTest_d,x_train = xtrain_d,x_test = xtest_d,x_train_pred = ytrain_pred\n      ,x_test_pred = ytest_pred,forecast = forecast,title='Daily')","a60b5be3":"from sklearn.linear_model import LinearRegression\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.stattools import durbin_watson\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import linear_rainbow\nfrom statsmodels.stats.api import het_goldfeldquandt\nfrom sklearn.metrics import r2_score","100a0434":"xtrain1,xtest1,ytrain1,ytest1 = train_test_split(LRFM[['Frequency', 'Length_of_stay']],LRFM.Monetary\n                                             ,test_size=.3,random_state=49,shuffle=True)\nprint(xtrain1.shape)\nprint(xtest1.shape)\n","5e44dcc9":"from sklearn.preprocessing import PolynomialFeatures","e0ca2803":"# poly = PolynomialFeatures(2)\n# xtest = poly.fit_transform(xtest1)\n# xtrain = poly.fit_transform(xtrain1)\n\ninp_test=sm.add_constant(np.log1p(xtest1))\ninp_train=sm.add_constant(np.log1p(xtrain1))\nytrain=np.log1p(ytrain1)\nytest=np.log1p(ytest1)\nols=sm.OLS(ytrain,inp_train)\nmodel=ols.fit()\nprint(model.summary())","5b50a050":"print('Below are the interprataion of all 2 coeficients')\nprint('if customers increase the frequency of purchase by 1 percent then we can expect that sales will increase by 1.3%')\nprint('As customers are staying long with business then we can expect that sales will increase by 0.05%')","0d766ad2":"ytrain_pred=model.predict(inp_train)\nytest_pred=model.predict(inp_test)\n\ntrain_res=ytrain-ytrain_pred\ntest_res=ytest-ytest_pred\n\ntrain_mse= np.mean(train_res**2)\ntest_mse= np.mean(test_res**2)\n\nprint('Train MSE:{0:.4f}'.format(train_mse))\nprint('Test MSE:{0:.4f}'.format(test_mse))\n","3c8cb296":"r2_train=r2_score(ytrain,ytrain_pred)\nr2_test=r2_score(ytest,ytest_pred)\nprint('R square Train:{0:.4f}'.format(r2_train))\nprint('R square Test:{0:.4f}'.format(r2_test))","5d1fbe0b":"pval=linear_rainbow(res=model,frac=0.5)[1]\nprint('P value {0:.2f}'.format(pval))","4a5bab4d":"homoscad = het_goldfeldquandt(train_res,inp_train)[1]\nprint('P Value {0:.2f}'.format(homoscad))","05751e29":"fig = make_subplots(rows=1, cols=2,subplot_titles=['Train','Test'])\nfig.add_trace(go.Scatter(x=ytrain,y=ytrain_pred, mode='markers'\n                         ,marker=dict(size=4,colorscale='Viridis',opacity=0.8)),row=1, col=1)\nfig.add_trace(go.Scatter(x=ytest,y=ytest_pred, mode='markers'\n                         ,marker=dict(size=4,colorscale='Viridis',opacity=0.8)),row=1, col=2)\nfig.update_xaxes(title_text=\"Actual\", row=1, col=1)\nfig.update_yaxes(title_text=\"Predicted\", row=1, col=1)\nfig.update_xaxes(title_text=\"Actual\", row=1, col=2)\nfig.update_yaxes(title_text=\"Predicted\", row=1, col=2)\nfig.update_layout(height=400, width=800,template='ggplot2')\nfig.show()","dc5af0b2":"textstr = \"<br>\".join((\"SKEW={:.2f}\".format(train_res.skew())\n                     ,'Linearity Test P value={:.2f}'.format(pval)\n                    ,'Homoscadastic Test P value={:.2f}'.format(homoscad)\n                    ,'Durbin Watson value = {:.2f}'.format(durbin_watson(train_res))\n                    ,'Train MSE ={:.3f}'.format(train_mse)\n                    ,'Test MSE ={:.3f}'.format(test_mse)\n                    ,'R square Train ={:.3f}'.format(r2_train)\n                    ,'R square Test ={:.3f}'.format(r2_test)))\n\nfig= px.scatter(x=ytrain,y=train_res,marginal_y=\"histogram\",\n               marginal_x=\"histogram\", trendline=\"ols\")\n\nfig.update_layout(title=\"Test of Assumptions\",xaxis_title=\"Predicted\",yaxis_title=\"Residual\",\n                            font=dict(family=\"Courier New, monospace\",size=10,color=\"RebeccaPurple\"))\n\nfig.add_annotation(x=4,y=4,xref=\"x\",yref=\"y\",text=textstr,showarrow=False,font=dict(size=10)\n                   ,align=\"left\",ax=20,ay=-30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"snow\",opacity=0.8)\nfig.show()","ed7d4429":"# Bivariate View of by Segment","f005d597":"As we can see that data type of invoice data is not correctly identified by pandas, so let\u2019s convert that into datetime format and create other features in time dimension","dfe8300e":"![image.png](attachment:image.png)","6a627082":"### Lets delete negative quantity from the data and proceed with further analysis","f93f7e83":"![image.png](attachment:image.png)\n\nBelow charts shows average sales by hours varies whereas average sales between 7AM to 10 AM is higher compare to other hours of the day. In right hand side chart, we can see that average sales on Sunday and Monday is low compare to other weekdays.\n\nTo proceed with further analysis, I would like to run few statistical tests to make sure the difference in average sales by hour and weekday are statistically significant\n","60a5e1c1":"# Get Error","3cfe9084":"# Iteration for Kmeans","27c17dff":"# Linearrainbow test\n\nH0 : Part of the data is Linear\n\nHa : Independent features does not have linear relation with target feature","c4378c12":"# Create Clusters\nI have selected 10 clusters to start with and understand how customers' profile look like. We can do several statistical tests to get optimal cluster size.","efda0846":"![image.png](attachment:image.png)\n\n#### Objective of this analysis is to find pattern in the data which can group customers based on certain statistical rules. Variables which are being used are Recency, Monitory and length of stay. Below is the detailed explanation of these features\n","405262ca":"## Split the data","5666c886":"# 3 D view of clusters","fcaf1a82":"# Daily: Train & Test Split","3bee9e11":"![image.png](attachment:image.png)\n* United Kingdom has highest sales and it alone contributes 84% of total sales\n* United Kingdom, Netherlands, EIRE (Republic of Ireland) , Germany, France, Australia, Spain are 7 countries which as 95% of total sales\n* Netherlands has highest average sales per transaction compare to other countries\n* United Kingdom has performed consistently each quarter followed by Netherlands whereas Germany which was on 6th Rank in Q1 2011 achieved 3rd rank in Q4 2011\n","4b2b2214":"![image.png](attachment:image.png)\n\nAs we know data has hourly transaction. For this analysis I will aggregate data at customer level and will create below features.\n\n* Length of Stay = How long customer has been associated with business\n* Frequency = How many times customer has given order in duration given in the data\n* Monitory = Sales amount generated by each customer\n\n","108081cc":"## What are the things we can see from this analysis?\n\nThis complete analysis is divided in 4 parts\n*  Exploratory data analysis: which will have visual interpretation and will trying to understand purchase behaviour of customers by time delta and geography. We will also see some statistical hypothesis testing.\n* Customer segmentation on LRFM data\n* Weekly\/Daily Time series Forecasting\n* Sales prediction of new customer or estimates new sales of existing customers by Log-Log models","8235b724":"![image.png](attachment:image.png)","01789155":"# Weekly: Test of Stationarity of Actual Series","a1a3462b":"# Plot between Actual and predicted to check relationship","1a58b403":"# ![image.png](attachment:image.png)\n\nQuarterly ranking all countries. This chart will show how quarterly ranking of each country is changing with respect sales.\n\nNote: - I was looking for a way to connect each circle with line so that I could connect ranking circle of each country. This may help viewer to identify country and their ranking easily. Currently work is in progress for this section.\n","be20929a":"# Weekly: Test of Stationarity with 1 differencing of series","cd8b2878":"# Log Log Model\n\nAfter several iteration I saw that Log-Log model seems better fit on the data which improves R square and following are all assumptions of regression. Please check chart of residual vs predicted for all assumption tests","d9b10c1d":"# Daily: Trend","dae8cbca":"# Weekly: ARIMA Model","4d9ffe40":"# Get R square","45cd6085":"# 3 D View of Sales vs Length of Stay vs Frequency\nThis chart shows that there are set of potential customers who has influential data points. This gives an intuition of finding cluster in the data.\n","31479706":"![image.png](attachment:image.png)\n\nThis data contains online retail store transaction from 38 countries by 4372 customers. There are 541,909 observations and 8 features for 53 weeks starting from 1st Dec 2010 to 9th Dec 2011.\n\n|Variable|Description|Label & code| Type of Variable|\n|:--|:--|:--|:--|\n|InvoiceNo| Invoice Number or online order |-|<h1 style=\"color:blue; font-size:100%;\">Nominal<\/h1>|\n|StockCode| Code of product |-|<h1 style=\"color:blue; font-size:100%;\">Nominal<\/h1>|\n|Description|Description of each transaction|-| <h1 style=\"color:blue; font-size:100%;\">Nominal<\/h1>|\n|Quantity| Order quantity||<h1 style=\"color:red; font-size:100%;\">Descreat<\/h1>|\n|InvoiceDate|Date of order||<h1 style=\"color:red; font-size:100%;\">Nominal<\/h1>|\n|UnitPrice| Price of each respective product|-|<h1 style=\"color:blue; font-size:100%;\">Continues<\/h1>|\n|CustomerID| ID of each customer ||<h1 style=\"color:blue; font-size:100%;\">Nominal<\/h1>|\n|Country| Name of country of customers||<h1 style=\"color:blue; font-size:100%;\">Nominal<\/h1>|","8b0fade8":"![image.png](attachment:image.png)","85464a24":"# Test of Assumptions of Regression\n\nTo meet all assumptions of linear regression below are the criteria must be followed\n1. **Normality** - Error Must be normally distributed \n    \n    * To test this assumption, we can run statistical tests or simply can take skewness of residual and can accept it as normally distributed if skewness is -.5 to +.5\n    \n2. **Independence** - Residual must not be autocorrelated \n    * To test this assumption, we can check Durbin Watson value \n        * if it is = 2, then there is no autocorrelation among residual points.\n        * if it is 0< Durbin Watson <2 then there is Positive autocorrelation\n        * if it is 2< Durbin Watson <4. Negative auto correlation\n    * For standard practice Durbin Watson value between 1.5 and 2.5 is acceptable range to pass this test\n3. **Homoscedasticity** - There should be uniform variance in error with respect target variable\n4. **Linearity** -The relationship between X and the mean of Y is linear.\n\n","59a015f0":"![image.png](attachment:image.png)\nCharts show the group vs average sales and number of customers belong to the group. \nCustomers from Cluster 1 and 5 can be considered on high priority and can be offered some promotions so that they can continue their association with business. Whereas customers from cluster 3, 7 and 9 can be treated as medium size customer. \n\nFor medium size customers we can understand their purchase pattern and provide some promotions accordingly.\n\nIn the end we have list of customers from remaining clusters who can be taken as seriously and if we see their length of stay is better than medium size customers but their order size and order frequency is very low. So, we may understand their purchases pattern, type of product they select, and we can offer them some discounts or vouchers, or we can do some cross sales with them.\n","71bbf7ef":"# Interpration","e91638f0":"![image.png](attachment:image.png)\n\nLets see how old customers are contributing in total sales over time. Below charts shows that the quarter of customer acquisition and their total purchase in future years. Customer who made their first purchase in Q4 of Dec 2010 had large contribution in future quarter of 2011, same observation applies to the customers who were acquired in first quarter of 2011.","0d1826d8":"# Daily: PACF & ACF","57c86059":"# Rpeat Purchase\n\n* Most of customers made their next purchase after one or two months of first purchase\n* There were 28 customers who made their first purchase in Dec 2010 also made their next purchase after 12 months\n* 33 customers made their second purchase after 7 months of their first purchase in Mar 2011.","6bc7116a":"![image.png](attachment:image.png)\nOnly description and customer ID is missing. This shows that there 25% of transactions are not mapped to any customer which can be a bigger issue while identifying potential customers","d4941e23":"Conclusion:\nThis is lot of scope in EDA as well as in modeling, which I will try in my next version of same analysis. Feel free to provide your thought and feedback.","c8d85993":"![image.png](attachment:image.png)\nModels which I have tried are not perfect one and certainly there are scope to improve the performance.","4f5ced95":"#### Steps\n\n* Run iteration for number of clusters to select optimal clusters\n* Get Error and Silhouette Distance and plot\n* Select optimal number of cluster and create group\n* Plot Silhouette values with respect to different sample size\n* Create Profile of customers by group\n* Create 3 D view of features divided by group\n* Create bi variate scatter plot coloured by group\n* Create two-dimensional view of points by t-SNE\n","937be679":"# Weekly: PACF & ACF","6a632957":"# Homoscadastic -- Uniform variance\nH0 : model is Homoscadastic\n\nHa : model is Heteroskedastic","b6e28339":"![image.png](attachment:image.png)\n\n* Hourly sales are normally distributed and with mean 709k. \n* There is no pattern in day wise sales in a month.\n* There is no business on Saturday.\n* Average sales are consistent from Monday to Friday\n* Sales starts at 6 in the morning and ends at 8 PM. \n* Peak hours of sales are between 10 AM to 3 PM\n* Monday and Tuesday Sales starts at 7 AM and close at 6 PM\n* Wednesday and Friday sales start at 7 AM and close at 8 PM\n* Business hours for Sunday is between 9 to 4 PM\n\n","41f90e14":"![image.png](attachment:image.png)\n\nBelow analysis exhibits the number of customer acquisition month over month and in the end of Nov 2011 there were approx. 4.2k customers who made their purchase during the given period.","09b45134":"# Weekly: Train & Test Split","6da862b2":"# Daily: ARIMA Model","b6016fd1":"# Daily: Test of Stationarity of Actual Series","ac73b03b":"![image.png](attachment:image.png)","3ce72e34":"# First 5 rows of the data","4548ba05":"![image.png](attachment:image.png)","8298d0a2":"# Daily: Test of Stationarity with 1 differencing of series","0cf87347":"# Silhouette vs Sample\nThis gives glimpse of Silhouette values with respect outlier present in the data.","8b10bdd3":"![image.png](attachment:image.png)\nDue to presence of anomaly in the data, we can see some spike in the trend. There is no more detail available so it will be difficult to anticipate that this spike is due to any events or promotion. For now, lets assume that it is a random event.","8ec0711f":"**Hour**\n\n--------------------------------------------------------------------------\n\nH0: Sales is same across hours from 6 AM to 10 PM\n\nHa: Sales varies by hour of the day\n\nWe can see that p value is less than .05 (a confidence threshold point to take risk). Hence, we can reject null hypothesis and concluded that average sales vary by hour of the day\n","79395976":"![image.png](attachment:image.png)","c6cc356f":"![image.png](attachment:image.png)\nThere are negative numbers under quantity hence sales are also negative. Negative number can be return transactions. Ideally, I should align each return with respective sales transaction and cancel out the entry. But for now, I will remove negative quantity for our further analysis.","cfb758a1":"![image.png](attachment:image.png)\n\nNow we are going to see a linear regression model based on features such as frequency and length of stay of customers which will predict Sales generated by customers.","1f8868e3":"# WEEKLY: SARIMAX Model","88b562ee":"# t-distributed Stochastic Neighbor Embedding.\nt-SNE is a method to visualize high dimensional data in two dimensions\nif the value of perplexity (It is the value to select nearest neighbors which will generate different samples) is more than 30 then algorithm is able to separate the points clearly.","cc043a53":"# Daily: SARIMAX Model","70f01d22":"**Weekday**\n\n--------------------------------------------------------------------------\n\nH0: Average Sales is same each weekday\n\nHa: Average Sales is not same for few days or all weekdays\n\nThis analysis also shows that p values is less than .05 , hence we can reject null hypothesis and conclude that average sales is different among weekdays."}}