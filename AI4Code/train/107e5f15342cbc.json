{"cell_type":{"b6f1e327":"code","71c5b151":"code","bc3a16d8":"code","c6fd5f72":"code","e248587e":"code","21f6c08b":"code","092add0d":"code","b6dfd929":"code","d1a7a6b1":"code","879ce228":"code","398a78e1":"code","71d2e791":"code","b9b9bd65":"markdown"},"source":{"b6f1e327":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","71c5b151":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf.head()","bc3a16d8":"from sklearn.model_selection import train_test_split\n\ntext = df['text']\nlabel = df['target']\n\ntrain_sentences, test_sentences, train_labels, test_labels = train_test_split(text, label, test_size=0.1, random_state=42 )\nprint(train_sentences.shape)\nprint(train_labels.shape)\nprint(test_sentences.shape)\nprint(test_labels.shape)","c6fd5f72":"import tensorflow as tf\nfrom tensorflow import keras\nprint(tf.__version__)","e248587e":"vocab_size=10000\nembedding_dim=4\nmax_length=50\ntrunc_type=\"post\"\noov_token=\"<OOV>\"\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(train_sentences)\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_sequences = [ [str(tok) for tok in seq] for seq in test_sequences ]\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, truncating=trunc_type)\n","21f6c08b":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    keras.layers.Flatten(),\n    keras.layers.Dense(10, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","092add0d":"num_epochs=10\nhistory=model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels))","b6dfd929":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nimport matplotlib.pyplot as plt\n%matplotlib inline","d1a7a6b1":"plt.plot(acc)\nplt.plot(val_acc)\nplt.show()","879ce228":"plt.plot(loss)\nplt.plot(val_loss)\nplt.show()","398a78e1":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsentences=test['text']\nids=test['id']\nsequences=tokenizer.texts_to_sequences(sentences)\npadded=pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\ntest.shape","71d2e791":"predictions=model.predict(padded).flatten()\npredictions=list(map(lambda x: 1 if x>0.5 else 0, predictions))\n\nresult=pd.DataFrame({'id':ids ,'target':predictions})\nresult.set_index('id', inplace=True)\nresult.to_csv('.\/result.csv')\nprint(result.shape)\nresult.head()\n","b9b9bd65":"## Inference"}}