{"cell_type":{"624be8ca":"code","945546d8":"code","d2833186":"code","0c8cc5aa":"code","881fa51c":"code","0baa0824":"code","5d580f82":"code","c1f25d65":"code","faafffac":"code","5950e184":"code","56e1e4f2":"code","308036a3":"code","f31070d4":"code","9f09a867":"code","f82be15b":"code","dd16323c":"code","5435b588":"code","289485da":"code","578289bc":"code","73679bf4":"code","ceb240fb":"code","297c791c":"code","23d16adc":"code","82a8eb35":"code","d71dff76":"code","49a8e089":"code","4558e4af":"code","ee90f370":"code","8e61d518":"code","3f40c76c":"code","18255695":"code","9f636a2e":"code","d83b71a1":"code","c9d988ca":"code","a447ee3c":"code","7d65ee31":"code","cdcee2cb":"code","35934c24":"code","e8b6025b":"code","a58c5c21":"code","7c041d00":"code","8549cda9":"code","9f3a5487":"code","1c687ea5":"code","c43efc41":"code","ac42911c":"code","3edbad8d":"code","db36cb4f":"code","849c8f55":"code","b2b37671":"code","0f50648d":"code","c748e46d":"code","1dc1ae3a":"code","eafbed9b":"code","5522afea":"code","6d5d4389":"code","6f669f33":"code","720aec4d":"code","24e876d8":"code","59b8f1f3":"code","c2880311":"code","738c4a8a":"code","47084846":"code","1f9f395c":"code","f982213f":"code","423ccbfc":"code","d91620c8":"code","50644dcf":"code","85bf1014":"code","2fe5ed9c":"code","9b54e667":"code","a7037cb3":"code","721be292":"code","a19adbce":"code","30b9ec9d":"code","930b3a6b":"code","e44bf176":"code","4c307fe6":"code","b4858304":"code","3ab99dc1":"code","cf3f66f2":"code","b8af77f7":"code","3816149f":"code","36fe9f8d":"code","3cc41566":"code","ae77719b":"code","d99719b7":"code","5bc8a1d5":"code","105c7a33":"code","ec52011c":"code","4d00e34e":"code","d3e9041a":"code","3c4863bc":"code","be576edc":"code","e32c172c":"code","7cb84b19":"code","135b42f3":"code","2c22cc55":"code","ab59906f":"code","9ee7d655":"code","cec36d99":"code","88d44f23":"code","2dd8a7b7":"code","a4f72b16":"code","f5e08819":"code","c39d6dab":"code","ab35a293":"code","4ab890a8":"code","60079cc7":"code","62189e07":"code","83c78afa":"code","ccca1b85":"code","e95816d0":"code","5f8dd030":"code","abb931b2":"code","4c0231e1":"code","88ffb1b3":"code","31d1ec7e":"code","bf12cdec":"code","d93b824d":"code","1a45b517":"code","3b6a3ec3":"code","0acec4f2":"code","2fbac7b0":"code","9f232ab4":"code","415b3100":"code","6c74fd53":"code","6de0efe0":"code","9376ea7d":"code","7ffae9b0":"code","857c08e9":"code","f35c7b0c":"code","aff0b77f":"code","4770490d":"code","bec14f0a":"code","40ef7590":"code","dc3e605e":"code","2d3a8a37":"code","7b85de48":"code","5507ba94":"code","cb5f5c8a":"code","7f089963":"code","16c93d17":"code","30e6b39d":"code","a1d0dda9":"code","048a023e":"code","eab0bc60":"markdown","ae1786bb":"markdown","71161b08":"markdown"},"source":{"624be8ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","945546d8":"#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport pylab as py\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.filterwarnings('ignore')\nsns.set(style = 'white')","d2833186":"df_train = pd.read_csv(\"..\/input\/insurance\/train_ZoGVYWq.csv\")\ndf_test = pd.read_csv(\"..\/input\/insurance\/test_66516Ee_qHJgfUk.csv\")","0c8cc5aa":"\ndf_train.shape,df_test.shape","881fa51c":"df_train.columns, df_test.columns","0baa0824":"#We will rename some of the columns:\ndf_train = df_train.rename(columns={'perc_premium_paid_by_cash_credit': 'prec_cash_credit', 'age_in_days': 'age','Count_3-6_months_late':'count_3-6','Count_6-12_months_late':'count_6-12','Count_more_than_12_months_late':'count_more12','application_underwriting_score':'a_u_c'})\ndf_test = df_test.rename(columns={'perc_premium_paid_by_cash_credit': 'prec_cash_credit', 'age_in_days': 'age','Count_3-6_months_late':'count_3-6','Count_6-12_months_late':'count_6-12','Count_more_than_12_months_late':'count_more12','application_underwriting_score':'a_u_c'})","5d580f82":"df_train.dtypes","c1f25d65":"df_train['residence_area_type'] = df_train['residence_area_type'].astype('category')\ndf_train['renewal'] = df_train['renewal'].astype('category')","faafffac":"#We know that from the data set that count related variables are categorical in nature and on the other hand have many classes. So we will use \"Combine Sparse Class\"\n#technique to combine sparse classes and than convert these variables to category data type.\ndf_train['count_3-6'].value_counts(normalize=True)","5950e184":"#We can see have all the categories from 1 to 12 are very less in number compared to category 0. Therefore we will combine all the classes from 1 to 12 and will name them 1.\nfil =   (df_train['count_3-6'] == 2) |(df_train['count_3-6'] == 3) | (df_train['count_3-6'] == 4) | (df_train['count_3-6'] == 5) | (df_train['count_3-6'] == 6) | (df_train['count_3-6'] == 7) | (df_train['count_3-6'] == 8) | (df_train['count_3-6'] == 9) | (df_train['count_3-6'] == 10) | (df_train['count_3-6'] == 11) | (df_train['count_3-6'] == 12) | (df_train['count_3-6'] == 13)\ndf_train.loc[fil, 'count_3-6'] = 1\ndf_train['count_3-6'].value_counts(normalize = True)","56e1e4f2":"#Count_6-12\nfil =  (df_train['count_6-12'] == 2) |(df_train['count_6-12'] == 3) |(df_train['count_6-12'] == 4) | (df_train['count_6-12'] == 5) | (df_train['count_6-12'] == 6) | (df_train['count_6-12'] == 7) | (df_train['count_6-12'] == 8) | (df_train['count_6-12'] == 9) | (df_train['count_6-12'] == 10) | (df_train['count_6-12'] == 11) | (df_train['count_6-12'] == 12) | (df_train['count_6-12'] == 13) | (df_train['count_6-12'] == 14) | (df_train['count_6-12'] == 15) | (df_train['count_6-12'] == 17)\ndf_train.loc[fil, 'count_6-12'] = 1\ndf_train['count_6-12'].value_counts()","308036a3":"#Count_more12\nfil =  (df_train['count_more12'] == 2) |(df_train['count_more12'] == 3) |(df_train['count_more12'] == 4) | (df_train['count_more12'] == 5) | (df_train['count_more12'] == 6) | (df_train['count_more12'] == 7) | (df_train['count_more12'] == 8) | (df_train['count_more12'] == 9) | (df_train['count_more12'] == 10) | (df_train['count_more12'] == 11) \ndf_train.loc[fil, 'count_more12'] = 1\ndf_train['count_more12'].value_counts()","f31070d4":"# We know that the \"count_3-6\", \"count_6-12\", \"count_more12\" are categorical variables. So we convert it to category\ndf_train['count_3-6'] = df_train['count_3-6'].astype('category')\ndf_train['count_6-12'] = df_train['count_6-12'].astype('category')\ndf_train['count_more12'] = df_train['count_more12'].astype('category')","9f09a867":"df_train.dtypes","f82be15b":"df_train['sourcing_channel'].value_counts(normalize = True)","dd16323c":"fil = (df_train['sourcing_channel'] == 'E') \ndf_train.loc[fil,'sourcing_channel'] = 'D'\ndf_train['sourcing_channel'].value_counts()","5435b588":"#We also know that \"sourcing channel\" is a category datatype. \ndf_train['sourcing_channel'] = df_train['sourcing_channel'].astype('category')","289485da":"df_train['residence_area_type'].value_counts(normalize = True)","578289bc":"df_train['renewal'].value_counts()","73679bf4":"#We will apply the smae technoques in test data set.\ndf_test.dtypes","ceb240fb":"#here we can see that the sourcing_channel,residence_Area_ and renewal are category type.\ndf_test['residence_area_type'] = df_test['residence_area_type'].astype('category')","297c791c":"df_test.dtypes","23d16adc":"#We can see have all the categories from 1 to 12 are very less in number compared to category 0. Therefore we will combine all the classes from 1 to 12 and will name them 1.\nfil =   (df_test['count_3-6'] == 2) |(df_test['count_3-6'] == 3) | (df_test['count_3-6'] == 4) | (df_test['count_3-6'] == 5) | (df_test['count_3-6'] == 6) | (df_test['count_3-6'] == 7) | (df_test['count_3-6'] == 8) | (df_test['count_3-6'] == 9) | (df_test['count_3-6'] == 10) | (df_test['count_3-6'] == 11) | (df_test['count_3-6'] == 12) \ndf_test.loc[fil, 'count_3-6'] = 1\ndf_test['count_3-6'].value_counts(normalize = True)","82a8eb35":"df_test['count_3-6'].value_counts()","d71dff76":"fil =  (df_test['count_6-12'] == 2) |(df_test['count_6-12'] == 3) |(df_test['count_6-12'] == 4) | (df_test['count_6-12'] == 5) | (df_test['count_6-12'] == 6) | (df_test['count_6-12'] == 7) | (df_test['count_6-12'] == 8) | (df_test['count_6-12'] == 9) | (df_test['count_6-12'] == 10) | (df_test['count_6-12'] == 11) | (df_test['count_6-12'] == 12) | (df_test['count_6-12'] == 13) | (df_test['count_6-12'] == 14) | (df_test['count_6-12'] == 15) | (df_test['count_6-12'] == 17)\ndf_test.loc[fil, 'count_6-12'] = 1\ndf_test['count_6-12'].value_counts()","49a8e089":"fil =  (df_test['count_more12'] == 2) |(df_test['count_more12'] == 3) |(df_test['count_more12'] == 4) | (df_test['count_more12'] == 5) | (df_test['count_more12'] == 6) | (df_test['count_more12'] == 7) | (df_test['count_more12'] == 8) | (df_test['count_more12'] == 9) | (df_test['count_more12'] == 10) | (df_test['count_more12'] == 11) \ndf_test.loc[fil, 'count_more12'] = 1\ndf_test['count_more12'].value_counts()","4558e4af":"fil = (df_test['sourcing_channel'] == 'E') \ndf_test.loc[fil,'sourcing_channel'] = 'D'\ndf_test['sourcing_channel'].value_counts()","ee90f370":"df_test['sourcing_channel'] = df_test['sourcing_channel'].astype('category')\ndf_test['count_3-6'] = df_test['count_3-6'].astype('category')\ndf_test['count_6-12'] = df_test['count_6-12'].astype('category')\ndf_test['count_more12'] = df_test['count_more12'].astype('category')","8e61d518":"#Univariate analysis\ncustomer_details = ['id', 'age', 'Income']\npremium_info = ['prec_cash_credit', 'a_u_c', 'premium', 'no_of_premiums_paid']","3f40c76c":"def UVA_numeric(df_train, var_group):\n  '''\u00a0\n  Univariate_Analysis_numeric\n  takes a group of variables (INTEGER and FLOAT) and plot\/print all the descriptives and properties along with KDE.\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,3), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    mini = df_train[i].min()\n    maxi = df_train[i].max()\n    ran = df_train[i].max()-df_train[i].min()\n    mean = df_train[i].mean()\n    median = df_train[i].median()\n    st_dev = df_train[i].std()\n    skew = df_train[i].skew()\n    kurt = df_train[i].kurtosis()\n\n    # calculating points of standard deviation\n    points = mean-st_dev, mean+st_dev\n\n    #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.kdeplot(df_train[i], shade=True)\n    sns.lineplot(points, [0,0], color = 'black', label = \"std_dev\")\n    sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = \"min\/max\")\n    sns.scatterplot([mean], [0], color = 'red', label = \"mean\")\n    sns.scatterplot([median], [0], color = 'blue', label = \"median\")\n    plt.xlabel('{}'.format(i), fontsize = 20)\n    plt.ylabel('density')\n    plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(points[0],2),round(points[1],2)),\n                                                                                                   round(kurt,2),\n\n                                                                                                   round(skew,2),\n                                                                                                   (round(mini,2),round(maxi,2),round(ran,2)),\n                                                                                                   round(mean,2),\n                                                                                                   round(median,2)))\n","18255695":"UVA_numeric(df_train,customer_details)","9f636a2e":"UVA_numeric(df_train,premium_info)","d83b71a1":"count = ['count_3-6', 'count_6-12','count_more12']\npolicy = ['sourcing_channel','residence_area_type', 'renewal']","c9d988ca":"def UVA_category(df1, var_group):\n\n  '''\n  Univariate_Analysis_categorical\n  takes a group of variables (category) and plot\/print all the value_counts and barplot.\n  '''\n  # setting figure_size\n  size = len(var_group)\n  plt.figure(figsize = (7*size,5), dpi = 100)\n\n  # for every variable\n  for j,i in enumerate(var_group):\n    norm_count = df_train[i].value_counts(normalize = True)\n    n_uni = df_train[i].nunique()\n\n  #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.barplot(norm_count, norm_count.index , order = norm_count.index)\n    plt.xlabel('fraction\/percent', fontsize = 20)\n    plt.ylabel('{}'.format(i), fontsize = 20)\n    plt.title('n_uniques = {} \\n value counts \\n {};'.format(n_uni,norm_count))","a447ee3c":"UVA_category(df_train, count )    ","7d65ee31":"UVA_category(df_train, policy)    ","cdcee2cb":"df_train.isnull().sum()","35934c24":"df_test.isnull().sum()","e8b6025b":"def UVA_outlier(data, var_group, include_outlier = True):\n  '''\n  Univariate_Analysis_outlier:\n  takes a group of variables (INTEGER and FLOAT) and plot\/print boplot and descriptives\\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it \\n\\n\n\n  data : dataframe from which to plot from\\n\n  var_group : {list} type Group of Continuous variables\\n\n  include_outlier : {bool} whether to include outliers or not, default = True\\n\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,4), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    quant25 = data[i].quantile(0.25)\n    quant75 = data[i].quantile(0.75)\n    IQR = quant75 - quant25\n    med = data[i].median()\n    whis_low = med-(1.5*IQR)\n    whis_high = med+(1.5*IQR)\n\n    # Calculating Number of Outliers\n    outlier_high = len(data[i][data[i]>whis_high])\n    outlier_low = len(data[i][data[i]<whis_low])\n\n    if include_outlier == True:\n      print(include_outlier)\n      #Plotting the variable with every information\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))\n      \n    else:\n      # replacing outliers with max\/min whisker\n      df_train = data[var_group][:]\n      df_train[i][df_train[i]>whis_high] = whis_high+1\n      df_train[i][df_train[i]<whis_low] = whis_low-1\n      \n      # plotting without outliers\n      plt.subplot(1,size,j+1)\n      sns.boxplot(df_train[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2)\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))  \n\n","a58c5c21":"UVA_outlier(df_train,customer_details)","7c041d00":"UVA_outlier(df_train,premium_info)","8549cda9":"#Bivariate Analysis\nnumerical = df_train[[ 'age', 'Income','prec_cash_credit', 'a_u_c', 'premium', 'no_of_premiums_paid']]","9f3a5487":"correlation = numerical.corr()\ncorrelation","1c687ea5":"plt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","c43efc41":"#BIVARIATE ANALYSIS: CONTINUOUS CATEGORICAL VARIABLES\n\n#List of Hypothesis and investigation to perform under this combination.  \n#1 Do age plays a role in determining the probability of renewal of insurance or not?\n#2 Do income plays a role in determining the probability of renewal of insurance or not?\n#3 Do premium amount plays a role in determining the probability of renewal of insurance or not?\n#4 Do no of premiums paid plays a role in determining the probability of renewal of insurance or not?\n#5 Do application underwriting score plays a role in determining the probability of renewal of insurance or not?\n#6 Do percentage premium paid by cash plays a role in determining the probability of renewal of insurance or not?\n\n  \ndef TwoSampZ(X1, X2, sigma1, sigma2, N1, N2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sampled Z-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import norm\n  ovr_sigma = sqrt(sigma1**2\/N1 + sigma2**2\/N2)\n  z = (X1 - X2)\/ovr_sigma\n  pval = 2*(1 - norm.cdf(abs(z)))\n  return pval\ndef TwoSampT(X1, X2, sd1, sd2, n1, n2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sample T-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import t as t_dist\n  ovr_sd = sqrt(sd1**2\/n1 + sd2**2\/n2)\n  t = (X1 - X2)\/ovr_sd\n  df = n1+n2-2\n  pval = 2*(1 - t_dist.cdf(abs(t),df))\n  return pval\n\ndef Bivariate_cont_cat(df_train, cont, cat, category):\n  #creating 2 samples\n  x1 = df_train[cont][df_train[cat]==category][:]\n  x2 = df_train[cont][~(df_train[cat]==category)][:]\n  \n  #calculating descriptives\n  n1, n2 = x1.shape[0], x2.shape[0]\n  m1, m2 = x1.mean(), x2.mean()\n  std1, std2 = x1.std(), x2.mean()\n  \n  #calculating p-values\n  t_p_val = TwoSampT(m1, m2, std1, std2, n1, n2)\n  z_p_val = TwoSampZ(m1, m2, std1, std2, n1, n2)\n\n  #table\n  table = pd.pivot_table(data=df_train, values=cont, columns=cat, aggfunc = np.mean)\n\n  #plotting\n  plt.figure(figsize = (15,6), dpi=140)\n  \n  #barplot\n  plt.subplot(1,2,1)\n  sns.barplot([str(category),'not {}'.format(category)], [m1, m2])\n  plt.ylabel('mean {}'.format(cont))\n  plt.xlabel(cat)\n  plt.title('t-test p-value = {} \\n z-test p-value = {}\\n {}'.format(t_p_val,\n                                                                z_p_val,\n                                                                table))\n\n  # boxplot\n  plt.subplot(1,2,2)\n  sns.boxplot(x=cat, y=cont, data=df_train)\n  plt.title('categorical boxplot')\n  ","ac42911c":"Bivariate_cont_cat(df_train, 'Income','renewal', 1)","3edbad8d":"Bivariate_cont_cat(df_train, 'age','renewal', 1)","db36cb4f":"Bivariate_cont_cat(df_train, 'no_of_premiums_paid','renewal', 1)","849c8f55":"Bivariate_cont_cat(df_train, 'a_u_c','renewal', 1)","b2b37671":"Bivariate_cont_cat(df_train, 'premium','renewal', 1)","0f50648d":"Bivariate_cont_cat(df_train, 'prec_cash_credit','renewal', 1)","c748e46d":"#Bivariate : Categorical-Categorical\ndef BVA_categorical_plot(data, tar, cat):\n  '''\n  take data and two categorical variables,\n  calculates the chi2 significance between the two variables \n  and prints the result with countplot & CrossTab\n  '''\n  #isolating the variables\n  data = df_train[[cat,tar]][:]\n\n  #forming a crosstab\n  table = pd.crosstab(df_train[tar],df_train[cat],)\n  f_obs = np.array([table.iloc[0][:].values,\n                    table.iloc[1][:].values])\n\n  #performing chi2 test\n  from scipy.stats import chi2_contingency\n  chi, p, dof, expected = chi2_contingency(f_obs)\n  \n  #checking whether results are significant\n  if p<0.05:\n    sig = True\n  else:\n    sig = False\n\n  #plotting grouped plot\n  sns.countplot(x=cat, hue=tar, data=df_train)\n  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n\n  #plotting percent stacked bar plot\n  #sns.catplot(ax, kind='stacked')\n  ax1 = df_train.groupby(cat)[tar].value_counts(normalize=True).unstack()\n  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n  int_level = df_train[cat].value_counts()\n  ","1dc1ae3a":"BVA_categorical_plot(df_train, 'count_3-6', 'renewal')","eafbed9b":"BVA_categorical_plot(df_train, 'count_6-12', 'renewal')","5522afea":"BVA_categorical_plot(df_train, 'count_more12', 'renewal')","6d5d4389":"BVA_categorical_plot(df_train, 'sourcing_channel', 'renewal')","6f669f33":"BVA_categorical_plot(df_train,'residence_area_type', 'renewal')","720aec4d":"#Preprocessing Data\n#We will apply the same preprocessing techniques in both the train and test data set.\n#Missing values\npd.isnull(df_train).sum()\n","24e876d8":"pd.isnull(df_test).sum()","59b8f1f3":"#Here we can see that we have missing values in both data set in variables count_3-6, count_6-12, count_12more and a_u_c\n#Because count_3-6, count_6-12 and count_12more are categorical variables. Therefore we will replace missing values by its mode.\n# And for a_u_c we will use median as this continuous variable has outliers.","c2880311":"df_train['count_3-6'].value_counts()","738c4a8a":"df_train['count_3-6'] = df_train['count_3-6'].fillna(0)","47084846":"df_test['count_3-6'].value_counts()","1f9f395c":"df_test['count_3-6'] = df_test['count_3-6'].fillna(0)","f982213f":"df_train['count_6-12'].value_counts()","423ccbfc":"df_train['count_6-12'] = df_train['count_6-12'].fillna(0)","d91620c8":"df_test['count_6-12'].value_counts()","50644dcf":"df_test['count_6-12'] = df_test['count_6-12'].fillna(0)","85bf1014":"df_train['count_more12'].value_counts()","2fe5ed9c":"df_train['count_more12'] = df_train['count_more12'].fillna(0)","9b54e667":"df_test['count_more12'].value_counts()","a7037cb3":"df_test['count_more12'] = df_test['count_more12'].fillna(0)","721be292":"df_train.isnull().sum()","a19adbce":"df_test.isnull().sum()","30b9ec9d":"df_train['a_u_c'].median()","930b3a6b":"df_train['a_u_c'] = df_train['a_u_c'].fillna(99.21)","e44bf176":"df_test['a_u_c'].median()","4c307fe6":"df_test['a_u_c'] = df_test['a_u_c'].fillna(99.21)","b4858304":"df_train.isnull().sum()","3ab99dc1":"df_test.isnull().sum()","cf3f66f2":"#Outlier Treatment\n##Income\nQ1 = df_train['Income'].quantile(0.25)\nQ3 = df_train['Income'].quantile(0.75)\n\nIQR = df_train['Income'].quantile(0.75) - df_train['Income'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","b8af77f7":"#We have seen in the box plot of income that all the outliers were above the upper whisker. So will use whisker_2 value to treat outliers\ndf_train['Income'] = np.where(df_train['Income'] >468210, whisker_2,df_train['Income'])","3816149f":"#Premium\nQ1 = df_train['premium'].quantile(0.25)\nQ3 = df_train['premium'].quantile(0.75)\n\nIQR = df_train['premium'].quantile(0.75) - df_train['premium'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","36fe9f8d":"df_train['premium'] = np.where(df_train['premium'] >26400, whisker_2,df_train['premium'])","3cc41566":"#age\nQ1 = df_train['age'].quantile(0.25)\nQ3 = df_train['age'].quantile(0.75)\n\nIQR = df_train['age'].quantile(0.75) - df_train['age'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","ae77719b":"df_train['age'] = np.where(df_train['premium'] >34129, whisker_2,df_train['age'])","d99719b7":"Q1 = df_train['a_u_c'].quantile(0.25)\nQ3 = df_train['a_u_c'].quantile(0.75)\n\nIQR = df_train['a_u_c'].quantile(0.75) - df_train['a_u_c'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","5bc8a1d5":"df_train['a_u_c'] = np.where(df_train['a_u_c'] < 97.795, whisker_2,df_train['a_u_c'])","105c7a33":"Q1 = df_train['no_of_premiums_paid'].quantile(0.25)\nQ3 = df_train['no_of_premiums_paid'].quantile(0.75)\n\nIQR = df_train['no_of_premiums_paid'].quantile(0.75) - df_train['no_of_premiums_paid'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","ec52011c":"df_train['no_of_premiums_paid'] = np.where(df_train['no_of_premiums_paid'] > 24.5, whisker_2,df_train['no_of_premiums_paid'])","4d00e34e":"Q1 = df_train['prec_cash_credit'].quantile(0.25)\nQ3 = df_train['prec_cash_credit'].quantile(0.75)\n\nIQR = df_train['prec_cash_credit'].quantile(0.75) - df_train['prec_cash_credit'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","d3e9041a":"df_train['prec_cash_credit'] = np.where(df_train['prec_cash_credit'] > 1.2939, whisker_2,df_train['prec_cash_credit'])","3c4863bc":"#We wii use same techniques to treat outliers in the test data set.\n#Outlier Treatment\n##Income\nQ1 = df_test['Income'].quantile(0.25)\nQ3 = df_test['Income'].quantile(0.75)\n\nIQR = df_test['Income'].quantile(0.75) - df_test['Income'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","be576edc":"#We have seen in the box plot of income that all the outliers were above the upper whisker. So will use whisker_2 value to treat outliers\ndf_test['Income'] = np.where(df_test['Income'] >465453.75, whisker_2,df_test['Income'])","e32c172c":"#Premium\nQ1 = df_test['premium'].quantile(0.25)\nQ3 = df_test['premium'].quantile(0.75)\n\nIQR = df_test['premium'].quantile(0.75) - df_test['premium'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","7cb84b19":"df_test['premium'] = np.where(df_test['premium'] >26400, whisker_2,df_test['premium'])","135b42f3":"Q1 = df_test['age'].quantile(0.25)\nQ3 = df_test['age'].quantile(0.75)\n\nIQR = df_test['age'].quantile(0.75) - df_test['age'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","2c22cc55":"df_test['age'] = np.where(df_test['premium'] >34132, whisker_2,df_test['age'])","ab59906f":"Q1 = df_test['a_u_c'].quantile(0.25)\nQ3 = df_test['a_u_c'].quantile(0.75)\n\nIQR = df_test['a_u_c'].quantile(0.75) - df_test['a_u_c'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","9ee7d655":"df_test['a_u_c'] = np.where(df_test['a_u_c'] < 97.795, whisker_2,df_test['a_u_c'])","cec36d99":"Q1 = df_test['no_of_premiums_paid'].quantile(0.25)\nQ3 = df_test['no_of_premiums_paid'].quantile(0.75)\n\nIQR = df_test['no_of_premiums_paid'].quantile(0.75) - df_test['no_of_premiums_paid'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","88d44f23":"df_test['no_of_premiums_paid'] = np.where(df_test['no_of_premiums_paid'] > 24.5, whisker_2,df_test['no_of_premiums_paid'])","2dd8a7b7":"Q1 = df_test['prec_cash_credit'].quantile(0.25)\nQ3 = df_test['prec_cash_credit'].quantile(0.75)\n\nIQR = df_test['prec_cash_credit'].quantile(0.75) - df_test['prec_cash_credit'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","a4f72b16":"df_test['prec_cash_credit'] = np.where(df_test['prec_cash_credit'] > 1.299, whisker_2,df_test['prec_cash_credit'])","f5e08819":"# Introducing Dummy Variables\ndf_train = pd.concat([df_train,pd.get_dummies(df_train['count_3-6'],prefix = str('count_3-6'),prefix_sep='_')],axis = 1)\ndf_train = pd.concat([df_train,pd.get_dummies(df_train['count_6-12'],prefix = str('count_6-12'),prefix_sep='_')],axis = 1)\ndf_train = pd.concat([df_train,pd.get_dummies(df_train['count_more12'],prefix = str('count_more12'),prefix_sep='_')],axis = 1)\ndf_train = pd.concat([df_train,pd.get_dummies(df_train['sourcing_channel'],prefix = str('sourcing_channel'),prefix_sep='_')],axis = 1)\ndf_train = pd.concat([df_train,pd.get_dummies(df_train['residence_area_type'],prefix = str('residence_area_type'),prefix_sep='_')],axis = 1)\n","c39d6dab":"df_train.columns","ab35a293":"df_train.drop(['count_3-6'], axis = 1, inplace  = True) \ndf_train.drop(['count_6-12'], axis = 1, inplace = True)\ndf_train.drop(['count_more12'], axis = 1, inplace = True)\ndf_train.drop(['sourcing_channel'], axis = 1, inplace = True)\ndf_train.drop(['residence_area_type'], axis = 1, inplace = True)","4ab890a8":"df_train1 = df_train.copy()","60079cc7":"df_train1.head()","62189e07":"# Introducing Dummy Variables\ndf_test = pd.concat([df_test,pd.get_dummies(df_test['count_3-6'],prefix = str('count_3-6'),prefix_sep='_')],axis = 1)\ndf_test = pd.concat([df_test,pd.get_dummies(df_test['count_6-12'],prefix = str('count_6-12'),prefix_sep='_')],axis = 1)\ndf_test = pd.concat([df_test,pd.get_dummies(df_test['count_more12'],prefix = str('count_more12'),prefix_sep='_')],axis = 1)\ndf_test = pd.concat([df_test,pd.get_dummies(df_test['sourcing_channel'],prefix = str('sourcing_channel'),prefix_sep='_')],axis = 1)\ndf_test = pd.concat([df_test,pd.get_dummies(df_test['residence_area_type'],prefix = str('residence_area_type'),prefix_sep='_')],axis = 1)\n","83c78afa":"df_test.drop(['count_3-6'], axis = 1, inplace  = True) \ndf_test.drop(['count_6-12'], axis = 1, inplace = True)\ndf_test.drop(['count_more12'], axis = 1, inplace = True)\ndf_test.drop(['sourcing_channel'], axis = 1, inplace = True)\ndf_test.drop(['residence_area_type'], axis = 1, inplace = True)","ccca1b85":"df_train.dtypes.reset_index()\ndf_test.dtypes.reset_index()\ndf_train1.dtypes.reset_index()","e95816d0":"#Now i will split the train data set into train and validation set. I will be using train1 data set.\n#We will first do standardization of all the continuous variables to bring them on the same scale.\nnum_cols = ['age', 'Income','prec_cash_credit', 'a_u_c', 'premium', 'no_of_premiums_paid']","5f8dd030":"for i in num_cols:\n    df_train1[i] = np.log(df_train1[i] + 10)","abb931b2":"df_train1.drop(['id'], axis = 1, inplace  = True) ","4c0231e1":"df_train1.head()","88ffb1b3":"y = df_train1.renewal\nx = df_train1.drop(['renewal'],axis = 1)","31d1ec7e":"pip install -U imbalanced-learn","bf12cdec":"pip install scikit-learn==0.23.1","d93b824d":"pip install imbalanced-learn==0.7.0","1a45b517":"from imblearn.combine import SMOTETomek ","3b6a3ec3":"smk = SMOTETomek(random_state = 42)\ndf_train1,y_all1 = smk.fit_resample(x,y)","0acec4f2":"xtrain1, xvald1, ytrain1, yvald1 = train_test_split(df_train1,y_all1,test_size=1\/3, random_state=11, stratify = y_all1)\nmodel = LogisticRegression()\nmodel.fit(xtrain1,ytrain1)\npred = model.predict_proba(xvald1)[:,1]","2fbac7b0":"from sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(yvald1,pred) \nauc = roc_auc_score(yvald1, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \nx = np.linspace(0, 1, 1000)\nplt.plot(x, x, linestyle='-')\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","9f232ab4":"pred_val = model.predict(xvald1)\n\nlabel_preds = pred_val\n\ncm = confusion_matrix(yvald1,label_preds)\n\n\ndef plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n\nplot_confusion_matrix(cm, ['No', 'Yes'])","415b3100":"recall_score(yvald1,pred_val)","6c74fd53":"#Reverse Feature Elimination and Backward Selection\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt","6de0efe0":"#Create the RFE object and rank each feature\nmodel = LogisticRegression()\nrfe = RFE(estimator = model, n_features_to_select =1, step = 1)\nrfe.fit(df_train1, y_all1)","9376ea7d":"ranking_df_train1 = pd.DataFrame()\nranking_df_train1['Feature_name'] = df_train1.columns\nranking_df_train1['Rank'] = rfe.ranking_","7ffae9b0":"ranked = ranking_df_train1.sort_values(by = [\"Rank\"])","857c08e9":"ranked\n","f35c7b0c":"def cv_score(ml_model, rstate = 12, thres = 0.5, cols = df_train1.columns):\n    i = 1\n    cv_scores = []\n    df2 = df_train1.copy()\n    df2 = df_train1[cols]\n    \n    # 5 Fold cross validation stratified on the basis of target\n    kf = StratifiedKFold(n_splits=5,random_state=rstate,shuffle=True)\n    for df_train1_index,vald_index in kf.split(df_train1,y_all1):\n        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n        xtr,xvl = df2.loc[df_train1_index],df2.loc[vald_index]\n        ytr,yvl = y_all1.loc[df_train1_index],y_all1.loc[vald_index]\n            \n        # Define model for fitting on the training set for each fold\n        model = ml_model\n        model.fit(xtr, ytr)\n        pred_probs = model.predict_proba(xvl)\n        pp = []\n         \n        # Use threshold to define the classes based on probability values\n        for j in pred_probs[:,1]:\n            if j>thres:\n                pp.append(1)\n            else:\n                pp.append(0)\n         \n        # Calculate scores for each fold and print\n        pred_val = pp\n        roc_score = roc_auc_score(yvl,pred_probs[:,1])\n        recall = recall_score(yvl,pred_val)\n        precision = precision_score(yvl,pred_val)\n        sufix = \"\"\n        msg = \"\"\n        msg += \"ROC AUC Score: {}, Recall Score: {:.4f}, Precision Score: {:.4f} \".format(roc_score, recall,precision)\n        print(\"{}\".format(msg))\n         \n         # Save scores\n        cv_scores.append(roc_score)\n        i+=1\n    return cv_scores\n","aff0b77f":"ref_top10_scores = cv_score(LogisticRegression(), cols = ranked['Feature_name'][:10].values, thres = 0.14)","4770490d":"#df\nnum_cols = ['age', 'Income','prec_cash_credit', 'a_u_c', 'premium', 'no_of_premiums_paid']","bec14f0a":"for i in num_cols:\n    df_train[i] = np.log(df_train[i] + 10)","40ef7590":"for i in num_cols:\n    df_test[i] = np.log(df_test[i] + 10)","dc3e605e":"df_train2 = df_train.drop(['id'],axis = 1)","2d3a8a37":"x = df_train2.drop(['renewal'], axis =1)\ny = df_train2['renewal']","7b85de48":"smk = SMOTETomek(random_state = 32)\nx_t,y_t = smk.fit_resample(x,y)","5507ba94":"model = LogisticRegression(random_state=0).fit(x_t, y_t)\nmodel.fit(x_t,y_t)","cb5f5c8a":"df_test1 = df_test.drop(['id'], axis = 1)","7f089963":"pred = model.predict(df_test1)","16c93d17":"df_test1","30e6b39d":"test_pid = df_test['id']\nsub = pd.DataFrame({'id':test_pid,'renewal':pred})","a1d0dda9":"df_test = df_test.merge(sub, on='id', how='left')","048a023e":"df_test","eab0bc60":"#From above we can infer that ID in uniformly distributed because every individual has different ID. So we can drop this variable.\n#Age is almost normally distributed. It seems to have some outliers on the upper side\n#Income is highly skewed as there are many individuals who have very high income.","ae1786bb":"#Here we can see that some of the variables data type is object which means that python is not able to predict about there data\n#type. Therefore we will have to assign a data type for those variables seeing their nature.\n#Also many of the categorical variables has different data types. So we also assign them category data type.","71161b08":"#From above analysis we can infer that all the numerical variables are significantly different for different categories. Therefore all the varibales statistically and significantly affect the probaility of default and not default."}}