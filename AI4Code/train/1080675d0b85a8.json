{"cell_type":{"d9381dd5":"code","edd4abf7":"code","79fb3338":"code","a8303214":"code","fce8b050":"code","0f10c38c":"code","80c98562":"code","8c06fe88":"code","18b04e27":"code","e0ed3bd6":"code","47e48c19":"code","797ff187":"code","1b39607b":"code","3b1bd997":"code","0ad4ab10":"code","0d31a076":"code","a22ff12a":"code","de44f546":"code","ca7c149a":"code","57ca1c2f":"code","d4fddb61":"code","acf44661":"code","3a14dd2b":"code","2b8b6ac0":"code","9a5d92c1":"code","160d8aac":"code","d317729f":"code","98d59ceb":"code","4fcd8c68":"code","e3527370":"code","9bddfebc":"code","130980e0":"code","f464e8f2":"code","1a6747d2":"code","297bc835":"code","a3919828":"code","4b7dba9b":"code","1037bc94":"code","6b35c558":"code","ed668c3c":"code","828dfc71":"code","f4584545":"code","af2af7e1":"code","9ce730d6":"code","94b53d7b":"code","e65bf977":"markdown","b98c515b":"markdown","9d71cb9a":"markdown","d6aabe16":"markdown","b9b02db3":"markdown"},"source":{"d9381dd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","edd4abf7":"L0_station = (1, 25)\nL1_station = (25, 27)\nL2_station = (27, 30)\nL3_station = (30, 53)","79fb3338":"import zipfile\n\nzf = zipfile.ZipFile('..\/input\/bosch-production-line-performance\/train_numeric.csv.zip') \ntrain_numeric_chunks = pd.read_csv(zf.open('train_numeric.csv'), iterator=True, chunksize=100000)\n\npath = '..\/input\/bosch-dataset\/station_one_hot.csv'\none_hot_stations = pd.read_csv(path)\n\nL0_one_hot = one_hot_stations.iloc[:,L0_station[0]:L0_station[1]]\nL1_one_hot = one_hot_stations.iloc[:,L1_station[0]:L1_station[1]]\nL2_one_hot = one_hot_stations.iloc[:,L2_station[0]:L2_station[1]]\nL3_one_hot = one_hot_stations.iloc[:,L3_station[0]:L3_station[1]]\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\npd.options.display.max_colwidth = None","a8303214":"# Drop rows with all 0 for each station\nL0_one_hot = L0_one_hot.loc[~(L0_one_hot==0).all(axis=1)]\nL1_one_hot = L1_one_hot.loc[~(L1_one_hot==0).all(axis=1)]\nL2_one_hot = L2_one_hot.loc[~(L2_one_hot==0).all(axis=1)]\nL3_one_hot = L3_one_hot.loc[~(L3_one_hot==0).all(axis=1)]\n\nprint(\"Parts in L0:{}\".format(len(L0_one_hot)))\nprint(\"Parts in L1:{}\".format(len(L1_one_hot)))\nprint(\"Parts in L2:{}\".format(len(L2_one_hot)))\nprint(\"Parts in L3:{}\".format(len(L3_one_hot)))","fce8b050":"L0_one_hot.insert(0, \"Id\",one_hot_stations[\"Id\"])\nL1_one_hot.insert(0, \"Id\",one_hot_stations[\"Id\"])\nL2_one_hot.insert(0, \"Id\",one_hot_stations[\"Id\"])\nL3_one_hot.insert(0, \"Id\",one_hot_stations[\"Id\"])","0f10c38c":"def get_numeric_frame():\n    for data_frame in train_numeric_chunks:\n        yield data_frame\n\nget_df_numeric = get_numeric_frame()     \ndf_numeric = next(get_df_numeric)","80c98562":"while True:\n    try:\n        response_column = pd.concat([response_column, df_numeric[['Response']]])\n    except:\n        response_column = df_numeric[['Response']]\n    try:\n        df_numeric = next(get_df_numeric)\n    except:\n        break\n\n        L0_one_hot.insert(0, \"Id\",one_hot_stations[\"Id\"])\nL0_one_hot.insert(1, 'Response', response_column['Response'])\nL1_one_hot.insert(1, 'Response', response_column['Response'])\nL2_one_hot.insert(1, 'Response', response_column['Response'])\nL3_one_hot.insert(1, 'Response', response_column['Response'])","8c06fe88":"L0_one_hot.head()","18b04e27":"L1_one_hot.head()","e0ed3bd6":"L2_one_hot.head()","47e48c19":"L3_one_hot.head()","797ff187":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt","1b39607b":"column_names = L1_one_hot.columns[2:]","3b1bd997":"inertias = []\n\nfor i in range(2, 4):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(L1_one_hot[column_names])\n    inertias.append(kmeans.inertia_)","0ad4ab10":"plt.plot(range(2, 4), inertias, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('inertia')\nplt.show()","0d31a076":"n_clusters = 3\nkmeans = KMeans(n_clusters=n_clusters)\npred_y = kmeans.fit_predict(L1_one_hot[column_names])\n\nL1_one_hot.insert(2, \"Cluster_Numbers_from_KMeans\", pred_y)","a22ff12a":"L1_one_hot.loc[L1_one_hot[\"Cluster_Numbers_from_KMeans\"] == 0].sample(5)","de44f546":"L1_one_hot.loc[L1_one_hot[\"Cluster_Numbers_from_KMeans\"] == 1].sample(5)","ca7c149a":"L1_one_hot.loc[L1_one_hot[\"Cluster_Numbers_from_KMeans\"] == 2].sample(5)","57ca1c2f":"column_names = L2_one_hot.columns[2:]","d4fddb61":"inertias = []\n\nfor i in range(4, 7):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(L2_one_hot[column_names])\n    inertias.append(kmeans.inertia_)","acf44661":"plt.plot(range(4, 7), inertias, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('inertia')\nplt.show()","3a14dd2b":"n_clusters = 6\nkmeans = KMeans(n_clusters=n_clusters)\npred_y = kmeans.fit_predict(L2_one_hot[column_names])\n\nL2_one_hot.insert(2, \"Cluster_Numbers_from_KMeans\", pred_y)","2b8b6ac0":"L2_one_hot.sample(20)","9a5d92c1":"pc_list = []\nfor i in range(0, len(L0_one_hot.iloc[:,2:].columns)):\n    pc_list.append('PC'+str(i))\n    \npca = PCA(whiten=True).fit(L0_one_hot.iloc[:,2:])\ndf_pca_summary = pd.DataFrame({'var': pca.explained_variance_ratio_, 'PC':pc_list})\n\ndf_pca_summary.plot.bar(x='PC', y='var', rot=0, figsize=(25,10))\nplt.xlabel(\"Variance explained\")\nplt.ylabel(\"Principle components\")\nplt.show()","160d8aac":"df_pca_summary.loc[0:8]['var'].sum()","d317729f":"n_components = 9\npca = PCA(n_components = n_components, whiten=True)\n\nsampled_data = L0_one_hot.sample(len(L0_one_hot))\nsampled_data_pca = pca.fit_transform(sampled_data.iloc[:,2:])\n\nPCA_comps = pd.DataFrame({\"Id\":sampled_data.Id , \"Response\":sampled_data.Response})\nfor i in range(n_components):\n    s = \"pc\"+str(i)\n    PCA_comps[s] = sampled_data_pca[:,i]\n    \n# PCA_comps.sort_values(by=['Id'], inplace=True)","98d59ceb":"column_names = PCA_comps.columns[2:]","4fcd8c68":"'''\nkmeans = KMeans(n_clusters=100)\nkmeans.fit(PCA_comps[column_names])\nprint(kmeans.inertia_)\n\n\nkmeans = KMeans(n_clusters=250)\nkmeans.fit(PCA_comps[column_names])\nprint(kmeans.inertia_)\n'''","e3527370":"first_split = PCA_comps[0:int(len(PCA_comps)\/4)]\nsecond_split = PCA_comps[int(len(PCA_comps)\/4):int(2*len(PCA_comps)\/4)]\nthird_split = PCA_comps[int(2*len(PCA_comps)\/4):int(3*len(PCA_comps)\/4)]\nfourth_split = PCA_comps[int(3*len(PCA_comps)\/4):]","9bddfebc":"from sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN()","130980e0":"preds_first_split = dbscan.fit_predict(first_split[column_names])\npreds_second_split = dbscan.fit_predict(second_split[column_names])\npreds_third_split = dbscan.fit_predict(third_split[column_names])\npreds_fourth_split = dbscan.fit_predict(fourth_split[column_names])\n","f464e8f2":"print(\"Shape of preds_first_split: {}\".format(preds_first_split.shape))\nprint(\"number of clusters in dbscan: {}\".format(np.max(preds_first_split)))\n\nprint(\"Shape of preds_first_split: {}\".format(preds_second_split.shape))\nprint(\"number of clusters in dbscan: {}\".format(np.max(preds_second_split)))\n\nprint(\"Shape of preds_first_split: {}\".format(preds_third_split.shape))\nprint(\"number of clusters in dbscan: {}\".format(np.max(preds_third_split)))\n\nprint(\"Shape of preds_first_split: {}\".format(preds_third_split.shape))\nprint(\"number of clusters in dbscan: {}\".format(np.max(preds_third_split)))","1a6747d2":"first_split.insert(2, \"Clusters\", preds_first_split)","297bc835":"first_split.head(100)","a3919828":"cluster_label_0 = first_split.loc[first_split['Clusters'] == 0]\npart_station_info_c0 = L0_one_hot.loc[L0_one_hot['Id'].isin(cluster_label_0[\"Id\"])]\npart_station_info_c0.insert(1, \"Cluster\", 0)","4b7dba9b":"part_station_info_c0.head(100)","1037bc94":"cluster_label_68 = first_split.loc[first_split['Clusters'] == 68]\npart_station_info_c68 = L0_one_hot.loc[L0_one_hot['Id'].isin(cluster_label_68[\"Id\"])]\npart_station_info_c68.insert(1, \"Cluster\", 68)","6b35c558":"part_station_info_c68.head(100)","ed668c3c":"cluster_label_43 = first_split.loc[first_split['Clusters'] == 43]\npart_station_info_c43 = L0_one_hot.loc[L0_one_hot['Id'].isin(cluster_label_43[\"Id\"])]\npart_station_info_c43.insert(1, \"Cluster\", 43)","828dfc71":"part_station_info_c43.head(100)","f4584545":"'''\nfrom sklearn.cluster import OPTICS\n\noptics = OPTICS(max_eps=2)\npreds = optics.fit_predict(PCA_comps[column_names].sample(100000))\n'''","af2af7e1":"'''\npc_list = []\nfor i in range(0, len(L3_one_hot.iloc[:,2:].columns)):\n    pc_list.append('PC'+str(i))\n    \npca = PCA(whiten=True).fit(L3_one_hot.iloc[:,2:])\ndf_pca_summary = pd.DataFrame({'var': pca.explained_variance_ratio_, 'PC':pc_list})\n\ndf_pca_summary.plot.bar(x='PC', y='var', rot=0, figsize=(25,10))\nplt.xlabel(\"Variance explained\")\nplt.ylabel(\"Principle components\")\nplt.show()\n'''","9ce730d6":"'''\ndf_pca_summary.loc[0:10]['var'].sum()\n'''","94b53d7b":"'''\nn_components = 11\npca = PCA(n_components = n_components, whiten=True)\n\nsampled_data = L3_one_hot.sample(len(L3_one_hot))\nsampled_data_pca = pca.fit_transform(sampled_data.iloc[:,2:])\n\nPCA_comps = pd.DataFrame({\"Id\":sampled_data.Id , \"Response\":sampled_data.Response})\nfor i in range(n_components):\n    s = \"pc\"+str(i)\n    PCA_comps[s] = sampled_data_pca[:,i]\n    \nPCA_comps.sort_values(by=['Id'], inplace=True)\n'''","e65bf977":"# KMEANS FOR L2","b98c515b":"# PCA for L3","9d71cb9a":"# PCA FOR L0","d6aabe16":"# KMEANS FOR L1","b9b02db3":"# DBSCAN L0"}}