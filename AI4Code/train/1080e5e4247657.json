{"cell_type":{"d7b40783":"code","619af110":"code","223d2cbc":"code","88328eb0":"code","13cce362":"code","bd52b82a":"code","56294738":"code","3f9198fd":"code","972af4ef":"code","809436b3":"code","3bfa3d81":"code","8feb63f0":"code","7393c807":"code","9799e6e0":"markdown"},"source":{"d7b40783":"import pandas as pd\nimport numpy as np\nimport pickle as pk\nfrom scipy import sparse as sp\nimport glob","619af110":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_area { max-width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.input_area { max-width:100% !important; }<\/style>\"))","223d2cbc":"%pylab inline\ndf = pd.read_csv('..\/input\/merge-multiple-json-files-to-a-dataframe\/df_train.csv').sample(1000)\ndocs = array(df['text'])","88328eb0":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\n\ndef docs_preprocessor(docs):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    for idx in range(len(docs)):\n        docs[idx] = str(docs[idx]).lower()  # Convert to lowercase.\n        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n    # Remove numbers, but not words that contain numbers.\n    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n    \n    # Remove words that are only one character.\n    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n    \n    # Lemmatize all words in documents.\n    lemmatizer = WordNetLemmatizer()\n    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n  \n    return docs","13cce362":"docs = docs_preprocessor(docs)","bd52b82a":"from gensim.models import Phrases\n# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\nbigram = Phrases(docs, min_count=10)\ntrigram = Phrases(bigram[docs])\n\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)\n    for token in trigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","56294738":"from gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\nprint('Number of unique words in initital documents:', len(dictionary))\n\n# Filter out words that occur less than 10 documents, or more than 20% of the documents.\ndictionary.filter_extremes(no_below=10, no_above=0.2)\nprint('Number of unique words after removing rare and common words:', len(dictionary))","3f9198fd":"corpus = [dictionary.doc2bow(doc) for doc in docs]","972af4ef":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","809436b3":"from gensim.models import LdaModel","3bfa3d81":"# Set training parameters.\nnum_topics = 10\nchunksize = 500 # size of the doc looked at every pass\npasses = 20 # number of passes through documents\niterations = 100\neval_every = 1  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","8feb63f0":"import pyLDAvis.gensim\nimport warnings\n\npyLDAvis.enable_notebook()\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","7393c807":"pyLDAvis.gensim.prepare(model, corpus, dictionary)","9799e6e0":"# [Show Us The Data] Topic Modelling with LDA\n\n## Reference\n\n- [LDA and T-SNE Interactive Visualization](https:\/\/www.kaggle.com\/ykhorramz\/lda-and-t-sne-interactive-visualization\/data?select=Papers.csv)\n- [Merge multiple JSON files to a DATAFRAME](https:\/\/www.kaggle.com\/hamditarek\/merge-multiple-json-files-to-a-dataframe?select=df_train.csv)"}}