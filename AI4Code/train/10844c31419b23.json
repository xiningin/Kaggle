{"cell_type":{"c5d16bf2":"code","ca88be5b":"code","f0726f11":"code","bb5c54e7":"code","c065877f":"code","7f37f0d4":"code","0c853b66":"code","88e2110f":"code","70f79097":"code","1e757d34":"code","d356bb0d":"code","7d51bee3":"code","22a51690":"code","0b1418d9":"code","6fa6894d":"code","726148f9":"code","ee6c1fd4":"code","fa9bf721":"code","b1350cb9":"code","22d52942":"code","c7357a50":"code","68d052a3":"code","e6780f42":"code","4914a3f6":"code","4db9e642":"code","47965646":"code","f1bac21b":"code","3eb05838":"code","3ca90708":"code","1a66edc3":"code","1c38c42f":"code","b38d06d1":"markdown"},"source":{"c5d16bf2":"!pip install plotly.express\n","ca88be5b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns","f0726f11":"dataset = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","bb5c54e7":"dataset","c065877f":"dataset.describe()\n","7f37f0d4":"dataset.info()\n","0c853b66":"dataset.isnull()","88e2110f":"sns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='Pastel1')","70f79097":"dataset.drop('Unnamed: 32',axis=1,inplace=True)","1e757d34":"dataset.shape","d356bb0d":"sns.set_style('darkgrid')\nsns.countplot(x='diagnosis',data=dataset)","7d51bee3":"dataset['diagnosis'].value_counts()","22a51690":"#encoding categorical data values\n# M = 1 , B = 0\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\ndataset.iloc[:,1] = labelencoder.fit_transform(dataset.iloc[:,1].values)\ndataset.iloc[:,1]\n","0b1418d9":"dataset","6fa6894d":"dataset.diagnosis.describe()","726148f9":"fig = px.histogram(dataset, \n                   x='diagnosis', \n                   marginal='box', \n                   nbins=0, \n                   title='Distribution of ')\nfig.update_layout(bargap=0.1)\nfig.show()","ee6c1fd4":"dataset.radius_mean.describe()","fa9bf721":"fig = px.histogram(dataset, \n                   x='radius_mean', \n                   marginal='box', \n                   nbins=21, \n                   title='Distribution of radius_mean')\nfig.update_layout(bargap=0.1)\nfig.show()","b1350cb9":"sns.pairplot(dataset.iloc[:,1:6], hue = 'diagnosis')","22d52942":"dataset.iloc[:,1:12].corr()","c7357a50":"plt.figure(figsize=(14,10))\nsns.heatmap(dataset.iloc[:,1:12].corr(), annot=True)","68d052a3":"X = dataset.iloc[:,2:31].values\nY = dataset.iloc[:,1].values","e6780f42":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)","4914a3f6":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","4db9e642":"def Models(X_train,Y_train):\n\n    #Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    import time\n    start = time.time()\n    log = LogisticRegression(random_state=0)\n    log.fit(X_train, Y_train)\n    stop = time.time()\n    print(f\"LogisticRegression Training time: {stop - start}s\")\n    \n    #Decision Tree Classifier\n    from sklearn.tree import DecisionTreeClassifier\n    import time\n    start=time.time()\n    regressor = DecisionTreeClassifier(criterion = 'entropy',random_state = 0)\n    regressor.fit(X_train, Y_train)\n    stop =time.time()\n    print(f\"DecisionTreeClassifier Training time: {stop - start}s\")\n\n\n     #Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    import time\n    start=time.time()\n    random_forest = RandomForestClassifier(n_estimators=10,criterion = 'entropy', random_state=3)\n    random_forest.fit(X_train, Y_train)\n    stop =time.time()\n    print(f\"RandomForestClassifier Training time: {stop - start}s\")\n    \n     #KNeighbors Classifier\n    from sklearn.neighbors import KNeighborsClassifier\n    import time\n    start=time.time()\n    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n    classifier.fit(X_train, Y_train)\n    stop=time.time()\n    print(f\"KNeighborsClassifier Training time: {stop - start}s\")\n    \n    \n    \n    \n        #Model Accuracy on Training Data\n    print(\"\\n\\n\\n\")\n    print('[0]Logistic Regression Training Acc:', log.score(X_train,Y_train))\n    print('[1]Decision Tree Training Acc:', regressor.score(X_train,Y_train))\n    print('[2]Random Forest Training Acc:', random_forest.score(X_train,Y_train))\n    print('[3]KNeighbor Acc:', classifier.score(X_train,Y_train))\n    \n    return log, regressor, random_forest,classifier\n\n    ","47965646":"model= Models(X_train,Y_train)","f1bac21b":"# Acc on Testing Data\nfrom sklearn.metrics import confusion_matrix\n\nfor i in range(len(model)):\n    print('model ', i)\n    cm = confusion_matrix(Y_test, model[i].predict(X_test))\n\n    tp = cm[0][0]\n    tn = cm[1][1]\n    fp = cm[1][0]\n    fn = cm[0][1]\n\n    print(cm)\n    print('Testing Acc = ', (tp + tn)\/(tp +tn +fn + fp))\n    print()","3eb05838":"# Logistic Regression\npred = model[0].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","3ca90708":"# DecisoinTree Regression\npred = model[1].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","1a66edc3":"# RandomForest Regression\npred = model[2].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","1c38c42f":"# KNeighbour Regression\npred = model[3].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","b38d06d1":"fig = px.histogram(dataset, \n                   x='diagnosis', \n                   marginal='box', \n                   nbins=1, \n                   title='Distribution of ')\nfig.update_layout(bargap=0.1)\nfig.show()"}}