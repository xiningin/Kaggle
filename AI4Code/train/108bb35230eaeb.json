{"cell_type":{"151cdad2":"code","a19541e5":"code","b45cdfe6":"code","5e5ab287":"code","4f7cf10a":"code","0c0e4529":"code","be33e9b1":"code","b9a6caa4":"code","233f162e":"code","d1791b55":"code","65ca057f":"code","08e81845":"code","4f07f17a":"code","829a9e0f":"code","0a24e4bd":"code","e77ae5db":"code","ea2a51b4":"code","c871a874":"code","c7d365d0":"code","f69f0a38":"code","1249f763":"code","1099e6fb":"code","cf17b2a4":"code","db39fe66":"markdown","aaa23a46":"markdown","a9bc1610":"markdown","1964438c":"markdown"},"source":{"151cdad2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport pickle\nfrom statsmodels.api import OLS\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\n\nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitem_categories = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nsales_train = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nshops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nsales_test = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nsales_train","a19541e5":"sns.boxplot(sales_train[\"item_cnt_day\"])","b45cdfe6":"sns.boxplot(sales_train[\"item_price\"])","5e5ab287":"sales_train[\"item_cnt_day\"] = sales_train[\"item_cnt_day\"].clip(0,200)\nsales_train[\"item_price\"] = sales_train[\"item_price\"].clip(0,5000)","4f7cf10a":"# item counts over months\nsales_train.groupby(\"date_block_num\")[\"item_cnt_day\"].sum().plot()","0c0e4529":"# distribution of shop_item combinations in train and test set\n# only a few shops\/items are in test and train set\n# no leakage can be identified, split seems random\n\ntrain_unique = sales_train.groupby([\"shop_id\", \"item_id\"]).size()\ntest_unique = sales_test.groupby([\"shop_id\", \"item_id\"]).size()\n\ntrain_unique = pd.DataFrame({\"in_train\":True}, index=train_unique.index)\ntest_unique = pd.DataFrame({\"in_test\":True}, index=test_unique.index)\n\ncombined = pd.merge(train_unique, test_unique, on=[\"shop_id\", \"item_id\"], how=\"outer\").fillna(False)\n\ncombined[\"in_both\"] = combined[\"in_train\"] & combined[\"in_test\"]\n\nnum_in_both = sum(combined[\"in_both\"] == True)\nnum_in_train = sum((combined[\"in_train\"] == True) & (combined[\"in_test\"] == False))\nnum_in_test = sum((combined[\"in_test\"] == True) & (combined[\"in_train\"] == False))\n\npd.DataFrame({\"in_both\":[num_in_both], \n              \"in_train\": [num_in_train],\n              \"in_test\": [num_in_test]}).T.plot.pie(subplots=True)","be33e9b1":"%%time\n#Process data into nice represenation of counts for each item, shop and date_block combination\nfrom itertools import product\n\nmatrix = []\nindex_cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = sales_train[sales_train.date_block_num == i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype=np.int16))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=index_cols)\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values(index_cols, inplace = True )\nmatrix","b9a6caa4":"# add target variable (item counts for each month) to data\nitem_counts = sales_train.groupby(index_cols).agg({\"item_cnt_day\": [\"sum\"]}).reset_index()\n\nmatrix = pd.merge(matrix, item_counts, on=index_cols, how=\"left\")\nmatrix.fillna(0, inplace=True)\nmatrix.rename(columns={matrix.columns[-1]: \"item_cnt_month\"}, inplace=True)\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].clip(0,20).astype(np.float16)\nmatrix","233f162e":"# add test data to matrix\nsales_test[\"date_block_num\"] = 34\nmatrix = pd.concat([matrix, sales_test.drop([\"ID\"], axis=1)], ignore_index=True, keys=index_cols)\nmatrix.fillna(0, inplace=True)\nmatrix","d1791b55":"# downgrade types\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.dtypes","65ca057f":"# lag features and mean encoding\n\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\nlag_dates = [1,2,3,6,12]\n\nmatrix = lag_feature(matrix, lag_dates, \"item_cnt_month\")\nmatrix.fillna(0, inplace=True)\n\n# mean item count per month\nfeature = matrix.groupby(['date_block_num']).agg({'item_cnt_month': 'mean'})\nfeature.index = feature.index+1\nfeature.columns = [\"item_cnt_month_mean_lag_1\"]\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\"], how=\"left\")\n\n# item category\nmatrix = pd.merge(matrix, items.drop([\"item_name\"], axis=1), how=\"left\", on=\"item_id\")\n\n# mean count of item_id\nfeature = matrix.groupby(['date_block_num', \"item_id\"]).agg({'item_cnt_month': 'mean'}).reset_index()\nfeature\nfeature.rename({\"item_cnt_month\": \"means_cnt_item_id_lag_1\"}, axis=1, inplace=True)\nfeature[\"date_block_num\"] += 1\nfeature\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n\n# mean count of shop_id\nfeature = matrix.groupby(['date_block_num', \"shop_id\"]).agg({'item_cnt_month': 'mean'}).reset_index()\nfeature\nfeature.rename({\"item_cnt_month\": \"means_cnt_shop_id_lag_1\"}, axis=1, inplace=True)\nfeature[\"date_block_num\"] += 1\nfeature\nmatrix = pd.merge(matrix, feature, on=[\"date_block_num\", \"shop_id\"], how=\"left\")\n\n# number of counts for shop\/item id combination per date_block_num\npiv = sales_train.pivot_table(values=\"item_price\", index=[\"shop_id\", \"item_id\"] ,columns=\"date_block_num\", aggfunc=\"mean\")\npiv = piv.fillna(method=\"ffill\", axis=1).fillna(method=\"bfill\", axis=1)\npiv_diff = piv.diff(axis=1)\npiv_diff.columns += 1\npiv_div = piv_diff.stack()\npiv_div.name = \"price_diff_lag_1\"\nmatrix = pd.merge(matrix, piv_div, on=[\"shop_id\", \"item_id\", \"date_block_num\"], how=\"left\")\n\nmatrix.fillna(0, inplace=True)","08e81845":"# textual and coordinate features from shop and category\n\nshops['city'] = shops['shop_name'].apply(lambda x: x.split()[0].lower())\nshops.loc[shops.city == '!\u044f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u044f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n\ncoords = dict()\ncoords['\u044f\u043a\u0443\u0442\u0441\u043a'] = (62.028098, 129.732555, 4)\ncoords['\u0430\u0434\u044b\u0433\u0435\u044f'] = (44.609764, 40.100516, 3)\ncoords['\u0431\u0430\u043b\u0430\u0448\u0438\u0445\u0430'] = (55.8094500, 37.9580600, 1)\ncoords['\u0432\u043e\u043b\u0436\u0441\u043a\u0438\u0439'] = (53.4305800, 50.1190000, 3)\ncoords['\u0432\u043e\u043b\u043e\u0433\u0434\u0430'] = (59.2239000, 39.8839800, 2)\ncoords['\u0432\u043e\u0440\u043e\u043d\u0435\u0436'] = (51.6720400, 39.1843000, 3)\ncoords['\u0432\u044b\u0435\u0437\u0434\u043d\u0430\u044f'] = (0, 0, 0)\ncoords['\u0436\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439'] = (55.5952800, 38.1202800, 1)\ncoords['\u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d'] = (0, 0, 0)\ncoords['\u043a\u0430\u0437\u0430\u043d\u044c'] = (55.7887400, 49.1221400, 4)\ncoords['\u043a\u0430\u043b\u0443\u0433\u0430'] = (54.5293000, 36.2754200, 4)\ncoords['\u043a\u043e\u043b\u043e\u043c\u043d\u0430'] = (55.0794400, 38.7783300, 4)\ncoords['\u043a\u0440\u0430\u0441\u043d\u043e\u044f\u0440\u0441\u043a'] = (56.0183900, 92.8671700, 4)\ncoords['\u043a\u0443\u0440\u0441\u043a'] = (51.7373300, 36.1873500, 3)\ncoords['\u043c\u043e\u0441\u043a\u0432\u0430'] = (55.7522200, 37.6155600, 1)\ncoords['\u043c\u044b\u0442\u0438\u0449\u0438'] = (55.9116300, 37.7307600, 1)\ncoords['\u043d.\u043d\u043e\u0432\u0433\u043e\u0440\u043e\u0434'] = (56.3286700, 44.0020500, 4)\ncoords['\u043d\u043e\u0432\u043e\u0441\u0438\u0431\u0438\u0440\u0441\u043a'] = (55.0415000, 82.9346000, 4)\ncoords['\u043e\u043c\u0441\u043a'] = (54.9924400, 73.3685900, 4)\ncoords['\u0440\u043e\u0441\u0442\u043e\u0432\u043d\u0430\u0434\u043e\u043d\u0443'] = (47.2313500, 39.7232800, 3)\ncoords['\u0441\u043f\u0431'] = (59.9386300, 30.3141300, 2)\ncoords['\u0441\u0430\u043c\u0430\u0440\u0430'] = (53.2000700, 50.1500000, 4)\ncoords['\u0441\u0435\u0440\u0433\u0438\u0435\u0432'] = (56.3000000, 38.1333300, 4)\ncoords['\u0441\u0443\u0440\u0433\u0443\u0442'] = (61.2500000, 73.4166700, 4)\ncoords['\u0442\u043e\u043c\u0441\u043a'] = (56.4977100, 84.9743700, 4)\ncoords['\u0442\u044e\u043c\u0435\u043d\u044c'] = (57.1522200, 65.5272200, 4)\ncoords['\u0443\u0444\u0430'] = (54.7430600, 55.9677900, 4)\ncoords['\u0445\u0438\u043c\u043a\u0438'] = (55.8970400, 37.4296900, 1)\ncoords['\u0446\u0438\u0444\u0440\u043e\u0432\u043e\u0439'] = (0, 0, 0)\ncoords['\u0447\u0435\u0445\u043e\u0432'] = (55.1477000, 37.4772800, 4)\ncoords['\u044f\u0440\u043e\u0441\u043b\u0430\u0432\u043b\u044c'] = (57.6298700, 39.8736800, 2) \n\nshops['city_coord_1'] = shops['city'].apply(lambda x: coords[x][0])\nshops['city_coord_2'] = shops['city'].apply(lambda x: coords[x][1])\nshops['country_part'] = shops['city'].apply(lambda x: coords[x][2])\n\nshops = shops[['shop_id', 'city_code', 'city_coord_1', 'city_coord_2', 'country_part']]\n\nmatrix = pd.merge(matrix, shops, on=[\"shop_id\"], how=\"left\")","4f07f17a":"# features about items and their category names (common category and category code)\n\nmap_dict = {\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)': '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)' : '\u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438',\n            'PC ': '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b',\n            '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435': '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 '\n            }\n\nitems = pd.merge(items, item_categories, on='item_category_id')\n\nitems['item_category'] = items['item_category_name'].apply(lambda x: x.split('-')[0])\nitems['item_category'] = items['item_category'].apply(lambda x: map_dict[x] if x in map_dict.keys() else x)\nitems['item_category_common'] = LabelEncoder().fit_transform(items['item_category'])\n\nitems['item_category_code'] = LabelEncoder().fit_transform(items['item_category_name'])\nitems = items[['item_id', 'item_category_common', 'item_category_code']]\n\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')","829a9e0f":"# interaction features: \n# - is item new\n# - has it been bought in shop before\n\nfirst_item_block = matrix.groupby(['item_id'])['date_block_num'].min().reset_index()\nfirst_item_block['item_first_interaction'] = 1\n\nfirst_shop_item_buy_block = matrix[matrix['date_block_num'] > 0].groupby(['shop_id', 'item_id'])['date_block_num'].min().reset_index()\nfirst_shop_item_buy_block['first_date_block_num'] = first_shop_item_buy_block['date_block_num']\n\nmatrix = pd.merge(matrix, first_item_block[['item_id', 'date_block_num', 'item_first_interaction']], on=['item_id', 'date_block_num'], how='left')\nmatrix = pd.merge(matrix, first_shop_item_buy_block[['item_id', 'shop_id', 'first_date_block_num']], on=['item_id', 'shop_id'], how='left')\n\nmatrix['first_date_block_num'].fillna(100, inplace=True)\nmatrix['shop_item_sold_before'] = (matrix['first_date_block_num'] < matrix['date_block_num']).astype('int8')\nmatrix.drop(['first_date_block_num'], axis=1, inplace=True)\n\nmatrix['item_first_interaction'].fillna(0, inplace=True)\nmatrix['shop_item_sold_before'].fillna(0, inplace=True)\n \nmatrix['item_first_interaction'] = matrix['item_first_interaction'].astype('int8')  \nmatrix['shop_item_sold_before'] = matrix['shop_item_sold_before'].astype('int8') ","0a24e4bd":"matrix.head()","e77ae5db":"# remove first 12 months and split into train, validation and test data\nmatrix = matrix[matrix[\"date_block_num\"] >= 12]\n\nX_train = matrix[matrix[\"date_block_num\"] < 33].drop([\"item_cnt_month\"], axis=1)\nX_val = matrix[matrix[\"date_block_num\"] == 33].drop([\"item_cnt_month\"], axis=1)\nX_test = matrix[matrix[\"date_block_num\"] == 34].drop([\"item_cnt_month\"], axis=1)\n\ny_train = matrix[matrix[\"date_block_num\"] < 33][\"item_cnt_month\"]\ny_val = matrix[matrix[\"date_block_num\"] == 33][\"item_cnt_month\"]","ea2a51b4":"# quick linear regression analysis on validation data\n\nresults = sm.OLS(y_val.to_numpy(), X_val.astype(float)).fit()\nresults.summary()","c871a874":"%%time\nxgb_model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nxgb_model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\npickle.dump(xgb_model, open(\"xgb_model.p\", \"wb\"))","c7d365d0":"lgbm_model = LGBMRegressor(\n    n_estimators=200,\n    learning_rate=0.03,\n    num_leaves=32,\n    colsample_bytree=0.9,\n    subsample=0.8,\n    max_depth=8,\n    reg_alpha=0.04,\n    reg_lambda=0.07,\n    min_split_gain=0.02,\n    min_child_weight=40,\n    seed=42\n)\n\nlgbm_model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\npickle.dump(lgbm_model, open(\"lgbm_model.p\", \"wb\"))","f69f0a38":"# xgb feature importance\ncols = X_val.columns\nplt.figure(figsize=(10,10))\nplt.barh(cols, xgb_model.feature_importances_)\nplt.show()","1249f763":"# lgbm feature importance\n\ncols = X_val.columns\nplt.figure(figsize=(10,10))\nplt.barh(cols, lgbm_model.feature_importances_)\nplt.show()","1099e6fb":"# predict and combine using a linear combination of results\n\nxgb_preds = xgb_model.predict(X_test)\nlgbm_preds = lgbm_model.predict(X_test)\n\nfinal_preds = (xgb_preds + lgbm_preds) \/ 2\nfinal_preds = final_preds.clip(0,20)\nfinal_preds","cf17b2a4":"submission = pd.DataFrame({\"item_cnt_month\": final_preds})\nsubmission.index.name=\"ID\"\nsubmission\nsubmission.to_csv(\"submission.csv\")","db39fe66":"**Feature Generation**","aaa23a46":"**Visualizing outliers and removing them**","a9bc1610":"**Validation and Testing**\n\nUse XGBoost and LightGBM to for prediction and combine the results","1964438c":"**Statistics and Visualizations**"}}