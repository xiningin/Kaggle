{"cell_type":{"7136c102":"code","54b48419":"code","0126dcd2":"code","51a80764":"code","4d74c219":"code","4e3c2f07":"code","6e37e2e9":"code","b12dc442":"code","3aa767f8":"code","f8943a2b":"code","3bb5b67d":"code","a6f511e8":"code","c19751da":"code","41312442":"code","9d13d5a0":"code","b9e8c6f5":"code","2654cf59":"code","4d1740fa":"code","ba5f6e26":"code","b2eb192f":"code","e959ced2":"code","ca9d9791":"code","e5129a04":"code","5d63b362":"code","b703bdd1":"code","6b9d0a03":"code","0adaf2a5":"markdown","9c139090":"markdown","62334086":"markdown","de701153":"markdown","37a39261":"markdown","08a8441d":"markdown","e81897b4":"markdown","3f7ffc48":"markdown","624df7d5":"markdown","cdc2b59e":"markdown","8da1bd36":"markdown","6c1848ec":"markdown","0503dd6b":"markdown","df83c9e3":"markdown","7d148cff":"markdown","ce538602":"markdown","bc435365":"markdown","fde4ebe8":"markdown","88e747c5":"markdown","eabe3b1d":"markdown","7148b559":"markdown"},"source":{"7136c102":"# !pip install --upgrade tensorflow\n# !pip install --upgrade tensorflow_hub\n!pip install apache_beam >> \/dev\/null\n# !pip install sklearn\n# !pip install annoy","54b48419":"import os\nimport sys\nimport pickle\nfrom collections import namedtuple\nfrom datetime import datetime\nimport numpy as np\nimport apache_beam as beam\nfrom apache_beam.transforms import util\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport annoy\nfrom sklearn.random_projection import gaussian_random_matrix\n\nimport time\nnotebookstart = time.time()","0126dcd2":"print('TF version: {}'.format(tf.__version__))\nprint('TF-Hub version: {}'.format(hub.__version__))\nprint('Apache Beam version: {}'.format(beam.__version__))","51a80764":"!wget 'https:\/\/dataverse.harvard.edu\/api\/access\/datafile\/3450625?format=tab&gbrecs=true' -O raw.tsv\n!wc -l raw.tsv\n!head raw.tsv","4d74c219":"!ls","4e3c2f07":"!rm -r corpus\n!mkdir corpus\n\nwith open('corpus\/text.txt', 'w') as out_file:\n    with open('raw.tsv', 'r') as in_file:\n        for line in in_file:\n            headline = line.split('\\t')[1].strip().strip('\"')\n            out_file.write(headline+\"\\n\")","6e37e2e9":"!tail corpus\/text.txt","b12dc442":"embed_fn = None\n\ndef generate_embeddings(text, module_url, random_projection_matrix=None):\n    # Beam will run this function in different processes that need to\n    # import hub and load embed_fn (if not previously loaded)\n    global embed_fn\n    if embed_fn is None:\n        embed_fn = hub.load(module_url)\n    embedding = embed_fn(text).numpy()\n    if random_projection_matrix is not None:\n        embedding = embedding.dot(random_projection_matrix)\n    return text, embedding","3aa767f8":"def to_tf_example(entries):\n    examples = []\n\n    text_list, embedding_list = entries\n    for i in range(len(text_list)):\n        text = text_list[i]\n        embedding = embedding_list[i]\n\n        features = {\n            'text': tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[text.encode('utf-8')])),\n            'embedding': tf.train.Feature(\n                float_list=tf.train.FloatList(value=embedding.tolist()))\n        }\n\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature=features)).SerializeToString(deterministic=True)\n\n        examples.append(example)\n\n    return examples","f8943a2b":"def run_hub2emb(args):\n    '''Runs the embedding generation pipeline'''\n\n    options = beam.options.pipeline_options.PipelineOptions(**args)\n    args = namedtuple(\"options\", args.keys())(*args.values())\n\n    with beam.Pipeline(args.runner, options=options) as pipeline:\n        (\n            pipeline\n            | 'Read sentences from files' >> beam.io.ReadFromText(\n                file_pattern=args.data_dir)\n            | 'Batch elements' >> util.BatchElements(\n                min_batch_size=args.batch_size, max_batch_size=args.batch_size)\n            | 'Generate embeddings' >> beam.Map(\n                generate_embeddings, args.module_url, args.random_projection_matrix)\n            | 'Encode to tf example' >> beam.FlatMap(to_tf_example)\n            | 'Write to TFRecords files' >> beam.io.WriteToTFRecord(\n                file_path_prefix='{}\/emb'.format(args.output_dir),\n                file_name_suffix='.tfrecords')\n        )","3bb5b67d":"def generate_random_projection_weights(original_dim, projected_dim):\n    random_projection_matrix = None\n    random_projection_matrix = gaussian_random_matrix(\n        n_components=projected_dim, n_features=original_dim).T\n    print(\"A Gaussian random weight matrix was creates with shape of {}\".format(random_projection_matrix.shape))\n    print('Storing random projection matrix to disk...')\n    with open('random_projection_matrix', 'wb') as handle:\n        pickle.dump(random_projection_matrix, \n                    handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return random_projection_matrix","a6f511e8":"module_url = 'https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1' #@param {type:\"string\"}\nprojected_dim = 64  #@param {type:\"number\"}","c19751da":"output_dir = '\/embeds'\noriginal_dim = hub.load(module_url)(['']).shape[1]\nrandom_projection_matrix = None\n\nif projected_dim:\n    random_projection_matrix = generate_random_projection_weights(\n        original_dim, projected_dim)\n\nargs = {\n    'job_name': 'hub2emb-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S')),\n    'runner': 'DirectRunner',\n    'batch_size': 1024,\n    'data_dir': 'corpus\/*.txt',\n    'output_dir': output_dir,\n    'module_url': module_url,\n    'random_projection_matrix': random_projection_matrix,\n}\n\nprint(\"Pipeline args are set.\")\nargs","41312442":"!rm -r {output_dir}\n\nprint(\"Running pipeline...\")\n%time run_hub2emb(args)\nprint(\"Pipeline is done.\")","9d13d5a0":"!ls {output_dir}","b9e8c6f5":"embed_file = os.path.join(output_dir, 'emb-00000-of-00001.tfrecords')\nsample = 5\n\n# Create a description of the features.\nfeature_description = {\n    'text': tf.io.FixedLenFeature([], tf.string),\n    'embedding': tf.io.FixedLenFeature([projected_dim], tf.float32)\n}\n\ndef _parse_example(example):\n    # Parse the input `tf.Example` proto using the dictionary above.\n    return tf.io.parse_single_example(example, feature_description)\n\ndataset = tf.data.TFRecordDataset(embed_file)\nfor record in dataset.take(sample).map(_parse_example):\n    print(\"{}: {}\".format(record['text'].numpy().decode('utf-8'), record['embedding'].numpy()[:10]))","2654cf59":"def build_index(embedding_files_pattern, index_filename, vector_length, \n        metric='angular', num_trees=100):\n    '''Builds an ANNOY index'''\n\n    annoy_index = annoy.AnnoyIndex(vector_length, metric=metric)\n    # Mapping between the item and its identifier in the index\n    mapping = {}\n\n    embed_files = tf.io.gfile.glob(embedding_files_pattern)\n    num_files = len(embed_files)\n    print('Found {} embedding file(s).'.format(num_files))\n\n    item_counter = 0\n    for i, embed_file in enumerate(embed_files):\n        print('Loading embeddings in file {} of {}...'.format(i+1, num_files))\n        dataset = tf.data.TFRecordDataset(embed_file)\n        for record in dataset.map(_parse_example):\n            text = record['text'].numpy().decode(\"utf-8\")\n            embedding = record['embedding'].numpy()\n            mapping[item_counter] = text\n            annoy_index.add_item(item_counter, embedding)\n            item_counter += 1\n            if item_counter % 100000 == 0:\n                print('{} items loaded to the index'.format(item_counter))\n\n    print('A total of {} items added to the index'.format(item_counter))\n\n    print('Building the index with {} trees...'.format(num_trees))\n    annoy_index.build(n_trees=num_trees)\n    print('Index is successfully built.')\n\n    print('Saving index to disk...')\n    annoy_index.save(index_filename)\n    print('Index is saved to disk.')\n    print(\"Index file size: {} GB\".format(\n        round(os.path.getsize(index_filename) \/ float(1024 ** 3), 2)))\n    annoy_index.unload()\n\n    print('Saving mapping to disk...')\n    with open(index_filename + '.mapping', 'wb') as handle:\n        pickle.dump(mapping, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    print('Mapping is saved to disk.')\n    print(\"Mapping file size: {} MB\".format(\n        round(os.path.getsize(index_filename + '.mapping') \/ float(1024 ** 2), 2)))","4d1740fa":"embedding_files = \"{}\/emb-*.tfrecords\".format(output_dir)\nembedding_dimension = projected_dim\nindex_filename = \"index\"\n\n!rm {index_filename}\n!rm {index_filename}.mapping\n\n%time build_index(embedding_files, index_filename, embedding_dimension)","ba5f6e26":"!ls","b2eb192f":"index = annoy.AnnoyIndex(embedding_dimension)\nindex.load(index_filename, prefault=True)\nprint('Annoy index is loaded.')\nwith open(index_filename + '.mapping', 'rb') as handle:\n    mapping = pickle.load(handle)\nprint('Mapping file is loaded.')","e959ced2":"def find_similar_items(embedding, num_matches=5):\n    '''Finds similar items to a given embedding in the ANN index'''\n    ids = index.get_nns_by_vector(\n    embedding, num_matches, search_k=-1, include_distances=False)\n    items = [mapping[i] for i in ids]\n    return items","ca9d9791":"# Load the TF-Hub module\nprint(\"Loading the TF-Hub module...\")\n%time embed_fn = hub.load(module_url)\nprint(\"TF-Hub module is loaded.\")\n\nrandom_projection_matrix = None\nif os.path.exists('random_projection_matrix'):\n    print(\"Loading random projection matrix...\")\n    with open('random_projection_matrix', 'rb') as handle:\n        random_projection_matrix = pickle.load(handle)\n    print('random projection matrix is loaded.')\n\ndef extract_embeddings(query):\n    '''Generates the embedding for the query'''\n    query_embedding =  embed_fn([query])[0].numpy()\n    if random_projection_matrix is not None:\n        query_embedding = query_embedding.dot(random_projection_matrix)\n    return query_embedding","e5129a04":"test_embeddings = extract_embeddings(\"Hello Machine Learning!\")\nprint(test_embeddings.shape)\nprint(test_embeddings[:10])","5d63b362":"def similar_items(query, n = 10):\n    \"\"\"\n    @param {type:\"string\"}\n    \"\"\"\n    print(f\"\\nNEXT QUERY:\\n{query}\")\n    query_embedding = extract_embeddings(query)\n    items = find_similar_items(query_embedding, n)\n\n    print(\"Results:\")\n    print(\"=========\")\n    for item in items:\n        print(item)","b703bdd1":"%%time\nlets_query = [\n    \"confronting global challenges\",\n    \"global sentiment of trump\",\n    \"internet and fake news\"\n    \"artificial intelligence\",\n    \"flu outbreak danger\",\n    \"python programming language portfolio projects\",\n    \"house cats and mental health\",\n    \"government corruption\",\n    \"the spread of love globally\",\n    \"the spread of hate globally\",\n    \"human health in the winter\",\n    \"human health in the summer\",\n    \"temper issues and metal music\",\n    \"mind control and pop music\",\n    \"legitimacy and vice of bitcoin\",\n    \"dollar economic policy\",\n    \"consumption and growth in the united states\",\n    \"future militerization of animals\",\n    \"google corporation issues and ethics\",\n    \"monsanto corporation issues and ethics\",\n    \"amazon corporation issues and ethics\",\n    \"science, progress, and pitfalls\",\n    \"aliens and life on mars\",\n    \"future of religious belief\",\n    \"faith in the modern world\",\n    \"day in the life of an intern\",\n    \"best way to waste time\",\n    \"you will not believe what happened\",\n    \"top five ways to make more money\"\n]\n\nfor q in lets_query:\n    similar_items(q, n = 20)","6b9d0a03":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","0adaf2a5":"Read some of the generated embeddings...","9c139090":"### Set parameters\nIf you want to build an index using the original embedding space without random projection, set the `projected_dim` parameter to `None`. Note that this will slow down the indexing step for high-dimensional embeddings.","62334086":"### Run pipeline","de701153":"For simplicity, we only keep the headline text and remove the publication date","37a39261":"Install the required libraries.","08a8441d":"## 4. Use the Index for Similarity Matching\nNow we can use the ANN index to find news headlines that are semantically close to an input query.","e81897b4":"Import the required libraries","3f7ffc48":"Not my work.. Taken From Google Tensorflow Repo in order to take a look at outputs [HERE](https:\/\/github.com\/tensorflow\/hub\/blob\/master\/examples\/colab\/tf2_semantic_approximate_nearest_neighbors.ipynb)\n\nMake sure to checkout the query similarity exploration at the end :)\n\n***\n\n# Semantic Search with Approximate Nearest Neighbors and Text Embeddings from TF-Hub\n\nThis tutorial illustrates how to generate embeddings from a [TensorFlow Hub](https:\/\/tfhub.dev) (TF-Hub) module given input data, and build an approximate nearest neighbours (ANN) index using the extracted embeddings. The index can then be used for real-time similarity matching and retrieval.\n\nWhen dealing with a large corpus of data, it's not efficient to perform exact matching by scanning the whole repository to find the most similar items to a given query in real-time. Thus, we use an approximate similarity matching algorithm which allows us to trade off a little bit of accuracy in finding exact nearest neighbor matches for a significant boost in speed.\n\nIn this tutorial, we show an example of real-time text search over a corpus of news headlines to find the headlines that are most similar to a query. Unlike keyword search, this captures the semantic similarity encoded in the text embedding.\n\nThe steps of this tutorial are:\n1. Download sample data.\n2. Generate embeddings for the data using a TF-Hub module\n3. Build an ANN index for the embeddings\n4. Use the index for similarity matching\n\nWe use [Apache Beam](https:\/\/beam.apache.org\/documentation\/programming-guide\/) to generate the embeddings from the TF-Hub module. We also use Spotify's [ANNOY](https:\/\/github.com\/spotify\/annoy) library to build the approximate nearest neighbours index.","624df7d5":"### Embedding extraction method","cdc2b59e":"### Load the index and the mapping files","8da1bd36":"### Extract embedding from a given query","6c1848ec":"### Generaring Random Projection Weight Matrix\n\n[Random projection](https:\/\/en.wikipedia.org\/wiki\/Random_projection) is a simple, yet powerfull technique used to reduce the dimensionality of a set of points which lie in Euclidean space. For a theoretical background, see the [Johnson-Lindenstrauss lemma](https:\/\/en.wikipedia.org\/wiki\/Johnson%E2%80%93Lindenstrauss_lemma).\n\nReducing the dimensionality of the embeddings with random projection means less time needed to build and query the ANN index.\n\nIn this tutorial we use [Gaussian Random Projection](https:\/\/en.wikipedia.org\/wiki\/Random_projection#Gaussian_random_projection) from the [Scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/random_projection.html#gaussian-random-projection) library.","0503dd6b":"### Beam pipeline","df83c9e3":"## Want to learn more?\n\nYou can learn more about TensorFlow at [tensorflow.org](https:\/\/www.tensorflow.org\/) and see the TF-Hub API documentation at [tensorflow.org\/hub](https:\/\/www.tensorflow.org\/hub\/). Find available TensorFlow Hub modules at [tfhub.dev](https:\/\/tfhub.dev\/) including more text embedding modules and image feature vector modules.\n\nAlso check out the [Machine Learning Crash Course](https:\/\/developers.google.com\/machine-learning\/crash-course\/) which is Google's fast-paced, practical introduction to machine learning.","7d148cff":"### Enter a query to find the most similar items","ce538602":"## 3. Build the ANN Index for the Embeddings\n\n[ANNOY](https:\/\/github.com\/spotify\/annoy) (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory. It is built and used by [Spotify](https:\/\/www.spotify.com) for music recommendations.","bc435365":"## 2. Generate Embeddings for the Data.\n\nIn this tutorial, we use the [Neural Network Language Model (NNLM)](https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1) to generate embeddings for the headline data. The sentence embeddings can then be easily used to compute sentence level meaning similarity. We run the embedding generation process using Apache Beam.","fde4ebe8":"## 1. Download Sample Data\n\n[A Million News Headlines](https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/SYBGZL#) dataset contains news headlines published over a period of 15 years sourced from the reputable Australian Broadcasting Corp. (ABC). This news dataset has a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia. \n\n**Format**: Tab-separated two-column data: 1) publication date and 2) headline text. We are only interested in the headline text.\n\n","88e747c5":"### Similarity matching method","eabe3b1d":"## Getting Started","7148b559":"### Convert to tf.Example method"}}