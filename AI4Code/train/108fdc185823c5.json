{"cell_type":{"0e770d5e":"code","530f460c":"code","27396855":"code","5db854fc":"code","cddd46f6":"code","2f9a7974":"code","2b072d10":"code","fc32ec45":"code","fdd09410":"code","ef3f7ae8":"code","14bf1b37":"code","540e1ce6":"code","427e42b1":"code","92c70d2a":"code","f49a7e2b":"code","e360ce43":"code","091c6472":"code","a407698d":"code","76213918":"code","7dbbb3ad":"code","ef4ddc6f":"code","b015e7ae":"code","c00b40dc":"code","8459aa9a":"code","3d5cce4a":"code","a7cf89ae":"code","89095cbb":"code","1da2907c":"code","a78d217d":"code","00127146":"code","aa58961b":"code","b3078ab3":"code","7d9ae110":"code","041bb063":"markdown","28dc4881":"markdown","79cddcbe":"markdown","22c4b7e9":"markdown","b74e4619":"markdown","c5e25ab2":"markdown","8650db78":"markdown"},"source":{"0e770d5e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n%matplotlib inline\nimport cv2\nfrom scipy.stats import itemfreq\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport random # for setting seed\nimport tensorflow\nimport keras\nimport IPython","530f460c":"MY_SEED = 42 # 480 could work too\nrandom.seed(MY_SEED)\nnp.random.seed(MY_SEED)\ntensorflow.set_random_seed(MY_SEED)\n\nprint(IPython.sys_info())\n# get module information\n!pip freeze > frozen-requirements.txt\n# append system information to file\nwith open(\"frozen-requirements.txt\", \"a\") as file:\n    file.write(IPython.sys_info())","27396855":"from tensorflow.python.client import device_lib\n# print out the CPUs and GPUs\nprint(device_lib.list_local_devices())","5db854fc":"# https:\/\/stackoverflow.com\/questions\/25705773\/image-cropping-tool-python\n# because painting images are hella big\nfrom PIL import Image\nImage.MAX_IMAGE_PIXELS = None","cddd46f6":"# globals\n\nDATA_DIR = '..\/input\/painters-train-part-1\/'\n\nTRAIN_1_DIR = '..\/input\/painters-train-part-1\/train_1\/train_1\/'\nTRAIN_2_DIR = '..\/input\/painters-train-part-1\/train_2\/train_2\/'\nTRAIN_3_DIR = '..\/input\/painters-train-part-1\/train_3\/train_3\/'\n\nTRAIN_4_DIR = '..\/input\/painters-train-part-2\/train_4\/train_4\/'\nTRAIN_5_DIR = '..\/input\/painters-train-part-2\/train_5\/train_5\/'\nTRAIN_6_DIR = '..\/input\/painters-train-part-2\/train_6\/train_6\/'\n\nTRAIN_7_DIR = '..\/input\/painters-train-part-3\/train_7\/train_7\/'\nTRAIN_8_DIR = '..\/input\/painters-train-part-3\/train_8\/train_8\/'\nTRAIN_9_DIR = '..\/input\/painters-train-part-3\/train_9\/train_9\/'\n\nTRAIN_DIRS = [TRAIN_1_DIR, TRAIN_2_DIR, TRAIN_3_DIR,\n             TRAIN_4_DIR, TRAIN_5_DIR, TRAIN_6_DIR,\n             TRAIN_7_DIR, TRAIN_8_DIR, TRAIN_9_DIR]\n\nTEST_DIR = '..\/input\/painter-test\/test\/test\/'","2f9a7974":"df = pd.read_csv(DATA_DIR + 'all_data_info.csv')\nprint(\"df.shape\", df.shape)","2b072d10":"# quick fix for corrupted files\nlist_of_corrupted = ['3917.jpg','18649.jpg','20153.jpg','41945.jpg',\n'79499.jpg','91033.jpg','92899.jpg','95347.jpg',\n'100532.jpg','101947.jpg']\n# display the corrupted rows of dataset for context\ncorrupt_df = df[df[\"new_filename\"].isin(list_of_corrupted) == True]\nprint(corrupt_df.head(len(list_of_corrupted)))\n\n# completely get rid of them\ndf = df[df[\"new_filename\"].isin(list_of_corrupted) == False]\n\n# try to see if they are still there\nprint(df[df[\"new_filename\"].isin(list_of_corrupted) == True])\n\nprint(\"df.shape\", df.shape)","fc32ec45":"\ntrain_df = df[df[\"in_train\"] == True]\ntest_df = df[df['in_train'] == False]\ntrain_df = train_df[['artist', 'new_filename']]\ntest_df = test_df[['artist', 'new_filename']]\n\nprint(\"test_df.shape\", test_df.shape)\nprint(\"train_df.shape\", train_df.shape)\n\nartists = {} # holds artist hash & the count\nfor a in train_df['artist']:\n    if (a not in artists):\n        artists[a] = 1\n    else:\n        artists[a] += 1\n\ntraining_set_artists = []\nfor a,count in artists.items():\n    if(int(count) >= 300):\n        training_set_artists.append(a)\n\nprint(\"number of artsits\",len(training_set_artists))\n\nprint(\"\\nlist of artists...\\n\", training_set_artists)\n","fdd09410":"t_df = train_df[train_df[\"artist\"].isin(training_set_artists)]\n\nt_df.head(5)","ef3f7ae8":"t1_df = t_df[t_df['new_filename'].str.startswith('1')]\n\nt2_df = t_df[t_df['new_filename'].str.startswith('2')]\n\nt3_df = t_df[t_df['new_filename'].str.startswith('3')]\n\nt4_df = t_df[t_df['new_filename'].str.startswith('4')]\n\nt5_df = t_df[t_df['new_filename'].str.startswith('5')]\n\nt6_df = t_df[t_df['new_filename'].str.startswith('6')]\n\nt7_df = t_df[t_df['new_filename'].str.startswith('7')]\n\nt8_df = t_df[t_df['new_filename'].str.startswith('8')]\n\nt9_df = t_df[t_df['new_filename'].str.startswith('9')]\n\nall_train_dfs = [t1_df, t2_df, t3_df,\n                t4_df, t5_df, t6_df,\n                t7_df, t8_df, t9_df]\n\nt9_df.head(5)","14bf1b37":"from tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","540e1ce6":"len(training_set_artists)","427e42b1":"num_classes = len(training_set_artists) # one class per artist\nweights_notop_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nmodel = Sequential()\nmodel.add(ResNet50(\n  include_top=False,\n  weights=weights_notop_path,\n  pooling='avg'\n))\nmodel.add(Dense(\n  num_classes,\n  activation='softmax'\n))\n\nmodel.layers[0].trainable = False","92c70d2a":"model.compile(\n  optimizer='adam', # lots of people reccommend Adam optimizer\n  loss='categorical_crossentropy', # aka \"log loss\" -- the cost function to minimize \n  # so 'optimizer' algorithm will minimize 'loss' function\n  metrics=['accuracy'] # ask it to report % of correct predictions\n)","f49a7e2b":"# model globals\nIMAGE_SIZE = 224\nBATCH_SIZE = 96\nTEST_BATCH_SIZE = 17 # because test has 23817 images and factors of 23817 are 3*17*467\n                     # it is important that this number evenly divides the total num images \nVAL_SPLIT = 0.25","e360ce43":"def setup_generators(\n    val_split, train_dataframe, train_dir,\n    img_size, batch_size, my_seed, list_of_classes,\n    test_dataframe, test_dir, test_batch_size\n):\n    print(\"-\"*20)\n    if not preprocess_input:\n          raise Exception(\"please do import call 'from tensorflow.python.keras.applications.resnet50 import preprocess_input'\")\n\n    # setup resnet50 preprocessing \n    data_gen = ImageDataGenerator(\n        preprocessing_function=preprocess_input,\n        validation_split=val_split)\n\n    print(len(train_dataframe), \"images in\", train_dir, \"and validation_split =\", val_split)\n    print(\"\\ntraining set ImageDataGenerator\")\n    train_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        target_size=(img_size, img_size),\n        subset=\"training\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    print(\"\\nvalidation set ImageDataGenerator\")\n    valid_gen = data_gen.flow_from_dataframe(\n        dataframe=train_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=train_dir,\n        x_col='new_filename',\n        y_col='artist',\n        has_ext=True,\n        subset=\"validation\",\n        batch_size=batch_size,\n        seed=my_seed,\n        shuffle=True,\n        target_size=(img_size,img_size),\n        class_mode='categorical',\n        classes=list_of_classes\n    )\n\n    test_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n    print(\"\\ntest set ImageDataGenerator\")\n    test_gen = test_data_gen.flow_from_dataframe(\n        dataframe=test_dataframe.reset_index(), # call reset_index() so keras can start with index 0\n        directory=test_dir,\n        x_col='new_filename',\n        y_col=None,\n        has_ext=True,\n        batch_size=test_batch_size,\n        seed=my_seed,\n        shuffle=False, # dont shuffle test directory\n        class_mode=None,\n        target_size=(img_size,img_size)\n    )\n\n    return (train_gen, valid_gen, test_gen)\n\nprint(\"defined setup_generators()\")","091c6472":"train_gens = [None]*len(TRAIN_DIRS)\nvalid_gens = [None]*len(TRAIN_DIRS)\ntest_gen  = None # only 1 test_gen\ni = 0\nfor i in range(0, len(TRAIN_DIRS)):\n    train_gens[i], valid_gens[i], test_gen = setup_generators(\n        train_dataframe=all_train_dfs[i], train_dir=TRAIN_DIRS[i],\n        val_split=VAL_SPLIT, img_size=IMAGE_SIZE, batch_size=BATCH_SIZE, my_seed=MY_SEED, \n        list_of_classes=training_set_artists, test_dataframe=test_df, \n        test_dir=TEST_DIR, test_batch_size=TEST_BATCH_SIZE\n    )\n    i += 1","a407698d":"# the tutorial had 10 epochs... \nMAX_EPOCHS = 2 * len(train_gens) # should be a multiple of 9 because need evenly train each train_dir\nDIR_EPOCHS = 1 # fit each train_dir at least this many times before overfitting","76213918":"histories = []\n\ne=0\nwhile ( e < MAX_EPOCHS):\n    for i in range(0, len(train_gens)):\n        # train_gen.n = number of images for training\n        STEP_SIZE_TRAIN = train_gens[i].n\/\/train_gens[i].batch_size\n        # train_gen.n = number of images for validation\n        STEP_SIZE_VALID = valid_gens[i].n\/\/valid_gens[i].batch_size\n        print(\"STEP_SIZE_TRAIN\",STEP_SIZE_TRAIN)\n        print(\"STEP_SIZE_VALID\",STEP_SIZE_VALID)\n        histories.append(\n            model.fit_generator(generator=train_gens[i],\n                                steps_per_epoch=STEP_SIZE_TRAIN,\n                                validation_data=valid_gens[i],\n                                validation_steps=STEP_SIZE_VALID,\n                                epochs=DIR_EPOCHS)\n        )\n        e+=1","7dbbb3ad":"# for i in range(0, len(valid_gens)):\n# model.evaluate_generator(generator=valid_gens[8])","ef4ddc6f":"accuracies = []\nval_accuracies = []\nlosses = []\nval_losses = []\nfor hist in histories:\n    if hist:\n        accuracies += hist.history['acc']\n        val_accuracies += hist.history['val_acc']\n        losses += hist.history['loss']\n        val_losses += hist.history['val_loss']","b015e7ae":"# Plot training & validation accuracy values\nplt.plot(accuracies)\nplt.plot(val_accuracies)\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(losses)\nplt.plot(val_losses)\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","c00b40dc":"model.save('painters_adam.h5')","8459aa9a":"PRED_STEPS = len(test_gen) #100 # default would have been len(test_gen)","3d5cce4a":"# Need to reset the test_gen before calling predict_generator\n# This is important because forgetting to reset the test_generator results in outputs with a weird order.\ntest_gen.reset()\npred=model.predict_generator(test_gen, verbose=1, steps=PRED_STEPS)","a7cf89ae":"print(len(pred),\"\\n\",pred)","89095cbb":"predicted_class_indices=np.argmax(pred,axis=1)","1da2907c":"print(len(predicted_class_indices),\"\\n\",predicted_class_indices)\nprint(\"it has values ranging from \",min(predicted_class_indices),\"...to...\",max(predicted_class_indices))","a78d217d":"labels = (train_gens[0].class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]","00127146":"print(\"*\"*20+\"\\nclass_indices\\n\"+\"*\"*20+\"\\n\",train_gens[0].class_indices,\"\\n\")\nprint(\"*\"*20+\"\\nlabels\\n\"+\"*\"*20+\"\\n\",labels,\"\\n\")\nprint(\"*\"*20+\"\\npredictions has\", len(predictions),\"values that look like\",\"'\"+str(predictions[0])+\"' which is the first prediction and corresponds to this index of the classes:\",train_gens[0].class_indices[predictions[0]])","aa58961b":"# Save the results to a CSV file.\nfilenames=test_gen.filenames[:len(predictions)] # because \"ValueError: arrays must all be same length\"\n\nreal_artists = []\nfor f in filenames:\n    real = test_df[test_df['new_filename'] == f].artist.get_values()[0]\n    real_artists.append(real)\n\nresults=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions,\n                      \"Real Values\":real_artists})\nresults.to_csv(\"results.csv\",index=False)","b3078ab3":"results.head()","7d9ae110":"count = 0\nmatch = 0\nfor p, r in zip(results['Predictions'], results['Real Values']):\n    count += 1\n    if p == r:\n        match += 1\n\nprint(\"test accuracy on never before seen images\")\nprint(match,\"\/\",count,\"=\",\"{:.4f}\".format(match\/count))","041bb063":"# specify the model that classifies 38 artists \ud83c\udfa8 \ud83d\udd8c","28dc4881":"# Predict the output \ud83d\udd2e \ud83c\udfa9","79cddcbe":"# Compile Model","22c4b7e9":"# Evaluate the model \ud83e\uddd0 \ud83e\udd14","b74e4619":"#\u00a0setup the image data generator for train_1 with a dataframe\n### https:\/\/medium.com\/@vijayabhaskar96\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c","c5e25ab2":"# TRAINING TIME!  \ud83c\udf89 \ud83c\udf8a \ud83c\udf81","8650db78":"# now we have the artists with over 300 paintings and are in the training set"}}