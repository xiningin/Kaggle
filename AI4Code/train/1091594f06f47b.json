{"cell_type":{"c4b3def3":"code","bca4755a":"code","96dafa43":"code","72eac512":"code","64b1875a":"code","6d3042ab":"code","cc481420":"code","12cd2bfa":"code","9f60aa96":"code","6614dad5":"code","08efe069":"code","730ace7a":"code","266a40d6":"code","bedb4404":"code","1cc400a7":"code","5e4b77bb":"code","eafc3679":"code","b4fb2e91":"markdown","2a7b5f04":"markdown","394016d5":"markdown","304f53ad":"markdown","24525580":"markdown","44ba4302":"markdown","3221cffc":"markdown","610813ff":"markdown","8c501730":"markdown"},"source":{"c4b3def3":"import pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score","bca4755a":"def train_test_split(df, train_proportion=0.7):\n    \"\"\"\n        This function splits the data into train\/test sets, and\n        returns their indices on the dataframe received as parameter\n    \"\"\"\n    \n    shuffled_indices = df.sample(frac=1, replace=False).index\n    pivot = int(len(shuffled_indices) * train_proportion)\n\n    return shuffled_indices[:pivot], shuffled_indices[pivot:]","96dafa43":"df = pd.read_csv(\"..\/input\/house-pricing-in-belo-horizonte\/data.csv\")\ndf.head(3)","72eac512":"feature_cols = [\"adm-fees\", \"garage-places\", \"square-foot\", \"price\"]\ntarget_col = \"less-than-4-rooms\"\n\ndf = df[\n    (~df[\"rooms\"].str.contains('-'))\n    & (~df[\"square-foot\"].str.contains('-'))\n    & (~df[\"garage-places\"].str.contains('-'))\n]\n\ndf[\"adm-fees\"] = df[\"adm-fees\"].fillna(0)\ndf = df.dropna(subset=[\"price\", \"neighborhood\"])\n\ndf.loc[:, [\"adm-fees\", \"garage-places\", \"rooms\", \"square-foot\"]] =  df.loc[\n    :, \n    [\"adm-fees\", \"garage-places\", \"rooms\", \"square-foot\"]\n].astype(\"int64\")\n\ndf[target_col] = (df[\"rooms\"] < 4).astype(int)","64b1875a":"df[feature_cols].describe()","6d3042ab":"df[feature_cols].isna().sum()","cc481420":"df_scaled = df.copy()","12cd2bfa":"scaler = MinMaxScaler()\n\ndf_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])","9f60aa96":"df_scaled[feature_cols].describe()","6614dad5":"model_raw = LogisticRegression()","08efe069":"train_idx, test_idx = train_test_split(df)\n\nmodel_raw.fit(df.loc[train_idx, feature_cols], df.loc[train_idx, target_col])\nraw_preds = model_raw.predict(df.loc[test_idx, feature_cols])\n\nf1_score(df.loc[test_idx, target_col], raw_preds)","730ace7a":"model_scaled = LogisticRegression()","266a40d6":"model_scaled.fit(df_scaled.loc[train_idx, feature_cols], df_scaled.loc[train_idx, target_col])\nnormed_preds = model_scaled.predict(df_scaled.loc[test_idx, feature_cols])\n\nf1_score(df_scaled.loc[test_idx, target_col], normed_preds)","bedb4404":"model_raw = XGBClassifier()\n\nmodel_raw.fit(df.loc[train_idx, feature_cols], df.loc[train_idx, target_col])\nraw_preds = model_raw.predict(df.loc[test_idx, feature_cols])\n\nf1_score(df.loc[test_idx, target_col], raw_preds)","1cc400a7":"model_scaled = XGBClassifier()\n\nmodel_scaled.fit(df_scaled.loc[train_idx, feature_cols], df_scaled.loc[train_idx, target_col])\nnormed_preds = model_scaled.predict(df_scaled.loc[test_idx, feature_cols])\n\nf1_score(df_scaled.loc[test_idx, target_col], normed_preds)","5e4b77bb":"scaler = StandardScaler()\n\ndf_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])","eafc3679":"model_scaled = LogisticRegression()\n\nmodel_scaled.fit(df_scaled.loc[train_idx, feature_cols], df_scaled.loc[train_idx, target_col])\nnormed_preds = model_scaled.predict(df_scaled.loc[test_idx, feature_cols])\n\nf1_score(df_scaled.loc[test_idx, target_col], normed_preds)","b4fb2e91":"We applied a Min Max Scaler, which places the value for each feature within the [0-1] range. See the description below.","2a7b5f04":"As we can see, under the same circumstances, the same model performed better with the scaled data (just like we expected). This result most likely derives from the large difference that `price` if compared to the remaining features.\n\n---\n\n## When is it Expendable?\n\nEven though this may be a crucial step towards building useful models, not all methods benefit from this approach. For example, **any kind of tree-based solution may prescind from scaling techniques** since they're perform splits directly on the feature values. For examplification purposes, lets consider XGBoost, a boosting technique built with Decision Trees.","394016d5":"## Introduction\n\nWhenever developing Machine Learning (ML) models, we need to first perform some pre-processing over the data to allow for better performances. Why is that useful? The vast majority of real datasets are contaminated with biases, mislabeling, data conversion problems, and so on. Another widespread issue with data sources is the heterogeneity of range and type on the data. For example, a real estate dataset may have information on the price of the property (a float value ranging from 120k up to 10kk), along with the number of rooms (an integer variable, usually smaller than 15).\n\nIn such scenarios, it might be crucial to set every feature within the same value range, so none of them assume prominence solely based on their absolute values. In this work, I'll talk a bit more about the importance of **Data Scaling** processes in ML pipelines, when they're expendable, as well as some alternatives.\n\n\n### Any feedback is very welcome. If you enjoy the content, please give your upvote.","304f53ad":"In this work, I've chosen to use a simple classification dataset which were at the same type simple, and fully capable of representing the most important aspects of the data scaling. To this purpose, we'll use [Belo Horizonte's real estate dataset](https:\/\/www.kaggle.com\/guilherme26\/house-pricing-in-belo-horizonte). As we see above, despite their small values, the features are **considerably different in their distributions**.","24525580":"Then, with the scaled data...","44ba4302":"Now lets build a ML model to be trained on both cases. Initially, with the raw data...","3221cffc":"There is no missing values on the data.\n\n---\n\n## Why is Data Scaling Useful?\n\nWhenever trying to solve problems, we need to, before anything, identify the nature of the problem. For example, is it a machine learning problem? If so, is it a classification, a regression, or an optimization problem? These questions must be answered according strictly to the data at hand. Once we undoubtedly define the nature of our problem, we need to find a way to mathematically solve it, i.e define how close our solution is to the expected output (1), how to walk towards better solutions (2), and when to stop searching (3). Every step within this process can vary a lot depending on each other.\n\nA commonly used optimization technique for Maximum Likelihood Estimation is the Gradient Descent\/Ascent algorithm. Usually, gradient-based approaches define a mathematical formula for the solution, and try to find the fittest parameters according to an error function. If we define a simple conjecture, e.g. $f(x) = \\theta_1x_1 + \\theta_2x_2 + \\theta_0$, we should update the coefficients according to an acceleration coefficient $\\lambda$, a prediction error $\\hat{y} - y$, and the previous values for the coefficients $\\theta^{t-1}_{1}$ and $\\theta^{t-1}_{2}$. See the example below.\n\n\n$\\theta_1 = \\theta^{t-1}_{1} + \\lambda (\\hat{y} - y) x_1$ <br>\n$\\theta_2 = \\theta^{t-1}_{2} + \\lambda (\\hat{y} - y) x_2$\n\n\nSince the major deviations will initially derive from features with the biggest value ranges, the updates will be beneficial to their respective coefficients, but little effective - or even disadvantageous - to the remaining ones. However, what if every feature had the same variation pattern? In other words, every feature assumed the same importance for the optimization algorithm? **For such purposes, we should use Data Scaling techniques.**\n\nLet's See this in practice!\n\n---\n\n## Practical Implications\n\nFirst of all, we should define two versions of the same data. One scaled, and the other left unchanged.","610813ff":"As expected, XGBoost performed equally great in both scenarios.\n\n---\n\n## What is the Difference Between Data Scaling and Normalization?\n\nThe main difference between these scaling approaches lies on an assumption about the data. That is, when **normalizing**, we are assuming that the data follows a **Normal Distribution** (usually with $\\mu=0$ and $\\sigma=1$), what won't always be a real assumption.\n\nTo feel the difference in practice, lets consider defining the `df_scaled` as the normalized data, and re-train the same **Logistic Regression** model.","8c501730":"Surprisingly, for this data, the **Stardard Scaling** helped the model to perform better than **Min Max Scaling**. Why did that happened?\n\n---\n\n## A Consequence of Standard Scaling\n\nAs we know, a Gaussian Distribution is a well known function which can describe the whole data roughly with just a $\\mu \\pm 4\\sigma$ interval. See the image below.\n\n![img_normal_distribution.svg](attachment:a1939bb7-a142-4378-abd7-6e248be08faa.svg)\n\nHow does it affects our problems? The Standard Scaling gives great importance and description to data points near the mean due to the nature of the Normal Distribution. **Hence, outliers are usually not very well represented (a potentially harmful consequence when dealing with Anomaly Detection problems).**\n\n\n## Conclusions\n\nAs we saw on this notebook, Data Scaling can be a very useful technique in some cases. However, it can also be useless, or even prejudicial depending on the context. Consequently, knowing the specificities of each method before using it can spare you a considerable amount of time, and maybe help you reach better results with your machine learning models.\n\n### If you enjoy it, upvote this notebook, and follow me for more similar content."}}