{"cell_type":{"df15126d":"code","5a46c234":"code","34f9ebbd":"code","b9c2e222":"code","21ee1b3f":"code","892f1a0c":"code","6c49015d":"code","d7eb7fe3":"code","75d437d6":"code","2a53486b":"code","6e19c896":"code","cb0103b6":"code","c2ee92f1":"code","1d9f1023":"code","59aefad6":"code","07420254":"code","2b810d81":"code","6d6c7910":"code","2779f824":"code","b521b35b":"code","a7c3d37a":"code","b4eaf6fd":"code","1ebf5e91":"code","0c322e0f":"code","5d657f74":"code","225138d2":"code","2801eb6b":"code","bbde68c3":"code","5f596857":"code","2def6b2b":"code","719911e9":"code","0fc01cfd":"code","4516cc1b":"code","d4ed5499":"code","46c8f144":"code","8085e5b3":"code","fab9a599":"code","412dc16b":"code","c7faf180":"code","05f05eb4":"code","1056e011":"code","c5ff72dc":"markdown","d0cb2be3":"markdown","faa71f0e":"markdown","0ad9712d":"markdown","81084e1b":"markdown","95a391f9":"markdown","7bdb66b6":"markdown","65f7749d":"markdown","9c4a81f3":"markdown","2424c764":"markdown","13875163":"markdown","c28367b4":"markdown","952ca255":"markdown","ac1f5399":"markdown","c13d3151":"markdown","ee64c207":"markdown","742ac036":"markdown","d6ca21c6":"markdown","5cd31e19":"markdown","1db2fb03":"markdown","f56b2cb9":"markdown","0891379a":"markdown","744cc1a1":"markdown","470ea102":"markdown","d2154318":"markdown","9d8e27ed":"markdown","5b840373":"markdown","b7bfee61":"markdown","15eeb0f7":"markdown"},"source":{"df15126d":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tqdm\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom keras import Input, Model, Sequential\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.utils import plot_model\n\n%matplotlib inline","5a46c234":"# Dict of labels\ncategories = {\n                'non-vehicles': 0,\n                'vehicles': 1 \n             }","34f9ebbd":"def get_paths_and_labels(images_folder, limit=7000):\n\n    # Store paths to images\n    image_path = []\n    labels = []\n    for dirname, _, filenames in os.walk(images_folder):\n        dir_class = dirname.split('\/')[-1]\n        for filename in filenames[:limit]:\n            image_path.append(os.path.join(dirname, filename))\n            labels.append(categories[dir_class])\n            \n            \n    return image_path, labels","b9c2e222":"images_folder = os.path.join('\/', 'kaggle', 'input', 'vehicle-detection-image-set', 'data')\nimage_path, labels = get_paths_and_labels(images_folder)\nprint(len(image_path))\nprint(len(labels))","21ee1b3f":"images_df = pd.DataFrame({\"image\": image_path, \"class\":labels})\nimages_df.head()","892f1a0c":"images_df = images_df.sample(frac=1,random_state=1).reset_index(drop=True)\nimages_df.head()","6c49015d":"images_df['class'].value_counts()","d7eb7fe3":"import gc \ndef load_images(images_path, img_size = (128,128), scale=True, pred_set=False):\n\n    # Load images and associated labels\n    images = []\n    labels = []\n    \n    for path in tqdm.tqdm(images_path):\n\n        img = cv2.imread(path)    \n        # img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)    \n        img = cv2.resize(img, img_size) # Resize the images\n        img = np.array(img)\n\n        images.append(img)\n        labels.append(categories[path.split('\/')[-2]]) # last folder before the image name is the category\n\n\n    images = np.array(images)  \n    images = images.astype(np.int64)\n    \n    if scale:\n        images = images\/255 # scale\n\n    gc.collect() # Clear some memory for Kaggle \n        \n    return images, np.asarray(labels)","75d437d6":"images, labels = load_images(images_df['image'], img_size = (112,112), scale=False) # scaling not needed with the efficient_net network we'll be using","2a53486b":"labels[:50]","6e19c896":"images.shape","cb0103b6":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)","c2ee92f1":"# Before encoding\nlabels","1d9f1023":"labels[0]","59aefad6":"labels = to_categorical(labels)\nprint(labels.shape)","07420254":"type(labels)","2b810d81":"# After encoding\nlabels[:10]","6d6c7910":"limit_train = 10000\nlimit_valid = 3900\nX_train = images[:limit_train] # Take the first x images in train set ...\ny_train = labels[:limit_train]\n\nX_valid = images[limit_train:limit_train+limit_valid] # ... and the next y images for validation ...\ny_valid = labels[limit_train:limit_train+limit_valid]\n\nX_test = images[limit_train+limit_valid:] # ... and the remaining for test.\ny_test = labels[limit_train+limit_valid:]\n\n\nprint(\"*-*-*-*-*-*\")\nprint(\"Train\")\nprint(X_train.shape)\nprint(y_train.shape)\nprint(\"*-*-*-*-*-*\")\nprint(\"Valid\")\nprint(X_valid.shape)\nprint(y_valid.shape)\nprint(\"*-*-*-*-*-*\")\nprint(\"Test\")\nprint(X_test.shape)\nprint(y_test.shape)","2779f824":"pd.DataFrame(y_valid).value_counts()","b521b35b":"pd.DataFrame(y_train).value_counts()","a7c3d37a":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(len(X_train),36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(X_train[image_ind]), cmap=plt.cm.binary)\n    label = list(categories.keys())[list(categories.values()).index(np.argmax(y_train[image_ind]))]\n    plt.title(label)\n    ","b4eaf6fd":"len(categories)","1ebf5e91":"from tensorflow.keras.applications import EfficientNetB0\nfrom keras.applications.xception import Xception\n\n\n\ndef build_model(input_shape = (112,112,3)):\n    \n    base_model = EfficientNetB0(weights = \"imagenet\",include_top = False,input_shape=input_shape)\n    \n    model = base_model.output\n    model = GlobalAveragePooling2D()(model)\n    model = Dropout(0.5)(model)\n    model = Dense(len(categories),activation='relu')(model)\n#     model = Flatten()(model)\n\n    model = Model(inputs = base_model.input, outputs = model)\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","0c322e0f":"tf.keras.utils.plot_model(model, show_shapes=True)","5d657f74":"checkpoint_filepath = '\/kaggle\/working\/checkpoint.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    save_freq=200)","225138d2":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=10, \n    min_delta=0.001, \n    mode='max',\n    restore_best_weights=True\n)","2801eb6b":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreducelr = ReduceLROnPlateau(monitor = \"val_accuracy\",factor = 0.3, patience = 3,\n                            min_delta = 0.001,mode = 'auto',verbose=1)","bbde68c3":"model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\",metrics = ['accuracy'])","5f596857":"history = model.fit(X_train, y_train, \n                    batch_size = 64, #128 \n                    epochs = 1000, \n                    verbose = 1, \n                    validation_data = (X_valid, y_valid),\n                    callbacks=[\n                        model_checkpoint_callback, \n                        early_stopping,\n                        reducelr\n                    ])","2def6b2b":"plt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","719911e9":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","0fc01cfd":"def predict_class(img):\n    # Resize\n    img = img.reshape(1,112,112,3)\n    # Predict\n    predictions = model.predict(img)\n    true_prediction = [tf.argmax(pred) for pred in predictions]\n    true_prediction = np.array(true_prediction)\n    \n    # Return label corresponding to predicted index\n    return list(categories.keys())[list(categories.values()).index(true_prediction)]\n    ","4516cc1b":"predict_class(X_test[0])","d4ed5499":"y_pred = model.predict(X_test)\ny_pred.shape","46c8f144":"y_test.shape","8085e5b3":"# Revert the to_categorical function result for the confusion matrix\ny_test_ = [np.argmax(y, axis=None, out=None) for y in y_test]\ny_pred_ = [np.argmax(y, axis=None, out=None) for y in y_pred]","fab9a599":"y_test_[:10]","412dc16b":"from sklearn.metrics import classification_report\ntarget_names = ['non-vehicles', 'vehicles']\nprint(classification_report(y_test_, y_pred_, target_names=target_names))","c7faf180":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(y_test_,y_pred_)\nplot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names =['Non Vehicles', 'Vehicles'],\n                     show_normed = True)","05f05eb4":"plt.figure(figsize=(10,10))\nrandom_inds = np.random.choice(X_test.shape[0],36)\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image_ind = random_inds[i]\n    plt.imshow(np.squeeze(X_test[image_ind]), cmap=plt.cm.binary)\n    \n    # Predict and get label\n    label = predict_class(X_test[image_ind])\n    plt.xlabel(label)","1056e011":"model.save(\"vehicle_or_not_model.h5\")","c5ff72dc":"# Vehicle or not?","d0cb2be3":"### Plot the model","faa71f0e":"### Suffle dataset","0ad9712d":"### Plot the performances","81084e1b":"Plot some of the images from the train set","95a391f9":"# Load images","7bdb66b6":"### Confusion matrix","65f7749d":"Okay, our data is not imbalanced. We can continue.","9c4a81f3":"Quick look at target values:","2424c764":"### Plot some images and predicted labels","13875163":"Credit photo: @samuele_piccarini - Unsplash","c28367b4":"We should avoid imbalanced dataset","952ca255":"### Save the model","ac1f5399":"* https:\/\/keras.io\/api\/applications\/efficientnet\/\n\n* https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n\n* https:\/\/towardsdatascience.com\/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29","c13d3151":"# Predict on new data","ee64c207":"# Model building","742ac036":"The technique used here is called transfer learning. We use a pre-trained model to extract features from images.","d6ca21c6":"### Convert to dataframe","5cd31e19":"## Split dataset","1db2fb03":"#### References","f56b2cb9":"# Convert labels to categorical","0891379a":"![samuele-errico-piccarini-FMbWFDiVRPs-unsplash (1).jpg](attachment:d057d017-fdc5-407f-8917-9728ceec02eb.jpg)","744cc1a1":"## Define some callbacks","470ea102":"Currently we have the images of each category in a row. If we keep it that way, we might have unbalanced sets for training, validation and testing.  ","d2154318":"### Compile model and train","9d8e27ed":"# Visualizing","5b840373":"### Check labels distribution","b7bfee61":"What do our data look like?","15eeb0f7":"## Get path to images"}}