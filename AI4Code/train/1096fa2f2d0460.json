{"cell_type":{"2e5e2fe9":"code","7c84b4d0":"code","46f16aaf":"code","ceab18da":"code","1f2ba0f5":"code","76645803":"code","4b4fe3f1":"code","4d52b7ca":"code","24c1d74f":"code","76aa71e3":"code","f9e7f612":"code","c3b3c73d":"code","d15f5341":"code","34b186aa":"code","b77846cb":"code","a7aa6c73":"code","162f24ba":"code","6e15e126":"code","dae86a2f":"code","8be4668f":"code","b79ab8c0":"code","b2f195db":"code","8c28bfe8":"code","86d64181":"code","7280871e":"code","6e7b6048":"code","e81c7ea5":"code","4498afc1":"code","87cde69f":"code","3593cb60":"code","cfe25c4a":"code","5784f39e":"code","dbe14bd5":"code","b4e989f5":"code","710a3f16":"code","225999d2":"code","b14dcc9b":"code","86a6feee":"code","217a1d3f":"code","8513a0a7":"markdown","9d4bcd34":"markdown","72ff8d98":"markdown","df8fbd6e":"markdown","e25c1968":"markdown","262a3423":"markdown","e273401d":"markdown","2899fecc":"markdown","277a4758":"markdown","a446041d":"markdown","fd7cb8c9":"markdown","97851e7d":"markdown","20f7f1e9":"markdown","d229c5a8":"markdown","66e1c546":"markdown","10665eba":"markdown","b1ef5358":"markdown","6e6fdf0e":"markdown","4ad86b16":"markdown","6e6f5dae":"markdown","5d122bc4":"markdown","b84c9491":"markdown","11053c79":"markdown","cea19703":"markdown","7fd94abb":"markdown","843b3431":"markdown","a5031da5":"markdown","06730081":"markdown","4a3be44a":"markdown","ef9876e9":"markdown","115c1aaf":"markdown","8cc3bd7d":"markdown","36b7c1c8":"markdown","2e6b9e65":"markdown","80e36454":"markdown","2d395c55":"markdown","05f973e7":"markdown"},"source":{"2e5e2fe9":"import numpy as np \nimport pandas as pd\nimport json\nimport bq_helper\nfrom pandas.io.json import json_normalize\nimport seaborn as sns \nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport numpy as np\ninit_notebook_mode(connected=True)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy.stats as st\nimport missingno as msno","7c84b4d0":"df_train = pd.read_csv(\"..\/input\/train.csv\",sep=',')\ndf_test = pd.read_csv(\"..\/input\/test.csv\",sep=',')","46f16aaf":"df_train.head()","ceab18da":"df_test.head()","1f2ba0f5":"json_columns = ['device', 'geoNetwork','totals', 'trafficSource']\ndef load_dataframe(filename):\n    path = \"..\/input\/\" + filename\n    df = pd.read_csv(path, converters={column: json.loads for column in json_columns}, \n                     dtype={'fullVisitorId': 'str'})\n   \n    for column in json_columns:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","76645803":"train = load_dataframe(\"train.csv\")\ntrain.head()","4b4fe3f1":"test = load_dataframe(\"test.csv\")\ntest.head()","4d52b7ca":"print(train.info(),test.info())","24c1d74f":"print(train.shape,test.shape)","76aa71e3":"numeric_features_train = train.select_dtypes(include=[np.number])\nnumeric_features_train.columns","f9e7f612":"numeric_features_test = test.select_dtypes(include=[np.number])\n\nnumeric_features_test.columns","c3b3c73d":"categorical_features_train = train.select_dtypes(include=[np.object])\ncategorical_features_train.columns","d15f5341":"categorical_features_test = test.select_dtypes(include=[np.object])\ncategorical_features_test.columns","34b186aa":"print (\"Before removing constant columns - shape of train & test datasets: \", train.shape,test.shape)\ntrain = train.loc[:, (train != train.iloc[0]).any()]\ntest = test.loc[:, (test != test.iloc[0]).any()]\nprint (\"After Removing Constant Columns - shape of train & test datasets: \", train.shape,test.shape)","b77846cb":"total_test = categorical_features_train.isnull().sum().sort_values(ascending=False)\npercent = (categorical_features_train.isnull().sum()\/categorical_features_train.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total_test, percent], axis=1,join='outer', keys=['Total Missing Count', ' % of Total Observations'])\nmissing_data.index.name ='Feature'\nmissing_data.head(14)","a7aa6c73":"missing_values = categorical_features_train.isnull().sum(axis=0).reset_index()\nmissing_values.columns = ['column_name', 'missing_count']\nmissing_values = missing_values.loc[missing_values['missing_count']>0]\nmissing_values = missing_values.sort_values(by='missing_count')\nind = np.arange(missing_values.shape[0])\nwidth = 0.1\nfig, ax = plt.subplots(figsize=(12,3))\nrects = ax.barh(ind, missing_values.missing_count.values, color='b')\nax.set_yticks(ind)\nax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Categorical Observations in Train Dataset\")\nplt.show()","162f24ba":"total_test = categorical_features_test.isnull().sum().sort_values(ascending=False)\npercent = (categorical_features_test.isnull().sum()\/categorical_features_test.isnull().count()).sort_values(ascending=False)*100\nmissing_data = pd.concat([total_test, percent], axis=1,join='outer', keys=['Total Missing Count', ' % of Total Observations'])\nmissing_data.index.name ='Feature'\nmissing_data.head(12)","6e15e126":"missing_values = categorical_features_test.isnull().sum(axis=0).reset_index()\nmissing_values.columns = ['column_name', 'missing_count']\nmissing_values = missing_values.loc[missing_values['missing_count']>0]\nmissing_values = missing_values.sort_values(by='missing_count')\nind = np.arange(missing_values.shape[0])\nwidth = 0.1\nfig, ax = plt.subplots(figsize=(12,3))\nrects = ax.barh(ind, missing_values.missing_count.values, color='b')\nax.set_yticks(ind)\nax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Categorical Observations in Test Dataset\")\nplt.show()","dae86a2f":"colorscale = [[0, 'rgb(102,194,165)'], [0.0005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = train[\"geoNetwork_country\"].value_counts().index,\n        locationmode = 'country names',\n        z = train[\"geoNetwork_country\"].value_counts().values,\n        marker = dict(\n            line = dict(color = 'rgb(250,250,225)', width = 1)),\n            colorbar = dict( title = 'Customer Visits ')\n            ) \n       ]\n\nlayout = dict(\n    height=600,\n    title = 'World Wide Customer Visit Distribution',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = 'rgb(28,107,160)',\n        projection = dict(\n        type = 'orthographic',\n            rotation = dict(\n                    lon = 50,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = True,\n                gridcolor = 'rgb(12, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = True,\n                gridcolor = 'rgb(12, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)","8be4668f":"colorscale = [[0, 'rgb(102,194,165)'], [0.0005, 'rgb(102,194,165)'], \n              [0.01, 'rgb(171,221,164)'], [0.02, 'rgb(230,245,152)'], \n              [0.04, 'rgb(255,255,191)'], [0.05, 'rgb(254,224,139)'], \n              [0.10, 'rgb(253,174,97)'], [0.25, 'rgb(213,62,79)'], [1.0, 'rgb(158,1,66)']]\n\ndata = [ dict(\n        type = 'choropleth',\n        autocolorscale = False,\n        colorscale = colorscale,\n        showscale = True,\n        locations = train[\"geoNetwork_country\"].value_counts().index,\n        locationmode = 'country names',\n        z = train[\"geoNetwork_country\"].value_counts().values,\n        marker = dict(\n            line = dict(color = 'rgb(250,250,225)', width = 1)),\n            colorbar = dict( title = 'Customer Visits ')\n            ) \n       ]\n\nlayout = dict(\n    height=600,\n    title = 'World Wide Customer Visit Distribution',\n    geo = dict(\n        showframe = True,\n        showocean = True,\n        oceancolor = 'rgb(28,107,160)',\n        projection = dict(\n        type = 'conic equal area',\n            rotation = dict(\n                    lon = 50,\n                    lat = 10),\n        ),\n        lonaxis =  dict(\n                showgrid = True,\n                gridcolor = 'rgb(12, 102, 102)'\n            ),\n        lataxis = dict(\n                showgrid = True,\n                gridcolor = 'rgb(12, 102, 102)'\n                )\n            ),\n        )\nfig = dict(data=data, layout=layout)\niplot(fig)","b79ab8c0":"geo_cols = [\"geoNetwork_city\", \"geoNetwork_country\", \"geoNetwork_subContinent\", \"geoNetwork_continent\"]\ncolors = [\"#d6a5ff\", \"#fca6da\", \"#f4d39c\", \"#a9fcca\"]\ntraces = []\nfor i, col in enumerate(geo_cols):\n    t = train[col].value_counts()\n    traces.append(go.Bar(marker=dict(color=colors[i]),orientation=\"h\", y = t.index[:15], x = t.values[:15]))\n\nfig = tools.make_subplots(rows=2, cols=2, \n                          subplot_titles=[\"Visits: City\", \"Visits: Country\",\"Visits: Sub Continent\",\"Visits: Continent\"]\n                          , print_grid=False)\nfig.append_trace(traces[0], 1, 1)\nfig.append_trace(traces[1], 1, 2)\nfig.append_trace(traces[2], 2, 1)\nfig.append_trace(traces[3], 2, 2)\n\nfig['layout'].update(height=600,width=1000, showlegend=False)\niplot(fig)\n\ntrain[\"totals_transactionRevenue\"] = train[\"totals_transactionRevenue\"].astype('float')\n\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=[\"Mean Revenue by City\", \"Mean Revenue by Country\",\"Mean Revenue by Sub Continent\",\"Mean Revenue by Continent\"], print_grid=False)\n\ncolors = [\"red\", \"green\", \"purple\",\"blue\"]\ntrs = []\nfor i, col in enumerate(geo_cols):\n    tmp = train.groupby(col).agg({\"totals_transactionRevenue\": \"mean\"}).reset_index().rename(columns={\"totals_transactionRevenue\" : \"Mean Revenue\"})\n    tmp = tmp.dropna()\n    tr = go.Bar(x = tmp[\"Mean Revenue\"], orientation=\"h\", marker=dict(opacity=0.5, color=colors[i]), y = tmp[col])\n    trs.append(tr)\n\nfig.append_trace(trs[0], 1, 1)\nfig.append_trace(trs[1], 1, 2)\nfig.append_trace(trs[2], 2, 1)\nfig.append_trace(trs[3], 2, 2)\n\nfig['layout'].update(height=600,width=1000, showlegend=False)\niplot(fig)","b2f195db":"t = train['channelGrouping'].value_counts()\nvalues1 = t.values \nindex1 = t.index\ndomain1 = {'x': [0.2, 0.50], 'y': [0.0, 0.33]}\nfig = {\n  \"data\": [\n    {\n      \"values\": values1,\n      \"labels\": index1,\n      \"domain\": {\"x\": [0, .48]},\n    \"marker\" : dict(colors=[\"#f77b9c\" ,'#ab97db',  '#b0b1b2']),\n      \"name\": \"Channel Grouping\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .7,\n      \"type\": \"pie\"\n    }\n   ],\n  \"layout\": {\"title\":\"Channel Grouping\",\n      \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\": \"Channel Grouping\",\n                \"x\": 0.11,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\niplot(fig)","8c28bfe8":"fig = tools.make_subplots(rows=1, cols=2,subplot_titles=[\"Traffic Source Campaign (not-set removed)\", \"Traffic Source Medium\"], print_grid=False)\n \ncolors = [\"#d6a5ff\", \"#fca6da\", \"#f4d39c\", \"#a9fcca\"]\nt1 = train[\"trafficSource_campaign\"].value_counts()\nt2 = train[\"trafficSource_medium\"].value_counts()\n\ntr1 = go.Bar(x = t1.index, y = t1.values, marker=dict(color=colors[1]))\ntr2 = go.Bar(x = t2.index, y = t2.values, marker=dict(color=colors[2]))\n\nfig.append_trace(tr1, 1, 1)\nfig.append_trace(tr2, 1, 2)\n\n\nfig['layout'].update(height=400, margin=dict(b=100), showlegend=False)\niplot(fig)","86d64181":"def add_date_features(df):\n    df['date'] = df['date'].astype(str)\n    df[\"date\"] = df[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    df[\"month\"]   = df['date'].dt.month\n    df[\"day\"]     = df['date'].dt.day\n    df[\"weekday\"] = df['date'].dt.weekday\n    return df ","7280871e":"train = add_date_features(train)","6e7b6048":"# Visualization for Visits by date\ntmp = train['date'].value_counts().to_frame().reset_index().sort_values('index')\ntmp = tmp.rename(columns = {\"index\" : \"dateX\", \"date\" : \"visits\"})\n\ntr = go.Scatter(mode=\"lines\", x = tmp[\"dateX\"].astype(str), y = tmp[\"visits\"])\nlayout = go.Layout(title=\"Visits by Date\", height=400)\nfig = go.Figure(data = [tr], layout = layout)\niplot(fig)\n# Visualization for Visits by monthly revenue\ntmp = train.groupby(\"date\").agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp = tmp.rename(columns = {\"date\" : \"dateX\", \"totals_transactionRevenue\" : \"mean_revenue\"})\ntr = go.Scatter(mode=\"lines\", x = tmp[\"dateX\"].astype(str), y = tmp[\"mean_revenue\"])\nlayout = go.Layout(title=\"Monthly Revenue by Date\", height=400)\nfig = go.Figure(data = [tr], layout = layout)\niplot(fig)","e81c7ea5":"fig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"Visits by Month\", \"Visits by Month Day\", \"Visits by Week Day\"], print_grid=False)\ntrs = []\nfor i,col in enumerate([\"month\", \"day\", \"weekday\"]):\n    t = train[col].value_counts()\n    tr = go.Bar(x = t.index, marker=dict(color=colors[i]), y = t.values)\n    trs.append(tr)\n\nfig.append_trace(trs[0], 1, 1)\nfig.append_trace(trs[1], 1, 2)\nfig.append_trace(trs[2], 1, 3)\nfig['layout'].update(height=400, showlegend=False)\niplot(fig)","4498afc1":"tmp1 = train.groupby('month').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp2 = train.groupby('day').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\ntmp3 = train.groupby('weekday').agg({\"totals_transactionRevenue\" : \"mean\"}).reset_index()\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=[\"MeanRevenue by Month\", \"MeanRevenue by MonthDay\", \"MeanRevenue by WeekDay\"], print_grid=False)\ntr1 = go.Bar(x = tmp1.month, marker=dict(color=\"yellow\", opacity=0.5), y = tmp1.totals_transactionRevenue)\ntr2 = go.Bar(x = tmp2.day, marker=dict(color=\"blue\", opacity=0.5), y = tmp2.totals_transactionRevenue)\ntr3 = go.Bar(x = tmp3.weekday, marker=dict(color=\"violet\", opacity=0.5), y = tmp3.totals_transactionRevenue)\n\nfig.append_trace(tr1, 1, 1)\nfig.append_trace(tr2, 1, 2)\nfig.append_trace(tr3, 1, 3)\nfig['layout'].update(height=400, showlegend=False)\niplot(fig)","87cde69f":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nagg_dict = {}\nfor col in [\"totals_bounces\", \"totals_hits\", \"totals_newVisits\", \"totals_pageviews\", \"totals_transactionRevenue\"]:\n    train[col] = train[col].astype('float')\n    agg_dict[col] = \"sum\"\ntmp = train.groupby(\"fullVisitorId\").agg(agg_dict).reset_index()\ntmp.head()","3593cb60":"non_zero = tmp[tmp[\"totals_transactionRevenue\"] > 0][\"totals_transactionRevenue\"]\nprint (\"There are \" + str(len(non_zero)) + \" visitors in the train dataset having non zero total transaction revenue\")\n\nplt.figure(figsize=(10,6))\nsns.distplot(non_zero)\nplt.title(\"Distribution of Non-Zero Total Transactions\");\nplt.xlabel(\"Total Transactions\");","cfe25c4a":"plt.figure(figsize=(12,6))\nsns.distplot(np.log1p(non_zero))\nplt.title(\"Natural Log Distribution of Non Zero Total Transactions\");\nplt.xlabel(\"Natural Log - Total Transactions\");","5784f39e":"## find constant columns\nconstant_columns = []\nfor col in train.columns:\n    if len(train[col].value_counts()) == 1:\n        constant_columns.append(col)\n\n## non relevant columns\nnon_relevant = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\"]","dbe14bd5":"test = add_date_features(test)","b4e989f5":"from sklearn.preprocessing import LabelEncoder\n\ncategorical_columns = [c for c in train.columns if not c.startswith(\"total\")]\ncategorical_columns = [c for c in categorical_columns if c not in constant_columns + non_relevant]\nfor c in categorical_columns:\n\n    le = LabelEncoder()\n    train_vals = list(train[c].values.astype(str))\n    test_vals = list(test[c].values.astype(str))\n    \n    le.fit(train_vals + test_vals)\n    \n    train[c] = le.transform(train_vals)\n    test[c] = le.transform(test_vals)","710a3f16":"def normalize_numerical_columns(df, isTrain = True):\n    df[\"totals_hits\"] = df[\"totals_hits\"].astype(float)\n    df[\"totals_hits\"] = (df[\"totals_hits\"] - min(df[\"totals_hits\"])) \/ (max(df[\"totals_hits\"]) - min(df[\"totals_hits\"]))\n\n    df[\"totals_pageviews\"] = df[\"totals_pageviews\"].astype(float)\n    df[\"totals_pageviews\"] = (df[\"totals_pageviews\"] - min(df[\"totals_pageviews\"])) \/ (max(df[\"totals_pageviews\"]) - min(df[\"totals_pageviews\"]))\n    \n    if isTrain:\n        df[\"totals_transactionRevenue\"] = df[\"totals_transactionRevenue\"].fillna(0.0)\n    return df ","225999d2":"train = normalize_numerical_columns(train)\ntest = normalize_numerical_columns(test, isTrain = False)","b14dcc9b":"from sklearn.model_selection import train_test_split\nfeatures = [c for c in train.columns if c not in constant_columns + non_relevant]\nfeatures.remove(\"totals_transactionRevenue\")\ntrain[\"totals_transactionRevenue\"] = np.log1p(train[\"totals_transactionRevenue\"].astype(float))\ntrain_x, valid_x, train_y, valid_y = train_test_split(train[features], train[\"totals_transactionRevenue\"], test_size=0.25, random_state=20)","86a6feee":"import lightgbm as lgb \n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n              \"num_leaves\" : 50, \"learning_rate\" : 0.02, \n              \"bagging_fraction\" : 0.75, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 9}\n    \nlgb_train = lgb.Dataset(train_x, label=train_y)\nlgb_val = lgb.Dataset(valid_x, label=valid_y)\nmodel = lgb.train(lgb_params, lgb_train, 700, valid_sets=[lgb_val], early_stopping_rounds=150, verbose_eval=20)","217a1d3f":"preds = model.predict(test[features], num_iteration=model.best_iteration)\ntest[\"PredictedLogRevenue\"] = np.expm1(preds)\nsubmission = test.groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"sum\"}).reset_index()\nsubmission[\"PredictedLogRevenue\"] = np.log1p(submission[\"PredictedLogRevenue\"])\nsubmission[\"PredictedLogRevenue\"] =  submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsubmission.to_csv(\"baseline.csv\", index=False)\nsubmission.head()","8513a0a7":"Here are some  comprehensive listing of the ** world projections ** which we can visualise .\n- equirectangular\n- mercator\n- orthographic\n- natural earth\n- kavrayskiy7\n- miller\n- robinson\n- eckert4\n- azimuthal equal area\n- azimuthal equidistant\n- conic equal area\n- conic conformal\n- conic equidistant\n- gnomonic\n- stereographic\n- mollweide\n- hammer\n- transverse mercator\n\nTo see the different visualisation effects using the above parameter replace the value of type = 'conic equal area' in the layout geo section .","9d4bcd34":"**Create LGBM model and train it**","72ff8d98":"Now let us apply natural log transformation on the transactions and visualise it.","df8fbd6e":"### Exploratory Data Analysis\n\nNow let us run the above function for both train and test datasets and view the first few rows to understand the dataset in detail and perform detailed exploratory data analysis.","e25c1968":"Now let us pass the train dataset to the above function to extract day,month and weekday column","262a3423":"**Categorical Feature Handling**","e273401d":"Let us look at what all are the **numerical variables** for both train and test sets\n- Train Set:","2899fecc":"## Baseline Model\n\n### PreProcessing\n\nDuring preprocessing step, let us identify which columns need to be removed considering the following factors.\n\n- Columns with constant values\n- Id columns and other non relevant columns","277a4758":"Let us now visualise by city, country ,sub continent and continent","a446041d":"### Exploratory Data Analysis\n\nNow that we mined the train and test data sets its time to do some exploratory data analysis. ","fd7cb8c9":"Let us first explore how many features in the datasets are JSON fields in both train and test datasets.\n\n**Train set:**","97851e7d":"**Total Revenue Distribution**\n\nLet us look at the distribution of total revenue without normalisation ","20f7f1e9":"Let us now explore the **missing values** in both train and test sets.\n","d229c5a8":"It is observed that there are some columns that contains \"not available in demo dataset\" as constant values predominently.So it is not going to be effective if we use these columns in our model prediction.So we can safely delete these features from both train and test datasets as below.","66e1c546":"**Let us visualise the missing categorical features for train set:**","10665eba":"#### Missing values for all categorical features in Bar chart Representation\n\n- **Train set**","b1ef5358":"### Visitor Profile\n\nLets create the visitor profile by aggregating the rows for every customer.\n","6e6fdf0e":"Lets now load test dataset and apply the date function defined above so that it can be used to make predictions","4ad86b16":"It is observed that there are four JSON features in the train & test datasets namely 'device', 'geoNetwork', 'totals', 'trafficSource.\nThese features need to be flattened out .For this  I will be using a function written by  julian in his kernel https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook.\n\nAlso, lets view the top rows of the dataset.","6e6f5dae":"# Google Analytics Customer Revenue Prediction - Simple Exploration + LGBM Model LB 1.291\n\n## Problem Statement\n\nThe 80\/20 rule has proven true for many businesses\u2013only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.\n\n### GStore\n\nRStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.\n\nIn this kaggle competition,we are challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. \n\n### File Descriptions\n- train.csv - the training set - contains the same data as the BigQuery rstudio_train_set.\n- test.csv - the test set - contains the same data as the BigQuery rstudio_test_set.\n- sampleSubmission.csv - a sample submission file in the correct format. Contains all fullVisitorIds in test.csv.\n\n### Data Fields\n- fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n- channelGrouping - The channel via which the user came to the Store.\n- date - The date on which the user visited the Store.\n- device - The specifications for the device used to access the Store.\n- geoNetwork - This section contains information about the geography of the user.\n- sessionId - A unique identifier for this visit to the store.\n- socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n- totals - This section contains aggregate values across the session.\n- trafficSource - This section contains information about the Traffic Source from which the session originated.\n- visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, we should use a combination of fullVisitorId and visitId.\n- visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n- visitStartTime - The timestamp (expressed as POSIX time).\n\n","5d122bc4":"**Let us visualise the missing categorical features test:**","b84c9491":"**Create a train and validation sets **","11053c79":"**Visits by Month, Month Day & Week Day**","cea19703":"Let us look at what all are the **categorical variables** for both train and test sets\n- Train Set ","7fd94abb":"Now let us visualise Visits by date & Monthly revenue by date using plotly ","843b3431":"**Channel Grouping**","a5031da5":"**Mean Revenue by Month, Month Day & Week Day**","06730081":"**Generate Predictions & Submission**","4a3be44a":"### Data Mining\n\nThe data is shared in big query and csv format. The csv files contains some filed with json objects. \nLet us first explore what features of the datasets are json fields and then use a function done by Julian in his kernel to convert them and explore the revised dataset.","ef9876e9":"- Test Set:\n","115c1aaf":"**Numerical Feature Handling**\n\nTo handle numerical features first let us create a function called \"normalize_numerical_columns\" that normalises the values ","8cc3bd7d":"Now let apply the above function on both train and test datasets.","36b7c1c8":"- Test Set:","2e6b9e65":"**Traffic Attributes**","80e36454":"### Univariate Analysis:\n\nLets perform  univariate analysis on some of the variables in the dataset and plot their distributions to unfold patterns or insights about the data\n\n**Geo Network Attributes:**\n\nLet us see what all geonetwork attributes are \n\n- geoNetwork_city\n- geoNetwork_continent\n- geoNetwork_country\n- geoNetwork_metro\n- geoNetwork_networkDomain\n- geoNetwork_region\n- geoNetwork_subContinent\n\nSo among the above geoNetwork attributes let us consider 'geoNetwork_country' attribute and visualise customer revenue","2d395c55":" **Visits by Date, Month and Day**\n \n Let us first create a function to create new features by month,day & weekday","05f973e7":"\n\n# If you liked this kernel greatly appreciate to UPVOTE"}}