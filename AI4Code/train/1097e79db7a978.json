{"cell_type":{"9cd58af1":"code","7641101a":"code","1526ebd9":"code","7b845764":"code","451c0d83":"code","7e919039":"code","3b9d6cf1":"code","14dfca47":"code","db852815":"code","82deacf3":"code","c66f608f":"code","5038f714":"code","0de69b85":"code","3dea4608":"code","e08214bd":"code","fc78d7d9":"code","2e194b7b":"code","a7848432":"code","f5253d70":"code","bc4a21d8":"code","df80a5a6":"code","5124ae08":"code","e3f91712":"code","7bb3d877":"code","fa558b07":"code","0da3d137":"code","0ed78b25":"code","5f84e2a4":"code","3671dcaa":"code","c89a03e9":"code","7040431a":"code","feb57825":"code","4a5ffeff":"code","18d81205":"code","6af0c537":"code","480d2abf":"code","a42589a6":"code","564cd738":"code","a778b7d8":"code","90c7b2ff":"code","57a81f6b":"code","6d09bdfa":"code","5f9cef35":"code","154b9786":"code","2268d6a2":"code","a71ebb6a":"code","dec3e6bb":"code","ae71318f":"code","9deebb51":"code","de4444de":"code","890955ed":"code","fe6fa852":"code","644a7f23":"code","510d9bf8":"markdown","ea9ea555":"markdown","84111d06":"markdown","0df83b1d":"markdown","4e56a87a":"markdown","aa11fe1b":"markdown","51c0ec2b":"markdown","e9a29fcb":"markdown","6bdb51be":"markdown","603d0ad5":"markdown","f40bdc37":"markdown","da735374":"markdown","d1c59468":"markdown","dbcd9274":"markdown","5d65b9e8":"markdown","8bda7ee9":"markdown","1b4940bb":"markdown","f6a5adf9":"markdown","97b4bab1":"markdown","f7aca990":"markdown","33476890":"markdown","af0e6a36":"markdown","d08667a3":"markdown","698fa3f1":"markdown","44b0a319":"markdown","f826f42a":"markdown","b9076458":"markdown","11ae999c":"markdown","dd89ef59":"markdown","dbd597e5":"markdown","93beeec5":"markdown","5bd3b4dd":"markdown","751b5033":"markdown","77fbaced":"markdown","c87d035b":"markdown","e8905bb4":"markdown","880137e6":"markdown","c81feff0":"markdown","6e4bbd9b":"markdown","dcd73a27":"markdown","d1a9e0ef":"markdown","bc1924d1":"markdown","6ee704d6":"markdown","9dd95d71":"markdown"},"source":{"9cd58af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7641101a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom pandas.plotting import parallel_coordinates\n%matplotlib inline","1526ebd9":"# Train Data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.columns","7b845764":"# Test Data\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.columns","451c0d83":"testId = test_data[\"PassengerId\"]","7e919039":"# First few row values\ntrain_data.head()","3b9d6cf1":"train_data.dtypes","14dfca47":"train_data.drop(['PassengerId','Ticket'], axis=1, inplace = True)","db852815":"train_data.Name.head() #train_data['Name'].head()","82deacf3":"# Getting title from Name\ntrain_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train_data[\"Name\"]]\ntrain_data[\"Title\"] = pd.Series(train_title)\ntrain_data[\"Title\"].head()","c66f608f":"# Getting title from Name\ntest_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test_data[\"Name\"]]\ntest_data[\"Title\"] = pd.Series(test_title)\ntest_data[\"Title\"].head()","5038f714":"plt.figure(figsize = (10, 5))\ng = sns.countplot(x=\"Title\",data=train_data)\ng = plt.setp(g.get_xticklabels(), rotation=45)","0de69b85":"# Convert to categorical values Title \ntrain_data[\"Title\"] = train_data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_data[\"Title\"] = train_data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain_data[\"Title\"] = train_data[\"Title\"].astype(int)  # used to change dtype of title count","3dea4608":"# Convert to categorical values Title \ntest_data[\"Title\"] = test_data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_data[\"Title\"] = test_data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest_data[\"Title\"] = test_data[\"Title\"].astype(int)  # used to change dtype of title count","e08214bd":"plt.figure(figsize = (10, 5))\ng = sns.countplot(x=\"Title\",data=train_data)\ng = plt.setp(g.get_xticklabels(), rotation=45)","fc78d7d9":"# Create a family size descriptor from SibSp and Parch\ntrain_data[\"Family_size\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"Family_size\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","2e194b7b":"train_data.isnull().sum()","a7848432":"#Fill Embarked nan values of dataset set with 'C' most frequent value\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"C\")\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].fillna(\"C\")\n\n#complete missing fare with median\ntrain_data['Fare'].fillna(train_data['Fare'].median(), inplace = True)\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace = True)\n\n## Assigning all the null values as \"N\"\ntrain_data.Cabin.fillna(\"N\", inplace=True)\ntest_data.Cabin.fillna(\"N\", inplace=True)","f5253d70":"# group by Sex, Pclass, and Title \ngrouped = train_data.groupby(['Sex','Pclass', 'Title'])  \n# view the median Age by the grouped features \ngrouped.Age.median()\n# apply the grouped median value on the Age NaN\ntrain_data.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))","bc4a21d8":"# group by Sex, Pclass, and Title \ngrouped = test_data.groupby(['Sex','Pclass', 'Title'])  \n# view the median Age by the grouped features \ngrouped.Age.median()\n# apply the grouped median value on the Age NaN\ntest_data.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))","df80a5a6":"train_data.isnull().sum()","5124ae08":"train_data['survived_dead'] = train_data['Survived'].apply(lambda x : 'Survived' if x == 1 else 'Dead')","e3f91712":"sns.clustermap(data = train_data.corr().abs(),annot=True, fmt = \".2f\", cmap = 'Reds')","7bb3d877":"plt.figure(figsize = (10, 5))\nsns.countplot('survived_dead', data = train_data)","fa558b07":"plt.figure(figsize = (10, 5))\nsns.countplot( train_data['Sex'],data = train_data, hue = 'survived_dead', palette='coolwarm')","0da3d137":"plt.figure(figsize = (10, 5))\nsns.countplot( train_data['Pclass'],data = train_data, hue = 'survived_dead')","0ed78b25":"plt.figure(figsize = (10, 5))\nsns.barplot(x = 'Pclass', y = 'Fare', data = train_data)","5f84e2a4":"plt.figure(figsize = (10, 5))\nsns.pointplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = train_data)","3671dcaa":"plt.figure(figsize = (10, 5))\nsns.barplot(x  = 'Embarked', y = 'Fare', data = train_data)","c89a03e9":"g = sns.FacetGrid(train_data, hue='Survived')\ng.map(sns.kdeplot, \"Age\",shade=True)","7040431a":"sns.catplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\",\n            col=\"Pclass\", kind = 'bar',data=train_data, palette = \"rainbow\")","feb57825":"sns.catplot(x='SibSp', y='Survived',hue = 'Sex',data=train_data, kind='bar')","4a5ffeff":"\nsns.catplot(x='Parch', y='Survived',hue = 'Sex',data=train_data, kind='point')","18d81205":"g= sns.FacetGrid(data = train_data, row = 'Sex', col = 'Pclass', hue = 'survived_dead')\ng.map(sns.kdeplot, 'Age', alpha = .75, shade = True)\nplt.legend()","6af0c537":"categoricals = train_data.select_dtypes(exclude=[np.number])\ncategoricals.describe()","480d2abf":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nlbl = LabelEncoder() \nlbl.fit(list(train_data['Embarked'].values)) \ntrain_data['Embarked'] = lbl.transform(list(train_data['Embarked'].values))\nlbl.fit(list(test_data['Embarked'].values)) \ntest_data['Embarked'] = lbl.transform(list(test_data['Embarked'].values))","a42589a6":"def encode(x): return 1 if x == 'female' else 0\ntrain_data['enc_sex'] = train_data.Sex.apply(encode)\ntest_data['enc_sex'] = test_data.Sex.apply(encode)","564cd738":"train_data[\"has_cabin\"] = [0 if i == 'N'else 1 for i in train_data.Cabin]\ntest_data[\"has_cabin\"] = [0 if i == 'N'else 1 for i in test_data.Cabin]","a778b7d8":"from collections import Counter\n# Outlier detection \n\ndef detect_outliers(train_data, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(train_data[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(train_data[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = train_data[(train_data[col] < Q1 - outlier_step) | (train_data[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train_data, 2, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])","90c7b2ff":"train_data.loc[Outliers_to_drop] # Show the outliers rows","57a81f6b":"# Drop outliers\ntrain_data = train_data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","6d09bdfa":"data = train_data.select_dtypes(include=[np.number]).interpolate().dropna()","5f9cef35":"# Featuring the X-train, y_train\ny_train = train_data[\"Survived\"]\n\nX_train = data.drop(labels = [\"Survived\"],axis = 1)","154b9786":"test_data = test_data.select_dtypes(include=[np.number]).interpolate().dropna()\ntest_data = test_data[X_train.columns]","2268d6a2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n\ntest_data = sc.transform(test_data)","a71ebb6a":"# Cross validate model with Kfold stratified cross val\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nkfold = StratifiedKFold(n_splits=10)","dec3e6bb":"#ExtraTrees \nfrom sklearn.ensemble import ExtraTreesClassifier\nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\":  [n for n in range(9, 14)],  \n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [n for n in range(4, 11)],\n              \"min_samples_leaf\": [n for n in range(2, 5)],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[n for n in range(10, 60, 10)],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","ae71318f":"# RFC Parameters tunning \nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\n\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\":  [n for n in range(9, 14)],  \n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [n for n in range(4, 11)],\n              \"min_samples_leaf\": [n for n in range(2, 5)],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[n for n in range(10, 60, 10)],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","9deebb51":"# Adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[30],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\n\ngsadaDTC.best_score_","de4444de":"### SVC classifier\nfrom sklearn.svm import SVC\n\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","890955ed":"# Gradient boosting tunning\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [n for n in range(10, 60, 10)],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth':  [n for n in range(9, 14)],  \n              'min_samples_leaf': [n for n in range(2, 5)],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","fe6fa852":"from sklearn.ensemble import VotingClassifier\n\nvotingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),('svm',SVMC_best),\n('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, y_train)","644a7f23":"test_Survived = pd.Series(votingC.predict(test_data), name=\"Survived\")\n\nSubmission = pd.concat([testId,test_Survived], axis=1)\nSubmission.to_csv(\"submission.csv\",index=False)","510d9bf8":"### Detecting & Counting Outliers","ea9ea555":"Count Plot on Survived & Dead by PClass","84111d06":"## Final Result:","0df83b1d":"## Feature Engineering: Family Size","4e56a87a":"### Checking the Data Type of Each column in Train","aa11fe1b":"### Bar Plot on PClass by Fare","51c0ec2b":"Same apply on Test data","e9a29fcb":"### Support Vector Classifier","6bdb51be":"Countplot for above","603d0ad5":"## Reading the dataset of train & test","f40bdc37":"Displaying the Outliers row","da735374":"As you can see above, all missing values are filled and specified.","d1c59468":"**Dealing with Missing Data**\n\n**Missing data**: In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. The goal of cleaning operations is to prevent problems caused by missing data that can arise when training a model.\n\n*Check wether data have null values or not*","dbcd9274":"## Working & Anlysis with Training Data","5d65b9e8":"### Now, Check for the passengers by Survived & Dead.","8bda7ee9":"### Feature Scaling","1b4940bb":"### Convert to the Categorical Values:","f6a5adf9":"And There is a lot more Visualization for analysis on Passengers Choice & Usability","97b4bab1":"### Pre-Processing","f7aca990":"### RandomForestClassifier","33476890":"### Count Plot according to Survived & Dead","af0e6a36":"### GredientBoostingClassifier","d08667a3":"### Encoding With User Defined Function","698fa3f1":"Same apply on the Test Data","44b0a319":"### Dropping outliers for more accurate solution","f826f42a":"Rechecking the missing values","b9076458":"## Importing the necessary libraries","11ae999c":"Categorical Values with Description","dd89ef59":"### Feel free to give feedback. Thanks in advance \ud83d\ude03\ud83d\ude03\ud83d\ude03","dbd597e5":"Clean Data by dropping columns which we are not using for visualization","93beeec5":"## Feature Engineering: Name\/Title","5bd3b4dd":"Same apply on the Test data","751b5033":"***Fare - Passenger Fare Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)***","77fbaced":"## Time for preprocessing the Data, Classify the Data & resultened for Submission","c87d035b":"### Cluster Map Visual","e8905bb4":"### VotingClassifier","880137e6":"### Point Plot on PClass by Sex & Survived","c81feff0":"Grouping the multiple columns for advance featuring","6e4bbd9b":"## Display the countplot to see which title used most","dcd73a27":"### Count Plot on Survived & Dead by Sex","d1a9e0ef":"### DecisionTreeClassifier","bc1924d1":"Filling the missing values by specific Chars","6ee704d6":"### ExtraTreeClassifier","9dd95d71":"## It's time of more visualization:"}}