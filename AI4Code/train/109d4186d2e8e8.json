{"cell_type":{"2fb5344b":"code","86a8a3c5":"code","1352d992":"code","2338455b":"code","dfed3b5f":"code","b9fe92c7":"code","203c7b44":"code","74980b07":"code","e97878ea":"code","6acc1e4e":"markdown","59cf8477":"markdown","713e5a11":"markdown","49a78bb9":"markdown","c39f9669":"markdown","e4082049":"markdown","c62a9b19":"markdown","13ecc30c":"markdown","7f935c7a":"markdown","fca3d9e6":"markdown"},"source":{"2fb5344b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86a8a3c5":"data = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\n\nanormality = data[data[\"class\"] == \"Abnormal\"]\n\nx = np.array(anormality.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(anormality.loc[:,'sacral_slope']).reshape(-1,1)\n\ndata.head()","1352d992":"data.info()","2338455b":"data.describe()","dfed3b5f":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","b9fe92c7":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","203c7b44":"plt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","74980b07":"from sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()\n\nlinear_reg.fit(x,y)\n\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n\nprediction = linear_reg.predict(x)\n\nprint('R^2 score: ',linear_reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(x, prediction, color='black', linewidth=3)\nplt.scatter(x,y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","e97878ea":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 3) #degree increases reliability\n\nx_poly = polynomial_regression.fit_transform(x) \n\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_poly,y)\n\n\nprediction_poly = linear_regression2.predict(x_poly)\nplt.scatter(x,y)\nplt.plot(x,prediction_poly,color=\"green\",label = \"poly\")\nplt.legend()\nplt.show()\n\n","6acc1e4e":"**Polynomial Regression**\n* To make our regression polynomial we demand help from PolynomialFeatures in sklearn.preprocessing.\n* In this method, our most crucial parameter is degree (n). Polynomial functions are formulized as y =b0 + b1*x + b2*x^2 + .. +bn*x^n . When n increases, we will get more complicated function. To be more obvious, it has both advantages and disadvantages. It does not mean that more degree, more reliable.","59cf8477":"Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )\/(y_actual - y_mean)^2","713e5a11":"Describe function helps us to make our data meaningful. Without plotting, we can understand the data thanks to those significant numbers.","49a78bb9":"red stars show \"Abnormal\" patients and greens show \"Normal\" ones.\npd.plotting.scatter_matrix:\n\n* green: normal and red: abnormal\n* c: color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type","c39f9669":"If we take a look to our output, we can observe that we have 210 range and float64 datatyped data.","e4082049":"Countplot function design a bar chart for illustrating \"class\" which is included in seaborn library.\nvalue_counts, show the numeric value of \"class\" data.","c62a9b19":"Reading data from target file and naming as data.\nNaming the data which are called \"Abnormal\" in anormality and, x and y stand for our features. ","13ecc30c":"**REGRESSION**\n* Supervised learning\n* We will use linear regression and polynomial regression.\n","7f935c7a":"As you can see, our 3rd degree polynom is not appropriately fitted. That means we cannot find a correlation between x_poly and prediction_poly in polynomial regression.","fca3d9e6":"**Linear Regression**\nAfter visualizing our data in a plot, we can move on to make linear regression."}}