{"cell_type":{"039ec942":"code","13437159":"code","0a4cbde7":"code","11b4ad78":"code","d13e7d36":"code","256449a2":"code","6b72b694":"code","b35836af":"code","964fe17f":"code","113ae904":"code","85da4764":"code","48c00df5":"code","8988783d":"code","6f5866da":"code","4d544599":"code","46628bf0":"code","ab58a523":"code","094f5e15":"code","a7fa5cd7":"code","a680117a":"code","dc10edc2":"code","b7cbc674":"code","a1a04430":"code","e19e8e2b":"code","eedd6b04":"code","39b71e39":"code","837bc39b":"code","e6515e16":"code","2f73c70c":"code","981249a1":"code","963cba09":"code","bae98d81":"code","497ce0a5":"code","175fd8b3":"code","cb40b9d3":"code","9a842b26":"code","d41911bf":"code","230f895d":"code","4be23ec4":"code","41abe799":"code","630d0d09":"code","45d071bc":"code","37e988fe":"code","bc135b9f":"code","9d718fef":"code","634bcdac":"code","11610fb4":"code","eaeb730b":"code","d741443e":"code","bd9a7a29":"code","88da41ee":"code","41e4ebff":"code","d2958900":"code","9f9e8495":"code","5af2ded7":"code","d509f4c6":"code","cd9dab80":"code","20ce6abe":"code","4ed8a69c":"code","3e756ffe":"markdown","d7f95aee":"markdown","714ea320":"markdown","a2909fd8":"markdown","f2adba3c":"markdown","64d03a57":"markdown","771380f6":"markdown","6c209416":"markdown","3f4d7890":"markdown","49f5701f":"markdown","1708b085":"markdown","a2134b1f":"markdown","42ac3a1d":"markdown","7f4cd1a3":"markdown","2426589d":"markdown","2578d003":"markdown","0332930c":"markdown","612dbf5b":"markdown","e8b6b432":"markdown","577f7777":"markdown"},"source":{"039ec942":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer, FunctionTransformer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, RandomizedSearchCV, StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, VarianceThreshold\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score, classification_report\nfrom sklearn.decomposition import PCA, FactorAnalysis, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\npd.set_option('display.max_columns', None)","13437159":"train = pd.read_csv(\"\/kaggle\/input\/av-janatahack-crosssell-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/av-janatahack-crosssell-prediction\/test.csv\")\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\nsample = pd.read_csv(\"\/kaggle\/input\/av-janatahack-crosssell-prediction\/sample.csv\")","0a4cbde7":"train.info()","11b4ad78":"train.head()","d13e7d36":"train.isna().sum()","256449a2":"test.isna().sum()","6b72b694":"for col in train.columns:\n    print(f\"{col} : {train[col].nunique()}\")\n    print(train[col].unique())","b35836af":"#separating continuous and categorical variables\ncat_var = [\"Gender\",\"Driving_License\",\"Previously_Insured\",\"Vehicle_Age\",\"Vehicle_Damage\"]\ncon_var = list(set(train.columns).difference(cat_var+[\"Response\"]))","964fe17f":"train.Response.value_counts(normalize=True)","113ae904":"sns.countplot(train.Response)\nplt.title(\"Class count\")\nplt.show()","85da4764":"sns.pairplot(train, hue='Response', diag_kind='hist')\nplt.show()","48c00df5":"def map_val(data):\n    data[\"Gender\"] = data[\"Gender\"].replace({\"Male\":1, \"Female\":0})\n    data[\"Vehicle_Age\"] = data[\"Vehicle_Age\"].replace({'> 2 Years':2, '1-2 Year':1, '< 1 Year':0 })\n    data[\"Vehicle_Damage\"] = data[\"Vehicle_Damage\"].replace({\"Yes\":1, \"No\":0})\n    return data\n\ntrain = map_val(train)\ntest = map_val(test)","8988783d":"fig, ax = plt.subplots(2,3 , figsize=(16,6))\nax = ax.flatten()\ni = 0\nfor col in cat_var:\n    sns.pointplot(col, 'Response', data=train, ax = ax[i])\n    i+=1\nplt.tight_layout()\nplt.show()","6f5866da":"sns.catplot('Gender', 'Response',hue='Vehicle_Age', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='point', height=3, aspect=2)\nplt.show()","4d544599":"fig, ax = plt.subplots(2,3 , figsize=(16,6))\nax = ax.flatten()\ni = 0\nfor col in con_var:\n    sns.boxplot( 'Response', col, data=train, ax = ax[i])\n    i+=1\nplt.tight_layout()\nplt.show()","46628bf0":"sns.catplot('Gender', 'Vintage',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","ab58a523":"sns.catplot('Gender', 'Age',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","094f5e15":"sns.catplot('Gender', 'Annual_Premium',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","a7fa5cd7":"plt.figure(figsize=(30,5))\nsns.heatmap(pd.crosstab([train['Previously_Insured'], train['Vehicle_Damage']], train['Region_Code'],\n                        values=train['Response'], aggfunc='mean', normalize='columns'), annot=True, cmap='inferno')\nplt.show()","a680117a":"corr = train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nplt.figure(figsize=(10,6))\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='YlGnBu', mask=mask)\nplt.title(\"Correlation Heatmap\")\nplt.show()","dc10edc2":"train.skew()","b7cbc674":"train['log_premium'] = np.log(train.Annual_Premium)\ntrain['log_age'] = np.log(train.Age)\ntest['log_premium'] = np.log(test.Annual_Premium)\ntest['log_age'] = np.log(test.Age)","a1a04430":"train.groupby(['Previously_Insured','Gender'])['log_premium'].plot(kind='kde')\nplt.show()","e19e8e2b":"train.groupby(['Previously_Insured','Gender'])['log_age'].plot(kind='kde')\nplt.show()","eedd6b04":"def feature_engineering(data, col):\n    mean_age_insured = data.groupby(['Previously_Insured','Vehicle_Damage'])[col].mean().reset_index()\n    mean_age_insured.columns = ['Previously_Insured','Vehicle_Damage','mean_'+col+'_insured']\n    mean_age_gender = data.groupby(['Previously_Insured','Gender'])[col].mean().reset_index()\n    mean_age_gender.columns = ['Previously_Insured','Gender','mean_'+col+'_gender']\n    mean_age_vehicle = data.groupby(['Previously_Insured','Vehicle_Age'])[col].mean().reset_index()\n    mean_age_vehicle.columns = ['Previously_Insured','Vehicle_Age','mean_'+col+'_vehicle']\n    data = data.merge(mean_age_insured, on=['Previously_Insured','Vehicle_Damage'], how='left')\n    data = data.merge(mean_age_gender, on=['Previously_Insured','Gender'], how='left')\n    data = data.merge(mean_age_vehicle, on=['Previously_Insured','Vehicle_Age'], how='left')\n    data[col+'_mean_insured'] = data['log_age']\/data['mean_'+col+'_insured']\n    data[col+'_mean_gender'] = data['log_age']\/data['mean_'+col+'_gender']\n    data[col+'_mean_vehicle'] = data['log_age']\/data['mean_'+col+'_vehicle']\n    data.drop(['mean_'+col+'_insured','mean_'+col+'_gender','mean_'+col+'_vehicle'], axis=1, inplace=True)\n    return data\n\ntrain = feature_engineering(train, 'log_age')\ntest = feature_engineering(test, 'log_age')\n\ntrain = feature_engineering(train, 'log_premium')\ntest = feature_engineering(test, 'log_premium')\n\ntrain = feature_engineering(train, 'Vintage')\ntest = feature_engineering(test, 'Vintage')","39b71e39":"X = train.drop([\"Response\"], axis=1)\nY = train[\"Response\"]","837bc39b":"dummy = [\"Vehicle_Age\"]\npassthru = con_var = list(set(X.columns).difference(dummy))\n\nonehot = OneHotEncoder(handle_unknown='ignore')\nlabel = OrdinalEncoder()\nscaler = StandardScaler()\n\nfeat_rf = RandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced_subsample')\nfeat_xgb = XGBClassifier(n_jobs=4, random_state=1, objective='binary:logistic')\nselector_rf = SelectFromModel(feat_xgb, threshold=0.001)\n\ntransformers_onehot = [('pass','passthrough',passthru),\n                       ('onehot', onehot, dummy) ]\nct_onehot = ColumnTransformer( transformers=transformers_onehot )\n\ntransformers_label = [('pass','passthrough',passthru),\n                      ('onehot', label, dummy) ]\nct_label = ColumnTransformer( transformers=transformers_label )\n\npipe = Pipeline([('ct', ct_onehot),\n                 ('scaler', scaler)])","e6515e16":"poly = PolynomialFeatures(degree= 2, interaction_only=True)\npca = PCA(n_components=0.99)\nkbest = SelectKBest(k=6)\n\npipe_pca = Pipeline([('ct', ct_onehot),\n                      ('poly', poly),\n                      ('scaler', scaler),\n                      ('pca',pca)])\n\npipe_kbest = Pipeline([('ct', ct_onehot),\n                       ('poly', poly),\n                       ('scaler', scaler),\n                       ('kbest',kbest)])\n\npipe_union = FeatureUnion([('pca',pipe_pca),\n                           ('kbest',pipe_kbest)])","2f73c70c":"# merging the PCA components and KBest features from the data\npipe_union.fit(X, Y)\nX_union = pipe_union.transform(X)\ntest_union = pipe_union.transform(test)\n#np.cumsum(pipe_union.transformer_list[0][1].named_steps['pca'].explained_variance_ratio_)","981249a1":"ct_onehot.fit(X)\ncategories = ct_onehot.named_transformers_['onehot'].categories_\nonehot_cols = [col+\"_\"+str(cat) for col,cats in zip(dummy, categories) for cat in cats]\nall_columns = passthru + onehot_cols\n\nX_transform = pd.DataFrame(pipe.fit_transform(X), columns = all_columns)\ntest_transform = pd.DataFrame(pipe.transform(test), columns = all_columns)\n\nselector_rf.fit(X_transform, Y)\nrf_cols = [col for col, flag in zip(X_transform.columns, selector_rf.get_support()) if flag]\nprint(rf_cols)\nX_select = pd.DataFrame(selector_rf.transform(X_transform), columns = rf_cols)\ntest_select = pd.DataFrame(selector_rf.transform(test_transform), columns = rf_cols)","963cba09":"def submission(preds, model):\n    sample[\"Response\"] = preds\n    sample.to_csv(\"model_\"+model+\".csv\", index=False)","bae98d81":"model_lr = LogisticRegression(n_jobs=4, random_state=1, class_weight='balanced')\nmodel_rfc = RandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced_subsample')\n# scale pos weight for class imbalance\nmodel_xgb = XGBClassifier(n_jobs=4, random_state=1, scale_pos_weight=7, objective='binary:logistic')\nmodel_lgbm = LGBMClassifier(n_jobs=4, random_state=1, is_unbalance=True, objective='binary')\nmodel_cat = CatBoostClassifier(random_state=1, verbose=0, scale_pos_weight=7, custom_metric=['AUC'])\n\nmodels = []\nmodels.append((\"LR\",model_lr))\nmodels.append((\"RF\",model_rfc))\nmodels.append((\"XGB\",model_xgb))\nmodels.append((\"LGBM\",model_lgbm))\nmodels.append((\"CAT\",model_cat))\n\ncv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.8)","497ce0a5":"results = []\nnames = []\nfor name, model in models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_select, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results.append(scores)\n    names.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","175fd8b3":"plt.boxplot(results)\nplt.xticks(np.arange(1,len(names)+1), names)\nplt.title(\"Model comparison\")\nplt.show()","cb40b9d3":"results_union = []\nnames = []\nfor name, model in models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_union, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results_union.append(scores)\n    names.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","9a842b26":"plt.boxplot(results_union)\nplt.xticks(np.arange(1,len(names)+1), names)\nplt.title(\"Model comparison\")\nplt.show()","d41911bf":"def eval_model(model, x, Y):\n    model.fit(x, Y)\n\n    trainpred  = model.predict(x)\n    proba = model.predict_proba(x)[:,1]\n\n    print(\"Accuracy score : %.4f\"%accuracy_score(Y, trainpred))\n    print(\"ROC AUC score : %.4f\"%roc_auc_score(Y, proba))\n    print(\"Classification report\")\n    print(classification_report(Y, trainpred))\n    \ndef metrics_score(model, X, Y):\n    pred = model.predict_proba(X)[:,1]\n    print(\"ROC AUC score : %.4f\"%roc_auc_score(Y, pred))","230f895d":"model_xgb = XGBClassifier(n_jobs=4, random_state=1, scale_pos_weight=7, objective='binary:logistic')\nmodel_lgbm = LGBMClassifier(n_jobs=4, random_state=1, is_unbalance=True, objective='binary')\nmodel_cat = CatBoostClassifier(random_state=1, verbose=0, scale_pos_weight=7, custom_metric=['AUC'])","4be23ec4":"model_xgb.fit(X_select, Y)\nmodel_lgbm.fit(X_select, Y)\nmodel_cat.fit(X_select, Y)\n\npred_xgb = model_xgb.predict_proba(test_select)[:,1]\npred_lgbm = model_lgbm.predict_proba(test_select)[:,1]\npred_cat = model_cat.predict_proba(test_select)[:,1]\n\nsubmission(pred_xgb, 'xgb')\nsubmission(pred_lgbm, 'lgbm')\nsubmission(pred_cat, 'cat')\n\nprediction = np.mean((pred_xgb, pred_lgbm, pred_cat), axis=0)\nsubmission(prediction, 'all')\n\nmetrics_score(model_xgb, X_select, Y)\nmetrics_score(model_lgbm, X_select, Y)\nmetrics_score(model_cat, X_select, Y)","41abe799":"cv = StratifiedShuffleSplit(n_splits=10, random_state=1, train_size=0.7)\npredictions_lgbm = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_lgbm.fit(xtrain, ytrain)\n    trainpred = model_lgbm.predict_proba(xtrain)[:,1]\n    testpred = model_lgbm.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_lgbm.predict_proba(test_select)[:,1]\n    predictions_lgbm.append(prediction)","630d0d09":"submission(np.mean(predictions_lgbm, axis=0), 'lgbm_stack')","45d071bc":"# run this again\n#cv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.9)\ncv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\npredictions_xgb = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_xgb.fit(xtrain, ytrain)\n    trainpred = model_xgb.predict_proba(xtrain)[:,1]\n    testpred = model_xgb.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_xgb.predict_proba(test_select)[:,1]\n    predictions_xgb.append(prediction)","37e988fe":"submission(np.mean(predictions_xgb, axis=0), 'xgb_stack')","bc135b9f":"cv = StratifiedShuffleSplit(n_splits=10, random_state=1, train_size=0.7)\npredictions_cat = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_cat.fit(xtrain, ytrain)\n    trainpred = model_cat.predict_proba(xtrain)[:,1]\n    testpred = model_cat.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_cat.predict_proba(test_select)[:,1]\n    predictions_cat.append(prediction)","9d718fef":"submission(np.mean(predictions_cat, axis=0), 'cat_stack')","634bcdac":"def plot_nn(history, metric):\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,5))\n    ax1.plot(history.history['loss'], color='r', label='Train loss')\n    ax1.plot(history.history['val_loss'], color='g', label='Validation loss')\n    ax1.legend()\n\n    ax2.plot(history.history[metric], color='r', label='Train '+metric)\n    ax2.plot(history.history['val_'+metric], color='g', label='Validation '+metric)\n    ax2.legend()\n    \n    plt.show()","11610fb4":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier","eaeb730b":"inputs = X_select.shape[1]\n\nes = EarlyStopping(monitor='val_loss', min_delta=0.01, patience = 50, mode='auto', baseline=0.85, restore_best_weights=True)\n\noptimizer = Adam(learning_rate=0.01)","d741443e":"model = Sequential()\nmodel.add( Dense( 64, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dense( 128, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dense( 256, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dropout(0.01))\nmodel.add( Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer = optimizer, loss='binary_crossentropy', metrics = ['AUC'])","bd9a7a29":"history = model.fit(X_select, Y, batch_size=128, epochs = 20, validation_split=0.3, verbose=0)","88da41ee":"plot_nn(history, 'auc')","41e4ebff":"pred_nn = model.predict_proba(test_select)\nsubmission(pred_nn, 'nn')","d2958900":"pred_stack = np.mean((pred_xgb, pred_lgbm, pred_cat, pred_nn[:,0]), axis=0)\nsubmission(pred_stack, 'stack')","9f9e8495":"from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier,EasyEnsembleClassifier\nfrom imblearn.metrics import classification_report_imbalanced","5af2ded7":"model_bbag = BalancedBaggingClassifier(n_jobs=4, random_state=1, base_estimator=model_xgb)\nmodel_brf = BalancedRandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced')\nmodel_easy = EasyEnsembleClassifier(n_jobs=4, random_state=1, base_estimator=model_xgb)\n\nimb_models = []\nimb_models.append(('Bag', model_bbag))\nimb_models.append(('BagRF', model_brf))\nimb_models.append(('Easy', model_easy))\n\ncv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.8)","d509f4c6":"results_imb = []\nnames_imb = []\nfor name, model in imb_models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_select, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results_imb.append(scores)\n    names_imb.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","cd9dab80":"plt.boxplot(results_imb)\nplt.xticks(np.arange(1,len(names_imb)+1), names_imb)\nplt.title(\"Model comparison Imblearn\")\nplt.show()","20ce6abe":"model_bbag.fit(X_select, Y)\nmodel_brf.fit(X_select, Y)\nmodel_easy.fit(X_select, Y)\n\nmetrics_score(model_bbag, X_select, Y)\nmetrics_score(model_brf, X_select, Y)\nmetrics_score(model_easy, X_select, Y)\n\npred_bbag = model_bbag.predict_proba(test_select)[:,1]\npred_brf = model_brf.predict_proba(test_select)[:,1]\npred_easy = model_easy.predict_proba(test_select)[:,1]","4ed8a69c":"submission(pred_bbag, 'imb_bbag')\nsubmission(pred_brf, 'imb_brf')\nsubmission(pred_easy, 'imb_easy')\n\npred_imb_stack = np.mean((pred_bbag, pred_brf, pred_easy), axis=0)\nsubmission(pred_imb_stack, 'imb_stack')","3e756ffe":"## Using ANN","d7f95aee":"### Customers who were not previously insured and their vehicle has been damaged have shown much better response, as expected","714ea320":"- No missing values in the data. Will confirm the same using unique values","a2909fd8":"- Males or customers with license or not previously insured or the vehicle was previously damaged have better response rate\n- With increasing vehicle age response improved","f2adba3c":"## Train data head","64d03a57":"## Training models for submission","771380f6":"### PCA + KBest pipeline output","6c209416":"## Preparing the data for training","3f4d7890":"### We can easily identify the regions where the response rate is high compared to others","49f5701f":"## Correlation Heatmap","1708b085":"## Current Age\/ Vintage\/ Annual Premium distributions are not helping very much so we will try mean transformation","a2134b1f":"## If you find my Kernel useful, please do upvote. Any suggestions are welcome.","42ac3a1d":"## Importing the libraries","7f4cd1a3":"### we can see that the data is imbalanced classification, hence we will not use accuracy as scoring metric. Instead we will use f1-score or more preferrably roc-auc","2426589d":"# Establishing Baseline accuracy for different models\nIt's good to establish baseline as we can check if model has improved with feature engineering or not","2578d003":"### Onehot and Feature selection Pipeline","0332930c":"# Testing models with PCA + KBest features","612dbf5b":"## Using Imblearn","e8b6b432":"### Around 12.26 % of customer have given a positive response","577f7777":"## Missing values check"}}