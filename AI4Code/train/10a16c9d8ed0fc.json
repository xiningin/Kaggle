{"cell_type":{"0c3374d7":"code","9b13c2e3":"code","a16ac594":"code","408c2efe":"code","0c2b10c2":"code","c3ea798d":"code","4ddc9338":"code","ac32853b":"code","5682b721":"code","4aa7771a":"code","c7ad4f37":"code","bde2c239":"code","a5617582":"code","f54ffb1a":"code","2ded5f54":"code","6704ac77":"code","a4f95488":"code","503dd3d0":"code","1ba08d07":"code","7d09344a":"code","a17b0d09":"code","be898b60":"code","61a2d08f":"code","e57a244b":"markdown","c2b65c11":"markdown","29b992e5":"markdown","0d664bbb":"markdown","5324698a":"markdown","7b1174bc":"markdown","cab2d4e4":"markdown","5ed836ba":"markdown","55d9d456":"markdown","e7499b2b":"markdown","33c5ef52":"markdown","0db3bfa0":"markdown","0f5220ec":"markdown","a0b8f4bb":"markdown","cb2cf855":"markdown","4dd57614":"markdown","6c7ee0c1":"markdown","f5ab9ec5":"markdown","a5267204":"markdown","8ce11eb3":"markdown","3e04cd77":"markdown","a54c7fec":"markdown"},"source":{"0c3374d7":"import numpy as np\nimport pandas as pd\nimport re, nltk, gensim\nimport requests\nimport json\nfrom sklearn.externals import joblib\n\n# Sklearn\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","9b13c2e3":"filename = \"..\/input\/brazilian-music-samba-lyrics\/samba_dataset.csv\"\ndf = pd.read_csv(filename, sep=\"|\")\ndf.head(3)","a16ac594":"# cada letra de samba \u00e9 um documento\ndata = [lyrics for lyrics in df.letra] \nprint(\"Temos %d documentos.\" %len(data))","408c2efe":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","0c2b10c2":"def removeStops(texts, stopwords):\n    texts_out = []\n    for sent in texts:\n        texts_out.append(\" \".join([token for token in sent if token not in stopwords]))\n    return texts_out\n\n\nstopwords = nltk.corpus.stopwords.words('portuguese')\nstopwords += [\"nao\", \"so\", \"pra\", \"pro\", \"pras\", \"pros\"]\n# Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_without_stops = removeStops(data_words, stopwords)\n\n# sem stopwords\nprint(data_without_stops[:2])","c3ea798d":"vectorizer = CountVectorizer(analyzer='word',       \n                             min_df=10,                        # minimum reqd occurences of a word \n                             # stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n                             # max_features=50000,             # max number of uniq words\n                            )\n\n# data_vectorized = vectorizer.fit_transform(data_lemmatized)\ndata_vectorized = vectorizer.fit_transform(data_without_stops)","4ddc9338":"# Materialize the sparse data\ndata_dense = data_vectorized.todense()\n\n# Compute Sparsicity = Percentage of Non-Zero cells\nprint(\"Sparsicity: \", round(((data_dense > 0).sum()\/data_dense.size)*100, 2), \"%\")","ac32853b":"lda_model = LatentDirichletAllocation(n_components=5,              \n                                      max_iter=10,               \n                                      learning_method='online',   \n                                      random_state=100,          \n                                      batch_size=128,            \n                                      evaluate_every = -1,       \n                                      n_jobs = -1             \n                                     )\nlda_output = lda_model.fit_transform(data_vectorized)\n\nprint(lda_model)","5682b721":"# Probabilidade logaritmica: quanto maior melhor\nprint(\"probabilidade logaritmica: \", round(lda_model.score(data_vectorized), 2))\n\n# Perplexidade: menor melhor.  exp(-1. * log-Probabilidade logaritmica por palavra)\nprint(\"Perplexidade: \", round(lda_model.perplexity(data_vectorized), 2))\n\nprint(\"Par\u00e2metros:\")\npprint(lda_model.get_params())","4aa7771a":"# Define Search Param\nsearch_params = {'n_components': [5, 10, 15], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation()\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n# Do the Grid Search\nmodel.fit(data_vectorized)","c7ad4f37":"# Melhor modelo\nbest_lda_model = model.best_estimator_\n\n# Hiperpar\u00e2metros do modelo\nprint(\"Melhores par\u00e2metros: \", model.best_params_)\n\n# probabilidade logar\u00edtmica\nprint(\"Melhor score de probabilidade logar\u00edtmica: \", model.best_score_)\n\n# Perplexidade\nprint(\"Perplexidade do modelo: \", best_lda_model.perplexity(data_vectorized))","bde2c239":"results = pd.DataFrame(model.cv_results_)\n\ncurrent_palette = sns.color_palette(\"Set2\", 3)\n\nplt.figure(figsize=(12,8))\n\nsns.lineplot(data=results,\n             x='param_n_components',\n             y='mean_test_score',\n             hue='param_learning_decay',\n             palette=current_palette,\n             marker='o'\n            )\n\nplt.show()","a5617582":"# Create Document - Topic Matrix\nlda_output = best_lda_model.transform(data_vectorized)\n\n# column names\ntopicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n\n# index names\ndocnames = [\"Doc\" + str(i) for i in range(len(data))]\n\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\n# Styling\ndef color_green(val):\n    color = 'green' if val > .1 else 'black'\n    return 'color: {col}'.format(col=color)\n\ndef make_bold(val):\n    weight = 700 if val > .1 else 400\n    return 'font-weight: {weight}'.format(weight=weight)\n\n# Apply Style\ndf_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)\ndf_document_topics_first10 = df_document_topic[:10].style.applymap(color_green).applymap(make_bold)\ndf_document_topics_first10","f54ffb1a":"df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\ndf_topic_distribution.columns = ['Topic Num', 'Num Documents']\ndf_topic_distribution","2ded5f54":"vocab = vectorizer.get_feature_names()\n\n# data_vectorized\ntopic_words = {}\nn_top_words = 5\n\nfor topic, comp in enumerate(best_lda_model.components_):\n    # for the n-dimensional array \"arr\":\n    # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n    # which contains the indices that would sort arr in a descending fashion\n    # for the ith element in ranked_array, ranked_array[i] represents the index of the\n    # element in arr that should be at the ith index in ranked_array\n    # ex. arr = [3,7,1,0,3,6]\n    # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n    # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n    # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n    word_idx = np.argsort(comp)[::-1][:n_top_words]\n\n    # store the words most relevant to the topic\n    topic_words[topic] = [vocab[i] for i in word_idx]\n    \nfor topic, words in topic_words.items():\n    print('Topic: %d' % topic)\n    print('  %s' % ', '.join(words))","6704ac77":"# tranformando objeto style em um dataframe pandas\ndf2 = pd.DataFrame(data=df_document_topics.data, columns=df_document_topics.columns)\ndf2.head()","a4f95488":"# associando os interpretes aos t\u00f3picos \n# dos sambas que eles cantam\ndf2[\"artista\"] = df[\"artista\"].tolist()\ndf2.head()","503dd3d0":"# Artistas que mais aparecem dentro de cada t\u00f3pico\ndf2.groupby([\"dominant_topic\"])['artista'].agg(pd.Series.mode).to_frame()","1ba08d07":"# os 5 artistas que mais aparecem no\n# t\u00f3pico 0 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==0].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","7d09344a":"# os 5 artistas que mais aparecem no\n# t\u00f3pico 1 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==1].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","a17b0d09":"# os 5 artistas que mais aparecem no\n# t\u00f3pico 2 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==2].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","be898b60":"# os 5 artistas que mais aparecem no\n# t\u00f3pico 3 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==3].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","61a2d08f":"# os 5 artistas que mais aparecem no\n# t\u00f3pico 4 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==4].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","e57a244b":"O gr\u00e1fico interativo que ser\u00e1 gerado ter\u00e1 a seguinte \"cara\":\n\n![](https:\/\/media.giphy.com\/media\/dt0myZkpmNjrW4IaXi\/giphy.gif)","c2b65c11":"# Verificando qu\u00e3o Esparsa \u00e9 a Matriz\n\nBasta verificar a porcentagem de pontos diferentes de zero na matriz documento-palavra.\n\nComo a maioria das c\u00e9lulas nessa matriz ser\u00e1 zero, estou interessado em saber qual porcentagem de c\u00e9lulas cont\u00e9m valores diferentes de zero.","29b992e5":"# Avaliando a performance do modelo com a perplexidade e probabilidade logar\u00edtmica\n\nUm modelo com maior probabilidade logar\u00edtmica e menor perplexidade (exp (-1. * Probabilidade logar\u00edtmica por palavra)) \u00e9 considerado bom. Vamos verificar o nosso modelo.","0d664bbb":"![](https:\/\/lh3.googleusercontent.com\/proxy\/z9ilC4bvRfBqWs83SCvP_xo8V7eGgsilTqugH0axpWybkxLXuZk87CApVuuOrbbeXRa10Ungs5oHfhnNeda5uyMdSjrwcg8uSJXFW1kZvLJmXERERxuZIVovtwlYB_xhYhgkewkGqBH0V0POEio)","5324698a":"# Importando os Dados","7b1174bc":"### Tokeniza\u00e7\u00e3o dos docs","cab2d4e4":"# Treinando o Modelo LDA com Sklearn\n\nTudo est\u00e1 pronto para criar um modelo de Aloca\u00e7\u00e3o Dirichlet Latente (LDA). Vamos inicializar um e chamar fit_transform () para criar o modelo LDA.\n\nNeste exemplo, eu defini os n_topics como 5. Mais tarde, encontraremos o n\u00famero ideal usando grid search.","5ed836ba":"# Tratando o texto","55d9d456":"# Escolhendo o \"melhor\" modelo","e7499b2b":"# Principais Artistas em cada T\u00f3pico","33c5ef52":"# Top 5 palavras por t\u00f3pico","0db3bfa0":"### Removendo Stopwords","0f5220ec":"# Comparando os scores de performance dos modelos LDA","a0b8f4bb":"A perplexidade pode n\u00e3o ser a melhor medida para avaliar modelos de t\u00f3picos, porque n\u00e3o considera o contexto e as associa\u00e7\u00f5es sem\u00e2nticas entre as palavras. Isso pode ser capturado usando a medida de coer\u00eancia de t\u00f3pico.","cb2cf855":"Para vizualizar o gr\u00e1fico, execute os comandos abaixo no seu jupyer notebook, isso ir\u00e1 abrir uma p\u00e1gina no seu localhost:\n```python\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\npyLDAvis.show(panel)```","4dd57614":"# UsandoGridSearch para encontrar melhor modelo LDA \n\nO par\u00e2metro de ajuste mais importante para modelos LDA \u00e9 ```n_components``` (n\u00famero de t\u00f3picos). Al\u00e9m disso, vamos pesquisar ```learning_decay``` (que controla a taxa de aprendizado) tamb\u00e9m.\n\nAl\u00e9m desses, outros poss\u00edveis par\u00e2metros de pesquisa podem ser ```learning_offset```  e ```max_iter```. Vale a pena experimentar se voc\u00ea tiver recursos de computa\u00e7\u00e3o suficientes.\n\nA pesquisa em grade constr\u00f3i v\u00e1rios modelos LDA para todas as combina\u00e7\u00f5es poss\u00edveis de valores de par\u00e2metros no par\u00e2metro param_grid. Portanto, <mark>esse processo pode consumir muito tempo e recursos<\/mark>.","6c7ee0c1":"# T\u00f3pico dominante em cada documento\n\nPara classificar um documento como pertencente a um t\u00f3pico espec\u00edfico, uma abordagem l\u00f3gica \u00e9 ver qual t\u00f3pico tem a maior contribui\u00e7\u00e3o para esse documento e atribu\u00ed-lo.\n\nNa tabela abaixo, destaquei em verde todos os principais t\u00f3picos de um documento e atribu\u00ed o t\u00f3pico mais dominante em sua pr\u00f3pria coluna.","f5ab9ec5":"# Visualizando nosso modelo LDA com o pyLDAvis","a5267204":"# Reconhecimentos\n\nO que fiz acima foi um estudo inspirado no artigo escrito por Selva Prabhakaran:\n\n* [LDA in Python \u2013 How to grid search best topic models?](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-python-sklearn-examples\/)","8ce11eb3":"# Aloca\u00e7\u00e3o Latente de Dirichlet (LDA)\n\n*\"No processamento de linguagem natural, a Aloca\u00e7\u00e3o Latente de Dirichlet (LDA) \u00e9 um <mark>modelo estat\u00edstico generativo que permite que conjuntos de observa\u00e7\u00f5es sejam explicados por grupos n\u00e3o observados que explicam o porqu\u00ea algumas partes dos dados s\u00e3o semelhantes<\/mark>. Por exemplo, se as observa\u00e7\u00f5es s\u00e3o palavras coletadas em documentos, ele postula que cada documento \u00e9 uma mistura de um pequeno n\u00famero de t\u00f3picos e que a cria\u00e7\u00e3o de cada palavra \u00e9 atribu\u00edvel a um dos t\u00f3picos do documento\"* - [Wikip\u00e9dia](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation)\n\n\n# LDAvis\n\nLDAvis s\u00e3o ferramentas para criar uma <mark>visualiza\u00e7\u00e3o interativa usando p\u00e1gina web<\/mark>, de um modelo de t\u00f3pico que foi ajustado a um corpus de dados de texto usando a **Aloca\u00e7\u00e3o de Dirichlet Latente (LDA)**. Dado os par\u00e2metros estimados do modelo de t\u00f3pico, ele calcula v\u00e1rias estat\u00edsticas de resumo como entrada para uma visualiza\u00e7\u00e3o interativa criada com o D3.js que \u00e9 acessada por meio de um navegador. <mark>O objetivo \u00e9 ajudar os usu\u00e1rios a interpretar os t\u00f3picos em seu modelo de t\u00f3pico LDA<\/mark>.\n\n* [LDAvis Introduction](https:\/\/cran.r-project.org\/web\/packages\/LDAvis\/vignettes\/details.pdf)","3e04cd77":"# Criando a matriz Documento-Palavra\n\nO algoritmo de modelo de t\u00f3pico LDA requer uma matriz de palavras do documento como entrada principal.\n\nVoc\u00ea pode criar um usando o CountVectorizer. No c\u00f3digo abaixo, eu configurei o CountVectorizer para considerar palavras que ocorreram pelo menos 10 vezes (min_df), remova palavras irrelevantes em ingl\u00eas, converta todas as palavras em min\u00fasculas e uma palavra pode conter n\u00fameros e alfabetos de pelo menos 3 para ser qualificado como uma palavra.","a54c7fec":"# Quantidade de Documentos em Cada T\u00f3pico"}}