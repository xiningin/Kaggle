{"cell_type":{"49a5bab3":"code","8afc6fe7":"code","a1293d0f":"code","e079b12d":"code","df072a43":"code","7917145e":"code","4239f487":"markdown","cfbece6a":"markdown","a84c8d43":"markdown","93bfb633":"markdown","ff812371":"markdown","46c9df28":"markdown","c584d404":"markdown","8d074481":"markdown","82fc5891":"markdown","29e8f840":"markdown","bc19afdd":"markdown","c2baba1c":"markdown","88023a9d":"markdown","0dfdf2cb":"markdown","10353ecf":"markdown","87f1fc71":"markdown","85e479b2":"markdown","87acd9cc":"markdown","2d3e8f3c":"markdown","d2a5bef5":"markdown","7912329b":"markdown","46a5966c":"markdown","d605436d":"markdown","60bdcec5":"markdown","c1114ca7":"markdown"},"source":{"49a5bab3":"# use this commands in 'Console' without '!' to install env\n#!cd ..\/input\/gymshops\/gym-shops\n#!pip install -e .","8afc6fe7":"import gym\nfrom gym import error, spaces, utils\nfrom gym.utils import seeding\n\nimport itertools\nimport random\nimport time\n\nclass ShopsEnv(gym.Env):\n  metadata = {'render.modes': ['human']}\n\n  def __init__(self):\n    self.state = [0, 0, 0]\n    self.next_state = [0, 0, 0]\n    self.done = False\n    self.actions = list(itertools.permutations([1, 2, 3]))\n    self.reward = 0\n    self.time_tracker = 0\n    \n    self.remembered_states = []\n    \n    t = int( time.time() * 1000.0 )\n    random.seed( ((t & 0xff000000) >> 24) +\n                 ((t & 0x00ff0000) >>  8) +\n                 ((t & 0x0000ff00) <<  8) +\n                 ((t & 0x000000ff) << 24)   )\n    \n  def step(self, action_num):\n    # check if the simulation is alredy done\n    if self.done:\n        return [self.state, self.reward, self.done, self.next_state]\n    else:\n        # select next state\n        self.state = self.next_state\n        \n        # remember state\n        self.remembered_states.append(self.state) \n    \n        # increment time tracker\n        self.time_tracker += 1\n        \n        # choose action according got action number\n        action = self.actions[action_num]\n        \n        # update state using action (add pies)\n        self.next_state = [x + y for x, y in zip(action, self.state)]\n        \n        # generate how much will be bought\n        self.next_state[0] -= (3 + random.uniform(-0.1, 0.1))\n        self.next_state[1] -= (1 + random.uniform(-0.1, 0.1))\n        self.next_state[2] -= (2 + random.uniform(-0.1, 0.1))\n        \n        # select reward for action\n        if any([x < 0 for x in self.next_state]):\n            self.reward = sum([x for x in self.next_state if x < 0])\n        else:\n            self.reward = 1\n            \n        # reset shop if it has negative state\n        if self.time_tracker >= 3:\n            remembered_state = self.remembered_states.pop(0)\n            self.next_state = [max(x - y, 0) for x, y in zip(self.next_state, remembered_state)]\n        else:\n            self.next_state = [max(x, 0) for x in self.next_state]\n        \n        \n        # check if game is done\n        self.done = self.time_tracker == 30\n\n        return [self.state, self.reward, self.done, self.next_state]\n    \n  def reset(self):\n    self.state = [0, 0, 0]\n    self.next_state = [0, 0, 0]\n    self.done = False\n    self.reward = 0\n    self.time_tracker = 0\n    \n    self.remembered_states = []\n    \n    t = int( time.time() * 1000.0 )\n    random.seed( ((t & 0xff000000) >> 24) +\n                 ((t & 0x00ff0000) >>  8) +\n                 ((t & 0x0000ff00) <<  8) +\n                 ((t & 0x000000ff) << 24)   )\n                 \n    return self.state\n    \n  def render(self, mode='human', close=False):\n    print('-'*20)\n    print('First shop')\n    print('Pies:', self.state[0])\n\n    print('Second shop')\n    print('Pies:', self.state[1])\n\n    print('Third shop')\n    print('Pies:', self.state[2])\n    print('-'*20)\n    print('')","a1293d0f":"try:\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n    import gym # for environments\n    import gym_shops # for our environment\n    from tqdm import tqdm # progress tracker\n\n    # for plots\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from IPython.display import clear_output\n    sns.set_color_codes()\n\n    # for modeling\n    from collections import deque\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.optimizers import Adam # adaptive momentum \n    import random # for stochasticity of our environment\nexcept:\n    pass","e079b12d":"class DQLAgent(): \n    \n    def __init__(self, env):\n        # parameters and hyperparameters\n        \n        # this part is for neural network or build_model()\n        self.state_size = 3 # this is for input of neural network node size\n        self.action_size = 6 # this is for out of neural network node size\n        \n        # this part is for replay()\n        self.gamma = 0.99\n        self.learning_rate = 0.01\n        \n        # this part is for adaptiveEGreedy()\n        self.epsilon = 0.99 # initial exploration rate\n        self.epsilon_decay = 0.99\n        self.epsilon_min = 0.0001\n        \n        self.memory = deque(maxlen = 5000) # a list with 5000 memory cell, if it becomes full first inputs will be deleted\n        \n        self.model = self.build_model()\n    \n    def build_model(self):\n        # neural network for deep Q learning\n        model = Sequential()\n        model.add(Dense(10, input_dim = self.state_size, activation = 'sigmoid')) # first hidden layer\n        model.add(Dense(50, activation = 'sigmoid')) # second hidden layer\n        model.add(Dense(10, activation = 'sigmoid')) # third hidden layer\n        model.add(Dense(self.action_size, activation = 'sigmoid')) # output layer\n        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n        return model\n    \n    def remember(self, state, action, reward, next_state, done):\n        # storage\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state):\n        # acting, exploit or explore\n        if random.uniform(0,1) <= self.epsilon:\n            return random.choice(range(6))\n        else:\n            act_values = self.model.predict(state)\n            return np.argmax(act_values[0])\n            \n    \n    def replay(self, batch_size):\n        # training\n        \n        if len(self.memory) < batch_size:\n            return # memory is still not full\n        \n        minibatch = random.sample(self.memory, batch_size) # take batch_size random samples from memory\n        for state, action, reward, next_state, done in minibatch:\n            if done: # if the game is over, I dont have next state, I just have reward \n                target = reward\n            else:\n                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n                # target = R(s,a) + gamma * max Q`(s`,a`)\n                # target (max Q` value) is output of Neural Network which takes s` as an input \n                # amax(): flatten the lists (make them 1 list) and take max value\n            train_target = self.model.predict(state) # s --> NN --> Q(s,a) = train_target\n            train_target[0][action] = target\n            self.model.fit(state, train_target, verbose = 0) # verbose: dont show loss and epoch\n    \n    def adaptiveEGreedy(self):\n        # decrease exploration rate\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n    ","df072a43":"try:\n    # initialize gym environment and agent\n    env = gym.make('shops-v0')\n    agent = DQLAgent(env)\n\n    # set training parameters\n    batch_size = 100\n    episodes = 1000\n\n    # start training\n    progress_bar = tqdm(range(episodes), position=0, leave=True)\n    for e in progress_bar:\n        # initialize environment\n        state = env.reset()\n        state = np.reshape(state, [1, 3])\n\n        # track current time step, taken actions and sum of rewards for an episode\n        time = 0\n        taken_actions = []\n        sum_rewards = 0\n\n\n        # process episode\n        while True:\n            # act\n            action = agent.act(state)\n\n            # remember taken action\n            taken_actions.append(action)\n\n            # step\n            next_state, reward, done, _ = env.step(action)\n            next_state = np.reshape(next_state, [1, 3])\n\n            # add got reward\n            sum_rewards += reward\n\n            # remember \/ storage\n            agent.remember(state, action, reward, next_state, done)\n\n            # update state\n            state = next_state\n\n            # replay\n            agent.replay(batch_size)\n\n            # adjust epsilon\n            agent.adaptiveEGreedy()\n\n            # increment time\n            time += 1\n\n            # show information about training state\n            progress_bar.set_postfix_str(s='mean reward: {}, time: {}, epsilon: {}'.format(round(sum_rewards\/time, 3), time, round(agent.epsilon, 3)), refresh=True)\n\n            # check if the episode is already done\n            if done:\n                # show distribution of actions during current episode\n                clear_output(wait=True)\n                sns.distplot(taken_actions, color=\"y\")\n                plt.title('Episode: ' + str(e))\n                plt.xlabel('Action number')\n                plt.ylabel('Occurrence in %')\n                plt.show()\n                break\nexcept:\n    pass","7917145e":"try:\n    import time\n    trained_model = agent # now we have trained agent\n    state = env.reset() # restart game\n    state = np.reshape(state, [1,3])\n    time_t = 0 # track time\n    MAX_EPISOD_LENGTH = 1000\n    taken_actions = []\n    mean_reward = 0\n\n    # simulate env with our trained model\n    progress_bar = tqdm(range(MAX_EPISOD_LENGTH), position=0, leave=True)\n    for time_t in progress_bar:\n        # simulate one step\n        action = trained_model.act(state)\n        next_state, reward, done, _ = env.step(action)\n        next_state = np.reshape(next_state, [1,3])\n        state = next_state\n        taken_actions.append(action)\n\n        # show result of the step\n\n        clear_output(wait=True)\n        env.render()\n        progress_bar.set_postfix_str(s='time: {}'.format(time_t), refresh=True)\n        print('Reward:', round(env.reward, 3))\n        time.sleep(0.5)\n\n        mean_reward += env.reward\n\n        if done:\n            break\n\n    # show distribution of actions during current episode\n    sns.distplot(taken_actions, color='y')\n    plt.title('Test episode - mean reward: ' + str(round(mean_reward\/(time_t+1), 3)))\n    plt.xlabel('Action number')\n    plt.ylabel('Occurrence in %')\n    plt.show()    \nexcept:\n    pass","4239f487":" The pipeline is built on two articles:\n1. [About deep Q learning](https:\/\/www.kaggle.com\/mehmetkasap\/reinforcement-learning-deep-q-learning-cartpole)\n2. [About custom gym env](https:\/\/medium.com\/@apoddar573\/making-your-own-custom-environment-in-gym-c3b65ff8cdaa)","cfbece6a":"# Formulation of the problem","a84c8d43":"So imagine the following picture","93bfb633":"# Technical aspects","ff812371":"Interesting in reinforcement learning? Do you like raspberry pies? And also with cherries? \n\nThen you went to the address. \n\nToday you will have a trip to our bakery, where the goods will be distributed between stores using AI!","46c9df28":"![v2.PNG](attachment:v2.PNG)","c584d404":"We (conditionally) do not know how much a ton of products will be bought every day in a particular store, but we set this distribution. We agree that in the first store 3 tons will be bought per day on average, in the second - 1 ton, and in the third - 2 tons. With a deviation of 0.1 units.","8d074481":"We have in our hands one bakery that produces 6 tons of raspberry pies per day.\n\nEvery day before the opening of our three outlets - stores number one, two and three - we distribute the produced 6 tons of goods across them.\n\nHowever, how exactly can this be done if we have only three cars, each of which has a capacity of 1, 2 and 3 tons, respectively? And besides, we do not know the true distribution of purchasing power in each store and our cars can do transpartation only once per day.","82fc5891":"# Defining agent","29e8f840":"# Main imports","bc19afdd":"# Introduction","c2baba1c":"# Testing agent","88023a9d":"Our state is three numbers - the remainder of the products in the shops today. And our actions are a number from 0 to 5 inclusive - the permutation index of numbers 1, 2, 3 - that is, how many tones we will take to a particular shop. Obviously, the ideal algorithm will always choose the 4th action.","0dfdf2cb":"Because IDK how to (and even is it possible to) install custom gym env on Kaggle - I will use some 'lifehacks' to produce the whole pipeline.","10353ecf":"# Conclusion","87f1fc71":"![MainEng.png](attachment:MainEng.png)","85e479b2":"# Custom environment","87acd9cc":"![v1.PNG](attachment:v1.PNG)","2d3e8f3c":"By the way, our pies are not eternal. Shelf life is three days. To simplify, we believe that the FIFO method works 100% in stores, so customers buy primarily cakes that were produced later than others, if any. If the cake was not purchased within three days, the store employees get rid of it.","d2a5bef5":"That is, the least amount of loss in the form of a tone of pies will be in the case if we will always take 3 tons to the first store, and 1 and 2 tons, respectively, to the second and third.","7912329b":"# Training agent","46a5966c":"# Disclaimer","d605436d":"I am not trying to solve the task as correctly \/ accurately \/ reasonably as possible, this is just an experiment.\nIf you know how to do it better \/ found mistakes in my understanding of the problem and so on - I will gladly listen to your thoughts. Good to all! :)","60bdcec5":"As we can see, our agent quickly have learned to optimally act in our custom env.","c1114ca7":"I'm going to create a custom environment using gym for this task and then use deep Q learning (keras implementation) to train our agent."}}