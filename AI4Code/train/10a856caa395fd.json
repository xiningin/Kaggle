{"cell_type":{"5f83c8a2":"code","a43b791d":"code","70aec138":"code","a3d998fa":"code","84bbb326":"code","c4dda15c":"code","a4f39e7c":"code","ca0804a6":"code","f47a15f6":"code","3e108145":"code","6da1f827":"code","4e3c2d7a":"code","0a4c5ebc":"code","48d011f2":"code","2354a250":"code","b563dfac":"code","4d1a585f":"code","b7f71de3":"markdown","67dadfff":"markdown","2bfa1813":"markdown","5b67956f":"markdown","d7d331d9":"markdown","6f5fe3fd":"markdown","1bf464bb":"markdown","9849b0ec":"markdown","e33724ba":"markdown","d56ccb86":"markdown","a91cdc19":"markdown","493b14b2":"markdown","02ba198f":"markdown","562860c0":"markdown","332295bc":"markdown","e339ad80":"markdown","957e1094":"markdown","02319905":"markdown","63b24be0":"markdown"},"source":{"5f83c8a2":"import nltk\nimport pandas as pd\nimport numpy as np\nimport re\n\n# If you would like to work with the raw text you can use 'moby_raw'\nwith open('..\/input\/moby.txt', 'r') as f:\n    moby_raw = f.read()\n    \n# If you would like to work with the novel in nltk.Text format you can use 'text1'\nmoby_tokens = nltk.word_tokenize(moby_raw)\ntext1 = nltk.Text(moby_tokens)","a43b791d":"def example_one():\n    \n    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n\nexample_one()","70aec138":"def example_two():\n    \n    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n\nexample_two()","a3d998fa":"from nltk.stem import WordNetLemmatizer\n\ndef example_three():\n\n    lemmatizer = WordNetLemmatizer()\n    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n\n    return len(set(lemmatized))\n\nexample_three()","84bbb326":"def answer_one():\n    \n    \n    return example_two() \/ example_one()\n\nanswer_one()","c4dda15c":"def answer_two():\n    tokens = nltk.word_tokenize(moby_raw)\n    whales = [w for w in tokens if  w =='whale' or w == 'Whale'] \n    return len(whales) \/ example_one()\n\nanswer_two()","a4f39e7c":"def answer_three():\n    tokens = nltk.word_tokenize(moby_raw)\n    dist = nltk.FreqDist(tokens)\n\n    return dist.most_common(20)\n\nanswer_three()","ca0804a6":"def answer_four():\n    tokens = nltk.word_tokenize(moby_raw)\n    dist = nltk.FreqDist(tokens)\n    vocab1 = list(dist.keys())\n    sel_list = [t for t in vocab1 if len(t)>5 and dist[t]>150]\n    return sorted(sel_list)\n\nanswer_four()","f47a15f6":"def answer_five():\n    tokens = nltk.word_tokenize(moby_raw)\n    tokens_uni = set(tokens)\n    longest_word = max(tokens_uni, key=len)\n    result = longest_word, len(longest_word)\n    \n    return result\n\nanswer_five()","3e108145":"def answer_six():\n    tokens = nltk.word_tokenize(moby_raw)\n    dist = nltk.FreqDist(tokens)\n    freq_words = [w for w in list(dist.keys()) if w.isalpha() and dist[w] > 2000]\n    \n    dict_I_want = { key: dist[key] for key in freq_words } # make new dict\n    result = sorted(dict_I_want.items(), key=lambda kv: kv[1], reverse=True)\n    # read more about *sorted* function\n\n    return result\n    \nanswer_six()","6da1f827":"def answer_seven():\n    # count tokens\n    tokens = len(nltk.word_tokenize(moby_raw))\n    # count sentences\n    sents = len(nltk.sent_tokenize(moby_raw))\n    # divide # of tokens by # of sentences\n    \n    return tokens\/sents\n\nanswer_seven()","4e3c2d7a":"def answer_eight():\n    tokens = nltk.word_tokenize(moby_raw)\n    tags = nltk.pos_tag(tokens)\n    frequencies = nltk.FreqDist([tag for (word, tag) in tags])\n    return frequencies.most_common(5)\n\nanswer_eight()","0a4c5ebc":"from nltk.corpus import words\nfrom nltk.metrics.distance import jaccard_distance, edit_distance\nfrom nltk.util import ngrams\n\ncorrect_spellings = words.words()","48d011f2":"entries=['cormulent', 'incendenece', 'validrate']","2354a250":"def jaccard(entries, gram_number):\n    \"\"\"find the closet words to each entry\n\n    Args:\n     entries: collection of words to match\n     gram_number: number of n-grams to use\n\n    Returns:\n     list: words with the closest jaccard distance to entries\n    \"\"\"\n    outcomes = []\n    for entry in entries:\n        spellings = [s for s in correct_spellings if s.startswith(entry[0])]\n        distances = ((jaccard_distance(set(ngrams(entry, gram_number)),\n                                       set(ngrams(word, gram_number))), word) for word in spellings)\n        closest = min(distances)\n        outcomes.append(closest[1])\n    return outcomes\n\njaccard(entries, 3)","b563dfac":"jaccard(entries, 4)","4d1a585f":"def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n    \"\"\"gets the nearest words based on Levenshtein distance\n\n    Args:\n     entries (list[str]): words to find closest words to\n\n    Returns:\n     list[str]: nearest words to the entries\n    \"\"\"\n    outcomes = []\n    for entry in entries:\n        distances = ((edit_distance(entry,\n                                    word), word)\n                     for word in correct_spellings)\n        closest = min(distances)\n        outcomes.append(closest[1])\n    return outcomes\n\nprint(answer_eleven())\n","b7f71de3":"---\n\n_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https:\/\/www.coursera.org\/learn\/python-text-mining\/resources\/d9pwm) course resource._\n\n---","67dadfff":"### Question 10\n\nFor this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n\n**[Jaccard distance](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index) on the 4-grams of the two words.**\n\n*This function should return a list of length three:\n`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*","2bfa1813":"### Question 8\n\nWhat are the 5 most frequent parts of speech in this text? What is their frequency?\n\n*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*","5b67956f":"# Assignment 2 - Introduction to NLTK\n\nIn part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. ","d7d331d9":"### Question 5\n\nFind the longest word in text1 and that word's length.\n\n*This function should return a tuple `(longest_word, length)`.*","6f5fe3fd":"### Question 9\n\nFor this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n\n**[Jaccard distance](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index) on the trigrams of the two words.**\n\n*This function should return a list of length three:\n`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*","1bf464bb":"### Example 3\n\nAfter lemmatizing the verbs, how many unique tokens does text1 have?\n\n*This function should return an integer.*","9849b0ec":"### Question 11\n\nFor this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n\n**[Edit distance on the two words with transpositions.](https:\/\/en.wikipedia.org\/wiki\/Damerau%E2%80%93Levenshtein_distance)**\n\n*This function should return a list of length three:\n`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*","e33724ba":"### Question 7\n\nWhat is the average number of tokens per sentence?\n\n*This function should return a float.*","d56ccb86":"It took much longer.","a91cdc19":"### Question 2\n\nWhat percentage of tokens is 'whale'or 'Whale'?\n\n*This function should return a float.*","493b14b2":"## Part 1 - Analyzing Moby Dick","02ba198f":"### Question 6\n\nWhat unique words have a frequency of more than 2000? What is their frequency?\n\n\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n\n*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*","562860c0":"### Question 1\n\nWhat is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n\n*This function should return a float.*","332295bc":"### Example 1\n\nHow many tokens (words and punctuation symbols) are in text1?\n\n*This function should return an integer.*","e339ad80":"## Part 2 - Spelling Recommender\n\nFor this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n\nFor every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n\n*Each of the three different recommenders will use a different distance measure (outlined below).\n\nEach of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`.","957e1094":"### Question 3\n\nWhat are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n\n*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*","02319905":"### Example 2\n\nHow many unique tokens (unique words and punctuation) does text1 have?\n\n*This function should return an integer.*","63b24be0":"### Question 4\n\nWhat tokens have a length of greater than 5 and frequency of more than 150?\n\n*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"}}