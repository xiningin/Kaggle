{"cell_type":{"d657011e":"code","a0d65f6e":"code","3e6a2111":"code","6bf0e64c":"code","187a234f":"code","3e59b4b4":"code","37b53942":"code","7a7f80bc":"code","63cd9adb":"code","6b9fed62":"code","067624f3":"code","7709e5af":"code","4f4fc28b":"code","bc685299":"code","bcd65914":"code","0ecff4a0":"code","36b9c46e":"code","980f1fdb":"code","7ec64d55":"code","e3a853c8":"code","c4fd0069":"code","24e229ce":"code","cd9d3f91":"code","47a0970f":"code","d6fb63ce":"code","7a68dd43":"code","0df299a1":"code","14bd9346":"code","69072a1d":"markdown","d6d1e9d5":"markdown","756145b3":"markdown","e7a069e5":"markdown","8d1f69e1":"markdown","36dbe3d4":"markdown","a1b592eb":"markdown","da10f23b":"markdown","15c2ce90":"markdown","b5043171":"markdown","5f85784e":"markdown","c369ed47":"markdown","f1f94831":"markdown","4c445406":"markdown","77c964cc":"markdown","5414ef35":"markdown","89e98f1f":"markdown","ad3c03af":"markdown"},"source":{"d657011e":"import numpy as np\nimport pandas as pd\n\n# Load train data as train_data\nin_file = '\/kaggle\/input\/titanic\/train.csv'\ntrain_data = pd.read_csv(in_file)\nprint(\"There are {} samples in train_data\".format(len(train_data)))\n\n# Load test data as test_data\nin_file = '\/kaggle\/input\/titanic\/test.csv'\ntest_data = pd.read_csv(in_file)\nprint('There are {} samples in test_data'.format(len(test_data)))\nPassengerId=test_data['PassengerId']\n\n# Set the labels of test_data as na\ntest_data['Survived'] = np.nan\n\n# Concat the features of train dataset and test dataset\nall_data = pd.concat([train_data, test_data], ignore_index=True)\n\n# Show the whole dataset informations\nall_data.info()\ndisplay(all_data.head())","a0d65f6e":"# Check how many null values in each feature.\nall_data.isnull().sum()","3e6a2111":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n%matplotlib inline","6bf0e64c":"# Show the samples with embarked null\ndisplay(all_data[all_data['Embarked'].isnull()])","187a234f":"#  Only reserve the numbers of the tickets.  &(all_data['Pclass']==1)\n#  We assume that the ticket number is probably same if it is bought from the same place\nall_data['Ticket_num'] = all_data['Ticket'].map(lambda x: re.sub(\"\\D\", \"\", x))\nall_data['Ticket_num'] = pd.to_numeric(all_data['Ticket_num'])\nall_data.head()","3e59b4b4":"all_data.isnull().sum()","37b53942":"#  Select the passangers relatives number equal to 0, and the pclass equal to 1, the ticket number between 113000 and 114000\nselect_data = all_data[(all_data['Parch']==0)&(all_data['SibSp']==0)\\\n                       &(all_data['Ticket_num']<114000)&(all_data['Ticket_num']>110000)]\nselect_data.sort_values('Ticket_num').head()","7a7f80bc":"select_data.Embarked.value_counts()","63cd9adb":"all_data['Embarked'] = all_data['Embarked'].fillna('S') ","6b9fed62":"facet = sns.FacetGrid(all_data[0:890], hue=\"Survived\",aspect=2)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, all_data.loc[0:890,'Age'].max()))\nfacet.add_legend()","067624f3":"from sklearn.ensemble import RandomForestRegressor\n\nage_df = all_data[['Age', 'Pclass','Sex','Parch','SibSp']]\nage_df=pd.get_dummies(age_df)\nknown_age = age_df[age_df.Age.notnull()].as_matrix()\nunknown_age = age_df[age_df.Age.isnull()].as_matrix()\ny = known_age[:, 0]\nX = known_age[:, 1:]\nrfr = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)\nrfr.fit(X, y)\npredictedAges = rfr.predict(unknown_age[:, 1::])\nall_data.loc[(all_data.Age.isnull()), 'Age'] = predictedAges ","7709e5af":"facet = sns.FacetGrid(all_data[0:890], hue=\"Survived\",aspect=2)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, all_data.loc[0:890,'Age'].max()))\nfacet.add_legend()","4f4fc28b":"fare=all_data.loc[(all_data['Embarked'] == \"S\") & (all_data['Pclass'] == 3), 'Fare'].median()\nall_data['Fare']=all_data['Fare'].fillna(fare)","bc685299":"# Extract the titles from name feature\n\n# Create a set for all unique titles\ntitles = set()\n\n# Extract from the name feature\nfor name in all_data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\nprint(titles)","bcd65914":"# Make a dictionary for mapping the titles into three kinds\nTitle_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Master\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\",\n    \"Dona\" : \"Royalty\"    \n}\n\ndef get_titles(data):\n    data['Title'] = data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    data['Title'] = data['Title'].map(Title_Dictionary)\n    return data\n\nget_titles(all_data)\nall_data['Title'].value_counts()  ","0ecff4a0":"# Extract from name feature\nall_data['Surname'] = all_data['Name'].map(lambda name:name.split(',')[0].strip())\n\n# Create a new feature as surname frequency\nall_data['FamilyGroup'] = all_data['Surname'].map(all_data['Surname'].value_counts()) \n\nsns.barplot(x='FamilyGroup', y='Survived', data=all_data, palette='Set3')\n\nFemale_Child_Group=all_data.loc[(all_data['FamilyGroup']>=2) & ((all_data['Age']<=16) | (all_data['Sex']=='female'))]\nFemale_Child_Group=Female_Child_Group.groupby('Surname')['Survived'].mean()\nDead_List=set(Female_Child_Group[Female_Child_Group.apply(lambda x:x==0)].index)\nprint(Dead_List)\n\nMale_Adult_Group=all_data.loc[(all_data['FamilyGroup']>=2) & (all_data['Age']>16) & (all_data['Sex']=='male')]\nMale_Adult_List=Male_Adult_Group.groupby('Surname')['Survived'].mean()\nSurvived_List=set(Male_Adult_List[Male_Adult_List.apply(lambda x:x==1)].index)\nprint(Survived_List)\n","36b9c46e":"all_data.loc[(all_data['Survived'].isnull()) & (all_data['Surname'].apply(lambda x:x in Dead_List)),\\\n             ['Sex','Age','Title']] = ['male',28.0,'Mr']\nall_data.loc[(all_data['Survived'].isnull()) & (all_data['Surname'].apply(lambda x:x in Survived_List)),\\\n             ['Sex','Age','Title']] = ['female',5.0,'Miss']","980f1fdb":"all_data['FamilySize']=all_data['SibSp']+all_data['Parch']+1\nsns.barplot(x=\"FamilySize\", y=\"Survived\", data=all_data, palette='Set3')","7ec64d55":"def Fam_label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 7)) | (s == 1):\n        return 1\n    elif (s > 7):\n        return 0\nall_data['FamilyLabel']=all_data['FamilySize'].apply(Fam_label)\nsns.barplot(x=\"FamilyLabel\", y=\"Survived\", data=all_data, palette='Set3')","e3a853c8":"all_data['Cabin'] = all_data['Cabin'].fillna('Unknown')\nall_data['Deck']=all_data['Cabin'].str.get(0)\nsns.barplot(x=\"Deck\", y=\"Survived\", data=all_data, palette='Set3')","c4fd0069":"Ticket_Count = dict(all_data['Ticket'].value_counts())\n\nall_data['TicketGroup'] = all_data['Ticket'].map(Ticket_Count)\nsns.barplot(x='TicketGroup', y='Survived', data=all_data, palette='Set3')","24e229ce":"def Ticket_Label(s):\n    if (s >= 2) & (s <= 4):\n        return 2\n    elif ((s > 4) & (s <= 8)) | (s == 1):\n        return 1\n    elif (s > 8):\n        return 0\n\nall_data['TicketGroup'] = all_data['TicketGroup'].apply(Ticket_Label)\nsns.barplot(x='TicketGroup', y='Survived', data=all_data, palette='Set3')","cd9d3f91":"all_data = all_data[['Survived','Pclass','Sex','Age','Fare','Embarked','Title','FamilyLabel','Deck','TicketGroup']]\nall_data.isnull().sum()","47a0970f":"all_data = all_data[['Survived','Pclass','Sex','Age','Fare','Embarked','Title','FamilyLabel','Deck','TicketGroup']]\nall_data = pd.get_dummies(all_data)\ntrain = all_data[all_data['Survived'].notnull()]\ntest = all_data[all_data['Survived'].isnull()].drop('Survived',axis=1)\nX = train.as_matrix()[:,1:]\ny = train.as_matrix()[:,0]","d6fb63ce":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\n\nselect = SelectKBest(k = 20)\nclf = RandomForestClassifier(random_state = 10, warm_start = True, \n                                  n_estimators = 26,\n                                  max_depth = 6, \n                                  max_features = 'sqrt')\n\npipeline = make_pipeline(select, clf)\npipeline.fit(X, y)\n\ncv_result = cross_validate(pipeline, X, y, cv= 10)\nprint(\"CV Test Score : Mean - %.7g | Std - %.7g \" % (np.mean(cv_result['test_score']), \\\n                                                     np.std(cv_result['test_score'])))\n\n","7a68dd43":"test_x = test.as_matrix()","0df299a1":"predictions = pipeline.predict(test_x)\nsubmission = pd.DataFrame({\"PassengerId\": PassengerId, \"Survived\": predictions.astype(np.int32)})\nsubmission.to_csv(\"submission_test.csv\", index=False)","14bd9346":"from sklearn.feature_selection import SelectKBest\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_validate\nimport warnings\nwarnings.filterwarnings('ignore')\n\nselect = SelectKBest(k = 20)\nclf = XGBClassifier(\n                     max_depth=3, \n                     learning_rate=0.1, \n                     n_estimators=100, \n                     silent=True, \n                     objective='binary:logistic', \n                     booster='gbtree', \n                     nthread=None, \n                     gamma=0, \n                     subsample=0.8, \n                     colsample_bytree=1, \n                     colsample_bylevel=1, \n                     reg_alpha=0, \n                     reg_lambda=1, \n                     scale_pos_weight=1.2, \n                     base_score=0.5)\n\npipeline = make_pipeline(select, clf)\npipeline.fit(X, y)\n\ncv_result = cross_validate(pipeline, X, y, cv= 10)\n\nprint(cv_result['test_score'])\nprint(\"CV Test Score : Mean - %.7g | Std - %.7g \" % (np.mean(cv_result['test_score']), \\\n                                                     np.std(cv_result['test_score'])))\n","69072a1d":"#### 4.2 XGBoost","d6d1e9d5":"#### 2.2 Age\n\nThe age feature has 263 null values. So we use random forest regression model to simulate the value. The features we use here are sex, title, pclass.\n","756145b3":"#### 2.1 Embarked\nFor the correctness, do not fill the null values only by the original feature values. We need to see other features too. So we can see about the passangers relatives number, the pclass, the fare, and the ticket.","e7a069e5":"#### 3.5 TicketGroup","8d1f69e1":"#### 3.4 Deck","36dbe3d4":"### 4.Model Prediction","a1b592eb":"#### 2.3 Fare\n\nThe fare feature has 1 null values. So we use median value to fill up.","da10f23b":"#### 3.3 FamilySize","15c2ce90":"**For the null values above, we dicide to deal as following.**\n- The age null values can be filled by the regression method.\n- The 'Embarked' features can be filled according to the fare and pclass.\n- There are many null values in 'Cabin' features. For the better predictions, it is believed to delete the feature of 'Cabin'. But here we need this feature because when the ship sinks, certain parts of the ship have different probability drown in water. So we deal to make a new feature to substitute the feature.\n- The fare can be filled up with median value or mean value. \n- The survived is the target to predict.","b5043171":"#### 4.1 Random Forest","5f85784e":"#### 3.2 Surname\n\nWe could make the exception list of male and female. Then change the features of test surname in the list into the dead features or survived features according to the list.","c369ed47":"## II Feature Analysis and Process\n\n### 1.Load data","f1f94831":"**The embarked place is obviously connected with the fare and the tickets. From the value counts, the probability of S is more possible than other two options.**","4c445406":"# Python Titanic\n## I Introduction\n\nFirst, I'm going to make one possible explanation about the ones who get 100% accuracy in the Leaderboard. They have obtained the whole dataset with train and test labels. Then they use the prediction model to fit and make an prediction. Since the rest of us only get the part of the information, there is no possibility to depict the picture in whole. \n\nNow let's go back my notebook. \n\n----------\n### 1.Features Analysis\nHere is the reference from Chris Deotte.\n\nhttps:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818 \n\nIn his simple model using R language, he scores 82% only using Names. He found that the male except boy is dead and the female except surname family dead are survived. This is a decision tree model which is simple and effective. So according to this logical method, other methods based on the trees should be tried. But before that, we will try to analyse the features and data cleaning. Then we create the feature mentioned by Chris Deotte and modify the test features accordingly.\n\n### 2.Supervised methods\nHere is the reference from Chris Deotte.\n\nhttps:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688\n\nIn his notebook, he scored 0.85167. The notebook has tried XGBoost with the previous features and other new added features. It seems that some features should be dropped since it is a kind of noise. So in my notebook, the random forest and XGBoost will be chosen. These two methods are better than other methods because they are based on the Cart tree model, which is a good chioce for this dataset.\n","77c964cc":"#### 3.1 Title","5414ef35":"### 3. Create new features\n\nWe all know the movie story of tatanic that women and children escapes first and their survival rate shall be higher than other category. It is a easy way to believe all women and children survived and all man died. But it is contracted with boys. So we gona to extract the titles as man, woman and boy.\n","89e98f1f":"### 2.Missing values process","ad3c03af":"> ### 5 Conclusion\n\nThe random forest is better than XGBoost. So we submit the random forest result."}}