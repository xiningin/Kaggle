{"cell_type":{"6415f91c":"code","2e6648e6":"code","16738d6b":"code","0a4ad92f":"code","b51161c1":"code","9dbca3d9":"code","befaecac":"code","5aff8fe9":"code","921c4a28":"code","29cd9f78":"code","24a2a142":"code","6454b1e2":"code","96cb594b":"code","d756f63c":"code","997b1aaa":"code","767f29d5":"code","5aa690d6":"code","6ecf7ff9":"code","89c9cd74":"code","b2f9a4c3":"markdown","97699410":"markdown","0fe5f335":"markdown","7679653b":"markdown","02b92a6f":"markdown","5e1cd5f0":"markdown","8ecca0f0":"markdown","5346cb82":"markdown","f03bfccf":"markdown","daf75367":"markdown","0e7b9373":"markdown","7e52ab9d":"markdown","6c786499":"markdown","8e97400e":"markdown","9e2fd704":"markdown","8f268e88":"markdown","52cf6afb":"markdown","a194a6c8":"markdown","1758fd24":"markdown","cd27a696":"markdown","a3746366":"markdown","155ddfdb":"markdown","5f9c19b2":"markdown","d689d73a":"markdown"},"source":{"6415f91c":"import optuna\nfrom sklearn.feature_selection import RFECV\nfrom lightgbm import LGBMClassifier\nimport pandas as pd\nimport joblib\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport optuna.integration.lightgbm as lgb\nfrom optuna.samplers import TPESampler\nimport matplotlib.pyplot as plt","2e6648e6":"used_data_types_dict = {\n    'timestamp': 'int64',\n    'user_id': 'int32',\n    'content_id': 'int16',\n    'answered_correctly': 'int8',\n    'prior_question_elapsed_time': 'float32',\n    'prior_question_had_explanation': 'boolean'\n}\n\ntrain_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/train.csv',\n    usecols=used_data_types_dict.keys(),\n    dtype=used_data_types_dict\n)\n\nquestions_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/questions.csv',\n    usecols=[0, 3],\n    dtype={'question_id': 'int16', 'part': 'int8'}\n)","16738d6b":"train_df = pd.merge(train_df, questions_df, left_on='content_id', right_on='question_id', how='left')\ntrain_df.drop(columns=['question_id'], inplace=True)","0a4ad92f":"train_df['prior_question_had_explanation'].fillna(bool(True), inplace=True)\ntrain_df = train_df.replace([-np.inf, np.inf], np.nan)\ntrain_df = train_df.fillna(train_df.mean())\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype(bool)","b51161c1":"labels = [bool(False), bool(True)]\nsizes = [list(train_df['prior_question_had_explanation']).count(labels[0]),\n        list(train_df['prior_question_had_explanation']).count(labels[1])]\n\nplt.pie(sizes, labels=labels, shadow=True, startangle=90, autopct='%1.1f%%')\nplt.show()","9dbca3d9":"train_df = train_df[train_df['answered_correctly'] != -1]\ntrain_questions_only_df = train_df\n\ngrouped_by_user_df = train_questions_only_df.groupby('user_id')\nuser_answers_df = grouped_by_user_df.agg({'answered_correctly': ['mean', 'count', 'skew',\n                                                                 'std', 'var', 'sem',\n                                                                               'sum']}).copy()\nuser_answers_df.columns = [\n    'mean_user_accuracy',\n    'questions_answered',\n    'questions_skew',\n    'questions_std',\n    'questions_var',\n    'questions_sem',\n    'questions_sum'\n]\nuser_answers_df.to_csv('user.csv')\n\ngrouped_by_content_df = train_questions_only_df.groupby('content_id')\ncontent_answers_df = grouped_by_content_df.agg({'answered_correctly': ['mean', 'count', 'skew',\n                                                                       'std', 'var', 'sem',\n                                                                                     'sum']}).copy()\ncontent_answers_df.columns = [\n    'content_mean',\n    'question_asked',\n    'content_skew',\n    'content_std',\n    'content_var',\n    'content_sem',\n    'content_sum'\n]\ncontent_answers_df.to_csv('content.csv')","befaecac":"train_df = train_df.merge(user_answers_df, how='left', on='user_id')\ntrain_df = train_df.merge(content_answers_df, how='left', on='content_id')\ntrain_df = train_df.replace([-np.inf, np.inf], np.nan)\ntrain_df = train_df.fillna(train_df.mean()","5aff8fe9":"features = [\n    'timestamp',\n    'user_id',\n    'content_id',\n    'prior_question_elapsed_time',\n    'prior_question_had_explanation',\n\n    'part',\n\n    'mean_user_accuracy',\n    'questions_skew',\n    'questions_std',\n    'questions_var',\n    'questions_sem',\n\n    'content_mean',\n    'content_skew',\n    'content_std',\n    'content_var',\n    'content_sem'\n]\n\ntarget = 'answered_correctly'","921c4a28":"selection_df = train_df[:20_000_000]\n\nselect_model = LGBMClassifier()\nselector = RFECV(select_model, step=1, cv=3, n_jobs=12, verbose=10, min_features_to_select=6)\nselector.fit(selection_df[features], selection_df[target])\n\njoblib.dump(selector, 'Selector.joblib') # save the selector on disk","29cd9f78":"features = [features[i] for i in range(len(selector.support_)) if selector.support_[i] == True]","24a2a142":"optuna_df = train_df[:20_000_000]\n\nXt, Xv, Yt, Yv = train_test_split(optuna_df[features], optuna_df[target], test_size=0.3, shuffle=True)\nlgb_train = lgb.Dataset(Xt, Yt)\nlgb_eval = lgb.Dataset(Xv, Yv)","6454b1e2":"def create_model(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 0.5),\n        'max_depth': trial.suggest_int('max_depth', 3, 18),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n        'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'random_state': 42\n    }\n    \n    model = LGBMClassifier(**params)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(Xt, Yt)\n    preds = model.predict_proba(Xv)[:, 1]\n    score = roc_auc_score(Yv, preds)\n    return score","96cb594b":"sampler = TPESampler(seed=42)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=50) # n_trials - number of parameter sets and model\n\nparams = study.best_params # get the best set of parameters from these n_trials sets\n\njoblib.dump(study, 'Study_optuna.joblib')","d756f63c":"best_params = {'num_leaves': 392, \n               'learning_rate': 0.14812766987568138, \n               'max_depth': 11, \n               'min_child_weight': 13, \n               'feature_fraction': 0.9829084591151024, \n               'bagging_fraction': 0.9793416187075863, \n               'bagging_freq': 5, \n               'min_child_samples': 22, \n               'reg_alpha': 0.8989695252132637,\n               'reg_lambda': 0.024084559071289695, \n               'n_estimators': 397\n              }","997b1aaa":"X_train, X_test, y_train, y_test = train_test_split(\n    train_df[features], train_df[target], test_size=0.2, random_state=42\n)","767f29d5":"model = LGBMClassifier(**params)\nmodel.fit(X_train, y_train)\n\njoblib.dump(model, 'Final_lgb.joblib')","5aa690d6":"env = riiideducation.make_env()\niter_test = env.iter_test()","6ecf7ff9":"questions_df = pd.read_csv(\n    '..\/input\/riiid-test-answer-prediction\/questions.csv',\n    usecols=[0, 3],\n    dtype={'question_id': 'int16', 'part': 'int8'}\n)\n\nmodel = joblib.load()\nselector = joblib.load('..\/input\/selector\/Selector.joblib')\n\nuser_answers_df = read_csv('..\/input\/preprocessingcontentuser\/user.csv')\ncontent_answers_df = read_csv('..\/input\/preprocessingcontentuser\/content.csv')","89c9cd74":"for (test_df, sample_prediction_df) in iter_test:\n    \n    test_df = pd.merge(test_df, questions_df, left_on='content_id', right_on='question_id', how='left')\n    test_df.drop(columns=['question_id'], inplace=True)\n    test_df['prior_question_had_explanation'].fillna(bool(True), inplace=True)\n    test_df = test_df.replace([-np.inf, np.inf], np.nan)\n    test_df = test_df.fillna(test_df.mean())\n    \n    test_df = test_df[test_df['content_type_id'] != 1]\n    \n    test_df = test_df.merge(user_answers_df, how='left', on='user_id')\n    test_df = test_df.merge(content_answers_df, how='left', on='content_id')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype(bool)\n    test_df = test_df.replace([-np.inf, np.inf], np.nan)\n    test_df = test_df.fillna(test_df.mean())\n\n    \n    test_df['answered_correctly'] = model.predict_proba(test_df[features])[:,1]\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n","b2f9a4c3":"## 3\n## Parameters selection with optuna.","97699410":"### 2.2\n### Using a logical mask(*selectoe.support_*), we select the most useful features. You can also find out the rank assigned to each feature(*selector.ranking_*).","0fe5f335":"### 1.4 \n### \u0421reate all possible features based by *'answered_correctly'*, grouping values first by *'user_id'*, then by *'content_id'* with using *dataframe* method *group_by* and genetating features with *agg* functions.\n\n### Write the resulting data for convenience in a file with help *read_csv*.","7679653b":"## 2\n## Features selection with RFECV.","02b92a6f":"### Good luck to all!","5e1cd5f0":"### 3.3 \n### Finally, let's create a optuna model that iterates through the parameters from the given list and selects the best ones.","8ecca0f0":"### 1.2 \n### Attach data frame from *questions_df* to *train_df* using pandas method *merge*. Joining occurs on the specified columns: *content_id* in *train_df* and *question_id* in *questions_df*.","5346cb82":"### 1.1 \n### Let's read the data from attached files(train.csv, questions.csv) into the specified columns.","f03bfccf":"### For speed, let's execute all the above code on the local computer and load the resulting models and data on kaggle.","daf75367":"### 1.5\n### Separating the training features from the target","0e7b9373":"## 5\n## Submitting result.","7e52ab9d":"### 1.3 \n### Fill NaN with mean values for numerical features and the most common for boolean ones. To find out the most common value in columns or distribution of values, we use  *matplotlib.pyplot* methods: *pie*(categorial\/boolean) or *hist*(numerical) or something else.\n","6c786499":"### 2.1\n### Let's create the simplest classifier for recursive selection of features using *sklearn.feature_selection.RFECV*. Train selector on our data.","8e97400e":"### 4.2\n### Prepare the model with the selected parameters and train it. ","9e2fd704":"### Repeat 1.2 to join the data from the previous point to the main data","8f268e88":"### 3.2\n### Let's define functions that return a model with a list of LGBMClassifier parameters(*create_model*) and its accuracy on test data(*objective*).","52cf6afb":"## 4\n## LGBMClassifier with chosen parameters and features training.","a194a6c8":"### The parameters I got:","1758fd24":"# Riiid! Answer Correctness Prediction with help lightgbm classifier with futures and parameters tuning. Beginners guide.","cd27a696":"### 4.1\n### Repeat 3.1, but on all dataset.","a3746366":"### That's all.","155ddfdb":"## 1. Data preprocessing.","5f9c19b2":"### The accuracy i got: 0.753","d689d73a":"### 3.1 \n### Create train and test data with *sklearn.model_selection.train_test_split*."}}