{"cell_type":{"a1bb9f41":"code","54a4f2a3":"code","42c2da9e":"code","9596eb36":"code","574e2dc0":"code","d564cbbe":"code","f9aa145d":"code","e07e58fb":"code","bd76d6f3":"code","7e889264":"code","b1ced1b1":"code","24c09424":"code","b341bf5b":"code","29d1c222":"code","8a582471":"code","3503996e":"code","ca8d37f4":"code","982c1148":"code","84e40e15":"code","9de12c31":"code","8f7b5993":"code","29bb4d30":"code","36271f55":"code","0378ddbb":"code","082d36c6":"code","d6e5637c":"code","19c7bb6c":"code","aba53bf2":"code","f831ae94":"code","2da8d8c5":"code","7183168d":"code","ebce02d7":"markdown","68eae0e3":"markdown","60fd6d0e":"markdown"},"source":{"a1bb9f41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport time\nimport copy\n\nimport numpy as np\n\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom plotly import tools\nfrom plotly.graph_objs import *\nfrom plotly.offline import init_notebook_mode, iplot, iplot_mpl\n\nfrom tqdm import tqdm_notebook as tqdm\n\ninit_notebook_mode()","54a4f2a3":"#import data from kaggle dataset called \"huge stock market\"\n\ntry:\n    data = pd.read_csv('..\/input\/Data\/Stocks\/goog.us.txt')\n    data['Date'] = pd.to_datetime(data['Date'])\n    data = data.set_index('Date')\n\nexcept (FileNotFoundError):\n\n    import datetime\n    import pandas_datareader as pdr\n    from pandas import Series, DataFrame\n\n    start = datetime.datetime(2010, 1, 1)\n    end = datetime.datetime(2017, 1, 11)\n\n    data = pdr.get_data_yahoo(\"AAPL\", start, end)\n\nprint(data.index.min(), data.index.max())\n\nsplit_index = int(len(data)\/2)\ndate_split = data.index[split_index]\ntrain = data[:split_index]\ntest = data[split_index:]\n\n#date_split = '2016-01-01'\n#train = data[:date_split]\n#test = data[date_split:]\n\nprint(len(data), len(train), len(test))\n\ndisplay(data)\n\n\n","42c2da9e":"def plot_train_test(train, test, date_split):\n    \n    data = [\n        Candlestick(x=train.index, open=train['Open'], high=train['High'], low=train['Low'], close=train['Close'], name='train'),\n        Candlestick(x=test.index, open=test['Open'], high=test['High'], low=test['Low'], close=test['Close'], name='test')\n    ]\n    layout = {\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)\n    ","9596eb36":"plot_train_test(train, test, date_split)","574e2dc0":"import matplotlib.pyplot as plt\n\ndata['Close'].plot(figsize=(23,8))\n\nplt.legend()\nplt.show()\n","d564cbbe":"class Environment:\n    \n    def __init__(self, data, history_t=90):\n        self.data = data\n        self.history_t = history_t\n        self.reset()\n        \n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.profits = 0\n        self.positions = []\n        self.position_value = 0\n        self.history = [0 for _ in range(self.history_t)]\n        return [self.position_value] + self.history # obs\n    \n    def step(self, act):\n        reward = 0\n        \n        # act = 0: stay, 1: buy, 2: sell\n        if act == 1:\n            self.positions.append(self.data.iloc[self.t, :]['Close'])\n        elif act == 2: # sell\n            if len(self.positions) == 0:\n                reward = -1\n            else:\n                profits = 0\n                for p in self.positions:\n                    profits += (self.data.iloc[self.t, :]['Close'] - p)\n                reward += profits\n                self.profits += profits\n                self.positions = []\n        \n        # set next time\n        self.t += 1\n        self.position_value = 0\n        for p in self.positions:\n            self.position_value += (self.data.iloc[self.t, :]['Close'] - p)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['Close'] - self.data.iloc[(self.t-1), :]['Close'])\n        \n        # clipping reward\n        if reward > 0:\n            reward = 1\n        elif reward < 0:\n            reward = -1\n        \n        return [self.position_value] + self.history, reward, self.done, self.profits # obs, reward, done, profits","f9aa145d":"env = Environment(train)\nprint(env.reset())\nfor _ in range(3):\n    pact = np.random.randint(3)\n    print(env.step(pact))","e07e58fb":"# DQN\n\ndef train_dqn(env, epoch_num=50):\n\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 20\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done, profit = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                        q = Q(b_pobs)\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards\n    ","bd76d6f3":"dqn, total_losses, total_rewards = train_dqn(Environment(train), epoch_num=25)","7e889264":"def plot_loss_reward(total_losses, total_rewards):\n\n    figure = tools.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n    figure['layout']['xaxis1'].update(title='epoch')\n    figure['layout']['xaxis2'].update(title='epoch')\n    figure['layout'].update(height=400, width=900, showlegend=False)\n    iplot(figure)\n    ","b1ced1b1":"plot_loss_reward(total_losses, total_rewards)","24c09424":"def plot_train_test_by_q(train_env, test_env, Q, algorithm_name):\n    \n    # train\n    pobs = train_env.reset()\n    train_acts = []\n    train_rewards = []\n    train_ongoing_profits = []\n\n    for _ in range(len(train_env.data)-1):\n        \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        train_acts.append(pact)\n            \n        obs, reward, done, profit = train_env.step(pact)\n        train_rewards.append(reward)\n        train_ongoing_profits.append(profit)\n\n        pobs = obs\n        \n    train_profits = train_env.profits\n    \n    # test\n    pobs = test_env.reset()\n    test_acts = []\n    test_rewards = []\n    test_ongoing_profits = []\n\n    for _ in range(len(test_env.data)-1):\n    \n        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n        pact = np.argmax(pact.data)\n        test_acts.append(pact)\n            \n        obs, reward, done, profit = test_env.step(pact)\n        test_rewards.append(reward)\n        test_ongoing_profits.append(profit)\n\n        pobs = obs\n        \n    test_profits = test_env.profits\n    \n    # plot\n    train_copy = train_env.data.copy()\n    test_copy = test_env.data.copy()\n    train_copy['act'] = train_acts + [np.nan]\n    train_copy['reward'] = train_rewards + [np.nan]\n    test_copy['act'] = test_acts + [np.nan]\n    test_copy['reward'] = test_rewards + [np.nan]\n    train0 = train_copy[train_copy['act'] == 0]\n    train1 = train_copy[train_copy['act'] == 1]\n    train2 = train_copy[train_copy['act'] == 2]\n    test0 = test_copy[test_copy['act'] == 0]\n    test1 = test_copy[test_copy['act'] == 1]\n    test2 = test_copy[test_copy['act'] == 2]\n    act_color0, act_color1, act_color2 = 'gray', 'cyan', 'magenta'\n\n    data = [\n        Candlestick(x=train0.index, open=train0['Open'], high=train0['High'], low=train0['Low'], close=train0['Close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=train1.index, open=train1['Open'], high=train1['High'], low=train1['Low'], close=train1['Close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=train2.index, open=train2['Open'], high=train2['High'], low=train2['Low'], close=train2['Close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2))),\n        Candlestick(x=test0.index, open=test0['Open'], high=test0['High'], low=test0['Low'], close=test0['Close'], increasing=dict(line=dict(color=act_color0)), decreasing=dict(line=dict(color=act_color0))),\n        Candlestick(x=test1.index, open=test1['Open'], high=test1['High'], low=test1['Low'], close=test1['Close'], increasing=dict(line=dict(color=act_color1)), decreasing=dict(line=dict(color=act_color1))),\n        Candlestick(x=test2.index, open=test2['Open'], high=test2['High'], low=test2['Low'], close=test2['Close'], increasing=dict(line=dict(color=act_color2)), decreasing=dict(line=dict(color=act_color2)))\n    ]\n    title = '{}: train s-reward {}, profits {}, test s-reward {}, profits {}'.format(\n        algorithm_name,\n        int(sum(train_rewards)),\n        int(train_profits),\n        int(sum(test_rewards)),\n        int(test_profits)\n    )\n    layout = {\n        'title': title,\n        'showlegend': False,\n         'shapes': [\n             {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}\n         ],\n        'annotations': [\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'},\n            {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}\n        ]\n    }\n    figure = Figure(data=data, layout=layout)\n    iplot(figure)\n    \n    return train_ongoing_profits, test_ongoing_profits\n    ","b341bf5b":"train_profits, test_profits = plot_train_test_by_q(Environment(train), Environment(test), dqn, 'DQN')","29d1c222":"plt.figure(figsize=(23,8))\nplt.plot(data.index,((data['Close']-data['Close'][0])\/data['Close'][-1]), label='buy and hold')\nplt.plot(train.index, ([0] + train_profits)\/data['Close'][-1], label='rl (train)')\nplt.plot(test.index, (([0] + test_profits) + train_profits[-1])\/data['Close'][-1], label='rl (test)')\nplt.ylabel('relative gain')\nplt.legend()\nplt.show()","8a582471":"# Double DQN\n\ndef train_ddqn(env, epoch_num=50):\n\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 50\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done, profit = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                        q = Q(b_pobs)\n                        \"\"\" <<< DQN -> Double DQN\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        === \"\"\"\n                        indices = np.argmax(q.data, axis=1)\n                        maxqs = Q_ast(b_obs).data\n                        \"\"\" >>> \"\"\"\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            \"\"\" <<< DQN -> Double DQN\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                            === \"\"\"\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n                            \"\"\" >>> \"\"\"\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards","3503996e":"ddqn, total_losses, total_rewards = train_ddqn(Environment(train), epoch_num=50)","ca8d37f4":"plot_loss_reward(total_losses, total_rewards)","982c1148":"train_profits, test_profits = plot_train_test_by_q(Environment(train), Environment(test), ddqn, 'Double DQN')","84e40e15":"plt.figure(figsize=(23,8))\nplt.plot(data.index,((data['Close']-data['Close'][0])\/data['Close'][-1]), label='buy and hold')\nplt.plot(train.index, ([0] + train_profits)\/data['Close'][-1], label='rl (train)')\nplt.plot(test.index, (([0] + test_profits) + train_profits[-1])\/data['Close'][-1], label='rl (test)')\nplt.ylabel('relative gain')\nplt.legend()\nplt.show()","9de12c31":"# Dueling Double DQN\n\ndef train_dddqn(env, epoch_num=50):\n\n    \"\"\" <<< Double DQN -> Dueling Double DQN\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, output_size)\n            )\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            y = self.fc3(h)\n            return y\n\n        def reset(self):\n            self.zerograds()\n    === \"\"\"\n    class Q_Network(chainer.Chain):\n\n        def __init__(self, input_size, hidden_size, output_size):\n            super(Q_Network, self).__init__(\n                fc1 = L.Linear(input_size, hidden_size),\n                fc2 = L.Linear(hidden_size, hidden_size),\n                fc3 = L.Linear(hidden_size, hidden_size\/\/2),\n                fc4 = L.Linear(hidden_size, hidden_size\/\/2),\n                state_value = L.Linear(hidden_size\/\/2, 1),\n                advantage_value = L.Linear(hidden_size\/\/2, output_size)\n            )\n            self.input_size = input_size\n            self.hidden_size = hidden_size\n            self.output_size = output_size\n\n        def __call__(self, x):\n            h = F.relu(self.fc1(x))\n            h = F.relu(self.fc2(h))\n            hs = F.relu(self.fc3(h))\n            ha = F.relu(self.fc4(h))\n            state_value = self.state_value(hs)\n            advantage_value = self.advantage_value(ha)\n            advantage_mean = (F.sum(advantage_value, axis=1)\/float(self.output_size)).reshape(-1, 1)\n            q_value = F.concat([state_value for _ in range(self.output_size)], axis=1) + (advantage_value - F.concat([advantage_mean for _ in range(self.output_size)], axis=1))\n            return q_value\n\n        def reset(self):\n            self.zerograds()\n    \"\"\" >>> \"\"\"\n\n    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n    Q_ast = copy.deepcopy(Q)\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(Q)\n\n    step_max = len(env.data)-1\n    memory_size = 200\n    batch_size = 50\n    epsilon = 1.0\n    epsilon_decrease = 1e-3\n    epsilon_min = 0.1\n    start_reduce_epsilon = 200\n    train_freq = 10\n    update_q_freq = 20\n    gamma = 0.97\n    show_log_freq = 5\n\n    memory = []\n    total_step = 0\n    total_rewards = []\n    total_losses = []\n\n    start = time.time()\n    for epoch in range(epoch_num):\n\n        pobs = env.reset()\n        step = 0\n        done = False\n        total_reward = 0\n        total_loss = 0\n\n        while not done and step < step_max:\n\n            # select act\n            pact = np.random.randint(3)\n            if np.random.rand() > epsilon:\n                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n                pact = np.argmax(pact.data)\n\n            # act\n            obs, reward, done, profit = env.step(pact)\n\n            # add memory\n            memory.append((pobs, pact, reward, obs, done))\n            if len(memory) > memory_size:\n                memory.pop(0)\n\n            # train or update q\n            if len(memory) == memory_size:\n                if total_step % train_freq == 0:\n                    shuffled_memory = np.random.permutation(memory)\n                    memory_idx = range(len(shuffled_memory))\n                    for i in memory_idx[::batch_size]:\n                        batch = np.array(shuffled_memory[i:i+batch_size])\n                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n\n                        q = Q(b_pobs)\n                        \"\"\" <<< DQN -> Double DQN\n                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n                        === \"\"\"\n                        indices = np.argmax(q.data, axis=1)\n                        maxqs = Q_ast(b_obs).data\n                        \"\"\" >>> \"\"\"\n                        target = copy.deepcopy(q.data)\n                        for j in range(batch_size):\n                            \"\"\" <<< DQN -> Double DQN\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n                            === \"\"\"\n                            target[j, b_pact[j]] = b_reward[j]+gamma*maxqs[j, indices[j]]*(not b_done[j])\n                            \"\"\" >>> \"\"\"\n                        Q.reset()\n                        loss = F.mean_squared_error(q, target)\n                        total_loss += loss.data\n                        loss.backward()\n                        optimizer.update()\n\n                if total_step % update_q_freq == 0:\n                    Q_ast = copy.deepcopy(Q)\n\n            # epsilon\n            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n                epsilon -= epsilon_decrease\n\n            # next step\n            total_reward += reward\n            pobs = obs\n            step += 1\n            total_step += 1\n\n        total_rewards.append(total_reward)\n        total_losses.append(total_loss)\n\n        if (epoch+1) % show_log_freq == 0:\n            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])\/show_log_freq\n            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])\/show_log_freq\n            elapsed_time = time.time()-start\n            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n            start = time.time()\n            \n    return Q, total_losses, total_rewards","8f7b5993":"dddqn, total_losses, total_rewards = train_dddqn(Environment(train), epoch_num=25)","29bb4d30":"plot_loss_reward(total_losses, total_rewards)","36271f55":"train_profits, test_profits = plot_train_test_by_q(Environment(train), Environment(test), dddqn, 'Dueling Double DQN')","0378ddbb":"plt.figure(figsize=(23,8))\nplt.plot(data.index,((data['Close']-data['Close'][0])\/data['Close'][-1]), label='buy and hold')\nplt.plot(train.index, ([0] + train_profits)\/data['Close'][-1], label='rl (train)')\nplt.plot(test.index, (([0] + test_profits) + train_profits[-1])\/data['Close'][-1], label='rl (test)')\nplt.plot(test.index, (([0] + test_profits) - data['Close'][0] + data['Close'][len(train_profits)])\/data['Close'][-1], label='rl (test)')\nplt.ylabel('relative gain')\nplt.legend()\nplt.show()","082d36c6":"def calcRsi(series, period):\n    \"\"\"\n    Calculate the RSI of a data series \n    \n    Parameters\n    ----------\n    series : pandas series\n        Candle sticks dataset\n    period : int\n        Period of each calculation\n        \n    Returns\n    -------\n    rsi : float\n        the calculated rsi\n    \"\"\"\n    try:\n        delta = series.diff().dropna()\n        u = delta * 0\n        d = u.copy()\n        u[delta > 0] = delta[delta > 0]\n        d[delta < 0] = -delta[delta < 0]\n        u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n        u = u.drop(u.index[:(period-1)])\n        d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n        d = d.drop(d.index[:(period-1)])\n\n        rs = u.ewm(com=period-1, adjust=False).mean() \\\n            \/ d.ewm(com=period-1, adjust=False).mean()\n        \n        rsi = 100 - 100 \/ (1 + rs)\n    except IndexError:\n        rsi = 0\n        \n    return rsi\n\ncolumn_value='Close'\ncolumn_index='Date'\n\ndata['RSI'] = calcRsi(data[column_value], 14)\n\n#RSI\nfig, ax = plt.subplots(figsize=(15,12))\nax.plot(data.index, data['RSI'])\nax.plot(data.index, data[column_value]\/data[column_value][0]*100.0)\nax.axhline(y=70,color='r')\nax.axhline(y=30,color='r')\nplt.text(s='Overbought', x=data.index[0], y=70, fontsize=14)\nplt.text(s='Oversold', x=data.index[0], y=30, fontsize=14)\nplt.legend()\n#p = plt.setp(ax.xaxis.get_majorticklabels(), rotation=90, fontsize=8)\nxmin, xmax = ax.get_xlim()\nax.set_xticks(np.linspace(xmin, xmax, 6))\nplt.show()","d6e5637c":"def addBollinger(df, period=20, col='Close'):\n    \"\"\"\n    Add the simple moving average column to dataframe \n\n    Parameters\n    ----------\n    df : pandas dataframe\n        Candle sticks dataset\n    period : int\n        Period of each calculation\n\n    Returns\n    -------\n    None\n    \"\"\"\n    bbmid_series = df[col].rolling(window=period).mean()\n    series_stdev = df[col].rolling(window=period).std()\n    df['BBUpperBand'] = bbmid_series + 2*series_stdev\n    df['BBLowerBand'] = bbmid_series - 2*series_stdev\n    df['BBBandwidth'] = df['BBUpperBand'] - df['BBLowerBand']  \n    df['BBMiddleBand'] = bbmid_series\n    \n    return df\n\n\ndata = addBollinger(data)\n\ncolumn_value='Close'\ncolumn_index='Date'\n\n#Bollinger Bands\nfig, ax = plt.subplots(figsize=(15,12))\nax.plot(data.index, data[column_value], label='Close')\nax.plot(data.index, data['BBUpperBand'], c='orange', label='Upper Band')\nax.plot(data.index, data['BBLowerBand'], c='orange', label='Lower Band')\nax.plot(data.index, data['BBMiddleBand'], c='black', label='Middle Band')\nplt.legend()\n#p = plt.setp(ax.xaxis.get_majorticklabels(), rotation=90, fontsize=8)\nxmin, xmax = ax.get_xlim()\nax.set_xticks(np.linspace(xmin, xmax, 6))\nplt.show()","19c7bb6c":"def addMACD(df, column_value='Close'):\n    ema_fast = df[column_value].ewm(span=12).mean()\n    ema_slow = df[column_value].ewm(span=26).mean()\n    signal_line = df[column_value].ewm(span=9).mean()\n    df['ema_fast'] = ema_fast\n    df['ema_slow'] = ema_slow\n    df['macd'] = ema_fast - ema_slow\n    df['macd_signal'] = df.macd.ewm(span=9, adjust=False).mean()\n    df['macdh'] = df['macd'] - df['macd_signal']\n    return df\n\ndata = addMACD(data)\n\nfig, ax = plt.subplots(figsize=(15,12))\nax.plot(data.index, data['ema_fast'], c='orange', label='fast')\nax.plot(data.index, data['ema_slow'], c='blue', label='slow')\nplt.legend()\np = plt.setp(ax.xaxis.get_majorticklabels(), rotation=90, fontsize=8)\nplt.show()\n\nfig, ax = plt.subplots(figsize=(15,12))\nax.plot(data.index, data['macd'], c='green')\n# ax.bar(, height=)\nax.plot(data.index, data['macd_signal'], c='blue')\nax.axhline(y=0,color='black')\nax.fill_between(data.index, data['macdh'], color = 'gray', alpha=0.5, label='MACD Histogram')\n#ax.set_xticklabels(data.index.reindex(ax.get_xticks()))\nplt.legend()\np = plt.setp(ax.xaxis.get_majorticklabels(), rotation=90, fontsize=8)\nplt.show()","aba53bf2":"# Initialize the short and long windows\nshort_window = 40\nlong_window = 100\n\n# Initialize the `signals` DataFrame with the `signal` column\nsignals = pd.DataFrame(index=data.index)\nsignals['signal'] = 0.0\n\n# Create short simple moving average over the short window\nsignals['short_mavg'] = data['Close'].rolling(window=short_window, min_periods=1, center=False).mean()\n\n# Create long simple moving average over the long window\nsignals['long_mavg'] = data['Close'].rolling(window=long_window, min_periods=1, center=False).mean()\n\n# Create signals\nsignals['signal'][short_window:] = np.where(signals['short_mavg'][short_window:] \n                                            > signals['long_mavg'][short_window:], 1.0, 0.0)   \n\n# Generate trading orders\nsignals['positions'] = signals['signal'].diff()","f831ae94":"fig = plt.figure(figsize=(10, 8))\n\n# Add a subplot and label for y-axis\nax1 = fig.add_subplot(111,  ylabel='Price in $')\n\n# Plot the closing price\ndata['Close'].plot(ax=ax1, color='grey', lw=2.)\n\n# Plot the short and long moving averages\nsignals[['short_mavg', 'long_mavg']].plot(ax=ax1, lw=2.)\n\n# Plot the buy signals\nax1.plot(signals.loc[signals.positions == 1.0].index, \n         signals.short_mavg[signals.positions == 1.0],\n         '^', markersize=10, color='m')\n         \n# Plot the sell signals\nax1.plot(signals.loc[signals.positions == -1.0].index, \n         signals.short_mavg[signals.positions == -1.0],\n         'v', markersize=10, color='k')\n         \n# Show the plot\nplt.show()","2da8d8c5":"# Set the initial capital\ninitial_capital= 100*data['Close'][0] #float(0.0)\n\n# Create a DataFrame `positions`\npositions = pd.DataFrame(index=signals.index).fillna(0.0)\n\n# Buy a 100 shares\npositions['AAPL'] = 100*signals['signal']   \n  \n# Initialize the portfolio with value owned   \nportfolio = positions.multiply(data['Close'], axis=0)\n\n# Store the difference in shares owned \npos_diff = positions.diff()\n\n# Add `holdings` to portfolio\nportfolio['holdings'] = (positions.multiply(data['Close'], axis=0)).sum(axis=1)\n\n# Add `cash` to portfolio\nportfolio['cash'] = initial_capital - (pos_diff.multiply(data['Close'], axis=0)).sum(axis=1).cumsum()   \n\n# Add `total` to portfolio\nportfolio['total'] = portfolio['cash'] + portfolio['holdings']\n\n# Add `returns` to portfolio\nportfolio['returns'] = portfolio['total'].pct_change()\n\nfig = plt.figure(figsize=(10, 8))\n\nax1 = fig.add_subplot(111, ylabel='Portfolio value in $')\n\n# Plot the equity curve in dollars\nportfolio['total'].plot(ax=ax1, lw=2.)\n\n# Plot the \"buy\" trades against the equity curve\nax1.plot(portfolio.loc[signals.positions == 1.0].index, \n         portfolio.total[signals.positions == 1.0],\n         '^', markersize=10, color='m')\n\n# Plot the \"sell\" trades against the equity curve\nax1.plot(portfolio.loc[signals.positions == -1.0].index, \n         portfolio.total[signals.positions == -1.0],\n         'v', markersize=10, color='k')\n\n# Show the plot\nplt.show()","7183168d":"env = Environment(data)\nenv.reset()\nongoing_profits = []\nfor i in range(1,len(signals['positions']-1)):\n    a = signals['positions'][i]\n    if a == -1:\n        a = 2\n    elif a == float('NaN'):\n        a = 0\n    obs, reward, done, profit = env.step(a)\n    ongoing_profits.append(profit)\n    \nplt.figure(figsize=(23,8))\nplt.plot(data.index,((data['Close']-data['Close'][0])\/data['Close'][-1]), label='buy and hold')\nplt.plot(data.index, ([0] + ongoing_profits)\/data['Close'][-1], label='crossing strategy')\nplt.ylabel('relative gain')\nplt.legend()\nplt.show()","ebce02d7":"**Bollinger Bands**\n\nBB is used to indicate possible trend reversals. Once the trendline crosses any of the upper or lower band then it means a reversal may occur","68eae0e3":"**Moving Average Convergence Divergence**\n\nMACD is another momentum indicator that helps in telling if the market is losing steam in its current trend and reversal may occur. Trend reversals are usually predicted during crossovers of the signal line and the MACD line","60fd6d0e":"**Relative Signal Strength**\n\nRSI is used to indicate the trend momentum\nThe upper region is called overbought and the lower region is oversold\nOnce the RSI reaches the overbought or oversold region it means the trend is strongly going down or up respectively, but once it exits the region then a trend reversal might occur."}}