{"cell_type":{"dd9ca6c2":"code","5a6fc56b":"code","1ce248c8":"code","1073e447":"code","e46fe23f":"code","2fe15b6d":"code","7d1e45d3":"code","ee3f2eb6":"code","5449096b":"code","d736ffb8":"code","cba9552e":"code","661cd250":"code","8864a5b0":"code","1c5bf11d":"code","aeea166e":"code","e75a800f":"code","55ff113e":"code","53a8c692":"code","c65c667f":"code","547d30f8":"code","04b935fa":"code","11777382":"code","fce38e44":"code","16f8ca3c":"code","992325d0":"code","0ff2ddc2":"code","02abc6ee":"code","b8379330":"code","79cc4ba1":"code","9a2f842b":"code","5ab2c122":"code","7662266e":"code","98d2e133":"code","bd9fa474":"code","5abc310f":"code","6a7ee3ef":"code","4241c71d":"markdown","103c62e2":"markdown","f5f549db":"markdown","05f68f01":"markdown","2fe16e6a":"markdown","bc56d4ca":"markdown","4a7eed81":"markdown","5aed3508":"markdown","30cb87a6":"markdown","6fa01961":"markdown","2a48c8bf":"markdown","d73779d4":"markdown","f8489a39":"markdown","ec445660":"markdown","5b9f8f72":"markdown","d4ca8f0a":"markdown","9cfaf3ae":"markdown","532c214e":"markdown","ea44aa69":"markdown","c3d395ce":"markdown","87a37e65":"markdown","3b66c63d":"markdown","53fe612f":"markdown","628825ef":"markdown"},"source":{"dd9ca6c2":"\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit, train_test_split\nfrom sklearn.metrics import make_scorer, mean_absolute_error, r2_score\nimport xgboost\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","5a6fc56b":"df_train = pd.read_csv('..\/input\/train_V2.csv', nrows=2000)\ndf_test  = pd.read_csv('..\/input\/test_V2.csv', nrows=2000)","1ce248c8":"df_train.describe()","1073e447":"df_test.describe()","e46fe23f":"df_train = df_train.drop(df_train[df_train.winPlacePerc.isnull()].index,inplace = False)","2fe15b6d":"f,ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(df_train.corr(), annot=True, linewidths=.5, fmt='.1f',ax=ax)\nplt.show()","7d1e45d3":"k = 5\nf,ax = plt.subplots(figsize=(6, 6))\ncm = df_train.corr().nlargest(k, 'winPlacePerc')\ncols = cm.index\ncm = cm[cols]\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","ee3f2eb6":"def obter_features(df):\n    #Obter a quantidade de jogadores por partida\n    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n\n    #Obter taxa de mortes por jogador por partida\n    df['killsPerMatch'] = df['kills'] \/ df['playersJoined']\n    df['killsPerMatch'].fillna(0,inplace=True)\n\n    #Obter taxa de dano por jogador por partida\n    df['damagePerMatch'] = df['damageDealt'] \/ df['playersJoined']\n    df['damagePerMatch'].fillna(0,inplace=True)\n\n    #Obter quantidade m\u00e9dia de dano por morte\n    df['damagePerKill'] = df['damageDealt'] \/ df['kills']\n    df['damagePerKill'].fillna(0,inplace=True)\n    df['damagePerKill'].replace(np.inf,0,inplace=True)\n\n    #Obter taxa de tiros na cabe\u00e7a por morte\n    df['headshotPerKill'] = df['headshotKills'] \/ df['kills']\n    df['headshotPerKill'].fillna(0, inplace=True) \n\n    #Obter dist\u00e2ncia total percorrida pelo jogador na partida\n    df['totalDistance'] = df['rideDistance'] + df['swimDistance'] + df['walkDistance']\n    \n    return df","5449096b":"df_train = obter_features(df_train)\ndf_test = obter_features(df_test)","d736ffb8":"features = df_train.columns\nfeatures = features.drop(['Id', 'groupId', 'matchId', 'winPlacePerc', 'matchType'])\nfeatures","cba9552e":"f,ax = plt.subplots(figsize=(8, 8))\nnew_features = df_train[['playersJoined', 'killsPerMatch', 'damagePerMatch', 'damagePerKill', 'headshotPerKill', 'totalDistance', 'winPlacePerc']]\nsns.heatmap(new_features.corr(), annot=True, linewidths=.5, fmt='.1f',ax=ax)\nplt.show()","661cd250":"#Separando a Classe das demais vari\u00e1veis\ntarget = df_train['winPlacePerc']\nids_train = df_train['Id']\nids_test = df_test['Id']\n#Retirando tamb\u00e9m as vari\u00e1veis winPlacePerc (alvo), Id, groupId e matchId\ntrain_norm = np.array(df_train.drop(['Id', 'groupId', 'matchId', 'winPlacePerc', 'matchType'], axis=1))\ntest_norm = np.array(df_test.drop(['Id', 'groupId', 'matchId', 'matchType'], axis=1))","8864a5b0":"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\n# labelencoder_train = LabelEncoder()\n# train_norm[:,12] = labelencoder_train.fit_transform(train_norm[:,12])\n# onehotencoder = OneHotEncoder(categorical_features=[12])\n# train_norm = onehotencoder.fit_transform(train_norm).toarray()\n\n# labelencoder_test = LabelEncoder()\n# test_norm[:,12] = labelencoder_test.fit_transform(test_norm[:,12])\n# onehotencoder = OneHotEncoder(categorical_features=[12])\n# test_norm = onehotencoder.fit_transform(test_norm).toarray()","1c5bf11d":"# #Normlizando usando o StandardScaler\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n\n# train_norm = scaler.fit_transform(train_norm)\n# #pd.DataFrame(train_norm).head()\n\n# test_norm = scaler.fit_transform(test_norm)\ntrain_norm = (train_norm-train_norm.min())\/(train_norm.max()-train_norm.min())\ntest_norm = (test_norm-test_norm.min())\/(test_norm.max()-test_norm.min())","aeea166e":"train_norm.shape","e75a800f":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ntrain_norm = reduce_mem_usage(pd.DataFrame(train_norm))\ntarget = reduce_mem_usage(pd.DataFrame(target))\n\ntest_norm = reduce_mem_usage(pd.DataFrame(test_norm))","55ff113e":"#Salvar os Ids de cada inst\u00e2ncia para ter como associar depois do split\ntrain_norm = train_norm.join(ids_train)\n\ndel ids_train\n\nX_train, X_test, Y_train, Y_test = train_test_split(train_norm, target, test_size=1\/3, random_state=0)\n#pd.DataFrame(X_train).describe()\n","53a8c692":"#Separar Ids dos conjuntos\nids_X_train = X_train['Id']\nids_X_test = X_test['Id']\n\nX_train = X_train.drop(['Id'], axis=1)\nX_test = X_test.drop(['Id'], axis=1)","c65c667f":"#Treinando o modelo\nmodel = xgboost.XGBRegressor(max_depth=17, gamma=0.3, learning_rate= 0.1)\nmodel.fit(X_train,Y_train)","547d30f8":"xgboost.plot_importance(model)","04b935fa":"pred = model.predict(test_norm)","11777382":"submit_xg = pd.DataFrame({'Id': ids_test, \"winPlacePerc\": pred} , columns=['Id', 'winPlacePerc'])\n\n# r2_test_XGB = r2_score(Y_test,pred)\n# mae_test_XGB = mean_absolute_error(Y_test,pred)\n\n# print('XGBoost Resultados para o conjunto de testes:')\n# print('\u00cdndice R^2: ' + str(r2_test_XGB))\n# print('Erro m\u00e9dio absoluto: ' + str(mae_test_XGB))\nprint(submit_xg.head())\n#submit_xg.to_csv(\"submission.csv\", index = False)","fce38e44":"from sklearn.tree import DecisionTreeRegressor \nregressor = DecisionTreeRegressor()\nregressor.fit(X_train,Y_train) #X s\u00e3o os previsores e Y os valores correspondentes\n#Para fazer uma previs\u00e3o:\nprevisoes = regressor.predict(X_test)","16f8ca3c":"score_train_DT = regressor.score(X_train,Y_train) #Valor do score na base de dados de treinamento\nscore_test_DT = regressor.score(X_test,Y_test) #Valor do Score na base de dados de teste\nacuracia_DT = r2_score(Y_test, previsoes)\nprint (score_train_DT)\nprint (score_test_DT)\nprint (acuracia_DT)\n","992325d0":"#calculando o erro de uma \u00e1rvore de decis\u00e3o para regress\u00e3o:\nmae_DT = mean_absolute_error(Y_test,previsoes)\n#mae cont\u00e9m o valor do mean absolute error\nprint (mae_DT)","0ff2ddc2":"#f,ax = plt.subplots(figsize=(20, 20))\n#sns.heatmap(df_train.corr(), annot=True, linewidths=.5, fmt='.1f',ax=ax)\n#plt.show()\n","02abc6ee":"def performance_metric(y_true, y_predict):\n    score = r2_score(y_true,y_predict)\n    return score","b8379330":"def grid_scores_to_df(grid_scores):\n    \"\"\"\n    Convert a sklearn.grid_search.GridSearchCV.grid_scores_ attribute to a tidy\n    pandas DataFrame where each row is a hyperparameter-fold combinatination.\n    \"\"\"\n    rows = list()\n    for grid_score in grid_scores:\n        for fold, score in enumerate(grid_score.cv_validation_scores):\n            row = grid_score.parameters.copy()\n            row['fold'] = fold\n            row['score'] = score\n            rows.append(row)\n    df = pd.DataFrame(rows)\n    return df","79cc4ba1":"# Gerar conjuntos de valida\u00e7\u00e3o-cruzada para o treinamento de dados\ncv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n\n#n_estimators =10\nrfr =  RandomForestRegressor(n_estimators=10, random_state=42)\n\n#Gerar um dicion\u00e1rio para o par\u00e2metro 'max_depth' com um alcance de 1 a 10\nparams ={'max_depth': list(range(1,5))}\n\n#Transformar 'performance_metric' em uma fun\u00e7\u00e3o de pontua\u00e7\u00e3o utilizando 'make_scorer' \nscoring_fnc = make_scorer(performance_metric)\n\n# Gerar o objeto de busca em matriz\ngrid = GridSearchCV(rfr, params, scoring=scoring_fnc, cv=cv_sets)\n\n# Ajustar o objeto de busca em matriz com os dados para calcular o modelo \u00f3timo\ngrid = grid.fit(X_train, Y_train)","9a2f842b":"# Usando o melhor modelo para predi\u00e7\u00e3o\nrfr = grid.best_estimator_\nprevisoes = rfr.predict(X_test)","5ab2c122":"#Valor do score na base de dados de treinamento\nscore_train_RFR = rfr.score(X_train,Y_train)\n\n#Valor do Score na base de dados de teste\nscore_test_RFR = rfr.score(X_test,Y_test)\nprint ('Random Forest Regressor Results: ')\nprint ('Score de treino: ' + str(score_train_RFR))\nprint ('Score de teste: ' + str(score_test_RFR))\n\n#calculando o erro de uma \u00e1rvore de decis\u00e3o para regress\u00e3o:\nmae_RFR = mean_absolute_error(Y_test,previsoes)\n#mae cont\u00e9m o valor do mean absolute error\nprint ('Erro m\u00e9dio absoluto: ' + str(mae_RFR))\n\n#Acur\u00e1cia do modelo\nr2_RFR = r2_score(Y_test, previsoes)\nprint ('\u00cdndice R\u00b2: ' + str(r2_RFR))","7662266e":"rfr_scores = grid_scores_to_df(grid.grid_scores_)\nrfr_scores","98d2e133":"from sklearn.svm import SVR\n\n# Gerar conjuntos de valida\u00e7\u00e3o-cruzada para o treinamento de dados\ncv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n\nsvr = SVR()\n\n#Gerar um dicion\u00e1rio para o par\u00e2metro 'max_depth' com um alcance de 1 a 10\nparams = {'kernel': ('rbf','linear','poly')}\n\n#Transformar 'performance_metric' em uma fun\u00e7\u00e3o de pontua\u00e7\u00e3o utilizando 'make_scorer' \nscoring_fnc = make_scorer(performance_metric)\n\n# Gerar o objeto de busca em matriz\ngrid = GridSearchCV(svr, params, scoring=scoring_fnc, cv=cv_sets)\n\n# Ajustar o objeto de busca em matriz com os dados para calcular o modelo \u00f3timo\ngrid = grid.fit(X_train, Y_train)","bd9fa474":"# Usando o melhor modelo para predi\u00e7\u00e3o\nsvr = grid.best_estimator_\nprevisoes = svr.predict(X_test)","5abc310f":"#Valor do score na base de dados de treinamento\nscore_train_SVR = svr.score(X_train,Y_train)\n\n#Valor do Score na base de dados de teste\nscore_test_SVR = svr.score(X_test,Y_test)\nprint ('SVR Results: ')\nprint ('Score de treino: ' + str(score_train_SVR))\nprint ('Score de teste: ' + str(score_test_SVR))\n\n#calculando o erro de uma \u00e1rvore de decis\u00e3o para regress\u00e3o:\nmae_SVR = mean_absolute_error(Y_test,previsoes)\n#mae cont\u00e9m o valor do mean absolute error\nprint ('Erro m\u00e9dio absoluto: ' + str(mae_SVR))\n\n#Acur\u00e1cia do modelo\nr2_SVR = r2_score(Y_test, previsoes)\nprint ('\u00cdndice R\u00b2: ' + str(r2_SVR))","6a7ee3ef":"svr_scores = grid_scores_to_df(grid.grid_scores_)\nsvr_scores","4241c71d":"*** SVR: Resultados obtidos ***","103c62e2":"**Estat\u00edsticas descritivas que resumem a tend\u00eancia central, a dispers\u00e3o e a forma da distribui\u00e7\u00e3o de um conjunto de dados**","f5f549db":"## **C\u00f3digo para Random Forest**","05f68f01":"** Definir m\u00e9trica de performance **","2fe16e6a":"** Treinamento do modelo **","bc56d4ca":"}**Reduzir o uso de mem\u00f3ria dos dados  **","4a7eed81":"**Dividindo entre Treinamento(2\/3) e Valida\u00e7\u00e3o(1\/3)**","5aed3508":"**Converter o retorno do m\u00e9todo sklearn.grid_search.GridSearchCV.grid_scores_ para um pandas DataFrame**","30cb87a6":"**Calculando o Erro usando Mean Absolute Error**","6fa01961":"**Retirando valores 'NaN'**","2a48c8bf":"** Predi\u00e7\u00e3o do modelo criado para o conjunto de teste **","d73779d4":"** Predi\u00e7\u00e3o do modelo criado para o conjunto de teste **","f8489a39":"*** Random Forest Regressor: Resultados obtidos ***","ec445660":"## ***C\u00f3digo para \u00c1rvore de Decis\u00e3o***","5b9f8f72":"**Separar os Ids dos conjuntos de treinamento e teste para que n\u00e3o afetem o modelo e que possam ser utilizados para submiss\u00e3o**","d4ca8f0a":"## **C\u00f3digo para um SVR**","9cfaf3ae":"**Treinando o modelo XGBoost Regressor**","532c214e":"**Obtendo as k vari\u00e1veis com maiores correla\u00e7\u00f5es em rela\u00e7\u00e3o ao alvo winPlacePerc**","ea44aa69":"**Calculando e exibindo heatmap da matriz de correla\u00e7\u00e3o do conjunto de treinamento**","c3d395ce":"**Transforma\u00e7\u00e3o de Vari\u00e1veis Categ\u00f3ricas em Num\u00e9ricas**\n\n*Verificar se matchType n\u00e3o vai precisar ser uma vari\u00e1vel dummy*","87a37e65":"**Criando novas features, baseando-se nas correla\u00e7\u00f5es entre as vari\u00e1veis**","3b66c63d":"**Normlizando usando o StandardScaler**","53fe612f":"**Mostrar as correla\u00e7\u00f5es das novas features em rela\u00e7\u00e3o ao alvo (winPlacePerc)**","628825ef":"## ** C\u00f3digo para XGBoost **"}}