{"cell_type":{"53c23568":"code","1ad189b3":"code","7e03e7eb":"code","fe7ecc8a":"code","51d4368b":"code","b9045325":"code","3e11d51b":"code","aae63b45":"code","12f7a58b":"code","6711b7c1":"code","7954886f":"code","39c02cc1":"code","68a6b0bd":"code","7db89d52":"code","9ce4d0aa":"code","e74bf942":"code","fa8b3f4c":"code","2252e581":"code","5ebaa22f":"code","a741a559":"code","721095d5":"code","38cabcd5":"code","5ee0a1de":"code","efadea22":"code","fc00f8d4":"code","25f3c42b":"code","b40c4f03":"code","6019a1c6":"code","bcfcadb9":"code","a98b60ce":"code","6afd0c93":"code","450646f9":"code","e7072781":"code","fba0aa81":"code","063a8397":"code","e988d5bd":"code","a2a824dd":"code","089947d1":"code","b031f5b4":"code","25079d19":"code","f60511b6":"code","532bbae6":"code","01eba100":"code","bdae0900":"code","3e6aa333":"code","3bae05e1":"code","8ca1ca37":"code","09c54b5c":"code","629d2500":"code","6bfb8c3b":"code","943731ca":"code","e416e577":"code","967e8847":"code","55672218":"code","bd4a93ea":"code","e88e8263":"code","b8092cf5":"code","cd6dafdc":"code","fd439645":"code","8cb10b09":"code","b3baa3d9":"code","578cfb71":"code","181c793d":"code","28dd33a2":"code","07157da6":"code","a8a6c170":"code","2fe749df":"code","c17408d2":"code","8d9fd6a2":"code","8ef50ec6":"code","fe403c72":"code","50932b8e":"code","1450ba59":"code","68c835b4":"code","98f249be":"code","be6e46da":"code","96693f51":"code","e88719d2":"code","f18eb216":"code","b6de2d78":"code","c296ee63":"code","5cae2319":"code","31396428":"code","530876ba":"code","a9659b3f":"code","50aeb301":"code","57a32c06":"code","b911e3e4":"code","31530afc":"code","494baf99":"code","b872dce8":"code","3509ee4d":"code","010d7a8d":"code","53748eba":"code","dca63da6":"code","51506699":"code","661dc857":"code","5d5c5613":"code","c8fb2050":"code","4c555786":"code","053167ec":"code","681519a8":"code","921277ae":"code","8ebf892e":"code","bb6da93c":"markdown","1c97ed24":"markdown","d507b4b5":"markdown","36f3746d":"markdown","b0c454e6":"markdown","d010941b":"markdown","dd90f1e0":"markdown","f8f920ea":"markdown","74551968":"markdown","2ed4df02":"markdown","960210e1":"markdown","d5b0bde0":"markdown","e99e428b":"markdown","c3de57ad":"markdown","a1097edd":"markdown","d688e9c9":"markdown","f04ca61f":"markdown","c74c5e0b":"markdown","d43b6460":"markdown","56f6cdc5":"markdown","9c032dce":"markdown","3c147dda":"markdown","abc0d725":"markdown","54652a6f":"markdown","75ec7983":"markdown","b6336558":"markdown","fdd41103":"markdown","fd5f4435":"markdown","1d0abeb6":"markdown","cf15bc6c":"markdown","81a098fc":"markdown","baca3c00":"markdown","ae3a6a5c":"markdown","ddfb6129":"markdown","883726a2":"markdown","85f17d4b":"markdown","42e9aca2":"markdown"},"source":{"53c23568":"# try the best model performence on different training set sizes","1ad189b3":"# It is a meal delivery company which operates in multiple cities. \n# They have various fulfillment centers (packing warehouse)\n# in these cities for dispatching meal orders to their customers. \n# The client wants you to help these centers with demand forecasting for upcoming weeks\n# so that these centers will plan the stock of raw materials accordingly.","7e03e7eb":"import numpy as np\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline","fe7ecc8a":"##demand = demand.merge(fulfilment_center, on='center_id').merge(meal_info, on='meal_id')","51d4368b":"# Food demand: predict num_orders\ndemand = pd.read_csv('..\/input\/food-demand\/demand.csv')\ndemand.drop('id',axis=1,inplace=True)\nfulfilment_center = pd.read_csv('..\/input\/food-demand\/fulfilment_center_info.csv')\nmeal_info = pd.read_csv('..\/input\/food-demand\/meal_info.csv')\ndemand = demand.merge(fulfilment_center, on='center_id')\ndemand = demand.merge(meal_info, on='meal_id')\ndemand = demand.sort_values(['week','meal_id','center_id'])","b9045325":"# show the data","3e11d51b":"demand.head(5)","aae63b45":"# Check how many examples and how many features are in the dataset","12f7a58b":"demand.shape","6711b7c1":"# We have 456,548 examples and 14 (after dropping 'id') columns (13 features and one label - num orders","7954886f":"# Check for missing values","39c02cc1":"demand.isna().mean()","68a6b0bd":"# no missing values","7db89d52":"from matplotlib import pyplot as py\n\nplt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\n\n# Plot Histogram on x\nx = demand['num_orders']\nplt.hist(x, 30, range=[0, 2000], facecolor='#86bf91', align='mid',zorder=2, rwidth=0.9)\n#py.xticks(range(0,5000))\nplt.gca().set(title='Label Histogram', ylabel='Frequency');","9ce4d0aa":"sns.jointplot(x='checkout_price',y='num_orders',data=demand)","e74bf942":"demand.groupby('week').num_orders.mean().plot()","fa8b3f4c":"sns.barplot(demand['cuisine'],demand['num_orders'])","2252e581":"sns.barplot(demand['category'],demand['num_orders'])\nplt.xticks(rotation = '90')","5ebaa22f":"plt.figure(figsize=(20,10))\nc=demand.corr()\nsns.heatmap(c,cmap=\"YlGnBu\",annot=True)","a741a559":"demand.corr()['num_orders'].sort_values().plot.bar()","721095d5":"sns.pairplot(demand)","38cabcd5":"# Show object columns","5ee0a1de":"demand.dtypes","efadea22":"# cusine, category, center_type are objects","fc00f8d4":"# Show Unique values for object columns","25f3c42b":"demand['center_type'].value_counts()","b40c4f03":"demand['category'].value_counts()","6019a1c6":"demand['cuisine'].value_counts()","bcfcadb9":"# get dummies for object type columns","a98b60ce":"demand = pd.get_dummies(demand)","6afd0c93":"# rename all the collumns to lower case","450646f9":"demand = demand.rename(columns=str.lower)","e7072781":"# Show the new data frame","fba0aa81":"demand.sample(10)","063a8397":"demand.shape","e988d5bd":"# 32 columns (31 features & 1 label)","a2a824dd":"# show the corr between the features","089947d1":"demand.corr()","b031f5b4":"# Corr plot for the decision factor (num_orders)","25079d19":"demand.corr()['num_orders'].sort_values()","f60511b6":"from sklearn.model_selection import train_test_split\nX, y = demand.drop('num_orders', axis=1), demand.num_orders\ntrain_test_ratio = 4566\/456548\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_test_ratio, random_state=0, shuffle=False)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","532bbae6":"# We took 0.01 of data to test set considering time continuess","01eba100":"X_train.head(451982)","bdae0900":"X_test.head(4566)","3e6aa333":"from sklearn.metrics import mean_absolute_error\n\n\ndef check_regressor(regressor, X_train, y_train, X_test, y_test):\n    # Fit regression model\n    regressor.fit(X_train, y_train)\n\n    # Predict\n    y_pred = regressor.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    y_pred_train = regressor.predict(X_train)\n    mae_train = mean_absolute_error(y_train, y_pred_train)\n    \n    model_name = regressor.__class__.__name__\n    print('{0} mean absolute error is {1:.4f} (mae train {2:.4f})'.format(model_name, mae, mae_train))\n    return mae, mae_train, model_name","3bae05e1":"models_errors = {}\nmodels_errors_train = {}","8ca1ca37":"# we will check the error between the test and the train to check the model and check overfit","09c54b5c":"class Benchmark:\n    def fit(self, x, y):\n        self.value = y.mean()\n        return self\n    \n    def predict(self, x):\n        return np.ones(len(x))*self.value\n    \nbenchmark = Benchmark()\nmodel_mae, model_mae_train, model_name = check_regressor(benchmark, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","629d2500":"# The mean absolute error for the testing set is 193.7072 and for the training set is 227.9756","6bfb8c3b":"from sklearn.linear_model import LinearRegression\n\n\nlinear_regressor = LinearRegression()\nmodel_mae, model_mae_train, model_name = check_regressor(linear_regressor, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","943731ca":"# the error on the testing set is 151.2310 and on tbhe tarining set is 162.3364","e416e577":"#K nearest neighbors, try 3 different k values.","967e8847":"from sklearn.neighbors import KNeighborsRegressor\n\nk = 3\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","55672218":"from sklearn.neighbors import KNeighborsRegressor\n\nk = 5\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","bd4a93ea":"from sklearn.neighbors import KNeighborsRegressor\n\nk = 7\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","e88e8263":"# the error on the testing set for k=5 is 79.8184 and on the training set is 67.0602","b8092cf5":"# K nearest neighbors with scaled values, try 3 different k values.","cd6dafdc":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nk = 3\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train_scaled, y_train, X_test_scaled, y_test)\nmodel_name += 'Scaled'\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","fd439645":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nk = 5\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train_scaled, y_train, X_test_scaled, y_test)\nmodel_name += 'Scaled'\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","8cb10b09":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nk = 7\nnearest_neighbors = KNeighborsRegressor(k)\nmodel_mae, model_mae_train, model_name = check_regressor(nearest_neighbors, X_train_scaled, y_train, X_test_scaled, y_test)\nmodel_name += 'Scaled'\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","b3baa3d9":"# the error on the testing set is 74.2631 and on the training set is 60.7128","578cfb71":"from sklearn.tree import DecisionTreeRegressor\n\nmax_depth = 17\ndecision_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\nmodel_mae, model_mae_train, model_name = check_regressor(decision_tree, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","181c793d":"from sklearn.tree import DecisionTreeRegressor\n\nmax_depth = 16\ndecision_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\nmodel_mae, model_mae_train, model_name = check_regressor(decision_tree, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","28dd33a2":"from sklearn.tree import DecisionTreeRegressor\n\nmax_depth = 3\ndecision_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\nmodel_mae, model_mae_train, model_name = check_regressor(decision_tree, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","07157da6":"# the error on the testing set (93.0577) and on the tarining set (66.9899) \n# 17 max depth will reduce the mae for training set and increase for testing set(overfit)","a8a6c170":"from IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\ndef plot_tree(tree, features=X_train.columns, labels=['0', '1']):\n    graph = Source(export_graphviz(tree, feature_names=features, filled = True))\n    display(SVG(graph.pipe(format='svg')))\n    \nplot_tree(decision_tree)","2fe749df":"# random forest, 3 different max depth values on n_estimator = 100","c17408d2":"from sklearn.ensemble import RandomForestRegressor\n\nn_estimators = 100\nmax_depth = 3\nrandom_forest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\nmodel_mae, model_mae_train, model_name = check_regressor(random_forest, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","8d9fd6a2":"from sklearn.ensemble import RandomForestRegressor\n\nn_estimators = 100\nmax_depth = 25\nrandom_forest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\nmodel_mse, model_mse_train, model_name = check_regressor(random_forest, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mse\nmodels_errors_train[model_name] = model_mse_train","8ef50ec6":"from sklearn.ensemble import RandomForestRegressor\n\nn_estimators = 100\nmax_depth = 20\nrandom_forest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\nmodel_mse, model_mse_train, model_name = check_regressor(random_forest, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mse\nmodels_errors_train[model_name] = model_mse_train","fe403c72":"# the error on the testing set is 81.7132 and on the training set is 47.4846","50932b8e":"# higher error for 4 max depth","1450ba59":"from sklearn.ensemble import AdaBoostRegressor\n\nn_estimators = 100\nmax_depth = 40\nbase_estimator = DecisionTreeRegressor(max_depth=max_depth)\nada_boost = AdaBoostRegressor(random_state=0, n_estimators=n_estimators, base_estimator=base_estimator)\nmodel_mae, model_mae_train, model_name = check_regressor(ada_boost, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","68c835b4":"from sklearn.ensemble import AdaBoostRegressor\n\nn_estimators = 100\nmax_depth = 25\nbase_estimator = DecisionTreeRegressor(max_depth=max_depth)\nada_boost = AdaBoostRegressor(random_state=0, n_estimators=n_estimators, base_estimator=base_estimator)\nmodel_mae, model_mae_train, model_name = check_regressor(ada_boost, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","98f249be":"from sklearn.ensemble import AdaBoostRegressor\n\nn_estimators = 100\nmax_depth = 10\nbase_estimator = DecisionTreeRegressor(max_depth=max_depth)\nada_boost = AdaBoostRegressor(random_state=0, n_estimators=n_estimators, base_estimator=base_estimator)\nmodel_mae, model_mae_train, model_name = check_regressor(ada_boost, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","be6e46da":"# the error on the testing set is 71.8249 and on the training set is 11.6979 for max depth 25","96693f51":"# Lasso Model on 3 different alphas","e88719d2":"from sklearn.linear_model import Lasso\n\n\nlasso = Lasso(alpha=1)\nmodel_mae, model_mae_train, model_name = check_regressor(lasso, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","f18eb216":"from sklearn.linear_model import Lasso\n\n\nlasso = Lasso(alpha=2)\nmodel_mae, model_mae_train, model_name = check_regressor(lasso, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","b6de2d78":"from sklearn.linear_model import Lasso\n\n\nlasso = Lasso(alpha=3)\nmodel_mae, model_mae_train, model_name = check_regressor(lasso, X_train, y_train, X_test, y_test)\nmodels_errors[model_name] = model_mae\nmodels_errors_train[model_name] = model_mae_train","c296ee63":"# the error on the testing set for alpha 1 is 148.2808 and on the training set is 161.2801","5cae2319":"import pandas as pd\n\ndef compare_performance(models_errors_train, models_errors):\n    return pd.DataFrame({'train':models_errors_train, 'test':models_errors}).sort_values('test')\n\ncofo = compare_performance(models_errors_train, models_errors)","31396428":"cofo","530876ba":"# PLOT FOR THE COMPRASION (powerpoint)","a9659b3f":"cofo.plot.bar(rot=10,colormap='cool',figsize = (16,9));","50aeb301":"lasso.coef_ , lasso.intercept_","57a32c06":"pd.DataFrame({'lasso_coef':lasso.coef_},index=X_train.columns).sort_values(by=['lasso_coef']).head(32)","b911e3e4":"pd.DataFrame({'importance':decision_tree.feature_importances_},index=X_train.columns).sort_values(by=['importance']).tail(14)","31530afc":"train_acc = []\ntest_acc = []\n\nfor i in range(1,21):\n    tree = DecisionTreeRegressor(random_state=0,max_depth=i)\n    tree.fit(X_train, y_train)\n    y_pred_train = tree.predict(X_train)\n    mae_train = mean_absolute_error(y_train, y_pred_train)\n    train_acc.append(mae_train)\n    y_pred = tree.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    test_acc.append(mae)\n\nfig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1, 21), train_acc, s=10, c='b', marker=\"s\", label='Training MAE')\nax1.scatter(range(1, 21) ,test_acc, s=10, c='r', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Max Depth\")\nplt.legend(loc='upper left');\nplt.show()","494baf99":"# The chosen depth in decision tree is 17 due to lowest mae","b872dce8":"train_acc = []\ntest_acc = []\n\nfor i in range(1,21):\n    \n    tree = DecisionTreeRegressor(random_state=0,max_depth=i)\n    ada_boost = AdaBoostRegressor(random_state=0,base_estimator=tree,n_estimators=20)\n    ada_boost.fit(X_train, y_train)\n    y_pred_train = ada_boost.predict(X_train)\n    mae_train = mean_absolute_error(y_train, y_pred_train)\n    train_acc.append(mae_train)\n    y_pred = ada_boost.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    test_acc.append(mae)\n\nfig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1, 21), train_acc, s=10, c='b', marker=\"s\", label='Training MAE')\nax1.scatter(range(1, 21) ,test_acc, s=10, c='r', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Max Depth\")\nplt.legend(loc='upper left');\nplt.show()","3509ee4d":"fig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1, 21), train_acc, s=100, c='b', marker=\"s\", label='Training MAE')\nax1.scatter(range(1, 21) ,test_acc, s=100, c='g', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Max Depth\")\nplt.legend(loc='lower left');\nplt.show()","010d7a8d":"from sklearn.neighbors import KNeighborsRegressor\n\ntrain_acc = []\ntest_acc = []\n\nfor i in range(1,5):\n    nearest_neighbors = KNeighborsRegressor(n_neighbors=3)\n    nearest_neighbors.fit(X_train_scaled, y_train)\n    y_pred_train = nearest_neighbors.predict(X_train_scaled)\n    mae_train = mean_absolute_error(y_train, y_pred_train)\n    train_acc.append(mae_train)\n    y_pred = nearest_neighbors.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    test_acc.append(mae)\n\nfig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1, 5), train_acc, s=10, c='b', marker=\"s\", label='Training MAE')\nax1.scatter(range(1, 5) ,test_acc, s=10, c='r', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Max Depth\")\nplt.legend(loc='upper right');\nplt.show()","53748eba":"train_acc = []\ntest_acc = []\n\nfor i in range(1,20):\n    tree = RandomForestRegressor(random_state=0,n_estimators=100,max_depth=i)\n    tree.fit(X_train, y_train)\n    y_pred_train = tree.predict(X_train)\n    mae_train = mean_absolute_error(y_train, y_pred_train)\n    train_acc.append(mae_train)\n    y_pred = tree.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    test_acc.append(mae)\n\nfig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1, 20), train_acc, s=10, c='b', marker=\"s\", label='Training MAE')\nax1.scatter(range(1, 20) ,test_acc, s=10, c='r', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Max Depth\")\nplt.legend(loc='upper right');\nplt.show()","dca63da6":"# checks the best mex depth","51506699":"q = test_acc[0]\nfor i in range(0,19):\n   if (test_acc[i] < q):\n        q = test_acc[i]\n        w = i\nprint(q)\nprint(w)","661dc857":"# 18 is the best max depth for random forest with 39.35 mae","5d5c5613":"# Performance vs. amount of data Using the best performing algorithm. Show a graph describing the test performance of the algorithm when using [10%\/30%\/50%\/70%\/100%] of the train set for training the algorithm. Would you recommend collecting more data for the problem?","c8fb2050":"# Hist for Knn scaled different training set size ","4c555786":"from sklearn.neighbors import KNeighborsRegressor\n\nlength = len(X_train_scaled)\nlst = [int(0.1*length),int(0.3*length),int(0.5*length),int(0.7*v),length]\navg_error_score = []\nfor item in lst:\n    q, w = X_train_scaled[:item], y_train[:item]\n    knn = KNeighborsRegressor(n_neighbors = 3) #'best performed'\n    knn.fit(q, w)\n    y_pred = knn.predict(X_test_scaled)\n    mae = mean_absolute_error(y_test, y_pred)\n    avg_error_score.append(mae)","053167ec":"fig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(0.1,1,0.2) ,avg_error_score, s=10, c='r', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Training Size\")\nplt.legend(loc='upper right');\nplt.show()","681519a8":"# ada boost mae on different training set size ","921277ae":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nlength = len(X_train)\nlst = [int(0.1*length),int(0.3*length),int(0.5*length),int(0.7*length),length]\navg_error_score = []\n\nn_estimators = 100\nmax_depth = 17 # the best performed\n\nfor item in lst: \n    q, w = X_train[:item], y_train[:item]\n    base_estimator = DecisionTreeRegressor(max_depth=max_depth)\n    ada_boost = AdaBoostRegressor(random_state=0, n_estimators=n_estimators, base_estimator=base_estimator)\n    ada_boost.fit(q, w)\n    y_pred = ada_boost.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    avg_error_score.append(mae)","8ebf892e":"# Hist for ada boost\n\nfig = plt.figure(figsize=(16,9))\nax1 = fig.add_subplot(111)\n\nax1.scatter(range(1,6) ,avg_error_score, s=100, c='g', marker=\"o\", label='Testing MAE')\nax1.set_ylabel(\"MAE\")\nax1.set_xlabel(\"Training Size\")\nplt.legend(loc='upper right');\nplt.show()","bb6da93c":"# 7. Algorithms introspection ","1c97ed24":"#### number of orders by checkout price","d507b4b5":"# Load Data","36f3746d":"## Nearest Neighbors Regressor","b0c454e6":"## Feature Engineering","d010941b":"### Check Hyper parameters - max depth & n_estimatores , for Random Forest","dd90f1e0":"#### number of orders by weeks ","f8f920ea":"#### corr table","74551968":"#### Label Distirbution","2ed4df02":"# Data Explore","960210e1":"## Scaling","d5b0bde0":"## Lasso","e99e428b":"#### All the features corr plots","c3de57ad":"## Random Forest Regressor","a1097edd":"## Keep Exploring","d688e9c9":"# Train test Split","f04ca61f":"# Performance Comparision","c74c5e0b":"# Helper function for evaluation","d43b6460":"## AdaBoost Regressor","56f6cdc5":"# 9. Additional analysis ","9c032dce":"## 8. Hyperparameters","3c147dda":"### Look for the hyper parameters of the algorithm that best improves the test performance, show a comparison in performance using their values.","abc0d725":"## Benchmark","54652a6f":"### Random Forest feature importance|","75ec7983":"#### number of orders by cuisine type","b6336558":"### Check Hyper parameters - different k , for Scaling KNN","fdd41103":"## Decision Tree Regressor","fd5f4435":"# Models","1d0abeb6":"#### number of orders by meal category ","cf15bc6c":"## describing plots on the data","81a098fc":"### Lasso coef in table ","baca3c00":"### Check Hyper parameters - different k , for ADABoost","ae3a6a5c":"## Linear Regressor","ddfb6129":"### Check Hyper parameters - max depth, for Decision Tree","883726a2":"### What are the weights of the lasso coefficients? ","85f17d4b":"#### corr of numb_orders with other features","42e9aca2":"### Visualize Tree"}}