{"cell_type":{"f72b08a7":"code","b6917b2d":"code","2fd6a5e0":"code","d8a69312":"code","a54bed19":"code","45c438df":"code","1d193483":"code","fa3e67dd":"code","50b5dfdc":"code","95cf0299":"code","2f8ee5d0":"code","1b72720b":"code","e80539a8":"code","83ed86a4":"code","4cb2d165":"code","05f5f8ef":"code","ba4cb38b":"code","dc076cea":"code","b2dd013b":"markdown"},"source":{"f72b08a7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"whitegrid\")","b6917b2d":"wine_data = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv', parse_dates=True, encoding = \"cp1252\")\nwine_data.head()","2fd6a5e0":"fig = plt.figure(figsize = (10,6))\nsns.countplot(data=wine_data, x='quality')","d8a69312":"X = wine_data[['fixed acidity',\n               'volatile acidity',\n               'citric acid',\n               'residual sugar',\n               'chlorides',\n               'free sulfur dioxide',\n               'total sulfur dioxide',\n               'density',\n               'pH',\n               'sulphates',\n               'alcohol']]\n\ny = wine_data['quality']","a54bed19":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)","45c438df":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scl = scaler.fit_transform(X_train)\nX_test_scl = scaler.transform(X_test)","1d193483":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, balanced_accuracy_score\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fa3e67dd":"models=[(\"Logistic Regression\", LogisticRegression()),\n        (\"Stochastic Gradient Descent\", SGDClassifier()),\n        (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n        (\"Decision Tree\", DecisionTreeClassifier()),\n        (\"Random Forest\", RandomForestClassifier()),\n        (\"Extra Trees\", ExtraTreesClassifier()),\n        (\"Gradient Boostin\", GradientBoostingClassifier()),\n        (\"KNeighbors\", KNeighborsClassifier()),\n        (\"SVM\", SVC()),\n        (\"Naive Bayes\", GaussianNB()),\n        (\"Ada Boost\", AdaBoostClassifier())]\n\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    print(f\"\\x1b[96m{name}\\x1b[0m: \\x1b[95m{results.mean():.4f}\\x1b[0m \u00b1 {results.std():.4f}\")","50b5dfdc":"import imblearn\nprint(imblearn.__version__)\n\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import SMOTEN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","95cf0299":"counter = Counter(y)\nprint(counter)","2f8ee5d0":"ros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\nprint(sorted(Counter(y_resampled).items()))","1b72720b":"oversample = SMOTE(sampling_strategy = {5: 5000, 6: 5000, 7: 5000, 4: 5000, 8: 5000, 3: 5000})\nX_smote, y_smote = oversample.fit_resample(X, y)\n\ncounter = Counter(y_smote)\nprint(counter)","e80539a8":"X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.25, random_state=0)\n\net = ExtraTreesClassifier()\net.fit(X_train_smote, y_train_smote)\net_predict = et.predict(X_test_smote)\n\nprint(f\"\u0422rain: {et.score(X_train_smote, y_train_smote)*100} - \u0422est: {et.score(X_test_smote, y_test_smote)*100}\")\n\nrfc_eval = cross_val_score(estimator = et, X = X_train_smote, y = y_train_smote, cv = 10)\nprint(\"cross_val_score: \", rfc_eval.mean()*100)\n\nprint(\"accuracy_score: \", accuracy_score(y_test_smote, et_predict)*100)\n\nprint(\"balanced_accuracy_score: \", balanced_accuracy_score(y_test_smote, et_predict)*100)","83ed86a4":"oversample = ADASYN(sampling_strategy=\"minority\")\nX_smote, y_smote = oversample.fit_resample(X, y)\n\ncounter = Counter(y_smote)\nprint(counter)","4cb2d165":"oversample = SMOTEN()\nX_smote, y_smote = oversample.fit_resample(X, y)\n\ncounter = Counter(y_smote)\nprint(counter)","05f5f8ef":"oversample = BorderlineSMOTE()\nX_smote, y_smote = oversample.fit_resample(X, y)\n\ncounter = Counter(y_smote)\nprint(counter)","ba4cb38b":"over = SMOTE()\nunder = RandomUnderSampler()\n\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\nX_rsmote, y_rsmote = pipeline.fit_resample(X, y)\n\ncounter = Counter(y_rsmote)\nprint(counter)","dc076cea":"X_train_rsmote, X_test_rsmote, y_train_rsmote, y_test_rsmote = train_test_split(X_rsmote, y_rsmote, test_size=0.25, random_state=0)\n\nmodels=[(\"Logistic Regression\", LogisticRegression()),\n        (\"Stochastic Gradient Descent\", SGDClassifier()),\n        (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis()),\n        (\"Decision Tree\", DecisionTreeClassifier()),\n        (\"Random Forest\", RandomForestClassifier()),\n        (\"Extra Trees\", ExtraTreesClassifier()),\n        (\"Gradient Boostin\", GradientBoostingClassifier()),\n        (\"KNeighbors\", KNeighborsClassifier()),\n        (\"SVM\", SVC()),\n        (\"Naive Bayes\", GaussianNB()),\n        (\"Ada Boost\", AdaBoostClassifier())]\n\nfor name, model in models:\n    kfold = KFold(n_splits=10)\n    results = cross_val_score(model, X_train_rsmote, y_train_rsmote, cv=kfold, scoring='accuracy')\n    print(f\"\\x1b[96m{name}\\x1b[0m: \\x1b[95m{results.mean():.4f}\\x1b[0m \u00b1 {results.std():.4f}\")","b2dd013b":"# Synthetic Minority Oversampling Technique"}}