{"cell_type":{"80167d7b":"code","8fbb95d8":"code","672ea5ca":"code","cee60212":"code","cdbd88ab":"code","a4f2f269":"code","6c465f14":"code","ab9b538c":"code","7dd846b2":"code","8d370675":"code","549d59ac":"code","959efa61":"code","a366f4cd":"code","cc550c2c":"code","a3cfeb84":"code","78752a00":"code","fedee0ef":"code","596425b6":"code","e941512c":"code","d5405222":"code","57f4983e":"code","201a21c8":"code","893c586d":"code","e7ee507c":"code","e92dcb48":"code","9eb2ffc8":"code","b43f92ed":"code","61cfa32a":"code","9c051b1b":"code","a1513ba0":"code","fdedb4bc":"code","ffb2dcda":"code","4605469b":"code","5e91e8b1":"code","922ee3fe":"code","dc023cb4":"code","dd9134a4":"code","7af383cb":"code","caa07c79":"code","ae36ff6f":"code","be5f1281":"code","82330224":"code","1abc78de":"code","9d1fce01":"markdown","8fa53a8b":"markdown","ccfb2160":"markdown","330f7f9e":"markdown","c7ad9494":"markdown","d2fd90f4":"markdown","cf6907a4":"markdown","54ee190b":"markdown","3427612e":"markdown","6a318b85":"markdown","b113f452":"markdown","a2bd6c54":"markdown","42e98b18":"markdown","d7350add":"markdown","4d2cf5cf":"markdown","ec9df278":"markdown","b9f0ae9b":"markdown","582d142e":"markdown","b7923d55":"markdown","f99ad3e8":"markdown","a455fe67":"markdown","3277e2f7":"markdown","78c179a6":"markdown","6ba8af48":"markdown","a67859f9":"markdown","5aca89a5":"markdown","6a7561a4":"markdown","f1ec1577":"markdown","0f039c93":"markdown","d02b60f4":"markdown","40a5dfac":"markdown","85fa5f1b":"markdown","d1ab34d6":"markdown","f1dd3578":"markdown","9a247180":"markdown","6e10d17b":"markdown","cc7d4ee1":"markdown","59d1bfad":"markdown","0bd930ff":"markdown","9467f42e":"markdown","94c8b59a":"markdown","64248d2e":"markdown","8b1971fb":"markdown","4f984abb":"markdown","fda7717a":"markdown","faf6601c":"markdown","62ba9acd":"markdown","e192ea82":"markdown","f6ecedd2":"markdown"},"source":{"80167d7b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom IPython.display import display, Markdown\n\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import svm\n\nimport scipy.cluster.hierarchy as hac\n\n%matplotlib inline\n%config IPCompleter.greedy=True\nwarnings.filterwarnings('ignore')","8fbb95d8":"# Set normalization\nenable_normalization = True\nnormalization_type = 'minmax' # 'minmax' or 'standard'\n\n# Exploratory analysis\n\n# Set correlation\nenable_correlation = False\nenable_dendrogram = False\nenable_heatmap = False\n\n# Features Selection\n\n# Set features selection with correlation criteria\nenable_correlation_selec = False\nfactor = 0.95 # number close to 1\n\n# Set features selection with univariate statitics test criteria\nenable_univariate_selec = True\nmethod_selec = 'selectkbest' # 'selectkbest', 'pca', ...\npca_variance = 0.95 \ncriteria_k_best = mutual_info_classif # chi2, mutual_info_classif\nk_best = 84 # number of best features to select on select k best.\n\n# Balancing\n\n# Train balancing\nenable_balancing = True\nnumber_samples = 2500\n\n# Machine learning method\n\nml_method = 'randomforestreg' # 'gradientboosting', 'svm', ...\ngbc_loss = 'deviance' # gradient boosting loss\nrfc_criterion = 'gini' # random forest criterion\nenable_cv = True # enable cross-validation\n","672ea5ca":"# Print the bar graph from data\ndef bar(acumm_data):\n    # Do plot\n    fig = plt.figure(figsize=(10,7))\n    ax = fig.add_subplot(111)\n    ax = sns.barplot(x=acumm_data.index, y=acumm_data.values, palette='tab20b', ax=ax)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)    \n    return ax\n\ndef dendrogram(df):    \n    # Do correlation matrix\n    corr_matrix = df.corr()\n\n    # Do the clustering\n    Z = hac.linkage(corr_matrix, 'single')\n\n    # Plot dendogram\n    fig, ax = plt.subplots(figsize=(25, 10))\n    plt.title('Hierarchical Clustering Dendrogram')\n    plt.xlabel('sample index')\n    plt.ylabel('distance')\n    groups = hac.dendrogram(\n        Z,\n        leaf_rotation=90.,  # rotates the x axis labels\n        leaf_font_size=8., # font size for the x axis labels\n        color_threshold = 0#,\n        #truncate_mode='lastp',\n        #p=30\n    )\n\n    labels_dict = pd.DataFrame(df.columns).to_dict()[0]\n    actual_labels = [item.get_text() for item in ax.get_xticklabels()]\n    new_labels = [labels_dict[int(i)] for i in actual_labels]\n    ax.set_xticklabels(new_labels)\n    plt.tight_layout()\n\ndef corr_drop(corr_m, factor=.9):\n    \n    global cm\n    cm = corr_m\n    # Get correlation score, as high as this score, more chances to be dropped.\n    cum_corr = cm.applymap(abs).sum()\n    def remove_corr():\n        global cm\n        for col in cm.columns:\n            for ind in cm.index:\n                if (ind in cm.columns) and (col in cm.index):\n                    # Compare if are high correlated.\n                    if (cm.loc[ind,col] > factor) and (ind!=col):\n                        cum = cum_corr[[ind,col]].sort_values(ascending=False)\n                        cm.drop(cum.index[0], axis=0, inplace=True)\n                        cm.drop(cum.index[0], axis=1, inplace=True)\n                        # Do recursion until the last high correlated.\n                        remove_corr()\n        return cm\n    return remove_corr()","cee60212":"train_features = pd.read_csv('..\/input\/aps_failure_training_set_processed_8bit.csv', na_values='na')\ntest_features =  pd.read_csv('..\/input\/aps_failure_test_set_processed_8bit.csv', na_values='na')\n\ntrain_labels = train_features['class']\ntest_labels = test_features['class']\ntrain_features = train_features.drop('class', axis=1)\ntest_features = test_features.drop('class', axis=1)","cdbd88ab":"train_features.describe()","a4f2f269":"flat_data = train_features.values.flatten()\ncount=0\nfor value in flat_data:\n    if value is not None:\n        continue\n    count+= 1\npct_nan = round(100*count\/len(flat_data))\nprint(f'{pct_nan}% of data are non-valid.')","6c465f14":"from sklearn.preprocessing import MinMaxScaler\nif enable_normalization and normalization_type=='minmax':\n    scaler = MinMaxScaler()\n    scaler.fit(train_features)\n    train_features = pd.DataFrame(scaler.transform(train_features), columns=train_features.columns)","ab9b538c":"train_features.describe()","7dd846b2":"train_labels = train_labels.apply(round)\ntrain_labels = train_labels.replace({-1:0})","8d370675":"bar(train_labels.value_counts())\nplt.show()","549d59ac":"if enable_correlation and enable_dendrogram:\n    corr_matrix = train_features.corr()\n    dendrogram(corr_matrix)\n    plt.tight_layout()","959efa61":"if enable_correlation and enable_heatmap:\n    fig, ax = plt.subplots(figsize=(10,10))\n    ax = sns.heatmap(corr_matrix, square=True, cmap='Purples', ax=ax)\n    plt.tight_layout()\n    plt.show()","a366f4cd":"# to enable run correlation selection without univariate selection.\nbest_train_features = train_features \nnew_corr_matrix = best_train_features.corr()","cc550c2c":"if enable_univariate_selec:\n    if method_selec=='selectkbest':\n        selectKBest = SelectKBest(chi2, k_best)\n        selectKBest.fit(train_features, train_labels)\n        best_train_features = selectKBest.transform(train_features)\n\n        idxs_selected = selectKBest.get_support(indices=True)\n        best_train_features = train_features.iloc[:,idxs_selected]\n","a3cfeb84":"if enable_univariate_selec:\n    if method_selec=='selectkbest':\n        print(best_train_features.columns) # selected columns","78752a00":"if enable_univariate_selec:\n    if method_selec=='selectkbest':\n        best_train_features.describe()","fedee0ef":"if enable_univariate_selec:\n    if method_selec=='selectkbest':\n        new_corr_matrix = best_train_features.corr()\n        dendrogram(new_corr_matrix)\n        plt.tight_layout()","596425b6":"if enable_correlation_selec:\n    new_new_corr_matrix = corr_drop(new_corr_matrix, factor)\n    print(f'Number of features selected is {len(new_new_corr_matrix.columns)}.')","e941512c":"if enable_correlation_selec:\n    dendrogram(new_new_corr_matrix)\n    plt.tight_layout()","d5405222":"if enable_correlation_selec:\n    fig, ax = plt.subplots(figsize=(10,10))\n    ax = sns.heatmap(new_new_corr_matrix, square=True, cmap='Purples', ax=ax)\n    plt.tight_layout()\n    plt.show()","57f4983e":"if enable_correlation_selec:\n    best_train_features = best_train_features.loc[:,new_new_corr_matrix.columns]","201a21c8":"if method_selec=='pca':\n    pca = PCA(pca_variance)\n    pca.fit(train_features)\n    best_train_features = pca.transform(train_features)\n    best_train_features = pd.DataFrame(best_train_features)","893c586d":"if method_selec=='pca':\n    print('Number of components {pca.n_components_}')","e7ee507c":"# to enable run without balancing\nbest_train_features_balanced = best_train_features\ntrain_labels_balanced = train_labels","e92dcb48":"if enable_balancing:\n    idxs_pos = train_labels[train_labels==1].index\n    idxs_neg = train_labels[train_labels==0].sample(n=number_samples, replace=False, random_state=0).index\n    idxs_balanced = np.concatenate((idxs_pos,idxs_neg))\n    best_train_features_balanced = best_train_features.loc[idxs_balanced]\n    train_labels_balanced = train_labels.loc[idxs_balanced]\n    print(f'Proportion balanced: {int(number_samples\/1000)}\/1')","9eb2ffc8":"if ml_method=='gradientboosting':\n    gbc = GradientBoostingClassifier(loss=gbc_loss, random_state=0)\n    if not enable_cv:\n        gbc.fit(best_train_features_balanced, train_labels_balanced)","b43f92ed":"if ml_method=='gradientboosting' and enable_cv:\n    #Seleciona os par\u00e2metros do GB que deseja testar\n    params = [{'loss': ['deviance', 'exponential']}]\n    \n    #Executa grid search com cross validation\n    gbcc = GridSearchCV(gbc, params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n    gbcc.fit(best_train_features_balanced, train_labels_balanced)\n    gbc = gbcc\n    ","61cfa32a":"if ml_method=='gradientboosting':\n    display(gbc)","9c051b1b":"if ml_method=='randomforestreg':\n    rfc = RandomForestRegressor(n_estimators=100, oob_score = True, random_state=0, n_jobs=3)\n    rfc.fit(best_train_features_balanced, train_labels_balanced)\n    print(rfc)","a1513ba0":"if ml_method=='randomforest':\n    rfc = RandomForestClassifier(criterion=rfc_criterion, random_state=0)\n    if not enable_cv:\n        rfc.fit(best_train_features_balanced, train_labels_balanced)\n    ","fdedb4bc":"if ml_method=='randomforest' and enable_cv:\n    #Seleciona os par\u00e2metros do GB que deseja testar\n    params = [{'criterion': ['gini', 'entropy'], 'max_features': ['sqrt', 'log2'], 'n_estimators': [10, 100]}]\n    rfc = RandomForestClassifier(random_state=0)\n    #Executa grid search com cross validation\n    rfcc = GridSearchCV(rfc, params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n    rfcc.fit(best_train_features_balanced, train_labels_balanced)\n    rfc = rfcc","ffb2dcda":"if ml_method=='randomforest':\n    display(rfcc)","4605469b":"if ml_method=='svm':\n    #Seleciona os par\u00e2metros da SVM que deseja testar\n    params = [{'kernel': ['rbf'], 'gamma': [0.01], 'C': [0.001, 0.01, 0.1, 1, 10]}, \n              {'kernel': ['linear'], 'gamma': [0.01],  'C':  [0.001, 0.01, 0.1, 1, 10]}\n             ]\n    #Executa grid search com cross validation\n    svmc = GridSearchCV(svm.SVC(C=1), params, cv=5, scoring='recall', verbose=10, n_jobs=3)\n    svmc.fit(best_train_features_balanced, train_labels_balanced)","5e91e8b1":"# to enable change feature selection method\nbest_test_features = test_features  ","922ee3fe":"if enable_normalization:\n    scaler.transform(best_test_features)\n    best_test_features = pd.DataFrame(scaler.transform(best_test_features), columns=best_test_features.columns)","dc023cb4":"if enable_univariate_selec:\n    if method_selec=='selectkbest':        \n        X = selectKBest.transform(best_test_features)\n        idxs_selected = selectKBest.get_support(indices=True)\n        best_test_features = best_test_features.iloc[:,idxs_selected]\n    if method_selec=='pca':        \n        best_test_features = pca.transform(best_test_features)\nif enable_correlation_selec:\n    best_test_features = best_test_features.loc[:,new_new_corr_matrix.columns]","dd9134a4":"test_labels = test_labels.apply(round)\ntest_labels = test_labels.replace({-1:0})","7af383cb":"if ml_method=='gradientboosting':\n    y_pred = gbc.predict(best_test_features)\n    report = classification_report(test_labels, y_pred)\n    print(report)","caa07c79":"if ml_method=='randomforestreg':\n    y_pred = rfc.predict(best_test_features)\n    y_pred = np.round(y_pred)\n    report = classification_report(test_labels, y_pred)\n    print(report)","ae36ff6f":"if ml_method=='randomforest':\n    y_pred = rfc.predict(best_test_features)\n    report = classification_report(test_labels, y_pred)\n    print(report)","be5f1281":"if ml_method=='svm':\n    y_pred = svmc.predict(best_test_features)\n    report = classification_report(test_labels, y_pred)\n    print(report)","82330224":"cm = confusion_matrix(test_labels, y_pred).ravel()\ncm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\ndisplay(cm)","1abc78de":"total_cost = 10*cm.fp + 500*cm.fn\ndef printmd(string):\n    display(Markdown(string))\nprintmd(f'Total cost is: \\n# <p><span style=\"color:purple\">${float(total_cost.values[0])}<\/span><\/p>')\n","9d1fce01":"Run optimization with cross-validation.\n\nRef. <br> http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","8fa53a8b":"Used method: Pearson distance. Ref: https:\/\/pt.wikipedia.org\/wiki\/Correla%C3%A7%C3%A3o#Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson","ccfb2160":"Correlation after feature selection","330f7f9e":"Apply MinMaxScaler with values between 0 and 1.","c7ad9494":"### Processing labels","d2fd90f4":"Files path `..\/input\/<file>`","cf6907a4":"Chosen method.","54ee190b":"### Scalling","3427612e":"### Feature extraction with PCA","6a318b85":"If doesn't exist, it is considered non-valid.","b113f452":"### Make prediction with trainned model choosen and show report","a2bd6c54":"As same as done with train labels.","42e98b18":"### Processing train labels","d7350add":"Correlation matrix heatmap.","4d2cf5cf":"### Show confusion matrix\nRef.\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html","ec9df278":"Exploratory analysis after scalling.","b9f0ae9b":"Ref.: \n<br> http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html\n<br> https:\/\/en.wikipedia.org\/wiki\/Chi-squared_distribution","582d142e":"import os\nos.listdir('..\/input')","b7923d55":"### Utility functions","f99ad3e8":"[](http:\/\/)The data has a lot of features, because of that, is very difficulty to visualize hierarchical graphs,\n<br> One option is to use a pipeline with selection attribute method to reduce number of features and than dropping the high correlated features.","a455fe67":"## Processing and analysis over data","3277e2f7":"### SVM\nRef. <br> http:\/\/scikit-learn.org\/stable\/modules\/svm.html","78c179a6":"\nSet normalization\n\n    enable_normalization = True\n    normalization_type = 'minmax' # 'minmax' or 'standard'\n\nExploratory analysis\n\nSet correlation\n\n    enable_correlation = False\n    enable_dendrogram = False\n    enable_heatmap = False\n\nFeatures Selection\n\nSet features selection with correlation criteria\n\n    enable_correlation_selec = False\n    factor = 0.95 # number close to 1\n\nSet features selection with univariate statitics test criteria\n\n    enable_univariate_selec = True\n    method_selec = 'selectkbest' # 'selectkbest', 'pca', ...\n    pca_variance = 0.95 \n    criteria_k_best = mutual_info_classif # chi2, mutual_info_classif\n    k_best = 84 # number of best features to select on select k best.\n\nBalancing\n\nTrain balancing\n\n    enable_balancing = True\n    number_samples = 2500\n\nMachine learning method\n\n    ml_method = 'randomforestreg' # 'gradientboosting', 'svm', 'randomforest'...\n    gbc_loss = 'deviance' # gradient boosting loss\n    rfc_criterion = 'gini' # random forest criterion\n    enable_cv = True # enable cross-validation\n\n","6ba8af48":"### Only mantain the best feature between the high correlated features","a67859f9":"## EDA","5aca89a5":"## Prediction","6a7561a4":"### Apply scalling over test data","f1ec1577":"### labels porportion","0f039c93":"Random Forest Classifier","d02b60f4":"### Gradient Boosting \nRef. <br> http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n<br> http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html","40a5dfac":"Predict with SVM.","85fa5f1b":"Selected columns.","d1ab34d6":"### Balancing","f1dd3578":"### Setup hyperparameters","9a247180":"Ref: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html","6e10d17b":"Predict with Gradient boosting.","cc7d4ee1":"Run optimization with cross-validation.","59d1bfad":"### Apply feature selection over test data","0bd930ff":"### Show total cost obtained","9467f42e":"### Flatten dataframe and check for non-valid values","94c8b59a":"Descritive analysis after feature selection","64248d2e":"## Pre-processing","8b1971fb":"### Correlation analysis","4f984abb":"### Random Forest","fda7717a":"Random Forest Regressor","faf6601c":"Round values and replace -1 with 0 making all labels positives.","62ba9acd":"## Modeling","e192ea82":"![](http:\/\/)__Obs: __ You can make a pipeline with feature selction method and feature selection by correlation, just enable both in *Setup hyperparameters*","f6ecedd2":"### Feature selection by univariate statistics test"}}