{"cell_type":{"ef884230":"code","ae5abb29":"code","c82c38dd":"code","fe86de66":"code","23590f47":"code","4892bd37":"code","d47ebe4f":"code","a74fc1c4":"code","32f1bd4a":"code","f5ce0db3":"code","94a760e2":"code","8cffb0fb":"code","d1f100b7":"code","dc057540":"code","ea550f67":"code","aa9abc9f":"code","5b454569":"code","098febc6":"code","66a7640c":"code","12ba5293":"code","81b4ca02":"markdown","2688ea86":"markdown","97313679":"markdown"},"source":{"ef884230":"#Imports\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom random import sample\nimport numpy as np\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nfrom random import shuffle\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\n# Imports BERT\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns","ae5abb29":"# Read dataset and create input column\n\nfilepath = \"..\/input\/real-and-fake-news-dataset\/news.csv\"\ndf = pd.read_csv(filepath)\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf['titletext'] = df['title'] + \" \" + df['text']\n# Cap the sentences length\ndf['titletext'] = df = df[df['titletext'].str.split().str.len().lt(1000)]\ndf.head()","c82c38dd":"#Vocabulary class\n\nUNK_TOKEN = 9\nclass Vocab:\n    def __init__(self):\n        self.word2id = {\"__unk__\": UNK_TOKEN}\n        self.id2word = {UNK_TOKEN: \"__unk__\"}\n        self.n_words = 1\n\n        self.tag2id = {\"FAKE\": 0, \"REAL\": 1}\n        self.id2tag = {0: \"FAKE\", 1: \"REAL\"}\n\n    def index_words(self, words):\n        word_indexes = [self.index_word(w) for w in words]\n        return word_indexes\n\n    def index_tags(self, tag):\n        tag_index = self.tag2id[tag]\n        return tag_index\n\n    def index_word(self, w):\n        if w not in self.word2id:\n            self.word2id[w] = self.n_words\n            self.id2word[self.n_words] = w\n            self.n_words += 1\n        return self.word2id[w]\n\n\n","fe86de66":"vocab = Vocab()\ndef prepare_data(data, vocab, input_field):\n    data_sequences = []\n\n    for _, row in data.iterrows():\n        words = row[input_field].split()\n        tags = row[\"label\"]\n        word_ids = torch.tensor(vocab.index_words(words), dtype=torch.long).to(DEVICE)\n        tag_ids = torch.tensor(vocab.index_tags(tags), dtype=torch.long).to(DEVICE)\n        data_sequences.append([word_ids, tag_ids])\n\n    return data_sequences, vocab\n\n","23590f47":"#Create data sequnce\n\nsequences, vocab = prepare_data(df, vocab, \"titletext\")\nx = [i[0] for i in sequences]\ny = [i[1] for i in sequences]\n\n# pad sentences to use batches\npadded_x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\nx = [i for i in padded_x]","4892bd37":"# Number of unique words\n\nprint(vocab.n_words)","d47ebe4f":"# Split data to train, validation and test\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\ntest_sequences = list(zip(x_test,y_test))\ntest_sequences = [list(x) for x in test_sequences]\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\ntrain_sequences = list(zip(x_train,y_train))\ntrain_sequences = [list(x) for x in train_sequences]\nval_sequences = list(zip(x_val,y_val))\nval_sequences = [list(x) for x in val_sequences]","a74fc1c4":"#LSTM class, architecture and forward\n\nclass LSTMNERNet(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, n_layers, directions, dropout):\n        super(LSTMNERNet, self).__init__()\n        self.input_size = input_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.directions = directions\n        self.bidirectional = True if directions == 2 else False\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, bidirectional=self.bidirectional, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size*directions, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_sentence):\n        num_dimensions = len(input_sentence)\n        sentence = input_sentence.clone().detach().to(DEVICE)\n        embedded = self.embedding(sentence)\n        packed_output, (hidden, cell) = self.lstm(embedded.view(num_dimensions, sentence.size()[1],embedding_size))\n        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n        output = self.dropout(self.fc1(hidden))\n        output = self.out(output)\n\n        return output","32f1bd4a":"def evaluate(eval_sequences, batch_size):\n  eval_loader = DataLoader(eval_sequences, batch_size=batch_size, shuffle=True)\n  preds = []\n  tags = []\n  with torch.no_grad():\n      for words, tag in eval_loader:\n          preds.append(model(words).argmax(dim=1).cpu().data.numpy()) \n          tags.append(tag.cpu().data.numpy())\n  preds = np.concatenate(preds).ravel()\n  tags = np.concatenate(tags).ravel()\n  accuracy = (preds == tags).sum() \/ len(tags) * 100\n  return accuracy","f5ce0db3":"def train_loop(model, n_epochs, batch_size, train_set, test_set):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n\n    for e in range(1, n_epochs + 1):\n        count = 0\n        for words, tags in  iter(train_loader):\n            model.zero_grad()\n            seq_len = len(words)\n            sentence_loss = 0\n            output = model(words)\n            sentence_loss = criterion(output, tags)\n            sentence_loss.backward()\n            optimizer.step()\n            if count % 100 == 0:\n                print(f\"Epoch #{e}, Batch: {count},  Loss: {sentence_loss}\")\n            count += 1\n\n\n        train_accuracy = evaluate(train_set, batch_size)\n        print(f\"Epoch {e}, Training Accuracy: {train_accuracy}%\")\n\n        test_accuracy = evaluate(test_set, batch_size)\n        print(f\"Epoch {e}, Validation Accuracy: {test_accuracy}%\")","94a760e2":"# See what more important, title or text\n\n# PreProcess data\ntitle_sequences, vocab = prepare_data(df, vocab, \"title\")\ntitle_x = [i[0] for i in title_sequences]\ntitle_y = [i[1] for i in title_sequences]\n\n# pad sentences to use batches\ntitle_padded_x = torch.nn.utils.rnn.pad_sequence(title_x, batch_first=True)\ntitle_x = [i for i in title_padded_x]\n\ntitle_x_train, title_x_test, title_y_train, title_y_test = train_test_split(title_x, title_y, test_size=0.2, random_state=42)\ntitle_test_sequences = list(zip(title_x_test,title_y_test))\ntitle_test_sequences = [list(x) for x in title_test_sequences]\ntitle_train_sequences = list(zip(title_x_train,title_y_train))\ntitle_train_sequences = [list(x) for x in title_train_sequences]\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [300, 500]\noutput_size = len(vocab.id2tag)\nn_layers = [2,3]\ndirections = [2]\nn_epochs = 10\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\n#Only title\nprint(\"Train only on Titles\")\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, title_train_sequences, title_test_sequences)\n                train_accuracy = evaluate(title_train_sequences, batch_size)\n                test_accuracy = evaluate(title_test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)\n\n# PreProcess data\ntext_sequences, vocab = prepare_data(df, vocab, \"text\")\ntext_x = [i[0] for i in text_sequences]\ntext_y = [i[1] for i in text_sequences]\n\n# pad sentences to use batches\ntext_padded_x = torch.nn.utils.rnn.pad_sequence(text_x, batch_first=True)\ntext_x = [i for i in text_padded_x]\n\ntext_x_train, text_x_test, text_y_train, text_y_test = train_test_split(text_x, text_y, test_size=0.2, random_state=42)\ntext_test_sequences = list(zip(text_x_test,text_y_test))\ntext_test_sequences = [list(x) for x in text_test_sequences]\ntext_train_sequences = list(zip(text_x_train,text_y_train))\ntext_train_sequences = [list(x) for x in text_train_sequences]\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [500]\noutput_size = len(vocab.id2tag)\nn_layers = [2]\ndirections = [2]\nn_epochs = 6\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\n\n#Only on text\nprint(\"Train only on Text\")\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, text_train_sequences, text_test_sequences)\n                train_accuracy = evaluate(text_train_sequences, batch_size)\n                test_accuracy = evaluate(text_test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)","8cffb0fb":"# Try diffrent Hyper params for the model\n\ninput_size = vocab.n_words\nembedding_size = 300\nhidden_sizes = [300, 500]\noutput_size = len(vocab.id2tag)\nn_layers = [2,3]\ndirections = [2]\nn_epochs = 10\ndropouts = [0.2]\nbatch_sizes = [32]\nresult_df = pd.DataFrame([])\n\nfor hidden_size in hidden_sizes:\n    for layers in n_layers:\n        for direction in directions:\n            for dropout in dropouts:\n              for batch_size in batch_sizes:\n                caption = f\"hidden_size - {hidden_size}, n_layers - {layers}, directions - {direction}, dropout {dropout}\"\n                print(caption)\n                model = LSTMNERNet(input_size, embedding_size, hidden_size, output_size, layers, direction, dropout)\n                train_loop(model, n_epochs, batch_size, train_sequences, test_sequences)\n                train_accuracy = evaluate(train_sequences, batch_size)\n                test_accuracy = evaluate(test_sequences, batch_size)\n                temp_df = pd.DataFrame([[train_accuracy, test_accuracy]], index=[caption], columns=[\"training_accuracy\", \"test_accuracy\"])\n                result_df = result_df.append(temp_df)","d1f100b7":"result_df","dc057540":"raw_data_path = '..\/input\/real-and-fake-news-dataset\/news.csv'\ndestination_folder = '.\/'\nsource_folder = '..\/input\/real-and-fake-news-dataset'\n\ntrain_test_ratio = 0.10\ntrain_valid_ratio = 0.80\n\nfirst_n_words = 200","ea550f67":"def trim_string(x):\n\n    x = x.split(maxsplit=first_n_words)\n    x = ' '.join(x[:first_n_words])\n\n    return x","aa9abc9f":"# Read raw data\ndf_raw = pd.read_csv(raw_data_path)\n\n# Prepare columns\ndf_raw['label'] = (df_raw['label'] == 'FAKE').astype('int')\ndf_raw['titletext'] = df_raw['title'] + \". \" + df_raw['text']\ndf_raw = df_raw.reindex(columns=['label', 'title', 'text', 'titletext'])\n\n# Drop rows with empty text\ndf_raw.drop( df_raw[df_raw.text.str.len() < 5].index, inplace=True)\n\n# Trim text and titletext to first_n_words\ndf_raw['text'] = df_raw['text'].apply(trim_string)\ndf_raw['titletext'] = df_raw['titletext'].apply(trim_string) \n\n# Split according to label\ndf_real = df_raw[df_raw['label'] == 0]\ndf_fake = df_raw[df_raw['label'] == 1]\n\n# Train-test split\ndf_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)\ndf_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)\n\n# Train-valid split\ndf_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)\ndf_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)\n\n# Concatenate splits of different labels\ndf_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\ndf_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\ndf_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n\n# Write preprocessed data\ndf_train.to_csv(destination_folder + '\/train.csv', index=False)\ndf_valid.to_csv(destination_folder + '\/valid.csv', index=False)\ndf_test.to_csv(destination_folder + '\/test.csv', index=False)","5b454569":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Model parameter\nMAX_SEQ_LEN = 128\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\n# Fields\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\nfields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n\n# TabularDataset\n\ntrain, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',\n                                           test='test.csv', format='CSV', fields=fields, skip_header=True)\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=DEVICE, train=True, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),\n                            device=DEVICE, train=True, sort=True, sort_within_batch=True)\ntest_iter = Iterator(test, batch_size=16, device=DEVICE, train=False, shuffle=False, sort=False)\n","098febc6":"class BERT(nn.Module):\n\n    def __init__(self):\n        super(BERT, self).__init__()\n\n        options_name = \"bert-base-uncased\"\n        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n\n    def forward(self, text, label):\n        loss, text_fea = self.encoder(text, labels=label)[:2]\n\n        return loss, text_fea","66a7640c":"# Training Function\n\ndef train(model,\n          optimizer,\n          criterion = nn.BCELoss(),\n          train_loader = train_iter,\n          valid_loader = valid_iter,\n          num_epochs = 5,\n          eval_every = len(train_iter) \/\/ 2,\n          best_valid_loss = float(\"Inf\")):\n    \n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n\n    # training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for (labels, title, text, titletext), _ in train_loader:\n            labels = labels.type(torch.LongTensor)           \n            labels = labels.to(DEVICE)\n            titletext = titletext.type(torch.LongTensor)  \n            titletext = titletext.to(DEVICE)\n            output = model(titletext, labels)\n            loss, _ = output\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update running values\n            running_loss += loss.item()\n            global_step += 1\n\n            # evaluation step\n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n\n                    # validation loop\n                    for (labels, title, text, titletext), _ in valid_loader:\n                        labels = labels.type(torch.LongTensor)           \n                        labels = labels.to(DEVICE)\n                        titletext = titletext.type(torch.LongTensor)  \n                        titletext = titletext.to(DEVICE)\n                        output = model(titletext, labels)\n                        loss, _ = output\n                        \n                        valid_running_loss += loss.item()\n\n                # evaluation\n                average_train_loss = running_loss \/ eval_every\n                average_valid_loss = valid_running_loss \/ len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n\n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}\/{}], Step [{}\/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n    print('Finished Training!')\n\nmodel = BERT().to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\ntrain(model=model, optimizer=optimizer)","12ba5293":"# Evaluation Function\n\ndef evaluate(model, test_loader):\n    y_pred = []\n    y_true = []\n\n    model.eval()\n    with torch.no_grad():\n        for (labels, title, text, titletext), _ in test_loader:\n\n                labels = labels.type(torch.LongTensor)           \n                labels = labels.to(DEVICE)\n                titletext = titletext.type(torch.LongTensor)  \n                titletext = titletext.to(DEVICE)\n                output = model(titletext, labels)\n\n                _, output = output\n                y_pred.extend(torch.argmax(output, 1).tolist())\n                y_true.extend(labels.tolist())\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n    \n    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n\n    ax.set_title('Confusion Matrix')\n\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n\n    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n\n\nevaluate(model, test_iter)","81b4ca02":"In This note book we train two models:\n* LSTM (Long Short-Term Memory)\n* BERT (Bidirectional Encoder Representations from Transformers)\n\nThe models will predict if a news report is real or fake","2688ea86":"* This project was created in collaboration with Tomer Segall","97313679":"# Real or Fake using LSTM & BERT"}}