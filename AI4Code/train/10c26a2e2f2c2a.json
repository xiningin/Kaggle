{"cell_type":{"8752a958":"code","c052d8df":"code","7d68892e":"code","0f0b5553":"code","5b30cc3b":"code","4a60368e":"code","b1ee75f9":"code","96511dfe":"code","c37ed5db":"code","12534764":"code","f986d923":"code","3b12c991":"code","4a3e709a":"code","f656643d":"code","6bbdc20d":"code","8a57d8ec":"code","7135ffd8":"code","5aa8892c":"code","4d3308c6":"code","cdb4e9f8":"code","e8beee2d":"code","066f52b1":"code","e0e0bac4":"code","f611094f":"code","135fcfb8":"code","82ad9c6d":"code","13878d5d":"code","7bcfcffa":"code","9d209135":"code","bf1b4740":"code","97d05d71":"code","bb8dfec1":"code","f7478c19":"code","2ddb74dd":"code","94024a4a":"code","97b3a7c7":"code","95e1d6a7":"code","90e53cbf":"code","a91b70a2":"code","222f63b3":"code","87903ae7":"code","133d679a":"code","43adbfa8":"code","a4a692ae":"code","34e77ef0":"code","55c28bd9":"code","2e5474e8":"code","7c287955":"code","4c784a46":"code","98e4875c":"code","08acf27e":"code","7ebcefbb":"code","49bbba7c":"code","0af0a8d1":"code","6430c846":"code","15a1d6bd":"code","5a0798e4":"code","3703503d":"code","42079994":"code","cd22941a":"code","3635c3de":"code","bdfb1583":"code","2fa5a090":"code","4d0a5612":"code","70f8ae02":"code","faa8bf17":"code","c4ef8f17":"code","6f662404":"code","430321be":"code","5f4da5c5":"code","c28261a5":"code","0de41bca":"code","26f1c194":"code","be37d061":"code","2b246ab2":"code","c54389af":"code","c97a7785":"code","1908ceb4":"code","d24eb380":"code","825fd3da":"code","1951b96b":"code","e2885f01":"code","08d154b2":"code","59103eb9":"markdown","90e9938b":"markdown","75f0068a":"markdown","1ad1c81b":"markdown","c83a4f42":"markdown","7c637c75":"markdown","4b1e10ed":"markdown","224be4fb":"markdown","495bca54":"markdown","991ff1b4":"markdown","274e49b7":"markdown","8c043824":"markdown","90b96c92":"markdown","f3fae46d":"markdown","ed3c2629":"markdown","1a0ce250":"markdown","a82c8a9b":"markdown","51c75ca7":"markdown","4f609c8f":"markdown","9bd194f3":"markdown","afb9e130":"markdown"},"source":{"8752a958":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c052d8df":"import numpy as np\nimport pandas as pd\nimport random\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score as r2\nfrom sklearn.model_selection import KFold, GridSearchCV\n\nfrom datetime import datetime\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7d68892e":"import warnings\nwarnings.filterwarnings('ignore')","0f0b5553":"matplotlib.rcParams.update({'font.size': 14})","5b30cc3b":"def evaluate_preds(train_true_values, train_pred_values, test_true_values, test_pred_values):\n    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))\n    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))\n    \n    plt.figure(figsize=(18,10))\n    \n    plt.subplot(121)\n    sns.scatterplot(x=train_pred_values, y=train_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Train sample prediction')\n    \n    plt.subplot(122)\n    sns.scatterplot(x=test_pred_values, y=test_true_values)\n    plt.xlabel('Predicted values')\n    plt.ylabel('True values')\n    plt.title('Test sample prediction')\n\n    plt.show()","4a60368e":"TRAIN_DATASET_PATH = '..\/input\/real-estate-price-prediction-moscow\/train.csv'\nTEST_DATASET_PATH = '..\/input\/real-estate-price-prediction-moscow\/test.csv'","b1ee75f9":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)","96511dfe":"train_df.shape[1] - 1 == test_df.shape[1]","c37ed5db":"train_df.dtypes","12534764":"train_df['Id'] = train_df['Id'].astype(str)\ntrain_df['DistrictId'] = train_df['DistrictId'].astype(str)","f986d923":"plt.figure(figsize = (16, 8))\n\ntrain_df['Price'].hist(bins=30)\nplt.ylabel('Count')\nplt.xlabel('Price')\n\nplt.title('Target distribution')\nplt.show()","3b12c991":"correlation = train_df.corrwith(train_df['Price']).sort_values(ascending=False)\ncorrelation.drop('Price', inplace=True)\n\nplt.figure(figsize = (16, 8))\nplt.bar(correlation.index, correlation)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Correlation', fontsize=15)\nplt.title('Feature correlation', fontsize=15)\nplt.show()","4a3e709a":"train_df.select_dtypes(include='object').columns.tolist()","f656643d":"train_df['DistrictId'].value_counts()","6bbdc20d":"train_df['Ecology_2'].value_counts()","8a57d8ec":"train_df['Ecology_3'].value_counts()","7135ffd8":"train_df['Shops_2'].value_counts()","5aa8892c":"train_df['Rooms'].value_counts()","4d3308c6":"train_df['Rooms_outlier'] = 0\ntrain_df.loc[(train_df['Rooms'] == 0) | (train_df['Rooms'] >= 6), 'Rooms_outlier'] = 1\ntrain_df.head()","cdb4e9f8":"train_df.loc[train_df['Rooms'] == 0, 'Rooms'] = 1\ntrain_df.loc[train_df['Rooms'] >= 6, 'Rooms'] = train_df['Rooms'].median()","e8beee2d":"train_df['Rooms'].value_counts()","066f52b1":"train_df['KitchenSquare'].value_counts()\n","e0e0bac4":"condition = (train_df['KitchenSquare'].isna()) \\\n             | (train_df['KitchenSquare'] > train_df['KitchenSquare'].quantile(.975))\n        \ntrain_df.loc[condition, 'KitchenSquare'] = train_df['KitchenSquare'].median()\n\ntrain_df.loc[train_df['KitchenSquare'] < 3, 'KitchenSquare'] = 3","f611094f":"train_df['HouseFloor'].sort_values().unique()","135fcfb8":"train_df['Floor'].sort_values().unique()","82ad9c6d":"(train_df['Floor'] > train_df['HouseFloor']).sum()","13878d5d":"train_df['HouseFloor_outlier'] = 0\ntrain_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\ntrain_df.loc[train_df['Floor'] > train_df['HouseFloor'], 'HouseFloor_outlier'] = 1","7bcfcffa":"train_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor'] = train_df['HouseFloor'].median()","9d209135":"floor_outliers = train_df.loc[train_df['Floor'] > train_df['HouseFloor']].index\nfloor_outliers","bf1b4740":"train_df.loc[floor_outliers, 'Floor'] = train_df.loc[floor_outliers, 'HouseFloor']\\\n                                                .apply(lambda x: random.randint(1, x))","97d05d71":"(train_df['Floor'] > train_df['HouseFloor']).sum()","bb8dfec1":"train_df['HouseYear'].sort_values(ascending=False)","f7478c19":"train_df.loc[train_df['HouseYear'] > 2020, 'HouseYear'] = 2020","2ddb74dd":"train_df.isna().sum()","94024a4a":"train_df['LifeSquare_nan'] = train_df['LifeSquare'].isna() * 1\n\ncondition = (train_df['LifeSquare'].isna()) \\\n             & (~train_df['Square'].isna()) \\\n             & (~train_df['KitchenSquare'].isna())\n        \ntrain_df.loc[condition, 'LifeSquare'] = train_df.loc[condition, 'Square'] \\\n                                            - train_df.loc[condition, 'KitchenSquare'] - 3\n","97b3a7c7":"train_df.drop('Healthcare_1', axis=1, inplace=True)","95e1d6a7":"class DataPreprocessing:\n    \"\"\"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\"\"\"\n\n    def __init__(self):\n        \"\"\"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043b\u0430\u0441\u0441\u0430\"\"\"\n        self.medians = None\n        self.kitchen_square_quantile = None\n        \n    def fit(self, X):\n        \"\"\"\u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\"\"\"       \n        # \u0420\u0430\u0441\u0447\u0435\u0442 \u043c\u0435\u0434\u0438\u0430\u043d\n        self.medians = X.median()\n        self.kitchen_square_quantile = X['KitchenSquare'].quantile(.975)\n    \n    def transform(self, X):\n        \"\"\"\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\"\"\"\n\n        # Rooms\n        X['Rooms_outlier'] = 0\n        X.loc[(X['Rooms'] == 0) | (X['Rooms'] >= 6), 'Rooms_outlier'] = 1\n        \n        X.loc[X['Rooms'] == 0, 'Rooms'] = 1\n        X.loc[X['Rooms'] >= 6, 'Rooms'] = self.medians['Rooms']\n        \n        # KitchenSquare\n        condition = (X['KitchenSquare'].isna()) \\\n                    | (X['KitchenSquare'] > self.kitchen_square_quantile)\n        \n        X.loc[condition, 'KitchenSquare'] = self.medians['KitchenSquare']\n\n        X.loc[X['KitchenSquare'] < 3, 'KitchenSquare'] = 3\n        \n        # HouseFloor, Floor\n        X['HouseFloor_outlier'] = 0\n        X.loc[X['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n        X.loc[X['Floor'] > X['HouseFloor'], 'HouseFloor_outlier'] = 1\n        \n        X.loc[X['HouseFloor'] == 0, 'HouseFloor'] = self.medians['HouseFloor']\n        \n        floor_outliers = X.loc[X['Floor'] > X['HouseFloor']].index\n        X.loc[floor_outliers, 'Floor'] = X.loc[floor_outliers, 'HouseFloor']\\\n                                            .apply(lambda x: random.randint(1, x))\n        \n        # HouseYear\n        current_year = datetime.now().year\n        \n        X['HouseYear_outlier'] = 0\n        X.loc[X['HouseYear'] > current_year, 'HouseYear_outlier'] = 1\n        \n        X.loc[X['HouseYear'] > current_year, 'HouseYear'] = current_year\n        \n        # Healthcare_1\n        if 'Healthcare_1' in X.columns:\n            X.drop('Healthcare_1', axis=1, inplace=True)\n            \n        # LifeSquare\n        X['LifeSquare_nan'] = X['LifeSquare'].isna() * 1\n        condition = (X['LifeSquare'].isna()) & \\\n                      (~X['Square'].isna()) & \\\n                      (~X['KitchenSquare'].isna())\n        \n        X.loc[condition, 'LifeSquare'] = X.loc[condition, 'Square'] - X.loc[condition, 'KitchenSquare'] - 3\n        \n        \n        X.fillna(self.medians, inplace=True)\n        \n        return X","90e53cbf":"binary_to_numbers = {'A': 0, 'B': 1}\n\ntrain_df['Ecology_2'] = train_df['Ecology_2'].replace(binary_to_numbers)\ntrain_df['Ecology_3'] = train_df['Ecology_3'].replace(binary_to_numbers)\ntrain_df['Shops_2'] = train_df['Shops_2'].replace(binary_to_numbers)","a91b70a2":"district_size = train_df['DistrictId'].value_counts().reset_index()\\\n                    .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n\ndistrict_size.head()","222f63b3":"train_df = train_df.merge(district_size, on='DistrictId', how='left')\ntrain_df.head()","87903ae7":"(train_df['DistrictSize'] > 100).value_counts()","133d679a":"train_df['IsDistrictLarge'] = (train_df['DistrictSize'] > 100).astype(int)","43adbfa8":"med_price_by_district = train_df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'Price':'median'})\\\n                            .rename(columns={'Price':'MedPriceByDistrict'})\n\nmed_price_by_district.head()","a4a692ae":"def floor_to_cat(X):\n\n    X['floor_cat'] = 0\n\n    X.loc[X['Floor'] <= 3, 'floor_cat'] = 1  \n    X.loc[(X['Floor'] > 3) & (X['Floor'] <= 5), 'floor_cat'] = 2\n    X.loc[(X['Floor'] > 5) & (X['Floor'] <= 9), 'floor_cat'] = 3\n    X.loc[(X['Floor'] > 9) & (X['Floor'] <= 15), 'floor_cat'] = 4\n    X.loc[X['Floor'] > 15, 'floor_cat'] = 5\n\n    return X\n\n\ndef floor_to_cat_pandas(X):\n    bins = [X['Floor'].min(), 3, 5, 9, 15, X['Floor'].max()]\n    X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n    \n    X['floor_cat'].fillna(-1, inplace=True)\n    return X\n\n\ndef year_to_cat(X):\n\n    X['year_cat'] = 0\n\n    X.loc[X['HouseYear'] <= 1941, 'year_cat'] = 1\n    X.loc[(X['HouseYear'] > 1941) & (X['HouseYear'] <= 1945), 'year_cat'] = 2\n    X.loc[(X['HouseYear'] > 1945) & (X['HouseYear'] <= 1980), 'year_cat'] = 3\n    X.loc[(X['HouseYear'] > 1980) & (X['HouseYear'] <= 2000), 'year_cat'] = 4\n    X.loc[(X['HouseYear'] > 2000) & (X['HouseYear'] <= 2010), 'year_cat'] = 5\n    X.loc[(X['HouseYear'] > 2010), 'year_cat'] = 6\n\n    return X\n\n\ndef year_to_cat_pandas(X):\n    bins = [X['HouseYear'].min(), 1941, 1945, 1980, 2000, 2010, X['HouseYear'].max()]\n    X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n    \n    X['year_cat'].fillna(-1, inplace=True)\n    return X","34e77ef0":"train_df = year_to_cat(train_df)\ntrain_df = floor_to_cat(train_df)\ntrain_df.head()","55c28bd9":"med_price_by_floor_year = train_df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n                                            rename(columns={'Price':'MedPriceByFloorYear'})\nmed_price_by_floor_year.head()","2e5474e8":"train_df = train_df.merge(med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\ntrain_df.head()","7c287955":"class FeatureGenetator():\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043d\u043e\u0432\u044b\u0445 \u0444\u0438\u0447\"\"\"\n    \n    def __init__(self):\n        self.DistrictId_counts = None\n        self.binary_to_numbers = None\n        self.med_price_by_district = None\n        self.med_price_by_floor_year = None\n        self.house_year_max = None\n        self.floor_max = None\n        self.house_year_min = None\n        self.floor_min = None\n        self.district_size = None\n        \n    def fit(self, X, y=None):\n        \n        X = X.copy()\n        \n        # Binary features\n        self.binary_to_numbers = {'A': 0, 'B': 1}\n        \n        # DistrictID\n        self.district_size = X['DistrictId'].value_counts().reset_index() \\\n                               .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n                \n        # Target encoding\n        ## District, Rooms\n        df = X.copy()\n        \n        if y is not None:\n            df['Price'] = y.values\n            \n            self.med_price_by_district = df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'Price':'median'})\\\n                                            .rename(columns={'Price':'MedPriceByDistrict'})\n            \n            self.med_price_by_district_median = self.med_price_by_district['MedPriceByDistrict'].median()\n            \n        ## floor, year\n        if y is not None:\n            self.floor_max = df['Floor'].max()\n            self.floor_min = df['Floor'].min()\n            self.house_year_max = df['HouseYear'].max()\n            self.house_year_min = df['HouseYear'].min()\n            df['Price'] = y.values\n            df = self.floor_to_cat(df)\n            df = self.year_to_cat(df)\n            self.med_price_by_floor_year = df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n                                            rename(columns={'Price':'MedPriceByFloorYear'})\n            self.med_price_by_floor_year_median = self.med_price_by_floor_year['MedPriceByFloorYear'].median()\n        \n\n        \n    def transform(self, X):\n        \n        # Binary features\n        X['Ecology_2'] = X['Ecology_2'].map(self.binary_to_numbers)  # self.binary_to_numbers = {'A': 0, 'B': 1}\n        X['Ecology_3'] = X['Ecology_3'].map(self.binary_to_numbers)\n        X['Shops_2'] = X['Shops_2'].map(self.binary_to_numbers)\n        \n        # DistrictId, IsDistrictLarge\n        X = X.merge(self.district_size, on='DistrictId', how='left')\n        \n        X['new_district'] = 0\n        X.loc[X['DistrictSize'].isna(), 'new_district'] = 1\n        \n        X['DistrictSize'].fillna(5, inplace=True)\n        \n        X['IsDistrictLarge'] = (X['DistrictSize'] > 100).astype(int)\n        \n        # More categorical features\n        X = self.floor_to_cat(X)  # + \u0441\u0442\u043e\u043b\u0431\u0435\u0446 floor_cat\n        X = self.year_to_cat(X)   # + \u0441\u0442\u043e\u043b\u0431\u0435\u0446 year_cat\n        \n        # Target encoding\n        if self.med_price_by_district is not None:\n            X = X.merge(self.med_price_by_district, on=['DistrictId', 'Rooms'], how='left')\n            X['MedPriceByDistrict'].fillna(self.med_price_by_district_median, inplace=True)\n            \n        if self.med_price_by_floor_year is not None:\n            X = X.merge(self.med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n            X['MedPriceByFloorYear'].fillna(self.med_price_by_floor_year_median, inplace=True)\n        \n        return X\n    \n    def floor_to_cat(self, X):\n        bins = [self.floor_min, 3, 5, 9, 15, self.floor_max]\n        X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n\n        X['floor_cat'].fillna(-1, inplace=True)\n        return X\n     \n    def year_to_cat(self, X):\n        bins = [self.house_year_min, 1941, 1945, 1980, 2000, 2010, self.house_year_max]\n        X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n\n        X['year_cat'].fillna(-1, inplace=True)\n        return X","4c784a46":"def df_fix_house_year_manual(df):\n    df.loc[df['HouseYear'] == 20052011, 'HouseYear'] = int((2005 + 2011) \/ 2)\n    df.loc[df['HouseYear'] == 4968, 'HouseYear'] = 1968\n    return df","98e4875c":"feature_names = ['Rooms', 'Square', 'LifeSquare', 'KitchenSquare', 'Floor', 'HouseFloor', 'HouseYear',\n                 'Ecology_1', 'Ecology_2', 'Ecology_3', 'Social_1', 'Social_2', 'Social_3',\n                 'Helthcare_2', 'Shops_1', 'Shops_2']\n\nnew_feature_names = ['Rooms_outlier', 'HouseFloor_outlier', 'HouseYear_outlier', 'LifeSquare_nan', 'DistrictSize',\n                     'new_district', 'IsDistrictLarge',  'MedPriceByDistrict', 'MedPriceByFloorYear']\n\ntarget_name = 'Price'","08acf27e":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering","7ebcefbb":"scaler = MinMaxScaler()","49bbba7c":"train_cluster = train_df.copy()","0af0a8d1":"train_cluster = df_fix_house_year_manual(train_cluster)","6430c846":"train_cluster = df_fix_house_year_manual(train_cluster)","15a1d6bd":"train_cluster_scaled = pd.DataFrame(scaler.fit_transform(\n    train_cluster.loc[:, ['HouseYear', 'Price']]), columns=['HouseYear', 'Price'])","5a0798e4":"inertias = []\n\nfor i in range(2, 10):\n    temp_model = KMeans(n_clusters=i, random_state=100)\n    temp_model.fit(train_cluster_scaled)\n    temp_inertia = temp_model.inertia_\n    inertias.append(temp_inertia)\n\nplt.plot(range(2, 10), inertias)\nplt.title('Inertias')\n\nplt.show()","3703503d":"plt.scatter(train_cluster_scaled['HouseYear'], train_cluster_scaled['Price'])\nplt.xlabel('HouseYear')\nplt.ylabel('Price')\nplt.show()","42079994":"kmeans_model = KMeans(n_clusters=5, random_state=100)","cd22941a":"train_labels = kmeans_model.fit_predict(train_cluster_scaled)","3635c3de":"plt.scatter(train_cluster_scaled['HouseYear'],\n            train_cluster_scaled['Price'], c=train_labels)\n\nplt.xlabel('HouseYear')\nplt.ylabel('Price')\n\nplt.title('Train data')","bdfb1583":"agglomerative_clustering_model = AgglomerativeClustering(n_clusters=5)","2fa5a090":"train_cluster['cluster_year'] = agglomerative_clustering_model.fit_predict(\n    train_cluster_scaled)","4d0a5612":"plt.scatter(train_cluster['HouseYear'],\n            train_cluster['Price'], c= train_cluster['cluster_year'])\nplt.xlabel('HouseYear')\nplt.ylabel('Price')\nplt.title('Train')","70f8ae02":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)\n\nX = train_df.drop(columns=target_name)\ny = train_df[target_name]\n","faa8bf17":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=21)","c4ef8f17":"preprocessor = DataPreprocessing()\npreprocessor.fit(X_train)\n\nX_train = preprocessor.transform(X_train)\nX_valid = preprocessor.transform(X_valid)\ntest_df = preprocessor.transform(test_df)\n\nX_train.shape, X_valid.shape, test_df.shape","6f662404":"features_gen = FeatureGenetator()\nfeatures_gen.fit(X_train, y_train)\n\nX_train = features_gen.transform(X_train)\nX_valid = features_gen.transform(X_valid)\ntest_df = features_gen.transform(test_df)\n\nX_train.shape, X_valid.shape, test_df.shape","430321be":"X_train = X_train[feature_names + new_feature_names]\nX_valid = X_valid[feature_names + new_feature_names]\ntest_df = test_df[feature_names + new_feature_names]","5f4da5c5":"X_train.isna().sum().sum(), X_valid.isna().sum().sum(), test_df.isna().sum().sum()","c28261a5":"rf_model = RandomForestRegressor(n_estimators=500, max_depth=14, max_features=10, random_state=42, criterion='mse')\nrf_model.fit(X_train, y_train)\n","0de41bca":"y_train_preds = rf_model.predict(X_train)\ny_test_preds = rf_model.predict(X_valid)\n\nevaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)","26f1c194":"from sklearn.ensemble import GradientBoostingRegressor","be37d061":"final_model = GradientBoostingRegressor(criterion='mse',\n                                        max_depth=4,\n                                        random_state=42,  \n                                        n_estimators=300)\n\nfinal_model.fit(X_train, y_train)","2b246ab2":"y_train_preds = final_model.predict(X_train)\ny_test_preds = final_model.predict(X_valid)\n\nevaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)","c54389af":"cv_score = cross_val_score(final_model, X_train, y_train, scoring='r2', cv=KFold(n_splits=5, shuffle=True, random_state=42))\ncv_score","c97a7785":"cv_score.mean()","1908ceb4":"test_df.shape","d24eb380":"test_df","825fd3da":"submit = pd.read_csv('..\/input\/real-estate-price-prediction-moscow\/sample_submission.csv')\nsubmit.head()","1951b96b":"predictions = final_model.predict(test_df)\npredictions","e2885f01":"submit['Price'] = predictions\nsubmit.head()","08d154b2":"submit.to_csv('final_model.csv', index=False)","59103eb9":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435: \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 train.csv, \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0446\u0435\u043d \u043d\u0430 \u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u044c (\u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b). \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0446\u0435\u043d\u044b \u0434\u043b\u044f \u043a\u0432\u0430\u0440\u0442\u0438\u0440 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430 test.csv.\n\n\u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f: Price\n\n\u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430: R2\n\n\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430: MSE - \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430 (sklearn.metrics.mean_squared_error)","90e9938b":"**\u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0442\u0438\u043f\u043e\u0432**","75f0068a":"**\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438**","1ad1c81b":"**\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432**","c83a4f42":"**MedPriceByDistrict**","7c637c75":"**Cluster**","4b1e10ed":"**MedPriceByFloorYear**","224be4fb":"**\u041e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**","495bca54":" **\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 train \u0438 test**","991ff1b4":"**HouseYear**","274e49b7":"**Function**","8c043824":"**HouseFloor, Floor**","90b96c92":"**\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432**","f3fae46d":"**\u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f**","ed3c2629":"### Load Dataset","1a0ce250":"**\u041a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f**","a82c8a9b":"### \u041a\u0443\u0440\u0441\u043e\u0432\u043e\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0434\u043b\u044f \u043a\u0443\u0440\u0441\u0430 \"Python \u0434\u043b\u044f Data Science\"","51c75ca7":"**KitchenSquare**","4f609c8f":"**Healthcare_1**","9bd194f3":"**Load data**","afb9e130":"**\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438**"}}