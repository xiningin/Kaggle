{"cell_type":{"9edc6345":"code","1fb54fb4":"code","dfec709e":"code","4711e315":"code","3e92b541":"code","592a5850":"code","10c48166":"code","44c8cd14":"code","0f66f101":"code","5a1d4da7":"code","d05452f0":"code","d92ca23f":"code","cf307aa1":"code","b654b7c2":"code","c72e16e2":"code","8ca1e52b":"code","f6a8023e":"code","0ab118fe":"code","9a11eaaa":"code","673babde":"code","19a34511":"code","461d62bd":"code","d25d793f":"code","35508cc9":"code","0b62d281":"code","457bc36c":"code","43f3f85d":"code","26cefa79":"code","9671e788":"code","34cbc09e":"code","158a548d":"code","62932ddd":"code","65cb19f1":"code","60e6fd2b":"code","ec7a2c4d":"code","d67ef9f6":"code","5c5fa40d":"code","f779ab07":"code","50fe4281":"code","3fc820ed":"code","c30aadb0":"code","fdd3bfb4":"code","504bc7ea":"code","b67e66c2":"code","43b903d4":"code","02f79f8c":"code","94f9e3ce":"code","6c6939a0":"code","8f4ea91a":"code","8084b420":"code","b5ff65ae":"code","630b8d2a":"code","a93af06a":"code","a7ca23b1":"code","f6168a9c":"code","6dfb13fc":"code","34621570":"code","ce955698":"code","6655e54f":"code","3e774078":"code","82e13cc0":"code","2e7ff41c":"code","d0916076":"code","20c19c84":"code","7421fa02":"code","07973199":"code","bb3e0a57":"code","c9386e39":"code","667085cb":"code","d8c5d8a2":"code","d1ecd5dd":"code","0fdb0036":"code","43c8e0c0":"code","ba16d105":"code","cae5b8e9":"code","fb10387f":"code","84bc2435":"code","5db137e2":"code","3f044b0a":"code","c2a4e45e":"code","de30d1ae":"code","4c6c2532":"code","63bf02da":"code","43564e35":"code","79b8ed67":"code","3720d3a2":"code","973d87ab":"code","d12d7b4a":"code","32fa2d6e":"code","d3eb0a24":"code","41b7d551":"code","8f16065a":"code","e3284fd6":"code","557849f6":"code","04d92b9b":"code","f58ecd6f":"code","cfc0d1e6":"code","b00f76b2":"code","6120b431":"code","f3f93446":"markdown","c06a5eb7":"markdown","d33daf3a":"markdown","5f5c07a0":"markdown","9407dbba":"markdown","5b5a62c3":"markdown","8727e9d9":"markdown","c2a01cfb":"markdown","7f491b29":"markdown","1455e6a7":"markdown","687f167d":"markdown","0a6c4ae2":"markdown","59edd62e":"markdown","bcc75062":"markdown","8d5f81b8":"markdown","6738b1dc":"markdown","29df924f":"markdown","2a0892e1":"markdown","2ae68c23":"markdown","2577d9ec":"markdown","58e485c0":"markdown","51895432":"markdown","3ea1327a":"markdown","dbbfa779":"markdown","79b26be1":"markdown","cc87b160":"markdown","0eb1176d":"markdown","23b0b76a":"markdown","8873a4de":"markdown","ba8ae0e6":"markdown","85dd5f25":"markdown","ebd67424":"markdown","b7093d6b":"markdown","dd5e50b4":"markdown","1cbe95b1":"markdown","4dfae90d":"markdown","05c4671e":"markdown","e78223c1":"markdown","4a12dcb7":"markdown","16ff545a":"markdown"},"source":{"9edc6345":"from IPython.core.display import display, HTML\ndisplay(HTML('<style>.container {width:98% !important;}<\/style>'))","1fb54fb4":"import numpy as np\nimport pandas as pd\n\nimport random\nrandom.seed(28)\nnp.random.seed(28)\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\nimport os\nimport copy\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nimport time\nfrom collections import Counter\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport time\nimport datetime\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\npd.set_option('max_rows', 500)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 1000)\nnp.random.seed(566)\npd.set_option('display.max_rows', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:20,.2f}'.format)\npd.set_option('display.max_colwidth', -1)","dfec709e":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))","4711e315":"train=pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\",low_memory=True)\ntrain.set_index(\"encounter_id\",inplace=True)\ntest=pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\",low_memory=True)\ntest.set_index(\"encounter_id\",inplace=True)\ndictionary=pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\nsolution_template = pd.read_csv(\"..\/input\/widsdatathon2021\/SolutionTemplateWiDS2021.csv\")\nsamplesubmission = pd.read_csv(\"..\/input\/widsdatathon2021\/SampleSubmissionWiDS2021.csv\")","3e92b541":"print('train ' , train.shape)\nprint('test ' , test.shape)\nprint('samplesubmission ' , samplesubmission.shape)\nprint('solution_template ' , solution_template.shape)\nprint('dictionary ' , dictionary.shape)","592a5850":"dico=pd.DataFrame(dictionary.T.head(6))\ndico.columns=list(dico.loc[dico.index == 'Variable Name'].unstack())\ndico = dico.loc[dico.index != 'Variable Name']\ndico.columns\ntrain_stat = pd.DataFrame(train.describe())\ntrain_stat2 = pd.concat([dico,train_stat],axis=0)\ntrain_stat2.head(20)","10c48166":"train_stat2.T.head(250)","44c8cd14":"print(train.shape)\nprint(test.shape)","0f66f101":"list1=[\"encounter_id\", \"hospital_id\", \"age\", \"elective_surgery\", \"height\", \"icu_id\", \"pre_icu_los_days\", \"readmission_status\", \"weight\", \n      \"albumin_apache\", \"apache_post_operative\", \"arf_apache\", \"bilirubin_apache\", \"bun_apache\", \"creatinine_apache\", \"fio2_apache\", \n      \"gcs_eyes_apache\", \"gcs_motor_apache\", \"gcs_unable_apache\", \"gcs_verbal_apache\", \"glucose_apache\", \"heart_rate_apache\", \"hematocrit_apache\",\n      \"intubated_apache\", \"map_apache\", \"paco2_apache\", \"paco2_for_ph_apache\", \"pao2_apache\", \"ph_apache\", \"resprate_apache\", \"sodium_apache\", \n      \"temp_apache\", \"urineoutput_apache\", \"ventilated_apache\", \"wbc_apache\", \"d1_diasbp_invasive_max\", \"d1_diasbp_invasive_min\", \n      \"d1_diasbp_max\", \"d1_diasbp_min\", \"d1_diasbp_noninvasive_max\", \"d1_diasbp_noninvasive_min\", \"d1_heartrate_max\", \"d1_heartrate_min\", \n      \"d1_mbp_invasive_max\", \"d1_mbp_invasive_min\", \"d1_mbp_max\", \"d1_mbp_min\", \"d1_mbp_noninvasive_max\", \"d1_mbp_noninvasive_min\", \n      \"d1_resprate_max\", \"d1_resprate_min\", \"d1_spo2_max\", \"d1_spo2_min\", \"d1_sysbp_invasive_max\", \"d1_sysbp_invasive_min\", \"d1_sysbp_max\", \n      \"d1_sysbp_min\", \"d1_sysbp_noninvasive_max\", \"d1_sysbp_noninvasive_min\", \"d1_temp_max\", \"d1_temp_min\", \"h1_diasbp_invasive_max\", \n      \"h1_diasbp_invasive_min\", \"h1_diasbp_max\", \"h1_diasbp_min\", \"h1_diasbp_noninvasive_max\", \"h1_diasbp_noninvasive_min\", \"h1_heartrate_max\", \n      \"h1_heartrate_min\", \"h1_mbp_invasive_max\", \"h1_mbp_invasive_min\", \"h1_mbp_max\", \"h1_mbp_min\", \"h1_mbp_noninvasive_max\", \n      \"h1_mbp_noninvasive_min\", \"h1_resprate_max\", \"h1_resprate_min\", \"h1_spo2_max\", \"h1_spo2_min\", \"h1_sysbp_invasive_max\", \n      \"h1_sysbp_invasive_min\", \"h1_sysbp_max\", \"h1_sysbp_min\", \"h1_sysbp_noninvasive_max\", \"h1_sysbp_noninvasive_min\", \"h1_temp_max\", \n      \"h1_temp_min\", \"d1_albumin_max\", \"d1_albumin_min\", \"d1_bilirubin_max\", \"d1_bilirubin_min\", \"d1_bun_max\", \"d1_bun_min\", \"d1_calcium_max\", \n      \"d1_calcium_min\", \"d1_creatinine_max\", \"d1_creatinine_min\", \"d1_glucose_max\", \"d1_glucose_min\", \"d1_hco3_max\", \"d1_hco3_min\", \n      \"d1_hemaglobin_max\", \"d1_hemaglobin_min\", \"d1_hematocrit_max\", \"d1_hematocrit_min\", \"d1_inr_max\", \"d1_inr_min\", \"d1_lactate_max\", \n      \"d1_lactate_min\", \"d1_platelets_max\", \"d1_platelets_min\", \"d1_potassium_max\", \"d1_potassium_min\", \"d1_sodium_max\", \"d1_sodium_min\", \n      \"d1_wbc_max\", \"d1_wbc_min\", \"h1_albumin_max\", \"h1_albumin_min\", \"h1_bilirubin_max\", \"h1_bilirubin_min\", \"h1_bun_max\", \"h1_bun_min\", \n      \"h1_calcium_max\", \"h1_calcium_min\", \"h1_creatinine_max\", \"h1_creatinine_min\", \"h1_glucose_max\", \"h1_glucose_min\", \"h1_hco3_max\", \n      \"h1_hco3_min\", \"h1_hemaglobin_max\", \"h1_hemaglobin_min\", \"h1_hematocrit_max\", \"h1_hematocrit_min\", \"h1_inr_max\", \"h1_inr_min\", \n      \"h1_lactate_max\", \"h1_lactate_min\", \"h1_platelets_max\", \"h1_platelets_min\", \"h1_potassium_max\", \"h1_potassium_min\", \"h1_sodium_max\", \n      \"h1_sodium_min\", \"h1_wbc_max\", \"h1_wbc_min\", \"d1_arterial_pco2_max\", \"d1_arterial_pco2_min\", \"d1_arterial_ph_max\", \"d1_arterial_ph_min\", \n      \"d1_arterial_po2_max\", \"d1_arterial_po2_min\", \"d1_pao2fio2ratio_max\", \"d1_pao2fio2ratio_min\", \"h1_arterial_pco2_max\", \n      \"h1_arterial_pco2_min\", \"h1_arterial_ph_max\", \"h1_arterial_ph_min\", \"h1_arterial_po2_max\", \"h1_arterial_po2_min\", \"h1_pao2fio2ratio_max\", \n      \"h1_pao2fio2ratio_min\", \"aids\", \"cirrhosis\", \"hepatic_failure\", \"immunosuppression\", \"leukemia\", \"lymphoma\", \"solid_tumor_with_metastasis\", \n      \"diabetes_mellitus\"]","5a1d4da7":"import missingno as msno\n%matplotlib inline","d05452f0":"msno.matrix(train.sample(1000),figsize=(35, 60), width_ratios=(10, 1), color=(.0, 0.5, 0.5),           fontsize=16)","d92ca23f":"# Define a function to visulize the features with missing values, and % of total values, & datatype\ndef missing_values_table(train):\n     # Total missing values\n    mis_val = train.isnull().sum()\n    # Percentage of missing values\n    mis_val_percent = 100 * train.isnull().sum() \/ len(train)\n    mis_val_type = train.dtypes\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_type], axis=1)\n        \n     # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'type'})\n        \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n        \n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(train.shape[1]) + \" columns.\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values.\")\n        \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","cf307aa1":"missing_values_table(train)","b654b7c2":"pct_null = train.isnull().sum() \/ len(train) \nmissing_features = pct_null[pct_null > 0.50].index \ntrain.drop(missing_features, axis=1, inplace=True)\ntest.drop(missing_features, axis=1, inplace=True)","c72e16e2":"# Fill Missing Values with median values of given variable \ntrain=train.fillna(train.median())\ntest=test.fillna(test.median())","8ca1e52b":"missing_values_table(train)\nmissing_values_table(test)","f6a8023e":"train['hospital_admit_source'].mode()[0]\ntest['hospital_admit_source'].mode()[0]","0ab118fe":"#Fill Empty NaN with Mode Value\ntrain['hospital_admit_source'] = train['hospital_admit_source'].fillna(train['hospital_admit_source'].mode()[0])\ntrain['ethnicity'] = train['ethnicity'].fillna(train['ethnicity'].mode()[0])\ntrain['icu_admit_source'] = train['icu_admit_source'].fillna(train['icu_admit_source'].mode()[0])\ntrain['gender'] = train['gender'].fillna(train['gender'].mode()[0])","9a11eaaa":"#Fill Empty NaN with Mode Value\ntest['hospital_admit_source'] = test['hospital_admit_source'].fillna(test['hospital_admit_source'].mode()[0])\ntest['ethnicity'] = test['ethnicity'].fillna(test['ethnicity'].mode()[0])\ntest['icu_admit_source'] = test['icu_admit_source'].fillna(test['icu_admit_source'].mode()[0])\ntest['gender'] = test['gender'].fillna(test['gender'].mode()[0])","673babde":"missing_values_table(train)\nmissing_values_table(test)","19a34511":"print(train.shape)\nprint(test.shape)","461d62bd":"# 0\/1 -> No\/Yes\ntrain[\"diabetes_mellitus\"] = train[\"diabetes_mellitus\"].map({0:'No',1:'Yes'})","d25d793f":"di = {1: \"Yes\", 0: \"No\"}\ntrain.replace({\"diabetes_mellitus\": di}, inplace=True)","35508cc9":"train.diabetes_mellitus.value_counts(normalize=True)","0b62d281":"#Listing Numerical Variables\nnumerical = list(train.select_dtypes(include=['float', 'int']).columns)\nids=[\"hospital_id\",\"icu_id\",\"Unnamed: 0\"]\nbinary=[\"elective_surgery\",\"readmission_status\",\"apache_post_operative\",\"arf_apache\",\n\"gcs_unable_apache\",\"intubated_apache\",\"ventilated_apache\",\"aids\",\"cirrhosis\",\"hepatic_failure\",\"immunosuppression\",\n\"leukemia\",\"lymphoma\",\"solid_tumor_with_metastasis\",\"diabetes_mellitus\"]\nstring=[\"bmi\",\"ethnicity\",\"gender\",\"hospital_admit_source\",\"icu_admit_source\",\"icu_admit_type\",\"icu_stay_type\",\n\"icu_type\",\"apache_2_diagnosis\",\"apache_3j_diagnosis\"]","457bc36c":"sns.pairplot(data=train,diag_kind='kde',vars=ids,hue='diabetes_mellitus')\nplt.show()","43f3f85d":"new_numerical = []\nfor v in numerical:\n    if v not in ids and v not in binary:\n        new_numerical.append(v)\nprint (new_numerical)","26cefa79":"from scipy.stats import ks_2samp\ndef run_KS_test(feature):\n    dist1 = train.loc[train.diabetes_mellitus == 'No',feature]\n    dist2 = train.loc[train.diabetes_mellitus == 'Yes',feature]\n    print(feature+':')\n    print(ks_2samp(dist1,dist2),'\\n')\n    \nfor a in new_numerical:\n    run_KS_test(a)","9671e788":"train.drop([\"h1_resprate_min\",\"h1_spo2_min\",\"d1_spo2_min\",\"height\"], axis=1, inplace=True)","34cbc09e":"test.drop([\"h1_resprate_min\",\"h1_spo2_min\",\"d1_spo2_min\",\"height\"], axis=1, inplace=True)","158a548d":"print(train.shape)\nprint(test.shape)","62932ddd":"binary=[\"elective_surgery\",\"readmission_status\",\"apache_post_operative\",\"arf_apache\",\n\"gcs_unable_apache\",\"intubated_apache\",\"ventilated_apache\",\"aids\",\"cirrhosis\",\"hepatic_failure\",\"immunosuppression\",\n\"leukemia\",\"lymphoma\",\"solid_tumor_with_metastasis\"]","65cb19f1":"from scipy.stats import chi2_contingency\ndef run_chi2_test(train, feature):\n\n    dist1 = train.loc[train.diabetes_mellitus == 'No',feature].value_counts().sort_index().tolist()\n    dist2 = train.loc[train.diabetes_mellitus == 'Yes',feature].value_counts().sort_index().tolist()\n    chi2, p, dof, expctd = chi2_contingency([dist1,dist2])\n    print(feature+':')\n    print(\"chi-square test statistic:\", chi2)\n    print(\"p-value\", p, '\\n')\n    \nfor a in binary:\n    run_chi2_test(train, a)","60e6fd2b":"train.drop([\"elective_surgery\",\"readmission_status\",\"intubated_apache\", \"ventilated_apache\", \n         \"immunosuppression\", \"leukemia\", \"lymphoma\"], axis=1, inplace=True)","ec7a2c4d":"test.drop([\"elective_surgery\",\"readmission_status\",\"intubated_apache\", \"ventilated_apache\", \n         \"immunosuppression\", \"leukemia\", \"lymphoma\"], axis=1, inplace=True)","d67ef9f6":"print(train.shape)\nprint(test.shape)","5c5fa40d":"# make general plots to examine each feature\ndef plot_var(col_name, full_name, continuous):\n    \"\"\"\n    Visualize a variable with\/without faceting on the Diabetes status.\n    - col_name is the variable name in the dataframe\n    - full_name is the full variable name\n    - continuous is True for continuous variables\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=False, figsize=(15,3))\n    # plot1: counts distribution of the variable\n    \n    if continuous:  \n        sns.distplot(train.loc[train[col_name].notnull(), col_name], kde=True, ax=ax1)\n    else:\n        sns.countplot(train[col_name], order=sorted(train[col_name].unique()), color='#5975A4', saturation=1, ax=ax1)\n    ax1.set_xlabel(full_name)\n    ax1.set_ylabel('Count')\n    ax1.set_title(full_name)\n\n          \n    # plot2: bar plot of the variable grouped by Diabetes or not\n    if continuous:\n        sns.boxplot(x=col_name, y='diabetes_mellitus', data=train, ax=ax2)\n        ax2.set_ylabel('')\n        ax2.set_title(full_name + ' by Diabetes Status')\n    else:\n        Diabetes_rates = train.groupby(col_name)['diabetes_mellitus'].value_counts(normalize=True)[:,1]\n        sns.barplot(x=Diabetes_rates.index, y=Diabetes_rates.values, color='#5975A4', saturation=1, ax=ax2)\n        ax2.set_ylabel('Fraction Diabetes')\n        ax2.set_title('Diabetes Rate by ' + full_name)\n        ax2.set_xlabel(full_name)\n    \n   # plot3: kde plot of the variable gropued by Diabetes_status\n    if continuous:\n        facet = sns.FacetGrid(train, hue = 'diabetes_mellitus', size=3, aspect=4)\n        facet.map(sns.kdeplot, col_name, shade=True)\n        facet.set(xlim=(train[col_name].min(), train[col_name].max()))\n        facet.add_legend()  \n    else:\n        fig = plt.figure(figsize=(12,3))\n        sns.countplot(x=col_name, hue='diabetes_mellitus', data=train, order=sorted(train[col_name].unique()) )\n     \n    plt.tight_layout()","f779ab07":"#Listing Numerical Variables\nnumerical1 = list(train.select_dtypes(include=['float', 'int']).columns)","50fe4281":"new_numerical1 = []\nfor v in numerical1:\n    if v not in ids and v not in binary:\n        new_numerical1.append(v)\nprint (new_numerical1)","3fc820ed":"for plot_element in new_numerical1: \n    plot_var(plot_element, plot_element, continuous=True)","c30aadb0":"# make general plots to examine each feature\ndef plot_var(col_name, full_name, continuous):\n    \"\"\"\n    Visualize a variable\n    - col_name is the variable name in the dataframe\n    - full_name is the full variable name\n    - continuous is True for continuous variables\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharex=False, figsize=(15,3))\n    # plot1: counts distribution of the variable\n    \n    if continuous:  \n        sns.distplot(train.loc[train[col_name].notnull(), col_name], kde=False, ax=ax1)\n    else:\n        sns.countplot(train[col_name], order=sorted(train[col_name].unique()), color='#5975A4', saturation=1, ax=ax1)\n    ax1.set_xlabel(full_name)\n    ax1.set_ylabel('Count')\n    ax1.set_title(full_name)\n\n          \n    # plot2: bar plot of the variable grouped by diabetes or not\n    if continuous:\n        sns.boxplot(x=col_name, y='diabetes_mellitus', data=train, ax=ax2)\n        ax2.set_ylabel('')\n        ax2.set_title(full_name + ' by Diabetes Status')\n    else:\n        Diabetes_rates = train.groupby(col_name)['diabetes_mellitus'].value_counts(normalize=True)[:].plot(kind='bar')\n        #sns.barplot(Diabetes_rates, color='#5975A4', saturation=1, ax=ax2)\n        ax2.set_ylabel('Fraction Diabetes')\n        ax2.set_title('Diabetes Rate by ' + full_name)\n        ax2.set_xlabel(full_name)\n    \n    # plot3: kde plot of the variable gropued by diabetes_status\n    if continuous:  \n        facet = sns.FacetGrid(train, hue = 'diabetes_mellitus', size=3, aspect=4)\n        facet.map(sns.kdeplot, col_name, shade=True)\n        #facet.set(xlim=(train[col_name].min(), train[col_name].max()))\n        facet.add_legend() \n    else:\n        fig = plt.figure(figsize=(12,3))\n        sns.countplot(x=col_name, hue='diabetes_mellitus', data=train, order=sorted(train[col_name].unique()) )\n     \n    plt.tight_layout()","fdd3bfb4":"new_binary=[\"apache_post_operative\", \"arf_apache\",\"gcs_unable_apache\",\"aids\",\"cirrhosis\",\"hepatic_failure\"]","504bc7ea":"for plot_element in new_binary: \n    plot_var(plot_element, plot_element, continuous=False)","b67e66c2":"string=[\"ethnicity\",\"gender\",\"hospital_admit_source\",\"icu_admit_source\",\"icu_stay_type\",\n\"icu_type\"]","43b903d4":"for plot_element in string: \n    plot_var(plot_element, plot_element, continuous=False)","02f79f8c":"train[\"diabetes_mellitus\"] = train[\"diabetes_mellitus\"].map({'Yes':1, 'No':0})","94f9e3ce":"train[\"diabetes_mellitus\"] .value_counts()","6c6939a0":"train.head()","8f4ea91a":"corr = train.drop([\"hospital_id\",\"icu_id\",\"Unnamed: 0\"], axis=1).corr()","8084b420":"corr = train.drop([\"hospital_id\",\"icu_id\",\"Unnamed: 0\"], axis=1).corr()\nfig, ax = plt.subplots(figsize=(15,10))\n#Sample fig size in inches\ncm_train = sns.heatmap(train.drop([\"hospital_id\",\"icu_id\",\"Unnamed: 0\"], axis=1).corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\", ax=ax)","b5ff65ae":"#Create correlation matrix\ncorr_matrix = train.drop([\"hospital_id\",\"icu_id\",\"Unnamed: 0\"], axis=1).corr().abs()","630b8d2a":"#Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper","a93af06a":"#Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nto_drop","a7ca23b1":"#Drop features\ntrain.drop(train[to_drop], axis=1, inplace=True)\ntest.drop(test[to_drop], axis=1, inplace=True)","f6168a9c":"print(train.shape)\nprint(test.shape)","6dfb13fc":"fig, ax = plt.subplots(figsize=(15,10)) \n# Sample figsize in inches \ncm_df = sns.heatmap(train.drop([\"hospital_id\",\"icu_id\",\"Unnamed: 0\"], axis=1).corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\", ax=ax)","34621570":"train.head()","ce955698":"train[\"diabetes_mellitus\"] .value_counts()","6655e54f":"## Part 2: Feature Preprocessing","3e774078":"a=train.select_dtypes(include=['object'])\ntrain_dummies = pd.get_dummies(a,drop_first=True)\ntrain_dummies.head()\na=test.select_dtypes(include=['object'])\ntest_dummies = pd.get_dummies(a,drop_first=True)\ntest_dummies.head()","82e13cc0":"train=pd.concat([train,train_dummies], axis=1)\ntrain.head()\ntest=pd.concat([test,test_dummies], axis=1)\ntest.head()","2e7ff41c":"train.drop(a.columns, axis=1, inplace=True)\ntest.drop(a.columns, axis=1, inplace=True)","d0916076":"print(train.shape)\nprint(test.shape)","20c19c84":"a=test.append(train)\na.head()","7421fa02":"missing_values_table(a)","07973199":"test['hospital_admit_source_Chest Pain Center']=0\ntest['hospital_admit_source_ICU']=0\ntest['hospital_admit_source_Observation']=0\ntest['hospital_admit_source_Other']=0\ntest['hospital_admit_source_PACU']=0","bb3e0a57":"print(train.shape)\nprint(test.shape)","c9386e39":"# Drop some useless columns\ndrop_list = [\"hospital_id\",\"icu_id\",\"Unnamed: 0\"]\ntrain.drop(drop_list, axis=1, inplace=True)\ntest.drop(drop_list, axis=1, inplace=True)","667085cb":"# Drop some useless columns\ndrop_list = [\"diabetes_mellitus\"]","d8c5d8a2":"train[\"diabetes_mellitus\"].value_counts()\n#test[\"diabetes_mellitus\"].value_counts()","d1ecd5dd":"train.head()\ntest.head()","0fdb0036":"import random \ndf=train.copy()\n#df = df.sample(n=1000)\n#df=train.copy()\ndf.shape","43c8e0c0":"df.head()","ba16d105":"X = df.drop(drop_list, axis=1)\ny = df.diabetes_mellitus.values","cae5b8e9":"X.head()","fb10387f":"# Train\/Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","84bc2435":"# Scale the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns)\n\n#X_train = pd.DataFrame(X_train.values, columns=X_train.columns)\n#X_test = pd.DataFrame(X_test.values, columns=X_test.columns)\n\nprint(\"Feature space holds %d observations and %d features\" % X_train.shape)\nprint(\"Unique target labels:\", np.unique(y_train))","5db137e2":"from collections import Counter\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom xgboost import  XGBClassifier","3f044b0a":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)","c2a4e45e":"# XGboost tunning\nxgb = GradientBoostingClassifier(random_state=0)\n\n#xgb.get_params().keys()\nxgb_param_grid = {\n    \"learning_rate\"    : [0.05,0.06, 0.07],\n    \"max_depth\"        : [2,3,4,5],\n    'subsample': [0.25, 0.5,1.0]\n}\ngrid_xgb = GridSearchCV(xgb, param_grid = xgb_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngrid_xgb.fit(X_train,y_train)","de30d1ae":"xgb_best = grid_xgb.best_estimator_\n# Best score\nprint('Best Score:', grid_xgb.best_score_)\nprint('Best parameters set: \\n', grid_xgb.best_params_)","4c6c2532":"y_pred_xgb = xgb_best.predict(X_test)\ny_prob_xgb = xgb_best.predict_proba(X_test)[:,1]","63bf02da":"# print a summary of the scores\ndef print_grid_search_metrics(gs):\n    print(\"Best score: %0.3f\" % gs.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = gs.best_params_\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","43564e35":"# Plot learning curves\n#grid_svm, grid_rf, grid_gb, grid_knn, grid_lr, grid_xgb\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(grid_xgb.best_estimator_,\"XGBoost learning curves\",X_train,y_train,cv=kfold)","79b8ed67":"from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score\n\ndef cal_evaluation(classifier, cm, auc):\n    tn = cm[0][0]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    tp = cm[1][1]\n    accuracy  = (tp + tn) \/ (tp + fp + fn + tn + 0.0)\n    precision = tp \/ (tp + fp + 0.0)\n    recall = tp \/ (tp + fn + 0.0)\n    f1 = 2 * precision * recall \/ (precision + recall)\n    print(classifier)\n    print(\"Accuracy is \" + str(accuracy))\n    print(\"Precision is \" + str(precision))\n    print(\"Recall is \" + str(recall))\n    print(\"F1 score is \" + str(f1))\n    print(\"ROC AUC is \" + str(auc))\n\ndef draw_confusion_matrices(confusion_matricies):\n    class_names = ['Not Diabetes','Diabetes']\n    for x in confusion_matrices:\n        classifier, cm, auc = x[0], x[1], x[2]\n        cal_evaluation(classifier, cm, auc)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n        plt.title('Confusion matrix for {}'.format(classifier))\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()","3720d3a2":"%matplotlib inline\n\ny = np.array(y)\nclass_names = np.unique(y)\nprint(class_names)\n\nconfusion_matrices = [(\"XGBoost Classifier\", confusion_matrix(y_test, y_pred_xgb), roc_auc_score(y_test, y_prob_xgb))]\n\ndraw_confusion_matrices(confusion_matrices)","973d87ab":"#nrows = ncols = 2\nnrows = 2\nncols = 1\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(5,15))\n\nnames_classifiers = [(\"XGBoost\", xgb_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    name = names_classifiers[nclassifier][0]\n    classifier = names_classifiers[nclassifier][1]\n    indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n    g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row])\n    g.set_xlabel(\"Relative importance\",fontsize=12)\n    g.set_ylabel(\"Features\",fontsize=12)\n    g.tick_params(labelsize=9)\n    g.set_title(name + \" feature importance\")","d12d7b4a":"# make the ROC curve\nfpr, tpr, thresh = roc_curve(y_test, y_prob_xgb, pos_label=1)\nroc_auc = roc_auc_score(y_test, y_prob_xgb)\n\n# These are the points at threshold = 0.1~0.5\nx1 = fpr[(thresh <= 0.5) & (thresh >= 0.1)] \nx2 = tpr[(thresh <= 0.5) & (thresh >= 0.1)]\n\nfig = plt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='b', lw=2, linestyle='--')\nplt.plot(x1, x2, color='k', lw=3, label='threshold = 0.1 ~ 0.5')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for XGBoost')\nplt.legend(loc=\"lower right\")\nplt.show()","32fa2d6e":"acc_grid = np.zeros(thresh.shape)\nprec_grid = np.zeros(thresh.shape)\nrecall_grid = np.zeros(thresh.shape)\nTP_grid = np.zeros(thresh.shape)\nFP_grid = np.zeros(thresh.shape)\nFN_grid = np.zeros(thresh.shape)\n\nfor i in range(thresh.shape[0]):\n    cm = confusion_matrix(y_test, y_prob_xgb >= thresh[i])\n    acc_grid[i] = accuracy_score(y_test, y_prob_xgb >= thresh[i])\n    prec_grid[i] = precision_score(y_test, y_prob_xgb >= thresh[i])\n    recall_grid[i] = recall_score(y_test, y_prob_xgb >= thresh[i])\n    TP_grid[i] = cm[1][1]\n    FP_grid[i] = cm[0][1]\n    FN_grid[i] = cm[1][0]","d3eb0a24":"fig = plt.figure()\nplt.plot(thresh, acc_grid, color='k', lw=2, label='Accuracy')\nplt.plot(thresh, prec_grid, color='b', lw=2, label='Precision')\nplt.plot(thresh, recall_grid, color='r', lw=2, label='Recall')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.legend(loc=\"lower center\")\nplt.show()","41b7d551":"test.head()","8f16065a":"test.head()","e3284fd6":"test1=test.copy()\ntest1.head()","557849f6":"from sklearn.preprocessing import StandardScaler\nscaled_test = StandardScaler().fit_transform(test1.values)\nscaled_test = pd.DataFrame(scaled_test, index=test1.index, columns=test1.columns)\nscaled_test.head()","04d92b9b":"y_pred_xgb = xgb_best.predict(scaled_test)\ny_prob_xgb = xgb_best.predict_proba(scaled_test)[:,1]\ny_prob_xgb","f58ecd6f":"scaled_test['diabetes_mellitus']=y_prob_xgb\nscaled_test.head()","cfc0d1e6":"scaled_test=scaled_test.reset_index()\nscaled_test.head()","b00f76b2":"test_id = scaled_test.encounter_id.values\ntest_preds = scaled_test.diabetes_mellitus.values\nsubmission = pd.DataFrame.from_dict({\n    'encounter_id':test_id,\n    'diabetes_mellitus':test_preds,\n})\nsubmission.to_csv('submission.csv', index=False)","6120b431":"submission = pd.read_csv('submission.csv')\nsubmission.head()","f3f93446":"## Have a look at missing value","c06a5eb7":"### Identify Highly Correlated Features","d33daf3a":"### Part 1.3: Understand the features & the target","5f5c07a0":"train.set_index(\"encounter_id\",inplace=True)\ntrain.set_index(\"encounter_id\",inplace=True)","9407dbba":"### Calculate Confusion Matrix (Precision, Recall, Accuracy)","5b5a62c3":"# convert column of a DataFrame\ntrain[list1] = train[list1].apply(pd.to_numeric)","8727e9d9":"### Feature importance of tree based classifiers\nIn order to see the most informative features for the prediction of Diabetes mellitus, i displayed the feature importance for the tree based classifiers.","c2a01cfb":"## XGBoost for predictions","7f491b29":"df=train.copy()\nimport random \ndf = df.sample(n=10000)","1455e6a7":"## Data Visualization for String Variables","687f167d":"import sweetviz as sv \nmy_report = sv.compare([train, \"Training Data\"], [test, \"Test Data\"]) \nmy_report.show_html()","0a6c4ae2":"### Part 3.3: Learning Curve\nI will take a closer look of the selected models: XGBoost. \n\n- Learning Curve\n- Confusion Matrix","59edd62e":"## Correct type of variables","bcc75062":"## Statistical overview","8d5f81b8":"### Target variable:","6738b1dc":"## Data Visualization for Numerical Variables","29df924f":"## File descriptions\n\n**TrainingWiDS2021.csv** - the training data. You should see 130,157 encounters represented here.\n\nPlease view the Data Dictionary file for more information about the columns.\n\n**UnlabeledWiDS2021.csv** - the unlabeled data (data without diabetes_mellitus provided). You are being asked to predict the diabetes_mellitus variable for these encounters.\n\n**SampleSubmissionWiDS2021.csv** - a sample submission file in the correct format.\n\n**SolutionTemplateWiDS2021.csv** - a list of all the rows (and encounters) that should be in your submissions.\n\n**DataDictionaryWiDS2021.csv** - supplemental information about the data.","2a0892e1":"## Contents","2ae68c23":"# WiDS Datathon 2021","2577d9ec":"## Plot learning curves\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","58e485c0":"## Data Visualization","51895432":"From the pvalues, we see that: h1_resprate_min, h1_spo2_min, d1_spo2_min, height, p-values (>0.05), meaning that we can't reject the null hypothesis that the distributions of with and without diabetes are the same. The difference between the two samples are not significant. These features are probably irrelevant to our target variable.\n\nThe pvalues for other features are very low, therefore the two samples are not drawn from the same distribution.\n\nNext, i will explore bineary feature","3ea1327a":"## OverView of the dataset","dbbfa779":"#Installing pip\n!pip install --upgrade sweetviz","79b26be1":"I now have an idea about how each feature behaves for Diabetes Status. I will check the correlation between features and with the target variable.","cc87b160":"import random \ntrain = train.sample(n=10000) ","0eb1176d":"The target variable is \"Diabetes Mellitus\". I will perform EDA on the features, the target varible and their correlations.","23b0b76a":"train.drop('Unnamed: 0', axis=1, inplace=True)\ntrain.head()\ntest.drop('Unnamed: 0', axis=1, inplace=True)\ntest.head()","8873a4de":"About 21% of the patients were diagnosed and 78% were not diagnosed. Unbalanced data. I will try to handle this unbalance in the cross validation and need to pick appropriate metrics.","ba8ae0e6":"**Analysing patient health, with an emphasis on the chronic condition of diabetes, through data from MIT\u2019s GOSSIS (Global Open Source Severity of Illness Score) initiative**","85dd5f25":"I plot the feature importance for the XGBoost","ebd67424":"### Linear Correlation","b7093d6b":"From the pvalues, we see that: elective_surgery: readmission_status, intubated_apache, ventilated_apache, immunosuppression, leukemia, lymphoma: p-values (>0.05), meaning that we can't reject the null hypothesis that there is no association between diabetes status and bineary variables. These features are probably irrelevant to our target variable.\n\nNext, i will explore and visualize selected feature","dd5e50b4":"## Part 3: Model Training and Result Evaluation","1cbe95b1":"## Data Visualization for Binary Variables","4dfae90d":"* Part 1: Data Exploration\n* Part 2: Feature Preprocessing\n* Part 3: Model Training and Results Evaluation\n* Part 4: Feature Selection\n* Part 5: Use Probabilities as Prediction Results","05c4671e":"I will implement machine learning pipelines consisting of one or more of the following steps, depending on the particular model:\n\n* Mean imputation of missing values\n* Data standardization: rescaling to zero mean and unit variance\n\nThe chosen model\n\nI will evaluate and compare the following models using a cross-validated Area Under the Receiver Operating Characteristic Curve (AUROC)** score on the training set\n\nI'll perform some hyperparameter tuning for each model to choose the most promising model, then more carefully tune the hyperparameters of the best-performing model.\n\n** For the metrics, both precision and recall of the result are important, as we care about true positives as well as false positives. I like to predict most of the patients that are likely to diagnosed with diabetes mellitus.","e78223c1":"Next, I will try to determine the optimal threshold. As true positive rate and recall are actually equal, therefore, one can use a lower threshold(<0.5).","4a12dcb7":"## Read the data","16ff545a":"## Part 1: Data Exploration\n\nPart 1.1: Understand the Raw Dataset"}}