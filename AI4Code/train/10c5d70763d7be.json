{"cell_type":{"3220bc91":"code","3fdf187b":"code","14018fd7":"code","72088a53":"code","53556cc1":"code","5b15bd30":"code","eb85f716":"code","e19eff46":"code","f9bf18bc":"code","8e9969e9":"code","e5ebf367":"code","5ef97d13":"code","8eef4d71":"code","804bfa3a":"code","8d788f87":"code","5ecdb839":"code","9ba5a45d":"code","ba8496b0":"code","29e3ca8d":"code","ca5ebedb":"code","c0db6d70":"code","59696600":"markdown","92a9eb49":"markdown","e87f3b18":"markdown","c5175ee6":"markdown","4dafefe2":"markdown","01dc68f7":"markdown","a070205d":"markdown","b3fe3612":"markdown","7eed126a":"markdown"},"source":{"3220bc91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3fdf187b":"# libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\n\n# dataset\nletters = pd.read_csv(\"..\/input\/letter-recognition.csv\")","14018fd7":"# about the dataset\n\n# dimensions\nprint(\"Dimensions: \", letters.shape, \"\\n\")\n\n# data types\nprint(letters.info())\n\n# head\nletters.head()","72088a53":"# a quirky bug: the column names have a space, e.g. 'xbox ', which throws and error when indexed\nprint(letters.columns)","53556cc1":"# let's 'reindex' the column names\nletters.columns = ['letter', 'xbox', 'ybox', 'width', 'height', 'onpix', 'xbar',\n       'ybar', 'x2bar', 'y2bar', 'xybar', 'x2ybar', 'xy2bar', 'xedge',\n       'xedgey', 'yedge', 'yedgex']\nprint(letters.columns)","5b15bd30":"order = list(np.sort(letters['letter'].unique()))\nprint(order)","eb85f716":"# basic plots: How do various attributes vary with the letters\n\nplt.figure(figsize=(16, 8))\nsns.barplot(x='letter', y='xbox', \n            data=letters, \n            order=order)","e19eff46":"letter_means = letters.groupby('letter').mean()\nletter_means.head()","f9bf18bc":"plt.figure(figsize=(18, 10))\nsns.heatmap(letter_means)","8e9969e9":"# average feature values\nround(letters.drop('letter', axis=1).mean(), 2)","e5ebf367":"# splitting into X and y\nX = letters.drop(\"letter\", axis = 1)\ny = letters['letter']","5ef97d13":"# scaling the features\nX_scaled = scale(X)\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 101)","8eef4d71":"# linear model\n\nmodel_linear = SVC(kernel='linear')\nmodel_linear.fit(X_train, y_train)\n\n# predict\ny_pred = model_linear.predict(X_test)","804bfa3a":"# confusion matrix and accuracy\n\n# accuracy\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n\n# cm\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","8d788f87":"# non-linear model\n# using rbf kernel, C=1, default value of gamma\n\n# model\nnon_linear_model = SVC(kernel='rbf')\n\n# fit\nnon_linear_model.fit(X_train, y_train)\n\n# predict\ny_pred = non_linear_model.predict(X_test)","5ecdb839":"# confusion matrix and accuracy\n\n# accuracy\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n\n# cm\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","9ba5a45d":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                 \n","ba8496b0":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","29e3ca8d":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n","ca5ebedb":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","c0db6d70":"# model with optimal hyperparameters\n\n# model\nmodel = SVC(C=1000, gamma=0.01, kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")\nprint(metrics.confusion_matrix(y_test, y_pred), \"\\n\")","59696600":"**Data Preparation**\n\nLet's conduct some data preparation steps before modeling. Firstly, let's see if it is important to rescale the features, since they may have varying ranges. For example, here are the average values:\n","92a9eb49":"\n\n\nThe plots above show some useful insights:\n\u2022Non-linear models (high gamma) perform much better than the linear ones\n\u2022At any value of gamma, a high value of C leads to better performance\n\u2022None of the models tend to overfit (even the complex ones), since the training and test accuracies closely follow each other\n\nThis suggests that the problem and the data is inherently non-linear in nature, and a complex model will outperform simple, linear models in this case.\n\n\n\n\nLet's now choose the best hyperparameters. \n","e87f3b18":"**Letter Recognition Using SVM**\n\nLet's now tackle a slightly more complex problem - letter recognition. We'll first explore the dataset a bit, prepare it (scale etc.) and then experiment with linear and non-linear SVMs with various hyperparameters.\n\n**Data Understanding**\n\nLet's first understand the shape, attributes etc. of the dataset.\n","c5175ee6":"**Conclusion**\n\nThe accuracy achieved using a non-linear kernel (~0.95) is mush higher than that of a linear one (~0.85). We can conclude that the problem is highly non-linear in nature.\n","4dafefe2":"The linear model gives approx. 85% accuracy. Let's look at a sufficiently non-linear model with randomly chosen hyperparameters.","01dc68f7":"In this case, the average values do not vary a lot (e.g. having a diff of an order of magnitude). Nevertheless, it is better to rescale them.","a070205d":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Evaluating the Final Model\n\nLet's now build and evaluate the final model, i.e. the model with highest test accuracy.\n","b3fe3612":"\n\n\nThe non-linear model gives approx. 93% accuracy. Thus, going forward, let's choose hyperparameters corresponding to non-linear models.\n\n\n\n\n**Grid Search: Hyperparameter Tuning**\n\nLet's now tune the model to find the optimal values of C and gamma corresponding to an RBF kernel. We'll use 5-fold cross validation.\n","7eed126a":"**Model Building**\n\nLet's fist build two basic models - linear and non-linear with default hyperparameters, and compare the accuracies.\n"}}