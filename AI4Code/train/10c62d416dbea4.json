{"cell_type":{"7ca9e273":"code","9db79d55":"code","becd171c":"code","05773cf5":"code","454f3b50":"code","71b447d4":"code","eb431d61":"code","1e9c16f3":"code","54f5633f":"code","53364297":"code","86af0b9e":"code","161a8639":"code","cce3214f":"code","fef9d565":"code","de813722":"code","4cb21e36":"code","92f91d15":"code","5130ecee":"code","24325afc":"code","4edd9f88":"code","2c3e4829":"code","0235d2d2":"code","62198f4e":"code","1ae7b69f":"code","2c7977b4":"code","c0d66523":"code","78f9a232":"code","5b256af0":"code","f5b67455":"code","240f6337":"code","59f935e6":"code","41e983a4":"code","03854724":"code","bf2958c4":"code","a12f6348":"code","ecde8be8":"code","43fdba8a":"code","c45d88f0":"code","ddd12784":"code","a4b0edcd":"code","26eb8204":"code","85497ced":"code","49d9229d":"code","e4bf91d9":"code","56c155db":"code","05bf0754":"code","250fb8fd":"code","7e61998e":"code","0f6c2270":"code","09cfd1dd":"code","cc6ef450":"code","6b83e9d1":"code","c5faae2a":"code","66f9715a":"code","b8e86d50":"code","ca183081":"code","462d8e34":"code","c7766e46":"code","3a48e6d6":"code","06928c9f":"code","f2fd2a45":"code","f1d38867":"code","14f1ce81":"code","c8348877":"code","98fa57a0":"markdown","ad6cbfa5":"markdown","0cb6527f":"markdown","13b1af05":"markdown","7099eea3":"markdown","1fa14077":"markdown","840e986a":"markdown","783ecd66":"markdown","942e2914":"markdown","beb016d2":"markdown","34fad0a1":"markdown","71d7dd6a":"markdown","a29da640":"markdown","c9690c21":"markdown","ac689d0b":"markdown","d3b4a97f":"markdown","8eff373c":"markdown","52a237f8":"markdown","cae22069":"markdown","c41cfa87":"markdown","9c430e95":"markdown","c8f41719":"markdown","374c2d29":"markdown","07adef95":"markdown"},"source":{"7ca9e273":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9db79d55":"df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","becd171c":"\nrslt_df = df[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nrslt_df2 = df[(df['toxic'] == 1) | (df['severe_toxic'] == 1) | (df['obscene'] == 1) | (df['threat'] == 1) | (df['insult'] == 1) | (df['identity_hate'] == 1)]\nnew1 = rslt_df[['id', 'comment_text', 'toxic']].iloc[:23891].copy() \nnew2 = rslt_df2[['id', 'comment_text', 'toxic']].iloc[:946].copy()\nnew = pd.concat([new1, new2], ignore_index=True)","05773cf5":"new.tail()","454f3b50":"#test train split\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new[\"comment_text\"], new['toxic'], test_size=0.33)","71b447d4":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5)\nX1 = vectorizer.fit_transform(X_train)\nX_test1= vectorizer.transform(X_test)","eb431d61":"class_names = ['nontoxic', 'toxic']","1e9c16f3":"from sklearn.model_selection import cross_val_score\n# from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nclf2 = svm.SVC(kernel='linear', C=1,probability=True)\n# clf2 = LogisticRegression(C=0.1, solver='sag')\n# scores = cross_val_score(clf2, X1,y_train, cv=5,scoring='f1_weighted')\n","54f5633f":"y_p1 = clf2.fit(X1, y_train).predict(X_test1)","53364297":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p1)\nprint('Accuracy: %f' % accuracy)","86af0b9e":"from sklearn.metrics import plot_roc_curve\nfrom sklearn.exceptions import NotFittedError\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndisp = plot_roc_curve(clf2,X_test1,y_test)\n# plot_roc_curve(dt,X_test_de,y_test_de,ax=disp.ax_)\n# plot_roc_curve(rf,X_test_de,y_test_de,ax=disp.ax_)","161a8639":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc = make_pipeline(vectorizer, clf2)","cce3214f":"new[\"comment_text\"][0]","fef9d565":"print(c.predict_proba([new[\"comment_text\"][0]]))","de813722":"from lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=class_names)","4cb21e36":"X_test = X_test.tolist()","92f91d15":"X_test[0]","5130ecee":"type(y_test)","24325afc":"y_test = y_test.tolist()","4edd9f88":"y_test = np.array(y_test)","2c3e4829":"type(y_test)","0235d2d2":"idx = 26\nexp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])\n\n","62198f4e":"exp.as_list()","1ae7b69f":"print('Original prediction:', clf2.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['talk']] = 0\ntmp[0,vectorizer.vocabulary_['pretending']] = 0\nprint('Prediction removing some features:', clf2.predict_proba(tmp)[0,1])\nprint('Difference:', clf2.predict_proba(tmp)[0,1] - clf2.predict_proba(X_test1[idx])[0,1])","2c7977b4":"%matplotlib inline\nfig = exp.as_pyplot_figure()","c0d66523":"exp.show_in_notebook()","78f9a232":"print('Original dataset shape %s' % Counter(y_train))\nsm = SMOTE(random_state=12)\nx_train_res, y_train_res = sm.fit_sample(X1, y_train)\nprint('Resampled dataset shape %s' % Counter(y_train_res))","5b256af0":"from sklearn.model_selection import cross_val_score\n# from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nclf = svm.SVC(kernel='linear', C=1, probability=True)\n# clf = LogisticRegression(C=0.1, solver='sag')\n# scores = cross_val_score(clf, x_train_res,y_train_res, cv=5,scoring='f1_weighted')\n# scores","f5b67455":"y_p2 = clf.fit(x_train_res, y_train_res).predict(X_test1)","240f6337":"\n\nfrom sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p2)\nprint('Accuracy: %f' % accuracy)\n\n","59f935e6":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc2 = make_pipeline(vectorizer, clf)","41e983a4":"print(c2.predict_proba([new[\"comment_text\"][0]]))","03854724":"idx =26\nexp = explainer.explain_instance(X_test[idx], c2.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c2.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","bf2958c4":"exp.as_list()","a12f6348":"print('Original prediction:', clf.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['wikipedia']] = 0\ntmp[0,vectorizer.vocabulary_['try']] = 0\nprint('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\nprint('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(X_test1[idx])[0,1])","ecde8be8":"%matplotlib inline\nfig = exp.as_pyplot_figure()","43fdba8a":"exp.show_in_notebook()","c45d88f0":"from imblearn.under_sampling import NearMiss\nnm = NearMiss()\nX_d, y_d = nm.fit_resample(X1, y_train)\nprint('Resampled dataset shape %s' % Counter(y_d))","ddd12784":"from sklearn.model_selection import cross_val_score\n# from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nclf1 = svm.SVC(kernel='linear', C=1, probability=True)\n# clf1 = LogisticRegression(C=0.1, solver='sag')\n# scores = cross_val_score(clf1, X_d,y_d, cv=5,scoring='f1_weighted')","a4b0edcd":"y_p3 = clf1.fit(X_d, y_d).predict(X_test1)","26eb8204":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(y_test, y_p3)\nprint('Accuracy: %f' % accuracy)","85497ced":"from lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nc3 = make_pipeline(vectorizer, clf1)","49d9229d":"print(c3.predict_proba([new[\"comment_text\"][0]]))","e4bf91d9":"idx = 26\nexp = explainer.explain_instance(X_test[idx], c3.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c3.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","56c155db":"exp.as_list()","05bf0754":"print('Original prediction:', clf1.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['block']] = 0\ntmp[0,vectorizer.vocabulary_['dare']] = 0\nprint('Prediction removing some features:', clf1.predict_proba(tmp)[0,1])\nprint('Difference:', clf1.predict_proba(tmp)[0,1] - clf1.predict_proba(X_test1[idx])[0,1])","250fb8fd":"%matplotlib inline\nfig = exp.as_pyplot_figure()","7e61998e":"exp.show_in_notebook()","0f6c2270":"#printing ids of comments which are toxic\ncount=-1\nfor x in y_test:\n    count=count+1\n    if x==1:\n        print(count)","09cfd1dd":"idx = 296\nexp = explainer.explain_instance(X_test[idx], c.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","cc6ef450":"exp.as_list()","6b83e9d1":"print('Original prediction:', clf2.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['fuck']] = 0\ntmp[0,vectorizer.vocabulary_['fucking']] = 0\nprint('Prediction removing some features:', clf2.predict_proba(tmp)[0,1])\nprint('Difference:', clf2.predict_proba(tmp)[0,1] - clf2.predict_proba(X_test1[idx])[0,1])","c5faae2a":"%matplotlib inline\nfig = exp.as_pyplot_figure()","66f9715a":"exp.show_in_notebook()","b8e86d50":"idx = 296\nexp = explainer.explain_instance(X_test[idx], c2.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c2.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","ca183081":"exp.as_list()","462d8e34":"print('Original prediction:', clf.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['fuck']] = 0\ntmp[0,vectorizer.vocabulary_['fucking']] = 0\nprint('Prediction removing some features:', clf.predict_proba(tmp)[0,1])\nprint('Difference:', clf.predict_proba(tmp)[0,1] - clf.predict_proba(X_test1[idx])[0,1])","c7766e46":"%matplotlib inline\nfig = exp.as_pyplot_figure()","3a48e6d6":"exp.show_in_notebook()","06928c9f":"idx = 296\nexp = explainer.explain_instance(X_test[idx], c3.predict_proba, num_features=10)\nprint('Document id: %d' % idx)\nprint('Probability(toxic) =', c3.predict_proba([X_test[idx]])[0,1])\nprint('True class: %s' % class_names[y_test[idx]])","f2fd2a45":"exp.as_list()","f1d38867":"print('Original prediction:', clf1.predict_proba(X_test1[idx])[0,1])\ntmp = X_test1[idx].copy()\ntmp[0,vectorizer.vocabulary_['shit']] = 0\ntmp[0,vectorizer.vocabulary_['bastard']] = 0\nprint('Prediction removing some features:', clf1.predict_proba(tmp)[0,1])\nprint('Difference:', clf1.predict_proba(tmp)[0,1] - clf1.predict_proba(X_test1[idx])[0,1])","14f1ce81":"%matplotlib inline\nfig = exp.as_pyplot_figure()","c8348877":"exp.show_in_notebook()","98fa57a0":"### EXPLAINING PREDICTION USING LIME","ad6cbfa5":"These weighted features are a linear model, which approximates the behaviour of the Support Vector classifier in the vicinity of the test example. Roughly, if we remove 'talk' and 'pretending' from the document , the prediction should move towards the opposite class by about the sum of the weights for both features.","0cb6527f":"The classifier got this example right (it predicted non-toxic).\nThe explanation is presented below as a list of weighted features.","13b1af05":"> we have converted X_test to list and y_test to ndarray because explain_instance takes input like this.","7099eea3":"##### Tf\/Idf","1fa14077":"#### Visualizing explanations for toxic comment after Oversampling the dataset","840e986a":"#### Visualizing explanations for toxic comment after Undersampling the dataset","783ecd66":"After Oversampling ","942e2914":"## LIME","beb016d2":"#### Visualizing explanations for non-toxic comment after Undersampling the dataset","34fad0a1":"#### Visualizing explanations of toxic comment in imbalance dataset","71d7dd6a":"NOW for TOXIC Comment ","a29da640":"#### Visualizing explanations for non-toxic comment after Oversampling the dataset","c9690c21":"OVERSAMPLE","ac689d0b":"## Example1","d3b4a97f":"#### Visualizing explanations non-toxic comment on unbalanced dataset","8eff373c":"> here we have to pass the raw text and it gives the prob for non-toxic and toxic class for perticular comment at index 0","52a237f8":"# Example 2","cae22069":"UNDERSAMPLE","c41cfa87":"features contributing for classification","9c430e95":"After Undersampling","c8f41719":"We will now  generate an explanation with at most 10 features for an arbitrary document in the test set.","374c2d29":"##### SVC","07adef95":"For reference please see the [paper](https:\/\/arxiv.org\/abs\/1602.04938) "}}