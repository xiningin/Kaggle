{"cell_type":{"19002418":"code","14a69bd6":"code","d79c3199":"code","63b7e750":"code","072a2cef":"code","088ea91b":"code","18c084ed":"code","b3ae2299":"code","fea03035":"code","bb6d8fd1":"code","b35bf6ef":"code","87800220":"code","ff51850c":"code","94a7549d":"code","18434583":"code","aed314c1":"code","95261692":"code","8e18f04c":"code","f5a8d646":"code","92aa07f0":"code","289b0bcd":"code","e9fe35da":"code","6a70c288":"code","ec23b063":"code","eb37de08":"code","cffa419d":"code","a4123428":"code","80d6571a":"code","1f32c8f0":"code","f0a0779b":"code","a18dee73":"code","d193844d":"code","b8da708a":"code","8ddc4c6a":"code","03e78e87":"code","aa8e80e4":"code","d2f51922":"code","b063701c":"code","44e588a0":"code","1b6cdbba":"code","bb3629c9":"code","c1b93e78":"code","20af894e":"code","08b3636d":"code","d1a1af4c":"code","0327b549":"code","44c9444b":"code","090628d4":"code","433df16c":"code","30f4be12":"code","76849a44":"code","b4cc7c2c":"code","2ca6d30a":"code","0efe8999":"code","58bc4812":"code","466faec7":"code","14a94deb":"code","420506d3":"code","8ce80ccd":"code","f81d742f":"code","7dfdbb7c":"code","0626d19c":"code","7aa9ac07":"code","3716a391":"code","fe123b47":"code","854eb513":"code","dc42abcc":"code","e943367f":"code","ec53ad27":"code","0e5cc96a":"code","38a49c2c":"code","dd12c9a1":"code","f55f404e":"code","edb3a646":"code","59840083":"code","0c6d5647":"code","dc639ff0":"code","d78e6462":"code","c7ca24dc":"code","d307fe4c":"code","202f5a88":"code","69811df6":"markdown","411901d7":"markdown","31e1db8e":"markdown","7b62fb1b":"markdown","f41fc967":"markdown","506ca109":"markdown","f61333b9":"markdown","9c08760a":"markdown","42188d3c":"markdown","cf9ccb22":"markdown","a5e3d4ff":"markdown","c1330f15":"markdown","6b2bab59":"markdown","45a6ca86":"markdown","01a7b148":"markdown","0863acbf":"markdown","570df41b":"markdown","3ca70f6c":"markdown","cba7f23d":"markdown","cf5f7035":"markdown","0c2a185d":"markdown","c008d51a":"markdown","508db5aa":"markdown","38fd3657":"markdown","173232ee":"markdown","5439a90c":"markdown","e5be8c02":"markdown","d2974fd8":"markdown","88e46b1d":"markdown","fc5f5762":"markdown"},"source":{"19002418":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n","14a69bd6":"import warnings\nwarnings.filterwarnings('ignore')","d79c3199":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","63b7e750":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.shape","072a2cef":"train_df.info()","088ea91b":"train_df.head()","18c084ed":"train_df.describe().transpose()","b3ae2299":"train_df.hist(figsize=(20, 20), bins=20);","fea03035":"plt.figure(figsize=(26, 16))\nsns.heatmap(train_df.corr(), cmap='rocket', annot=True, fmt=f'0.1', cbar=False);","bb6d8fd1":"plt.figure(figsize=(12, 4))\nsns.distplot(train_df['SalePrice']);","b35bf6ef":"plt.figure(figsize=(12, 4))\nsns.distplot(np.log(train_df['SalePrice']));","87800220":"train_df.shape","ff51850c":"# Add price logarithm to dataset\ntrain_df['LogPrice'] = np.log(train_df['SalePrice'])\n\n# and remove SalePrice \ntrain_df = train_df.drop('SalePrice', axis=1)","94a7549d":"# Correlation target feature with others features\ntrain_df.corr()['LogPrice'].sort_values(ascending=False)","18434583":"sns.barplot(x='OverallQual', y='LogPrice', data=train_df);","aed314c1":"sns.scatterplot(x='GrLivArea', y='LogPrice', data=train_df);","95261692":"sns.scatterplot(x='GarageArea', y='LogPrice', data=train_df);","8e18f04c":"sns.scatterplot(x='TotalBsmtSF', y='LogPrice', data=train_df);","f5a8d646":"sns.scatterplot(x='LotFrontage', y='LogPrice', data=train_df);","92aa07f0":"sns.scatterplot(x='LotArea', y='LogPrice', data=train_df);","289b0bcd":"test_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","e9fe35da":"# Save train and test ID for final prediction on test part\ntest_id = test_df.pop('Id')\ntrain_id = train_df.pop('Id')\n\n# Save train length \nn_train = train_df.shape[0]\n\n# Set target variable and drop it from dataset\nlabels = train_df.pop('LogPrice')","6a70c288":"# Concatenate train and test part\ndf = pd.concat([train_df, test_df], axis=0)\ndf.reset_index(inplace=True, drop=True)","ec23b063":"df.shape","eb37de08":"test_df.shape, train_df.shape","cffa419d":"# Check empty values\npd.DataFrame({'Amount': df.isnull().sum(),\n             'Percent': (df.isnull().sum() \/ len(df)) *100}).sort_values(by='Percent', ascending=False)\n","a4123428":"df = df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)","80d6571a":"# FireplaceQu\ndf['Fireplaces'].value_counts()","1f32c8f0":"df['FireplaceQu'].value_counts()","f0a0779b":"df['FireplaceQu'].fillna('NA', inplace=True)","a18dee73":"# LotFrontage \nsns.distplot(df['LotFrontage'])","d193844d":"lot_frontage_median = df['LotFrontage'].median()\ndf['LotFrontage'] = df['LotFrontage'].fillna(lot_frontage_median)","b8da708a":"# Garages' features\ndf[df['GarageYrBlt'].isnull()].head()","8ddc4c6a":"# Numerical features replace with number\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(0) \n\n# Features replace with 'NA'\nfor column in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df[column] = df[column].fillna('NA')","03e78e87":"# Bsmts' features\ndf[df['BsmtExposure'].isnull()].head()","aa8e80e4":"for column in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df[column] = df[column].fillna('NA')","d2f51922":"# MasVnrType\ndf[df['MasVnrType'].isnull()].head() ","b063701c":"df['MasVnrType'].value_counts()","44e588a0":"df['MasVnrType'] = df['MasVnrType'].fillna('None')\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)","1b6cdbba":"# MSZoning\ndf['MSZoning'].value_counts()","bb3629c9":"df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])","c1b93e78":"# BsmtBaths\ndf['BsmtHalfBath'].value_counts()","20af894e":"df['BsmtFullBath'].value_counts()","08b3636d":"df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\ndf['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)","d1a1af4c":"# Functional\ndf['Functional'].value_counts()","0327b549":"df['Functional'] = df['Functional'].fillna(df['Functional'].mode()[0])","44c9444b":"# Electrical\ndf['Electrical'].value_counts()","090628d4":"df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])","433df16c":"# Utilities\ndf['Utilities'].value_counts()","30f4be12":"df['Utilities'] = df['Utilities'].fillna(df['Utilities'].mode()[0])","76849a44":"# TotalBsmtSF\ndf[df['TotalBsmtSF'].isnull()]","b4cc7c2c":"df.head(5)","2ca6d30a":"df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(df['1stFlrSF'])","0efe8999":"# BsmtUnfSf, BsmtFinSF2, BsmtFinSF1\ndf['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)\ndf['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\ndf['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)","58bc4812":"# Garage\ndf[df['GarageCars'].isnull()]","466faec7":"df['GarageCars'] = df['GarageCars'].fillna(0)\ndf['GarageArea'] = df['GarageArea'].fillna(0)","14a94deb":"# Rest empty categorical values fill with mode\ndf['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\ndf['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\ndf['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])","420506d3":"pd.DataFrame({'Amount': df.isnull().sum(),\n              'Percent': (df.isnull().sum() \/ len(df)) *100}).sort_values(by='Percent', ascending=False)\n","8ce80ccd":"qual_columns = ['GarageCond', 'GarageQual', 'FireplaceQu', 'KitchenQual', 'HeatingQC', \n           'BsmtCond', 'BsmtQual', 'ExterCond', 'ExterQual'] \nbsmt_columns = ['BsmtFinType2', 'BsmtFinType1'] \nexposure_columns = ['BsmtExposure']\n\nqual_rates = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nbsmtype_rates = {'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 2, 'LwQ': 1, 'Unf': -1, 'NA': 0}\nexposure_rates = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': -1, 'NA': 0}","f81d742f":"# Map features with clear scale\nfor feats, rate in ((qual_columns, qual_rates),  (bsmt_columns, bsmtype_rates), (exposure_columns, exposure_rates)):\n    for feat in feats:\n        df[feat] = df[feat].map(rate)","7dfdbb7c":"# LabelEncoder \nencode = ['Functional', 'CentralAir', 'PavedDrive', 'GarageFinish', 'Street', 'LandSlope']\n\nfor feat in encode:\n    df['{0}_cat'.format(feat)] = pd.factorize(df[feat])[0]\n\n# OneHotEncoding\ncategorical_features = [x for x in df.select_dtypes(include=np.object).columns if x not in encode]\n\nfor feat in categorical_features:\n    dummies = pd.get_dummies(df[feat], prefix='{0}'.format(feat), drop_first=True)\n    df = pd.concat([df, dummies], axis=1)","0626d19c":"df['BsmtFin'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] \ndf['TotalBsmt'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']","7aa9ac07":"df.shape","3716a391":"train_set = df[:n_train]\ntest_set = df[n_train:]","fe123b47":"# import necessary libraries\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","854eb513":"# Set X and y\nX = train_set[train_set.select_dtypes(include=np.number).columns].values\n\n# Normalise features\nscalar = MinMaxScaler()\nX_scaled = scalar.fit_transform(X)","dc42abcc":"# Create list of models\nlasso_model = Lasso()\nelastic_model = ElasticNet()\nsvr_model = SVR()\ntree_model = ExtraTreesRegressor()\nxgb_model = XGBRegressor()\nknn_model = KNeighborsRegressor()\n\nmodels = {'lasso_model': lasso_model,\n         'elastic_model': elastic_model,\n         'svr_model': svr_model,\n         'tree_model': tree_model,\n         'xgb_model': xgb_model,\n         'knn_model': knn_model}","e943367f":"def cross_validation(model, X, y):\n    \"Check model with cross validation\"\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    cross_score = np.sqrt(-score)\n    return round(np.mean(cross_score), 4)","ec53ad27":"# Check models with cross validation\nmodels_evaluation = {}\nfor model_name, model in models.items():\n    models_evaluation[model_name] = cross_validation(model, X_scaled, labels)\n    \npd.DataFrame(data=models_evaluation.items(), columns=['Model', 'RMSE']).sort_values(by='RMSE')","0e5cc96a":"xgb_model.fit(X_scaled, labels)","38a49c2c":"# Get features importances\nfeatures_list = sorted(zip(xgb_model.feature_importances_, train_set.select_dtypes(include=np.number).columns), reverse=True)\nfeatures_list","dd12c9a1":"# Leave only useful features\nimp_feats = [feat for (n, feat) in features_list if n > 0.001]","f55f404e":"# Set X with new feature set\nX = train_set[imp_feats].values\nX_scaled = scalar.fit_transform(X)","edb3a646":"cross_validation(xgb_model, X_scaled, labels)","59840083":"# Search better parameters for xgb_model \nparam_grid = {'n_estimators': np.arange(100, 1500),\n             'learning_rate': np.arange(0.01, 1, 0.01),\n             'max_depth': np.arange(1, 20),\n             'colsample_bytree': np.arange(0, 1, 0.1)}\n\nrandom_search = RandomizedSearchCV(xgb_model, param_grid, cv=10, scoring='neg_mean_squared_error', n_iter=100)\nrandom_search.fit(X_scaled, labels)\n\nbest_xgb = random_search.best_estimator_","0c6d5647":"cross_validation(best_xgb, X_scaled, labels)","dc639ff0":"# Search better parameters for svm model \nsvr_params = {'C': np.arange(1, 30),\n             'kernel': ('linear', 'poly', 'rbf', 'sigmoid')}\n\nhyperopt_svr = RandomizedSearchCV(svr_model, svr_params, cv=10, scoring='neg_mean_squared_error', n_iter=100)\nhyperopt_svr.fit(X_scaled, labels)\n\nbest_svr = hyperopt_svr.best_estimator_","d78e6462":"cross_validation(best_svr, X_scaled, labels)","c7ca24dc":"# Ensemble better models\nvoting_reg = VotingRegressor(estimators=[('xgb', best_xgb), ('svr', best_svr)])\n\ncross_validation(voting_reg, X_scaled, labels)","d307fe4c":"# Final model training \nvoting_reg.fit(X_scaled, labels)","202f5a88":"# Get X for test part\nX_test = test_set[imp_feats].values\nX_test_scaled = scalar.transform(X_test)\n\n# Make prediction\ny_pred = voting_reg.predict(X_test_scaled)\n\n# Convert LopPrice to normal and save it to csv in order to upload on Kaggle\ntest_file = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(y_pred)})\ntest_file.to_csv('submission.csv', index=False)","69811df6":"Same here. Where 'MasVnrType' is null, there MasVnrArea - 0","411901d7":"I will use all numerical features for training models  (labels is LogPrice). I will try a few models and pick the bests. ","31e1db8e":"It seems empty FireplaceQu in houses without Fireplace at all. I fill it with NA.","7b62fb1b":"Rest columns have a few empty values, but as I concatenate train and test part I can't delete it. <br>\nI'm going to fill rest values with 0, 'None' or with most occurred value.","f41fc967":"Let's add some new features.","506ca109":"## Feature engineering","f61333b9":"Let's see on some features with strong correlation.","9c08760a":"For 'LotFrontage' we have 486 empty records, there are too many to delete. And we can see that there are no values  equal to 0. So we can try to fill these empty values with median (because we have some outliers).","42188d3c":"Let's load train (and later test) data and have a look on it. In train dataset exist target variable 'SalePrice', in test dataset - no.","cf9ccb22":"So let's try to achieve a little bit more. I'm going to improve model using: <br>\n\n* Features importances (leave only significant features).\n* Search better parameters for models ","a5e3d4ff":"I'm going to unite SVM and XGB and train it with VotingRegressor. But first I will improve model's parameters.","c1330f15":"I will separate data on train and test datasets and I will store test data for final prediction. <br>","6b2bab59":"TotalBsmtSf has strong correlation with 1stFlrSf","45a6ca86":"## Exploratory data analysis","01a7b148":"## Hyperoptimization","0863acbf":"So, I'm going to concatenate train and test data in order to avoid duplicating code when I will be cleaning data. And later I will saparate it before training part.","570df41b":"## Test part","3ca70f6c":"\n\nSo, now is no more empty values. Let's transform categorical data to numbers.\nI will use 3 methods:\n\n*     Label encoder \n*     One hot encoding\n*     And for features with clear scale I will map these features\n\nGenerally we have 3 main feature types with clear scale, I will separate these features depends which scale their have.","cba7f23d":"In PoolQC, MiscFeature, Alley, Fence most of data is missing. I'm goind to drop all these columns.","cf5f7035":"## Data cleaning","0c2a185d":"Data is right-skewed, let's see if log of price can handle with outliers.","c008d51a":"So, these steps improved our models. Now let's combine better models to one and train our final model.","508db5aa":"## Machine learing part","38fd3657":"Let's take a look  little bit closer to our target feature.","173232ee":"The same situation as above: where 'BsmtExposure' is 'null', there other 'Bsmt' features are 'null', as well.","5439a90c":"Notebook have following structure:\n1. Exploratory data analysis\n1. Data cleaning\n1. Feature engineering\n1. Train part\n1. Hyperoptimization\n1. Test part","e5be8c02":"So, I achieved 0.12089 score for test set in Kaggle competition (Kaggle use RMSLE metric).","d2974fd8":"\n\nWhere 'GarageYrBlt' equals 'NaN', there other empty 'garage' values is empty, as well. Follow desciription data it means no garage.\n","88e46b1d":"Yes, logarithmic 'SalePrice' looks better due to normal distribution and I will use LogPrice as target variable.","fc5f5762":"Now let's encode rest of categorical features with LabelEncoder and OneHotEncoding. I will use pandas function **factorize** and **get_dummies**, it gives the same result as LebelEncoder and OneHotEncoder from sklearn."}}