{"cell_type":{"9159dfff":"code","e1098110":"code","fe5b945c":"code","51c07166":"code","82a6db8c":"code","29a015b4":"code","d6b44222":"code","f5694af6":"code","eca0adae":"code","07554854":"code","46479647":"code","12ce3437":"code","3c2ac72f":"code","120e192b":"code","732f2bae":"code","40935468":"code","8def1093":"code","d7b51e59":"code","4a6119fa":"code","fa69b24f":"code","43693742":"code","17aba91f":"code","a5a94b94":"code","b10ad49b":"code","fa43ea93":"code","a0aa3ded":"code","41feb686":"code","cab66da6":"code","65873f6b":"code","d0cefb2f":"code","1ae7bc40":"code","3549cc93":"code","77968d78":"code","18d301fe":"code","cbc21b20":"code","8d4f115a":"code","6cd9bfe4":"code","32330bee":"code","49b15f15":"code","a8b748a3":"code","d529e317":"code","b5430f08":"code","9aaba31e":"code","738e809d":"code","990f04ca":"code","7b3561d9":"code","67391464":"code","9a76b3c8":"code","7fa05add":"code","714debcd":"code","7117f30d":"code","0e63178e":"code","c11e7ffb":"code","45f76a9b":"code","f02a684a":"code","74864047":"code","f214756d":"code","f1d59d58":"code","7200931d":"code","d3406703":"code","5caadbdd":"code","49e9a350":"code","88a427a8":"markdown","db146313":"markdown","d69cfe45":"markdown","7d241309":"markdown","b1075d99":"markdown","87888e74":"markdown","03e50125":"markdown","9602c8e7":"markdown","c86e1e2d":"markdown","cafb1dfc":"markdown","c45fffec":"markdown","e196f9e6":"markdown","45e159b5":"markdown","6fe20bad":"markdown","6bb120d2":"markdown","a202f901":"markdown","aac5eaeb":"markdown","b8925a7b":"markdown","488b82aa":"markdown","bfa3025e":"markdown","e836c0f4":"markdown","c7adc8bd":"markdown","dc7f03b7":"markdown","68247304":"markdown","62784d74":"markdown","994cab35":"markdown","a703002a":"markdown","c9b83d86":"markdown","1a46de56":"markdown","4a0ba077":"markdown","8ace2f03":"markdown","af5b1abe":"markdown","a940163e":"markdown","ac86756d":"markdown","0a4ffd77":"markdown","3c0e6804":"markdown","84087484":"markdown","5d07e6f9":"markdown","f1726f54":"markdown","da05d5ca":"markdown","d5e89e42":"markdown","c7475911":"markdown","52272d9c":"markdown","cd71d202":"markdown","188ba6e2":"markdown","ffa2272b":"markdown","69bd45b5":"markdown","72f048db":"markdown"},"source":{"9159dfff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1098110":"pd.set_option('display.max_columns', None)","fe5b945c":"import matplotlib.pyplot as plt\nimport seaborn as sns","51c07166":"family_income_data = pd.read_csv('..\/input\/family-income-and-expenditure\/Family Income and Expenditure.csv')","82a6db8c":"family_income_data.head()","29a015b4":"import missingno as msno","d6b44222":"msno.matrix(family_income_data)","f5694af6":"expenditures = [column for column in family_income_data.columns if 'Expenditure' in column]","eca0adae":"expenditures","07554854":"X = family_income_data.loc[:, expenditures]\ny = family_income_data['Total Household Income']","46479647":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.preprocessing import Normalizer, PolynomialFeatures, MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import IncrementalPCA, SparsePCA, KernelPCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, precision_score,recall_score, classification_report, confusion_matrix","12ce3437":"plt.figure(figsize=(20, 20))\ni = 1\nfor exp in expenditures :\n    plt.subplot(6,3,i)\n    sns.regplot(x=X[exp], y=y)\n    i += 1","3c2ac72f":"Xy = X.copy()\nXy['THI'] = y","120e192b":"Xy_corr = Xy.corr()","732f2bae":"plt.figure(figsize=(10,10))\nsns.heatmap(Xy_corr, square=True)","40935468":"plt.figure(figsize=(10,10))\nplt.subplot(3,1,1)\nsns.regplot(x=family_income_data['Total Rice Expenditure'], y = family_income_data['Bread and Cereals Expenditure'])\nplt.subplot(3,1,2)\nsns.regplot(x=family_income_data['Total Food Expenditure'], y = family_income_data['Vegetables Expenditure'])\nplt.subplot(3,1,3)\nsns.regplot(x=family_income_data['Total Food Expenditure'], y = family_income_data['Meat Expenditure'])","8def1093":"plt.figure(figsize=(15, 25))\ni = 1\nfor exp in expenditures :\n    plt.subplot(6,3,i)\n    sns.distplot(X[exp])\n    i += 1","d7b51e59":"plt.figure(figsize=(10, 10))\nsns.distplot(y, bins=1000)","4a6119fa":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)","fa69b24f":"rfr = RandomForestRegressor(verbose=True, n_jobs=-1, n_estimators=1000)\nrfr.fit(X_train, y_train)","43693742":"rfr.score(X_test, y_test)","17aba91f":"y_rfr_predict = rfr.predict(X)\nmean_squared_error(y, y_rfr_predict)\nplt.figure(figsize=(20,5))\nax = sns.regplot(x=y, y = y_rfr_predict)\nax.set(xlabel='Total Household Income', ylabel='Predicted TIH')","a5a94b94":"knr = KNeighborsRegressor(n_neighbors=15, n_jobs=-1, leaf_size=50)\nknr.fit(X_train, y_train)","b10ad49b":"knr.score(X_test, y_test)","fa43ea93":"y_knr_predict = knr.predict(X)\nmean_squared_error(y, y_knr_predict)\nplt.figure(figsize=(20,5))\nax = sns.regplot(x=y, y = y_knr_predict)\nax.set(xlabel='Total Household Income', ylabel='Predicted TIH')","a0aa3ded":"xgbr = XGBRegressor(nthread = -1, eta=0.1, subsample=0.5)\nxgbr.fit(X_train, y_train)","41feb686":"xgbr.score(X_test, y_test)","cab66da6":"y_xgbr_predict = xgbr.predict(X)\nmean_squared_error(y, y_xgbr_predict)\nsns.regplot(x=y, y = y_xgbr_predict)\nplt.figure(figsize=(20,5))","65873f6b":"class_fid = family_income_data.loc[: , ['Household Head Sex' ,'Household Head Age' ,'Household Head Marital Status' ,'Household Head Highest Grade Completed' ,'Household Head Job or Business Indicator' ,'Household Head Class of Worker' ,'Type of Household' ,'Total Number of Family members' ,'Total number of family members employed' ,'Type of Building\/House' ,'Type of Roof' ,'Type of Walls' ,'House Floor Area' ,'House Age' ,'Number of bedrooms' ,'Tenure Status' ,'Toilet Facilities' ,'Electricity' ,'Main Source of Water Supply' ,'Number of Television' ,'Number of CD\/VCD\/DVD' ,'Number of Component\/Stereo set' ,'Number of Refrigerator\/Freezer' ,'Number of Washing Machine' ,'Number of Airconditioner' ,'Number of Car, Jeep, Van' ,'Number of Landline\/wireless telephones' ,'Number of Cellular phone' ,'Number of Personal Computer' ,'Number of Stove with Oven\/Gas Range' ,'Number of Motorized Banca' ,'Number of Motorcycle\/Tricycle' ]]","d0cefb2f":"class_fid_cat = []\nfor col in class_fid.columns :\n    if class_fid[col].dtype == object :\n        class_fid_cat.append(col)\n        print(col,\" : \",len(class_fid[col].value_counts()))","1ae7bc40":"for item in family_income_data['Household Head Highest Grade Completed'].value_counts().index :\n    print(\".\", item)","3549cc93":"educ_attainment = { 'DNA\/Primary\/Elementary' : ['Elementary Graduate', 'Grade 4', 'Grade 5', 'Grade 3', 'Grade 2', 'Grade 1', 'Grade 6', 'No Grade Completed', 'Preschool'], \n                    'Secondary' : ['High School Graduate', 'Second Year High School', 'Third Year High School', 'First Year High School'],\n                    'Attended College' : ['Second Year College', 'Third Year College', 'First Year College', 'Second Year Post Secondary', 'Fourth Year College', 'First Year Post Secondary'],\n                    'Post Baccalaureate' : ['Post Baccalaureate'], \n                    'Degrees\/Programs' : ['Business and Administration Programs', 'Teacher Training and Education Sciences Programs', 'Engineering and Engineering Trades Programs', 'Engineering and Engineering trades Programs', 'Engineering and Engineering trades Programs', 'Health Programs', 'Computing\/Information Technology Programs', 'Security Services Programs', 'Agriculture, Forestry, and Fishery Programs',\n                                  'Transport Services Programs', 'Social and Behavioral Science Programs', 'Social and Behavioral Science Programs', 'Personal Services Programs', 'Humanities Programs', 'Other Programs in Education at the Third Level, First Stage, of the Type that Leads to an Award not Equivalent to a First University or Baccalaureate Degree',\n                                  'Law Programs', 'Architecture and Building Programs', 'Basic Programs', 'Journalism and Information Programs', 'Arts Programs', 'Life Sciences Programs', 'Manufacturing and Processing Programs',\n                                  'Social Services Programs', 'Physical Sciences Programs', 'Other Programs of Education at the Third Level, First Stage, of the Type that Leads to a Baccalaureate or First University\/Professional Degree (HIgher Education Level, First Stage, or Collegiate Education Level)',\n                                  'Veterinary Programs', 'Environmental Protection Programs'\n                                 ]\n                    }","77968d78":"family_income_data['Household Head Highest Grade Completed (Simplified)'] = family_income_data['Household Head Highest Grade Completed'].apply(lambda x : ''.join([key for key in educ_attainment.keys() if x in educ_attainment[key]]))","18d301fe":"family_income_data.head()","cbc21b20":"family_income_data['Total Household Income'].describe()","8d4f115a":"family_income_data['Income Category'] = pd.qcut(family_income_data['Total Household Income'], q=4, labels=['Category 1', 'Category 2', 'Category 3', 'Category 4'])","6cd9bfe4":"family_income_data","32330bee":"XX =  family_income_data.loc[:, [ 'Household Head Sex', 'Household Head Age', 'Household Head Marital Status','Household Head Highest Grade Completed (Simplified)','Household Head Job or Business Indicator','Household Head Class of Worker' , 'Type of Household' , 'Total Number of Family members' , 'Total number of family members employed', 'Type of Building\/House', 'Type of Roof' , 'Type of Walls' , 'House Floor Area' , 'House Age' , 'Number of bedrooms' , \n'Tenure Status' , 'Toilet Facilities' , 'Electricity' , 'Main Source of Water Supply' , 'Number of Television' , 'Number of CD\/VCD\/DVD' , 'Number of Component\/Stereo set' , 'Number of Refrigerator\/Freezer' , 'Number of Washing Machine' , \n'Number of Airconditioner' , 'Number of Car, Jeep, Van' , 'Number of Landline\/wireless telephones' , 'Number of Cellular phone' , 'Number of Personal Computer' , 'Number of Stove with Oven\/Gas Range' , 'Number of Motorized Banca' , 'Number of Motorcycle\/Tricycle']]\nyy = family_income_data['Income Category']","49b15f15":"XX","a8b748a3":"from sklearn.preprocessing import OneHotEncoder","d529e317":"msno.matrix(XX)","b5430f08":"XX['Household Head Class of Worker'].fillna('N\/A', inplace=True)","9aaba31e":"class_fid_cat","738e809d":"class_fid_cat[2] = 'Household Head Highest Grade Completed (Simplified)'","990f04ca":"XX_cat = XX.loc[:, class_fid_cat]","7b3561d9":"ohe = OneHotEncoder(sparse=False) #setting sparse=False since I want to see my array\nXX_t = ohe.fit_transform(XX_cat)","67391464":"XX_t","9a76b3c8":"class_fid_num = [col for col in X.columns if col not in class_fid_cat]\nXX_tt_0 = family_income_data[class_fid_num]\nXX_tt = XX_tt_0.to_numpy() #necessary step since I'm scared that I won't be able to merge a numpy array and a dataframe","7fa05add":"XX_T = np.concatenate((XX_t, XX_tt), axis=1) #axis 1 tells me I am concatenating column-wise","714debcd":"XX_train, XX_test, yy_train, yy_test = train_test_split(XX_T, yy, test_size=0.2, random_state=41)","7117f30d":"rfc = RandomForestClassifier(verbose = True, n_jobs=-1)\nyy_fit_rfc = rfc.fit(XX_train, yy_train)","0e63178e":"yy_pred_rfc = yy_fit_rfc.predict(XX_T)","c11e7ffb":"print(classification_report(yy, yy_pred_rfc))","45f76a9b":"labels = ['Category 1', 'Category 2', 'Category 3', 'Category 4']\ncm = confusion_matrix(yy, yy_pred_rfc, labels=labels)\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm, cmap='cubehelix')\nplt.title('Confusion Matrix - Random Forest Classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\n","f02a684a":"knc = KNeighborsClassifier(n_jobs=-1)\nyy_fit_knc= knc.fit(XX_train, yy_train)","74864047":"yy_pred_knc = yy_fit_knc.predict(XX_T)","f214756d":"print(classification_report(yy, yy_pred_knc))","f1d59d58":"labels = ['Category 1', 'Category 2', 'Category 3', 'Category 4']\ncm = confusion_matrix(yy, yy_pred_knc, labels=labels)\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm, cmap='cubehelix')\nplt.title('Confusion Matrix - KNeighbors Classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","7200931d":"xbc = XGBClassifier(verbose = True, n_jobs=-1)\nyy_fit_xbc = xbc.fit(XX_train, yy_train)","d3406703":"yy_pred_xbc = yy_fit_xbc.predict(XX_T)","5caadbdd":"print(classification_report(yy, yy_pred_xbc))","49e9a350":"labels = ['Category 1', 'Category 2', 'Category 3', 'Category 4']\ncm = confusion_matrix(yy, yy_pred_knc, labels=labels)\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm, cmap='cubehelix')\nplt.title('Confusion Matrix - XGB Classifer')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","88a427a8":"Next, my goal is to see how the features and the income is correlated, and I want to expose this visually using a heatmap and the .corr() method.","db146313":"At this point, I want to show how skewed the data is. This is to set my expectation for regression modelling later.","d69cfe45":"Woops, need to clean that up with `fillna`. Let's just place a string called 'N\/A' for those.","7d241309":"That diagonal lighter shade tells me that a large part of my Predicted matched the Actual values.","b1075d99":"Looks great! Now time to One-Hot encode using sklearn's preprocessing library.","87888e74":"### Random Forest Classifier","03e50125":"Okay, so I tried to see how the data points are distributed and there is a heavy skew to the left. This tells me that a large number of participants in the dataset spends about the same amount for expenses. The KDE's, however, tell a different story. Most peaks are below the bin with the most number of datapoints, which tells us there are values to the extreme right that are skewing the curve heavily. \n\nWhile a lot of people agree on spending a certain range, there also exists a group that are spending at a maximum.\n\nAll the while, the distribution of income is this:","9602c8e7":"Let's see if we can predict `Total Household Income` through the expenditures each family makes. I initiate this by taking all the column names with 'expenditures' in it.","c86e1e2d":"I have to create the income categories first before fitting a model. I decided to make four categories that will be used as labels based on the Total Household Income values:\n\n1. **Category 1** - (< 25% of the distribution)\n2. **Category 2** - (25%-50% of the distribution)\n3. **Category 3** - (50%-75% of the distribution)\n4. **Category 4** - (> 75% of the distribution)\n\nI think to make things simple, dividing using the quartiles will suffice.","cafb1dfc":"### XGB Classifier","c45fffec":"### Random Forest Regressor","e196f9e6":"Earlier, I made a list of categorical values, and I remember I created a new feature called 'Household Head Highest Grade Completed (Simplified)'. I want to use this in place of the huge 'Household Head Highest Grade Completed ' column.","45e159b5":"Lastly, let's do an XGB Classifier.","6fe20bad":"I've pulled this dataset from Kaggle.com. The file describes the income and expenditure characteristics of Filipino households.","6bb120d2":" ### K-Nearest Neighbors Regressor","a202f901":"This is an extremely ridiculous distribution curve. I set 20 bins and still the most distinguishable is still three.\n\nWhat this tells me is that almost everyone in the dataset has a `Total Household Income` of between 5000-10000. It's quite disturbing.","aac5eaeb":"### KNeighbors Classifier","b8925a7b":"Now, I can train-test-split my data. Similar to the regression part, I'm using an 80%-20% division:","488b82aa":"From here, it seems like the best correlation I have is 0.8, which is `Total Rice Expenditure` and `Bread and Cereals Expenditure`. Next to that, we have `Total Food Expenditure` has a somehow high correlation value with `Meat Expenditure` and `Vegetables Expenditure`. Makes sense especially that **Pinoys** are a culture of *rice and ulam*. It's not surprising that these are related.\n\nVisualizing these three in a regression plot:","bfa3025e":"But before I go to training a model, I wanna see first how the the features appear in a scatterplot when treated as a function of the label.","e836c0f4":"Now, I can make the categorical section of my XX data set, and then One-Hot encode it.","c7adc8bd":"First thing I want to is to check if there are missing values in my dataset. I accomplish this using the **missingno** library's Matrix method. This helps me visualize where the null values are.","dc7f03b7":"Next, I manually sort them out to five categories:","68247304":"Before I get to the task of one-hot encoding it, I have to know how big I could make the dataset by checking the unique values per feature columns. Just the ones with types 'object':","62784d74":"I set my features as the splice of the original dataset where the column names are expenditures, and set the target as the `Total Household Income` column","994cab35":"Importing the necessary libraries for fitting.","a703002a":"I'm afraid the `Household Head Highest Grade Completed` could slow down my simulation, so I'm going to simplify this:","c9b83d86":"Okay, now let's train a model. I'm going to pick `RandomForestRegressor` and `KNeighborsRegressor`, since these are great picks for chaotic scatterplots. Then, I'm also throwing in an `XGBRegressor` in the end to see if an optimized model can perform better than the earlier two.","1a46de56":"With respect to the selected label, a lot of the features show a huge variance. This will limit the reliability of the regression model later.","4a0ba077":"From the three models used, it seems like the best to yield score is the Random Forest Regressor. This model can still be improved by scaling the data points and normalizing the distribution.","8ace2f03":"## TASK 1: Make a model to Predict Household Income through the Expenditures","af5b1abe":"Moreover, I'm adding a comparative regplot for the actual values for Total Household Income and the ones predicted by the model. Ideally, we want to see them fit inside the regression line to say that \"Okay, this is a good model.\"","a940163e":"Then, I use pd.Series.apply() to create a new column with these categories","ac86756d":"## TASK 2: Predicting Income Bracket Through Classification","0a4ffd77":"KNeighbors didn't do as well as the Random Forest Regressor.","3c0e6804":"Same with earlier, I need to know if I have any null values for any of my features.","84087484":"I use pd.qcut to divide the data by quartiles, and use custom labels as I have described earlier:","5d07e6f9":"Checking the values I have:","f1726f54":"Awesome! I think this model did pretty well. Let's add a confusion matrix comparison just to see how the two values vary","da05d5ca":"### XGBRegressor","d5e89e42":"First, I have to check the unique values for the column.","c7475911":"For this task, I aim to predict the income category of each household using the following features:\n -  Household Head Sex\n -  Household Head Age\n -  Household Head Marital Status\n -  Household Head Highest Grade Completed\n -  Household Head Job or Business Indicator\n -  Household Head Class of Worker\n -  Type of Household\n -  Total Number of Family members\n -  Total number of family members employed\n -  Type of Building\/House\n -  Type of Roof\n -  Type of Walls\n -  House Floor Area\n -  House Age\n -  Number of bedrooms\n -  Tenure Status\n -  Toilet Facilities\n -  Electricity\n -  Main Source of Water Supply\n -  Number of Television\n -  Number of CD\/VCD\/DVD\n -  Number of Component\/Stereo set\n -  Number of Refrigerator\/Freezer\n -  Number of Washing Machine\n -  Number of Airconditioner\n -  Number of Car, Jeep, Van\n -  Number of Landline\/wireless telephones\n -  Number of Cellular phone\n -  Number of Personal Computer\n -  Number of Stove with Oven\/Gas Range\n -  Number of Motorized Banca\n -  Number of Motorcycle\/Tricycle\n \n For consistency, I will also be using the same models I used in the Regression task: Random Forest,KNeigbors, and XGBoost Classifiers.","52272d9c":"Let's visually observe how the predicted values and the actual values correlate in a 45-degree line. The closer the points are in the line, the more it tells us that the model we selected did a great job in terms of predicting values.\n\nWe'll also repeat this method for the next models we use.","cd71d202":"Okay, at this point, I can create my feature and label sets:","188ba6e2":"Great! Now, my categorical features are encoded to ones and zeroes. Time to append those to my numerical ones and create my final XX set.","ffa2272b":"I'm setting verbose to True so that I can see what's happening under the model as it happens.","69bd45b5":"Let's see how well the predictions made are using the classification report:","72f048db":"Seeing this final product, the best classifier to use to predict the Income Category was the Random Forest Classifier. KNN predicted worse possibly because it's hard to define a nearest neighbor classification with the huge variance in our dataset. XGBoost, on the other hand, performs better for Time-Series data."}}