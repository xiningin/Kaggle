{"cell_type":{"1fee907d":"code","b140b0d6":"code","0d4b6573":"code","6ef9958f":"code","82796315":"code","54c5a444":"code","71ba0318":"code","b5f08b4c":"code","6e861d76":"code","fdb5a5f1":"code","7b1455c0":"code","9d6cb0f3":"code","3cf2621f":"code","4c2d17fc":"code","554f7e14":"code","1c2e3e3c":"code","1c16cd69":"code","ae284880":"code","70eea339":"code","3bfe1144":"code","3cdeba1c":"code","9ca2a1a0":"code","5a96b477":"code","c6f3d2b0":"code","310f9080":"code","206d37ff":"code","d369239c":"code","971c6201":"code","b305c781":"code","d29d9acc":"code","8b987a52":"code","a224127f":"code","6320a271":"code","51aa2ebb":"code","d069933b":"code","3be651c4":"code","f3a40b10":"code","888a4fb0":"code","4fef49d5":"code","f2be9511":"code","57caac97":"code","f70a32c5":"code","7b885ea5":"code","ccb57ca1":"code","aa7d00de":"code","fe6d432b":"code","9bbf1cc3":"code","7d84f8b8":"code","4c45b777":"code","653125c9":"code","0a563a3f":"code","b7e5193b":"code","7c502675":"code","da8e14af":"code","3cbc2a19":"code","1ff0319c":"code","da13f587":"code","be70b117":"code","9d6543e4":"markdown","f612e723":"markdown","298ca83d":"markdown","ef5466dc":"markdown","a043eb7a":"markdown","21719d0c":"markdown","932b41af":"markdown","58a8b729":"markdown","d4c16e63":"markdown","f5c0b795":"markdown","0d73a5e0":"markdown","22213eb6":"markdown","189d1e28":"markdown","35a7edf1":"markdown","f36bca2b":"markdown","16557dda":"markdown","c2f01b4f":"markdown","c59ae463":"markdown","d8afe3e5":"markdown","2e4891ee":"markdown","87721f3e":"markdown","350d0130":"markdown","7e4bfbb2":"markdown","fe1e95ba":"markdown","23cd4847":"markdown","afeb7288":"markdown","729c22d7":"markdown","463c98f5":"markdown","008f5641":"markdown","bd375d54":"markdown","8f4debd3":"markdown","4030ef30":"markdown","06b94eb5":"markdown","21791621":"markdown","c1047b83":"markdown","298263a7":"markdown","ef87d09a":"markdown","84f23035":"markdown","fc1dbeb0":"markdown","42195f92":"markdown","8caee2a9":"markdown","432cb16d":"markdown","2451b9a8":"markdown"},"source":{"1fee907d":"#Needed libraries and modules\nimport pandas as pd                                  # data processing, reading CSV file, dealing with dataframes etc\nimport numpy as np                                   # linear algebra functionalities \nimport seaborn as sns                                # visualization library\nimport matplotlib.pyplot as plt                      # visualization library\n\n#Modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport time\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b140b0d6":"path = '..\/input\/titanic\/train.csv'\ndf = pd.read_csv(path)\ndf","0d4b6573":"df.describe()","6ef9958f":"#Description of non-numerical features\ndf.describe(include=['object'])","82796315":"df.info()","54c5a444":"df.corr()","71ba0318":"ContVars = [\"Fare\", \"Age\"]\n\nfor n in ContVars:\n    width = 12\n    height = 4\n    plt.figure(figsize=(width, height))\n    plt.hist(df[n], bins = 50)\n    plt.xlabel(n)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of {} distrubution\".format(n))\n    plt.show","b5f08b4c":"CatVars1 = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\nfor var in CatVars1:\n    A= df[var].value_counts()\n    width = 9\n    height = 3\n    plt.figure(figsize=(width, height))\n    plt.bar(A.index, A)\n    plt.ylabel(\"Amount\")\n    plt.title(var)\n    plt.show()\n    print(var,\"\\n\", A)\n ","6e861d76":"CatVars2 = ['Ticket', 'Cabin']\n\nfor var in CatVars2:\n    print(var, \"\\n\", df[var].value_counts(), \"\\n\")\n","fdb5a5f1":"#Correlation\nsns.heatmap(df[[\"Fare\", \"Age\", 'Survived']].corr(), annot = True)","7b1455c0":"#Scatter plot Age, Fare for survied and non-survived passengers\nCartPl = sns.FacetGrid(df, col = \"Survived\")\nCartPl.map(sns.scatterplot, \"Age\", \"Fare\")\nfig = plt.gcf()\nfig.set_size_inches(18, 6)\nplt.show()\n\n","9d6cb0f3":"#Scatter plot Age, Fare for both survived and non-survived\n\nsns.lmplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", data=df)\nfig = plt.gcf()\nfig.set_size_inches(21, 7)\nplt.show()","3cf2621f":"#Distribution plots Age and  Fare for survied and non-survived passengers\nCartPlA = sns.FacetGrid(df, col = \"Survived\")\nCartPlA.map(sns.distplot, \"Age\")\nfig = plt.gcf()\nfig.set_size_inches(18, 6)\nplt.show()\n\nCartPlF = sns.FacetGrid(df, col = \"Survived\")\nCartPlF.map(sns.distplot, \"Fare\")\nfig = plt.gcf()\nfig.set_size_inches(18, 6)\nplt.show()\n\n","4c2d17fc":"CatVars3 = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Ticket', 'Cabin']\n\nfor var in CatVars3:\n    Aux = df[[str(var),'Survived']].groupby([str(var)],as_index=False).mean().sort_values(by=\"Survived\", ascending = False)\n    print(Aux, \"\\n\")","554f7e14":"#list(df.columns)\nModel_DF = df[['Survived', 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\nModel_DF","1c2e3e3c":"# replace all the entries \"?\" by NaN. \nModel_DF.replace(\"?\", np.nan, inplace=True)\nModel_DF","1c16cd69":"Model_DF.iloc[4]['Age']","ae284880":"#Finding columns with missing values\n\nModel_DF.columns[Model_DF.isnull().any()]","70eea339":"#Amount of missing values per column\n\nModel_DF.isnull().sum()","3bfe1144":"Model_DF[Model_DF[\"Embarked\"].isnull()]","3cdeba1c":"sns.boxplot(x=\"Embarked\", y=\"Fare\", data=Model_DF)","9ca2a1a0":"CartPlA = sns.FacetGrid(Model_DF, col=\"Embarked\")\ng = CartPlA.map(sns.distplot, \"Fare\")\nfig = plt.gcf()\nfig.set_size_inches(15, 5)\nplt.show()\n","5a96b477":"Emb = ['S', 'C', 'Q']\n\nfor i in Emb:\n    dp = sns.distplot(Model_DF[Model_DF['Embarked']==i]['Fare'], bins = range(5,500, 10), hist = True)\n    plt.title('Embarked '+ i)  \n    plt.xlabel('Fare')\n    plt.show()\n    plt.close()\n    #we measure the height if the bin in the histogram around 80\n    l = [[h.xy[0], h.get_width(), h.get_height()] for h in dp.patches]\n    print('Left edge, width and height of the bin around Fare=80:', l[7] )","c6f3d2b0":"print('Amount for S:', len(Model_DF[(Model_DF['Fare']>=75) &  (Model_DF['Fare']<85) & (Model_DF['Embarked'] == 'S')]))\nprint('Amount for C:', len(Model_DF[(Model_DF['Fare']>=75) &  (Model_DF['Fare']<85) & (Model_DF['Embarked'] == 'C')]))\nprint('Amount for Q:', len(Model_DF[(Model_DF['Fare']>=75) &  (Model_DF['Fare']<85) & (Model_DF['Embarked'] == 'Q')]))","310f9080":"#Filling mising values for \"embaqued\"\nModel_DF[\"Embarked\"] = Model_DF[\"Embarked\"].fillna(\"C\")\n\nModel_DF[Model_DF[\"Embarked\"].isnull()]","206d37ff":"Model_DF[Model_DF[\"Age\"].isnull()]","d369239c":"sns.factorplot(x = \"Sex\", y = \"Age\", data = Model_DF, kind = \"box\")\nsns.factorplot(x = \"Pclass\", y = \"Age\", data = Model_DF, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = Model_DF, kind = \"box\")\nsns.factorplot(x = \"Parch\", y = \"Age\", data = Model_DF, kind = \"box\")\nplt.show()\n\n\nsns.scatterplot( Model_DF[\"Age\"], Model_DF[\"Fare\"])\nplt.show()\n\n\n","971c6201":"Model_DF[[\"Age\", \"Fare\"]].corr()","b305c781":"#Indixes missing age in a list\nIndexes_missing_age = list(Model_DF[\"Age\"][Model_DF[\"Age\"].isnull()].index)\n\nfor j in Indexes_missing_age:\n    Pred_Age = Model_DF[\"Age\"][(Model_DF[\"SibSp\"] == Model_DF.iloc[j][\"SibSp\"]) & (Model_DF[\"Pclass\"] == Model_DF.iloc[j][\"Pclass\"]) & (Model_DF[\"Parch\"] == Model_DF.iloc[j][\"Parch\"])].mean()\n    Model_DF[\"Age\"][j] = Pred_Age","d29d9acc":"Model_DF[Model_DF[\"Age\"].isnull()]","8b987a52":"\n# Getting all the indixes missing age in a list\nIndexes_missing_age1 = list(Model_DF[\"Age\"][Model_DF[\"Age\"].isnull()].index)\n\nPred_Age =  Model_DF[\"Age\"].mean()\n\nfor j in Indexes_missing_age1:\n    Model_DF[\"Age\"][j] = Pred_Age","a224127f":"Model_DF[Model_DF[\"Age\"].isnull()]","6320a271":"Model_DF.describe()","51aa2ebb":"Model_DF['Age_Nor'] = (Model_DF['Age']-Model_DF['Age'].mean())\/Model_DF['Age'].std()\nModel_DF['Fare_Nor'] = (Model_DF['Fare']-Model_DF['Fare'].mean())\/Model_DF['Fare'].std()\nModel_DF['Pclass_Nor'] = (Model_DF['Pclass']-Model_DF['Pclass'].mean())\/Model_DF['Pclass'].std()\nModel_DF['SibSp_Nor'] = (Model_DF['SibSp']-Model_DF['SibSp'].mean())\/Model_DF['SibSp'].std()\nModel_DF['Parch_Nor'] = (Model_DF['Parch']-Model_DF['Parch'].mean())\/Model_DF['Parch'].std()","d069933b":"#Before and after normalization\nModel_DF[['Age', 'Fare', 'Pclass', 'SibSp', 'Parch', 'Age_Nor', 'Fare_Nor', 'Pclass_Nor', 'SibSp_Nor', 'Parch_Nor']]","3be651c4":"#Deleting the redundant columns \nModel_DF.drop(['Age'], axis=1, inplace=True)\nModel_DF.drop(['Fare'], axis=1, inplace=True)\nModel_DF.drop(['Pclass'], axis=1, inplace=True)\nModel_DF.drop(['SibSp'], axis=1, inplace=True)\nModel_DF.drop(['Parch'], axis=1, inplace=True)\n\n\n#Checking the trasnformed data\nModel_DF.head()","f3a40b10":"Model_DF = pd.get_dummies(Model_DF, columns=['Sex'])\nModel_DF","888a4fb0":"Model_DF = pd.get_dummies(Model_DF, columns=['Embarked'])\nModel_DF","4fef49d5":"Model_DF.drop(['Sex_male'], axis=1, inplace=True)\nModel_DF.drop(['Embarked_S'], axis=1, inplace=True)\n\nModel_DF","f2be9511":"#Let's check that our features are of correct type\nModel_DF.info()","57caac97":"Model_DF.describe()","f70a32c5":"#Depenent variable\nY = Model_DF[\"Survived\"]\n\n#Independent variables\nX = Model_DF.drop(\"Survived\",axis=1)\n\n#Splitting\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n\nprint(\"Shape of train and test vectors\", X_train.shape, X_test.shape)","7b885ea5":"LogReg_param_grid = [{'penalty': [ 'l1', 'l2' ], 'C': list(np.linspace(0,3,8))}]\n\nLogReg_best_estimators = []\nLogReg_best_scores = []\nLogReg_best_params = []\n\ndegrees = 5\n\ntic = time.time()\n\nfor d in range(degrees):\n    pf = PolynomialFeatures(degree=d+1)\n    Xd_train = pf.fit_transform(X_train)\n    Grid = GridSearchCV(estimator=LogisticRegression(), param_grid=LogReg_param_grid, scoring='accuracy', \n                        cv=4, n_jobs=multiprocessing.cpu_count())\n    Grid.fit(Xd_train, Y_train)\n    LogReg_best_estimators.append(Grid.best_estimator_)\n    LogReg_best_scores.append(Grid.best_score_)\n    LogReg_best_params.append(Grid.best_params_)\n    \ntoc = time.time()\nprint('Done in:', toc-tic)","ccb57ca1":"\nplt.plot(range(1,degrees+1), LogReg_best_scores)\nplt.title('Accurcay_vs_Degree') \nplt.show()","aa7d00de":"print(\"Best Logistic regression model:\\n\", LogReg_best_estimators[1])\nprint(\"Best Logistic regression parameters:\\n\", LogReg_best_params[1])\nprint(\"Score for this model:\", LogReg_best_scores[1])","fe6d432b":"SVM_param_grid = [{'kernel': ['rbf', 'poly', 'sigmoid'], \n               'C': list(np.linspace(0,15,25)),\n               'degree': [2,3,4,5], 'probability':[True]\n              }]\n \nGrid_SVM = GridSearchCV(estimator=SVC(), param_grid=SVM_param_grid, scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\ntic = time.time()\n\nGrid_SVM.fit(X_train,Y_train)\n\ntoc = time.time()\nprint('Done in:', toc-tic)\n","9bbf1cc3":"print(\"Best SVM model:\\n\", Grid_SVM.best_estimator_)\nprint(\"Best SVM parameters:\\n\", Grid_SVM.best_params_)\nprint(\"Score for this model:\", Grid_SVM.best_score_)","7d84f8b8":"DT_param_grid = [{\n               'criterion': ['gini', 'entropy'], \n               'max_features': ['auto', 'log2', None],\n               'min_samples_split': list(range(0,300,5)), \n               'max_depth': list(range(0,50,5))\n              }]\n\nGrid_DT = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=DT_param_grid, \n                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\n\n\ntic = time.time()\n\nGrid_DT.fit(X_train, Y_train)\n\ntoc = time.time()\nprint('Done in:', toc - tic)\n","4c45b777":"print(\"Best Decision Tree model:\\n\", Grid_DT.best_estimator_)\nprint(\"Best Decision Tree parameters:\\n\", Grid_DT.best_params_)\nprint(\"Score for this model:\", Grid_DT.best_score_)","653125c9":"RF_param_grid = [{\n               'criterion': ['entropy'], \n               'max_features': [2,3],\n               'min_samples_split': [10],  \n               'max_depth': [21],\n               'min_samples_leaf' : [1,10],\n               'bootstrap' : [True],\n               'n_estimators' : [100,200]\n              }]\n\nGrid_RF = GridSearchCV(estimator=RandomForestClassifier(),param_grid=RF_param_grid, \n                  scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())\n\ntic = time.time()\n\nGrid_RF.fit(X_train, Y_train)\n\ntoc = time.time()\nprint('Done in:', toc - tic)\n","0a563a3f":"print(\"Best Random Forest model:\\n\", Grid_RF.best_estimator_)\nprint(\"Best Random Forest parameters:\\n\", Grid_RF.best_params_)\nprint(\"Score for this model:\", Grid_RF.best_score_)","b7e5193b":"D_cv = {\"Cross Validation Means\": [LogReg_best_scores[1], Grid_SVM.best_score_, Grid_DT.best_score_, Grid_RF.best_score_],\n     \"Models\": ['Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest']\n    }\n\nBest_model_CV_performances = pd.DataFrame(D_cv)\nprint(Best_model_CV_performances)\n\ng = sns.barplot(\"Cross Validation Means\", \"Models\", data = Best_model_CV_performances)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","7c502675":"BestEstimators = [LogReg_best_estimators[1], Grid_SVM.best_estimator_, Grid_DT.best_estimator_, Grid_RF.best_estimator_]\n\nD_test = {\"Test scores\": [],\n     \"Models\": ['Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest']\n    }\n\nfor est in BestEstimators:\n    if est == LogReg_best_estimators[1]:\n        pf = PolynomialFeatures(degree=2)\n        Xd_test = pf.fit_transform(X_test)\n    else:\n        Xd_test = X_test\n    D_test['Test scores'].append(est.score(Xd_test, Y_test))\n    \nBest_model_test_performances = pd.DataFrame(D_test)\nprint(Best_model_test_performances)\n\ng = sns.barplot(\"Test scores\", \"Models\", data = Best_model_test_performances)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Test Scores\")\n    \n    ","da8e14af":"VC = VotingClassifier(estimators=[('SVM', Grid_SVM.best_estimator_), ('DecTree', Grid_DT.best_estimator_), ('RandFor',Grid_RF.best_estimator_)], voting='soft', n_jobs=4)\n\nVC.fit(X,Y)","3cbc2a19":"path = '..\/input\/titanic-cleantestset\/CleanTestSet.csv'\ntest_df = pd.read_csv(path)\ntest_df","1ff0319c":"#We drop the column PassengerId so that we can make predictions\n\ntest = test_df[['Age_Nor', 'Fare_Nor', 'Pclass_Nor', 'SibSp_Nor', 'Parch_Nor', 'Sex_female', 'Embarked_C', \n                'Embarked_Q']]\n\nYhat = VC.predict(test)\n","da13f587":"#Output dataframe\n\nSurvived = list(Yhat)\nOutput = test_df[['PassengerId']]\nOutput['Survived'] = Survived\nOutput.reset_index(drop=True, inplace=True)\nOutput","be70b117":"Output.to_csv('TitanicPrediction_VotingClassifier.csv', index=False)","9d6543e4":"It seems that Age is correlated with Pclass, SibSp and Parch. \n\nWe thus fill the missing age value of the passenjer j using the mean age of the group of passengers with the same  \nPclass, SibSp and Parch as j.\n","f612e723":"### 4.1 Droping fetures we won't use <a class=\"anchor\" id=\"SubSection_4_1\"><\/a> \n\n","298ca83d":"We see that our two continuous variables \"Age\" and \"Fare\" have very dfferent ranges [0, 80] and [0,512]. We will normalize them to avoid that  the nature of the data biases our models to weigh one feature more heavily than other\njust by their ranges.\n\nWe will also normalize SipSp, Pclass and Parch. So that we have all variables (relatively) in the same range.\n\nFor normalization we use the z-score, i.e. replace original value by (original value - mean)\/standard deviation\n\nThe new values will normally range from -3 to 3.\n\n\n\n\n","ef5466dc":"The above graphics reveal that the Fare feature seems to be correlated to the survival feature (The higher the fare, the most chances of survival).\n\nSimilarly, age seems to have a impact (although the correlation Age vs Survived is close to zero). In the above graphics we see that children have a higer survivial rate. \n\nSo, we use Age and Fare features in our model.","a043eb7a":"### 3.1 Continuous features <a class=\"anchor\" id=\"SubSection_3_1\"><\/a> ","21719d0c":"### 3.1 Categorical features <a class=\"anchor\" id=\"SubSection_3_1\"><\/a> ","932b41af":" # 6. Prediction on test set <a class=\"anchor\" id=\"Section6\"><\/a>","58a8b729":"Continuous Variables: Age, Fare\n\nDiscrete Variables:  Survived, Pclass, Sex, SibSp, Parch, Ticket, Cabin, Embarked ","d4c16e63":"### 5.3 Decision Tree <a class=\"anchor\" id=\"SubSection_5_3\"><\/a> ","f5c0b795":"# 3. Looking for relevant features <a class=\"anchor\" id=\"Section3\"><\/a>","0d73a5e0":"We divide the discrete (or catagorical) variables into two groups. \n\nThe first group CatVars1 contains the variables with a small number of possible observations.\n\nThe second group CatVars2 contains the variables with a big number of possible observations.","22213eb6":"### 2.1Discrete Variables <a class=\"anchor\" id=\"SubSection_2_1\"><\/a> ","189d1e28":"### 2.1 Continuous Variables <a class=\"anchor\" id=\"SubSection_2_1\"><\/a> ","35a7edf1":"Let's have an intitial look to the correlations","f36bca2b":"### 4.3 Normalizing the data <a class=\"anchor\" id=\"SubSection_4_3\"><\/a> ","16557dda":"\n# 4. Feature Engineering <a class=\"anchor\" id=\"Section4\"><\/a>\n    \n    ","c2f01b4f":"### 4.4 Indicator variables <a class=\"anchor\" id=\"SubSection_4_4\"><\/a> ","c59ae463":"We see that still there are missing ages, this is because in this group of passengers there is no age data available. In this case, we fill these missing ages with the mean of all the ages.","d8afe3e5":"### 5.2 Support Vector Machine <a class=\"anchor\" id=\"SubSection_5_2\"><\/a> ","2e4891ee":" # 1. Loading and checking the data <a class=\"anchor\" id=\"Section1\"><\/a>\n\n","87721f3e":"\n# 5. Training Models <a class=\"anchor\" id=\"Section5\"><\/a>","350d0130":"We will transform the features \"Sex\",\"Embarked\", into indicator variables (or dummy variables), so that they are suitable for the classification models.","7e4bfbb2":"### 4.2 Missing values <a class=\"anchor\" id=\"SubSection_4_2\"><\/a> ","fe1e95ba":"We now examine the fraction of survived paseengers according to each categrory for the discrete variables.","23cd4847":"# 2. Individual Feature analysis <a class=\"anchor\" id=\"Section2\"><\/a>","afeb7288":"There is a correlation between the Fare and Embarked variables. Thus, we fill the missing values of 'Embarqued' by determining the probabilities of begin Embarked S, C or Q provided that the fare is around 80 (Fare value of the passengers missing Embarked). We can do this by analyzing the above distribution plots as follows.","729c22d7":"The above computations show that the following features are relevant for our model: 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'\n\nWe won't use Ticket, Name, IdPassenger, Cabin.","463c98f5":"Performances in the test set","008f5641":"In a new data frame 'Model_DF' we will transform our data and have it ready for modelling.","bd375d54":"### 5.5 Model's perfomance comparison <a class=\"anchor\" id=\"SubSection_5_5\"><\/a> ","8f4debd3":"### 5.1 Logistic regression <a class=\"anchor\" id=\"SubSection_5_1\"><\/a> ","4030ef30":"Cross validation performances:","06b94eb5":"### Table of Contents\n\n* [1. Loading and checking the data ](#Section1)\n* [2. Individual Feature analysis](#Section2)\n    * [2.1 Continuous Variables](#SubSection_2_1)\n    * [2.2 Discrete Variables](#SubSection_2_2)\n* [3. Looking for relevant features](#Section3)\n    * [3.1 Continuous Features](#SubSection_3_1)\n    * [3.2 Discrete features](#SubSection_3_2)\n* [4. Feature Engineering](#Section4)\n    * [4.1 Droping fetures we won't use](#SubSection_4_1)\n    * [4.2 Missing values](#SubSection_4_2)\n    * [4.3 Normalizing the data](#SubSection_4_3)\n    * [4.4 Indicator variables](#SubSection_4_4)\n* [5. Training Models](#Section5) \n    * [5.1 Logistic regression](#SubSection_5_1)\n    * [5.2 Support Vector Machine](#SubSection_5_2)\n    * [5.3 Decision Tree](#SubSection_5_3)\n    * [5.4 Random Forest](#SubSection_5_4)\n    * [5.5 Model's perfomance comparison](#SubSection_5_5)\n* [6. Prediction on the test set](#Section6) \n     \n  ","21791621":"We will combine the predictions from the last 3 classifiers. We pass the argument 'soft' to the voting parameter to take the probability of each classifier into consideration.","c1047b83":"### Replacing Age (177) ","298263a7":"We drop one of each variable in each of the dummy categories since they don't give extra information. For example the Sex_female = 1 implies Sex_male = 0. So, Sex_male is redundat if one knows Sex_female.","ef87d09a":"### Replacing Embarqued (only 2 missing values) ","84f23035":"The higest probability (bin hight = 0.009) belongs to the category 'Embarqued'= C. \n\nThis is basic bayesian probability. We can get the same result by just counting the amount of pasengers embarqued S, C or Q with fares in the range [75,85).","fc1dbeb0":"We have pre-processed the test data and stored it on 'CleanTestSet.csv'.  This test data is in the suitable format  for making predictions with our model. ","42195f92":"### 5.4 Random Forest <a class=\"anchor\" id=\"SubSection_5_4\"><\/a> ","8caee2a9":"<h1 align=center><font size=5>Titanic. <\/font><\/h1>","432cb16d":"At a first glance, 'PClass' and 'Fare' seem to be corelated with 'Survived'.","2451b9a8":"This shows that the best model for Logisitc regression is of degree 2. "}}