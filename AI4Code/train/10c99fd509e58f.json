{"cell_type":{"0ffd92f9":"code","8768bd3f":"code","996fca72":"code","551c5341":"code","de411613":"code","e644c60b":"code","dc3f915a":"code","45618204":"code","52486480":"code","c7312488":"code","40578078":"code","6a8d1f0a":"code","26cd0462":"code","26cb6254":"code","c88b40a6":"code","960f3800":"code","711b23ed":"markdown","78d8a9d9":"markdown","c0dd5c0b":"markdown"},"source":{"0ffd92f9":"!pip install tokenizers transformers > \/dev\/null","8768bd3f":"from tokenizers import ByteLevelBPETokenizer\n\npaths = [\"..\/input\/si_dedup.txt\"]\n\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()","996fca72":"# Customize training\ntokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"<\/s>\",\n    \"<unk>\",\n    \"<mask>\",\n])","551c5341":"!mkdir SinhalaBERTo\ntokenizer.save_model(\"SinhalaBERTo\")","de411613":"from tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\n\ntokenizer = ByteLevelBPETokenizer(\n    \"SinhalaBERTo\/vocab.json\",\n    \"SinhalaBERTo\/merges.txt\",\n)","e644c60b":"tokenizer._tokenizer.post_processor = BertProcessing(\n    (\"<\/s>\", tokenizer.token_to_id(\"<\/s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512)","dc3f915a":"from transformers import RobertaConfig\n\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n)","45618204":"from transformers import RobertaTokenizerFast\nfrom transformers import RobertaForMaskedLM\n\ntokenizer = RobertaTokenizerFast.from_pretrained(\"SinhalaBERTo\", max_len=512)\nmodel = RobertaForMaskedLM(config=config)","52486480":"model.num_parameters()","c7312488":"%%time\n\nfrom transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"..\/input\/si_dedup.txt\",\n    block_size=128,\n)","40578078":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","6a8d1f0a":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"SinhalaBERTo\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)","26cd0462":"%%time\ntrainer.train()","26cb6254":"trainer.save_model(\"SinhalaBERTo\")","c88b40a6":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"SinhalaBERTo\",\n    tokenizer=\"SinhalaBERTo\"\n)","960f3800":"fill_mask(\"\u0db8\u0db8 \u0d9c\u0dd9\u0daf\u0dbb <mask>.\")","711b23ed":"Let's just train 1 epoch. (in order to get better results make sure to train more epochs!)","78d8a9d9":"Let's save the model","c0dd5c0b":"Let's do a simple task!"}}