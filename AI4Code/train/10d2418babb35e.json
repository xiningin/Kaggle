{"cell_type":{"be2cd76c":"code","a6afa11d":"code","b56f15ba":"code","8ff1ae35":"code","72bd4194":"code","46b7fab9":"code","b0e5ea21":"code","12610042":"code","ca836a42":"code","2f76d399":"code","24fdd67d":"code","35e8fa88":"code","ffcf0095":"code","76b92f06":"code","4151ae9b":"code","415dafa2":"code","c6511a86":"code","e44f1693":"code","16504987":"code","71c8fb21":"code","9f5d282b":"code","5b4376ec":"code","3b992d57":"code","bfb55d6b":"code","e251ebc7":"code","3e9d4323":"code","b928a02c":"code","e5515c41":"code","b7beb8dc":"code","fe185bf3":"code","d7c499e0":"code","13c52792":"code","b6fc218d":"code","94d12903":"code","7b7c8f70":"code","5866a25b":"code","c76790a8":"code","892e41ec":"code","759945fd":"code","32e55c30":"code","ea4bd045":"code","55fbe1ca":"code","738632d8":"code","194fdca6":"code","f53197d0":"code","5b8712fc":"code","0713375e":"code","21335da5":"code","7999e2c5":"code","4c50fbda":"code","33a572d3":"code","1c2ca69d":"markdown","1dbbb770":"markdown","8f3d5eba":"markdown","a71e4113":"markdown","e6320e99":"markdown","2169b807":"markdown","a6153e99":"markdown","4b45b700":"markdown","2b6e041b":"markdown","f2d13fa4":"markdown","dfe222a0":"markdown","65e62ff1":"markdown","0995b45f":"markdown","9fd0ff69":"markdown","b7dd3b2f":"markdown","bb879db6":"markdown","2b9c77b2":"markdown","cd12ddb2":"markdown","7448ce43":"markdown","4624dfd2":"markdown","c604a8dd":"markdown"},"source":{"be2cd76c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a6afa11d":"sample=False","b56f15ba":"from fastai import *\nfrom fastai.text import *","8ff1ae35":"defaults.device = torch.device('cuda',0) if torch.cuda.is_available() else torch. device('cpu')\ndefaults.device","72bd4194":"DATA_PATH = Path('\/kaggle\/input\/usinlppracticum\/')\nDATA_PATH.ls()","46b7fab9":"lm_data=pd.read_csv(DATA_PATH\/'imdb_train.csv')\nlm_data.head()","b0e5ea21":"lm_data1=pd.read_csv(DATA_PATH\/'imdb_train.csv')\nlm_data1['sentiment']=0\nlm_data2=pd.read_csv(DATA_PATH\/'imdb_test.csv')\nlm_data2['sentiment']=0\nlm_data= pd.concat([lm_data1, lm_data2], ignore_index=True)\nlm_data=lm_data[['review','sentiment']]\nlm_data.to_csv('lm_data.csv',index=False)\nlm_data.shape","12610042":"if sample:\n    lm_data=pd.read_csv('lm_data.csv').sample(10000).reset_index(drop=True)\nelse:\n    lm_data=pd.read_csv('lm_data.csv')\n#------------\nlm_data.head()","ca836a42":"from sklearn.model_selection import train_test_split\n\ntrain_lm, val_lm = train_test_split(lm_data,test_size=0.10)\ntrain_lm.shape,val_lm.shape","2f76d399":"data_lm = TextLMDataBunch.from_df(DATA_PATH, train_lm,val_lm,text_cols='review', label_cols='sentiment')\ndata_lm.save('\/kaggle\/working\/data_lm_export.pkl')","24fdd67d":"data_lm.vocab.itos[:10]","35e8fa88":"data_lm.train_ds[0][0]","ffcf0095":"data_lm.train_ds[0][0].data[:100]","76b92f06":"len(data_lm.vocab.itos),len(data_lm.train_ds)","4151ae9b":"data_lm.train_ds[0][0].data.shape","415dafa2":"data_lm.show_batch()","c6511a86":"learn_lm = language_model_learner(data_lm, AWD_LSTM)","e44f1693":"import pickle\nwiki_itos = pickle.load(open('\/kaggle\/input\/wiki-vocab\/itos_wt103.pkl', 'rb'))","16504987":"wiki_itos[:10]","71c8fb21":"vocab = data_lm.vocab","9f5d282b":"vocab.stoi[\"stingray\"]","5b4376ec":"vocab.itos[vocab.stoi[\"stingray\"]]","3b992d57":"vocab.itos[vocab.stoi[\"mobula\"]]","bfb55d6b":"awd = learn_lm.model[0]\nprint(awd)","e251ebc7":"enc = learn_lm.model[0].encoder","3e9d4323":"enc.weight.size()","b928a02c":"len(wiki_itos)","e5515c41":"len(vocab.itos)","b7beb8dc":"i, unks = 0, []\nwhile len(unks) < 50:\n    if data_lm.vocab.itos[i] not in wiki_itos: unks.append((i,data_lm.vocab.itos[i]))\n    i += 1","fe185bf3":"wiki_words = set(wiki_itos)\nimdb_words = set(vocab.itos)","d7c499e0":"wiki_not_imbdb = wiki_words.difference(imdb_words)\nimdb_not_wiki = imdb_words.difference(wiki_words)","13c52792":"wiki_not_imdb_list = []\n\nfor i in range(100):\n    word = wiki_not_imbdb.pop()\n    wiki_not_imdb_list.append(word)\n    wiki_not_imbdb.add(word)","b6fc218d":"wiki_not_imdb_list[:15]","94d12903":"imdb_not_wiki_list = []\n\nfor i in range(100):\n    word = imdb_not_wiki.pop()\n    imdb_not_wiki_list.append(word)\n    imdb_not_wiki.add(word)","7b7c8f70":"imdb_not_wiki_list[:15]","5866a25b":"TEXT = \"The color of the sky is\"\nN_WORDS = 40\nN_SENTENCES = 2\n\nprint(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","c76790a8":"# doc(LanguageLearner.predict)","892e41ec":"print(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.20) for _ in range(N_SENTENCES)))","759945fd":"learn_lm.fit_one_cycle(1, 2e-2, moms=(0.8,0.7), wd=0.1)","32e55c30":"learn_lm.unfreeze()\nlearn_lm.fit_one_cycle(10, 2e-3, moms=(0.8,0.7), wd=0.1)","ea4bd045":"learn_lm.path = Path('\/kaggle\/working') \nlearn_lm.model_dir= Path('.')","55fbe1ca":"learn_lm.save_encoder('fine_tuned_enc')","738632d8":"TEXT = \"i liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\nprint(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","194fdca6":"if sample:\n    data_cls=pd.read_csv(DATA_PATH\/'imdb_train.csv').sample(1000).reset_index(drop=True)\nelse:\n    data_cls=pd.read_csv(DATA_PATH\/'imdb_train.csv')\n#----------\ndata_cls.head()","f53197d0":"# Classifier model data\nfrom sklearn.model_selection import train_test_split\ntrain, val = train_test_split(data_cls,test_size=0.10, random_state=42)\nlabel_col= 'sentiment'\n\nlabel_mapping= {'negative':0,'positive':1}\ntrain[label_col]=train[label_col].map(label_mapping)\nval[label_col]=val[label_col].map(label_mapping)\ntrain.head()","5b8712fc":"data_clas = TextDataBunch.from_df(DATA_PATH, train, val,\n                  vocab=data_lm.train_ds.vocab,\n                  text_cols=\"review\",\n                  label_cols='sentiment',\n                  bs=64,device = defaults.device)","0713375e":"learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3) #.to_fp16()\nlearn_c.path = Path('\/kaggle\/working') \nlearn_c.model_dir= Path('.')\nlearn_c.load_encoder('fine_tuned_enc')\nlearn_c.freeze()","21335da5":"learn_c.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))","7999e2c5":"learn_c.freeze_to(-2)\nlearn_c.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","4c50fbda":"learn_c.freeze_to(-3)\nlearn_c.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","33a572d3":"learn_c.unfreeze()\nlearn_c.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","1c2ca69d":"We will be looking at IMDB movie reviews.  We want to determine if a review is negative or positive, based on the text.  In order to do this, we will be using **transfer learning**.\n\nTransfer learning has been widely used with great success in computer vision for several years, but only in the last year or so has it been successfully applied to NLP (beginning with ULMFit, which we will use here, which was built upon by BERT and GPT-2).\n\nAs Sebastian Ruder wrote in [The Gradient](https:\/\/thegradient.pub\/) last summer, [NLP's ImageNet moment has arrived](https:\/\/thegradient.pub\/nlp-imagenet\/).","1dbbb770":"### But the underlying data is all numbers","8f3d5eba":"#### And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:","a71e4113":"### Language Modeling & Sentiment Analysis of IMDB movie reviews","e6320e99":"### Generating fake movie reviews (using wiki-text model)","2169b807":"### Training the Langauge Model","a6153e99":"### Difference in vocabulary between IMDB and Wikipedia\nWe will compare the `vocabulary from wikitext with the vocabulary in IMDB`.  It is to be expected that the two sets have some different vocabulary words, and that is no problem for `transfer learning!`","4b45b700":"### Training Classifier on finetuned Language Models","2b6e041b":"#### data Preparation for Language Model data","f2d13fa4":"###  splitting langauge model data","dfe222a0":"### Numericalization\nOnce we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at list twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n\nThe correspondance from ids tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string).","65e62ff1":"### loading wikitext vocab","0995b45f":"### All words that appear in the IMDB vocab, but not the wikitext-103 vocab, will be initialized to the same random vector in a model.  `As the model trains, we will learn these weights.`","9fd0ff69":"### Creating the TextLMDataBunch\nThis is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes).","b7dd3b2f":"### More generated movie reviews\nHow good is our model? Well let's try to see what it predicts after a few given words.","bb879db6":"**References:**\n* https:\/\/github.com\/fastai\/fastai\/blob\/master\/examples\/ULMFit.ipynb\n* https:\/\/github.com\/fastai\/course-nlp\/blob\/master\/5-nn-imdb.ipynb\n* http:\/\/nlp.fast.ai\/classification\/2018\/05\/15\/introducing-ulmfit.html","2b9c77b2":"**SELECTING DEVICE: CPU\/CUDA**","cd12ddb2":" We first have to convert words to numbers. This is done in two differents steps: \n*  tokenization \n* numericalization. \n\nA `TextDataBunch` does all of that behind the scenes for you.","7448ce43":"importing fastai libraries","4624dfd2":"### Tokenization\nThe first step of processing we make texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n\n- we need to take care of punctuation\n- some words are contractions of two different words, like isn't or don't\n- we may need to clean some parts of our texts, if there's HTML code for instance\n\nTo see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch.\n\nThe texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n- the \"'s\" are grouped together in one token\n- the contractions are separated like his: \"did\", \"n't\"\n- content has been cleaned for any HTML symbol and lower cased\n- there are several special tokens (all those that begin by xx), to replace unkown tokens (see below) or to introduce different text fields (here we only have one).","c604a8dd":"#### Transfer Learning in NLP -ULMFiT\nAuthors: Vikas Kumar (vikkumar@deloitte.com) | Abhishek Aditya Kashyap (abhikashyap@deloitte.com)"}}