{"cell_type":{"a234aa9c":"code","803d1268":"code","30b838d1":"code","2d92396d":"code","401eade9":"code","6ecf6b7f":"code","454f0617":"code","e04cd120":"code","d9b09ecd":"code","f0b53b9e":"code","c782a6c9":"code","c0970a2b":"code","044c9bfc":"code","6b8de2b5":"code","1a02658c":"code","a689fed4":"code","608a047d":"code","9ebfe24e":"code","01dceb82":"code","4a936b05":"code","1220841d":"code","03432c40":"code","bc5ca8f9":"code","3709931e":"code","1f59e400":"code","88e3f50d":"code","8d9ecc9b":"code","108868dd":"code","9b39eedb":"code","6a7bfa89":"code","d2d4eab4":"code","c8f9630c":"code","9fe4f6ae":"code","f3ae8cbf":"code","1504c192":"code","fd311f8f":"code","18c739eb":"markdown","4a3b3707":"markdown","a46071f0":"markdown","9bcea4ed":"markdown","f24184c2":"markdown","1f9499e3":"markdown","77ade235":"markdown","432db0c9":"markdown","8fc4e788":"markdown","cf8299e5":"markdown","0c0e0f86":"markdown","e76542ea":"markdown","2fb43a61":"markdown","58cb09c2":"markdown","d6390dcd":"markdown","c6f23b76":"markdown"},"source":{"a234aa9c":"# Some mandatory Libraries\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\n\n#EDA Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Classifier Libraries\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\n# Other Libraries\nfrom tqdm import tqdm\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","803d1268":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30b838d1":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","2d92396d":"print(df.columns)","401eade9":"print('Our total data point: ', df.shape)","6ecf6b7f":"df.describe()","454f0617":"df.isnull().sum()\n\n# No null values present in data frame.","e04cd120":"# The classes are heavily skewed we need to solve this issue later.\nprint('No. of no frauds in persentage: ', round(df['Class'].value_counts()[0]\/len(df) * 100,2))\nprint('No. of frauds in persentage: ', round(df['Class'].value_counts()[1]\/len(df) * 100,2))","d9b09ecd":"ax = sns.countplot(x='Class', data=df)","f0b53b9e":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 50\n\nax1.hist(df.Time[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Time[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()\n","c782a6c9":"print (\"Fraud\")\nprint (df.Amount[df.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (df.Amount[df.Class == 0].describe())\n","c0970a2b":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 30\n\nax1.hist(df.Amount[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Amount[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()\n","044c9bfc":"df['Amount_max_fraud'] = 1\ndf.loc[df.Amount <= 2125.87, 'Amount_max_fraud'] = 0\n","6b8de2b5":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n\nax1.scatter(df.Time[df.Class == 1], df.Amount[df.Class == 1])\nax1.set_title('Fraud')\n\nax2.scatter(df.Time[df.Class == 0], df.Amount[df.Class == 0])\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()\n","1a02658c":"#Select only the anonymized features.\nimport matplotlib.gridspec as gridspec\n\nv_features = df.iloc[:,1:29].columns\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","a689fed4":"corr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nwith sns.axes_style(\"white\"):\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15,10))\n    ax = sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","608a047d":"# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x='Class', y='V2', data=df, ax=axes[0])\naxes[0].set_title('V2 vs Class')\n\nsns.boxplot(x='Class', y='V4', data=df, ax=axes[1])\naxes[1].set_title('V4 vs Class')\n\nsns.boxplot(x='Class', y='V11', data=df, ax=axes[2])\naxes[2].set_title('V11 vs Class')\n\nsns.boxplot(x='Class', y='V19', data=df, ax=axes[3])\naxes[3].set_title('V19 vs Class')\n\nplt.show()","9ebfe24e":"# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x='Class', y='V10', data=df, ax=axes[0])\naxes[0].set_title('V10 vs Class')\n\nsns.boxplot(x='Class', y='V12', data=df, ax=axes[1])\naxes[1].set_title('V12 vs Class')\n\nsns.boxplot(x='Class', y='V14', data=df,  ax=axes[2])\naxes[2].set_title('V14 vs Class')\n\nsns.boxplot(x='Class', y='V17', data=df, ax=axes[3])\naxes[3].set_title('V17 vs Class')\n\nplt.show()","01dceb82":"#to edit later ","4a936b05":"print('Our current dataset shape based on class is', df[df['Class'] == 0].shape,' and ',df[df['Class'] == 1].shape)","1220841d":"# Class count\ncount_class_0, count_class_1 = df.Class.value_counts()\n\n# Divide by class\ndf_0 = df[df['Class'] == 0]\ndf_1 = df[df['Class'] == 1]\n\n# Create new dataset\ndf_1_over_sampled = df_1.sample(count_class_0, replace=True)\ndf_final = pd.concat([df_0, df_1_over_sampled], axis=0)\n\nprint('New dataset contain: ', df_final[df_final['Class'] == 0].shape, df_final[df_final['Class'] == 1].shape)","03432c40":"print('Distribution of the Classes in the subsample dataset')\nprint(df_final['Class'].value_counts()\/len(df_final))","bc5ca8f9":"# New_df is from the random undersample data (fewer instances)\n\nX = df_final.drop('Class', axis=1)\nY = df_final['Class']","3709931e":"# Before StandardScaler:\n\nf, axes = plt.subplots(ncols=2, figsize=(10,4))\n\nsns.distplot(df_final['Amount'], kde=False, color=\"g\", ax=axes[0])\n\nsns.distplot(df_final['Time'], kde=False, color=\"g\", ax=axes[1])\n\nplt.tight_layout()","1f59e400":"# Create the Scaler object\nscaler = StandardScaler()\n\n# Fit and Transform Amount\nscaler.fit(df_final['Amount'].values.reshape(-1, 1))\ndf_final['std_Amount'] = scaler.transform(df_final['Amount'].values.reshape(-1, 1))\n\n# Fit and Transform Time\nscaler.fit(df_final['Time'].values.reshape(-1, 1))\ndf_final['std_Time'] = scaler.transform(df_final['Time'].values.reshape(-1, 1))\n\n# Delete old features\ndf_final.drop(['Time','Amount'], axis=1, inplace=True)","88e3f50d":"# After StandardScaler:\n\nf, axes = plt.subplots(ncols=2, figsize=(10,4))\n\nsns.distplot(df_final['std_Amount'], kde=False, color=\"g\", ax=axes[0])\n\nsns.distplot(df_final['std_Time'], kde=False, color=\"g\", ax=axes[1])\n\nplt.tight_layout()","8d9ecc9b":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=22)","108868dd":"# Define classifiers with default parameters.\n\nclassifiers = {\n    \"Logisitic Regression\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(),\n    \"Linear Regression\":linear_model.LinearRegression(),\n    \"Gaussian NB\": GaussianNB()\n}","9b39eedb":"for name, classifier in classifiers.items():\n    classifier.fit(X_train, y_train) \n    training_score = cross_val_score(classifier, X_train, y_train, cv=3)\n    print('Classifiers: ',name, 'has a training score of', round(training_score.mean(),2) * 100)","6a7bfa89":"# LogisticRegression\nparams = {\"penalty\": ['l1', 'l2'],\n          'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngs = GridSearchCV(LogisticRegression(), \n                  params, \n                  cv = 3, \n                  n_jobs=-1)\ngs_results = gs.fit(X_train, y_train)\n\nlog_reg = gs.best_estimator_ # store best estimators for future analysis\n\nprint('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","d2d4eab4":"# KNN\nparams = {'n_neighbors':list(range(1, 50, 2)), 'weights':['uniform', 'distance']}\n\ngs = GridSearchCV(KNeighborsClassifier(),\n                  params, \n                  cv = 3, \n                  n_jobs=-1)\ngs_results = gs.fit(X_train, y_train)\n\nknears_neighbors = gs.best_estimator_ # store best estimators for future analysis\n\nprint('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","c8f9630c":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=3)\nprint('LR CV Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=3)\nprint('KNN CV Score: ', round(knears_score.mean() * 100, 2).astype(str) + '%')","9fe4f6ae":"model = LogisticRegression(penalty = 'l2', C = 0.1)\nmodel.fit(X_train,y_train)\npridict = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, pridict))","f3ae8cbf":"cm = confusion_matrix(y_test, pridict)\nlabels = ['No Fraud', 'Fraud']\nprint(pd.DataFrame(cm, index=labels, columns=labels))","1504c192":"lr_pred_prob = model.predict_proba(X_test)[:,1]\nfpr,tpr,thrsld = roc_curve(y_test,lr_pred_prob)\nprint('AUC score:',roc_auc_score(y_test,lr_pred_prob))","fd311f8f":"# Ploting ROC Curve \n\nplt.figure(figsize=(7,5))\nplt.plot([0,1],[0,1])\nplt.plot(fpr,tpr,':', color='red')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.grid()","18c739eb":"### Some insights from above metrice:\n\n* Now let's analyse each features (except Amount & Time) by devideing them w.r.t possitive and negative corelation. \n* These features can be use for reduce the dimentions. \n* Here we can use box plot to measure outliers and distributions at a time.","4a3b3707":"## Data visualisation:","a46071f0":"### Confusion Matrix:","9bcea4ed":"We can see here in these features have some extream outliers.\n\n### dimensionality Reduction:","f24184c2":"## Data Preprocessing:","1f9499e3":"### Train Test Split:\n\n__Note:__ Accuracy score might not work as expected as we have some extream outliers and immabalced nature of data.","77ade235":"### ROC Curve:","432db0c9":"# Problem Statement:\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.","8fc4e788":"###  Model Training:","cf8299e5":"The 'Time' feature looks pretty similar across both types of transactions. You could argue that fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. This could make it easier to detect a fraudulent transaction during at an 'off-peak' time.\n\nNow let's see if the transaction amount differs between the two types.\n\n","0c0e0f86":"### Remove Outlers:","e76542ea":"### StandardScaler:","2fb43a61":"## classification models:","58cb09c2":"As we know KNN impact allot by outliers so let's go with the LR and check our test accuracy.\n\n### Accuracy on Test Data:","d6390dcd":"KNN's test accuracy is quite good in compare to other. Let's try to improve LR model using hyper parameter tuning.\n\n### Optomise Parameters:","c6f23b76":"Here we can see we have some extream outliers in V10, V11, V12, V14. We will remove those when we preprocess our data."}}