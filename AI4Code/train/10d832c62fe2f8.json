{"cell_type":{"3b4dcec6":"code","62cf49e8":"code","43af4377":"code","e6b87efd":"code","1158ad8e":"code","c7462005":"code","8ca130b3":"code","7acf90c0":"code","e0db1bbb":"code","cc6e815f":"code","82f1297d":"code","bdfd2efa":"code","6742444c":"code","d3b0a736":"code","6dad03d7":"code","504c7452":"code","e960f7b1":"code","d92b6e5c":"code","29e66c2b":"code","143e6f7f":"code","7878117a":"code","5b0fa84a":"code","ea09797b":"code","ef3cdaae":"code","d8ab9dba":"code","d13448ab":"code","dc453a07":"code","d6176d8b":"code","4c0ecfd8":"code","d191be65":"code","9c635cc1":"code","e7e43c55":"code","adbc5a9c":"code","d108cca1":"code","c4fe29dc":"code","a3496253":"code","bd6c2c41":"code","be90c2fd":"code","aa97423c":"code","a4ba273b":"code","d6f490f4":"code","6760306e":"code","412360e7":"markdown","0a8d341c":"markdown","b7608e78":"markdown","1f80bbe9":"markdown","c8b27cda":"markdown","6dc560b7":"markdown","9194cff8":"markdown","b0f4307a":"markdown","c61eb594":"markdown","ebffe261":"markdown","36dbf0cb":"markdown","f9ba57a8":"markdown","4a76ac13":"markdown","0a19d8d5":"markdown","d43b23cd":"markdown","070fe141":"markdown","1bb4dde4":"markdown","fa75d670":"markdown","d53bb51e":"markdown","ea2bdbb3":"markdown","0ffaba4e":"markdown","2f764dd4":"markdown"},"source":{"3b4dcec6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62cf49e8":"#base libraries for data handling\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\n#visualization imports\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\n# consistent plots #\nfrom pylab import rcParams\nrcParams['figure.figsize']= 12,5\nrcParams['xtick.labelsize']= 12\nrcParams['ytick.labelsize']= 12\nrcParams['axes.labelsize']= 12\n#handle unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)","43af4377":"#load the data -- > .npz format\ndata = np.load('\/kaggle\/input\/face-recognition\/ORL_faces.npz')","e6b87efd":"#check the type of the data\ntype(data)","1158ad8e":"#check the content of the files in a sorted fashion.. \nsorted(data.files)","c7462005":"#view the content of the npzfiles\ndata['trainX']","8ca130b3":"data['trainY']","7acf90c0":"#check the type of the data -- > confirm it is numpy array\ntype(data['trainX']), type(data['trainY'])","e0db1bbb":"#check the shape of the data\ndata['trainX'].shape, data['trainY'].shape","cc6e815f":"#reshape the content to the dimension as stated in the problem description\nn_rows = 112\nn_cols = 92\nn_channels = 1\n\n#trainX = data['trainX']\n#testX = data['testX']\n\n#trainX = np.array(list(map(lambda p: cv2.cvtColor(p, cv2.COLOR_GRAY2BGR), np.float32(data['trainX']))))\/255.\n#testX = np.array(list(map(lambda p: cv2.cvtColor(p, cv2.COLOR_GRAY2BGR), np.float32(data['testX']))))\/255.\n\n#trainX =  np.reshape(trainX,newshape=(trainX.shape[0],n_rows,n_cols,3))\n#testX =  np.reshape(testX,newshape=(testX.shape[0],n_rows,n_cols,3))\n\ntrainX =  np.reshape(data['trainX'],newshape=(data['trainX'].shape[0],n_rows,n_cols))\ntestX =  np.reshape(data['testX'],newshape=(data['testX'].shape[0],n_rows,n_cols))\n\ntrainX = np.array(list(map(lambda p: cv2.cvtColor(p, cv2.COLOR_GRAY2BGR), np.float32(trainX))))\/255.\ntestX = np.array(list(map(lambda p: cv2.cvtColor(p, cv2.COLOR_GRAY2BGR), np.float32(testX))))\/255.\n\n\ntrainY = data['trainY']\ntestY = data['testY']","82f1297d":"#check the new shape\ntrainX.shape, testX.shape","bdfd2efa":"#lets visualize random images\ndelta = 5\nn = np.random.randint(low=0,high=trainX.shape[0]- 2*delta,dtype=int)\n\n#plot the faces \nplt.subplot(2,2,1)\nplt.imshow(trainX[n])\nplt.subplot(2,2,2)\nplt.imshow(trainX[n+delta])\nplt.subplot(2,2,3)\nplt.imshow(trainX[n-delta])\nplt.subplot(2,2,4)\nplt.imshow(trainX[n+delta*2])\nplt.show()","6742444c":"from sklearn.model_selection import train_test_split\nseed = 51\ntest_size = 0.1\nX_train, X_valid, y_train, y_valid = train_test_split(trainX,trainY,random_state=seed,test_size=test_size)","d3b0a736":"#shape post split of data\nX_train.shape, X_valid.shape","6dad03d7":"#for consitency in the variable names\nX_test = testX","504c7452":"y_test = testY","e960f7b1":"y_train[0]","d92b6e5c":"#load the pretrained facenet model\nfrom keras.models import load_model\nmodel = load_model('\/kaggle\/input\/facenet-keras\/facenet_keras.h5')\nprint(model.inputs)\nprint(model.outputs)","29e66c2b":"#repeat the same color channels 3 times to make the shape compatible with the facenet model\n#X_train = np.repeat(X_train[..., np.newaxis], 3, -1)\n#X_valid = np.repeat(X_valid[..., np.newaxis], 3, -1)\n#X_test = np.repeat(X_test[..., np.newaxis], 3, -1)","143e6f7f":"X_train.shape","7878117a":"#resize the array to make it compatible with facenet. \nX_train = np.resize(X_train,(X_train.shape[0],160,160,3))\nX_valid = np.resize(X_train,(X_valid.shape[0],160,160,3))\nX_test = np.resize(X_train,(X_test.shape[0],160,160,3))","5b0fa84a":"#check the new shapes after array resize\nX_train.shape, X_valid.shape, X_test.shape","ea09797b":"#scale the inputs\n#X_train = X_train \/ 255.\n#X_valid = X_valid \/ 255.\n#X_test = X_test \/ 255.","ef3cdaae":"#use the pretrained model to generate the embeddings \ny_train_embeddings = model.predict(X_train)\ny_valid_embeddings = model.predict(X_valid)\ny_test_embeddings = model.predict(X_test)","d8ab9dba":"#check the shape of the embeddings of the images in the training set \ny_train_embeddings.shape","d13448ab":"#apply the L2 norm on the generated embeddings using facenet model\nfrom sklearn.preprocessing import Normalizer\ninput_encoder = Normalizer(norm='l2')\nembed_train_norm = input_encoder.transform(np.reshape(y_train_embeddings,(-1,128)))\nembed_valid_norm = input_encoder.transform(np.reshape(y_valid_embeddings,(-1,128)))\nembed_test_norm = input_encoder.transform(np.reshape(y_test_embeddings,(-1,128)))","dc453a07":"#check the shape post normalization \nembed_train_norm.shape","d6176d8b":"#import the needed libraries and metrics\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","4c0ecfd8":"#instantiate the model \nsvc_clf = SVC(kernel ='linear',C=10.0,probability=True)\n#fit the model\nsvc_clf.fit(embed_train_norm,y_train)","d191be65":"#predict the classes (faces)\ny_pred_train = svc_clf.predict(embed_train_norm)\ny_pred_valid = svc_clf.predict(embed_valid_norm)\ny_pred_test = svc_clf.predict(embed_test_norm)","9c635cc1":"#check the accuracy score on the various dataset \nprint('Accuracy on train set %.3f' %accuracy_score(y_train,y_pred_train))\nprint('Accuracy on the validation set %.3f' %accuracy_score(y_valid,y_pred_valid))\nprint('Accuracy on the test set %.3f' %accuracy_score(testY,y_pred_test))","e7e43c55":"#reshape the content to the dimension as stated in the problem description\nn_rows = 112\nn_cols = 92\nn_channels = 1\n\ntrainX =  np.reshape(data['trainX'],newshape=(data['trainX'].shape[0],n_rows,n_cols,n_channels))\ntestX =  np.reshape(data['testX'],newshape=(data['testX'].shape[0],n_rows,n_cols,n_channels))\ntrainY = data['trainY']\ntestY = data['testY']","adbc5a9c":"#train the model on the entire train data after shuffling the dataset \nshuffled_indices = np.random.permutation(len(trainX))\nX_train = trainX[shuffled_indices]\ny_train = trainY[shuffled_indices]","d108cca1":"#optional familiar variable naming \nX_test = testX\ny_test = testY","c4fe29dc":"#tensorflow\nimport tensorflow as tf","a3496253":"def convolutional_model(input_shape,n_filters=8,kernel_size=4,strides=1,padding='same',units=1024,activation='relu',\n                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)):\n    \n    '''CNN and Dense Model using Functional API'''\n    \n    input_img = tf.keras.Input(shape=input_shape[1:])\n    #Conv2D layer\n    Z1 = tf.keras.layers.Conv2D(filters=n_filters,kernel_size=(kernel_size,kernel_size),strides=strides,padding=padding)(input_img)\n    ##ReLU activation layer\n    A1 = tf.keras.layers.Activation(activation)(Z1)\n    ##MAXPOOL\n    P1 = tf.keras.layers.MaxPool2D(pool_size=(n_filters,n_filters),strides=strides*8,padding=padding)(A1)\n    ##Conv2D\n    Z2 = tf.keras.layers.Conv2D(filters=n_filters*2,kernel_size=(int(kernel_size\/2),int(kernel_size\/2)),strides=strides,padding=padding)(P1)\n    ##ReLU\n    A2 = tf.keras.layers.Activation(activation)(Z2)\n    ##MAXPool\n    P2 = tf.keras.layers.MaxPool2D(pool_size=(int(n_filters\/2)),strides=strides*4,padding=padding)(A2)\n    ##Flatten\n    F = tf.keras.layers.Flatten()(P2)\n    ##Dense layers\n    D1 = tf.keras.layers.Dense(units=units,activation=activation)(F)\n    D2 = tf.keras.layers.Dense(units=int(units\/2),activation=activation,kernel_regularizer=kernel_regularizer)(D1)\n    D3 = tf.keras.layers.Dense(units=int(units\/4),activation=activation,kernel_regularizer=kernel_regularizer)(D2)\n    ##output layer\n    outputs = tf.keras.layers.Dense(units=20,activation='softmax')(D3)\n    \n    #instantiate the model\n    model = tf.keras.Model(inputs=input_img,outputs=outputs)\n    return model    ","bd6c2c41":"#set the shape and the regularizer to be applied\nINPUT_SHAPE = (X_train.shape[0],112,92,1)\nkernel_regularizer = tf.keras.regularizers.L2(l2=0.05)\n\n#call the convolutional model \nconv_model = convolutional_model(input_shape=INPUT_SHAPE,n_filters=8,kernel_size=4,strides=1,padding='same',units=1024,activation='relu',\n                                kernel_regularizer=kernel_regularizer)\n#compile the model\nconv_model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\nconv_model.summary()","be90c2fd":"class MyQualityThreshold(tf.keras.callbacks.Callback):\n    def __init__(self,cl):\n        super(MyQualityThreshold, self).__init__()\n        self.cl = cl\n  \n    def on_epoch_end(self,epoch,logs=None):\n        testScore = logs['val_accuracy']\n        trainScore = logs['accuracy']\n\n        if testScore > self.cl:\n              self.model.stop_training=True","aa97423c":"#fit the model\nhistory = conv_model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test),verbose=2,\n                        callbacks=[MyQualityThreshold(0.92)],batch_size=32)","a4ba273b":"#create a dataframe of the model training history\nresults = pd.DataFrame(history.history)\nresults.head()","d6f490f4":"#plot the accuracy over the training and test set\nresults[['accuracy','val_accuracy']].plot()\nplt.title('Model Accuracy Plot')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.grid()\nplt.show()","6760306e":"#plot the loss over the training and test set\nresults[['loss','val_loss']].plot()\nplt.title('Model Loss Plot')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.grid()\nplt.show()","412360e7":"<a name='1'><\/a>\n## _1 - Import Libraries_","0a8d341c":"_The data is already split into the train and test set. For model building, we would split the train set into traina and dev set. Based on the best converged model, the predictions would be carried out on the test set_","b7608e78":"<a name='5'><\/a>\n## _5 - Approach 2: Face Classification using the normal image classification way_\n_The biggest disadvantage with this approach is that the model would need to be retrained every time the same person shows up the face to verify the person. Secondly, if a new face is added to the data set, model retraining is unavoidable. That is the biggest advantage we get using the FaceRecognition model is the embeddings which is easier to store and need not be computed for the same face again and again. if a new person comes in then the new embedding of the person can be concatenated into the list of the existing embeddings_","1f80bbe9":"_As per description, each image is of the size 112 x 92. Secondly, trainX contains 240 such images and remaining 160 images are part of the testX. The only way to visualize these images would be reshape the npzfiles arrays into the shape of 112x92. 112*92 = 10304 which is equal to the second value in the shape of data['trainX']. Hence the color channel is 1 which means all images are grayscale images._  ","c8b27cda":"### _Split the data into train and validation dataset_","6dc560b7":"<a name='4-1'><\/a>\n### _4.1 - Use Support Vector Machines to classify the face embeddings vectors_","9194cff8":"<font color ='red'> <b> _The accuracy score on all the datasets is terrible. With the hyperparameter value of C, the model overfits to the training data and performs poorly to the valid and test data set. Adding more regularization by reducing the value of C, the model also underfits substantially to the training data.The reasons could be the distortion in the input images due to resizing and repeating the arrays to make the 4th dimension to be 3. The original dataset indicated only 1 channel._ <\/font> <\/b>\n\n","b0f4307a":"_It is a general practice to normalize the embeddings generated by FaceNet_","c61eb594":"<a name='4'><\/a>\n## _4 - Approach 1: Generate Embeddings using the FaceNet_\n- The FaceNet model would generate embeddings per face image \n- Two images would be the same if the distance between them is less than a particular threshold or a specified value \n- The embeddings is a vector of 128 points \n- These embeddings can then be fed into a classifier model to predict the label. Support Vector Classifier is a usual choice. \n- The only challenge is to make the input compatible to the input shape required by the FaceNet model\n- <b> The advantage of generating embeddings is the faster computation and when a new person's face is added to the dataset, then the entire embedding need not be generated again for the existing face. This is not directly the case when only image classification approach is exercised. In that case, the entire model would need to be retrained which does not sound like a good solution <\/b>\n","ebffe261":"<a name='5-2'><\/a>\n### _5.2 - Define custom callback_","36dbf0cb":"<a name='2'><\/a>\n# _2 - Problem Statement_\n\nDESCRIPTION\n\nProblem Statement:\nFacial recognition is a biometric alternative that measures unique characteristics of a human\nface. Applications available today include flight check in, tagging friends and family members in\nphotos, and \u201ctailored\u201d advertising. You are a computer vision engineer who needs to develop a\nface recognition programme with deep convolutional neural networks.\nObjective: Use a deep convolutional neural network to perform facial recognition using Keras.\nDataset Details:\nORL face database composed of 400 images of size 112 x 92. There are 40 people, 10 images\nper person. The images were taken at different times, lighting and facial expressions. The faces\nare in an upright position in frontal view, with a slight left-right rotation.\n\n\nSteps to be followed:\n1. Input the required libraries\n2. Load the dataset after loading the dataset, you have to normalize every image.\n3. Split the dataset\n4. Transform the images to equal sizes to feed in CNN\n5. Build a CNN model that has 3 main layers:\n- i. Convolutional Layer\n- ii. Pooling Layer\n- iii. Fully Connected Layer\n6. Train the model\n7. Plot the result\n8. Iterate the model until the accuracy is above 90%","f9ba57a8":"<a name='6'><\/a>\n## _6 - Conclusion_\n- For face recognition task, usage of detection models to generate the face embeddings is always the most preferred solution. However, data preparation for facenet model led to distortion in the image resulting in poor classification performance. \n- The current problem is solved using the image classification mechanism and without the use of any pre-trained classification model\n- Functional API instead of the Sequential API used. Adding kernel regularizer to the dense layer provided a much better performance. \n- Custom callback defined as the model checkpoint. ","4a76ac13":"<a name='5-4'><\/a>\n### _5.4 - Visualize Model Performance_","0a19d8d5":"<a name='3'><\/a>\n## _3 - Loading Data_","d43b23cd":"<a name='5-1'><\/a>\n### _5.1 - Define the Functional API based Model Architecture_","070fe141":"_In order to make use of the pre-trained facenet model, the input array needs to be resized to be compatible to the required shape of facenet as indicated by the model.inputs._ ","1bb4dde4":"## Table of Contents\n\n- [1 - Import Libraries and Load Data](#1)\n- [2 - Problem Statement](#2)\n- [3 - Loading Data](#3)\n    - [3.1 - Basic exploration of the npz file content](#3-1)\n    - [3.2 - Reshape the data and plot random images](#3-2)\n    - [3.3 - Visualize the Faces](#3-3)\n- [4 - Approach 1: Generate Embeddings using the FaceNet Model](#4)\n    - [4.1 - Use Support Vector Machines to classify the face embeddings vectors](#4-1)       \n- [5 - Approach 2: Face Classification using the normal image classification way](#5)\n    - [5.1 Define the Functional API based Model Architecture](#5-1)\n    - [5.2 Define Custom Callback](#5-2)\n    - [5.3 Train the Model](#5-3)\n    - [5.4 - Visualize the Model Performance](#5-4)\n- [6 - Conclusion)](#6)\n","fa75d670":"<a name='3-1'><\/a>\n### _3.1 - Basic exploration of the npz file content_","d53bb51e":"<a name='3-2'><\/a>\n### _3.2 - Reshape the data and plot random images_","ea2bdbb3":"_Clearly the labels in the test folder indicates there are faces of 20 people. Secondly, the trainY content indicates that the data is not randomized. This can be done at the time of splitting the data into train and dev set so that the model can generalize well to the variation and does not learn the pattern from the way the data is structured and stored_","0ffaba4e":"<a name='5-3'><\/a>\n### _5.3 - Train the model_","2f764dd4":"<a name='3-3'><\/a>\n### _3.3 - Visualize the faces_"}}