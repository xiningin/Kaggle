{"cell_type":{"63e08cfe":"code","d0a4aa5e":"code","17530f4b":"code","6b6d90c4":"code","ebe56112":"code","bc0d3e2c":"code","8d71de3a":"code","75ae29c3":"code","c09326f4":"code","cfca7956":"code","1eb6e3d5":"code","2e4ebdd9":"code","bed101bf":"code","ebb54a9b":"code","30aab518":"code","1da1bd56":"code","5a7db19f":"code","ea2f7f82":"code","07866a37":"code","129cd811":"code","fb8a9829":"code","9ee42d93":"code","83063634":"code","3cc2ca8e":"code","f6bf803f":"code","b05fe653":"code","0b3f6f81":"code","bf7fd0b7":"code","f2d0ae5b":"code","af75a02a":"code","2baf47cc":"code","7862128c":"code","e9fb12a6":"code","4f2fc192":"code","767fffa0":"code","715e7e7d":"code","7db1113f":"markdown","f2abe88f":"markdown","870f7f70":"markdown","5500cc96":"markdown","1b8dfa2c":"markdown","91766fb2":"markdown","42228706":"markdown","131530c7":"markdown","98de4c59":"markdown","109d6d4e":"markdown","02cb7b9c":"markdown","cdc72dec":"markdown","3f9a5889":"markdown","24bcd80c":"markdown","64e1ddc9":"markdown","5a56150c":"markdown","e2e34845":"markdown","088eed49":"markdown","e79b5e39":"markdown","7287d0c3":"markdown"},"source":{"63e08cfe":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport matplotlib.pylab as plt\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nprint(\"TF version:\", tf.__version__)\nprint(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")","d0a4aa5e":"validation_dir = '\/kaggle\/input\/cat-and-dogs\/dataset\/test_set'\ntrain_dir = '\/kaggle\/input\/cat-and-dogs\/dataset\/training_set'\n\nBATCH_SIZE = 32\nIMG_SIZE = (160,160)","17530f4b":"train_dataset = image_dataset_from_directory(train_dir, shuffle = True, batch_size = BATCH_SIZE, image_size = IMG_SIZE)","6b6d90c4":"validation_dataset = image_dataset_from_directory(validation_dir, shuffle = True, batch_size = BATCH_SIZE, image_size = IMG_SIZE)","ebe56112":"class_names = train_dataset.class_names\n\nplt.figure(figsize = (10,10))\nfor images,labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3,3,i+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","bc0d3e2c":"val_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches \/\/ 5)\nvalidation_dataset = validation_dataset.skip(val_batches \/\/ 5)\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))","8d71de3a":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","75ae29c3":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","c09326f4":"for image, _ in train_dataset.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] \/ 255)\n        plt.axis('off')","cfca7956":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","1eb6e3d5":"rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5,offset = -1)","2e4ebdd9":"IMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","bed101bf":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","ebb54a9b":"base_model.trainable = False","30aab518":"# Summary\nbase_model.summary()","1da1bd56":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","5a7db19f":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","ea2f7f82":"inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","07866a37":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","129cd811":"# Summary\nmodel.summary()","fb8a9829":"len(model.trainable_variables)","9ee42d93":"initial_epochs = 15\nloss0, accuracy0 = model.evaluate(validation_dataset)","83063634":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","3cc2ca8e":"history = model.fit(train_dataset,epochs=initial_epochs,\n                    validation_data=validation_dataset)","f6bf803f":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","b05fe653":"base_model.trainable = True","0b3f6f81":"print(\"Number of layers in the base model: \", len(base_model.layers))","bf7fd0b7":"fine_tune_at = 80\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","f2d0ae5b":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])","af75a02a":"# Summary\nmodel.summary()","2baf47cc":"len(model.trainable_variables)","7862128c":"fine_tune_epochs = 25\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_dataset,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=validation_dataset)","e9fb12a6":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","4f2fc192":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","767fffa0":"loss, accuracy = model.evaluate(test_dataset)\nprint('Test accuracy :', accuracy)","715e7e7d":"#Retrieve a batch of images from the test set\nimage_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\n\n# Apply a sigmoid since our model returns logits\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(10, 10))\nfor i in range(16):\n  ax = plt.subplot(4, 4, i + 1)\n  plt.imshow(image_batch[i].astype(\"uint8\"))\n  plt.title(class_names[predictions[i]])\n  plt.axis(\"off\")","7db1113f":"# Evaluating and Predicting","f2abe88f":"# Importing Liberaries","870f7f70":"**Rescaling Pixel Values**","5500cc96":"**Data Augmentation**","1b8dfa2c":"# Summary\n\n* **Using a pre-trained model for feature extraction:** When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training. In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n\n* **Fine-tuning a pre-trained model:** To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.","91766fb2":"**Compiling the Model**","42228706":"# Fine Tuning the Model","131530c7":"**Comparing Curves**","98de4c59":"# Training The Model","109d6d4e":"# Training on Tuned Model","02cb7b9c":"**Creating base model from the pre-trained convnets**","cdc72dec":"**Loading Data**","3f9a5889":"**Add Classification Head**","24bcd80c":"# Data Preprocessing","64e1ddc9":"**Displaying Some Data**","5a56150c":"# Compiling Tuned Model","e2e34845":"# Feature Extraction","088eed49":"**Configuring Dataset**","e79b5e39":"**Comparing Curves**","7287d0c3":"**Creating Test Data Set in Training data**"}}