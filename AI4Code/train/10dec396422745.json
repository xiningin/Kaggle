{"cell_type":{"31abc707":"code","26bfb64e":"code","bd9bc57b":"code","8ca77fb2":"code","a850dfb6":"code","c436f080":"code","a4539a0d":"code","21929602":"code","a8fa4a20":"code","fc9b6136":"code","7e3520ef":"code","20ff6c62":"code","96422c97":"code","4a8013aa":"code","012da199":"code","4b4f51f2":"code","684664ce":"code","bfd11187":"code","b3c1d5bb":"code","1263f681":"code","18b53e63":"code","2d14a23d":"code","c344c803":"code","73917305":"code","a68a2010":"code","131e7a3b":"code","ceb47a22":"code","f41a3327":"code","334bdf6f":"code","635137a4":"code","a75ce4a6":"code","d04c7e8e":"code","22522002":"code","dd13eed6":"code","ed2c5091":"code","7bd28a48":"code","b348f9d2":"code","69bf131c":"code","e1e6ece3":"code","5f385b10":"code","20ada074":"code","21c1fef3":"code","e139d13e":"code","feb9eed6":"code","f3adffc8":"code","73703ced":"code","06958fd5":"code","a53885d0":"code","c0c7dd99":"markdown","1d70582e":"markdown","19404f43":"markdown","80c9181d":"markdown","976635f3":"markdown","16ad30d0":"markdown","d787cafa":"markdown","75f8fea5":"markdown","5c5afd57":"markdown","71d894fc":"markdown","dbf1878a":"markdown","345b6d73":"markdown","161b0f4a":"markdown","62c627fd":"markdown","f9139c56":"markdown","645f4a81":"markdown","86488896":"markdown","eb73ddf2":"markdown","dcfb2406":"markdown","8ddac2cb":"markdown","bc95d478":"markdown","d92d3121":"markdown","3b04bdee":"markdown","a73353ab":"markdown","ea9fdb4e":"markdown","495cb2cc":"markdown","076409a5":"markdown","80fba2fe":"markdown","6dac6ca4":"markdown","84cc3a47":"markdown","aba42ecc":"markdown","1def291b":"markdown"},"source":{"31abc707":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26bfb64e":"df_train = pd.read_csv(\"\/kaggle\/input\/nyc-taxi-trip-duration\/train.zip\", compression=\"zip\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/nyc-taxi-trip-duration\/test.zip\", compression=\"zip\")\ndf_sub = pd.read_csv(\"\/kaggle\/input\/nyc-taxi-trip-duration\/sample_submission.zip\", compression=\"zip\")","bd9bc57b":"for name, ds in zip([\"df_train\",\"df_test\",\"df_sub\"],[df_train, df_test, df_sub]):\n    \n    print(\"---------------\")\n    print(\"{}\\n\".format(name))\n    print(ds.info())\n    print(\"\\n\")","8ca77fb2":"for name, ds in zip([\"df_train\",\"df_test\",\"df_sub\"],[df_train, df_test, df_sub]):\n    \n    print(\"---------------\")\n    print(\"{}\\n\".format(name))\n    print(ds.isnull().sum())\n    print(\"\\n\")","a850dfb6":"pd.concat([df_train.head(),df_train.tail()],axis=0)","c436f080":"pd.concat([df_test.head(),df_test.tail()],axis=0)","a4539a0d":"print(\"cat features train set: {}\".format(df_train.select_dtypes(exclude=\"number\").columns))\nprint(\"\\n\")\nprint(\"numeric features train set: {}\".format(df_train.select_dtypes(include=\"number\").columns))","21929602":"print(\"cat features test set: {}\".format(df_test.select_dtypes(exclude=\"number\").columns))\nprint(\"\\n\")\nprint(\"numeric features test set: {}\".format(df_test.select_dtypes(include=\"number\").columns))","a8fa4a20":"df_train[\"pickup_datetime\"] = pd.to_datetime(df_train[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\ndf_train[\"dropoff_datetime\"] = pd.to_datetime(df_train[\"dropoff_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n\ndf_test[\"pickup_datetime\"] = pd.to_datetime(df_test[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")","fc9b6136":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\ndf_train[\"year\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.year)\ndf_train[\"month\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.month)\ndf_train[\"day\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.day)\ndf_train[\"hour\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.hour)\ndf_train[\"minute\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.minute)\n\ndf_test[\"year\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.year)\ndf_test[\"month\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.month)\ndf_test[\"day\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.day)\ndf_test[\"hour\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.hour)\ndf_test[\"minute\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.minute)","7e3520ef":"print(\"df_train: {}\".format(df_train[\"year\"].unique()))\nprint(\"df_test: {}\".format(df_test[\"year\"].unique()))","20ff6c62":"fig, sub = plt.subplots(2,3,figsize=(25,6))\ncounter = 0\n\nfor feat, subplot in zip([\"month\",\"day\",\"hour\",\"month\",\"day\",\"hour\"], sub.flatten()):\n    \n    if counter<3:\n        sns.barplot(x=df_train[feat].value_counts().index, y = df_train[feat].value_counts().values, ax= subplot, palette=\"CMRmap\")\n        subplot.grid()\n        subplot.set_title(\"Train set {}\".format(feat))\n    else:\n        sns.barplot(x=df_test[feat].value_counts().index, y = df_test[feat].value_counts().values, ax= subplot, palette=\"CMRmap\")\n        subplot.grid()\n        subplot.set_title(\"Test set {}\".format(feat))\n    \n    counter+=1\n    \nfig.tight_layout()","96422c97":"fig,sub = plt.subplots(2,3,figsize=(12,6))\n\nsns.distplot(df_train[\"trip_duration\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][0])\nsns.distplot(np.log1p(df_train[\"trip_duration\"]), hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][1])\nsns.distplot(df_train[df_train[\"trip_duration\"]<60*120][\"trip_duration\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][2])\n\nprob = stats.probplot(df_train[\"trip_duration\"], dist=stats.norm, plot=sub[1][0])\nprob = stats.probplot(np.log1p(df_train[\"trip_duration\"]), dist=stats.norm, plot=sub[1][1])\nprob = stats.probplot(df_train[df_train[\"trip_duration\"]<60*120][\"trip_duration\"], dist=stats.norm, plot=sub[1][2])\n\n\ncounter = 0\n\nfor name, subplot in zip([\"duration [raw data]\",\"duration [log]\", \"duration [< 2 h]\",\"probPlot duration [raw data]\",\"probPlot duration [log]\", \"probPlot duration [< 2 h]\"], sub.flatten()):\n    subplot.set_title(\"{}\".format(name))\n    subplot.grid()\n    \n    if counter < 3:\n        if name == \"duration [log]\": \n            subplot.set_xlabel(\"trip duration in log(sec)\")\n        else:\n            subplot.set_xlabel(\"trip duration in sec\")\n    \n    counter += 1\n\nfig.tight_layout()","4a8013aa":"def get_outlier(df):\n    \n    outlier_index = []\n    \n    q1 = np.quantile(df[\"trip_duration\"],0.25)\n    q3 = np.quantile(df[\"trip_duration\"],0.75)\n    IQR = q3 - q1\n    outlier_step = 1.5 * IQR\n    \n    lower_barreer = q1 - outlier_step\n    upper_barreer = q3 + outlier_step\n    \n    outlier_list_col = df[(df[\"trip_duration\"] < lower_barreer) | (df[\"trip_duration\"] > upper_barreer)].index\n    outlier_index.extend(outlier_list_col)\n    \n    return outlier_index","012da199":"print(\"tuckey outlier df_train: {}\".format(df_train.iloc[get_outlier(df_train)].shape))","4b4f51f2":"print(\"min duration: {} sec \".format(df_train[\"trip_duration\"].min()))\nprint(\"max duration: {} hour \".format(np.round(df_train[\"trip_duration\"].max()\/(60**2)),2))","684664ce":"from math import sin, cos, sqrt, atan2, radians\n\ndef get_distance(lon_1, lon_2, lat_1, lat_2):\n\n    # approximate radius of earth in km\n    R = 6373.0\n\n    lat1 = radians(lat_1)\n    lon1 = radians(lon_1)\n    lat2 = radians(lat_2)\n    lon2 = radians(lon_2)\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n\n    return distance","bfd11187":"df_train[\"distance\"] = df_train.progress_apply(lambda x: get_distance(x[\"pickup_longitude\"],x[\"dropoff_longitude\"],x[\"pickup_latitude\"],x[\"dropoff_latitude\"]),axis=1)\ndf_test[\"distance\"] = df_test.progress_apply(lambda x: get_distance(x[\"pickup_longitude\"],x[\"dropoff_longitude\"],x[\"pickup_latitude\"],x[\"dropoff_latitude\"]),axis=1)","b3c1d5bb":"fig,sub = plt.subplots(2,3,figsize=(12,6))\n\nsns.distplot(df_train[\"distance\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][0])\nsns.distplot(np.log1p(df_train[\"distance\"]), hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][1])\nsns.distplot(df_train[df_train[\"distance\"]<15][\"distance\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][2])\n\nprob = stats.probplot(df_train[\"distance\"], dist=stats.norm, plot=sub[1][0])\nprob = stats.probplot(np.log1p(df_train[\"distance\"]), dist=stats.norm, plot=sub[1][1])\nprob = stats.probplot(df_train[df_train[\"distance\"]<15][\"distance\"], dist=stats.norm, plot=sub[1][2])\n\n\ncounter = 0\n\nfor name, subplot in zip([\"distance [raw data]\",\"distance [log]\", \"distance [< 15 km]\",\"probPlot distance [raw data]\",\"probPlot distance [log]\", \"probPlot distance [< 2 h]\"], sub.flatten()):\n    subplot.set_title(\"{}\".format(name))\n    subplot.grid()\n    \n    if counter < 3:\n        if name == \"km [log]\": \n            subplot.set_xlabel(\"distance in log(km)\")\n        else:\n            subplot.set_xlabel(\"distance in km\")\n    \n    counter += 1\n\nfig.tight_layout()","1263f681":"df_train.head()","18b53e63":"import folium\nf = folium.Figure(width=1500, height=500)\nmapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)\n\nfor index, row in df_train.sample(1000).iterrows():\n    folium.Marker([row[\"pickup_latitude\"], row[\"pickup_longitude\"]], icon=folium.Icon(color=\"blue\")).add_to(mapa)\n    folium.Marker([row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]], icon=folium.Icon(color=\"red\")).add_to(mapa)\n\n\ndisplay(mapa)","2d14a23d":"import folium\nf = folium.Figure(width=1500, height=500)\nmapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)\n\nfor index, row in df_train[df_train[\"distance\"]>20].sample(200).iterrows():\n    folium.Marker([row[\"pickup_latitude\"], row[\"pickup_longitude\"]], icon=folium.Icon(color=\"blue\")).add_to(mapa)\n    folium.Marker([row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]], icon=folium.Icon(color=\"red\")).add_to(mapa)\n\n\ndisplay(mapa)","c344c803":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\ndf_train[\"pickUp_coordinates\"] = df_train.progress_apply(lambda x: (x[\"pickup_latitude\"], x[\"pickup_longitude\"]), axis=1)\ndf_train[\"dropOff_coordinates\"] = df_train.progress_apply(lambda x: (x[\"dropoff_latitude\"], x[\"dropoff_longitude\"]), axis=1)","73917305":"fig ,sub = plt.subplots(1,1,figsize=(12,4))\n\nsns.barplot(x = df_train[\"passenger_count\"].value_counts().index, y = df_train[\"passenger_count\"].value_counts().values, ax= sub, palette=\"PuBu_r\")\nsub.grid()\nsub.set_xlabel(\"Passenger per ride\");","a68a2010":"correlation = df_train[[\"trip_duration\",\"month\",\"day\",\"hour\",\"distance\"]].corr()\nmask = np.triu(np.ones_like(correlation, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(correlation, mask=mask, cmap=cmap, linecolor = \"black\",lw=0.09);","131e7a3b":"fig, sub = plt.subplots(1,3,figsize=(25,5))\n\nfor name, subplot in zip([\"month\",\"day\", \"hour\"], sub.flatten()):\n    \n    data = df_train.groupby(name)[\"trip_duration\"].mean()\n    sns.barplot(x=data.index, y=data.values, ax=subplot, palette=\"CMRmap\")\n    subplot.grid(color=\"lightgrey\")\n\nfig.tight_layout()","ceb47a22":"print(\"ANOVA month\/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"month\"]==feat][\"trip_duration\"]) for feat in df_train[\"month\"].unique()])))\nprint(\"ANOVA day\/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"day\"]==feat][\"trip_duration\"]) for feat in df_train[\"day\"].unique()])))\nprint(\"ANOVA hour\/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"hour\"]==feat][\"trip_duration\"]) for feat in df_train[\"hour\"].unique()])))","f41a3327":"from itertools import combinations \nhour_list = df_train[\"month\"].unique()\n\nfor feat1, feat2 in combinations(hour_list,2):\n    t, p = stats.ttest_ind(np.log1p(df_train[df_train[\"month\"]==feat1][\"trip_duration\"]),np.log1p(df_train[df_train[\"month\"]==feat2][\"trip_duration\"]))\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","334bdf6f":"from itertools import combinations \nhour_list = df_train[\"hour\"].unique()\n\nfor feat1, feat2 in combinations(hour_list,2):\n    t, p = stats.ttest_ind(np.log1p(df_train[df_train[\"hour\"]==feat1][\"trip_duration\"]),np.log1p(df_train[df_train[\"hour\"]==feat2][\"trip_duration\"]))\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","635137a4":"fig, sub = plt.subplots(1,1,figsize=(12,6))\n\nsns.barplot(x=df_train.groupby(\"passenger_count\")[\"trip_duration\"].mean().index, y= df_train.groupby(\"passenger_count\")[\"trip_duration\"].mean(), ax=sub)\nsub.grid()","a75ce4a6":"print(\"ANOVA passenger count\/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"passenger_count\"]==feat][\"trip_duration\"]) for feat in df_train[\"passenger_count\"].unique()])))","d04c7e8e":"from itertools import combinations \nimport warnings \nwarnings.filterwarnings(\"ignore\")\npassenger_list = df_train[\"passenger_count\"].unique()\n\nfor feat1, feat2 in combinations(passenger_list,2):\n    \n    feat1_data = np.log1p(df_train[df_train[\"passenger_count\"]==feat1][\"trip_duration\"])\n    feat2_data = np.log1p(df_train[df_train[\"passenger_count\"]==feat2][\"trip_duration\"])\n    \n    t, p = stats.ttest_ind(feat1_data, feat2_data)\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","22522002":"from sklearn.preprocessing import LabelEncoder\n\nX, y= df_train[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]], df_train[\"trip_duration\"]\n\nenc = LabelEncoder()\nX[\"store_and_fwd_flag\"] = enc.fit_transform(X[\"store_and_fwd_flag\"])","dd13eed6":"from sklearn.model_selection import KFold \nkf = KFold(n_splits=5) \nkf.get_n_splits(X,y)","ed2c5091":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error\n\nxgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"learning_rate\": 0.1,\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","7bd28a48":"fold_dict = {}\n\nfor fold,(train_index, test_index) in enumerate(kf.split(X)):\n\n    print(\"Training the fold {}\".format(fold+1))\n    reg = xgb.XGBRegressor(**xgboost_params)\n    reg.fit(X.values[train_index],y.values[train_index])\n    xgb_preds = abs(reg.predict(X.values[test_index]))\n    fold_dict[fold] = mean_squared_log_error(y.values[test_index], xgb_preds)\n    print(\"Result for fold {}: {}\".format(fold+1, mean_squared_log_error(y.values[test_index], xgb_preds)))","b348f9d2":"booster = reg.get_booster()\nimportance = booster.get_fscore()\nimp_dict = {X.columns[i]:float(importance.get('f'+str(i),0.)) for i in range(len(X.columns))}\nsorted_importance = {k: v for k, v in sorted(imp_dict.items(), key=lambda item: item[1])}\n\nfig, sub = plt.subplots(1,1,figsize=(12,4))\nsns.barplot(x=list(sorted_importance.keys()),y=list(sorted_importance.values()), ax=sub);","69bf131c":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2)","e1e6ece3":"train_errors = []\nval_errors = []\n\nfor i in range(1, X_train.shape[0],2*10**5):\n    print(\"Round training from sample 1 to sample {}\".format(i))\n    reg.fit(X_train.iloc[:i],y_train.iloc[:i])\n    train_preds = abs(reg.predict(X_train.iloc[:i]))\n    val_preds = abs(reg.predict(X_val))\n    train_errors.append(mean_squared_log_error(y_train.iloc[:i], train_preds))\n    val_errors.append(mean_squared_log_error(y_val, val_preds))","5f385b10":"fig, sub = plt.subplots(1,1,figsize=(12,6))\n\nsns.lineplot(x=range(1, X_train.shape[0],2*10**5), y=train_errors, label=\"Training Error\")\nsns.lineplot(x=range(1, X_train.shape[0],2*10**5), y=val_errors, label=\"Validation Error\")\n\nsub.grid()","20ada074":"xgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","21c1fef3":"train_errors = []\nval_errors = []\n\nfor i in np.arange(0.05, 1.0, 0.1):\n    print(\"Learning rate {}\".format(i))\n    \n    xgboost_params[\"learning_rate\"] = i \n    \n    reg = xgb.XGBRegressor(**xgboost_params)\n    reg.fit(X_train,y_train)\n    train_preds = abs(reg.predict(X_train))\n    val_preds = abs(reg.predict(X_val))\n    train_errors.append(mean_squared_log_error(y_train, train_preds))\n    val_errors.append(mean_squared_log_error(y_val, val_preds))","e139d13e":"fig, sub = plt.subplots(1,1,figsize=(12,5))\n\nsns.lineplot(x=np.arange(0.05, 1.0, 0.1),y=train_errors, label=\"Training loss\",color=\"blue\", ax=sub)\nsns.lineplot(x=np.arange(0.05, 1.0, 0.1),y=val_errors, label=\"validation loss\",color=\"dimgrey\", ax=sub)\n\nsub.set_xticks(np.arange(0,1.1,0.1))\n\nsub.grid()","feb9eed6":"X_train, y_train= df_train[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]], df_train[\"trip_duration\"]\n\nenc = LabelEncoder()\nX_train[\"store_and_fwd_flag\"] = enc.fit_transform(X_train[\"store_and_fwd_flag\"])","f3adffc8":"X_test = df_test[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]]\n\nenc = LabelEncoder()\nX_test[\"store_and_fwd_flag\"] = enc.fit_transform(X_test[\"store_and_fwd_flag\"])","73703ced":"xgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"learning_rate\": 0.1,\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","06958fd5":"reg = xgb.XGBRegressor(**xgboost_params)\nreg.fit(X_train,y_train)\nxgb_preds = abs(reg.predict(X_test))","a53885d0":"f = {\"id\":df_sub[\"id\"],\"trip_duration\":xgb_preds}\nf = pd.DataFrame(f)\nf.to_csv(\"submission.csv\",index=False)","c0c7dd99":"## Which time frame is considered in the dataset?","1d70582e":"### Are there many outlier present in the dataset?","19404f43":"## is the trip duration time-dependent?","80c9181d":"* the datasets focus only the year 2016","976635f3":"__First of all, it is important to know the data itself a little better. it would be nice to know:__\n  * how many datasets are relevant\n  * which columns are present in the data sets\n  * which datatypes are present \n  * how many rows are in the datasets\n  * how heavy the dataset are in terms of memory usage","16ad30d0":"## Which distances are travelled?","d787cafa":"* many of the ride > 20 are rides to the or from the airports","75f8fea5":"* most of the rides are people taking a taxi alone","5c5afd57":"* no missing values in the data set","71d894fc":"* only the combinations above are not signific. different from each other","dbf1878a":"* the log of duration seems to be the most suitable to use anova and t-test","345b6d73":"#### Another important question to get to know the data is the structure concerning missingness. Are there many values gone missing or is the dataset complete?","161b0f4a":"* right skewed distribution: main part of the data is distributed between 0 and 58 min (2092 s)\n* partly heavy outliers (in total 74,2 k outlier)\n* max duration 980 h","62c627fd":"Analysis of Learning curve:\n   * Training error and Validation error are converging --> Adding more training data doesn't improve the model\n   * the error level of ~ 0.33 is concerning the ranking not terrible\n   * concerning variance, the model generalizes well on the validation set (the gap between Training and validation error is low and get narrower by adding more training data) ","f9139c56":"* in all three time categories at least two means are significantly different from each other","645f4a81":"# Multivariate Analysis","86488896":"### to verify this hypothesis: we'll conduct some hypothesis test","eb73ddf2":"For closing this first description of the datasets, it's maybe also interesting to have a look on the raw data","dcfb2406":"* all records are from the year 2016 and regarding the months the rides took place between January and June 2016\n* the rides are approx. equally distributed to the days per months. however at the end of the month there are fewer rides\n* regarding the time of the day, most of the clients have been driven between 18 - 23 h and the least has been transported between 0 and 6 h ","8ddac2cb":"* no linear relationship observable between the features","bc95d478":"but first of all, we have to know which test we can use to verify\nto use anova or t-tests to verify, the data needs to be normally distributed <br>\n--> to check if this prerequisite is met, we'll have a look on the q-q-plots of the feature \"trip_duration\"","d92d3121":"### Validation curve Learning Rate ","3b04bdee":"# First Model","a73353ab":"# General Information about the dataset","ea9fdb4e":"by night and in the morning, the trip duration is signific. lower than in the time frame between 14-18 h","495cb2cc":"* only month january and february are not signific. different from each other in terms of trip duration","076409a5":"* the rides take place in the newyork region with focus on manhattan\n* some passengers set out to be dropped off in areas outside newyork or the airport","80fba2fe":"## Data exploration","6dac6ca4":"## Where do the rides take exactly place?","84cc3a47":"#### Anova","aba42ecc":"Map where the passengers have been picked up (blue) and dropped off (red)","1def291b":"* the main goal of the competition is to predict the trip duration of a taxi drive based on several attributes describing the drive\n* three dataset are provided: train set, test set and submission file\n* train set\/test set\n    * time features: pickup and dropoff datetime\n    * geographical features: pickup and drop off longitude\/latitude\n    * others: store-and fwd Flag, passenger count\n* submission file:\n    * structure of the submission "}}