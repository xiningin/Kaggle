{"cell_type":{"59e18b2b":"code","1b2552c9":"code","8f6b1abe":"code","e7b72b7b":"code","0a6042ca":"code","5e02cedb":"code","2c988340":"code","ea524e8b":"code","6bb9abb8":"code","9099b1ae":"code","5e76fbaa":"code","43f02c8e":"code","547c159b":"code","14aa3ec9":"code","0c169614":"code","541f7912":"code","861c3178":"code","df927a9a":"code","674551b0":"code","d7dcf5ad":"code","c6268e73":"code","657d93dd":"code","78856a9d":"code","0a610a6c":"code","69380ee3":"code","c31ef634":"code","37acd97c":"code","fbf533dd":"code","1c1e1c26":"code","af8a56df":"code","9a270b9b":"code","ee774d9c":"code","50b820fd":"code","1fecf1f7":"code","4f12b269":"code","cf37d14e":"code","34810c51":"code","c5d6820e":"code","cd6a1a17":"code","ca74e3fb":"code","53808fac":"code","4cd604f1":"code","248f03b0":"code","78aba7b6":"code","b1dea52d":"code","18ae22c8":"code","a1abae33":"code","50a46b6e":"code","884ae990":"code","07f81aba":"code","666f447f":"code","ee589c14":"code","1e115d81":"code","c1ed3940":"code","19aaadd2":"code","28e7ebb2":"code","78636fe1":"code","c3e94e3c":"code","9d4a326b":"code","64300f20":"code","bf1ffc54":"code","7c457630":"code","348de574":"markdown","6a60f344":"markdown","6242e713":"markdown","dddb2408":"markdown","7c39c1ff":"markdown","8aed93eb":"markdown","dc9a5416":"markdown","4b155887":"markdown","323a401e":"markdown","eefef3bc":"markdown","0eb5a086":"markdown","02ad709c":"markdown","8d818670":"markdown","61bd151e":"markdown","9dd5a4f6":"markdown","e251558c":"markdown","81db8c74":"markdown","e716de8b":"markdown","4bd6db0a":"markdown","b05af857":"markdown","b9c32b78":"markdown","a60b7022":"markdown","a209ac7b":"markdown","ca784a4d":"markdown"},"source":{"59e18b2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport datetime as dt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1b2552c9":"# Read the dataset\nacc_df = pd.read_csv('..\/input\/us-accidents\/US_Accidents_May19.csv')","8f6b1abe":"# First Look into the data\nprint('Rows, Columns - ', acc_df.shape)\nacc_df.head()","e7b72b7b":"# Get Attribute and Datatype Information\nacc_df.info()","0a6042ca":"# Data represents how many States ?\nprint(f\"Data represents {acc_df['State'].unique().shape[0]} states\")","5e02cedb":"# Converting timestamp fileds into datetime.\nacc_df['Start_Time'] = pd.to_datetime(acc_df['Start_Time'], infer_datetime_format=True)\nacc_df['End_Time'] = pd.to_datetime(acc_df['End_Time'], infer_datetime_format=True)\nacc_df['Weather_Timestamp'] = pd.to_datetime(acc_df['Weather_Timestamp'], infer_datetime_format=True)","2c988340":"# Missing Data\nmissing_data = []\ntotal = acc_df.isna().sum()\npercentage = round((acc_df.isna().sum()\/acc_df.isna().count())*100, 0).astype(int)\nmissing_data = pd.concat([total, percentage], axis=1, keys=['Sum', 'Percentage(%)'])\nmissing_data = missing_data.sort_values('Percentage(%)', ascending=False)\nmissing_data","ea524e8b":"# Replace missing values with 0s\nacc_df = acc_df.fillna(0)","6bb9abb8":"# Missing Data\nmissing_data = []\ntotal = acc_df.isna().sum()\npercentage = round((acc_df.isna().sum()\/acc_df.isna().count())*100, 0).astype(int)\nmissing_data = pd.concat([total, percentage], axis=1, keys=['Sum', 'Percentage(%)'])\nmissing_data = missing_data.sort_values('Percentage(%)', ascending=False)\nmissing_data","9099b1ae":"acc_df['Counter']=1","5e76fbaa":"# Determining the timeframe of the data\nprint( 'State Date - ', acc_df['Start_Time'].min(), '\\nEnd Date   - ', acc_df['Start_Time'].max())","43f02c8e":"# Different sources providing accident details \nprint('Source of information:')\nprint('----------------------')\nprint((round(acc_df['Source'].value_counts(normalize=True)*100, 0)).astype(int).astype(str) + '%')\nax = sns.countplot(x=\"Source\", data=acc_df, order = acc_df['Source'].value_counts().index)","547c159b":"# Frequency Distribution of TMC (Note - 0 represents missing data. 0 is not a valid TMC Code)\nprint('Frequency Distribution of TMC')\nprint('Code       Frequency ')\nprint('-----------------------------')\nplt.figure(figsize = (16, 6))\nprint(acc_df['TMC'].value_counts())\nsns.countplot(x=\"TMC\", data=acc_df, order = acc_df['TMC'].value_counts().index)","14aa3ec9":"# TMC Labels refence - https:\/\/wiki.openstreetmap.org\/wiki\/TMC\/Event_Code_List\ntmc_code_labels = ['(Q) accident(s)','Missing TMC Code',\n'(Q) accident(s). Right lane blocked',\n'(Q) accident(s). Two lanes blocked',\n'(Q) accident(s). Slow traffic',\n'multi-vehicle accident (involving Q vehicles)',\n'(Q) accident(s). Queuing traffic',\n'(Q) accident(s). Hard shoulder blocked',\n'(Q th) entry slip road closed',\n'(Q) serious accident(s)',\n'(Q) accident(s). Three lanes blocked',\n'accident. Delays (Q)',\n'(Q) earlier accident(s)',\n'(Q) accident(s). Heavy traffic',\n'accident. Delays (Q) expected',\n'(Q) fuel spillage accident(s)',\n'(Q) jackknifed trailer(s)',\n'(Q) jackknifed articulated lorr(y\/ies)',\n'multi vehicle pile up. Delays (Q)',\n'(Q) oil spillage accident(s)',\n'(Q) accident(s). Traffic building up',\n'(Q) accident(s) in roadworks area']","0c169614":"# Create a lookup of TMC Code and Description. This will give us an idea of whats these codes mean\nvalues = acc_df['TMC'].value_counts().keys().astype(int).tolist()\ncounts = acc_df['TMC'].value_counts().tolist()\ntmc_lookup_df = pd.DataFrame({'TMC_CODE':values, 'TMC_CODE_DESC': tmc_code_labels, 'COUNT':counts})\ntmc_lookup_df","541f7912":"# Frequency Distribution for Severity\nplt.figure(figsize = (5, 5))\nprint('Severity:')\nprint('---------')\nprint((acc_df['Severity'].value_counts(normalize=True)*100).astype(int).astype(str) + '%')\nsns.countplot(x='Severity', data=acc_df)","861c3178":"# Creating a new column called Accident Duration\nacc_df['Accident_Duration_Mins'] = (acc_df['End_Time'] - acc_df['Start_Time']).astype('timedelta64[m]').astype(int)","df927a9a":"# Accident Duration by Severity\nacc_duration = acc_df[['Accident_Duration_Mins']].groupby(acc_df['Severity']).agg(['count', 'mean', 'std', 'median', 'min', 'max']).astype(int).round()\nacc_duration","674551b0":"# No of accidents by States\nplt.figure(figsize = (16, 6))\nsns.countplot(x='State', data=acc_df, order = acc_df['State'].value_counts().index)\nplt.title('Count of accidents by State')","d7dcf5ad":"# Select top 5 states\nacc_df_top_state = acc_df[acc_df['State'].isin(['CA', 'TX', 'FL', 'NC', 'NY'])][['Start_Time', 'State', 'Severity']]","c6268e73":"# How many accidents happened in last 4 years in top 5 states\nacc_df_top_state['State'].value_counts()","657d93dd":"# Breakup of accidents by Severity\nacc_df_top_state['COUNTER'] =1\nacc_df_top_state.groupby(['State','Severity'])['COUNTER'].sum()","78856a9d":"# Extract Year and Month of accident from Start_Time\nacc_df_top_state['Year'] = acc_df_top_state['Start_Time'].map(lambda x: x.year)\nacc_df_top_state['Month'] = acc_df_top_state['Start_Time'].map(lambda x: x.month)\nacc_df_top_state[['Start_Time', 'Year', 'Month']].head()","0a610a6c":"# Breakup of Accident by Year\nplt.figure(figsize = (10, 6))\nprint(acc_df_top_state.groupby(['Year'])['COUNTER'].sum())\nax = acc_df_top_state.groupby(['Year'])['COUNTER'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_top_state.groupby(['Year'])['COUNTER'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3'], title='No of Accidents by Year', ax=ax)","69380ee3":"# Breakup of Accident by Month\nplt.figure(figsize = (10, 6))\nprint(acc_df_top_state.groupby(['Month'])['COUNTER'].sum())\nax = acc_df_top_state.groupby(['Month'])['COUNTER'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_top_state.groupby(['Month'])['COUNTER'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4', 'C5'], title='No of Accidents by Month', ax=ax)","c31ef634":"# Which months have most severe 2 accidents\nplt.figure(figsize = (10, 6))\nprint(acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 2].groupby(['Month'])['COUNTER'].sum())\nax = acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 2].groupby(['Month'])['COUNTER'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 2].groupby(['Month'])['COUNTER'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4', 'C5'], title='Months have Severity 2 accidents', ax=ax)","37acd97c":"# Which months have most severe 3 accidents\nplt.figure(figsize = (10, 6))\n# print(acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 2].groupby(['Month'])['COUNTER'].sum())\nax = acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 3].groupby(['Month'])['COUNTER'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 3].groupby(['Month'])['COUNTER'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4', 'C5'], title='Months have Severity 3 accidents', ax=ax)","fbf533dd":"# Which months have most severe 4 accidents\nplt.figure(figsize = (10, 6))\n# print(acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 4].groupby(['Month'])['COUNTER'].sum())\nax = acc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 4].groupby(['Month'])['COUNTER'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_top_state.loc[acc_df_top_state[\"Severity\"] == 4].groupby(['Month'])['COUNTER'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4', 'C5'], title='Months have Severity 4 accidents', ax=ax)","1c1e1c26":"# Breakup of accidents by Severity and Year\nplt.figure(figsize = (16, 6))\nsby_df = acc_df_top_state.loc[acc_df_top_state[\"State\"] == \"CA\"].groupby(['Year', 'Severity', 'State'])['COUNTER'].sum().reset_index()\nsns.factorplot(\"Severity\", \"COUNTER\", col=\"Year\", data=sby_df, kind=\"bar\")","af8a56df":"# Which state has highest severity 4 ?\nplt.figure(figsize = (10, 5))\nacc_sev = acc_df[acc_df.Severity == 4][['State', 'Severity']]\nsns.countplot(x='State', data=acc_sev, order = acc_sev['State'].value_counts().iloc[:10].index)","9a270b9b":"# Which state has highest severity 3 ?\nplt.figure(figsize = (10, 5))\nacc_sev = acc_df[acc_df.Severity == 3][['State', 'Severity']]\nsns.countplot(x='State', data=acc_sev, order = acc_sev['State'].value_counts().iloc[:10].index)","ee774d9c":"# Which state has highest severity 2 ?\nplt.figure(figsize = (10, 5))\nacc_sev = acc_df[acc_df.Severity == 2][['State', 'Severity']]\nsns.countplot(x='State', data=acc_sev, order = acc_sev['State'].value_counts().iloc[:10].index)","50b820fd":"# Top 10 County having highest accidents\nplt.figure(figsize = (10, 5))\nsns.countplot(x='County', data=acc_df, order = acc_df['County'].value_counts().iloc[:10].index)","1fecf1f7":"# Top 10 Zip Codes having highest accidents\nplt.figure(figsize = (10, 5))\nsns.countplot(x='Zipcode', data=acc_df, order = acc_df['Zipcode'].value_counts().iloc[:10].index)","4f12b269":"# DescriptionShows natural language description of the accident.\nplt.figure(figsize=(10, 6))\ndesc = acc_df[\"Description\"].str.split(\"(\").str[0].value_counts().keys()\nwc_desc = WordCloud(scale=5,max_words=100,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(desc))\nplt.imshow(wc_desc,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Top 100 Accident Description\",color='b')\nplt.show()","cf37d14e":"# Distance(mi) by Severity The length of the road extent affected by the accident.\nacc_df[['Distance(mi)']].groupby(acc_df['Severity']).agg(['count', 'mean', 'median', 'min', 'max'])","34810c51":"# Which side of Lane has more accidents\nprint((acc_df['Side'].value_counts(normalize=True)*100).astype(int).astype(str) + '%')","c5d6820e":"#SideShows the relative side of the street (Right\/Left) in address field.\nside_df = acc_df.groupby(['Side', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Side\", data=side_df, kind=\"bar\")","cd6a1a17":"# Severity Impact by Temperature\nplt.figure(figsize = (16, 6))\nax = sns.violinplot(y=\"Temperature(F)\", x=\"Severity\", data=acc_df, palette=\"Set1\")","ca74e3fb":"# Severity Impact by Wind_Chill\nplt.figure(figsize = (16, 6))\nax = sns.violinplot(y=\"Wind_Chill(F)\", x=\"Severity\", data=acc_df, palette=\"Set2\")","53808fac":"# Severity Impact by Humidity\nplt.figure(figsize = (16, 6))\nax = sns.violinplot(y=\"Humidity(%)\", x=\"Severity\", data=acc_df, palette=\"Set3\")","4cd604f1":"# Severity Impact by Visibility\nplt.figure(figsize = (16, 6))\nax = sns.violinplot(y=\"Visibility(mi)\", x=\"Severity\", data=acc_df, palette=\"Set3\")","248f03b0":"# Severity Impact by Windspeed\nplt.figure(figsize = (16, 6))\nax = sns.violinplot(y=\"Wind_Speed(mph)\", x=\"Severity\", data=acc_df)","78aba7b6":"# Severity Impact by Precipitation\nplt.figure(figsize = (16, 6))\nax = sns.boxplot(y=\"Precipitation(in)\", x=\"Severity\", data=acc_df)","b1dea52d":"# Co-relation between Precipitation and Sverity\nacc_df[['Precipitation(in)', 'Severity']].corr()","18ae22c8":"# Weather_Condition\nacc_df['Weather_Condition'].unique().tolist()","a1abae33":"# Top 10 weather condition\nplt.figure(figsize = (15, 6))\nacc_df[acc_df['Weather_Condition'] != 0]['Weather_Condition'].value_counts().iloc[:10].plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4'], title='Top 10 Weather Condition')","50a46b6e":"# Weather_Condition Word Cloud\nfrom wordcloud import WordCloud\nweather_cond = acc_df[\"Weather_Condition\"].str.split(\"(\").str[0].value_counts().keys()\nwc = WordCloud(scale=5,max_words=15,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(weather_cond))\nplt.figure(figsize=(10,10))\nplt.imshow(wc,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Top 15 Weather conditions\",color='b')\nplt.show()","884ae990":"# Start_LatShows latitude in GPS coordinate of the start point.\n# Start_LngShows longitude in GPS coordinate of the start point.","07f81aba":"# Severity by Bump\nbump_df = acc_df.groupby(['Bump', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Bump\", data=bump_df, kind=\"bar\")","666f447f":"# Severity by Crossing\ncross_df = acc_df.groupby(['Crossing', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Crossing\", data=cross_df, kind=\"bar\")","ee589c14":"# Severity by Junction \njunc_df = acc_df.groupby(['Junction', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Junction\", data=junc_df, kind=\"bar\")","1e115d81":"# Severity by Roundabouts \nrndabt_df = acc_df.groupby(['Roundabout', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Roundabout\", data=rndabt_df, kind=\"bar\")","c1ed3940":"# Severity by Stop Sign \nstop_df = acc_df.groupby(['Stop', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Stop\", data=stop_df, kind=\"bar\")","19aaadd2":"# Severity by Traffic Signal \nstop_df = acc_df.groupby(['Traffic_Signal', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Traffic_Signal\", data=stop_df, kind=\"bar\")","28e7ebb2":"# Severity by Traffic Signal \nssnt_df = acc_df[acc_df['Sunrise_Sunset'] != 0].groupby(['Sunrise_Sunset', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Sunrise_Sunset\", data=ssnt_df, kind=\"bar\")","78636fe1":"# Severity by Civil Twilight\nssnt_df = acc_df[acc_df['Civil_Twilight'] != 0].groupby(['Civil_Twilight', 'Severity'])['Counter'].sum().reset_index()\nprint(ssnt_df)\nsns.factorplot(\"Severity\", \"Counter\", col=\"Civil_Twilight\", data=ssnt_df, kind=\"bar\")","c3e94e3c":"# Severity by Nautical_Twilight\nssnt_df = acc_df[acc_df['Nautical_Twilight'] != 0].groupby(['Nautical_Twilight', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Severity\", \"Counter\", col=\"Nautical_Twilight\", data=ssnt_df, kind=\"bar\")","9d4a326b":"# Let's zoom into start_time column to see what hours are more severe\nacc_df_ts = acc_df[['Start_Time', 'Severity', 'Counter']]\nacc_df_ts['Time'] = acc_df_ts['Start_Time'].map(lambda x: x.time().strftime('%H:%M'))\nacc_df_ts.head()","64300f20":"# Extracting hrs. from Start_time\nacc_df_ts['Hrs'] = acc_df_ts['Start_Time'].map(lambda x: x.time().strftime('%H'))\nacc_df_ts.head()","bf1ffc54":"# Plotting hrs against complete accidient list\nplt.figure(figsize = (10, 6))\nax = acc_df_ts.groupby(['Hrs'])['Counter'].sum().plot(kind='line', linestyle='-', marker='o', use_index=False)\nacc_df_ts.groupby(['Hrs'])['Counter'].sum().plot(kind='bar', color=['C0', 'C1', 'C2', 'C3', 'C4', 'C5'], title='Accidents by Hr', ax = ax)","7c457630":"# Severity by Hrs.\nplt.figure(figsize = (15, 10))\nacc_time_df = acc_df_ts[acc_df_ts['Severity']>1].groupby(['Hrs', 'Severity'])['Counter'].sum().reset_index()\nsns.factorplot(\"Hrs\", \"Counter\", col=\"Severity\", data=acc_time_df, kind=\"bar\")","348de574":"* Thank You for going thru the Kernel. If you liked it please feel free to upvote.\n* If you want to get connected, my linked profile - https:\/\/www.linkedin.com\/in\/jagannath-banerjee\/","6a60f344":"Which months have more Severe accidents ?\n* Severity 2 : Mar & Aug\n* Severity 3 : Mar, Aug & Nov\n* Severity 4 : Mar and Jan","6242e713":"* Daytime has more severe accidents.\n","dddb2408":"Observations:\n* Severity - 4 : Mean 1102 mins with std dev 21613 mins means, (1102+21613)\/60 = 378 hrs =  15 days. I doubt the data here. Rather we will use the median. It creates 360 mins or 6 hrs delays to clear a severity 4 accidents.\n* Severity - 3 : Median 29 mins . Min shows -31. Presence of Data issue (Start_Time > End_Time).\n* Severity - 2 : Median 40 mins. Min shows -31. Presence of Data issue (Start_Time > End_Time).\n* Severity -1  : Only 17 records. So difficult to reach any conclusion as sample size is statistically insignificant (min 30 samples required). ","7c39c1ff":"* Median temperature for all severity is at 50 to 60 degrees (F)\n* Severity 2, 3, 4 - Density is high around 0 and crosses -50. So mostly winters due to snow, ice etc\n* Severity 2, 3, 4 - Density is high across 60-70 degree. Its summer time and everyone is driving around. Higher chances of accidents.","8aed93eb":"We have 4 years worth of data. We will use the timeframe reference down the line","dc9a5416":"Can't determine from data if there is a clear co-relation between precipitation and Severity[](http:\/\/)","4b155887":"* Severe accidents happen with windchill 0 degree F in peak of winter.","323a401e":"* Most accidents happend in clear conditions.","eefef3bc":"* Severe accidents occur when humidity is around 80% ","0eb5a086":"* Accidents peak between 7am to 9am and between 4 pm to 5pm.","02ad709c":"* Since 2018 leads in accident, Severity data follows accordingly. Sever 2, 3 4 increased between 2016 to 2018.","8d818670":"* US drives on right. Right Side is obvious choice","61bd151e":"* Accidents mostly happend where there were no bumps","9dd5a4f6":"* 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay)\n* Severity 2 constitute the Majority","e251558c":"# TMC\nReference : https:\/\/en.wikipedia.org\/wiki\/Traffic_message_channel\n\nTraffic Message Channel (TMC) is a technology for delivering traffic and travel information to motor vehicle drivers. \nIt is digitally coded using the ALERT C or TPEG protocol into RDS Type 8A groups carried via conventional FM radio broadcasts.\nWhen data is integrated directly into a navigation system, traffic information can be used in the system's route calculation.","81db8c74":"* March, August, October and November has highest number of accidents","e716de8b":"California, Texas, Florida, North Carolina and New York recoded maximum number of accidents.\n\nLet's deep dive more into these states","4bd6db0a":"* Severe accidents occur when visibility is low (0-5 miles)","b05af857":"**** Missing Summary and Action :****\n* Precipitation has 88% missing data. Meaning, 88% of the days, it didn't rain. We will replace the missing values with 0.\n* Wind Chill has 83% missing data. So windchill was not presen during those days and will be replaced by 0.\n* End_Lat and End_Lng has 77% missing data. We will not use this field for analysis. We will use Start_Lat and Start_Lng intead.\n* Number - 65% Street numbers missing. These could be accidents on main highways (US Routes, Interstates etc). We will replace them with 0s. 0s will represent highways and others as Non highways.\n* TMC - 20% missing data. Traffic Message Channel (TMC) is a technology for delivering traffic and travel information to motor vehicle drivers. We will replace missing values with 0\n* Wind Speed - 20% missing. We will assume there is no wind and replace them with 0.\n* Others with 2-3% missing, replace with 0.","b9c32b78":"* We cannot clearly demarkate the impact of windspeed with severity\n* Windspeed ranges between 0 to 800. Highest winspeed ever recorded in US in 254 miles\/hr (https:\/\/www.thoughtco.com\/fast-wind-speed-recorded-3444498)","a60b7022":"* Severity 4  - Florida Leads followed by California and New York\n* Severity 3  - California Leads followed by Texas and Florida\n* Severity 2  - California followed by Texas and Florida\n* Severity 1  - California followed by Texas\n* Severity 0  - 10 records only. Sample size insufficient to conclude.","a209ac7b":"* About the data:\nThis is a countrywide traffic accident dataset, which covers 49 states of the United States. The data is continuously being collected from February 2016, using several data providers, including two APIs which provide streaming traffic event data. These APIs broadcast traffic events captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 2.25 million accident records in this dataset.\n                     \n* What questions are we trying to answer:\n    * Source of Information\n    * How Severe the accidents are\n    * Which County, zip codes recorded maximum accidents    \n    * Major Contributors for Severe accidents\n    * Which month\/year had how much accidents and if there id a trend\n    * Relationship of environmental factors contributing to the accident\n    * Which time of the day has most accidents\n    * Deep dive into bi-varite relationship of the columns with Severity","ca784a4d":"* Accidents Kept increasing from 2016 to 2018.\n* Accidents comparatively looks low in 2016 since data start from Mar 2016. Jan\/feb is missing.\n* Accident Sharply declined in 2019 since for 2019, we have data till Apr'19."}}