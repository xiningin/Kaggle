{"cell_type":{"4ba4abc1":"code","672c8c1d":"code","50858493":"code","5a5d937c":"code","02a2ed56":"code","862ef383":"code","7ddeab9a":"code","fa0a5902":"code","1430cf70":"code","6b4c288f":"code","32c2d7ed":"code","ec4cbced":"code","38111362":"code","81984170":"code","8c6b61ab":"code","0d79a57e":"code","dfc8bb59":"code","5b1bfc16":"code","1e0e6423":"code","33604b81":"code","28cfc790":"code","4e916d06":"code","383a1687":"code","70100bb1":"markdown","63c7ce38":"markdown","52b24ae9":"markdown","e4a81290":"markdown","5e90a108":"markdown","aef7209a":"markdown","bc8317f4":"markdown","3ae43949":"markdown","4210f9fe":"markdown","402e7481":"markdown","561111de":"markdown","b63381ea":"markdown","beaa495e":"markdown","113f9dc0":"markdown","e689c8e0":"markdown","e9efec8d":"markdown","5caec04e":"markdown","e085fd09":"markdown","0b040b2e":"markdown","bc935a64":"markdown","b256fe94":"markdown","b594ed2b":"markdown"},"source":{"4ba4abc1":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nkeras.backend.set_image_data_format('channels_last')","672c8c1d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50858493":"mnist_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nmnist_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ndisplay(\" train data\",mnist_train )\ndisplay(\" test data\",mnist_test )\n","5a5d937c":"image_size=28*28\nimage_size","02a2ed56":"# Convert to train and test data; Preserve original dataset\nX_train = mnist_train.drop('label', axis=1).copy()\nX_test = mnist_test.copy()\nY_train = mnist_train['label'].copy()","862ef383":"X_train.describe()","7ddeab9a":"# Normalize values\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","fa0a5902":"# Reshape to 28 x 28 so that we can see the image ie. handwritten number\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)","1430cf70":"\nimport random\nno_images=len(X_train)\n\n# Display random Image\nfig, ax = plt.subplots(figsize=(10, 10))\n\nplt.imshow(X_train[random.randint(0,no_images), :, :, 0], cmap='Greys', interpolation='nearest') \n\n# replace random.randint(0,no_images) in code above with a number if you want to see specific image. \n#This dispalys a random image each time\n\nplt.title(\"Sample Image\")\nplt.show()","6b4c288f":"\n# Display random Image\nfig, ax = plt.subplots(figsize=(2,2)) # now fix size is 2 x 2\n\nplt.imshow(X_train[random.randint(0,no_images), :, :, 0], cmap='Greys', interpolation='nearest')\nplt.title(\"Sample Image\")\nplt.show()","32c2d7ed":"# Split between train and validation set\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)","ec4cbced":"# Get one hot encoding\nY_train = keras.utils.to_categorical(Y_train, num_classes=10)\nY_val = keras.utils.to_categorical(Y_val, num_classes=10)","38111362":"# Build CNN Model\ndef CNN():\n    model = keras.Sequential()\n    # CONV > CONV > BN > RELU > MAXPOOLING > DROPOUT\n    model.add(layers.Conv2D(32, (3, 3), (1, 1), padding='valid', input_shape=(28, 28, 1), name='conv2d_1_1'))\n    model.add(layers.Conv2D(32, (3, 3), (1, 1), padding='same', name='conv2d_1_2'))\n    model.add(layers.BatchNormalization(name='bn_1'))\n    model.add(layers.Activation('relu', name='relu_1'))\n    model.add(layers.MaxPooling2D((2, 2), (2, 2), padding='valid', name='mp2d_1'))\n    model.add(layers.Dropout(0.2, name='drop_1'))\n    # CONV > CONV > BN > RELU > MAXPOOLING > DROPOUT\n    model.add(layers.Conv2D(64, (3, 3), (1, 1), padding='valid', name='conv2d_2_1'))\n    model.add(layers.Conv2D(64, (3, 3), (1, 1), padding='same', name='conv2d_2_2'))\n    model.add(layers.BatchNormalization(name='bn_2'))\n    model.add(layers.Activation('relu', name='relu_2'))\n    model.add(layers.MaxPooling2D((2, 2), (2, 2), padding='valid', name='mp2d_2'))\n    model.add(layers.Dropout(0.2, name='drop_2'))\n    # FLATTEN > DENSE > CLASSIFICATION\n    model.add(layers.Flatten())\n    model.add(layers.Dense(100, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n    \n    return model","81984170":"model = CNN()","8c6b61ab":"model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])","0d79a57e":"model.summary()","dfc8bb59":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=64, epochs=50, verbose=1)","5b1bfc16":"plt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc='lower right')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()","1e0e6423":"def predict(model, X, imgs):\n    s = int(np.sqrt(imgs))\n    fig, ax = plt.subplots(s, s, sharex=True, sharey=True, figsize=(15, 15))\n    ax = ax.flatten()\n    preds = model.predict(X[:imgs])\n    for i in range(imgs):\n        y_pred = np.argmax(preds[i])\n        img = X[i].reshape(28, 28)\n        ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n        ax[i].set_title(f'p: {y_pred}')","33604b81":"predict(model, X_test, 25)","28cfc790":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)","4e916d06":"name=\"Arushi\" #Add your name here\n\nfile_name=name+\"_mnist_submission.csv\"","383a1687":"y_pred = pd.Series(y_pred, name='Label')\nsub = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), y_pred], axis=1)\nsub.to_csv(file_name, index=False)","70100bb1":"# Get train and test data","63c7ce38":"### Data Conversion\nWe have a validation set of 20%","52b24ae9":"## Load the Data\n\n","e4a81290":"## Training and Prediction\n\nWe will train the model for 50 epochs, with a batch size of 64.","5e90a108":"You can try different models- Resnet or Efficientnet and see how they work!\n\nHere is a link to my model on multiclass classification of Human Protiens which was in the top 4% in the in-class competition\nThis uses CNN,  Resnet34,  Resnet50 and Resnet101 : [Human Protein Classification (top 4%) : PyTorch](https:\/\/www.kaggle.com\/kmldas\/human-protein-classification-top-4-pytorch)\n","aef7209a":"# Import Libraries","bc8317f4":"# Acknowledgement, Sources and Suggestions\n\n\n\n\nkernel by Chris: https:\/\/www.kaggle.com\/christianwallenwein\/beginners-guide-to-mnist-with-fast-ai\n\nkernel by Timothy: https:\/\/www.kaggle.com\/susantotm\/digit-recognizer\n\nkernel by Yassine: https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\n\ncan you use FASTAI to do this faster?\nhttps:\/\/www.fast.ai\n\nLook at my colab notenook on using 5 lines of code in FASTAI to get similar results! \nhttps:\/\/colab.research.google.com\/drive\/1tuKzXuWgYuJVa83k6NiL0GVKy_hyJJni?usp=sharing (link updated for viewing only- Pls copy to own colab\/download to run. This does not have edit access)\n \n \nTry out how to get the computer to generate MNIST data (deep fake numbers in handwritten format)\n\nLink: https:\/\/www.kaggle.com\/kmldas\/mnist-generative-adverserial-networks-in-pytorch\/\n\n","3ae43949":"You can make verbose=0, in the code above if you dod not  want to see each step in the process","4210f9fe":"# Read Directories & Folders","402e7481":"#  MNIST Classification\n\nMNIST is a Multiclass Classification project involving image recognition. We have to classify handwritten digits as 0 to 9.\n\n\nFurther reading:\nLinked this? Try out how to get the computer to generate MNIST data (deep fake numbers in handwritten format)\n\nLink: https:\/\/www.kaggle.com\/kmldas\/mnist-generative-adverserial-networks-in-pytorch\/\nNote this will be more advanced as it talks about GANs (generative adversrial networks) but good to know!\n","561111de":"### Graphing Accuracy\n\nLet us check how our model went by graphing accuracy with validation accuracy, and our training loss with validation loss.","b63381ea":"# What is 784?\n\nEach image is 28 pixels wide and 28 pixles long..... 28x28=","beaa495e":"##  Display images\n\nTo check whether everything worked as expected, let's take a look at a few images from each folder.","113f9dc0":"As you can see 20 epochs should have been fine. Not much improvement in scores after 20 epochs.\n\nWhat can you change to get better validation? Try our different parameter changes and see!!","e689c8e0":"### Predictions\n\nBelow is a function to help see whether we have trained the model properly. \"imgs\" is a parameter to see the first x number of images in our test dataset.","e9efec8d":"Why 255?\n\n1 byte of information = 8 bits. each bit has 2 values 0 or 1; the color intensity is 2^8=256 possible value; \ni.e. goes from 0 to 255\n\nby dividing by 255, we are making maximum value 1. Now black will be 1 and 0 is white; with various shades of grey in between","5caec04e":"Download the predictions from folder \"output\" \n\nrefresh  if not visible\n\nSUBMIT the predictions here: https:\/\/www.kaggle.com\/c\/digit-recognizer","e085fd09":"### Model Compilation\n\nHere we will use an Adam optimizer with a Cross entropy loss function.","0b040b2e":"This code gets you into the top 35% of the competition. \n\nWhat more can you do? try and experiment. Happy learning!\n\n\nLook below for other attempts and read more kernels on this competition","bc935a64":"Here 1 is black and 0 is white....after normalization\n\nSmaller size helps us visualise better?","b256fe94":"## Model Architecture\n\n\nDefining the Model (Convolutional Neural Network)\n\nThe 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel \u201cslides\u201d over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [SOURCE- read more click here](https:\/\/towardsdatascience.com\/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n\n![A standard convolution](https:\/\/miro.medium.com\/max\/535\/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n\n\nBelow is a function defining a CNN model with 3 main blocks of Convolutional layers. The first two blocks follow the same structure:\n1. Apply a Conv2D layer with 3x3 kernel size and valid padding, then another Conv2D layer with 3x3 kernel but with same padding to keep the same dimensions\n2. Apply a BatchNormalization layer to avoid layers being too depended from one another and allowing each activation to have 0 mean\n3. A RELU activation and a MaxPooling2D layer with 2x2 kernel size and stride=2\n4. An element-wise Dropout layer applied to MaxPooling2D keeping 80% of the activation units\n\nThe final 2 layers consists of a Fully Connected Layer that uses a Softmax activation for classification.","b594ed2b":"## Submission\n\nWe create the full prediction and place the predictions into the requested format."}}