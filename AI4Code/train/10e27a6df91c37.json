{"cell_type":{"2ff60a8f":"code","fc7cb710":"code","c53a6e80":"code","acb444a9":"code","64bb7d7b":"code","d5309ca2":"code","ab8c596a":"code","0309c6c9":"code","33b1385a":"code","60bc0184":"code","9b091e3d":"code","7043f70d":"code","d69ced76":"code","3467a051":"code","f0884c92":"code","e8268ac5":"code","454bd2de":"code","c720b8d5":"code","566ed167":"code","60334254":"code","8bb81255":"code","27df9dd0":"code","9d8488ee":"code","519db7bf":"code","adb98698":"code","c83fc748":"code","2233eac7":"code","9796121e":"code","2476ea2f":"code","8e2bdbe5":"code","5629373b":"code","22cc9103":"code","6de73017":"code","9130a254":"code","0d4c3a5e":"code","6c4c9494":"code","abef2a71":"code","5fcab018":"code","2609a847":"code","cccf530e":"code","11d3d28b":"code","ff14e343":"code","80189a85":"code","3092d2fc":"code","21c9b33c":"code","e372f985":"code","81b2911b":"code","87a22b3f":"code","5dd5d7ea":"code","c812f8c1":"code","ee3caed0":"code","bf5d7773":"code","f4581e89":"code","4de55548":"code","79189542":"code","078c6088":"markdown","a20da49e":"markdown","5018c238":"markdown","cd220e96":"markdown","72ea85c9":"markdown"},"source":{"2ff60a8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import chi2\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc7cb710":"import dask\nimport dask.dataframe as dd","c53a6e80":"path ='\/kaggle\/input\/ashrae-energy-prediction\/'\nbuilding_meta=dd.read_csv(f'{path}building_metadata.csv')\nweather_tr=dd.read_csv(f'{path}weather_train.csv')                         \ntest=dd.read_csv(f'{path}test.csv')\ntrain=dd.read_csv(f'{path}train.csv')\nweather_tst=dd.read_csv(f'{path}weather_test.csv')","acb444a9":"train.head()","64bb7d7b":"# del building_meta, weather_tr, train, test, weather_tst","d5309ca2":"train.head()","ab8c596a":"print(building_meta.head())\nprint(building_meta.shape)\nprint(train.shape)\nprint(test.shape)","0309c6c9":"## Memory optimization\n\n# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","33b1385a":"# train=reduce_mem_usage(train,use_float16=True)\n# weather_tr=reduce_mem_usage(weather_tr,use_float16=True)\n# building_meta=reduce_mem_usage(building_meta,use_float16=True)","60bc0184":"# sample_train_nj = train.sample(frac=.1) # sample nj: not joined\n# sample_train_nj.shape","9b091e3d":"# sample_train_nj.head()","7043f70d":"train_merge = train.merge(building_meta , on='building_id' , how ='left')\ntrain_merge = train_merge.merge(weather_tr, on=['site_id', 'timestamp'], how='left')\ntrain_merge.head()","d69ced76":"# most building in the sample has null values in floor count and year built so we can remove them \nprint('features with null values:\\n' ,train_merge.isnull().sum() *100 \/train_merge.shape[0])","3467a051":"## note: every building may have multiple meters(0: electricity, 1: chilledwater, 2: steam, 3: hotwater) \n","f0884c92":"from sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import LabelEncoder","e8268ac5":"le = LabelEncoder()\ntrain_merge['primary_use'] = le.fit_transform(train_merge['primary_use'])\ntrain_merged_anova = train_merge.sample(frac = 0.1)\nprint(le.classes_)\ntrain_merged_anova.head()","454bd2de":"train_merged_anova_nona = train_merged_anova.dropna()\ntrain_merged_anova_nona.head()","c720b8d5":"train_merged_anova_nona.min()","566ed167":"train_merged_anova_nona.dew_temperature += 9.4\ntrain_merged_anova_nona.precip_depth_1_hr += 1","60334254":"f_classif(train_merged_anova_nona.meter.values.reshape((-1,1)), train_merged_anova_nona.meter_reading.values.reshape((-1)))","8bb81255":"f_classif(train_merged_anova_nona.primary_use.values.reshape((-1,1)), train_merged_anova_nona.meter_reading.values.reshape((-1)))","27df9dd0":"f_classif(train_merged_anova_nona.floor_count.values.reshape((-1,1)), train_merged_anova_nona.meter_reading.values.reshape((-1)))","9d8488ee":"train_merge['timestamp'] = dd.to_datetime(train_merge['timestamp'] )","519db7bf":"fig, axes = plt.subplots(1, 1, figsize=(14, 6), dpi=100)\ntrain_merge[['timestamp', 'meter_reading']].set_index('timestamp').resample('H').mean()['meter_reading'].plot(ax=axes, label='By hour', alpha=0.8).set_ylabel('Meter reading', fontsize=14);\ntrain_merge[['timestamp', 'meter_reading']].set_index('timestamp').resample('D').mean()['meter_reading'].plot(ax=axes, label='By day', alpha=1).set_ylabel('Meter reading', fontsize=14);\naxes.set_title('Mean Meter reading by hour and day', fontsize=16);\naxes.legend();","adb98698":"train_merge.groupby('meter').meter_reading.agg(['mean', 'std', 'median'])","c83fc748":"weather_tr.dropna().groupby('site_id').mean()","2233eac7":"columns = weather_tr.drop(['timestamp', 'site_id'], axis =1).columns\nweather_tr_scaled = weather_tr.copy()\nweather_tr_scaled[columns] = MinMaxScaler().fit_transform(weather_tr_scaled[columns])","9796121e":"weather_tr_scaled.groupby('site_id').boxplot(column=list(columns),\n                                             layout=(-1,1), sharex=False,\n                                             fontsize=13, figsize=(15,50))\nplt.tight_layout()","2476ea2f":"\ncorr = train_merge.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, annot = True)","8e2bdbe5":"train_merge_reduce=reduce_mem_usage(train_merge,use_float16=True)","5629373b":"columns = train_merge.drop(['building_id', 'timestamp','site_id',], axis=1).columns\ncolumns = list(columns)\ncolumns","22cc9103":"round(len(columns)\/2)","6de73017":"del train_merge, train","9130a254":"train_merge_reduce.head()","0d4c3a5e":"\nplt.figure(figsize=(15, 20))\nfor i, column in enumerate(columns):\n    ax = plt.subplot(round(len(columns)\/2) +1, 2, i+1)\n    train_merge.plot(ax=ax, kind = 'scatter', x = column, y = 'meter_reading')\n    \nplt.tight_layout()","6c4c9494":"train_merge.groupby('building_id').air_temperature.agg(['mean', 'std', 'min', 'max']).compute()","abef2a71":"train_merge_reduce.head()","5fcab018":"train_merge['month'] = train_merge.timestamp.map(lambda x:x.month)\ntrain_merge['day'] = train_merge.timestamp.map(lambda x:x.day)\ntrain_merge['hour'] = train_merge.timestamp.map(lambda x:x.hour)","2609a847":"train_merge_reduce.tail()","cccf530e":"train_merge.groupby(['month','day']).air_temperature.agg(['mean', 'std', 'min', 'max']).compute()","11d3d28b":"del train, train_merge","ff14e343":"test.head()","80189a85":"test_merge = test.merge(building_meta , on='building_id' , how ='left')\ntest_merge = test_merge.merge(weather_tst, on=['site_id', 'timestamp'], how='left')\ntest_merge.head()","3092d2fc":"test_merge = reduce_mem_usage(test_merge)","21c9b33c":"import gc\n","e372f985":"gc.collect()","81b2911b":"del weather_tr, weather_tst,test","87a22b3f":"test_merge['timestamp'] = dd.to_datetime(test_merge['timestamp'] )","5dd5d7ea":"test_merge['month'] = test_merge.timestamp.map(lambda x:x.month)\ntest_merge['day'] = test_merge.timestamp.map(lambda x:x.day)\ntest_merge['hour'] = test_merge.timestamp.map(lambda x:x.hour)","c812f8c1":"train_merge_reduce.head()","ee3caed0":"train_merge_reduce.head()","bf5d7773":"def baseline_predict(sample):\n    #find the mean of rows in train data where month&day&hour is same as given sample\n    return train_merge[(train_merge.building_id==sample.building_id) & (train_merge.month==sample.month) & (train_merge.day==sample.day) & (train_merge.hour==sample.hour)].meter_reading.mean()","f4581e89":"test_merge['pred_meter_reading'] = test_merge.apply(baseline_predict, axis = 1)","4de55548":"baseline_predict(test_merge.iloc[4000,:])","79189542":"train_merge.loc[0, :].compute()","078c6088":"## Anova\n\nH0: feature and target are uncorrelated\n\nH1: feature and target are correlated\nf_classif result is [f_value, p_value]. p_value<0.05 rejects the null hypothesis(H0) and means that feature and target are correlated","a20da49e":"## Feature engineering\n* mean & std, Min & Max of air_temperature of sites\n","5018c238":"## Creating a Baseline.\nBaseline: mean of meter_reading in previus years","cd220e96":"## relation of different parameters on meter reading","72ea85c9":"![image.png](attachment:image.png)"}}